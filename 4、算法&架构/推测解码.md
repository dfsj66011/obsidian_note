
## 背景

LLMs，如 GPT-4、Llama 和 T5，在自然语言理解、推理和生成任务中展现出卓越的能力。这些模型通常采用下一个 token 预测（NTP）范式进行训练——即在给定一系列 tokens 的情况下，模型被优化以预测下一个 token。尽管这种方法有效，但也带来了效率低下的问题，尤其是在推理过程中。

**为什么推理这么慢？**

在自回归 LLM 中，生成 K 个 tokens 的序列需要进行 K 次顺序前向传播。每一步都利用模型的预测来计算下一个输入。这种串行特性成为生成过程中的瓶颈，因为每个 token 都依赖于前一个 token，从而无法实现并行化。

内存带宽瓶颈加剧了这一问题：在生成每个 token 的步骤中，模型参数都必须从内存加载到 accelerator。随着模型规模的扩大（例如参数达到数千亿），推理的计算和内存成本也随之增加。

**推测解码的动机：**

为了解决自回归 LLM 中的推理瓶颈问题，推测性解码已成为一类旨在加速生成而不以统计学显著方式改变模型输出分布的策略。与逐词预测不同，这类方法尝试并行推测多个未来 tokens 并进行高效验证。这一核心思想可通过多种方式实现，以下列举三种最常见方法：

1. ​**通过草稿模型进行推测性解码**​：一个更小、更快的“草稿模型”在主模型之前生成一系列候选 tokens。然后，主模型并行验证这些 tokens，接受正确的预测，仅在检测到不匹配时回退到标准的自回归解码。这种方法在实际应用中可实现 2 倍至 6 倍的加速。
2. **基于树的多头验证（Medusa）**： 无需使用单独的草稿模型，主模型配备了多个并行的“验证头”，这些验证头以树状结构生成并检查替代 token 路径。这种方法能够同时探索多个候选延续路径，从而减少所需的全前向传播次数。
3. **多 tokens 预测头（自推测解码）​**​：该模型通过增加额外的输出头进行增强，这些输出头经过训练可直接从当前隐藏状态预测多个未来 tokens。这些预测在内部进行验证，使模型能够绕过多个顺序步骤，而无需引入外部模型或分支搜索结构。

在这些方法中，核心原则始终如一：在每次前向传播中执行更多推测性工作，然后并行验证正确性，从而缓解下一个 token 生成严格的串行特性。

**示例场景：**

假设我们要生成 5 个 tokens。传统的解码方法需要进行 5 次前向传播。而使用推测性解码时，草稿模型可能一次性提出所有 5 个 tokens。然后目标模型会验证这一批 tokens——根据需要、接受或修正。例如，如果有 4 个 tokens 被接受，则只需要进行 2 次前向传播，从而实现 2.5 倍的加速。

这一优化特别适用于：

* 实时应用（聊天机器人、代码补全）
* 边缘部署场景
* 高负载服务器环境

下图（[来源](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)）直观展示了非推测性生成（左侧）与推测性生成（右侧）的对比。

![](https://pytorch.org/wp-content/uploads/2024/11/fig1.gif)

## 核心技术

推测式解码有多种实现方式，但其核心理念是一致的：使用轻量级方法预测若干 tokens，再通过原始（或“目标”）模型进行验证。接下来我们将探讨主要策略、实现模式及其权衡取舍。

### 通过草稿模型进行推测解码

由 Leviathan 等人（2023年）在 [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)  中提出。

**流程概述：**

1. 草拟：使用较小（更快）的模型生成 $γ$ 个推测性 tokens。
2. 验证：运行大模型对所有 tokens 进行评分，直至 $γ$。
3. 接受：接受与大模型最高预测相匹配的前缀 tokens。
4. 回退：如果 tokens 出现分歧，则回退到大模型采样进行纠正。

论文中的下图展示了无条件语言建模案例中所阐释的技术。每条线代表算法的一次迭代。绿色标记表示近似模型（此处为一个拥有 600 万参数、在 1m1b 数据集上训练并处理 8k tokens 的类 GPT Transformer 解码器）提出的建议被目标模型（此处为相同设置下拥有 9700万 参数的类 GPT Transformer 解码器）采纳，而红色和蓝色标记分别表示被拒绝的建议及其修正。例如，在第一行中，目标模型仅运行了一次，并生成了 5 个 tokens。
    

![](https://aman.ai/images/papers/SD.jpg)

**算法摘要**（由 Leviathan 等人简化）：

```python
def speculative_decode(draft_model, target_model, prompt, gamma):
      draft_tokens = draft_model.generate(prompt, max_new_tokens=gamma)
      scores = target_model.score(prompt + draft_tokens)
        
      # accept up to the first mismatch
      n_accept = count_agreement(draft_tokens, scores)
      accepted = draft_tokens[:n_accept]
        
      # complete the next token from the target model
      next_token = target_model.sample(prompt + accepted)
      return accepted + [next_token]
```

**优势：** 无需重新训练即可插入现有模型；无需对大模型进行架构更改；完全保留输出分布。
**挑战：** 维护单独的草稿模型会增加系统复杂性；草稿模型与目标模型之间的分布不匹配可能会降低接受率；如果两个模型都很大，会增加内存和计算压力。

#### 情景假设

当前上下文： $x_1, x_2, x_3, x_4, x_5$，我们希望预测下一个 token。

**第 1 步：** 小模型（draft model）猜测，小模型（轻量、快）一次生成多个 tokens：$x_6, x_7, x_8$

所以草稿序列是：$\text{draft: } [x_6, x_7, x_8]$

**第 2 步**：大模型并行验证，让大模型并行计算每一步对应的真实条件概率：

1. $P_T(x_6 \mid x_1, x_2, x_3, x_4, x_5)$
2. $P_T(x_7 \mid x_1, x_2, x_3, x_4, x_5, x_6)$
3. $P_T(x_8 \mid x_1, \ldots, x_7)$

这些可以 **并行计算**，因为我们可以一次性把 `[x1..x8]` 丢进大模型，输出 logits，然后切片取出每个位置对应的 token 概率。

**第 3 步：** 接受或拒绝，对于每个 token，计算大模型和小模型的概率比值：$$
a_i = \min\left(1, \frac{P_T(x_i \mid \text{context})}{P_D(x_i \mid \text{context})}\right)$$

然后进行采样（或直接比较阈值）：

- 如果 $a_i$ 足够大（例如 0.8, 0.9 等），接受；
- 如果太小，拒绝，并从这一点重新采样。
    
举例说明，假设：

| token | 大模型概率 ($P_T$) | 小模型概率 ($P_D$) | 比例 ($P_T / P_D$) | 结果             |
| ----- | ------------- | ------------- | ---------------- | -------------- |
| x₆    | 0.25          | 0.22          | 1.13             | ✅ 接受           |
| x₇    | 0.02          | 0.15          | 0.13             | ❌ 拒绝           |
| x₈    | —             | —             | —                | 🚫 不检查（因之前已拒绝） |

于是：接受 $x_6$，在 $x_7$ 处拒绝并重新采样一个新的 $x_7'$。最终生成序列为：$[x_1, \ldots, x_6, x_7']$ 

之后可以再继续进行下一轮 speculative decoding（让小模型从这里再猜几步）。


### 基于树的多头验证 (Medusa)

Medusa（由 Cai 等人在 2024 年发表的论文 [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)  中提出）通过采用树状注意力机制，对基础的多重推测解码方法进行了优化改进。

<img src="https://lh3.googleusercontent.com/sitesv/AAzXCkfNF6u97oW1AoGOMyD027S0hnyNnAlz7SEoABrvBhz-r3KPwS7WdTSOg9bzj8LrsuR_PHAFABhVTggZmw4bENGJKHeevxFJAvTpD8jmcZaciKzkHPwNG9belq2Be7qPVjylW_XEVVttPbki7zaSDnxTUzBAXu0VS4TF9k02soDCIXPd6RjpJ6MzOlXVVh02IQCbY5DHg0CFsV3xKAdn0n4shCoDU1EKGOMoiQw=w1280" width="500">


**核心特性：**

* Medusa Heads：每个头基于最后的隐藏状态预测 $k+1$ 个未来 token。
* 候选组合：每个头输出的前 $k$ 个结果被合并以形成推测树。
* 树注意力：自定义注意力掩码确保 token 仅关注其路径上的前驱。
* 接受方案：两种选择：
	* 拒绝采样（与基础模型匹配）
	* 典型接受（启发式，更快）

- 优势：
    - 在质量下降最小的情况下实现更高的加速（生产环境中约 2.3-2.8 倍）。
    - 无需重新训练即可轻松集成到现有模型中（Medusa-1），或通过联合训练实现（Medusa-2）。
    - 适用于批处理大小为 1 的场景，符合实际应用需求（如聊天）。

#### 情景假设

假设当前上下文（已有生成的 token）为：$x_1, x_2, x_3, x_4, x_5$

**步骤 1：** 进入模型 → 主模型隐藏状态，将上下文输入主模型（Transformer backbone），得到最后一层隐藏状态 $h_t$​（$t = 5$ 的状态）。

**步骤 2**：多个 Medusa heads 并行预测，在这个隐藏状态基础上，新增 $K$ 个 heads（例如 $K = 3$）同时做预测：

- Head 1 用 $h_t$​ 预测候选 token(s) for position $t+1$（即 $x_6$）
- Head 2 用 $h_t$ 预测候选 token(s) for position $t+2$（即 $x_7$）
- Head 3 用 $h_t$ 预测候选 token(s) for position $t+3$（即 $x_8$）

（每个 head 可以输出 Top-k 候选，而非只是一个）

**步骤 3**：构建候选序列树（Tree of paths），因为每 head 有多个候选，组合就形成一个 **树结构**：从 head1 的候选 → head2 的候选 → head3 的候选。这个策略允许探索多个 tokens 未来路径。

**步骤 4**：验证候选序列，对于每条候选路径（例如 $x_6 → x_7 → x_8$ …），使用主模型进行验证（即用原模型计算这些 tokens 在相应位置的条件概率），判断是否可以接受一整个前缀。即：如果候选路径前两步可信，就一次接受这两步，跳过后续串行。

**步骤 5**：接受与推进，选出“最长可信前缀链”——例如有的链前三个可信，有的前 4 个可信等，如图所示；

**步骤 6**：继续下一轮





### 多 tokens 预测头（deepseek 采用模式）

由 Gloeckle等人（2024年）在 [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737) 一文中提出。

最近的一个趋势是不使用单独的草稿模型，而是直接在主模型中构建推测能力。这就是多 tokens 预测头的作用所在。

（具体内容略，详见 Deepseek 笔记内容）

**优势：** 无需单独的草稿模型；统一架构（更易于部署、量化和训练）；兼容推测性解码方法，如块并行或 Medusa。

**缺点：** 需要在预训练阶段修改模型；只有在大规模模型（7B 参数以上）上才能显现效果；微调这些模型时需谨慎，以保持对齐性。

## 比较分析

在本节中，我们将系统比较前文讨论的关键推测解码策略——基于草稿模型的解码、多 token 预测头和 Medusa 方法。我们将从性能表现、集成便捷性、训练要求及部署复杂度等维度展开权衡分析。

| 标准       | 草稿模型(Leviathan et al., Nov 2022) | Medusa 树注意力(Cai et al., Jan 2024) | 多 token 预测头(Gloeckle et al., Apr 2024)          |
| -------- | -------------------------------- | --------------------------------- | ----------------------------------------------- |
| 需要修改模型   | 不需要                              | 可选（Medusa-1）/联合（Medusa-2）         | 是（需要在预训练期间修改输出头）                                |
| 训练成本     | 低（可以使用现成的模型作为草稿和目标模型）            | 适度（微调额外头）                         | 高（需要预训练）                                        |
| 推理加速（实测） | ∼2×–3×                           | ∼2.2×–3.6× (typically 2.3×–2.8×)  | ∼3× (4‑token), up to ∼6× (8‑token draft window) |
| 输出质量     | 与基础模型相同                          | 高（拒绝 + 典型接受方案）                    | 匹配下一个 head                                      |
| 部署简便     | 中等（双模型系统）                        | 高（单模型带额外头）                        | 高（如果是从预训练集成而来的单一模型）                             |
| 内存开销（训练） | 高（两种模型状态 / KV缓存）                 | 低（单主干 + 小头层）                      | 高效（峰值内存为O(V+d)）                                 |
| 批量友好性    | 高                                | 针对批次大小=1进行优化                      | 高                                               |
| 实施成熟度    | 自 2022 年起广泛使用（T5、GPT）            | 早期采用如 Vicuna、Zephyr等              | DeepSeek V3                                     |

### 何时使用每种技术

* 草案模型：
	* 当您无法修改或重新训练基础模型时，这是理想的选择。
	* 适用于遗留系统或商业API。
	* 提供“即插即用”的推理加速，集成开销极低。
	* 当已经有一个强大而紧凑的草稿模型时效果最佳。

- Medusa：
	- 非常适合单用户交互场景（如聊天机器人）。
	- 通过 Medusa-1（冻结主干）或 Medusa-2（联合微调）提供精细控制。
	- 引入树注意力机制来优化推测性 token 验证。
	- 当输出多样性或控制是关键时，能够超越他人。
    
- 多 token 预测头：
	- 建议在完整模型预训练期间使用。
	- 最适合从头开始或大规模训练模型的机构。
	- 以最小的架构占用实现自我推测解码。
	- 对于较长的输入或批量解码工作负载非常高效。


### 关键要点

速度与简洁性：基于草稿的方法更简单，但长期效率较低。集成头解锁了更好的扩展性。
训练预算：如果是从零开始训练，建议投资多 token 或 Medusa 头。
服务限制：对于分布式服务或边缘部署，Medusa-1 或下一 head 头提供了简洁的集成方案。

## 实现深度剖析：如何构建推测解码器

本节重点介绍推测解码的具体实现细节。我们将涵盖三种主要方法的架构布局、核心训练流程、内存优化技巧以及参考代码模式。

### 基于模型的推测解码草案

该方法涉及使用两个模型：

* 目标模型​：大型、精准的 LLM，其输出必须得到保留
* 草稿模型：一种经过训练的小型模型，用于近似目标模型的预测结果。

 **架构概述：**

论文中的下图展示了基于草稿模型的推测解码工作流程：提议、并行验证和选择性接受。在无条件语言建模的情况下，每一行代表算法的一次迭代。绿色 token 是近似模型（此处为一个在 lm1b 数据集上训练、具有 8k  tokens 的 6M 参数 GPT 式 Transformer 解码器）提出的建议，被目标模型（此处为相同设置下 97M 参数的 GPT 式 Transformer 解码器）接受；而红色和蓝色标记分别表示被拒绝的建议及其修正。例如，第一行中目标模型仅运行一次，生成了 5 个 tokens。

![](https://aman.ai/images/papers/SD.jpg)

每个解码步骤按如下方式进行：
1. 使用草稿模型生成 $γ$ 个 tokens 的推测前缀（例如，$γ=4$）。
2. 并行运行目标模型以验证每个 token。
3. 接受匹配的 token；拒绝不匹配的 token，并从中断处恢复标准解码。

**关键实施要素：**

1. 推测性采样​：使用拒绝采样来确保分布等价性：

```python
def accept_token(p_large, p_draft, x):
    if p_draft[x] <= p_large[x]:
        return True
    else:
        accept_prob = p_large[x] / p_draft[x]
        return random.random() < accept_prob
```

解释：
- `p_large[x]` 表示大模型（target model）对 token $x$ 的概率 $P_T(x)$    
- `p_draft[x]` 表示草稿模型（draft model）对 token $x$ 的概率 $P_D(x)$
- 函数返回 `True` 表示接受小模型的这个 token，`False` 表示拒绝（并需要重新采样）

*为什么需要“拒绝采样”*，在 Speculative Decoding 里：小模型先生成若干 token，大模型再检查这些 token 是否“合理”。但是我们希望最终生成的 token 严格服从大模型的分布 $P_T(x)$，而不是小模型的分布 $P_D(x)$。于是必须设计一个机制来“校正”小模型的输出分布，使得最终被接受的 token 集合依然符合 $P_T(x)$。这个机制就是拒绝采样（rejection sampling）。

*重要性采样 / 拒绝采样思想*，一般思路是：我们想从目标分布 $P_T(x)$ 采样，但只能从提议分布 $P_D(x)$ 中采样。于是我们：

1. 从 $P_D(x)$ 采一个样本 $x$
2. 以概率 $\frac{P_T(x)}{c P_D(x)}$​ 接受它（其中 $c \ge \max_x \frac{P_T(x)}{P_D(x)}$）
3. 若被拒绝，则重新采样

这样被接受的样本的分布就会与 $P_T$​ 相同。Speculative Decoding 做了一个简化的变体。

*代码逐行解释*：

* 第 1 行：如果草稿模型认为这个 token 的概率不超过大模型的概率，说明草稿“保守”了，大模型甚至更支持这个 token。  → 直接接受，无需进一步随机采样。（这种情况不会破坏目标分布一致性）
* 第 2–4 行：如果草稿模型“太自信”——即它给这个 token 的概率比大模型更高，那我们要“惩罚”它一点，用一个随机过程以概率 $a = \frac{P_T(x)}{P_D(x)}$ 来接受它。例如：大模型认为某个词概率 0.1，小模型认为 0.2 → 接受概率 0.1/0.2 = 0.5。说明小模型太激进，这个 token 只能以一半概率被接受，这就是重要性采样修正权重。

*为什么这样能保证分布等价*，核心数学结论是：被接受的 token 的分布 ∝ $P_D(x) \times \min(1, \frac{P_T(x)}{P_D(x)})$，直观地说：

- 如果小模型低估了 token 概率（$P_D < P_T$​），我们总是接受；
- 如果小模型高估了 token 概率（$P_D > P_T$），我们以比例 $\frac{P_T}{P_D}$​​ 接受；
- 这样一来，整体接受概率的期望值就是大模型的概率 $P_T$。

因此，最终保留的样本分布与大模型一致，这就是“使用拒绝采样来确保分布等价性”。

2. 并行验证：运行目标模型的 $γ + 1$ 次并行前向传递：

```python
with torch.no_grad():
    logits = target_model(prefix + draft_tokens)
    verified_probs = softmax(logits)
```

3. 回退修正：如果某个 token 被拒绝，则从调整后的分布中重新采样：

```python
residual = torch.clamp(p_large - p_draft, min=0)
residual /= residual.sum()
next_token = torch.multinomial(residual, num_samples=1)
```


这是当草稿 token 被拒绝时的补救机制。假设草稿模型的 token $x_i$​ 被拒绝。那说明小模型在该位置“预测过头”——  我们需要重新从大模型的真实分布 $P_T$​ 中抽样一个 token。但是我们希望尽可能复用小模型计算结果，而不是完全重跑。于是我们构造了一个 “残差分布 (residual distribution)。

代码第 1 行，对大模型分布与草稿分布做差：$r(x) = \max(0,\, P_T(x) - P_D(x))$ 这表示，只保留大模型认为“更高概率”的部分，把小模型“多估”的区域剪掉（clamp 到 0）。直观理解：在草稿分布之外，大模型还有哪些 token 是更合理的？” 我们只从这部分区域重新采样。这就是 “residual” 的含义：大模型尚未被草稿覆盖的剩余概率质量。

代码第 2 行，对残差重新归一化，得到一个有效的概率分布：$\tilde{P}(x) = \frac{r(x)}{\sum_x r(x)}$，这样我们就可以在这部分 token 空间上采样。

代码第 3 行，从归一化的残差分布中抽取一个 token；这就是新的「被拒绝位置」的 token；它保证：不再偏向小模型；与大模型的概率分布一致；避免重复计算大模型 softmax。

具体例子：大模型分布 ABCD 分别是 [0.3, 0.2, 0.1, 0.4] 草稿模型分布 [0.25, 0.35, 0.1, 0.3]，残差为 [0.05, 0, 0, 0.1]，归一化后为 AD [0.33, 0.67] 这样如果从 B 的位置被拒绝，就会在 AD 中采样，可能采到 D。

*为什么要这样设计？* 如果我们直接重新从 $P_T$​ 采样整个分布，就浪费了草稿信息；但如果完全信任草稿模型，就会偏离目标分布。所以残差分布是一种折中方案：“利用草稿模型的结果，但只在它‘不自信’的部分让大模型纠正。”  数学上可以证明（论文中的推导）：在接受/拒绝 + 残差重采样后，最终采样分布仍等价于 $P_T$。

**优化提示**：缓存重复使用前缀的激活值，避免冗余计算。


### Medusa: 树注意力 + 并行头

美杜莎通过一种新颖的注意力机制扩展了多 token 解码功能，该机制能够同时验证多条推测路径。

**架构概述：** 论文中的下图展示了 Medusa 提出的树形注意力机制：来自多头机制的并行候选形成多个分支，这些分支可同时进行验证。
   ![|500](https://aman.ai/primers/ai/assets/speculative-decoding/Medusa_treeattn.jpg)
从最后一个隐藏状态投影出多个轻量级美杜莎头。每个头都会在未来的位置（$t+1$、$t+2$、…、$t+K$）提出 token。树状注意力掩码控制信息流动以确保正确性。

1. Medusa 头定义：

```python
def medusa_head(h_t, W1_k, W2_k):
    ff_out = F.silu(W1_k @ h_t) + h_t
    return softmax(W2_k @ ff_out)
```

$W_{1_k}$ 初始化为零，$W_{2_k}$ 从 LM 头克隆而来。

**解释：** 不同于普通 LM 头（只预测下一个 token），Medusa head 能直接从当前隐藏状态 $h_t$​ 直接预测更远的 token。

代码第 1 行，这相当于在当前隐藏状态上做一次轻量的非线性变换：$\tilde{h}_t^{(k)} = \text{silu}(W_{1k} h_t) + h_t$.  `F.silu` 是激活函数（Sigmoid Linear Unit），定义为：$\text{silu}(x) = x \cdot \sigma(x)$，它常用于现代 Transformer（比 ReLU 平滑）。这一步的效果是：引入一个轻微的非线性扰动；同时保持原隐藏状态（通过 `+ h_t` 残差连接）；使每个 Medusa head 能够学习到“看得更远”的方向，而不破坏基础语义

代码第 2 行，这与普通语言模型头的最后一步相同：

*此外，$W_{1_k}$ 初始化为零，$W_{2_k}$ 从 LM 头克隆而来的原因*：

$W_{2_k} \leftarrow W_{\text{LM}}$ （克隆主 LM 头）：LM 头 $W_{\text{LM}}$​ 是训练好、稳定的；克隆它能让新 head 一开始就拥有合理的词汇投影；避免重新学习词汇分布；减少训练不稳定性。

$W_{1_k} = 0$（零初始化）这样一来，初始时 $\tilde{h}_t^{(k)} = \text{silu}(0) + h_t = h_t$；各个 Medusa head 的初始输出等价于原本的 LM 头输出；所以模型初始不会被扰乱；随着训练，$W_{1k}$​ 逐渐学会如何从当前状态推测更远 token；换句话说：从“预测下一个”慢慢学到“预测下两个、下三个”……这是一种非常聪明的 “渐进式偏离” 设计：开始时完全不影响原模型，训练时逐步让头学会预判未来。

2. 树注意力实现：构建每个头的 top-k 预测的笛卡尔积。使用仅允许分支内通信的注意力掩码。修改基于树的候选验证的位置编码。

3. 候选验证：

```python
# Assume 2 heads with top-2 and top-3 predictions
# Generate 6 branches, verify each in parallel
mask = build_tree_attention_mask(branch_structure)
attention_output = transformer_with_mask(input_ids, mask)
```

4. 接纳策略：拒绝采样确保准确性。典型接纳（基于与目标偏差的启发式截断）提高速度。

*策略 1：精确型（严格拒绝采样）*  这是理论上“保证与原模型分布一致”的做法。原理是对每个 token 使用类似 speculative decoding 的接受判定，在路径上逐步进行：如果当前 token 被接受，继续；一旦某个 token 被拒绝，该路径终止。最终结果：被接受的最长前缀作为当前可采纳输出。其概率分布严格等价于目标模型的真实分布，例如下表：

|路径|Token₁ 判定|Token₂ 判定|最终结果|
|---|---|---|---|
|(A,C)|✅|❌|接受 A|
|(A,D)|✅|✅|接受 A D|
|(B,E)|❌|—|拒绝 B|

最终选择路径 (A,D)。这种方式最精确，但每步都要计算并比较概率 → 稍慢。

*策略 2：启发式型（典型接纳策略）* Medusa 在实际推理中常采用这种更高效的启发式选择，论文里称为 Heuristic Acceptance 或 Top-Path Acceptance。核心思想：

1. 计算每条路径在大模型验证下的总 log-prob：$s_{\text{path}} = \sum_i \log P_T(x_{t+i} | x_{1:t+i-1})$
2. 选取得分最高的路径（或 top-k）；
3. 若该路径的平均概率高于阈值（或低于偏差界 $ε$），就一次性接受整条路径；
4. 否则仅接受部分前缀（或回退一步重新预测）。

|路径|log Pₜ 总分|平均 Pₜ|是否接受|
|---|---|---|---|
|(A,C)|–2.1|0.12|否|
|(A,D)|–1.0|0.37|✅|
|(B,E)|–3.2|0.04|否|

→ 接受路径 (A,D)，并一次生成两个 token。优点：速度更快；实现简单；实测几乎不损失精度。

如果多个路径都合格怎么办？有两种常见处理方式：

|策略|描述|典型用途|
|---|---|---|
|**贪心 (Greedy)**|选 log Pₜ 最高的路径|高速推理、chat 模式|
|**采样 (Sampling)**|按各路径的总概率归一化采样|创造性生成、写作模式|

大多数推理系统采用贪心路径；采样策略则保持与大模型分布更一致（尤其在温度 > 0 时）。

### Multi-Token 预测头

这种方法通过修改 LLM 架构，在训练过程中一次性预测 $n$ 个未来 token。

**架构概述：** 论文中的下图展示了多 token 预测的实现结构：一个主干、多个未来预测头以及分阶段的损失计算。
   ![|500](https://aman.ai/images/papers/MTP.jpg)
共享的 transformer 主干生成一个隐藏状态。轻量级输出头解码 token $t+1$ 到 $t+n$。

**模型结构：**

```css
          ┌──────────────┐
Input ───▶│ Transformer  │─── z_t ─┬──▶ Head_1 → P(x_{t+1})
          └──────────────┘          ├──▶ Head_2 → P(x_{t+2})
                                    ├──▶ Head_3 → P(x_{t+3})
                                    └──▶ ... → P(x_{t+n})

```

用相同的隐藏状态 $z_t$​，去预测未来第 $i$ 个 token：

```python
# Trunk
z = transformer_trunk(x)

# Heads
logits = [head_i(z) for i in range(n)]
outputs = [softmax(logit) for logit in logits]
```

每个 head 都计算自己的交叉熵损失：
```python
loss = sum([F.cross_entropy(logits[i], target[i]) for i in range(n)])
```

$$\mathcal{L} = \sum_{i=1}^{n} \text{CE}(\text{softmax}(W_i z_t), x_{t+i})$$
**内存优化**：顺序计算梯度以节省显存

```python
for head in heads:
    output = head(z)
    loss = F.cross_entropy(output, target)
    loss.backward(retain_graph=True)
```

**推理策略**：论文提出两种模式：

传统生成 (Next-token Mode)：只用第一个 head（下一个 token 的预测）。这就等价于普通语言模型的推理方式。  此时模型输出与标准 LLM 完全一致。

块式推理 (Blockwise / Speculative Mode)：更高级，使用多个 head 提出的“未来候选 token”，一次生成多个 token，再进行验证。流程与我们之前讲的 _speculative decoding_ 类似：

- Head₁ 给出下一个 token；
- Head₂, Head₃ … 给出未来几个预测；
- 将这些组合成一个 **speculative block**；
- 用主模型（或验证机制）检查是否可接受；
- 若都合理 → 一次性输出多个 token。

这就是 _multi-token prediction → speculative decoding_ 的结合点。

**与 Medusa 的关系与区别**：

| 特征       | Multi-Token Prediction      | Medusa          |
| -------- | --------------------------- | --------------- |
| **训练阶段** | 改变训练目标，让模型学会一次预测多个 token    | 保持原训练，仅添加多个推理头  |
| **结构差异** | 多个“未来预测头”共享 trunk           | 多个“解码头”共享 trunk |
| **训练需求** | 需重新训练或微调                    | 可在推理时外挂         |
| **目标**   | 同时提升能力与推理速度                 | 仅加速推理           |
| **推理策略** | 块式生成 / speculative decoding | 树验证 / 并行解码      |



## 未来方向

这一领域仍在快速发展。从最初的推测采样开始，如今已衍生出混合处理流程、自适应接纳机制和树状推理路径等多种分支。随着量化模型和边缘可部署模型的整合，推测解码不仅成为一种优化手段，更将成为未来大语言系统设计的核心范式。

推测解码的核心技术为大型语言模型推理开辟了一系列优化机会。本节将探讨新兴变体、混合模型以及有望在保持输出准确性的同时进一步加速解码的研究方向。

### 混合方法: 合并 Draft + Head

一些系统现在将草稿模型与多 token 或 Medusa 头相结合，以最大限度地提高接受率和吞吐量。

**动机：** 使用草稿模型进行长推测前缀。使用 Medusa 或多 token 头来批量验证预测，而不是逐个 token 验证。

**Example Pipeline**: 草案模型提议 $γ$ 个 tokens；美杜莎风格的头在大模型中用于验证候选分支；接受最长的有效候选者。

**优势：** 结合草稿的高质量近似与结构验证效率。支持更深层次的 pipeline（例如，分层草稿检查循环）。自然可扩展至分布式和批量解码。

### 集成量化与剪枝

推测性解码可以与模型压缩技术协同作用：

量化模型（例如，QLoRA、GPTQ）：美杜莎头可以在冻结的量化模型之上进行训练/微调。即使是用于多 token 预测的主干部分也可以被量化（如 Medusa-1 所示）。

修剪头数​：轻量级推测头使用不到 0.1% 的模型参数。这使得它们成为训练后特定头剪枝或低秩近似的理想候选方案。

共享 KV 缓存​：如 IBM 的 PyTorch 实现所示，通过适配分页注意力内核，推测性 token 和主干输出可以复用相同的注意力缓存，且开销极小。

### 超越解码：用于多样化输出的推测采样

虽然最初的工作集中在贪婪或 top-k 解码上，但推测技术正在扩展到支持：

* 多样化采样​（通过 top-p 或温度控制的典型解码）；
* 束搜索变体（推测性束候选+最高分路径验证）；
* 随机接受（在 Wasserstein 距离或 KL 阈值下接受“足够接近”的 token）

这使得推测性解码适用于需要多样性的任务，例如故事生成、摘要和开放式问答。

### 未来研究方向

仍存在一些开放性问题和发展方向：

**推测性训练**​：能否通过显式训练来提高模型对推测性 token 的接受率（例如，通过对比 token 对齐）？这将使训练和解码在共同目标下统一起来。
**强化学习微调的投机者**​：RLHF 式对齐如何引导草稿模型的预测或头输出，以更好地符合人类偏好？
**自适应草拟**​：模型能否根据不确定性、熵或输入复杂度动态调整推测前缀长度？
**无标记解码**​：最近提出的“潜在解码”（直接生成隐藏状态）等方法可以与推测性策略结合，进一步降低推理延迟。

## 要点

推测解码代表了在保持模型准确性且无需大规模重新训练的前提下，使 LLM 推理更快、更高效、更具可扩展性的关键性进展。在本入门指南中，我们探讨了推测解码背后的概念基础、设计模式和技术实现。

虽然混合推测模型（如Medusa + draft）为实现更高速度和灵活性提供了一条路径，但未来的系统很可能会采用动态、训练时感知的推测推理管道，这些管道将根据具体用例和设备限制进行定制。

**要点：** 自回归推理本质上是顺序进行的，但推测性解码通过“猜测”未来的 token 并验证它们，引入了并行性。

目前主要有三种策略：
* 草稿模型解码：使用一个独立的小模型来生成推测性建议。
* 多 token 预测头：在预训练时内置到模型中，允许原生推测性输出。
* Medusa：通过树状注意力和灵活的接受方案增强多头预测。

加速效果真实可测量：2–3倍（草稿模型），3–6倍（多 token 头），2.3–2.8倍（Medusa 在实际单批次使用中）。内存高效的实现对于充分发挥推测性解码的优势至关重要，尤其是在处理大规模词汇和长序列时。

视使用场景而定：草稿模型在低延迟部署流程中表现优异，Medusa 非常适合聊天机器人和单用户场景。多 token 头在从头训练时效果最佳。


## Further Reading

- [The Hitchhiker’s Guide to Speculative Decoding (PyTorch blog)](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)
- [An Optimal Lossy Variant of Speculative Decoding (Mentored Decoding)](https://huggingface.co/blog/vivien/optimal-lossy-variant-of-speculative-decoding)
- [Speculative Decoding in vLLM: Overview & Integration](https://blog.vllm.ai/2024/10/17/spec-decode.html)
- [Looking back at speculative decoding (Google Research blog)](https://research.google/blog/looking-back-at-speculative-decoding/)
- [Speculative Decoding for Faster LLMs (Medium article)](https://medium.com/foundation-models-deep-dive/speculative-decoding-for-faster-llms-55353785e4d7)
- [How Speculative Decoding Boosts vLLM Performance by up to 2.8×](https://blog.vllm.ai/2024/10/17/spec-decode.html)

## References

- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- [MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)
- [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
- [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318)
- [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://arxiv.org/abs/2310.08461)
- [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/pdf/2402.11131)
- [Recurrent Drafter for Fast Speculative Decoding in Large LLMs](https://arxiv.org/pdf/2403.09919)

