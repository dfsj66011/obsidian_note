
Paper: 

[The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://ppc.land/content/files/2025/06/the-illusion-of-thinking.pdf)

思考的错觉：从问题复杂性的角度理解推理模型的优势与局限


讨论的一个问题就是我们现在所使用的 large reasoning model，其推理能力是不是真正的我们人类所想的 thinking，总的结论是，其实并不是真正意义上的思考，它有自己的一些局限性。

撸一下。第一个 我想先给大家撸一下这个abstract
让大家大概有一个简单的认识 这边paper做了一个什么样的工作。第二个
我想从方法论上面给大家讨论或者分享一下我对于做这个XAI
也就是explainable AI一些大概的思路 因为我觉得把一个paper里面的方法论给抽提出来
可能对大家更有帮助 第三个 我们再开始撸一下正文 那话不多说
我们就直接开始 那我们就先来看一下abstract 那第一句话就表明这篇paper它要研究的一个问题是什么
这里研究的问题就是large reasoning model 它的一个特点就是它可以产生思维链
这类的模型可以产生具体的思考过程 那这里有一个问题就是对于这一类的模型
它们的limitation有哪些 最本质那些能力 还有一些泛化能力
其实没有被很好的研究 所以这篇paper就是要研究这些问题 那之前的方法有什么问题呢
那之前的方法主要是在这些数学或者是编程一类的benchmark上面进行评价
主要就是评价了最后的那个答案 中间的思考过程对不对 可能都不是特别的关心
另外一个很重要的问题就是data contamination 因为现在的大语言模型在训练的时候
很多时候这数据都是从网上爬的 那爬的这些数据很可能就包含了这些测试的benchmark
所以现在这些工作并不能很好的去研究大语言模型它真正的性能
以及这个reasoning trace 也就是COT 它到底是什么样的一个结构以及质量
其实我们都不知道 那这篇paper他们主要是用了一些puzzle的environment 这具体是什么puzzle
正文里面等会会具体的解释 这么做的一个目的就是为了很好的去控制这个环境
然后去研究他们想研究的问题 这样做的好处就是他们不光能看到最后的答案
还能看到模型中间的这个思考过程 然后他们就发现了一个很有意思的现象
就是当这个puzzle的复杂度超过了一定的程度之后 这个模型的准确度就会collapse,就是突然的断崖式的下降
当这个问题的时候 那这个模型如果它会思考的话 那么它应该会主动的去增加这种思考的能力
就是应该会使用更多的token 但其实在这篇paper里面并没有观察到这样的一个现象
那这篇文章就把他们的研究的问题分成了三个部分 第一种就是根据这个任务的难度
分成了低复杂度 中复杂度和高复杂度 然后在低复杂度下面
他们观察到了一般的大语言模型 也就是不带reasoning的模型 性能其实是比带reasoning模型要好的
对于中复杂度的这一类的任务 reasoning model是有一定的好处的 对于高复杂度的任务
reasoning和non-reasoning model都出现了这种collapse 也就是accuracy直线下降的现象
最后作者这里还做了一个实验 这个实验就是说在解这些puzzle的时候
直接把这个解题的方法提供给这个模型 但即使这样 这一类模型还是没有能够去使用人类告诉他们的方法
所以这种结果仍然很差 所以总的来说 这篇paper就是通过创建了一个可控的环境
这个环境是使用的puzzle去研究reasoning model 然后发现了一些有趣的现象
这些有趣的现象主要体现在任务复杂度上面 而且这些现象给我们一般的直觉
可能是相反的 所以作者这里最主要的一个结论就是 我们现在所谓的reasoning model
它到底有没有真实的reasoning的能力 还只是说通过死记硬背训练数据当中的某些pattern
展现出来了一定的类似于reasoning的能力 但一旦在真实的环境里面遇到了超过训练数据以外的任务
就会出现这篇paper所提到的collapse的现象 也正是因为这样的一个结论或者是讨论
在网上引起了很大的反响 很多人认为这篇paper其实间接的证明了现在的大语言模型并没有像我们想象的那么的好
即使我们让现在的大语言模型产生了一些思维链 看起来有一些思考能力
但其实这样的大语言模型并没有真正的像人类那样有一定的泛化能力
那我们等会儿就来具体看一下一些实验的结果 在给大家撸正文之前 我想按照我的理解
总结和抽象一下这篇paper的方法论 我觉得学习一篇paper的研究思路比学习这篇paper具体的结论可能对我们更有帮助一些
这篇paper其实是关于XAI XAI是explainable AI的一个缩写
这个领域主要就是研究如何去理解机器学习模型这样的一个黑箱
那在大语言模型的时代 我觉得XAI这方面的工作有
但是不多 相比较每天层出不穷的AI agent的paper 这方面的paper还是偏少,但相反
我觉得XAI方面的工作其实 对我们理解大语言模型是非常必要,而且有帮助的
我觉得XAI总的有两种研究思路 第一种就是创建一种controllable的environment
第二种研究思路就是non-controllable的environment 这个controllable environment具体是什么意思呢
因为我们研究的很多机器学习模型 它其实是一个黑箱 也就是black box
所以你没有办法很容易的直接去研究这个模型 所以一般的思想就是用一些类似于控制变量法的方法
那比如说在互联网领域大家都非常熟悉的AB testing 其实就是一个控制变量法的变种
也就是你控制这个环境 根据你感兴趣的研究问题,创造几个不同的环境
然后把你想要研究的模型放到这样的环境当中去做对比 那具体到Apple这篇paper
作者其实研究了三个变量 第一个变量就是任务的complexity
作者在这里把任务分成了low, medium和high三个等级 这个complexity等会在paper里面会具体的定义
那第二个factor就是这种能思考的模型和不能思考的模型
比如说DeepSeek V3和DeepSeek R1 那第三个factor就是不同的模型
比如说在这篇paper里面 作者就比较了DeepSeek和Claude 作者主要研究了三个变量
然后类似于这种AB testing的方式,创造了不同的环境 然后在这个环境里面去对比每个factor所得到的影响
那这一类的研究方法里面还有一类的研究思路 就是gain或者是loss of function
那这个词我是借鉴于 生物实验里面的一个概念 那在生物实验里面
你可以在一个细胞里面过量的表达一个基因 或者是直接把那个基因敲除掉
然后再观察这个基因和这个细胞性状改变之间的关系 那具体到大语言模型里面
你可能会把某个transformer里面的neuron给去掉 然后来观察大语言模型它的输出有什么变化。第三类
我认为还有一类的方法 我叫做surrogate model 一些复杂的模型比较难以研究
这个时候一般会用一个比较简单的model去近似一个比较复杂的model
如果这个简单的model能局部或者是能在一定范围类似这个比较复杂的model
那么我们就可以用这个简单的model去研究这个复杂的model 那最典型的一个代表就是你可以用linear model去研究
non-linear model 如果大家对泰勒展开比较了解的话 其实泰勒展开的本质思想就是在局部
你可以用一个线性的函数去逼近它 所以对于一个复杂的函数,它的局部
你都可以用一个线性model去近似 所以你就可以用linear model去解释这个复杂的model
那这里就不得不稍微的提一下anthropic前不久发表的两篇论文
专门研究large language model的里面的思考过程 那其实它用的这个思路就是用了surrogate model
的思路 他们用了一个稍微简单的具有可解释性的model去近似 一个大语言模型
然后通过去研究这个更简单的模型 来理解这个复杂的大语言模型的一些行为
我个人觉得这个工作它的结论还是挺有意思的 但是方法我觉得仍然是太复杂
而且因为中间存在这样近似的一个过程 所以在简单模型上面成立的结论
在复杂模型上面不一定成立 那到non-controllable这类方法里面
很重要的一类方法 就是直接通过real world data去做一些统计分析
那这一类的方法和上面类的方法有什么区别呢 我这里可以给大家举一个简单的例子 比如说如果你想研究吸烟对人类的危害
那你不可能去设计一个实验 让一群人去吸烟 一群人不吸烟
或者你想研究吸毒对一类人的危害,同理 你不可能刻意的去召集两拨人
一波人让他们吸毒 一波人让他们不吸毒 这种实验是没有办法做的 但是在真实世界里面
其实大量存在于这种数据 有些人就会抽烟 有些人就会吸毒 有些人不抽烟 有些人不吸毒
但这样的数据存在一个问题 就是其他的变量你没办法控制 在我们前面做类似于这种AB testing的时候
我们只改变AB这两个变量 其他的变量全部都是一样的 但是在non-controllable的这种环境里面
除了你感兴趣的变量以外 其他变量你也没办法控制 这一类方法在生物医药领域里面
尤其是做临床实验的时候非常的常见 那具体到大语言模型里面 在已有的几个数据集
上面比较模型的一些性质 比如说思维链的长度 最终的结果等等
在Apple这篇paper里面 作者就在figure2里面比较了三个数据集 通过比较这三个数据集,模型passK的性能
从而间接的推断出模型的一些性质 但我觉得这一类的方法整体比controllable environment要更难一些
因为很多时候有很多的变量你没办法控制 大家可以一边读这篇paper 一边去体会一下这篇paper最底层的方法论
那我们来看一下introduction部分 前面第一段 主要就是说现在的大语言模型都有一定的reasoning的能力
但是对于大语言模型reasoning能力其实并没有很好的理解 那作者在这里问了几个我觉得是发人深省的问题
第一个就是这种大语言模型,它的思考能力是否具有一定的泛化能力
也就是generalizable的reasoning 或者是说现在的这种大语言模型 它只是一种pattern matching
也就是雅乐坤经常说的 现在大语言模型只不过是学习的一些statistical association
只不过就是做的一些简单的模式匹配 如果这个模型只会做一些简单的模式匹配的话
那么它是没办法泛化的 第二个问题 作者就感兴趣这个模型的性能随着这个任务的复杂度
它是如何变化的 以及这种变化的关系在non-reasoning model和reasoning model之间
有什么区别 另外也就是现在这种reasoning model它还有一些什么样的局限性
如果我们想进一步的提高现在reasoning model的话 应该考虑哪些方向 这些问题问的还是非常关键的
发现问题其实比解决问题更重要 接下来作者就主要提了一下现在研究这类问题
它有一些局限性 主要的局限性就是说现在的方法主要就是在数学或者是coding的benchmark上面进行研究
这样的话容易受到data contamination 也就是数据污染的影响 以及这一类的实验它其实没有办法control environment
对于这个模型中间的思考过程,具体有什么问题 目前的方法也没有很好的研究
所以基于这一类的想法 作者就提出了他们的思路 这里写了很多
简单来说就是不要再像之前的工作一样去研究一些数学或者编程的问题
而是去研究一些puzzle 这个研究puzzle有好处就是这个实验可以控制
你可以把它变得很复杂 也可以把它变得很简单 然后你可以在仿真的环境当中去进行仿真
并且这一类的任务,它的规则比较的清晰 所以模型能够用reason的能力去解决这样的一些问题
然后作者在这里就大概的解释了一下他们的发现 那这个我们等一会儿再仔细的解释
那我们先来看一下这个figure2 主要想表达的一个结论 就是目前用这种数学的benchmark
它有一些什么样的问题 那这个图展示的是在Mass500,M24,M25这三个数据集上模型的表现
模型作者主要是评价了Claude3.7, no thinking和thinking model
以及DeepSeek V3 和DeepSeek R1 那V3就是一个no thinking model R1就是一个thinking model
然后纵坐标是passK 这个passK要稍微的解释一下 模型产生K次尝试之后
只要有一次通过了就算它通过 所以这个passK的指标可以理解为是模型能力的一个上限
也就是upper bound 那这里的横坐标是inference computer budget 也就是tokens 这里可以简单理解为思考的时间
因为对于算法来说 它的思考时间就是token 那这个图展现了几个有意思的结论
我们先看Mass500这个数据集 这个数据集相对来说是比较早的一个数据集
我们可以看到在相同的computer budget的情况下 这个thinking model和no thinking model,它的这个gap是非常小的
基本上是没有gap 也就是说这两类模型,它的性能是差不多的 但是如果我们换到AIM24这个数据集
注意AIM24这个数据集,问题的难度要大于Mass500 Mass500大概是一个小学水平的数据集
AIM24大概是一个高中数学奥林匹克难度的数据集 在这个数据集上面就可以看到
这种thinking model,它的性能就要比no thinking model性能要好 所以这就出现了一个gap
不光是在Claude3.7上面 在DeepSeek这一类的模型上面我们也能观测得到 但是比较有意思的是
当我们继续来看AIM25这个数据集的时候 注意对于AIM25来说
人类的performance在AIM25上面其实要好于AIM24的 也就是说AIM25这个数据集它的难度其实要小于AIM24的
但是比较有意思的是 可以观察到这两类模型它的gap其实是变大了 所以作者在这里就提出了两个假设
第一个就是这个complexity增加 但是因为人类在AIM25上面的性能是提高的
所以我们可以简单的认为AIM25这个数据集的complexity其实要小于AIM24的
所以这个假设其实是不成立的 第二个可能的原因就是data contamination 这个原因就是AIM25相对于AIM24来说
它是一个更新的数据集 所以在这些大语言模型训练的时候 这个数据集数据泄露到训练集里面的可能性更小一些
所以导致大语言模型没有见过这一类的数据 所以性能就会下降 作者认为这是一个更可能的原因
现在很多的工作都是用这些benchmark去研究模型的性能 其实并不能真正的反映所要研究的那个问题
因为这里面有一个很大的confounding factor 就是data contamination在这里面影响
作者就提出 那我们就用puzzle来研究大语言模型的性能
尤其是reasoning的能力 这篇paper引入了四种puzzle, Tower of Hanoi
Checkers Jumping River Crossing Blocks World 我记得我高中的时候 文曲星开始流行
文曲星上面就有很多类似于这样的一些小游戏 这个哈塔的规则就是说 说给定这样的几个碟子
在这边paper里面叫disk 然后要把它比如说挪到第三根柱子上面 因为每次只能挪动一个这个碟子
所以它这里面有一定的技巧 那这一类任务 它的任务复杂度就跟这个最开始的碟子的数量有关系
比如说这里是三个碟子 那如果碟子更多的话 这个任务就更复杂 那后面的这些checkers jumping也是类似的
这里就是有一些不同的 我就叫棋子吧 然后这里会有一些空格 那主要的规则就是去挪动这些棋子
让它变成最终的这样的形式 那后面的两个游戏我就不具体解释了 那总的来说
这些游戏它都有一个共同的点 就是它的complexity是可以控制的 你可以把初始的这些棋子数量增加
那么对应这个问题的complexity就会随之而增加 所以作者就想通过这样的一个控制实验
去研究在不同的复杂度下 模型它的性能到底是什么样子的 希望大家现在明白了作者的研究动机以及他实验的设计
那我们再回过头来看一下figure1 上面这个图就是直观的解释了一下他们这个实验是怎么做的
简单的说就是给大语言模型这样一个汉诺塔的任务 然后让大语言模型去解决
那给了这个任务之后 大语言模型就会回答 然后会输出它的思考过程 这个思考过程都是以这种矩阵的形式所表示的
然后作者就把这样的一个思考过程给抽出来做分析 可以分析这个大语言模型
它在解决这个任务的时候 是具体怎么在思考的 然后有什么问题 最后大语言模型也会产生一个答案
作者就把这样的答案给抽出来 算一下它的accuracy 所以反映到下面这个图
这里的accuracy就是模型最后输出的答案 response lens就是看模型在思考的时候输出了多少个token
这里的position within thoughts指的就是这个大语言模型,它产生的COT的过程当中
比如说它尝试了很多的思路 那个正确思路在它整个COT里面所处的位置
然后把它normalize成1 根据整个COT的长度来normalize 这个指标主要是想判断这个大语言模型
它产生正确的思路是在COT比较靠前的地方 还是COT比较靠后的地方 然后这里的横坐标就是任务的复杂度
那这里用的是汉诺塔的碟子的个数 碟子越多这个复杂度就越高
那我们来挨个的看一下这几个图它所表明的意义 我们先看一下accuracy
作者这里有意把complexity分了几个区域 可以看到黄色、蓝色和粉色
黄色的就是low complexity 大概在三个碟子以下 medium complexity,也就是大概在10个碟子以下
三个碟子以上,high complexity 就是10个碟子以上了 在low complexity的时候
Claude3.7 thinking和no thinking model,这个性能基本上是差不多的
也就是说在这种简单的任务上面 thinking model其实没有太多的优势 但是当这个complexity增加到了medium之后
这个thinking model优势就体现出来了 可以看到 这个实线是要高于这个虚线的
但是一旦当这个任务的复杂度超过一定阈值的时候 也就是这里的10 这两个模型的性能都基本趋于零了
也就是作者在文章中所使用的一个词 叫collapse 而且它这种accuracy的降低
是一种陡降 并不是缓慢的降低 所以作者在这里认为 是由于模型的thinking
也就是思考能力 它没有泛化能力 才会出现这种现象 那我的一个猜测就是
对于这一类的复杂任务 对于模型来说 其实是OOD 也就是out of distribution
有可能在训练集里面 大部分的哈塔 或者是类似的任务 它的复杂度都是在这个区间的
换句话说 模型没有见过这一类复杂度的task 所以一旦出现OOD之后
模型就不会了 虽然现在的thinking model都是用reinforcement learning的方法出来的 出来的
reinforcement learning有一定的程度可以增强模型的泛化能力 也就是体现在这里的medium complexity上面
但是对于OOD比较严重的情况 我觉得当前的reinforcement learning的方法可能还是不够的好
那这里我能想到的一个解决方法 就是使用前面所提到的TTRL那样一类的方法
也就是test time training 或者是domain adaptation 也就是在一些新的任务上面
out of distribution的任务上面 在test time做一些模型的微调 也许可以解决这样的问题
我们继续来看一下第二个图 这个图是response lens 也就是token的数量
作者把这里分成了三个不同的complexity的程度 一旦这个任务的难度超过了threshold
也就是这条线 模型输出的这个token数目突然就减少了 这个就有一点违反直觉
就比如说如果人类在解决一类任务的时候 如果这个任务很难的话 那么人应该花更多的时间去思考
如果我类比一下 对于大语言模型来说 如果它遇到更难的问题 它应该花更多的token去思考
所以这个曲线应该是继续往上的 但是作者在这里观察到是超过了一定阈值之后
这个模型所产生的token数不升反降 作者把这样的一类现象叫做scaling limitation
那我觉得这一类的问题还是可以用前面所提到的OOD来解释 可能是因为这一类的模型在训练的时候没有见到如此复杂的任务
所以一旦遇到这样的任务OOD之后 它就不知道该如何去思考了 所以它的token数目就会变少
我们再来看一下第三个图 这里展示的是模型在思考的过程当中
正确的答案出现在COT的具体的位置 对于这些比较简单的task 模型正确答案出现的COT位置是比较靠中的位置
也就是说模型先花了一定的时间或者是token去思考 得到了正确的答案之后
模型还花了一些时间去思考一些不正确的答案 虽然这个任务非常的简单
但是模型仍然花了一些不必要的额外的思考的时间 所以这其实就是overthinking的一个体现
所以这一部分的 思考其实都浪费了 因为在中间这个地方其实就已经得到正确答案了
那对于那些中等难度的task, 模型会先尝试一些不正确的思路
然后慢慢的一直到在思维链比较靠后期的时候才得到正确的答案 那对于那些非常复杂的任务,模型出现了collapse的症状
所以这里就没有任任何的结果 前面的figure1基本就把这篇paper最主要的结论给展示出来了
接下来的一些结果就是具体的展开 那我们来看一下figure4 figure4主要就是把figure1里面accuracy那个图给展开了
那这里画的就是不同的任务 四个任务 然后有两类的模型
一个是Claude3.7 一个是DeepC 然后每个模型里面有thinking和non-thinking model
同样呢横坐标就是任务的complexity 然后把任务也分成了三类
在low complexity的情况下 reasoning和non-reasoning model性能是差不多的
在medium complexity的情况下 reasoning model性能要好于non-reasoning model 但是对于high complexity
两类模型都会出现性能急剧的下降 也就是collapse的症状 在不同的任务上面
这个collapse的陡峭程度是不一样的 在不同的模型上面 这个陡峭的程度也是不一样的
但基本还是那个结论 不管是Claude3.7还是DeepSeek一系列的模型
当任务复杂度超过一定阈值之后 模型突然就不会了 Fake5这里作者展示的是passK的性能
前面提到passK是模型的upper bound 你基本可以理解为模型的可以能达到的最好的性能
那这里的横坐标变成了computer budget 也就是tokens 在这些比较复杂的任务上面
虽然模型它有一定的computer budget 但是模型却没有利用这些computer budget去思考
我给大家打个比方 就好比你在考数学考试的时候 最后有一道大题 那前面的题你可能很快就做完了
最后回到大题,因为很难你不会做 你不会做 所以你直接就放弃了 虽然你还有时间 这个时候你就直接交卷
这就是这个图所要表达的一个现象 然后作者在fig6里面继续的展示了一下collapse这个现象
这里作者引入了更多的reasoning model 包括O3 Mini 不同的颜色就表示一个模型
那第一行是结果的accuracy 第二行是这个COT所使用的token 那每一列就是一类的任务
作者在这里主要想表达两个现象 其实在fig1里面也展示过了 第一个就是对于accuracy来说
当任务复杂度逐渐增加的时候 accuracy会突然出现下降 也就是collapse的现象
我们可以看到 在不同的任务上面 这个accuracy就是很快的下降 对于不同的reasoning model我们都能观察得到
第二个现象就是模型在accuracy下降的同时 它所使用的token数也在减少
这也是我们前面所提到的一个违反直觉的现象 当这个任务变难了之后
模型所使用的token数应该是变多了 所以它这个曲线应该是向上的 这就好比你在考试的时候遇到一个难题
那么你思考的时间应该更多 而不是遇到难题之后你就直接放弃了 我们再来看一下figure7
这个图基本也是figure1的一个扩展 这里展示的就是任务的复杂度和position in thinking的关系
position in thinking就是这个模型得到正确答案在COT当中的位置 然后这里展示了四个任务
基本的结论跟前面说的是一样的 对于这些比较简单的task, 模型可能在比较早期的COT的阶段就能把正确答案给找到
但是还是花了比较多的时间继续的去探索 那这一部分探索的这一部分其实就是属于overthinking
也就是有浪费 但是比较有意思的是 当这个任务变得越来越复杂之后 这个现象似乎就出现了翻转
翻转的意思就是说模型前期尝试的一些思路都是错误的 只有在思维链比较靠后的时候才能找到正确的答案
这个图上面作者是用了一个distribution的形式来表示的 绿色的distribution表示的是正确的答案
红色的distribution表示的是错误的答案 可以看到这个绿色的distribution要更靠近一些
红色的distribution更靠近一些 这也就是表明这个模型前期所探索的思路都是错误的
就到后期才探索一些正确的思路 然后就是对于这些比较复杂的任务 就出现了collapse的现象
也就是这里已经没有绿色的distribution了 只有红色的 那右边这个图基本上就是另外一种画法
其实表达的意思跟这个图的意思差不多 所以这里我就不再赘述了 作者在fig8里面又做了一个比较有意思的实验
这个实验就是在model的prompt里面 直接把解决这个任务的algorithm交给这个模型
但是作者发现 交和不交,这个模型 它的性能都差不多的 所以看到这个实线和这个虚线基本都是重合的
而这个现象对于DeepR1和Claude3.7 thinking model都能观察得到 那在prompt里面给这个算法
具体是什么意思呢 就是对于这个哈纳塔的任务来说 你可以在这个prompt里面给它一个pseudo code
也就是这里所描述的 如果你想解决哈纳塔的话 这样的一个伪大码大概是什么样子的
这就好比在考试的时候 你可能记不住一些物理或者是数学公式
那老师直接就把这样的物理或者数学公式给你 你只需要照着套入进行计算就可以答题了
但即使这样 这个大语言模型还是不会 那我觉得这个实验作者在这里做的并不是很严谨
我觉得有可能是因为这个模型following instruction本身的问题 这里最好再添加一个实验
做一些few shot learning的实验 也就是不光要给这个pseudo code
最好再给一些利用这个pseudo code解题的例子 通过这种few shot learning的形式
看一下模型是不是还是不会利用这种给定的algorithm去解决问题
作者这里还放了两个图 也就是figC和figD,这个模型
它在第一次出现错误的动作的时候 它的这个部署大概是多少
对于这两类任务 一个是哈纳塔 一个是这个River Crossing 模型的表现是不一样的
比如说在River Crossing上面 对于比较简单的任务模型的性能也表现的不是特别的好 模型的性能也表现的不是特别的好
那作者这里想得到的一个结论就是说 River Crossing这类任务在网上的数据可能不是特别多
所以模型没有见过 所以即使这个比较简单的情况下 模型性能也不是特别的好
那我个人觉得作者在这里放这两个图 似乎跟他这个文章的主要观点不是很搭
显得有点特别的突兀 那这边paper主要内容就讲到这里了 那我觉得这边paper
并不是特别的复杂 做的实验相对来说也比较的straightforward 但是能得到一些比较有意思的现象或者是观察
第一个就是从方法论上来说 我觉得作者跳出了一般研究大语言模型的习惯
没有在benchmark上面去研究 因为benchmark上面可能不太好控制一些变量
所以作者就在这个puzzle environment上面做了一些研究 那我觉得这个现象的本质就是模型在training的时候OOD
我能想到的解决方案就是可以类比于我们人类 我们人类在学习的过程当中
虽然有一定的触类旁通的能力 但也不是说对于任何新的任务我们人类都会解决 我们人类都会解决
所以人类也是在通过和环境不断的互动当中去积累一些经验或者是知识
者是知识 当经验和知识越来越多之后 人类的解决问题的能力会越来越强 所以我的一个基本想法
可能和The arrow of experience里面提到的一样 就是让尽可能的让现在的这些大语言模型或者是agent多去跟环境互动
多去接触一些不一样的任务 从而避免或者是减少这种OOD情况的出现
我觉得这可能是一个潜在的解决方案 另外一个 网上大家都在讨论这个大语言模型是否真正具有reasoning能力
我个人的观点是 不管这个大语言模型有没有我们人类所谓的reasoning能力
我觉得这个点不是特别的重要 因为换一个角度来说 我们人类其实也是很多神经元的连接
当我们在思考的时候 也是神经地址在传递 我们人类会不会也是某种程度上来说在做pattern matching呢
就拿我自己来说 我在解决问题的时候 我习惯性的会从我的经验当中去找一些类似的问题
然后套用类似问题的解决方案去解决一些新的问题 这本质上也是一个pattern matching的过程
那回到大语言模型 回到现在的agent 那如果agent会使用pattern matching
能够从它的训练数据当中去套用一些它有的知识或者能力解决一些新的问题
我觉得这已经能达到人类的水平了 所以我本质想表达的一个观点就是 只要现在的大语言模型或者是agent能够帮助我们实实在在的解决一些问题
不论它有没有我们所谓的reasoning能力 我觉得这个都不是特别的重要 当然对于去理解大语言模型reasoning能力的产生
像Apple这篇工作是非常有意义的 这里作者就提了一些他们自己工作的limitation
其实我是非常同意的 他们虽然巧妙的使用了一个puzzle environment去设计实验
去研究大语言模型 但同时这也是他们的一个limitation 因为现实环境当中很多的任务并没有像puzzle那样简单
现实世界的任务是非常的复杂的 环境是多样的 所以他们这种研究得出来的结论不一定能够直接外推到真实世界大语言模型的表现上面
我觉得这是这篇paper的一个很大的局限 另外我想再提出一个我个人的观点
就是这篇paper它主要研究的一个方向就是任务的复杂度 但这个任务的复杂度,我可以把它抽象成一个数据的dimension
从统计的角度来讲 一个数据的dimension一旦增大之后 从low dimension变成了high dimension之后
就会出现很多违背我们人类直觉的现象 比如说我记得在我读书的时候学习的随机过程
老师就举了一个例子 对于一个1D世界的醉汉 它可以随机的往左走,往右走
因为它喝醉了 所以它走的时候就是随机的 那如果它从家里出发 它是否经过一定的步骤之后一定能回家呢
对于1D的情况 答案是肯定的 对于2D的情况 如果这个醉汉活在一个城市里面
它能够按照这个四个方向随机游走,同样它从这个家里出发 它从这个家里出发 经过一定的步骤之后
是否一定能再回到原点呢 答案是肯定的 那对于一个3D的世界呢 它就有更多的方向随机游走
那它的人是否一定就能回到最初的原点呢 这个时候答案就是不一定了 这个就有点违反我们的直觉了
为什么在1D2D的情况下可以 到3D就不行了。类似的 在high dimension的情况下
还有一些非常有趣的现象 比如说这里就有一个网页专门介绍了counter intuitive properties of high dimensional space
dimensional space 就是在high dimensional space下面会有很多违反我们直觉的一些现象
那这里我就具体不再说了 感兴趣的可以去看一下 回到我们的这个paper 所以这里就让我思考
会不会作者在这里研究的这类任务的复杂度也跟这个high dimension有一定的关系呢
在high dimension space下面就会出现这种违反直觉的一些现象 也就是这边paper所提到的collapse的现象
数学背景比较好的同学可以想一想 也欢迎你给我留言 我们一起讨论
我们再来看一下Gary Marcus两天前写的一个blog 那Gary Marcus是Twitter上面非常活跃的一个人物
先说Gary Marcus的结论 他的结论就是用他之前的一个Twitter post来表达的, AI is not hitting a wall
but LLM probably are 它这句话意思就是说 AI其实并没有到达一个瓶颈
相反是现在的大语言模型到达了一个瓶颈 所以他认为需要一些新的方法
总的来说 他认为大语言模型现在的发展趋势是不正确的 如果我们想实现AGI的话
可能这不是一个正确的方法 它在这个blog里面首先引用了一个Josh Wolf
这个人应该是一个venture capitalist 也就是一个风险投资家 这个风险投资家把这个paper总结的非常的好
第一个他就是说现在的reasoning model 当任务复杂的时候 模型会collapse
而是completely collapse 第二个就是现在的模型会give up 也就是会遇到难题的时候提前交卷
第三个就是即使给大语言模型一个解决的方法 它也不一定会使用
所以它这里提出了execution不等于understanding 我想可能就是类似于我们会解题
但不一定真正的明白这个题目背后的知识点 然后就是overthinking的问题
对于比较简单的问题 模型会想的比较多 但是反而对于比较难的问题
模型会想的比较少 所以Apple最后的结论就是现在这一些模型它并不是真正的在reasoning
然后Gary Marcus就在后面的讨论当中提了一个观点 就是training distribution的问题
他认为早期的neural network都有这种generation的问题 也就是它只能在training distribution of data
也就是训练数据的范围内进行泛化 一旦这个数据超出了这个训练数据的distribution
这类模型它就没有办法泛化了 也就是generation tend to break down 所以的Apple这边paper其实也是类似的一个问题
一旦遇到的任务超出了模型training distribution以外 这个模型就没办法泛化了 另外Gary Marcus在这里引用了之前的一个computer scientist
应该是个印度人 然后在这里缩写成Rao Rao的一个观点 就是说人们过度的把现在这种大语言模型的reasoning trace进行拟人化
并把它称为思考 但其实这个模型并没有真正的思考 所以Rao的这个结论跟Apple这边paper的结论基本是一致的
最后Gary在这里又写了一大通 主要就是说汉诺塔这一个任务其实在很早之前就已经被人解决了
但是现在的大语言模型仍然不能解决汉诺塔这样比较简单的任务
他这里提到就是人类在对于有八个碟子的汉诺塔任务的时候 结果会非常的差
所以他这里用这个例子主要是想引出来 他认为AGI的实现不应该是AI完全取代人类
而是由人类和AI进行协同的工作 也就是他这里说的combines the strength of humans with the strength of the machine
overcoming the weakness of humans 通过和AI合作去帮助人类
从而起到一加一大于二的效果 所以他这里主要想表达AI应该是一个助手
而不是一个完全取代人类的角色 最后这一段Gary Marcus的意思就是说现在的大语言模型,其实离我们所想要的AGI还差得非常的远
但是他也提到 就是说在下一个十年里面 大语言模型其实还是有一些可以使用的地方
比如说coding呀 brainstorming呀, 或者是writing呀 虽然大语言模型不是实现AGI的一个正确的方向
但是它还是有自己的用武之地 最后就得出了他的结论 我觉得他的观点可能就是不要太乐观的看待
大语言模型可以实现AGI 但是我们用大语言模型解决一些特定领域的问题
比如说像他刚才提到的coding, writing, 这些还是可以的 但不管怎么说 回到Apple这篇paper本身
我觉得这篇paper在某种程度上面 从侧面给大家提供了一些更好的证据去理解大语言模型目前的一些缺陷
我相信这些缺陷也许是可以逐步解决的,当然 当然具体大语言模型是否是实现AGI的一个可能途径
我觉得这里还没有定论 可能需要大家共同的去努力研究和推动
但我觉得Gary Marcus在这里说的非常的好 就是我们需要不断的去探索一些新的道路