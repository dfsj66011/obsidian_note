


The lowest it can get is zero, and the lower it is, the better off your model is because it's assigning high probabilities to your data. Now let's estimate the probability over the entire training set just to make sure that we get something around 2.4. Let's run this over the entire, oops. Let's take out the print statement as well.

Okay, 2.45 over the entire training set. Now what I'd like to show you is that you can actually evaluate the probability for any word that you want. Like for example, if we just test a single word, Andre, and bring back the print statement, then you see that Andre is actually kind of like an unlikely word, or like on average, we take three log probability to represent it.

And roughly that's because EJ apparently is very uncommon as an example. Now think through this. When I take Andre and I append Q, and I test the probability of it, Andre Q, we actually get infinity.

And that's because JQ has a 0% probability according to our model. So the log likelihood, so the log of zero will be negative infinity. We get infinite loss.

So this is kind of undesirable, right? Because we plugged in a string that could be like a somewhat reasonable name. But basically what this is saying is that this model is exactly 0% likely to predict this name. And our loss is infinity on this example.

And really what the reason for that is that J is followed by Q zero times. Where's Q? JQ is zero. And so JQ is 0% likely.

So it's actually kind of gross, and people don't like this too much. To fix this, there's a very simple fix that people like to do to sort of like smooth out your model a little bit, and it's called model smoothing. And roughly what's happening is that we will add some fake counts.

So imagine adding a count of one to everything. So we add a count of one like this, and then we recalculate the probabilities. And that's model smoothing.

And you can add as much as you like. You can add five, and it will give you a smoother model. And the more you add here, the more uniform model you're gonna have.

And the less you add, the more peaked model you're gonna have, of course. So one is like a pretty decent count to add, and that will ensure that there will be no zeros in our probability matrix P. And so this will, of course, change the generations a little bit. In this case, it didn't, but in principle, it could.

But what that's gonna do now is that nothing will be infinity unlikely. So now our model will predict some other probability, and we see that JQ now has a very small probability. So the model still finds it very surprising that this was a word or a bigram, but we don't get negative infinity.

So it's kind of like a nice fix that people like to apply sometimes, and it's called model smoothing. Okay, so we've now trained a respectable bigram character-level language model. And we saw that we both sort of trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions.

We saw that we can also then use those parameters of this model to perform sampling of new words. So we sample new names according to those distributions. And we also saw that we can evaluate the quality of this model.

And the quality of this model is summarized in a single number, which is the negative log likelihood. And the lower this number is, the better the model is, because it is giving high probabilities to the actual next characters in all the bigrams in our training set. So that's all well and good, but we've arrived at this model explicitly by doing something that felt sensible.

We were just performing counts, and then we were normalizing those counts. Now, what I would like to do is I would like to take an alternative approach. We will end up in a very, very similar position, but the approach will look very different because I would like to cast the problem of bigram character-level language modeling into the neural network framework.

And in the neural network framework, we're going to approach things slightly differently, but again, end up in a very similar spot. I'll go into that later. Now, our neural network is going to be still a bigram character-level language model.

So it receives a single character as an input. Then there's neural network with some weights or some parameters W, and it's going to output the probability distribution over the next character in a sequence. It's going to make guesses as to what is likely to follow this character that was input to the model.

And then in addition to that, we're going to be able to evaluate any setting of the parameters of the neural net because we have the loss function, the negative log likelihood. So we're going to take a look at its probability distributions, and we're going to use the labels, which are basically just the identity of the next character in that bigram, the second character. So knowing what the second character actually comes next in the bigram allows us to then look at how high of a probability the model assigns to that character.

And then we of course want the probability to be very high. And that is another way of saying that the loss is low. So we're going to use gradient-based optimization then to tune the parameters of this network, because we have the loss function and we're going to minimize it.

So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character. So let's get started. The first thing I want to do is I want to compile the training set of this neural network, right? So create the training set of all the bigrams.

Okay. And here I'm going to copy paste this code because this code iterates over all the bigrams. So here we start with the words.

We iterate over all the bigrams. And previously, as you recall, we did the counts, but now we're not going to do counts. We're just creating a training set.

Now this training set will be made up of two lists. We have the inputs and the targets, the labels. And these bigrams will denote XY.

Those are the characters, right? And so we're given the first character of the bigram, and then we're trying to predict the next one. Both of these are going to be integers. So here we'll take Xs.append is just X1, Ys.append, IX2.

And then here, we actually don't want lists of integers. We will create tensors out of these. So Xs is torch.tensor of Xs, and Ys is torch.tensor of Ys.

And then we don't actually want to take all the words just yet because I want everything to be manageable. So let's just do the first word, which is Emma. And then it's clear what these Xs and Ys would be.

Here, let me print character one, character two, just so you see what's going on here. So the bigrams of these characters is .eemmmama. So this single word, as I mentioned, has one, two, three, four, five examples for our neural network. There are five separate examples in Emma.

And those examples I'll summarize here. When the input to the neural network is integer zero, the desired label is integer five, which corresponds to E. When the input to the neural network is five, we want its weights to be arranged so that 13 gets a very high probability. When 13 is put in, we want 13 to have a high probability.

When 13 is put in, we also want one to have a high probability. When one is input, we want zero to have a very high probability. So there are five separate input examples to a neural net in this dataset.

I wanted to add a tangent of a note of caution to be careful with a lot of the APIs of some of these frameworks. You saw me silently use torch.tensor with a lowercase T and the output looked right. But you should be aware that there's actually two ways of constructing a tensor.

There's a torch.lowercaseTensor and there's also a torch.capitalTensor class, which you can also construct. So you can actually call both. You can also do torch.capitalTensor and you get an Xs and Ys as well.

So that's not confusing at all. There are threads on what is the difference between these two. And unfortunately, the docs are just like not clear on the difference.

And when you look at the docs of loweredcaseTensor, construct tensor with no autograd history by copying data. It's just like, it doesn't make sense. So the actual difference, as far as I can tell, is explained eventually in this random thread that you can Google.

And really it comes down to, I believe, that, where is this? Torch.tensor infers the D type, the data type automatically, while torch.tensor just returns a float tensor. I would recommend to stick to torch.lowercaseTensor. So indeed, we see that when I construct this with a capital T, the data type here of Xs is float 32. But torch.lowercaseTensor, you see how it's now X.Dtype is now integer.

So it's advised that you use lowercase T and you can read more about it if you like in some of these threads. But basically, I'm pointing out some of these things because I want to caution you and I want you to get used to reading a lot of documentation and reading through a lot of Q&As and threads like this. And some of this stuff is unfortunately not easy and not very well documented and you have to be careful out there.

What we want here is integers because that's what makes sense. And so lowercaseTensor is what we are using. Okay, now we want to think through how we're going to feed in these examples into a neural network.

Now, it's not quite as straightforward as plugging it in because these examples right now are integers. So there's like a zero, five, or 13. It gives us the index of the character and you can't just plug an integer index into a neural net.

These neural nets, right, are sort of made up of these neurons and these neurons have weights. And as you saw in micrograd, these weights act multiplicatively on the inputs, WX plus B, there's 10 Hs and so on. And so it doesn't really make sense to make an input neuron take on integer values that you feed in and then multiply on with weights.

So instead, a common way of encoding integers is what's called one-hot encoding. In one-hot encoding, we take an integer like 13 and we create a vector that is all zeros except for the 13th dimension, which we turn to a one. And then that vector can feed into a neural net.

Now, conveniently, PyTorch actually has something called the one-hot function inside Torch and in functional. It takes a tensor made up of integers. Long is an integer.

And it also takes a number of classes, which is how large you want your tensor, your vector to be. So here, let's import Torch.nn.functional as F. This is a common way of importing it. And then let's do F.one-hot. And we feed in the integers that we wanna encode.

So we can actually feed in the entire array of Xs. And we can tell it that numClass is 27. So it doesn't have to try to guess it.

It may have guessed that it's only 13 and would give us an incorrect result. So this is the one-hot. Let's call this xInc for xEncoded.

And then we see that xEncoded.shape is five by 27. And we can also visualize it, plt.imshow of xInc, to make it a little bit more clear because this is a little messy. So we see that we've encoded all the five examples into vectors.

We have five examples, so we have five rows. And each row here is now an example into a neural net. And we see that the appropriate bit is turned on as a one and everything else is zero.

So here, for example, the zeroth bit is turned on. The fifth bit is turned on. Thirteenth bits are turned on for both of these examples.

And then the first bit here is turned on. So that's how we can encode integers into vectors. And then these vectors can feed in to neural nets.

One more issue to be careful with here, by the way, is let's look at the data type of xEncoding. We always want to be careful with data types. What would you expect xEncoding's data type to be? When we're plugging numbers into neural nets, we don't want them to be integers.

We want them to be floating-point numbers that can take on various values. But the Dtype here is actually 64-bit integer. And the reason for that, I suspect, is that one hot received a 64-bit integer here and it returned the same data type.

And when you look at the signature of one hot, it doesn't even take a Dtype, a desired data type of the output tensor. And so we can't, in a lot of functions in Torch, we'd be able to do something like Dtype equals Torch.float32, which is what we want. But one hot does not support that.

So instead, we're going to want to cast this to float like this. So that these, everything is the same. Everything looks the same, but the Dtype is float32.

And floats can feed into neural nets. So now let's construct our first neuron. This neuron will look at these input vectors.

And as you remember from micrograd, these neurons basically perform a very simple function, wx plus b, where wx is a dot product, right? So we can achieve the same thing here. Let's first define the weights of this neuron, basically. What are the initial weights at initialization for this neuron? Let's initialize them with Torch.random. Torch.random fills a tensor with random numbers drawn from a normal distribution.

And a normal distribution has a probability density function like this. And so most of the numbers drawn from this distribution will be around zero, but some of them will be as high as almost three and so on. And very few numbers will be above three in magnitude.

So we need to take a size as an input here. And I'm going to use size as to be 27 by one. So 27 by one, and then let's visualize w. So w is a column vector of 27 numbers.

And these weights are then multiplied by the inputs. So now to perform this multiplication, we can take X encoding and we can multiply it with w. This is a matrix multiplication operator in PyTorch. And the output of this operation is five by one.

The reason it's five by five is the following. We took X encoding, which is five by 27, and we multiplied it by 27 by one. And in matrix multiplication, you see that the output will become five by one because these 27 will multiply and add.

So basically what we're seeing here out of this operation is we are seeing the five activations of this neuron on these five inputs. And we've evaluated all of them in parallel. We didn't feed in just a single input to the single neuron.

We fed in simultaneously all the five inputs into the same neuron. And in parallel, PyTorch has evaluated the WX plus B, but here it's just WX, there's no bias. It has valued W times X for all of them independently.

Now, instead of a single neuron though, I would like to have 27 neurons. And I'll show you in a second why I want 27 neurons. So instead of having just a one here, which is indicating this presence of one single neuron, we can use 27.

And then when W is 27 by 27, this will in parallel evaluate all the 27 neurons on all the five inputs, giving us a much better, much, much bigger result. So now what we've done is five by 27 multiplied 27 by 27. And the output of this is now five by 27.

So we can see that the shape of this is five by 27. So what is every element here telling us, right? It's telling us for every one of 27 neurons that we created, what is the firing rate of those neurons on every one of those five examples? So the element, for example, three comma 13 is giving us the firing rate of the 13th neuron looking at the third input. And the way this was achieved is by a dot product between the third input and the 13th column of this W matrix here.

Okay, so using matrix multiplication, we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons where all of those neurons have weights in the columns of those Ws. And in matrix multiplication, we're just doing those dot products in parallel. Just to show you that this is the case, we can take Xnk and we can take the third row.

And we can take the W and take its 13th column. And then we can do Xnk at three element-wise multiply with W at 13 and sum that up. That's WX plus B. Well, there's no plus B, it's just WX dot product.

And that's this number. So you see that this is just being done efficiently by the matrix multiplication operation for all the input examples and for all the output neurons of this first layer. Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons.

So we have 27 inputs and now we have 27 neurons. These neurons perform W times X. They don't have a bias and they don't have a nonlinearity like tanh. We're going to leave them to be a linear layer.

In addition to that, we're not going to have any other layers. This is going to be it. It's just going to be the dumbest, smallest, simplest neural net, which is just a single linear layer.

And now I'd like to explain what I want those 27 outputs to be. Intuitively, what we're trying to produce here for every single input example is we're trying to produce some kind of a probability distribution for the next character in a sequence. And there's 27 of them.

But we have to come up with precise semantics for exactly how we're going to interpret these 27 numbers that these neurons take on. Now, intuitively, you see here that these numbers are negative and some of them are positive, et cetera. And that's because these are coming out of a neural net layer initialized with these normal distribution parameters.

But what we want is we want something like we had here, like each row here told us the counts, and then we normalize the counts to get probabilities. And we want something similar to come out of a neural net. But what we just have right now is just some negative and positive numbers.

Now, we want those numbers to somehow represent the probabilities for the next character. But you see that probabilities, they have a special structure. They're positive numbers and they sum to one.

And so that doesn't just come out of a neural net. And then they can't be counts because these counts are positive and counts are integers. So counts are also not really a good thing to output from a neural net.

So instead, what the neural net is going to output and how we are going to interpret the 27 numbers is that these 27 numbers are giving us log counts, basically. So instead of giving us counts directly, like in this table, they're giving us log counts. And to get the counts, we're going to take the log counts and we're going to exponentiate them.

Now, exponentiation takes the following form. It takes numbers that are negative or they are positive. It takes the entire real line.

And then if you plug in negative numbers, you're going to get e to the x, which is always below one. So you're getting numbers lower than one. And if you plug in numbers greater than zero, you're getting numbers greater than one all the way growing to the infinity.

And this here grows to zero. So basically we're going to take these numbers here and instead of them being positive and negative and all over the place, we're going to interpret them as log counts. And then we're going to element-wise exponentiate these numbers.

Exponentiating them now gives us something like this. And you see that these numbers now, because they went through an exponent, all the negative numbers turned into numbers below one, like 0.338. And all the positive numbers originally turned into even more positive numbers, sort of greater than one. So like for example, seven is some positive number over here that is greater than zero.

But exponentiated outputs here basically give us something that we can use and interpret as the equivalent of counts originally. So you see these counts here, 112, 751, one, et cetera. The neural net is kind of now predicting counts.

And these counts are positive numbers. They can never be below zero. So that makes sense.

And they can now take on various values depending on the settings of W. So let me break this down. We're going to interpret these to be the log counts. In other words for this, that is often used is so-called logits.

These are logits, log counts. And these will be sort of the counts, logits exponentiated. And this is equivalent to the N matrix, sort of the N array that we used previously.

Remember this was the N. This is the array of counts. And each row here are the counts for the next character, sort of. So those are the counts.

And now the probabilities are just the counts normalized. And so I'm not going to find the same, but basically I'm not going to scroll all over the place. We've already done this.

We want to counts.sum along the first dimension. And we want to keep dims as true. We've went over this and this is how we normalize the rows of our counts matrix to get our probabilities.

Problems. So now these are the probabilities and these are the counts that we have currently. And now when I show the probabilities, you see that every row here, of course, will sum to one because they're normalized.

And the shape of this is five by 27. And so really what we've achieved is for every one of our five examples, we now have a row that came out of a neural net. And because of the transformations here, we made sure that this output of this neural net now are probabilities or we can interpret to be probabilities.

So our WX here gave us logits. And then we interpret those to be log counts. We exponentiate to get something that looks like counts.

And then we normalize those counts to get a probability distribution. And all of these are differentiable operations. So what we've done now is we are taking inputs, we have differentiable operations that we can back propagate through, and we're getting out probability distributions.

So for example, for the zeroth example that fed in, which was the zeroth example here was a one-half vector of zero. And it basically corresponded to feeding in this example here. So we're feeding in a dot into a neural net.

And the way we fed the dot into a neural net is that we first got its index, then we one-half encoded it, then it went into the neural net and out came this distribution of probabilities. And its shape is 27, there's 27 numbers. And we're going to interpret this as the neural net's assignment for how likely every one of these characters, the 27 characters are to come next.

And as we tune the weights W, we're going to be of course getting different probabilities out for any character that you input. And so now the question is just, can we optimize and find a good W such that the probabilities coming out are pretty good? And the way we measure pretty good is by the loss function. Okay, so I organized everything into a single summary so that hopefully it's a bit more clear.

So it starts here. We have an input data set. We have some inputs to the neural net and we have some labels for the correct next character in a sequence.

And these are integers. Here I'm using torch generators now so that you see the same numbers that I see. And I'm generating 27 neurons weights and each neuron here receives 27 inputs.

Then here, we're going to plug in all the input examples, X's into a neural net. So here, this is a forward pass. First, we have to encode all of the inputs into one hot representations.

So we have 27 classes, we pass in these integers and X inc becomes a array that is five by 27. Zeros, except for a few ones. We then multiply this in the first layer of a neural net to get lower.

Logits, exponentiate the logits to get fake counts, sort of, and normalize these counts to get probabilities. So these last two lines, by the way here, are called the softmax, which I pulled up here. Softmax is a very often used layer in a neural net that takes these Z's, which are logits, exponentiates them and divides and normalizes.

It's a way of taking outputs of a neural net layer and these outputs can be positive or negative. And it outputs probability distributions. It outputs something that is always sums to one and are positive numbers, just like probabilities.

So it's kind of like a normalization function if you want to think of it that way. And you can put it on top of any other linear layer inside a neural net and it basically makes a neural net output probabilities. That's very often used and we used it as well here.

So this is the forward pass and that's how we made a neural net output probability. Now, you'll notice that all of these, this entire forward pass is made up of differentiable layers. Everything here, we can back propagate through.

And we saw some of the back propagation in micrograd. This is just multiplication and addition. All that's happening here is just multiplying and add and we know how to back propagate through them.

Exponentiation, we know how to back propagate through. And then here we are summing and sum is easily back propagatable as well and division as well. So everything here is differentiable operation and we can back propagate through.

Now, we achieve these probabilities, which are five by 27. For every single example, we have a vector of probabilities that sum to one. And then here I wrote a bunch of stuff to sort of like break down the examples.

So we have five examples making up Emma, right? And there are five bigrams inside Emma. So bigram example one is that E is the beginning character right after dot. And the indexes for these are zero and five.

So then we feed in a zero. That's the input to the neural net. We get probabilities from the neural net that are 27 numbers.

And then the label is five because E actually comes after dot. So that's the label. And then we use this label five to index into the probability distribution here.

So this index five here is zero, one, two, three, four.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)
