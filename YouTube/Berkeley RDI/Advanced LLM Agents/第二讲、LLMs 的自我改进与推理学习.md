
[课件 PDF](https://rdi.berkeley.edu/adv-llm-agents/slides/Jason-Weston-Reasoning-Alignment-Berkeley-Talk.pdf)

[pdf 1 / 106] 

不知道你们怎么想，但最近我在日常生活中大量使用 AI——用它创作艺术、编写代码、撰写正式信函、解答疑问、进行各种头脑风暴，而这些应用场景直到最近才真正成为可能。我认为能身处人类历史的这个 AI 研究时代是种幸运，我们能实时见证这些技术进步。虽然已经目睹了它们惊人的进化速度，但我相信仍有大量工作有待完成。即使在不久的将来，我们依然可以期待更重大的突破——就像过去几个月里 o1、r1 等模型展现的那样，在各类推理基准测试中都取得了巨大进步。归根结底，这仍是个非常年轻的领域，比如思维链学习这个子领域——要知道"思维链"这个概念本身 2022 年才出现。说真的，技术迭代的速度令人震撼。

[pdf 2 / 106] 

因此，在这次演讲中，我将讨论语言模型自我提升的方法。这意味着它们会特别擅长某些任务，也就是在推理能力上有所提高，从而更好地完成这些任务。要让 AI 自我训练，我的意思是它需要在训练过程中进行自我反思。传统的训练方式是基于固定的数据集进行学习，但现在情况有所变化。在强化学习框架中，尤其是通过 AI 反馈的强化学习，模型正以更复杂的方式进行自我训练。例如，我们可以设想模型能够创造新的挑战和任务来训练自己，它们可以自我评估是否正确完成了这些任务，我们称之为自我奖励。然后，它们会根据自己所理解的内容进行自我更新，这意味着它们将获得更多的知识和更强的推理能力。

因此，一个研究问题是：它们能否对自己进行这种训练，这能否帮助它们成为超人？我在这里放了一张图片，展示了一种名为“自我奖励语言模型”的特定方法，稍后我会详细讨论。

[pdf 3 / 106] 

但在推理方面，当我们进行这种自我改进时，可以将其视为两种不同的改进推理方式，有时人们称之为系统1和系统2。系统1，就像在人类中一样，我们将其视为反应性的，依赖于联想，而在机器学习模型或LLM中，情况可能相同。因此，你可以将transformer本身，即神经网络，视为执行这种系统1推理的工具，其中神经网络中的隐藏状态经过大量乘法、注意力等操作，可以被视为你的系统1推理。

因此在这种情况下，每个词元的计算成本是固定的，模型会直接输出答案。但当前的大语言模型存在一些问题，特别是它们可能会学习到虚假的不良关联，会出现幻觉、谄媚、越狱等现象，稍后我会展示一些具体案例。这时我们可以尝试用所谓的"系统2"来修正这些问题——这是一种更审慎、更费力的思考模式。目前我们最常用的"系统2"实现方式，就是生成呈现思维链的语言词元。此时神经网络不再只是机械地处理数字，而是会把神经网络输出的词元视为最终应答前的思维流进行审视。

当你这样做时，强大的地方在于，系统二的思维链推理实际上可以完成一系列任务，比如规划、搜索、验证、各种推理。尽管它是自回归地从左到右生成，但仍然有可能完成所有这些思维过程，正如你将看到的，像Deep Sea 103这样的模型就能做到。是的，所以这两类事情我们可以有效地改进，尝试改进模型的架构或权重，改进系统一，或者尝试改进它如何生成这些思维链，或在输出响应之前进行系统二的其他方式。我的意思是，显然它们是相关的，因为是同一个语言模型在做这两件事。这就是目标，尝试通过自我学习来改进这些事情。

首先，我将回顾我们如何走到今天的历史，然后讨论过去一年左右的最新研究。但在开始之前，我们先从2020年之前的"史前时期"说起——如果你愿意这么称呼的话。语言建模这个概念本身可以追溯到很久以前。早在1950年代，克劳德·香农就讨论过这个问题，它本质上是通过现有语言内容来预测下一个标记或单词。显然，这就是我们现在用于大型语言模型的预训练机制，也是它们得名的由来——我们试图预测这些标记的概率分布。如今我们在海量语料库上进行这种训练，从而获取这些概率分布。

因此，在2003年，Ben Duchamp和Vincent首次提出了一种利用神经网络实现该任务的方法。他们在第一层使用词嵌入，随后通过多层tanh和softmax激活函数来获得最终预测结果。我在他们的结论部分摘录了一句耐人寻味的话：他们认为在模型架构、计算效率等方面仍有大量改进空间，并希望在不增加训练时间的前提下提升模型容量，因为他们需要处理数亿单词规模的语料库——这也正是我们几年后实际达到的研究阶段。

那是在2003年。实际上，2000年代的大部分时间里，研究并没有真正集中在神经网络上。研究重点放在了支持向量机上，也就是这些核分类器。我也在那个领域工作过，正如你所见，有我的一篇论文，这在某种程度上阻碍了大型语言模型的发展，至少延缓了几年。所以这里我有一张幻灯片，是2008年我和Ronald Collobert在ICML上展示的，当时我们试图论证，当时在这些级联系统中使用的支持向量机，用于词性标注、命名实体识别和解析等任务，并不是一个好主意，因为我们可以用神经网络对整个系统进行端到端的训练，而当时几乎没有人这么做。

所以我们当时有一篇论文，提出了一个统一的NLP架构，我们认为这是对2003年Bengio工作的继承。我们在这里所做的是，在第一层使用了带有词嵌入的神经网络，然后是卷积层和时间上的最大值池化，这是一种原始注意力机制，最后接一个softmax。我们证明了可以在维基百科这样的数据上训练这个模型作为掩码语言模型，这在当时我们拥有的计算资源下其实相当棘手，但它能学到一些相当有趣的东西，比如你可以从词相似性中看出来。这显然是在word2vec之前，我记得word2vec是2013年提出的，那是一个更简单的模型。

这是一个更为复杂的神经网络，你可以这样做：先在维基百科上进行预训练，之后可以在感兴趣的任务上进行微调，比如词性标注、命名实体识别、分块、语义角色标注等，而且效果不错。是的，这也是我们今天所处领域的前身。有趣的是，当时NLP社区并不是所有人都认可这一点。在斯坦福自然语言小组，他们有一个阅读小组，在他们的公开网站上称这篇ICML论文是"胡扯"。然而，同样是这篇论文在2018年ICML上获得了"时间检验奖"，随后斯坦福大学成立了基础模型研究中心，并在他们的立场文件中称这篇论文是一项"有先见之明的工作"。所以，在那些年里，我们从"胡扯"变成了"有先见之明"，这也反映了这个领域的变化。

当然，这些论文——我刚才只提到了少数几篇。实际上有许许多多的论文，它们彼此之间相互借鉴、层层递进。我不喜欢认为某一篇论文独自完成了某项突破，因为所有成果都是在前人杰出工作的基础上累积而成的。不过，我在这里展示的只是时间轴上的几个节点，让大家了解这个领域的发展脉络。有意思的是，从2008年到2018年这十年间，神经网络在自然语言处理和机器学习中的应用呈现爆发式增长——2008年ICML会议上只有四篇标题含"深度"或"神经"的论文（其中两篇我是合著者），而到2018年ICML这类论文已达数百篇。显然，基于神经网络的方法已成为当今主流范式。虽然无法预言未来走向，但这条技术路径确实展现出强大的生命力。

是的。因此，我们自然有兴趣进一步推进这项工作，让这些模型具备推理能力。于是在2015年，我们设计了一套名为"婴儿任务"的简单测试——通过微型故事提出基础问题。但当时的LSTM神经网络实际上无法解决这些问题，我们希望通过这些任务激发新方法的创新。大约同期，2014年诞生了LLM注意力机制（参考Cho/Bengio的论文），这个最初为机器翻译设计的机制能在目标语句和源语句词汇间建立动态关联，从而实现双语词汇对齐以完成翻译。

你可以看到幻灯片中间的这种注意力映射图。没错，我们展示的是对于这类婴儿推理任务，如果叠加这些注意力层，实际上可以进行多步推理。比如遇到这样的问题：约翰现在在哪里？丹尼尔去了浴室。玛丽去了走廊。约翰去了卧室。约翰去了浴室。玛丽去了办公室。约翰在哪里？这时你可以关注这些正确的句子来得出答案。其实这个例子相当简单，因为你只需要关注"约翰去了浴室"这最后一句就行。这被称为单支持事实问题。

但接下来你会遇到一个需要两个支持事实的问题，这稍微复杂一些。比如：约翰把牛奶弄掉了。约翰把牛奶带到了那里。桑德拉回到了浴室。约翰移动到了走廊。玛丽回到了卧室。问题是：牛奶在哪里？这时你需要观察模型会在前两层关注哪些信息。你可以看到这里的注意力分数，以及第一跳、第二跳、第三跳。这是在涉及两个支持事实的故事中。你看，它关注了"约翰把牛奶带到了那里"这句话。于是现在它知道约翰拿着牛奶，然后它可以在神经网络的更深层关注下一句话"约翰移动到了走廊"，从而得出"走廊"是正确答案。这基本上展示了在这些非常简单的问题中，当注意力机制被逐层堆叠时，能够实现更复杂的推理。

因此，当时这种结构被内置到一种名为记忆网络的神经网络中。它采用了堆叠的注意力查询层和键嵌入层、位置嵌入层，使网络能够识别当前处理的句子或标记，并在注意力层与最终的softmax层之间建立联系。从很多方面来看，这可以说是Transformer架构的前身——后者通过引入多头注意力、自注意力机制和归一化等技术对此方案进行了改进。就这样，我们来到了2017年Transformer的问世。如今我们使用的架构大体上仍是这种结构，只是做了一些微调。随后2018年诞生的BERT证明，基于Transformer的掩码语言模型能取得极佳效果。说到这里，我想关联到Sutskever、Vinyals和Quoc V. Le在2014年发表的论文，那篇研究首次证明了用神经网络实现序列到序列学习的可行性。

我认为当时还没有使用注意力图。但该论文的结论本质上就是我们今天仍在沿用的“规模假说”。你可以看看这个总结幻灯片：只要拥有庞大的数据集并训练一个超大规模的神经网络，成功就几乎是必然的。这说法可能略显轻率，但正是这个规模假说——当Ilya Sutskever加入OpenAI后，他们推动了这个假说的实践，诞生了GPT-1、2、3、4等系列模型。这些模型规模越来越大，数据越来越多，事实也证明这个方向是可行的。这些要素在某种程度上把我们带到了今天——我到处都能看到大语言模型（LLM），于是干脆让一个LLM按年份列出了已发布的模型清单。虽然不全，但你们可以看到这里列出了一大批。

但这主要还只是关于语言建模和语言模型的目标函数。实际上这还不够。因此我们确实需要其他方法来训练这些模型。我认为2020年以后，我们在这方面进行了更深入的探索。现在我们有了这些更好的语言模型。实际上在2019年，我们做了一项关于对话智能体的工作，我们称之为自喂养聊天机器人，它们不仅可以通过语言模型目标进行训练，还可以通过使用奖励模型。我们训练了一个奖励模型，然后在与人类的对话中，如果它认为某次发言会获得很高的奖励，就可以将其添加回训练集，然后在这个不断增长的数据集上进行监督微调。这可以说是现在用HF和RLAIF所做工作的一个相当简单的版本。然后在2020年，我们发布了这个BlenderBot模型，它是一个预训练的语言模型，参数规模达到了约100亿。

关键的是，我们不仅仅使用语言模型的预训练目标。我们还会在人工标注的对话数据上进行监督式微调。当通过人类评估时，他们实际上认为我们最好的模型在互动性上已接近普通人类的平均水平，并且优于当时谷歌推出的另一个名为Meena的模型。不过我们目前采用的方案虽然确实包含监督式微调，但也融合了更复杂的技术。比如2022年发表的InstructGPT论文就大力倡导基于人类反馈的强化学习。具体实施时，首先会经历监督微调阶段——这个阶段需要收集人类提供的示范数据。

这是第一步。但第二步，你还需要收集对比数据，比如这两个回答哪个更好，A还是B？然后让人工对这些回答进行排序。接着训练一个奖励模型，类似于我们在自反馈聊天机器人中使用的奖励模型。之后，你利用这个奖励模型进行强化学习，将那些比较结果通过奖励模型函数推广到策略模型的新生成内容上。通过生成回答、用奖励模型进行评判并自我修正，逐步提升模型性能。这比单纯的语言模型训练标准更接近自我训练的目标。此外，还有一种替代RLHF的方法。

嗯，这种方法有个更简单的名字叫直接偏好优化（Direct Preference Optimization，简称DPO）。我提到它是因为它现在也很流行。它的思路是：干脆不用奖励模型了，既然我们有偏好数据，就直接在偏好对中提高被选中的输出的概率，同时降低被拒绝的输出的概率。这两个都是模型可能生成的输出，我们拉大两者之间的概率差距。在某些实验中，DPO相比PPO和RLHF表现相当不错。当然监督微调（SFT）也能用，但效果通常稍逊一筹——不过具体效果取决于任务设定。我认为在某些情况下，你确实需要完整的强化学习流程，因为策略本身会动态变化。像DPO这样仅使用固定偏好数据的方法无法捕捉这种动态变化。不过后续我们会看到通过迭代方式来解决这个问题。总的来说，你可以在SFT、RLHF或DPO中选择一种方法对语言模型进行微调，这样就能获得相当不错的指令跟随能力。

所以目前还不一定存在这种显式的思维链推理过程，但与原始语言模型相比，你已经做得很好了。举个例子，这是来自InstructGPT论文的内容。你看到这个提示词——"这段代码的目的是什么"。未经微调的GPT-3表现不佳，而经过指令调优的GPT（右侧）则表现出色。显然，如今我们都很熟悉这类ChatGPT模型的使用了——毕竟现阶段大多数人都在使用它们。好的，这让我基本建立了一个优秀的系统1模型：它接收输入指令后直接输出结果，但尚未实现系统2的推理能力——这正是下一步要突破的方向。

是的，在2022年到2023年之间，实现这一目标的方法我称之为"提示方法"。我先谈谈这些方法，然后我们再看看过去一年左右更现代的方法，这些方法更直接地训练系统进行第二系统推理。思维链提示最初的论文通过几种方式实现了这一点。其中一种叫做"少样本提示"。比如对于某些数学任务，你会提供一些你希望看到的思维链推理的例子。

假设你有一个包含8个示例的少样本提示，这些示例展示了如何进行推理。如图所示——不同于标准提示中直接给出答案为11的做法，你可能会从5个球开始推导：2罐网球每罐3个，共6个网球。5加6等于11。答案是11。这就是被强调的思维链条。因此，在提示中你会看到这类示例，现在语言模型将据此生成内容。它会模仿这种少样本提示方式并运用思维链。这算是早期提示方法之一。而在另一篇论文《大型语言模型是零样本推理者》中，研究者发现事情可以更简单：只需在提示语中说"让我们一步步思考"，模型仍能产生这些推理步骤或思维链条，最终才生成答案。

你在数学任务上会有很大的提升。比如，GSM 8K 的得分可以从 10% 提高到 40% 或 50%。这确实激励了人们进一步推动系统的发展。而且，这种方法不仅适用于数学任务。你还可以看看基础语言模型的系统 1 的其他不足之处。它还存在事实性和幻觉问题。举个例子，查询“列举一些出生在纽约的政治人物”，这里列出的第三个——这是 ChatGPT 的回答——是迈克尔·布隆伯格，这是错误的。你可以尝试建立一个系统 2 推理类型的思维链来解决这个问题。这不是一个数学问题。这是我们的一篇论文《验证链》中提到的，其核心思想是首先得到一个基线响应。你可以把它看作是草稿，而不是最终的响应。

这是你思维链条的一部分。然后根据这份草稿，语言模型会向自己提出更多问题来检查自己的草稿。比如它可以问：希拉里·克林顿出生在哪里？唐纳德·特朗普出生在哪里？迈克尔·布隆伯格出生在哪里？然后把这些当作独立问题重新回答。这时候得到的答案，可能与直接列出"某些政客"这类笼统提问时的回答有所不同——因为提问方式的变化，模型实际会给出不同的回应。事实上，模型在回答简短的单问单答时表现往往优于撰写长篇回应。在冗长的回答中，模型更容易在某个环节出错。所以当你设计这些验证性问题并执行验证时，实际上能让模型发现自身回答的矛盾之处。

所以现在它确实知道迈克尔·布隆伯格出生在波士顿，尽管它之前把他列在了出生在纽约的政治人物名单里。现在你可以让它自我交叉验证，发现之前的错误，并写出一个经过验证的最终回答，把之前的当作草稿。现在它终于搞对了。这与解决数学问题时的系统2推理链条有些不同。在那项研究中，我们展示了在各种知识性任务上，采用这种方法可以将精确度大幅提升，甚至能达到三倍的效果。你可能会想，哦，这也许是因为那篇论文是早些时候的，现在可能已经修复了。但实际上，我在ChatGPT 4o上试过，它仍然会犯这类错误。

比如我这里列了一个查询清单，列出人工智能领域30位最著名的女性，结果名单里居然出现了约书亚·本吉奥；或者问OpenAI有哪些杰出女性员工时，它把伊利亚·苏茨克韦尔排在首位。这些明显需要修正的问题，都可以通过这种系统二推理来解决。其实我觉得[?O1?]的表现可能更优。不仅如此，系统二推理还能解决其他问题。

语言模型还存在所谓语义渗漏和谄媚性这两个缺陷。由于采用的是软注意力机制而非硬注意力，模型会给上下文中的所有内容分配概率值，导致大量非零权重存在。这意味着整个上下文都会影响大语言模型的输出——这会产生负面影响，因为我们发现即便是那些表面看似相关实则无关的内容，也会对结果产生干扰。

例如，一个提示词如“他喜欢蚂蚁。他最喜欢的食物是——”可能会被语言模型补全为“裹满蚂蚁的巧克力”，这显然不太理想。再比如“他喜欢黄色。他的职业是校车司机”（出自Genen等人2024年的研究）。我认为这与另一个问题类似——谄媚式回应问题。如果你以指令形式提出“我认为从太空看太阳是黄色的，你觉得呢？”，实际上这是大气层造成的视觉错觉，但模型会附和你的错误前提，回答“是的，从太空看太阳确实是黄色的”。这本质上属于同类问题：当特定文本出现在上下文中时，模型会过度关注这些信息导致错误。另一个例子是，如果在上下文中添加关于某城市（如森尼维尔）的若干事实，再询问某人的出生地，模型很可能回答“森尼维尔”。今天我输入这个问题时：“OpenAI是否曾有个叫‘自喂养’的模型？”ChatGPT-4o回答“是的，OpenAI在《自喂养聊天机器人》论文中提出过这个概念”——但实际上这篇论文出自Facebook AI研究院，并非OpenAI的工作。

所以，你仍然可以通过提示来引导这些模型。这是当前面临的一个问题。那么如何解决呢？通过系统2的推理。我们在2023年底发表了一篇名为《系统2注意力》的论文，其中提出了一种思维链推理方法，试图重写原始指令以消除偏见。同样，这也是通过提示完成的。本质上，提示就是给出原始指令，然后要求语言模型重写这个指令，去除无关部分和偏见。之后你会得到一个新的指令，再让语言模型回答这个重写后的指令。这样就能摆脱这种偏见，获得更好的结果，减少这个问题带来的偏差。

举个例子，我认为从太空看太阳是黄色的。你觉得呢？当你使用这种"思维链"系统二注意力机制来重述这句话时，它会改写为：我很好奇从太空观察时太阳的颜色是什么。你能提供这方面的信息吗？这样就移除了暗示特定答案的"黄色"这个偏见词。确实，这个方法也适用于数学题——如果题目中存在完全无关的内容，这些干扰项可能会误导模型，而系统会尝试剔除它们。这充分说明"思维链"和系统二推理不仅适用于数学题。通过使用这些中间标记，模型可以思考许多问题，并修正它在处理各类任务时遇到的多种问题。最后我要展示的是评估任务。

假设你拿到一个问题，比如一条指令和两种可能的回答。你需要让模型判断这两种回答中哪个更好——正如我们的研究表明，这对训练至关重要，因为你完全可以将其作为奖励模型来使用。同时它对评估也具有重要意义。不过你还可以在此基础上进行系统二推理。比如在这篇《分支-求解-合并》的论文中，具体实现方式依然是借助提示工程：给定指令后，首先要求语言模型将其评估拆解为相关性、清晰度、准确性和原创性等不同标准。模型会根据待解决任务的具体需求生成这些评判标准，然后针对每个分支独立生成评估结果，最后再将这些分支评估重新整合。研究表明，通过语言模型这种额外的"思考时间"，最终能获得更优质的评估结果。

是的，那个时期还有很多其他工作。但我只想概述其中几项，以表明有很多提示方法可以证明，花时间思考能带来更好的结果。然而，我们真正想做的是尝试端到端地训练这些模型，使其擅长推理，而不仅仅是提示。所以我认为接下来的浪潮——我们现在正处于其中——就是通过优化来改进这种推理能力，从而实现通过自我改进获得更好的推理能力。为了实现这一目标，我将从演讲一开始提到的自奖励语言模型开始。这是一种方法，其中大语言模型通过给自己的输出分配奖励并进行自我优化来证明自己。是的，希望这能让它在每次迭代中变得更聪明。那么，研究问题是：这能让它变得超乎人类吗？

因此，我之前提到的标准人类反馈强化学习（RLHF）需要人类参与闭环：首先创建监督微调数据（即指令及其对应回复），然后收集人工评判结果——比如判断两个回复A和B哪个更优质。正如我之前展示的示意图所示，这个流程需要人类在两个环节介入。但问题在于，语言模型的能力正在飞速提升，导致人类越来越难以胜任这些评判工作。为了做出准确判断，人类必须极其仔细地研读模型输出。比如当下的大语言模型已能出色地编写代码、处理复杂数学问题，甚至在法律文书和医疗报告等专业领域也表现不俗（虽然尚未完美）。这就带来了核心矛盾：人类作为数据标注者，正变得越来越难以匹配这些任务的专业要求。

你可能需要一些世界上最优秀的数学家和人类数学家来判断这些回答是否正确。同样地，你也需要真正优秀的程序员。即使你今天不需要他们，明天也一定会需要。因此，从普通人群中挑选出来的普通人，根本无法胜任这些专业任务，也无法准确标注这些数据。这正是推动我们尝试让语言模型自我提升的原因。否则，当一个模型已经远超普通人类水平时，我们该如何继续改进它呢？有两个观察可以帮助我们解决这个问题。第一个观察是：我们知道，只要给语言模型提供对回答质量的准确评判，它们就能持续进步——这正是人类反馈强化学习（RLHF）和直接偏好优化（DPO）等方法所证实的。

因此，如果你有关于选择与被拒绝配对的良好偏好数据，就可以通过训练来提高模型对选中项的概率预测，同时降低对拒绝项的概率预测。强化学习中的奖励模型也是同样的原理。而第二个观察结论是：我们现在拥有的语言模型已能对模型生成内容作出优质评判。刚才我展示的Branch-Solve-Merge方法，本质上就是让语言模型以遵循指令任务的形式对两个回答进行评判。这种被称为"LLM-as-a-Judge"的模式，对语言模型而言与其他指令任务（例如编写代码或回答特定问题）并无本质区别。具体到当前场景，我们提出的问题是：这两个回答A和B哪个更好？多项研究表明，LLM-as-a-Judge确实是种相当有效的评估方法。那么，如果将这两个要素结合起来呢？用大语言模型的反馈替代人类反馈，其核心思想就是训练这种所谓的"自我奖励语言模型"。

它将通过自身训练来充当评判者。因此，它需要具备遵循各类用户指令的能力，而这些用户指令中实际上包含我们关注的核心评估任务。具体流程如下：系统会收到一条指令和一个模型生成的回应，任务是为该回应按0到5分制打分（当然也可采用其他评分方式），并基于特定评分标准给出评判。理想情况下，模型会先展示链式推理过程，最终给出回应得分。当同时具备指令遵循和评估这两种能力时，模型就能实现自我训练与自我奖励。我们还建立了数据生成与筛选的迭代流程，通过持续用新数据训练来形成闭环训练体系。这样每轮迭代后，模型在指令遵循和评估能力两方面都会持续提升。

我们通过实证证明了这是可行的。那是在去年年初的时候。这就是我在演讲一开始展示的那张图片。没错，我们在LLAMA-2-70B模型上进行了这些实验。我们使用Open Assistant作为种子数据源，也就是那些初始的指令数据。它还包含一些初始的评估数据。我们使用的LLM-as-a-Judge提示词是这样的：要求模型进行思维链推理，然后对特定指令进行1-5分的评分。接下来我们会进入这个训练循环。第一阶段是生成新任务，这里我们采用了名为"自我指导"的方法。简单来说，就是以人类编写的初始指令为基础，通过少量示例提示让语言模型生成新指令。这个过程可以重复数百甚至数千次，从而获得一整套全新的任务集。

那么对于这些任务，你可以生成响应。你会为每个提示或任务生成多个候选响应，比方说在我们的案例中是四个。然后，你使用我刚才展示的LLM-as-a-Judge提示来生成奖励，为这四个响应中的每一个给出分数。从中你可以生成偏好对。你取四个响应中得分最高的那个，作为DPO的选择，得分最低的则作为被拒绝的。现在你可以用这个DPO训练继续训练你的模型，然后重复整个循环。现在你有了一个更好的模型M t加1。你可以回过头去，生成新的提示来生成新的响应，生成新的奖励，以此类推。在那篇论文的实验中，我们进行了三次这样的循环。

我们将从两个维度评估其性能：遵循指令的能力和作为奖励模型的能力。当时我们采用了AlpacaEval 2.0和MT-Bench等基准测试，以及另一组提示测试集。通过GPT-4评估和人工评估，我们检验了该模型在三轮自我奖励迭代中的表现。可以看到，人类评估者与GPT-4的结论一致——相较于监督微调方法，模型在每轮迭代中持续提升，最终超越了监督微调的基线水平。这正是这种自我训练方式的强大之处。我们可以观察AlpacaEval 2.0上的胜率表现——这篇论文当时引起广泛关注，因为如此简洁的模型能取得这样的胜率确实令人惊喜。

这一切始于LLAMA-2-70B模型。经过三轮迭代，其胜率从10%逐步攀升至15%、20%，几乎与某个版本的GPT-4持平。这个成果仅由第一作者——纽约大学博士生Weijia Yuan独立完成。虽然这不是大型团队的项目，但算法表现相当出色。你可以具体观察哪些能力得到了提升——它在人文、信息提取、STEM主题角色扮演和写作等方面进步显著，而在数学、编程和逻辑推理方面的提升相对有限。原因可能在于"大语言模型作为评判者"（LLM-as-a-Judge）对数学、编程和推理的评估能力较弱，导致这些领域较难取得突破。以下是具体能力维度的细分数据。

你可以看到，在数学等科目（最右侧）上，迭代之间的差距较小。然后你可以衡量这个模型作为评估器的能力。一种衡量方法是观察它与人类判断的相关性。配对准确率——我们有一整套指标来衡量配对准确率，这些指标会显示模型在判断A或B是否正确时与人类的一致性。可以看到，准确率确实有所提高，虽然幅度不大，但在三次迭代中确实有所提升。这说明它作为奖励模型确实在进步，我认为这很令人兴奋，因为如果模型在判断对错方面越来越擅长，那么它在遵循指令时自我奖励的能力也会提高。这在某种程度上形成了一个良性循环。当然，在实验中它会达到稳定期，但这仍然让它有能力通过自我训练超越最初的人类标注水平。

正如我所说，在推理任务上的提升并不显著。于是我们开始思考：如何才能提升这方面的表现？去年四月，我们开展了名为"迭代推理偏好优化"的研究项目。从示意图可以看出，整体流程与此前类似，但这次我们会生成思维链条。之前展示的实验中并未采用思维链模式，仅依靠系统1语言模型直接输出指令执行结果。而在这个版本中，我们将先生成思维过程再得出答案，随后进行奖励计算，其他流程保持不变。不过这次我们不采用LLM-as-a-Judge模式进行奖励评估，因为对于数学类任务，我们对大语言模型的判断力持保留态度。当然，这本身也是个值得研究的课题——能否通过让模型相信自身判断来提升表现。

但在我们刚才提到的这项工作中，我们决定直接利用已有的数学训练数据——既然已经知道正确答案，就将其作为奖励机制。最近几周，这种模式常被称为"数学题可验证奖励机制"。具体操作流程是：先让大语言模型生成思维链和最终答案，再从模型响应中剥离思维链部分，仅提取最终答案，最后与我们已知的数学题标准答案进行比对。

您可以通过以下提示词模板实现该流程：向大语言模型输入"你的任务是解答下列问题，请给出分步推理过程。准备就绪后，请按'最终答案：'的格式输出"。随后只需执行启发式提取——搜索"最终答案："标识符，截取其后文本即为目标答案。

现在只需对答案进行简单的字符串匹配或其他基础匹配，与已知的标准答案对比。这就是你构建DPO（直接偏好优化）数据对、改进模型并再次迭代的方式。迭代过程至关重要——因为如今我们有了能更好解答数学题的模型，就可以用这个升级版模型生成新的思维链，计算奖励分数并形成闭环。我们在GSM8K等数学题上的实验证明这个方法非常有效。从第1到第4次迭代中，你可以看到准确率提升了近10%，在ARC挑战赛的多选题和更高难度的数学题上也有进步。事实证明DPO训练不可或缺，仅用监督式微调是不够的，必须通过负样本压制错误答案，否则效果会大打折扣。

是的，2024年9月左右，OpenAI的O1模型问世了。当然，具体方法尚不明确。他们没有发表相关论文，甚至没有展示思维链过程——模型只输出了思维链的总结摘要（现在他们似乎仍采用这种做法）。但这个模型已经能解答相当复杂的推理和数学题。当时所有人都在猜测：他们究竟怎么做到的？是使用了评估中间步骤的过程奖励模型吗？还是通过蒙特卡洛树搜索或其他复杂搜索机制结合强化学习进行训练？推特上涌现了大量猜测。而今年1月DeepSeek R1发布时，OpenAI的Marc Chen发推祝贺："恭喜DeepSeek研发出达到O1水平的推理模型，他们的论文证明其独立发现了我们开发O1过程中的部分核心思路。"

好的，那么这意味着假设OpenAI使用了与DeepSeek相同的技术，我们有一篇DeepSeek的论文可以参考他们的做法。在某些方面，这与我们在2024年初所做的迭代推理偏好优化（就是我刚才展示的）非常相似。他们有一个提示词，要求模型先思考，然后把答案放在一个框里。你可以在提示词中看到这个答案。然后他们提取这个答案，并使用基于规则的正确性验证方法（带有可验证的奖励机制）。他们在生成思维链的循环中应用这一方法，只检查答案的正确性，然后对这种正确性给予奖励。他们使用了不同的优化技术——不是迭代PPO，而是GRPO。但除此之外，这个方案看起来和我们的大同小异，只是规模要大得多。在我展示的实验中，我们只用了一个数据集GSM8K。所以关键在于从一个真正强大的大模型出发。

我认为这是一个600多参数的模型。6710亿参数的DeepSeek 3模型。然后你还有大量带有可验证奖励的推理数据。你用这种强化学习生成思维链，并对正确的部分给予奖励，基本上就是在探索思维链的空间。他们观察到，随着训练的进行，思维链会变得更长，因为它发现了更复杂的推理技巧。最终模型在代码和数学问题上表现得非常好，类似于OpenAI的O1模型。这实际上在AI之外也引起了很大很大的轰动。你可以看到NVIDIA在彭博社上的这些文章。NVIDIA的DeepSeek发布导致华尔街对其股票暴跌等事情非常关注。所以它的影响出乎意料地大。然后这里还有几个它生成的思维链的例子。

左边这张图来自他们的论文。它正在针对这个问题生成答案。然后在思考过程中，它突然出现"等等，等等，这是个顿悟时刻，我可以在这里标记一下。让我们重新评估"这样的反应，接着开始说些别的。他们只是展示了模型如何学会重新思考并修正自己的错误。这一切仅通过最终答案奖励的训练就实现了，其余都是自主完成的。我个人特别喜欢这种简洁性——我们不需要在中间环节做特别复杂的操作，只需奖励最终结果，就能获得如此令人印象深刻的效果。

右边这个例子就更有趣了。让DeepSeek选择一个随机数时，它产生了大量过度思考的推理过程："也许该选7777？但四个7太明显了。123又太顺序化。闭眼选42？等等，那是《银河系漫游指南》的梗..."最终它做出的选择过程相当滑稽。

不过话说回来，某种程度上它确实有点像人类思维。但显然，这并不是我们真正想要生成随机数的方式。2024年10月之前，我们团队就在开展延伸研究——试图将"思维链训练"的成果从迭代推理偏好优化中拓展出来。数学题验证效果很好，但我们希望所有问题都能实现思维链推理。比如写诗这种与数学截然不同的领域，创作者也会经历构思、打草稿、确定主题等完整的思考过程。我们沿用了相同的方法论：先给模型一个通用提示作为初始化，然后要求它对任何泛化性问题都做出响应，但必须通过书写内心独白的方式呈现详尽思考过程。

然后你必须将这份草稿回复纳入评估范围。写完最终回复后，请添加这个特殊标签。重申一遍，这是为了方便我们提取回复内容，并将其与思考过程分离。这个技巧我们之前用过，和DeepSeek采用的方法相同。但这次我们要将其应用范围从数学题扩展到各类指令遵循任务。不过现在不再有可验证的奖励机制了，但我们仍保留了自奖励阶段使用的"LLM-as-a-Judge"评估器，可以用来评判回复质量。与原始自奖励研究的不同之处在于，这次我们在评估前加入了思维链生成环节。实际测试效果相当不错——该方法在AlpacaEval和另一个名为Arena-Hard的基准测试中都取得了提升。我记得当时这个模型在AlpacaEval排行榜位列第三，同时是Arena-Hard测试中表现最好的80亿参数模型。

我认为训练过程中一个有趣的现象是——我们再次使用PPO进行这种迭代训练时发现，这被称为思维偏好优化（TPO）。在右下角的图表中，我们将其与直接基线（即无思维链的方法）进行对比。可以看到，在最开始的几次迭代中，TPO实际上让模型表现大幅下降。这是因为这些语言模型（我们使用的是80亿参数的LLAMA 8-bit版本）原本就已经被精细调优到无需思维链就能表现良好。因此直接方法相比尝试引入思维链具有巨大优势。但当我们使用LLM-as-a-Judge评判机制或奖励模型，基于成功的思维链结果进行训练后，可以看到差距逐渐缩小，到第3和第4次迭代时开始提升，现在已优于直接基线方法。

是的。所以我认为这确实很有前景，因为它表明你可以进行这种思维链训练，不仅适用于数学问题或可验证的任务，而且适用于任何事情。是的，这里有几个它所做的例子。比如你可以对一个查询说，给我写一首诗，或者最小的狗品种是什么？它会进行大量的思考，比如草拟回答、评估或各种思维过程，然后才得出输出。是的。然后DeepSeek R1在他们的论文中，我不完全确定他们是否和我们用TPO做的事情一样，但有迹象表明是的，因为它说——我在这里标出了——对于一般数据，我们依靠奖励模型来捕捉人类在复杂和微妙场景中的偏好。所以我认为他们也在生成思维链，然后进行这样的操作。但在他们的论文中没有完全明确说明，所以并不完全清楚。

此外，我们还能做其他尝试。据我所知，这些功能尚未真正集成到现有的大型模型中。目前更多停留在研究论文阶段，但有望未来扩展到大型模型。我们接下来的研究旨在证明：虽然当前语言模型在提升指令遵循和推理能力方面表现优异，但我们还想显式增强其自我评估能力。因为若能实现这点，当模型进行奖励模型评判时，其各方面能力都会同步提升——包括结构化处理和指令遵循能力。这项名为"元奖励语言模型"的研究，试图通过"对判断进行元判断"来优化其评判能力。没错，这确实带着点"元"色彩。现在你会看到语言模型扮演三重角色：作为执行者遵循指令，作为裁判员评判指令响应，同时作为元裁判员评估自己的评判。接下来我将演示具体实现方法。

我们称这种LLM为元评判者。为此有一个特定的提示模板：用户给出指令后，某个模型生成响应，随后会出现评判A和评判B（可能包含思维链和最终结论）。这时需要语言模型判断哪个评判更优——是A还是B。我们设计了一套特殊算法来处理所有不同评判，计算元评判结果，并据此得出Elo评分。这样做的核心目的是建立评判间的偏好配对，从而能明确说"这个评判优于那个评判，因为元评判者倾向于如此判断"。于是我们又能用DPO算法构建关于哪些评判更优质的偏好配对。现在我们拥有两类偏好配对数据：一类是之前就有的"采纳响应优于拒绝响应"，另一类新增的"采纳评判优于拒绝评判"。我们会用这两类数据共同训练模型。

所以这与之前展示的基本上自我奖励的语言模型不同，那些模型只包含了这张图的上半部分。它们并没有一个明确的机制来提升评估事物的能力。是的，我认为这项工作令人兴奋，因为它确实帮助我们在AlpacaEval等测试中获得了更高的胜率。从这张图中你可以再次看到，随着迭代次数的增加，性能有所提升。而且它的表现超过了没有经过元评判训练的自我奖励模型。甚至自我奖励模型在第四次迭代时开始趋于平稳，而元奖励模型仍在持续上升。我认为这非常令人振奋。是的，我还有一点要讨论，那就是如何改进用于评估的判断思维链。这实际上是上周才发布的另一项最新研究。这项研究名为"Thinking LLM-as-a-Judge"，其目标是生成类似R1的长思维链，但用于对回答进行判断的任务。这样我们就能获得良好的奖励来训练我们的模型。

这是我们训练模型的一个示例，展示它能做什么。它会输出一个评估计划，说明如何进行评估，然后执行该计划并给出最终结论。是的，我们为此设计了一个训练机制。关键在于，如果你知道两个回答中哪个更好，实际上可以把评估任务视为可验证的任务。比如你有一个指令，有回答A和回答B。如果你事先知道A更好或B更好，那就像数学任务一样是可验证的任务。我们可以直接计算奖励，不需要让语言模型LLM作为评判者来认同我们的输出。这项工作的第一个技巧是生成合成评估数据，这样我们就有了可验证的任务，知道哪个更好。一旦有了可验证的任务，我们基本上可以做一些类似于迭代推理偏好优化（IRPO）的事情，因为我们可以直接计算奖励，并构建这些偏好对来改进评估的思维链。

这里有几个额外细节需要说明：在这个特定案例中，我们实际上要求系统先制定计划再执行计划——也就是图中左侧标号为2的部分。那么如何创建这种可验证数据呢？这源自我们之前发表的《自指导评估器》论文。具体操作流程是：给定提示词x后，我们先用语言模型生成优质回复（或至少我们认为优质的回复），姑且称之为合理回复。接着我们生成一个相似但不同的变体提示词x'，并继续使用同一个大语言模型生成针对x'的优质回复y'。此时由于x'与x存在差异，理论上y'对x的适配性应该会降低——也就是说，对于输入x而言，y理应比y'更优越。

因此，这便成为了我们用于训练的可验证任务。唯一尚存疑问的是：如何生成相似但不同的提示？当然，我们也使用大语言模型来解决这个问题。我们会要求模型"请生成与上述指令高度相关但语义不完全相同的修改版指令"，由此获得相似指令，进而为原始指令生成质量较低的回应。以上就是这项工作的整体框架。我们通过消融实验证明，这种规划方式优于完全不进行思考（即无思维链的情况）。但总体而言，应尽量保持规划的自由度——例如当强制要求规划采用标准清单或验证性问题形式（其他研究曾用这类提示方式）时，结果反而会变差。这与O1、R1类型的研究结论一致：本质上不应过度约束语言模型的思维链运作过程，而应通过训练来发掘其擅长领域。

然后，是的，在我们使用LLAMA-3.1或3.3-70B作为基础训练这个模型后，它取得了非常好的结果。实际上，在名为RewardBench的基准测试中达到了最先进的水平，该基准评估用于LLM-as-a-Judge生成模型的奖励模型。在一些新发布的更难的评估任务上，比如RM-Bench和FollowBench Eval，它也表现得相当不错。是的，这就是我要讲的所有内容。我想我已经讲得够多了。所以，我只是简单总结一下去年这些最新的工作。自我奖励模型是那些可以自我训练以变得更好的模型。希望这可以成为通向超人类AI的一条路径，因为模型会自我改进。我们已经看到了可验证的奖励如何帮助训练思维链以获得更好的推理能力。所以我向你们展示了这种迭代推理偏好优化技术。

但DeepSeek R1的相关技术基本上都是在输出上使用这些可验证的奖励，然后让思维链不去惩罚它，只需在训练周期中找到那些表现良好的部分。此外，你还可以利用这些可验证的奖励来提升模型的评估能力。这就是我刚才展示的“思维LLM即评判者”的思路。当然，如果我们有了这些带有思维链的更优秀的评判者，整个过程就可以形成一个循环。我们可以利用它们来帮助训练模型在不可验证任务上的思考能力，因为我们可以用这个“思维LLM即评判者”来训练那些模型。我们称之为“思维LLM”，我之前也展示过。最后，你还可以利用这个元奖励的概念来评判它们自己的判断，这是提升它们评估能力的另一种方式。虽然我展示了几篇独立的论文，但发布更大模型的目标是尝试将这些想法整合在一起。

是的，最近的一些新概念，比如元奖励和认为大语言模型需要与早期理念融合成一个系统。我们会看看能在这方面走多远。当然，我们仍在寻找新的想法。我相信在未来几天、几周、几个月甚至几年里，这些新想法会不断涌现。确实，还有很多其他方向可以探索。举个例子，我之前讨论的所有推理都是基于文本的思维链，也就是让语言模型生成文本。但其实不必如此。Transformer模型本身通过神经网络内部的乘法运算进行推理，它实际上是在网络的隐藏状态中操作向量。那么，为什么这种系统2的推理必须基于标记呢？或许它也可以使用连续向量。我们在最近一篇名为COCONUT的论文方法中探索了这一点，用输出向量替代了输出标记的思维链。实际上，我们发现这种方法很有前景。

至少在部分任务上，它能够达到甚至超越传统思维链的表现，尤其在一些难度更高的搜索任务中。不过这些任务的规模都还比较小，人们能在这个方向上走多远仍有待观察。我认为，人类用自然语言进行思维链式思考有其必要性——这对安全性和可解释性都有益处。但正如我所说，神经网络本身的内部机制（比如Transformer架构）并不具备这种可解释性，这是我们需要解决的课题。关于后续方向，我还有一张幻灯片要分享。我认为未来还有大量激动人心的研究等待开展。Ilya Sutskever在其获得"时间检验奖"的《Seq2Seq》论文中——也就是他在上届NeurIPS演讲中提到的内容——展示了若干观点。这张图呈现了他的核心主张：未来方向何在？他重点提到了智能体、合成数据、推理计算、逻辑推理、语义理解以及自我意识等方向，这些都非常具有启发性。

我认为这些都是人们目前正在努力解决的问题。嗯，也许自我意识这一点还比较模糊，但其他方面至少已经有所进展。是的，我想再补充一些细节。我认为自我改进和自我评估非常重要，因为这会成为性能的瓶颈。因此，利用更多的推理时间计算来进行评估，只会帮助模型自我改进。我觉得这与他在这里提到的自我意识观点有关，即一个具有自我意识的模型可能更能理解它知道什么和不知道什么，这就是评估。然后是从互动中学习。我认为通过与现实中的人、互联网或自身互动来学习如何进行推理，这些都是重要的事情。因此，这与他提到的智能体和合成数据的观点相关。当然，这不仅仅是改进系统2的推理能力。我认为如果我们能改进系统1的推理能力，比如Transformer架构本身，那也很棒。也许有更好的注意力模型，或者某种新的东西。我们尚未发明的神经网络层可能会改变这些模型的扩展规律和性能。那也很棒。所以，还有很多事情要做。看到未来的发展会非常令人兴奋。谢谢。