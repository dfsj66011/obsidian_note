
> 课程主页: https://rdi.berkeley.edu/adv-llm-agents/sp25
> [陈昕昀](https://jungyhuk.github.io/)：谷歌 DeepMind 的研究科学家

[课件 PDF](https://rdi.berkeley.edu/adv-llm-agents/slides/llm-agents-berkeley-intro-sp25.pdf)

[page 5/16]  LLM agents，其核心在于以 LLM 作为"大脑"，通过逐步推理与规划来执行动作。每次行动与环境交互后，系统会接收反馈并更新内部记忆，从而优化后续决策。因此，通过这个 Agent 框架，LLMs 能够利用外部反馈和工具进一步扩展其能力，并在决策过程中做出更明智的选择。Agent 框架中常见的组件包括工具使用和检索功能。

[page 6/16] 那么，为什么要通过 Agent 框架来增强这些 LLM 的能力呢？因为解决现实世界中的任务通常需要一个试错的过程，尤其是当我们将 LLMs Agent 部署到一个新环境中时。它需要与环境互动，以理解不同行动的成功和失败模式，从而能够更新其内部记忆。同时，利用外部工具和从外部知识库中检索信息，可以扩展 LLMs 的能力。有了这样的 Agent，你的工作流程设计也会变得更加成熟。它还有助于完成复杂的任务，包括任务分解、子任务分配、不同专业模块之间的协作分工以及多智能体激发更优响应。

[page 8/16] 近期推理模型的快速发展进一步推动 LLMs Agent 开发热潮，从去年 9 月 OpenAI 发布 o1 模型开始，我们就见证了推理能力的显著提升，12 月 Gemini 2.0 Flash Thinking 模型和 OpenAI o3 模型的发布，1 月又迎来了 DeepSeek-R1 和 Kimi k1.5 的问世。即便在同一系列模型中，不同版本之间也能看到巨大的进步——比如从 OpenAI o1 到 o3 的性能飞跃，或是将当前 Gemini Thinking 模型与之前发布的初始版本对比时可见的诸多改进。

[page 9/16] 若聚焦推理能力的进展，目前最令人印象深刻的表现在数学和编程领域，尤其是在解决高难度问题方面。例如，Google DeepMind 开发的 Alpha Proof 和 Alpha Geometry 系统，在国际数学奥林匹克（IMO）中获得了银牌（接近金牌）的成绩；而最新的 OpenAI o3 模型更是在 Codeforces 编程竞赛中展现出足以跻身人类参赛者前 200 名的实力。回望这些里程碑，恐怕在它们实现前的几个月都难以想象其可能性。

[page 10/16] 本学期我们的课程名为《Advanced LLM Agents》，我们将*深入探讨方法论部分，特别是推理技术*。那么**我们将讨论推理时间的基本技术——推理时间的扩展、训练技术，以及搜索和规划**。在应用方面，我们将更侧重于软件工程和数学。这包括代码生成和验证，以及自动形式化和定理证明。在深入探讨这些方法之后，我们将讨论 LLM Agent 在现实企业应用中的用例，以及更高级的代理工作流设计。此外，还将涉及这些 LLM 代理在现实世界部署中的安全和伦理问题。

-----------------

[课件 PDF](https://rdi.berkeley.edu/adv-llm-agents/slides/inference_time_techniques_lecture_sp25.pdf)

[page 1-2/74] 在这次讲座中，将讨论 *LLM 推理时技术*。在课程介绍中，我们会简要讨论推理模型的进展，这在某种程度上是去年 LLM 发展的亮点之一。在这里，我将更详细地探讨我们可以从这些推理模型中学到什么。幻灯片上是从 o1 博客中引用的，从这里我们可以看到，实际上 OpenAI 的 o1 模型开始在多个极具挑战性的任务上取得了相当令人印象深刻的性能，而这些任务之前的模型确实难以应对。这包括竞赛级别的数学问题，比如去年的 AIME 竞赛，以及 Codeforces 上的竞争性编程竞赛。

[page 3/74] 在最近的 o3 结果中，他们进一步展示了这一点。性能可以随着更多的推理时间计算而进一步提高。例如这里展示的这些曲线，它们在一个名为 ARC-AGI 的基准测试上得到了证明。

这是一个包含一系列谜题的基准测试，旨在评估人类或 AI 模型在给定示范示例的情况下学习某些推理模式的能力。这是一项极具挑战性的任务。对于 o1 之前的现有模型，如果我们不应用任何特殊的推理技术，只进行常规推理，那么所有现有模型的准确率都不到 25%。即使是 o1 模型，准确率也只能达到 30% 左右。但最新的 o3 模型，如果将推理时间预算设定为每任务约 20 美元，其表现可以接近人类标注员或众包工作者的平均水平。如果进一步将推理时间成本扩大到每个问题 1000 美元这样的疯狂数字，模型的准确率实际上可以达到 87.5%。显然，这是一种非常昂贵的方式来完成这项特定任务，但它展示了在不太关心推理时间计算成本的理想情况下，该模型的表现如何。

[page 6/74] 在这些推理模型的突破中，一个核心的共同理念是，我们需要某种方式来触发 LLMs 在得出最终解决方案之前生成长链的思维过程。在文献中，有不同的方法来触发这种思维链的生成。比如在最初的思维链论文中，他们设计了一个*少样本提示方案*，基本上在示例中展示了这种思维过程。后来，有一些研究表明，我们可以通过*指令提示* 来实现这一点。例如，我们可以使用指令“让我们一步步思考”。对于聊天模型来说，我们可以看到，即使我们没有给模型任何特殊指令，模型也倾向于在得出最终答案之前生成一些思考，无论这些思考是短还是长。因此，有不同的方法来训练模型实现这一点。比如在*指令微调* 中，我们实际上将这种思维链数据放入监督微调的混合中。从最近的推理模型中我们还可以看到，我们可以应用 *强化学习* 来实现这一目标。在本次讲座中，我将更多地关注推理时技术以扩展 token 预算。

[page 7-8/74] 那么在这节课中，我将讨论推理时使用的*三种推理技巧*。首先我会介绍基础提示技巧，这些技巧的共同核心是让模型能够使用更多的 token 预算来生成单一解决方案。第二部分我将探讨从多个候选方案中进行搜索和筛选的技术，其核心理念是通过拓宽探索范围来寻找解决方案。最后一部分我将讲解迭代自我优化方法，这种方法能让模型通过增加思考深度来获得最终答案。在分别讨论完这些模块后，我们还将探讨如何最优组合这些技术，从而最有效地利用推理时的计算资源来提升大语言模型的推理性能。

[page 9/74] 在标准提示中，比如我们要解决幻灯片上展示的数学题时，在思维链提示方法出现之前，常规做法是直接在提示中输入若干问答对。问题就是数学题目，而答案就是最终结果。

因此在后训练技术取得进展之前，如果我们直接采用标准提示方法，在推理基准测试上的表现会相当糟糕。根本问题在于，如果我们观察这些标准示例，它们仅提供了最终答案的格式信息，却未向模型阐明推导解决方案的内在逻辑。就像如果我们在课堂上，老师只机械地呈现标准答案而不作任何背景阐释，而学生自身又缺乏相关基础知识，这种教学方式将变得极其低效。

[page 10/74] 思维链提示的核心思想，就是要在少量示例中融入推理过程。再次以数学题为例——在这些示范案例中，会提供详细的计算步骤，向模型演示如何逐步推导直至获得最终答案。当模型接触到这种包含思维链原理的示例后，就能正确解决问题了。

[page 11/74] 实践中，模型从思维链提示中获得的效益相较于标准提示，本质上取决于其基础能力水平。幻灯片上展示的图表来自原始思维链论文及相关研究，揭示了不同模型家族中一个共同规律：*随着模型规模的扩大，思维链性能呈现出更显著的提升趋势。*

特别是，如果我们观察思维链提示与标准提示之间的差距，就会发现更好的模型在思维链生成中获益更多。这表明，一旦模型达到一定规模，其推理性能就会有所提升。需要指出的是，这些曲线数据来自 2022 年的论文，因此这些实验仅使用了预训练的 LLMs。这就是为什么应用某些提示技术非常重要，以便模型能够立即生成结果。此外，随着最近的后训练技术的发展，我们可能会看到一些不同的扩展曲线，就像最近出现的一些强大但非常轻量级的 LLMs 所展示的那样。但主要结论仍然成立，我们仍然会看到这种总体趋势。基本上，一旦模型变得更好，它就会从思维链生成中获益更多。

[page 12/74] 在后续的研究《LLMs 作为零样本推理器》中，进一步发现，即使仅使用预训练的 LLMs，我们也不必为模型提供示例来引发思维链生成。相反，我们可以通过指令来实现这一点。例如，在应用 “Let’s think step by step”  指令后，再要求模型生成答案时，模型也可以在生成最终解决方案之前产生一些思考。

[page 13-14/74] 实验证明，这种零样本思维链方法在性能上显著优于零样本方法，尤其是在更复杂的任务（如数学和符号推理任务）上。

因此，零样本思维链比少样本思维链更方便，因为我们不再需要手动标注示例。但零样本思维链的性能仍然远不如少样本思维链。于是，一个很自然的问题是：如何在不需要手动标注样本的情况下获得最佳的思维链性能？

------------


[page 15-16/74] 这正是我们在论文《LLMs 作为类比推理器》中想要实现的目标，其核心思路是：我们不直接为模型提供示例，而是指导模型先记录相关示例来解决测试问题。然后基于这些模型生成的示例，模型会尝试解决测试问题。可以想象这些自生成的示例就像是外部提供的思维链(CoT)示例。

这种设计有双重优势：首先，这些示例由大语言模型自主生成，实现了我们的初衷——完全无需人工标注。其次，由于该指令适用于每个测试问题，模型能为不同题目生成更具针对性的示例，这些示例与题目涉及的具体主题更相关，因此能更精准地适配个体问题。

[page 17/74]

我们将该方法称为"类比提示"，因为它实际上受到人类类比推理的启发。细想少样本提示(few-shot prompting)，若从人类推理角度考量，这并不超乎自然——人类并非每次遇到新问题时都会获得明确示范，而是会本能地调用过往相关经验。

在波利亚的经典推理方法论著作《怎样解题》中，对这种名为"解决相关问题"的推理方式有精彩阐述：解题时，我们会寻找通过泛化、特化或类比与当前问题相关联的已解决问题，并从中借鉴结果或方法以更好地解决新任务。

[page 18-20/74]

从人类推理视角延伸思考，除了生成示例，我们还可以指导模型生成更高阶的知识——这些知识能更精要地总结解决新问题所需要素。这类生成的知识能为问题提供更广阔的见解补充。

因此，让我们以解决编程问题为例。当面对一个新的编程问题时，我们可以先指导模型自主生成一些知识——这些知识更多是关于解决问题所需算法的高层次教程。同时，模型需要识别所需的主要算法，从而能提供相关问题的示例及其对应代码。在完成这一指导步骤，且模型生成包含高层次教程和示例的背景知识后，模型会返回去解决最初的测试问题。

[page 21/74]

结果显示，类比提示法不仅优于零样本思维链（CoT），实际上还超越了人工设计的少量样本思维链。我们在数学问题求解、代码生成以及 BIG-Bench 推理任务中都观察到了这种性能提升。

初次看到这些结果时，你可能会感到惊讶，因为这里的所有问题示例或知识都是由模型生成的。直觉上，这些内容可能比人工标注的示例包含更多错误。实证研究也确实印证了这一趋势：例如在数学问题中，模型生成的低质量示例约有 70% 是准确且相关的，但其余部分可能存在完全无关或计算步骤错误等问题。然而即便如此，即使演示样本中存在噪声，模型仍能从中显著受益。

[page 22/74]

这一发现表明，模型至少需要能够生成合理的示例，否则它根本无法从中学习。基于这一直觉，我们还对类比提示法进行了初步的规模扩展研究。这里我们比较了 GPT 文本模型系列中不同型号的表现（虽然这些模型现已停用，但趋势仍然成立）。基本上可以看到，性能较弱的大型语言模型从类比提示法中获益较少——尽管与零样本思维链相比，其表现仍持平或更优。而对于 GPT 家族中从 text-adventure-002 开始的更强语言模型，类比提示法开始超越人工设计示例的思维链提示法。

请注意，这种方法实际上甚至可能超越检索示例的效果。对于检索方法，我们的做法是针对每个测试问题，实施一种检索方法，以便在类似 GSM8K 这样的海量训练数据中找到最相关的问题。这里的一个解释是，模型生成的思维链可以更贴合这些预训练或指令调整后的大型语言模型的底层推理风格。这一发现也与当前趋势相符，即当我们尝试开发推理模型时，与其使用人工编写的思维链，我们可以采用一些训练方法，使模型能够自行发现最佳的推理策略。

[page 23/74]

现在回到指令本身。“Let’s think step by step”，为什么必须使用这句话？它有什么特别之处？在最初的零样本思维链（Zero-shot CoT）论文中，他们对不同版本的指令进行了消融研究，以触发思维链的生成。其中一些指令没有太大意义，因此表现不佳并不令人意外。但如果我们看看那些表现最好的指令类别，即指导性指令，如果我们屏蔽掉准确率数字，可能不太直观地知道哪个指令应该表现更好。甚至可以看到，即使只改变几个词，准确率数字也可能发生巨大变化。这表明当前的大型模型对提示设计非常敏感。而且在许多情况下，如何编写最优性能的提示并没有明确的原则。这就是为什么现在仍然有很多人在撰写大量关于如何进行最佳提示工程实践的指南。

基于这一观察，接下来我想提出的一个很自然的问题是：我们如何减少编写提示的手动工作？

[page 24/74]

一个想法是，既然我们的大型模型能够解决许多具有挑战性的推理问题并回答其他查询，为什么不用它来帮助我们进行提示工程或一般的提示设计呢？

在 ICLR 2023 会议上，有一篇题为《大语言模型已达到人类水平的提示词工程能力》的论文。其核心思想是运用大模型自动生成提示词，使模型生成的提示效果优于人工编写的提示词。这里展示的是该团队方法的完整流程图，包含两个关键阶段：

首先是提案生成阶段——基于任务描述，利用大语言模型生成初始指令集。当模型产出多组候选提示后，我们会通过小型问题集的预测准确率对每条指令进行评分。这个验证集的设置类似于传统机器学习中的训练集/测试集划分，但实际仅需数十至百余个样本即可完成验证。

[page 25/74]

在我们去年发表于 ICLR 的工作中，研究更进一步：不仅让语言模型进行指令提案和搜索变异，还将其作为优化器来迭代改进提示词。我们的方法核心是引导大语言模型分析历史轨迹（表现为排序后的解决方案-得分配对），从而逐步生成更优质的提示。这本质上构建了一个自然语言空间的通用优化框架，具体到提示优化场景，则体现为双模型架构：一个作为优化器（负责基于历史指令和任务样本生成新指令），另一个作为评估器（负责衡量指令的准确率表现）。

[page 26/74]

现在让我们看看如何设计元提示（meta-prompt）来实现这一目标。幻灯片展示的是针对 GSM8K 数学解题任务的元提示范例。虽然提示文本较长，但主要包含两大组件：其一是历史轨迹记录（包含过往探索过的指令及其对应准确率）...

在实践中，我们会按升序排列它们，如果上下文不足以将所有内容放入历史记录中，我们可以删除那些较早的内容。我们还提供了这些示例来说明我们想要优化的任务。基本上，在这里，例如对于数学问题，我们只需要包含这些问答对以及最终答案。因此，再次强调，无需标注任何解题思路或关于如何得出这个解决方案的解释。

[page 27/74]

至于结果，我们再次在仅预训练的大型语言模型上测试了所有这些指令。当时我们使用了提示工具。我们还尝试使用不同的模型作为优化器，包括 PaLM 模型和 GPT 模型。对于所有这些方法，我们从初始指令 "Let’s solve the problem." 开始，该指令在 GSM8K 任务上达到了约 61% 的准确率。这是一个相当通用的指令，比 “Let’s think step by step” 的指令低了约 10%。如果我们查看这个列表中最优的 LLM 生成提示，可以发现实际上 “深呼吸并一步步解决这个问题” 的指令比 “让我们一步步思考” 的指令高出约 8%。

实际上，如果你将这个表现与少样本思维链（few-shot CoT）进行比较，它与 PaLM 2 论文中报告的表现相当，其中思维链示例是由人类编写的。因此，我们可以从两个方面来思考。一方面，我们看到了另一种实现与少样本思维链相当竞争力的方法，即我们自己手动编写示例。另一方面，通过这种基于 LLM 的提示优化，不仅节省了我们手动调整提示的时间，而且实际上，当它提出新的提示时，有时还会提供令人惊讶的不同角度。例如，我可能不会想到 “深呼吸” 这样的短语会如此有效，但实证表明，通过这种优化循环的大型模型确实能够提出非常适合该模型的指令。

[page 28/74]

需要注意的是，在这个基于 LLM 的优化过程中，实际上这并不是真正的优化，因为我们在这里没有任何训练。这完全是提示驱动的。因此无法保证模型能持续生成越来越好的提示。但在实践中我们发现，这种优化曲线适用于不同模型。可以看到随着优化步骤（即迭代提示步骤）增加，准确率会逐步提升直至趋于平稳——这与传统优化曲线非常相似。

[page 29/74]

好，现在让我们回到思维链。从方法论角度看，思维链究竟为推理带来了什么？为何如此有效？究其本质，思维链提示为我们提供了一种动态计算过程，使得用于解决问题的 token 数量能够适配不同难度级别的任务。

对比标准提示方式：对于相同任务，若最终答案相同，直接提供答案意味着强制模型使用相同推理时间预算来解决问题，而忽略任务难度差异。但思维链机制表明，面对更复杂的问题（例如需要更多计算步骤的数学题），模型可以在 token 空间执行更多推理步骤来更好解决问题。研究论文显示，即使示例中的数学题非常简单，该方法也能推广到需要更多推理步骤的测试题。

在高层推理策略方面，思维链整合了诸多对人类解决复杂任务至关重要的推理过程，如任务分解、规划等。若要进一步提升模型性能，特别是当我们明确特定任务所需的推理策略时，可以直接指导大语言模型采用预期的问题解决策略。

[page 30/74]

因此，进一步推进这种分解策略的一个思路被称为"从简至繁提示法"。该方法的目标是通过明确告诉模型分解原始问题的最佳实践，实现从易到难的泛化能力。在幻灯片中，我展示了这个解决数学问题的示例。基本上，这篇来自谷歌 DeepMind 同事的原创性论文包含两个阶段：第一阶段进行问题简化——给定数学题时，先将问题拆解为更简单的子问题，使得每个子数学题更易求解，所需计算步骤更少；随后模型就能按顺序解决这些子问题，并通过整合解决方案来攻克原始问题。

[page 31/74]

这项研究最令人瞩目的成果在于：通过这种从简至繁的提示设计，首次证明我们无需构建神经符号框架或依赖程序执行，就能以接近完美的准确率解决组合泛化基准测试。论文中展示的 SCAN 基准测试结果就是例证——该测试要求将程序化生成的自然语言指令翻译为动作序列，其最大挑战在于长度分割设置：测试集中的动作序列全都长于训练样本，导致传统序列到序列学习方法的测试准确率不超过 25%（除非采用神经符号技术）。然而使用当时的尖端模型 code-davinci-002 配合从简至繁提示法，研究者仅需 0.1% 的训练样本作为示例，就实现了近乎完美的测试准确率，既高效又性能卓越。

[page 32/74]

我们在去年 ICLR 发表的后续研究中进一步证明，这一思路可扩展至更复杂的组合泛化场景，例如将自然语言翻译为现实编程语言。正如幻灯片所示案例：将自然语言问题转换为 SPARQL 查询语句。

因此，这里的主要挑战在于，对于更贴近现实世界的语言（包括自然语言或编程语言），它们的语法规则更为复杂，词汇量也更大。这意味着，特别是如果我们没有一个具有超长上下文的模型，那么单个提示将不足以涵盖解决任务所需的所有语法规则。实际上，这一挑战也体现了分解的一个优势，即允许模型在不同推理阶段使用不同的提示。同时，在智能体开发的背景下，这也使得模型能够为每个子问题或子任务使用定制化的提示。

[page 33/74]

以下是我们的动态最少到最多提示方法的概述。有些细节在这里并不太重要，比如我们如何进行分解。但我们添加的所谓动态选择阶段是为每个子问题动态选择示例。基本上，对于每个测试句子，我们可以发现那些具有最相关语法规则的示例问题。这样，我们就能确保对于句子的每个部分，总能检索到最有用的语法规则来解决任务，从而使模型有足够的上下文来解决问题。

[page 34/74]

所有这些组合性的三个问题基准，如 CFQ 数据集，再次表明我们的动态最少到最多提示方法优于所有基线方法，这些基线方法需要在完整训练集上训练模型。此外，如果我们把动态最少到最多提示与思维链提示和其他提示方法进行比较，当增加示例问题以生成少量示例时，它的扩展性也更好。

[page 35/74]

到目前为止，我们已经讨论了如何通过少量示例指示的推理策略来指导模型解决问题。但如果我们深入思考，可以假设对于不同的推理任务，它们可能需要非常不同的推理结构。

例如，分解任务和规划每个阶段会有不同的方法。同时，我们可能不想为每类任务都标注所有这些最佳实践。再者，我们也不希望为每个问题手动标记范例。因此，在我们去年发表在 NeurIPS 上的自我发现工作中，我们基本上是指导模型构建特定任务的推理结构，而无需手动编写示范。在实践中，我们的做法是列出一些推理策略的最佳实践，比如思维链推理、任务分解、自我反思等，然后针对不同的新任务，模型会思考如何利用最适合当前任务的推理策略，并构建推理结构，按照这个结构来解决问题。

[page 36/74]

在评估方面，我们也展示了这种自我发现方法在多个基准测试中优于思维链提示和其他基线方法，包括BIG-Bench-Hard 推理任务、MATH 基准测试以及其他一些推理基准测试。

[page 37/74]

总结一下，在第一部分中，我首先谈到了思维链生成的基本理念，即允许模型进行可变计算以触发思维过程，这种思维过程可以适应不同难度级别的任务。我们还讨论了如何在推理时提高思维链的表现。在最初的思维链论文中，我们谈到了通过标注思维来进行少量示例提示的想法，以及通过指令提示来触发思维链生成。此外，我们还讨论了如何指导大型语言模型自动设计提示并自行发现更好的问题。值得注意的是，这种与大型语言模型交互的最佳实践会随着时间的推移而演变。例如，最近随着后训练大型语言模型的出现，你不必为每个任务都提供示例让模型学习如何执行。对于某些模型来说，最好只是提供非常清晰的指导和要求，然后希望模型能够遵循你的指令。

有些模型更喜欢互动讨论式的交流，而有些模型则可能更倾向于一次性接收详尽的长篇指令。但探索优质推理提示策略的核心原则依然适用。其中有两个关键标准：我们希望这种方法能激励模型在面对复杂任务时生成更长的思维链，同时要求策略能随任务难度灵活扩展。此外，提示技术还需支持任务所需的特定推理策略。

[page 38-39/74]

现在让我们进入第二部分内容：如何从多候选方案中进行搜索筛选，并通过拓展探索广度来扩大解决方案空间。在完成第一部分讲解后，大家觉得当前方法还存在什么不足？

其实这个问题并不难回答，因为我已经展示了本讲稿的框架。从第一部分可以看出，我们不应限制大语言模型对每个问题仅生成单一解决方案——毕竟单次推理可能出现错误。相反，应允许模型探索多个分支路径，使其能在单次生成中自我纠错。拓宽解决方案探索主要有两种方式：其一是针对每个问题生成多个候选答案；其二是在每个推理步骤允许模型产生多个潜在后续步骤。

不难想象，通过这种多样本生成方式，模型性能上限必然提升——因为我们赋予了模型选择更优解的机会。但这类方法面临的核心挑战在于：如何从多个候选中筛选最佳响应？因为在现实应用中，我们无法在推理时获得"先知学者"的指导。例如在聊天界面中，虽然可以让模型生成多个方案供用户手动选择（这属于优选模式），但更理想的情况是模型能自主完成筛选，避免人工审查数十个解决方案。

[page 40/74]

沿着这个思路，我想先介绍谷歌 DeepMind 同事提出的"自我一致性"研究。以现今视角看，这个构思异常简单却极为有效，能显著提升各类模型的性能表现。假设我们要解决数学问题...

而这一次，我们允许模型生成多个响应。因此，在获得所有这些响应后，我们仍然不知道哪一个准确。但在这里，我们有一个基于一致性的选择标准。也就是说，对于这个最终答案为单个数字的数学问题，我们会查看这三个解决方案。其中两个的答案是 18，另一个是 26。因此，18 是最一致的答案，所以我们选择对应的响应，即出现频率最高的答案。需要注意的是，这里的一个重要点是，这种选择仅基于最终答案，而不是推理过程。模型可以采用任何它想要的推理过程，只要它们得出相同的最终答案，我们就认为它们都是好的。

[page 41/74]

这是一个相当简单的想法，但实际上，它在不同的模型和基准测试中都提升了性能。在他们的论文中，他们还评估了当时所有可用的模型，特别是在数学问题上，这些模型显示出非常显著的性能提升。

[page 42/74]

现在，让我们再次用自一致性方法来讨论这种扩展效应。在这里的曲线中，他们与样本排序基线进行了比较。这实际上是一个非常自然的基线，它表示我们可以选择具有最高对数概率的响应。因此，这里的假设是，如果模型对当前响应给出了更高的概率，它应该对此更有信心。但实际上，如果我们看这些曲线，可以发现自一致性的性能扩展比基于概率的排序要好得多。因此，随着响应数量的增加，比如达到 40 个响应，自一致性的性能仍在提升。而这个样本排序基线在约 10 个响应时就已经停止提升了。

但有一个例外。这个例外就是，如果我们把模型训练成一个对所关注任务非常优秀的验证器，我们或许能通过自我一致性超越这条扩展曲线。这一点我们稍后会详细讨论。

[page 43/74]

另一个促成自我一致性成功的关键因素是，我们需要模型生成多样化的响应，以便进行这种基于一致性的选择。实际上，在这些结果中，他们还对比了不同的基线方法。束搜索（beam search）在自然语言处理领域是一种非常经典的搜索算法，尤其是在大语言模型（LLM）出现之前。其基本思路是，在每一步解码时保留概率最高的前 k 个标记，最终选择整体概率最高的句子。他们还比较了不同的采样基线方法，其中通过对提示的不同变体应用贪婪解码来生成响应。正如之前提到的，大语言模型对不同的提示非常敏感。此外，他们还比较了在提示中排列示例顺序的不同方式对性能的影响。结果表明，使用采样的自我一致性方法在样本量增加时表现更好。这里的关键在于，采样方法需要确保响应的多样性。我们可以通过提高采样温度或合理设置参数进行核采样（nucleus sampling）来强制实现这种多样性。

[page 44/74]

这项研究的一个分析非常有趣，它涉及模型的校准问题，即基于一致性的选择如何提升模型性能。当模型对某一任务能生成更一致的响应时，这意味着什么？他们通过一条曲线展示了准确性与一致性之间的相关性。可以看到，如果样本响应中有更多指向相同的最终答案，那么一致性百分比就更高。从中我们可以得出两个结论：一是大语言模型对其最终预测结论更有把握，即使这些结论是通过不同的推理过程得出的；二是经验表明，这种针对更一致响应的聚合解决方案更有可能准确，最终答案可能就是原始测试问题的正确答案。这也解释了为什么一致性可以作为一个非常简单却强大的标准来选择最终解决方案。

[page 45/74]

在自洽性论文中，研究者们将这种方法应用在了基于测试的推理问题上。实际上，这种基于一致性的筛选方法在代码生成领域同样效果显著。我们在谷歌 DeepMind 研发的 AlphaCode 系统就是一个相当复杂的案例，它包含了模型训练和推理阶段的多项技术。不过在这里，我将重点介绍推理阶段的核心技术——筛选与聚类。特别是这个聚类环节，本质上就是根据代码执行结果的一致性来进行筛选。

[page 46/74]

首先让我们简要了解一下编程竞赛的问题背景。通常这类题目会包含一个冗长复杂的问题描述，交代背景故事和编程要求，同时附带若干输入输出示例作为测试用例。要判定解决方案是否正确，生成的代码不仅要通过题目中提供的测试用例，还要能通过通常规模更大、难度更高且包含边界案例的隐藏测试集。

[page 47/74]

由于我们已经掌握部分输入示例，可以先筛除无法通过给定测试的代码。但剩余程序仍可能在隐藏测试集上失败——参加过编程比赛的人应该深有体会：明明觉得代码完美，却总在某个隐藏测试用例上栽跟头，最终与理想分数失之交臂。

AlphaCode 系统的解决方案是：先训练大语言模型为题目生成新的测试输入，然后在所有模型生成的测试案例上运行候选程序。接着将所有输出相同的程序归入同一聚类。其核心假设是：只要模型生成的测试输入足够多且质量够高，那么同一聚类中的所有程序在语义上就是等价的——因为它们产生了完全相同的执行结果。最终我们从十大聚类中各抽取一个程序样本。

[page 48-49 /74]

评估数据显示，相较于单纯筛选，这种聚类方法能带来额外的性能提升。但显然，这与 Oracle 选择仍存在差距。因为我们仍无法保证一致性响应恰好就是候选池中的最佳答案。

现在让我们重新审视这种自洽解码流程。其中一个局限在于：原始自洽方法需要答案提取步骤才能对候选响应进行聚合。若要将这种基于一致性的解码推广到更广泛场景，自然引出的问题是——对于自由格式生成任务（这类场景难以天然支持聚合与答案提取流程），我们该如何实现？这正是我们通过"通用自洽"研究想要突破的方向。

[page 50/74]

核心思路是摒弃答案提取步骤，转而让大语言模型执行基于一致性的选择。具体而言，我们向模型发出指令，要求其根据多数共识原则选择最具一致性的响应，同时需要综合审视所有候选答案。其内在逻辑在于：虽然模型难以自行判断答案正确性，但一致性理应成为更易衡量的标准。

[page 51/74]

通过实证研究，我们在不同应用场景验证了通用自洽方法的有效性。对于摘要生成和问答等自由格式的低风险任务（其事实性要求最终答案），该方法在原始自洽无法直接适用的基线模型上仍能实现提升。而在数学推理和编程等适用原始自洽的场景中，通用版本无需答案提取和代码执行就能达到同等性能。值得注意的是，该方法的性能受限于长上下文处理能力——我们当时使用的早期模型在长上下文推理表现欠佳，因此当提示中包含更多响应时，其性能提升幅度可能不及原始自洽方法。

但我们可以看到，随着长上下文模型的最新发展，这种基于模型的选择方法将能够随着响应数量的增加而更好地扩展。实际上，对于大多数任务来说，候选池中只需约 10 个响应就能很好地解决问题，因此这仍然展示了基于模型的一致性选择的实用性。

[page 52/74]

之前我们讨论过，对于基于模型的选择，使用原始模型的对数概率进行排名的方法，其扩展性不如基于一致性的选择。为了进一步改进这种基于一致性的选择，一个想法是我们可以训练一个大型语言模型作为排名器。我们希望至少这个排名器能比简单的一致性标准表现得更好。实际上，它可以对哪些响应更可能是准确的有所感知。

在幻灯片上，我展示了来自 OpenAI 原始论文的图表，该论文介绍了 GSM8K 数据集。他们还展示了如何训练一个验证器来判断匹配解决方案的正确性。后来，他们又发表了后续论文《让我们逐步验证》。总的来说，对于这类基于 LLM 的排名工作，有两种设计基于 LLM 的方法来评估答案质量。第一种是最初也是最标准的方法，称为结果监督奖励模型。它会在解决方案层面验证正确性，例如给定一个数学问题和解决方案，模型会告诉你解决方案是否正确。在《让我们逐步验证》论文中介绍的另一种方法称为过程监督奖励模型。其思想是，我们可以在步骤层面验证每个步骤的准确性，而不仅仅是在解决方案层面。

[page 53/74]

他们的研究表明，与基于结果的奖励模型和多数投票基线相比，这种过程监督奖励模型在样本数量增加时表现更好。但显然，这里的性能高度依赖于验证器的质量，而且同一个验证器可能无法跨任务泛化。因为我们知道，基于一致性的标准非常简单，实际上可以直接应用于许多任务。但对于这些模型，你需要设计良好的训练策略和训练数据，使其真正能在你关注的各个领域发挥作用。

[page 54/74]

目前我们仅从解决方案层面讨论了响应选择——即只在完整响应生成后才进行筛选。但假设我们拥有一个优秀的逐步评分器，那么这种基于解决方案的响应选择实际上浪费了逐步评分器的潜力。因为借助这种更强大、更细粒度的验证器，我们本可以做得更多。

如果我们确实拥有这种优质的逐步评分功能，就能结合大语言模型进行树状搜索。其核心在于：无需等待解决方案完全生成，在搜索过程中即可优先探索步骤层级的局部解决方案，并在其他流程前优先发掘更具潜力的步骤。幻灯片上展示的图表来自《思维树提示法》论文，通过图示能清晰对比不同提示方法的差异——链式思维提示如同单线列车，自洽方法类似多链并行，而思维列车则在每一步都可分叉探索后续步骤，无需完整走完所有路径。

[page 55/74]

接下来我将以"24点游戏"为例演示如何应用思维树理念。每一步包含两个关键阶段：首先是"思维生成"，即提示大语言模型提出可能的后续思考步骤。例如在 24 点游戏中，这些步骤形式简单——只需选择两个数字进行运算即可。然后是"思维评估"阶段，再次调用大语言模型评估当前状态的发展潜力。比如需要判断经过所有运算后是否可能得到 24 这个最终结果。

[page 56/74]

此前我们已演示过如何在步骤层级进行评估，这种方法能精准判定每个独立步骤的质量。但与我们之前关于该评分函数基于一致性的选择讨论相关，我们也可以将其视为一种从候选状态中筛选最佳方案的方式。我们无需让模型仅在单点层面做出决策。其核心理念在于，可以要求大语言模型从所有可能的后续步骤中选出最优响应。此外，这种方法支持多轮投票机制——由于所有结果都基于大语言模型采样，通过汇总不同投票结果，模型最终能以多数表决确定后续执行方案。

[page 57/74]

在结果展示部分，我再次列出24点游戏评估指标。原论文中还呈现了其他基准测试数据，可见采用广度优先搜索的"思维树"方法，在 token 预算效率上显著优于标准提示法和思维链策略。更进一步说，我们显然能整合更先进的搜索算法。原始"思维树"论文仅使用了最基础的 BFS 和 DFS 算法，但后续研究已开始引入蒙特卡洛树搜索等方法。另有工作尝试训练独立评估函数，而非单纯依赖模型提示。此处关键仍在于设计优质语言模型及提示方案，以确保自我评估的有效性。

[page 58/74]

总结第二部分内容：我展示了通过在解空间进行多分支采样，能进一步扩展推理时的计算能力。我们探讨了基于一致性的选择机制——这个看似简单却高效通用的原则；介绍了通过边缘化推理过程、依据最终答案进行筛选的自我一致性方法；针对代码生成任务，还可实施基于执行一致性的重排序策略。当大语言模型的自我评估机制能精准判断局部解决方案或阶段性结构质量时，这种搜索方法将更具扩展优势，同时通过实时淘汰低潜力过程来降低 token 消耗。

[page 59-60/74]

最后我将探讨通过迭代自我提升来深化解决方案的探索深度。因此，即便是最新的先进模型（包括那些近期发布的模型），我们也能看到它们在解决复杂数学和编程问题的竞争性基准测试中取得了令人惊艳的成绩。但这些模型有时仍会犯相当明显的错误，人们对此也持续提出质疑。不过反观人类自身，我们初看问题时也常会犯些低级错误。

之前我们讨论过可以通过采样多个解决方案，来减少单一预测带来的错误。但细想之下，这种纠错方式其实并不高效——因为这些响应都是并行生成的，模型无法从过往错误中学习，可能导致相同错误反复出现。在本次关于推理时自我提升的环节中，我将重点介绍如何让大语言模型对给定任务进行迭代式自我改进。这种方法更符合人类纠错的思维过程。

[page 61/74]

相关开创性论文包括《Reflexion》与《Self-Refine》等研究，其核心流程包含两个关键步骤：模型生成解决方案后，会基于观察结果生成反馈（此阶段可结合外部评估机制）。例如在《反思》论文中，研究者构建了智能体框架——大语言模型作为智能体向环境提交动作，环境则通过状态变化提供外部信号，以此判断当前策略是否可行。模型随后会综合自我反思的反馈与外部观察信号，对下一轮预测进行校准优化。

[page 62/74]

研究表明，当存在高质量评估机制或可靠外部信号时（如 ALFWorld 导航任务中清晰的步骤观测信号），这种自我反思与精炼机制效果显著。研究团队在 HotPotQA 问答任务中也验证了该方法——他们将传统问答重构为智能体环境，假设模型生成答案后会收到环境提供的正误反馈信号。这就是为什么随着尝试次数增加，你能持续看到这种改进。我们稍后还会重新审视这个假设。

[page 63/74]

如果我们把这种自我改进循环应用到程序开发中，代码生成其实是个非常自然的用例。因为当我们编写代码时——以我们自身为例——在集成开发环境中调试代码会更高效。我们并非写完代码就停止，而是深度依赖这种交互式循环来检查代码执行结果，判断逻辑是否合理，以及哪些部分需要后续修改。这同时也是近期智能编程助手的核心理念之一。

[page 64/74]

在我们的《自我调试》论文中，我们深入探讨了哪些反馈格式能帮助模型更好地自我纠错。最基础的反馈信息是仅告知当前代码正确与否的简单提示；单元测试反馈则包含执行结果，比如测试是否通过或存在运行时错误；代码解释反馈要求模型用自然语言逐行说明实现逻辑——这个思路源于人类调试时"自言自语"的习惯；最后的执行追踪反馈则让模型自行模拟逐行执行过程，这也是程序员常用的调试方法。

[page 65/74]

实证研究中，我们评估了去年论文发表时的最新大语言模型。数据显示，这种自我调试循环能持续提升不同能力层级模型的表现，且信息量更丰富的反馈能带来额外增益。

[page 66/74]

现在回到问答任务的自我修正。先前我们讨论过热饮知识库（HotPotQA）的结果，其他研究也显示该方法在数学解题等推理任务中有效。但所有这些显著改进都依赖于"先知验证器"的假设——而现实中，这种能判定绝对正确答案的验证器往往不可得，就像参加数学考试时你不可能随时获得标准答案。否则，考试对你来说就太简单了。

所以，我们在这里想研究的问题是：如果我们不给模型提供关于答案正确性的非常明确和准确的外部反馈，这些大语言模型的表现会如何。

[page 67/74]

在我们去年发表在 ICLR 上的论文《大语言模型目前还无法自我纠正推理》中，我们实际上展示了这项研究的一些负面结果。我们与这个 Oracle 基线进行了比较，其中我们利用了问题的真实答案。与之前的工作类似，我们展示了巨大的改进。然后我们研究了真正的自我纠正情况，即不允许模型利用任何 Oracle 反馈。模型需要自行判断答案的正确性。这些结果我们展示在底部的表格中。我们可以看到，这里的主要问题是因为大语言模型只能判断其预测的正确性，它经常会把原本正确的解决方案变成错误的。这就是为什么在没有外部评估的自我纠正过程中，每一轮之后性能反而会变得更差。

[page 68/74]

通过这项研究，一个自然而然的问题是：也许我们没有很好地调整反馈提示，所以才会失败。为了解决这个问题，我们还设计了这个反馈提示的几种变体。需要注意的是，所有这些反馈提示都非常通用，它们并不是针对某一项任务专门调整的。我们可以看到，如果我们只有这个简单的通用反馈提示，如果我们没有更多的模型训练，那么编辑这个反馈提示确实会影响自我纠正的行为。但这更多是关于调整模型保留初始答案的倾向。所以在某些情况下，实际上在反馈提示更新后，推理性能并没有太大下降。但在这种情况下，这也没有帮助模型在初始性能的基础上有所提高。

[page 69/74]

之前我们讨论了如果我们只对一个初始答案进行自我纠正的比较。我们进一步与另一种方法进行了比较，基本上我们是在多个答案之间进行自我纠正。为此，我们与多智能体辩论基线进行了比较。其思路是我们让模型同时生成多个答案，然后让大语言模型审视这些多个答案并给出一个更新后的答案。对于这种多智能体辩论，我们将与自我一致性进行比较。回想一下，自我一致性只是完全并行生成的解决方案，并选择最终答案最常见的那个。在之前的多智能体辩论论文中，它们展示了一些相对于自我一致性的改进。

但我们发现主要问题在于他们在 token 预算方面缺乏严谨的比较。实际上，在多智能体辩论中，这个特定实验每轮会产生三个响应。但基本上，如果我们保持相同的响应数量和预算，就会发现自洽性方法的表现仍然优于多智能体辩论。因为多智能体辩论的性能在某个阶段会停止提升，而随着响应数量增加，自洽性方法在 GSM8K 数学基准测试中的表现仍在持续改善。

[page 70/74]

基于这些讨论，核心问题在于如何通过整合不同方法来最优利用 token 预算。本质上，我们需要理解如何平衡并行或顺序生成多个样本的推理预算。这实际上是个相当复杂的问题，因为它高度依赖于具体任务和模型特性。关键在于评估模型是否擅长针对特定任务进行自我反思和修正。

这里展示了我谷歌同事的一项案例研究：他们通过训练专门修正数学解题步骤的模型，针对不同难度题目研究了并行生成与顺序生成的最佳比例。具体数值会因模型差异而变化，但总体结论是：对于简单题目，模型能从自我修正中获益更多，因为它能更清晰判断当前解法错误并修正；而对于难题，需要在并行生成和顺序优化之间找到平衡点。研究表明，这种根据不同题目难度调整的计算优化曲线，表现优于完全并行的生成方式。

[page 71/74]

在考虑推理时间计算时，另一个关键因素是模型规模。直观来看，在相同浮点运算预算下，轻量级模型能生成更多解决方案。如果昂贵模型的性能提升有限，调用高成本模型进行推理就不是最优选择。根据论文《推理缩放法则》的实证研究显示：在不同推理预算下，最优模型选择会发生变化——当推理时间预算较小时，可能更需要依赖轻量级模型。

但归根结底，当预算更充足时，你或许可以采用更昂贵的模型，这些模型很可能在应对那些极具挑战性的难题时带来性能提升。而使用较小模型时，无论如何都很难得出最终解决方案。

[page 72/74]

在本讲中，我们主要探讨了如何通过增加 token 预算来实现推理阶段的扩展。我们讨论了基础提示技术，包括通过搜索和选择来拓宽解决方案空间的探索广度，同时也探讨了迭代自我优化的方法。

需要再次强调的是，整合大语言模型的最佳实践应根据其实际能力进行调整。你必须综合考虑底层模型的能力特性以及手头任务的具体需求，思考如何最优整合这些不同技术，为你的任务设计出更高效、更有效的范式。

[page 73-74/74]

最后我还想谈谈设计高效推理技术的通用原则——这些原则实际上对推理阶段技术和训练阶段技术都具有指导意义。相信在座许多人可能都听说过理查德·萨顿提出的《苦涩的教训》。如果还没读过，我强烈建议大家阅读这篇网页文章。可以说，这是指导当今众多机器学习工作的重要准则，特别是在大语言模型和推理技术快速发展的当下。虽然原文篇幅较长，我将摘录部分最具启发性的观点：我们真正需要开发的是那些能随着算力增长持续扩展的方法。当提升 AI 智能体时，我们应该教会模型去发现未知领域，而非仅传递已知知识。关键在于建立某种机制，能够触发模型持续自我完善的能力，并掌握正确的推理方法。

本次课程到此结束，感谢聆听。