
> 课程主页：https://rdi.berkeley.edu/adv-llm-agents/sp25
> 陈新云：谷歌 DeepMind 的研究科学家

[page 5/16]

大语言模型智能体。其核心在于以大型语言模型作为"大脑"，通过逐步推理与规划来执行动作。每次行动与环境交互后，系统会接收反馈并更新内部记忆，从而优化后续决策。因此，通过这个智能体框架，LLMs 能够利用外部反馈和工具（当可用时），从而进一步扩展其能力，并在决策过程中做出更明智的选择。智能体框架中常见的组件包括工具使用和检索功能。

[page 6/16]

那么，为什么要通过智能体框架来增强这些大语言模型的能力呢？因为解决现实世界中的任务通常需要一个试错的过程，尤其是当我们将 LLMs 智能体部署到一个新环境中时。它需要与环境互动，以理解不同行动的成功和失败模式，从而能够更新其内部记忆。同时，利用外部工具和从外部知识库中检索信息，可以扩展 LLMs 的能力。有了这样的智能体，你的工作流程设计也会变得更加成熟。它还有助于完成复杂的任务，包括任务分解、子任务分配、不同专业模块之间的协作分工以及多智能体生成。

[page 7/16]

许多人认为 2025 年将是智能体的元年。如今，我们已经看到各种应用的 LLMs 智能体爆发式增长。在许多领域，这些智能体实际上改变了人们之前对 AI 模型行为的认知。一些非常成功的演示和实际应用案例包括代码生成、计算机使用、个人助理和机器人技术。而且，未来这个应用列表还会不断扩展。此外，在教育、金融等领域，我们已经看到了一些相当不错的基于 LLMs 的智能体出现。

[page 8/16]

另一个进一步推动 LLMs 智能体开发热潮的因素是近期推理模型的快速发展，从去年 9 月 OpenAI 发布 o1 模型开始，我们就见证了推理能力的显著提升。在此期间，例如上个月我们看到了 Gemini 2.0 Flash Thinking 模型和 OpenAI o3 模型的发布，而这个月又迎来了 DeepSeek-R1 和 Kimik1.5 的问世。即便在同一系列模型中，不同版本之间也能看到巨大的进步——比如从 OpenAI o1 到 o3 的性能飞跃，或是将当前 Gemini Thinking 模型与上个月发布的初始版本对比时可见的诸多改进。

[page 9/16]

若聚焦推理能力的进展，目前最令人印象深刻的表现在数学和编程领域，尤其是在解决高难度问题方面。例如，去年我的 Google DeepMind 同事开发的 Alpha Proof 和 Alpha Geometry 系统，在国际数学奥林匹克（IMO）中获得了银牌（接近金牌）的成绩；而最新的 OpenAI o3 模型更是在 Codeforces 编程竞赛中展现出足以跻身人类参赛者前 200 名的实力。回望这些里程碑，恐怕在它们实现前的几个月都难以想象其可能性。

[page 10/16]

本学期我们的课程名为《Advanced LLM Agents》，我们将深入探讨方法论部分，特别是推理技术。那么我们将讨论推理时间的基本技术——推理时间的扩展、训练技术，以及搜索和规划。在应用方面，我们将更侧重于软件工程和数学。这包括代码生成和验证，以及自动形式化和定理证明。在深入探讨这些方法之后，我们将讨论 LLM 代理在现实企业应用中的用例，以及更高级的代理工作流设计。此外，还将涉及这些 LLM 代理在现实世界部署中的安全和伦理问题。

[page 1-2/74]

在这次讲座中，我将讨论 LLM 推理的 Inference-Time 技术。在课程介绍中，我们会简要讨论推理模型的进展，这在某种程度上是去年大型语言模型发展的亮点之一。在这里，我将更详细地探讨我们可以从这些推理模型中学到什么。在幻灯片上，我展示了一个结果，这是从 o1 博客中引用的。从这里我们可以看到，实际上 OpenAI 的 o1 模型开始在多个极具挑战性的任务上取得了相当令人印象深刻的性能，而这些任务之前的模型确实难以应对。这包括竞赛级别的数学问题，比如去年的 AIME 竞赛，以及 Codeforces 上的竞争性编程竞赛。

[page 3/74]

在最近的 o3 结果中，他们进一步展示了这一点。性能可以随着更多的推理时间计算而进一步提高。例如，我这里展示的这些曲线，它们在一个名为 ARC-AGI 的基准测试上得到了证明。

因此，这是一个包含一系列谜题的基准测试，旨在评估人类或 AI 模型在给定示范示例的情况下学习某些推理模式的能力。这是一项极具挑战性的任务。对于 o1 之前的现有模型，如果我们不应用任何特殊的推理技术，只进行常规推理，那么所有现有模型的准确率都不到 25%。即使是 o1 模型，准确率也只能达到 30% 左右。但最新的 o3 模型，如果将推理时间预算设定为每任务约 20 美元，其表现可以接近人类标注员或众包工作者的平均水平。如果进一步将推理时间成本扩大到每个问题 1000 美元这样的疯狂数字，模型的准确率实际上可以达到 87.5%。显然，这是一种非常昂贵的方式来完成这项特定任务，但它展示了在不太关心推理时间计算成本的理想情况下，该模型的表现如何。

[page 4/74]

在幻灯片上，我展示了一个非常简单的 OpenAI o1 演示，基本上会让模型解决一个规划问题。那么 o1 模型有什么不同呢？主要区别在于，对于 o1 之前的现有 LLM，当我们输入用户查询时，模型会直接生成响应。但在 o1 界面中，他们加入了这个思考过程。基本上，这个过程会根据任务的难度花费不同的时间。


For example, for the specific question I asked the model, it takes a slightly more than one minute
to generate the final solution. And in the o1 interface, they didn't show the raw thought
process. Instead, they show a summarized version of the hidden thought. So from this hidden thought for this specific planning problem,
we can see that it summarizes some key stages in the reasoning process, including laying out an initial plan,
evaluating it based on the model's internal belief, and then revise the plan so that it can satisfy the constraints
and improve the optimality. Then, with the Gemini 2.0 thinking model,
we also make this sort tokens visible to users. So on slides, I'm showing a demo where actually this question
includes an image input. So this question is-- the math part of this question is pretty simple.
So basically, there are four balls in the image, and the question is asking the model to take three out
of [? list ?] [? of ?] numbers so that the sum is 30. And so the thought here, as you can see, is a pretty long.
So after we enter the query, the model immediately generates these thoughts. And we don't need to read the reasoning process
token by token. I know it's pretty hard to read, but the most interesting step we can see from this multi-modal query is queries that there is
a-- so at the beginning of this image, there is a specific number, which is 9.
So at some point, the model figures out a very important step for this question. So because if you want to get the sum of 30,
actually, we need to turn this 9 into 6 by rolling this ball. So this is a very interesting feature
of this query when we put these numbers on some balls. And also, this step itself is not
that kind of difficult mathematically, but actually it is an interesting trick.
And if we-- earlier, if we do not allow the model to generate this long thought, this specific step will be very hard to figure out.
And also like-- besides these models from OpenAI and Google
DeepMind, there are other reasoning models out there recently. At the beginning of the lecture, I mentioned,
there are also models from the DeepSeek team and also the Kimi team. And out of all these breakthrough
in reasoning models, one core shared idea is that we need some way to trigger the large language
model to generate long chain of thought before it actually concludes with the final solution.
In the literature, there are different approaches to trigger this train-of-thought generation.
Like in the original chain of thought paper, they designed a few-shot prompting scheme, which basically demonstrates this thought
process in exemplars. And later on, there are some works showing that we can apply an instruction prompting
to achieve this. Like, we can use the instruction, let's think step by step.
And for the reason that chat models, we can see that even if we do not give any special instructions to the model,
the model also tends to generate some thoughts before coming up with the final answer, no matter whether the thought is
short or long. So there are different ways to train the model to achieve this. Like in for instruction tuning, where
we actually put this train-of-thought data into our supervised fine tuning mixture.
And also what we can see from recent reasoning models, we can apply reinforcement learning to achieve this goal.
In this lecture, I will focus more
on inference time techniques for scaling token budget. And in the future lectures, we will also dive deeper
into reasoning techniques. And there will be some other guest speakers who will come here to share their thoughts.
So for this lecture, I will discuss
three parts about reasoning techniques at inference time. I will first discuss the basic prompting techniques.
And the common idea among these prompting techniques is that we want to allow the model to use more token budget to generate a single solution.
Then the second part, I will talk about techniques to perform search and selection from multiple candidates.
And the common idea here is that if you want to increase the width to explore the solution space.
In the last part, I will talk about iterative self-improvement will allow the model to increase its depth
to reach the final solution. Then, after all these discussions of different parts,
we will also see how we can best utilize different combinations of these techniques to seek the best
way to utilize the inference time compute to improve the LLM reasoning performance.
So let's get started on the first part about basic prompting techniques.
Before we talk about more advanced prompting techniques, let's first review the background about standard prompting.
So basically in standard prompting, let's say we want to solve a math problem as shown on the slides, before the line of works
on chain-of-thought prompting, the common practice is that we just put some question-answer pairs
into the prompt. Like here. The question is the math question, and the answer is the final answer.
So before the advancement in post-training techniques, if we directly apply the standard prompting,
then the performance is pretty poor on reasoning benchmarks. The fundamental issue is that if we
look at this standard future exemplars, they only provide information on the final solution format,
but it doesn't tell the model about the rationale to derive the solution. Now we can imagine that if we are taking a class
and the teachers are basically just teaching you with the final, like official solution without any more
context, then if you do not have a relative background knowledge yourself, then it will make your teaching a lot more difficult.
So for the chain-of-thought prompting, the idea is that we will include these few short exemplars
with sorts. Again, let's take a look at this example of solving math problems.
So here, instead in exemplars, we will provide this calculation steps to basically demonstrate
the model how we should derive this solution step by step
to achieve the final answer. Then after giving this exemplars with chain-of-thought
rationales, then the model is able to solve this problem correctly.
In practice, how these models can benefit more from chain-of-thought prompting compared to standard prompting
is also dependent on the model's fundamental capabilities. So here on the slides, I'm showing
the scaling curves presented by the original chain-of-thought papers and also some other related papers
about this chain-of-thought properties. So we can see that among the different model families,
one common trend is that this CoT performance improves more significantly with the increase of the model size.
In particular, if we look at this gap of chain-of-thought prompting versus standard prompting,
then better models benefit more with CoT generation, and there, we suggest improvement
on reasoning performance once the model reaches a certain scale.
And I also want to note that here for these curves, they are from papers in 2022.
So these experiments are done using pre-trained only LLMs. So this is why it is important to apply
some prompting so that the model can generate the results immediately. And also with this recent post-training techniques,
they might show some different scaling curves like we have seen some strong but very lightweight
LLMs recently. But the main conclusions will still hold. We will still see this overall trend.
Basically, once the model becomes better, it will benefit more from this allowance
of the train-of-thought generation. Then in the follow up work called large language models
are zero-shot reasoners. They further find that even with pre-trained only large language models, we do not have to provide a model with exemplars
to elicit these CoT generation. Instead, we can achieve this with an instruction.
Like in the paper, they demonstrate this instruction. Let's think step by step. Then, after we apply this instruction,
before we ask the model to generate the answer, the model can also generate some thoughts before generating the final solution.
And they empirically they demonstrate that this zero-shot CoT significantly
outperforms zero-shot performance, especially on harder, like, mass and symbolic reasoning tasks.
So zero-shot CoT is more convenient than few-shot CoT in the sense that we no longer need to manually annotate
source ourselves as exemplars. But if we compare their performance,
zero-shot CoT performance is still much worse than few-shot CoT. Then, a very natural question I want to ask
is, how can we get the best of both worlds? How to CoT performance without manually labeling samplers?
So this is what we want to achieve with our paper, large language models, as analogical reasoners.
And this paper was published at ICLR last year. The idea is that, instead of directly providing the model
with exemplars, we instruct the model to first record relevant exemplars
to solve the test problem. Then, based on this model generated examplars,
then the model will try to solve the test problems. Just imagine that these are self-generated exemplars
are just some CoT examples provided by external sources.
The benefit of this design lies in two folds. First, these exemplars are self-generated
by the large language models, so we fulfill our original goal. We don't need to do any manual labeling here.
On the other hand, because this instruction is applied to each test question, so basically for different test question,
the model is able to generate different exemplars which are more relevant to the concrete topics covered
in the test question. So these exemplars are more tailored to individual problems.
We call our approach analogical prompting, because this methodology is actually
can be motivated by human analogical reasoning. If we think about few-shot prompting,
it is not supernatural if we think about it in the human reasoning way, because humans are not
explicitly given demonstrations every time for each single new problem.
And instead, humans also intrinsically recall from past relevant experience.
So in a classic reasoning methodology book called How to Solve It from Polya, they have a very nice description about this reasoning
method named, do a related problem? So the idea is that when solving a problem,
we look for a pre-formally solved problem, which is linked to our present one by generalization, specialization,
or analogy. And we want to learn from these past results or their methods,
so that it can help us solve new task problems better.
If we think about it from the human reasoning perspective, then besides exemplars, we can also
instruct a model to generate some higher level knowledge, which can be a more better summary of what we
need to solve the new problems. And this generated knowledge can complement the problems with broader insights.
So here, let's take solving coding problems as an example.
So given a new coding problem, we can first instruct the model to self generate some knowledge, which is more of higher level
tutorial about the algorithms we need to solve the problem. And also, the model needs to identify
what are the main algorithms we need so that it can come up with related examples of problems
with their corresponding code. Then after this instruction, and also once the model generated
has related a background knowledge including higher level tutorial and exemplars, then the model
goes back to solve the initial test problem. So in the results, we show that basically analogical
prompting not only outperforms zero-shot CoT, but it actually outperforms manual few-shot CoT as well.
And we show this performance improvement on a math problem solving, code generation, and also
like BIG-Bench reasoning tasks. If you look at these results at the first time,
you might be a bit surprised because knowledge here, all these examples of problems or knowledge,
are generated by the model. So intuitively it might contain more errors
than human annotated exemplars. Empirically, we indeed observe this trend.
We see that, for example, for low stack exemplars generated by the model for math problems, around 70% of them are accurate,
and also they are relevant. But the rest can have some issues, they are completely irrelevant, or there are some actually wrong
calculation steps. But even so, there are some noises in the demonstrations. The model still benefits a lot from it.
So given this like a finding, you might imagine that this
means at least the model should be able to generate some reasonable exemplars, or otherwise it cannot learn from
it at all. So based on this intuition, we also conduct a pretty rough scaling study
for this analogical prompting. So here we are comparing models among different--
among the GPT text model families. I know these models are all shut down and deprecated now,
but this trend will still hold. So basically, we can see that weaker large language
models benefit less from analogical prompting. Although if we compare the performance
to zero-shot train-of-thought, it is on par or still outperforms their performance.
And with stronger language models, starting from text adventure 002 in the GPT family,
analogical prompting starts to outperform chain-of-thought prompting with manually designed exemplars.
And notice that actually, this approach can even outperform retrieved exemplars.
So for retrieval method, what we did is that basically for each test problem, we implement a retrieval approach
so that it will find those most relevant problems in the least like GSM8K mass corpus training data.
So one explanation here is that this generated train-of-thought by the model can be more tailored
to the underlying reasoning style of these, like pre-trained or instruction tuned large language models.
So this finding actually is also in line with the recent trend, when we are trying to develop the reasoning models instead
of putting human written thoughts, we can have some training methods
so that the model is able to discover the best like reasoning strategies by itself.
Now let's go back to the instruction. Let's think step by step. So why do we have to use this sentence?
Is there something special about it? So in the original zero-shot CoT paper,
they conducted this ablation study of different variants of these instructions to trigger the train-of-thought generation.
So some of them do not make much sense. So it is, not surprising that they do not perform well.
But if we look at those different instructions in the top category, instructive, If we mask out this accuracy numbers,
it might be not that intuitive know which instruction should be performed better.
And it can be kind of not very good to see that even if you just change a couple of words,
then the accuracy number can change very dramatically. So this shows that current large models are pretty
sensitive to prompt design. And because-- and also in many cases,
there is no clear principle about how to write optimal performance. And this is why nowadays there are still
many people who are writing a lot of guidelines about how to have the best practice to do
prompt engineering. So given this observation, a very natural question
I want to ask next is, how we can reduce this manual work for writing prompts?
So one idea is that since our large models can solve many challenging reasoning problems
and also answering other queries, so why don't we also use it to help us do the prompt engineering or prompt design in general?
So in ICLR 2023 there is a paper called Large Language Models are
Human Level Prompt Engineers. So the high level idea is basically to apply a large model to propose prompts,
so that we hope this model generated prompts can also perform even better than human written
prompts. So here I'm showing LLM complete diagram of their method. So there are two important stages.
The first is this the proposal generation where we can leverage this large language model to generate initial instructions, given our task
description. Then once the model generate different proposed prompts,
then we'll score each instruction based on the prediction correctness on a small set of problems.
So here, basically we will hold out a small validation set, which can tell us the quality of each prompt.
So here the setup might be more similar to the traditional machine learning task
where we have this separation of training and test. But actually we do not need to include so many examples here
for this validation purpose. Like in most cases, it is sufficient to include tens of examples or just around 100 problems.
Then in our work published at ICLR last year, we go further where we try to use this [? language ?] model,
not only just to propose some instructions and do some kind of a search and mutation,
we also want to apply it as an optimizer to iteratively improve the prompt.
So the idea of our approach is that we want to instruct the LLM to leverage the past or partition
trajectory represented as sorted solution score pairs, so that it can learn to iteratively generate
better prompts with more steps. So in this work, we frame it as a more general optimization
framework in the natural language space. But in this specific use case of prompt optimization,
basically this means that we will have two large language models as optimizers and evaluators.
So basically this optimizer is the language model, which is responsible for proposing
a new instruction, given old ones and also the task samplers. And the evaluator is the scholar,
which will evaluate the accuracy or performance of a given instruction.
Now let's take a look at how we can design a meta-prompt to achieve this goal. So on the slides, I'm showing you
an example, meta-prompt for GSM8K to solve the math problems. The prompt itself looks pretty long,
but basically there are two main components. The first one is this [? text ?] trajectory,
including the instructions we have explored in the past steps, and with their corresponding accuracies.
And in practice, we will sort them in the ascending order and we can remove those older ones
if the context is not enough to put all of them in the history.
And we also provide this exemplars to show what's the task we want to optimize. And basically here, for example, for math problems
we only need to include this QA pairs and with the final answer. So again, there's no need to annotate
any train-of-thought or any explanations of how we can derive this solution.
Then for the results, basically, again we
are testing all these instructions on pre-trained only large language models. And we use prompt tool at that time.
And we also try using different models as optimizers including PaLM model and the GPT model.
So for all these approaches, we are starting from the initial instruction with let's solve
the problem, which achieves an accuracy of around 61% for this GSM8K task.
And this is a pretty general purpose instruction. It is around 10% lower than the instruction
let's see step by step. And if we look at the best LLM generative prompt
among this list, we can see that actually this instruction takes a deep breath and work on this problem step by step.
Achieves around 8% like low-- like higher than this, let's think, step by step instruction.
And actually if you compare this performance to few-shot CoT, it matches the performance reported in the PaLM 2 paper,
where the train-of-thought examplars are written by humans. So there are two things we can think about here.
One is that basically here where I seeing another way to basically achieve very competitive performance
to few-shot CoT, we saw that manually writing examples ourselves. Another perspective is that with this type of LLM based
prompt optimization, it doesn't only save us time to so that we do not need to spend so much time to manually tune
the prompts ourselves. But actually, when it comes up with new prompts, it also provides sometimes surprising different angles.
Like I can assume that, for example, like [INAUDIBLE] take a deep breath.
At least for me, if I'm trying to revise the prompt by myself,
I won't feel that this phrase will be so, so helpful, but empirically, the large model with this optimization
loop actually it can somehow come up with this instruction that fits pretty well for this model.
And note that, in this [? text ?]
LLM-based optimization process, actually, this is not a real optimization in the sense that we do not have any training here.
It is completely prompting only. So there's no guarantee that the model can just keep coming up
with better and better prompts. But in practice, we find that, actually, this optimization
curve holds for different models. So we can see that basically with more optimization
steps, which is basically this iterative prompting steps, the accuracy increases with more steps and then plateaus.
So it looks pretty like similar to the traditional optimization graphs.
OK. Now let's go back to Chain-of-Thought. So from the methodology perspective,
what does Chain-of-Thought really bring into our reasoning? Why is it so effective?
If we think about it, Chain-of-Thought prompting is basically giving us a way to perform
this variable computation of the process, which will lead the number of tokens used
for problem will be adapted to task of different difficulty levels.
So if we think about standard prompting, basically, for the same task, if the answer learns are the same,
then basically, if we just provide the final answer directly, it basically means that we are forcing the model to use the same amount of time, same amount of inference time
budget to solve each problem, regardless of its difficulty. But with this, like a Chain-of-Thought,
basically, it shows that for more complex questions, because it also require more reasoning steps to solve
the problem, like for math problems, it might require more calculation steps. So basically, with this Chain-of-Thought idea,
it allows the model to also perform more reasoning steps in the token space so that it can better
solve the task problems. And in the Chain-of-Thought paper, they also show that even if the difficulty of the problems
is in the exemplars are pretty simple math problems, but actually, the same problem can also
generalize to test problems that require much more reasoning steps.
Then, if we think about the high level reasoning strategies, Chain-of-Thought also incorporates a lot
of reasoning process that are very important for us humans to reason about difficult task such as decomposition, planning,
and et cetera. So if you want to further improve the model performance,
and especially if we know what are some good guidelines on the reasoning strategies we need to use for a specific task,
we can also explicitly instruct the LLM with the desired reasoning strategies for problem solving.
So one idea to further push this decomposition strategy is called least-to-most prompting.
And the goal of this approach is to achieve easy-to-hard generalization by explicitly telling
the model what's the best practice to decompose the original problem. So in the slides, I'm showing this example
for solving math problems. And basically, in this original least-to-most prompting paper
from my colleagues at Google DeepMind, it includes two stages. So in the first stage, it performs the problem reduction.
Like given a math problem, it first reduces the problems into simpler subproblems.
So each sub math problem can be easier to solve, requires fewer calculation steps.
Then given this subquestions the model will be able to sequentially solve these subproblems
and combine the solutions to solve the original problem.
The most impressive results from this work is that, actually, with this least-to-most prompting design,
it shows for the first time that we do not need to design a [? neural-symbolic ?] frameworks or requiring any
program execution. But it can still solve this compositional generalization
benchmarks with nearly perfect accuracy. So the results show in their paper is on this SCAN benchmark.
So SCAN is a synthetic data about translating programmatically generated natural language
commands into action sequences. The most challenging part of this benchmark
is about this length split. Basically, in their [? text ?] splits, all the action sequences are longer
than the training samples. So this makes it very challenging for the traditional like sequence-to-sequence
learning if we directly do this training on their training set. And then on this, length split, the test accuracy
will be no more than 25% unless we apply some neural-symbolic techniques.
But then basically, with least-to-most prompting, they show that if they use the latest models
at that time, which is the code-davinci-002, they are able to achieve nearly perfect test accuracy.
And at the same time, they only need to use 0.1% of the training samples as exemplars.
So it is both test efficient or data efficient to adapt to new tasks while also the performance
is very competitive. Learning our follow up work published at ICLR last year,
we further show that we can extend this least-to-most prompting idea to solve more complicated
compositional generalization in the case of translating text to more realistic programming languages.
Like here on the slides, I'm showing this example of translating natural language questions to SPARQL queries.
So the main challenge here is that for more real world languages, including natural language or programming
languages, there are more complicated grammar rules and also the vocabulary size is larger.
This means that, especially, if we do not have a model with a super long context,
then a single prompt will be no longer enough to cover all the grammar rules needed to solve the task.
So actually, with this challenge, it shows one benefit of decomposition
by allowing the model to use different prompts for different reasoning stages. It also enables the model to use customized prompts
for each subproblem or each subtask in the context of agent development.
So here is an overview of our dynamic least-to-most prompting approach. So some details do not matter too much here like how
we do the decomposition. But the [? menu ?] stage we add is this so-called dynamic selection of exemplars
for each subproblem. So basically here, for each of test, like a sentence,
we can discover those example questions with the most relevant grammar rules.
So basically, we can assure that for each part of the sentence, we can always retrieve for the most useful grammar rules
to solve the task so that the model can have enough context to solve the problem.
So all these compositional [? three ?] [? base ?] question benchmark, the CFQ data set.
Again, we show that, actually, our dynamic least-to-most prompting approach outperforms all these baselines, which
where we need to train a model on the complete training set. And again, if we compare this dynamic least-to-most prompting
with Chain-of-Thought prompting and other prompting approaches, it also scales better when we increase this examplar
[? problem ?] to generate these few-shot [? examplars. ?] So far, we have talked about how we can instruct the model
to solve the problem in some like with some reasoning strategies indicated by the few-shot examplars.
But if we think about it, we can assume that for different reasoning tasks, they can require very different reasoning structures.
Like there will be different ways to decompose the task and plan for each stage.
At the same time, we might not want to annotate all these best practices for each type of task.
Again, we don't want to do this manual labeling of exemplars for every single problem.
So in our self-discover work, we published at NeurIPS last year, we basically instruct the model to compose
task-specific reasoning structures without manually written demonstrations. So in practice, how we do it is that basically, we
will list a kind of [INAUDIBLE] of best practice of reasoning strategies, including
like a Chain-of-Thought reasoning, doing decomposition, doing some self-reflection, et cetera, then given a different, a new task,
the model will think about how it can utilize the reasoning
strategies that are best suited for the current task and compose the reasoning structure. Follow it to solve the problem.
And for evaluation, we also show that with this self-discovery
methodology, it outperforms Chain-of-Thought prompting and also, other baselines on different benchmarks,
including BIG-Bench-Hard reasoning task, [? MATH ?] benchmark, and also some other benchmarks
for [? socio-agent ?] reasoning. So to summarize, in the first part I talked about,
I first talked about Chain-of-Thought generation. And the fundamental idea of allowing the model
to perform variable computation to trigger the thought process. And this thought process can adapt to tasks
of different difficulty levels. And we talked about how to improve
the Chain-of-Thought performance at inference time. So in the original Chain-of-Thought paper,
we talk about the few-shot prompting idea with labeling of thoughts. And we talked about instruction prompting
to trigger this Chain-of-Thought generation. And also, we talked about how we can instruct a large language
model to automate the prompt design and discover better problems by themselves.
So notice that this best practice of interacting with large language models can evolve over time.
Like for example, recently with this [? post-trained ?] large language models, you do not have to put exemplars for every
single task for the model to learn how to do it. And for some models, for example, it is better for you
to just provide very clear guidelines and instructions on what you want. And then hope the model can just follow your instructions.
And some models prefer those like an interactive discussion and some models might prefer just a very long and detailed
instruction at once. But the principles of how to discover good prompting strategies for reasoning still hold here.
Like two very important criteria is that we want this approach to encourage the model
to generate longer Chain-of-Thought for complex tasks. We want it to scale with different task difficulty.
And also, we need this prompting technique to be able to support reasoning strategies required for the task.
So now, let's start the second part about perform search and selection from multiple candidates and increasing the width
to explore the solution space. So after we talk about the first part, what is missing so far?
Yeah, I guess this question is not so hard since I already showed the outline of this slides.
So basically, from first part, we can see that we should not limit the large language
model to generate only one solution per problem. Because the model can just have some mistakes in its
[? single-pass ?] solution. Instead, we should allow the model to explore multiple branches, which
will allow the large language model to recover from mistakes in a single generation. And there are two ways to basically increase
the width of this solution exploration. One is that we can generate multiple candidate
solutions per problem. Another is that, at each step, we can allow the model to generate multiple potential next
reasoning steps. It is not that hard to think that with this multiple sample
generation, then the upper bound of the performance will be definitely higher because we allow the model to select an alternative,
probably better solution. But the fundamental challenge for approaches along this line
is that how to select the best response from multiple candidates.
Because in the real world use case, we do not have an Oracle Scholar at inference time. So surely, for example, in the chat interface,
we can just have the model generate multiple solutions and the user can pick whatever they want.
Then this is kind of a best of [? selection ?] case. But it will be more ideal if the model can just
do this kind of process internally so that we do not have to review like tens of solutions
by ourselves to select what we want. So along the line of these ideas,
I want to first talk about this self-consistency work from my colleagues at Google DeepMind.
So if we look at this idea nowadays, we will find that this is an incredibly simple idea.
But it is actually very effective. And it actually can improve performance a lot
with all the different models. So let's say we want to also solve a math problem.
And this time, we allow the model to generate multiple responses. So basically, after we have all these responses,
we still do not know which one is accurate. But here, we have this consistency based selection criterion.
So basically, if let's say for this math problem with a single number as the final answer,
we will look at these three solutions. Two of them have the answer 18 and of them have the answer 26.
So basically 18 is the most consistent answer. So we pick the response corresponding to this.
Like most frequently appeared answer. And note that here, one important thing
is that this selection is only based on the final answer. It is not based on the reasoning process.
The model can come up with whatever reasoning process it wants. So as long as they lead to the same final answer,
we consider them to be all good. This is a pretty simple idea. But actually, it boosts the performance
across different models and benchmarks. And here, in their paper, they also--
I evaluate all the models available at that time, and especially on math problems.
They show very significant performance improvement. Now let's talk about this scaling effect
again with the self-consistency approach. So for the baseline here in the curves,
they compare with sample-and-rank baseline. This is actually a pretty natural baseline,
which says that we can select the response with the highest log probability.
So then here, the assumption is that if the model gives higher probability to the current response,
it should be more confident about it. But actually, if we look at these curves, we can see that self-consistency performance scales much better
than probability-based ranking. So it's more and more responses, like up to 40 responses.
Self-consistency performance is still improving. But these sample-and-rank baseline has already stopped improving around 10 responses already.
But there is an exception. So the exception is that, if we train the model to be a very good verifier for the task we care
about, we might be able to outperform this scaling curve with self-consistency.
And we will talk about it later. Another very important factor that contributes to self-consistency success
is that we need the model to generate very diverse responses
to perform this consistency-based selection. So here, basically in this results,
they also compare with different baselines. So the beam search is actually a very classic search algorithm
like in the NLP literature before the [? LLM ?] error. So basically, the idea is that at each decoding step,
we will keep the top [? k ?] like tokens with the highest probabilities. Then in the end, we will use a sentence with the highest
overall probability. And then they also compare with this [? sample ?] baselines,
where they apply greedy decoding among different variants of how
you formulate the prompt. Because earlier we mentioned that the LLMs are sensitive to different prompts. Also, they compare the performance to different ways
to permute the exemplars orders in the prompt. So again, they show that self-consistency
using sampling scales better with more samples. So the important factor here is that basically for the sampling
method, it needs to ensure that the responses are very diverse. So we can do this sampling with a high temperature.
Or we can perform nucleus sampling by setting the parameters in the right way
to enforce this diversity, et cetera. One analysis from this work is actually pretty interesting.
It is related to model calibration. So this is about actually how does
this consistency-based selection improve the model performance. What does it mean when the model, for a given task,
the model can come up with a more consistent response? So here, they show this curve to show
the correlation between the accuracy and consistency. So here, we can see that if it follows,
sample responses, if more of them lead to the same final answer, this means that this consistency percentage is higher.
Then there are two conclusions we can see from here. One is that this large language model
is more certain with its final predicted conclusion even if they are coming up with the same conclusion
with different reasoning process. And empirically, we see that this aggregated solution
for this, more consistent [? text ?] responses, can be more likely to be accurate.
The final solution can just be the right one for the original test problem. So this can also explain why this consistency can
be a very simple, but very powerful criterion to select the final solution.
So in the self-consistency paper, they demonstrate this approach on those test-based reasoning
problems. And actually, this consistency-based selection approach is also very powerful for code generation.
And in our AlphaCode work at Google DeepMind,
this AlphaCode system itself is a pretty complicated one. It includes the model training and inference time techniques.
But here, I will mainly focus on this inference stage called
filtering and clustering. And especially, this clustering is basically tries
to do this like a code selection based on the consistency [? of ?] execution results.
So let's first briefly discuss about the background, the problem setting of competitive programming.
So basically, in a competitive programming problem, in the problem description, it will
include a very long and complicated task to tell about the background story, about what we want for the problem
for the corresponding code. And then also, it includes a few input-output pairs as test cases.
Then for the solution to be considered correct, the generated code needs to pass both the given test
cases in the prompt. And also, usually, we will have some held-out test cases. And these test cases are typically of larger scale
and they will be more difficult than this example test cases. And they will cover some corner cases.
So since we already have some [? input ?] examples, we can filter the programs that fail the given test
cases already. But for the remaining programs, they might still fail on the held-out test cases.
Like for those of you who have participated in these coding
contests, you might feel the same way. You feel that your code is great. But it just fail on some mysterious held-out test cases
and you didn't get the score you want. So basically, for the AlphaCode system, what we do
is that we will train an LLM model to generate new test inputs for these test problems.
Then we will execute those like sample programs on all these model generated inputs.
And then we cluster all programs with the same outputs together. The assumption here is basically if we
have the model to generate enough number of test inputs. And also, they are diverse of higher quality.
Then actually, we can assume that all programs in the same cluster are semantically equivalent because they lead to the same execution results.
And then finally, we sample one program from each of the 10 largest clusters.
In our [? evaluation, ?] we show that this clustering provides additional performance gain compared to the filter only method.
But clearly, there is still a gap from the Oracle selection. Because there's still no guarantee
that the consistent response will be just the best one out of the candidate pools.
Now let's revisit this self-consistency decoding pipeline. So one limitation here is that for
the original self-consistency, we need an answer extraction process so that we can do this aggregation among the candidate
responses. So if we want to apply this consistency based decoding to broader used cases, the natural next question
is how we can do it for free-form generation, where it is not that natural to have this aggregation and answer
extraction process? So this is we want to achieve with our work on universal self-consistency.
The idea is that, instead of having this answer extraction process, we ask the large language model to perform
consistency-based selection. So specifically, we throw an instruction to the model asking it to select the most consistent response
based on the majority consensus. And also, it needs to take a look at all these candidate responses.
So the intuition here is that, clearly, we know that it will be hard for the model to judge the answer
correctness by itself. But consistency should be a simpler criterion to measure.
Then empirically, we also evaluate this universal self-consistency method for different applications.
So for [? low-stack, ?] free-form generation problems, like summarization and question answering,
we stack low [? factual ?] like final answers. We see that the universal self-consistency also still
can improve over the baselines, where the original self-consistency is not directly applicable.
An old math reasoning and coding problems where we can apply this original self-consistency idea.
Actually, universal self-consistency can match its performance without the requirement of performing this answer extraction and code execution.
And notice here, clearly, this universal self-consistent performance is bounded by the long-context capability.
So at that time, we are using an earlier model, which is not super great at long-context reasoning.
So we can see that when we include more responses in the prompt, the performance might not
go up as well as the original self-consistency.
But we can see that with the recent development of long-context models. This will allow such kind of model-based selection approach
to scale more with more responses. And also, in practice, for most tasks, it can be well-solved within, let's say,
a 10 responses in the candidate pool. So this will still showcase the usability of this model based
consistency selection. Previously, we talked about, for the model-based selection,
using the log probability from the original model, this approach of ranking doesn't scale better
than the consistency-based selection. So to improve further over this consistency-based selection,
one idea is that we can train a large language model as the ranker. And we hope that at least this ranker
can perform better than this simple consistency criterion. And actually, it can have some kind of sense of which responses
are more likely to be accurate. So here, on the slides, I'm showing this diagram
from the original OpenAI paper, which introduced the GSM8K data set.
And also, they demonstrate this method of how to train a verifier to show the-- to basically judge
the correctness of the matched solution. And also, later on, they have this follow up paper called
"Let's Verify Step by Step." So in total, for this line of LLM-based ranking works,
there are two ways to design the LLM-based method
to judge the answer quality. The first way is the most--
the initial and the most standard approach, which is called outcome-supervised reward model. Basically, it will verify the correctness
at the solution level. Like given a math problem, the solution, the model will tell you whether the solution is correct or not.
Another approach introduced in this "Let's Verify Step by Step"
paper is called process-supervised reward model. So the idea is that, instead of verifying [? LLMs ?] solution
space, we can also very, at a step level, we can see whether each step is accurate.
So in their work, they show that this process-supervised reward model actually scales better with more samples
compared to this outcome-based reward model and also this majority voting baseline.
But clearly, here, the performance will be highly dependent on the verifier quality. And also, the same verifier might not
generalize across tasks. Because we know that this consistency-based criterion is very simple and actually can be directly applied to many tasks.
But for these [? rank ?] models, you need to design a good training strategy and also design a good training data so that it can really
work across different domains you care about. So far, we have talked about response selection
only at the solution level. So we only perform this selection after the full responses are generated.
But assuming that we have a very good step-wise scorer, then actually, this solution-based response
selection is a waste of using this step-wise scorer. Because actually, we can do more with such kind
of a more powerful and more fine-grained verifier. So if we really have this good step-wise scorer function,
then actually, we can perform this tree search with large language model. Where basically, instead of waiting until the solution
becomes complete, during the search process, we can prioritize those exploration of partial solutions
at the step level. And we can explore more promising steps before exploring other [? processes. ?]
So on the slides, I'm showing this list, like diagram from the Tree of Thought prompting paper.
So from this illustration, you can clearly see what are the differences of different prompting methods.
Like Chain-of-Thought prompting is basically a train. This self-consistency method is like multiple parallel chains.
And then for this Train of Thought, basically, at each step, it can have multiple branches
to explore the next step. And you don't have to complete all these step passes until the end.
So here, I will show this how we can apply this Tree of Thought idea with a simple example of game
of 24. So basically, at each step, there are two important stages we want to do.
One is called thought generation. Basically, in this step, we will prompt the large language model
to propose possible next thinking steps. Like in the game of 24, these next thinking steps
are pretty simple in format. You just need to select the two numbers to perform this calculation.
Then there is another stage called thought evaluation. Where again, here, we can perform the large language
model to evaluate how promising the current state is. Like for the game of 24, you need
to decide whether it is possible to reach the final number of 24 after the list, all the manipulations.
So previously, we show how-- we can show how we can do this [? text ?]
evaluation at step level. So it will determine the quality of each individual step.
But related to our previous consistency-based selection discussion for this scoring function,
we can also think about it as a way to just select the best state among the candidates. We don't need to just have the model
to make this decision at the point level. So the idea is that we can ask the large language
model to select the best responses among all these possible next steps.
And then, we can also do this voting multiple times. Because since all these [INAUDIBLE]
here is based on large language model sampling. Then with all these different votes, the model can finally select the majority vote
as the final choice to proceed.
Here, for the results, I'm showing the list again of 24 evaluation. And in the original paper, they also show some other benchmarks.
So we can see that with Tree of Thought using breadth-first search, it scales
better than the standard prompting and Chain-of-Thought with respect to token budget.
And going further, clearly, we can integrate some more advanced search algorithms. Like in this original Tree of Thought paper,
they use the most basic BFS and [? DFS ?] algorithms. But in the later one, we also see some papers
which are coming up with these Monte-Carlo Tree Search methods. And some works are also trying to train a separate evaluation
function instead of just prompting the model. Again here, the important consideration
is that we need to design a good language model and prompting scheme so that it can be
effective for self-evaluation. So to summarize, in this second part,
I show that we can further scale this inference time compute by sampling multiple branches in the solution space.
And we talked about consistency-based selection, which is a pretty simple, but effective and general principle.
We talked about this self-consistency, where we can marginalize out reasoning processes and do the selection-based on the final answer.
And for code generation, we can also do this reranking based on execution consistency.
When this large language model self-evaluation works well so that it can really judge the quality of partial solutions or structure quality
at a step level. We can also apply this search method. And in this case, sometimes, it will scale better.
And also, it can reduce the token cost because it can already eliminates those nonpromising
[? process ?] during the mean time. So finally, I will talk about iterative self-improvement,
which will try to increase the depth to reach the final solution.
So even with the most recent advanced models, including those recent models, we
can see that they can achieve pretty amazing numbers on those competitive and challenging benchmarks
for solving hard maths and coding questions. But they can still, sometimes, make pretty obvious mistakes
and people are keeping complaining about it. But if you think about us as humans,
we also tend to make sometimes pretty trivial mistakes at first thought.
So previously, we talked about we can sample multiple solutions to help us reduce these mistakes from a single prediction.
But if we think about it again, it is actually a pretty suboptimal way to do this error correction.
Because all these responses are generated in parallel. So we do not really learn from the past mistakes,
we can just have the model generate the same wrong predictions again and again.
So basically, in this part about inference time self-improvement,
I will talk about techniques using large language models to iteratively improve its own response for the given task.
And also, this will align better with us humans like an error correction process.
So the first papers about these ideas include this like reflection and self-refine.
So basically, in this process, there are also two important steps after generating each solution.
So basically, after the model generates the solution, the model will also generate some feedback based on what it observes.
And in this step, it can use those external evaluation when available. So for example, in the original reflection paper,
they focus more on this agentic setup, where basically, the LLM, as the agent will propose actions
to the environment. And the environment will give the model some kind
of changes of the observations. So this will be external signal to tell the model whether the current [? state ?] is promising.
Then based on this model generated reflection feedback and also these external observations,
then this large language model will refine its output to revise this next prediction step given
both this internal and external signals. In this reflection paper, they show
that this self-reflection and self-refinement works pretty well, especially when we can have a scheme.
So that basically, the model can utilize pretty high quality evaluation or when the external signals are very reliable.
So for example, in the case of a ALFWorld, it is an agent, like a navigation task.
So there will be clear signals about observations at each step. And they also evaluate on their approach on HotPotQA.
But they kind of repurpose this QA task into an agent environment. In the sense that after the model generates the solution,
they assume that this environment will give a signal about whether the current solution is correct or not.
This is why you can keep seeing this improvement with more trials. And we will revisit this assumption later.
And if we think about this like self-improvement loop
for applications, then code generation is actually a pretty natural used case.
Because when we are writing code, for us for example, we also debug our code better within an IDE.
And we do not just write the code and we stop. We actually really rely on this interactive loop
when we are trying to investigate all these code execution, results. And see whether it makes sense and also which
parts we should revise later. And this is also one fundamental idea
of the recent coding agents. So in our paper of self-debugging,
we also do a deeper dive into what kind of feedback formats will be helpful for the model to better correct its own mistakes.
So for example, the most simple feedback message
is basically a short universal information about whether a current code is correct or not
without more information. And then for unit test feedback, we also
include the execution results. Like whether the current unit test has passed or whether there are any runtime errors.
Then for code explanation, basically, at each step, we also ask the model to self-generate some line
by line explanation of the implementation in natural language. So this idea is that kind of a motivated by human
[INAUDIBLE] debugging, trying to ask the model to see what it's trying to do in the code.
And finally, in the trace feedback. Basically, we ask the model to perform some line
by line simulation of the execution trace by itself. This is also one way of debugging by human programmers.
Empirically, we evaluated the latest large language models
at the time of paper publication last year. So basically, we can see that with this self-debugging loop,
it consistently improves the performance across different large language models of different capabilities.
And also, if we give more informative feedback, it can further improve the debugging performance.
Now let's go back to this self-correction for question
answering tasks. Previously, we talked about these results of HotPotQA. And also, in some other works, it also
show this improvement of self-correction on other reasoning task like math problem solving.
But all of these significant improvement comes from the assumption with an Oracle verifier.
But in practice, this Oracle verifier
will not be available in many scenarios. Especially, in the case of solving, reasoning problems.
You can imagine that when you are taking a math test, you don't really have the official solution at hand.
Otherwise, it will be just trivial for you to finish the exam. So basically, the question here we want to study
is how do these large language models perform if we do not give the model such very
explicit and accurate external feedback about the response correctness.
So in our paper published last year at ICLR called Large Language Models Cannot Self-Correct Reasoning Yet,
we actually show some negative results from this study. So we compare with this Oracle baseline,
where we utilize the ground truth answer for question. And same as previous works, we show this huge improvement.
Then we studied the case of the real self-correction, where we do not allow the model to utilize any Oracle feedback.
The model needs to judge the response correctness by themselves. Then these are the results, we show in the bottom table.
We can see that the main issue here is that because large language models can only judge the correctness of its predictions,
it often turns an originally correct solution into a wrong one. So this is why actually in this self-correction
without external evaluation, this process can lead to worse performance after each round.
So with this study, one natural question
is probably we didn't tune the feedback prompt well so this is why it fails.
So to address this question, we also design several variants of this feedback prompt.
And notice that all these feedback prompts are pretty general. They are not specific tuned for one task.
So we can see that if we just have this simple general-purpose feedback prompt, if we do not have any more of the model
training, then editing this feedback prompt, indeed, can affect the self-correction behavior.
But this is more about adjusting the tendency for the model to keep the initial response.
So in some cases actually, this feedback, after the feedback prompt update,
there's no much regression on the reasoning performance. But then in this case, this also doesn't
help the model to improve over the initial performance. So previously, we discussed about this comparison
if we just do this self-correction over one initial response. And we further compare with another method
where basically we do this self-correction among multiple responses. So for this, we compare with the multi-agent debate baseline.
The idea is that we will have the model generate several responses in parallel once.
Then we perform the large language model to reveal these multiple responses and give an updated one.
And for this multi-agent debate, we will compare with self-consistency. Recall that self-consistency is just completely generated
solutions in parallel and select the response with the most common final answer.
So in the previous multi-agent debate papers, they are showing some improvement over self-consistency.
But we find that the major issue is that they do not have a very rigorous comparison in terms
of the token budget. So actually, because in the multi-agent debate, each round for this specific experiment has three responses.
But basically, if we keep the same number of response and budget, we will see that actually still
self-consistency scales better than the multi-agent debate. Because at some point, again, this multi-agent debate
performance stops improving. But with more responses, self-consistency performance is still improving on this GSM8K [? MATH ?] problem benchmark.
So given all this discussion, then the question
here is how we can best utilize the token budget by integrating all these different methods together.
So the fundamental question is here, basically, we need to understand how to balance the inference budget
for generating multiple samples in parallel or sequentially.
So actually, this is a pretty difficult question because it really depends on the task and the model.
And in essence, it's about whether the model for evaluation
is good at self-reflection and correction for your specific task.
So here, I'm showing an example study from my colleagues at Google, where they are trying to--
basically, in this specific experiments, they are trying to find the optimal way to scale this token budget to solve the math problems.
So actually here, they trained a model, which is supposed to be more specialized at correcting
math solutions. And also here, for the different difficulty level of problems,
they study what's the optimal ratio to perform this parallel generation
versus sequential generation. So I would say this concrete numbers will really vary
a lot among different models. But basically here, the overall conclusion is that, for simple problems--
actually, the model can benefit more from self-correction because it knows a more clear about
whether the current solution is wrong and how it should revise. But for harder problems, actually, there is a better point in the middle about how much
you should perform parallel generation. And also, this sequential refinement.
And then with this kind of compute optimal factor for different task difficulty of problems,
they show that this compute optimal curve can also scale better than this complete parallel generation.
And when we think about inference time compute, another factor we should consider is about the model size.
Because intuitively, with the same FLOPs budget, we can sample much more solutions from a lighter model.
So more expensive model does not perform much better than a cheaper model, then it will not be a very optimal way
to do the inference by calling the more expensive model.
So basically, from this empirical study from the paper inference scaling laws,
again, they will show that with the optimal model with different inference budget can be a different.
Like when you have a smaller inference time budget, maybe you want to rely more on the lighter models.
But basically, when you have more budget, probably you can utilize the more expensive models, which can probably give you some boost on those very
challenging problems. Where with the smaller models, it will be very hard to come up with the final solution anyway.
So basically, in this lecture, we
have covered different ways to perform this inference time scaling with the more token budget.
So we talk about the basic prompting techniques about this search and selection to increase
the width of exploring the solution space. And also, we talk about iterative self-improvement.
And again, the best practice to integrate with a large language model should be adapted according to its capabilities.
So you really need to consider the underlying model capability and also the task in your hand to think about how you can best
combine all these different techniques and design a more efficient and effective paradigm
for your task. And finally, I also want to talk about the general principle
of how to design effective reasoning techniques. And this actually will be helpful for both inference time
techniques and training time techniques. So I guess many of you might have heard about this The Bitter
Lesson from Richard Sutton. But if not, I highly encourage you to read this web page.
Actually, I will say this is a very important guideline for many of these modern machine learning works.
And especially, with the recent development of large language models and reasoning techniques.
So this article itself has a lot of text. But I will quote some most interesting text in my opinion
here. So basically, what we really want to develop as a method is basically [? those ?] that continue to scale with
increased computation. And when we want to improve the AI agents,
we want to teach the model to discover what we can,
not which contain what we have discovered. So it is more about having some method
so that we can trigger the model to know how it can keep improving itself and how to have the right way
to perform the reasoning. So this is the end of this lecture. Thanks for listening.