
MCP - Model Context Protocol

ChatGPT：

* 第一阶段，纯好奇，纯好玩
* 第二阶段，可以人机协作提供生产力
* 第三阶段：API 革命，集成到现有软件中

Context，简单的说，上下文就是 AI 在生成响应时能看到的一切。最大挑战是在真实的专业场景中进行上下文组装，上下文存在于各个系统中，将所有这些内容复制粘贴到一个地方，然后与 ChatGPT 或任何 AI 软件进行交流，是一件非常非常困难的事情。

随着工具功能的引入，当函数调用/工具的解决方案出现时，人们以为已经解决了上下文整合的问题。但过了一段时间后，他们意识到，尽管这个基于工具的解决方案确实有效，而且所有的上下文都会提供给你，但这种方法有一个很大的缺陷，问题在于每个 AI 工具都在构建自己调用每个 API 的方式。

而 MCP 的出现就是为了统一这种形式。在 MCP 中有两个实体，一个是客户端，另一个是服务器，整个通信就在这两个实体之间进行。

在 MCP 中，客户端就是 AI 聊天机器人，如 ChatGPT、Cursor，而服务器基本上就是你想让你的 AI 聊天机器人连接的服务，比如 GitHub、Google Drive。所有这些完整的通信，客户与服务器之间进行的全部对话，它们所用的那种语言，我们称之为 MCP 或模型上下文协议。

从技术层面来理解一下 MCP 与功能或工具调用有何不同，在功能或工具调用中，你需要与你想要连接的任何工具对接，为了访问它的 API，你需要在你的 AI 聊天机器人内部编写一个函数，所以这里其实也是遵循一种客户端-服务器模型。

MCP 中也会有一个天气服务器，天气数据会被记录在那里。只是这里要建的服务器会借助 MCP 库的帮助来构建。代码有点不同，但内部使用的是相同的 API。主要的区别来自客户端，在客户端这边，你不需要在你的 AI 聊天机器人中写任何代码，因为你已经预先将客户端和服务器集成并配置好了，而且它们使用的是相同的语言。这就是模型上下文协议，所以你不需要单独编写代码来获取这个 API 的数据，服务器会处理所有这些工作。

在函数调用中，有一个由公司编写的服务器代码，以及一个由你编写的客户端代码，这两部分代码一起工作时，整个任务就完成了。在MCP中，所有的繁重工作都由服务器完成，客户端这边你什么都不用做，只需要将客户端和服务器连接起来。一旦两者连接成功，你就可以从服务器内部获取你喜欢的工具来完成你的工作。这是 MCP 与函数/工具调用的主要区别，在 MCP 中服务器承担了繁重的工作，而 AI 聊天客户端这边你什么都不用做，只需连接并等待就行。

在 MCP 中，所有的繁重工作都由服务器完成，在客户端，AI 聊天机器人什么都不需要做。基本上只需要维护一个 JSON 文件来连接各种服务器，就可以了。

----------

### MCP 的架构 

*简化版本*：最简单的版本中，MCP 的架构中只有两个东西，一个是 Host，另一个是 Server。Host 基本上就是我们的 AI 聊天机器人，用户与之交互，用户以 prompt 的形式向 Host 提问，而在幕后，Host 会连接到某个 LLM。Server 基本上是一个工具，它具有执行某个特定任务的能力。

**例子：** 假设用户问了一个问题，GitHub 仓库上有新的提交吗？用户会将这个问题作为 prompt 发送给 host。host 目前可能会原封不动地将这个 prompt 发送给 LLM，LLM 会阅读这个问题，并意识到其答案不在其训练数据中。这意味着，它需要某种外部工具或 server 来回答这个问题。然后它会去检查有哪些 server 可用。然后发现有一个 github server。因此，它会立即告诉 host：我不知道这个问题的答案，但为了找到答案，去问 github server。现在 host 会去和 github server 交流，并向它提交查询。github server 在后台会检查 github 仓库中是否有新的提交，并将所有新提交的列表返回给 host。现在，host 会带着这些信息回到 LLM。此时，LLM 已经拥有了足够的信息来回答问题。因此，LLM 会将其答案返回给 host，而 host 会将同样的答案通过用户界面展示给用户。

*add client*：简化版本中 所有的通信都在 host 和 server 两部分之间进行，但这一点并不完全正确。实际上，host 从来不会直接与 server 对话，而是通过 client，client 本质上就是帮助 host 与 server 进行通信的实体。client 最大的优点就是它能够使用与 server 相同的 MCP 语言进行交流，这就是为什么 client 与 server 之间的通信变得超级简单。

**示例**  用户再次向 host 询问：告诉我，我的 github 仓库中是否有最近的提交？Host 会接收这个 prompt，并从中生成一个高级请求：查找 github 仓库中的最近提交，然后把这个高级请求交给 client。client 将把这个高级请求转换为 MCP 兼容的请求，一旦转换完成，client 就会将这个请求发送到 server。现在，由于该请求是 MCP 格式的，server 可以轻松读取和理解它，然后将开始其工作。工作完成后，它将再次生成一个结构化的 MCP 响应并将其返回到 client。client 将这个结构化的 MCP 响应翻译成 host 能够理解并执行其工作的语言。在 MCP 中，client 和 server 之间的关系是一对一的关系，这意味着 client 一次只能连接并与一个 server 进行通信。当需要与多个不同 server 通信，这需要多个 client。

**好处：** 1. 架构是解耦的。2. 在系统中，只需持续为每个 host 部署 client，host 内有多少 server 就有多少 client。这种架构能无限扩展规模

*原语（primitives）*：原语是 server 可以提供给 host 的东西。基本上，这个整个架构为 host 提供了哪些服务，我们就把这些称为原语。

server 总共为我们提供了三种东西：
1. 工具，指 AI 可以要求服务器执行的操作，例如 github server 所具备那些功能
2. 资源，指 AI 可以读取的结构化数据源，例如 github 服务器上，你可以拉取任何github 仓库的 readme 文件。这个 readme 文件是一种静态资源。
3. prompt，是 server 提供的预定义提示模板或指令，用于帮助塑造 AI 的行为。

**示例：** 假设有一个 AI 聊天机器人，有一个 github server，现在有个问题，需要为一个 bug 创建一个 issue，说明登录按钮不起作用。

首先，host 会告诉 client：“你去 server 那边查一下，哪个工具能帮我们创建issue。” 于是发现 “create issue” 的工具。在幕后，AI host 会生成一段文字，其中包含标题 “登录错误” 和正文 “登录按钮不起作用”。然后，host 会把这段文字交给 client，并告诉他：“去告诉 server，调用 ‘create server’ 功能，在 github 仓库上创建这个 issue。”

这里的问题是，你通过 LLM 或 AI host 生成的这段文本有点模糊。通常在 github 仓库上创建 issue 时，描述应该更详细一些，但由于我们没有给 LLM 或 AI host 提供任何关于创建 issue 时应该使用什么格式的指导，所以由 Prompt Primitive 进行改写。

所以我们需要在服务器端创建并保留了一种类似指南的东西，LLM 可以通过阅读指南来学习如何更好地使用服务器，所以基本上提示原语的概念就是通过它的帮助，Host 可以学习更有效地使用，但这并不适用于所有情况，有一些特定的场景下你会发现它很有用。

*MCP 架构的数据层*

通信语言，MCP，在 MCP 中，JSON RPC 2.0 是数据层的基础。JSON RPC 的全称是 JavaScript Object Notation Remote Procedure Call（JavaScript 对象表示法远程过程调用）。

RPC，多用于分布式应用中，例如与其像调用函数那样写 `add(2,3)`，你只需发送一个 RPC 请求，说请用参数 2 和 3 运行 `add`，在另一台机器上执行函数，这就是 RPC。JSON-RPC 将远程过程调用的概念与 JSON 的简单性相结合，允许开发者以标准化的 JSON 格式构建 RPC 请求和响应。

示例请求：

```json
{
	"jsonrpc": "2.0",
	"method": "add",
	"params": [2, 3],
	"id": 1       // id 用于，当返回响应时，需要匹配它。
}
```

示例响应体：

```json
{
	"jsonrpc": "2.0",
	"result": 5,
	"id": 1
}
{
	"jsonrpc": "2.0",
	"error": {"code": -32601, "message": "Method not found"},
	"id": 1
}
```

在 JSON RPC 中可以同时发送多个请求到服务器，这是可能的，所以这被称为请求批处理。还可以发送通知，客户端可以选择向服务器发送通知，服务器也可以选择向客户端发送通知。

通知和普通请求-响应之间的区别：请求-响应的概念是，如果从客户端生成了一个请求并到达服务器，那么服务器的责任就是生成一个响应返回，而通知，无须对方回复。（订阅的含义），这里没有 id。

*为什么用 JSON-RPC 而不是 Rest API？*

1. JSON-RPC 是一个轻量级协议。而 REST API 在幕后是通过 HTTP 进行的。HTTP 通信时，在 REST API 中，会有很多 headers 附加，请求中也会有很多 metadata 附加，所以基本上，整个请求的创建有点费劲，如果其中出现一些错误，调试起来也很费劲，再加上由于其中包含的内容较多，从一个地方发送到另一个地方也会有点耗时，成本较高。而在 JSON-RPC 中，你只需要简单地写文本，并用它来建立通信，编写起来容易，调试也容易，通过传输媒介发送也很容易，从一台机器到另一台机器，这是一个很大的原因。
2. JSON-RPC 支持双向通信。REST API 是一种单向通信。
3. JSON-RPC 是与传输无关的。REST API 的传输方式就是 HTTP，JSON-RPC 中，它可以通过 HTTP 工作，也可以通过 STDIO 工作，甚至可以通过 Web Sockets 工作，如果你愿意，你甚至可以编写自己的自定义传输方式。
4. 支持批量处理。在 REST API 中，一次只能向服务器发送一个请求。
5. JSON RPC 支持通知功能。


*MCP 架构的传输层*

传输层是一种在客户端和服务器之间移动 JSON-RPC 消息的机制，即整个消息的传输媒介。

从客户端到服务器以及从服务器到客户端传输时，传输层始终存在一种传输模式，而这种传输模式取决于你正在处理的是哪种类型的服务器。

在 MCP 中服务器分为两种类型：远程服务器，本地服务器。

对于本地服务器，那么在传输层中使用的传输模式为 STDIO，而对于远程服务器所使用的传输模式我们称之为 HTTP SSE。

*STDIO 的全称是标准输入输出*，基本概念是，当你运行任何程序时，它的进程可以通过两种方式与外部世界通信。一种方式是通过标准输入，通过它可以接收来自外部世界的输入；另一种方式是通过标准输出，通过它的帮助，他可以向外部世界发送一些输出。

整个过程分为三个步骤：

1. Host，即 AI 聊天机器人，会在同一台机器上作为子进程启动你的 Server。所以每当主机启动服务器时，就会在主机和服务器之间建立一种父子关系。这意味着主机现在可以控制服务器的标准输入和输出。
2. 借助这种控制能力，主机的客户端可以非常轻松地向服务器的标准输入发送 JSON-RPC 消息，而服务器则会接收这些消息。
3. 它读取、处理，然后将输出返回到标准输出。

**好处**：

1. 通信速度非常非常快。实际上是在同一台机器上的两个进程之间交换数据，显然会很快。
2. 这种传输方式非常非常安全，因为我们没有打开任何网络端口，所有操作都在同一台机器上进行，所以任何形式的攻击都是不可能的。
3. 实现起来非常简单，因为您使用的所有语言都支持 STDIO，所以在 MCP 中实现它并不需要太多努力。


*远程服务器，HTTP+SSE*

HTTP 是互联网上最流行的应用协议。借助 HTTP 的帮助，可以访问世界上任何地方的服务器。主机向服务器发送一个 POST 请求。SSE 的全称是 Server Sent Events，你可以认为它是 HTTP 的一个扩展。SSE 被创建或用于流式传输。

从技术上讲，使用 SSE，我们的服务器可以通过一个开放的连接向客户端发送多条消息。所以，如果我的主机/客户端在某个点上通过连接与服务器相连，那么通过 SSE 的帮助，服务器可以通过该连接向客户端发送多条消息。因此，与其发送一个大的 JSON，我们的服务器会开始数据流，它会逐块发送数据，就像数据一旦准备好就发送出去一样。这样，我们就能看到数据以这种方式陆续到达。


回过来理解 *JSON-RPC 是与传输无关的*。我们为本地服务器使用了 stdio 传输，为远程服务器使用了 http+sse。在这两种情况下，我们写消息的方式都是一样的，那就是json rpc。我们之所以能做到这一点，是因为json rpc并不关心它是通过哪种传输模式从一个地方传送到另一个地方的，它支持两种模式。


设计者将数据层（data layer）与传输层（transport layer）彻底解耦。即便日后更换传输层协议或引入新型传输机制，也完全无需改动数据层逻辑——这正是 MCP 架构最核心的设计哲学。
