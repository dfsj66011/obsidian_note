
LangGraph æ˜¯ä¸€ä¸ªä½çº§åˆ«çš„ç¼–æ’æ¡†æ¶å’Œè¿è¡Œæ—¶ç¯å¢ƒï¼Œä¸“ä¸ºæ„å»ºã€ç®¡ç†å’Œéƒ¨ç½²é•¿æœŸè¿è¡Œä¸”å…·æœ‰çŠ¶æ€çš„æ™ºèƒ½ä½“è€Œè®¾è®¡ã€‚åŒ…æ‹¬ Klarnaã€Replitã€Elastic ç­‰æ­£åœ¨å¡‘é€ æ™ºèƒ½ä½“æœªæ¥çš„å…¬å¸éƒ½å¯¹å®ƒä¿¡èµ–æœ‰åŠ ã€‚

LangGraph æ˜¯ä¸€ä¸ªéå¸¸åº•å±‚çš„æ¡†æ¶ï¼Œå®Œå…¨ä¸“æ³¨äºæ™ºèƒ½ä½“ç¼–æ’ã€‚åœ¨ä½¿ç”¨ LangGraph ä¹‹å‰ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å…ˆç†Ÿæ‚‰æ„å»ºæ™ºèƒ½ä½“æ‰€éœ€çš„ä¸€äº›ç»„ä»¶ï¼Œä» *æ¨¡å‹* å’Œ *å·¥å…·* å¼€å§‹ã€‚

åœ¨æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†é¢‘ç¹ä½¿ç”¨ LangChain ç»„ä»¶æ¥é›†æˆæ¨¡å‹å’Œå·¥å…·ï¼Œä½†ä½¿ç”¨ LangGraph å¹¶ä¸å¼ºåˆ¶è¦æ±‚ä½¿ç”¨ LangChainã€‚å¦‚æœä½ æ˜¯æ™ºèƒ½ä½“å¼€å‘çš„æ–°æ‰‹ï¼Œæˆ–å¸Œæœ›è·å¾—æ›´é«˜å±‚æ¬¡çš„æŠ½è±¡æ¡†æ¶ï¼Œæˆ‘ä»¬æ¨èæ‚¨ä½¿ç”¨ LangChain æä¾›çš„ agents æ¶æ„â€”â€”å®ƒå·²ç»ä¸ºå¸¸è§çš„ LLM è°ƒç”¨å’Œå·¥å…·å¾ªç¯åœºæ™¯é¢„ç½®äº†ç°æˆè§£å†³æ–¹æ¡ˆã€‚

LangGraph ä¸“æ³¨äºå¯¹æ™ºèƒ½ä½“ç¼–æ’è‡³å…³é‡è¦çš„åº•å±‚èƒ½åŠ›ï¼šæŒä¹…æ‰§è¡Œã€æµå¼å¤„ç†ã€äººæœºäº¤äº’ç­‰ã€‚

## <Icon icon="download" size={20} /> Install

hello world:

```python  theme={null}
from langgraph.graph import StateGraph, MessagesState, START, END

def mock_llm(state: MessagesState):
    return {"messages": [{"role": "ai", "content": "hello world"}]}

graph = StateGraph(MessagesState)
graph.add_node(mock_llm)
graph.add_edge(START, "mock_llm")
graph.add_edge("mock_llm", END)
graph = graph.compile()

graph.invoke({"messages": [{"role": "user", "content": "hi!"}]})
```

## æ ¸å¿ƒä¼˜åŠ¿

LangGraph ä¸ºä»»ä½•é•¿æ—¶é—´è¿è¡Œçš„æœ‰çŠ¶æ€å·¥ä½œæµæˆ–ä»£ç†æä¾›åº•å±‚æ”¯æŒåŸºç¡€è®¾æ–½ã€‚LangGraph ä¸ä¼šæŠ½è±¡æç¤ºæˆ–æ¶æ„ï¼Œå¹¶æä¾›ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼š

* æŒä¹…æ‰§è¡Œï¼šæ„å»ºèƒ½å¤Ÿç»å—æ•…éšœå¹¶é•¿æœŸè¿è¡Œçš„ä»£ç†ï¼Œå¯ä»¥ä»ä¸­æ–­å¤„æ¢å¤ç»§ç»­æ‰§è¡Œã€‚
* äººåœ¨å›è·¯ï¼šé€šè¿‡éšæ—¶æ£€æŸ¥å’Œä¿®æ”¹ä»£ç†çŠ¶æ€ï¼Œå¼•å…¥äººå·¥ç›‘ç£ã€‚
* å…¨é¢è®°å¿†ï¼šåˆ›å»ºå…·å¤‡çŸ­æœŸå·¥ä½œè®°å¿†ï¼ˆç”¨äºæŒç»­æ¨ç†ï¼‰å’Œè·¨ä¼šè¯é•¿æœŸè®°å¿†çš„æœ‰çŠ¶æ€ä»£ç†ã€‚
* ä½¿ç”¨ LangSmith è¿›è¡Œè°ƒè¯•ï¼šé€šè¿‡å¯è§†åŒ–å·¥å…·æ·±å…¥æ´å¯Ÿå¤æ‚çš„ä»£ç†è¡Œä¸ºï¼Œè¿½è¸ªæ‰§è¡Œè·¯å¾„ã€æ•æ‰çŠ¶æ€è½¬æ¢å¹¶æä¾›è¯¦ç»†çš„è¿è¡Œæ—¶æŒ‡æ ‡ã€‚
* ç”Ÿäº§å°±ç»ªçš„éƒ¨ç½²ï¼šå€ŸåŠ©ä¸“ä¸ºå¤„ç†æœ‰çŠ¶æ€ã€é•¿æ—¶é—´è¿è¡Œå·¥ä½œæµç‹¬ç‰¹æŒ‘æˆ˜è€Œè®¾è®¡çš„å¯æ‰©å±•åŸºç¡€è®¾æ–½ï¼Œè‡ªä¿¡éƒ¨ç½²å¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚


## LangGraph ç”Ÿæ€

è™½ç„¶ LangGraph å¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œä½†å®ƒä¹Ÿèƒ½ä¸ä»»ä½• LangChain äº§å“æ— ç¼é›†æˆï¼Œä¸ºå¼€å‘è€…æä¾›æ„å»ºæ™ºèƒ½ä»£ç†çš„å®Œæ•´å·¥å…·å¥—ä»¶ã€‚ä¸ºäº†æå‡æ‚¨çš„ LLM åº”ç”¨å¼€å‘æ•ˆç‡ï¼Œå»ºè®®å°† LangGraph ä¸ä»¥ä¸‹å·¥å…·æ­é…ä½¿ç”¨ï¼š

* LangSmith â€” æœ‰åŠ©äºä»£ç†è¯„ä¼°å’Œå¯è§‚æµ‹æ€§ã€‚è°ƒè¯•æ€§èƒ½ä¸ä½³çš„ LLM åº”ç”¨è¿è¡Œï¼Œè¯„ä¼°ä»£ç†è½¨è¿¹ï¼Œåœ¨ç”Ÿäº§ä¸­è·å¾—å¯è§æ€§ï¼Œå¹¶éšæ—¶é—´æ¨ç§»æå‡æ€§èƒ½ã€‚
* LangGraph â€”â€” é€šè¿‡ä¸“ä¸ºé•¿æ—¶é—´è¿è¡Œçš„æœ‰çŠ¶æ€å·¥ä½œæµè®¾è®¡çš„éƒ¨ç½²å¹³å°ï¼Œè½»æ¾éƒ¨ç½²å’Œæ‰©å±•æ™ºèƒ½ä»£ç†ã€‚è·¨å›¢é˜Ÿå‘ç°ã€å¤ç”¨ã€é…ç½®å’Œå…±äº«ä»£ç† â€”â€” å¹¶åˆ©ç”¨ Studio ä¸­çš„å¯è§†åŒ–åŸå‹è®¾è®¡å¿«é€Ÿè¿­ä»£ã€‚
* LangChain - æä¾›é›†æˆå’Œå¯ç»„åˆç»„ä»¶ï¼Œä»¥ç®€åŒ– LLM åº”ç”¨ç¨‹åºå¼€å‘ã€‚åŒ…å«åŸºäº LangGraph æ„å»ºçš„ä»£ç†æŠ½è±¡ã€‚

# äºŒã€Quickstart

æœ¬å¿«é€Ÿå…¥é—¨æŒ‡å—å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangGraph å›¾ API æˆ–å‡½æ•°å¼ API æ„å»ºä¸€ä¸ªè®¡ç®—å™¨ä»£ç†ã€‚

* [Use the Graph API](#use-the-graph-api) å¦‚æœæ‚¨æ›´å€¾å‘äºå°†æ‚¨çš„ä»£ç†å®šä¹‰ä¸ºèŠ‚ç‚¹å’Œè¾¹çš„å›¾ã€‚
* [Use the Functional API](#use-the-functional-api) å¦‚æœä½ æ›´å€¾å‘äºå°†ä½ çš„ä»£ç†å®šä¹‰ä¸ºä¸€ä¸ªå•ä¸€å‡½æ•°ã€‚

å¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ª Claudeï¼ˆAnthropicï¼‰è´¦æˆ·å¹¶è·å– API å¯†é’¥ã€‚ç„¶åï¼Œåœ¨ä½ çš„ç»ˆç«¯ä¸­è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡ã€‚

----------
ï¼ˆä»¥ä¸‹å†…å®¹ä½¿ç”¨ Graph APIï¼‰

## 1. å®šä¹‰å·¥å…·å’Œæ¨¡å‹

åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Claude Sonnet 4.5 æ¨¡å‹ï¼Œå¹¶å®šä¹‰åŠ æ³•ã€ä¹˜æ³•å’Œé™¤æ³•çš„å·¥å…·ã€‚

```python  theme={null}
    from langchain.tools import tool
    from langchain.chat_models import init_chat_model


    model = init_chat_model(
        "claude-sonnet-4-5-20250929",
        temperature=0
    )


    # Define tools
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply `a` and `b`.

        Args:
            a: First int
            b: Second int
        """
        return a * b


    @tool
    def add(a: int, b: int) -> int:
        """Adds `a` and `b`.

        Args:
            a: First int
            b: Second int
        """
        return a + b


    @tool
    def divide(a: int, b: int) -> float:
        """Divide `a` and `b`.

        Args:
            a: First int
            b: Second int
        """
        return a / b


    # Augment the LLM with tools
    tools = [add, multiply, divide]
    tools_by_name = {tool.name: tool for tool in tools}
    model_with_tools = model.bind_tools(tools)
    ```

## 2. å®šä¹‰çŠ¶æ€

è¯¥å›¾çš„çŠ¶æ€ç”¨äºå­˜å‚¨æ¶ˆæ¯å’Œ LLM è°ƒç”¨çš„æ¬¡æ•°ã€‚çŠ¶æ€åœ¨ LangGraph ä¸­è´¯ç©¿ä»£ç†çš„æ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ã€‚å¸¦æœ‰ `operator.add` çš„ `Annotated` ç±»å‹ç¡®ä¿æ–°æ¶ˆæ¯ä¼šè¢«è¿½åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­ï¼Œè€Œä¸æ˜¯æ›¿æ¢å®ƒã€‚

```python  theme={null}
    from langchain.messages import AnyMessage
    from typing_extensions import TypedDict, Annotated
    import operator


    class MessagesState(TypedDict):
        messages: Annotated[list[AnyMessage], operator.add]
        llm_calls: int
    ```

## 3. å®šä¹‰æ¨¡å‹èŠ‚ç‚¹

æ¨¡å‹èŠ‚ç‚¹ç”¨äºè°ƒç”¨ LLM å¹¶å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ã€‚

```python  theme={null}
    from langchain.messages import SystemMessage


    def llm_call(state: dict):
        """LLM decides whether to call a tool or not"""

        return {
            "messages": [
                model_with_tools.invoke(
                    [
                        SystemMessage(
                            content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
                        )
                    ]
                    + state["messages"]
                )
            ],
            "llm_calls": state.get('llm_calls', 0) + 1
        }
    ```

## 4. å®šä¹‰å·¥å…·èŠ‚ç‚¹

å·¥å…·èŠ‚ç‚¹ç”¨äºè°ƒç”¨å·¥å…·å¹¶è¿”å›ç»“æœã€‚

```python  theme={null}
    from langchain.messages import ToolMessage


    def tool_node(state: dict):
        """Performs the tool call"""

        result = []
        for tool_call in state["messages"][-1].tool_calls:
            tool = tools_by_name[tool_call["name"]]
            observation = tool.invoke(tool_call["args"])
            result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
        return {"messages": result}
    ```

## 5. å®šä¹‰ç»“æŸé€»è¾‘

æ¡ä»¶è¾¹ç¼˜å‡½æ•°ç”¨äºæ ¹æ® LLM æ˜¯å¦è°ƒç”¨äº†å·¥å…·æ¥è·¯ç”±åˆ°å·¥å…·èŠ‚ç‚¹æˆ–ç»“æŸã€‚

```python  theme={null}
    from typing import Literal
    from langgraph.graph import StateGraph, START, END


    def should_continue(state: MessagesState) -> Literal["tool_node", END]:
        """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

        messages = state["messages"]
        last_message = messages[-1]

        # If the LLM makes a tool call, then perform an action
        if last_message.tool_calls:
            return "tool_node"

        # Otherwise, we stop (reply to the user)
        return END
    ```
 
## 6. æ„å»ºå¹¶ç¼–è¯‘ä»£ç†

è¯¥ä»£ç†æ˜¯ä½¿ç”¨ `StateGraph` ç±»æ„å»ºçš„ï¼Œå¹¶é€šè¿‡ `compile` æ–¹æ³•è¿›è¡Œç¼–è¯‘ã€‚

```python  theme={null}
    # Build workflow
    agent_builder = StateGraph(MessagesState)

    # Add nodes
    agent_builder.add_node("llm_call", llm_call)
    agent_builder.add_node("tool_node", tool_node)

    # Add edges to connect nodes
    agent_builder.add_edge(START, "llm_call")
    agent_builder.add_conditional_edges(
        "llm_call",
        should_continue,
        ["tool_node", END]
    )
    agent_builder.add_edge("tool_node", "llm_call")

    # Compile the agent
    agent = agent_builder.compile()

    # Show the agent
    from IPython.display import Image, display
    display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

    # Invoke
    from langchain.messages import HumanMessage
    messages = [HumanMessage(content="Add 3 and 4.")]
    messages = agent.invoke({"messages": messages})
    for m in messages["messages"]:
        m.pretty_print()
    ```

æ­å–œï¼æ‚¨å·²ä½¿ç”¨ LangGraph å›¾ API æ„å»ºäº†æ‚¨çš„ç¬¬ä¸€ä¸ªä»£ç†ã€‚

```python
      # Step 1: Define tools and model

      from langchain.tools import tool
      from langchain.chat_models import init_chat_model


      model = init_chat_model(
          "claude-sonnet-4-5-20250929",
          temperature=0
      )


      # Define tools
      @tool
      def multiply(a: int, b: int) -> int:
          """Multiply `a` and `b`.

          Args:
              a: First int
              b: Second int
          """
          return a * b


      @tool
      def add(a: int, b: int) -> int:
          """Adds `a` and `b`.

          Args:
              a: First int
              b: Second int
          """
          return a + b


      @tool
      def divide(a: int, b: int) -> float:
          """Divide `a` and `b`.

          Args:
              a: First int
              b: Second int
          """
          return a / b


      # Augment the LLM with tools
      tools = [add, multiply, divide]
      tools_by_name = {tool.name: tool for tool in tools}
      model_with_tools = model.bind_tools(tools)

      # Step 2: Define state

      from langchain.messages import AnyMessage
      from typing_extensions import TypedDict, Annotated
      import operator


      class MessagesState(TypedDict):
          messages: Annotated[list[AnyMessage], operator.add]
          llm_calls: int

      # Step 3: Define model node
      from langchain.messages import SystemMessage


      def llm_call(state: dict):
          """LLM decides whether to call a tool or not"""

          return {
              "messages": [
                  model_with_tools.invoke(
                      [
                          SystemMessage(
                              content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
                          )
                      ]
                      + state["messages"]
                  )
              ],
              "llm_calls": state.get('llm_calls', 0) + 1
          }


      # Step 4: Define tool node

      from langchain.messages import ToolMessage


      def tool_node(state: dict):
          """Performs the tool call"""

          result = []
          for tool_call in state["messages"][-1].tool_calls:
              tool = tools_by_name[tool_call["name"]]
              observation = tool.invoke(tool_call["args"])
              result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
          return {"messages": result}

      # Step 5: Define logic to determine whether to end

      from typing import Literal
      from langgraph.graph import StateGraph, START, END


      # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call
      def should_continue(state: MessagesState) -> Literal["tool_node", END]:
          """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

          messages = state["messages"]
          last_message = messages[-1]

          # If the LLM makes a tool call, then perform an action
          if last_message.tool_calls:
              return "tool_node"

          # Otherwise, we stop (reply to the user)
          return END

      # Step 6: Build agent

      # Build workflow
      agent_builder = StateGraph(MessagesState)

      # Add nodes
      agent_builder.add_node("llm_call", llm_call)
      agent_builder.add_node("tool_node", tool_node)

      # Add edges to connect nodes
      agent_builder.add_edge(START, "llm_call")
      agent_builder.add_conditional_edges(
          "llm_call",
          should_continue,
          ["tool_node", END]
      )
      agent_builder.add_edge("tool_node", "llm_call")

      # Compile the agent
      agent = agent_builder.compile()


      from IPython.display import Image, display
      # Show the agent
      display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

      # Invoke
      from langchain.messages import HumanMessage
      messages = [HumanMessage(content="Add 3 and 4.")]
      messages = agent.invoke({"messages": messages})
      for m in messages["messages"]:
          m.pretty_print()

      ```


--------------

# ä¸‰ã€Run a local server

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åœ¨æœ¬åœ°è¿è¡Œ LangGraph åº”ç”¨ç¨‹åºã€‚

## å…ˆå†³æ¡ä»¶

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å…·å¤‡ä»¥ä¸‹æ¡ä»¶ï¼š

* LangSmith çš„ API å¯†é’¥ - å…è´¹æ³¨å†Œ

## 1. å®‰è£… LangGraph CLI

```bash pip theme={null}
  # Python >= 3.11 is required.
  pip install -U "langgraph-cli[inmem]"
```

  ```bash uv theme={null}
  # Python >= 3.11 is required.
  uv add 'langgraph-cli[inmem]'
  ```

## 2. åˆ›å»º LangGraph app

ä» [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project) æ¨¡æ¿åˆ›å»ºä¸€ä¸ªæ–°åº”ç”¨ã€‚è¯¥æ¨¡æ¿å±•ç¤ºäº†ä¸€ä¸ªå•èŠ‚ç‚¹åº”ç”¨ç¨‹åºï¼Œä½ å¯ä»¥ç”¨è‡ªå·±çš„é€»è¾‘è¿›è¡Œæ‰©å±•ã€‚

```shell
langgraph new path/to/your/app --template new-langgraph-project-python
```

> **å…¶ä»–æ¨¡æ¿**â€‹ å¦‚æœä½ ä½¿ç”¨Â `langgraph new` è€Œä¸æŒ‡å®šæ¨¡æ¿ï¼Œå°†ä¼šå‡ºç°ä¸€ä¸ªäº¤äº’å¼èœå•ï¼Œè®©ä½ ä»å¯ç”¨æ¨¡æ¿åˆ—è¡¨ä¸­é€‰æ‹©ã€‚

## 3. å®‰è£…ä¾èµ–é¡¹

åœ¨ä½ çš„æ–° LangGraph åº”ç”¨çš„æ ¹ç›®å½•ä¸­ï¼Œä»¥ç¼–è¾‘æ¨¡å¼å®‰è£…ä¾èµ–é¡¹ï¼Œä»¥ä¾¿æœåŠ¡å™¨ä½¿ç”¨ä½ çš„æœ¬åœ°æ›´æ”¹ï¼š


In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

<CodeGroup>
  ```bash pip theme={null}
  cd path/to/your/app
  pip install -e .
  ```

  ```bash uv theme={null}
  cd path/to/your/app
  uv sync
  ```
</CodeGroup>

## 4. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

```bash  theme={null}
LANGSMITH_API_KEY=lsv2...
```

## 5. Launch Agent server

Start the LangGraph API server locally:

```shell  theme={null}
langgraph dev
```

Sample output:

```
INFO:langgraph_api.cli:

        Welcome to

â•¦  â”Œâ”€â”â”Œâ”â”Œâ”Œâ”€â”â•”â•â•—â”¬â”€â”â”Œâ”€â”â”Œâ”€â”â”¬ â”¬
â•‘  â”œâ”€â”¤â”‚â”‚â”‚â”‚ â”¬â•‘ â•¦â”œâ”¬â”˜â”œâ”€â”¤â”œâ”€â”˜â”œâ”€â”¤
â•©â•â•â”´ â”´â”˜â””â”˜â””â”€â”˜â•šâ•â•â”´â””â”€â”´ â”´â”´  â”´ â”´

- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs

This in-memory server is designed for development and testing.
For production use, please use LangSmith Deployment.
```

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the [Platform setup overview](/langsmith/platform-setup).

## 6. Test your application in Studio

[Studio](/langsmith/studio) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

```
>    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
```

For an Agent Server running on a custom host/port, update the `baseUrl` query parameter in the URL. For example, if your server is running on `http://myhost:3000`:

```
https://smith.langchain.com/studio/?baseUrl=http://myhost:3000
```

<Accordion title="Safari compatibility">
  Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

  ```shell  theme={null}
  langgraph dev --tunnel
  ```
</Accordion>

## 7. Test the API

<Tabs>
  <Tab title="Python SDK (async)">
    1. Install the LangGraph Python SDK:
       ```shell  theme={null}
       pip install langgraph-sdk
       ```
    2. Send a message to the assistant (threadless run):
       ```python  theme={null}
       from langgraph_sdk import get_client
       import asyncio

       client = get_client(url="http://localhost:2024")

       async def main():
           async for chunk in client.runs.stream(
               None,  # Threadless run
               "agent", # Name of assistant. Defined in langgraph.json.
               input={
               "messages": [{
                   "role": "human",
                   "content": "What is LangGraph?",
                   }],
               },
           ):
               print(f"Receiving new event of type: {chunk.event}...")
               print(chunk.data)
               print("\n\n")

       asyncio.run(main())
       ```
  </Tab>

  <Tab title="Python SDK (sync)">
    1. Install the LangGraph Python SDK:
       ```shell  theme={null}
       pip install langgraph-sdk
       ```
    2. Send a message to the assistant (threadless run):
       ```python  theme={null}
       from langgraph_sdk import get_sync_client

       client = get_sync_client(url="http://localhost:2024")

       for chunk in client.runs.stream(
           None,  # Threadless run
           "agent", # Name of assistant. Defined in langgraph.json.
           input={
               "messages": [{
                   "role": "human",
                   "content": "What is LangGraph?",
               }],
           },
           stream_mode="messages-tuple",
       ):
           print(f"Receiving new event of type: {chunk.event}...")
           print(chunk.data)
           print("\n\n")
       ```
  </Tab>

  <Tab title="Rest API">
    ```bash  theme={null}
    curl -s --request POST \
        --url "http://localhost:2024/runs/stream" \
        --header 'Content-Type: application/json' \
        --data "{
            \"assistant_id\": \"agent\",
            \"input\": {
                \"messages\": [
                    {
                        \"role\": \"human\",
                        \"content\": \"What is LangGraph?\"
                    }
                ]
            },
            \"stream_mode\": \"messages-tuple\"
        }"
    ```
  </Tab>
</Tabs>

## Next steps

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

* [Deployment quickstart](/langsmith/deployment-quickstart): Deploy your LangGraph app using LangSmith.

* [LangSmith](/langsmith/home): Learn about foundational LangSmith concepts.

* [SDK Reference](https://reference.langchain.com/python/langsmith/deployment/sdk/): Explore the SDK API Reference.

***

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/local-server.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>


---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt





æˆ‘çš„ç›®æ ‡æ˜¯ä»¥éå¸¸æœ‰æ¡ç†çš„æ–¹å¼æŠŠäº‹æƒ…åšå¥½ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨è¿™ä¸ªé¢‘é“å­¦ä¹ äº†æœºå™¨å­¦ä¹ ï¼Œç„¶åå­¦ä¹ äº†æ·±åº¦å­¦ä¹ ï¼Œæ¥ç€åˆå¼€å§‹äº†Lang Chainç­‰ï¼Œè¿˜æ¶‰è¶³äº†ç”Ÿæˆå¼AIã€‚åˆ°äº†è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä¸ªäººè§‰å¾—æˆ‘ä»¬å·²ç»å­¦å¾—è¶³å¤Ÿå¤šï¼Œå·®ä¸å¤šå‡†å¤‡å¥½å­¦ä¹ å’Œç†è§£Lang Graphä»¥åŠå¦‚ä½•æ„å»ºAIä»£ç†äº†ï¼Œè¿™å°±æ˜¯ç¬¬ä¸‰ä¸ªåŸå› ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘æƒ³è°ˆè°ˆæˆ‘å¯åŠ¨è¿™ä¸ªæ’­æ”¾åˆ—è¡¨èƒŒåçš„æ„¿æ™¯ã€‚æ— è®ºä½ åšä»€ä¹ˆäº‹æƒ…ï¼ŒèƒŒåéƒ½åº”è¯¥æœ‰ä¸€ä¸ªå¼ºå¤§çš„æ„¿æ™¯ã€‚

å¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘æƒ³ä¸æ‚¨åˆ†äº«æˆ‘çš„æ„¿æ™¯ï¼Œå³é€šè¿‡è¿™ä¸ªæ’­æ”¾åˆ—è¡¨æˆ‘å¸Œæœ›å®ç°ä»€ä¹ˆç›®æ ‡ã€‚å¦‚æœæˆ‘å¦ç™½å‘Šè¯‰æ‚¨ï¼Œå½“Landgraffè¿›å…¥å¸‚åœºå¹¶é€æ¸ä»æ‚¨çš„ç½‘ç«™æ”¶åˆ°æ¶ˆæ¯è¯´â€œå…ˆç”Ÿï¼Œè¯·æ•™æˆLandgraffâ€æ—¶ï¼Œæˆ‘åšçš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯ä¸ŠYouTubeæœç´¢ç›®å‰æœ‰å“ªäº›å…³äºLandgraffçš„ç°æœ‰å†…å®¹å¯ç”¨ï¼Œè€Œæˆ‘æ³¨æ„åˆ°æœ‰ä¸¤ç§ç±»å‹çš„å†…å®¹ã€‚

åœ¨YouTubeä¸Šï¼Œç¬¬ä¸€ç§å†…å®¹æ˜¯é€šè¿‡ä½¿ç”¨Derick Le Landgraffæ¥æ•™æˆå¦‚ä½•åˆ›å»ºé¡¹ç›®çš„ã€‚è¿™æ˜¯ä¸€ç§ç±»å‹çš„å†…å®¹ã€‚ç„¶åè¿˜æœ‰ç¬¬äºŒç§ç±»å‹çš„å†…å®¹ï¼Œä¸»è¦æ˜¯æ•™æˆLandgrafféå¸¸åŸºç¡€çš„åŸºç¡€çŸ¥è¯†ã€‚åœ¨è¿™ä¸¤ç§å†…å®¹ä¸­ï¼Œæˆ‘å‘ç°äº†ä¸€ä¸ªç¼ºé™·ï¼šåœ¨æ•™æˆåˆ›å»ºé¡¹ç›®çš„åœ°æ–¹ï¼ŒåŸºç¡€çŸ¥è¯†æ²¡æœ‰å¾—åˆ°å……åˆ†è®¨è®ºï¼›è€Œåœ¨ä¸“æ³¨äºåŸºç¡€çŸ¥è¯†çš„åœ°æ–¹ï¼Œè§†é¢‘åˆå¤ªçŸ­äº†ã€‚


