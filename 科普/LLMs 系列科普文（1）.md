
> 注: 本系列科普文内容，全部来自于 [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy) 的 [# Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=5462s) 视频讲座，几乎是文字版的转录，略有调整。
> 
>之所以将其整理为文字版，一是个人看完该系列讲座后受益匪浅，很多问题的确是之前不曾考虑到位的，其次，本身该讲座的定位就是普通大众，我想将其作为学习笔记并传播给更多的大众，因为大语言模型是目前非常热门的一个话题，我认为每个人都应该或多或少的去了解一些。
>
> 虽然这是科普系列，但里面的内容也是极具深度和广度的，真心推荐给你们，如果对某部分内哦哪个感兴趣，建议你认真阅读，并对感兴趣的点，深入阅读其他技术性材料。
> 
> 最后引用费曼的一句话，我们共勉：what I cannot create, I do not understand（**无创造，则无真知**）


这是一个面向普通大众、较为全面介绍 chatgpt 等大预言模型（Large Language Models, LLMs）的系列科普文章，有助于帮大家了解这类模型的创建过程以及背后的相关运作机制，同时也从技术原理上帮助大家消除一些对 LLMs 错误的或理想化的观念。

尽管 LLMs 在某些方面充满魔力且令人惊叹，有些事它做得非常出色，但也有一些则不尽人意，同时还有许多需要注意的比较尖锐的问题。当我们在同 chatpgt 或 deepseek 窗口进行聊天的时候，这个文本框背后究竟是什么？我们应该向它输入什么？这些生成出来的文字又是从何而来？它的运作原理是什么？我们实际上是在和什么东西对话？接下来我们一一探索这些话题。


## 一、准备训练资料

模型的训练就像从小培养一个小孩学习一样，如果我们期望未来这个小家伙非常聪明，那必然需要向他灌输很多学习资料中的内容，并且如果我们期望他不仅懂数学、物理，也懂天文、地理，那这就必然需要向其灌输各种不同类型的资料，再进一步，如果我们希望它未来知道的更多呢？那我们就想办法把我们能搜集到的所有的数据通通都喂给它。

因此我们致力于获取大量质量极高的文档资料。同时我们也追求文档内容的广泛多样性，因为这些模型需要吸纳海量知识。简而言之，我们既需要大批优质文档，又要求这些文档覆盖领域足够宽广。

拥有一份数据庞大且高质量的训练资料，是训练 LLMs 中非常重要也极其耗时耗力的一个环节，所有的 LLMs 供应商，如 OpenAI、Anthropic、Google 等内部都会有专门的团队负责这件事情，他们的数据可能来自于网络采集、公司内部数据等，可以猜得到，这样的数据他们是从来不会公开的，一方面这本身就是极具价值的公司财产，另一方面这些数据中可能存在敏感数据，一旦公开可能会面临版权问题。

<img src="https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/assets/images/fineweb-recipe.png" width="500">

然后，一家名为 HuggingFace 的公司，收集并精心整理了一份名为 FineWeb 的数据集，并在他们撰写的博客[^1]中，详细介绍了他们的收集及处理过程，这是一份非常详实的实验报告。

以 FineWeb 数据集为例——它可以代表目前 LLMs 训练中的典型数据量规模——实际占用的磁盘空间仅为 44TB 左右。如今，一个 U 盘就能轻松存储 1TB 数据，甚至几乎可以装进一块单独的硬盘里。因此归根结底，*这并非海量数据*（但对普通大众，这也挺大哈😄）。尽管互联网规模极其庞大，但我们处理的是文本数据，并且进行了严格筛选，最终只剩下了约为 44TB。

正如图中所示，实现这一目标其实相当复杂，需要多个阶段才能做好。接下来，我们稍微看看其中一些阶段的具体情况。

### 1、获取网络数据

毫无疑问，如果想尽可能多的搜集训练数据，就需要通过网络采集，理想情况下，如果能将整个网络中所有数据全部囊括进来，这基本代表人类目前为止的所有数据。因此目前这类工作的起点，基本是借助 Common Crawl 的数据。

Common Crawl[^2] 是一个自 2007 年起就在持续抓取互联网数据的组织，截止到目前，以采集超过 2500 亿个网页，跨越 18 年。每月新增 30 至 50 亿个页面的采集。其基本运作原理是：从少量种子网页出发，然后追踪这些种子页面中所有的链接进行爬取。你只需不断追踪链接，持续索引所有信息，久而久之就会积累大量互联网数据。因此，这通常是许多此类工作的起点。不过，Common Crawl 的数据相当原始，需要经过多种方式的过滤处理。

**URL 过滤**：首先是 URL 过滤的环节。这里指的是存在一些需要被屏蔽的网站列表，本质上就是你不希望从中获取数据的域名或 URL 清单[^3]。因此，通常这包括恶意软件网站、垃圾邮件网站、营销网站、种族主义网站、成人网站等类似内容。在这一阶段，我们会剔除大量此类网站，因为我们不希望它们出现在数据集中。

![[page_html.png|500]]

**文本提取**：你需要知道，所有采集到的这些网页，都是最原始的 HTML 代码。就像当你在页面中邮件，选择“检查”时，呈现出来的就是原始 HTML 的样子。你会注意到它包含所有这些标记，比如列表之类的东西，还有 CSS 以及所有这些内容。所以这几乎就是这些网页的计算机代码。但我们真正想要的只是页面中的正常文本内容，而不需要导航栏之类的东西。因此，需要大量的过滤、处理和启发式方法，才能充分筛选出这些网页中的优质内容。

**语言过滤**：FineWeb 会使用语言分类器进行过滤。他们会尝试猜测每个网页所使用的语言，然后只保留英语内容占比超过 65% 的网页。因此你可以理解，这就像不同公司可以自行决定的设计策略。我们要在数据集中涵盖多大比例的不同类型语言？举例来说，如果过滤掉所有日语数据，你可能会想到，后续模型在处理日语时表现不佳，因为它从未接触过足够多的该语言数据。不同公司对多语言性能的重视程度可以有所差异，可以想象得到，deepseek 在这里则是尽可能多的保留中文内容。

**去重等操作**：经过语言过滤后，还会进行其他几步筛选步骤，包括去重等处理。这里最著名的相关工作是利用 minhash 算法对数据进行去重的操作，采集到的网页数据会存在很多近似重复的内容，这一点很容易理解，例如博客的转载，镜像网站等，最后以去除个人身份信息（PII）作为收尾工作。这类信息包括地址、社保号码等敏感数据。我们需要在数据集中检测此类内容，并将含有这类信息的网页过滤剔除。这里有很多步骤，就不详细展开了，具体可以查看 FineWeb Blog 内容。

需要注意的是，尽管我们在我们只是轻描淡写，但实际在面临几十甚至几百 TB 级的原始数据，这里任何一个步骤都需要消耗大量的计算机资源，通常这都是由分布式集群处理的。个人或一般型企业公司，一般是不具备这种能力的。

### 2、数据窥探

不管怎么样，感谢 HuggingFace 的奉献，任何人都可以在 Hugging Face 网页上[^4]下载这份数据，

![[fineweb_data.png|600]]

第一条数据是介绍 2012 年龙卷风的一篇文章，第二条是说人体内有两个小小的黄色肾上腺，貌似是医学类的吧，总之在这里我们可以看到网页数据基本都被清洗干净，得到这样的一份份文本数据，这样的数据我们现在有很多，有 40TB 那么多。

![[train_data_raw_text.png|600]]

此处图示为我们将前两个网页数据拼接在一起的示意图，这是一段未经处理的原始文本，即我们人类可读的语言文本。现在我们手头有了这么一大段文本素材，接下来就需要构建能模仿生成它的神经网路。不过在将文本输入神经网络之前，我们必须先确定 *如何表示这些文本以及如何输入*。

### 3、文本表示

当前我们的技术方案是：这些神经网络需要接收一维的符号序列，并且要求这些符号来自有限的预设集合。

这是什么意思呢？正如上图显示的这样，文本就像由一个个符号，自左向右一个个按顺序展列的，这个符号可以按单词为单位，也可以按字符为单位，甚至可以按某种片段为单位；而所谓的有限的预设集合需要我们一开始就限定，模型可以识别的所有符号有哪些。因此我们必须先确定符号体系，再将数据表示为这些符号的一维序列。

显然我们目前的文本数据本身就是一维的文本序列，这一点无须再做其他处理，既然这段文本将来由计算机处理，自然就存在这些文本的底层数据表示形式，实际上是文本的 UTF-8 编码，这样我们就能得到这些文本在计算机存储中的原始比特数据。

![[bits.png|600]]

它的呈现形式是这样的。这实际上就是这段文本在计算机实际存储中的样子，即计算机的世界是 2 进制的，只有 0 和 1，很显然，这样的转换后，这个序列将非常长，实际上，这个序列长度在我们的神经网络中是一种非常有限且宝贵的资源，我们并不希望 *只有 2 个符号* 却生成 *极其冗长* 的序列。相反，我们**需要在符号集（即词汇表）的大小与最终序列长度之间做出权衡**。因此，我们不想仅用两个符号却得到超长的序列。

我们需要更多的符号和更短的序列。那么，一种最自然的压缩或缩短序列长度的方法是：将连续的比特位（例如八位）组合成一个称为“字节”的单元。例如这里的前 8 位 '01001000' 实际上就是大写字母 'H'，与我们原始文本实际看到的完全一致。由于这些比特位只有 0 或 1 两种状态，如果我们取八位一组，实际上只存在 $2^8=256$ 种可能的组合。因此，我们可以将这个序列重新表示为字节序列。这样字节序列的长度将缩短为原来的八分之一，但我们现在有 256 种可能的符号。这里的每个数字范围都是从 0 到 255，这实际上就是 ASCII 编码[^5]。

![[bytes.png|600]]

第一个数字 72 正是二进制数串 '01001000' 转为十进制后的数字，但真心建议你们不要把这些当作数字，而是看作独特的 ID 或符号看待。或许更恰当的做法是……用独特的表情符号来替换每一个数字。这样你就会得到类似这样的结果。

![[emoji.png|600]]

所以我们基本上有一个表情符号序列，共有 256 种可能的符号。你可以这样理解。但事实证明，在生产最先进的语言模型时，实际上需要超越这个范围。

### 4、Tokenization

我们希望继续缩短序列长度，因为这同样是一种宝贵的资源，用以换取词汇表中更多的符号。实现这一目标的方法是运行所谓的 BPE 编码算法。

其工作原理是，我们本质上是在寻找那些频繁出现的连续字节或符号。例如，我们发现序列 101 后面接 32 的情况非常普遍且频繁出现，在我们示例中出现了 177 次（实际 101 是字母 'e'，32 是空格）。因此我们我们将 (101,32) 这组配对合并为一个新的符号，即创建 ID 为 256 的符号（此前只有 0~255），这意味着我们将文本中所有的 '101 32' 替换为 256，如下图所示。

![[bpe.png|600]]

然后可以在修改后的基础上再次寻找最频繁出现的组合，将其作为 ID 257 等，一直迭代下去，每次生成新符号时，都会减少序列长度并增大符号规模。实践证明，将词汇表大小设定为约 10 万个可能的符号是较为理想的选择。具体而言，GPT-4 使用了 100,277 个符号。这种将原始文本转换为这些符号（我们称之为 token）的过程就称为**标记化(tokenization)**。

现在让我们来看看 GPT-4 是如何执行 tokenization 的——它是如何将文本转换为 token，又如何将 token 转换回文本，以及这一过程实际呈现的效果。

![[tiktokenizer.png|600]]

有一个用来探索这些 token 表示的网站叫 Ticktokenizer[^6]。在右上角下拉菜单中选择 ‘cl100k_base'，这是 GPT-4 base 模型的标记器。左侧可以输入文本，它会显示该文本的标记化结果。例如，“hello world” 实际上恰好由两个 token 组成：一个是 ID 为 15,339 的 “hello” token，另一个是 ID 为 1,917 的 “ world”（注意前面有个空格，与 'world' 不是一个东西） token。

现在，如果将输入改为 'helloworld'（中间不带空格）我会再次得到两个标记，但这次是标记 “h” 和 “elloworld”。如果我在 hello 和 world 之间加入两个空格，会得到 15339, 220, 1917。这里会出现一个新标记 220。你可以自己尝试调整，看看会发生什么变化。另外请注意，这是区分大小写的。所以如果是大写的 H，那就是另一个东西了。或者如果是 "HELLO WORLD"，实际上这会变成 3 个 token，而不仅仅是两个 token。没错，你可以通过这个工具来把玩体验，直观感受这些 token 的工作原理。

![[data_tokenizer.png|600]]

我想向你展示的是，我们原始的文本归根结底可以这样理解：例如，如果我在这里选取一行，GPT-4 将会将其解析成图片中的样子。这段文本将是一个长度为 43 的序列，这就是文本块与这些符号的对应关系。同样地，这里共有 100,277 种可能的符号。现在我们得到的是这些符号的一维序列。

当我们回到 FindWeb 数据集时，他们提到这不仅占据了 44TB 的磁盘空间，而且该数据集包含约 15T（T, trillion, 万亿） 个 tokens 序列。这里展示的只是该数据集前几个 tokens 示例。但请记住，整个数据集实际上包含 15T 个 tokens。


-----

[^1]: FineWeb bolg: https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
[^2]: Common Crawl: https://commoncrawl.org/
[^3]: blacklists: https://dsi.ut-capitole.fr/blacklists/
[^4]: FineWeb data: https://huggingface.co/datasets/HuggingFaceFW/fineweb
[^5]: ASCII code: https://www.ascii-code.com/
[^6]: Tiktokenizer: https://tiktokenizer.vercel.app/