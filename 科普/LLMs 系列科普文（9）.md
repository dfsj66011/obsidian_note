

## 九、推理模型 - 模型需要 token 去思考

接下来要讨论的这部分内容涉到模型在解决问题场景中的计算能力，或者更准确地说，是它们固有的计算能力。因此，我们**在构建对话示例时必须格外小心，这里有很多需要特别注意的地方**。

当我们思考这些模型如何思考时，它们看起来还挺有趣的。所以请考虑以下来自人类的提示，假设我们正在构建一段对话，准备将其加入我们的对话训练集中。

![[appledemo.png|500]]

我们要用这个示例来训练模型，我们正在教它如何基本解决简单的数学问题，示例中，这是个非常简单的数学问题，左边和右边有两个答案，它们都是正确答案，它们都说答案是三，这是正确的。

但对于助手来说，这两个答案中有一个明显比另一个更好。假设你是数据标注员，正在创建这样的答案，其中一个对助手来说会是非常糟糕的回答，而另一个则勉强可以。所以我希望你能停下来，思考一下为什么其中一个答案比另一个好得多。

如果你选用了不好的标注数据，它可能在数学方面表现的会非常糟糕，导致不良结果。在训练人员为助手创建理想回复时，这一点需要在标注文档中格外注意。这个问题的关键在于要认识到并记住：无论是训练还是推理时，模型都是在从左到右处理一维的 token 序列。

为了始终生成序列中的下一个 token，我们将所有这些 token 输入到神经网络中，然后神经网络会给出序列中下一个 token 的概率。还记得我们使用的神经网络的样子吗？

![[visualization.png|600]]

现在，重要的是要认识到，大致来说，这里发生的计算层数基本上是有限的。例如，这里的这个模型（右侧图）只有三层所谓的注意力机制和 MLP（多层感知机）。而一个典型的现代最先进网络可能会有大约 100 层或类似的结构，但从之前的 token 序列到下一个 token 的概率，只有大约 100 层的计算量。因此，对于每一个 token，这里发生的计算量是有限的，虽然这整个一次 token 的预测计算需要涉及到几十亿个参数规模的计算，但相比较你需要在这样的计算中能准备的预测出 10 万级别数量的概率，即从 10 万个可能的 token 中找出正确的那个，你确实应该把这看作是非常少量的计算。而且这个计算量对于序列中的每个 token 来说几乎是固定的，虽然这并不完全正确，因为输入的 token 越多，神经网络的前向传递成本就越高，但增加得并不多。

所以你应该考虑这一点，对于每一个 token 来说，这个模型里的计算量是固定的。而且这个计算量不可能太大，因为这里的层数并不多，从上到下没有那么多计算会发生。因此，你无法想象模型仅通过一次前向传递就能完成任意计算来生成单个 token。这意味着我们实际上必须将推理和计算分散到多个 token 上，因为每个 token 只能承载有限的计算量。因此，我们希望将计算任务分摊到多个 token 上，而不能指望模型在单个 token 上完成过多计算。

![[appledemo2.png|500]]
(你猜对了吗？)

所以这就是为什么左侧这个答案明显更差。原因在于想象一下从左到右移动，在问题的基础上，然后预测下一个 token，“答”，“案”，“是”，“3” 

问题就出现 “3” 这个 token 这里，们期望它基本上将所有关于这个问题的计算都压缩到这个单一的 token 中。它必须输出正确答案三。一旦我们输出了答案三，我们期望它会说出后续的其他 token。但到了这一步，我们已经得出了答案，并且这个答案已经存在于后续所有标记的上下文窗口中。因此，后续的一切都只是在事后解释为什么这个答案是正确的，因为答案早已生成，已经在 token 窗口中了。

因此，如果你直接且立即回答问题，你就是在训练模型试图用一个 token 来猜测答案。而这正是因为每个 token 的计算量是有限的，所以那种方式行不通。右边的答案之所以明显更好，是因为我们将计算分散在整个回答过程中。实际上，我们是在让模型逐步得出答案。从左到右，我们得到了中间结果。也就是说，橙子的总成本是 4，所以 13-4=9。我们正在创建中间计算。这些计算中的每一个本身并不那么昂贵。

实际上，我们基本上是在某种程度上猜测模型在这些单个 token 中能够处理的难度。从计算角度来看，这些 token 中的任何一个都不可能有太多工作，否则模型在测试时就无法完成。因此，我们在这里教导模型将其推理和计算分散到各个 token 上。这样一来，每个 token 只需处理非常简单的问题，而这些简单问题可以累积起来。到了接近尾声的时候，它已经将所有之前的结果都存储在工作记忆中。这时它更容易确定答案，看，答案就是 3。

因此，右侧这个对我们的计算来说是一个明显更好的标签。所以，这是一个值得记住的有趣现象：在你的提示中，通常你不需要明确考虑这一点，因为 OpenAI 的工作人员有标注员等人员专门负责处理这个问题，他们会确保答案在文本中的分布是均匀的，实际上，OpenAI 会做出正确的处理。

![[chatgptapple.png|450]]

它会定义变量，建立方程，然后就会产生所有这些中间结果，这些其实并不是给你看的，是给模型的。如果模型没有为自己生成这些中间结果，它就无法达到 3。

![[chatgptapple2.png|500]]

我还想向你展示，对模型稍微苛刻一点也是可以的。我们可以直接索要答案。举个例子，我给了它完全相同的提示，我说，用一个 token 回答问题。事实证明，对于这个简单的提示，它实际上能够一次性完成，它仅通过神经网络的一次前向传递就做到了这一点，这是因为这里的数字非常简单，而 gpt-4o 是一个非常庞大的模型，对于这个简单的问题，它可以一步计算出来。

![[chatgptapple3.png|500]]

所以我故意增加了一点难度，让模型稍微吃点苦头。所以我说，我把数字稍微调大了一点，我只是想让模型的计算更难一点。我要求它在单个 token 内完成更多计算，结果它给出了 5，而正确答案实际是 7。因此，模型未能在网络的一次前向传递中完成所有这些计算。它未能从输入 token 开始，然后通过网络的一次前向传递，一次性产生正确的结果。

![[chatgptapple4.png|400]]

然后我说，可以使用更多的 tokens 重新解决这个问题，然后它会罗列方程，处理所有的中间结果，每一个中间计算步骤对模型来说都更加容易。而且，每个 token 的工作量并不算太大，最后得出了一个正确的解，也就是七。

网络无法在一次前向传播中完成这一任务。所以我觉得这更像是一个有趣的例子，值得我们去思考。而且我认为，这再次阐明了这些模型的工作原理。

---

#### 虽然、但是

虽然上面的示例中，模型可以算出正确答案 7，但我建议你，如果在日常实践中真的试图用模型解决此类问题，我们并不要轻易相信模型在这里的所有中间过程都是正确的，朴素的模型内部其实并不存在这样的计算器运算逻辑，它甚至都并不懂得如何计算 2+2=4，在它眼里，这就是一堆 token 序列而已。

当然更好的方式是开发一套计算器工具供 LLMs 使用，就像使用搜索引擎那样，借助一些外部工具帮我们计算一些复杂的数学运算，当 LLMs 遇到了此类问题，它将整理出它的计算表达式，然后呼叫工具帮它计算。

当然作为计算机从业者，我们可能会使用代码进行解决，这也是一个工具。原则上，模型在任何一个中间步骤都可能出错。我们正在使用神经网络进行心算，就像你在大脑中做心算一样。这可能会搞砸一些中间结果。但它确实能进行这种心算，其实还挺惊人的。但不管怎样，我们使用想要更靠谱的答案，我们演示一下让它使用 Python 解释器——一种非常简单的编程语言——来编写计算结果的代码。

![[chatgptapple5.png|500]]

（图片左下角所示，其实我勾选了 “Write or code” 这个工具了）

如果看不懂这段代码也没关系，总之它帮我们写了一段代码，并通过运行一下这段代码，最后输出了 7.0 正确答案。

我个人会更信任这个结果，因为它是由 Python 程序生成的。我认为相比语言模型的心算，Python 程序能提供更高的准确性保证。这再次提示我们，如果你遇到这类问题，或许可以干脆让模型使用代码解释器来解决。就像我们之前看到的网络搜索功能一样，模型有专门的 token 来调用这些工具。

它背后实际发生的事情是，模型根据我们的问题，它会编写程序，然后将该程序发送到计算机的另一部分，这部分实际上只是运行该程序并返回结果。然后，模型就能获取该结果，并告诉你，好的，每个苹果的成本是 7，就像图片左上角的答复那样。


---------

这就是为什么我将这一部分命名为“模型需要 Token 才能思考”。将你的计算分散到多个 Token 上。要求模型生成中间结果。或者，只要有可能，就尽量依靠工具和工具的使用，而不是让模型在内存中完成所有这些事情。所以，如果它们试图在内存中完成所有操作，*不要完全信任它，尽可能优先使用工具*。

