
## 十二、推理模型

许多公司，比如 OpenAI 和其他大型语言模型提供商，已经内部试验强化学习对 LLMs 进行微调有一段时间了，但他们没有公开讨论过。这些都是在公司内部进行的。这就是为什么今年春节期间 DeepSeek 如此火爆的原因，deepseek 不仅在于它是我国本土化开发出来可以接近 openai 等顶尖闭源模型的开源模型（有点绕嘴，闭源模型就是他们不愿意分享他们的模型，只能通过付费接口请求它，就像我们在网页上使用那种，而开源模型是我们可以下载他们分享出来的模型，自己部署使用），最重要的是他们发表的 deepseek-r1 系列论文[^1]详细地探讨了如何通过强化学习对大语言模型进行微调，强调其对大语言模型的至关重要性，以及如何显著提升模型的推理能力。

该论文重新激发了公众对运用强化学习优化大语言模型的兴趣，并提供了大量可复现实验结果的关键细节，为实际应用于大语言模型奠定了基础。

下面我们简单介绍一下这篇 DeepSeek-R1 的论文，当你真正正确地将强化学习应用于语言模型时会发生什么，它看起来是什么样子，以及它能给你带来什么。
<img src="https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_aime_with_maj.png" width="500">
这是论文中的图 2，我们正在观察模型在解决数学问题方面的改进。这是在 AIME 数据集上的准确率。

![[AIME.png|500]]

这里有一些示例，基本是这是面向高中生的数学竞赛题目。

可以看到模型一开始它们的表现并不理想，但随着模型经过数千步的更新，它们的准确率会逐渐提升。因此，随着在这类问题的大数据集上不断试错，模型也在不断改进，它们能以更高的准确率解决这些问题。这些模型正在学习如何解决数学问题，但比用更高准确率解决这些问题的量化结果更令人难以置信的，是模型实现这些结果的定性方式。
<img src="https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_length.png" width="500">
这是论文中的图 3，这里有一个相当有趣的数据点：在优化过程的后期阶段，模型似乎开始增加每个回答的平均长度。这意味着模型正在通过使用更多 tokens 来获得更高的准确率结果。换句话说，它正在学习生成极其冗长的解决方案。可以看到后期基本达到 8000 多步（每一步可以理解为生成一个新的 token）

![[DeepSeektable3.png|450]]

为什么这些解决方案如此冗长？我们可以在这里进行定性分析。基本上，他们发现模型生成的解决方案变得非常、非常长，部分原因是这样的：如图所示，首先有一个问题，然后是大致是模型给出的答案。模型学会的做法（这是优化过程中自然涌现的特性，它只是发现这对解决问题有帮助）就是开始做类似这样的事情。

<font color="red">Wait, wait. Wait. That’s an aha moment I can flag here.</font> Let’s reevaluate this step-by-step to identify if the correct sum can be ⋯

> 翻译：等等，等等。等一下。这是个值得标记的顿悟时刻。让我们一步步重新评估，看看正确的总和是否可能是⋯

那么模型在这里做什么呢？模型基本上是在重新评估步骤。它发现，为了提升准确性，尝试多种思路、从不同角度探索、回溯、重构和重新审视更为有效。这与我们在解决数学问题时所做的许多事情类似，但它是在重新发现你脑海中发生的过程，而非你写在解答上的内容。没有任何人能将这些东西硬编码到理想助手的回应中。

这只有在强化学习的过程中才能发现，因为你不知道该在这里放什么。结果证明这对模型有效，并提高了其解决问题的准确性。因此，模型学会了我们称之为你头脑中的这些思维链，这是优化的一个*涌现特性*。

而这正是导致响应时间膨胀的原因，但同时也提升了问题解决的准确性。令人惊叹的是，这个模型正在探索思考的方式，它正在学习一种认知策略的东西——如何操控一个问题，如何从不同角度切入，如何引入类比或进行类似的不同操作，以及如何随着时间的推移尝试多种方法，从不同视角检验结果，最终解决问题。但在这里，它某种程度上是被强化学习发现的。能在优化过程中看到这种现象自然浮现，而无需在任何地方硬编码，实在令人难以置信。我们唯一提供的就是正确答案，而它仅仅是通过尝试正确解题就自行涌现出来，这简直不可思议。

----

现在让我们回到我们一直在处理的问题上，看看这种我们称之为推理或思考模型的方法会如何解决这个问题。

![[r1apple.png|500]]

论文中描述的 DeepSeek R1 模型可以在 chat.deepseek.com 上使用。这是开发它的公司托管的一个平台，但该平台经常出现请求失败的情况，我们可以使用其他厂商部署的 deepseek-r1 服务。

当我们把问题输入给它，他开始罗列方程求解，然后又再次验证，并且最后对单位也进行验证，最终认为没有问题，然后，它在完成思考过程后，会为人类写出一份漂亮的解决方案，甚至在最后答案中的 3
 会被粗体标注（太长，图片未截全，自行脑补）。

因此，现在考虑的是，这部分更侧重于正确性方面，而另一部分则更侧重于呈现方式，即如何清晰地展示并在底部用框或者字体加粗的形式呈现出正确答案。令人惊叹的是，我们*得以窥见模型的思考过程*。这正是强化学习过程所带来的成果，这就是导致 token 序列长度膨胀的原因。它们在进行思考，并尝试不同的方法。这正是在解决问题时为你提供更高准确性的关键所在。正是在这里，我们见证了那些顿悟时刻、各种策略以及确保获得正确答案的创意方法。

deepseek-r1 是一个开源模型或开放权重的模型，任何人都可以下载和使用它，你可能无法以全精度运行完整的模型，更不会在个人电脑上运行它，因为这是一个相当大的模型。但许多公司都在托管完整的最大模型，如今你可以在许多地方见多它的身影并可以免费使用。

如果你是 chatgpt 付费用户，在页面的左上角也会看到下拉菜单，里面提供了很多 “o” 系列的模型，如  o1、o3 mini 等，它们都提到了"使用高级推理"。这里所说的"使用高级推理"，指的是根据 OpenAI 员工的公开声明，这些模型采用了与 DeepSeek R1 非常相似的强化学习技术进行训练。

而像你们在免费版里能用的 gpt-4o 等模型，你们应该把它们主要看作是监督微调模型，它们实际上并不像你们在强化学习模型中看到的那样进行这种思考。尽管这些模型也涉及一点点强化学习——我稍后会提到这一点——但它们主要还是监督微调模型。

当这里需要稍微说明一点，如果你是用付费版 chatgpt，并使用 “o” 系列这样的推理模型，它也会像 deepseek 那样显示一些中间的思考内容，但非常简略，我们在这里看到的，并不是模型实际的完整思考内容，我们看到的，更像是被总结后的简略版思考过程，所以，尽管在底层，模型会产生这种思维链，但 OpenAI 选择不在网页界面上展示确切的思维链。它展示了这些思维链的简要概述。 OpenAI 这么做，部分原因是他们担心所谓的"蒸馏风险"，即有人可能会试图模仿这些推理痕迹，仅通过复制思维链就能恢复大量的推理能力。所以他们有点隐藏它们，只展示一些简短的摘要，使你无法完全获得像 DeepSeek 中关于推理本身的内容。

尽管我们看不到底层的全部细节，就性能而言，我认为这些模型和 DeepSeek 模型目前大致相当。由于评估的原因，很难确切判断。但如果你每月支付 20 或 200 美元给 OpenAI，我认为目前 gpt 系列有些模型看起来仍然更胜一筹。不过，DeepSeek R1 作为一款思维模型，目前仍是相当可靠的选择。

---

那么，目前的总结是什么呢？我们讨论了强化学习，以及在优化过程中思维的产生——当我们基本上在许多数学和可验证解决方案的代码问题上运行强化学习时。比如有答案 3 等等。现在，你可以在 DeepSeek 或任何推理提供商中访问这些思维模型，并在那里选择 DeepSeek。这些思维模型在 chatgpt 的 o1 或 o3 模型下同样可用。

如果你有一个需要高级推理的提示词，那么你可能应该使用一些思维模型，或者至少尝试一下。但根据我的经验，在很多情况下，当你问一个更简单的问题时，比如基于知识的问题或类似的问题，这可能就有点小题大做了。比如，没有必要花 30 秒去思考一个事实性问题。因此，我有时会默认直接使用 gpt-4o。根据经验，我大约 80% 到 90% 的使用场景都是 gpt-4o。当我遇到非常困难的问题，比如数学和代码等方面时，我会选择使用推理模型，但这样我就得多等一会儿，因为它们需要思考。

除此之外，目前已经有很多推理模型，如 google 的 gemini 系列中和 Anthropic 的 claude 系列中都包含一些推理模型，但 google 做的产品体验体验非常差，很杂乱，相比较而言你基本很难找到它的页面入口，尽管 gemini 号称目前最强。基本上这几个供应商的模型就代表着这些大语言模型的前沿发展情况。

我认为强化学习正处于一个令人兴奋的新阶段，但要准确把握细节仍具挑战性。正因如此，截至 2025 年初——所有这些模型和思维模型都还处于实验性阶段。这就像是利用优化过程中涌现的推理能力，在解决这些高难度问题方面进行的前沿性突破。


[^1]: deepseek-r1 paper: https://arxiv.org/pdf/2501.12948
