
前文，我们对 LLMs 的网络架构做了粗略的说明，网络训练实际是在做什么，并重点解释了什么是推理，推理过程是如何进行的。接下来让我们来看一个具体的训练和推理示例，这样你就能了解这些模型在训练时的实际运作情况。

## 三、案例演示 —— GPT-2

![[GPT2_paper.png|500]]

GPT 代表 Generatively Pre-trained Transformer，这是 OpenAI 推出的 GPT 系列的第二代版本。今天当你与 ChatGPT 对话时，支撑这一神奇交互的底层模型正是 GPT-4，也就是该系列的第四代版本。而 GPT2 则是 OpenAI 在 2019 年发表的成果，上图是 GPT-2 的论文[^1]。

之所以钟爱 GPT-2，是因为它首次完整呈现了可辨识的现代技术架构。按照现代标准来看，GPT-2 的所有组件至今仍具有辨识度。只不过一切都变得更庞大了。当然，由于这是篇技术论文，我们无法在此详述其全部细节。但可以罗列一些重点细节内容：

* GPT2 是一个 Transformer 神经网络，就像你今天会使用的神经网络一样。它有 1.5B（B, billion, 十亿） 个参数，而如今，现代 Transformer 模型的参数量已高达数十、数百 B。
* 最大上下文长度是 1,024 个 tokens。因此，当我们从数据集中采样 token 窗口的片段时，我们永远不会取超过 1,024 个 tokens。当你试图预测序列中的下一个 token 时，你的上下文中永远不会有超过 1,024 个 token 来进行预测。这在现代标准下也是微不足道的，如今，上下文长度已经大幅提升至数十万甚至可能达到百万级别。这样一来，历史记录中可容纳的上下文信息更多，token 数量也大幅增加。通过这种方式，你能够更准确地预测序列中的下一个 token。
* 最后，GPT2 的训练数据大约是 100B 个 tokens。按现代标准来看，这个规模也相当小。正如前文提到的，fineweb 数据集有 1.5T 个 tokens，所以 100B 其实很少。

2019 年训练 GPT2 的成本估计约为 4 万美元。但如今，你能做得比这好得多而且这还没怎么费劲。我觉得今天你可以把价格降到 100 美元左右。为什么成本下降这么多呢？首先，这些数据集的质量大幅提升。其次，我们筛选、提取和准备数据的方式也变得更加精细。因此，数据集的质量要高得多。这是一方面。但最大的不同在于，我们的计算机硬件速度大幅提升。此外，用于运行这些模型并尽可能从硬件中榨取所有速度的软件，随着大家都专注于这些模型并试图以极快的速度运行它们，这些软件也有了很大的改进。

网上存在很多复现 GPT-2 工作的相关项目，例如 llm.c Let's Reproduce GPT-2，这是一篇技术性很强的长文[^2]，使用的是一个  8XH100 的节点（可以理解为一个具有 8 张 H100 显卡的服务器），复现工作只花了一天时间和大约 600 美元。

我们不在这里详细介绍 gpt-2 的复现过程，但可以稍微解释下这其中的训练过程，可以让你有一个直观的感受。

![[train_gpt2.png|600]]

上图是训练 gpt-2 模型过程中的中间日志内容，这里的 step 开头的每一行，都是对模型的一次更新。所以请记住，我们基本上是在为每一个 token 改进预测。同时我们也在更新这些神经网络的权重或参数。我们通过微调其参数，使其能更准确地预测下一个 token。

具体来说，这里的每一行都在提升对训练集中 1M（M，million，百万） 个 token 的预测能力。也就是说，我们实际上是从这个数据集中提取了 1M 个 token 来进行优化。我们试图同时改进对所有 1M 个 token 的下一个 token 的预测。在每一个步骤中，我们都会对网络进行相应的更新。每一行最后可以看到 1 秒钟大概可以完成 15 万个 tokens 的预测，这意味这每一行大概耗时 6、7s 的时间，则 32000 步需要大概 60 个小时，显然图中展示的训练速度比较慢一些，这取决于所使用的机器硬件性能。

现在，需要密切关注的一个数字就是这个叫做 loss(损失)的值。损失值是一个单一的数字，它告诉你神经网络当前的表现如何。这个数值的设计初衷是越低越好。因此，你会看到随着我们对神经网络进行更多更新，损失值在不断下降，这意味着对序列中下一个 token 的预测会越来越准确。因此，损失值就是作为神经网络研究者所关注的数字。你只能耐心等待，百无聊赖地消磨时间。同时你也在确保一切看起来不错，这样每次更新时，你的损失都在减少，网络的预测能力也在不断提高。

图中显示我们目前训练到第 540 步左右，所以，我们只完成了略多于 1% 的工作量。大概进行了 15 分钟左右，现在，每 20 步我都会配置这个优化进行推理。所以在图中下方，我们正在运行这个推理步骤。而这个模型就是在预测序列中的下一个 token。显然，生成出来的文本乱七八糟，还不够连贯。但请记住，这只是训练进度的 1%。因此，模型在预测序列中的下一个 token 方面还不够熟练。所以输出的内容其实有点像是胡言乱语，但其实它仍然保留了一些局部的连贯性。

![[train_gpt2_start.png|600]]

如果你愿意的话，你可以自己动手尝试复现运行一下，并每隔一段时间对比查看一下模型的预测能力，你会发现，它在一开始的时候，看起来完全就是在随机输出（见上图），随着训练进度的展开，它的预测能力逐渐得到提升，事实上，如果我们等待完整的 32,000 步训练过程，模型的改进程度会达到生成相当连贯英语的水平。生成的词汇流准确无误，整体英语表达也显得更加自然流畅。在这个阶段，我们只需要确保损失在减少，让一切看起来都很顺利，而我们只能耐心的等待。

### 计算资源

现在，我们来谈谈所需的计算资源。显然，这样的训练过程，是没办法在个人计算机中进行的，因为网络实在太庞大了，而且个人计算机的硬件配置是远远不够的。这一切都在云端的计算机上运行。

以上文复现项目为例，他们使用的是一个 8xH100 的节点。也就是说，一台电脑里有八张 H100 的显卡（市场价大概 1 张二三十万人民币），实际上这样的显卡资源是可以按需租赁的，提供这项服务的国内外供应商有很多，租赁价可能是每小时每个 GPU 收费 3 美元。因此，如果你想体验，可以租用这些设备，然后你在云端获得一台机器，你可以进入并训练这些模型。
<img src="https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs22/data-center/h100/h100-og.jpg" width="600">
H100 GPU 看起来就是这个样子，。所以这是一块H100 GPU。GPU 非常适合用于训练神经网络，因为它们需要极高的计算量，这类计算能展现出高度的并行性。因此，你可以让许多独立的工作单元同时运作，共同解决神经网络训练背后涉及的矩阵乘法运算。这只是其中一块 H100 GPU，但实际上，你会把多块组合在一起。比如可以把八块堆叠成一个节点。然后，你可以将多个节点堆叠成整个数据中心或整个系统。因此，当我们观察数据中心时，就会开始看到类似这样的结构，
<img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdbfd21a-4bdc-4fa4-a6d0-5e9d82795e28_1793x897.png" width="600">
从一个 GPU 扩展到八个 GPU，再到单个系统，再到多个系统。这些就是规模更大的数据中心。当然，它们的价格会高得多。目前的情况是，所有大型科技公司都非常渴望获得这些 GPU，因为它们功能强大，可以用来训练各种语言模型，这从根本上推动了英伟达股价自 2023 年的飙升。

![[nvidia_stock.png|500]]


他们争抢 GPU，获取足够多的 GPU 让它们能够协同工作来完成这种优化。那 GPUs 都在做什么呢？它们都在协作预测像 FindWeb 数据集这样的数据集上的下一个 token。这是极其昂贵的计算流程。GPU 越多，就能尝试预测和改进更多 token，处理数据集的速度也会更快。你可以更快地进行迭代，获得更大的网络，训练更大的网络，以此类推。这就是所有这些机器正在做的事情。这就是为什么这一切如此重要。所有这些 GPU 都非常昂贵，也将消耗大量的电力。但它们都只是在试图预测序列中的下一个 token，并通过这样做来改进网络。而且可能会比我们在这里看到的要快得多地生成更加连贯的文本。

### 基础模型

好吧，遗憾的是，我等凡人都没有几千万或几亿美元来训练一个像这样真正庞大的模型。但幸运的是，我们可以求助于一些大型科技公司，它们会定期训练这些模型，并在训练完成后发布部分模型。他们投入了大量计算资源来训练这个网络，并在优化结束时发布该网络。因此这非常有用，因为他们为此进行了大量计算，但实际上，其中发布所谓 *基础模型* 的公司并不多。

在这里训练结束后得到的这个模型被称为基础模型。什么是基础模型？它本质上是个 token 模拟器，一个互联网文本 token 的模拟器。就其本身而言，它目前还*不具备实用价值*。因为我们想要的是一种所谓的助手。我们希望能提出问题并得到回答。这些模型*无法做到这一点*。他们只是对互联网进行某种混杂创作。因此，基础模型并不经常发布，因为它们只是我们迈向智能助手所需的几个步骤中的第一步。

不过，已经有几个版本发布了。举个例子，GPT-2 模型在 2019 年发布了 1.5B 参数的版本，GPT-2 模型就是一个基础模型。那么，什么是模型发布？发布这些模型是什么样的？

![[gpt2_github.png|500]]

这是 GitHub 上的 GPT-2 仓库[^3]。基本上，发布模型需要两样东西：

第一，我们通常需要 Python 代码，详细描述模型中执行的操作序列，这里释放出来的通常都是推理阶段需要使用的代码，或其他一些可公开的工具，而训练阶段使用的代码一般不会被公开。`encoder.py` 中重点是关于 BPE 算法的实现，也就是我们在第一篇文章中介绍的 tokenizer 方法；`model.py` 是关于 gpt-2 具体网络架构的实现；各类 `sample.py` 就是在第二篇文章中有所提到的，当网络输出一个 10万维的概率向量，我们如何采样得到具体的下一个 token 是谁。所以这都只是一段计算机代码，通常也就几百行代码。没什么大不了的。这些代码都相当容易理解，而且通常相当标准。

第二样东西是网络的参数值，真正的价值就在那里。这个神经网络实际有 1.5B 个参数，因此，除了源代码之外，他们还发布了参数，这些参数只是一串数字，也就是一个包含 1.5B 个数字的单一列表。目前被公开的代码部分通常会选择 github 平台，这是一个专门用于托管各类代码的平台，参数文件通常较大，几个到几百 GB 等，通常会托管到 Huggingface 平台下。

再次说明，gpt-2 是一个非常古老的模型，在上面图中，可以看到，它发布于 6 年前了，这是一个非常卷的领域，可以用“洞中方七日，世上已千年”来形容，所以让我们再看一个较为近期的模型。




[^1]: gpt-2 paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

[^2]: llm.c: https://github.com/karpathy/llm.c/discussions/677

[^3]: gpt-2 github: https://github.com/openai/gpt-2/tree/master/src
