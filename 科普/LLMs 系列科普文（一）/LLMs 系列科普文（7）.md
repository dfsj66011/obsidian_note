
在 chatgpt 刚火爆的时候，那时候有一个问题是，“LLMs 能否称为下一代搜索引擎”，就像如今，或许我们更乐意在 LLMs 页面窗口下输入你想查询的问题，而不是像过去那样打开谷歌或者必应等搜索引擎（你说啥，你还在用 baidu？）

然而在 LLMs 发展的初期，就像我们在前文中介绍的那样，模型训练结束，网络参数固定后，模型不会再发生改变，因此它的知识受限于训练资料的截止日期

![[gpt4o202310.png|400]]

就像这里，我们直接询问 gpt4o 的训练资料截止日期，它会告诉你是截止到 2023 年 10 月，问它现任总统是谁，它会说是拜登。

在那个时候，如果你想让模型掌握最新的现实世界知识，这基本意味着你需要搜集最新的一些数据资料，继续训练你的模型，然而这种方式的弊端显而易见，没有人会实时不断的去这么做。如今，我们在使用 chagpt 或 deepseek 等一些页面时，输入框下方都会提供是否需要“联网搜索”的选项。

![[deepseek61.png|500]]

这里是“腾讯元宝” APP 页面，我们可以将“联网搜索”点亮，然后在上方我们会看到它确实通过联网检索到了一些资料作为参考，可以点开查看它具体引用的网页内容，如右侧所示，然后将这些资料进行汇总整理，并以非常友好的方式输出展示给我们。

如今，搜索引擎的使用，已经成为 LLMs 最高频使用的工具之一，我们接下来会介绍关于 LLMs 使用工具的相关内容，以及引发的一些相关技术性问题。

## 七、使用工具

针对前文的幻觉问题，与其只是简单地说我们不知道，我们可以引入第二种缓解措施，让大语言模型有机会基于事实来回答问题。那么，如果我问你一个事实性问题而你并不知道答案，你会怎么做？你会采取什么方式来回答这个问题？你可能会去搜索一下，*利用互联网找到答案*，然后告诉我。同样我们也可以让这些模型做同样的事情。

所以，请将神经网络内部的知识，即其数十亿参数中的信息，想象成模型在很久以前的预训练阶段所见过事物的模糊记忆，就像把这些参数中的知识，当作你一个月前读过的东西来理解。如果你持续阅读某样东西，你就会记住它，模型也会记住。但如果是罕见的内容，你可能就不会对那信息有很好的记忆。而你我做的只是去查找它。现在，当你去查找时，你基本上是在用信息刷新你的工作记忆，然后你就能提取它、谈论它等等。因此，我们需要某种等效的方法让模型能够刷新它的记忆或回忆。我们可以通过为模型 *引入工具* 来实现这一点。

![[mitigtion.png|500]]

因此，我们的解决思路是：与其简单地说"抱歉，我不知道"，不如尝试借助工具。我们可以设计一种机制，让语言模型能够输出特殊 token——这些是我们即将引入的全新 token。例如，在这里我引入了两个 token，并为模型如何使用这些 token 定义了一种格式或协议。举例来说，当模型不知道答案时，它不再只是简单地说“我不知道，抱歉”，而是可以选择发出特殊 token  `<SEARCH_START>`，这个查询会被发送到像 OpenAI 使用的 bing.com 或 Google.com 等搜索平台。然后模型会发出查询内容，接着再发出 `<SEARCH_END>` token。

**然后这里会发生的是，当运行推理的模型采样程序看到特殊 token `<SEARCH_END>` 时，它不会继续采样序列中的下一个 token，而是会暂停从模型生成内容。它会转而启动一个与 bing.com 的会话，将搜索查询粘贴到必应中，然后获取检索到的所有文本内容。基本上，它会获取这些文本，可能会用其他特殊 token 重新表示这些内容，然后将这些文本复制粘贴到这里，就像我试图用括号展示的那样。所有这些文本都会汇集到这里，当这些文本被塞到这里时，它就会进入上下文窗口。**

因此，这个模型将网络搜索的文本纳入到上下文窗口中，这些内容随后会被输入到神经网络中。你可以把上下文窗口想象成模型的工作记忆区。还记得我们前文介绍得 few-shot 吗，我们会像模型中提供一些示例辅助模型理解我们向他下达的一些任务指令，只不过在那里，使我们人工事先将这些示例准备好，一次性喂入模型中，而这里则是模型通过搜索引擎，将其检索到的知识追加到我们的 prompt 中，这样模型就可以直接访问这些数据并基于这样的上下文进行知识抽取与整理。

因此，在上下文窗口中拥有的这些数据，它不再是一个模糊的记忆，是可以直接使用的。现在当它在此后采样新 token 时，可以非常轻松地引用已复制粘贴到那里的数据。

这就是这些工具大致的工作原理。而网络搜索只是这些工具之一。我们稍后会看看其他一些工具。但基本上，你需要引入新的 token，引入一些模式，让模型可以利用这些 token 并调用这些特殊功能，比如网络搜索功能。

-------

那么，你如何教会模型正确使用这些工具呢？比如网络搜索开始、搜索结束等等。那么，同样地，你需要通过训练集来实现这一点。所以我们现在需要大量数据和大量对话，通过这些示例向模型展示如何使用网络搜索，这一切就像上面图片中展示的那个样子。

如果你在训练集中有几千个这样的例子，模型实际上会很好地理解这个工具的工作原理。它还会知道如何构建查询结构。当然，由于预训练数据集及其对世界的理解，它实际上对网络搜索是什么有一定的理解。因此，它实际上对什么是好的搜索查询有着相当不错的原生理解。所以这一切就像自然而然就能运作一样。你只需要几个例子来展示如何使用这个新工具。然后它就可以依靠这个来检索信息，并将其放入上下文窗口中。这相当于你我在查阅资料。因为一旦信息进入上下文，它就进入了工作记忆，非常容易操作和访问。

![[DeepSeek_search.png|500]]

但这里其实有一些比较微妙的东西，通过我们上文的介绍，实际上通过特殊 token 的方式，是让模型自己学习，当遇到了自身无法回答的问题后，自主产生特殊 token  `<SEARCH_START>`，然后产生查询的问题，再产生特殊 token  `<SEARCH_END>`，至此它会暂停下一个 token 的预测，而是开始呼叫搜索引擎这个工具进行查询，查询结束后的文本，插入到特殊 token  `<SEARCH_END>` 后面，并继续预测。也就是说，一个问题进来，需不需要使用搜索引擎，取决于模型能不能自身回答出这个问题。

然而，事实上，目前很多 LLMs 供应商释放出的联网搜索功能，它会一股脑的先把你的问题扔给搜索引擎进行搜索，然后基于检索到的内容进行回答，即便是我们向它询问 “2+2=?” 这样问题，它也会先去搜索再回答，也许他们是出于尽可能保证回答的更准确一些的考虑，也就是全部借助于外部知识来源而不是自身模糊的记忆来作答。他们是将是否搜索的选择权释放给用户来控制，只要勾选就一定会先搜索再回答，而不管你的问题是否简单到无须搜索也可以正确回答。

接下来我们将“联网搜索”点灭，即我们不强制性要求它必须每次都要进行联网，而是期望模型能自主判断是否需要使用联网搜索这个工具。

![[DeepSeeknosearch.png|500]]

当我们再次询问端午假期期间的天气情况，模型知道它自身无法解决这个问题，然后便自主进行了搜索引擎工具呼叫。而当我们询问一个简单的问题时，它明白自身就可以成功的解决这个问题，因此无须联网，即便我最后要求它必须联网搜索，它其实也并没有去搜索，因为它认为这个问题的确太简单了，事实上，我们可以通过 prompt 的形式去引导它调用或者不调用工具。

所以，你现在分得清，当你勾选和不勾选“联网搜索”这个按钮，背后实际发生的事情了吗？


#### 知识与工作区记忆

在上文，我们其实提到了。整个训练过程中看的的学习资料，都会被压缩到模型的参数中，当然这是一个有损压缩，或者叫模糊的记忆，当某个具体的学习资料被多次学习，则可能就会被很多的记住，就像前文我们演示过，它基本可以大段大段的复述出维基百科页面内容，而一些领域中的资料如果很少，模型则可能对此学习的不多，总之，这些训练阶段看过的资料，都被模型永久的存储到参数中，这是静态的知识，只要模型的参数不发生变化，则这些知识就始终不会得到改变。因此，依靠知识回答问题，在某种程度上说它只是依赖自己的记忆，因为大概它对自身的权重、参数和激活函数有足够的信心，认为这些信息可以直接从记忆中检索出来。

但反过来，你也可以使用网络搜索来确认。然后对于同样的查询，它实际上会去搜索，找到一堆来源，所有这些内容都会被复制粘贴进去，然后它再次告诉我们答案并附上引用来源。维基百科文章实际上也提到了这一点，这也是我们获取这一信息的来源。这些工具包括网络搜索，模型会自行决定何时进行搜索，大致就是这些工具的工作原理。这也能在一定程度上减少幻觉和事实性错误的发生。因此，我想再次强调这个非常重要的心理学观点。神经网络参数中的知识是一种模糊的记忆。构成上下文窗口的知识和 token 才是工作记忆。

粗略地说，它的运作方式与我们大脑中的机制类似。我们记住的内容相当于参数，而刚刚经历的事情——比如几秒或几分钟前发生的——则可以被视为上下文窗口中的信息。这个上下文窗口会随着你对周围环境的感知体验不断构建起来。因此，这对你在实践中使用 LLM 也有很多影响。

![[knowledgetype.png|500]]

比如，当我们使用 chatgpt 时，我可以要求说，你能帮我总结一下简·奥斯汀的《傲慢与偏见》的第一章吗？这是一个完全合理的要去，chatgpt 实际上在这里做了一个相对合理的回答。chatgpt 之所以能做到这一点，是因为它对《傲慢与偏见》这样的名著有相当不错的记忆。它可能已经看过大量关于这本书的资料，很可能还存在专门讨论这本书的论坛。但通常当我们真正与大型语言模型互动并希望它们记住特定内容时，直接提供信息总是效果会更好。

![[workingmemory.png|500]]

所以我认为一个更好的提示应该是这样的。你能为我总结一下简·奥斯汀的《傲慢与偏见》第一章吗？然后把第一章的全部内容，真的手动找到并粘贴在这里。这么做是因为当内容在上下文窗口中时，模型可以直接访问它，不需要回忆就能准确获取，它可以直接访问上下文中的这些信息。因此，这份摘要的质量预计会明显更高，事实上看起来确实这份摘要要更好。我想即便是人类也会以同样的方式工作。如果你想的话，在总结这一章之前重新阅读一遍，你会写出更好的摘要。

