
接下来，我们再来看一个较为近期的模型，LLaMA-3，GPT-2 的参数规模是 1.5B，训练数据量是 10B tokens。LLaMA3 则是一个规模更大、更现代化的模型。它由 Meta 发布并训练，是一个拥有 405B 参数、基于 15T tokens 训练的模型。

## 四、LLaMA-3.1：基础模型推理

![[llama_paper.png|600]]

在这篇论文[^1]详细介绍了他们发布的 LLaMA3.1 405B 参数模型，这是基础模型。除此之外，他们还发布了指令模型，指令模型意味着这是一个助手。你可以向它提问，它会给你答案。我们稍后还会讲到那部分。

目前，我们不妨先看看这个基础模型——这个 token 模拟器，来把玩一番，试着思考：这究竟是什么？它是如何运作的？如果让它在海量数据上运行到极致，训练出一个庞大的神经网络，最终我们能得到什么？

![[llama3_demo1.png|600]]

这里推荐的基础模型互动的平台是 Hyperbolic[^2]，他们主要提供 405B 参数的 LLaMA3.1 基础模型。当你进入网站时（可能需要注册等操作），请务必在模型选项中确认你选用的是 LLaMA3.1-405B base 版，必须是基础模型。这里有个参数叫 "max tokens"，它决定了我们将生成多少个 token。

由于这是付费接口（4 美元/ 1M tokens），所以我们就稍微减少一点，以免浪费计算资源。我们只需要接下来的 128 个 token，其他的参数就不用管了，温度参数用于控制文本多样性，基本上这意味着值越小，同一个问题多次重复询问，生成的内容会高度雷同，值越高，则基本每次得到的内容都相差较大，而 Top P 主要用于解码策略，我们此处不会详细解释这些，系统提示参数在后文中会介绍。

现在，从根本上说，这里发生的事情与我们推理过程中发生的事情是一样的。所以这只会在你给它任何前缀的 token 序列基础上继续做文字接龙游戏。

因此，我想先向你们展示，这里的这个模型还不是一个助手。例如，你可以问它 2+2 等于几，它不会直接告诉你“哦，等于四。还有什么可以帮你的吗？”，它不会这么做。此处真正发生的事情是，它会将你的输入内容，“what is 2+2=?” 会被 tokenizer 处理。然后这些 token 就充当了前缀。接下来模型要做的就是获取下一个 token 的概率。说白了就是个高级的自动补全。这不过是一个非常、非常昂贵的自动补全功能，用于预测接下来的内容。它基于训练文档（基本上是网页）中看到的统计数据进行预测。所以，我们只需按下回车键，看看它会生成什么样的续写 token。

图中实际上已经回答了这个问题，给出了正确答案 4，我们再多尝试几次，对于相同的 token 前缀，我们总是得到不同的答案。原因在于我们得到的是概率分布，并从中进行采样。

![[llama3_demo2.png|600]]


再次说明，首先，它还不是一个助手，这只是一个 token 自动完成的工具。其次，它是个随机系统。

现在，关键之处在于，尽管这个模型本身对许多应用来说还不太实用，但它仍然非常有用，因为在预测序列中下一个 token 的任务中，模型已经学到了很多关于世界的知识。所有这些知识都存储在网络的参数中。还记得我们的文本是什么样的吗？互联网网页。而现在，这一切在某种程度上都被压缩进了网络的权重中。所以你可以把这 405B 个参数看作是对互联网的一种压缩，但它并不是无损压缩，这是一种有损压缩。

现在，我们可以通过适当提示基础模型来引出其中一些隐藏的知识。例如，这里有一个提示，可能有助于引出隐藏在参数中的某些知识。例如，此处我们让它列出巴黎的十大景点，这里把前缀写成这个样子是想引导模型继续完成这个列表。

![[llama3_demo3.png|600]]


它已经给出了一些地标信息，它在这里试图提供大量信息，甚至最后超过我们指定的最大 max tokens 512，被迫终止。不过，你也不能完全相信这里的一些信息。请记住，这一切只是对部分网络资料的回忆。因此，在网络数据中频繁出现的内容可能比那些极少出现的内容更容易被准确记住。因为这些信息并未明确存储在任何参数中，一切都只是模糊的回忆。你可以看到我们已经引出了模型的很多知识，虽然这些知识可能并不准确。这种知识是模糊的、概率性的和统计性的，经常发生的事情往往更容易在模型中被记住。

为了说明或者验证这件事，我们可以再展示几个示例，通常维基百科中的文章被认为是高质量的文本，在训练模型的过程中这部分数据可能会被模型多次学习使用，因此它很有可能会被模型记住，让我们试一下。

![[llama3_demo4.png|600]]

上面图片中左侧是维基百科关于斑马的介绍，右侧使我们将维基百科文章开头部分内容作为前缀输入给 LLaMA3.1，可以看到 LLaMA3.1 在相当长的内容中与维基中完全一致，这个示例中生成的内容是对维基百科条目的精确复述，它纯粹依靠记忆来背诵这段维基百科内容。而这些记忆都储存在它的参数中。

我们预期的是，在后续某个部分，由于模型采样的概率问题，有可能后续生成的内容会偏离维基百科内容，实际上在生成了相当长的一段文本后，它确实偏离了，因为它无法准确记住所有的内容。之所以在前面它能生成相当长的与维基页面完全一致的内容，是因为这些模型可能非常擅长记忆，但通常，这并不是你在最终模型中想要的，这种现象被称为"*反刍或照搬（regurgitation）*"。

实际上这是一种不好的现象，一旦生成出的内容是某个未授权对象的文章，这可能存在版权问题，或者敏感信息泄露等，其次创造力会受限，因为它几乎只会照搬训练资料中已有的内容。通常这种情况发生的原因在于，对于许多文档（比如维基百科），当这些文档被视为非常高质量的信息来源时，在训练模型的过程中往往会优先从这些来源采样。也就是说，模型很可能已经对这些数据进行了多次训练周期，这意味着它可能已经看过这个网页大约 10 次左右。

这有点像我们平时学习，当你反复阅读某段文字很多很多次，比如说读了一百遍，那么你就能背诵它。这个模型也是如此。如果它看到某样东西太多次，它就能凭记忆背诵出来。但这些模型的效率可能比人类高得多，所以它可能只看了 10 次这个维基百科条目，但基本上它已经准确地把这篇文章记在了参数里。

-----

接下来，我们展示的是这个模型在训练过程中*绝对没有见过的东西*。例如，LLaMA-3 论文的预训练数据部分，他们提到，他们训练模型所使用的数据集的知识截止日期是 2023 年底。这意味着它没有包含关于 2024 年美国选举及其结果的信息。

![[llama3_demo5.png|600]]

我们再次复制一小部分内容作为前缀，让模型继续生成后续 token 序列，

> 【2024年11月5日，美国举行了总统选举。共和党候选人——2017至2021年担任美国第45任总统的唐纳德·特朗普】及其副总统搭档迈克·彭斯——击败了民主党候选人、前国务卿希拉里·克林顿与参议员蒂姆·凯恩的组合，赢得大选。这是自1948年以来共和党候选人首次赢得普选多数票，自1988年以来首次获得超过300张选举人票，也是自1984年以来首次赢得超过50%的普选票。

你仍然可以用同样的前缀多尝试几次，每次都会生成不同的内容，但如果你对美国总统选举比较熟悉的话，你会发现这段内容存在多处事实性错误，可以说这是一份虚假报告。这种现象，其实就是所谓的 *"幻觉"*（一本正经的胡说八道），模型只是以概率的方式做出下一个 token 最佳猜测而已。

----

接下来我想展示的是，尽管这只是一个基础模型，还不是一个助手模型，但如果你在 prompt 设计上足够巧妙，它仍然可以应用于实际场景。*prompt*（提示） 就是指你输出给模型的前缀部分内容。

我们要演示的内容是 “few-shot prompt”，具体来说就是需要在 prompt 提供一些相关任务的示例，
这就是我们所说的 “few-shot”，如果只提供一个示例就是 “one-shot”，如果一个示例不给，直接描述任务，则叫作 “zero-shot”。一些简单的任务，zero-shot 或 one-shot，模型已经有足够的能力解决，而对于复杂的任务，few-shot 通常是一个可能有效的方式。

![[llama3_demo6.png|600]]

例如这里有 10 对词组，每对都是一个英文单词后接冒号，然后是它的韩语翻译。我们总共有 10 组这样的对应关系。该模型的作用是，在最后看到 “teacher:” 时，期望的是，会生成出 teacher 的韩语翻译。

这些模型确实具备我们所说的上下文学习能力。当模型在读取这段上下文时，它会在过程中学习到数据中存在某种算法模式，并知道要继续遵循这种模式。这就是所谓的上下文学习。它扮演了翻译者的角色，最后它确实生成的韩语是正确的。由此可见，即使目前我们仅拥有基础模型，通过巧妙地设计提示词，也能构建出实用的应用程序。这里，它还额外生成了一个新的英文单词 “morning:”，也许对模型而言它学会这种模式，一个韩语单词后还要再接一个新的英文单词。

（注：LLaMA 主要是基于英文资料训练的，对中文的理解较差一些，这主要是其训练资料中包含的中文资料较少，国内有一些将其在中文数据上进行微调的 llama 版本）

最后，我想告诉大家，其实有一种巧妙的方法，仅通过 prompt 就能实例化一个完整的语言模型助手。其诀窍在于，我们将设计一个 prompt，首先告诉它的角色是一个乐于助人的助手，然后给他提供一些示例等。实际上，为了撰写这里的提示词，我们可以直接求助了 deepseek 等模型本身，具体使用的 prompt 为：

> I want to create an LLM Assistant but all I have is the LLM base model. So it is just a "document completor". To get it to take on the role of an LLM Assistant, I need to prompt it to be an Assistant. I need your help to write a good prompt for me to use. It should basically look a bit like a web page along the lines of:​​
> 
> Here is a conversation between an AI Assistant and a Human. The AI Assistant is helpful, knowledgeable, and always responds in a clear and polite manner.​​
> 
> After a brief description about the assistant, follow that with a few-shot prompt where the Assistant alternates a dialog with a Human. I will take what you write as the prompt for the base model, and it will answer it as the AI Assistant in the conversation.​​

chatgpt 给出的内容为：

![[Pasted image 20250601201738.png|600]]

说实话，这非常不错，而我们接下来要做的事情就是把 chatgpt 生成的这一大段内容作为前缀，输入给 LLaMA，并在最后，以同样的格式，输入你的问题，

![[llama3_demo7.png|600]]

蓝色框中的内容就是直接从 chatgpt 那边复制过来的，绿色部分是我们需要仿照示例部分的格式，写出我们的问题（添加了前缀 Human:）整个红色部分就是 LLaMA 模型的输出，可以通过这种方式引导它作答。但依然同前前面的示例那样，最后它又自己幻想了要给新的人类问题出来，如果你尝试阅读这里的内容，它确实是在回答我们的问题。

> 翻译：天空呈现蓝色是由于瑞利散射现象。阳光由不同波长的光组成，当穿过地球大气层时，短波长光（如蓝、紫色）比长波长光（如红、橙色）更容易被空气中的分子和尘埃粒子散射。由于人眼对蓝光更敏感，因此白天的天空呈现蓝色。而在日出日落时，阳光角度变化使更多红橙光直达人眼，故此时天空呈现红或橙色。这个解释清楚吗？

作为对比，如果我们直接询问该问题，它会回答吗？

![[llama3_demo8.png|600]]

> 翻译：为什么天空是蓝色的？云如何形成？飓风怎样诞生？为何会有四季？这些问题的答案尽在精彩的教育展览《天气：从魔法到气象学》中。该展览展示了从古代天象观测到现代预报技术的气象学发展史，并探讨天气与气候对人类......的影响。

但我们其实有更好的方式，可以将基础模型直接打造为一个对人类友好的助手，这又会将我们代入到一个新的主题，后训练（post-training）



[^1]: LLaMA Paper: https://arxiv.org/pdf/2407.21783

[^2]: hyperbolic: https://hyperbolic.xyz/
