
## 四、推理模型的使用

 在上一期文章中，我们介绍到，模型的训练有多个阶段。预训练进入监督微调阶段，再进入强化学习阶段。强化学习是模型在大量类似教科书练习题的问题上进行实践的过程。它还能针对众多数学和编程问题进行训练。  
  
在强化学习的过程中，模型会探索出能带来良好结果的思维策略。当你观察这些策略时，会发现它们与你解决问题时的内心独白非常相似。因此，模型会尝试不同的想法，回溯步骤，重新审视假设，并执行诸如此类的操作。  
  
现在很多这样的策略很难由人工标注员硬编码出来，因为思考过程并不明确。只有在强化学习中，模型才能尝试大量方法，并根据其知识和能力找到适合它的思考过程。因此，这是训练这些模型的第三阶段。  
  
这个阶段相对较新，大约只是一两年前的事。过去一年里，所有不同的 LLMs 实验室都在对这些模型进行实验。这被视为最近的一项重大突破。我们之前看了 DeepSeek 的论文，他们是第一个公开讨论这个话题的。他们写了一篇不错的论文，探讨如何通过强化学习来激励大型语言模型的推理能力。 
  
![[draft2.png|600]]
  
因此，我们现在需要对卡通形象稍作调整，因为目前看来，我们的表情符号多了一个可选的思考气泡。当你使用一个会进行额外思考的思维模型时，你实际上是在使用一个经过强化学习额外调优的模型。那么从定性角度来看，这个模型会进行更多的思考。你可以期待的是，你将获得更高的准确性，尤其是在数学、编程等需要大量思考的问题上。*那些非常简单的问题可能不会因此受益，但那些真正深奥且困难的问题可能会获益良多*。基本上，你支付的是让模型进行思考的能力，而这有时可能需要几分钟时间，因为模型会在数分钟内生成大量 tokens，你必须等待，模型就像人类一样在思考，在面对非常棘手的问题时，这种方法可能会带来更高的准确度。  

我们来看一个具体的示例，此处我随便提问了一个中学阶段的物理题目

> [!question]
> 一个初速度为零的物体从高度 $h$ 沿着斜面向下运动，然后以一定的初速度沿同一斜面向上运动，直到停在相同的高度 $h$。哪种情况下的运动时间更长？


<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">
    <img src="physics1.png" alt="Image 1" style="width: 30%; border: 2px solid black; margin: 5px;">
    <img src="physics2.png" alt="Image 2" style="width: 30%; border: 2px solid black; margin: 5px;">
    <img src="physics3.png" alt="Image 3" style="width: 30%; border: 2px solid black; margin: 5px;">
    <img src="physics4.png" alt="Image 4" style="width: 30%; border: 2px solid black; margin: 5px;">
    <img src="physics5.png" alt="Image 5" style="width: 30%; border: 2px solid black; margin: 5px;">
    <img src="physics6.png" alt="Image 6" style="width: 30%; border: 2px solid black; margin: 5px;">
</div>
  
> 各模型的完整解答过程详见附录

这里我们分别讲同一个问题，测验了 6 个不同的模型，具体结果：

* GPT-4o：非推理模型，不具有思考能力，只考虑非摩擦力场景下的结果，30 分
* GPT-o1：推理模型，我不清楚是因为本身 OpenAI 隐藏掉了还是因此我使用的 chatwise 工具的原因，总之看不到任何思考过程，但结果是 6 个模型中唯一准确的，100 分
* Claude-4-Sonnet：非推理模型，但我认为它也有一定的思考能力，尤其在编程代码方面，仅考虑摩擦力场景，但结论错误，0 分
* Claude-4-Opus-Thinking：推理模型，思考 45 秒时间，也仅考虑摩擦力场景，结论正确，70 分
* Gemini-2.5-Flash-Thinking：推理模型，思考 8 秒，但用的是英文思考的，也仅考虑了摩擦力场景，结论正确，70 分
* DeepSeek Reasoner：推理模型，思考了 171 秒，值得吐槽的是，它在此处的思考过程大半是咬文嚼字的书呆子行为，且仅考虑无摩擦场景，30 分。

我个人认为这个题的核心考察点是在考虑阻力等真实场景下，因此该场景给 70 分，如果考虑到思考过程以及解题时间因素，在该场景下 o1 > gemini 2.5 flash thinking > claude 4 opus thinking > 4o > deepseek > claude 4 sonnet.

当然仅用一道题目就给出测评结果，这是非常武断的，并且如果考虑到价格成本问题，我们又会做出新的选择，下面的价格均为，每百万 tokens，输入和输出的价格（美元）：

* GPT-4o：2.5 / 10
* GPT-o1：150 / 600
* Claude-4-Sonnet：3 / 15
* Claude-4-Opus-Thinking：15 / 75
* Gemini-2.5-Flash-Thinking：未标注
* DeepSeek Reasoner：0.55 / 2.19

所以从价格上看 o1 绝对不是一个好的选择，而 deepseek 是一个很亲民的选择。

---------

上面的演示，我使用的是 chatwise 第三方工具，通过配置 API 的方式使用的这些大模型，我本身没有这些平台的原始账户信息，因此无法直接在各家平台上进行过多演示，

![[chatgpttools.png|400]]

以 chatgpt.com 平台为例，如果是免费用户，可在输出框 “工具” 菜单栏里勾选 “深入思考”，但作为免费用户，openai 提供的是 o4-mini，带有 mini 的都是轻量化模型，性能较差。

请注意，这里不要与 “运行深度研究” 选项混淆，这是一个单独的服务，而不是模型的版本选择，我们后文会再介绍。
![[deepseek.png|500]]
作为国内用户，如果使用 chat.deepseek.com，模型平台会为你勾选 R1 和 Search 功能，R1 是 deepseek 具备推理功能的模型，Search 是要求其联网搜索。

-----------

### 应该在何时使用推理模型

简单来说，我们此处想展示的是，有一类我们称之为思维模型的模型。不同的供应商可能有也可能没有思维模型。这些模型在解决数学、代码等难题时最为有效。在这种情况下，它们可以提升你的表现准确性。

很多时候，比如你在询问旅行信息时，使用*思考模型并不会带来额外的好处*。没有必要等待一分钟让它思考你可能想去的目的地。就我个人而言，我通常会尝试非推理模型，因为它们的响应速度非常快。但当我怀疑响应效果可能不够理想，并且希望给模型更多思考时间时，我就会切换到思考模型——具体取决于你手头可用的选项。  例如，当你在 deepseek-r1 下输入 “你好”，它都要思考 10s 钟，这是对推理模型的错误使用方式，当然目前这已经作为前沿课题在研究，使模型内部可以自行判断是否需要进入思考模式。但目前几乎所有不同的 LLM 提供商都会有一个选择器，让你自行决定是否希望模型进行思考。


## 五、使用工具

关于使用工具，在前一期文章中，我们已经详细的说明并演示过了，我们使用的这些语言模型实际上就是一个文件夹中的压缩文件。它是惰性的，是封闭的，没有任何工具，只是一个能输出 token 的神经网络。然而，我们现在要做的是超越这一点，赋予模型使用一系列工具的能力。其中最有用的工具之一就是*互联网搜索*。

今天是 2025 年高考首日，目前语文科目已经结束，我们可以要求它整理高考作文题目，并尝试作答。

![[zuowentimu.png|600]]

这里我们明确的要求其联网查询，因为这是今天上午刚发生的事情，模型自身肯定是不清楚这件事的，而我们也能看到，它这里引用的内容主要来自于 观察网 的一篇文章。

然后我们可以取消掉联网功能，让模型选其中的一个题目继续作答，例如

![[zuowenfanben.png|600]]

（完整示例见附录）

我也不清楚这能达到什么水平，反正我是写不出来，不过写了大概 900 多字，与预期的 800 字不太吻合。

---------

单纯从使用联网搜索的功能上看，首先这比自行通过谷歌等搜索引擎搜索要方便的多，总会有一些烦人的广告内容，各种乱七八糟的东西冒出来，这种体验实在不太愉快，如果有个模型能替你完成这种搜索任务，访问所有网页，然后把所有网页内容塞进上下文窗口，最终直接给你反馈结果，岂不是很好？

本质上我们设计了一种机制——当模型遇到需要联网搜索的情况时，它会发出一个特殊 token 来触发搜索功能。当模型发出“搜索互联网”的指令 token 时，你所使用的 ChatGPT 应用程序或其他任何大语言模型应用程序将停止从模型中采样。它会获取模型生成的查询请求，执行搜索操作，访问网页，提取所有文本内容并将其全部置入上下文窗口。此时我们就拥有了这个能主动为上下文窗口贡献 token 的互联网搜索工具——在这个案例中，这些 token 可能来自数十个网页的聚合内容，最终可能产生数千个 tokens，就像我们亲自浏览这些网页时获取信息的方式一样。当所有网页内容被注入上下文窗口后，系统会回溯你最初关于"整体全国高考语文作文题目"的提问，通过引用这些文本内容给出准确答案。

请注意，这个例子很好地说明了为什么我们需要互联网搜索功能。如果没有互联网搜索，这个模型根本无法给出正确答案。正如我之前提到的，这个模型是几个月前训练的，关于今年高考的信息并不属于模型的实际知识范畴，因为这是今天上午才发生的事情，于是模型必须通过互联网搜索来获取这个知识，它会像你我一样从网页中学习这些信息。一旦这些信息被载入上下文窗口，模型就能回答问题。要记住，上下文窗口就像是工作内存——当我们加载这些文章时，想象它们的文本内容被复制粘贴进上下文窗口，此时这些信息就存在于工作内存中，模型就能据此回答相关问题了。

通常答案会附带引用来源，因此你可以亲自访问这些网页进行核实，确保这些信息并非模型凭空捏造。你还可以二次验证答案的准确性，因为*从原理上来说，模型并不能保证百分之百正确*——它给出的结果可能有效也可能无效。

在某些情况下，模型实际上会意识到这是最新知识，它可能并不了解，因此会主动发起搜索。而在其他情况下，我们需要明确声明要进行搜索。就我个人使用经验而言，我会知道模型并不了解某些信息，因此我会直接选择搜索功能。但不同的模型，甚至不同的应用程序，对这种功能的集成程度各不相同，因此你需要对此保持警惕。有时候模型会自动检测到它们需要进行搜索，而有时候你最好明确告诉模型你希望它执行搜索。所以当我使用 GPT-4o 时，如果我知道这个问题需要搜索，你最好勾选那个选项。这就是搜索工具的作用。


### API 的方式使用工具

除了使用各供应商提供的在线页面上提供的一些组件去使用相应的功能，更直接的使用的方式是利用 API 的方式，但目前各厂商提供的 API 并不统一、各自内置的工具也各不相同，有些功能只有网页上可用，而有些是 API 方式支持，外部的一些工具也五花八门，实际上目前关于工具，Anthropic 已经主推出 MCP（模型上下文协议），目前也是非常火热的方向。

通过 API 的使用方式并不面向大众，主要是面向程序员开发群体，此处仅以 openai 为例，这里 `tools` 参数是个列表，罗列你需要使用的一些工具名称，这里罗列的工具都是模型自己判断是否需要使用的，并不强制，如果需要强制使用某工具，可以通过 "tool_choise" 参数指定

![[websearch1.png|600]]

这里中文给出的是 unicode 编码，我们稍微转下格式：

![[websearch2.png|800]]

它竟然没有搜索到，怎么回事？

![[websearch3.png|500]]
不清楚为什么这个网络搜索工具还有时间戳，看起来这个工具并不是真实的浏览器引擎，查询的信息给出的也是 2025 年 3 月份之前的信息，我重新测试一次，这次竟然给我的是 2024 年高考信息，而它竟然开始说这是 2025 年的，所以*即使联网也是存在幻觉的*。

![[websearch4.png]]

所以你看，这里其实目前还存在很多潜在的问题，在具体使用上都需要稍加留意，以实测为准。



