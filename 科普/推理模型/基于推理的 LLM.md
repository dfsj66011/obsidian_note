
人类大脑推理分为两种类型：快速推理和慢速推理，也是我们常说的系统一推理和系统二推理。系统一帮助我们迅速思考并得出答案，它几乎占用了我们大脑 95% 的部分。它由两个因素驱动：我们的直觉，包括所有过去的经验等，以及我们的本能；系统二则是我们需要花时间思考某个主题的部分，它需要我们付出努力去思考。

OpenAI 于 2024.09.12 发布了首个推理模型 o1。
2025 年初，DeepSeek-R1 发布，1. 开源；2. 准确性与 o1 相当；3. 纯 RL 实现推理能力。


**方法一：推理时间计算缩放**：更多时间意味着能思考得更充分，或许能得出更好的答案。测试时计算是模型在推理过程中使用的计算资源数量。

一般来说，可以分为两大类：

* 第一类被称为提示（prompts），如 CoT 方法，2022，Google Brain
* 第二类是验证器（verifiers）。

**方法二：纯 RL 方法：** aha moment，模型学会了在没有明确提示的情况下进行自主推理。通过设定奖励函数，要求模型学习达成目标奖励的正确路径。

**方法三：监督微调和强化学习方法**。人们认为 OpenAI 的 o1 模型很可能是用这种方法制作的。这种方法也是 DeepSeek-R1 最终采用的。因此在文献中，这被称为构建推理模型的蓝图。

**方法四：纯监督微调与蒸馏**。









我们本质上是在利用一个大型模型，一个拥有海量参数的大型语言模型。你可以把它想象成类似DeepSeq R1这样的模型，具备推理能力。现在我们要做的是：获取这个模型后，生成一组输入-输出配对样本。什么是输入-输出配对？比如你输入500个问题，就能得到对应的500个答案。这些问题可能是什么样的？可能是解谜题，或是类似这样的问题：如果罗杰有5个网球，他又买了2罐网球，每罐有3个，现在他一共有多少个网球？而在输出中，你需要给出完整的推理过程和正确答案。

这就是我所说的输入-输出对，另一种命名方式是监督微调数据。但现在先别管这个术语，在你理解这个概念之前完全没关系。我们现在要做的是把这些数据作为输入，交给一个目前完全没有推理能力的小模型。以微软的QUEN模型或PHY模型为例，这些都是参数规模很小的模型，根本算不上推理模型。你的做法是：用这些输入-输出对数据对这个小型模型进行微调，最终就能获得具备推理能力的小型模型。

所以这几乎就像是POPOI，最初POPOI就像是一个没有推理能力的小模型，然后Spinach就像是来自具备推理能力的大模型的数据集，最终你得到了一个具备推理能力的小模型。这个例子不要在面试中使用，仅供你自己理解。不过这在深海碳技术报告中也展示过，这是一个非常棒的过程，我们实际上将利用这个蒸馏过程来构建我们自己的推理模型，这个模型被称为CHI-T1开源推理AI模型。有一个很受欢迎的GitHub仓库就是关于这个主题的，而且名字就是这个，我们将从零开始构建它。

总结一下，我们涵盖了以下四种方法：推理时间计算扩展、纯强化学习、监督微调、强化学习与纯监督微调以及蒸馏技术。其中后两种方法将包含实践环节——我们将学习如何通过纯强化学习为模型赋予推理能力，以及如何运用蒸馏技术让模型具备推理能力。好的，第一讲到此结束。希望大家已完全理解课程内容，接下来我们将共同踏上探索之旅，学习如何为大型语言模型赋予推理能力。

我希望你们所有人都能在这门课程的所有讲座中保持高度的专注。我们发现，在最初的讲座中，学生们都非常兴奋，但慢慢地这种兴奋感会逐渐消退。我向你们保证，如果你们在这门课程的所有讲座中都能保持兴奋和积极主动的态度，你们将获得与当前行业需求非常相关的技能组合。非常感谢大家，能与你们所有人交谈让我感到非常高兴，我对我的第一门课程感到非常兴奋，并希望能够严格遵循我的教学风格，确保你们在这门课程中都能获得很多乐趣。非常感谢。


hello everyone and welcome to this second lecture on the reasoning based large language models if you have missed the first lecture I strongly recommend you to go back to the introduction video where we have properly introduced the series and how we are going to be structuring this entire course in today's lecture we will start with the first method which can be used to impart reasoning capabilities to large language models imagine that the time frame that we are starting from is December 22 when GPT 3.5 was released first remember that it was made to answer like humans but it was not made to think like humans it was not a reasoning large language model slowly from there on we have come to a stage where we have have large language models that can actually reason that can give you the series of steps which they go through before they give you the final answer open A's OV model is was the first reasoning based model after that there have been a number of reasoning based llms which have come into the picture uh for example deep C carbon so whenever you are listening to this lecture keep in mind that the time frame is 2022 onwards because that is the time after which people started to think about reasoning based llm seriously and uh nowadays a lot of companies and uh almost all llm providers have a reasoning llm in their Armory okay so now we are going to cover method number one which is inference time compute scaling so let us understand what is inference time compute scaling uh all of us are humans and we know that we tend to give better answers when we think about the answer for more time remember when you're writing an exam uh sometimes you are stuck in a question because it involves layers of thoughts only after you spend enough time on that problem are you able to answer the question correctly some other examples of these type of problems where more thinking is required is complex math problems or even puzzles like PSE Sudoku I'm sure all of us have played some kind of crossword puzzles which appear in newspapers well you can't answer all the puzzles at once right there are some horizontal uh answers there are some vertical answers and many of them are connected with each other so unless you have an idea of all the clues in the horizontal and vertical columns you will not be able to fill each and every single entry in the crossword puzzle uh another example is of a mathematical problem where we ask the question are the number of prime numbers finite or infinite this is also not very easy to answer and it is probably going to require a fair require a fair amount of thought before you come to the answer let's think of chess chess is an example where whenever a player makes makes a move the move is never made immediately you might have seen matches where Grand Masters are playing against each other and sometimes they take 10 or 20 minutes or even more than that to play their next move why because they are thinking through all possible scenarios before they make their next move so generally we have observed that as as humans when we think more about a problem we tend to come to the correct answer now similarly we apply the same logic to large language models and and we ask the question what will happen if large language models think more before they give you an answer just pause here for some time before you proceed with the video and uh and try to answer this question do you think llms will give a better answer if you just ask them to spend more time before they give you the answer that is a very interesting question and we are going to find out the answer soon in this lecture let us take a very simple example Roger has five tennis balls and he buys two more cans of tennis balls each can has three tennis balls each and the question we are asking is how many tennis balls does he have now now let's look at the answer given by a regular llm over here so you can see that when when a regular llm tries to answer this question the answer it gives is answer is 11 which is in fact the correct answer and the number of tokens it uses is only three now we will force the model to think more before the model gives an answer so instead of directly giving the answer which is 11 the reasoning based large language model comes up with a series of steps the first step is Roger started with five balls this step uses four tokens the second step is two cans of three tennis balls each is six tennis balls this step uses 11 tokens and the last step is 5 + 6 = to 11 this step uses five tokens so when we ask the model to think more it certainly uses more number of of tokens before it gives you the answer so this is called as test time compute the amount of computational resources the model uses before it gives you an answer is called as test time compute which is Illustrated in this figure from the above example you can clearly see that reasoning based llms uh have significantly higher test time compute compared to regular LMS almost it's three and 23 so it's maybe somewhere between s to eight times but why are we spending so much compute in the testing time how does that make sense because in the previous example even the accuracy stayed the same both the llms gave the correct answer anyways well it turns out that uh if you spend more computational resources in the testing time and you ask the model to think more the model starts to reason for other questions as well and the accuracy of the model significantly improves and this is what we'll be looking at in detail in the remainder of this course the main thing which we look at is uh why does allocating more computation resources in testing uh help increase the accuracy of the model to answer complex Lex reasoning based questions which is very similar to what happens in the case of humans right humans also tend to give better answers when they think more about a problem it turns out that exact same thing happens with large language models as well in this picture you can see that uh in reasoning based llms there is a significant computational resources being used in the inference which I will Mark with my pen over here and in the regular llms inference happen pretty much instantaneously it's it's it's very fast and there is a reason why people do this the reason is that uh we see a scaling which looks like this as the test time compute increases the accuracy of the model also typically tends to increase there are a lot of nuances when I say this sentence uh however I want you to get an idea of why is the word SK scaling in inference time compute scaling the reason is that as you increase the number of resources uh computation resources in the testing time we see that the accuracy of the large language model typically tends to increase we will look at uh more nuances about this statement as as as we follow this lecture okay so uh this this sounds quite intuitive right giving more time to thinking leads to a better answer when did we think of all of this so the first paper which came out which showed that uh if you tell an llm if you give an llm a series of prompts where they are not just input output prompts but you have given them a Chain of Thought which links the input to the output then the llm learns to reason for all other prompts which are not given in the example prompts by the user so have a look at uh what they say in the paper we explore how generating a Chain of Thought which is a series of intermediate reasoning steps significantly improves the ability of llms to perform complex reasoning so what does this mean uh we are evaluating large language models on the ability to perform complex reasoning tasks and it turns out that uh if you ask the llm to generate a series of thought before it gives you the final answer the accuracy on this complex reasoning tasks also increases so in particular we show that how reasoning abilities emerge naturally in sufficiently large language models via a simple method called Chain of Thought prompting so this is the first technique that we are going to learn which is called as Chain of Thought reasoning this paper came out in the year 2022 and it was released by the brain team at uh Google research this work primarily showed that reasoning abilities in a large language model can be unlocked by two main ideas idea number one is that arithmetic reasoning can benefit from generating natural language IR rationals that lead to the final answer so this this uh these words I know sound a bit complicated but natural language rationals mean the series of steps which take you from the question to the answer and idea number two is that we can do this by simply prompting the llms so we don't change anything with the model weights we don't change anything in the model architecture the llm stays the same the brain stays the same we just prompt the llm with a Chain of Thought by using a series of input output examples and then through this prompting the llm will learn to answer in a similar way to other complex reasoning task asks which we ask it which are different from the initial input output examples which we are giving so let's let's try to understand what I meant by that instead of finetuning a model one can simply provide the model with a few input output examples demonstrating the task now remember that when you are actually asking a complex puzzle uh let's say I am asking asking the llm to solve a Sudoku at that time I'm not giving the input output examples this is given beforehand to the large language model only once let's say there are a series of 20 input output example pairs which you are giving to the large language model that's all and then whenever we ask a new problem like a sudo Sudoku examples uh the large language models knows that okay so uh I should answer this in a similar way as the input output examples so if in the input output example you have included a series of thoughts maybe I should answer the final Sudoku puzzle also in a similar way let us try to understand and look at some practical examples later on as well so what is Chain of Thought prompting um have a look at the difference between these two so here uh uh this is one input example input output pair this is the input and this is the output so the input is the same example which we asked before in which Roger has five tennis balls and he buys two more cans of three tenis balls now in the output we don't directly write the answer is 11 but in the output what we write is we write all the series of steps that are required to come up with the answer for example step number one is Roger started with five balls step number two is two cans of three tennis balls each is six balls step number three is 5 + 6 = to 11 and step number four is the answer is 11 so there are four steps which I have explicitly mentioned uh in the output this is called as Chain of Thought prompting so uh let us look at this example again here what you can see is that there are four thoughts which are linked with each other uh as as I've have shown in this figure thought number one thought number two thought number three and thought number four and these four thoughts are linked with each other that is why it is called as a chain of thought and then uh What You observe is that once you give a Chain of Thought the model tends to answer to other inputs also in a similar way for example let's say You have given this uh in one of the example in the input output prompts now if you ask a question which is a new question let's say cafeteria has 23 apples and if they use 20 to make lunch and bought six more how many apples do they have so the model learns to uh think in a similar way for new input examples as well for example you can can see here that the model output is the cafeteria had 23 apples originally they used 20 to make lunch so they had 23 - 20 = to 3 they brought six more apples so they have 3 + 6 equal to 9 the answer is N9 and in the standard prompting the answer it gives is 27 which is the wrong answer now uh okay so so it turns out that this paper now has 12,000 plus citation uh uh so far we have understood that okay so what this paper has uh taught us is that it has taught us uh that introducing Chain of Thought in the input output prompts helps the model to reason well for complex reasoning tasks right but then how come so many citations 12,000 is a huge number right are we really understanding the importance of this paper so uh let's look at the contributions right the first contribution is sufficiently large language models sufficiently large models can generate chains of thought if demonstrations of Chain of Thought reasoning are provided in the examples of few short prompting so what does this statement mean uh this is an example of a demonstration of a Chain of Thought prompt so if the model is provided with examples of these demonstrations maybe 10 or 20 demonstrations then the model actually uh can generate chains of thoughts for new questions new reasoning questions also so again try to read this uh this statement now sufficiently large models can generate chain of thought if demonstrations of chains of thought reasoning which are the input output examples are provided in the examples of few shot prompting so this is a new word which I have not introduced before which says few shot prompting so fot prompting is uh the uh the practice of of providing a number of in input output examples in the prompt so for example uh this is one example of input output right this is one example similar to this you might provide 10 more examples in the input output prompt this method of providing multiple examples in the input output prompt to kind of guide the llm or nudge the llm towards the answer um or or to nudge the llm to understand and how are we expecting the output in what format we are expecting the output that is called as few short prompting why few because we are providing a number of input output examples uh okay so so you can notice here that uh this statement also mentions sufficiently large models what does this mean what do I mean by sufficiently large models I will come to that eventually and uh the paper evaluated the performance of the technique for three different reasoning tasks one is arithmetic reasoning tasks second is common sense reasoning tasks and third is symbolic reasoning tasks by the end of this lecture you should be uh you should be clear with respect to the difference between arithmetic common sense and symbolic reasoning tasks we need to understand the difference between these three okay so uh few shot prompting remember had already existed before people were already providing input output examples in the prompt uh that paper came out in the year 2020 actually where few shot prompting was actually released so let's let's try to look at that paper few shot prompting foundational paper uh this is the paper which I'm talking about so this paper came out in 2020 and there they introduced uh a few short prompting technique however they did not uh they did not talk about Chain of Thought which means that all the examples which they included in the input output examples they looked like this question and directly the answer it did not include the Chain of Thought which which leads to the answer that was the main contribution of the Chain of Thought reasoning paper now uh let's let's look at the different examples uh in the Chain of Thought reasoning which can be used in the prompts uh I have talked several times about different kinds of examples to be given to the large language model in the prompting uh step itself maybe 10 or 20 examples with the chain of thought so let's look at some other examples the first example we have already looked at let's look at an example in the common sense category Sammy wanted to go to where the people were where might he go options are RAC track populated areas desert apartment roadblock so the answer that we we given the prompt is um the answer must be a place with a lot of people RAC tracks desert apartments and roadblocks don't have a lot of people but populated areas do so the answer is B so here you can see the Chain of Thought which we are mentioning in the prompt before we give the final answer we are not directly mentioning The Final Answer over here uh let's look at another example which is in the symbolic reasoning category a coin is heads up Maybel flips the coin Shalonda does not flip the coin is the coin still heads up the coin was flipped by mebel so the coin was flipped one time which is an odd number the coin started heads up so after an odd number of flips it will be tails up so the answer is no so here also we are asked a question and then we know the answer but we are not only giving the answer we are giving the Chain of Thought which leads the llm to the answer so these are the number of different input output examples which are given in the prompt through which the llm can understand that oh okay maybe the user wants me to not directly give an answer but to think of the steps before I give the answer and this small nudge actually improves the accuracy of the llm in complex reasoning tasks dramatically and that's the reason why uh you know this paper has more than 12,000 citations often Concepts which appear simple they are not very simple to conceive and these are the concepts which have the broadest of utility and Chain of Thought reasoning I believe is one such concepts with a concept with a massive utility and very simple to understand because all of us think uh like a Chain of Thought reasoning so so we uh all of us understand this technique very intuitively Okay so it was a very simple idea but uh let's look at the kind of results that they got now whenever you go through any U llm paper you will see a bunch of results and I want you to also understand how to read these results properly this is one of the graphs which they got in the paper the first question we ask is okay which is the large language model which they are using when they get this graph so the large language model is called as palm in case you are not aware of palm model Palm is a large language model which is released by Google and uh it is uh it comes in different sizes the maximum size is 540 billion parameter okay so they have used a pal model and uh what is this study telling us so this study tells us that on the XA you have scaled right so uh as we increase the number of parameters uh which is on the x- axis on the y axis you see the accuracy whenever you see an accuracy metric you should always ask the question that on which data set is this accuracy calculated so this accuracy is calculated for a GSM 8K data set GSM 8K is a arithmetic reasoning data set with a bunch of arithmetic reasoning questions gsmk data set we are going to look at a project after this course where we will understand the different questions which are asked in this data set typically the questions are something like this uh let's say Natalia sold Clips to 48 of her friends in April and then she sold half of that many Clips in May how many Clips did did Natalia sell Al together in April and May so we know the answer is 48 + 24 which is 72 so the model the pal model was given a bunch of questions from the GSM 8 uh GSM 8K data set and then the accuracy was evaluated on that data set how was the accuracy calculated how many questions was the llm able to answer correctly now uh let's look at standard prompting first we see that the accuracy of standard prompting is is really low but as soon as you introduce Chain of Thought prompting the accuracy significantly increases you know there is a big gap uh between standard prompting and Chain of Thought prompting uh okay that is the first observation uh can you observe anything else from this figure the second thing I can observe is that the accuracy of the Chain of Thought prompting actually increases with the scale of the model which means that bigger and bigger uh models are very good at reasoning and they have very good accuracy okay that is fine um any more observations from this graph well I can see one more uh um one more thing from this graph uh if you look at very large Palm models they are almost comparable with uh the best supervised finetune models which means that without going through the computationally expensive uh task of supervised fine tuning we can still achieve the same accuracy on the arithmetic reasoning data set uh by simply using a prompting technique which is the Chain of Thought prompting okay so we have observed um uh three different OB observations first is that uh Chain of Thought prompting Compares significantly well with respect to standard prompting second is that the accuracy increases with the scale of the model and third is that for very large Palm models the accuracy is almost comparable with supervised fine tuning um okay in case you're not familiar with supervised fine tuning it's the process of uh let's say you have a pre-trained large language model um and then you have an additional data set of input output pairs which you again use to train the model so that is called as the step of fine tuning it is usually uh computationally expensive but it improves the llm significantly now here what we see is that we achieve an accuracy which is similar to that of supervised fine tuning without going to uh through the computationally expensive task of the finetuning process okay so uh as we noted uh there are three observations here accuracy is low for small models for higher models accuracy is comparable to supervised fine tuning the second observation is that uh Chain of Thought prompting outperform standard prompting on the arithmetic data set concerned and uh the last uh observation which is very interesting is that Chain of Thought reasoning is an emergent ability of increasing model scale uh this is something which is mentioned multiple times in this paper uh and I want to highlight this statement because I think it is pretty significant I think what this statement means is that as you increase the model size and you try to nudge the model in the direction of Chain of Thought reasoning the model is able to understand that n much better and is able to give you answers which are aligned with that nudge with smaller models simply don't capture and the way I think about it is that if you have a person who uh has a small brain let's say a person who is not very intelligent right uh who is not good at grasping uh grasping Concepts now even if you try to nudge this person person in the direction of reasoning they are never going to get it as opposed to a person who has a who has a refined brain and who has a brain with with an ability to grasp uh Concepts easily now such a person uh a small nudge is going to be sufficient to understand what is expected for complex reasoning tasks and that is exactly U what is happening in the case of these large language models which is which is very interesting to note uh and in this paper uh there is also an analysis of why does the model scale improve the Chain of Thought prompting and you can clearly see that uh here they have analyzed three different types of problems semantic understanding um One Step missing and and the third type of problem uh so what we can see is that as you increase the model as you scale the model from 62 billion to 540 billion parameters these are the errors which are fixed by the model so the model starts to fix errors which the smaller models were initially making and that is because the model is understanding the nudge which has been given in the prompting stage itself so it's able to answer these questions in a much better way compared to the smaller model and this is one outcome which I want all of you to remember okay Chain of Thought reasoning is incredibly effective compared to standard prompting we have understood that but it's not very effective for very small models it's only effective for large models where reasoning is an emergent ability where the model understands the nudge much much better okay so uh here is an example of uh how the larger models gives correct answer compared to the smaller models the question we are asking is that uh Tracy is using a piece of wire which is 4T long and the wire was cut into six uh pieces pieces 6 in long okay how many pieces did she obtain so we know that 1 ft is 12 in so uh let's see okay so so totally there are 4 into 12 which is 48 in uh Ines and then uh the number of pieces are 48 divided by 6 which is 8 which which the uh larger model answers quite correctly but the smaller model gets confused it actually just multiplies 4 into six and gives the answer as 28 so this is a semantic understanding error which is fixed in the larger model this is one example out of all these examples which have been highlighted in the previous figure okay now uh I hope all of you have understood uh how uh how Chain of Thought reasoning works very effectively for arithmetic reasoning tasks but what about common sense reasoning and symbolic reasoning tasks is it that effective or or it's not very effective let's understand firstly uh Common Sense reasoning I had given an example uh previously also here uh if you look at it we had looked at an example of Common Sense reasoning another example is uh would a steel ball sink in water uh so even for common sense reasoning uh from these figures you can see that Chain of Thought prompting always outperforms uh standard prompting although the difference is not very high for csqa uh but for all other examples especially for sports related questions the difference is huge and here it outper performs uh human level performance as well and then we come to symbolic reasoning uh symbolic reasoning is an interesting terminology because uh it is used to define problems like these a coin is heads up AE flips the coin DTI does not flip the coin is the coin still heads up so uh we know the answer is is false to this right because now we know that the coin is is Tails up but it turns out that llms are not very good in answering these symbolic reasoning type of questions and they make errors uh and that is why it is important for us to understand how well an llm is performing on symbolic reasoning tasks as well and uh here also you can see that Chain of Thought prompting significantly outperforms uh standard prompting the main result uh from this study is that Chain of Thought prompting works very well in arithmetic common sense and symbolic reasoning tasks so now we are going to go ahead and we are going to look at technique number two which is zero short reasoning before we go ahead I want all of you to take a step back pause look at whatever we have looked at so far also look at the corresponding paper which we have referenced in this study make sure you have understood all the concepts then we'll move to zero shot reasoning and then finally we'll do some Hands-On projects and in the Hands-On projects we'll look at all the uh all the nuances that we have talked about in the lecture which will give you a practical understanding of how inference type compute scaling is really implemented in practice okay uh so we are going to look at technique number two now in in the method of inference time compute scaling which is zero short reasoning uh and this method is even more simpler than uh Chain of Thought reasoning so let's take an example of a statement uh era go and study in your room you have to write the table of nine make sure to think step by step this simple prompt uh to a person uh makes them understand that they should think step by step before they come to the this exact same logic was used uh by the authors of the zero short reasoning paper uh let's take an example so firstly plain zero shot the meaning of zero shot is if you remember what I explained before about few shot prompting that we provide a series of input output examples in the prompting stage zero shot means that we provide absolutely no examples in the promp in stage at all that is we provide zero examples so this is simple zero shot where there is a question uh and then there is no example provided here as you can see here the answer is blank and then uh this is the output which is wrong over here since it should be uh 16 / 2 ided 2 which is 4 now in this paper they introduced zero short Chain of Thought which means that we still have zero examples as input output pairs but instead of that we only write this let's think step by step which is very similar to uh the example of era which I talked about before so what happens here is that the question stays the same but instead of providing uh an answer in the prompt we just add a sentence which says let's think step by step and then interestingly enough the model learns to reason so how is this different from plain Chain of Thought or few short Chain of Thought in the example of uh Chain of Thought reasoning that we looked at before we provided an input output examples where we told the model that if this is the input you should reason and then you should provide the output now here we are saying that okay we are lazy to do all of that but all I want to do is that I just want to add a sentence which says let's think step by step and then the model learns to reason on its own that's all so uh why zero in the name it's because we are not giving any input output examples in the prompt now uh what what is mentioned in the paper is that uh they are prompting the llm twice in this example the the first prompt is where they ask let's think step by step which we looked at before then we get an answer from the llm then that answer is passed back to the llm and then we also give another prompt which says therefore the answer is okay so uh here what will be the final answer the final answer will be this which is the first answer provided by the llm and then after that there for the answer is 375 so they extract the answer in two steps and uh they obtain fantastic results uh you can see from the graph that zero shot Chain of Thought significantly outperforms zero shot reasoning and now you can read these plots as well this is very similar to the plots uh which we had in the Chain of Thought example here the model is the Palm model the data set which we are using is the arithmetic reasoning data set which is the GSM 8K data set and here also you can see that zero shot Chain of Thought significantly outperform zero shot for very big models that is for 540 billion parameter models so larger Scale Models exhibit better reasoning remind you of anything this is very similar to our observation in the Chain of Thought reasoning case as well okay so uh today we discussed two techniques one is few short Chain of Thought or simple Chain of Thought reasoning and secondly we discussed zero short Chain of Thought which means that we are not going to provide any input output examples but we are only going to tell the large language model to things step by step now in the last part of this lecture we are going to look at two Hands-On projects and these projects I have specifically designed for all the students who might be having the question that where do I exactly specify the uh these input output example prompts I had the same question when I read the original Chain of Thought uh prom paper I was confused that uh where do I write these input output examples and what do they actually mean so whenever we are going to look at projects in these courses uh or in this course we are going to look at specific problem statement which are going to help us understand something on a deeper level so we are going to prob some questions for example in Project one we are going to understand that can we evaluate the effect of model size on Chain of Thought reasoning by ourselves okay so we are going to understand how does model size affect the Chain of Thought reasoning we are going to consider small model models for this uh it takes significantly High time to evaluate on larger models the models which we have considered are flant T5 small flant T5 base flant T5 large tiny Lama 52 and zire 7 billion model as you can see the number of parameters are varying from 80 million to 50 million right up to 7 billion parameters the data set which we are considering are is gsm 8K which is the arithmetic reasoning data set okay uh before we get into the Hands-On demo I want all of you to stop this video and make some predictions how do you think these models will perform uh on this data set if we use Chain of Thought reasoning what kind of answers will they give will they give correct answers or will they give wrong answers now let's dive right into the project uh the projects okay so uh the first project we are going to look at is this which is module one inference time compute scaling effect of model size on accuracy with Chain of Thought reasoning I am going to share all these collap files with you in the chat so you can follow as I'm speaking by interacting with these notebooks the first step is installing the dependencies which are Transformers data sets and accelerate in this case in the second step we are going to load the GSM 8K data set over here and we have extracted 50 examples from it as you can see uh you can ignore this swamp data set for now this is an exploration phase where I was exploring with multiple data sets in including the GSM 8K as well as the Swarm data set now uh this is where uh we Define The Chain of Thought prompts which are the input output examples which we are going to provide to the model there are three examples which I have considered here the first example is Emily has three apples her friend gives her two more how many apples does Emily have now and the answer we have given is Emily starts with three apples her friend gives her two more so 3 + 2 equal to 5 the answer is five similarly like this I have provided three examples then here I have uh written a function which can be used to evaluate the models is my model giving the correct answer on the GSM 8K data set or not and then finally I will plot as you can see here I have taken these three different models uh I always choose models which are open source and uh there is no gating restriction from hugging face so it becomes very easy to use them in Google collab uh so this is the plot uh which I got uh the accuracy of all these models as you can see is not very high it is in fact very very low uh but it does uh increase with the model size slightly now why am I getting accuracy which is bad which is not very high is it is it consistent with the results which are obtained in the paper let's have a look so uh let's see firstly let's try to see the actual results which are given by our model to get some understanding right so okay the first question is Janet's ducks lay 16x per day she eats three for breakfast every morning and Bs buffins for her friends every day with four she sells the remainder for $2 per EG how much dollar does she make and the correct answer is 18 but our model gives 12 similarly in example two the correct answer is three but our model gives 10 you can see all of these are very practical examples which require some sort of arithmetic reasoning and we have compared the actual answer with the answer which our model is giving in this output uh okay so why does our model perform low from this graphs you can see that we are somewhere uh in in this category which is the initial part of the x-axis and you can see from here that the accuracy is almost between 0 to 5% even up till 8 billion parameters it only increases above maybe 20 30 100 billion parameters so what we get is absolutely consistent with the results which we obtain from the paper also Chain of Thought reasoning is in fact incredibly effective for only larger models in our case we are using smaller scale models that's why the accuracy that we get from these models is not very high uh okay so let's try to see what will happen if I use bigger models instead right so now in the next part of the code I have uh used bigger models in which uh these are the three models I've used zire 7 billion parameter 52 which is 2.5 billion and Tiny Lama which is 1.1 billion and uh here are some examples uh which I get so you can see that correct answer 18 and our model is giving 300 so model does not give correct answer even for the higher scale models and this is the comparison of all six models on the few short Chain of Thought reasoning U which we have imp implemented so what did we learn Chain of Thought reasoning is only effective for very big models not for the models we are considering here this is in fact consistent with the results which we see from the paper as well the model starts to give reasoning steps if input output Chain of Thought examples are provided yes this was a very interesting observation as you can see in the model output our model is giving a series of steps that means that by it understands the initial nudge which we have provided over here this was the nudge that we provided to the model here and then it understands to reason that okay even if the answer is wrong but it still attempts to reason before it comes to the final answer I want all of you to uh go ahead and implement this notebook and uh try to see these differences and answers you are getting by yourself uh I have subscribed to Google collap pro as you can see here so it allows me to run on a100 but you can use a T4 GPU as well the second Hands-On project which we are going to look at is uh we are going to compare three different methods few short prompting uh that is without Chain of Thought this is going to be our Base Line and on top of that we are going to add few short Chain of Thought prompting and zero shot Chain of Thought prompting we are going to compare these three we are going to use 52 and Zi 7 billion parameter model for these comparisons and the data set that we are going to use uh are the logical and symbolic reasoning tasks related data set so let's have a look at this notebook um again this uh file will be shared with you so don't worry the first step is to install dependencies next we are going to load the model we are using Microsoft 52 model over here and uh here are the mixed reasoning questions logical plus symbolic which I have defined over here you can see that they are very different from the previous arithmetic examples which I had and then these are the few short Chain of Thought uh prompt and then just the few short prompt which is the Baseline where you directly see that only the answer is given here right you don't give the Chain of Thought like you do over here and then uh we go down and we run the models you can see that for zero shot we have only given let thing step by step which is what I mentioned before from the miror board as well and then this is the display that I get for these five questions so when you look at uh this table closely you can actually pause on this video and look at it you you see that only zero short Chain of Thought is more effective compared to the others we also evaluate the accuracy in a quantitative way where you see that zero shot accuracy is 20% whereas the other methods is almost 0% so uh I was wondering that why is few short Co and few short no coot giving such a low accuracy and very random answers right and uh I tried to see whether adding prompt more prompt examples made uh sense so we have given five examples before I tried to see if we had 10 and then I also tried with the different model with a higher model and try to see whether that makes a difference so it turned out that even in in that case uh the answers which you okay so these are the answers which you get for uh you know for giving 10 examples and even here zero short Z is very effective and even for the higher models which are used which is the zire 7 billion parameter model so zero short Co is effective compared to other methods that is my conclusion from this analysis although we are not looking at models more than 7 billion parameters for which might be all these three methods are very effective as you see from the graphs but for a from a point of view of academic study to understand the nuances the differences between these three approaches it is very important for you to implement this notebook and see how the model learns to reason you can see here that uh just by giving let's think step by step it actually learns to understand correctly for example a train travels 60 km for 3 hours how far does it go to find the distance we multiply speed by time so the train goes 60 into 3 which is 180 km the answer is 180 km okay so uh all of you just implement this uh second uh notebook as well the final notebook which I want to share with you is from a LinkedIn post I saw from Sebastian rashka where he implemented a Lama 3.2 completely from scratch what does from scratch mean from these in these other examples you can see that I'm uh you know importing the models from somewhere or the other uh so you can see here that okay so I'm I'm loading the model from somewhere right from hugging face but here uh the entire script is within my control I'm not loading the model from anywhere which is hosted on hugging face entirely so it's like an llm is built from scratch l 3.2 is built from scratch and I wanted to just experiment with it a little bit so you can see here uh if I give the input as uh okay so the input input is here what do llamas eat and the output is over here at the bottom Lamas are heror which means that and this only works well for 3 billion parameters I noticed I uh if you use 1 billion parameter model it doesn't work uh that well so there is a feeling of satisfaction when you implement this notebook when you feel that you have implemented the llm from scratch although the code was not written by you but our objective is then to apply uh zero shot prompting uh zero shot reasoning Chain of Thought reasoning over here and try to see how the model performs on Common Sense reasoning and simple arithmetic tasks so this is another uh interesting homework for all of you uh which will give you an understanding of uh implementing Chain of Thought prompts on not models which are hosted somewhere but by on models which are built from scratch and how well do they perform this is a very fertile ground for experiments understanding and building uh building intuition uh okay so with this uh let us conclude our lecture uh I hope this lecture was very interesting for all of you and uh in the next lecture we will continue to discuss about module one which is inference time comput scaling thank you very much everyone have a good day