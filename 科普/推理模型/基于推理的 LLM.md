
人类大脑推理分为两种类型：快速推理和慢速推理，也是我们常说的系统一推理和系统二推理。系统一帮助我们迅速思考并得出答案，它几乎占用了我们大脑 95% 的部分。它由两个因素驱动：我们的直觉，包括所有过去的经验等，以及我们的本能；系统二则是我们需要花时间思考某个主题的部分，它需要我们付出努力去思考。

OpenAI 于 2024.09.12 发布了首个推理模型 o1。
2025 年初，DeepSeek-R1 发布，1. 开源；2. 准确性与 o1 相当；3. 纯 RL 实现推理能力。


**方法一：推理时间计算缩放**：更多时间意味着能思考得更充分，或许能得出更好的答案。测试时计算是模型在推理过程中使用的计算资源数量。

一般来说，可以分为两大类：

* 第一类被称为提示（prompts），如 CoT 方法，2022，Google Brain
* 第二类是验证器（verifiers）。

**方法二：纯 RL 方法：** aha moment，模型学会了在没有明确提示的情况下进行自主推理。通过设定奖励函数，要求模型学习达成目标奖励的正确路径。

**方法三：监督微调和强化学习方法**。人们认为 OpenAI 的 o1 模型很可能是用这种方法制作的。这种方法也是 DeepSeek-R1 最终采用的。因此在文献中，这被称为构建推理模型的蓝图。

**方法四：纯监督微调与蒸馏**。




-----

大家好，欢迎来到基于推理的大型语言模型系列第二讲。如果您错过了第一讲，我强烈建议您先回看介绍视频，我们在其中详细介绍了本系列内容以及整个课程的结构安排。在今天的讲座中，我们将从第一种方法开始讲解，这种方法可用于赋予大型语言模型推理能力。请注意，我们的起始时间框架设定为2022年12月。

还记得GPT 3.5刚发布时吗？它的设计初衷是模仿人类应答方式，而非具备人类思维——它并非一个具备推理能力的大语言模型。而如今，我们已发展到拥有真正能进行逻辑推理的大语言模型阶段，这些模型能在给出最终答案前展示完整的思考步骤。OpenAI的OV模型是首个基于推理架构的模型，此后陆续涌现了多个具备推理能力的LLM（例如DeepSeek Carbon）。请注意，本讲座涉及的时间范围是从2022年起至今。

因为那正是人们开始认真思考基于推理的大型语言模型的时候，呃，如今很多公司以及几乎所有的LLM提供商都在他们的武器库中配备了具备推理能力的LLM。好了，现在我们要介绍第一种方法，即推理时间计算扩展。那么，让我们来理解什么是推理时间计算扩展。呃，我们都是人类，我们知道当我们花更多时间思考答案时，往往会给出更好的回答。还记得你在考试时写答案的情景吗？呃，有时候你会被一个问题卡住，因为它涉及多层次的思考。只有当你在这个问题上花费足够的时间后，你才能正确地回答这个问题。

这类需要更多思考的问题的其他例子包括复杂的数学问题，甚至是像PSE数独这样的谜题。我相信我们所有人都玩过报纸上出现的某种填字游戏，你不可能一下子回答所有的谜题，对吧？有一些横向的答案，有一些纵向的答案，其中很多是相互关联的。因此，除非你对横向和纵向列中的所有线索都有所了解，否则你将无法填满填字游戏中的每一个条目。另一个例子是一个数学问题，我们问的问题是：素数的数量是有限的还是无限的。

这个问题也不容易回答，很可能需要经过相当多的思考才能得出答案。让我们以国际象棋为例，象棋中每当棋手走一步棋时，这一步棋从来都不是立即走出的。你可能看过大师之间的对弈，有时他们需要10分钟、20分钟甚至更长时间才走出下一步棋。为什么？因为他们在走下一步棋之前，会思考所有可能的情况。因此，我们通常观察到，作为人类，当我们对一个问题思考得越多，就越容易得出正确的答案。

同样地，我们将同样的逻辑应用于大型语言模型，并提出一个问题：如果大型语言模型在给出答案之前多思考一会儿，会发生什么？在继续观看视频之前，请在这里暂停片刻，尝试回答这个问题。你认为如果让大型语言模型在给出答案之前多花点时间思考，它们会给出更好的答案吗？这是一个非常有趣的问题，我们将在本讲座中很快找到答案。让我们举一个非常简单的例子：罗杰有五个网球，他又买了两罐网球，每罐有三个网球。我们问的问题是：他一共有多少个网球？

现在，我们来看看一个常规的大型语言模型在这里给出的答案。你可以看到，当一个常规的大型语言模型试图回答这个问题时，它给出的答案是11，这实际上是正确答案，并且它只使用了三个标记。现在，我们将在模型给出答案之前，迫使它进行更多的思考。因此，基于推理的大型语言模型不是直接给出11这个答案，而是提出了一系列步骤。第一步是“罗杰一开始有五个球”，这一步使用了四个标记。第二步是“两罐各有三个网球，共六个网球”，这一步使用了11个标记。最后一步是“5 + 6 = 11”，这一步使用了五个标记。

因此，当我们要求模型进行更多思考时，它确实会在给出答案之前使用更多的令牌数量，这就是所谓的测试时间计算。模型在给出答案之前所使用的计算资源量被称为测试时间计算，这一点在上面的示例图中得到了说明。你可以清楚地看到，基于推理的大型语言模型（LLMs）的测试时间计算量显著高于常规语言模型（LMS），几乎是其三到八倍。但为什么我们要在测试时间花费如此多的计算资源呢？这有什么意义呢？因为在之前的例子中，即使准确性保持不变。

无论如何，两个大语言模型都给出了正确答案。但事实证明，如果在测试阶段投入更多计算资源，并要求模型进行更深入的思考，它也能对其他问题展开推理，模型的准确性会显著提升——这正是本课程后续内容要详细探讨的重点。我们主要研究的是：为什么在测试阶段分配更多计算资源能帮助模型提升回答复杂词汇推理问题的准确率？这种现象与人类行为高度相似——当人们对问题思考得更深入时，往往也能给出更好的答案。

事实证明，在大语言模型中也会发生完全相同的情况。在这张图中你可以看到，在基于推理的大语言模型中，推理过程会消耗大量计算资源（我会用笔在这里标注），而常规大语言模型的推理几乎是瞬间完成的——速度非常快。人们这样做是有原因的：因为我们观察到这样的扩展规律——随着测试时计算量的增加，模型的准确性通常也会提高。虽然这句话包含很多微妙之处，但我想让你理解为什么会出现"推理时计算量扩展"（inference time compute scaling）这个概念。

原因是随着你在测试时增加计算资源的数量，我们发现大语言模型的准确性通常会提高。随着课程的深入，我们会更细致地探讨这个观点。这听起来很直观对吧？给予更多思考时间通常能得到更好的答案。我们是什么时候开始意识到这点的呢？最早的一篇论文表明：如果你给大语言模型提供的不只是简单的输入-输出提示，而是构建一条连接输入与输出的思维链（Chain of Thought），那么大语言模型就能学会推理。

对于用户示例提示中未提供的所有其他提示，请参考他们在论文中的论述。我们探讨了如何通过生成“思维链”——一系列中间推理步骤——显著提升大型语言模型（LLMs）执行复杂推理的能力。这意味着什么？我们正在评估大型语言模型执行复杂推理任务的能力，结果发现，如果要求LLM在给出最终答案前先生成一系列思考步骤，其在复杂推理任务上的准确性也会提高。具体而言，我们展示了推理能力如何通过一种名为“思维链提示”的简单方法，在足够大的语言模型中自然涌现。

这是我们即将学习的第一种技术，称为"思维链推理"。这篇论文发表于2022年，由谷歌研究院的Brain团队发布。该研究主要揭示了通过两个核心理念可以解锁大语言模型的推理能力：第一个理念是，算术推理可以通过生成引导至最终答案的自然语言推导过程而受益——我知道这些术语听起来有点复杂，但所谓"自然语言推导"其实就是指从问题到答案的一系列步骤；第二个理念是，我们只需通过简单的提示就能让大语言模型实现这种推理过程。

所以我们不改变模型的权重，也不改变模型架构，大语言模型（LLM）保持不变，其核心处理能力也保持不变。我们只是通过一系列输入输出示例，用思维链（Chain of Thought）的方式提示大语言模型。通过这种提示，大语言模型将学会以类似的方式回答其他复杂的推理任务问题，这些问题与我们最初提供的输入输出示例不同。让我们试着理解我的意思：与其对模型进行微调，不如直接向模型提供一些展示任务的输入输出示例。请记住，当你实际提出一个复杂的谜题时...

比如说，我在让大语言模型解数独时，并没有实时提供输入输出示例——这些示例是预先一次性提供给大语言模型的。假设你给大语言模型提供了20组输入输出示例对，仅此而已。之后每当我们提出一个新问题，比如数独题时，大语言模型就会知道：哦，我应该按照之前输入输出示例的方式来回答。如果在那些示例中包含了思维过程，那么我也应该以类似的方式给出最终的数独答案。稍后我们也可以通过实际案例来更好地理解这一点。

那么什么是思维链提示呢？让我们看看这两者的区别。这里有一个输入输出对的例子，这是输入，这是输出。输入部分与我们之前问的问题相同：罗杰有五个网球，他又买了两罐，每罐有三个网球。在输出部分，我们不会直接写答案是11，而是会写出得出答案所需的所有步骤。例如，第一步是罗杰一开始有五个球；第二步是两罐网球，每罐三个，共六个球；第三步是5加6等于11；第四步是答案是11。

因此，我明确列出了四个步骤，在输出中这被称为“思维链提示”。让我们再看一下这个例子，你会发现这里有四个相互关联的思考点，正如我在图中展示的那样：思考点一、思考点二、思考点三和思考点四。这四个思考点相互连接，因此被称为思维链。接下来你会发现，一旦给出思维链，模型对其他输入也会以类似方式作出回应。例如，假设你在某个输入输出示例中给出了这样的提示。

现在，如果你问一个新问题，比如说食堂有23个苹果，用掉了20个来做午餐，然后又买了6个，那么他们现在有多少个苹果？这个模型学会了以类似的方式对新输入的示例进行思考。例如，你可以在这里看到模型的输出是：食堂最初有23个苹果，他们用掉了20个来做午餐，所以剩下23 - 20 = 3个。他们又买了6个苹果，所以现在有3 + 6 = 9个。答案是9。而在标准提示下，模型给出的答案是27，这是错误的。好吧，现在看来，这篇论文已经有超过12,000次的引用。到目前为止，我们已经理解了这一点。

那么这篇论文告诉我们的是，它向我们展示了在输入输出提示中引入思维链（Chain of Thought）有助于模型在复杂推理任务中更好地进行推理，对吧？但为什么会有这么多引用呢？12,000次引用可是一个巨大的数字啊。我们真的理解这篇论文的重要性了吗？那么，让我们来看看它的贡献吧。第一个贡献是：足够大的语言模型，如果在小样本提示（few-shot prompting）的例子中提供了思维链推理的示范，那么这些足够大的模型就能够生成思维链。那么这句话是什么意思呢？这是一个思维链提示示范的例子。

因此，如果模型获得了这些演示示例（可能是10个或20个演示），那么模型实际上可以针对新的问题、新的推理问题生成思维链。再次尝试理解这句话：足够大的模型如果获得了思维链推理的演示（即在少样本提示的示例中提供了输入输出示例），就可以生成思维链。这里有一个我之前没有介绍过的新术语——少样本提示（few shot prompting）。少样本提示是指在提示中提供一定数量的输入输出示例的做法。例如，这就是一个输入输出的示例。

这是一个类似的例子，你可以在输入输出提示中提供10个以上的例子。这种在输入输出提示中提供多个例子的方法，是为了引导或推动LLM（大型语言模型）理解我们期望的答案，或者让LLM明白我们期望的输出格式。这就是所谓的“少量提示”（few-shot prompting）。为什么叫“少量”？因为我们提供了一定数量的输入输出例子。好的，那么你可以注意到，这个陈述中还提到了“足够大的模型”，这是什么意思呢？我所说的“足够大的模型”具体指什么？我稍后会解释这一点。

这篇论文评估了该技术在三种不同推理任务上的表现：一是算术推理任务，二是常识推理任务，三是符号推理任务。到本讲座结束时，你们应该能够清楚地区分算术、常识和符号推理任务之间的差异。我们需要理解这三者之间的区别，明白吗？其实小样本提示（few-shot prompting）在2020年那篇论文发表前就已经存在了，人们早就在提示中提供输入输出示例了。那篇论文正式提出了小样本提示的概念。

那么我们来看看那篇关于少样本提示的基础论文吧。这篇就是我要说的论文，它发表于2020年。论文中作者们介绍了几种少样本提示技术，但并没有提及思维链（Chain of Thought）的概念。也就是说，他们提供的所有输入输出示例都是"问题-直接答案"的形式，没有包含推导出答案的思维过程——而这正是《思维链推理》论文的主要贡献所在。现在，让我们来看看可以在提示中使用的几种思维链推理示例。

呃，我已经多次谈到在提示步骤中给大型语言模型提供不同类型的例子，可能需要10到20个带有思维链的例子。那么让我们看看其他一些例子。第一个例子我们已经看过了，现在来看一个常识类别的例子。Sammy想去人多的地方，他可能会去哪里？选项有：赛车场、人口稠密地区、沙漠、公寓、路障。我们给出的提示答案是：答案必须是一个人很多的地方。赛车场、沙漠、公寓和路障通常人不多，但人口稠密地区人多。

所以答案是B，这里你可以看到我们在提示中提到的“思维链”——在给出最终答案之前，我们并没有直接在这里给出最终答案。嗯，让我们再看另一个符号推理类别的例子：一枚硬币正面朝上，梅布尔翻转了硬币，莎伦达没有翻转硬币，硬币仍然正面朝上吗？硬币被梅布尔翻转了一次，这是一个奇数次数。硬币最初是正面朝上，所以经过奇数次翻转后会是反面朝上。所以答案是否。在这里，我们同样被问到一个问题，然后我们知道答案，但我们不只是给出答案，我们还给出了引导大语言模型得出答案的“思维链”。

因此，这些是提示中给出的不同输入输出示例的数量，通过这些示例，LLM可以理解：“哦，好吧，也许用户希望我不要直接给出答案，而是在给出答案之前先思考步骤。”这种小小的提示实际上极大地提高了LLM在复杂推理任务中的准确性，这就是为什么这篇论文有超过12,000次引用。通常看起来简单的概念并不容易构思，而这些概念具有最广泛的实用性。我认为“思维链推理”就是这样一个概念，它实用性巨大，又非常易于理解。

因为我们大家都以思维链的方式进行推理，所以我们都能非常直观地理解这种技术。好吧，虽然这是个非常简单的想法，但让我们看看他们得到的结果。每当你阅读任何大型语言模型的论文时，都会看到一系列结果，我希望你们也能学会如何正确解读这些结果。这是他们在论文中得到的一张图表。我们首先要问的是，他们在绘制这张图表时使用的是哪种大型语言模型？这个大型语言模型叫做Palm（如果你还不了解Palm模型的话）。Palm是由谷歌发布的大型语言模型。

呃，这个模型有不同的规模，最大规模是5400亿参数。他们使用了一个PAL模型。那么这项研究告诉我们什么呢？这项研究表明，在X轴上随着参数数量的增加（X轴代表参数数量，Y轴代表准确率），当你看到准确率指标时，应该始终问一个问题：这个准确率是在哪个数据集上计算的？这里的准确率是在GSM8K数据集上计算的。GSM8K是一个算术推理数据集，包含一系列算术推理问题。在本课程结束后，我们将研究一个关于GSM8K数据集的项目。

我们将了解这个数据集中通常提出的不同问题。问题通常是这样的：比如说，娜塔莉亚在四月份向她的48个朋友出售了夹子，然后在五月份她出售了四月份数量的一半。那么娜塔莉亚在四月和五月总共出售了多少个夹子？我们知道答案是48加24等于72。所以，这个PAL模型被提供了来自GSM8K数据集的一系列问题，然后在该数据集上评估了准确性。准确性是如何计算的？这个大型语言模型能够正确回答多少个问题？

现在，我们先来看看标准提示法。可以看到，标准提示法的准确率确实很低，但一旦引入思维链提示法，准确率就会显著提高。标准提示法和思维链提示法之间存在很大差距。这是第一个观察结果。你还能从这张图中看出其他什么吗？我观察到的第二点是，思维链提示法的准确率实际上随着模型规模的增大而提高，这意味着越大的模型在推理方面表现越好，准确率也越高。好的，这没问题。你还能从这张图中看出其他什么吗？

从这张图表中我还能看出一点，嗯...如果你观察那些非常大的Palm模型，它们几乎能与最好的监督微调模型相媲美。这意味着，我们无需经历计算成本高昂的监督微调过程，仅通过使用思维链提示（Chain of Thought prompting）这种提示技术，就能在算术推理数据集上达到相同的准确率。好的，那么我们总结出三个关键发现：首先，思维链提示法相比标准提示法具有显著优势；其次，模型规模越大，准确率就越高。

第三点是，对于非常大的Palm模型，其准确率几乎可以与有监督微调相媲美。嗯，如果你不熟悉有监督微调，这个过程是这样的：假设你有一个预训练的大型语言模型，然后你还有一个额外的输入输出对数据集，你再次使用这个数据集来训练模型，这个步骤就称为微调。通常这个过程计算成本很高，但它能显著提升大语言模型的性能。现在我们看到的是，我们达到了与有监督微调相似的准确率，而无需经历计算成本高昂的微调过程。嗯，好的。

所以，正如我们提到的，这里有三个观察结果：首先，对于小型模型来说，准确性较低，而对于更大的模型，其准确性与有监督的微调相当。第二个观察结果是，在相关的算术数据集上，思维链提示（Chain of Thought prompting）的表现优于标准提示。最后一个非常有趣的观察是，思维链推理是随着模型规模增大而涌现的一种能力——这一点在论文中被多次提及，我想强调这个观点，因为它非常重要。我认为这句话的意思是，随着模型规模的增加……

当你试图引导模型进行“思维链”推理时，模型能更好地理解这种引导，并给出符合预期的答案——这是小模型根本无法捕捉的。我认为这就像面对一个大脑发育有限的人（比如智力水平较低、理解能力薄弱的人），即使你努力引导他们进行逻辑推理，他们也永远无法掌握要领。而拥有发达大脑的人则截然不同，他们天生具备轻松理解复杂概念的能力。

现在这样一个人只需要一个小小的提示就能理解复杂推理任务的预期结果，而这正是大型语言模型正在发生的情况，这一点非常有趣。在这篇论文中，他们还分析了为什么模型规模的扩大会改善思维链提示的效果。你可以清楚地看到，这里他们分析了三种不同类型的问题：语义理解、单步缺失以及第三种问题类型。我们可以观察到，当模型参数从620亿扩展到5400亿时，这些错误会被模型自动修正。

因此，模型开始修正较小模型最初犯的错误，这是因为模型理解了在提示阶段给出的引导，所以它能够以比小模型更好的方式回答这些问题。这是我希望大家记住的一个结果：与标准提示相比，思维链推理极其有效，我们已经理解了这一点，但它对非常小的模型效果不佳，只对大型模型有效，因为在大型模型中推理是一种涌现能力，模型能更好地理解引导。

好的，那么这里有一个例子，展示了较大模型如何比小模型更准确地回答问题。我们提出的问题是：Tracy正在使用一根4英尺长的电线，并将其剪成6英寸长的小段。那么她最终得到了多少段呢？我们知道1英尺等于12英寸，所以总共有4乘以12等于48英寸。然后，段数就是48除以6等于8段。较大模型正确地给出了这个答案，但较小的模型却搞混了，它直接将4乘以6，得出28段的错误答案。这就是一个语义理解上的错误，而在较大模型中得到了修正。

这是众多示例中的一个，这些示例在前图中已经突出显示。好了，现在我希望大家都已经理解了思维链推理在算术推理任务中是如何高效运作的。但对于常识推理和符号推理任务呢？它是否同样有效，或者效果不佳？让我们首先来理解常识推理。我之前也举过一个例子，如果你看一下，我们曾看过一个常识推理的例子。另一个例子是：钢球会在水中下沉吗？所以，即使对于常识推理，从这些图中也可以看出。

你可以看到，思维链提示法总是优于标准提示法，尽管在CSQA数据集上差异不大，但在其他所有案例中，尤其是体育类问题时，差距非常显著。这里它甚至超越了人类水平表现。接着我们来看符号推理——这是个有趣的术语，因为它被用来定义这类问题：一枚硬币正面朝上，AE翻转了硬币，DTI没有翻转硬币，那么硬币仍然正面朝上吗？我们知道正确答案是否定的，因为此时硬币应该是反面朝上。但事实证明，大语言模型在回答这类符号推理问题时表现不佳。

它们会犯错，呃，这就是为什么我们必须理解大型语言模型在符号推理任务上的表现如何。呃，在这里你还可以看到，思维链提示法明显优于标准提示法。这项研究的主要结果是，思维链提示法在算术、常识和符号推理任务中表现非常出色。那么现在，我们要继续前进，看看第二种技术——零样本推理。在继续之前，我希望大家都能退一步，暂停一下，回顾一下我们到目前为止所看的内容，同时也看看我们引用的相关论文。

在这项研究中，请确保你理解了所有概念，然后我们将进入零样本推理环节，最后我们会进行一些实践项目。在这些实践项目中，我们会探讨讲座中提到的所有细节，这将让你对推理类型计算缩放的实际应用有更深入的理解。好，现在我们要来看第二种技术——零样本推理，这种方法甚至比思维链推理更简单。让我们来看一个例子：“Era，去你的房间学习。”

你必须写出九的乘法表，确保一步一步地思考。这个简单的提示能让一个人明白，在得出结论之前应该逐步思考。零样本推理论文的作者们使用的正是同样的逻辑。举个例子，首先是普通的零样本。零样本的含义是，如果你还记得我之前解释过的少样本提示，即在提示阶段提供一系列输入输出示例，那么零样本意味着我们在提示阶段完全不提供任何示例，也就是提供零个示例。所以这是简单的零样本，其中有一个问题……

然后这里没有提供示例，如你所见，答案这里是空白的，然后呃，这里的输出是错误的，因为应该是呃16除以2再除以2等于4。现在这篇论文中他们引入了零样本思维链，意思是我们仍然没有输入输出对的示例，但取而代之的是我们只写上“让我们一步步思考”，这非常类似于呃我之前提到的era的例子。所以这里发生的是问题保持不变，但我们不在提示中提供呃答案，而是只添加一句话“让我们一步步思考”，然后有趣的是，模型学会了推理。

那么这与我们之前看到的思维链推理示例中的普通思维链或简短思维链有何不同呢？我们之前提供了输入输出示例，告诉模型如果这是输入，你应该推理然后提供输出。而现在我们说的是，好吧，我们懒得做所有这些，我只想添加一句话，说“让我们一步步思考”，然后模型就学会自己推理了，仅此而已。那么为什么名字里有“零”呢？这是因为我们在提示中没有给出任何输入输出示例。论文中提到的是，呃...

在这个例子中，他们两次提示大语言模型。第一次提示是要求“让我们一步步思考”，这是我们之前看过的，然后从大语言模型得到一个答案。接着，这个答案被传回大语言模型，同时给出另一个提示：“因此，答案是”。那么，这里的最终答案会是什么呢？最终答案将是第一个由大语言模型提供的答案，然后经过“因此，答案是”这一步后，得到的答案是375。所以他们分两步提取答案，并且取得了非常好的结果。从图表中可以看出，零样本思维链显著优于零样本推理，现在你也可以读懂这些图表了。

这与我们在“思维链”示例中的图表非常相似。这里使用的模型是Palm模型，数据集是算术推理数据集GSM 8K。同样可以看到，对于5400亿参数的大模型而言，零样本思维链的表现显著优于零样本。更大规模的模型展现出更强的推理能力——这让你联想到什么了吗？这与我们在思维链推理案例中的观察也非常相似。好的，今天我们讨论了两项技术：一是少量样本思维链，二是简单思维链推理。

其次，我们讨论了零短思维链，这意味着我们不会提供任何输入输出示例，而是只告诉大型语言模型逐步处理事情。在本讲座的最后部分，我们将看两个实践项目。这些项目是我专门为所有可能有疑问的学生设计的，他们可能会问：我究竟在哪里具体指定这些输入输出示例提示？当我阅读原始的思维链提示论文时，我也有同样的问题，我困惑于这些输入输出示例应该写在哪里，它们实际上意味着什么。

因此，在本课程中，我们研究项目时都会聚焦具体的命题，这些命题能帮助我们更深入地理解某些问题。例如在项目一中，我们将探究：能否自行评估模型规模对思维链推理的影响？也就是说，我们要研究模型规模如何影响思维链推理效果。为此我们会选用小型模型——因为评估大模型需要耗费大量时间。具体选用的模型包括：Flan-T5 Small（小规模）、Flan-T5 Base（基础版）、Flan-T5 Large（大规模）以及TinyLlama 52。

正如你所看到的，Zire 7B模型的参数量从8000万到5000万不等，最高可达70亿参数。我们使用的数据集是GSM 8K，这是一个算术推理数据集。在我们开始动手演示之前，我希望大家暂停视频，做一些预测：你认为这些模型在这个数据集上表现会如何？如果我们使用思维链推理，它们会给出什么样的答案？是正确的还是错误的？现在让我们直接进入项目。好的，我们要看的第一个项目是这个，即模块一：推理时间计算——模型大小对准确性的影响。

通过思维链推理，我将在聊天中与您分享所有这些Collab文件，这样您就可以在我讲解时通过互动这些笔记本来跟进。第一步是安装所需的依赖项，包括Transformers、datasets和accelerate。第二步，我们将在这里加载GSM 8K数据集，并从中提取了50个示例，如您所见。呃，您现在可以暂时忽略这个Swarm数据集，这是一个探索阶段，我同时研究了包括GSM 8K和Swarm在内的多个数据集。现在，呃，这里是我们定义思维链提示的地方，也就是输入输出示例。

我们将提供给模型的示例有三个，这里我列举了第一个例子：Emily有三个苹果，她的朋友又给了她两个，现在Emily有多少个苹果？我们给出的答案是：Emily一开始有三个苹果，朋友又给了她两个，所以3加2等于5，答案是五个。类似这样，我提供了三个例子。然后在这里，我编写了一个函数，用于评估模型在GSM8K数据集上的答案是否正确。最后，如图所示，我选取了这三个不同的开源模型进行对比分析。

而且，Hugging Face 没有任何门槛限制，所以在 Google Colab 中使用它们变得非常容易。呃，这是我得到的图表，你可以看到所有这些模型的准确率都不是很高，实际上非常非常低。呃，但确实随着模型大小的增加，准确率略有提高。那么，为什么我得到的准确率这么差，不是很高呢？这与论文中得到的结果一致吗？让我们来看看。呃，首先，让我们试着看看我们的模型给出的实际结果，以便更好地理解。好的，第一个问题是：珍妮特的鸭子每天下16个蛋，她每天早上吃三个当早餐。

她每天为朋友们制作Bs buffins，四个自己留下，其余的以每个2美元的价格出售。她赚了多少钱？正确答案是18美元，但我们的模型给出的答案是12美元。同样，在第二个例子中，正确答案是3美元，但我们的模型给出的答案是10美元。你可以看到这些都是非常实际的例子，需要某种算术推理。我们将实际答案与模型在此输出中给出的答案进行了比较。嗯，好吧，为什么我们的模型表现不佳？从这些图表中你可以看到，我们处于x轴的初始部分这一类别中，从这里你可以看到，即使参数高达80亿，准确率也几乎在0到5%之间。

只有当参数规模增加到大约200亿、300亿甚至1000亿时，性能才会显著提升。因此，我们的实验结果与论文结论完全一致。此外，"思维链"推理方法实际上仅对大规模模型效果显著——而我们当前使用的是小规模模型，这直接导致了模型输出准确率偏低。好的，接下来我们尝试切换到大模型会如何影响结果。在代码的下一部分，我已改用以下三个大模型：Zire（70亿参数）、52（25亿参数）和Tiny Lama（11亿参数）。这里展示部分输出示例，可以看到正确答案18的出现情况。

而我们的模型给出的数值是300，这意味着即使是更高规模的模型也无法给出正确答案。这是我们在少量短链思维推理U上对所有六个模型进行的比较，我们已经实施了这些模型。那么我们学到了什么呢？思维链推理只对非常大的模型有效，而不适用于我们这里考虑的模型。事实上，这与我们从论文中看到的结果也是一致的。如果提供了输入输出的思维链示例，模型开始给出推理步骤。是的，这是一个非常有趣的观察，正如你在模型输出中看到的，我们的模型给出了一系列步骤，这意味着它理解了最初的提示。

我们在这里提供的这个提示就是我们给模型的推动，然后它会理解并推理，即使答案错了，它仍然会在得出最终答案之前尝试推理。我希望你们都能继续实施这个笔记本，并亲自尝试看看这些差异和得到的答案。正如你们在这里看到的，我已经订阅了Google Collab Pro，所以它允许我在A100上运行，但你们也可以使用T4 GPU。我们要看的第二个实践项目是比较三种不同的方法：少量提示（即没有思维链），这将作为我们的基线。

除此之外，我们还将加入少量短链思维提示（Chain of Thought prompting）和零样本链式思维提示（zero shot Chain of Thought prompting），并对这三种方法进行比较。我们将使用52亿和70亿参数的模型进行这些对比实验，而数据集则会选用与逻辑及符号推理任务相关的数据集。那么，让我们来看看这个笔记本吧——再次说明，这个文件会分享给大家，所以不用担心。第一步是安装依赖项，接下来我们将加载模型。这里我们使用的是微软的52亿参数模型。这里还有一些我定义的混合推理问题（逻辑加符号类），你可以发现它们与之前的算术例题截然不同。

我曾有过这些简短的思维链提示，然后就是一些简单的基准提示，在这里你直接看到只给出了答案，没有像这边这样给出思维链。接着我们往下运行模型，可以看到对于零样本，我们只给出了逐步的提示，这也是我之前提到的来自miror板的内容。然后这是我针对这五个问题得到的显示结果。当你仔细观察这个表格时，可以暂停视频仔细看，你会发现只有零样本思维链比其他方法更有效。

我们还以量化的方式评估了准确性，可以看到零样本准确率为20%，而其他方法的准确率几乎为0%。所以我在想，为什么少样本Co和少样本no coot给出的准确率如此之低，而且答案非常随机。然后我尝试看看增加提示示例是否有意义，所以我们之前给出了五个示例，我尝试看看如果我们有10个示例会怎样。我还尝试了不同的模型，一个更高级的模型，看看是否会有所不同。结果发现，即使在这种情况下，得到的答案——好吧，这些就是你给出10个示例后得到的答案。

即便在这里，零样本Z方法也非常有效，甚至对于更高阶的模型（如70亿参数的Zire模型）也是如此。因此，与其他方法相比，零样本Co方法更为有效——这是我从分析中得出的结论。尽管我们没有研究超过70亿参数的模型（对于这些模型，这三种方法可能都非常有效，正如你在图表中所见），但从学术研究的角度来看，为了理解这三种方法之间的细微差别，实现这个笔记本并观察模型如何学习推理是非常重要的。你可以看到，仅仅通过给出"让我们一步一步思考"的提示，模型实际上就学会了正确理解。

例如，一列火车以60公里的时速行驶3小时，它行驶了多远？要计算距离，我们用速度乘以时间，所以火车行驶的距离是60乘以3，等于180公里。答案是180公里。好了，所以大家只要把这个方法应用到第二个笔记本上。我想和大家分享的最后一个笔记本来自Sebastian Rashka在LinkedIn上的一篇帖子，他在那里从头开始实现了Lama 3.2。从头开始是什么意思呢？在这些其他例子中，你可以看到我在从某个地方导入模型，或者从其他地方导入。你可以看到这里，我正在从某个地方加载模型，比如从Hugging Face。

但在这里，整个脚本都在我的掌控之中。我没有从任何地方加载模型，比如托管在Hugging Face上的模型，完全是从头开始构建的。L3.2也是从头开始构建的，我只是想稍微实验一下。你可以在这里看到，如果我输入“羊驼吃什么”，输出就在底部：“羊驼是食草动物”。这只有在30亿参数的情况下效果不错。我注意到，如果你使用10亿参数的模型，效果就没那么好。当你实现这个笔记本时，会有一种满足感，感觉你从头开始实现了一个LLM。

虽然这段代码并非由你们编写，但我们的目标是通过零样本提示（zero-shot prompting）和零样本思维链推理（Chain of Thought reasoning）来测试模型在常识推理和简单算术任务上的表现。这对大家来说是另一项有趣的作业，能帮助你们理解如何对从零构建的模型（而非托管模型）实施思维链提示，并评估它们的表现效果。这个实验领域充满探索价值，能培养你们的认知直觉。好的，本次课程到此结束，希望各位觉得内容有趣。下节课我们将继续讨论模块一——推理时间计算扩展。谢谢大家，祝你们有愉快的一天！
