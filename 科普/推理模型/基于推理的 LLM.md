
人类大脑推理分为两种类型：快速推理和慢速推理，也是我们常说的系统一推理和系统二推理。系统一帮助我们迅速思考并得出答案，它几乎占用了我们大脑 95% 的部分。它由两个因素驱动：我们的直觉，包括所有过去的经验等，以及我们的本能；系统二则是我们需要花时间思考某个主题的部分，它需要我们付出努力去思考。

OpenAI 于 2024.09.12 发布了首个推理模型 o1。
2025 年初，DeepSeek-R1 发布，1. 开源；2. 准确性与 o1 相当；3. 纯 RL 实现推理能力。


**方法一：推理时间计算缩放**：更多时间意味着能思考得更充分，或许能得出更好的答案。测试时计算是模型在推理过程中使用的计算资源数量。

一般来说，可以分为两大类：

* 第一类被称为提示（prompts），如 CoT 方法，2022，Google Brain
* 第二类是验证器（verifiers）。

**方法二：纯 RL 方法：** aha moment，模型学会了在没有明确提示的情况下进行自主推理。通过设定奖励函数，要求模型学习达成目标奖励的正确路径。

**方法三：监督微调和强化学习方法**。人们认为 OpenAI 的 o1 模型很可能是用这种方法制作的。这种方法也是 DeepSeek-R1 最终采用的。因此在文献中，这被称为构建推理模型的蓝图。

**方法四：纯监督微调与蒸馏**。




-----

#### 1、推理时间计算扩展

代表：CoT

模型在给出答案之前所使用的计算资源量被称为测试时间计算，基于推理的 LLMs 的测试时间计算量显著高于常规语言模型，几乎是其 3-8 倍。

截止 25.12 该论文超过 2.3W 次的引用。核心贡献是：足够大的语言模型，如果在 few-shot 中提供了思维链推理的示范，那么这些足够大的模型就能够生成思维链。思维链推理是随着模型规模增大而涌现的一种能力。

#### 2、Zero-shot 推理

make sure to think step by step，让我们一步步思考

-------

#### 3、基于验证器的搜索

验证指的是生成不同答案，然后从所有生成的答案中最终选出最佳答案的过程。例如，并行提供四个不同的答案，中间有一个验证层，它会验证这四个答案中哪个是最好的，最后给出最佳答案。






Well remember that if we have a verification layer we talked about test time compute in the first and second lecture when you have a verification layer it's going to increase the computational resources which the LLM uses during inference. That is why it comes under inference time compute scaling because the inference time is going to increase.Let's look at it in the form of a diagram whenever you are using an LLM there is a pre-training then there is fine tuning and then there is inference. So without verification inference is like this but when you increase verification you are basically increasing the time which the LLM takes to give you the answer because you are sampling from different answers and you are selecting the best answer. That's why it increases the inference time compute.Now this verification layer that we looked at it can either be done by humans or it can be done by a model. The models who do this verification are called as reward models. Why don't you go to hugging face and type this reward model it's called reward model Deberta V3 large V2.What it says is that this reward model is trained to predict which generated answer is better judged by a human given a question. So it's used for model evaluation reward score in RLHF and detect potential toxic responses via ranking. So this model is trained such that if you give an input to this model it will give you a score based on how good the generated answer is.So the training data that this model has been trained on was generated by humans itself and then that data is being used to train the model. So we are essentially mimicking the ability of humans to verify answers by replacing humans with a model. So these are called as reward models because the best answer is rewarded with the maximum score.If the answer is not good then the score will be less and the reward will be less. In the next section we are going to look at reward models in detail. So more specifically there are two types of reward models.There are outcome reward models and there are process reward models and the purpose of both these type of models are pretty much explained in their names. So let's look at the first category which is the outcome reward model. So imagine that you have a question and you pass this question to a reasoning model.So the reasoning model is going to give you a chain of thought. You know it's going to give you the thought process through which it thinks before it gives you the answer. Now what the outcome reward models do is that they don't evaluate these thoughts.They don't care about the thoughts basically. They only care about the final answer and only the final outcome is scored. So here ORM represents the outcome reward model and 0.45 is the score which the ORM gives to the final answer.The next category which are more popular are process reward models and these make more sense because when you are using a reasoning model the thought process is very important for you. So there needs to be a mechanism which judges not just the final answer but also the individual thoughts which are leading to the final answer. So here what you see is that thought 1 is sent to the process reward model and then the model is scoring the thought.Thought 2 is again sent to the process reward model and the model is again scoring the thought. So here the main difference compared to the ORM is that individual reasoning steps are scored and the final answer is not scored. So you are assuming that if the reasoning steps are correct obviously the final answer will also be correct.So you are giving a lot of emphasis on the chain of thoughts which are leading to the final answer. That is why it is called as a process reward model. So now let us look at a practical example of a process reward model and how does a process reward model work in practice.So let us take a simple example let us say the example which we are considering is Roger has 5 tennis balls and he buys 2 more cans of tennis balls each can has 3 tennis balls how many tennis balls does he have now. So like we saw in the last lecture we are going to use a chain of thought reasoning to come up with the final answer. So we have 3 steps.Step 1 is the reasoning LLM says Roger started with 5 balls. Step 2 is 2 cans of 3 tennis balls each is 5 tennis balls and step 3 is so the total number of tennis balls is 5 plus 6 which is 11. Now read these and just try to understand which one is correct and which one is not correct.The first one is correct Roger starts with 5 tennis balls so you get a higher score. The second one 2 cans of 3 tennis balls should be totally 6 tennis balls not 5. So this one gets a lower score see and then so the total number of tennis balls the final answer is correct. So this one gets a higher score.So wrong reasoning gets a lower reward. The main advantage of using verifiers is basically there is no need to fine-tune or retrain the LLM which you are using to answer the question. So whenever we are working with reasoning models whichever reasoning models you see in practice whether it's O1 deep seek etc.

it is likely that these reasoning models before they give you an answer a verification layer has been added where there is a process reward model that is evaluating the answers and it is giving you the answer which has the best possible reasoning path. Okay so now let's move ahead and we will look at the different types of verifiers broadly speaking we will look at three different types of verifiers we will look at majority voting as the first type of verifier then we will look at best of n and then finally we will look at beam search. All of these are very intuitive and simple to understand but we are going to have a look at these three types in detail so that you understand the intuition behind them.Okay so the first one let's look at it the first type of verifier is called as majority voting and before we go ahead I want all of you to download this paper which I have enjoyed thoroughly there might be some aspects of this paper which you do not understand but I hope this paper will if you have gone through the first three lectures I hope you will understand some aspects of this paper and really appreciate this paper because this paper specifically discusses about test time compute so you will be able to understand the theory which has been outlined in this paper. Okay so after you have downloaded this paper we will start to now discuss about the three main types of verifiers the first verifier is called as majority voting and this is clearly the most intuitive in majority voting what happens is that there is there is not even a verifier used actually what happens is let's say you probe an LLM to generate samples of answers let's say there are 10 answers generated by the LLM and you pick the answer which appears maximum number of times that's all so something similar to this let's say you ask the question are the number of prime numbers finite and then you give it to the reasoning model and the reasoning model generates six chains of thoughts which you are not concerned about at all you just look at the final answer okay so you can see that there are two yeses but there are four noes so what's the majority the majority is no so we will choose the majority in this case and the final answer we will pick as no so this method is also called as self consistency but this is probably the first thing which came to the mind whenever people when people thought of a verifier is that let's sample a lot of answers and pick the answer which comes the maximum number of times okay so this is the first answer and now here mind you the main drawback of this method is that the the answer can be wrong because you are not using any any verifier to evaluate the answer so there is no way to know whether the answer is correct or not but we are simply taking the majority and that concern is actually addressed in the next method which is called as best of n samples so here again what happens is that there are n samples which are generated by the LLM but the difference from the previous method is that you apply a verification layer here you don't directly choose the majority but you apply a verification layer and then you choose the answer which the verifier is giving the maximum score to and this can either be done by a output reward model or a process reward model so let us look at both these examples now at the bottom. So the question that we have asked is how many moons does Jupiter have? It is passed through a reasoning model and then we have generated six chains of thoughts T1, T2, T3, T4, T5, T6 so as we are looking at ORM here we are not concerned with the chains of thoughts at all we are only concerned with the final answer these final answers which are 90, 1, 95, 82, 76 and 14 they are passed through the output reward models and then this verifier scores all these answers 0.6, 0.2, 0.9, 0.7, 0.3, 0.5 and then you pick the answer with the best score so the number of moons in Jupiter which Jupiter has is 95 by using the ORM this is the answer we get.Now when you look at PRM, in PRM what we do is we do not score the final answer like we do in the previous step but let us imagine that this is a question and we pass this question through to the reasoning model and here we are evaluating three chains of thoughts which are T1, T2 and T3 ok so T1 goes down to T11, T2 goes down to T21 and T3 goes down to T31 both these reasoning steps are passed through the PRM and we are getting scores 0.1 and 0.2 and we are doing a weighted average so it is 0.15 here again we are doing an average which is 0.6 and here we are doing an average which is 0.55 and now we are not even bothered about the final answer which the reasoning model is giving because the verification layer only verifies the reasoning steps which are appearing before the final answer so from these three we can see that this answer has the maximum score so that's why we will pick this answer so the answer with the best average is finally selected as the final answer ok so this is using PRM for best of n sampling and then you can also imagine an example for this which looks very similar to the above example but the primary motive behind showing you both these methods is to understand the differences between the two and you can see in this paper also which I have shared before in this paper they have predominantly focused on process reward models and these models are more effective and they are more frequently used in practice by researchers ok now we will go to the next algorithm or next verifier in this case which is using the technique of or we can call it verification using the algorithm of beam search. So the beam search algorithm was originally proposed in the year 1976 and it was proposed in this paper which is shown below the title of the paper is the Harpy speech recognition system and this paper actually came out in the year 1976 so this algorithm was first used in the speech recognition domain and here you can see that the search strategy the name of the search strategy is a few best paths in parallel. So this is quite an ancient algorithm we can say it is not an algorithm which is developed by LLM researchers or AI researchers it was developed way back around 50 years back we can say where the name was not even given as beam search but the algorithm was very similar to the modern algorithm and it was used for speech recognition tasks.So beam search continued to be used for speech recognition and translation tasks in subsequent years it was a popular algorithm you can even go to Wikipedia and search about it you can see that the references date back to earlier years. So why are we discussing beam search in this course of reasoning models and particularly how can we construct an effective verifier using the algorithm of beam search. So let us look at that in detail.So first we will understand how the traditional beam search algorithm works and so that your intuition is developed and then we will apply that algorithm to construct a verifier which is very effective and then finally we will run a hands on a Google Colab notebook which will give you a practical understanding of how beam search is used in actual LLMs for verification layer. Okay so we will take a very simple example which is a sentence in Hindi. So the sentence says that main Delhi ja raha hu.This is a sentence in Hindi which translates to English as I am going to Delhi and this is the task that I want to do and now I am going to explain to you about beam search algorithm specifically applied to this task. So firstly we have this text corpus which is let us say around 50,000 words and we can only use these words in the translation tasks we cannot use any other word when we are translating this sentence from Hindi to English. Okay so the approach that we are going to take is we are going to translate this sentence word by word.So now let us look at it step 0 is we are given a text corpus a list of 10,000 words or we can even call it say 50,000 words and we have to use only the words in the text corpus for the translation task. Now the next step is out of these 50,000 words we will select the most probable three words for the first word in the translated sentence. So if we go through all these words we have selected I we have selected Delhi and we have selected zoo we have eliminated all other words you might ask the question how are these selections made these selections are made via an encoder decoder model but we are not concerned about how these selections are made right now imagine that there is a model which is making this selection we are going to understand about the beam search strategy this model can be anything it can be a black box for all we know.So we pass this text corpus to the model and we select the three words which have the maximum probability that is step number one. So these words are I Delhi and zoo now the interesting thing comes in step number two in step number two what we do is we pass these three words again through the entire corpus and now we find the next three combinations of two words which have the maximum probability. So now how many combinations we have to see let's say 10,000 plus 10,000 plus 10,000.So now we have to eval sorry this is 50,000 plus 50,000 plus 50,000 out of 1,50,000 combinations we have to choose the best three combinations of the first two words. So you can see here that we have eliminated I was we have eliminated Delhi is we have eliminated all others and we have selected only three again we have selected I am we have selected I love that we have selected zoo is remember that in the first step we passed 50,000 words in the second step we are searching across 1,50,000 entries and then we are again choosing the three most probable entries by passing it through the model which is a black box for all we know right now. So step number two is for each of these words we will select the most probable combination of first word and second word and select the best three.So these are I am I love and zoo is okay so now one thing you can immediately notice from this is that there is no Delhi here so you can actually completely eliminate this Delhi. So that reduces the chances of first word from three to only two that is the first word can either be I or zoo it cannot be Delhi okay now in the next step what we do is we again pass these combinations of three to 50,000 50,000 and 50,000 so again we evaluate across 1,50,000 combinations and now we choose the next best possible three which can be let's say I am I am going so these after this here we again have to pick three so let's say I am going I love Delhi and zoo is green we select these or let's not or third one is I love zoo so now you can see here that zoo is not coming anywhere first so this means that we can eliminate zoo and now we fix our first word which is I and then this process will continue till we have translated the entire sentence. So beam search is essentially a technique so now here you can see that why did we select three at every step we were saying let's select three let's select three three is called as the beam width so instead of selecting just one we are selecting three in each layer then we are proceeding ahead then again we are selecting three in the next layer etc so three in this case is called as the beam width and I think the name beam search came because it's like imagine there is a huge library and you are shining a torch in that library and there is a beam which is created through that torch shining and that beam falls on three books or three letters or three words so here I think it's like that right it's like it's branches into three so from a huge corpus it branches into three then again it branches into three so probably it widens your search scope and that's why it's called as beam search but this is the exact algorithm which was used in the original paper which came out in 1976 and subsequently it has been a popular model in speech recognition tasks.Now for us and for this course we are going to look at how beam search can be combined with a reasoning large language model and a reward model to search many different reasoning paths and come up with an optimum answer. So, in this case we searched across many different words right but here when we are looking at reasoning models we are going to use beam search to explore multiple reasoning paths before we come to the final answer. I hope the intuition is clear for all of you and with this intuition in mind now we are going to move towards understanding the beam search algorithm and how can we develop a verifier which uses a beam search algorithm to effectively search across multiple reasoning paths.Okay. So, now.