
> source: https://bryanyzhu.github.io/posts/2024-06-20-elo-part1/

Chatbot Arena 由 LMSYS 和加州大学伯克利分校 SkyLab 的成员开发，是一个基准测试平台，旨在通过众包环境中的匿名随机对战来评估大语言模型（LLMs）。该平台于 2023 年 5 月推出，并持续更新以反映该领域的最新进展。其排行榜被广泛认为是 LLM 排名最可信的来源之一。

但我们要如何获取这个排行榜呢？竞技场 Elo 评分究竟是什么？这个排名是由专家小组手动决定的吗？一个新发布的模型为何能获得如此多的投票并迅速攀升排名？人们为何如此信任这个排行榜？所有这些问题的答案都藏在 Elo 评分系统中，这是一种用于各类游戏和体育项目排名的奇妙方法。

在这篇博客文章中，我们将深入探讨 Elo 评分系统，并解析其运作原理。这是系列文章的第一部分，我们将分解基础知识，向你展示为什么它是排名玩家（和聊天机器人！）的流行方法。

### 为什么采用 Elo 评分系统？

在寻找最佳模型或确定哪些模型表现更优的过程中，一个公正可靠的排行榜至关重要。构建此类排行榜的一种方法是计算准确率等指标，并直接根据得分从高到低对模型进行排序。然而，由于用户查询的开放性，对大语言模型进行基准测试面临着重大挑战。传统指标无法自动评估这些模型，因为它们必须考虑多种视角和微妙回答的复杂性。

尽管一些文献建议使用 AI 模型作为评判者——例如流行的 MTBench 就采用 GPT-4 作为评估工具——但这种方法存在局限性。AI 评判者往往难以把握冗长复杂回答中的微妙之处，尤其是在现实应用场景中。这是因为它们不具备人类的情感、动机和价值观。简而言之，它们尚未与我们完全契合。因此，人工评估仍然不可或缺。像 Chatbot Arena 这样的平台通过众包方式进行两两比较，让模型在"对战"中相互较量以判断孰优孰劣。

为了将这些两两比较转化为有意义的排名，我们采用了 Elo 评分系统。Elo 评分系统特别适合这一用途，因为它在基于两两比较的基准测试中具有优势特性：

* 可扩展性：Elo 评分系统能够高效处理大量模型。它不需要为每一对可能的模型组合准备大量数据，使得对众多模型进行基准测试成为可能。
* 增量性：新模型可以通过相对较少的测试进行评估。这一特性允许快速将新模型整合到排名系统中并进行评估。
* 唯一排序：Elo 系统为所有模型提供了清晰、唯一的排名。对于任意两个模型，它可以确定哪一个排名更高或是否并列，确保排行榜简单易懂。

通过利用 Elo 评分系统，我们可以维护一个动态且准确的排行榜，该排行榜基于全面的两两比较，反映各种模型的性能。

### 什么是 Elo 评分系统？

Elo 评分系统是一种被广泛认可的方法，用于计算零和游戏中玩家的相对技能水平，包括国际象棋、电子竞技，以及现在的 LLM 评估。在博弈论中，零和游戏是指资源总量固定的一种情境。一个玩家的任何收益都会导致另一个玩家的损失，意味着所有玩家的收益和损失之和为零。一个玩家的成功必然意味着其他玩家的失败。

在大语言模型竞赛中，Elo 评分系统可用于通过模型间的直接对抗表现来评估和排名模型。直观来说，该过程包含三个步骤：

1. 初始分数：每个模型开始时都有一个初始分数，通常设为 1000 分。
2. 比赛机制：当两个模型进行竞争时，如果模型 A 的回答比模型 B 的回答更受青睐，模型 A 就“获胜”，并从模型 B 那里获得一些分数。
3. 分数调整：经过多轮比赛后，那些表现稳定且更符合人类偏好的模型（例如 GPT-4）的分数会高于初始评分。相反，表现不佳的模型分数会降低，因为它们会输给更强的模型。这将自然形成排行榜的排名。

但这究竟是如何运作的？模型 A 应该从模型 B 那里获取多少分数？该系统如何适用于多个模型，并以持续稳定的方式更新它们的排名？

### 深入了解 Elo 评分系统

要回答上述问题，让我们看看计算 Elo 评分的最简单在线线性更新算法。以下是从 Chatbot Arena 的这份笔记中借鉴的 Python 实现代码。

```python
def compute_online_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):
    rating = defaultdict(lambda: INIT_RATING)

    for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'winner']].itertuples():
        ra = rating[model_a]
        rb = rating[model_b]
        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))
        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))
        if winner == "model_a":
            sa = 1
        elif winner == "model_b":
            sa = 0
        elif winner == "tie" or winner == "tie (bothbad)":
            sa = 0.5
        else:
            raise Exception(f"unexpected vote {winner}")
        rating[model_a] += K * (sa - ea)
        rating[model_b] += K * (1 - sa - eb)
```

给定一组对战结果 `battles`，我们遍历这些结果来更新模型的排名。对于每一场对战，我们首先计算每个模型的预期结果，分别记为 `ea` 和 `eb`。然后将这些预期结果与实际比赛结果 `sa` 进行比较，并分别更新模型的评分 `rating[model_a]` 和 `rating[model_b]`。有两个关键部分需要详细说明：(1) 计算每个模型的预期结果，(2) 更新模型的评分。

#### 计算每个模型的预期结果

首先，为什么我们要计算预期结果？预期结果至关重要，因为它使我们能够根据模型当前的评分量化每个模型获胜的概率。这种概率方法确保了评分调整是公平的，并且与模型的性能预期成比例。如果一个高评分模型击败了一个低评分模型，评分变化应该较小，因为这一结果是预期的。相反，如果一个弱势模型获胜，评分变化应该更显著，以反映这一出人意料的结果。这将有助于稳定排名系统。例如， GPT4 在大多数情况下可以击败大多数模型，但由于其预期胜率较高，实际的评分变化微乎其微。否则，稍强的模型会迅速获得极高的分数，而稍弱的模型则会很快被淘汰。

其次，我们为何使用这个公式来计算预期结果，例如模型 A 的预期结果 `ea = 1 / (1 + BASE ​**​ ((rb - ra) / SCALE))`？该公式源自逻辑分布，旨在提供一个平滑、连续的函数，将评分差异映射为获胜概率。谈到概率，这意味着 `ea` 和 `eb` 的取值范围在 0 到 1 之间。

- 当 `rb` 远高于 `ra` 时，分母会变得非常大，导致 `ea` 趋近于 0。
- 当 `rb` 远低于 `ra` 时，这部分 `BASE ​**​ ((rb - ra) / SCALE)` 会变得非常小，几乎为 0，因此分母会收敛于 1。此时 ea 简化为 1，即上限值。
- 当 `ra = rb` 时，`ea` 为 `1 / (1 + 1) = 0.5`，表示双方获胜概率均等。

总而言之，随着评分差异的增大，预期结果会偏向评分较高的模型。选择 `BASE = 10` 和 `SCALE = 400` 是惯例，确保 400 分的评分差异对应 10 比 1 的预期胜率。这一比例因子使系统直观且易于理解。

#### 更新模型评级

一旦我们获得了预期的结果和战斗的实际结果，我们就能够更新模型的评分。评分更新公式为：

```python
rating[model_a] = ra + K * (sa - ea)
```

其中：

* `ra` 是模型 A 在比赛前的原始评分。
* `K` 表示评分的最大变化值（国际象棋通常设为 32，但可能有所不同）。Chatbot Arena 的默认值为 `K=4`，因为他们希望让 Elo 评分更加稳定，减少对近期比赛的偏向性。我们稍后将在一分钟内讨论这个偏向性问题。
- `sa` 代表比赛后的实际结果（1 表示胜利，0.5 表示平局，0 表示失败）。

有趣的是，如果你仔细观察这个公式，你会发现即使比赛结果是平局，评分较低的玩家也会从评分较高的玩家那里获得几分。这意味着这个评分系统是自我修正的。从长远来看，评分过低或过高的玩家表现会相应地比评分系统预测的要好或差，从而获得或失去评分，直到评分反映出他们真实的比赛水平。

另一个有趣的点是，这个公式看起来与随机梯度下降（SGD）中使用的更新规则非常相似。在 SGD 中，更新规则是：

```python
w' = w − η * ∇L(w)
```

相比之下，Elo 评分更新规则可以理解为：

* 模型的评分 `ra` 类似于 SGD 中的模型参数 `w`
- 缩放因子 `K` 类似于学习率 `η`，控制步长大小
- 分数差 `sa - ea` 类似于梯度，表示实际结果（模型预测）与预期结果（真实值）之间的误差

这种相似性说明了 Elo 评分系统可以被视为一种迭代优化过程。就像机器学习中模型通过训练不断改进一样，Elo 评分系统让模型能够从每场比赛中“学习”，逐步完善其评分。

### Some notes

虽然 Elo 评分系统在许多情况下被广泛使用且有效，但仍有一些值得讨论的有趣之处。

#### Elo 排名是一个绝对指标？

Elo评分仅具有相对性，其有效性仅限于计算时所依据的评分群体内部，而非衡量玩家实力的绝对标准。即使你的能力保持不变，今年的评分也可能与明年不同。高 Elo 排名仅表明该玩家在当前群体中表现出色，并不代表其普遍优秀。

#### 偏向最近的战斗

就计算 Elo 评分的在线线性更新算法而言，它对近期比赛结果较为敏感。因为评分是依次更新的，这意味着每个新结果都基于上一次更新的评分。这种顺序依赖性会放大近期比赛结果的影响，尤其是当这些结果与预期明显不符时。如果改变评分更新顺序，可能会导致评分出现显著波动。

为了证明这一点，在 Chatbot Arena 的笔记本中，他们通过反转比赛顺序重新计算 Elo 评分，并观察到由于 Elo 在线更新对近期比赛的偏倚而导致的显著差异。我们可以看到，当顺序反转时，胜者从 `gemini-1.5-pro-api-0409-preview` 变成了 `gpt-4o-2024-05-13`。所有其他模型的评分也发生了显著变化。

#### 对匹配的敏感度

评分更新在很大程度上依赖于匹配过程。如果匹配不平衡（例如，经常将高评分模型与极低评分模型配对），评分可能会失真。因此，匹配过程也需要谨慎处理。

聊天机器人竞技场提到，他们采用了多种不同的匹配和采样算法。他们使用了均匀采样以及加权采样方法，后者会给表现更好的模型分配更大的权重。这很可能就是为什么我们可以看到像 GPT-4o 这样的新模型在发布后不久就能登上排行榜榜首。

### Summary

所以，下次当你需要对某些事物进行排名，却苦于没有明确的衡量标准时，请记住 Elo 评分系统。这是一个经过验证的方法，可以将一系列个体比较转化为一个有意义的、动态的排行榜。


----

## # Chatbot Arena 和 Elo 评级系统 - 第 2 部分

https://bryanyzhu.github.io/posts/2024-08-16-elo-part2/



