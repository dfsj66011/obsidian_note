

##### Discriminative Reward Model

- Train a classifierÂ fÏ•(x)Â predictingÂ P(ycall=1âˆ£x)Â using human-labeled examples indicating if/how strongly the query requires tool use.
- This mirrors methodology from RLHF as inÂ [InstructGPT](https://arxiv.org/abs/2203.02155)Â by Ouyang et al. (2022).

##### Generative Reward Model (LLM-as-a-Judge)

- Use a judge model (e.g., DeepSeek-V3 perÂ [DeepSeek-R1](https://arxiv.org/abs/2501.12948)):
    
- Prompt: â€œGiven this user query and available tools, should the agent call a tool at this stage? Provide yes/no and reasoning.â€
    
- Extract a scalar reward from the generative verdict.
    
- This can capture nuanced timing requirements over multiple steps.
    

#### Reward Component: Tool Selection (Choosing â€œWhichâ€ Tool)

- This component supports theÂ **which**Â dimension: Given that a tool is to be called, was theÂ _correct_Â tool chosen?

##### Rule-based Supervision

- If rules map tasks to a specific tool or tool category, then:
    
    - If the predicted tool matches the ruleÂ â†’Â +reward
    - OtherwiseÂ â†’Â âˆ’reward
- This is similar to mapping tool types inÂ [ReAct](https://arxiv.org/abs/2210.03629)Â by Yao et al. (2022).
    

##### Discriminative Reward Model

- Train a classifierÂ fÏˆ(st,at)Â that judges whether the selected tool matches human expectations for that state.

##### Generative Reward Model

- Ask a judge LLM: â€œWas TOOL_X the best tool choice for this request at this step?â€
    
- Score the answer and normalize.
    

#### Reward Component:Â **Tool-Syntax Correctness**

- Supports theÂ **how**Â dimension partially, focusing onÂ _format_:
    
    - JSON validity
    - Required argument fields
    - Correct schema shape

##### Rule-based

- JSON parse success
- Schema validation
- Argument-type validation
    
- **Reward:**
    
    rsyntaxt={+1if JSON + schema validÂ âˆ’1otherwise
    
- This echoes structured action enforcement inÂ [ReAct](https://arxiv.org/abs/2210.03629).

##### Discriminative Reward Model

- Classify correct vs incorrect tool-call formats.

##### Generative Reward Model

- Ask an LLM judge whether the formatting is correct (1â€“10), normalize to reward.

#### Reward Component:Â **Tool-Execution Correctness**

- Did the tool run without error?

##### Rule-based

- HTTP 200 or success flagÂ â†’Â +reward
- Errors / exceptionsÂ â†’Â âˆ’reward

##### Discriminative Reward Model

- Trained to predict execution feasibility or correctness.

##### Generative Reward Model

- Judge evaluates based on logs and outputs.

#### Reward Component: Argument Quality (Deciding â€œHowâ€ to Call a Tool)

- This is the core of theÂ **how**Â dimension: constructing appropriate arguments.

##### Rule-based

- For numeric or structured problems:

rargst=âˆ’|apredâˆ’agold|

- For strings, use embedding similarity or fuzzy match.

##### Discriminative Reward Model

- Trained to identify argument errors (bad city name, missing date, etc.).

##### Generative Reward Model

- LLM-as-a-Judge evaluates argument plausibility/fit to the query.

#### Reward Component:Â **Final Task Success**

- Whether the overall trajectory produced a correct answer.

##### Rule-based

- Unit test pass
- Exact match
- Tolerance-based numeric match

##### Discriminative Reward Model

- Using preference modeling as inÂ [Deep RL from Human Preferences](https://arxiv.org/abs/1706.03741)Â by Christiano et al. (2017), train:

îˆ¸RM=âˆ’logerÏ•(Ï„A)erÏ•(Ï„A)+erÏ•(Ï„B).

##### Generative Reward Model

- Judge LLM compares model prediction with ground truth (as inÂ [DeepSeek-R1](https://arxiv.org/abs/2501.12948)).

#### Merged Preference-Based Rewards (For â€œCallâ€, â€œWhichâ€, and â€œHowâ€)

- You can construct pairs of trajectories differing in:
    
    - timing of tool calls (call),
    - choice of tool (which), and
    - argument construction (how)
- Let the judge or human annotator choose the better one.
    
- Train a preference RM to provide combined signals.
    

#### Unified Reward Formulation

- All reward signalsâ€”process and outcomeâ€”are merged into one scalar:
    
    R=wcallrcallî„½î„¾î…when+(wtoolrtool)î„½î„¾î…î…‹î…‹which+(wsyntaxrsyntax+wexecrexec+wargsrargs)î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹how+(wtaskrtask+wprefrpref)î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹outcome-level
    
    - where:
        
        - TheÂ **when**Â group controlsÂ _whether_Â a tool is invoked.
        - TheÂ **which + how**Â group supervisesÂ _tool choice_Â andÂ _argument construction_.
        - TheÂ **outcome-level**Â group ensures the final result is correct and aligns with human/judge preferences.
- This single scalar rewardÂ RÂ is what enters the RL optimizer (e.g., PPO or GRPO).
    
- WeightsÂ wÂ are tuned to balance shaping vs final correctness.
    

#### Asymmetric Rewards in Tool-Calling RL

- This section explains why tool-calling RL systems useÂ **asymmetric rewards**Â (positive rewards much larger than negative rewards), how this stabilizes PPO/GRPO, and how asymmetry applies across theÂ **when / which / how**Â components. A full worked example and a comprehensive reward table are included.
    
- Asymmetric reward schedules are used in practical tool-use RL systems such as ReTool, ToolRL, DeepSeek-R1, and RLHF pipelines. They ensure that:
    
    - Success is highly rewarded.
    - Failure incurs penalties but not catastrophic ones.
    - Exploration does not collapse into inert policies (e.g., â€œnever call toolsâ€).
    - The hierarchy â€” decidingÂ **when**Â to call tools,Â **which**Â tool to call, andÂ **how**Â to construct correct arguments â€” all receive stable and interpretable feedback.

##### Why Asymmetry is Required

- Because tool-calling introduces many potential failure points (incorrect timing, wrong tool, malformed arguments, bad final answer), symmetric rewards would cause massive early negative returns. The policy would quickly learn the degenerate strategy: â€œNever call any tool; always respond directly.â€
    
- Asymmetric rewards avoid this by:
    
    - UsingÂ **large positive**Â rewards for correct full trajectories.
    - UsingÂ **mild or moderate negative**Â rewards for mistakes.
    - Ensuring that exploratory attempts are onlyÂ _slightly_Â penalized.
    - Allowing the policy to differentiate between â€œbad idea but learningâ€ vs â€œexcellent behavior.â€
- This encourages exploration in the factored action space and prevents PPO/GRPO from collapsing into trivial policies.
    

##### Reward Table: Positive and Negative Rewards by Category

- Below is a consolidated table representingÂ **typical**Â asymmetric reward magnitudes for each component. These values are illustrative and are often tuned per domain.

###### Reward Values for â€œWhen / Which / Howâ€ and Outcome-Level Components

|**Reward Component**|**Description**|**Positive Reward Range**|**Negative Reward Range**|
|---|---|---|---|
|**When** (call decision)|Correctly calling a tool when needed|+0.5 to +1.5|âˆ’0.2 (tool required but not called)|
||Correctly not calling a tool|+0.3 to +1|âˆ’0.2 (tool called when unnecessary)|
|**Which** (tool selection)|Selecting correct tool|+0.5 to +2.0|âˆ’0.3 to âˆ’0.7 (wrong tool)|
|**How: Syntax**|JSON validity and schema correctness|+0.3 to +1.0|âˆ’1.0 (malformed JSON or wrong schema)|
|**How: Execution**|Tool executes successfully (HTTP 200, etc.)|+0.5 to +1.0|âˆ’1.0 to âˆ’2.0 (execution error)|
|**How: Argument Quality**|High-quality arguments (correct fields, values)|+0.5 to +2.0|âˆ’0.5 to âˆ’1.5 (missing/incorrect/poor arguments)|
|**Outcome: Final Task Success**|Producing correct final answer using tool output|+8.0 to +15.0|âˆ’0.3 to âˆ’1.0 (incorrect final answer)|
|**Outcome: Preference/Judge Score**|Judge or LLM-as-a-critic evaluation of final output|+1.0 to +5.0|âˆ’0.1 to âˆ’1.0|

- This table reflects the following structural principles:
    
    - TheÂ **largest rewards**Â are reserved for correctÂ _end-to-end_Â solution quality.
    - TheÂ **largest penalties**Â correspond only to errors that break execution (syntax, runtime failure).
    - Small errors in timing, selection, or argument quality incurÂ **light penalties**.
    - Rewards across â€œwhen / which / howâ€ are significantlyÂ **lower**Â than final-task success, ensuring shaping rewards guide early learning but final correctness dominates late learning.

##### Worked Example with Asymmetric Rewards

- Consider the user query: â€œWhatâ€™s the weather in Paris tomorrow?â€
    
- Correct behavior requires:
    
    1. Deciding a tool is required (**when**).
    2. Selecting the weather API (**which**).
    3. Providing correct arguments in JSON (**how**).
    4. Producing the correct final answer using the tool output.
- Below are two trajectories demonstrating asymmetry.
    

###### Trajectory A: Imperfect but Reasonable Exploration

1. **When**Â decision correctÂ â†’Â +1.0
2. **Which**Â tool wrongÂ â†’Â âˆ’0.5
3. JSON syntax validÂ â†’Â +0.5
4. Tool executes (but irrelevant)Â â†’Â 0
5. Final answer wrongÂ â†’Â âˆ’0.5

- Total reward:

RA=1.0âˆ’0.5+0.5+0âˆ’0.5=0.5

- Even though the overall answer is wrong, the trajectory gets aÂ _small positive_Â reward because several subcomponents were correct. This prevents the model from concluding that tool use is too risky.

###### Trajectory B: Full Correct Behavior

1. CorrectÂ **when**Â â†’Â +1.0
2. CorrectÂ **which**Â â†’Â +1.5
3. Correct JSON argumentsÂ â†’Â +1.0
4. Successful tool executionÂ â†’Â +1.0
5. Correct final answerÂ â†’Â +10.0

- Total reward:

RB=1.0+1.5+1.0+1.0+10.0=14.5

- The tremendous difference between +14.5 and +0.5 clearly guides PPO/GRPO toward producing the full correct behavior.

##### How Asymmetry Stabilizes PPO/GRPO

- Advantages are computed via:

At=Rtâˆ’V(st)

- With asymmetric rewards:
    
    - Failed trajectories receive slightly negative or slightly positive returns.
    - Successful trajectories receive large positive returns.
    - Advantage variance stays manageable.
    - Exploration does not collapse into â€œnever call tools.â€
    - The policy improves steadily across â€œwhen / which / howâ€ dimensions.
- If rewards were symmetric (e.g., +10 vs âˆ’10), then most exploratory episodes would produce extreme negative advantages, instantly pushing the model toward refusing all tool calls. Asymmetry prevents this collapse.
    

##### Takeaways

- Asymmetric rewards are essential for training LLM tool-calling policies because they:
    
    - Preserve exploration.
    - Deliver stable gradients for PPO/GRPO.
    - Avoid trivial degenerate strategies.
    - Properly balance shaping rewards (for â€œwhen / which / howâ€) with outcome-level rewards.
    - Distinguish partial correctness from catastrophic failure.
    - Encourage correct final answers without over-penalizing small mistakes.
- The reward table and examples above provide a practical blueprint for implementing and tuning asymmetric rewards in your own RL tool-calling system.
    

### RL Optimization Pipeline: Shared Flow + PPO Vs GRPO

- This section describes how to take the unified reward from Section 3 and plug it into a full reinforcement learning (RL) pipelineâ€”including both Proximal Policy Optimization (PPO) byÂ [Schulman et al., 2017](https://arxiv.org/abs/1707.06347)Â and Group Relative Policy Optimization (GRPO) byÂ [Shao et al., 2024](https://arxiv.org/abs/2402.03300). We present first the shared components, then algorithmâ€specific losses and update rules.
- A detailed discourse of preference optimization algorithms is available in theÂ [Preference Optimization](https://aman.ai/primers/ai/preference-optimization)Â primer.

#### Shared RL Training Flow

1. **Rollout Generation**:
    
    - Use the policyÂ Ï€Î¸Â (based on the LLM) to interact with the toolâ€calling environment defined in Section 2.
    - At each stepÂ tÂ you have stateÂ st, select actionÂ atÂ (`CALL`Â tool orÂ `ANSWER`), observe next stateÂ st+1, and receive scalar rewardÂ rtÂ (from the unified reward).
    - Repeat until terminal (ANSWER) or maximum stepsÂ T.
    - Collect trajectoriesÂ Ï„=(s0,a0,r0),â€¦,(sTâˆ’1,aTâˆ’1,rTâˆ’1),(sT).
2. **Return and Advantage Estimation**:
    
    - Compute discounted return:
        
        Rt=âˆ‘k=tTÎ³kâˆ’t,rk
        
    - Estimate value baselineÂ VÏˆ(st)Â (for PPO) or compute groupâ€relative statistics (for GRPO).
        
        - Advantage (for PPO):
            
            At=Rtâˆ’VÏˆ(st)
            
            - Use Generalized Advantage Estimation (GAE) if desired (as typically done in PPO):
                
                A(Î»)t=âˆ‘l=0âˆ(Î³Î»)lÎ´t+l,Î´t=rt+Î³VÏˆ(st+1)âˆ’VÏˆ(st)
                
3. **Policy Update**:
    
    - Use a surrogate objective (dependent on algorithm) to update Î¸ (policy), and update value parameters Ïˆ where needed.
    - Optionally include a KL-penalty or clipping to ensure policy stability.
4. **Repeat**:
    
    - Collect new rollouts, update, evaluate. Monitor metrics such as toolâ€call decision accuracy (â€œwhenâ€), correct tool selection (â€œwhichâ€), argument correctness (â€œhowâ€), and final task success.

#### PPO: Losses and Update Rules

##### Surrogate Objective

- For PPO the objective is using clipped surrogate:

LPPO(Î¸)=ğ”¼s,aâˆ¼Ï€Î¸old[min(rt(Î¸)At,clip(rt(Î¸),1âˆ’Ïµ,1+Ïµ)At)]

- where:

rt(Î¸)=Ï€Î¸(atâˆ£st)Ï€Î¸old(atâˆ£st)

- â€¦ andÂ Ïµâ‰ˆ0.1âˆ’0.3.

##### Value Loss

Lvalue(Ïˆ)=ğ”¼stâˆ¼Ï€[(VÏˆ(st)âˆ’Rt)2]

##### KL/Entropy Penalty

- Often a term is added:

LKL(Î¸)=Î²,ğ”¼st,atâˆ¼Ï€Î¸[logÏ€Î¸(at|st)Ï€ref(at|st)]

- â€¦ to keep the policy close to either the old policy or a reference SFT policy.

##### Full PPO Loss

LtotalPPO=âˆ’LPPO(Î¸)+cv,Lvalue(Ïˆ)+cKL,LKL(Î¸)

- â€¦ with coefficientsÂ cv,cKL.

##### Implementation Notes

- Use mini-batches and multiple epochs per rollout.
- Shuffle trajectories, apply Adam optimizer.
- Clip gradients; log metrics for tool decisions and argument quality.

#### GRPO: Losses and Update Rules

##### Group Sampling & Relative Advantage

- In GRPO [Shao et al., 2024] you sample a group ofÂ GÂ actionsÂ (a1,â€¦,aG)Â under the same stateÂ s. Compute each rewardÂ r(s,aj). Then define group mean and standard deviation:Â Î¼,Ïƒ. Advantage for each is:

AGRPO(s,aj)=r(s,aj)âˆ’Î¼Ïƒ

##### GRPO Surrogate

LGRPO(Î¸)=1Gâˆ‘j=1Gğ”¼s,a1:Gâˆ¼Ï€Î¸old[min(rj(Î¸)AGRPO(s,aj),clip(rj(Î¸),1âˆ’Ïµ,1+Ïµ)AGRPO(s,aj))]

- â€¦ with the same ratio definitionÂ rj(Î¸)=Ï€Î¸(ajâˆ£s)/Ï€Î¸old(ajâˆ£s).

##### Value Loss

- GRPO typicallyÂ **omits**Â a parametric value estimatorâ€”baseline derived via group statistics.

##### KL/Entropy Penalty

- Same form as in PPO if desired.

##### Full GRPO Loss

LtotalGRPO=âˆ’LGRPO(Î¸)+cKLLKL(Î¸)

##### Implementation Notes

- At each state draw multiple candidate tool/answer actions, compute rewards, form group.
- This is particularly suited for LLM tool-calling contexts where you can generate multiple alternate completions.
- GRPO reduces reliance on value network.

#### Integrating the Unified Reward

- Given the unified rewardÂ RÂ from the prior step, each stepâ€™sÂ rtÂ is used in return and advantage estimation. The policy thus simultaneously learns â€œwhen/which/howâ€ tool calling by maximizing return:

J(Î¸)=ğ”¼Ï„âˆ¼Ï€Î¸[âˆ‘t=0TÎ³trt]

- Both PPO and GRPO approximate gradient ascent onÂ J(Î¸)Â under stability constraints.

### Curriculum Design, Evaluation Strategy, and Diagnostics for Tool-Calling RL

- This section describes how to structure training so the model reliably learnsÂ **when**,Â **which**, andÂ **how**Â to call tools, and how to evaluate progress during RL. Curriculum design is crucial because tool-calling is a hierarchical skill; introducing complexity too early destabilizes learning, and introducing it too late yields underfitting.

#### Curriculum Design Overview

- Curriculum design gradually increases difficulty along three axes:
    
    1. **When**Â â†’Â recognizing tool necessity vs non-necessity
    2. **Which**Â â†’Â selecting the correct tool
    3. **How**Â â†’Â providing high-quality arguments
- Each axis has its own progression. The curriculum alternates between breadth (many domains/tools) and depth (multi-step workflows).
    
- This staged approach mirrors the structured curricula seen in code-generation RL (e.g., unit-testsÂ â†’Â multi-step tasks) in works likeÂ [Self-Refine](https://arxiv.org/abs/2303.17651)Â by Madaan et al. (2023).
    

#### Stage 0: Pure Supervised Bootstrapping (SFT)

- Before RL begins, do supervised fine-tuning on a dataset that explicitly includes:
    
    - Examples requiring a tool,
    - Examples that mustÂ _not_Â use a tool,
    - Examples mapping queries to correct tool types,
    - Examples showing valid argument formats.
- The SFT initializes:
    
    - An approximately correct â€œwhenÂ â†’Â whichÂ â†’Â howâ€ policy,
    - JSON formatting reliability,
    - Stable tool-calling syntax.
- This prevents â€œflailingâ€ during early RL where the model might emit random tool calls.
    

#### Stage 1: Binary Decision Curriculum (LearningÂ **When**)

- **Focus:**Â detect whether a tool is required.
    
- **Task mix:**
    
    - 50% queries that require a specific tool (weather/math/search)
    - 50% queries that must be answered without tools
- **Goal:**Â learn the call/no-call boundary.
    
- **Metrics:**
    
    - Call precision
    - Call recall
    - False-positive rate (unnecessary calls)
    - False-negative rate (missed calls)
- **Reward emphasis:**
    
    - Increase (w_{\text{call}})
    - Reduce penalties for syntax/execution errors early on

#### Stage 2: Tool-Selection Curriculum (LearningÂ **Which**)

- Add tasks that require choosingÂ _between_Â tools:
    
- **Task examples:**
    
    - Weather vs. news
    - Search vs. calculator
    - Translation vs. summarization (if tools exist)

**Goal:**Â learn discriminative mapping from task intentÂ â†’Â tool identity.

- **Curriculum trick:**
    
    - For ambiguous queries, include diverse examples so the RL agent learns to think (internal chain-of-thought) before issuing tool calls.
- **Metrics:**
    
    - Tool-selection accuracy
    - Confusion matrix across tool categories
    - Average number of tool attempts per query
- **Reward emphasis:**
    
    - Shift weight from (w_{\text{call}})Â â†’Â (w_{\text{which}})
    - Introduce penalties for repeated incorrect tool choices

#### Stage 3: Argument-Construction Curriculum (LearningÂ **How**)

- Introduce tasks with argument complexity:
    
    - **Task examples:**
        
        - Weather(city, date)
        - Maps(location, radius)
        - Calculation(expressions with multiple steps)
        - API requiring nested JSON fields
    - **Training strategy:**
        
        - Start with minimal arguments (one field)
        - Add multi-argument calls
        - Introduce noisy contexts (typos, ambiguity)
    - **Metrics:**
        
        - Argument correctness (string similarity or numeric error)
        - Schema completeness
        - Tool execution success rate
    - **Reward emphasis:**
        
        - IncreaseÂ wargs
        - Tighten penalty for malformed JSON or missing fields

#### Stage 4: Multi-Step Tool Use (Pipelines)

- Introduce tasks requiringÂ **multiple sequential tool calls**, e.g.:
    
    1. Search for restaurants
    2. Get the address
    3. Query weather at that address
    4. Produce a combined answer
- Here the agent must plan sequences and must choose when to stop calling tools.
    
- **Metrics:**
    
    - Number of steps per episode
    - Optimality of tool sequence
    - Rate of premature or redundant tool calls
- **Reward emphasis:**
    
    - Add step penalties
    - Strengthen outcome reward since multi-step tasks dominate final task success

#### Stage 5: Open-Domain Free-Form Tasks

- Finally, mix in diverse real-world questions with unconstrained natural-language variety.
    
- **Goal:**Â produce a robust â€œuniversalâ€ tool-use agent.
    
- **Metrics:**
    
    - Overall episodic return
    - Win-rate vs evaluator models (LLM-as-a-Judge)
    - Human preference win-rate
    - Task success accuracy in open benchmarks

#### Diagnostics and Monitoring

##### Process-Level Metrics

- Aligned with theÂ **whenÂ â†’Â whichÂ â†’Â how**Â decomposition:
    
    - **When:**
        
        - Call precision/recall
        - Unnecessary call rate
        - Missed call rate
        - Call timing consistency
    - **Which:**
        
        - Tool selection accuracy
        - Error matrix across tools
        - Repeated incorrect tool selection episodes
    - **How:**
        
        - Argument correctness scores
        - JSON validity rate
        - Execution success rate

##### Outcome-Level Metrics

- **Final answer accuracy:**
    
    - Exact match
    - Tolerance-based match
    - Semantic similarity
    - Pass rate vs LLM-judge (DeepSeek-V3, GPT-4, etc.)
- **Task efficiency:**
    
    - Number of steps per solved task
    - Number of tool calls per successful episode
    - Reward per timestep
- **User-facing metrics:**
    
    - Latency per episode
    - Number of external API calls

#### Detecting Skill Collapse

- **Red flags include:**
    
    - Spike in JSON errorsÂ â†’Â syntax collapse
    - Rising unnecessary tool useÂ â†’Â call collapse
    - Tool-selection deteriorationÂ â†’Â â€œwhichâ€ collapse
    - Rising tool execution failuresÂ â†’Â argument collapse
    - Flat final-task accuracyÂ â†’Â plateau due to overfitting on shaping rewards
- **Solutions:**
    
    - Adjust reward weightsÂ wâ‹…
    - Reintroduce supervised examples
    - Increase entropy regularization
    - Add KL penalties to keep model close to reference

#### Curriculum Scheduling (Putting It All Together)

- **A typical recipe:**
    
    1. **Stage 0 (SFT):**Â 30kâ€“200k examples
    2. **Stage 1 (When):**Â 1â€“5 RL epochs
    3. **Stage 2 (Which):**Â 3â€“10 RL epochs
    4. **Stage 3 (How):**Â 5â€“20 RL epochs
    5. **Stage 4 (Pipelines):**Â 10â€“30 RL epochs
    6. **Stage 5 (Open-domain):**Â continuous RL/adaptation
- **Dynamic curriculum:**Â shift task sampling probabilities based on evaluation metricsâ€”for example, increase argument-focused tasks if argument correctness stagnates.
    

#### Final Note

- A well-designed curriculum ensures the policy does not simply memorize tool-call structures but truly internalizes:
    
    - **when**Â tool use is warranted,
    - **which**Â tool to call,
    - **how**Â to call it correctly,
    - â€¦ and how to combine tools into multi-step workflows to solve real tasks.

### Reinforcement Learning and the Emergence of Intelligent Agents

- With the rise of Large Language Models (LLMs) and multimodal foundation models, RL has become a critical mechanism for developing autonomous, reasoning-capable agents. Early efforts demonstrated that LLMs could act as agents that browse the web, search for information, and perform tasks by issuing actions and interpreting observations.
    
- One of the first large-scale examples wasÂ **[WebGPT](https://arxiv.org/abs/2112.09332)**Â by Nakano et al. (2022), which extended GPT-3 to operate in a simulated text-based browsing environment. The model was trained through a combination of imitation learning and reinforcement learning from human feedback (RLHF).
    
    - WebGPT introduced aÂ **text-based web interface**Â where the model interacts via discrete commands such asÂ _Search_,Â _Click_,Â _Quote_,Â _Scroll_, andÂ _Back_, using the Bing Search API as its backend. Human demonstrators first generated browsing traces that the model imitated throughÂ **behavior cloning**, after which it was fine-tuned viaÂ **PPO**Â against aÂ **reward model**Â trained on human preference data. The reward model predicted human judgments of factual accuracy, coherence, and overall usefulness.
    - Each browsing session ended when the model issued â€œEnd: Answer,â€ triggering a synthesis phase where it composed a long-form response using the collected references. The RL objective included both a terminal reward from the reward model and a per-token KL penalty to maintain policy stability. Empirically, the best 175B â€œbest-of-64â€ WebGPT model achieved human-preference rates ofÂ **56% over human demonstrators**Â andÂ **69% over Reddit reference answers**, showing the success of combining structured tool use with RLHF.
    - The following figure ([source](https://arxiv.org/abs/2112.09332)) shows the text-based browsing interface used in WebGPT, where the model issues structured commands to retrieve and quote evidence during question answering.
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/WebGPT.jpg)
    
- Subsequent systems expanded these capabilities.Â **[Agent Q](https://arxiv.org/abs/2408.07199)**Â by Putta et al. (2024) introduced a hybrid RL pipeline that integratesÂ **Monte Carlo Tree Search (MCTS)**Â withÂ **Direct Preference Optimization (DPO)**.
    - Agent Q formalizes decision making as aÂ **reasoning tree**, where each node represents a thoughtâ€“action pair and edges correspond to plausible continuations. MCTS explores multiple reasoning branches guided by a value model estimating downstream reward. During training, preference data between trajectories is used to train a DPO objective, directly optimizing the policy toward preferred rollouts without relying on an explicit reward scalar.
    - This setup enablesÂ **off-policy reuse**Â of exploratory trajectories: the model learns from both successes and failures by evaluating them through a learned preference model. Empirically, this led to substantial gains in reasoning depth and factual accuracy across multi-step question answering benchmarks, demonstrating that structured search and preference-based policy updates can yield stronger reasoning alignment than gradient-only PPO approaches.
- More recent advancements such asÂ **[OpenWebVoyager](https://arxiv.org/abs/2410.19609)**Â by He et al. (2024) brought these ideas into the multimodal realm. OpenWebVoyager extends open-source multimodal models (Idefics2-8B-Instruct) to perform real-world web navigation using bothÂ **textual accessibility trees**Â andÂ **visual screenshots**. The training process unfolds in two phases:
    
    1. **Imitation Learning (IL)**: The model first learns from expert trajectories collected with GPT-4o via the WebVoyager-4o system. Each trajectory contains sequences ofÂ _thoughts_Â andÂ _actions_Â derived from multimodal observations (screenshot + accessibility tree). The IL objective jointly maximizes the log-likelihood of both action and reasoning token sequences:
        
        JIL(Î¸)=E(q,Ï„)âˆ¼DILâˆ‘t[logÏ€Î¸(at|q,ct)+logÏ€Î¸(ht|q,ct)]
        
    2. **Explorationâ€“Feedbackâ€“Optimization Cycles**: After imitation, the agent autonomously explores the open web, generating new trajectories. GPT-4o then acts as anÂ _automatic evaluator_, labeling successful trajectories that are retained for fine-tuning. Each cycle introduces newly synthesized tasks using theÂ **Self-Instruct**Â framework, ensuring continuous policy improvement. Iteratively, the task success rate improves fromÂ **19.9% to 25.8%**Â on WebVoyager test sets and fromÂ **6.3% to 19.6%**Â on cross-domain Mind2Web tasks.
        
    
    - The following figure ([source](https://arxiv.org/abs/2410.19609)) shows the overall process of OpenWebVoyager, including the Imitation Learning phase and the explorationâ€“feedbackâ€“optimization cycles.
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/OpenWebVoyager.jpg)
    
    - The following figure ([source](https://arxiv.org/abs/2410.19609)) shows the model architecture of OpenWebVoyager. The system uses the most recent three screenshots and the current accessibility tree to guide multimodal reasoning, ensuring temporal grounding across page transitions.
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/OpenWebVoyager2.jpg)
    
- Alongside real-environment exploration, a complementary approach is to scale policy learning with synthetic but reasoning-grounded interaction data.Â **DreamGym**, proposed in ([Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025)), formalizes this by training a reasoning-basedÂ _experience model_Â that serves as both a generative teacher and an adaptive simulator. This model produces synthetic task curricula and consistent next-state transitions, enabling closed-loop reinforcement learning at scale.
    
    - The framework introducesÂ _experience synthesis_Â as a core principleâ€”training a language-conditioned simulator capable of generating realistic interaction traces that preserve reasoning consistency and causal coherence. By jointly optimizing the policy and the experience model under trust-region constraints, DreamGym maintains stability and theoretical convergence guarantees: if the model error and reward mismatch remain bounded, improvements in the synthetic domain provably transfer to real-environment performance.
    - The result is a unified infrastructure that decouples exploration (handled by the experience model) from policy optimization, dramatically reducing real-environment sample costs while preserving fidelity in reasoning tasks. Empirically, DreamGym demonstrates significant gains in multi-tool reasoning, long-horizon planning, and web navigation.
    - The following figure illustrates that compared to the traditional agent learning paradigm, DreamGym provides the first scalable and effective RL framework with unified infrastructure.
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/DreamGym1.jpg)
    
- **Early Experience**, proposed in ([Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al. (2025)), establishes a two-stage curriculumâ€”implicit world modeling and self-reflection over alternative actionsâ€”that uses only language-native supervision extracted from the agentâ€™s own exploratory branches, before any reward modeling or PPO/GRPO.
    
    - The first stage,Â _implicit world modeling_, trains the agent to predict environmental dynamics and next states, effectively learning the structure of interaction without any external reward. The second stage,Â _self-reflection_, asks the agent to introspectively compare expert and non-expert behaviors, generating rationale-based preferences that bootstrap value alignment.
    - These objectives serve as pre-RL signals that warm-start the policy, leading to faster and more stable convergence once reinforcement learning begins. In empirical evaluations, the Early Experience framework significantly improves downstream success rates across both web-based and software-agent benchmarks, and integrates seamlessly with later RL fine-tuning methods like PPO or GRPO.
    - The following figure shows the progression of training paradigms. (Left:) The Era of Human Data relies on expert demonstrations, where supervision comes from human-/expert-curated actions; it is reward-free (i.e., does not require the environment to provide verifiable reward) but not data-scalable. (Right:) The envisioned Era of Experience builds upon environments with verifiable rewards, using them as the primary supervision for reinforcement learning; however, many environments either lack such rewards (Xue et al., 2025) or require inefficient long-horizon rollouts (Xie et al., 2024a). Center: Our Early Experience paradigm enables agents to propose actions and collect the resulting future states, using them as a scalable and reward-free source of supervision
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/EarlyExperience1.jpg)
    

### The Role of Reinforcement Learning in Self-Improving Agents

- RL serves as the foundation ofÂ _self-improving_Â artificial agents. These agents do not depend solely on human-provided supervision; instead, they learn continuously from their own experiences.
    
- A representative example of this approach isÂ [Large Language Models Can Self-improve at Web Agent Tasks](https://arxiv.org/abs/2405.20309)Â by Patel et al. (2024), which introduced a looped learning process where an agent repeatedly performs tasks, evaluates its own performance, and fine-tunes itself on the best results. In their experiments, agents improved their web-navigation success rates by over 30% without any additional human data, demonstrating that RL can bootstrap the agentâ€™s progress over time.
    
- The following figure shows ([source](https://arxiv.org/abs/2405.20309)) theÂ _self-improvement loop_Â used in Patel et al. (2024), illustrating how the agent collects trajectories, filters low-quality outputs, fine-tunes itself, and iterates for continual improvement.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/WebArena.jpg)

- Synthetic-experience RL closes the loop for self-improving agents by letting a reasoning experience model synthesize adaptive rollouts and curricula matched to the current policy, yielding consistent gains in both synthetic and sim-to-real settings; theory further bounds the sim-to-real gap by reward-accuracy and domain-consistency errors, rather than strict pixel/state fidelity metrics (cf.Â [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025)).
    
- This iterative process typically follows these stages:
    
    1. **Data Collection:**Â The agent generates task trajectories by interacting with the environment.
    2. **Filtering and Evaluation:**Â The system automatically assesses each trajectory, discarding low-quality samples.
    3. **Fine-Tuning:**Â The agent is retrained using successful examples, effectively reinforcing good behavior.
    4. **Re-evaluation:**Â The improved agent is tested, and the cycle repeats.
- This form of continual self-improvement makes RL a key enabler for developing general-purpose, autonomous web and software agents.
    

### Environments for Reinforcement Learning in Modern Agents

- To support these learning processes, researchers have developed structured environments that simulate the complexity and variety of real-world digital interactions. One comprehensive framework isÂ [AgentGym](https://arxiv.org/abs/2406.04151)Â by Xi et al. (2024), which defines a unified interface for training and evaluating LLM-based agents across 14 environment typesâ€”ranging from academic reasoning and games to embodied navigation and web interaction.
    
- The following figure ([source](https://arxiv.org/abs/2406.04151)) shows theÂ _AgentGym framework_, illustrating the standardized environment interface, modular design, and integration of various environment types for LLM-driven agent training.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/AgentGym.jpg)

- In AgentGym, an agentâ€™s experience is modeled as a trajectory consisting of repeatedÂ _thoughtâ€“actionâ€“observation_Â cycles:
    
    Ï„=(h1,a1,o1,...,hT,aT)âˆ¼Ï€Î¸(Ï„|e,u)
    
    - whereÂ htÂ represents the agentâ€™s internal reasoning (its â€œthoughtâ€),Â atÂ the action it takes,Â otÂ the resulting observation, andÂ e,uÂ the environment and user prompt respectively.
- This approach bridges the symbolic reasoning capabilities of LLMs with the sequential decision-making framework of RL, forming the basis for modern interactive agents.

--------

## The Three Major Types of Reinforcement Learning Environments

- Modern RL environments for language-based and multimodal agents are generally organized into three broad categories. Each category captures a distinct interaction pattern and optimizes the agent for a different type of intelligence or capability.

### Single-Turn Environments (SingleTurnEnv)

- These environments are designed for tasks that require only a single inputâ€“output interaction, where the agent must produce one decisive response and then the environment resets. Examples include answering a question, solving a programming challenge, or completing a math problem.
    
- In this setting, the reward signal directly evaluates the quality of the single output. Training methods usually combine supervised fine-tuning with RL from human or synthetic feedback (RLHF). For instance, in coding problems or reasoning benchmarks, the agentâ€™s response can be automatically graded using execution correctness or symbolic validation. Such setups are ideal for optimizing precision and factual correctness in domains where each query is independent of the previous one.
    
- SingleTurnEnv tasks are computationally efficient to train because there is no need to maintain long-term memory or context. They are commonly used to bootstrap an agentâ€™s basic competencies before moving to more complex, multi-step environments.
    

### Tool-Use Environments (ToolEnv)

- Tool-use environments focus on enabling agents to perform reasoning and decision-making that involve invoking external toolsâ€”such as APIs, search engines, calculators, code interpreters, or databasesâ€”to complete a task. These environments simulate the agentâ€™s ability to extend its cognitive boundaries by interacting with external systems.
    
- InÂ [Tool Learning with Foundation Models](https://doi.org/10.1145/3704435)Â by Qin et al. (2024), the authors surveyed a wide range of approaches where foundation models learn to select, call, and integrate the outputs of external tools into their reasoning processes. This kind of training allows the model to perform symbolic computation, factual verification, and data retrieval in ways that pure text-based reasoning cannot.
    
- The following figure shows ([source](https://doi.org/10.1145/3704435)) theÂ _conceptual overview of tool learning with foundation models_, where models dynamically decide when and how to invoke tools such as web search and other APIs to solve complex problems.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/ToolLearningOverview.jpg)

- A related innovation isÂ [Tool-Augmented Reward Modeling](https://arxiv.org/abs/2310.01045)Â by Li et al. (2024), which enhanced RL reward models by giving them access to external APIs such as search engines or translation systems. This modification made reward models not only more accurate but also more interpretable, as each decision could be traced through explicit tool calls.
    
- The following figure ([source](https://arxiv.org/abs/2310.01045)) shows illustrates the pipeline of (a) Vanilla reward models (RMs); (b) Tool-augmented RMs, namely Themis; (c) RL via proximal policy optimization (PPO) on above RMs; (d) Examples of single or multiple tool use process in the proposed approach.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/Themis.jpg)

- Tool-use environments test the agentâ€™s ability to decideÂ _when_Â andÂ _how_Â to use a tool, what input arguments to provide, and how to interpret the returned results. This capability is crucial for building practical software assistants and web agents that interact with real systems.

### Multi-Turn, Sequential Environments (MultiTurnEnv)

- Multi-turn environments represent the most complex and realistic category of RL settings. In these environments, an agent engages in extended, multi-step interactions where each decision depends on the evolving context and memory of previous steps. Examples include navigating a website, writing and revising code iteratively, managing files on a computer, or executing multi-phase workflows such as online booking or document editing.
    
- Agents operating in these environments must reason about long-term goals, plan multiple actions in sequence, and interpret feedback dynamically. Systems such as WebArena, WebShop,Â [Agent Q](https://arxiv.org/abs/2408.07199)Â by Putta et al. (2024), andÂ [OpenWebVoyager](https://arxiv.org/abs/2410.19609)Â by He et al. (2024) exemplify this paradigm. They train agents through multi-step RL using trajectory-based feedback, where each complete sequence of actions and observations contributes to the learning signal.
    
- These environments are optimized for developing autonomy and adaptability. The agent must not only predict the next best action but also understand how that action contributes to the overall task objective. MultiTurnEnv scenarios are thus the closest analogs to real-world usage, making them essential for training general-purpose digital agents.
    

### Implications

- Agentic RL, which is the evolution of RL for agentsâ€”from single-turn tasks to tool-augmented reasoning and complex multi-turn workflowsâ€”reflects a progressive layering of capabilities. Each environment type plays a distinct role:
    
    - Single-turn environments emphasizeÂ _accuracy and efficiency_, teaching agents to produce correct, concise responses.
    - Tool-use environments focus onÂ _functional reasoning and integration_, giving agents the ability to extend their knowledge through computation and external APIs.
    - Multi-turn environments trainÂ _autonomy and planning_, enabling agents to navigate, adapt, and make decisions across extended sequences of interactions.
- Together, these environments form the backbone of modern RL for LLM-based and multimodal agents. They provide a structured pathway for training models that can perceive, reason, and actâ€”bringing us closer to general-purpose artificial intelligence capable of performing diverse tasks in real-world digital environments.

---

## Reinforcement Learning for Web and Computer-Use Agents

- A detailed discourse on RL can be found in ourÂ [Reinforcement Learning](https://aman.ai/primers/ai/reinforcement-learning)Â primer.

### Background: Policy-Based and Value-Based Methods

- At its core, RL employs two broad families of algorithmic approaches:
    
    - Value-based methods, which learn a value function (e.g.,Â Q(s,a)Â orÂ V(s)) that estimates the expected return of taking actionÂ aÂ in stateÂ sÂ (or being in stateÂ s).
    - Policy-based (or actor-critic) methods, which directly parameterize a policyÂ Ï€Î¸(aâˆ£s)Â and optimize its parametersÂ Î¸Â to maximize expected return
        
        J(Ï€Î¸)=ğ”¼Ï„âˆ¼Ï€Î¸[âˆ‘t=0TÎ³tR(st,at)]
        
- In modern agentic applications (web agents, computer-use agents), policyâ€based methods tend to dominate because the action space is large, discrete (e.g., â€œclick linkâ€, â€œinvoke APIâ€, â€œenter codeâ€), and policies must be expressive.
    
- One widely used algorithm is Proximal Policy Optimization (PPO)Â [Schulman et al. (2017)](https://arxiv.org/abs/1707.06347), which introduces a clipped surrogate objective to ensure stable updates and avoid large shifts in policy space.
    
- The surrogate objective can be expressed as:
    
    LCLIP(Î¸)=ğ”¼s,aâˆ¼Ï€Î¸old[min(rt(Î¸)At,clip(rt(Î¸),1âˆ’Ïµ,1+Ïµ)At)]
    
    - whereÂ rt(Î¸)=Ï€Î¸(atâˆ£st)Ï€Î¸old(atâˆ£st)Â andÂ AtÂ is the advantage estimate at timeÂ t.
- This ensures that the policy update does not diverge too far from the previous one while still improving expected return.
    

### Background: Process-Wise Rewards vs. Outcome-Based Rewards

- When designing RL systems for digital agents, one of the most consequential design choices lies inÂ _how_Â rewards are provided to the model.
    
- **Outcome-based rewards**Â give feedback only at the end of a taskâ€”for instance, a success/failure score after the agent completes a booking or answers a question. This is common inÂ _SingleTurnEnv_Â tasks and short workflows, where each interaction produces a single measurable outcome.
    
    - While simple, outcome-based rewards areÂ _sparse_, often forcing the agent to explore many possibilities before discovering actions that yield high return.
- **Process-wise (step-wise) rewards**, in contrast, provide incremental feedback during the task. In a web-navigation scenario, for example, the agent might receive positive reward for successfully clicking the correct link, partially filling a form, or retrieving relevant informationâ€”even before the final goal is achieved.
    
    - This approach is critical inÂ _MultiTurnEnv_Â orÂ _ToolEnv_Â setups where tasks span many steps. By assigning intermediate rewards, process-wise systems promoteÂ _shaped learning_â€”accelerating convergence and improving interpretability of the agentâ€™s learning process.
- Formally, if an episode runs forÂ TÂ steps, the total return under step-wise rewards is:
    
    Rt=âˆ‘k=tTÎ³kâˆ’trk
    
    - whereÂ rkÂ are per-step rewards. In outcome-based schemes,Â rk=0Â for allÂ k<T, andÂ rTÂ encodes task success. Choosing between these schemes depends on the environmentâ€™s complexity and availability of fine-grained performance metrics.
- For web agents, hybrid strategies are often used: process-wise signals derived fromÂ _browser state_Â (e.g., correct navigation, reduced error rate) combined with final outcome rewards (task completion). This hybridization reduces the high variance of pure outcome-based rewards while preserving the integrity of long-horizon objectives.
    

### Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)

- For web/computer-use agents built on LLMs or similar, one key method is RL from Human Feedback (RLHF). The standard RLHF pipeline is:
    
    1. Supervised fine-tune a base language model on promptâ€“response pairs.
    2. Collect human preference data: for each prompt, have humans rank multiple model responses (or choose preferred vs. non-preferred).
    3. Train a reward modelÂ rÏ•(x,y)Â to predict human preferences.
    4. Use an RL algorithm (often PPO) to optimize the policyÂ Ï€Î¸Â to maximise expected reward under the reward model, possibly adding KL-penalty to stay close to base model.
- For example, the survey articleÂ [Reinforcement Learning Enhanced LLMs: A Survey](https://arxiv.org/abs/2412.10400v1)Â provides an overview of this field.
    
- However, RLHF can be unstable, costly in compute, and sensitive to reward-model errors. Enter Direct Preference Optimization (DPO)Â [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290), which posits that one can skip the explicit reward model + RL loop and simply fine-tune the model directly to optimize human preference pairwise comparisons.
    
- The DPO loss in the pairwise case (winnerÂ yw, loserÂ yl) is approximately:
    
    îˆ¸DPO=âˆ’ğ”¼(x,yw,yl)[lnÏƒ(Î²lnÏ€Î¸(yw|x)Ï€ref(yw|x)âˆ’Î²lnÏ€Î¸(yl|x)Ï€ref(ylâˆ£x))]
    
    - whereÂ Ï€refÂ is the reference model (often the supervised fine-tuned model), andÂ Î²Â is a temperature-like constant.
- Some practical analyses (e.g.,Â [Is DPO Superior to PPO for LLM Alignment?](https://arxiv.org/abs/2404.10719)) compare PPO vs DPO in alignment tasks.
    

### Why These Algorithms Matter for Web & Computer-Use Agents

- When training agents that interact with the web or software systems (for example, clicking links, filling forms, issuing API calls), several factors make the choice of algorithm especially important:
    
    - Action spaces are large and heterogeneous (e.g., browser UI actions, tool function calls).
    - The reward signals may be sparse (e.g., task success only after many steps) or come from human annotation (in RLHF).
    - Policies must remain stable and avoid drift (especially when built on pretrained LLMs).
    - Computation cost is high (LLM inference, environment simulation), so sample efficiency matters.
- Thus:
    
    - Algorithms like PPO are well-suited because of their stability and simplicity (compared to e.g. TRPO) in high-dimensional policy spaces.
    - RLHF/DPO are relevant because many web-agents and computer-agents are aligned to human goals (helpfulness, correctness, safety) rather than just raw reward.
    - There is an increasing trend toward hybrid methods that combine search, planning (e.g., MCTS) plus RL fine-tuning for complex workflows.

### Key Equations

#### Advantage Estimation & Value Networks

- In actorâ€“critic variants (including PPO), we often learn a value functionÂ VÏˆ(s)Â to reduce variance:
    
    At=Rtâˆ’VÏˆ(st)Rt=âˆ‘k=0âˆÎ³krt+k
    
    - **where:**
        
        - At: theÂ **advantage estimate**Â at timestepÂ t, measuring how much better an action performed compared to the policyâ€™s expected performance.
        - Rt: theÂ **discounted return**, or the total expected future reward from timeÂ t.
        - Î³: theÂ **discount factor**Â (0<Î³â‰¤1), controlling how much future rewards are valued compared to immediate ones.
        - rt+k: theÂ **immediate reward**Â received at stepÂ t+k.
        - VÏˆ(st): theÂ **criticâ€™s value estimate**Â for stateÂ st, parameterized byÂ Ïˆ, representing the expected return from that state under the current policy.
- The update for the critic aims to minimize:
    
    Lvalue(Ïˆ)=ğ”¼stâˆ¼Ï€[(VÏˆ(st)âˆ’Rt)2]
    
    - **where:**
        
        - Lvalue(Ïˆ): theÂ **value loss**, quantifying how far the criticâ€™s predictions are from the actual returns.
        - ğ”¼stâˆ¼Ï€[â‹…]: theÂ **expectation**Â over statesÂ stÂ sampled from the current policy (\pi).
        - The squared termÂ (VÏˆ(st)âˆ’Rt)2: penalizes inaccurate value predictions, guiding the critic to estimate returns more accurately.

#### KL-penalty / Trust Region

- Some RLHF implementations add a penalty to keep the new policy close to the supervised model:
    
    LKL(Î¸)=Î²â‹…ğ”¼x,yâˆ¼Ï€[logÏ€Î¸(y|x)Ï€SFT(y|x)]
    
    - **where:**
        
        - LKL(Î¸): theÂ **KL-divergence loss**, which penalizes the new policyÂ Ï€Î¸Â if it deviates too far from the supervised fine-tuned (SFT) reference policyÂ Ï€SFT.
        - Î²: aÂ **scaling coefficient**Â controlling the strength of this regularization; largerÂ Î²Â enforces tighter adherence to the reference model.
        - ğ”¼x,yâˆ¼Ï€[â‹…]: theÂ **expectation**Â over sampled inputâ€“output pairs from the current policyâ€™s distribution.
        - Ï€Î¸(yâˆ£x): theÂ **current policyâ€™s probability**Â of generating outputÂ yÂ given inputÂ x.
        - Ï€SFT(yâˆ£x): theÂ **reference policyâ€™s probability**, often from the supervised model used before RL fine-tuning.
    - â€¦ so the total objective may combine PPOâ€™s surrogate loss with this KL penalty (and possibly an entropy bonus) to balance exploration, stability, and fidelity to the base model.*
        

#### Preference Optimization (DPO)

- As shown above, DPO reframes alignment as maximising the probability that the fine-tuned model ranks preferred outputs higher than non-preferred ones, bypassing the explicit RL loop.

#### Sample Efficiency & Off-policy Corrections

- For agents interacting with web or tools where running many episodes is costly, sample efficiency matters. Off-policy methods (e.g., experience replay) or offline RL variants (e.g.,Â [A Survey on Offline Reinforcement Learning](https://arxiv.org/abs/2203.01387)Â by Kumar et al. (2022)) may become relevant.

### Agentic Reinforcement Learning Via Policy Optimization

- InÂ **policy optimization**, the agent learns from a unified reward function that draws its signal fromÂ **one or more available sources**â€”such asÂ **rule-based rewards**, a scalar reward output from aÂ **learned reward model**, or another model that is proficient at grading the task (such as anÂ **LLM-as-a-Judge**). Each policy update seeks to maximize the expected cumulative return:
    
    J(Î¸)=ğ”¼Ï€Î¸[âˆ‘tÎ³trt]
    
    - whereÂ rtÂ represents whichever reward signal is active for the current environment or training regime. In some settings, this may be a purely rule-based signal derived from measurable events (like navigation completions, form submissions, or file creations). In others, the reward may come from a trained modelÂ RÏ•(ot,at,ot+1)Â that generalizes human preference data, or from an external proficient verifier (typically a larger model) such as an LLM-as-a-Judge.
- These components areÂ **modular and optional**â€”only one or several may be active at any time. The optimization loop remains identical regardless of source: the policy simply maximizes whichever scalar feedbackÂ rtÂ it receives. This flexible design allows the same framework to operate with deterministic, model-based, or semantic reward supervision, depending on task complexity, available annotations, and desired interpretability.
    
- **Rule-based rewards**Â form the foundation of this framework, providing deterministic, auditable feedback grounded inÂ **explicit environment transitions and observable state changes**. As demonstrated inÂ [DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models](https://arxiv.org/abs/2501.12948)Â by Gao et al. (2025), rule-based rewards yield transparent and stable optimization signals that are resistant to reward hacking and reduce reliance on noisy human annotation. In the context of computer-use agents, rule-based mechanisms correspond directly toÂ **verifiable milestones**Â in user interaction sequencesâ€”for example:
    
    - InÂ **web navigation**, detecting a URL transition, page load completion, or DOM state change (`NavigationCompleted`,Â `DOMContentLoaded`).
    - InÂ **form interaction**, observing DOM model deltas that indicate fields were populated, validation succeeded, or a â€œSubmitâ€ action triggered a confirmation dialog.
    - InÂ **file handling/artifact generation**, confirming the creation or modification of a file within the sandbox (e.g., registering successful exports such asÂ `.csv`,Â `.pdf`, orÂ `.png`Â outputs following specific actions).
    - InÂ **application state transitions**, monitoring focus changes, dialog closures, or process launches via OS accessibility APIs.
    - InÂ **UI interaction success**, verifying that a button, link, or menu item was activated and that the resulting accessibility tree or visual layout changed accordingly.
    - These measurable indicators serve as theÂ **atomic verification layer**Â of the reward system, ensuring that each environment step corresponds to reproducible, auditable progress signals without requiring human intervention.
- To generalize beyond fixed rules, aÂ **trainable reward model**Â RÏ•(ot,at,ot+1)Â can be introduced. This model is trained onÂ **human-labeled or preference-ranked trajectories**, similar to the reward modeling stage in PPO-based RLHF pipelines. Once trained,Â RÏ•Â predicts scalar reward signals that approximate human preferences for unseen tasks or ambiguous states. It operates faster and more consistently than a generative LLM-as-a-Judge (which can be implemented as a Verifier Agent), while maintaining semantic fidelity to human supervision.
    
- TheÂ **three-tier reward hierarchy**Â thus becomes:
    
    1. **Rule-based rewards (preferred default):**Â deterministic, event-driven, and auditable (no reward hacking).
    2. **Learned, discriminative reward model (RÏ•):**Â generalizes human feedback for subtle, unstructured, or context-dependent goals where rules are insufficient.
    3. **Generative reward model (e.g., LLM-as-a-Judge):**Â invoked only when both rule-based detectors andÂ RÏ•Â cannot confidently score outcomes (e.g., for semantic reasoning, style alignment, or multimodal understanding). This is similar to howÂ [DeepSeek-R1](https://aman.ai/primers/ai/deepseek-R1)Â uses a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment during the rejection sampling stage for reasoning data.
- This architecture ensures that theÂ **primary training flow remains rule-grounded and verifiable**, while allowing smooth fallback to preference-aligned modeling when necessary. The hybrid setupâ€”selectively combining rule-based rewards, learned reward estimation, and verifier agent interventionâ€”balancesÂ **scalability, auditability, and semantic depth**Â across diverse computer-use tasks.
    
- During training, theÂ **reward selection and routing process**Â is adaptive. When deterministic milestone detectors emit valid scores, they take precedence as the most reliable supervision. If the environment lacks such instrumentation, the learned modelÂ RÏ•Â dynamically provides substitute scalar feedback inferred from trajectory context. In the rare case that both mechanisms yield low confidence, the system escalates to the Verifier Agent for semantic adjudication. This cascading reward flow ensures the agent always receives a stable optimization signalâ€”grounded when possible, inferred when necessary, and judged when ambiguity demands interpretive reasoning.
    

#### Milestone-Based Reward System

- AnyÂ **reward formulation**â€”whether deterministic, learned, or model-evaluatedâ€”can be decomposed into a sequence ofÂ **milestones or checkpoints**Â that represent measurable progress toward the task goal. Each milestone corresponds to a verifiable state transition, UI event, or observable change in the environment, providing interpretable signals even within complex or hierarchical workflows. In practice, a reward function can therefore be aÂ **composite of multiple sources**:Â **rule-based rewards**, scalar predictions from aÂ **learned, discriminative reward model**, or aÂ **generative model**Â that is proficient at grading the task, such as anÂ **LLM-as-a-Judge**.
    
- In general,Â **rule-based rewards**Â are preferred because they areÂ **deterministic, easy to verify, and resistant to reward hacking**, consistent with the design principles demonstrated in theÂ [DeepSeek-R1](https://arxiv.org/abs/2501.12948)Â framework by Gao et al. (2025). These rewards are derived fromÂ **concrete, environment-observable events**â€”such as file creation, DOM or AX tree changes, navigation completions, or dialog confirmationsâ€”and can be validated directly through structured logs and system hooks. Their reproducibility and transparency make them ideal for large-scale, self-contained policy optimization loops, where interpretability and auditability are crucial.
    
- In this system, theÂ **rule-based layer**Â serves as the foundational signal generator for all common computer-use tasks. It captures events such as:
    
    - File downloads or artifact creation
    - Successful form submissions or dialog confirmations
    - UI transitions, window focus changes, or navigation completions
    - Text field population or data transfer between applications
    - Screenshot or state deltas indicating successful subgoal completion
        
    - These reward components directly populate the tupleÂ (ot,at,rt,ot+1)Â used by the policy optimizer for learning stable, interpretable control policies. Each milestone event contributes either a discrete tick or a weighted scalar toward cumulative progress.
- However, not all task goals can be described exhaustively through deterministic rules. To extend coverage, the architecture includes aÂ **learned reward model**Â RÏ•(ot,at,ot+1)Â trained specifically onÂ **human preferences or ranked trajectories**.
    
    - This model generalizes beyond hand-engineered events to scoreÂ **semantic correctness, contextual relevance, and user-aligned outcomes**.
    - RÏ•Â can be continuously fine-tuned as new preference data accumulates, adapting reward shaping dynamically to novel workflows or unseen UIs.
    - During training, the optimizer consumes a blended reward signal that can combine multiple sources:
        
        rÌƒÂ t=Î±r(rule)t+Î²RÏ•(ot,at,ot+1)+Î³r(judge)t
        
        - whereÂ Î±,Î²,Î³âˆˆ[0,1]Â represent trust weights for deterministic, learned, and model-evaluated components respectively, withÂ Î±+Î²+Î³=1.
- In cases where both rule-based detectors and the learned reward model fail to provide a confident or interpretable score, aÂ **generative model (such as an LLM-as-a-Judge)**Â may be selectively invoked. This verifier acts as a high-capacity,Â _LLM-as-a-Judge_Â module that semantically evaluates whether the observed trajectory satisfies implicit or fuzzy success criteria. Its role parallels that of a preference model but operates at runtime for difficult or open-ended cases.
    
- Scenarios where rule-based and model-based scoring may be insufficientâ€”and thus require a Verifier Agentâ€”include:
    
    - **Subjective or semantic correctness:**Â determining if a written summary or chart interpretation matches the instruction intent.
    - **Cross-context validation:**Â verifying that data copied from a spreadsheet was correctly inserted into a report or email draft.
    - **Goal inference under ambiguity:**Â tasks like â€œopen the latest invoice,â€ where the target must be inferred dynamically.
    - **Complex recovery handling:**Â identifying whether the system has correctly recovered from an unintended dialog or misclick.
    - **Language or multimodal alignment:**Â verifying tone, structure, or layout across applications.
- TheÂ **reward system hierarchy**Â therefore consists of three complementary and optionally composable layers:
    
    1. **Rule-based rewards**: deterministic, verifiable, and fully auditable signals derived from concrete milestones (default and preferred).
        
    2. **Learned, discriminative reward model (RÏ•)**: trained on human preferences to generalize beyond explicit rules and produce scalar feedback for unstructured tasks.
        
    3. **Generative reward model (e.g., LLM-as-a-Judge)**: semantic fallback for nuanced, subjective, or multimodal evaluation where neither rules nor learned models suffice. This is similar to howÂ [DeepSeek-R1](https://aman.ai/primers/ai/deepseek-R1)Â uses a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment during the rejection sampling stage for reasoning data.
        
- Together, these layers enableÂ **robust, explainable, and modular reward shaping**. Any reward function within the system can thus be expressed as aÂ **milestone-weighted combination**Â of deterministic, learned, and interpretive componentsâ€”ensuring scalability, transparency, and semantic alignment across all computer-use reinforcement learning setups.
    

##### Example Milestones by Task Category

1. **Web Navigation and Data Extraction**
    
    - **Milestone:**Â Target URL loaded successfully (`NavigationCompleted`Â event).Â _Reward:_Â +0.25
    - **Milestone:**Â Element with specific role/name detected (e.g., â€œReports Tableâ€ or â€œDashboard Summaryâ€).Â _Reward:_Â +0.25
    - **Milestone:**Â Successful data scrape or DOM text retrieval logged.Â _Reward:_Â +0.5
2. **Form Interaction**
    
    - **Milestone:**Â Input field focused and filled (text pattern matched).Â _Reward:_Â +0.2
    - **Milestone:**Â Submit button clicked and confirmation dialog appears.Â _Reward:_Â +0.3
    - **Milestone:**Â Success banner or confirmation element detected.Â _Reward:_Â +0.5
3. **File Handling and Downloads**
    
    - **Milestone:**Â File creation event observed inÂ `/Downloads`.Â _Reward:_Â +1.0
    - **Milestone:**Â File hash or extension matches expectation (e.g.,Â `.csv`,Â `.pdf`).Â _Reward:_Â +0.5
    - **Milestone:**Â Directory updated without error.Â _Reward:_Â +0.25
4. **Email or Document Workflows**
    
    - **Milestone:**Â Email editor loaded and populated with recipient and subject.Â _Reward:_Â +0.25
    - **Milestone:**Â Attachment successfully added.Â _Reward:_Â +0.5
    - **Milestone:**Â Message successfully sent (UI confirmation or state change).Â _Reward:_Â +1.0
5. **System Configuration and Settings**
    
    - **Milestone:**Â Settings panel opened (window title match).Â _Reward:_Â +0.25
    - **Milestone:**Â Checkbox or toggle successfully modified (UIA/AX event).Â _Reward:_Â +0.25
    - **Milestone:**Â â€œChanges Savedâ€ notification observed.Â _Reward:_Â +0.5
6. **Search and Information Retrieval**
    
    - **Milestone:**Â Query field populated with correct term.Â _Reward:_Â +0.25
    - **Milestone:**Â Search executed and result list rendered.Â _Reward:_Â +0.5
    - **Milestone:**Â Target entry clicked or opened.Â _Reward:_Â +0.5

#### Example Reward Function

- Each environment step returns a shaped reward based on concrete, verifiable milestones. Instead of relying on subjective evaluators, the reward function is composed of measurable subcomponents derived from observable state transitions, UI changes, and artifact events.
    
- At stepÂ t, the total reward is given by:
    
    rt=wnavr(nav)t+wUIr(UI)t+wformr(form)t+wfiler(file)t+wgoalr(goal)t
    
    - where each component represents a verifiable milestone type:
- r(nav)t: Navigation progress reward â€” triggered by measurable page transitions such asÂ `NavigationCompleted`Â events, URL match, or window title change.
    
    r(nav)t=ğŸ™{urltâ‰ urltâˆ’1}
    
- r(UI)t: UI element interaction reward â€” triggered when a UI control with a matching role or label is successfully targeted (e.g., a button click or field focus event).
    
    r(UI)t=ğŸ™{clicked(role,name)=expected(role,name)
    
- r(form)t: Form completion reward â€” triggered when an editable control is filled and validated (value non-empty, regex match, or field count).
    
    r(form)t=NfilledNexpected
    
- r(file)t: File-handling reward â€” derived from filesystem or artifact deltas (e.g., a newÂ `.csv`,Â `.pdf`, orÂ `.json`Â created).
    
    r(file)t=ğŸ™{âˆƒfâˆˆîˆ­t:f.event=''created"}
    
- r(goal)t: Task completion reward â€” triggered by a high-level terminal condition, such as detection of success text, matched hash, or closed loop condition.
    
    r(goal)t=ğŸ™{goal_verified(ot)}
    
- The weightsÂ wnav,wUI,wform,wfile,wgoalÂ balance short-term shaping with terminal rewards, typically normalized so that:
    

âˆ‘iwi=1{wgoalâ‰¥wfileâ‰¥wUI}

#### Example Instantiation

|**Component**|**Description**|**Weight**|**Range**|
|---|---|---|---|
|r(nav)t|Successful navigation|0.1|0,1|
|r(UI)t|Correct element interaction|0.2|0,1|
|r(form)t|Partial form completion|0.2|[0,1]|
|r(file)t|Artifact creation (e.g., download)|0.3|0,1|
|r(goal)t|Verified task completion|0.2|0,1|

- This formulation ensuresÂ **all reward components are physically measurable**â€”no human labels are required. Each event corresponds to structured data observable through CDP logs, accessibility APIs, or filesystem monitors, making it reproducible and auditable across training runs.

### Agent Training Pipeline

- A typical pipeline to train a web or computer-use agent might follow:
    
    1. Pre-train the model (e.g., a large language model) via supervised learning.
    2. Optionally fine-tune on domain-specific prompts (supervised fine-tuning, SFT).
    3. Collect human preference data (rankings of model responses).
    4. Choose alignment method:
        - **RLHF:**Â train reward modelÂ â†’Â use PPO (or other RL algorithm) to optimise policy.
        - **DPO:**Â directly fine-tune model on preference data (skipping RL loop).
    5. Launch agent into simulated environment (SingleTurnEnv, ToolEnv, MultiTurnEnv).
    6. Run RL policy optimisation in the environment: sample trajectories, estimate advantages/returns, update policy using PPO or variants.
    7. Periodically evaluate and filter trajectories, adjust reward shaping, fine-tune further for tool-use or long-horizon behaviours.
- By selecting algorithms appropriate for the interaction type (single turn vs tool vs multi-turn), one can tailor the training for efficiency, stability, and scalability.

----

## Environment Interaction Patterns for Agent Design

### Environment Design in Reinforcement Learning for Agents

- Modern RL environments for web and computer-use agents are designed to capture the diversity and complexity of real-world interactions while maintaining enough structure for stable learning. Unlike classical RL benchmarks (e.g., Atari or MuJoCo), these environments involve language, symbolic reasoning, tool use, and visual perception.
    
- They are not simply â€œgamesâ€ or â€œcontrol systemsâ€ butÂ **interactive ecosystems**Â that test an agentâ€™s ability to perceive context, reason over multi-step processes, and execute goal-directed actions.
    
- To support the training of increasingly capable language-based and multimodal agents, recent frameworks such asÂ [AgentGym](https://arxiv.org/abs/2406.04151)Â by Xi et al. (2024) have introduced a unified taxonomy of environments, each corresponding to a particularÂ _interaction modality_.
    
- At the highest level, these can be grouped into three archetypes:
    
    1. **Single-Turn Environments**, designed for one-shot problem solving and precision reasoning.
    2. **Tool-Use Environments**, optimized for integrating external functions, APIs, or computation tools.
    3. **Multi-Turn Sequential Environments**, which simulate complex, long-horizon workflows requiring memory, planning, and context adaptation.
- Each environment type not only changes how agents act but also howÂ _rewards, policies, and credit assignment mechanisms_Â must be designed to drive meaningful learning.
    

### Single-Turn Environments (SingleTurnEnv)

- **Single-turn environments**Â represent the simplest and most direct form of RL training. In this setup, each episode consists of a single interaction: the agent receives an input (prompt, question, or task description), produces one output (answer, code snippet, or solution), and immediately receives feedback.
    
- These environments are ideal for optimizing agents that must produce highly accurate outputs in one stepâ€”such as coding assistants, math solvers, or document completion systems.
    
- **Examples:**
    - Code completion and debugging tasks inÂ _CodeRL_Â ([CodeRL: Mastering Code Generation through RL](https://arxiv.org/abs/2207.01780)Â by Le et al., 2022).
    - Question-answering benchmarks like WebGPT ([WebGPT](https://arxiv.org/abs/2112.09332)Â by Nakano et al., 2022)), where the agentâ€™s final response is scored based on correctness and citation quality.
- **Reward Structure:**Â Single-turn environments typically useÂ _outcome-based rewards_Â rather than step-wise feedback because there is only one output to evaluate. For example:
    
    - In a coding task,Â r=+1Â if the code executes successfully, andÂ r=0Â otherwise.
    - In a factual QA task,Â rÂ may represent an F1 score or BLEU score.
- Formally, the optimization objective reduces to:
    
    J(Ï€)=ğ”¼xâˆ¼D,yâˆ¼Ï€(â‹…|x)[R(x,y)]
    
    - whereÂ R(x,y)Â is the final outcome reward.
- While simple, such environments serve as critical pretraining stages, allowing models to build domain accuracy before engaging in multi-step reasoning or tool-use.

### Tool-Use Environments (ToolEnv)

- **Tool-use environments**Â introduce an additional layer of reasoning: instead of solving a task in one step, the agent must decide when and how to invoke external tools. Tools may include:
    
    - API calls (e.g., search, translation, or computation),
    - external functions (e.g., symbolic calculators, Python interpreters), or
    - system-level commands (e.g., file access, browser manipulation).
- The core challenge isÂ _tool orchestration_â€”learning when to rely on external computation versus internal reasoning. For instance, in a data retrieval task, the agent might issue an API query, parse results, and compose a natural-language summary.
    
- **Reward Structure**:
    - In ToolEnv, bothÂ _process-wise_Â andÂ _outcome-based_Â rewards are valuable:
        
        - _Step-wise rewards_Â can score the accuracy or efficiency of each tool invocation (e.g., correct API parameters or valid JSON structure).
        - _Outcome-based rewards_Â measure task completion or user satisfaction.
    - The combined reward signal is often expressed as:
        
        Rt=Î±rprocess+(1âˆ’Î±)routcome,
        
        - whereÂ Î±Â controls the balance between short-term and final goal feedback.
- **Algorithmic Approaches**: Because the action space now includes function arguments and results, methods like policy gradient with structured action representations, hierarchical RL, or model-based planning (e.g., MCTS as inÂ [Agent Q](https://arxiv.org/abs/2408.07199)Â by Putta et al., 2024) become necessary.
    
- [Tool Learning with Foundation Models](https://doi.org/10.1145/3704435)Â by Qin et al. (2024) provides a comprehensive survey of how foundation models learn to invoke external tools to augment their reasoning capabilities.

### Multi-Turn Sequential Environments (MultiTurnEnv)

- **Multi-turn environments**Â simulate complex, multi-step workflows where each decision influences future context. These environments are designed for agents that need to plan, adapt, and maintain consistency across many turns of interaction.
    
- **Examples:**
    
    - Web navigation agents such asÂ [OpenWebVoyager](https://arxiv.org/abs/2410.19609)Â by He et al. (2024), where the agent browses, clicks, and fills forms over multiple steps.
    - Software operation tasks like system configuration, spreadsheet editing, or email management.
    - Interactive tutoring and dialogue planning systems.
- **Reward Structure:**
    - In MultiTurnEnv setups, pure outcome-based rewards (success/failure) can causeÂ _credit assignment problems_Â because the agent receives feedback only after many steps. To address this, researchers combineÂ **process-wise rewards**â€”for subgoal completion, error reduction, or partial correctnessâ€”withÂ **final outcome rewards**.
        
    - Formally, the expected return in such environments can be represented as:
        
        J(Ï€)=ğ”¼[âˆ‘t=1TÎ³t(rprocesst+Î»,routcomeT)]
        
        - whereÂ Î»Â balances intermediate and terminal objectives.
    - In OpenWebVoyager, for example, each sub-action (like opening the correct link) contributes partial reward, guiding the agent toward long-term success while preventing divergence from optimal sequences.
        
- **Learning Dynamics:**Â Training in MultiTurnEnv requires:
    
    - Long-horizon credit assignment via temporal-difference learning or advantage estimation.
    - Hierarchical RL for decomposing tasks into sub-policies.
    - Trajectory filtering and reward shaping to combat sparse or noisy signals.

### Designing Rewards for Complex Agent Environments

- Reward engineering is arguably the most critical part of environment design. Different environment types benefit from distinct reward strategies:

|**Environment Type**|**Reward Type**|**Typical Signal**|**Optimization Goal**|
|---|---|---|---|
|SingleTurnEnv|Outcome-based|Correctness, BLEU/F1 score|Precision and factual accuracy|
|ToolEnv|Hybrid (step-wise + outcome)|Tool correctness, API success|Functional reasoning, tool reliability|
|MultiTurnEnv|Step-wise + delayed outcome|Subgoal completion, navigation success|Long-horizon planning, autonomy|

- Balancing process-wise and outcome-based rewards ensures that agents receiveÂ _dense feedback for learning efficiency_Â while still optimizing towardÂ _global objectives_Â like success rate or user satisfaction.

### Implications for Agent Design and Evaluation

- Each environment type imposes unique requirements on model architecture, reward shaping, and evaluation metrics.
    
    1. **SingleTurnEnv**Â favors compact policies and fast evaluation loops, suitable for smaller RL batches or DPO-based optimization.
    2. **ToolEnv**Â requires compositional reasoning and structured memory to maintain tool-call histories and argument dependencies.
    3. **MultiTurnEnv**Â demands long-context modeling, world-state tracking, and temporal credit assignment across potentially hundreds of steps.
- Evaluation metrics vary accordingly:
    
    - _Single-turn_: Accuracy, F1, pass rate.
    - _Tool-use_: Tool-call correctness, latency, success ratio.
    - _Multi-turn_: Task completion rate, cumulative reward, consistency, and planning efficiency.
- When integrated properly, these environment classes form aÂ **curriculum**Â for RL-based agent development: agents begin with static, outcome-driven reasoning (SingleTurnEnv), progress to dynamic, tool-integrated reasoning (ToolEnv), and culminate in fully autonomous multi-turn reasoning (MultiTurnEnv).
    

### Comparative Analysis

- Environment design is the foundation on which modern RL agents learn to generalize and act. The interplay betweenÂ **interaction modality**,Â **reward granularity**, andÂ **algorithmic strategy**Â determines not only how fast an agent learns but also what kinds of intelligence it develops.
    
    - Single-turn environments teachÂ _accuracy_.
    - Tool-use environments teachÂ _functional reasoning_.
    - Multi-turn environments teachÂ _autonomy and adaptability_.
- Together, they form a progression of increasing sophisticationâ€”mirroring the cognitive layers of reasoning, planning, and execution. RL algorithms like PPO and DPO serve as the connective tissue between these layers, transforming static pretrained models into active, evolving agents capable of navigating and operating within real digital ecosystems.