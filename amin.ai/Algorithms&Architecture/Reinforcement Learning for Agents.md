
  
- The following figure shows ([source](https://doi.org/10.1145/3704435)) theÂ _conceptual overview of tool learning with foundation models_, where models dynamically decide when and how to invoke tools such as web search and other APIs to solve complex problems.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/ToolLearningOverview.jpg)

- A related innovation isÂ [Tool-Augmented Reward Modeling](https://arxiv.org/abs/2310.01045)Â by Li et al. (2024), which enhanced RL reward models by giving them access to external APIs such as search engines or translation systems. This modification made reward models not only more accurate but also more interpretable, as each decision could be traced through explicit tool calls.
    
- The following figure ([source](https://arxiv.org/abs/2310.01045)) shows illustrates the pipeline of (a) Vanilla reward models (RMs); (b) Tool-augmented RMs, namely Themis; (c) RL via proximal policy optimization (PPO) on above RMs; (d) Examples of single or multiple tool use process in the proposed approach.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/Themis.jpg)

- Tool-use environments test the agentâ€™s ability to decideÂ _when_Â andÂ _how_Â to use a tool, what input arguments to provide, and how to interpret the returned results. This capability is crucial for building practical software assistants and web agents that interact with real systems.

### Multi-Turn, Sequential Environments (MultiTurnEnv)

- Multi-turn environments represent the most complex and realistic category of RL settings. In these environments, an agent engages in extended, multi-step interactions where each decision depends on the evolving context and memory of previous steps. Examples include navigating a website, writing and revising code iteratively, managing files on a computer, or executing multi-phase workflows such as online booking or document editing.
    
- Agents operating in these environments must reason about long-term goals, plan multiple actions in sequence, and interpret feedback dynamically. Systems such as WebArena, WebShop,Â [Agent Q](https://arxiv.org/abs/2408.07199)Â by Putta et al. (2024), andÂ [OpenWebVoyager](https://arxiv.org/abs/2410.19609)Â by He et al. (2024) exemplify this paradigm. They train agents through multi-step RL using trajectory-based feedback, where each complete sequence of actions and observations contributes to the learning signal.
    
- These environments are optimized for developing autonomy and adaptability. The agent must not only predict the next best action but also understand how that action contributes to the overall task objective. MultiTurnEnv scenarios are thus the closest analogs to real-world usage, making them essential for training general-purpose digital agents.
    

### Implications

- Agentic RL, which is the evolution of RL for agentsâ€”from single-turn tasks to tool-augmented reasoning and complex multi-turn workflowsâ€”reflects a progressive layering of capabilities. Each environment type plays a distinct role:
    
    - Single-turn environments emphasizeÂ _accuracy and efficiency_, teaching agents to produce correct, concise responses.
    - Tool-use environments focus onÂ _functional reasoning and integration_, giving agents the ability to extend their knowledge through computation and external APIs.
    - Multi-turn environments trainÂ _autonomy and planning_, enabling agents to navigate, adapt, and make decisions across extended sequences of interactions.
- Together, these environments form the backbone of modern RL for LLM-based and multimodal agents. They provide a structured pathway for training models that can perceive, reason, and actâ€”bringing us closer to general-purpose artificial intelligence capable of performing diverse tasks in real-world digital environments.

---

## Reinforcement Learning for Web and Computer-Use Agents

- A detailed discourse on RL can be found in ourÂ [Reinforcement Learning](https://aman.ai/primers/ai/reinforcement-learning)Â primer.

### Background: Policy-Based and Value-Based Methods

- At its core, RL employs two broad families of algorithmic approaches:
    
    - Value-based methods, which learn a value function (e.g.,Â Q(s,a)Â orÂ V(s)) that estimates the expected return of taking actionÂ aÂ in stateÂ sÂ (or being in stateÂ s).
    - Policy-based (or actor-critic) methods, which directly parameterize a policyÂ Ï€Î¸(aâˆ£s)Â and optimize its parametersÂ Î¸Â to maximize expected return
        
        J(Ï€Î¸)=ğ”¼Ï„âˆ¼Ï€Î¸[âˆ‘t=0TÎ³tR(st,at)]
        
- In modern agentic applications (web agents, computer-use agents), policyâ€based methods tend to dominate because the action space is large, discrete (e.g., â€œclick linkâ€, â€œinvoke APIâ€, â€œenter codeâ€), and policies must be expressive.
    
- One widely used algorithm is Proximal Policy Optimization (PPO)Â [Schulman et al. (2017)](https://arxiv.org/abs/1707.06347), which introduces a clipped surrogate objective to ensure stable updates and avoid large shifts in policy space.
    
- The surrogate objective can be expressed as:
    
    LCLIP(Î¸)=ğ”¼s,aâˆ¼Ï€Î¸old[min(rt(Î¸)At,clip(rt(Î¸),1âˆ’Ïµ,1+Ïµ)At)]
    
    - whereÂ rt(Î¸)=Ï€Î¸(atâˆ£st)Ï€Î¸old(atâˆ£st)Â andÂ AtÂ is the advantage estimate at timeÂ t.
- This ensures that the policy update does not diverge too far from the previous one while still improving expected return.
    

### Background: Process-Wise Rewards vs. Outcome-Based Rewards

- When designing RL systems for digital agents, one of the most consequential design choices lies inÂ _how_Â rewards are provided to the model.
    
- **Outcome-based rewards**Â give feedback only at the end of a taskâ€”for instance, a success/failure score after the agent completes a booking or answers a question. This is common inÂ _SingleTurnEnv_Â tasks and short workflows, where each interaction produces a single measurable outcome.
    
    - While simple, outcome-based rewards areÂ _sparse_, often forcing the agent to explore many possibilities before discovering actions that yield high return.
- **Process-wise (step-wise) rewards**, in contrast, provide incremental feedback during the task. In a web-navigation scenario, for example, the agent might receive positive reward for successfully clicking the correct link, partially filling a form, or retrieving relevant informationâ€”even before the final goal is achieved.
    
    - This approach is critical inÂ _MultiTurnEnv_Â orÂ _ToolEnv_Â setups where tasks span many steps. By assigning intermediate rewards, process-wise systems promoteÂ _shaped learning_â€”accelerating convergence and improving interpretability of the agentâ€™s learning process.
- Formally, if an episode runs forÂ TÂ steps, the total return under step-wise rewards is:
    
    Rt=âˆ‘k=tTÎ³kâˆ’trk
    
    - whereÂ rkÂ are per-step rewards. In outcome-based schemes,Â rk=0Â for allÂ k<T, andÂ rTÂ encodes task success. Choosing between these schemes depends on the environmentâ€™s complexity and availability of fine-grained performance metrics.
- For web agents, hybrid strategies are often used: process-wise signals derived fromÂ _browser state_Â (e.g., correct navigation, reduced error rate) combined with final outcome rewards (task completion). This hybridization reduces the high variance of pure outcome-based rewards while preserving the integrity of long-horizon objectives.
    

### Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)

- For web/computer-use agents built on LLMs or similar, one key method is RL from Human Feedback (RLHF). The standard RLHF pipeline is:
    
    1. Supervised fine-tune a base language model on promptâ€“response pairs.
    2. Collect human preference data: for each prompt, have humans rank multiple model responses (or choose preferred vs. non-preferred).
    3. Train a reward modelÂ rÏ•(x,y)Â to predict human preferences.
    4. Use an RL algorithm (often PPO) to optimize the policyÂ Ï€Î¸Â to maximise expected reward under the reward model, possibly adding KL-penalty to stay close to base model.
- For example, the survey articleÂ [Reinforcement Learning Enhanced LLMs: A Survey](https://arxiv.org/abs/2412.10400v1)Â provides an overview of this field.
    
- However, RLHF can be unstable, costly in compute, and sensitive to reward-model errors. Enter Direct Preference Optimization (DPO)Â [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290), which posits that one can skip the explicit reward model + RL loop and simply fine-tune the model directly to optimize human preference pairwise comparisons.
    
- The DPO loss in the pairwise case (winnerÂ yw, loserÂ yl) is approximately:
    
    îˆ¸DPO=âˆ’ğ”¼(x,yw,yl)[lnÏƒ(Î²lnÏ€Î¸(yw|x)Ï€ref(yw|x)âˆ’Î²lnÏ€Î¸(yl|x)Ï€ref(ylâˆ£x))]
    
    - whereÂ Ï€refÂ is the reference model (often the supervised fine-tuned model), andÂ Î²Â is a temperature-like constant.
- Some practical analyses (e.g.,Â [Is DPO Superior to PPO for LLM Alignment?](https://arxiv.org/abs/2404.10719)) compare PPO vs DPO in alignment tasks.
    

### Why These Algorithms Matter for Web & Computer-Use Agents

- When training agents that interact with the web or software systems (for example, clicking links, filling forms, issuing API calls), several factors make the choice of algorithm especially important:
    
    - Action spaces are large and heterogeneous (e.g., browser UI actions, tool function calls).
    - The reward signals may be sparse (e.g., task success only after many steps) or come from human annotation (in RLHF).
    - Policies must remain stable and avoid drift (especially when built on pretrained LLMs).
    - Computation cost is high (LLM inference, environment simulation), so sample efficiency matters.
- Thus:
    
    - Algorithms like PPO are well-suited because of their stability and simplicity (compared to e.g. TRPO) in high-dimensional policy spaces.
    - RLHF/DPO are relevant because many web-agents and computer-agents are aligned to human goals (helpfulness, correctness, safety) rather than just raw reward.
    - There is an increasing trend toward hybrid methods that combine search, planning (e.g., MCTS) plus RL fine-tuning for complex workflows.

### Key Equations

#### Advantage Estimation & Value Networks

- In actorâ€“critic variants (including PPO), we often learn a value functionÂ VÏˆ(s)Â to reduce variance:
    
    At=Rtâˆ’VÏˆ(st)Rt=âˆ‘k=0âˆÎ³krt+k
    
    - **where:**
        
        - At: theÂ **advantage estimate**Â at timestepÂ t, measuring how much better an action performed compared to the policyâ€™s expected performance.
        - Rt: theÂ **discounted return**, or the total expected future reward from timeÂ t.
        - Î³: theÂ **discount factor**Â (0<Î³â‰¤1), controlling how much future rewards are valued compared to immediate ones.
        - rt+k: theÂ **immediate reward**Â received at stepÂ t+k.
        - VÏˆ(st): theÂ **criticâ€™s value estimate**Â for stateÂ st, parameterized byÂ Ïˆ, representing the expected return from that state under the current policy.
- The update for the critic aims to minimize:
    
    Lvalue(Ïˆ)=ğ”¼stâˆ¼Ï€[(VÏˆ(st)âˆ’Rt)2]
    
    - **where:**
        
        - Lvalue(Ïˆ): theÂ **value loss**, quantifying how far the criticâ€™s predictions are from the actual returns.
        - ğ”¼stâˆ¼Ï€[â‹…]: theÂ **expectation**Â over statesÂ stÂ sampled from the current policy (\pi).
        - The squared termÂ (VÏˆ(st)âˆ’Rt)2: penalizes inaccurate value predictions, guiding the critic to estimate returns more accurately.

#### KL-penalty / Trust Region

- Some RLHF implementations add a penalty to keep the new policy close to the supervised model:
    
    LKL(Î¸)=Î²â‹…ğ”¼x,yâˆ¼Ï€[logÏ€Î¸(y|x)Ï€SFT(y|x)]
    
    - **where:**
        
        - LKL(Î¸): theÂ **KL-divergence loss**, which penalizes the new policyÂ Ï€Î¸Â if it deviates too far from the supervised fine-tuned (SFT) reference policyÂ Ï€SFT.
        - Î²: aÂ **scaling coefficient**Â controlling the strength of this regularization; largerÂ Î²Â enforces tighter adherence to the reference model.
        - ğ”¼x,yâˆ¼Ï€[â‹…]: theÂ **expectation**Â over sampled inputâ€“output pairs from the current policyâ€™s distribution.
        - Ï€Î¸(yâˆ£x): theÂ **current policyâ€™s probability**Â of generating outputÂ yÂ given inputÂ x.
        - Ï€SFT(yâˆ£x): theÂ **reference policyâ€™s probability**, often from the supervised model used before RL fine-tuning.
    - â€¦ so the total objective may combine PPOâ€™s surrogate loss with this KL penalty (and possibly an entropy bonus) to balance exploration, stability, and fidelity to the base model.*
        

#### Preference Optimization (DPO)

- As shown above, DPO reframes alignment as maximising the probability that the fine-tuned model ranks preferred outputs higher than non-preferred ones, bypassing the explicit RL loop.

#### Sample Efficiency & Off-policy Corrections

- For agents interacting with web or tools where running many episodes is costly, sample efficiency matters. Off-policy methods (e.g., experience replay) or offline RL variants (e.g.,Â [A Survey on Offline Reinforcement Learning](https://arxiv.org/abs/2203.01387)Â by Kumar et al. (2022)) may become relevant.

### Agentic Reinforcement Learning Via Policy Optimization

- InÂ **policy optimization**, the agent learns from a unified reward function that draws its signal fromÂ **one or more available sources**â€”such asÂ **rule-based rewards**, a scalar reward output from aÂ **learned reward model**, or another model that is proficient at grading the task (such as anÂ **LLM-as-a-Judge**). Each policy update seeks to maximize the expected cumulative return:
    
    J(Î¸)=ğ”¼Ï€Î¸[âˆ‘tÎ³trt]
    
    - whereÂ rtÂ represents whichever reward signal is active for the current environment or training regime. In some settings, this may be a purely rule-based signal derived from measurable events (like navigation completions, form submissions, or file creations). In others, the reward may come from a trained modelÂ RÏ•(ot,at,ot+1)Â that generalizes human preference data, or from an external proficient verifier (typically a larger model) such as an LLM-as-a-Judge.
- These components areÂ **modular and optional**â€”only one or several may be active at any time. The optimization loop remains identical regardless of source: the policy simply maximizes whichever scalar feedbackÂ rtÂ it receives. This flexible design allows the same framework to operate with deterministic, model-based, or semantic reward supervision, depending on task complexity, available annotations, and desired interpretability.
    
- **Rule-based rewards**Â form the foundation of this framework, providing deterministic, auditable feedback grounded inÂ **explicit environment transitions and observable state changes**. As demonstrated inÂ [DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models](https://arxiv.org/abs/2501.12948)Â by Gao et al. (2025), rule-based rewards yield transparent and stable optimization signals that are resistant to reward hacking and reduce reliance on noisy human annotation. In the context of computer-use agents, rule-based mechanisms correspond directly toÂ **verifiable milestones**Â in user interaction sequencesâ€”for example:
    
    - InÂ **web navigation**, detecting a URL transition, page load completion, or DOM state change (`NavigationCompleted`,Â `DOMContentLoaded`).
    - InÂ **form interaction**, observing DOM model deltas that indicate fields were populated, validation succeeded, or a â€œSubmitâ€ action triggered a confirmation dialog.
    - InÂ **file handling/artifact generation**, confirming the creation or modification of a file within the sandbox (e.g., registering successful exports such asÂ `.csv`,Â `.pdf`, orÂ `.png`Â outputs following specific actions).
    - InÂ **application state transitions**, monitoring focus changes, dialog closures, or process launches via OS accessibility APIs.
    - InÂ **UI interaction success**, verifying that a button, link, or menu item was activated and that the resulting accessibility tree or visual layout changed accordingly.
    - These measurable indicators serve as theÂ **atomic verification layer**Â of the reward system, ensuring that each environment step corresponds to reproducible, auditable progress signals without requiring human intervention.
- To generalize beyond fixed rules, aÂ **trainable reward model**Â RÏ•(ot,at,ot+1)Â can be introduced. This model is trained onÂ **human-labeled or preference-ranked trajectories**, similar to the reward modeling stage in PPO-based RLHF pipelines. Once trained,Â RÏ•Â predicts scalar reward signals that approximate human preferences for unseen tasks or ambiguous states. It operates faster and more consistently than a generative LLM-as-a-Judge (which can be implemented as a Verifier Agent), while maintaining semantic fidelity to human supervision.
    
- TheÂ **three-tier reward hierarchy**Â thus becomes:
    
    1. **Rule-based rewards (preferred default):**Â deterministic, event-driven, and auditable (no reward hacking).
    2. **Learned, discriminative reward model (RÏ•):**Â generalizes human feedback for subtle, unstructured, or context-dependent goals where rules are insufficient.
    3. **Generative reward model (e.g., LLM-as-a-Judge):**Â invoked only when both rule-based detectors andÂ RÏ•Â cannot confidently score outcomes (e.g., for semantic reasoning, style alignment, or multimodal understanding). This is similar to howÂ [DeepSeek-R1](https://aman.ai/primers/ai/deepseek-R1)Â uses a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment during the rejection sampling stage for reasoning data.
- This architecture ensures that theÂ **primary training flow remains rule-grounded and verifiable**, while allowing smooth fallback to preference-aligned modeling when necessary. The hybrid setupâ€”selectively combining rule-based rewards, learned reward estimation, and verifier agent interventionâ€”balancesÂ **scalability, auditability, and semantic depth**Â across diverse computer-use tasks.
    
- During training, theÂ **reward selection and routing process**Â is adaptive. When deterministic milestone detectors emit valid scores, they take precedence as the most reliable supervision. If the environment lacks such instrumentation, the learned modelÂ RÏ•Â dynamically provides substitute scalar feedback inferred from trajectory context. In the rare case that both mechanisms yield low confidence, the system escalates to the Verifier Agent for semantic adjudication. This cascading reward flow ensures the agent always receives a stable optimization signalâ€”grounded when possible, inferred when necessary, and judged when ambiguity demands interpretive reasoning.
    

#### Milestone-Based Reward System

- AnyÂ **reward formulation**â€”whether deterministic, learned, or model-evaluatedâ€”can be decomposed into a sequence ofÂ **milestones or checkpoints**Â that represent measurable progress toward the task goal. Each milestone corresponds to a verifiable state transition, UI event, or observable change in the environment, providing interpretable signals even within complex or hierarchical workflows. In practice, a reward function can therefore be aÂ **composite of multiple sources**:Â **rule-based rewards**, scalar predictions from aÂ **learned, discriminative reward model**, or aÂ **generative model**Â that is proficient at grading the task, such as anÂ **LLM-as-a-Judge**.
    
- In general,Â **rule-based rewards**Â are preferred because they areÂ **deterministic, easy to verify, and resistant to reward hacking**, consistent with the design principles demonstrated in theÂ [DeepSeek-R1](https://arxiv.org/abs/2501.12948)Â framework by Gao et al. (2025). These rewards are derived fromÂ **concrete, environment-observable events**â€”such as file creation, DOM or AX tree changes, navigation completions, or dialog confirmationsâ€”and can be validated directly through structured logs and system hooks. Their reproducibility and transparency make them ideal for large-scale, self-contained policy optimization loops, where interpretability and auditability are crucial.
    
- In this system, theÂ **rule-based layer**Â serves as the foundational signal generator for all common computer-use tasks. It captures events such as:
    
    - File downloads or artifact creation
    - Successful form submissions or dialog confirmations
    - UI transitions, window focus changes, or navigation completions
    - Text field population or data transfer between applications
    - Screenshot or state deltas indicating successful subgoal completion
        
    - These reward components directly populate the tupleÂ (ot,at,rt,ot+1)Â used by the policy optimizer for learning stable, interpretable control policies. Each milestone event contributes either a discrete tick or a weighted scalar toward cumulative progress.
- However, not all task goals can be described exhaustively through deterministic rules. To extend coverage, the architecture includes aÂ **learned reward model**Â RÏ•(ot,at,ot+1)Â trained specifically onÂ **human preferences or ranked trajectories**.
    
    - This model generalizes beyond hand-engineered events to scoreÂ **semantic correctness, contextual relevance, and user-aligned outcomes**.
    - RÏ•Â can be continuously fine-tuned as new preference data accumulates, adapting reward shaping dynamically to novel workflows or unseen UIs.
    - During training, the optimizer consumes a blended reward signal that can combine multiple sources:
        
        rÌƒÂ t=Î±r(rule)t+Î²RÏ•(ot,at,ot+1)+Î³r(judge)t
        
        - whereÂ Î±,Î²,Î³âˆˆ[0,1]Â represent trust weights for deterministic, learned, and model-evaluated components respectively, withÂ Î±+Î²+Î³=1.
- In cases where both rule-based detectors and the learned reward model fail to provide a confident or interpretable score, aÂ **generative model (such as an LLM-as-a-Judge)**Â may be selectively invoked. This verifier acts as a high-capacity,Â _LLM-as-a-Judge_Â module that semantically evaluates whether the observed trajectory satisfies implicit or fuzzy success criteria. Its role parallels that of a preference model but operates at runtime for difficult or open-ended cases.
    
- Scenarios where rule-based and model-based scoring may be insufficientâ€”and thus require a Verifier Agentâ€”include:
    
    - **Subjective or semantic correctness:**Â determining if a written summary or chart interpretation matches the instruction intent.
    - **Cross-context validation:**Â verifying that data copied from a spreadsheet was correctly inserted into a report or email draft.
    - **Goal inference under ambiguity:**Â tasks like â€œopen the latest invoice,â€ where the target must be inferred dynamically.
    - **Complex recovery handling:**Â identifying whether the system has correctly recovered from an unintended dialog or misclick.
    - **Language or multimodal alignment:**Â verifying tone, structure, or layout across applications.
- TheÂ **reward system hierarchy**Â therefore consists of three complementary and optionally composable layers:
    
    1. **Rule-based rewards**: deterministic, verifiable, and fully auditable signals derived from concrete milestones (default and preferred).
        
    2. **Learned, discriminative reward model (RÏ•)**: trained on human preferences to generalize beyond explicit rules and produce scalar feedback for unstructured tasks.
        
    3. **Generative reward model (e.g., LLM-as-a-Judge)**: semantic fallback for nuanced, subjective, or multimodal evaluation where neither rules nor learned models suffice. This is similar to howÂ [DeepSeek-R1](https://aman.ai/primers/ai/deepseek-R1)Â uses a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment during the rejection sampling stage for reasoning data.
        
- Together, these layers enableÂ **robust, explainable, and modular reward shaping**. Any reward function within the system can thus be expressed as aÂ **milestone-weighted combination**Â of deterministic, learned, and interpretive componentsâ€”ensuring scalability, transparency, and semantic alignment across all computer-use reinforcement learning setups.
    

##### Example Milestones by Task Category

1. **Web Navigation and Data Extraction**
    
    - **Milestone:**Â Target URL loaded successfully (`NavigationCompleted`Â event).Â _Reward:_Â +0.25
    - **Milestone:**Â Element with specific role/name detected (e.g., â€œReports Tableâ€ or â€œDashboard Summaryâ€).Â _Reward:_Â +0.25
    - **Milestone:**Â Successful data scrape or DOM text retrieval logged.Â _Reward:_Â +0.5
2. **Form Interaction**
    
    - **Milestone:**Â Input field focused and filled (text pattern matched).Â _Reward:_Â +0.2
    - **Milestone:**Â Submit button clicked and confirmation dialog appears.Â _Reward:_Â +0.3
    - **Milestone:**Â Success banner or confirmation element detected.Â _Reward:_Â +0.5
3. **File Handling and Downloads**
    
    - **Milestone:**Â File creation event observed inÂ `/Downloads`.Â _Reward:_Â +1.0
    - **Milestone:**Â File hash or extension matches expectation (e.g.,Â `.csv`,Â `.pdf`).Â _Reward:_Â +0.5
    - **Milestone:**Â Directory updated without error.Â _Reward:_Â +0.25
4. **Email or Document Workflows**
    
    - **Milestone:**Â Email editor loaded and populated with recipient and subject.Â _Reward:_Â +0.25
    - **Milestone:**Â Attachment successfully added.Â _Reward:_Â +0.5
    - **Milestone:**Â Message successfully sent (UI confirmation or state change).Â _Reward:_Â +1.0
5. **System Configuration and Settings**
    
    - **Milestone:**Â Settings panel opened (window title match).Â _Reward:_Â +0.25
    - **Milestone:**Â Checkbox or toggle successfully modified (UIA/AX event).Â _Reward:_Â +0.25
    - **Milestone:**Â â€œChanges Savedâ€ notification observed.Â _Reward:_Â +0.5
6. **Search and Information Retrieval**
    
    - **Milestone:**Â Query field populated with correct term.Â _Reward:_Â +0.25
    - **Milestone:**Â Search executed and result list rendered.Â _Reward:_Â +0.5
    - **Milestone:**Â Target entry clicked or opened.Â _Reward:_Â +0.5

#### Example Reward Function

- Each environment step returns a shaped reward based on concrete, verifiable milestones. Instead of relying on subjective evaluators, the reward function is composed of measurable subcomponents derived from observable state transitions, UI changes, and artifact events.
    
- At stepÂ t, the total reward is given by:
    
    rt=wnavr(nav)t+wUIr(UI)t+wformr(form)t+wfiler(file)t+wgoalr(goal)t
    
    - where each component represents a verifiable milestone type:
- r(nav)t: Navigation progress reward â€” triggered by measurable page transitions such asÂ `NavigationCompleted`Â events, URL match, or window title change.
    
    r(nav)t=ğŸ™{urltâ‰ urltâˆ’1}
    
- r(UI)t: UI element interaction reward â€” triggered when a UI control with a matching role or label is successfully targeted (e.g., a button click or field focus event).
    
    r(UI)t=ğŸ™{clicked(role,name)=expected(role,name)
    
- r(form)t: Form completion reward â€” triggered when an editable control is filled and validated (value non-empty, regex match, or field count).
    
    r(form)t=NfilledNexpected
    
- r(file)t: File-handling reward â€” derived from filesystem or artifact deltas (e.g., a newÂ `.csv`,Â `.pdf`, orÂ `.json`Â created).
    
    r(file)t=ğŸ™{âˆƒfâˆˆîˆ­t:f.event=''created"}
    
- r(goal)t: Task completion reward â€” triggered by a high-level terminal condition, such as detection of success text, matched hash, or closed loop condition.
    
    r(goal)t=ğŸ™{goal_verified(ot)}
    
- The weightsÂ wnav,wUI,wform,wfile,wgoalÂ balance short-term shaping with terminal rewards, typically normalized so that:
    

âˆ‘iwi=1{wgoalâ‰¥wfileâ‰¥wUI}

#### Example Instantiation

|**Component**|**Description**|**Weight**|**Range**|
|---|---|---|---|
|r(nav)t|Successful navigation|0.1|0,1|
|r(UI)t|Correct element interaction|0.2|0,1|
|r(form)t|Partial form completion|0.2|[0,1]|
|r(file)t|Artifact creation (e.g., download)|0.3|0,1|
|r(goal)t|Verified task completion|0.2|0,1|

- This formulation ensuresÂ **all reward components are physically measurable**â€”no human labels are required. Each event corresponds to structured data observable through CDP logs, accessibility APIs, or filesystem monitors, making it reproducible and auditable across training runs.

### Agent Training Pipeline

- A typical pipeline to train a web or computer-use agent might follow:
    
    1. Pre-train the model (e.g., a large language model) via supervised learning.
    2. Optionally fine-tune on domain-specific prompts (supervised fine-tuning, SFT).
    3. Collect human preference data (rankings of model responses).
    4. Choose alignment method:
        - **RLHF:**Â train reward modelÂ â†’Â use PPO (or other RL algorithm) to optimise policy.
        - **DPO:**Â directly fine-tune model on preference data (skipping RL loop).
    5. Launch agent into simulated environment (SingleTurnEnv, ToolEnv, MultiTurnEnv).
    6. Run RL policy optimisation in the environment: sample trajectories, estimate advantages/returns, update policy using PPO or variants.
    7. Periodically evaluate and filter trajectories, adjust reward shaping, fine-tune further for tool-use or long-horizon behaviours.
- By selecting algorithms appropriate for the interaction type (single turn vs tool vs multi-turn), one can tailor the training for efficiency, stability, and scalability.

----

## Environment Interaction Patterns for Agent Design

### Environment Design in Reinforcement Learning for Agents

- Modern RL environments for web and computer-use agents are designed to capture the diversity and complexity of real-world interactions while maintaining enough structure for stable learning. Unlike classical RL benchmarks (e.g., Atari or MuJoCo), these environments involve language, symbolic reasoning, tool use, and visual perception.
    
- They are not simply â€œgamesâ€ or â€œcontrol systemsâ€ butÂ **interactive ecosystems**Â that test an agentâ€™s ability to perceive context, reason over multi-step processes, and execute goal-directed actions.
    
- To support the training of increasingly capable language-based and multimodal agents, recent frameworks such asÂ [AgentGym](https://arxiv.org/abs/2406.04151)Â by Xi et al. (2024) have introduced a unified taxonomy of environments, each corresponding to a particularÂ _interaction modality_.
    
- At the highest level, these can be grouped into three archetypes:
    
    1. **Single-Turn Environments**, designed for one-shot problem solving and precision reasoning.
    2. **Tool-Use Environments**, optimized for integrating external functions, APIs, or computation tools.
    3. **Multi-Turn Sequential Environments**, which simulate complex, long-horizon workflows requiring memory, planning, and context adaptation.
- Each environment type not only changes how agents act but also howÂ _rewards, policies, and credit assignment mechanisms_Â must be designed to drive meaningful learning.
    

### Single-Turn Environments (SingleTurnEnv)

- **Single-turn environments**Â represent the simplest and most direct form of RL training. In this setup, each episode consists of a single interaction: the agent receives an input (prompt, question, or task description), produces one output (answer, code snippet, or solution), and immediately receives feedback.
    
- These environments are ideal for optimizing agents that must produce highly accurate outputs in one stepâ€”such as coding assistants, math solvers, or document completion systems.
    
- **Examples:**
    - Code completion and debugging tasks inÂ _CodeRL_Â ([CodeRL: Mastering Code Generation through RL](https://arxiv.org/abs/2207.01780)Â by Le et al., 2022).
    - Question-answering benchmarks like WebGPT ([WebGPT](https://arxiv.org/abs/2112.09332)Â by Nakano et al., 2022)), where the agentâ€™s final response is scored based on correctness and citation quality.
- **Reward Structure:**Â Single-turn environments typically useÂ _outcome-based rewards_Â rather than step-wise feedback because there is only one output to evaluate. For example:
    
    - In a coding task,Â r=+1Â if the code executes successfully, andÂ r=0Â otherwise.
    - In a factual QA task,Â rÂ may represent an F1 score or BLEU score.
- Formally, the optimization objective reduces to:
    
    J(Ï€)=ğ”¼xâˆ¼D,yâˆ¼Ï€(â‹…|x)[R(x,y)]
    
    - whereÂ R(x,y)Â is the final outcome reward.
- While simple, such environments serve as critical pretraining stages, allowing models to build domain accuracy before engaging in multi-step reasoning or tool-use.

### Tool-Use Environments (ToolEnv)

- **Tool-use environments**Â introduce an additional layer of reasoning: instead of solving a task in one step, the agent must decide when and how to invoke external tools. Tools may include:
    
    - API calls (e.g., search, translation, or computation),
    - external functions (e.g., symbolic calculators, Python interpreters), or
    - system-level commands (e.g., file access, browser manipulation).
- The core challenge isÂ _tool orchestration_â€”learning when to rely on external computation versus internal reasoning. For instance, in a data retrieval task, the agent might issue an API query, parse results, and compose a natural-language summary.
    
- **Reward Structure**:
    - In ToolEnv, bothÂ _process-wise_Â andÂ _outcome-based_Â rewards are valuable:
        
        - _Step-wise rewards_Â can score the accuracy or efficiency of each tool invocation (e.g., correct API parameters or valid JSON structure).
        - _Outcome-based rewards_Â measure task completion or user satisfaction.
    - The combined reward signal is often expressed as:
        
        Rt=Î±rprocess+(1âˆ’Î±)routcome,
        
        - whereÂ Î±Â controls the balance between short-term and final goal feedback.
- **Algorithmic Approaches**: Because the action space now includes function arguments and results, methods like policy gradient with structured action representations, hierarchical RL, or model-based planning (e.g., MCTS as inÂ [Agent Q](https://arxiv.org/abs/2408.07199)Â by Putta et al., 2024) become necessary.
    
- [Tool Learning with Foundation Models](https://doi.org/10.1145/3704435)Â by Qin et al. (2024) provides a comprehensive survey of how foundation models learn to invoke external tools to augment their reasoning capabilities.

### Multi-Turn Sequential Environments (MultiTurnEnv)

- **Multi-turn environments**Â simulate complex, multi-step workflows where each decision influences future context. These environments are designed for agents that need to plan, adapt, and maintain consistency across many turns of interaction.
    
- **Examples:**
    
    - Web navigation agents such asÂ [OpenWebVoyager](https://arxiv.org/abs/2410.19609)Â by He et al. (2024), where the agent browses, clicks, and fills forms over multiple steps.
    - Software operation tasks like system configuration, spreadsheet editing, or email management.
    - Interactive tutoring and dialogue planning systems.
- **Reward Structure:**
    - In MultiTurnEnv setups, pure outcome-based rewards (success/failure) can causeÂ _credit assignment problems_Â because the agent receives feedback only after many steps. To address this, researchers combineÂ **process-wise rewards**â€”for subgoal completion, error reduction, or partial correctnessâ€”withÂ **final outcome rewards**.
        
    - Formally, the expected return in such environments can be represented as:
        
        J(Ï€)=ğ”¼[âˆ‘t=1TÎ³t(rprocesst+Î»,routcomeT)]
        
        - whereÂ Î»Â balances intermediate and terminal objectives.
    - In OpenWebVoyager, for example, each sub-action (like opening the correct link) contributes partial reward, guiding the agent toward long-term success while preventing divergence from optimal sequences.
        
- **Learning Dynamics:**Â Training in MultiTurnEnv requires:
    
    - Long-horizon credit assignment via temporal-difference learning or advantage estimation.
    - Hierarchical RL for decomposing tasks into sub-policies.
    - Trajectory filtering and reward shaping to combat sparse or noisy signals.

### Designing Rewards for Complex Agent Environments

- Reward engineering is arguably the most critical part of environment design. Different environment types benefit from distinct reward strategies:

|**Environment Type**|**Reward Type**|**Typical Signal**|**Optimization Goal**|
|---|---|---|---|
|SingleTurnEnv|Outcome-based|Correctness, BLEU/F1 score|Precision and factual accuracy|
|ToolEnv|Hybrid (step-wise + outcome)|Tool correctness, API success|Functional reasoning, tool reliability|
|MultiTurnEnv|Step-wise + delayed outcome|Subgoal completion, navigation success|Long-horizon planning, autonomy|

- Balancing process-wise and outcome-based rewards ensures that agents receiveÂ _dense feedback for learning efficiency_Â while still optimizing towardÂ _global objectives_Â like success rate or user satisfaction.

### Implications for Agent Design and Evaluation

- Each environment type imposes unique requirements on model architecture, reward shaping, and evaluation metrics.
    
    1. **SingleTurnEnv**Â favors compact policies and fast evaluation loops, suitable for smaller RL batches or DPO-based optimization.
    2. **ToolEnv**Â requires compositional reasoning and structured memory to maintain tool-call histories and argument dependencies.
    3. **MultiTurnEnv**Â demands long-context modeling, world-state tracking, and temporal credit assignment across potentially hundreds of steps.
- Evaluation metrics vary accordingly:
    
    - _Single-turn_: Accuracy, F1, pass rate.
    - _Tool-use_: Tool-call correctness, latency, success ratio.
    - _Multi-turn_: Task completion rate, cumulative reward, consistency, and planning efficiency.
- When integrated properly, these environment classes form aÂ **curriculum**Â for RL-based agent development: agents begin with static, outcome-driven reasoning (SingleTurnEnv), progress to dynamic, tool-integrated reasoning (ToolEnv), and culminate in fully autonomous multi-turn reasoning (MultiTurnEnv).
    

### Comparative Analysis

- Environment design is the foundation on which modern RL agents learn to generalize and act. The interplay betweenÂ **interaction modality**,Â **reward granularity**, andÂ **algorithmic strategy**Â determines not only how fast an agent learns but also what kinds of intelligence it develops.
    
    - Single-turn environments teachÂ _accuracy_.
    - Tool-use environments teachÂ _functional reasoning_.
    - Multi-turn environments teachÂ _autonomy and adaptability_.
- Together, they form a progression of increasing sophisticationâ€”mirroring the cognitive layers of reasoning, planning, and execution. RL algorithms like PPO and DPO serve as the connective tissue between these layers, transforming static pretrained models into active, evolving agents capable of navigating and operating within real digital ecosystems.


## Reward Modeling

### The Role of Reward Modeling

- Reward modeling lies at the heart of RL systems for language, web, and computer-use agents. In traditional RL, the reward function is hand-crafted to quantify successâ€”for example, the score in a game or the distance to a goal. In contrast, modern LLM-based agents operate in open-ended environments where the notion of â€œcorrectnessâ€ or â€œhelpfulnessâ€ is inherently subjective and context-depe
ximate human judgment. Instead of manually defining numerical rewards, the system learns a functionÂ rÏ•(x,y)Â that predicts the quality of an agentâ€™s outputÂ yÂ for a given inputÂ x. These RMs are usually fine-tuned on preference datasets where human annotators rank outputs from best to worst.
    
- Formally, given a dataset of comparisonsÂ D=(xi,y+i,yâˆ’i), the reward model is trained to maximize:
    
    îˆ¸RM=âˆ’ğ”¼(x,y+,yâˆ’)âˆ¼D[logÏƒ(rÏ•(x,y+)âˆ’rÏ•(x,yâˆ’))]
    
    - whereÂ ÏƒÂ is the logistic function, andÂ rÏ•Â outputs a scalar reward. The resulting model can then guide PPO updates, Direct Preference Optimization (DPO), or other RL pipelines.
- Reward modeling thus replaces explicit rule-based objectives withÂ _learned evaluators_â€”a fundamental shift that enables agents to align with nuanced human preferences across web, reasoning, and tool-use tasks.
    
- [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al. (2025)) states that in practice, reward signals can be complemented by reward-free, language-native supervision gathered before RLâ€”so the policy starts â€œaligned to the environmentâ€ even without verifiable rewards. Two pre-RL objectives from early, agent-generated interaction data are especially useful: an implicit world-modeling loss that predicts next states given stateâ€“action pairs, and a self-reflection loss that learns to compare expert vs. non-expert actions in natural language. Concretely:
    
    LIWM(Î¸)=âˆ’âˆ‘(si,aji,sji)âˆˆîˆ°rolloutlogpÎ¸(sji,âˆ£âˆ£,si,aji),LSR(Î¸)=âˆ’âˆ‘iâˆ‘j=1KlogpÎ¸(cji,âˆ£âˆ£,si,;aji,;ai,;si+1,;sji),
    
    - which warm-start policies and reduce distribution shift ahead of PPO/GRPO or DPO, improving sample efficiency in web and tool-use settings.
        
    - The following figure shows an overview of the two early experience approaches. Implicit world modeling (left) augments expert trajectories with alternative actions and predicted next states, training the policy to internalize transition dynamics before deployment. Self-reflection (right) augments expert actions with self-generated explanations c1, training the policy to reason about and revise its own decisions. Both methods use alternative actions proposed by the initial policy (LLM). The number of alternativesÂ KÂ is a hyperparameter; for brevity, only one is illustrated.
        
    
    ![](https://aman.ai/primers/ai/assets/RL-for-agents/EarlyExperience2.jpg)
    

### Process-Wise and Outcome-Based Reward Integration

- When training agents in realistic, multi-step environments, reward signals can be categorized asÂ **process-wise (step-wise)**Â orÂ **outcome-based**. Both serve complementary roles:
    
    1. **Outcome-Based Rewards:**
        - These are terminal signals received once the task is completeâ€”such as a success flag, accuracy score, or human satisfaction rating.
        - For instance, in a booking agent, a positive reward may be given only when the reservation is successfully completed.
    2. **Process-Wise (Step-Wise) Rewards:**
        - These provide intermediate feedback after each step or subgoal, rewarding partial correctness, progress, or efficiency.
        - In web navigation, an agent might receive a small positive reward for clicking the correct button or locating relevant text, even before reaching the final goal.
- The challenge is balancing the two. Purely outcome-based training can lead toÂ _sparse reward problems_, while purely process-based training risksÂ _overfitting local heuristics_Â that do not generalize.
    
- A common hybrid formulation is:
    
    rt=Î±,rprocesst+(1âˆ’Î±),Î´t=T,routcomeT
    
    - whereÂ Î±âˆˆ[0,1]Â controls the tradeoff between intermediate shaping and final goal alignment.
- In practical web-agent training, hybrid reward models may leverage both:
    
    - **Synthetic process feedback**Â (automated evaluators for substeps),
    - **Human outcome feedback**Â (ranking complete trajectories).
- A scalable way to create dense, shaped feedback is to synthesize experience with a reasoning-based experience model that produces consistent next states and vectorized, unified feedback signals in a textual state space. This enables closed-loop RL without expensive real-environment rollouts and supports curriculum generation that targets the current policyâ€™s weaknesses; empirically it yields >30% gains on non-RL-ready tasks like WebArena and can match PPO/GRPO using only synthetic interactions ([Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025)).
    

### Tool-Augmented Reward Modeling (TARM)

- [Tool-Augmented Reward Modeling (Themis)](https://arxiv.org/abs/2310.01045)Â by Li et al. (2024) proposes Tool-Augmented Reward Modeling (TARM) (also called Tool-Integrated Reward Modeling (TIRM)), which represents a significant evolution in RL for agents that operate within complex, tool-augmented environments. TARM integrates external computational and retrieval tools into the reward generation process itself. Instead of merely training language models to use tools during inference, TIRM embeds tool engagement as part of the reward modelâ€™s reasoning and supervision pipeline.
    
- This approach extends the conventional Reinforcement Learning from Human Feedback (RLHF) paradigmâ€”used in models such asÂ [InstructGPT](https://arxiv.org/abs/2203.02155)Â by Ouyang et al. (2022)â€”by introducingÂ **tool-augmented reasoning traces**Â andÂ **context-sensitive reward estimation**, enabling more accurate alignment between model outputs and human evaluatorsâ€™ expectations.
    
- Put simply, tool-Integrated Reward Modeling advances RLHF by embedding reasoning transparency, external computation, and factual grounding directly into the reward modeling process. Through supervised fine-tuning on tool-augmented datasets and RL on process- and outcome-based signals, these models redefine how reward functions are constructed for intelligent agents. The resulting agents not only learn to act effectively but also toÂ _evaluate_Â their own reasoning with access to external world modelsâ€”laying the foundation for trustworthy, explainable, and verifiable AI systems.
    
- Reward-free early experience, proposed inÂ [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al. (2025), can seed TARM and RLHF alike: implicit world modeling grounds the policy in environment dynamics, while self-reflection generates rationale-style preferences that complement pairwise comparisons used by reward modelsâ€”providing a bridge from imitation/preference learning to full RL.
    

#### Motivation and Background

- Traditional reward models in RLHF are trained using paired preference data, where a scalar reward is assigned based on human judgments. These models often struggle with factual reasoning, arithmetic operations, and real-world lookups due to their reliance on static, in-model knowledge representations ([Christiano et al., 2017](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)). Tool-Integrated Reward Models mitigate this by allowing the reward model itself to call APIs, calculators, code interpreters, or search engines during evaluation.
    
- Themis demonstrated that augmenting reward models with tools increased factual accuracy and truthfulness on benchmarks like TruthfulQA by 7.3% over large baselines such as Gopher 280B, while achieving a 17.7% average improvement in preference ranking accuracy across tasks.
    

#### Structure and Workflow of Tool-Augmented Reward Models

- The tool-integrated reward modeling process can be decomposed into sequential reasoning stagesâ€”each enhancing the modelâ€™s interpretability and precision in assigning rewards:
    
    1. **Thought**: The model assesses whether external information is required and determines which tool to invoke.
    2. **Action**: The model generates an API call with specified parameters.
    3. **Observation**: The system retrieves and processes tool outputs.
    4. **Rationale**: The model integrates the external information into a reasoning chain, constructing an interpretable trace of decision-making.
    5. **Reward Generation**: A scalar reward is computed from the aggregated reasoning trace.
- Formally, the total reasoning trajectory is denoted as:
    

c1:T=(a1,o1,â€¦,aT,oT,sT)

- â€¦ and the scalar reward is defined as:
    
    rÎ¸(x,y,c1:T)
    
    - whereÂ xÂ is the input,Â yÂ is the modelâ€™s output, andÂ c1:TÂ represents the full reasoning and observation history.
- The total loss function combines pairwise ranking and autoregressive modeling losses:
    
    Ltotal=LRM+Î±âˆ‘t=1T(Ltool(t)+Î²Lobs(t))+Ï‰Lrat
    
    - whereÂ LRMÂ corresponds to the pairwise ranking loss from preference modeling,Â LtoolÂ supervises tool invocation accuracy,Â LobsÂ captures fidelity to observed results, andÂ LratÂ trains the model to generate coherent rationales.
- The following figure ([source](https://arxiv.org/abs/2310.01045)) shows illustrates the pipeline of (a) Vanilla reward models (RMs); (b) Tool-augmented RMs, namely Themis; (c) RL via proximal policy optimization (PPO) on above RMs; (d) Examples of single or multiple tool use process in the proposed approach.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/Themis.jpg)

- PerÂ [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025), when paired with synthetic experience generation, tool-augmented evaluators can operate at scale with consistent, informative feedback, while curriculum generation focuses on high-entropy tasks that maximize learning signalâ€”closing the loop between reward modeling and data generation in RL training.

#### Role of Supervised Fine-Tuning and Reinforcement Learning

- Themisâ€”and, more broadly, TIRMâ€”relies on aÂ **hybrid SFT + RL training approach**.
    
    - **SFT Stage**: The reward model learns to imitate tool usage traces from curated datasets (e.g., theÂ [TARA dataset](https://github.com/ernie-research/Tool-Augmented-Reward-Model)). These traces include natural-language thoughts, API calls, and tool results generated via multi-agent interactions between LLMs and simulated human labelers.
        
    - **RL Stage**: Once pre-trained, the reward model is further optimized via RL objectives like Proximal Policy Optimization (PPO) ([Schulman et al., 2017](https://arxiv.org/abs/1707.06347)). The model refines its reward predictions using outcome-based feedback, achieving stable convergence even under high variance tool-call trajectories.
        
- This two-stage setup enablesÂ **process-based reward shaping**, in which partial rewards are granted for intermediate reasoning correctness (process rewards), andÂ **outcome-based rewards**Â for overall task success. This balance is critical when agents operate in environments requiring both reasoning depth and correct final results.
    
- Reward-free early experience provides a natural pretraining curriculumâ€”first fittingÂ LIWMÂ to learn dynamics, thenÂ LSRÂ to internalize preference signalsâ€”before introducing PPO/GRPO or DPO on either real or synthetic rollouts (cf.Â [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al. (2025);Â [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025)).
    

#### The Tool-Augmented Reward Dataset (TARA)

- A key component of TIRM research is the creation of datasets that reflect real-world reasoning and tool usage patterns. TheÂ [TARA dataset](https://github.com/ernie-research/Tool-Augmented-Reward-Model)Â contains over 15,000 instances combining human preferences with explicit tool-invocation traces across seven tool categories, including search, translation, weather, calculator, and code execution.
    
- The following figure ([source](https://arxiv.org/abs/2310.01045)) shows the data collection pipeline for TARA, depicting human-LLM interaction, tool invocation, and rationale generation. It the four-step process: (1) Question-answer collection, (2) ToolBank construction, (3) Tool invocation via multi-agent simulation, and (4) Filtering for data integrity.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/TARA_Pipeline.jpg)

#### Empirical Results and Observations

- Experiments show that Themis enhances bothÂ **single-tool**Â andÂ **multi-tool**Â scenarios. For example:
    
    - Accuracy improved by +19.2% in single-tool and +17.7% in mixed-tool setups.
    - Perfect accuracy (100%) was achieved in calendar and weather reasoning tasks.
    - Models learned when and whether to call tools autonomouslyâ€”a form of learned tool invocation policy.
    - The observation and rationale components contributed significantly to reward accuracy, proving thatÂ **process supervision**Â is critical to model interpretability and consistency.
- Further, when integrated into an RLHF pipeline (referred to as RLTAF: Reinforcement Learning from Tool-Augmented Feedback), Themis-trained models achieved a 32% higher human preference win rate compared to vanilla RMs, highlighting its ability to generate more trustworthy and factual responses.
    
- Complementarily,Â [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025) proposes scaling RL with synthetic rollouts generated by a reasoning experience model, which yields substantial downstream gains and lowers on-environment data needs; e.g., DreamGym reports >30% improvements on WebArena and policy parity with PPO/GRPO using only synthetic interactions, after which real-environment fine-tuning brings additional gains.
    
- The following figure illustrates an overview of the proposed DreamGym agent training framework. Given a set of seed tasks, a reasoning-based experience model interacts with the agent to generate informative, diverse tasks and trajectories for RL training. At each step, the agent takes actions based on its current state and receives next states and reward signals derived by the experience model through CoT reasoning based on both interaction history and top-k similar experiences from an active replay buffer. To expose the agent to increasingly informative scenarios, tasks with high reward entropy are proposed by the curriculum task generator for future training. With this unified design, DreamGym addresses both task and reward sparsity while enabling scalable RL with diverse and curriculum-driven environments.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/DreamGym2.jpg)

#### Connection to Reinforcement Learning for Agents

- Tool-integrated reward modeling bridges the gap betweenÂ **tool-augmented reasoning**Â andÂ **agentic RL**. By enabling the reward function itself to utilize external resources, agents trained under TIRM learn a deeper mapping between reasoning actions and value estimation. This structure is directly applicable to RL-driven computer-use agents, where bothÂ **process-level**Â (step-wise) andÂ **outcome-based**Â (goal completion) rewards must be optimized.
    
- In this framework, process-based rewards correspond to accurate intermediate reasoning and correct tool usage, while outcome-based rewards correspond to successful task completion. The combined signal provides agents with fine-grained credit assignment, improving learning efficiency and interpretability in web-based or API-integrated environments.
    
- PerÂ [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025), when training in synthetic environments, policy improvements can provably transfer to the real environment under standard trust-region updates. Writing the real MDP asÂ îˆ¹=(S,A,P,R,Î³)Â and the synthetic one asÂ îˆ¹ÌƒÂ =(S,A,PÌƒÂ ,RÌƒÂ ,Î³)Â with bounded reward and transition errorsÂ ÎµR,ÎµP, a KL-bounded update fromÂ Ï€â†’Ï€â€²Â (as in PPO/GRPO) yields a lower bound of the form:
    
    Jîˆ¹(Ï€â€²)âˆ’Jîˆ¹(Ï€)â‰¥11âˆ’Î³,ğ”¼sâˆ¼dîˆ¹ÌƒÂ Ï€,aâˆ¼Ï€â€²[Aîˆ¹ÌƒÂ Ï€(s,a)]âˆ’KL trust-region penaltyî„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹(per-state KL radius)âˆ’2(ÎµR1âˆ’Î³+2Î³Rmax(1âˆ’Î³)2ÎµP)î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹experience-model error
    
    - â€¦ so synthetic surrogate gains exceeding these penalties guarantee real-environment improvement.

### Feedback Alignment and Human Preference Modeling

- Reward models provide scalar supervision, but alignment requiresÂ _structured feedback_. Human evaluators often give comparative, categorical, or qualitative feedback (e.g., â€œresponse A is clearer, but response B is more completeâ€).
    
- To convert such structured feedback into training signals, systems employÂ **preference aggregation**Â methods such as:
    
    - _Bradleyâ€“Terry models_Â to infer pairwise preference probabilities.
    - _Elo-style scoring_Â to maintain global quality rankings across responses.
    - _Bayesian aggregation_Â for uncertain or noisy feedback.
- In advanced systems likeÂ [Large Language Models Can Self-improve at Web Agent Tasks](https://arxiv.org/abs/2405.20309)Â by Patel et al. (2024), self-feedback mechanisms replace human labeling. The agent critiques its own trajectories using LLM-based evaluators, ranking which paths yielded the best progress and then re-finetuning on its own top-performing examples.
    
- This method creates aÂ **feedback alignment loop**, where models not only learn from human signals but also gradually calibrate their own evaluators.
    

### Multi-Objective Reward Modeling

- As agents evolve to handle multi-modal and multi-task objectivesâ€”such as reasoning, retrieval, and tool orchestrationâ€”single scalar reward functions become insufficient.
- Instead,Â **multi-objective reward modeling (MORM)**Â decomposes total reward into several components:
    
    rt=âˆ‘k=1Kwk,r(k)t
    
    - where eachÂ r(k)tÂ corresponds to a distinct objective (e.g., factual accuracy, efficiency, safety, fluency), andÂ wkÂ are learned or manually tuned weights.
- This decomposition enables flexible tradeoffsâ€”for example, prioritizing accuracy over verbosity or reliability over speed. In web and software agents, multi-objective RMs can encode:
    
    - Functional correctness (execution success),
    - Temporal efficiency (fewer steps or tool calls),
    - Adherence to user goals (alignment quality),
    - Safety and compliance (filtered language use).
- Combining these objectives helps agents develop a balanced understanding of what constitutes â€œgood behaviorâ€ in dynamic and human-centric environments.

### Evaluation Frameworks for RL-Based Agents

- Evaluating agents trained through RL requires going beyond static benchmarks. Instead of only measuring final success, modern frameworks evaluateÂ _trajectory quality, interpretability, and generalization_.

#### Key Evaluation Metrics Include

- **Success Rate:**Â Fraction of episodes where the agent achieves its goal (e.g., booking completed, question answered).
- **Cumulative Reward:**Â Sum of step-wise rewards, indicating the efficiency of action selection.
- **Action Accuracy:**Â Proportion of correct API or tool calls.
- **Trajectory Efficiency:**Â Number of steps or actions required to reach completion.
- **Human Preference Score:**Â Alignment with human judgment over multiple outputs.
- **Robustness:**Â Performance under perturbed or unseen web environments.
    
- Frameworks such as WebArena, Mind2Web, and AgentBench (as catalogued inÂ [AgentGym](https://arxiv.org/abs/2406.04151)Â by Xi et al., 2024) provide unified benchmarks with standardized reward metrics and simulator APIs for reproducible agent training.

### Takeaways

- Reward modeling and feedback alignment form the core of how RL agents evolve from static predictors intoÂ _adaptive decision-makers_. The design of these mechanisms determines whether agents learn to pursue shallow, short-term signals or to internalize long-term, value-aligned behavior.
    
    - **Outcome-based rewards**Â ensure goal fidelity but suffer from sparsity.
    - **Process-wise rewards**Â provide dense guidance and interpretability.
    - **Tool-augmented reward models**Â enhance factual grounding and transparency.
    - **Human and self-generated feedback**Â create continuous learning loops.
    - **Multi-objective reward modeling**Â allows flexible alignment across multiple competing priorities.
- Together, these innovations define the modern ecosystem of RL-based agentic trainingâ€”where the agent not onlyÂ _acts_Â in its environment but alsoÂ _learns how to evaluate its own progress_.



---------


## Search-Based Reinforcement Learning, Monte Carlo Tree Search (MCTS), and Exploration Strategies in Multi-Step Agents

### Motivation: Exploration vs. Exploitation in Complex Agentic Systems

- In RL, agents must navigate the fundamental trade-off betweenÂ **exploration**â€”trying new actions to discover better strategiesâ€”andÂ **exploitation**â€”using known information to maximize immediate reward.
    
- For simple environments (like tabular Q-learning), this trade-off can be controlled byÂ Ïµ-greedy or softmax policies. However, for web and computer-use agents operating in open-ended, high-dimensional spacesâ€”such as browsing dynamic web pages, calling APIs, or managing multi-turn dialoguesâ€”naive exploration is computationally infeasible and unsafe.
    
- Thus, modern agentic RL systems combineÂ _search-based exploration_Â withÂ _learned policy optimization_, blending symbolic planning with neural policy priors. This hybrid paradigm is exemplified by recent works likeÂ [Agent Q: Efficient Online Adaptation via Monte Carlo Tree Search](https://arxiv.org/abs/2408.07199)Â by Putta et al. (2024) andÂ [OpenWebVoyager](https://arxiv.org/abs/2410.19609)Â by He et al. (2024), both of which adapt classic search strategies (like MCTS) for reasoning-driven web environments.
    
- Complementary to these,Â [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al. (2025) shows that exploration itself can beginÂ _before_Â any reward modeling, by leveraging self-reflective rollouts and implicit world modeling to pretrain a policy that already encodes structured exploration biases. Similarly,Â [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025) formalizes a scalable simulation frameworkâ€”**DreamGym**â€”that generates synthetic exploratory rollouts under theoretical guarantees of policy improvement transfer to real environments.
    
- The following figure shows theÂ _Agent Q architecture_, demonstrating how an agent integrates Monte Carlo Tree Search (MCTS) with an internal policy model to efficiently explore and adapt to dynamic environments.
    

![](https://aman.ai/primers/ai/assets/RL-for-agents/AgentQ.jpg)

- The following figure illustrates that Agent Q is provided the following input format to the Agent, consisting of the system prompt, execution history, the current observation as a DOM representation, and the user query containing the goal. We divide our Agent output format into an overall step-by-step plan, thought, a command, and a status code.

![](https://aman.ai/primers/ai/assets/RL-for-agents/AgentQ1.jpg)

### Monte Carlo Tree Search (MCTS) in RL-Based Agents

- **Monte Carlo Tree Search (MCTS)**Â is a planning algorithm that estimates the value of actions through simulation. Each node in the search tree represents a state, and edges represent actions. During training, the agent builds a partial search tree by simulating action sequences, updating node values using empirical rollouts.
    
- At each decision step, MCTS performs four core operations:
    
    1. **Selection:**Â Traverse the current tree from the root to a leaf, selecting child nodes using theÂ _Upper Confidence Bound_Â (UCB) rule:
        
        at=argmaxa[Q(st,a)+clnN(st)1+N(st,a)â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš]
        
        - whereÂ Q(st,a)Â is the estimated action value,Â N(st,a)Â the visit count, andÂ cÂ a confidence constant.
    2. **Expansion:**Â Add one or more new child nodes to the tree.
        
    3. **Simulation:**Â Run a rollout (either with a learned policy or random actions) to estimate the outcome.
        
    4. **Backpropagation:**Â UpdateÂ Q(st,a)Â values along the traversed path with the observed return.
        
- This method balances exploration and exploitation dynamicallyâ€”favoring actions with high potential but uncertain estimates.
    
- In the context of LLM-based web agents, MCTS is adapted to exploreÂ _semantic_Â andÂ _structural_Â decision spaces rather than numeric ones. Each node can represent:
    
    - A browser state (DOM snapshot, active page).
    - A reasoning context (prompt, plan, partial output).
    - A tool invocation (function call, API parameterization).
- MCTS then simulates different reasoning or action trajectories, evaluates their predicted rewards (using a reward model or preference score), and backpropagates this information to refine the policy.
    
- Recent approaches such asÂ [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al. (2025) extend this principle by introducing aÂ **reasoning-based experience model**Â that performs analogous â€œtree searchâ€ operations within a learned world modelâ€”sampling synthetic trajectories that approximate MCTS rollouts without direct environment interaction, thereby dramatically improving sample efficiency.
    

### Neural-Guided Search: Policy Priors and Value Models

- In environments too large for exhaustive search, modern agents employÂ **neural-guided search**â€”a synergy betweenÂ _planning algorithms_Â andÂ _deep models_. Here, the policy modelÂ Ï€Î¸(aâˆ£s)Â provides prior probabilities for which actions to explore first, and the value modelÂ VÎ¸(s)Â predicts the expected return from each state. These models drastically reduce the branching factor and enable more efficient exploration.
    
- This framework mirrors the principles that poweredÂ **AlphaGo**Â ([Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961)Â by Silver et al., 2016), but applied toÂ _symbolic and text-based tasks_Â instead of games.
    
- Formally, the modified UCB rule becomes:
    
    U(s,a)=Q(s,a)+cpuctP(a|s)N(s)â€¾â€¾â€¾â€¾âˆš1+N(s,a)
    
    - whereÂ P(aâˆ£s)Â is the prior probability from the policy model. This ensures that exploration is guided by learned likelihoods, not uniform randomness.
- InÂ [Agent Q](https://arxiv.org/abs/2408.07199)Â by Putta et al. (2024), this concept is applied toÂ **online adaptation**: the agent uses MCTS for planning while simultaneously updating its local policy parameters via gradient descent, achieving a form of continual self-improvement.
    
- Early Experience pretraining complements neural-guided search by shaping the priorsÂ P(aâˆ£s)Â and valuesÂ V(s)Â before any explicit MCTS integration. By learning predictive transitions and reflective rationales ([Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)Â by Zhang et al., 2025), the agent begins search from a semantically meaningful latent space rather than random initializationâ€”reducing both exploration cost and tree-depth requirements.
    

### Integration of Search with Reinforcement Learning and Fine-Tuning

- Search algorithms such as MCTS can be integrated with RL training in three primary ways:
    
    1. **Search as Pretraining:**Â Generate high-quality trajectories via MCTS and use them for supervised fine-tuning (similar to imitation learning).
        
    2. **Search as Online Exploration:**Â Use MCTS during training to propose promising action sequences; the policy learns to imitate successful trajectories while exploring uncertain branches.
        
    3. **Search as Evaluation:**Â Use MCTS only at inference to refine action selection, keeping policy updates purely gradient-based.
        
- InÂ [Agent Q](https://arxiv.org/abs/2408.07199), this second modeâ€”_online search and adaptation_â€”proved especially effective, enabling agents to generalize across unseen tasks without explicit retraining.
    
- DreamGymâ€™s synthetic environment model provides a complementary fourth paradigm:Â **Search via Experience Synthesis.**Â Here, simulated rollouts within a learned reasoning environment substitute for explicit tree expansion, allowing policies to update from a massive, low-cost replay buffer of synthetic â€œsearch traces.â€ This merges the sample efficiency of model-based RL with the decision quality of tree search ([Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)Â by Chen et al., 2025).
    

### Process-Wise Reward Shaping in Search-Based RL

- A key enhancement in modern search-based RL pipelines is the introduction ofÂ **process-wise reward shaping**Â to complement sparse terminal rewards. In multi-turn or tool-using agents, MCTS nodes can be augmented with intermediate reward estimates derived from:
    
    - Successful API or function calls,
    - Reduced error rates or failed action counts,
    - Improved subgoal completion,
    - Positive sentiment or human approval scores.
- This transforms the reward signal from a binary success/failure into a smooth landscape that supportsÂ _credit assignment_Â across deep search trees.
    
- The adjusted value propagation for a trajectory of lengthÂ TÂ becomes:
    
    Q(st,at)â†(1âˆ’Î·)Q(st,at)+Î·âˆ‘k=tTÎ³kâˆ’trprocessk
    
    - whereÂ rprocesskÂ captures per-step quality signals. This formulation allows the agent to refine sub-policies even when full-task success has not yet been achievedâ€”vital for real-world agents that must learn under incomplete supervision.

### Integration of Search with Reinforcement Learning and Fine-Tuning

- Search algorithms such as MCTS can be integrated with RL training in three primary ways:
    
    1. **Search as Pretraining:**Â Generate high-quality trajectories via MCTS and use them for supervised fine-tuning (similar to imitation learning).
        
    2. **Search as Online Exploration:**Â Use MCTS during training to propose promising action sequences; the policy learns to imitate successful trajectories while exploring uncertain branches.
        
    3. **Search as Evaluation:**Â Use MCTS only at inference to refine action selection, keeping policy updates purely gradient-based.
        
- InÂ [Agent Q](https://arxiv.org/abs/2408.07199), this second modeâ€”_online search and adaptation_â€”proved especially effective, enabling agents to generalize across unseen tasks without explicit retraining.
    

### Exploration Strategies in Web and Computer-Use Environments

- In high-dimensional digital environments, exploration must be structured and interpretable. Several strategies are commonly used:
    
    - **Entropy-Regularized Exploration:**Â Adding an entropy term to the objective encourages diversity in action selection:
        
        J(Ï€)=ğ”¼Ï€[âˆ‘t(rt+Î²,H(Ï€(â‹…|st)))]
        
        - whereÂ H(Ï€)Â is policy entropy andÂ Î²Â controls exploration intensity.
    - **Curiosity-Driven Exploration:**Â Agents are rewarded for discovering novel or unpredictable states using intrinsic motivation models such asÂ [Random Network Distillation](https://arxiv.org/abs/1810.12894)Â by Burda et al. (2019).
        
    - **Goal-Conditioned Exploration:**Â Particularly in web tasks, exploration can be constrained by semantic or user-defined goals, ensuring the agent does not perform irrelevant actions.
        
    - **State Abstraction and Clustering:**Â Complex environments can be segmented into abstract state representations (e.g., webpage templates or tool invocation graphs), allowing for hierarchical exploration.
        
- These approaches are especially effective inÂ _multi-turn environments_Â scenarios where the state space expands combinatorially with each decision.
    

### Planning and Value Composition Across Multiple Environments

- The integration of search-based reasoning with learned RL policies allows agents toÂ _compose behaviors across environment types_. For instance:
    
    - InÂ **single-turn environments**, search helps refine output reasoning (e.g., multi-step chain-of-thought validation).
    - InÂ **tool-use environments**, it aids in selecting optimal tool invocation sequences.
    - InÂ **multi-turn environments**, it supports long-horizon planning and dynamic replanning when goals change.
- The combined expected return from multi-environment value composition can be expressed as:
    
    Jglobal=âˆ‘eâˆˆEÏ‰eğ”¼Ï€e[âˆ‘tÎ³tr(e)t]
    
    - whereÂ EÂ denotes environment types (SingleTurn, Tool, MultiTurn) andÂ Ï‰eÂ are task-specific weights.
- This hierarchical structure aligns exploration depth with task complexity, improving sample efficiency and stability.
    

### Summary and Outlook

- Search-based RL represents a crucial step in bridgingÂ **symbolic planning**Â andÂ **neural policy learning**Â for complex, real-world agents.
    
    - **Monte Carlo Tree Search (MCTS)**Â provides structured exploration with statistical guarantees.
    - **Neural-guided search**Â integrates learned policy and value priors for scalability.
    - **Process-wise rewards**Â smooth sparse reward landscapes, enabling deeper credit assignment.
    - **Hybrid searchâ€“RL systems**Â enable online adaptation and continual learning.
- As web and computer-use agents evolve, search-based strategies are increasingly viewed not as add-ons but asÂ _core cognitive modules_, empowering agents to deliberate, simulate, and refine decisionsâ€”much like human reasoning.

----

