
[Distilled AI](https://aman.ai/primers/ai/)[Back to aman.ai](https://aman.ai/)

# Primers • Model Acceleration

- [Inference Optimizations](https://aman.ai/primers/ai/model-acceleration/#inference-optimizations)
    - [Overview](https://aman.ai/primers/ai/model-acceleration/#overview-1)
    - [KV Cache](https://aman.ai/primers/ai/model-acceleration/#kv-cache)
        - [Background: Self-Attention](https://aman.ai/primers/ai/model-acceleration/#background-self-attention)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation)
            - [The Problem: Quadratic Recomputation in Naive Generation](https://aman.ai/primers/ai/model-acceleration/#the-problem-quadratic-recomputation-in-naive-generation)
            - [Why Naive Generation Fails](https://aman.ai/primers/ai/model-acceleration/#why-naive-generation-fails)
            - [The Solution: Reusing Cached Representations](https://aman.ai/primers/ai/model-acceleration/#the-solution-reusing-cached-representations)
            - [Why This Matters](https://aman.ai/primers/ai/model-acceleration/#why-this-matters)
        - [Structure and Size of the KV Cache](https://aman.ai/primers/ai/model-acceleration/#structure-and-size-of-the-kv-cache)
        - [Caching Self-Attention Values](https://aman.ai/primers/ai/model-acceleration/#caching-self-attention-values)
            - [Why Not Cache Prior Queries?](https://aman.ai/primers/ai/model-acceleration/#why-not-cache-prior-queries)
        - [Autoregressive Decoding Process with Caching](https://aman.ai/primers/ai/model-acceleration/#autoregressive-decoding-process-with-caching)
        - [Implementation Details](https://aman.ai/primers/ai/model-acceleration/#implementation-details)
            - [Cache Tensor Shape](https://aman.ai/primers/ai/model-acceleration/#cache-tensor-shape)
            - [Prefill Phase](https://aman.ai/primers/ai/model-acceleration/#prefill-phase)
            - [Updates to the KV Cache](https://aman.ai/primers/ai/model-acceleration/#updates-to-the-kv-cache)
        - [Latency Optimization/Savings](https://aman.ai/primers/ai/model-acceleration/#latency-optimizationsavings)
            - [Projection Cost](https://aman.ai/primers/ai/model-acceleration/#projection-cost)
            - [Attention Score Computation](https://aman.ai/primers/ai/model-acceleration/#attention-score-computation)
            - [Total Complexity](https://aman.ai/primers/ai/model-acceleration/#total-complexity)
        - [Practical Deployment Considerations](https://aman.ai/primers/ai/model-acceleration/#practical-deployment-considerations)
        - [Practical Deployment Considerations](https://aman.ai/primers/ai/model-acceleration/#practical-deployment-considerations-1)
            - [Memory Management](https://aman.ai/primers/ai/model-acceleration/#memory-management)
            - [Dynamic Batching](https://aman.ai/primers/ai/model-acceleration/#dynamic-batching)
            - [Cache Parallelism](https://aman.ai/primers/ai/model-acceleration/#cache-parallelism)
            - [Why You Can’t Always Cache Everything](https://aman.ai/primers/ai/model-acceleration/#why-you-cant-always-cache-everything)
        - [Multi-Head Attention and KV Cache](https://aman.ai/primers/ai/model-acceleration/#multi-head-attention-and-kv-cache)
        - [Summary of KV Cache Benefits](https://aman.ai/primers/ai/model-acceleration/#summary-of-kv-cache-benefits)
        - [KV Sharing](https://aman.ai/primers/ai/model-acceleration/#kv-sharing)
            - [How KV Sharing Works](https://aman.ai/primers/ai/model-acceleration/#how-kv-sharing-works)
            - [FLOP Savings](https://aman.ai/primers/ai/model-acceleration/#flop-savings)
            - [Why KV Sharing Can Work](https://aman.ai/primers/ai/model-acceleration/#why-kv-sharing-can-work)
            - [Memory Benefits](https://aman.ai/primers/ai/model-acceleration/#memory-benefits)
            - [Deployment Notes](https://aman.ai/primers/ai/model-acceleration/#deployment-notes)
    - [Model Quantization](https://aman.ai/primers/ai/model-acceleration/#model-quantization)
        - [Why Quantize?](https://aman.ai/primers/ai/model-acceleration/#why-quantize)
        - [Types of Quantization](https://aman.ai/primers/ai/model-acceleration/#types-of-quantization)
            - [Post-Training Quantization (PTQ)](https://aman.ai/primers/ai/model-acceleration/#post-training-quantization-ptq)
            - [Quantization-Aware Training (QAT)](https://aman.ai/primers/ai/model-acceleration/#quantization-aware-training-qat)
        - [Static vs. Dynamic Quantization](https://aman.ai/primers/ai/model-acceleration/#static-vs-dynamic-quantization)
        - [Quantization in Transformers](https://aman.ai/primers/ai/model-acceleration/#quantization-in-transformers)
        - [Tooling and Frameworks](https://aman.ai/primers/ai/model-acceleration/#tooling-and-frameworks)
    - [Operator Fusion](https://aman.ai/primers/ai/model-acceleration/#operator-fusion)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-1)
        - [Common Fusion Patterns](https://aman.ai/primers/ai/model-acceleration/#common-fusion-patterns)
        - [Fusion in Transformers](https://aman.ai/primers/ai/model-acceleration/#fusion-in-transformers)
        - [Implementation Details](https://aman.ai/primers/ai/model-acceleration/#implementation-details-1)
            - [Graph-Level Fusion (Ahead-of-Time)](https://aman.ai/primers/ai/model-acceleration/#graph-level-fusion-ahead-of-time)
            - [Kernel-Level Fusion (Runtime)](https://aman.ai/primers/ai/model-acceleration/#kernel-level-fusion-runtime)
            - [3. Custom Kernel Generation](https://aman.ai/primers/ai/model-acceleration/#3-custom-kernel-generation)
        - [Performance Impact](https://aman.ai/primers/ai/model-acceleration/#performance-impact)
        - [Tooling and Ecosystem](https://aman.ai/primers/ai/model-acceleration/#tooling-and-ecosystem)
    - [Speculative Decoding](https://aman.ai/primers/ai/model-acceleration/#speculative-decoding)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-2)
        - [Basic Algorithm](https://aman.ai/primers/ai/model-acceleration/#basic-algorithm)
        - [Pseudocode](https://aman.ai/primers/ai/model-acceleration/#pseudocode)
        - [Key Parameters](https://aman.ai/primers/ai/model-acceleration/#key-parameters)
        - [Mathematical View](https://aman.ai/primers/ai/model-acceleration/#mathematical-view)
        - [Implementation Details](https://aman.ai/primers/ai/model-acceleration/#implementation-details-2)
        - [Notable Variants](https://aman.ai/primers/ai/model-acceleration/#notable-variants)
        - [Benefits](https://aman.ai/primers/ai/model-acceleration/#benefits)
        - [Limitations](https://aman.ai/primers/ai/model-acceleration/#limitations)
    - [FlashAttention and Efficient Attention Kernels](https://aman.ai/primers/ai/model-acceleration/#flashattention-and-efficient-attention-kernels)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-3)
        - [FlashAttention: Key Concepts](https://aman.ai/primers/ai/model-acceleration/#flashattention-key-concepts)
            - [High-Level Algorithm](https://aman.ai/primers/ai/model-acceleration/#high-level-algorithm)
            - [Numerical Stability](https://aman.ai/primers/ai/model-acceleration/#numerical-stability)
        - [Implementation Details](https://aman.ai/primers/ai/model-acceleration/#implementation-details-3)
        - [FlashAttention-2 Improvements](https://aman.ai/primers/ai/model-acceleration/#flashattention-2-improvements)
        - [Other Efficient Kernels](https://aman.ai/primers/ai/model-acceleration/#other-efficient-kernels)
        - [Performance Gains](https://aman.ai/primers/ai/model-acceleration/#performance-gains)
        - [Use in Inference](https://aman.ai/primers/ai/model-acceleration/#use-in-inference)
        - [Integration](https://aman.ai/primers/ai/model-acceleration/#integration)
    - [Batching, Sequence Packing, and Prefilling](https://aman.ai/primers/ai/model-acceleration/#batching-sequence-packing-and-prefilling)
        - [Batching](https://aman.ai/primers/ai/model-acceleration/#batching)
            - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-4)
            - [Types of Batching](https://aman.ai/primers/ai/model-acceleration/#types-of-batching)
            - [Padding and Masking](https://aman.ai/primers/ai/model-acceleration/#padding-and-masking)
            - [Performance Benefits](https://aman.ai/primers/ai/model-acceleration/#performance-benefits)
        - [Sequence Packing](https://aman.ai/primers/ai/model-acceleration/#sequence-packing)
            - [Example](https://aman.ai/primers/ai/model-acceleration/#example)
            - [Benefits](https://aman.ai/primers/ai/model-acceleration/#benefits-1)
            - [Trade-offs](https://aman.ai/primers/ai/model-acceleration/#trade-offs)
        - [Prefilling](https://aman.ai/primers/ai/model-acceleration/#prefilling)
            - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-5)
            - [Prefilling Logic](https://aman.ai/primers/ai/model-acceleration/#prefilling-logic)
            - [Optimizations](https://aman.ai/primers/ai/model-acceleration/#optimizations)
            - [Real-World Use](https://aman.ai/primers/ai/model-acceleration/#real-world-use)
            - [Performance Benefits](https://aman.ai/primers/ai/model-acceleration/#performance-benefits-1)
    - [Prompt Caching](https://aman.ai/primers/ai/model-acceleration/#prompt-caching)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-6)
        - [Basic Mechanism](https://aman.ai/primers/ai/model-acceleration/#basic-mechanism)
        - [Implementation Details](https://aman.ai/primers/ai/model-acceleration/#implementation-details-4)
            - [Cache Granularity](https://aman.ai/primers/ai/model-acceleration/#cache-granularity)
            - [Cache Storage](https://aman.ai/primers/ai/model-acceleration/#cache-storage)
            - [Integration with Serving Systems](https://aman.ai/primers/ai/model-acceleration/#integration-with-serving-systems)
        - [Performance Impact](https://aman.ai/primers/ai/model-acceleration/#performance-impact-1)
        - [Applications](https://aman.ai/primers/ai/model-acceleration/#applications)
        - [Limitations](https://aman.ai/primers/ai/model-acceleration/#limitations-1)
    - [Early Exit and Token Pruning](https://aman.ai/primers/ai/model-acceleration/#early-exit-and-token-pruning)
        - [Early Exit](https://aman.ai/primers/ai/model-acceleration/#early-exit)
            - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-7)
            - [Mechanism](https://aman.ai/primers/ai/model-acceleration/#mechanism)
            - [Implementation](https://aman.ai/primers/ai/model-acceleration/#implementation)
            - [Benefits](https://aman.ai/primers/ai/model-acceleration/#benefits-2)
            - [Limitations](https://aman.ai/primers/ai/model-acceleration/#limitations-2)
        - [Token Pruning](https://aman.ai/primers/ai/model-acceleration/#token-pruning)
            - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-8)
            - [Mechanism](https://aman.ai/primers/ai/model-acceleration/#mechanism-1)
            - [Implementation](https://aman.ai/primers/ai/model-acceleration/#implementation-1)
            - [Benefits](https://aman.ai/primers/ai/model-acceleration/#benefits-3)
            - [Limitations](https://aman.ai/primers/ai/model-acceleration/#limitations-3)
        - [Tools and Research](https://aman.ai/primers/ai/model-acceleration/#tools-and-research)
    - [Hardware-Aware Scheduling](https://aman.ai/primers/ai/model-acceleration/#hardware-aware-scheduling)
        - [Motivation](https://aman.ai/primers/ai/model-acceleration/#motivation-9)
        - [Core Techniques](https://aman.ai/primers/ai/model-acceleration/#core-techniques)
            - [Stream Parallelism](https://aman.ai/primers/ai/model-acceleration/#stream-parallelism)
            - [Tensor Core Utilization](https://aman.ai/primers/ai/model-acceleration/#tensor-core-utilization)
            - [Operator Placement and Reordering](https://aman.ai/primers/ai/model-acceleration/#operator-placement-and-reordering)
            - [KV Cache Management](https://aman.ai/primers/ai/model-acceleration/#kv-cache-management)
            - [Pipeline and Model Parallelism](https://aman.ai/primers/ai/model-acceleration/#pipeline-and-model-parallelism)
            - [Custom Kernel Scheduling](https://aman.ai/primers/ai/model-acceleration/#custom-kernel-scheduling)
            - [Cache and Memory Prefetching](https://aman.ai/primers/ai/model-acceleration/#cache-and-memory-prefetching)
        - [Deployment-Aware Strategies](https://aman.ai/primers/ai/model-acceleration/#deployment-aware-strategies)
        - [Ecosystem Support](https://aman.ai/primers/ai/model-acceleration/#ecosystem-support)
        - [Performance Impact](https://aman.ai/primers/ai/model-acceleration/#performance-impact-2)
    - [Comparative Analysis](https://aman.ai/primers/ai/model-acceleration/#comparative-analysis)
- [References](https://aman.ai/primers/ai/model-acceleration/#references)
- [Citation](https://aman.ai/primers/ai/model-acceleration/#citation)

## 训练优化

### 概述

LLM 的训练优化旨在降低训练阶段的计算和内存开销，同时保持模型质量。随着 LLM 规模和序列长度的增加，传统的注意力机制和密集架构因其高计算和内存需求（尤其是自注意力机制的二次复杂度）而成为瓶颈。

本节探讨旨在通过算法和系统层面的改进来加速训练的创新。这些创新包括：

* **内存感知注意力算法**：类似 FlashAttention 和 FlashAttention-2 这类，通过优化 GPU 内存层级（例如从高带宽存储器 HBM 到静态随机存储器 SRAM）之间的数据传输，显著降低了内存带宽占用和计算时间。这些方法采用分块计算、重计算以及注意力模块并行化等技术，优先提升硬件运行效率。
* **多查询和分组查询注意力方法**，例如在快速 Transformer 解码和 GQA 论文中提出的那些方法，通过共享键/值投影来减少注意力头中的冗余。这些技术对于加速解码和推理尤其有价值，同时还能减少训练期间的参数数量和计算成本。
* **稀疏和局部注意力机制**，如 Longformer 中引入的方案，用局部窗口化和任务特定的全局注意力组合取代全局自注意力。这种方法将内存消耗和计算时间从序列长度的二次方降低到线性，从而能够在更长的序列上进行高效训练。

这些方法共同代表了越来越多重新思考 Transformer 架构及其内存-计算权衡的研究成果。它们旨在使 LLM 的训练更具可扩展性、高效性和普及性，为快速迭代和在受限硬件上部署性能日益强大的模型铺平道路。后续章节将更详细地探讨具体技术及其实证结果。


### FlashAttention

由斯坦福大学的 Dao 等人提出的 [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)。Transformer 模型在处理长序列时速度较慢且内存消耗大，因为自注意力机制的时间和内存复杂度与序列长度呈平方关系。近似注意力方法试图通过牺牲模型质量来降低计算复杂度，但往往无法实现实际运行速度的提升。他们认为关键在于使注意力算法具备 IO 感知能力——即考虑 GPU 内存层级之间的读写操作。

该算法通过分块技术减少 GPU 高带宽内存（HBM）与 GPU 片上 SRAM 之间的内存读写次数。具体而言，FlashAttention 通过重新排序注意力计算过程，并运用经典技术（分块、重计算），显著提升了计算速度，同时将内存占用从序列长度的平方级降低至线性级。

他们分析了 FlashAttention 的 IO 复杂度，结果表明其比标准注意力机制需要更少的 HBM 访问次数，并且对一系列 SRAM 大小来说都是最优的。他们还把 FlashAttention 扩展到块稀疏注意力，产生了一种比现有任何近似注意力方法都更快的近似注意力算法。

FlashAttention 训练 Transformer 模型的速度远超现有基准：在 BERT-large（序列长度 512）上，相比 MLPerf 1.1 的训练速度记录，实现了 15% 的端到端实际时间加速；在 GPT-2（序列长度 1K）上提速 3 倍；在长序列竞技场基准（序列长度 1K-4K）上提速 2.4 倍。

FlashAttention 和块稀疏 FlashAttention 技术能够扩展 Transformer 模型的上下文长度，从而提升模型质量（在 GPT-2 上困惑度降低 0.7，长文档分类任务提升 6.4 个百分点），并赋予模型全新能力：首次实现 Path-X 挑战（序列长度 16K，准确率 61.4%）和 Path-256 挑战（序列长度 64K，准确率63.1%）中超越随机猜测性能的 Transformer 模型。

论文中的下图展示了：（左）FlashAttention 通过分块处理避免了大型 $N×N$ 注意力矩阵（虚线框）在（相对）较慢的 GPU 高带宽存储器上的实体化。在外层循环（红色箭头）中，FlashAttention 遍历 $K$ 矩阵和 $V$ 矩阵的块，并将它们加载到快速的片上静态随机存取存储器中。在每个块中，FlashAttention 循环遍历 $Q$ 矩阵的块（蓝色箭头），将它们加载到 SRAM中，并将注意力计算的输出写回 HBM。右图：与 GPT-2 上 PyTorch 实现的注意力机制相比的速度提升。FlashAttention 不会读写大型 $N×N$ 注意力矩阵到 HBM，从而使注意力计算速度提高了 7.6 倍。

![](https://aman.ai/images/papers/FlashAttention.jpg)


### FlashAttention-2

由普林斯顿大学和斯坦福大学的 Dao 在 [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://tridao.me/publications/flash2/flash2.pdf) 中提出。 将 Transformer 模型扩展到更长的序列长度是过去几年中的一个主要问题，有望提升语言建模和高分辨率图像理解的性能，并解锁代码、音频和视频生成中的新应用。注意力层是扩展到更长序列的主要瓶颈，因为其运行时间和内存消耗随序列长度呈二次方增长。

FlashAttention 利用 GPU 内存层次结构的不对称性，实现了显著的内存节省（线性而非二次方）和运行速度提升（相比优化基准快 2-4 倍），且无需近似计算。然而，FlashAttention 的速度仍远不及优化的矩阵乘法（GEMM）操作，仅达到理论最大 FLOPs/s 的 25-40%。

他们观察到，这种低效是由于 GPU 上不同线程块和线程束之间的工作分配不够优化，导致要么占用率低，要么出现不必要的共享内存读写。

普林斯顿大学和斯坦福大学的 Dao 团队在本篇论文中提出了 FlashAttention-2 算法，通过优化工作分区策略解决上述问题。具体而言，他们（1）调整算法以减少非矩阵乘法浮点运算量；（2）将注意力计算（包括单头注意力）并行化分配到不同线程块以提高占用率；（3）在每个线程块内通过 warp 间任务分配减少共享内存通信。相比 FlashAttention，这些优化实现了约 2 倍的加速效果，在 A100 显卡上达到理论最大浮点运算性能的 50-73%，接近通用矩阵乘法（GEMM）运算的效率水平。

他们通过实证验证，当采用端到端方式训练 GPT 风格模型时，FlashAttention-2 在每块 A100 GPU 上可实现高达 225 TFLOPs/s 的训练速度（模型 FLOPs 利用率达 72%）。[Sebastian Raschka](https://www.linkedin.com/in/sebastianraschka/)提供的下图总结了 FlashAttention-2：

![](https://aman.ai/images/papers/FlashAttention-2.webp)

### FlashAttention-3

由 Shah 等人（来自 Colfax Research、Meta、NVIDIA、佐治亚理工学院、普林斯顿大学和 Together AI）在 [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608) 中提出。FlashAttention-3 是针对 NVIDIA Hopper 架构GPU（H100）优化的注意力机制，通过利用硬件异步性和 FP8 低精度计算能力，实现了显著的加速效果和精度提升。

**主要贡献：**

* **生产者-消费者异步模式**：通过环形共享内存缓冲区实现 warp 专用软件流水线，将生产者 warp（通过 TMA 进行数据移动）与消费者 warp（张量核心 GEMM 运算）分离，从而隐藏内存和指令延迟。
  - **GEMM-softmax重叠**：通过“乒乓”调度在 warpgroups 之间以及 warpgroup 内部采用两阶段流水线技术，打破顺序依赖关系，实现分块 $QK^⊤$和 PV GEMM 运算与 softmax 的流水线并行，从而同时保持张量核心和特殊功能单元的活跃状态。
  - **FP8 低精度支持**：通过内核内转置（使用 LDSM/STSM）和寄存器置换，使 FlashAttention 适应 FP8 WGMMA 布局约束，并通过块量化和使用随机正交变换的非相干处理来提高 FP8 的精度。

**架构和实现：**

**输入**：查询（$Q$）、键（$K$）、值（$V$）矩阵被分割成小块；头维度 $d$，序列长度 $N$，查询块大小 $B_r$，键块大小 $B_c$。

**前向过程（FP16）：**

* 生产者扭曲：使用 TMA 从 HBM 依次加载 $Q_{i}、K_{j}、V_{j}$ 图块到 SMEM，并通过屏障通知消费者。
- 消费者扭曲：执行SS-GEMM（$Q_iK^⊤_j$）、逐行最大值跟踪、局部 softmax、RS-GEMM（$P_{ij}​V_j$​），并进行缩放以确保稳定性，将 $O_i$ 和对数求和指数值 $L_i$ 写入 HBM。
- 流水线版本：将迭代 $j$ 的 GEMM 与迭代 $j+1$ 的 softmax 重叠执行，需要额外的寄存器缓冲区（$S_\text{next}$）。

**FP8 模式**

* 布局处理：通过内核内转置确保第二个 GEMM 中的 $V$ 采用 $k$ 主序操作数布局；寄存器排列使 FP32 累加器与 FP8 操作数布局对齐。
* 量化：块级缩放（针对每个 $B_r×d$ 或 $B_c×d$ 分块）和非相干处理（哈达玛变换+随机±1对角矩阵）可降低异常值密集张量的均方根误差。
* 论文中的下图展示了两个 warpgroup 之间的乒乓调度，以实现 softmax 和 GEMM 的重叠计算：当一个 warpgroup 执行 GEMM 运算时，应调度另一个 warpgroup 执行 softmax 运算。相同颜色代表同一迭代步骤。

![](https://aman.ai/images/papers/FlashAttention%E2%80%913_1.jpg)
  - 论文中的下图展示了 2 阶段 WGMMA-softmax 流水线处理。
  
  ![](https://aman.ai/images/papers/FlashAttention%E2%80%913_2.jpg)

**基准测试：**

* 在 H100 SXM5 上，FP16 前向传递最高可达 740 TFLOPs/s（利用率 75%），比 FlashAttention-2 快 1.5-2.0 倍，比标准注意力机制快 3-16 倍；后向传递速度提升 1.5-1.75 倍。
* FP8前向传递接近1.2 PFLOPs/s，在某些头维度和序列长度上优于cuDNN。
* 准确性：FP16 与 FlashAttention-2 的误差相当（约 1.9×10−4 RMSE），两者均优于标准 FP16 注意力机制；采用块量化和非相干处理的 FP8，其 RMSE 比基线 FP8 逐张量缩放低 2.6 倍。

**消融研究：**

* 移除 GEMM-softmax 流水线或 warp specialization 会将吞吐量从 661 TFLOPs/s 降低至约 570-582 TFLOPs/s。
* 这两种优化都对性能提升有显著贡献。


### Multi-Query Attention (MQA)

在 [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150) 中被提出，Transformer 神经网络序列模型中采用的多头注意力层，是替代循环神经网络（RNN）在序列间传递信息的强大方案。虽然由于序列长度的可并行性，训练这些层通常快速且简单，但在无法实现这种并行化的增量推理场景中，由于需要反复加载庞大的"键"和"值"张量所带来的内存带宽开销，推理过程往往较为缓慢。

这篇由谷歌的 Shazeer 于 2019 年发表的论文提出了一种名为"多头查询注意力"(MQA)的变体，其中键和值在所有不同的注意力"头"之间共享，从而大大减少了这些张量的大小，进而降低了增量解码对内存带宽的需求。

他们通过实验验证，所得到的模型确实可以显著加快解码速度，并且与基线相比仅造成轻微的质量下降。

### Grouped-Query Attention (GQA)

在 [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) 中被提出。MQA（仅使用单个键值头）显著加速了解码器推理。然而，MQA 可能导致质量下降，而且仅为加速推理而训练单独模型可能并不理想。

来自谷歌研究的 Ainslie 等人的论文提出：(1) 一种方法，用原始预训练计算量的5%将现有多头语言模型检查点升级为 MQA 模型；(2) 引入分组查询注意力（GQA），这是多查询注意力（MQA）的泛化形式，使用介于多头和单头之间的键值头数量。论文中的下图展示了分组查询方法的概览。多头注意力有 $H$ 个查询、键和值头。多查询注意力在所有查询头之间共享单个键和值头。分组查询注意力则是在每组查询头之间共享单个键和值头，介于多头和多查询注意力之间。

![](https://aman.ai/images/papers/GQA.jpg)

MQA 采用单一键值头来加速解码器推理，但可能导致质量下降。作者提出了一种新颖方法，将现有的多头注意力（MHA）语言模型检查点转化为带有 MQA 的模型，仅需原始预训练计算量的 5%。

该论文提出了分组查询注意力（GQA），这是一种介于多头注意力和多查询注意力之间的中间方法。在 GQA 中，查询头被分成若干组，每组共享一个键头和值头。这种方法使得经过训练的 GQA 模型能够以接近 MHA 的质量和与 MQA 相当的速度运行。

在 T5.1.1 架构上进行的多项实验（涵盖 CNN/Daily Mail、arXiv、PubMed、MediaSum、Multi-News、WMT 和 TriviaQA 等数据集）表明，GQA 模型在推理速度与质量之间实现了良好平衡。该研究包含消融实验，用于评估不同的建模选择，例如 GQA 组的数量和检查点转换方法。这些实验提供了模型在不同配置下性能的深入见解。

该论文承认了一些局限性，例如对较长序列的评估挑战，以及缺乏与从头开始训练的模型进行比较。论文还指出，研究结果尤其适用于编码器-解码器模型，并表明 GQA 在仅解码器模型中可能具有更强的优势。他们证明，经过优化的 GQA 在质量上接近多头注意力机制，同时速度与 MQA 相当。

### Linear Attention

 由 Facebook AI 的 Wang 等在 [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768) 中提出。作者提出了一种优化 Transformer 模型中自注意力机制的新方法，将复杂度从序列长度的平方级降低至线性级。该方法名为 Linformer，在保持与标准 Transformer 模型相当性能的同时，显著提升了时间和内存使用效率。
 
Linformer 通过引入自注意力机制的低秩近似，从实证和理论层面证明了自注意力矩阵具有低秩特性。作者提出将原始的缩放点积注意力分解为多个通过线性投影实现的较小注意力模块。这种因式分解方法将自注意力的空间和时间复杂度从 $O(n²)$ 显著降低至 $O(n)$，有效解决了传统 Transformer 模型的可扩展性问题。

该模型架构在计算注意力之前，将键矩阵和值矩阵投影到低维空间，从而在保持模型有效性的同时降低了计算需求。该方法还包括跨投影参数共享的选项，可在不明显影响性能的情况下进一步减少可训练参数的数量。

总而言之，Linformer 通过以下方式实现线性时间注意力机制：

1. **低秩近似**：Linformer 的核心思想在于观察到自注意力机制可以通过低秩矩阵来近似。这意味着 Transformer 中自注意力捕获的复杂关系并不一定需要满秩矩阵，从而可以实现更高效的表示。
2. **降低复杂度**：传统 Transformer 中的标准自注意力机制在序列长度 $n$ 方面具有 $O(n²)$ 的时间与空间复杂度，而 Linformer 将其降低至 $O(n)$。这种显著的复杂度降低同时体现在时间与空间维度上，使其在处理长序列时效率大幅提升。
3. **线性自注意力机制**：Linformer 通过线性投影将缩放点积注意力分解为多个较小的注意力来实现这一目标。具体来说，它引入了两个线性投影矩阵 $E_i$ 和 $F_i$，用于计算键矩阵和值矩阵。通过首先将原始高维键矩阵和值矩阵投影到低维空间（$n×k$），Linformer 有效地降低了注意力机制的复杂度。
4. **操作组合**：这些操作的组合形成了原始注意力矩阵的低秩分解。本质上，Linformer 通过用一系列更小、更易管理的操作来近似完整的注意力机制，从而简化了计算过程，这些操作共同捕捉了原始全秩注意力的基本特征。

论文中的下图展示了：（左下方和右下）所提出的多头线性自注意力架构及示例；（右上）不同 Linformer 模型的推理时间与序列长度的关系。

![|600](https://aman.ai/images/papers/Linformer.jpg)

实验验证表明，Linformer 在标准自然语言处理任务（如情感分析和问答）上，使用 GLUE 和 IMDB 评论等数据集时，其性能与原版 Transformer 相当甚至更优。值得注意的是，该模型在训练和推理速度上有显著提升，尤其对处理较长序列更为有利。

此外，还测试了多种提升 Linformer 效率的策略，包括不同级别的参数共享以及根据模型内部不同层的特定需求采用非均匀投影维度的方法。作者们指出，Linformer 降低的计算需求不仅使高性能模型更易于获取且更具成本效益，还因能耗减少为更环保的人工智能实践打开了大门。

总之，Linformer 通过利用自注意力矩阵的低秩特性，为 Transformer 提出了一种更高效的自注意力机制。该方法将注意力计算的时间与空间复杂度从二次方降至线性，显著降低了计算负担，尤其适用于长序列处理。这使得 Linformer 在处理大规模数据集或长序列输入任务时成为极具吸引力的选择，而传统 Transformer 由于较高的计算需求在这些场景下可能不太适用。

### Longformer

在 [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) 中被提出，基于 Transformer 的模型由于自注意力操作而无法处理长序列，因为该操作的计算复杂度随序列长度呈二次方增长。

艾伦人工智能研究所的 Beltagy 等人于 2020 年发表的这篇论文，旨在通过引入一种注意力机制（该领域通常称为滑动窗口注意力）来解决这一局限性，这种注意力机制的计算复杂度随序列长度呈线性增长，从而能够轻松处理数千个甚至更多标记的长文档。

Longformer 的注意力机制可直接替代标准自注意力机制，它结合了局部窗口注意力与任务驱动的全局注意力。下图来自论文，比较了完整的自注意力模式和 Longformer 中的注意力模式配置。

![](https://aman.ai/images/papers/Longformer.jpg)

在先前关于长序列变换器研究的基础上，他们对 Longformer 进行了字符级语言建模评估，并在 text8 和 enwik8 数据集上取得了最先进的成果。与大多数先前的工作不同，他们还对 Longformer 进行了预训练，并在各种下游任务上进行了微调。

他们预训练的 Longformer 在长文档任务上始终优于 RoBERTa，并在 WikiHop 和 TriviaQA 上创造了新的最先进成果。最后他们推出了 Longformer-Encoder-Decoder（LED），这是支持长文档生成式序列到序列任务的 Longformer 变体，并在 arXiv 摘要数据集上证明了其有效性。

论文中的下图展示了完整自注意力机制与 Longformer 自注意力不同实现方式的运行时和内存占用情况：Longformer-loop 是非向量化实现，Longformer-chunk 是向量化实现，Longformer-cuda 则是定制 CUDA 内核实现。与完整自注意力机制不同（在当前 GPU 上处理长序列时会耗尽内存），Longformer 的内存占用随序列长度呈线性增长。不同实现方式的速度存在差异，其中向量化实现的Longformer-chunk 速度最快。

![](https://aman.ai/images/papers/Longformer2.jpg)


## 推理优化

### 概述

推理优化是 Transformer 模型部署中一个至关重要的研究和工程领域，特别是在实时性和资源受限的环境中。其目标是在不牺牲预测准确性的前提下，最大限度地降低运行 LLMs 的计算成本和延迟。推理阶段的优化直接影响这些模型在生产系统中的响应速度、可扩展性和实际可行性。

推理中的一个核心挑战在于许多大型语言模型的自回归特性，即每个标记都依赖于先前生成的序列。这种顺序依赖性导致朴素推理的成本高昂，尤其对于长序列而言。为解决这一问题，业界开发了一系列优化技术来提升基于 Transformer 的模型在推理时的性能：

* KV缓存：Transformer 模型中的 KV 缓存是一项关键优化技术，它显著提升了序列生成的效率和速度，成为这类模型在实际应用中部署的核心组件。KV 缓存在自回归解码过程中的应用，及其在延迟优化和可扩展性方面的作用，使其成为高效服务基于 Transformer 的模型不可或缺的部分。它允许存储并复用自注意力层先前计算得到的键值投影，避免冗余计算。这一技术大幅降低了首个标记之后的每个标记推理时间，支持生成长序列，对于实现聊天、流式处理和交互式代理等应用中的低延迟、高吞吐服务至关重要。
* 模型量化：模型量化将权重和激活值的精度从 32 位浮点数（float32）降低为更低位格式，如int8、float8，甚至 4 位表示（如 int4）。这能显著减少内存占用和带宽消耗，使其可在更小的硬件上部署并提升吞吐量。训练后量化（PTQ）和量化感知训练（QAT）是两种常用方法。量化模型得益于更快的矩阵运算和更低能耗，现代工具链（如 NVIDIA TensorRT、Intel Neural Compressor）可对量化算子进行硬件加速，同时将精度损失降至最低。
* 算子融合：算子融合将多个连续的操作（如线性投影、偏置加法、层归一化和激活函数）合并为单一计算内核。这种技术减少了 GPU 或 TPU 上的内存读写操作次数和内核启动开销，从而提升执行效率。例如，将全连接层与 ReLU 激活函数融合为单一内核后，不仅能降低延迟，还能更高效地利用原本因操作碎片化而未被充分利用的 SIMD 或 CUDA 核心。
* 推测解码：推测解码通过使用轻量级草稿模型在单次前向传递中预测多个未来标记，从而加速自回归生成过程。这些候选标记随后由完整但较慢的模型并行验证。若验证通过，则批量接受；否则回滚生成过程。该流程在保持生成保真度的同时，减少了昂贵全模型调用的次数。诸如草稿与目标模型、Medusa 方案、自推测解码、FastRAG 以及英伟达的预填充推测解码等方法均利用该技术，在保证模型输出质量的前提下显著提升吞吐量。
* FlashAttention 与高效注意力内核：FlashAttention 是一种内存高效的注意力算法，它通过分块、融合且适配 GPU 的方式计算注意力输出，避免了生成大型中间注意力矩阵的需求。该算法利用 GPU 的 SRAM 将频繁访问的数据块保留在高速内存中，并通过流式传输部分结果来最小化内存带宽压力。相比传统的基于 softmax 的注意力实现，这种方法在序列长度和批量大小上具有更好的扩展性。FlashAttention-2 及类似内核（如 xFormers、Triton）现已成为高性能 Transformer 推理堆栈中的标准组件。
* 批处理、序列打包和预填充：
	* 批处理将多个推理请求分组为单次执行，从而最大化 GPU 利用率、分摊内核启动开销并提高吞吐量。动态批处理能自适应传入请求的模式，而 token 级批处理（如 vLLM）则通过同步解码步骤来同时服务多个请求，且不会阻塞新请求。
	* 序列打包（Sequence Packing）通过将多个短序列拼接成批次元素内的单个序列张量，并使用注意力掩码防止跨序列注意力，从而最小化填充浪费。这提高了每批次处理的有用 token 密度，减少了内存占用并提升了有效吞吐量，尤其适用于序列长度变化较大的工作负载。
	* 预填充技术会在自回归解码开始前预先计算所有提示词（prompt tokens）的 KV 缓存，从而避免生成过程中的冗余计算。通过融合式预填充内核、提示共享和分层流式处理等优化手段，可进一步降低提示阶段的延迟——该阶段通常是长输入场景下开销最高的环节。
	这三种技术共同作用，能实现硬件高利用率、降低填充开销，并将每个 token 的计算成本降至最低。

* 提示缓存：缓存常用或重复提示的键值（KV）状态——例如系统指令、少量示例或用户自定义模板——从而无需为每个请求重新计算。在聊天或 API 驱动的系统中尤为有效，因为这类系统通常会跨会话使用相同的初始上下文（如“你是一个乐于助人的助手…”）。通过复用提示的 KV 状态，服务器可以完全跳过提示处理阶段，直接利用已初始化的缓存开始生成内容，显著缩短首词生成时间和整体计算量。
* 提前退出与 token 剪枝：提前退出机制允许 Transformer 层在达到置信度阈值或基于熵的停止标准时，终止对特定 token 的推理计算，从而节省后续层的计算开销。token 剪枝则根据学习得到的重要性分数或门控函数，在推理过程中动态移除被判定为无关的 token 或注意力路径。这些技术能在不明显牺牲模型输出质量的前提下降低计算成本，尤其适用于优先考虑速度而非完全精度的部署场景。
* 硬件感知调度：该优化旨在将推理工作负载与底层硬件的具体特性对齐，例如 GPU 内存层次结构、张量核心可用性或流水线并发性。调度策略包括算子放置、内存预取、流优先级设置以及多 GPU 设置的负载均衡。例如，在 NVIDIA GPU 上，框架可能利用 CUDA 流、共享内存和内核融合来最大化吞吐量，而 TPU 推理则可能利用 XLA 编译进行图级优化。经过精细调度的策略可减少资源争用、提高并行性，并最大化每瓦特的总推理吞吐量。

### KV Cache

#### 背景：自注意力机制

在一种简单的实现中，对于每个解码步骤，我们必须为当前序列中的所有标记计算所有层的K和V。如果n是目前为止的标记数量，l是层数，那么每一步需要进行l×(n−1)次矩阵乘法，每次乘法的成本为O(d²)，从而导致：Cost per token=O(l⋅n⋅d2)

#### 动机

在服务 Transformer 模型的场景中，键值（KV）缓存是一项核心优化技术，它能显著提升自回归解码的效率。该技术通过存储先前解码步骤中自注意力机制产生的中间计算结果（特别是键张量和值张量），使得这些数据无需在每个新生成步骤中重复计算。这既降低了推理时间，又减少了冗余内存访问，从而使大语言模型能够高效处理长文本生成任务。


##### The Problem: Quadratic Recomputation in Naive Generation

- During autoregressive generation, each new token depends on all previously generated tokens. In a **naive transformer implementation**, the model recomputes the key K and value V representations for **all tokens** in the sequence at every decoding step and across all layers. This quickly becomes computationally expensive, since the total cost per predicted token for a single attention head is:
    
    O(l⋅n⋅d2)
    
    - where:
        
        - n = number of tokens seen so far (sequence length)
        - l = number of layers (depth)
        - d = model (embedding) dimension
- Without caching, predicting each new token involves:
    
    1. Computing the key and value matrices for all past tokens and for every layer.
    2. Performing matrix multiplications of the form:
        
        K=XWK,V=XWV
        
        - where X is the layer input, and WK, WV are fixed weight matrices.

##### Why Naive Generation Fails

- KV caching fundamentally **solves the quadratic recomputation problem** that arises from this naive approach.
- Without a KV cache, generating even a 100-token response leads to massive redundant computation:
    
    - **Token 1:** compute attention over 1000 context tokens
    - **Token 2:** recompute attention over all 1001 tokens
    - **Token 100:** recompute attention over all 1099 tokens
- The total number of attention computations can be derived from the arithmetic sum of all attention lengths per decoding step:

∑t=1100(1000+t−1)=1000×100+100×992=55,000

- Here’s what each term means:
    
    - The **1000** represents the fixed-length context available before generation begins (e.g., the prompt).
    - The **(t − 1)** accounts for the number of **previously generated tokens** already added before generating token t. At step t, the model has already generated t−1 new tokens on top of the initial context, so it must now attend to 1000+(t−1) total tokens.
    - The summation over all 100 decoding steps gives the total number of attention operations across the full generation.
- Thus, to generate 100 tokens, the model performs approximately **55,000 redundant attention computations** — most of which are recomputations of previously calculated keys and values.
    
- The inefficiency is striking:
    
    > Without KV cache: 100 output tokens = ~55,000 attention operations With KV cache: 100 output tokens = 100 attention operations (≈550× reduction)
    
- This highlights the key trade-off: **KV caching exchanges memory usage for compute savings**. By storing previously computed keys and values, the model avoids redoing work it has already completed—unlocking massive gains in speed and scalability.
    

##### The Solution: Reusing Cached Representations

- The KV cache optimization addresses this problem by **reusing previously computed K and V** representations for all past tokens. Instead of recalculating them every time a new token is generated, the model simply:
    
    - Reuses cached K1:(n−1) and V1:(n−1),
    - Computes only the new kt and vt for the current token,
    - And appends them to the cache.
- This approach effectively removes redundant computation, changing the per-step cost from O(l⋅n⋅d2)toO(l⋅d2—an **n-times speedup** in the sequence dimension.
    
- The following figure ([source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)) illustrates a typical self-attentioblock in transformers:
    

![Self-Attention Block](https://aman.ai/primers/ai/assets/model-acceleration/SA.jpg)

##### Why This Matters

- This improvement is especially critical for long sequences, where n can reach thousands or even millions of tokens. Without caching, latency would scale quadratically with sequence length, quickly becoming impractical. With KV caching, inference scales linearly, enabling efficient streaming and low-latency text generation for modern LLMs.

#### Structure and Size of the KV Cache

- The KV cache stores the key and value tensors for each transformer layer, attention head, the sample indices within each batch, and the token prefix length (i.e., the number of tokens already processed, including the prompt and any previously generated tokens, not just the immediate past token).
- Assuming a transformer with:
    
    - Sequence length so far: n
    - Number of layers: l
    - Number of attention heads per layer: h
    - Head dimension: dk
    - Batch size: b
- The KV cache for the above setup would consist of two main tensors per layer:
    
    1. A **key tensor** of shape (b,h,n,dk), which stores the projected keys K for all past tokens.
        
    2. A **value tensor** of shape (b,h,n,dk) which stores the projected values V for all past tokens.
        
- Since each layer requires its own copy of both (K and V) tensors, the total number of stored elements is:

Total elements=2⋅l⋅b⋅h⋅n⋅dk

- If we assume each element is stored in 16-bit floating point precision (`float16`), then the total KV cache size in bytes is:
    
    Size (bytes)=2⋅l⋅b⋅h⋅n⋅dk⋅2
    
    - where the final factor of 2 accounts for the 2 bytes per `float16` element.
- **Example:**
    
    - For a model with l=32 layers, h=32 heads, dk=128, b=1, and =1000:
    
    Size=2⋅32⋅1⋅32⋅1000⋅128⋅2=524,288,000 bytes (≈500 MB)
    
- This shows that the KV cache can become a significant memory consumer for long sequences, which is why optimizations such as quantization or chunked attention are often used in large language model inference.
    

#### Caching Self-Attention Values

- KV caching exploits two key properties:
    
    1. The model weights (WK and WV) are fixed during inference.
    2. The K and V representations for a given token depend on that token and all prior tokens (via its hidden state), but they do not depend on or change with any future tokens (i.e., they are **immutable** for for all subsequent decoding steps).
- Therefore, once we compute the K and V representations for a given (token, layer, head) tuple, we can store them and reuse them in all subsequent decoding steps.
    
- At decoding step t:
    
    - **Without caching**: recompute K1:n and V1:n from scratch for all t tokens.
    - **With caching**: reuse K1:(n−1) and V1:(n−1) compute only the new kt and vt and append them to the cache.
- The following figure ([source](https://huggingface.co/blog/not-lain/kv-caching)) illustrates the KV caching process, showing how only the new token’s K and V are computed while the rest are reused:
    

![KV Caching Illustration](https://aman.ai/primers/ai/assets/model-acceleration/KV.png)

- This optimization changes the cost from O(l⋅n⋅d2) to O(l⋅d2) per decoding step — an **n-times speedup** in the sequence dimension.
    
- The improvement is especially significant for long sequences, where n can reach thousands or even millions of tokens. By eliminating redundant attention computations, KV caching enables efficient, low-latency generation at scale.
    

###### Why Not Cache Prior Queries?

- Only the most recent query qt is used in the self-attention operation (which is recomputed at every step because it depends on the most recent token’s embedding), so caching prior queries (q1:(n−1)) offers no benefit. Put simply, only the query corresponding to the latest token in the sequence is needed for decoding.

#### Autoregressive Decoding Process with Caching

1. **Initial Sequence (Prefill Phase)**:
    
    - Given a prompt sequence S=[x1,x2,…,xn] the model computes K and V tensors for all prompt tokens in all layers and stores them in the KV cache.
        
    - This step still incurs the full cost O(l⋅n⋅d2) because we have no cached values yet.
        
    - After this prefill step, the model transitions to the _decode phase_, where we process one token per step.
        
2. **Predict Next Token (Decode Phase)**:
    
    - At decoding step n+1:
        
        - Compute the query vector qn+1=xn+1WQ for the new token.
            
        - Retrieve all previous keys and values from the cache:
            
        
        K1:n,V1:n
        
        - Compute the attention output for the new token using:
        
        Attention(qn+1,K1:n,V1:n)
        
3. **Update Cache**:
    
    - Compute the new key and value vectors for the current token:
        
        kn+1=xn+1WK,vn+1=xn+1WV
        
    - Append these to the KV cache so they can be reused in future decoding steps:
        
        Kcache←[K1:n,kn+1]
        
        Vcache←[V1:n,vn+1]
        
4. **Repeat**:
    
    - Continue until the end-of-sequence (EOS) token is generated or the maximum token limit is reached.

#### Implementation Details

##### Cache Tensor Shape

- Assuming:
    
    - Batch size B
    - Max sequence length n
    - Number of heads H
    - Head dimension dk
    - Number of layers l
- The KV cache is structured to store K and V for each (token,layer,head) tuple. For a **given layer** and **head**, the cache tensor shapes and sizes are:
    
    - **Key cache (per layer, per head):**
        
        K(l,h)cache∈ℝB×n×dk
        
        - **Key cache size (in bytes):**
        
        S(l,h)K=B×n×dk×sizeof(dtype)
        
    - **Value cache (per layer, per head):**
        
        V(l,h)cache∈ℝB×n×dk
        
        - **Value cache size (in bytes):**
        
        S(l,h)V=B×n×dk×sizeof(dtype)
        
    - **Total memory size per layer and head (in bytes):**
        
        Slayer, head=2×B×n×dk×sizeof(dtype)
        
        - The factor of 2 accounts for both the key and value tensors.
- When combining all layers and attention heads, the total KV cache represents the complete memory footprint required to store all key and value tensors across the entire model. The following equations describe the full model-wide KV Cache cache dimensions and their corresponding memory requirements:
    
    - **Key cache (all layers, all attention heads):**
        
        K(total)cache∈ℝB×l×H×n×dk
        
        - **Key cache size (in bytes):**
        
        S(total)K=B×l×H×n×dk×sizeof(dtype)
        
    - **Value cache (all layers, all attention heads):**
        
        V(total)cache∈ℝB×l×H×n×dk
        
        - **Value cache size (in bytes):**
        
        S(total)V=B×l×H×n×dk×sizeof(dtype)
        
    - **Total memory size across the entire model (in bytes):**
        
        Stotal=2×B×l×H×n×dk×sizeof(dtype)
        
- **Notes:**
    
    - The cache size scales linearly with B, l, H, n, and dk. During autoregressive generation, the model processes one token per step in the decode phase, only **one** new key and one new value vector are appended at each step for every layer and head. Consequently, as decoding progresses, the total cache grows linearly with the number of generated tokens—reflecting the incremental accumulation of key-value pairs over time.
    - In practice:
        - sizeof(dtype)=2 bytes for FP16/BF16 caches.
        - sizeof(dtype)=1 byte for INT8 caches.
    - Example: For a model with l=32, H=64, dk=128, B=8, and n=4096, the KV cache can easily consume tens of gigabytes of GPU memory. Efficient cache management (e.g., truncation, quantization, or offloading) is thus essential for real-world deployment.
    - Efficient memory layout is crucial — contiguous buffers enable fast appends and reduce memory copy overhead.

##### Prefill Phase

- When the prompt is first processed:
    
    - The model computes K and V for **all** prompt tokens in every layer, filling the cache.
    - This initial step has the same cost as the naive approach:
    
    O(l⋅n⋅d2)
    
- After this, we move into the decode phase, where caching delivers the performance benefits.
    

##### Updates to the KV Cache

- During autoregressive decoding, the K and V projections are cached for every processed token, across all layers and heads.
- Each time a new token is generated:
    
    1. The model computes kt and vt for that token in each layer.
    2. These vectors are appended to the existing Kcache and Vcache.
    3. The updated cache is then used to compute the attention output for the next token.

#### Latency Optimization/Savings

##### Projection Cost

- **Without caching**:
    
    - For a single head, at decoding step with sequence length n, the self-attention module recomputes K and V for all n tokens across all l layers. Put simply, each new generated token waits for full attention recomputation.
    - Computational cost per predicted token:
        
        O(l⋅n⋅d2)
        
- **With caching**:
    
    - Only the key and value for the **new** token are computed, while the rest are reused from the cache. Put simply, each token only computes new attention scores.
    - Computational cost per predicted token:
        
        O(l⋅d2)
        
- This represents an n-times speedup in the sequence dimension. For large n (e.g., thousands or millions of tokens), the cost reduction is dramatic.
    

##### Attention Score Computation

- **Without caching**:
    
    - At sequence length n, computing the attention scores requires multiplying the query for the new token with all n keys. This is done for every layer, so the attention score computation cost per predicted token is:
        
        O(l⋅n⋅d)
        
    - Because n increases with each generated token, the latency for this step grows linearly in n per token generation, but overall decoding (projection + attention) without caching still has quadratic growth in n.
        
- **With caching**:
    
    - Keys from all previous tokens are already stored. At sequence length n, we only compute the dot products between the new query and the cached keys:
        
        O(l⋅n⋅d)
        
    - The cost per token still grows linearly with n, but caching removes the quadratic growth that comes from recomputing keys and values for older tokens.
        

##### Total Complexity

- KV caching transforms overall decoding latency from quadratic in n to approximately linear in n, a major improvement for long-sequence generation. Specifically, KV caching changes the dominant scaling term from O(n2⋅d2) to O(n2⋅d) which, for typical transformer sizes, is a substantial improvement in long-sequence latency. This is mathematically represented below.
    
- **Without caching**:
    
    - Total cost per predicted token = projection cost + attention score computation:
        
        O(l⋅n⋅d2)+O(l⋅n⋅d)≈O(l⋅n⋅d2)
        
    - Over an entire sequence of length **n**, the total decoding cost is:
        
        O(l⋅n2⋅d2)
        
- **With caching**:
    
    - Total cost per predicted token = projection cost + attention score computation:
        
        O(l⋅d2)+O(l⋅n⋅d)≈O(l⋅n⋅d)
        
    - Over an entire sequence of length **n**, the total decoding cost is:
        
        O(l⋅n2⋅d)
        

#### Practical Deployment Considerations

#### Practical Deployment Considerations

##### Memory Management

- Managing KV caches efficiently is one of the main engineering challenges in large-scale transformer deployment. The cache for each sequence grows linearly with the number of processed tokens, since for every new token the model must store its key and value representations for each layer and attention head. Consequently, even moderate increases in context length can result in exponential GPU memory pressure when serving multiple concurrent requests.
    
- To mitigate this, systems adopt several strategies:
    
    - **Sliding Window Caching**: Instead of maintaining the entire attention history, servers often retain only the most recent N tokens per request. This sliding window allows for long-running conversations without exceeding memory budgets, at the cost of slightly reduced long-range recall. For instance, if the model supports 32k tokens but memory is constrained, the cache may only keep the last 8k–16k tokens.
        
    - **Cache Truncation and Compression**: For extreme cases, caches can be truncated or quantized. Truncation drops the oldest segments of the context when the memory budget is reached, while compression methods—like storing keys and values in lower precision (e.g., FP16 or INT8)—trade off a small amount of accuracy for substantial memory savings.
        
    - **Layer-Aware Cache Allocation**: Not all layers contribute equally to performance. Some deployment systems dynamically allocate higher precision or longer cache retention to the most attention-sensitive layers while reducing resource usage for others.
        
    - **Offloading to Host Memory**: For very long contexts or multi-turn conversations, GPU memory may not suffice. Systems can offload part of the cache to CPU memory or even NVMe-based memory pools, fetching it back as needed. However, this introduces latency trade-offs and requires careful memory pinning to minimize data transfer overheads.
        

##### Dynamic Batching

Dynamic batching is essential for maximizing GPU utilization in real-time inference scenarios. Since users issue requests of varying lengths and progress asynchronously through token generation, each request maintains an independent KV cache that grows at its own rate. A well-designed system must:

- Efficiently **group requests with similar decoding steps** to form micro-batches without breaking sequence dependencies.
- Maintain **per-request cache isolation**, ensuring that the correct KV history is retrieved during each attention computation.
- Implement **fast lookup and append mechanisms**, typically backed by memory pools or custom allocators, allowing concurrent cache updates without heavy synchronization locks.
- Use **streaming attention scheduling**: at each decoding step, the system identifies which requests are ready to decode and merges them temporarily into a batch. Once the next token is produced, each request’s cache is updated independently.

Systems such as vLLM and TensorRT-LLM provide specialized runtime schedulers that dynamically manage per-request caches while achieving near-optimal GPU occupancy. In such architectures, the ability to reuse KV states and batch across requests determines overall throughput.

##### Cache Parallelism

- In large-scale multi-GPU or distributed serving environments, KV caches themselves become distributed data structures. When the model’s layers or attention heads are sharded across devices, the corresponding keys and values must follow the same partitioning strategy. Typical configurations include:
    
    - **Tensor Parallelism**: Each GPU holds only a subset of the attention heads. During cross-device attention computations, the K and V tensors are exchanged or synchronized across GPUs via collective operations (e.g., all-gather). Efficient implementations overlap communication and computation to minimize latency.
        
    - **Pipeline Parallelism**: Layers are distributed across GPUs. Each GPU must maintain the KV cache for only the layers it owns. However, during forward passes, intermediate activations are streamed between devices. The system must ensure that caches align temporally across pipeline stages to preserve attention correctness.
        
    - **Model Parallel + Data Parallel Hybridization**: In highly scalable deployments, KV caches are both sharded (for model parallelism) and replicated (for data parallelism). Systems must handle synchronization and memory consistency between replicas, often through NCCL-based communication backends.
        
    - **Cross-Node Caching**: When models run across multiple nodes, caches may be stored in distributed shared memory or remote memory access (RDMA)-capable hardware, allowing direct GPU-to-GPU cache retrieval without CPU intervention.
        

##### Why You Can’t Always Cache Everything

- Memory growth in KV caching is linear in sequence length and proportional to the number of layers, heads, and hidden dimensions:
    
    - Memory scaling: For large models (tens of billions of parameters), a single sequence of 1000 tokens may consume roughly 1 GB of KV cache.
    - Batch size impact: The more concurrent sequences are cached, the fewer requests can fit into GPU memory, directly impacting throughput.
    - Context length: With ultra-long contexts (e.g., 100k tokens), a naive full cache could exceed 100 GB—far beyond the capacity of even high-end GPUs.

#### Multi-Head Attention and KV Cache

- In practice, self-attention is implemented with multiple attention heads, each operating in a subspace of the embedding dimension. For head h in {1,…,H}, we have:
    
    Q(h)=XWQ(h),K(h)=XWK(h),V(h)=XWV(h)
    
- The attention outputs from each head are concatenated:
    
    Q=concat(Q(1),Q(2),…,Q(H))
    
    - and similarly for K and V.
- **Caching in multi-head attention**:
    
    - The KV cache stores keys and values for every head and every layer.
    - Shape for the key and value cache:
    
    Kcache∈ℝB×H×n×dk
    
    Vcache∈ℝB×H×n×dk
    
    - **where**
        
        - B = batch size (number of sequences processed in parallel)
        - H = number of attention heads
        - n = sequence length (number of tokens stored in the cache)
        - dk = dimension of the key (and value) vectors per head
- **Performance implications**:
    
    - Since each head’s KV cache is independent, the caching logic operates head-wise, but the storage is typically implemented as a unified tensor for efficiency.
    - This unified tensor is arranged to be friendly to GPU tensor cores, enabling very fast read and write operations during decoding.
- While KV caching greatly reduces the sequence dimension cost, the **depth dimension** (number of layers l) is still a significant contributor to compute. This leads to the _KV Sharing_ idea, covered in detail in the section on [KV Sharing](https://aman.ai/primers/ai/model-acceleration/#kv-sharing) — reusing K and V representations across the last half (or fraction) of layers to further cut computation. KV sharing builds on KV caching, but attacks the problem from the layer/depth dimension rather than the token dimension.
    

#### Summary of KV Cache Benefits

- **Reduces repeated computation** by storing and reusing K, V tensors instead of recomputing them at every step.
- **Enables efficient decoding** in autoregressive generation by cutting per-step cost from O(l⋅n⋅d2) to O(l⋅d2) — an **n-times speedup** in the sequence dimension.
- **Optimized for hardware acceleration** via unified tensor layouts that are friendly to GPU tensor cores.
- **Scales well** to large models and long contexts, with latency growing linearly rather than quadratically with sequence length.
- **Maintains accuracy** because cached K and V are identical to recomputed values, given fixed weights.

#### KV Sharing

- KV caching, introduced in [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254) by Sun et al. (2024), optimizes the **sequence dimension** (n) cost, but the **depth dimension** (l) — the number of layers — still incurs full computation for each layer’s K and V.
- **KV Sharing** addresses this by reducing the cost of computing K and V along the depth dimension.
- The intuition behind why this can work comes from studies such as [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898) by Csordás et al., which show empirically that in a deep transformer-like model, the last layers are correlated with each other. This means the final few layers are not necessarily adding much new information, but rather tweaking the output produced so far. This redundancy can potentially be exploited to save computation without significantly degrading model quality.

##### How KV Sharing Works

- The core idea: share **actual K and V representations** (not just weight matrices) across the last fraction of layers.
    
- For example, if we share across the last half of the layers (l2 layers):
    
    1. The final layer before the shared region computes K and V normally.
    2. All subsequent layers in the shared region reuse these K and V without recomputation, regardless of their inputs.
    3. Other parameters (e.g., WQ, MLP weights) remain distinct per layer.
- Mathematically:
    
    - Let Lshare be the index of the first shared layer.
    - For any layer j≥Lshare:
    
    K(j)=K(Lshare),V(j)=V(Lshare)
    
- The following figure ([source](https://arxiv.org/abs/2405.05254)) illustrates KV Sharing across the last half of the layers, showing how a single computed K and V set is reused instead of recalculated:
    

![KV Sharing Illustration](https://aman.ai/primers/ai/assets/model-acceleration/KVS.jpg)

##### FLOP Savings

- If the last lk layers share K and V, we avoid computing them in lk layers entirely.
- FLOP reduction: Savings=lkl=1k fraction of the total keys and values computation.
- Combined with KV caching:
    
    - KV caching cuts cost in n (sequence) dimension.
    - KV sharing cuts cost in l (layer) dimension.

##### Why KV Sharing Can Work

- Empirical studies referenced in the paper show that in deep transformer models, the last few layers often produce correlated outputs.
- This suggests that later layers are mostly fine-tuning rather than introducing fundamentally new information.
- Reusing K and V in these layers therefore has minimal impact on output quality while significantly reducing compute and memory usage.

##### Memory Benefits

- **No need to store keys and values** for the shared layers at all.
- Reduces memory footprint in both inference and training.
- Particularly valuable when serving long sequences, where cache size is dominated by B×H×n×dk×l scaling.

##### Deployment Notes

- KV sharing must be considered at **training time** for best results, since models not trained with this constraint may suffer quality drops if sharing is applied post hoc.
- Works alongside KV caching since KV sharing tackles **depth**, while KV caching tackles **sequence length**.

### Model Quantization

- Model quantization is a technique used to reduce the precision of numerical values (typically weights and activations) in a neural network from high-precision formats like 32-bit floating point (`float32`) to lower-precision formats such as `int8`, `float8`, or even `int4`. This allows for faster inference, reduced memory usage, and lower power consumption, particularly on hardware that supports low-precision arithmetic.
- A detailed discourse on this topic is available in our [Model Compression](https://aman.ai/primers/ai/model-compression) primer.

#### Why Quantize?

- Quantization can lead to significant improvements in efficiency:
    
    - **Reduced Memory Footprint**: An `int8` model consumes 75% less memory than its `float32` counterpart.
    - **Faster Arithmetic**: Lower-precision operations (like `int8` or `int4` matmuls) are natively supported and highly optimized on modern accelerators (e.g., NVIDIA Tensor Cores, Intel AVX-512 VNNI).
    - **Lower Latency**: With less data to move and faster compute kernels, quantized models offer reduced end-to-end inference time.

#### Types of Quantization

##### Post-Training Quantization (PTQ)

- PTQ involves converting a pre-trained `float32` model to a lower-precision model without retraining. It works by calibrating the ranges of tensors using a small sample of data.
    
- **Key steps in PTQ:**
    
    - **Range Calibration**: Identify the min/max values of weights and activations from a calibration dataset.
        
    - **Scale and Zero-Point Calculation**: For each quantized tensor, calculate:
        
        q=round(rs)+z
        
        - where:
            
            - r is the real-valued number
            - s is the scale (i.e., step size)
            - z is the zero-point to preserve zero mapping in the quantized domain
            - q is the quantized value (e.g., 8-bit integer)
- **Weight and Activation Clipping**: Clip values to fit within the representable range of the target bit-width (e.g., [-128, 127] for signed `int8`).
    

##### Quantization-Aware Training (QAT)

- QAT simulates quantization during training. Fake quantization layers are added to mimic low-precision computation while maintaining gradients in high precision.
    
- **Advantages:**
    
    - More accurate than PTQ for sensitive models (e.g., GPT, BERT).
    - Allows the model to adapt to quantization errors during fine-tuning.
- **Implementation Details:**
    
    - Frameworks like PyTorch and TensorFlow include fake quantization modules (e.g., `torch.quantization.FakeQuantize`).
    - Quant-dequant pairs are inserted in the model graph to simulate the behavior of actual quantized operations.

#### Static vs. Dynamic Quantization

- **Static Quantization**: Activations are quantized ahead of time using calibration. Requires representative input data and is more performant but less flexible.
- **Dynamic Quantization**: Weights are quantized ahead of time, but activations are quantized at runtime based on actual values. More flexible and easier to integrate but slightly slower.

#### Quantization in Transformers

- In transformer models like GPT or BERT, quantization is applied to:
    
    - **Linear layers**: Including query, key, value, and output projections in attention layers.
    - **GEMM-heavy blocks**: MLP (feed-forward) layers.
    - **Embedding layers**: Often quantized with special handling to preserve lookup efficiency.
- **Special Considerations**:
    
    - LayerNorm and Softmax are sensitive to quantization and often kept in `float32`.
    - Attention scores may require FP16 or `float32` to avoid instability.
    - Mixed-precision quantization (e.g., `float8` weights with `int8` activations) is sometimes used.

#### Tooling and Frameworks

- **NVIDIA TensorRT / FasterTransformer**
- **Intel Neural Compressor (INC)**
- **PyTorch Quantization Toolkit**
- **ONNX Runtime Quantization**
- **BitsAndBytes (for 8-bit and 4-bit LLMs)**
    
- These tools offer end-to-end pipelines for quantizing, validating, and deploying models.

### Operator Fusion

- Operator fusion is an inference optimization technique that combines multiple adjacent operations in a neural network computation graph into a single composite operation. This is done to reduce overhead from memory reads/writes, kernel launches, and inter-operation communication, especially on GPU- or TPU-based systems.
    
- Fusion reduces latency and increases compute efficiency by keeping data in faster registers or shared memory, rather than flushing it out to slower global memory between every small operation.
    

#### Motivation

- Modern deep learning workloads often involve many small operations executed sequentially—e.g., matrix multiplications followed by bias addition, normalization, and non-linear activations:

x→Linear→AddBias→LayerNorm→ReLU

- Each of these operations might otherwise be implemented as a separate kernel. This leads to:
    
    - Increased kernel launch overhead.
    - Inefficient use of GPU parallelism.
    - Repeated memory access and latency.
    - Limited optimization opportunities for compilers.
- By fusing them, the computation becomes more compact, minimizing overhead and maximizing performance.
    

#### Common Fusion Patterns

- Some of the most commonly fused sequences in transformer inference include:
    
    - **GEMM + Bias Add + Activation**
        
        - Example: Y=ReLU(X@W+b)
        - Typically fused in MLP layers.
    - **Residual Add + LayerNorm + Dropout**
        
        - Used in transformer blocks.
    - **Query/Key/Value Linear Projections**
        
        - Three `Linear` ops fused into a single matmul followed by splitting heads.
    - **Softmax + Masking**
        
        - In attention, softmax is often fused with masking logic to avoid branch divergence on GPUs.

#### Fusion in Transformers

- In transformer architectures, operator fusion is especially valuable in:
    
    - **Multi-Head Attention Blocks**:
        
        - Combine Q/K/V projections and reshape + transpose logic into a single kernel.
        - Fuse attention score computation, masking, and softmax into one efficient operation.
    - **Feed-Forward Networks (FFNs)**:
        
        - Fuse two linear layers with intermediate activation (e.g., GELU or ReLU).

#### Implementation Details

- Fusion can be implemented in several ways:

##### Graph-Level Fusion (Ahead-of-Time)

- High-level compilers like XLA (for TensorFlow) or TorchScript (for PyTorch) can analyze the computational graph and fuse operations during compilation.
    
- Example in PyTorch:
    

![](https://aman.ai/images/copy.png)

`@torch.jit.script def fused_layer(x, w1, b1, w2, b2):     return F.relu(F.linear(x, w1, b1)) @ w2.T + b2`

- TorchScript may fuse `linear + relu` into a single kernel.

##### Kernel-Level Fusion (Runtime)

- Frameworks like NVIDIA’s TensorRT and FasterTransformer include hand-written CUDA kernels that combine multiple operations (e.g., QKV projection + transpose + scale + matmul) in one pass.
    
- Example: A fused transformer kernel might compute:
    

![](https://aman.ai/images/copy.png)

`qkv = fused_linear_bias_act(x);  // one call q, k, v = split_heads(qkv);      // internal fused transpose and reshape`

- This reduces global memory traffic and utilizes registers/shared memory for intermediate results.

##### 3. Custom Kernel Generation

- Libraries like TVM or Triton enable defining custom fused kernels in a hardware-optimized DSL. These can be compiled just-in-time for maximum throughput.
    
- Example in Triton:
    

![](https://aman.ai/images/copy.png)

`@triton.jit def fused_gemm_relu(...):     # Define fused matmul + bias + relu logic using GPU thread blocks`

#### Performance Impact

Operator fusion can lead to:

- **30–50% improvement in latency** for attention blocks.
- **Higher hardware utilization**, especially on GPUs with tensor cores or vectorized ALUs.
- **Reduced memory bandwidth pressure**, which is often the bottleneck in LLM inference.

#### Tooling and Ecosystem

- **TensorRT**: Extensive fusion for transformer blocks.
- **FasterTransformer**: Fused QKV and FFN kernels.
- **ONNX Runtime with Graph Optimizer**: Automatic fusion passes.
- **TorchScript + FBGEMM**: Fusion of linear + activation ops.
- **TVM / Triton**: Customizable and tunable fusion kernels.

### Speculative Decoding

- Speculative decoding is an inference-time optimization technique designed to reduce the latency of autoregressive sequence generation in large language models (LLMs). Instead of generating one token at a time using the full, expensive model, speculative decoding uses a smaller, faster “draft” model to guess multiple tokens in parallel, then validates these guesses with the full “target” model. If the guesses are correct, they are accepted as part of the output. Otherwise, they are partially or fully discarded and recomputed.
    
- This method maintains the output quality of the original model while significantly improving throughput.
    

#### Motivation

- Autoregressive decoding is inherently sequential. In a naive setup, the model generates one token, then feeds it back as input to generate the next. This sequential loop introduces latency and becomes a bottleneck during long-form generation.
    
- Let:
    
    - f be the full model (large, accurate but slow)
    - g be the draft model (smaller, less accurate but fast)
- Naively, generation requires T forward passes of f for a sequence of T tokens. Speculative decoding aims to reduce the number of times f is called.
    

#### Basic Algorithm

1. **Initialize Context**: Use a prompt or previous tokens x.
2. **Draft Generation**: Use the draft model g to generate a sequence of k speculative tokens:
    
    y1,y2,...,yk=g(x)
    
3. **Validation**: Use the full model f to compute the log-probabilities pf(yt‖x,y1,...,yn−1).
4. **Accept or Reject Tokens**:
    
    - Accept as many tokens as f agrees with (within a confidence threshold or by matching top-1 outputs).
    - Rewind to the last agreed-upon token and resume with the draft model from there.

#### Pseudocode

![](https://aman.ai/images/copy.png)

`x = initial_prompt while not done:     draft_tokens = g.generate_next_k(x)     probs_f = f.get_probs(x + draft_tokens)     accepted_prefix = match(draft_tokens, probs_f)     x = x + accepted_prefix`

#### Key Parameters

- **Draft Model Quality**: Must be fast enough to justify speculative overhead but good enough to match the full model reasonably often.
- **Block Size k**: Number of speculative tokens generated per iteration. Larger blocks = fewer full model calls, but higher risk of rejection.
- **Matching Strategy**: Usually uses top-1 match or a log-prob threshold.

#### Mathematical View

- Let the probability of accepting each token be α. Then the expected number of full-model calls is:

𝔼[full passes]≈Tk⋅α

- If α≈0.7 and k=4, we reduce full-model calls by nearly 3×.

#### Implementation Details

- **Parallel Calls**: f can validate all k tokens in one forward pass by using cached KV states and batched logits.
- **KV Cache Management**: Efficient speculative decoding updates the cache only after validation.
- **Multimodel Serving**: Systems like NVIDIA’s FasterTransformer or Hugging Face’s `transformers` can host both f and g concurrently with shared memory or GPU residency.

#### Notable Variants

- **Medusa** (Meta): Uses a tree-structured decoder to validate multiple candidates at once.
- **FastRAG**: Combines speculative decoding with retrieval-based models.
- **Draft & Verify** (Google): A formalized framework for plug-and-play speculative decoding with checkpointing.

#### Benefits

- **Latency Reduction**: 2×–4× speedup in decoding for long sequences.
- **Full-Model Accuracy**: Final output matches the output of the full model f, so there’s no accuracy loss.
- **Compatibility**: Can be layered on top of existing decoding strategies (e.g., greedy, top-k, nucleus).

#### Limitations

- Requires additional memory and compute for the draft model.
- Effectiveness depends on alignment between the draft and full model distributions.
- Complex cache management and integration overhead.

### FlashAttention and Efficient Attention Kernels

- In transformer models, self-attention is a core operation that enables the model to learn relationships between tokens. However, traditional attention implementations scale poorly with sequence length due to quadratic memory and compute complexity. **FlashAttention** and other efficient attention kernels address these bottlenecks by optimizing the attention computation to reduce memory overhead and improve performance.

#### Motivation

- The standard attention computation involves the following operations for a sequence of length L and hidden dimension d:
    
    Attention(Q,K,V)=softmax(QKTd‾‾√)V
    
- This requires:
    
    - Computing a full L×L attention matrix (expensive for long sequences).
    - Storing intermediate results like logits and softmax scores in global memory.
    - Limited reuse of on-chip memory (registers, shared memory), resulting in bandwidth-bound performance.
- FlashAttention addresses these inefficiencies by restructuring the attention algorithm to use memory-efficient block-wise computation.
    

#### FlashAttention: Key Concepts

- Originally proposed in Dao et al., 2022 in [FlashAttention: Fast and Memory‑Efficient Exact Attention with IO‑Awareness](https://arxiv.org/abs/2205.14135), FlashAttention is a fused, tiled implementation of scaled dot-product attention that:
    
    - **Eliminates materialization of the full attention matrix**: Avoids creating and storing the entire L×L attention score matrix in GPU memory. Instead, computes small blocks of logits on-chip, applies masking and softmax immediately, and discards them, drastically reducing memory usage for long sequences.
        
    - **Uses tiling to partition queries, keys, and values into small blocks that fit in GPU shared memory**: Splits Q, K, and V into manageable tiles (e.g., 64×64) that can be loaded into fast on-chip shared memory or registers. This improves memory locality, reduces global memory reads/writes, and allows the GPU to reuse loaded data for multiple computations within the block.
        
    - **Fuses softmax, scaling, masking, and matmul into a single kernel**: Combines these operations into one GPU kernel to avoid storing intermediate results in memory. By performing scaling, masking, softmax computation, and the weighted sum with V in a single pass, FlashAttention reduces memory bandwidth usage and improves computational efficiency.
        

##### High-Level Algorithm

1. Load a block of queries Qi and keys Kj into shared memory.
2. Compute attention logits QiKTjd√ for the block.
3. Apply mask and softmax **in-place**, updating the running sum of exponents and maximums for numerical stability.
4. Accumulate partial outputs Ai,j=softmax(QiKTj/d‾‾√)Vj without storing intermediate attention matrices.
5. Repeat across blocks until the full result is computed.

##### Numerical Stability

- To avoid numerical overflow when computing softmax in a block-by-block fashion, FlashAttention keeps running statistics for each query row:
    
    - mi=maxjzij — the maximum logit value seen so far for that row, used to shift logits and prevent large exponentials.
    - si=∑jexp(zij−mi) — the running sum of the shifted exponentials, which forms the softmax denominator.
- As new blocks are processed, these values are updated using associative operations that merge current and previous block statistics without loss of precision. This ensures the final softmax is mathematically equivalent to computing it on the full L×L matrix, but without ever storing that matrix.
    

#### Implementation Details

- Written as a custom CUDA kernel.
- Uses **shared memory** to hold Q/K/V tiles and compute locally.
- Optimized to run in **mixed precision** (e.g., FP16 or BF16) for speed and memory efficiency.
- Compatible with dropout, masking, and rotary embeddings.

#### FlashAttention-2 Improvements

- Adds support for **non-causal attention**, **variable-length sequences**, and better **warp-level parallelism**.
- Removes redundant memory loads through more aggressive caching and loop unrolling.
- Enables **backward pass efficiency**, making it useful not only for inference but also for training.

#### Other Efficient Kernels

- **xFormers** (Meta): Modular attention implementations that support Flash, sparse, and memory-efficient variants.
- **Triton-based Attention**: Enables easy definition of fused attention kernels using Triton’s GPU DSL.
- **PagedAttention (vLLM)**: Optimizes KV cache access for batch inference, reducing memory fragmentation and improving latency.

#### Performance Gains

- FlashAttention reduces attention memory complexity from:
    
    - **(L2)** to **(L)** for memory consumption.
    - Achieves **1.7–2.7× speedup** on A100 GPUs for long sequence lengths (> 1k tokens).
    - Maintains exact attention output (within floating-point precision), unlike approximate methods.

#### Use in Inference

- FlashAttention is especially beneficial for:
    
    - Long-context models (e.g., 4k to 128k tokens).
    - Multi-head attention, where per-head memory use adds up quickly.
    - Deployment on GPUs with large shared memory (e.g., NVIDIA A100, H100).

#### Integration

- Supported in:
    
    - **Hugging Face Transformers** via `use_flash_attention_2=True`
    - **PyTorch** through custom CUDA extensions or Triton kernels
    - **DeepSpeed**, **FasterTransformer**, and **xFormers**

### Batching, Sequence Packing, and Prefilling

- **Batching** and **prefilling** are inference-time optimization techniques that improve efficiency and throughput by better utilizing hardware and avoiding redundant computations. These are especially critical when serving LLMs in real-time or at high concurrency.

#### Batching

- Batching refers to the process of grouping multiple inference requests into a single forward pass through the model. This increases hardware utilization, amortizes overhead, and reduces latency per request (on average), particularly on GPUs that are optimized for matrix-heavy workloads.

##### Motivation

- Without batching, each request results in an under-utilized forward pass:
    
    - Small input tensor → Poor occupancy/utilization of GPU cores
    - High overhead per kernel launch
    - Wasted memory bandwidth
- Batching solves this by aligning multiple requests into a tensor of shape:
    
    Batch Tensor: (B,L,d)
    
    - where:
        - B is batch size
        - L is sequence length
        - d is hidden dimension

##### Types of Batching

1. **Static Batching**: Requests are grouped together at fixed time intervals. Simple but less flexible.
2. **Dynamic Batching**: Requests are buffered and grouped at runtime based on heuristics like request arrival time, sequence length, or prompt similarity.
3. **Token-Level Batching**: Pioneered by vLLM, this groups sequences by shared decoding step instead of sequence. Supports long-running generation jobs without blocking new ones.
4. **Asynchronous Batching**: Uses request queues and a scheduler to decide when to batch based on hardware load.

##### Padding and Masking

- Since sequences may vary in length, shorter ones are padded and masked accordingly. Padding increases memory cost but enables unified matrix operations.
    
- Example:
    
    - Sequence A: `[Hello, how, are, you]` → length 4
    - Sequence B: `[Hi]` → length 1
    - Batched input: `[[Hello, how, are, you], [Hi, PAD, PAD, PAD]]`

##### Performance Benefits

- Higher throughput: GPUs can process large matrices in parallel.
- Lower kernel launch overhead.
- Amortized use of KV cache and memory bandwidth.

#### Sequence Packing

- **Sequence packing** is an optimization that reduces padding overhead when batching variable-length sequences. Instead of padding all sequences in a batch to the maximum length, multiple shorter sequences are concatenated into a single continuous sequence within the same batch element.
    
- This approach stores and processes only actual tokens, using an **attention mask** to ensure tokens from different original sequences do not attend to each other.
    

##### Example

- Without packing:
    
    ![](https://aman.ai/images/copy.png)
    
    `[Hello, how, are, you, PAD, PAD, PAD] [Hi, there, PAD, PAD, PAD, PAD, PAD]`
    
    - **Memory usage:** proportional to 7 tokens per sequence (including pads).
- With packing:
    
    ![](https://aman.ai/images/copy.png)
    
    `[Hello, how, are, you, Hi, there]`
    
    - Plus a mask to block attention between `you` and `Hi`.

##### Benefits

- **Reduced memory footprint** — fewer padding tokens stored and processed.
- **Better hardware utilization** — higher effective sequence density in each batch.
- **Lower latency for mixed-length workloads** — especially beneficial when many short sequences are served alongside long ones.

##### Trade-offs

- Slight overhead in constructing and applying more complex attention masks.
- May require specialized batching logic and kernel support for optimal performance.

#### Prefilling

- Prefilling refers to the one-time computation of model activations (primarily KV cache) for the prompt or context tokens before autoregressive decoding begins.

##### Motivation

- Transformer inference separates the process into:
    
    1. **Prompt Phase (Prefill)**: Process entire prompt to initialize the KV cache.
    2. **Generation Phase (Decode)**: Generate one token at a time using cached keys and values.
- The prompt phase is significantly more expensive because it processes multiple tokens without caching, while the decode phase uses KV caching for each new token.
    

##### Prefilling Logic

- Given a prompt of n tokens:
    
    - The model performs a full forward pass to compute attention outputs for all n positions.
    - During this, it initializes the KV cache tensors:
        
        K1:n,V1:n
        
    - These are used in all subsequent generation steps to avoid recomputation.

##### Optimizations

- **Fused Prefill Kernels**: Libraries like FasterTransformer use specialized kernels to batch and prefill KV caches in a single efficient pass.
- **Prompt Sharing**: If multiple requests use the same prompt (e.g., “You are a helpful assistant…”), cache the prefilled results and reuse them across requests.
- **Layer-Wise Streaming**: Some implementations stream KV cache population layer-by-layer to overlap computation and memory operations.

##### Real-World Use

- In production systems:
    
    - Prompt prefill is often the **dominant source of latency**, especially with long prompts (e.g., 1k+ tokens).
    - Prefilling is **not cacheable** unless the prompt is reused. That’s where [prompt caching](https://aman.ai/primers/ai/model-acceleration/#prompt-caching) comes in.
    - Systems may delay decoding until all requests in a batch complete their prefill phase.

##### Performance Benefits

- Avoids redundant computation across decoding steps.
- Enables efficient reuse of memory and attention context.
- Critical for long-context inference and multi-user serving.

### Prompt Caching

- Prompt caching is an inference-time optimization that reuses the computed key-value (KV) attention states for frequently occurring or repeated prompt tokens. It eliminates the need to recompute the prefill phase of autoregressive decoding, which is typically the most computationally expensive part of the inference pipeline for long prompts.
    
- This technique is especially effective in systems with repeated system messages, user templates, or static few-shot examples.
    

#### Motivation

- During autoregressive generation, transformer models process the prompt (or context) once to initialize the attention cache. For a prompt of length n, this involves a full forward pass through all transformer layers to compute the KV tensors:

K(l)1:n,V(l)1:nfor all layers l

- This prefill step is expensive and must be repeated for every new request — even if the prompt is the same.
    
- **Observation**: Many applications use identical or highly similar prompts repeatedly. For example:
    
    - Instructional prompts like: “You are a helpful assistant.”
    - Few-shot templates in customer support bots.
    - System prompts in chat APIs.
- Prompt caching avoids repeated prefill for these common contexts.
    

#### Basic Mechanism

1. **Cache Initialization**:
    
    - Compute and store KV tensors for a given prompt:
        
        KVprompt=f(prompt)
        
    - Store in memory or disk with a unique key (e.g., hash of token IDs).
        
2. **Cache Lookup**:
    
    - For each incoming request, compute a cache key from its prompt.
    - If a match is found, retrieve KV tensors instead of recomputing them.
3. **Continue Decoding**:
    
    - Begin token-by-token generation using the cached KV state:
        
        Generate(xn+1∣KVprompt)
        

#### Implementation Details

##### Cache Granularity

- **Full Prompt Cache**: Caches the entire KV cache of a prompt. Simple and effective but can use a lot of memory.
- **Prefix Sharing**: If prompts differ by suffix (e.g., `Prompt A + User 1` and `Prompt A + User 2`), share the KV prefix and compute only the delta.
- **Subgraph Caching**: In more advanced systems, only the first few layers or tokens may be cached.

##### Cache Storage

- **In-Memory KV Cache**: For maximum performance, use GPU or CPU memory with LRU eviction.
- **On-Disk Cache**: Slower but scalable for cold-start scenarios.
- **Keyed by Hash**: Tokenized input is hashed using SHA or CRC to form a cache key. Some systems normalize prompts before hashing.

##### Integration with Serving Systems

- Requires cache-aware batch scheduling.
- Works best when integrated with dynamic batching and token-level schedulers (e.g., vLLM).
- May include cache warming: preloading common prompts at system startup.

#### Performance Impact

- Let:
    
    - Tp = time to prefill prompt
    - Td = time per token for decode
- For long prompts (e.g., 1000+ tokens), Tp≫Td, so caching the prefill can save **80–95%** of per-request compute for repeated prompts.
    

#### Applications

- **Chat APIs**: System messages or few-shot exemplars remain fixed across turns.
- **Agent Frameworks**: Tools like LangChain often replay the same template structure.
- **Batch Inference**: Multi-user prompts often share context headers (e.g., “Summarize the following…”).

#### Limitations

- Prompt cache is only useful for **identical** or **prefix-matching** prompts.
- Memory usage scales with prompt length and cache size.
- May add overhead for hash computation or miss rate handling.
- Not helpful for fully dynamic, unique user inputs.

### Early Exit and Token Pruning

- **Early exit** and **token pruning** are inference-time optimizations designed to reduce computation in large transformer models by selectively skipping or trimming parts of the computation graph that have diminishing contribution to the final output. These methods exploit redundancy in token representations and layer-wise stability in transformer models.
    
- Both techniques aim to speed up inference without significantly affecting model output quality, making them valuable in latency-sensitive or resource-constrained applications.
    

#### Early Exit

- **Early exit** allows the model to stop processing certain tokens or even entire sequences at intermediate layers if the model’s confidence in the prediction is already high.

##### Motivation

- Transformer models use a fixed number of layers (e.g., 24 or 96), but not all tokens require the full depth to make a confident prediction. For example, easily classifiable tokens (like punctuation or common stopwords) may converge earlier than rare or ambiguous tokens.

##### Mechanism

- At each transformer layer l, evaluate a confidence metric based on the current token representation:

1. **Entropy-Based Confidence**:
    
    - Compute the softmax output p(l) from the current logits.
    - Compute entropy:
        
        H(p(l))=−∑ip(l)ilogp(l)i
        
    - If entropy < threshold, consider the prediction confident enough to exit.
2. **Cosine Similarity to Previous Layer**:
    
    - If representation at layer l is similar to layer l−1, the token may have converged.
3. **Learned Gates**:
    
    - Add a small classification head to each layer to learn exit decisions during training (as in **BranchyNet** or **LayerDrop** approaches).

##### Implementation

- Models like **BERT with Early Exit** (DEEPL) implement classifier heads at multiple depths.
- Hugging Face `transformers` has prototype support for early exit in sequence classification.
- Requires threshold tuning to balance accuracy and latency.

##### Benefits

- Reduces average inference depth (e.g., from 24 layers to 12–16 for many tokens).
- Saves computation for simpler or high-confidence examples.
- Ideal for classification or QA tasks where tokenwise prediction is not necessary.

##### Limitations

- Adds overhead from confidence computation at intermediate layers.
- Not widely adopted in generation tasks due to sequential dependencies between tokens.

#### Token Pruning

- **Token pruning** reduces the number of tokens that are propagated through the deeper layers of a transformer by identifying and removing tokens with low contextual importance.

##### Motivation

- In many attention-based computations, some tokens contribute very little to the output. For example, padding tokens or tokens with low attention weights to the rest of the sequence.
    
- Pruning these tokens saves compute in later layers, especially in long-context models or batch scenarios.
    

##### Mechanism

1. **Attention-Based Pruning**:
    
    - Compute the **attention score variance** or **total attention mass** a token receives:
        
        αi=∑jAttention(xi,xj)
        
    - Prune tokens with low total attention received or given.
        
2. **Top-k Token Selection**:
    
    - Keep only the top-k most important tokens per head or per sequence based on learned importance scores.
3. **Dynamic Thresholding**:
    
    - Use learned or rule-based thresholds to drop tokens whose impact is below a tunable cutoff.
4. **Progressive Pruning**:
    
    - Start with full tokens, and prune more aggressively as layers go deeper.

##### Implementation

- Typically done at attention module boundaries.
- Can be combined with sparse attention mechanisms.
- Token indices need to be tracked to reconstruct output or map back to the original sequence.

##### Benefits

- Reduces computation in deeper layers, especially for long sequences.
- Improves throughput with minimal impact on quality in summarization, QA, and retrieval tasks.
- Can be applied during training for alignment with inference.

##### Limitations

- May degrade quality if pruning is too aggressive or incorrectly calibrated.
- Requires complex index tracking and masking logic.
- Harder to apply in autoregressive settings where all tokens are sequentially dependent.

#### Tools and Research

- **DeLighT**, **LayerDrop**, and **EarlyBERT** for early exit variants.
- **SparseFormer**, **Synthesizer**, and **Longformer** introduce related token reduction ideas.
- Hugging Face and NVIDIA’s Megatron support token pruning hooks in research branches.

### Hardware-Aware Scheduling

- **Hardware-aware scheduling** refers to a set of optimization strategies that tailor the execution of neural network inference to the specific architecture and performance characteristics of the underlying hardware—such as GPUs, TPUs, or specialized accelerators. These optimizations aim to improve compute throughput, memory utilization, and latency by orchestrating how and when operations are executed.
    
- This is especially important for transformer inference, where workloads are large, heterogeneous (e.g., KV cache lookups, matrix multiplies, normalization), and sensitive to memory bandwidth and parallelism.
    

#### Motivation

- Transformer inference involves many stages of computation and memory access:
    
    - Matrix multiplications (GEMMs) in attention and feed-forward blocks.
    - Data movement between layers and devices.
    - KV cache management and resizing.
    - Softmax, activation, and normalization operations.
- Without careful scheduling, bottlenecks can emerge due to:
    
    - Underutilized compute units (e.g., Tensor Cores).
    - Memory stalls and cache thrashing.
    - Synchronization overhead between layers or streams.
- Hardware-aware scheduling optimizes these execution flows to keep the pipeline full and latency low.
    

#### Core Techniques

##### Stream Parallelism

- Modern GPUs support multiple concurrent execution streams (e.g., via CUDA). In transformer inference:
    
    - Use separate CUDA streams for different model stages (e.g., one for KV cache update, one for GEMM).
    - Overlap memory copies (e.g., `cudaMemcpyAsync`) with compute to hide latency.
- **Example**:
    
    ![](https://aman.ai/images/copy.png)
    
     `cudaMemcpyAsync(..., stream1);  cublasGemmEx(..., stream2);  // runs concurrently with stream1`
    

##### Tensor Core Utilization

- Tensor cores are specialized units in NVIDIA GPUs for low-precision matrix ops (e.g., `float16`, `bfloat16`, `int8`). To maximize their usage:
    
    - Ensure all matrix multiplications are aligned to multiple-of-8 dimensions.
    - Use fused kernels to eliminate intermediate `float32` conversions.
    - Prefer mixed-precision pipelines (AMP / `float16`) for higher throughput.
- Libraries like **cuBLAS**, **FlashAttention**, and **TensorRT** handle these optimizations automatically when configured correctly.
    

##### Operator Placement and Reordering

- Efficient inference scheduling may involve reordering or co-locating operations based on:
    
    - **Memory locality:** Fuse or group operations that share data.
    - **Execution time:** Prioritize long-running ops earlier in the pipeline.
    - **Device affinity:** Keep frequently accessed data on the same GPU or chip.
- **Example**: Run attention blocks first in multi-layer transformer if they dominate compute time, allowing FFNs to be prefetched concurrently.
    

##### KV Cache Management

- Efficient KV cache handling is essential in decoder models:
    
    - **Paged KV Cache**: Used in systems like vLLM, stores KV in contiguous memory pages and allows random-access updates.
    - **Memory Pools**: Preallocate KV buffers for each request and reuse them to avoid memory fragmentation.
    - **Lazy Allocation**: Delay cache instantiation until first generation step to save memory for short prompts.

##### Pipeline and Model Parallelism

- In large-model deployments:
    
    - **Pipeline Parallelism**: Distribute transformer layers across devices. Stage execution overlaps compute and communication.
    - **Tensor Parallelism**: Split individual tensor dimensions (e.g., weights) across devices for large GEMMs.
- Combined, these allow serving models with billions of parameters across multiple GPUs efficiently.
    

##### Custom Kernel Scheduling

- Frameworks like Triton and TVM allow defining and tuning custom kernels:
    
    - Auto-tune tiling sizes and shared memory usage.
    - Schedule GPU threads based on warp/block-level parallelism.
    - Implement custom token-wise or layer-wise scheduling logic.

##### Cache and Memory Prefetching

- Use `__prefetch` instructions or async loads to bring data into shared memory before it is needed.
- Overlap KV fetches with matmul execution to hide memory latency.

#### Deployment-Aware Strategies

- **Load Balancing**: Use dynamic batching queues with GPU-aware request routing (e.g., based on latency or memory pressure).
- **Thread Affinity**: Bind computation to specific CPU cores or NUMA zones in CPU-bound systems.
- **Execution Profiling**: Use profilers like NVIDIA Nsight Systems or PyTorch Profiler to tune for bottlenecks.

#### Ecosystem Support

- **NVIDIA TensorRT** and **FasterTransformer**: Hardware-aware fused kernels and scheduling policies.
- **ONNX Runtime (ORT)**: Execution providers tuned for different hardware (CUDA, DirectML, TensorRT).
- **DeepSpeed**, **vLLM**, **Triton**, and **TVM**: Offer fine-grained control over scheduling and memory layout.

#### Performance Impact

- Hardware-aware scheduling can yield:
    
    - **1.5×–4× speedup** over naive scheduling for long sequences or large batches.
    - Better **multi-GPU scaling** for high-throughput inference.
    - Lower **latency variability** in real-time serving environments.

### Comparative Analysis

|**Technique**|**Purpose**|**Key Benefits**|**Primary Use Cases**|**Implementation Notes**|
|---|---|---|---|---|
|KV Caching|Reuse attention keys/values from previous tokens|Reduces per-token latency after first step|Autoregressive decoding (GPT, LLaMA)|Requires careful cache management; starts from second token onward|
|Model Quantization|Use lower-precision weights/activations|Reduces memory and compute cost|Edge inference, high-throughput serving|`int8`/PTQ for speed; QAT for better accuracy; needs hardware with quantization support|
|Operator Fusion|Combine adjacent ops into single kernel|Reduces memory access and kernel launch overhead|Attention blocks, FFNs, LayerNorm + activation|Use graph compilers (XLA, TorchScript), or fused CUDA kernels (TensorRT, FasterTransformer)|
|Speculative Decoding|Use draft model to guess multiple tokens|Reduces number of full-model forward passes|Long-form generation, chatbots|Needs a lightweight auxiliary model; uses top-1 match or log-prob threshold for validation|
|FlashAttention & Kernels|Memory-efficient attention computation|Reduces memory usage and boosts speed|Long-sequence LLMs, multi-head attention|Implemented with CUDA (FlashAttention), or Triton/xFormers; avoids storing full attention matrix|
|Batching|Process multiple requests together|Increases throughput and GPU utilization|High-concurrency inference (API servers, batch jobs)|Dynamic and token-level batching supported in vLLM, DeepSpeed, TensorRT|
|Prefilling|Precompute KV cache from prompt tokens|Avoids recomputation in autoregressive models|Chat and generation tasks with long prompts|Often paired with batching; prompt KV cache initialized before decoding begins|
|Prompt Caching|Cache KV states of repeated prompts|Saves time and compute on repeated static contexts|Chat APIs, few-shot prompt templates|Requires hashing/tokenizing prompt and storing cache; memory usage grows with cache diversity|
|Early Exit|Stop processing tokens/layers early based on confidence|Reduces per-token compute in deep models|Classification, QA tasks|Needs entropy or learned gating logic; difficult to apply in token-dependent generation|
|Token Pruning|Discard low-importance tokens during inference|Reduces sequence length in deeper layers|Long-sequence summarization, QA|Attention-based importance scoring; careful masking and index tracking required|
|Hardware-Aware Scheduling|Optimize kernel execution for specific hardware|Maximizes throughput and minimizes latency|All transformer-based workloads|Includes stream parallelism, memory prefetch, cache layout, tensor core tuning, and multi-GPU distribution|

## References

- [KV Caching Explained: Optimizing Transformer Inference Efficiency](https://huggingface.co/blog/not-lain/kv-caching)
- [Gaurav’s Blog – Efficient AI: KV Caching and KV Sharing](https://blog.gaurav.ai/2025/08/05/kv-caching-kv-sharing/)
- [Let’s build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)

## Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledModelAcceleration,   title   = {Model Acceleration},   author  = {Chadha, Aman},   journal = {Distilled AI},   year    = {2020},   note    = {\url{https://aman.ai}} }`

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)| 

[www.amanchadha.com](https://www.amanchadha.com/)