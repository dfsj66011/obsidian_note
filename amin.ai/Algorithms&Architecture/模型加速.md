
## 训练优化

### 概述

LLM 的训练优化旨在降低训练阶段的计算和内存开销，同时保持模型质量。随着 LLM 规模和序列长度的增加，传统的注意力机制和密集架构因其高计算和内存需求（尤其是自注意力机制的二次复杂度）而成为瓶颈。

本节探讨旨在通过算法和系统层面的改进来加速训练的创新。这些创新包括：

* *内存感知注意力算法*：如 FlashAttention 和 FlashAttention-2 这类，通过优化 GPU 内存层级（例如从高带宽存储器 HBM 到静态随机存储器 SRAM）之间的数据传输，显著降低了内存带宽占用和计算时间。这些方法采用分块计算、重计算以及注意力模块并行化等技术，优先提升硬件运行效率。
* *多查询和分组查询注意力方法*，例如在快速 Transformer 解码和 GQA 论文中提出的那些方法，通过共享键/值投影来减少注意力头中的冗余。这些技术对于加速解码和推理尤其有价值，同时还能减少训练期间的参数数量和计算成本。
* *稀疏和局部注意力机制*，如 Longformer 中引入的方案，用局部窗口化和任务特定的全局注意力组合取代全局自注意力。这种方法将内存消耗和计算时间从序列长度的二次方降低到线性，从而能够在更长的序列上进行高效训练。

### FlashAttention-1
### FlashAttention-2
### FlashAttention-3

由 Shah 等人（来自 Colfax Research、Meta、NVIDIA、佐治亚理工学院、普林斯顿大学和 Together AI）在 [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608) 中提出。FlashAttention-3 是针对 NVIDIA Hopper 架构GPU（H100）优化的注意力机制，通过利用硬件异步性和 FP8 低精度计算能力，实现了显著的加速效果和精度提升。

**主要贡献：**

* **生产者-消费者异步模式**：通过环形共享内存缓冲区实现 warp 专用软件流水线，将生产者 warp（通过 TMA 进行数据移动）与消费者 warp（张量核心 GEMM 运算）分离，从而隐藏内存和指令延迟。
  - **GEMM-softmax重叠**：通过“乒乓”调度在 warpgroups 之间以及 warpgroup 内部采用两阶段流水线技术，打破顺序依赖关系，实现分块 $QK^⊤$和 PV GEMM 运算与 softmax 的流水线并行，从而同时保持张量核心和特殊功能单元的活跃状态。
  - **FP8 低精度支持**：通过内核内转置（使用 LDSM/STSM）和寄存器置换，使 FlashAttention 适应 FP8 WGMMA 布局约束，并通过块量化和使用随机正交变换的非相干处理来提高 FP8 的精度。

**架构和实现：**

**输入**：查询（$Q$）、键（$K$）、值（$V$）矩阵被分割成小块；头维度 $d$，序列长度 $N$，查询块大小 $B_r$，键块大小 $B_c$。

**前向过程（FP16）：**

* 生产者扭曲：使用 TMA 从 HBM 依次加载 $Q_{i}、K_{j}、V_{j}$ 图块到 SMEM，并通过屏障通知消费者。
- 消费者扭曲：执行SS-GEMM（$Q_iK^⊤_j$）、逐行最大值跟踪、局部 softmax、RS-GEMM（$P_{ij}​V_j$​），并进行缩放以确保稳定性，将 $O_i$ 和对数求和指数值 $L_i$ 写入 HBM。
- 流水线版本：将迭代 $j$ 的 GEMM 与迭代 $j+1$ 的 softmax 重叠执行，需要额外的寄存器缓冲区（$S_\text{next}$）。

**FP8 模式**

* 布局处理：通过内核内转置确保第二个 GEMM 中的 $V$ 采用 $k$ 主序操作数布局；寄存器排列使 FP32 累加器与 FP8 操作数布局对齐。
* 量化：块级缩放（针对每个 $B_r×d$ 或 $B_c×d$ 分块）和非相干处理（哈达玛变换+随机±1对角矩阵）可降低异常值密集张量的均方根误差。
* 论文中的下图展示了两个 warpgroup 之间的乒乓调度，以实现 softmax 和 GEMM 的重叠计算：当一个 warpgroup 执行 GEMM 运算时，应调度另一个 warpgroup 执行 softmax 运算。相同颜色代表同一迭代步骤。

![](https://aman.ai/images/papers/FlashAttention%E2%80%913_1.jpg)
  - 论文中的下图展示了 2 阶段 WGMMA-softmax 流水线处理。
  
  ![](https://aman.ai/images/papers/FlashAttention%E2%80%913_2.jpg)

**基准测试：**

* 在 H100 SXM5 上，FP16 前向传递最高可达 740 TFLOPs/s（利用率 75%），比 FlashAttention-2 快 1.5-2.0 倍，比标准注意力机制快 3-16 倍；后向传递速度提升 1.5-1.75 倍。
* FP8前向传递接近1.2 PFLOPs/s，在某些头维度和序列长度上优于cuDNN。
* 准确性：FP16 与 FlashAttention-2 的误差相当（约 1.9×10−4 RMSE），两者均优于标准 FP16 注意力机制；采用块量化和非相干处理的 FP8，其 RMSE 比基线 FP8 逐张量缩放低 2.6 倍。

**消融研究：**

* 移除 GEMM-softmax 流水线或 warp specialization 会将吞吐量从 661 TFLOPs/s 降低至约 570-582 TFLOPs/s。
* 这两种优化都对性能提升有显著贡献。


### Multi-Query Attention (MQA)

在 [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150) 中被提出，Transformer 神经网络序列模型中采用的多头注意力层，是替代循环神经网络（RNN）在序列间传递信息的强大方案。虽然由于序列长度的可并行性，训练这些层通常快速且简单，但在无法实现这种并行化的增量推理场景中，由于需要反复加载庞大的"键"和"值"张量所带来的内存带宽开销，推理过程往往较为缓慢。

这篇由谷歌的 Shazeer 于 2019 年发表的论文提出了一种名为"多头查询注意力"(MQA)的变体，其中键和值在所有不同的注意力"头"之间共享，从而大大减少了这些张量的大小，进而降低了增量解码对内存带宽的需求。

他们通过实验验证，所得到的模型确实可以显著加快解码速度，并且与基线相比仅造成轻微的质量下降。

### Grouped-Query Attention (GQA)

在 [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) 中被提出。MQA（仅使用单个键值头）显著加速了解码器推理。然而，MQA 可能导致质量下降，而且仅为加速推理而训练单独模型可能并不理想。

来自谷歌研究的 Ainslie 等人的论文提出：(1) 一种方法，用原始预训练计算量的5%将现有多头语言模型检查点升级为 MQA 模型；(2) 引入分组查询注意力（GQA），这是多查询注意力（MQA）的泛化形式，使用介于多头和单头之间的键值头数量。论文中的下图展示了分组查询方法的概览。多头注意力有 $H$ 个查询、键和值头。多查询注意力在所有查询头之间共享单个键和值头。分组查询注意力则是在每组查询头之间共享单个键和值头，介于多头和多查询注意力之间。

![](https://aman.ai/images/papers/GQA.jpg)

MQA 采用单一键值头来加速解码器推理，但可能导致质量下降。作者提出了一种新颖方法，将现有的多头注意力（MHA）语言模型检查点转化为带有 MQA 的模型，仅需原始预训练计算量的 5%。

该论文提出了分组查询注意力（GQA），这是一种介于多头注意力和多查询注意力之间的中间方法。在 GQA 中，查询头被分成若干组，每组共享一个键头和值头。这种方法使得经过训练的 GQA 模型能够以接近 MHA 的质量和与 MQA 相当的速度运行。

在 T5.1.1 架构上进行的多项实验（涵盖 CNN/Daily Mail、arXiv、PubMed、MediaSum、Multi-News、WMT 和 TriviaQA 等数据集）表明，GQA 模型在推理速度与质量之间实现了良好平衡。该研究包含消融实验，用于评估不同的建模选择，例如 GQA 组的数量和检查点转换方法。这些实验提供了模型在不同配置下性能的深入见解。

该论文承认了一些局限性，例如对较长序列的评估挑战，以及缺乏与从头开始训练的模型进行比较。论文还指出，研究结果尤其适用于编码器-解码器模型，并表明 GQA 在仅解码器模型中可能具有更强的优势。他们证明，经过优化的 GQA 在质量上接近多头注意力机制，同时速度与 MQA 相当。

### Linear Attention

 由 Facebook AI 的 Wang 等在 [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768) 中提出。作者提出了一种优化 Transformer 模型中自注意力机制的新方法，将复杂度从序列长度的平方级降低至线性级。该方法名为 Linformer，在保持与标准 Transformer 模型相当性能的同时，显著提升了时间和内存使用效率。
 
Linformer 通过引入自注意力机制的低秩近似，从实证和理论层面证明了自注意力矩阵具有低秩特性。作者提出将原始的缩放点积注意力分解为多个通过线性投影实现的较小注意力模块。这种因式分解方法将自注意力的空间和时间复杂度从 $O(n²)$ 显著降低至 $O(n)$，有效解决了传统 Transformer 模型的可扩展性问题。

该模型架构在计算注意力之前，将键矩阵和值矩阵投影到低维空间，从而在保持模型有效性的同时降低了计算需求。该方法还包括跨投影参数共享的选项，可在不明显影响性能的情况下进一步减少可训练参数的数量。

总而言之，Linformer 通过以下方式实现线性时间注意力机制：

1. **低秩近似**：Linformer 的核心思想在于观察到自注意力机制可以通过低秩矩阵来近似。这意味着 Transformer 中自注意力捕获的复杂关系并不一定需要满秩矩阵，从而可以实现更高效的表示。
2. **降低复杂度**：传统 Transformer 中的标准自注意力机制在序列长度 $n$ 方面具有 $O(n²)$ 的时间与空间复杂度，而 Linformer 将其降低至 $O(n)$。这种显著的复杂度降低同时体现在时间与空间维度上，使其在处理长序列时效率大幅提升。
3. **线性自注意力机制**：Linformer 通过线性投影将缩放点积注意力分解为多个较小的注意力来实现这一目标。具体来说，它引入了两个线性投影矩阵 $E_i$ 和 $F_i$，用于计算键矩阵和值矩阵。通过首先将原始高维键矩阵和值矩阵投影到低维空间（$n×k$），Linformer 有效地降低了注意力机制的复杂度。
4. **操作组合**：这些操作的组合形成了原始注意力矩阵的低秩分解。本质上，Linformer 通过用一系列更小、更易管理的操作来近似完整的注意力机制，从而简化了计算过程，这些操作共同捕捉了原始全秩注意力的基本特征。

论文中的下图展示了：（左下方和右下）所提出的多头线性自注意力架构及示例；（右上）不同 Linformer 模型的推理时间与序列长度的关系。

![|600](https://aman.ai/images/papers/Linformer.jpg)

实验验证表明，Linformer 在标准自然语言处理任务（如情感分析和问答）上，使用 GLUE 和 IMDB 评论等数据集时，其性能与原版 Transformer 相当甚至更优。值得注意的是，该模型在训练和推理速度上有显著提升，尤其对处理较长序列更为有利。

此外，还测试了多种提升 Linformer 效率的策略，包括不同级别的参数共享以及根据模型内部不同层的特定需求采用非均匀投影维度的方法。作者们指出，Linformer 降低的计算需求不仅使高性能模型更易于获取且更具成本效益，还因能耗减少为更环保的人工智能实践打开了大门。

总之，Linformer 通过利用自注意力矩阵的低秩特性，为 Transformer 提出了一种更高效的自注意力机制。该方法将注意力计算的时间与空间复杂度从二次方降至线性，显著降低了计算负担，尤其适用于长序列处理。这使得 Linformer 在处理大规模数据集或长序列输入任务时成为极具吸引力的选择，而传统 Transformer 由于较高的计算需求在这些场景下可能不太适用。

### Longformer

在 [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) 中被提出，基于 Transformer 的模型由于自注意力操作而无法处理长序列，因为该操作的计算复杂度随序列长度呈二次方增长。

艾伦人工智能研究所的 Beltagy 等人于 2020 年发表的这篇论文，旨在通过引入一种注意力机制（该领域通常称为滑动窗口注意力）来解决这一局限性，这种注意力机制的计算复杂度随序列长度呈线性增长，从而能够轻松处理数千个甚至更多标记的长文档。

Longformer 的注意力机制可直接替代标准自注意力机制，它结合了局部窗口注意力与任务驱动的全局注意力。下图来自论文，比较了完整的自注意力模式和 Longformer 中的注意力模式配置。

![](https://aman.ai/images/papers/Longformer.jpg)

在先前关于长序列变换器研究的基础上，他们对 Longformer 进行了字符级语言建模评估，并在 text8 和 enwik8 数据集上取得了最先进的成果。与大多数先前的工作不同，他们还对 Longformer 进行了预训练，并在各种下游任务上进行了微调。

他们预训练的 Longformer 在长文档任务上始终优于 RoBERTa，并在 WikiHop 和 TriviaQA 上创造了新的最先进成果。最后他们推出了 Longformer-Encoder-Decoder（LED），这是支持长文档生成式序列到序列任务的 Longformer 变体，并在 arXiv 摘要数据集上证明了其有效性。

论文中的下图展示了完整自注意力机制与 Longformer 自注意力不同实现方式的运行时和内存占用情况：Longformer-loop 是非向量化实现，Longformer-chunk 是向量化实现，Longformer-cuda 则是定制 CUDA 内核实现。与完整自注意力机制不同（在当前 GPU 上处理长序列时会耗尽内存），Longformer 的内存占用随序列长度呈线性增长。不同实现方式的速度存在差异，其中向量化实现的Longformer-chunk 速度最快。

![](https://aman.ai/images/papers/Longformer2.jpg)


## 推理优化

### 概述

推理优化是 Transformer 模型部署中一个至关重要的研究和工程领域，特别是在实时性和资源受限的环境中。其目标是在不牺牲预测准确性的前提下，最大限度地降低运行 LLMs 的计算成本和延迟。推理阶段的优化直接影响这些模型在生产系统中的响应速度、可扩展性和实际可行性。

推理中的一个核心挑战在于许多大型语言模型的自回归特性，即每个标记都依赖于先前生成的序列。这种顺序依赖性导致朴素推理的成本高昂，尤其对于长序列而言。为解决这一问题，业界开发了一系列优化技术来提升基于 Transformer 的模型在推理时的性能：

* KV缓存：Transformer 模型中的 KV 缓存是一项关键优化技术，它显著提升了序列生成的效率和速度，成为这类模型在实际应用中部署的核心组件。KV 缓存在自回归解码过程中的应用，及其在延迟优化和可扩展性方面的作用，使其成为高效服务基于 Transformer 的模型不可或缺的部分。它允许存储并复用自注意力层先前计算得到的键值投影，避免冗余计算。这一技术大幅降低了首个标记之后的每个标记推理时间，支持生成长序列，对于实现聊天、流式处理和交互式代理等应用中的低延迟、高吞吐服务至关重要。
* 模型量化：模型量化将权重和激活值的精度从 32 位浮点数（float32）降低为更低位格式，如int8、float8，甚至 4 位表示（如 int4）。这能显著减少内存占用和带宽消耗，使其可在更小的硬件上部署并提升吞吐量。训练后量化（PTQ）和量化感知训练（QAT）是两种常用方法。量化模型得益于更快的矩阵运算和更低能耗，现代工具链（如 NVIDIA TensorRT、Intel Neural Compressor）可对量化算子进行硬件加速，同时将精度损失降至最低。
* 算子融合：算子融合将多个连续的操作（如线性投影、偏置加法、层归一化和激活函数）合并为单一计算内核。这种技术减少了 GPU 或 TPU 上的内存读写操作次数和内核启动开销，从而提升执行效率。例如，将全连接层与 ReLU 激活函数融合为单一内核后，不仅能降低延迟，还能更高效地利用原本因操作碎片化而未被充分利用的 SIMD 或 CUDA 核心。
* 推测解码：推测解码通过使用轻量级草稿模型在单次前向传递中预测多个未来标记，从而加速自回归生成过程。这些候选标记随后由完整但较慢的模型并行验证。若验证通过，则批量接受；否则回滚生成过程。该流程在保持生成保真度的同时，减少了昂贵全模型调用的次数。诸如草稿与目标模型、Medusa 方案、自推测解码、FastRAG 以及英伟达的预填充推测解码等方法均利用该技术，在保证模型输出质量的前提下显著提升吞吐量。
* FlashAttention 与高效注意力内核：FlashAttention 是一种内存高效的注意力算法，它通过分块、融合且适配 GPU 的方式计算注意力输出，避免了生成大型中间注意力矩阵的需求。该算法利用 GPU 的 SRAM 将频繁访问的数据块保留在高速内存中，并通过流式传输部分结果来最小化内存带宽压力。相比传统的基于 softmax 的注意力实现，这种方法在序列长度和批量大小上具有更好的扩展性。FlashAttention-2 及类似内核（如 xFormers、Triton）现已成为高性能 Transformer 推理堆栈中的标准组件。
* 批处理、序列打包和预填充：
	* 批处理将多个推理请求分组为单次执行，从而最大化 GPU 利用率、分摊内核启动开销并提高吞吐量。动态批处理能自适应传入请求的模式，而 token 级批处理（如 vLLM）则通过同步解码步骤来同时服务多个请求，且不会阻塞新请求。
	* 序列打包（Sequence Packing）通过将多个短序列拼接成批次元素内的单个序列张量，并使用注意力掩码防止跨序列注意力，从而最小化填充浪费。这提高了每批次处理的有用 token 密度，减少了内存占用并提升了有效吞吐量，尤其适用于序列长度变化较大的工作负载。
	* 预填充技术会在自回归解码开始前预先计算所有提示词（prompt tokens）的 KV 缓存，从而避免生成过程中的冗余计算。通过融合式预填充内核、提示共享和分层流式处理等优化手段，可进一步降低提示阶段的延迟——该阶段通常是长输入场景下开销最高的环节。
	这三种技术共同作用，能实现硬件高利用率、降低填充开销，并将每个 token 的计算成本降至最低。

* 提示缓存：缓存常用或重复提示的键值（KV）状态——例如系统指令、少量示例或用户自定义模板——从而无需为每个请求重新计算。在聊天或 API 驱动的系统中尤为有效，因为这类系统通常会跨会话使用相同的初始上下文（如“你是一个乐于助人的助手…”）。通过复用提示的 KV 状态，服务器可以完全跳过提示处理阶段，直接利用已初始化的缓存开始生成内容，显著缩短首词生成时间和整体计算量。
* 提前退出与 token 剪枝：提前退出机制允许 Transformer 层在达到置信度阈值或基于熵的停止标准时，终止对特定 token 的推理计算，从而节省后续层的计算开销。token 剪枝则根据学习得到的重要性分数或门控函数，在推理过程中动态移除被判定为无关的 token 或注意力路径。这些技术能在不明显牺牲模型输出质量的前提下降低计算成本，尤其适用于优先考虑速度而非完全精度的部署场景。
* 硬件感知调度：该优化旨在将推理工作负载与底层硬件的具体特性对齐，例如 GPU 内存层次结构、张量核心可用性或流水线并发性。调度策略包括算子放置、内存预取、流优先级设置以及多 GPU 设置的负载均衡。例如，在 NVIDIA GPU 上，框架可能利用 CUDA 流、共享内存和内核融合来最大化吞吐量，而 TPU 推理则可能利用 XLA 编译进行图级优化。经过精细调度的策略可减少资源争用、提高并行性，并最大化每瓦特的总推理吞吐量。

### KV Cache

#### 背景：自注意力机制

在一种简单的实现中，对于每个解码步骤，我们必须为当前序列中的所有标记计算所有层的K和V。如果n是目前为止的标记数量，l是层数，那么每一步需要进行l×(n−1)次矩阵乘法，每次乘法的成本为O(d²)，从而导致：Cost per token=O(l⋅n⋅d2)

#### 动机

在服务 Transformer 模型的场景中，键值（KV）缓存是一项核心优化技术，它能显著提升自回归解码的效率。该技术通过存储先前解码步骤中自注意力机制产生的中间计算结果（特别是键张量和值张量），使得这些数据无需在每个新生成步骤中重复计算。这既降低了推理时间，又减少了冗余内存访问，从而使大语言模型能够高效处理长文本生成任务。

##### 问题：朴素生成中的二次重新计算

在自回归生成过程中，每个新生成的标记都依赖于之前生成的所有标记。在简单的 Transformer 实现中，模型在每一个解码步骤和所有层中，都会为序列中的所有标记重新计算键 K 和值 V 的表示。这会迅速增加计算成本，因为单个注意力头预测每个标记的总成本为：O(l⋅n⋅d2)

其中：
* $n$ = 当前已处理的标记数量（序列长度）
- $l$ = 层数（深度）
- $d$ = 模型（嵌入）维度

如果不使用缓存，预测每个新标记需要以下步骤：

1. 为所有过去的标记和每一层计算键矩阵和值矩阵。
2. 执行以下形式的矩阵乘法：$K = XW_K, V = XW_V$，其中，$X$ 是层输入，$W_K$ 和 $W_V$ 是固定的权重矩阵。

##### 为什么朴素生成会失败

KV 缓存从根本上解决了这种简单方法导致的二次重复计算问题。如果没有 KV 缓存，即使生成 100 个 tokens 的响应也会导致大量冗余计算：

- Token 1: 计算 1000 个上下文标记的注意力
- Token 2: 重新计算所有 1001 个标记的注意力
- Token 100:​ 重新计算所有 1099 个标记的注意力

注意力计算的总数可以通过每个解码步骤的所有注意力长度的算术和得出：$$\sum_{t=1}^{100}(1000+t-1)=104950$$
（原文计算结果 55000 次有误）
其中：

- 1000 表示生成开始前可用的固定长度上下文（例如提示）。
- $(t − 1)$ 表示在生成第 $t$ 个标记之前已经添加的先前生成的标记数量。在第 $t$ 步时，模型已经在初始上下文的基础上生成了 $t−1$ 个新标记，因此现在它必须关注 $1000+(t−1)$ 个总标记。
- 对所有 100 个解码步骤的求和给出了整个生成过程中的注意力操作总数。
- 因此，要生成 100 个标记，模型需要进行大约 55,000 次冗余注意力计算——其中大部分是对先前计算过的键和值的重新计算。
- 效率低下得惊人：不使用 KV 缓存：100 个输出标记 ≈ 55,000 次注意力运算；使用 KV 缓存：100 个输出标记 = 100 次注意力运算（运算量减少约 550倍）

这突显了一个关键的权衡：KV 缓存通过内存使用换取计算节省。通过存储先前计算的键和值，模型避免了重复已完成的工作，从而显著提升了速度和可扩展性。

##### 解决方案：重用缓存表示

KV 缓存优化通过复用所有过往 token 预先计算好的 K 和 V 表征来解决这个问题。模型无需在每次生成新 token 时重新计算，而是直接：

- 重用缓存的 $K_{1:(n−1)}$ 和 $V_{1:(n−1)}$，
- 仅计算当前 tokens 的新 $k_t$ 和 $v_t$，
- 并将它们追加到缓存中。

这种方法有效消除了冗余计算，将每一步的计算成本从 $O(l·n·d²)$ 降低到 $O(l·d²)$——在序列维度上实现了 $n$ 倍的加速。

下图（[source](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)）展示了 transformers 中典型的自注意力模块：

![Self-Attention Block|500](https://aman.ai/primers/ai/assets/model-acceleration/SA.jpg)

##### 为什么这很重要

这一改进对于长序列尤为重要，因为序列长度 $n$ 可能达到数千甚至数百万个标记。如果不使用缓存，延迟会随着序列长度呈二次方增长，很快就会变得不切实际。而通过键值缓存，推理过程仅呈线性增长，从而为现代大语言模型实现了高效的流式处理和低延迟文本生成。

#### KV 缓存的结构与大小

KV 缓存存储了每个 Transformer 层的键和值张量、注意力头、每个批次内的样本索引以及 token 前缀长度（即已处理的 token 数量，包括提示和任何先前生成的 token ，而不仅仅是紧邻的前一个 token）。

假设一个 transformer 具有以下参数：

- 当前序列长度：$n$
- 层数：$l$ 
- 每层注意力头数：$h$
- 头维度：$d_k$
- 批量大小：$b$

上述设置的 KV 缓存将由每层的两个主要张量组成：

1. 一个形状为 $(b,h,n,d_k)$ 的键张量，用于存储所有过去标记的投影键 $K$。
2. 一个形状为 $(b,h,n,d_k)$ 的**值张量**，用于存储所有过去标记的投影值 $V$。

由于每一层都需要自己的（K 和 V）张量副本，因此存储的元素总数为：
$$\text{Total elements}=2⋅l⋅b⋅h⋅n⋅d_{k}$$
如果我们假设每个元素都以 16 位浮点精度（`float16`）存储，那么 KV 缓存的总大小（以字节为单位）为：$$\text{Size (bytes)}=2⋅l⋅b⋅h⋅n⋅d_{k}⋅2$$其中最后的因子 2 表示每个 `float16` 元素占 2 字节。例如，对于具有 $l=32$ 层、$h=32$ 个头、$d_k=128$、$b=1$ 和 $n=1000$ 的模型：`Size=2⋅32⋅1⋅32⋅1000⋅128⋅2=524,288,000 bytes (≈500 MB)`

这表明 KV 缓存可能成为长序列内存消耗的主要来源，因此在大语言模型推理中通常会采用量化或分块注意力等优化手段。

#### 缓存自注意力值

KV 缓存利用了以下两个关键特性：

1. 模型权重（$W_K$ 和 $W_V$）在推理过程中是固定的。
2. 给定 token 的 K 和 V 表示取决于该 token 及其之前的所有 token（通过其隐藏状态），但它们不依赖于或随任何未来 token 而变化（即，对于所有后续解码步骤来说，它们是**不可变的**）。

因此，一旦我们计算出给定（token、layer、head）元组的 K 和 V 表示，我们就可以存储它们并在所有后续解码步骤中重复使用。

￼​在解码步骤 $t$ 时：​ ​ ​

* 无缓存：为所有 $t$ 个标记从头重新计算 $K_{1:n}$ 和 $V_{1:n}$。​ ​
* 有缓存：复用 $K_{1:(n−1)}$ 和 $V_{1:(n−1)}$，仅计算新的 $k_t$ 和 $v_t$ 并将它们附加到缓存中。

下图（[source](https://huggingface.co/blog/not-lain/kv-caching)）展示了 KV 缓存的过程，说明如何仅计算新 token 的 K 和 V，而其余部分则被重复利用：

![KV Caching Illustration](https://aman.ai/primers/ai/assets/model-acceleration/KV.png)

此优化将每个解码步骤的成本从 $O(l⋅n⋅d²)$ 降至 $O(l⋅d²)$ —— 在序列维度上实现了 $n$ 倍的加速。

对于长序列而言，这种改进尤为显著，其中 $n$ 可以达到数千甚至数百万个标记。通过消除冗余的注意力计算，KV 缓存实现了高效、低延迟的大规模生成。

###### 为什么不缓存之前的查询？

在自注意力操作中，仅使用最近的查询 $q_t$（由于依赖于最新 token 的嵌入，每一步都会重新计算），因此缓存先前的查询（$q_{1:(n−1)}$）并无益处。简而言之，解码时只需要序列中最新 token 对应的查询。

#### 带缓存的自动回归解码过程

1. 初始序列（预填充阶段）
	
	* 给定一个提示序列 `S=[x1,x2,…,xn]`，模型会计算所有层中所有提示标记的 K 和 V 张量，并将它们存储在 KV 缓存中。
	* 这一步仍然会产生全部成本 $O(l⋅n⋅d²)$，因为我们还没有缓存任何值。
	* 在完成预填充步骤后，模型会进入解码阶段，此时每一步处理一个标记。
    
2. 预测下一个标记（解码阶段）
	
	在解码步骤 $n+1$ 时：
	* 为新令牌计算查询向量 $q_{n+1} = x_{n+1}W_Q$。
	* 从缓存中检索所有先前的键和值：$K_{1:n}, V_{1:n}$。
	* 使用以下公式计算新 token 的注意力输出：`Attention(qn+1, K1:n, V1:n)`。

3. 更新缓存：
    
    - 计算当前 token 的新键和值向量：$k_{n+1}=x_{n+1}W_K,v_{n+1}=x_{n+1}W_V$
    - 将这些向量追加到 KV 缓存中，以便在未来的解码步骤中重复使用：
        Kcache←[K1:n,kn+1]
        Vcache←[V1:n,vn+1]

3. 重复：持续生成直至遇到序列结束标记（EOS）或达到最大标记限制。

#### 实施细节

##### 缓存张量形状

假设：批次大小 $B$，最大序列长度 $n$，头数 $H$，头维度 $d_k$，层数 $l$，

KV 缓存的构建方式是为每个（token、layer、head）元组存储 K 和 V。对于给定的 layer 和 head，缓存张量的形状和大小如下：

- 键缓存（每层、每头）：$K(l,h) cache \in ℝ^{B×n×d_k}$，键缓存大小（字节）：`(l,h)K=B×n×dk×sizeof(dtype)`
- 值缓存（每层、每头）：​$V(l,h)cache in ℝ^{B×n×d_k}$，值缓存大小（字节）：`S(l,h)V=B×n×dk×sizeof(dtype)`
- 每层每头的总内存大小（字节）：`Slayer, head=2×B×n×dk×sizeof(dtype)`，系数 2 表示同时包含键和值张量。

将所有层和注意力头结合起来时，KV 缓存的总量代表了存储整个模型中所有键和值张量所需的完整内存占用。以下方程描述了全模型范围内的 KV 缓存维度及其对应的内存需求：

- 键缓存（所有层，所有注意力头）：​K(total) cache ∈ ℝB×l×H×n×dk，键缓存大小（字节）：`S(总)K = B × l × H × n × dk × sizeof(数据类型)`
- 值缓存（所有层，所有注意力头）：V(total) chche ∈ ℝB×l×H×n×dk，值缓存大小（字节）：`S(总)V = B × l × H × n × dk × sizeof(数据类型)`
- 整个模型的总内存大小（字节）：`S总 = 2 × B × l × H × n × dk × sizeof(数据类型)`

注意：

* 缓存大小与 $B$、$l$、$H$、$n$ 和 $d_k$ 呈线性比例关系。在自回归生成过程中，模型在解码阶段每一步仅处理一个标记，每一层和每个注意力头在每一步仅追加一个新键向量和一个新值向量。因此，随着解码的进行，总缓存会随着生成标记数量的增加而线性增长——这反映了键值对随时间逐步累积的特性。
* 实践中：
	* sizeof(dtype)=2 bytes for FP16/BF16 caches.
	* sizeof(dtype)=1 byte for INT8 caches.
* 示例：对于一个参数为 $l=32、H=64、d_k=128、B=8、n=4096$ 的模型，KV 缓存很容易消耗数十 GB 的 GPU 内存。因此，高效的缓存管理（如截断、量化或卸载）对于实际部署至关重要。
- 高效的内存布局至关重要——连续缓冲区能够实现快速追加并减少内存复制开销。

##### 预填阶段

首次处理提示时：

- 模型会为每一层中的所有提示标记计算 K 和 V，填充缓存。
- 这一初始步骤的成本与原始方法相同：$O(l⋅n⋅d^2)$

此后，我们进入解码阶段，此时缓存会带来性能优势。


##### KV 缓存的更新

在自回归解码过程中，K 和 V 投影会为每个处理过的 token 在所有层和注意力头中进行缓存。每次生成一个新 token 时：

1. 模型会为每个层中的该标记计算 $k_t$ 和 $v_t$。
2. 这些向量会被追加到现有的 $K_{\text{cache}}$ 和 $V_{\text{cache}}$ 中。
3. 更新后的缓存随后用于计算下一个标记的注意力输出。

#### 延迟优化/节约

##### 投影成本

**无缓存：** 对于单个注意力头，在解码步长为n的序列中，自注意力模块会为所有l层中的n个token重新计算K和V。简而言之，每个新生成的token都需要等待完整的注意力重新计算。每个预测标记的计算成本：$O(l⋅n⋅d^2)$

**使用缓存时：** 仅计算新 token 的键和值，其余部分从缓存中重复使用。简而言之，每个 token 仅计算新的注意力分数。每个预测 token 的计算成本：$O(l·d^2)$

这意味着在序列维度上实现了 $n$ 倍的加速。对于较大的 $n$ 值（例如数千或数百万个 tokens），成本降低效果显著。

##### 注意力分数计算

**无缓存：** 在序列长度为 $n$ 时，计算注意力分数需要将新 token 的查询与所有 $n$ 个键相乘。每一层都需要进行此操作，因此每个预测 token 的注意力分数计算成本为：$O(l⋅n⋅d)$，由于 $n$ 随着每个生成的标记而增加，这一步的延迟在每标记生成时呈线性增长，但整体解码（投影+注意力）在没有缓存的情况下仍然呈二次增长。

**使用缓存：** 所有先前 token 的键已存储。在序列长度 $n$ 时，我们仅计算新查询与缓存键的点积：$O(l·n·d)$；每个 token 的成本仍然随着 $n$ 线性增长，但缓存消除了因重新计算旧 token 的键和值而产生的二次增长。

##### 总复杂度

KV 缓存技术将整体解码延迟从与 $n$ 的平方关系转变为近似线性关系，这对生成长序列而言是一项重大改进。具体来说，KV 缓存将主导计算量级从 $O(n^2·d^2)$ 降至 $O(n^2·d)$，对于典型 Transformer 模型的规模而言，这显著改善了长序列生成的延迟问题。其数学表达如下。

**无缓存：** 每个预测标记的总成本 = 投影成本 + 注意力分数计算：`O(l⋅n⋅d²) + O(l⋅n⋅d) ≈ O(l⋅n⋅d²)`，对于长度为 $n$ 的整个序列，总解码成本为：`O(l⋅n²⋅d²)`

**使用缓存：** 每个预测标记的总成本 = 投影成本 + 注意力分数计算：`O(l·d²) + O(l·n·d) ≈ O(l·n·d)`，对于长度为 $n$ 的整个序列，总解码成本为：`O(l·n²·d)`

#### 实际部署注意事项

##### 内存管理

高效管理键值缓存是大规模 Transformer 模型部署中的主要工程挑战之一。每个序列的缓存会随着已处理 token 数量的增加而线性增长，因为模型需要为每个新 token 存储其在每一层和每个注意力头中的键值表示。因此，当处理多个并发请求时，即使上下文长度只是适度增加，也可能导致 GPU 内存压力呈指数级上升。

为了缓解这种情况，系统采用了多种策略：

* 滑动窗口缓存：服务器通常不会保留完整的注意力历史记录，而是为每个请求仅保留最近的 $N$ 个标记。这种滑动窗口机制可以在不超出内存预算的情况下维持长时间对话，但代价是长距离记忆能力略有下降。例如，如果模型支持 $32k$ 标记但内存受限，缓存可能仅保留最后 8k-16k 标记。
* 缓存截断与压缩：在极端情况下，可以对缓存进行截断或量化处理。当达到内存预算时，截断操作会丢弃上下文中最旧的部分；而压缩方法（例如以较低精度存储键值和数值，如 FP16 或 INT8）则通过牺牲少量精度来显著节省内存空间。
* 分层感知缓存分配：并非所有层对性能的贡献均等。某些部署系统会动态地为注意力最敏感的层分配更高精度或更长的缓存保留时间，同时减少其他层的资源使用。
* 卸载到主机内存：对于超长上下文或多轮对话，GPU 内存可能不足。系统可将部分缓存卸载到 CPU 内存甚至基于 NVMe 的内存池中，按需取回。但这会带来延迟权衡，且需要谨慎进行内存固定以最小化数据传输开销。  

##### 动态批处理

动态批处理对于在实时推理场景中最大化 GPU 利用率至关重要。由于用户发出的请求长度不一，并且在 token 生成过程中异步推进，每个请求都维护着一个独立的 KV 缓存，该缓存按照自己的速率增长。一个设计良好的系统必须：

- 高效地将具有相似解码步骤的请求分组，形成微批次，同时不破坏序列依赖性。
- 保持每个请求的缓存隔离，确保在每次注意力计算时检索到正确的键值历史。
- 实现快速查找和追加机制，通常由内存池或自定义分配器支持，允许在不使用繁重同步锁的情况下并发更新缓存。
- 使用流式注意力调度：在每个解码步骤中，系统识别哪些请求已准备好解码，并将它们临时合并为一个批次。一旦生成下一个 token，每个请求的缓存将独立更新。

诸如 vLLM 和 TensorRT-LLM 等系统提供了专门的运行时调度器，能够动态管理每个请求的缓存，同时实现接近最优的 GPU 占用率。在此类架构中，重用 KV 状态和跨请求批处理的能力决定了整体吞吐量。



##### 缓存并行性

在大规模多 GPU 或分布式服务环境中，KV 缓存本身成为分布式数据结构。当模型的层或注意力头被分片到不同设备时，对应的键和值必须遵循相同的分区策略。典型配置包括：

* 张量并行：每个 GPU 仅持有部分注意力头。在跨设备注意力计算过程中，K 和 V 张量通过集合操作（如全收集）在 GPU 之间交换或同步。高效的实现方式通过重叠通信与计算来最小化延迟。
* 流水线并行：将各层分布在多个 GPU 上。每个 GPU 只需维护其所属层的 KV 缓存。然而，在前向传播过程中，中间激活值会在设备间流动传输。系统必须确保各流水线阶段的缓存在时间上对齐，以保持注意力机制的正确性。
* 模型并行+数据并行混合：在高度可扩展的部署中，KV 缓存既被分片（用于模型并行）又被复制（用于数据并行）。系统必须处理副本之间的同步和内存一致性，通常通过基于 NCCL 的通信后端实现。
* 跨节点缓存：当模型在多个节点上运行时，缓存可以存储在分布式共享内存或支持远程内存访问（RDMA）的硬件中，从而实现无需 CPU 干预的直接 GPU 到 GPU 缓存检索。

##### 为什么你不能总是缓存一切

KV 缓存中的内存增长与序列长度呈线性关系，并与层数、注意力头数和隐藏维度成正比：

- 内存扩展：对于大型模型（参数达数百亿），单个包含 1000 个标记的序列可能消耗约 1GB 的键值缓存。
- 批次大小影响：缓存的并发序列越多，GPU 内存能容纳的请求就越少，直接影响吞吐量。
- 上下文长度：在超长上下文场景下（例如 10 万个标记），简单的全缓存可能超过 100 GB，远超高端 GPU 的容量上限。

#### 多头注意力与键值缓存

在实践中，自注意力机制通过多个注意力头实现，每个头在嵌入维度的子空间中运行。对于头 $h$（$h∈\{1,…,H\}$），我们有：`Q(h)=XWQ(h),K(h)=XWK(h),V(h)=XWV(h)`，每个头的注意力输出被串联起来：`Q=concat(Q(1),Q(2),…,Q(H))`

多头注意力中的缓存机制：KV缓存存储每个头和每个层的键和值。键和值缓存的形状：`Kcache∈ℝB×H×n×dk`，`Vcache∈ℝB×H×n×dk`，这里：

- $B$ = 批处理大小（并行处理的序列数量）
- $H$ = 注意力头数量
- $n$ = 序列长度（缓存中存储的标记数量）
- $d_k$ = 每个注意力头的键（和值）向量维度

**性能影响：**

- 由于每个头的 KV 缓存是独立的，缓存逻辑按头操作，但存储通常实现为一个统一的张量以提高效率。
- 这个统一的张量被安排为对 GPU 张量核心友好，使得解码期间的读写操作非常快速。

虽然 KV 缓存大幅降低了序列维度的计算成本，但深度维度（层数 $l$）仍然是计算量的主要来源。这催生了  KV共享理念（详见 KV 共享章节）——通过在后半段（或部分）网络层中复用 K 和 V 表征来进一步削减计算量。KV 共享机制建立在 KV 缓存基础上，但它从网络层/深度维度而非 token 维度来解决问题。

#### KV 缓存优势总结

* 通过存储和重用 K、V 张量而非每一步重新计算，减少重复计算。
* 在自回归生成中实现高效解码，将每一步成本从 `O(l⋅n⋅d²)` 降至 `O(l⋅d²)` ——在序列维度上实现 `n` 倍加速。
* 通过统一且对 GPU 张量核心友好的张量布局，优化硬件加速性能。
* 可良好扩展至大型模型与长上下文场景，延迟随序列长度呈线性而非二次增长。
* 保持准确性，因为缓存的 K 和 V 与固定权重下的重新计算值完全一致。

#### KV 共享

KV 缓存技术由 Sun 等人在 2024 年发表的论文 [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)  中提出，该技术优化了序列维度（n）的计算成本，但在深度维度（l）——即模型的层数——方面，每一层的键（K）和值（V）仍需进行完整的计算。

KV 共享通过降低沿深度维度计算 K 和 V 的成本来解决这一问题。这种方法的可行性直觉来自于Csordás 等人的研究 [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898) ，该研究通过实证表明，在类似 Transformer 的深度模型中，最后几层之间具有相关性。这意味着最后几层并不一定会添加太多新信息，而更多是在微调已经生成的输出。这种冗余性可以被利用来节省计算量，而不会显著降低模型质量。

##### KV 共享的工作原理

核心理念：在最后一部分层之间共享实际的键（K）和值（V）表示（而不仅仅是权重矩阵）。例如，如果我们共享后半部分层（l2 层）：

1. 共享区域之前的最后一层正常计算 K 和 V。
2. 共享区域中的所有后续层无论输入如何，都复用这些 K 和 V 而无需重新计算。
3. 其他参数（例如 WQ、MLP 权重）在每层中保持独立。

从数学角度来说，设 Lshare 为第一个共享层的索引。对于任意层 `j≥Lshare`：`K(j)=K(Lshare), V(j)=V(Lshare)`

下图（来源）展示了在后半部分层中共享 KV 的机制，说明了如何重复使用一组已计算的 K 和 V 值，而非重新计算： 
![KV Sharing Illustration|500](https://aman.ai/primers/ai/assets/model-acceleration/KVS.jpg)

##### FLOP Savings

如果最后的 lk 层共享 K 和 V，我们就能完全避免在这 lk 层中计算它们。FLOP 减少：节省=lkl=1k，占总键值计算的一部分。结合 KV 缓存：KV 缓存降低了 $n$（序列）维度的成本，KV 共享降低了 l（层）维度的成本。

##### 为什么 KV 共享可以成功

论文中引用的实证研究表明，在深度 transformer 模型中，最后几层往往会产生相关的输出。这表明后面的层主要是在进行微调，而不是引入根本性的新信息。因此，在这些层中重用 K 和 V 对输出质量的影响微乎其微，同时显著降低了计算和内存的使用量。

##### Memory Benefits

无需存储键值对，对于共享层来说完全不需要，减少推理和训练过程中的内存占用，在处理长序列时尤其有价值，因为缓存大小主要由 $B×H×n×d_{k}×l$ 的比例决定。

##### 部署说明

为了获得最佳效果，必须在训练时考虑 KV 共享，因为如果事后应用共享，未受此约束训练的模型可能会出现质量下降。KV 共享与 KV 缓存协同工作，前者处理深度问题，后者处理序列长度问题。

### 模型量化

模型量化是一种技术，用于将神经网络中的数值（通常是权重和激活值）从高精度格式（如 32 位浮点数 float32）降低到低精度格式（如 int8、float8 甚至 int4）。这种方法可以实现更快的推理速度、减少内存使用并降低功耗，特别是在支持低精度运算的硬件上。

#### 为什么要量化？

量化可以显著提高效率：

* 减少内存占用：int8 模型比 float32 模型少占用 75% 的内存。
* 更快的运算：现代加速器（如 NVIDIA Tensor Cores、Intel AVX-512 VNNI）原生支持并高度优化了低精度运算（如 int8 或 int4 矩阵乘法）。
* 降低延迟：由于需要移动的数据更少且计算内核更快，量化模型可以减少端到端的推理时间。

#### 量化类型

##### 后训练量化 (PTQ)

PTQ（后训练量化）无需重新训练，即可将预训练的 float32 模型转换为低精度模型。其原理是通过少量数据样本校准张量的数值范围。

PTQ 的关键步骤：

* 范围校准：从校准数据集中识别权重和激活的最小/最大值。
* 量化和零点计算：对于每个量化张量，计算：`q=round(rs)+z`，其中：
    - $r$ 是实数值
    - $s$ 是比例（即步长）
    - $z$ 是零点，用于在量化域中保留零映射
    - $q$ 是量化值（例如 8 位整数）

权重和激活值裁剪：将数值裁剪至目标位宽的可表示范围内（例如，对于有符号 int8 类型，范围为`[-128, 127]`）。

##### 量化感知训练（QAT）

QAT 在训练过程中模拟量化。通过添加伪量化层来模拟低精度计算，同时保持高精度的梯度。优势是，对于敏感模型（如 GPT、BERT）比 PTQ 更准确，允许模型在微调过程中适应量化误差。

实现细节上，PyTorch 和 TensorFlow 等框架包含伪量化模块（例如`torch.quantization.FakeQuantize`）。在模型图中插入量化-反量化对，以模拟实际量化操作的行为。

#### 静态量化与动态量化

静态量化：激活值通过校准预先量化。需要代表性输入数据，性能更高但灵活性较差。
动态量化：权重预先量化，但激活值根据运行时实际值进行量化。更灵活且易于集成，但速度稍慢。

#### Transformer中的量化

在诸如 GPT 或 BERT 这样的 Transformer 模型中，量化技术主要应用于以下部分：

- 线性层：包括注意力机制中的查询（query）、键（key）、值（value）以及输出投影层。
- 矩阵乘法密集型模块：如多层感知机（MLP，即前馈网络）层。
- 嵌入层：通常会进行特殊处理的量化，以保持查找效率。

特殊注意事项：

* LayerNorm 和 Softmax 对量化敏感，通常保持为 `float32` 格式。
* 注意力分数可能需要 FP16 或 float32 精度以避免不稳定性。
* 有时会使用混合精度量化（例如 `float8` 权重搭配 `int8` 激活值）。

#### 工具和框架

- **NVIDIA TensorRT / FasterTransformer**
- **Intel Neural Compressor (INC)**
- **PyTorch Quantization Toolkit**
- **ONNX Runtime Quantization**
- **BitsAndBytes (for 8-bit and 4-bit LLMs)**

这些工具提供从量化、验证到部署模型的端到端流程。

### 算子融合

算子融合是一种推理优化技术，它将神经网络计算图中多个相邻操作合并为一个复合操作。这样做可以减少内存读写、内核启动以及操作间通信的开销，特别是在基于 GPU 或 TPU 的系统上。

融合通过将数据保留在更快的寄存器或共享内存中，而不是在每次小操作之间将其刷新到较慢的全局内存，从而减少延迟并提高计算效率。

#### 动机

现代深度学习工作负载通常涉及许多按顺序执行的小型操作——例如矩阵乘法后接偏置加法、归一化和非线性激活。
```python
x → Linear → AddBias → LayerNorm → ReLU
```

否则，这些操作中的每一个都可能作为单独的内核来实现。这会导致：

- 内核启动开销增加。
- GPU 并行性利用效率低下。    
- 内存访问重复且延迟高。
- 编译器优化机会有限。

通过融合它们，计算变得更加紧凑，最大限度地减少开销并最大限度地提高性能。

#### 常见融合模式

Transformer 推理中最常融合的序列包括：

* GEMM + Bias Add + Activation：例如 `Y=ReLU(X@W+b)`，典型用于 MLP 层
* Residual Add + LayerNorm + Dropout：用于 transformer 块
* Query/Key/Value Linear Projections：三个 `Linear` 操作融合成一个矩阵乘法后再分割。
* Softmax + Masking：在注意力机制中，softmax 通常与掩码逻辑融合，以避免 GPU 上的分支发散。

#### Transformers 中的融合

在 Transformer 架构中，算子融合在以下方面特别有价值：

* 多头注意力模块：
	* 将 Q/K/V 投影和 reshape + transpose 逻辑合并到一个内核中。
	* 将注意力分数计算、掩码处理和 softmax 融合为一个高效操作。
* 前馈网络（FFNs）：
	* 融合两个带有中间激活函数（如 GELU 或 ReLU）的线性层。

#### 实现细节

融合可以通过多种方式实现：

##### 图级融合（提前编译）

像 XLA（用于 TensorFlow）或 TorchScript（用于 PyTorch）这样的高级编译器可以在编译过程中分析计算图并融合操作。PyTorch 示例：

```python
@torch.jit.script 
def fused_layer(x, w1, b1, w2, b2):     
	return F.relu(F.linear(x, w1, b1)) @ w2.T + b2
```

TorchScript 可能会将 `linear + relu` 融合到单个内核中。

##### 内核级融合（运行时）

英伟达的 TensorRT 和 FasterTransformer 等框架包含了手工编写的 CUDA 内核，这些内核将多个操作（例如 QKV 投影+转置+缩放+矩阵乘法）一次性完成。

示例：一个融合的变压器内核可能会计算：

```python
qkv = fused_linear_bias_act(x);  // one call 
q, k, v = split_heads(qkv);      // internal fused transpose and reshape
```

这减少了全局内存流量，并利用寄存器/共享内存来存储中间结果。

##### 自定义内核生成

像 TVM 或 Triton 这样的库允许在硬件优化的 DSL 中定义自定义融合内核。这些内核可以即时编译以实现最大吞吐量。

Triton 示例：

```python
@triton.jit def 
fused_gemm_relu(...):     # 使用GPU线程块定义融合矩阵乘法+偏置+ReLU的逻辑
```


#### 性能影响

算子融合可能导致：

- 注意力模块的延迟降低了 30% 至 50%。
- 硬件利用率更高，尤其是在配备张量核心或向量化 ALU 的 GPU上。
- 降低了内存带宽压力，这通常是LLM推理中的瓶颈。

#### 工具与生态

- TensorRT：针对 Transformer 块进行广泛的融合优化。
- FasterTransformer：融合 QKV 和 FFN 核心运算。
- ONNX Runtime 结合图优化器：自动执行融合优化流程。
- TorchScript + FBGEMM：线性运算与激活函数的融合操作。
- TVM / Triton：可定制且可调优的融合核心运算。


### 推测解码（略）

### FlashAttention 与高效注意力内核（略）

将 softmax、缩放、掩码和矩阵乘法融合为单一内核：将这些操作合并为一个 GPU 内核，避免在内存中存储中间结果。通过一次性完成缩放、掩码、softmax 计算以及与 V 的加权求和，FlashAttention 降低了内存带宽占用并提升了计算效率。

##### 高级算法

1. 将查询块 Qi和键块 Kj 加载到共享内存中。
2. 计算该块的注意力对数QiKTj/√d。
3. 就地应用掩码和 softmax，更新指数的运行总和和最大值以确保数值稳定性。
4. 在不存储中间注意力矩阵的情况下，累加部分输出 Ai,j=softmax(QiKTj/√d)Vj。
5. 跨块重复计算，直到得到完整结果。


#### FlashAttention-2 改进

* 新增对非因果注意力机制、可变长度序列以及更优的**线程束级并行**的支持。
- 通过更激进的缓存和循环展开策略减少冗余内存加载。
- 提升反向传播效率，使其不仅适用于推理场景，还能有效支持训练任务。

#### 其他高效内核

* xFormers（Meta）：模块化注意力实现，支持 Flash、稀疏和内存高效变体。
* 基于 Triton 的注意力：使用 Triton 的 GPU DSL 轻松定义融合注意力内核。
* PagedAttention（vLLM）：优化批量推理的 KV 缓存访问，减少内存碎片并降低延迟。

#### 性能提升

FlashAttention 将注意力内存复杂度从：

- $O(L^2)$ 优化为 $O(L)$​ 以降低内存消耗。
- 在 A100 GPU 上处理长序列（超过 1000 个token）时，实现 1.7–2.7 倍加速。
- 与近似方法不同，本方案在浮点精度范围内保持注意力输出的精确性。

#### 用于推理

FlashAttention 特别适用于：

- 长上下文模型（例如，4k 至 128k tokens）。
- 多头注意力机制，其中每个头的内存使用会迅速累积。
- 部署在具有大共享内存的 GPU 上（例如，NVIDIA A100、H100）。

#### 集成

- Hugging Face Transformers 通过 `use_flash_attention_2=True` 实现
- PyTorch 通过自定义 CUDA 扩展或 Triton 内核实现
- DeepSpeed、FasterTransformer​ 和 xFormers

### 批处理、序列打包和预填充

批处理和预填充是推理时的优化技术，通过更高效地利用硬件和避免冗余计算来提高效率和吞吐量。这些技术在实时或高并发情况下服务大语言模型时尤为重要。

#### Batching

批处理是指将多个推理请求分组，通过模型进行一次前向传播的过程。这能提高硬件利用率、分摊开销并降低每个请求的平均延迟，尤其适用于针对矩阵密集型工作负载优化的 GPU。

##### 动机

如果不进行批处理，每个请求都会导致前向传播的利用率不足。

- 小输入张量 → GPU 核心占用率/利用率低
- 每个内核启动的开销高
- 浪费内存带宽

批处理通过将多个请求对齐到一个形状为的张量中来解决这个问题。Batch Tensor: `(B,L,d)`

##### Batching 类型

1. 静态批处理：请求按固定时间间隔分组。简单但灵活性较低。
2. 动态批处理：请求在运行时根据启发式规则（如请求到达时间、序列长度或提示相似性）进行缓冲和分组。
3. token 级批处理：由 vLLM 首创，按共享解码步骤而非序列对序列进行分组。支持长时间运行的生成任务而不阻塞新任务。
4. 异步批处理：利用请求队列和调度器，根据硬件负载决定批处理时机。

##### Padding 和 Masking

由于序列长度可能不同，较短的序列会进行相应的填充和掩码处理。填充会增加内存开销，但能实现统一的矩阵运算。例如：

* 序列 A： `[Hello, how, are, you]` →  长度 4
* 长度 B： `[Hi]` → 长度 1
* 批量输入：`[[Hello, how, are, you], [Hi, PAD, PAD, PAD]]`

##### 性能优势

- 更高的吞吐量：GPU 可以并行处理大型矩阵。
- 更低的内核启动开销。
- KV 缓存和内存带宽的分摊使用。

#### Sequence Packing

序列打包是一种优化技术，用于减少批量处理变长序列时的填充开销。它不再将批次中的所有序列填充至最大长度，而是将多个较短序列拼接成同一批次元素中的单个连续序列。

这种方法仅存储和处理实际的标记，使用注意力掩码来确保来自不同原始序列的标记不会相互关注。

##### 示例

不打包的情况：`[Hello, how, are, you, PAD, PAD, PAD] [Hi, there, PAD, PAD, PAD, PAD, PAD]`，内存使用：与每序列 7 个 tokens（包括填充）成比例。

打包的情况：`[Hello, how, are, you, Hi, there]`，加上 mask，用于阻止 `you` 和 `Hi` 之间的注意力。

##### 收益

* 减少内存占用——存储和处理的填充标记更少。
- 提高硬件利用率——每批次的有效序列密度更高。
- 降低混合长度工作负载的延迟——在长短序列混合处理时尤为有利。

##### 权衡

- 构建和应用更复杂的注意力掩码时会有轻微的开销。
- 可能需要专门的批处理逻辑和内核支持以达到最佳性能。

#### Prefilling

预填充是指在自回归解码开始之前，对提示或上下文标记的模型激活（主要是 KV 缓存）进行一次性计算。

##### 动机

Transformer 推理将过程分为：

1. 提示阶段（预填充）：处理整个提示以初始化 KV 缓存。
2. 生成阶段（解码）：使用缓存的键和值逐个生成 tokens。

提示阶段成本显著更高，因为它需要处理多个未经缓存的 tokens，而解码阶段则通过 KV 缓存机制逐个处理新生成的 tokens。

##### 预填充逻辑

给定一个包含 n 个标记的提示：

* 该模型执行完整的前向传递，以计算所有 n 个位置的注意力输出。
* 在此期间，它初始化了 KV 缓存张量：`K1:n,V1:n`
* 这些被用于所有后续生成步骤中，以避免重复计算。

##### 优化

- 融合预填充内核：像 FasterTransformer 这样的库使用专用内核，以高效的单次处理方式批量预填充 KV 缓存。
- 提示共享：如果多个请求使用相同的提示（例如“你是一个乐于助人的助手……”），则缓存预填充结果并在请求间重复使用。
- 分层流式处理：某些实现采用逐层填充 KV 缓存的方式，以重叠计算和内存操作。

##### 实际应用

在生产环境中：

- 提示预填充通常是延迟的主要来源，尤其是在处理长提示时（例如超过 1000 个标记）。
- 预填充不可缓存，除非提示被重复使用。这就是提示缓存发挥作用的地方。
- 系统可能会延迟解码，直到批次中的所有请求完成预填充阶段。

##### 性能收益

- 避免跨解码步骤的冗余计算。
- 实现内存和注意力上下文的高效复用。
- 对长上下文推理和多用户服务至关重要。

### Prompt 缓存

提示缓存是一种推理时优化技术，它通过复用频繁出现或重复提示词符的已计算键值（KV）注意力状态，消除了自回归解码过程中预填充阶段的重计算需求。这一阶段通常是长提示推理流程中计算开销最大的环节。这种技术在具有重复系统消息、用户模板或静态少量示例的系统中特别有效。

#### 动机

在自回归生成过程中，Transformer 模型会对提示（或上下文）进行一次处理以初始化注意力缓存。对于长度为 n 的提示，这需要完整地通过所有 Transformer 层进行一次前向传播来计算 KV 张量：

`K(l)1:n,V(l)1:n  for all layers l`

这个预填充步骤成本高昂，且必须为每个新请求重复执行——即使提示内容相同。

观察：许多应用程序反复使用相同或高度相似的提示语。例如：

- 指令提示，如：“你是一个乐于助人的助手。”
- 客户支持机器人中的少量示例模板。
- 聊天 API 中的系统提示。

提示缓存可避免对这些常见上下文进行重复预填充。

#### 基本机制

1. 缓存初始化：计算并存储给定提示的 KV 张量，`KVprompt=f(prompt)`，使用唯一键（例如 token ID的哈希值）存储在内存或磁盘中。
2. 缓存查找：对于每个传入的请求，从其提示中计算一个缓存键。如果找到匹配项，则检索 KV 张量而非重新计算它们。
3. 继续解码：开始使用缓存的 KV 状态逐个 token 生成：`Generate(xn+1∣KVprompt)`

#### 实现细节

##### 缓存粒度

* 完整提示缓存：缓存整个提示的 KV 缓存。简单有效，但可能占用大量内存。
* 前缀共享：如果提示仅后缀不同（例如，提示 `Prompt A + User 1` 和 `Prompt A + User 2`），则共享 KV 前缀并仅计算差异部分。
* 子图缓存：在更高级的系统中，可能仅缓存前几层或部分 token。

##### 缓存存储

- 内存键值缓存（In-Memory KV Cache）：为获得最佳性能，建议使用 GPU 或 CPU 内存并采用 LRU 淘汰策略。
- 磁盘缓存（On-Disk Cache）：速度较慢，但适合冷启动场景，具备扩展性。
- 哈希键生成（Keyed by Hash）：对分词后的输入使用 SHA 或 CRC 算法生成缓存键。部分系统会在哈希前对提示词进行规范化处理。

##### 与服务系统集成

* 需要支持缓存的批量调度。
* 与动态批处理和 token 级调度器（如 vLLM）集成时效果最佳。
* 可能包含缓存预热：在系统启动时预加载常见提示。

#### 性能影响

设 Tp = 预填充提示的时间，Td = 解码每个标记的时间，对于长提示（例如，1000+ 标记），Tp≫Td，因此缓存预填充可以为重复提示节省每请求计算的 80–95%。

#### 应用

- 聊天 API：系统消息或少量示例在对话轮次间保持不变。
- 代理框架：像 LangChain 这样的工具通常会重复使用相同的模板结构。
- 批量推理：多用户提示通常共享相同的上下文标题（例如“总结以下内容…”）。

#### 局限

- 提示缓存仅对完全相同或前缀匹配的提示有效。
- 内存使用量随提示长度和缓存大小而变化。
- 可能会增加哈希计算或未命中率处理的开销。
- 对完全动态、独特的用户输入没有帮助。

### 提前退出与 token 剪枝

提前退出和 token 剪枝是推理时的优化技术，旨在通过选择性跳过或修剪计算图中对最终输出贡献逐渐减小的部分，来减少大型 Transformer 模型的计算量。这些方法利用了 token 表示中的冗余性和 Transformer 模型中逐层的稳定性。

这两种技术的目标都是在不显著影响模型输出质量的情况下加速推理过程，因此在延迟敏感或资源受限的应用中具有重要价值。

#### 提前退出

提前退出机制允许模型在中间层停止处理某些标记甚至整个序列，前提是模型对预测结果的置信度已经很高。

##### 动机

Transformer 模型使用固定数量的层（例如24或96层），但并非所有标记都需要完整的深度才能做出可靠的预测。例如，容易分类的标记（如标点符号或常见停用词）可能比罕见或模糊的标记更早收敛。

##### 机制

在每一层 transformer l 中，基于当前 token 表示评估置信度指标：

1. 基于熵的置信度，计算当前逻辑值的 softmax 输出 $p(l)$。计算熵，$H(p(l))=−∑_ip(l)_i \log p(l)_i$，如果熵值小于阈值，则认为预测足够可信，可以退出。
2. 与上一层的余弦相似度：如果第 l 层的表示与第 l−1 层相似，则该标记可能已经收敛。
3. 学习门控 Gates：在训练过程中为每一层添加一个小型分类头以学习退出决策（类似于 BranchyNet 或 LayerDrop 方法）。

##### 实现

像带有早期退出的 BERT 模型（DEEPL）在多个深度实现了分类器头。Hugging Face 的 transformers在序列分类中对早期退出有原型支持。需要调整阈值以平衡准确性和延迟。

##### 收益

减少平均推理深度（例如，对许多标记而言，从 24 层减少到 12-16 层）。为较简单或高置信度的示例节省计算量。非常适合不需要逐标记预测的分类或问答任务。

##### 局限性
在中间层增加了置信度计算的开销。由于标记之间的顺序依赖性，在生成任务中未被广泛采用。

#### Token 剪枝

Token 剪枝通过识别并移除上下文重要性较低的 token，减少了在 Transformer 深层网络中传播的 token 数量。

##### 动机

在许多基于注意力的计算中，某些标记对输出的贡献微乎其微。例如，填充标记或对序列其余部分注意力权重较低的标记。修剪这些标记可以节省后续层的计算量，特别是在长上下文模型或批量场景中。

##### 机制

1. 基于注意力的剪枝：计算一个标记所获得的注意力分数方差或总注意力质量。`αi=∑jAttention(xi,xj)`：修剪接收或给予总注意力较低的 tokens。
2. Top-k token 选择：仅根据学习到的重要性分数，保留每个头或每个序列中最重要的前 k 个标记。
3. 动态阈值：使用学习或基于规则的阈值来丢弃影响低于可调截止点的标记。
4. 渐进式剪枝：从完整的标记开始，随着层级的深入，逐步加大剪枝力度。

##### 实现

通常在注意力模块边界处完成。可与稀疏注意力机制结合使用。需要跟踪标记索引以重建输出或映射回原始序列。

##### 收益

减少深层计算量，尤其针对长序列场景。在摘要、问答和检索任务中，以最小质量影响提升吞吐量。可在训练期间应用，以便与推理保持一致。

##### 局限

如果剪枝过于激进或校准不当，可能会降低质量。需要复杂的索引跟踪和掩码逻辑。在自回归设置中更难应用，因为所有标记都是顺序依赖的。

#### 工具与研究

* DeLighT、LayerDrop 和 EarlyBERT 用于早期退出变体。
* SparseFormer、Synthesizer 和 Longformer 引入了相关的 token 减少思想。
* Hugging Face 和 NVIDIA 的 Megatron 在研究分支中支持 token 修剪钩子。

### 硬件感知调度

硬件感知调度是指一系列优化策略，这些策略根据底层硬件（如 GPU、TPU 或专用加速器）的特定架构和性能特征来定制神经网络推理的执行过程。这些优化通过协调操作的执行方式和时机，旨在提高计算吞吐量、内存利用率和降低延迟。

这对于 Transformer 推理尤为重要，因为其工作负载量大、异构性强（例如 KV 缓存查找、矩阵乘法、归一化），并且对内存带宽和并行性非常敏感。

#### 动机

Transformer 推理涉及多个计算和内存访问阶段：

- 注意力机制和前馈网络块中的矩阵乘法（GEMM）。
- 层间和设备间的数据移动。
- KV 缓存的管理与大小调整。
- Softmax、激活函数和归一化操作。

如果不仔细安排调度，可能会出现以下瓶颈：

- 未充分利用的计算单元（例如张量核心）。
- 内存停顿和缓存抖动。
- 层间或流之间的同步开销。

硬件感知调度优化这些执行流程，以保持流水线满载并降低延迟。

#### 核心技术
##### 流式并行

现代 GPU 支持多个并发执行流（例如通过 CUDA）。在 Transformer 推理中：

* 为不同的模型阶段使用独立的 CUDA 流（例如，一个用于 KV 缓存更新，一个用于 GEMM）。
* 重叠内存拷贝（例如 `cudaMemcpyAsync`）与计算以隐藏延迟。

示例：

```python
cudaMemcpyAsync(..., stream1);  
cublasGemmEx(..., stream2);  // runs concurrently with stream1
```


##### Tensor Core 利用率

Tensor Cores 是 NVIDIA GPU 中专用于低精度矩阵运算（如 `float16`、`bfloat16`、`int8`）的特殊单元。要最大化其利用率：

- 确保所有矩阵乘法运算对齐到 8 的倍数维度。
- 使用融合内核消除中间 `float32` 转换。
- 优先采用混合精度流水线（AMP/`float16`）以提高吞吐量。

像 cuBLAS、FlashAttention 和 TensorRT 这样的库在正确配置时会自动处理这些优化。

##### 算子共置与重新排序

高效的推理调度可能涉及基于以下因素重新排序或共置操作：

* 内存局部性：将共享数据的操作进行融合或分组。
* 执行时间：在流水线中优先处理耗时较长的操作。
* 设备亲和性：将频繁访问的数据保留在同一 GPU 或芯片上。

示例：如果注意力块在多层级 transformer 中占据主要计算时间，则应优先运行它们，以便同时预取前馈神经网络。

##### KV 缓存管理

高效的键值缓存处理在解码器模型中至关重要：

- 分页键值缓存：用于 vLLM 等系统，将键值存储在连续的内存页中，并允许随机访问更新。
- 内存池：为每个请求预分配键值缓冲区并重复使用，以避免内存碎片。
- 延迟分配：推迟缓存实例化，直到首次生成步骤，从而为短提示节省内存。

##### 管道与模型并行

在大模型部署中：

* 流水线并行：将 Transformer 层分布到不同设备上。阶段执行重叠计算与通信。
* 张量并行：将单个张量维度（如权重）拆分到不同设备上，以处理大规模 GEMM 运算。

综合这些技术，可以高效地在多个 GPU 上运行具有数十亿参数的模型。

##### 自定义内核调度

像 Triton 和 TVM 这样的框架允许定义和调优自定义内核：

- 自动调整分块大小和共享内存使用量。
- 基于线程束/块级并行性调度 GPU 线程。
- 实现自定义的按词元或按层的调度逻辑。

##### 缓存与内存预取

在需要数据之前，使用 `__prefetch` 指令或异步加载将数据预取到共享内存中，将 KV 获取与矩阵乘法执行重叠，以隐藏内存延迟。

#### 部署感知策略

* 负载均衡：使用支持 GPU 感知请求路由的动态批处理队列（例如基于延迟或内存压力）。
* 线程亲和性：在 CPU 密集型系统中将计算绑定到特定的 CPU 核心或 NUMA 区域。
* 执行性能分析：使用 NVIDIA Nsight Systems 或 PyTorch Profiler 等分析工具来优化性能瓶颈。

#### 生态系统支持

* NVIDIA TensorRT 和 FasterTransformer：硬件感知的融合内核和调度策略。
* ONNX Runtime (ORT)：针对不同硬件（CUDA、DirectML、TensorRT）优化的执行提供程序。
* DeepSpeed、vLLM、Triton 和 TVM：提供对调度和内存布局的细粒度控制。

#### 性能影响

硬件感知调度可以实现：

- 对于长序列或大批量任务，相比简单调度实现了 1.5 倍至 4 倍的加速。
- 在高吞吐量推理场景下，具备更优的多 GPU 扩展能力。
- 在实时服务环境中，显著降低了延迟波动性。

### 对比分析

| 技术                | 目的                   | 主要优势                 | 主要用例                   | 具体实现                                                         |
| ----------------- | -------------------- | -------------------- | ---------------------- | ------------------------------------------------------------ |
| KV 缓存             | 重用之前 tokens 的注意力 K/V | 在第一步之后减少每个 token 的延迟 | 自回归解码(GPT, LLaMA)      | 需要谨慎管理缓存；从第二个 token 开始                                       |
| 模型量化              | 使用低精度权重/激活           | 降低内存和计算成本            | 边缘推理，高吞吐量服务            | `int8`/PTQ 用于提速；QAT 用于提高精度；需要硬件支持量化                          |
| 算子融合              | 将相邻操作合并为单个内核算子       | 减少内存访问和内核启动开销        | 注意力块、前馈网络、层归一化 + 激活    | 使用图编译器（XLA、TorchScript）或融合CUDA内核（TensorRT、FasterTransformer） |
| 推测解码              | 使用草稿模型猜测多个 tokens    | 减少全模型前向传递次数          | 长文本生成，聊天机器人            | 需要一个轻量级的辅助模型；使用 top-1 匹配或对数概率阈值进行验证                          |
| FlashAttention与内核 | 高效内存注意力计算            | 减少内存使用并提高速度          | 长序列 LLM，多头注意力          | 采用 CUDA（FlashAttention）、Triton 或 xFormers 实现；避免存储完整的注意力矩阵    |
| Batching          | 批量处理多个请求             | 提高吞吐量和 GPU 利用率       | 高并发推理（API 服务器、批量作业）    | vLLM、DeepSpeed 和 TensorRT 支持动态和 token 级批处理                   |
| 预填充               | 预计算提示 token 的 KV 缓存  | 避免在自回归模型中重复计算        | 长提示的聊天和生成任务            | 通常与批处理搭配使用；在解码开始前预先初始化提示 KV 缓存                               |
| 提示缓存              | 重复提示的缓存 KV 状态        | 节省重复静态上下文的时间和计算资源    | 聊天 API，少量示例提示模板        | 需要对提示进行哈希/标记化处理并存储缓存；内存使用量随缓存多样性增加而增加                        |
| 提前退出              | 根据置信度提前停止处理 token/层  | 减少深度模型中每个 token 的计算量 | 分类、问答任务                | 需要熵或学习到的门控逻辑；难以应用于依赖标记的生成                                    |
| Token 剪枝          | 在推理过程中丢弃低重要性的标记      | 在更深层次减少序列长度          | 长序列摘要、问答               | 基于注意力的重要性评分；需要仔细掩码和索引跟踪                                      |
| 硬件感知调度            | 优化特定硬件的内核执行          | 最大化吞吐量并最小化延迟         | 所有基于 Transformer 的工作负载 | 包括流并行处理、内存预取、缓存布局、张量核心调优和多 GPU 分配                            |
