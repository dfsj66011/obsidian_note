

[source](https://aman.ai/primers/ai/speculative-decoding/)

## ä¸€ã€èƒŒæ™¯

ä¸åŒäºæ¯æ¬¡ä»…é¢„æµ‹ä¸€ä¸ªæ ‡è®°ï¼Œè¿™äº›æ–¹æ³•å°è¯•å¹¶è¡ŒçŒœæµ‹å¤šä¸ªæœªæ¥æ ‡è®°å¹¶è¿›è¡Œé«˜æ•ˆéªŒè¯ã€‚è¿™ä¸€é€šç”¨ç†å¿µå¯é€šè¿‡å¤šç§æ–¹å¼å®ç°ï¼Œä»¥ä¸‹åˆ—ä¸¾ä¸‰ç§æœ€å¸¸è§çš„æ–¹æ³•ï¼š

1. é€šè¿‡è‰ç¨¿æ¨¡å‹è¿›è¡Œæ¨æµ‹æ€§è§£ç ï¼šä¸€ä¸ªæ›´å°ã€æ›´å¿«çš„â€œè‰ç¨¿æ¨¡å‹â€åœ¨ä¸»æ¨¡å‹ä¹‹å‰ç”Ÿæˆä¸€ç³»åˆ—å€™é€‰æ ‡è®°ã€‚ç„¶åï¼Œå®Œæ•´æ¨¡å‹å¹¶è¡ŒéªŒè¯è¿™äº›æ ‡è®°ï¼Œæ¥å—æ­£ç¡®çš„é¢„æµ‹ï¼Œä»…åœ¨æ£€æµ‹åˆ°ä¸åŒ¹é…æ—¶å›é€€åˆ°æ ‡å‡†çš„è‡ªå›å½’è§£ç ã€‚è¿™ç§æ–¹æ³•åœ¨å®è·µä¸­å¯ä»¥å®ç° 2 å€è‡³ 6 å€çš„åŠ é€Ÿã€‚

2. åŸºäºæ ‘çš„å¤šå¤´éªŒè¯ï¼ˆMedusaï¼‰ï¼šä¸åŒäºä½¿ç”¨å•ç‹¬çš„è‰ç¨¿æ¨¡å‹ï¼Œä¸»æ¨¡å‹é…å¤‡äº†å¤šä¸ªå¹¶è¡Œçš„â€œéªŒè¯å¤´â€ï¼Œä»¥æ ‘çŠ¶ç»“æ„ç”Ÿæˆå’Œæ£€æŸ¥æ›¿ä»£ä»¤ç‰Œè·¯å¾„ã€‚è¿™ä½¿å¾—å¯ä»¥åŒæ—¶æ¢ç´¢å¤šä¸ªå€™é€‰å»¶ç»­è·¯å¾„ï¼Œä»è€Œå‡å°‘æ‰€éœ€çš„å…¨å‰å‘ä¼ æ’­æ¬¡æ•°ã€‚

3. å¤šä»¤ç‰Œé¢„æµ‹å¤´ï¼ˆè‡ªæ¨æµ‹è§£ç ï¼‰ï¼šæ¨¡å‹é€šè¿‡é¢å¤–å¢åŠ çš„è¾“å‡ºå¤´è¿›è¡Œå¢å¼ºï¼Œè¿™äº›è¾“å‡ºå¤´ç»è¿‡è®­ç»ƒå¯ç›´æ¥ä»å½“å‰éšè—çŠ¶æ€é¢„æµ‹å¤šä¸ªæœªæ¥ä»¤ç‰Œã€‚è¿™äº›é¢„æµ‹åœ¨å†…éƒ¨è¿›è¡ŒéªŒè¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç»•è¿‡å¤šä¸ªé¡ºåºæ­¥éª¤ï¼Œè€Œæ— éœ€å¼•å…¥å¤–éƒ¨æ¨¡å‹æˆ–åˆ†æ”¯æœç´¢ç»“æ„ã€‚

åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæ ¸å¿ƒåŸåˆ™å§‹ç»ˆå¦‚ä¸€ï¼šåœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶æ‰§è¡Œæ›´å¤šæ¨æµ‹æ€§å·¥ä½œï¼Œç„¶åå¹¶è¡ŒéªŒè¯æ­£ç¡®æ€§ï¼Œä»è€Œç¼“è§£é€ä¸ªç”Ÿæˆä¸‹ä¸€ä¸ªæ ‡è®°çš„ä¸¥æ ¼ä¸²è¡Œç‰¹æ€§ã€‚

**ç¤ºä¾‹åœºæ™¯ï¼š**

å‡è®¾æˆ‘ä»¬è¦ç”Ÿæˆ 5 ä¸ªæ ‡è®°ã€‚ä¼ ç»Ÿè§£ç æ–¹æ³•éœ€è¦è¿›è¡Œ 5 æ¬¡å‰å‘ä¼ æ’­ã€‚è€Œä½¿ç”¨æ¨æµ‹æ€§è§£ç æ—¶ï¼Œè‰ç¨¿æ¨¡å‹å¯èƒ½ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰ 5 ä¸ªæ ‡è®°ã€‚ç„¶åç›®æ ‡æ¨¡å‹ä¼šå¯¹è¿™æ‰¹æ ‡è®°è¿›è¡ŒéªŒè¯â€”â€”æ ¹æ®éœ€è¦æ¥å—æˆ–ä¿®æ­£ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ 4 ä¸ªæ ‡è®°è¢«æ¥å—ï¼Œåˆ™ä»…éœ€ 2 æ¬¡å‰å‘ä¼ æ’­ï¼Œä»è€Œå®ç° 2.5 å€çš„åŠ é€Ÿã€‚

è¿™ä¸€ä¼˜åŒ–ç‰¹åˆ«é€‚ç”¨äºï¼š

* å®æ—¶åº”ç”¨ï¼ˆèŠå¤©æœºå™¨äººã€ä»£ç è¡¥å…¨ï¼‰
* è¾¹ç¼˜éƒ¨ç½²åœºæ™¯
* é«˜è´Ÿè½½æœåŠ¡å™¨ç¯å¢ƒ

ä¸‹å›¾ï¼ˆæ¥æºï¼‰ç›´è§‚å±•ç¤ºäº†éæ¨æµ‹æ€§ç”Ÿæˆï¼ˆå·¦ï¼‰ä¸æ¨æµ‹æ€§ç”Ÿæˆï¼ˆå³ï¼‰çš„å¯¹æ¯”ã€‚

## äºŒã€æ ¸å¿ƒæŠ€æœ¯

æ¨æµ‹è§£ç æœ‰å¤šç§å½¢å¼ï¼Œä½†æ ¸å¿ƒç†å¿µæ˜¯ä¸€è‡´çš„ï¼šä½¿ç”¨è½»é‡çº§æ–¹æ³•çŒœæµ‹å¤šä¸ªæ ‡è®°ï¼Œç„¶åé€šè¿‡åŸå§‹ï¼ˆæˆ–â€œç›®æ ‡â€ï¼‰æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢è®¨ä¸»è¦ç­–ç•¥ã€å®ç°æ¨¡å¼åŠå…¶æƒè¡¡ã€‚

### 2.1 é€šè¿‡è‰ç¨¿æ¨¡å‹è¿›è¡Œæ¨æµ‹è§£ç 

ç”±Leviathanç­‰äººåœ¨2023å¹´å‘è¡¨çš„ã€Šé€šè¿‡æ¨æµ‹è§£ç å®ç°Transformerå¿«é€Ÿæ¨ç†ã€‹ä¸­æå‡ºã€‚

Pipeline æ¦‚è¿°ï¼š

1. **èµ·è‰**ï¼šä½¿ç”¨ä¸€ä¸ªæ›´å°ï¼ˆæ›´å¿«ï¼‰çš„æ¨¡å‹æ¥ç”Ÿæˆ $Î³$ æ¨æµ‹æ€§æ ‡è®°ã€‚
2. **éªŒè¯**ï¼šè¿è¡Œå¤§æ¨¡å‹å¯¹æ‰€æœ‰æ ‡è®°è¿›è¡Œè¯„åˆ†ï¼Œç›´è‡³ $Î³$ã€‚
3. **æ¥å—åº¦ï¼š** æ¥å—ä¸å¤§æ¨¡å‹é¢„æµ‹ç»“æœç›¸åŒ¹é…çš„å‰ç¼€æ ‡è®°ã€‚
4. **åå¤‡æ–¹æ¡ˆ**ï¼šè‹¥ä»¤ç‰Œå‡ºç°åå·®ï¼Œåˆ™å›é€€è‡³å¤§æ¨¡å‹é‡‡æ ·è¿›è¡Œä¿®æ­£ã€‚

è®ºæ–‡ä¸­çš„ä¸‹å›¾å±•ç¤ºäº†æ— æ¡ä»¶è¯­è¨€å»ºæ¨¡æ¡ˆä¾‹ä¸­è¯´æ˜çš„ä¸€ç§æŠ€æœ¯ã€‚æ¯æ¡çº¿ä»£è¡¨ç®—æ³•çš„ä¸€æ¬¡è¿­ä»£ã€‚ç»¿è‰²æ ‡è®°æ˜¯è¿‘ä¼¼æ¨¡å‹ï¼ˆæ­¤å¤„ä¸ºä¸€ä¸ªåœ¨1m1bæ•°æ®é›†ä¸Šè®­ç»ƒã€å…·æœ‰600ä¸‡å‚æ•°ã€å¤„ç†8kæ ‡è®°çš„ç±»GPT Transformerè§£ç å™¨ï¼‰æå‡ºçš„å»ºè®®ï¼Œè¢«ç›®æ ‡æ¨¡å‹ï¼ˆæ­¤å¤„ä¸ºç›¸åŒè®¾ç½®ä¸‹å…·æœ‰9700ä¸‡å‚æ•°çš„ç±»GPT Transformerè§£ç å™¨ï¼‰æ¥å—ï¼›è€Œçº¢è‰²å’Œè“è‰²æ ‡è®°åˆ†åˆ«æ˜¯è¢«æ‹’ç»çš„å»ºè®®åŠå…¶ä¿®æ­£ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€è¡Œä¸­ç›®æ ‡æ¨¡å‹ä»…è¿è¡Œä¸€æ¬¡ï¼Œç”Ÿæˆäº†5ä¸ªæ ‡è®°ã€‚

ç®—æ³•æ¦‚è¦ï¼ˆæ ¹æ® Leviathan ç­‰äººç®€åŒ–ï¼‰ï¼š

```python
def speculative_decode(draft_model, target_model, prompt, gamma):
      draft_tokens = draft_model.generate(prompt, max_new_tokens=gamma)
      scores = target_model.score(prompt + draft_tokens)
        
      # accept up to the first mismatch
      n_accept = count_agreement(draft_tokens, scores)
      accepted = draft_tokens[:n_accept]
        
      # complete the next token from the target model
      next_token = target_model.sample(prompt + accepted)
      return accepted + [next_token]
```

**ä¼˜ç‚¹ï¼š**:

- æ— éœ€é‡æ–°è®­ç»ƒå³å¯æ’å…¥ç°æœ‰æ¨¡å‹ã€‚
- æ— éœ€å¯¹å¤§æ¨¡å‹è¿›è¡Œæ¶æ„æ›´æ”¹ã€‚
- å®Œå…¨ä¿ç•™è¾“å‡ºåˆ†å¸ƒã€‚

**æŒ‘æˆ˜**:

- ç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„è‰ç¨¿æ¨¡å‹ä¼šå¢åŠ ç³»ç»Ÿå¤æ‚æ€§ã€‚
- è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…å¯èƒ½ä¼šé™ä½æ¥å—ç‡ã€‚
- å¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½å¾ˆå¤§ï¼Œä¼šå¸¦æ¥å†…å­˜å’Œè®¡ç®—å‹åŠ›ã€‚

### 2.2 åŸºäºæ ‘çš„å¤šå¤´éªŒè¯ï¼ˆMedusaï¼‰

ç¾æœèï¼ˆMedusaï¼‰ç”±Caiç­‰äººï¼ˆ2024å¹´ï¼‰åœ¨è®ºæ–‡ã€Šç¾æœèï¼šåŸºäºå¤šé‡è§£ç å¤´çš„ç®€æ˜“LLMæ¨ç†åŠ é€Ÿæ¡†æ¶ã€‹ä¸­æå‡ºï¼Œè¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨æ ‘çŠ¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹åŸºç¡€çš„å¤šå¤´æ¨æµ‹è§£ç æŠ€æœ¯è¿›è¡Œäº†æ”¹è¿›ã€‚

åšå®¢ï¼ˆæ¥æºï¼‰ä¸­çš„ä¸‹å›¾å±•ç¤ºäº†Medusaåœ¨Vicuna-7bä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**

* ç¾æœèå¤´ï¼šæ¯ä¸ªå¤´ä»æœ€åä¸€ä¸ªéšè—çŠ¶æ€é¢„æµ‹æœªæ¥ $k+1$ ä¸ªæ ‡è®°ã€‚
* å€™é€‰ç»„åˆï¼šå°†æ¯ä¸ªå¤´éƒ¨çš„ top-k è¾“å‡ºç»„åˆèµ·æ¥ï¼Œå½¢æˆæ¨æµ‹æ ‘ã€‚
* æ ‘æ³¨æ„åŠ›ï¼šä¸€ç§è‡ªå®šä¹‰çš„æ³¨æ„åŠ›æ©ç ç¡®ä¿æ ‡è®°ä»…å…³æ³¨å…¶è·¯å¾„ä¸Šçš„å‰é©±èŠ‚ç‚¹ã€‚
* æ¥çº³æ–¹æ¡ˆï¼šä¸¤ç§é€‰æ‹©ï¼š
	* æ‹’ç»é‡‡æ ·ï¼ˆåŒ¹é…åŸºç¡€æ¨¡å‹ï¼‰
	* å…¸å‹éªŒæ”¶ï¼ˆå¯å‘å¼ï¼Œæ›´å¿«ï¼‰

**å¥½å¤„ï¼š**

- åœ¨è´¨é‡ä¸‹é™æœ€å°çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„åŠ é€Ÿï¼ˆç”Ÿäº§ç¯å¢ƒä¸­çº¦ 2.3-2.8 å€ï¼‰ã€‚
- æ— éœ€é‡æ–°è®­ç»ƒï¼ˆMedusa-1ï¼‰æˆ–é€šè¿‡è”åˆè®­ç»ƒï¼ˆMedusa-2ï¼‰å³å¯è½»æ¾é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ã€‚
- é€‚ç”¨äºæ‰¹é‡å¤§å°ä¸º 1ï¼Œè¿™ä¸å®é™…ä½¿ç”¨åœºæ™¯ï¼ˆå¦‚èŠå¤©ï¼‰ç›¸ç¬¦ã€‚

**å®ç°ç»†èŠ‚ï¼š**

pkt=softmax(W2kâˆ—(SiLU(W1kâˆ—ht)+ht))

å…¶ä¸­ W1k åˆå§‹åŒ–ä¸º 0ï¼ŒW2k æ˜¯åŸºç¡€æ¨¡å‹è¯­è¨€æ¨¡å‹å¤´éƒ¨çš„å…‹éš†å‰¯æœ¬ã€‚

### 2.3 å¤šä»¤ç‰Œé¢„æµ‹å¤´ï¼ˆè‡ªæ¨æµ‹è§£ç ï¼‰

ç”±Gloeckleç­‰äººï¼ˆ2024å¹´ï¼‰åœ¨ã€Šé€šè¿‡å¤šä»¤ç‰Œé¢„æµ‹å®ç°æ›´å¥½æ›´å¿«çš„è¯­è¨€æ¨¡å‹ã€‹ä¸­æå‡ºã€‚

æœ€è¿‘çš„ä¸€ä¸ªè¶‹åŠ¿æ˜¯ä¸ä½¿ç”¨å•ç‹¬çš„è‰ç¨¿æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥åœ¨ä¸»æ¨¡å‹ä¸­æ„å»ºæ¨æµ‹èƒ½åŠ›ã€‚è¿™å°±æ˜¯å¤šä»¤ç‰Œé¢„æµ‹å¤´ï¼ˆmulti-token prediction headsï¼‰çš„ç”¨æ­¦ä¹‹åœ°ã€‚

è®ºæ–‡ä¸­çš„ä¸‹å›¾å±•ç¤ºäº†å¤šä»¤ç‰Œé¢„æµ‹çš„æ¦‚è§ˆã€‚ï¼ˆä¸Šï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡å…±äº«ä¸»å¹²å’Œ4ä¸ªä¸“ç”¨è¾“å‡ºå¤´ï¼Œä¸€æ¬¡æ€§é¢„æµ‹4ä¸ªæœªæ¥ä»¤ç‰Œã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„è¾“å‡ºå¤´ã€‚å¯é€‰åœ°ï¼Œå…¶ä»–ä¸‰ä¸ªå¤´å¯ç”¨äºåŠ é€Ÿæ¨ç†æ—¶é—´ã€‚ï¼ˆä¸‹ï¼‰å¤šä»¤ç‰Œé¢„æµ‹æé«˜äº†MBPPä»£ç ä»»åŠ¡ä¸­çš„pass@1æ€§èƒ½ï¼Œä¸”éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§æ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚è¯¯å·®æ¡è¡¨ç¤ºé€šè¿‡æ•°æ®é›†æ ·æœ¬çš„bootstrapè®¡ç®—å¾—å‡ºçš„90%ç½®ä¿¡åŒºé—´ã€‚

**æ¶æ„**

- å…±äº«çš„å˜å‹å™¨ä¸»å¹²å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç ã€‚
- å¤šä¸ªè§£ç å™¨å¤´ï¼ˆæ¯ä¸ªå¯¹åº”ä¸€ä¸ªæœªæ¥æ ‡è®°ï¼‰è¿›è¡Œç‹¬ç«‹é¢„æµ‹ã€‚
- ç¬¬ä¸€ä¸ªå¤´éƒ¨æ˜¯æ ‡å‡†çš„ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹å™¨ï¼›å…¶ä»–å¤´éƒ¨é¢„æµ‹ç¬¬äºŒã€ç¬¬ä¸‰â€¦â€¦ç¬¬nä¸ªä»¤ç‰Œã€‚

æ¯ä¸ªå¤´éƒ¨éƒ½é€šè¿‡äº¤å‰ç†µæŸå¤±åœ¨å…¶å„è‡ªçš„ä½ç½®ä¸Šè¿›è¡Œè®­ç»ƒï¼šLn=âˆ’Î£tÎ£ni=1logP(xt+i|z1:t)

å…¶ä¸­ğ‘§1:ğ‘¡æ˜¯å…±äº«çš„æ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªğ‘ƒ(ğ‘¥ğ‘¡+ğ‘–)é€šè¿‡å…¶ä¸“ç”¨å¤´è®¡ç®—å¾—å‡º

**å†…å­˜ä¼˜åŒ–**

è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ˜¯ä¸ºæ‰€æœ‰ğ‘›ä¸ªå¤´ç”Ÿæˆæ‰€æœ‰logitsï¼Œè€Œæ˜¯é¡ºåºå¤„ç†æ¯ä¸ªå¤´ä»¥å‡å°‘GPUå†…å­˜å ç”¨ï¼š

* è®¡ç®—å‰å‘å’Œåå‘ä¼ æ’­ï¼ˆå¤´éƒ¨1ï¼‰
* é‡Šæ”¾å¯¹æ•°æ¦‚ç‡ï¼Œç§»è‡³å¤´éƒ¨2
* åœ¨å…±äº«ä¸»å¹²ä¸Šç´¯ç§¯æ¢¯åº¦

è¿™å°†å³°å€¼å†…å­˜ä»ğ‘‚(ğ‘›ğ‘‰+ğ‘‘)é™ä½åˆ°ğ‘‚(ğ‘‰+ğ‘‘)ï¼Œä¸”ä¸ä¼šå½±å“é€Ÿåº¦ã€‚

**ä¼˜åŠ¿**:

* æ— éœ€å•ç‹¬çš„è‰ç¨¿æ¨¡å‹ã€‚
* ç»Ÿä¸€æ¶æ„ï¼ˆæ›´æ˜“äºéƒ¨ç½²ã€é‡åŒ–å’Œè®­ç»ƒï¼‰ã€‚
* å…¼å®¹æ¨æµ‹è§£ç æ–¹æ³•ï¼Œå¦‚å—çº§å¹¶è¡Œæˆ–Medusaã€‚

**ç¼ºç‚¹**ï¼š

* éœ€è¦åœ¨é¢„è®­ç»ƒæœŸé—´ä¿®æ”¹æ¨¡å‹
* æ”¶ç›Šä»…åœ¨å¤§è§„æ¨¡æ¨¡å‹ï¼ˆ70äº¿ä»¥ä¸Šï¼‰ä¸­æ˜¾ç°
* å¾®è°ƒè¿™äº›æ¨¡å‹å¯èƒ½éœ€è¦å°å¿ƒæ“ä½œä»¥ä¿æŒå¯¹é½æ€§ã€‚

## ä¸‰ã€å¯¹æ¯”åˆ†æ

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒå‰æ–‡è®¨è®ºçš„å…³é”®æ¨æµ‹å¼è§£ç ç­–ç•¥â€”â€”åŸºäºè‰ç¨¿æ¨¡å‹çš„è§£ç ã€å¤šä»¤ç‰Œé¢„æµ‹å¤´ä»¥åŠMedusaæ–¹æ³•ã€‚æˆ‘ä»¬å°†ä»æ€§èƒ½è¡¨ç°ã€é›†æˆä¾¿æ·æ€§ã€è®­ç»ƒéœ€æ±‚ä»¥åŠéƒ¨ç½²å¤æ‚åº¦ç­‰ç»´åº¦æƒè¡¡è¿™äº›ç­–ç•¥çš„ä¼˜åŠ£ã€‚

|**Criteria**|**Draft Model  <br>(Leviathan et al., Nov 2022)**|**Medusa Treeâ€‘Attention  <br>(Cai et al., Jan 2024)**|**Multiâ€‘Token Prediction Heads  <br>(Gloeckle et al., Apr 2024)**|
|---|---|---|---|
|Model changes required|None|Optional (Medusaâ€‘1) / joint (Medusaâ€‘2)|Yes (requires modifying output heads during pre-training)|
|Training cost|Low (can use off-the-shelf models as draft and target models)|Moderate (fineâ€‘tune extra heads)|High (requires pre-training)|
|Inference speedup (observed)|âˆ¼2Ã—â€“3Ã—|âˆ¼2.2Ã—â€“3.6Ã—Â (typicallyÂ 2.3Ã—â€“2.8Ã—)|âˆ¼3Ã—Â (4â€‘token), up toÂ âˆ¼6Ã—Â (8â€‘token draft window)|
|Output quality|Identical to base model|High (rejection + typical acceptance schemes)|Matches nextâ€‘token head|
|Deployment ease|Moderate (dualâ€‘model system)|High (single model with extra heads)|High (single model if integrated from pretraining)|
|Memory overhead (training)|High (two model states / KVâ€‘cache)|Low (single trunk + small head layers)|Efficient (O(V+d)Â peak memory)|
|Batchâ€‘size friendliness|High|Optimized for batch size = 1|High|
|Implementation maturity|Widely used since 2022 (T5, GPT)|Early adoption in LLMs like Vicuna, Zephyr|[DeepSeek V3](https://arxiv.org/html/2412.19437v1)|
### When to Use Each Technique

- **Draft Model (Leviathan-style speculative decoding)**:
    
    - Ideal when you canâ€™t modify or retrain the base model.
    - Suitable for legacy systems or commercial APIs.
    - Offers â€œplug-and-playâ€ inference acceleration with minimal integration overhead.
    - Best when a strong, compact draft model is already available.
- **Medusa (Cai et al., 2024)**:
    
    - Ideal for single-user interactive settings (e.g., chatbots).
    - Offers fine-grained control via Medusa-1 (frozen backbone) or Medusa-2 (joint fine-tuning).
    - IntroducesÂ **tree attention**Â to optimize speculative token verification.
    - Can outperform others when output diversity or control is key.
- **Multi-token Prediction Heads (Gloeckle et al., 2024)**:
    
    - Recommended during full model pretraining.
    - Best for institutions training models from scratch or at scale.
    - EnablesÂ **self-speculative decoding**Â with minimal architectural footprint.
    - Very efficient for longer inputs or batch decoding workloads.

### Implementation Details

- **Draft-based Implementation**:
    
    - Ensure the draft model isÂ **close enough**Â in distribution to the main model; divergence kills speedup.
    - Batch speculative runs and base model verifications.
    - Use caching (KV cache reuse) to reduce redundant computations.
- **Multi-token Heads Implementation**:
    
    - Train with n-token loss: each head predicts future token i.
    - Use gradient checkpointing or staggered backprop to control memory.
    - At inference, use blockwise or greedy speculative decoding.
- **Medusa Implementation**:
    
    - Add feedforward speculative heads:
        
        pkt=softmax(W2k@(SiLU(W1k@ht)+ht))
        
    - ForÂ **tree attention**, modify attention masks to ensure tokens only see ancestors.
    - UseÂ **typical acceptance**Â scheme to boost accepted token length without complex sampling.

### Empirical Results Snapshot

- FromÂ [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)Â by Gloeckle et al. (2024), using 4-token prediction on a 7B model:
    
    - 3Ã—Â faster inference.
    - +4% higher pass@1 on HumanEval (code generation).
    - Optimal token prediction window (n) varies: 4 is best for natural language, 8 for byte-level models.
- FromÂ [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)Â by Cai et al. (2024), Medusa on Vicuna-7B:
    
    - 2.3â€“2.8Ã—Â speedup.
    - Quality preserved via training schemes (especially Medusa-2).
    - Compatible with quantized backbones (QLoRA).

### Key Takeaways

- **Speed vs Simplicity**: Draft-based methods are simpler but less efficient long-term. Integrated heads unlock better scaling.
- **Training Budget Matters**: If youâ€™re training from scratch, invest in multi-token or Medusa heads.
- **Serving Constraints**: For distributed serving or edge deployment, Medusa-1 or next-token heads provide clean integration.

## Implementation Deep Dive: How to Build Speculative Decoders

- This section focuses on the nuts and bolts of implementing speculative decoding. We cover architecture layouts, essential training routines, memory-saving tricks, and reference code patterns for each of the three major approaches.

### Draft Model-Based Speculative Decoding

- This method involves using two models:
    
    - **Target model**: The large, accurate LLM whose output must be preserved.
    - **Draft model**: A smaller model trained to approximate the target modelâ€™s predictions.
- **Architecture Overview**:
    
    - The following figure from the paper shows the workflow of draft model-based speculative decoding: proposal, parallel verification, selective acceptance. In the case of unconditional language modeling, each line represents one iteration of the algorithm. The green tokens are the suggestions made by the approximation model (here, a GPT-like Transformer decoder with 6M parameters trained on lm1b with 8k tokens) that the target model (here, a GPT-like Transformer decoder with 97M parameters in the same setting) accepted, while the red and blue tokens are the rejected suggestions and their corrections, respectively. For example, in the first line the target model was run only once, and 5 tokens were generated.
    
    ![](https://aman.ai/images/papers/SD.jpg)
    
- Each decoding step proceeds as follows:
    
    1. Generate a speculative prefix ofÂ Î³Â tokens using the draft model (e.g.,Â Î³Â = 4).
    2. Run the target model in parallel to verify each token.
    3. Accept matching tokens; reject mismatches and resume standard decoding from there.
- **Key Implementation Elements**:
    
    - **Speculative Sampling**: Uses rejection sampling to ensure distributional equivalence:
        
        ![](https://aman.ai/images/copy.png)
        
        `def accept_token(p_large, p_draft, x):     if p_draft[x] <= p_large[x]:         return True     else:         accept_prob = p_large[x] / p_draft[x]         return random.random() < accept_prob`
        
    - **Parallel Verification**: RunÂ Î³Â + 1 parallel forward passes of the target model:
        
        ![](https://aman.ai/images/copy.png)
        
        `with torch.no_grad():     logits = target_model(prefix + draft_tokens)     verified_probs = softmax(logits)`
        
    - **Fallback Correction**: If a token is rejected, sample again from an adjusted distribution:
        
        ![](https://aman.ai/images/copy.png)
        
        `residual = torch.clamp(p_large - p_draft, min=0) residual /= residual.sum() next_token = torch.multinomial(residual, num_samples=1)`
        
- **Optimization Tip**: Cache activations across reused prefixes to avoid redundant computation.

### Medusa: Tree Attention + Parallel Heads

- Medusa extends multi-token decoding with a novel attention mechanism that verifies multiple speculative paths simultaneously.
    
- **Architecture Overview**:
    
    - The following figure from the paper shows the proposed tree attention in Medusa: parallel candidates from multiple heads form branches that are verified simultaneously.
    
    ![](https://aman.ai/primers/ai/assets/speculative-decoding/Medusa_treeattn.jpg)
    
    - Multiple lightweight Medusa heads project from the last hidden state.
    - Each head proposes tokens at future positions (t+1,Â t+2,Â â€¦,Â t+K).
    - Tree-structured attention masks control information flow to ensure correctness.
- **Medusa Head Definition**:
    
    ![](https://aman.ai/images/copy.png)
    
      `def medusa_head(h_t, W1_k, W2_k):       ff_out = F.silu(W1_k @ h_t) + h_t       return softmax(W2_k @ ff_out)`
    
- - W1kÂ is initialized as zero,Â W2kÂ cloned from LM head.
- **Tree Attention Implementation**:
    
    - Construct Cartesian product of top-k predictions from each head.
    - Use attention mask that only allows intra-branch communication.
    - Modify positional encodings for tree-based candidate verification.
- **Candidate Verification**:
    
    ![](https://aman.ai/images/copy.png)
    
      `# Assume 2 heads with top-2 and top-3 predictions   # Generate 6 branches, verify each in parallel   mask = build_tree_attention_mask(branch_structure)   attention_output = transformer_with_mask(input_ids, mask)`
    
- **Acceptance Strategy**:
    
    - **Rejection sampling**Â ensures fidelity.
    - **Typical acceptance**Â (heuristic cutoff on deviation from target) boosts speed.

### Multi-Token Prediction Heads

- This approach modifies the LLM architecture to predictÂ nÂ future tokens at once during training.
    
- **Architecture Overview**:
    
    - The following figure from the paper shows the implementation structure of multi-token prediction: one trunk, multiple future-predicting heads, and staged loss computation.
    
    ![](https://aman.ai/images/papers/MTP.jpg)
    
    - A shared transformer trunk generates a hidden state.
    - nÂ lightweight output heads decode tokensÂ t+1Â toÂ t+n.
- **Model Structure**:
    
    ![](https://aman.ai/images/copy.png)
    
      `# Trunk   z = transformer_trunk(x)    # Heads   logits = [head_i(z) for i in range(n)]   outputs = [softmax(logit) for logit in logits]`
    
    - Each head minimizes its own cross-entropy loss:
        
        ![](https://aman.ai/images/copy.png)
        
          `loss = sum([F.cross_entropy(logits[i], target[i]) for i in range(n)])`
        
- **Memory-Efficient Training**:
    
    - **Sequential gradient computation**Â for each head reduces memory:
        
        ![](https://aman.ai/images/copy.png)
        
        `for head in heads:     output = head(z)     loss = F.cross_entropy(output, target)     loss.backward(retain_graph=True)`
        
    
    **Inference Options**:
    
    - Use the next-token head for traditional generation.
    - Use the other heads to propose speculative sequences for greedy decoding (e.g., blockwise).

## Future Directions

- The field is still rapidly evolving. What began with speculative sampling is now branching into hybrid pipelines, adaptive acceptance, and tree-structured reasoning paths. With integration into quantized and edge-deployable models, speculative decoding is becoming not just an optimizationâ€”but a design paradigm for future LLM systems.
- The core techniques of speculative decoding have opened the door to a range of optimization opportunities for LLM inference. In this section, we explore emerging variants, hybrid models, and promising research directions that could further accelerate decoding while maintaining output fidelity.

### Hybrid Approaches: Combining Draft + Head

- Some systems now combineÂ **draft models**Â withÂ **multi-token or Medusa heads**Â to maximize acceptance rates and throughput.
    
- **Motivation**:
    
    - Use a draft model for a long speculative prefix.
    - Use Medusa or multi-token heads to verify batches of predictions instead of verifying token-by-token.
- **Example Pipeline**:
    
    1. Draft model proposesÂ Î³Â tokens.
    2. Medusa-style heads are used within the large model to validate candidate branches.
    3. Longest valid candidate is accepted.
- **Advantages**:
    
    - Combines high-quality approximation from draft with structural verification efficiency.
    - Supports deeper pipelines (e.g., hierarchical draft-check loops).
    - Naturally extensible to distributed and batched decoding.

### Integration with Quantization & Pruning

- Speculative decoding can synergize with model compression techniques:
    
    - **Quantized Models**Â (e.g., QLoRA, GPTQ):
        - Medusa heads can be trained/fine-tuned atop a frozen quantized model. Even the trunk used in multi-token prediction can be quantized (as in Medusa-1).
    - **Pruned Heads**:
        - Lightweight speculative heads use <0.1% of model parameters. This makes them ideal candidates for post-training head-specific pruning or low-rank approximations.
    - **Shared KV Caches**:
        - As seen in IBMâ€™s PyTorch implementation, speculative tokens and trunk outputs can reuse the same attention cache with minimal overhead by adapting the paged attention kernel.

### Speculative Decoding for Byte-Level Model

- Recent experiments show that speculative decoding isÂ **especially effective for byte-level tokenization**Â models.
    
- **Why?**
    
    - Byte-level tokenizers (e.g., Tiktoken with vocab size 256) produce longer sequences for the same semantic content.
    - This increases the number of decoding steps per input and exacerbates autoregressive latency.
- **Findings fromÂ [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)Â by Gloeckle et al. (2024)**:
    
    - 8-byte prediction outperforms single-token next prediction by 67% on MBPP pass@1.
    - Inference speedup of 6.4Ã—, fully amortizing byte-level overhead.

### Beyond Decoding: Speculative Sampling for Diverse Output

- While initial work focused on greedy or top-kÂ decoding, speculative techniques are being extended to support:
    
    - **Diverse sampling**Â (via top-pÂ or temperature-controlled typical decoding)
    - **Beam search variants**Â (speculative beam candidates + top-scoring path verification)
    - **Stochastic acceptance**Â (accept â€œclose enoughâ€ tokens under Wasserstein distance or KL threshold)
- This makes speculative decoding viable for tasks requiring diversity, such as story generation, summarization, and open-ended Q\&A.

### Future Research Directions

- Several open questions and promising directions remain:
    
    - **Speculative Training**: Can models be explicitly trained to improve speculative token acceptance rates (e.g., contrastive token alignment)? This would unify training and decoding under a shared goal.
        
    - **Reinforcement-Tuned Speculators**: How can RLHF-style alignment guide draft model predictions or head outputs for better human preference alignment?
        
    - **Adaptive Drafting**: Can models dynamically adjust the speculative prefix length based on uncertainty, entropy, or input complexity?
        
    - **Token-Free Decoding**: Recent proposals like â€œlatent decodingâ€ (generating hidden states directly) could be paired with speculative strategies to push inference latency even lower.


## Takeaways

- Speculative decoding represents a pivotal advancement in making LLM inference faster, more efficient, and more scalableâ€”without compromising model accuracy or requiring massive retraining. In this primer, weâ€™ve explored the conceptual underpinnings, design patterns, and technical implementations behind speculative decoding.
- While hybrid speculative models (e.g., Medusa + draft) offer a path to greater speed and flexibility, future systems will likely featureÂ _dynamic, train-time-aware_Â speculative inference pipelines tailored to use case and device constraints.
    
- **Takeaways**:
    - **Autoregressive inference is inherently sequential**, but speculative decoding introduces parallelism by â€œguessingâ€ future tokens and verifying them.
        
    - **Three main strategies dominate**:
        
        - **Draft model decoding**: Uses a separate small model for speculative suggestions.
        - **Multi-token prediction heads**: Built into the model at pretraining time, allowing for native speculative output.
        - **Medusa**: Enhances multi-head prediction with tree attention and flexible acceptance schemes.
    - **Speedups are real and measurable**:
        
        - 2â€“3Ã—Â (draft models),
        - 3â€“6Ã—Â (multi-token heads),
        - 2.3â€“2.8Ã—Â (Medusa in real-world batch-1 usage).
    - **Memory-efficient implementations**Â are critical to unlocking the full benefits of speculative decoding, especially when dealing with large vocabularies and long sequences.
        
    - **Use-case dependent**:
        
        - Draft models excel in low-latency deployment pipelines.
        - Medusa is great for chatbots and single-user scenarios.
        - Multi-token heads are most effective when trained from scratch.


# Model Acceleration


## Training Optimizations

### Overview

- Training optimizations for large language models (LLMs) focus on reducing computational and memory overhead during the training phase while preserving model quality. As LLMs scale in size and sequence length, traditional attention mechanisms and dense architectures become bottlenecks due to their high compute and memory requirementsâ€”most notably the quadratic complexity of self-attention.
    
- This section explores innovations aimed at accelerating training through both algorithmic and systems-level enhancements. These include:
    
    - **Memory-aware attention algorithms**Â like FlashAttention and FlashAttention-2 that optimize data movement between GPU memory hierarchies (e.g., from HBM to SRAM), significantly reducing memory bandwidth usage and computation time. These approaches prioritize hardware efficiency through techniques such as tiling, recomputation, and parallelization of attention blocks.
        
    - **Multi-query and grouped-query attention methods**, such as those proposed in the Fast Transformer Decoding and GQA papers, which reduce redundancy in attention heads by sharing key/value projections. These techniques are especially valuable for speeding up decoding and inference but also reduce the number of parameters and computational cost during training.
        
    - **Sparse and localized attention schemes**Â like those introduced in Longformer, which replace global self-attention with a combination of local windowed and task-specific global attention. This approach reduces memory consumption and compute time from quadratic to linear with respect to sequence length, enabling efficient training on longer sequences.
        
- Together, these methods represent a growing body of work that rethinks the Transformer architecture and its memory-compute tradeoffs. They aim to make LLM training more scalable, efficient, and accessibleâ€”paving the way for faster iterations and the deployment of increasingly capable models on constrained hardware. Subsequent sections provide a closer look at specific techniques and their empirical results.


### [FlashAttention](https://arxiv.org/abs/2205.14135)

- Proposed inÂ [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)Â by Dao et al. from Stanford.
- Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. They argue that a missing principle is making attention algorithms IO-aware â€“ accounting for reads and writes between levels of GPU memory.
- This paper by Dao et al. from Stanford in 2022 proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. Specifically, FlashAttention reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length.
- They analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. They also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.
- FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3x speedup on GPT-2 (seq. length 1K), and 2.4x speedup on long-range arena (seq. length 1K-4K).
- FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).
- The figure below from the paper shows: (Left) FlashAttention uses tiling to prevent materialization of the largeÂ NÃ—NÂ attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of theÂ KÂ andÂ VÂ matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks ofÂ QÂ matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the largeÂ NÃ—NÂ attention matrix to HBM, resulting in an 7.6x speedup on the attention computation.

![](https://aman.ai/images/papers/FlashAttention.jpg)

- [Code](https://github.com/Dao-AILab/flash-attention)
- A detailed discourse on this topic is available in ourÂ [FlashAttention](https://aman.ai/primers/ai/flashattention)Â primer.


