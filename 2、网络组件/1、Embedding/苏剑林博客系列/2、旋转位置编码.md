
> [Transformer升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)

 
上一篇文章中，我们对原始的正弦位置编码做了较为详细的推导和理解，总的感觉是正弦位置编码是一种“想要成为相对位置编码的绝对位置编码”。一般来说，绝对位置编码具有实现简单、计算速度快等优点，而相对位置编码则直接地体现了相对位置信号，跟我们的直观理解吻合，实际性能往往也更好。由此可见，如果可以通过绝对位置编码的方式实现相对位置编码，那么就是“集各家之所长”、“鱼与熊掌兼得”了。正弦位置编码隐约做到了这一点，但并不够好。

本文将会介绍我们自研的 Rotary Transformer（RoFormer）模型，它的主要改动是应用了笔者构思的“旋转式位置编码（Rotary Position Embedding，RoPE）”，这是一种配合 Attention 机制能达到“绝对位置编码的方式实现相对位置编码”的设计。而也正因为这种设计，它还是目前唯一一种可用于线性 Attention 的相对位置编码。

> **RoFormer：[https://github.com/ZhuiyiTechnology/roformer](https://github.com/ZhuiyiTechnology/roformer)**

### 基本思路 

在之前的文章[《让研究人员绞尽脑汁的Transformer位置编码》](https://spaces.ac.cn/archives/8130)中我们就简要介绍过RoPE，当时称之为“融合式”，本文则更加详细地介绍它的来源与性质。在 RoPE 中，我们的出发点就是“通过绝对位置编码的方式实现相对位置编码”，这样做既有理论上的优雅之处，也有实践上的实用之处，比如它可以拓展到线性 Attention 中就是主要因为这一点。

为了达到这个目的，我们假设通过下述运算来给 $\boldsymbol{q},\boldsymbol{k}$ 添加绝对位置信息： $$\begin{equation}\tilde{\boldsymbol{q}}_m = \boldsymbol{f}(\boldsymbol{q}, m), \quad\tilde{\boldsymbol{k}}_n = \boldsymbol{f}(\boldsymbol{k}, n)\end{equation}
\tag{1}$$
也就是说，我们分别为 $\boldsymbol{q},\boldsymbol{k}$ 设计操作 $\boldsymbol{f}(\cdot, m),\boldsymbol{f}(\cdot, n)$，使得经过该操作后，$\tilde{\boldsymbol{q}}_m,\tilde{\boldsymbol{k}}_n$ 就带有了位置 $m,n$ 的绝对位置信息。Attention 的核心运算是内积，所以我们希望的内积的结果带有相对位置信息，因此假设存在恒等关系：  $$\begin{equation}\langle\boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n)\rangle = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}
\tag{2}$$
所以我们要求出该恒等式的一个（尽可能简单的）解。求解过程还需要一些初始条件，显然我们可以合理地设 $\boldsymbol{f}(\boldsymbol{q}, 0)=\boldsymbol{q}$ 和 $\boldsymbol{f}(\boldsymbol{k}, 0)=\boldsymbol{k}$。

### 求解过程

同上一篇思路一样，我们先考虑二维情形，然后借助复数来求解。在复数中有 $\langle\boldsymbol{q},\boldsymbol{k}\rangle=\text{Re}[\boldsymbol{q}\boldsymbol{k}^*]$，$\text{Re}[]$ 代表复数的实部，所以我们有  $$\begin{equation}\text{Re}[\boldsymbol{f}(\boldsymbol{q}, m)\boldsymbol{f}^*(\boldsymbol{k}, n)] = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}
\tag{3}$$
简单起见，我们假设存在复数 $\boldsymbol{g}(\boldsymbol{q},\boldsymbol{k},m-n)$，使得 $\boldsymbol{f}(\boldsymbol{q}, m)\boldsymbol{f}^*(\boldsymbol{k}, n) = \boldsymbol{g}(\boldsymbol{q},\boldsymbol{k},m-n)$，然后我们用复数的指数形式，设  $$\begin{equation}\begin{aligned} \boldsymbol{f}(\boldsymbol{q}, m) =&\, R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)} \\ \boldsymbol{f}(\boldsymbol{k}, n) =&\, R_f (\boldsymbol{k}, n)e^{\text{i}\Theta_f(\boldsymbol{k}, n)} \\ \boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) =&\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n)e^{\text{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)} \\ \end{aligned}\end{equation}
\tag{4}$$
那么代入方程后就得到方程组  $$\begin{equation}\begin{aligned} 
R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, n) =&\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n) \\ 
\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, n) =&\, \Theta_g (\boldsymbol{q}, \boldsymbol{k}, m-n) 
\end{aligned}\end{equation}
\tag{5}$$
对于第一个方程，代入 $m=n$ 得到  $$\begin{equation}R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, m) = R_g (\boldsymbol{q}, \boldsymbol{k}, 0) = R_f (\boldsymbol{q}, 0) R_f (\boldsymbol{k}, 0) = \Vert \boldsymbol{q}\Vert \Vert \boldsymbol{k}\Vert\end{equation}
\tag{6}$$
最后一个等号源于初始条件 $\boldsymbol{f}(\boldsymbol{q}, 0)=\boldsymbol{q}$ 和 $\boldsymbol{f}(\boldsymbol{k}, 0)=\boldsymbol{k}$。所以现在我们可以很简单地设 $R_f (\boldsymbol{q}, m)=\Vert \boldsymbol{q}\Vert, R_f (\boldsymbol{k}, m)=\Vert \boldsymbol{k}\Vert$，即它不依赖于 $m$。至于第二个方程，同样代入 $m=n$ 得到 $$\begin{equation}\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, m) = \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 0) = \Theta_f (\boldsymbol{q}, 0) - \Theta_f (\boldsymbol{k}, 0) = \Theta (\boldsymbol{q}) - \Theta (\boldsymbol{k})\end{equation}
\tag{7}$$ 
这里的 $\Theta (\boldsymbol{q}),\Theta (\boldsymbol{k})$ 是 $\boldsymbol{q},\boldsymbol{k}$ 本身的幅角，最后一个等号同样源于初始条件。根据上式得到 $\Theta_f (\boldsymbol{q}, m) - \Theta (\boldsymbol{q}) = \Theta_f (\boldsymbol{k}, m) - \Theta (\boldsymbol{k})$，所以 $\Theta_f (\boldsymbol{q}, m) - \Theta (\boldsymbol{q})$ 应该是一个只与 $m$ 相关、跟 $\boldsymbol{q}$ 无关的函数，记为 $\varphi(m)$，即 $\Theta_f (\boldsymbol{q}, m) = \Theta (\boldsymbol{q}) + \varphi(m)$。接着代入 $n=m−1$，整理得到$$\begin{equation}\varphi(m) - \varphi(m-1) = \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 1) + \Theta (\boldsymbol{k}) - \Theta (\boldsymbol{q})\end{equation}
\tag{8}$$
即 $\{\varphi(m)\}$ 是等差数列，设右端为 $\theta$，那么就解得 $\varphi(m)=m\theta$。

### 编码形式

综上，我们得到二维情况下用复数表示的 RoPE：  $$\begin{equation} 
\boldsymbol{f}(\boldsymbol{q}, m) = R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)} 
= \Vert q\Vert e^{\text{i}(\Theta(\boldsymbol{q}) + m\theta)} = \boldsymbol{q} e^{\text{i}m\theta}\end{equation}
\tag{9}$$
根据复数乘法的几何意义，该变换实际上对应着向量的旋转，所以我们称之为“旋转式位置编码”，它还可以写成矩阵形式：$$\begin{equation} \boldsymbol{f}(\boldsymbol{q}, m) =\begin{pmatrix}\cos m\theta & -\sin m\theta\\ \sin m\theta & \cos m\theta\end{pmatrix} \begin{pmatrix}q_0 \\ q_1\end{pmatrix}\end{equation}
\tag{10}$$
由于内积满足线性叠加性，因此任意偶数维的 RoPE，我们都可以表示为二维情形的拼接，即  $$\begin{equation}\scriptsize{\underbrace{\begin{pmatrix} \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\ \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0 \\ 0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\ 0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1} \\ \end{pmatrix}}_{\boldsymbol{\mathcal{R}}_m} \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1}\end{pmatrix}}\end{equation}
\tag{11}$$
也就是说，给位置为 $m$ 的向量 $\boldsymbol{q}$ 乘上矩阵 $\boldsymbol{\mathcal{R}}_m$、位置为 $n$ 的向量 $\boldsymbol{k}$ 乘上矩阵 $\boldsymbol{\mathcal{R}}_n$，用变换后的 $\boldsymbol{Q},\boldsymbol{K}$ 序列做 Attention，那么 Attention 就自动包含相对位置信息了，因为成立恒等式： $$\begin{equation}(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n \boldsymbol{k} = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_{n-m} \boldsymbol{k}\end{equation}
\tag{12}$$
值得指出的是，$\boldsymbol{\mathcal{R}}_m$ 是一个正交矩阵，它不会改变向量的模长，因此通常来说它不会改变原模型的稳定性。

由于 $\boldsymbol{\mathcal{R}}_m$ 的稀疏性，所以直接用矩阵乘法来实现会很浪费算力，推荐通过下述方式来实现 RoPE：  $$\begin{equation}\begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1} 
\end{pmatrix}\otimes\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1} 
\end{pmatrix} + \begin{pmatrix}-q_1 \\ q_0 \\ -q_3 \\ q_2 \\ \vdots \\ -q_{d-1} \\ q_{d-2} 
\end{pmatrix}\otimes\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1} 
\end{pmatrix}\end{equation}
\tag{13}$$
其中 $\otimes$ 是逐位对应相乘，即 Numpy、Tensorflow 等计算框架中的 $∗$ 运算。从这个实现也可以看到，RoPE 可以视为是乘性位置编码的变体。

### 远程衰减

可以看到，RoPE 形式上和正弦位置编码有点相似，只不过正弦位置编码是加性的，而 RoPE 可以视为乘性的。在 $\theta_{i}$ 的选择上，我们同样沿用了正弦位置编码的方案，即 $\theta_i = 10000^{-2i/d}$，它可以带来一定的远程衰减性。

具体证明如下：将 $\boldsymbol{q},\boldsymbol{k}$ 两两分组后，它们加上 RoPE 后的内积可以用复数乘法表示为 $$\begin{equation} (\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right]\end{equation}
\tag{14}$$ 
记 $h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}$，并约定 $h_{d/2}=0,S_0=0$，那么由 [Abel变换（分部求和法）](https://zh.wikipedia.org/wiki/%E5%88%86%E9%83%A8%E6%B1%82%E5%92%8C%E6%B3%95)可以得到：  $$\begin{equation}\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i} = \sum_{i=0}^{d/2-1} h_i (S_{i 
+1} - S_i)  = -\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\end{equation}
\tag{15}$$
所以$$\begin{equation}\begin{aligned} 
\left|\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right| =&\, \left|\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\right| \\ 
\leq&\, \sum_{i=0}^{d/2-1} |S_{i+1}| |h_{i+1} - h_i| \\ 
\leq&\, \left(\max_i |h_{i+1} - h_i|\right)\sum_{i=0}^{d/2-1} |S_{i+1}| 
\end{aligned}\end{equation}
\tag{16}$$
因此我们可以考察 $\frac{1}{d/2}\sum\limits_{i=1}^{d/2} |S_i|$，随着相对距离的变化情况来作为衰减性的体现，Mathematica 代码如下：

```mathematica
d = 128;
\[Theta][t_] = 10000^(-2*t/d);
f[m_] = Sum[
    Norm[Sum[Exp[I*m*\[Theta][i]], {i, 0, j}]], {j, 0, d/2 - 1}]/(d/2);
Plot[f[m], {m, 0, 256}, AxesLabel -> {相对距离, 相对大小}]
```

![RoPE的远程衰减性（d=128）|500](https://spaces.ac.cn/usr/uploads/2021/03/1347893165.png)

RoPE 的远程衰减性（$d=128$）

从图中我们可以可以看到随着相对距离的变大，内积结果有衰减趋势的出现。因此，选择 $\theta_i = 10000^{-2i/d}$，确实能带来一定的远程衰减性。当然，同上一篇文章说的一样，能带来远程衰减性的不止这个选择，几乎任意的光滑单调函数都可以，这里只是沿用了已有的选择而已。笔者还试过以 $\theta_i = 10000^{-2i/d}$ 为初始化，将 $\theta_{i}$ 视为可训练参数，然后训练一段时间后发现 $\theta_{i}$ 并没有显著更新，因此干脆就直接固定 $\theta_i = 10000^{-2i/d}$ 了。

### 线性场景

最后，我们指出，RoPE 是目前唯一一种可以用于线性 Attention 的相对位置编码。这是因为其他的相对位置编码，都是直接基于 Attention 矩阵进行操作的，但是线性 Attention 并没有事先算出 Attention 矩阵，因此也就不存在操作 Attention 矩阵的做法，所以其他的方案无法应用到线性 Attention 中。而对于 RoPE 来说，它是用绝对位置编码的方式来实现相对位置编码，不需要操作 Attention 矩阵，因此有了应用到线性 Attention 的可能性。

关于线性 Attention 的介绍，这里不再重复，有需要的读者请参考[《线性Attention的探索：Attention必须有个Softmax吗？》](https://spaces.ac.cn/archives/7546)。线性 Attention 的常见形式是： $$\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})_i = \frac{\sum\limits_{j=1}^n \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits_{j=1}^n \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)} = \frac{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)}\end{equation}
\tag{17}$$
其中 $\phi,\varphi$ 是值域非负的激活函数。可以看到，线性 Attention 也是基于内积的，所以很自然的想法是可以将 RoPE 插入到内积中：$$\begin{equation}\frac{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]\boldsymbol{v}_j}{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]}\end{equation}
\tag{18}$$
但这样存在的问题是，内积 $[\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]$ 可能为负数，因此它不再是常规的概率注意力，而且分母有为 0 的风险，可能会带来优化上的不稳定。考虑到 $\boldsymbol{\mathcal{R}}_i,\boldsymbol{\mathcal{R}}_j$ 都是正交矩阵，它不改变向量的模长，因此我们可以抛弃常规的概率归一化要求，使用如下运算作为一种新的线性 Attention：$$\begin{equation}\frac{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]\boldsymbol{v}_j}{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)}\end{equation}
\tag{19}$$
也就是说，RoPE 只插入分子中，而分母则不改变，这样的注意力不再是基于概率的（注意力矩阵不再满足非负归一性），但它某种意义上来说也是一个归一化方案，而且也没有证据表明非概率式的注意力就不好（比如 [Nyströmformer](https://spaces.ac.cn/archives/8180) 也算是没有严格依据概率分布的方式构建注意力），所以我们将它作为候选方案之一进行实验，而我们初步的实验结果显示这样的线性 Attention 也是有效的。

此外，笔者在[《线性Attention的探索：Attention必须有个Softmax吗？》](https://spaces.ac.cn/archives/7546)中还提出过另外一种线性 Attention 方案：$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)$，它不依赖于值域的非负性，而 RoPE 也不改变模长，因此 RoPE 可以直接应用于此类线性 Attention，并且不改变它的概率意义。

### 文章小结

本文介绍了我们自研的旋转式位置编码 RoPE 以及对应的预训练模型 RoFormer。从理论上来看，RoPE 与 正弦位置编码有些相通之处，但 RoPE 不依赖于泰勒展开，更具严谨性与可解释性；从预训练模型 RoFormer 的结果来看，RoPE 具有良好的外推性，应用到 Transformer 中体现出较好的处理长文本的能力。此外，RoPE 还是目前唯一一种可用于线性 Attention 的相对位置编码。