
https://zhuanlan.zhihu.com/p/643560888
https://www.nowcoder.com/discuss/646037378791940096
https://zhuanlan.zhihu.com/p/710164031


coder-only架构已成主流，进一步地，Decoder-only架构可以细分为因果解码器（Causal Decoder）和前缀解码器（Prefix Decoder）。学术界提到解码器架构时，通常指因果解码器
下图对三种解码器架构进行对比：



Encoder-Decoder

Encoder-Decoder架构即原始Transformer的架构，机器翻译
编码器端：双向自注意力机制对输入信息编码处理
解码器端：交叉注意力与掩码自注意力机制，进而通过自回归方式生成
目前只有FLAN-T5等少数LLM是基于Encoder-Decoder搭建
Causal-Decoder

主流框架（GPT系列），因果语言模型，包括LLaMa也是
使用单向注意力掩码，以确保每个输入token只能注意到过去的token以及本身
输入和输出的token通过Decoder以相同方式进行处理
在图中，灰色代表两个token互相之间看不到，否则就可以看到，例如Survey可以看到前面的A，但看不到后面的of，Causal Decoder的Sequence Mask矩阵是一种典型的下三角矩阵
代表模型：GPT系列，LLaMa（Meta）
Prefix-Decoder

又称为非因果解码器架构，对掩码机制修改
前缀解码器对于输入（前缀）部分使用双向注意力进行编码，而对于输出部分利用单向掩码注意力，即利用该token本身和前面的token进行自回归预测
代表模型：GLM-130B和U-PaLM（Google）
总结：三者区别在于attention mask不同

Encoder-Decoder（T5）

输入采用双向注意力，对问题编码理解更充分
在偏理解的NLP任务上效果好
长文本生成效果差，训练效率低
Causal Decoder（GPT）

自回归LM，预训练和下游应用完全一致，严格遵守只有后面的token才能看到前面token的规则
文本生成效果好
训练效率高，zero-shot能力更强，具有涌现能力
Prefix Decoder（GLM）

prefix部分的token相互能看到
文本生成效果好
6. LLMs中常用的预训练任务（目标）
主要分为三类：



语言建模（LM）：
目标函数：每个token的最大似然
本质上，是一种多任务学习过程，因为不同token的预测对应不同的任务（数量、情感等）
因此可以潜在地学习到解决不同任务的知识与能力
训练效率：Prefix Decoder < Causal Decoder
Causal Decoder架构会在所有token上计算损失，而Prefix Decoder只在输出上计算损失
去噪自编码（Denoising AutoEncoder）：
BERT，T5
文本经过一系列随即替换、删除操作，形成损坏的文本，模型需要恢复原文本
目标函数就是被损坏的token的最大似然
任务设定更为复杂，需要设定token替换策略，替换长度，替换比例，都影响训练效果。目前应用有限，主要是FLAN-T5（不过我记得GLM的预训练任务也是类似的一种创新的任务）
混合去噪器（Mixture-of-Denoisers，MoD）：统一了DAE和LM
7. LLMs中涌现能力是啥原因
参考资料：张俊林老师《大语言模型的涌现能力：现象与解释》

涌现能力：复杂系统由很多微小个体构成，当这些微小个体凑到一起，相互作用，当数量足够多时，在宏观层面上展现出微观个体无法解释的特殊现象（效果突然变好了）

猜想一：任务的评价指标不够平滑：

以Emoji_movie任务来解释，任务是输入Emoji图像，LLM给出完全正确的电影名称，一字不差算对
例子中，2M到128B后，模型完全猜对，但其实中途已经慢慢感觉猜对了，还差一点，但是评价指标是精准匹配，因此导致模型的评估出现突然增长。
改成平滑的指标就不会有这种跳跃了。
猜想二：复杂任务 v.s. 子任务

复杂任务是由多个子任务构成，只有当子任务都学会了，复杂任务才能做对
因此发生性能条约
这里以国际象棋AI训练为例，合法移动 v.s. 将死
LLM预测下一步，最终评价指标是将死才算赢，如果按将死评估，发现模型增大，模型缓慢上升，符合涌现的表现。
若评估LLM合法移动，而在合法的移动步骤里进行正确选择才够最后将死是个子任务，所以其实这是比将死简单的子任务
我们看合法移动随着模型规模，效果持续上升，其实并没有涌现
8. 什么是Scaling Law？谈谈对它的理解
什么是Scaling Law？

在训练之前了解模型的能力，以改善大模型的对齐、安全和部署决定。

通过测试不同尺寸下模型的性能，然后对大尺寸模型的性能进行预测

GPT-4 technical report里对GPT-4性能边界的预测（https:/cdn.openai.com/papers/gpt-4.pdf）

定义：用计算量、数据集规模和模型尺寸，来预测模型的最终能力（通常以相对简单的函数形态，如Linear relationship）

在LLM中，我们期望模型能够理解人类语言的一般规律，从而做出与人类相似的表达方式，通过使用大量的数据进行训练从而获得使得模型学习到数据背后的一般规律。

LLM预训练，主要是围绕训练的计算量，数据集规模和模型规模的三方博弈

但是三者的作用到底是多少呢？Scaling Law就是做这个的



OpenAI和DeepMind这两家有代表性研究（前者是做AGI，后者着重于高精专的AI，如AlphaGo）

2020年，OpenAI的Kaplan团队在Scaling Laws for Neural Language Models，这个我之前看过，博客125139643，arxiv.2001.08361

他们发现模型尺寸，数据集大小和训练计算量，三者任一受限时，Loss与其之间存在幂律关系（即两个变量中的一个变量与另一个变量的某个幂次呈正比）
因此为了获得最佳性能，必须将三者同步扩大
当没有受到其他两个因素限制时，性能与每个单独因素之间呈幂律关系
影响模型性能的三个要素之间，每个参数会受到其他两个参数影响。当没有其他两个瓶颈时，性能会急剧上升，影响程度为计算量>参数量>>数据集大小
在固定计算预算下，最佳性能可以通过训练参数量非常大的模型，并非在远离收敛前停止实现（early stop）
更大的模型在样本效率方面表现更好，能以更少的优化步骤和使用更少的数据量达到相同的性能水平。
实际应用中，应该优先考虑训练较大的模型
2022年，DeeoMind的Hoffmann团队，在Training Compute-Optimal Large Language Models（arxiv.2203.15556）提出了与OpenAI截然不同的观点

OpenAI建议在计算预算增加10倍的情况下，如果想保持效果，模型大小应该增加5.5倍，但DeepMind认为是增加1.8倍
DeepMind认为模型大小和训练Token的数量都应该按等比例扩展，暗示GPT3过度参数化，也就是说参数量太多了，超过了实际所需，且训练不足
结论：
对于给定的FLOP预算，损失函数有明显的谷底值：
模型太小时，在较少数据上训练较大模型是一种改进
模型太大时，在更多数据上训练较小模型时一种改进
也就是说，在给定计算量下，数据量和模型参数量之间平衡存在最优解
在计算成本达到最优情况下，模型大小和训练数据量应该等比例放缩。对于给定参数量的模型，最佳训练数据集大小约为模型参数量的20倍，比如7B模型应该是140B的tokens训练
大模型训练需要更加关注数据集的扩展，高质量数据集，数据越多才有用
总结：

定义：计算量、数据集规模、模型大小，来预测性能
OpenAI：三者两两相关，当两个没有瓶颈时，性能会急剧上升，重要性计算量>参数量>>数据集大小
DeepMind：三者应等比例扩展
大模型幻觉相关
1. 什么是大模型幻觉？
定义：（一本正经的胡说八道）模型生成的文本不遵循原文（一致性，Faithfulness）或者不符合事实（事实性，Factualness）
Faithfulness：是否遵循输入的上下文
Factualness：是否符合世界知识
在传统任务中，幻觉大都是指Faithfulness：
信息冲突（Instrinsic Hallucination）：LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致
无中生有（Extrinsic Hallucination）：LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假
而面向LLMs，我们通常考虑的幻觉则是Factualness：
因为我们应用LLMs的形式是open-domain chat，而非局限于特定任务，所以数据源可以看作任意的世界知识。LLMs如果生成不在输入信息中的额外信息，但是符合事实的，也是有帮助的。
2. 发生幻觉的原因
从数据角度：训练数据可信度问题，重复数据问题
从模型角度（主要原因）
模型结构：如果是较弱的backbone（如RNN），可能导致严重的幻觉问题，但LLMs时代不太可能存在这一问题
解码算法：研究表明，如果使用不稳定性较高的采样算法（如top-p）会诱导LLMs出现严重的幻觉问题。甚至可以故意在解码算法中加入一些随机性，进一步让LLMs胡编乱造（利用该方法可以生成一些负样本）
top-p采样（也称为核采样）是一种引入不确定性的采样算法，常用来生成更加多样化和创造性的问题，原理是模型从预测概率最高的token开始累加，当这些token的概率综合达到设定的阈值（p值）后停止，从而在这些候选token中随机选取一个词生成。可以避免仅生成概率最高的词，从而提升文本的流畅性和丰富度。
暴露偏差：训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候
训练阶段一切都是真实的文本，但生成时，模型只能按照自己之前生成的文本继续生成，蝴蝶效应。
参数知识：LLMs在预训练阶段记忆的错误知识，导致严重的幻觉问题
3. 如何评估大模型幻觉问题？
现有的传统幻觉评估指标和人类结果相关性往往较低，同时大多是task-specific的
主要评估方法：基于参考的评估和无参考的评估
方法一：基于参考的评估（reference-based）

通常只能评价Faithfulness，无法评价Factualness，因此通常不适用于LLMs
指在评估生成内容的准确性时，使用参考文本（如人类标准答案）或原始信息源作为对比
衡量两者的重叠程度或相似度，指标如ROUGE和BLEU
优点：适合一些标准化的生成任务
缺点：许多任务标准答案可能并非唯一，生成内容多样化，因此该方法的灵活性不足。而且LLMs开放生成任务，很难找到完美的参考答案
指标主要有两类：
BLEU和ROUGE这种统计重叠度的指标（Source information + Target Reference）
Knowledge F1（Source information only）
Knowledge F1时一种用于评估NLG的指标，主要用于检测幻觉
思路：比较模型生成内容和参考知识老远之间的匹配度，判断准确性和一致性
计算原理：
知识检索：首先，从任务相关的知识库或上下文信息中提取模型生成时可参考的源知识
知识匹配：然后，将模型的生成输出和源知识中的信息进行比对，找出哪些时和源知识一致的
F1得分计算：最后，通过Precision和Recall来计算F1得分
方法二：无参考评估

旨在不用标准答案或特定参考来检测模型生成内容的准确性和一致性
各种方法：
基于信息抽取（IE）：将生成内容转化为结构化知识，如RDF三元组，然后用另一个模型来验证三元组的真实性
缺点：IE模型本身可能出错，抽取的信息不对，后续检验也就无效了；且只能受限于三元组只知识，很多知识不能用三元组表达，局限性。
基于问答（QA）：
首先，使用一个问题生成模型，根据模型的生成内容来产生一系列相关的问答对
然后，利用源信息，使用问答模型回答这些问题
最后，将问答模型的答案和最初生成的答案对比，通过匹配度评估生成内容的真实性
缺点：IE模型的错误传播，QA过程依然依赖IE模型生成的；而且难以评估Factualness，因为QA模型回答问题时，源信息未必包含所有所需知识，可能无法准确回答或验证的情况。
基于自然语言推理（NLI）：
即通过验证生成文本是否由源信息的蕴含来判断其是否存在幻觉
但是问题是幻觉未必和蕴含划等号
缺点：
性能有限：目前NLI模型在事实喝茶方面表现一般，难以准确验证生成内容
无法检测世界知识相关的幻觉：世界知识太大了，很难检测蕴含关系
粒度有限：局限于句子级别的蕴含检测，无法更细粒度
幻觉和蕴含不等价：幻觉不仅仅是不蕴含，比如Putin is president和Putin is U.S. president在语义上并非幻觉，但是会被判断为蕴含
NLI中，蕴含意味着一个句子能够逻辑推导出另一个句子，但不涉及判断信息的真实性
基于事实性分类指标（Factualness Classification Metric）
通过人工标注或构造包含幻觉和真实信息的数据集，训练分类模型来检测新生成的文本是否符合事实。但是依赖标注，成本高昂。
人工评估
总结：分为基于参考的评估和**无参考的评估

4. 如何缓解LLMs幻觉问题
基于数据的工作：高质量数据集构造
人工标注：训练数据、评测数据
自动筛选：筛除不良数据、数据加权，如给可靠度高的数据赋予高的权重，如wikipedia，不可靠的数据赋予低的权重
模型层面：从模型结构和训练方式入手
模型结构：模型结构方面的改进主要关注在设计能够更好地利用来源信息的结构，例如
编码更多信息：用GNN编码这种融入能反映人类思维偏好的结构，更好地专注输入信息
减少生成随机性：在解码时减少模型生成内容地随机性（多样性和准确性是互相掣肘的），提高准确性（temperature）
检索增强：引入外部检索系统（如LLaMaIndex）
训练方式：
可控文本生成：将幻觉控制设为一个可控属性，让模型生成时更少产生幻觉
提前规划内容框架：采用sketch-to-content方法，先规划一个大致的框架再生成具体内容，有助于结构化信息并减少偏差
强化学习：通常模型使用MLE来优化训练目标，这可能会暴露偏差。通过引入强化学习，将减少幻觉的目标作为奖励函数，调整模型生成过程
多任务学习：设计额外任务，使模型在执行多项任务时能提升应对幻觉的能力
后处理：纠错模块设计
一篇相关的论文：

A Stitch in Time Savess Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation
arxiv.2307.03987
幻觉的生成是会传播的，比如一句话出现幻觉，后续可能会更加严重。（预防很重要，防患于未然）
logit输出值可以用来获取幻觉信号。比如，计算了一个logits值，并展示了当这个得分很低时，模型更容易出现幻觉。其实就是置信度很低的解码输出时不可信的，容易出幻觉
基于上述两个发现，作者提出了主动检测和减轻幻觉的方法，如图所示：



首先抽取输入语句种的关键概念（实体、关键词），然后计算它们的不确定性（这个就是单纯基于生成模型解码的logits来判断的，虽然解释性一般，但是的确有用）
这里就是发现出生地和出生日期是很不确定的
然后基于这些不确定的phrase，我们去检索相关的知识（self-inquiry，上下文搜索；网络搜索，外部搜索），作为辅助的信息
在检测阶段，首先识别重要概念，计算模型在这些概念上的不确定性，并通过检索相关知识验证这些不确定概念的正确性
消除阶段，基于输入+检索得到的辅助信息，使用问题生成模型生成问答对，根据问答内容来修正这些实体，然后将修正好的句子放回去。
在缓解阶段，使用检测到的知识作为依据，修复存在幻觉的句子
即，将修复后的句子添加到输入中（和之前生成的句子一起），然后生成下一个句子
幻觉是有蝴蝶效应的，前面有幻觉，后面幻觉就会越来越大，有点像束搜索，所以要提前防控。

RAG相关
1. 什么是RAG，它有什么特点？
RAG为生成式模型提供了与外部世界互动的解决方案
RAG的主要作用类似搜索引擎，找到用户提问最相关的知识或对话历史，并结合原始问题（查询），创造信息丰富的prompt，指导模型生成准确输出
本质是利用了In-context Learning的原理
RAG = 检索技术 + LLM提示
RAG特点：

依赖大语言模型来强化信息检索和输出：RAG需要结合大型语言模型(LLM)来进行信息的检索和生成，但如果单独使用RAG它的能力会受到限制。也就是说，RAG需要依赖强大的语言模型支持，才能更有效地生成和提供信息。
能与外部数据有效继承：能与外部数据有效集成:RAG能够很好地接入和利用外部数据库的数据资源。这一特性弥补了通用大模型在某些垂直或专业领域的知识不足，比如行业特定的术语和深度内容，能提供更精确的答案。
数据隐私和安全保障：通常，RAG所连接的私有数据库不会参与到大模型的数据集训练中。因此，RAG既能提升模型的输出表现，又能有效地保护这些私有数据的隐私性和安全性，不会将敏感信息暴露给大模型的训练过程。
表现效果因多方面因素而异：RAG的效果受多个因素的影响，比如所使用的语言模型的性能、接入数据的质量、AI算法的先进性以及检索系统的设计等。这意味着不同的RAG系统之间效果差异较大，不能一概而论。
2. RAG的总体思路
总体思路：参考https://aibook.ren/archives/what-is-rag



RAG可分为5个基本流程：知识文档准备、嵌入模型、向量数据库、查询检索、生产回答

知识文档准备：

文档格式：WORD，TXT，CSV，EXCEL，PDF，图片，视频
需要使用专门的文档加载器（如PDF提取）或多模态模型（如OCR技术），将丰富的知识源转换为LLMs可理解的纯文本数据
文档切片：针对长文档，需要切割，以便更高效地
嵌入模型：text-to-tensor，稀疏离散的文本转为密集精确的张量表征，捕捉上下文的关系和核心含义

向量数据库：嵌入模型生成的张量存储到数据库

Chroma：

轻量级、易用性、开源。
快速搭建小型语义搜索，提供高效的近似最近邻搜索（ANN），支持多种向量数据类型和索引方法，易于集成到现有的应用程序中。
小型语义搜索原型、研究或教学项目。
适合初学者和小型项目
Pinecone：

实时性、高性能、可扩展。
大规模数据集上的实时搜索，亚秒级的查询响应时间，支持大规模向量集的高效索引和检索，提供高度可伸缩的分布式架构。
实时推荐系统、大规模电商搜索引擎、社交媒体内容过滤。
适合需要高性能和实时性的大型应用
Weaviate：

语义搜索、图数据库、多模态。
构建智能助手、知识图谱，支持多模态数据（文本、图像等）的语义搜索，提供强大的查询语言和推理能力。
复杂知识图谱应用、智能问答系统、多模态内容管理平台。
适合需要复杂查询和推理能力的知识密集型应用
Milvus：

大规模数据、云原生、高可用性。
专为处理超大规模向量数据而设计，提供云原生的分布式架构和存储方案，支持多种索引类型和查询优化策略。
大规模内容检索平台、图像和视频搜索、智能安防系统。
适合需要处理超大规模数据的云端应用
Faiss：

高效性、灵活性、Facebook支持。
提供高效的相似度搜索和稠密向量聚类能力，支持多种索引构建方法和查询优化策略，易于与深度学习框架集成（如PyTorch）。
Facebook内部语义搜索和推荐系统、广告技术平台、深度学习应用中的向量检索模块。
适合需要高效相似度搜索和丰富社区支持的大型应用
查询检索：用户问题会被输入到嵌入模型中进行向量化处理，然后系统在向量数据库中搜索与该问题语义相似的知识文本或历史对话记录返回。

生产回答：最终用户提问会和检索得到的信息结合，构建一个提示模板，输入到LLMs中，生成回答。

3. 如何评价RAG项目效果的好坏
针对检索环节的评估：

MRR（平均倒数排名），查询（或推荐请求）的排名倒数

MEAN Reciprocal Rank，MRR，多用于衡量搜索引擎、推荐系统等根据查询返回的多个结果的相关性
M R R = 1 n ∑ i = 1 n 1 r i MRR=\frac1n\sum_{i=1}^n \frac1{r_i}MRR= 
n
1
​
 ∑ 
i=1
n
​
  
r 
i
​
 
1
​
 
Hits Rate（命中率）：前k项中，包含正确信息的检索项数目占比

NDCG（归一化折扣累计增益）：DCG的两个思想

高关联度的结果比一般关联度的结果更影响最终的指标得分
有高关联度的而己过出现在更靠前的位置时，指标会越高


参考：排序算法评估：NDCG(归一化折扣累计增益Normalized Discounted Cumulative Gain)

针对生成环节的评估

非量化：完整性、正确性、相关性

量化：Rouge-L指标

Rouge-L是一种用于评价文本生成质量的指标，通常在自动病要、机器翻译和文本生成任务中使用。它是Rouge（Recall-Oriented Understudy for Gisting Evaluation）评估指标系列中的一种，专门通过**最长公共子序列（Longest Common Subsequence，LCS）**来测量生成文本和参考文本之间的相似性。

基本思想大由多个专家分别生成人工摘要，构成标准搞要集，将系统生成的自动摘要与人工生成的标准摘要相对比，通过统计二者之间重叠的基本单元（n-gram、词序列和词对）的数目，来评价摘要的质量。

Rouge-L的计算主要包括两个方面：

Recall：参考文本中与生成文本匹配的最长公共子序列的长度，与参考文本的总长度之比

Precision：生成文本中与参考文本匹配的最长公共子序列的长度，与生成文本的总长度之比

然后用这个PR值计算F1，即：
R o u g e − L = 2 P R P + R Rouge-L = \frac{2PR}{P+R}
Rouge−L= 
P+R
2PR
​
 

Rouge-L比Rouge-1或Rouge-2更能衡量文本生成的结构和顺序是否与参考文本接近，因此在长文段的连贯性和句子顺序检测上具有优势。

4. RAG的优化策略（重要）
4.1 知识文档准备阶段（数据清洗、分块处理）
数据清洗

RAG依赖准确和清洁的原始知识

表格结构会在单纯的文本转换后丢失原有结构，因此需要引入额外机制来保持表格结构（如使用分号或其他符号来区分数据）

其他数据清洗操作：

基本文本清理：规范格式、去除特殊字符、不相关信息、重复文档、冗余信息
实体解析：消歧，如将LLMs，大语言模型，大模型类似的标准化为通用术语
文档划分：按主题划分，不同主题的文档集中或分散？人类都不能判段用哪些文档来回答提问，检索系统也不能
数据增强：同义词、释义、其他语言的翻译来增加语料库的多样性
RLHF：基于现实世界用户的反馈不断更新数据库，标记真实性
时间敏感数据：对于经常更新的主题，删除过期文档
分块处理：Chunk

在RAG系统中，文档需要分割成多个文本块再进行向量嵌入。

在不考虑大模型输入长度限制和成本问题情况下，其目的是在保持语义上的连贯性的同时，尽可能减少嵌入内容中的噪声，从而更有效地找到与用户查询最相关的文档部分

如果分块太大，可能包含太多不相关的信息，从而降低了检索的准确性。相反，分块太小可能会丢失必要的上下文信息，导致生成的回应缺乏连贯性或深度。

在RAG系统中实施合适的文档分块策略，旨在找到这种平衡，确保信息的完整性和相关性。一般来说，理想的文本块应当在没有周围上下文的情况下对人类来说仍然有意义，这样对语言模型来说也是有意义的。

分块方法的选择：

固定大小的分块:这是最简单和直接的方法，我们直接设定块中的字数，并选择块之间是否重复内容
通常，我们会保持块之间的一些重叠，以确保语义上下文不会在块之间丢失。与其他形式的分块相比，固定大小分块简单易用且不需要很多计算资源。
内容分块

顾名思义，根据文档的具体内容进行分块，例如根据标点符号（如句号）分割。或者直接使用更高级的NLTK或者spaCy库提供的句子分割功能。
递归分块：在大多数情况下推荐的方法。

其通过重复地应用分块规则来递归地分解文本
例如，在langchain中会先通过段落换行符(\n\n)进行分割。然后，检查这些块的大小。如果大小不超过一定阈值，则该块被保留。对于大小超过标准的块，使用单换行符(\n)再次分割。以此类推，不断根据块大小更新更小的分块规则(如空格，句号)。这种方法可以灵活地调整块的大小。例如，对于文本中的密集信息部分，可能需要更细的分割来捕捉细节;而对于信息较少的部分，则可以使用更大的块。而它的挑战在于，需要制定精细的规则来决定何时和如何分割文本。
从小到大分块

既然小的分块和大的分块各有各的优势，一种更为直接的解决方案是把同一文档进行从大到小所有尺寸的分割，然后把不同大小的分块全部存进向量数据库，并保存每个分块的上下级关系，进行递归搜索。但可想而知，因为我们要存储大量重复的内容，这种方案的缺点就是需要更大的储存空间。
特殊结构分块
针对特定结构化内容的专门分割器。这些分割器特别设计来处理这些类型的文档，以确保正确地保留和理解其结构。
langchain提供的特殊分割器包括：Markdown文件，Latex文件，以及各种主流代码语言分割器。
分块大小的选择

上述方法中无一例外最终都需要设定一个参数——一块的大小，那么我们如何选择呢?
首先不同的嵌入模型有其最佳输入大小。比如Openai的text-embedding-ada-002的模型在256或512大小的块上效果更好。
其次，文档的类型和用户查询的长度及复杂性也是决定分块大小的重要因素。处理长篇文章或书籍时，较大的分块有助于保留更多的上下文和主题连贯性；而对于社交媒体帖子，较小的分块可能更适合捕捉每个帖子的精确语义。如果用户的查询通常是简短和具体的，较小的分块可能更为合适;相反，如果查询较为复杂，可能需要更大的分块。
实际场景中，我们可能还是需要不断实验调整，在一些测试中，128大小的分块往往是最佳选择，在无从下手时，可以从这个大小作为起点进行测试。
4.2 嵌入模型阶段
我们提到过嵌入模型能帮助我们把文本转换成向量，显然不同的嵌入模型带来的效果也不尽相同，例如，Word2Vec模型，尽管功能强大，但存在一个重要的局限性：其生成的词向量是静态的。一旦模型训练完成，每个词的向量表示就固定不变，这在处理一词多义的情况时可能导致问题。
语义完全不一样的词向量却是固定的。相比之下，引入自注意力机制的模型，如BERT，能够提供动态的词义理解。这意味着它可以根据上下文动态地调整词义，使得同一个词在不同语境下有不同的向量表示。
有些项目为了让模型对特定垂直领域的词汇有更好的理解，会嵌入模型进行微调。但在这里我们并不推荐这种方法，一方面其对训练数据的质量有较高要求，另一方面也需要较多的人力物力投入，且效果未必理想，最终得不偿失。
在这种情况下，对于具体应该如何选择嵌入模型，推荐参考HuggingFace推出的嵌入模型排行榜MTEB（https://huggingface.co/spaces/mteb/leaderboard）。这个排行榜提供了多种模型的性能比较，能帮助我们做出更明智的选择。同时，要注意并非所有嵌入模型都支持中文，因此在选择时应查阅模型说明。
目前SOTA表现是北大和腾讯团队开源的Conan embedding
C-MTEB（Chinese Massive Text Embedding Benchmark）：中文海量文本嵌入测试基准
4.3 向量数据库阶段（元数据）
当在向量数据库中存储向量数据时，某些数据库支持将向量与元数据（即非向量化的数据）一同存储、为向量添加元数据标注是一种提高检索效率的有效策略，它在处理搜索结果时发挥着重要作用。
例如，日期就是一种常见的元数据标签。它能够帮助我们根据时间顺序进行筛选。设想一下，如果我们正在开发一款允许用户查询他们电子邮件历史记录的应用程序。在这种情况下，日期最近的电子邮件可能与用户的查询更相关。然而，从嵌入的角度来看，我们无法直接判断这些邮件与用户查询的相似度。通过将每封电子邮件的日期作为元数据附加到其嵌入中，我们可以在检索过程中优先考虑最近日期的邮件，从而提高搜索结果的相关性。
此外，我们还可以添加诸如章节或小节的引用，文本的关键信息、小节标题或关键词等作为元数据。这些元数据不仅有助于改进知识检索的准确性，还能为最终用户提供更加丰富和精确的搜索体验。
4.4 查询索引阶段（检索找回、重排）
多级索引：

元数据无法充分区分不同上下文类型的情况下，我们可以考虑进一步尝试多重索引技术
多重索引技术的核心思想是将庞大的数据和信息需求按类别划分，并在不同层级中组织，以实现更有效的管理和检索。
这意味着系统不仅依赖于单一索引，而是建立了多个针对不同数据类型和查询需求的索引。
例如，可能有一个索引专门处理摘要类问题，另一个专门应对直接寻求具体答案的问题，还有一个专门针对需要考虑时间因素的问题。这种多重索引策略使RAG系统能够根据查询的性质和上下文，选择最合适的索引进行数据检索，从而提升检索质量和响应速度。
不过为了引入多重索引技术，我们还需配套加入多级路由机制。多级路由机制确保每个查询被高效引导至最合适的索引。查询根据其特点（如复杂性、所需信息类型等）被路由至一个或多个特定索引。这不仅提升了处理效率，还优化了资源分配和使用，确保了对各类查询的精确匹配。
例如，对干查询最新上映的科幻电影推荐，RAG系统可能首先将其路由至专门处理当前热点话题的索引，然后利用专注于娱乐和影视内容的索引来生成相关推荐。
总的来说，多级索引和路由技术可以进一步帮助我们对大规模数据进行高效处理和精准信息提取，从而提升用户体验和系统的整体性能。
索引或查询算法：

我们可以利用索引筛选数据，但说到底我们还是要从筛选后的数据中检索出相关的文本向量。
由于向量数据量庞大且复杂，寻找绝对的最优解变得计算成本极高，有时甚至是不可行的。加之，大模型本质上并不是完全确定性的系统，这些模型在搜索时追求的是语义上的相似性——一种合理的匹配即可。从应用的角度来看，这种方法是合理的。
例如，在推荐系统中，用户不太可能察觉到或关心是否每个推荐的项目都是绝对的最佳匹配
他们更关心的是推荐是否总体上与他们的兴趣相符
因此查找与查询向量完全相同的项通常不是目标，而是要找到足够接近或相似的项，这便是最近邻搜索（ApproximateNearest Neighbor Search，ANNS）。这样做不仅能满足需求，还为检索优化提供了巨大的优化潜力。
常用算法：
聚类：参数选择（如簇数）
位置敏感哈希：
沿着缩小搜索范围的思路（束搜索）
在传统哈希算法中，我们通常希望每个输入对应唯一输出，并努力减少输出的重复
然而，在位置敏感哈希中，目标恰恰相反，我们需要增加输出值碰撞的概率
这种碰撞正是分组的关键，哈希值相同的向量进入同一个组（桶），此外，哈希函数还需满足一个条件：空间上距离相近的向量更有可能分入同一个桶，这样在搜索时，只要获取目标向量的哈希值，找到相应的桶进行搜神记即可。
量化乘积：
上面我们介绍了两种牺牲搜索质量来提高搜索速度的方法，但除了搜索速度外，内存开销也是一个巨大挑战。
在实际应用场景中，每个向量往往都有上千个维度，数据数量可达上亿。每条数据都对应着一个实际的的信息，因此不可能删除数据来减少内存开销，那唯一的选择只能是把每个数据本身大小缩减。
图像有一种有损压缩的方法是把一个像素周围的几个像素合并（Superpixel），来减少需要储存的信息。同样我们可以在聚类的方法之上改进一下，用每个簇的中心点来代替簇中的数据点。虽然这样我们会丢失向量的具体值信息，但考虑到聚类中心点和簇中向量相关程度，再加上可以不断增加簇的数量来减少信息损失，所以很大程度上我们可以保留原始点的信息。而这样做带来的好处是十分可观的。
如果我们给这些中心点编码，我们就可以用单个数字储存一个向量来减少存储的空间。而我们把每个中心向量值和他的编码值记录下来形成一个码本，这样每次使用某个向量的时候，我们只需用他的编码值通过码本找到对应的的中心向量的具体值
虽然这个向量已经不再是当初的样子了，但就像上面所说，问题不大。而这个把向量用其所在的簇中心点表示的过程就是量化。
分层导航小世界：
从客户的角度来看，内存开销可能并不是最重要的考量因素。他们更加关注的是应用的最终效果，也就是回答用户问题的速度和质量。
导航小世界（Navigable Small World，NSW）算法正是这样一种用内存换取更快速度和更高质量的实现方式
这个算法的思路和六度分割理论类似——你和任何一个陌生人之间最多只隔六个人，也就是说，最多通过六个人你就能够认识任何一个陌生人。
我们可以将人比作向量点，把搜索过程看作是从一个人找到另一个人的过程。在查询时，我们从一个选定的起始点A开始，然后找到与A相邻且最接近查询向量的点B，导航到B点，再次进行类似的判断，如此反复，直到找到一个点C，其所有相邻节点都没有比它更接近目标。最终这个点C便是我们要找的最相似的向量。
查询转换：

在RAG系统中，用户的查询问题被转化为向量，然后在向量数据库中进行匹配。不难想象，查询的措辞会直接影响投索结果。

如果搜索结果不理想，可以尝试以下几种方法对问题进行重写，以提升召回效果:

a. 结合历史对话的重新表述

在向量空间中，对人类来说看似相同的两个问题其向量大小并不一定很相似。我们可以直接利用LLM 重新表述问题来进行尝试。
此外，在进行多轮对话时，用户的提问中的某个词可能会指代上文中的部分信息，因此可以将历史信息和用户提问一并交给LLM重新表述。
b. 假设文档嵌入

假设文档嵌入（Hypothetical DocumentEmbedding，HyDE）的核心思想是:
接收用户提问后，先让LLM在没有外部知识的情况下生成一个假设性的回复。
然后，将这个假设性回复和原始查询一起用于向量检索。
假设回复可能包含虚假信息，但蕴含着LLM认为相关的信息和文档模式，有助于在知识库中寻找类似的文档。
主要关注点：通过为传入查询生成一个假想文档，从而增强和改善相似性搜索。
c. 退后提示

如果原始查询太复杂或返回的信息太广泛，可以选择生成一个抽象层次更高的“退后“问题，与原始问题一起用于检索，以增加返回结果的数量。这就是退后提示（Step BackPrompting）的思想。
例如，原问题是张三 在 1954年8月至 1954年 11月期间去了哪所学校?，这类问题对于 LLM 来说很容易答错。但是如果后退一步，站在更高层次对问题进行抽象，提出一个新的问题：**张三的教育历史是怎样的?**那LLMs可以先将张三都列出来，然后将这些信息和原始问题放在一起，那么对于 LLM 来说就可以很容易给出正确的答案。
d.多查询检索/多路召回

多查询检索/多路召回（Multi-Query Retrieval）也是一种不错的方法。
使用LLM生成多个搜索查询，特别适用于一个问题可能需要依赖多个子问题的情况。
检索参数：

终于我们把查询问题准备好了，可以进入向量数据库进行检索。在具体的检索过程中，我们可以根据向量数据库的特定设置来优化一些检索参数，以下是一些常见的可设定参数：
稀疏和稠密搜索权重

稠密搜索即通过向量进行搜索。然而，在某些场景下可能存在限制，此时可以尝试使用原始字符串进行关键字匹配的稀疏搜索。

一种有效的稀疏搜索算法是最佳匹配25(BM25)，它基于统计输入短语中的单词频率，频繁出现的单词得分较低，而稀有的词被视为关键词，得分会较高。我们可以结合稀疏和稠密搜索得出最终结果。

向量数据库通常允许设定两者对最终结果评分的权重比例，如langchain的某个参数，0.6表示40%的得分来自稀疏搜索，60%来自稠密搜索。

结果数量（topk）

检索结果的数量是另一个关键因素。

足够的检索结果可以确保系统覆盖到用户查询的各个方面。在回答多方面或复杂问题时，更多的结果提供了丰富的语境，有助于RAG系统更好地理解问题的上下文和隐含细节，

但需注意，结果数量过多可能导致信息过载，降低回答准确性并增加系统的时间和资源成本

相似度度量方法

计算两个向量相似度的方法也是一个可选参数。这包括使用欧式距离或Jaccard距离计算两个向量的差异，以及利用余弦相似度衡量夹角的相似性。
通常，余弦相似度更受青睐，因为它不受向量长度的影响，只反映方向上的相似度。这使得模型能够忽略文本长度差异，专注于内容的语义相似性。
需要注意的是，并非所有嵌入模型都支持所有度量方法，具体可参考所用嵌入模型的说明。
高级检索策略：

终于我们来到最为关键和复杂的步骤——在向量数据库检索之上如何具体开发或改进整个系统的策略，这部分的内容足够写成一篇独立文章。为了保持简洁，我们只讨论一些常用或者新提出的策略。

a. 上下文压缩：

我们提到过当文档文块过大时，可能包含太多不相关的信息，传递这样的整个文档可能导致更昂贵的LLM调用和更差的响应。
上下文压缩的思想就是通过LLM的帮助根据上下文对单个文档内容进行压缩，或者对返回结果进行一定程度的过滤仅返回相关信息。
b. 句子窗口搜索

相反，文档文块太小会导致上下文的缺失。

其中一种解决方案就是窗口搜索，该方法的核心思想是当提问匹配好分块后，将该分块周围的块作为上下文一并交给LLM进行输出。来增加LLM对文档上下文的理解

c. 父文档搜索

无独有偶，父文档搜索也是一种很相似的解决方案，父文档搜索先将文档分为尺寸更大的主文档，再把主文档分割为更短的子文档两个层级，用户问题会与子文档匹配，然后将该子文档所属的主文档和用户提问发送给LLMs。
d. 自动合并

自动合并是在父文档搜索上更进一步的复杂解决方案。
同样地，我们先对文档进行结构切割，比如将文档按三层树状结构进行切割，顶层节点的块大小为1024，中间层的块大小为512，底层的叶子节点的块大小为128。
而在检索时只拿叶子节点和问题进行匹配，当某个父节点下的多数叶子节点都与问题匹配上则将该父节点作为结果返回。
e. 多向量检索

多向量检索同样会给一个知识文档转化成多个向量存入数据库，不同的是，这些向量不仅包括文档在不同大小下的分块，还可以包括该文档的摘要，用户可能提出的问题等，有助于检索的信息。
在使用多向量查询的情况下，每个向量可能代表了文档的不同方面，使得系统能够更全面地考虑文档内容，并在回答复杂或多方面的查询时提供更精确的结果。
例如，如果查询与文档的某个具体部分或摘要更相关，那么相应的向量就可以帮助提高这部分内容的检索排名。
f. 多代理检索

多代理检索，简而言之就是选取我们提及的12大优化策略中的部分交给一个智能代理合并使用。

就比如使用子问题查询，多级索引和多向量查询结合，
先让子问题查询代理把用户提问拆解为多个小问题，再让文档代理对每个字问题进行多向量或多索引检索，最后排名代理将所有检索的文档总结再交给LLM。
这样做的好处是可以取长补短。比如，子问题查询引擎在探索每个子查询时可能会缺乏深度，尤其是在相互关联或关系数据中。相反，文档代理递归检索在深入研究特定文档和检索详细答案方面表现出色，以此来综合多种方法解决问题。
需要注意的是现在网络上存在不同结构的多代理检索，具体在多代理选取哪些优化步骤尚未有确切定论，我们可以结合使用场景进行探索。

g. Self-RAG（左右互搏）

自反思搜索增强是一个新的RAG框架，其与传统RAG最大的区别在于通过检索评分(令牌)和反思评分(令牌)来提高质量。

它主要分为三个步骤：检索、生成和批评。

SeIf-RAG首先用检索评分来评估用户提问是否需要检索，如果需要检索，LLM将调用外部检索模块查找相关文档。

接着，LLM分别为每个检索到的知识块生成答案，
然后为每个答案生成反思评分来评估检索到的文档是否相关,
最后将评分高的文档当作最终结果一并交给LLM。

重排模型：

在完成语义搜索的优化步骤后，我们能够检索到语义上最相似的文档，但不知你是否注意到一个关键问题：语义最相似是否总代表最相关？答案是不一定。

例如，当用户查询最新上映的科幻电影推荐时，可能得到的结果是科幻电影的历史演变，虽然从语义上这与科幻电影相关，但并未直接回应用户关于最新电影的查询。
重排（Re-ranking）模型可以帮助我们缓解这个问题，重排模型通过对初始检索结果进行更深入的相关性评估和排序，确保最终展示给用户的结果更加符合其查询意图。

该过程会考虑更多的特征，如查询意图、词汇的多重语义、用户的历史行为和上下文信息等。

举个例子，对于查询最新上映的科幻电影推荐，在首次检索阶段，系统可能基于关键词返回包括科幻电影的历史文章、科幻小说介绍、最新电影的新闻等结果。
然后，在重排阶段，模型会对这些结果进行深入分析，并将最相关、最符合用户查询意图的结果（如最新上映的科幻电影列表的评论或推荐）排在前面，同时将那些关于科幻电影历史或不太相关的内容排在后面。
这样，重排模型就能有效提升检索结果的相关性和准确性，更好地满足用户的需求。
在实践中，使用RAG构建系统时都应考虑尝试重排方法，以评估其是否能够提高系统性能。

4.5 生成回答阶段（提示工程）
提示词：

LLMs的解码器部分通常基于给定输入来预测下一个词。

这意味着设计提示词或问题的方式将直接影响模型预测下一个词的概率。这也给了我们一些启示：通过改变提示词的形式，可以有效地影响模型对不同类型问题的接受程度和回答方式，比如修改提示语，让LLM知道它在做什么工作，是十分有帮助的。

为了减少模型产生主观回答和幻觉的概率，一般情况下，RAG系统中的提示词中应明确指出回答仅基于搜索结果，不要添加任何其他信息。例如，可以设置提示词如：

你是一名智能客服。你的目标是提供准确的信息，并尽可能帮助提问者解决问题。你应保持友善，但不要过于啰嗦。请根据提供的上下文信息，在不考虑已有知识的情况下，回答相关查询。

当然你也可以根据场景需要，也可以适当让模型的回答融入一些主观性或其对知识的理解。

此外，使用少量样本（few-shot）的方法，将想要的问答例子加入提示词中，指导LLM如何利用检索到的知识，也是提升LLM生成内容质量的有效方法。这种方法不仅使模型的回答更加精准，也提高了其在特定情境下的实用性。

大语言模型：

LLM是生成响应的核心组件。与嵌入模型类似，可以根据自己的需求选择LLM，例如开放模型与专有模型、推理成本、上下文长度等。
此外，可以使用一些LLM开发框架来搭建RAG系统，比如，Llamalndex或LangChain。这两个框架都拥有比较好用的debugging工具，可以让我们定义回调函数，查看使用了哪些上下文，检查检索结果来自哪个人档等等。
知识蒸馏相关
1. 什么是知识蒸馏
把大的教师模型的知识萃取出来，浓缩到一个小的学生模型中，就是大模型转为小模型

这里有一个知识迁移的过程，从教师网络迁移到学生网络上



2. 知识蒸馏的目的
深度学习在计算机视觉、语音识别、自然语言处理等内的众多领域中均取得了令人难以置信的性能。但是，大多数模型在计算上过于昂贵，无法在移动端或嵌入式设备上运行。因此需要对模型进行压缩，这样小的模型就适用于部署在终端设备上了，

提升模型精度：如果对目前的网络模型A的精度不是很满意，那么可以先训练一个更高精度的teacher模型B(通常参数量更多，时延更大)，然后用这个训练好的teacher模型B对student模型A进行知识蒸馏，得到一个更高精度的A模型。
降低模型时延，压缩网络参数：如果对目前的网络模型A的时延不满意，可以先找到一个时延更低，参数量更小的模型B。通常来讲，这种模型精度也会比较低，然后通过训练一个更高精度的teacher模型C来对这个参数量小的模型B进行知识蒸馏，使得该模型B的精度接近最原始的模型A、从而达到降低时延的目的。
标签之间的域迁移：假如使用狗和猫的数据集训练了一个teacher模型A，使用香蕉和苹果训练了一个teacher模型B、那么就可以用这两个模型同时蒸馏出-个可以识别狗、猫、香蕉以及苹果的模型，将两个不同域的数据集进行集成和过移。
3. 传统的知识蒸馏方法
根据蒸馏知识的不同，主要有两种类型：

基于反馈的知识蒸馏
基于特征的知识蒸馏
参考资料《大语言模型》赵鑫等著


基于反馈的知识蒸馏：

关注教师模型最后一层输出的logits，这些logits经过softmax变换后，可以用作学生模型的软标签进行学习
蒸馏损失函数为：L ( l t , l s ) = L R ( p t ( ⋅ ) , p s ( ⋅ ) ) \mathcal{L}(l_t,l_s)=L_R(p_t(\cdot),p_s(\cdot))L(l 
t
​
 ,l 
s
​
 )=L 
R
​
 (p 
t
​
 (⋅),p 
s
​
 (⋅))，其中：
l t l_tl 
t
​
 和l s l_sl 
s
​
 分别表示教师模型和学生模型的输出logits
L R L_RL 
R
​
 通常采用KL散度作为指标
p t p_tp 
t
​
 和p s p_sp 
s
​
 分别表示教师模型和学生模型的logits经过softmax变换后的概率值
核心就是让学生模型输出的logits去逼近教师模型输出的logits
基于特征的知识蒸馏：

与基于预测分布的蒸馏相比，基于中间特征表示的蒸馏关注教师模型中间层输出的激活值，并用这些激活值作为监督信息训练学生模型
例如在多层Transformer架构的模型中，每一层的输出特征都可以作为知识
相应的蒸馏损失就是：L ( f t ( x ) , f s ( x ) ) = L F ( Φ ( f t ( x ) ) , Φ ( f s ( x ) ) ) \mathcal{L}(f_t(x),f_s(x))=L_F(\Phi(f_t(x)),\Phi(f_s(x)))L(f 
t
​
 (x),f 
s
​
 (x))=L 
F
​
 (Φ(f 
t
​
 (x)),Φ(f 
s
​
 (x)))，这里的Φ \PhiΦ函数哟关于处理形状不匹配的情况的变换函数。
显然中间层特征包含更为丰富的信息，有助于模型蒸馏过程中实现更为有效的知识迁移
然而，这种方法也存在技术难点，如消除架构不一致的影响（两个模型架构不同），选哪些层的输出作为参考（目标层自动化选择）
4. 大语言模型的知识蒸馏方法
白盒模型蒸馏方法（开源），黑盒模型蒸馏方法（闭源）
白盒：可以获取模型权重来指导学生模型，典型方法为MINILLM，最大可将LLaMA的13B蒸馏到7B
黑盒：无法获取模型权重，只能使用输出信息来训练小模型。经典的方法主要是关注大模型的关键能力，如上下文学习能力，思维链推理能力，指令遵从能力


一篇综述：A Survey on Knowledge Distillation of Large Language Models（arxiv.2402.13116）

Knowledge Elicitation（知识提取）：标签、扩展、数据治疗，特征挖掘，反馈，自蒸馏（自己监督自己，学生教师合二为一）
蒸馏算法（右侧）：SFT，缩小差异性，增加相似性，强化学习，排序优化
5. 谈谈对模型量化的了解
量化（quantize）是一种优化深度学习模型的技术
其目标是减小模型的存储需求和计算复杂度，从而使模型在资源有限的设备上运行更高效
量化的主要方式：

使用更小的数据类型：fp32到bf16，int8
使用压缩算法：
利用Huffmann编码或其他压缩算法：Huffmann编码是一种基于贪心算法的无损数据压缩技术，用于最优地表示符号（字符）或数据流。它通过为频率较高地符号分配更短地编码，为频率较低地符号分配较长的编码，实现数据压缩
量化的优势：减少存储空间，内存占用，加快推理速度

量化的缺点：精度损失，权衡效率与精度

6. 模型压缩和加速的方法有哪些
可以有以下方法作为参考：

知识蒸馏（Knowledge Distillation）

原理：使用一个大模型（教师模型）的输出结果，指导小模型（学生型）的训练。
目标：保留大模型的知识，同时显著降低模型的复杂度和大小。
优点：提高小模型的泛化能力和推理速度。
参数剪枝（Parameter Pruning）

原理：通过分析模型中各参数对性能的贡献，删除那些兄余或贡献较小的参数，减小模型大小。
优势：有效减少存储需求，适合模型过大且训练过拟合的情况。
实现：例如，删除绝对值较小的权重。
网络剪枝（Network Pruning）

原理：通过分析神经网络结构，删除冗余或不重要的神经元。
区别：与参数剪枝不同，它针对神经元而非单个参数。
效果：进一步压缩模型，同时保持性能。
蒸馏对抗网络（Distillation Adversarial Networks）4.

原理：结合知识蒸馏和对抗训练，通过生成扰动样本，提高模型的鲁棒性和抗干扰能力
适用性：对需要高可靠性和稳定性的场景效果显著
层次化剪枝（Layer-wise Pruning）

原理：根据每一层对整体性能的影响，针对不同层设置不同程度的剪枝策略。
优势：避免对关键层的过度剪枝，提升剪枝效率
低秩分解（Low-Rank decomposition）:

原理：将较大的矩阵分解为几个小矩阵的乘积，减少参数量和计算开销
典型应用：矩阵分解在全连接层和卷积层尤为常用
卷积分解：

原理：将复杂的卷积操作分解为简单的计算模块，如深度可分离卷积。
优势：大幅提升模型推理速度，适合实时任务
这些方法各有侧重，可结合实际需求选择合适的技术，比如边缘设备倾向量化和剪枝，大规模服务场景适合低秩分解和知识蒸馏

微调相关
从总体上看，大模型的训练可以分为四个关键阶段:预训练、有监督微调、奖励建模和强化学习。

训练的四个关键阶段：
预训练：这是整个训练过程的核心和最耗时的部分，占据了99%的资源。需要大规模的计算能力（如超级计算机或大型GPU集群）和海量数据（例如文本语料库）。预训练的目的是让模型学习语言的基本规则、语义和上下文关系。由于资源需求巨大，普通开发者难以独立完成这一步。
有监督微调：基于预训练模型，通过提供带标签的数据（例如问题和正确答案），让模型学会执行具体任务。
奖励建模：创建一个模型（奖励模型）来评估生成结果的质量，并指导主模型朝更优质的方向优化。
强化学习：通常采用强化学习（如人类反馈的强化学习，RLHF）。通过奖励信号进一步优化模型，使其更贴近用户需求。
2.资源需求对比：

预训练：极高的硬件和计算成本，以及长时间的运行。
微调阶段（有监督微调、奖励建模、强化学习）：相对轻量，仅需几块GPU和较短的时间（几小时到几天）。
3.微调的核心目标：微调是为了在预训练模型的基础上，针对特定任务（如写文章、回答问题）进一步优化模型性能。通过调整模型的参数，可以让它更加准确地完成具体任务。

1. 什么时候需要对大模型进行微调
微调(Fine-tuning)的需求主要取决于两个方面:模型现有表现是否达标 和 任务的具体需求。具体来说，当以下场景出现时，可以考虑对大型语言模型(LLM)进行微调:

任务复杂度高，情境学习效果不足

**情境学习(in-context Learning)**是通过在提示中加入任务示例，让模型更好理解任务需求。虽然这种方法灵活且不需.要更新模型权重，但有时模型对复杂任务的理解力不足，
对较小规模的模型，这种方法的效果有限
如果单靠调整提示不能显著提升性能，就需要进一步优化模型。.
零样本或少样本推理效果欠佳

零样本推理(Zero-shot Inference)：模型仅根据问题上下文和提示进行推理，不依赖任何示例。虽然适合通用任务，但对于专业任务，模型可能难以理解语境或任务逻辑。
少样本推理(Few·shotInference)：在提示中加入一到多个示例，帮助模型更精准地生成期望的输出。如果这种方式仍然无法满足准确性或一致性需求，微调成为更有效的选择。
领域或任务需求高度专业化

预训练的大型语言模型(LLM)设计通用，覆盖广泛领域。但在以下情况下，模型可能需要微调以提升特定任务表现:
涉及专业术语、领域知识(如法律、医学、工程)。
需要模型对高度特定的输出格式或逻辑规则保持一致性。
某些任务需要高精度、低错误率，例如客户服务、医学诊断、自动化文档处理。
输出结果不符合用户需求

即使通用模型输出具有一定准确性，但在用户偏好或特定任务中可能不够符合要求。例如：
输出风格、语气不匹配，
需要更个性化或品牌化的结果。
总结：当情境学习和零样本、单样本或少样本推理不能满足需求，或者需要在特定任务和领域中提升模型表现时，微调是有效策路。通过有监督学习过程，微调能显著提高模型在特定任务上的准确性和可靠性。

2. LLMs中微调方法有哪些
微调技术可以分为全量微调（FFT）和PEFT（参数高效微调）

下表展示了在一张A100 GPU（80G显存）以及CPU内存64GB以上硬件上进行模型全量微调以及PEFT对于CPU和GPU的消耗情况



全量微调会损失多样性，存在灾难性遗忘问题
微调策略方面，有SFT（监督微调）和**RLHF（人类反馈强化学习）**两种
SFT的主要技术：
基本超参数调整
迁移学习
多任务学习
少样本学习
任务特定微调
RLHF主要技术：
奖励建模
邻近策略优化（PPO）：在确保策略更新平稳的情况下优化模型行为
比较排名：通过人类评估不同输出的优劣，来优化模型
偏好学习：从人类偏好中学习，优化输出
参数高效微调：最小化训练参数数量，提高特定任务性能
3. 主流PEFT的方法有哪些
主要是Adapter，Prefix Tuning和LoRA三大类。各具特点，在模型结构中所嵌入的位置也有所不同



图来自论文：TOWARDS A UNIFED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING（arxiv.2110.04366）

Adapter类：
PEFT 技术通过在预训练模型的各层之间插入较小的神经网络模块，这些新增的神经模块被称为“适配器"，在进行下游任务的微调时，只需对适配器参数进行训练便能实现高效微调的目标。
此基础上衍生出了AdapterP、Parallel等高效微调技术
Prefix Tuning类：
PEFT 技术通过在模型的输入或隐层添加k kk个额外可训练的前缀标记，模型微调时只训练这些前缀参数便能实现高效微调的目标。
在此基础上衍生出了P-Tuning、P-Tuningv2等高效微调技术;
LoRA类：
PEFT 技术则通过学习小参数的低秩矩阵来近似模型权重矩阵W的参数更新，微调训练时只需优化低秩矩阵参数便能实现高效微调的目标。
在此基础上衍生出AdaLORA、QLORA等高效微调技术
4. Adapters类微调
论文：Parameter-Efficient Transfer Learning for NLP，发表于2019年，当时主要是基于BERT改进（arxiv.1902.00751）

背景：

预训练模型参数量越来越多，在训练下游任务时进行全量微调变得昂贵且费时
基于此，提出Adapter Tuning，Adapter在预训练模型每层中插入用于下游任务的参数（针对每个下游任务，仅增加3.6%的参数），在微调时将模型主体冻结，仅训练特定于任务的参数，从而减少了训练时的算力开销。
Adapter Tuning 主要思想:

作者设计了一种新的Adapter结构，并将其嵌入Transformer的结构里面

针对每一个Transformer层，增加了两个Adapter结构（分别是多头注意力的投影之后和第二个feed-forward层之后）

在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter 结构和Layer Norm 层进行微调，从而保证了训练的高效性。每当出现新的下游任务，通过添加Adapter模块来产生一个易于扩展的下游模型，从而避免全量微调与灾难性遗忘的问题。



适配器模块的结构及其在Transformer中的集成方式
左图：我们在每一层Transformer中两次插入适配器模块，分别位于多头注意力机制后的投影操作之后，以及两个前馈层之后
右图：适配器模块的核心是一个参数较少的瓶颈结构，相较于原始模型中的注意力和前馈层，它的参数量非常少。此外，适配器模块中还包含一个跳跃连接（skip-connection）。在适配器微调阶段，绿色部分的层会基于下游任务的数据进行训练，包括适配器模块、层归一化参数，以及最终的分类层（未在图中显示）。
具体细节：

每个 Adapter 模块主要由两个前馈（Feed forward）子层组成，

第一个前馈子层（down-project）将Transformer块的输出作为输入，将原始输入维度d（高维特征)投影到m（低维特征）通过控制m的大小来限制Adapter模块的参数量，通常情况下，m<<d

然后，中间通过一个非线形层（Nonlinearity）。

在输出阶段，通过第二个前馈子层(up·project)还原输入维度，将m（低维特征）重新映射回d（原来的高维特征），作为Adapter模块的输出。。

同时，通过一个跳跃连接(skip·connection)来将Adapter的输入重新加到最终的输出中去，这样可以保证，即便 Adapter一开始参数初始化接近0，Adapter也由于skip connection的设置而接近于一个恒等映射，从而确保训练的有效性。

通过实验发现，只训练少量参数的Adapter方法的效果可以媲美全量微调，这也验证了Adapter是一种高效的参数训练方法，可以快速将语言模型的能力迁移到下游任务中去。

Adapter通过引入0.5%~5%的模型参数可以达到不落后全量微调模型1%的性能

Adapter类的其他微调方法：

Adapter Fusion：
通过将Adapter的训练分为知识提取和知识组合两部分，解决了灾难性遗忘、任务间干扰和训练不稳定的问题
但是，Adapter模块的添加也导致模型整体参数量的增加，降低了模型推理时的性能
Adapter Drop：
通过从较低的Transformer层删除可变数量的Adapter来提升推理速度（删除无关的Adapters）。当对多个任务执行推理时，动态地减少了运行时的计算开销，并保持任务性能。
实战中用的并不多，但也要做一些了解。

5. Prefix类微调
Prefix类微调的几种方法：

Prefix Tuning：

在Prefix Tuning之前的工作主要是人工设计离散的模版或者自动化搜索离散的模版。

对于人工设计的模版，模版的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化。

而对于自动化搜索模版，成本也比较高

同时，以前这种离散化的token搜索出来的结果可能并不是最优的。

除此之外，传统的微调范式利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权重，一方面微调整个模型耗时长，另一方面也会占很多存储空间。

技术原理：

基于上述两点，Prefix Tuning（论文：prefix-Tuning: Optimizing Continuous Prompts for Generation）提出固定预训练语言模型，为语言模型添加可训练，任务特定的前缀，这样就可以为不同任务保存不同的前缀，微调成本也小。

Prefix Tuning，在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM中的其他部分参数固定。



上图：微调（顶部）更新所有Transformer参数（红色的Transformer框），并要求为每个任务存储一个完整的模型副本
下图：Prefix Tuning提出了前缀调优，它冻结了Transformer参数，只优化了前缀（红色前缀块），因此只需要存储每个任务的前缀，使前缀调优模块化节约空间
Prompt Tuning：

大模型全量微调对每个任务训练一个模型，开销和部署成本都比较高。

同时，离散的 prompts 方法，成本比较高，并且效果不太好

除此之外，之前的 Prefix Tuning 在更新参数的时候还是有些复杂。

技术原理：

基于此，作者提出了Prompt Tuning，通过反向传播更新参数来学习prompts，而不是人工设计prompts，同时冻结模型原始权重，只训练prompts参数，训练完以后，用同一个模型可以做多任务推理。

Prompt Tuning（论文：The Power of Scale for Parameter-Efficient PromptTuning），该方法可以看作是Prefix Tuning的简化版本

它给每个任务定义了自己的prompt，然后拼接到数据上作为输入，但只在输入层加入prompt tokens，并且不需要加入多层感知器（MLP）进行调整来解决难训练的问题。



Model Tuning 需要为每个下游任务制作整个预训练模型的任务特定副本，并且必须分批进行推理。

Prompt Tuning 只需要为每个任务存储一个特定于任务的小提示，并使用原始预训练模型进行混合任务推理。

通过实验发现，随着预训练模型参数量的增加，Prompt Tuning的方法会逼近全参数微调的结果。



T5经过调优后模型可以实现不错的性能，但弊端是需要为每个最终任务存储单独微调后的模型
随着模型参数的增加，对T5的 prompt tuning与model tuning 的能力差不多。
该方法明显优于使用GPT-3 few-shot prompt 设计
Prefix Tuning和Prompt Tuning在微调上有哪些区别？

PromptTuning 和 Prefix Tuning,都是在自然语言处理任务中对预训练模型进行微调的方法，但它们在实现细节和应用场景上有所不同。
以下是它们之间的主要区别:
参数更新位置：Prompt Tuning通常只在输入层添加参数，而Prefix Tuning在每一层都添加了参数。
参数数量：Prefix Tuning 通常比 Prompt Tuning 有更多的可学习参数（因为它为模型的每一层都添加了前缀）
适用任务：Prompt Tuning 更适合于分类任务，而 Prefix Tuning 更适合于生成任务（因为它可以在不同层次上调整模型的行为）
训练效率：Prompt Tuning 通常有更高的训练效率
P-tuning：

P-tuning 方法的提出同样是为了解决之前提到的两个问题：大模型的Prompt构造方式严重影响下游任务的效果，

比如：GPT-3采用人工构造的模版来做上下文学习（in-context learning），但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。
近期，自动化搜索模版工作成本也比较高，以前这种离散化的token的搜索出来的结果可能并不是最优的，导致性能不稳定。

技术原理：

基于此，作者提出了P-Tuning（论文：GPTUnderstands, Too），设计了一种连续可微的virtual token。

该方法将Prompt转换为可以学习的Embedding层，并对Prompt Embedding进行一层处理。



一个快速搜索**英国首都是[MASK]**的例子：
图示中，颜色代表内容：

上下文（蓝色区域，英国）

目标（红色区域，[MASK]）

橙色区域指的是提示，

在左图(a)中，提示生成器仅接收离散奖励；相反，在**右图(b)**中，连续提示嵌入和提示编码器可以以可微的方式进行优化。
相比Prefix Tuning，P-Tuning加入了可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token

P-tuning v2：

之前的Prompt Tuning和P-Tuning等方法存在两个主要的问题：

第一，缺乏模型参数规模和任务通用性。

缺乏规模通用性：Prompt Tuning论文中表明当模型参数规模超过10B时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。
缺乏任务普遍性：尽管Prompt Tuning和P-tuning在一些NLU基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。
第二，缺少深度提示优化。

在Prompt Tuning和P-tuning中，连续提示只被插入transformer第一层的输入embedding序列中。在接下来的transformer层中，插入连续提示的位置的embedding是由之前的transformer层计算出来的，这可能导致两个可能的优化挑战：

由于序列长度的限制，可调参数的数量是有限的
输入embedding对模型预测只有相对间接的影响
考虑到这些问题，作者提出了P-tuningv2，它对PromptTuning和P-Tuning进行改进，作为一个跨规模和NLU任务的通用解决方案。

**技术原理：**P-Tuning v2（论文：P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks）方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这带来两个方面的好处：

更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%）
加入到更深层结构中的Prompt能给模型预测带来更直接的影响。


从P-tuning到P-tuning v2的变化:

橙色块（即h 0 h_0h 
0
​
 , …, h i h_ih 
i
​
 ）表示可训练的 prompt embeddings

蓝色块是有冻结的预训练语言模型存储或计算的embeddings

P-Tuningv2是一种在不同规模和任务中都可与微调相媲美的提示方法。P-Tuning v2对从330M到10B的模型显示出一致的改进，并在序列标注等困难的序列任务上以很大的幅度超过了PromptTuning和P-Tuning


SuperGLUE的平均得分：使用0.1%的任务特定参数，P-tuning v2可以在预训练模型的大范围内进行微调，而 P-tuning 可以在10B的范围内进行有条件的微调。
Prefix类微调方法总结：

Prefix Tuning

在每一个Transformer层都带上一些virtual token作为前缓，以适应不同的任务。
优化多层prefix
与fine-tuning比肩
Prompt Tuning

该方法可以看着是Prefx Tunine的简化版本，针对不同的任务，仅在输入层引入virtualtoken形式的软提示(soft prompt)
优化单层prefix
大尺寸模型下与fine-tuning比肩
P-Tuning

将 Prompt 转换为可以学习的Embedding层。相比Prefix Tuning；仅在输入层加入的可微的vitualtoken;另外，virtual token的位置也不一定是前缀，插入的位置是可选的，
优化单层prefix
大尺寸模型下与fine-tuning比肩
P-Tuning v2

该方法在每一个Transformer层都加入了prompt token作为输入，引入多任务学习，针对不同任务采用不同的提示长度,
优化多层prefix
小尺寸和大尺寸模型下均与fine-tuning比肩
Prefix类微调方法介绍

Prefix Tuning

在每一个Transformer层都带上一些virtual token作为前缓，以适应不同的任务。
优化多层prefix
与fine-tuning比肩
Prompt Tuning

该方法可以看着是Prefx Tunine的简化版本，针对不同的任务，仅在输入层引入virtualtoken形式的软提示(soft prompt)
优化单层prefix
大尺寸模型下与fine-tuning比肩
P-Tuning

将 Prompt 转换为可以学习的Embedding层。相比Prefix Tuning；仅在输入层加入的可微的vitualtoken;另外，virtual token的位置也不一定是前缀，插入的位置是可选的，
优化单层prefix
大尺寸模型下与fine-tuning比肩
P-Tuning v2

该方法在每一个Transformer层都加入了prompt token作为输入，引入多任务学习，针对不同任务采用不同的提示长度,
优化多层prefix
小尺寸和大尺寸模型下均与fine-tuning比肩
PrefixTuning (2021.01)

论文题目:Prefix-Tuning: Optimizing Continuous Prompts for Generation
论文地址:hups://arxiv.org/pdf/2101.00190.pdf
论文源码:https://github,comxiangLi1999/PrefixTuning
P-tuning(2021.03)

论文题目:GPT Understands,Too
论文地址:https://arxiv,org/pdf/2103.10385.pdf
论文源码:https://github,com/THUDM/P-tuning
Prompt Tuning(2021.09)

论文题目:The Power of Scale for Parameter-Efficient Prompt Tuning
论文地址:https://arxiv,org/pdf/2104.08691.pdf
论文源码:https://github.com/google-research/prompt-tuning
P-tuning-v2(2022.03)

论文题目: P-Tuning v2: Prompt Tuning Can Be Comparable to finetuning Universally Across Scales and Tasks
论文地址:htps://arxiv,org/pdf/2110.07602.pdf
论文源码:https://github.com/THUDM/P-tuning-v2
6. LoRA相关问题
LoRA应该作用于Transformer的哪个参数矩阵（Q, K, V）？

答：根据之前2110.04366里的图，应该是作用于Q和K



拓展：

什么是rank？
在机器学习中，rank通常指矩阵的秩，表示矩阵中线性独立行或列的数量
在LoRA方法中，rank用于限制可训练参数的数量，通过低秩表示来高效调整模型。（本质上是通过将高维网络层改写为几个低维的网络层的迭加，矩阵表示类似于几个低秩矩阵的乘积）
通过降低秩，减少参数空间的自由度，使得模型在训练时更加高效，而不会显著影响性能，这是LoRA的核心思想。
上表中，当提到rank为8或4时，表示的是原始大的权重矩阵进行近似表示时所用的线性独立向量的数量，从而调整的可训练参数的数量。

表中部分内容的介绍：

WikiSQL：专注于SQL生成任务的数据集，提供自然语言问题和SQL查询的对照
MultiNLI：用于NLP推理的数据集，包含不同体裁的文本对，任务是判断一个给定前提和假设之间的关系，比如蕴含、矛盾还是中立。这个数据集用于评估模型在跨领域推理任务中的表现
几个权重矩阵：W q , W k , W v , W o W_q,W_k,W_v,W_oW 
q
​
 ,W 
k
​
 ,W 
v
​
 ,W 
o
​
 ，分别是生成Q , K , V Q,K,VQ,K,V向量的权重矩阵，最后一个W o W_oW 
o
​
 是将多头注意力的输出组合起来的输出投影权重矩阵
表中内容的翻译：

在对GPT3的不同注意权重应用LoRA后，WikiSQL和MultiNLI数据集上的验证准确性。这里使用的是相同数量的可训练参数，调整W q W_qW 
q
​
 和W v W_vW 
v
​
 一起提供了最佳的整体性能。
需要注意的是，仅调整Δ W q \Delta W_qΔW 
q
​
 或Δ W k \Delta W_kΔW 
k
​
 会导致性能显著下降，而同时调整Δ W q \Delta W_qΔW 
q
​
 和Δ W k \Delta W_kΔW 
k
​
 能得到不错的效果。
这说明，即使秩设为4，Δ W \Delta WΔW中也能捕获足够的信息，因此比起只调整单一类型的大秩权重，调整更多种类的权重矩阵效果会更好。
表总结：

将所有微调参数都放到attention的某一参数矩阵的效果并不好，将可微调参数分配到W q W_qW 
q
​
 和W v W_vW 
v
​
 的效果更好
即使是秩仅取4也能在Δ W \Delta WΔW中获得足够信息
因此在实际操作中，应当将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵。
如何在已有LoRA模型上继续训练？

理解此问题的情形是：已有的lora模型只训练了一部分数据，要训练另一部分数据的话。

是在这个lora上继续训练呢？
还是和base模型合并后再套一层lora
或者从头开始训练一个lora？
直接在现有的lora模型上继续训练

适用情况：新的数据与之前数据相似，任务也相似
操作步骤
将新的数据用于继续训练现有的LoRA模型
这样，LoRA模型的权重将进一步更新，融合新知识
优点：保留模型之前学习的知识，节省训练资源和时间
注意事项：需要注意过拟合问题，可以适当使用正则化技术。且如果新数据分布有差异，可能需要调整学习率或其他超参。
将LoRA和BASE合并后得到新的BASE模型，再训练新的Lora

适用情况：想要在模型中固化之前的知识，然后再新任务上进一步微调
操作步骤：
将现有的Lora权重合并到Base模型中，得到新的Base模型
在新的Base模型上训练Lora层
优点：知识固化，新的Lora专注于学习新任务的特征。有助于模块化地管理不同任务的适应
缺点：模型容量增加，占用更多存储空间
从头训练Lora：

适用情况：新任务与之前任务完全不同，或者担心以前的知识会干扰新的学习
操作步骤：使用基础模型，直接训练Lora层
优点：模型更加专注，避免旧知识的干扰。模型更加专注于新任务
缺点：无法利用之前训练中获得的知识，可能需要更多的训练数据和时间。完全无法使用旧知识，有点浪费资源。
总结：根据任务需求选择（任务相似、任务不同但有相关性、任务完全不同）

LoRA权重是否可以合入原模型？

可以，将训练好的低秩矩阵( B × A ) + 原模型权重合并（即相加） (B\times A)+原模型权重合并（即相加）(B×A)+原模型权重合并（即相加），计算
LoRA微调方法为什么能加速训练

只更新了部分参数：比如LoRA原论文就选择只更新self-attention的参数，实际使用时我们还可以选择只更新部分层的参数

减少了通信时间：参数少，需要传输的数据量也就变少了

采用了各种低精度加速技术：FP16 FP8 INT8

低秩分解的直观性：LoRA使用低秩分解方式更新和表示参数。这种方法再不少场景种能够很好地保持与全量微调相同的效果，同时本身非常直观易于理解

预测阶段不增加推理成本：LoRA的设计确保再推理阶段不会额外增加计算成本。因为微调的调整是通过低秩矩阵的形式添加的，并且再应用时已经被整合到模型参数中，不需要额外的运算，这有利于保持推理速度。

LoRA中的rank如何选取？

作者对比了1-64的rank，效果上4-8之间最好，再高没有效果提升
不过论文的实验是面向下游单一监督任务的，因此在指令微调上根据指令分布的广度，rank选择还是需要在8以上的取值进行测试的。
LoRA如何避免过拟合？

在使用LoRA进行微调时，过拟合是一个常见的问题（训练数据表现得好，但是在测试集上表现差，通常因为模型过度学习训练数据细节和噪声，而未抓住数据得普遍规律）

具体方法：

减小rank值
增加数据集大小
增加优化器得权重衰减率（weight decay）
增加LoRA层的dropout值
解释：Dropout是一种防止过拟合的技术，通过在训练过程中随机忽略部分神经元，使模型不依赖于特定的神经元
如何避免过拟合：在LoRA层增加Dropout，可以随机屏蔽部分LoRA层的参数，使模型更具鲁棒性，减少对特定参数的过度依赖，从而降低过拟合的风险。
LoRA矩阵初始化？

前面我们已经知道：

降维 矩阵A AA采用高斯分布（正态分布） 来初始化，以赋予其随即特性
而升维矩阵B BB初始化为零矩阵，这样开始训练时就不会影响原有模型的输出，确保训练稳定性
权重更新方式为：

W = W 0 + A B ⊤ W=W_0 + AB^\top
W=W 
0
​
 +AB 
⊤
 

这种操作有如下的考量：

为什么不把A AA和B BB都初始化为零？

此时W = W 0 W=W_0W=W 
0
​
 ，意味着训练开始时，模型参数没有任何变化
缺点：
可能出现梯度消失和对称性问题：所有神经元的初始状态和更新方向都相同，导致网络无法打破对称性。这样一来，神经元无法学习到多样化的特征，影响模型的表达能力
训练困难：梯度更新可能会因为缺乏初始扰动而过于缓慢，导致训练过程收敛速度变慢，甚至无法收敛
为什么不把A AA和B BB都用高斯初始化？

此时，初始权重更新为：Δ W = A B ⊤ \Delta W=AB^\topΔW=AB 
⊤
 ，由于A AA和B BB都是随机初始化的，因此Δ W \Delta WΔW也是一个随机矩阵，并且可能具有较大的值。

缺点：

初始扰动过大：过大的Δ W \Delta WΔW会在训练开始时对原有的预训练模型参数造成过大扰动，可能导致模型的输出偏离预期，训练不稳定
收敛困难：过大的初始噪声可能导致梯度爆炸，模型难以找到正确的优化方向，从而影响训练效果。
为什么不用高斯分布初始化A AA，零矩阵初始化B BB

理论上，LoRA矩阵初始化可以对调，由于LoRA的核心思想是通过低秩分解来更新预训练权重矩阵W 0 W_0W 
0
​
 ，最终训练的效果取决于模型对Δ W = A B ⊤ \Delta W=AB^\topΔW=AB 
⊤
 的学习能力，而不是特定的初始化方式。
可能的影响：
优化过程：梯度如何影响B BB和A AA的学习方向。
数值稳定性：论文中推荐的方式可能经过了实验验证，确保在实际应用中具有较好的数值稳定性，如果对调初始化，可能需要重新调试超参数。
总结：通过矩阵A AA采用高斯分布（正态分布） 来初始化，升维矩阵B BB初始化为零矩阵，可以：

保持模型初始输出与预训练模型一致，避免初始扰动过大
利用A AA的随机性打破对称性，提供丰富的梯度信息
在训练过程中，B BB从零开始逐步学习，有效控制权重更新幅度，促进模型稳定收敛。
6.1 AdaLoRA
LoRA通过低秩分解来模拟参数的该变量，从而以极小的参数量来实现大模型的间接训练

AdaLoRA是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵，将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合，并节省计算成本。

AdaLoRA讨论了如何更好地进行秩的设置：

它引入了一种动态低秩适应技术，在训练过程中动态调整每个参数矩阵需要训练的秩同时控制训练的参数总量。
具体来说，模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的参数矩阵被赋予比较高的秩，进而能够更好地学习到有助于任务的信息。相对而言，不太重要的参数矩阵被给予比较低的秩，来防止过拟合并节省计算资源。

论文：arxiv@2303.10512

代码：https://github.com/QingruZhang/AdaLoRA

7. QLoRA的思路
论文：arxiv@2305.14314

代码：https://github.com/artidoro/glora

尽管 LORA 已经轻量化了，但由于使用 BFloat16 进行训练，微调特别大的模型(65B以上)时无法使用单张卡或几张卡进行训练。
QLORA是 LORA 的改进版，可以减少内存使用，可以在单个48GB GPU上微调 65B 的大模型，同时保留完整的16位微调任务性能。
其工作原理是首先将 LLM 进行4位量化，从而显著减少模型的内存占用;然后使用 LORA 对量化的LLM进行微调。
使用 QLORA可以节省 33%的GPU内存。然而，由于 QLORA 中预训练模型权重的额外量化和去量化，训练时间增加了39%。
大致思想:

使用一种新颖的高精度技术将预训练模型量化为 4bit;
然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。
特点:

使用 QLORA 微调模型，可以显著降低对于显存的要求。但是，模型训练的速度会慢于LORA。
8. PEFT中，基座模型应该选用Chat版本还是Base版本？
如果监督任务是对话生成相关的任务

示例：生成对话回复、对话情感分析、多轮对话管理等
建议：选择ChatGPT类模型作为基座
原因：
ChatGPT 模型经过专门的对话数据训练，具备更强的对话交互能力。
能更好地理解上下文，处理多轮对话中的语义关联。
在生成对话回复时，能够提供更加自然和连贯的回应。
如果监督任务是单轮文本生成或非对话生成任务：

示例：文本摘要、机器翻译、文本分类、问答系统（非对话式）等。
建议：选择 Base GPT 模型 作为基座模型
原因：
Base 模型未经过对话数据的特化训练，保持了模型的通用性
在单轮文本生成和理解任务上表现出色，能够生成更加准确和贴合任务需求的结果。
避免了对话特征对非对话任务可能带来的干扰。
9. 预训练和微调哪个阶段注入知识？
简答：预训练和微调都注入知识，但注入的方式和范围不同。

预训练阶段注入的是通用的语言知识，使模型具备广泛的语言理解和生成能力。
微调阶段注入的是与特定任务相关的知识，使模型在特定任务上表现出色。
解析：

预训练：

目的：让模型从大量的未标注文本数据中学习语言的基本结构、语法、语义以及通用知识。

知识注入方式：

大规模数据学习：通过在海量的文本语料(如互联网数据、维基百科文章等)上进行训练，模型学习到了广泛的语言特征和常识性知识。
自监督学习：使用语言模型任务（如下一个词预测、掩码预测）让模型自我训练，学习词与词之间的关系、句法结构和上下文语义。
注入的知识类型：

通用语言知识：例如词汇含义、惯用表达、句法结构。
世界常识：由于训练数据的广泛性，模型也学习到了人类社会的常识性知识。
结果：预训练后的模型具备了对语言的基本理解和生成能力，能够在没有特定任务指导的情况下生成连贯的文本。

微调：

目的：让预训练模型适应特定的下游任务需求，提高在特定任务上的性能
知识注入方式：
监督学习：通过在标注了任务标签的数据集上训练，模型学习到了任务特定的模式和知识。
参数调整：在微调过程中，模型的参数会针对特定任务进行调整和优化。
注入的知识类型：
任务特定知识：例如对于情感分类任务，模型学习到哪些词语或表达与积极或消极情感相关。
领域专业知识：在特定领域的数据上微调，模型可以学习到该领域的专业知识和术语
结果：微调后的模型在特定任务上表现优异，能够准确完成任务，例如分类、问答、翻译等
总结:

预训练阶段：通过大规模未标注数据，模型学习到了通用的语言知识和世界常识，建立了语言理解的基础。
微调阶段：通过特定任务的标注数据，模型学习到了任务相关的知识，使其能够专注于具体任务并提升性能
两者的结合，使得模型既有广泛的语言理解能力，又能够在特定任务上发挥出色的表现。

10. 多轮对话任务如何微调模型？
在多轮对话任务中，微调模型的目标是使模型能够理解和生成连贯的多轮对话回复，具备上下文理解和一致性回复的能力。

下面详细解释如何在多轮对话任务中微调模型，包括每个步骤的目的和方法。

10.1 数据准备
目标：收集或创建适用于多轮对话任务的数据集。

方法：

收集现有数据集：使用公开的多轮对话数据集，如Persona-Chat、DailyDialog等。这些数据集包含大量的人类对话，涵盖各种话题和情境。
创建自定义数据集：如果有特定的领域或任务需求，可能需要自行收集或生成对话数据。
数据清洗和预处理：确保数据质量，去除噪声、重复或不相关的内容。
注意：

上下文信息：确保每条对话包含足够的上下文，多轮对话的连续性对模型的训练至关重要
数据多样性：包含不同的话题、情感和语言风格，有助于模型学习更丰富的表达方式
10.2 构建输入输出格式
目标：将原始对话数据转换为适合模型训练的格式。

方法:

输入格式:将多轮对话的历史(上下文)拼接成一个输入序列。

示例：

 [用户] 你好！
 [机器人] 你好，请问有什么可以帮到您的吗？
 [用户] 我想预定一张去北京的火车票。
1
2
3
输出格式：模型需要生成的下一轮回复（y标签，即标注），即问题的答案

使用特殊标记，在不同的说话者之间添加特殊标记（如[用户][机器人]这样的）有助于模型区分不同角色，提高对话连贯性

输入长度限制：模型的最大输入长度有限，需要合理截断或摘要过长的对话历史

10.3 模型选择
目标：选择适合多轮对话任务的预训练模型。
常用模型：
DialoGPT：微软发布的对话生成模型，基于GPT-2，专为对话生成设计，适合多轮对话任务
OLLAMA：通用的语言生成模型，具备强大的文本生成能力。
BERT：主要用于理解任务(如分类、问答)，不适合直接用于生成对话回复，但可用于理解型对话任务
其他：Qwen，ChatGLM，Pangu等
考虑因素：
任务类型：生成型任务选择生成模型(如GPT系列)，理解型任务可考虑BERT等
模型大小：朴根据可用的计算资源和任务需求，选择合适的模型规模。
预训练数据：选择已经在对话数据上预训练的模型有助于提升初始性能。
10.4 微调模型
初始化模型参数:

加载预训练模型:从预训练模型中加载参数，作为微调的起点。
定义损失函数：

生成任务:通常使用交叉熵损失函数，计算模型生成的回复与真实回复之间的差异。
特殊任务：根据任务需求，可能需要自定义损失函数，如引入情感倾向、特定词汇等。
进行反向传播和参数更新：

前向传播：将输入数据传入模型，得到模型输出。
计算损失：根据模型输出和真实标签(或目标回复)，计算损失值。
反向传播：计算损失对模型参数的梯度。
重复训练步骤：

多轮迭代：遍历整个训练数据集多个epoch，不断更新模型参数
验证：在验证集上评估模型性能，防止过拟合。
注意：
学习率设定：微调时通常采用较小的学习率，防止模型参数发生过大变化，导致预训练知识遗失
梯度剪裁：防止梯度爆炸，保持训练的稳定性
10.5 超参数调优
可调超参数：lr（影响参数更新速度），batchsize（英雄模型训练速度和泛化性能），epoches（过多可能过拟合，反之欠拟合），权重衰减（wd，用于正则化的参数，防止过拟合）

方法：gridsearch，random search，Bayesian Optimization（利用贝叶斯理论智能地探索参数空间）

评估：

验证集表现：根据验证集的损失或评价指标，选择最佳的超参数组合。
早停(Early Stopping)：当验证集性能不再提升时，提前停止训练。
10.6 评估和测试
目标：客观评价模型在多轮对话任务上的性能，确保模型的有效性和可靠性

评估指标：

自动评估：
BLEU，ROUGE：统计匹配程度
Perplexity（困惑度，PPL），衡量模型对测试集地拟合程度
Distinct-N：评估生成回复的多样性，计算生成文本中不同n-gram的比例。
人工评估：
流畅性：回复是否语法正确、表达流畅，
相关性：回复与上下文是否相关。
连贯性：在多轮对话中，回复是否前后连贯，
信息性：回复中是否包含有用的信息。
测试集评估：使用未参与训练和验证的测试集，评估模型的泛化能力和实际表现，

错误分析：

类别分析：识别模型在哪些类型的对话中表现较差，如涉及特定话题、情感等
案例分析：深入分析错误案例，理解模型的不足之处，指导后续改进
10.7 特定技巧的应用
使用对话策略进行训练：

目标：让模型学习合理的对话行为，提高对话的有效性和用户满意度。
方法：
策略建模：定义一系列对话策略，让模型学习何时提问、何时提供信息、如何引导对话等。
强化学习：使用奖励信号，训练模型在对话中采取最优策略。
数据增强：

目标：扩大训练数据的规模和多样性，提升模型的泛化能力。
方法：
同义替换：用同义词或短语替换原有的词汇。
随机插入或删除：在句子中随机插入或删除词语，生成新的对话实例。
翻译回译：将原句翻译成另一个语言，再翻译回来，产生语义相近的句子
情感和个性化建模：

目标：使模型的回复具有特定的情感倾向或人设，提升用户体验。
方法：
情感标签：在训练数据中标注情感，指导模型生成带有特定情感的回复。
人格特征：为模型设定特定的性格特征，在生成回复时体现出来
总结：

数据是基础：高质量、多样化的多轮对话数据集是成功微调模型的关键，
模型选择与调整：根据任务需求选择合适的预训练模型，并通过微调使其适应特定的对话任务。
训练过程：细心设计训练流程，注意超参数的设置和模型的稳定性。
评估与改进：持续评估模型性能，针对不足之处进行改进，如引入注意力机制、策略训练等
创新应用：结合任务特点，应用特殊技巧(如情感建模、数据增强)提升模型的实际效果。
Transformers相关
参考资料:

李宏毅老师介绍：https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJY_el3uVTSOK_ZK5LOlv_EOoL1JefRL4&index=61
可视化说明：https://bbycroft.net/llm
其他大佬文章:
https://baijiahao.baidu.com/s?id=1651219987457222196&wfr=spider&for=pc
https://jalammar.github.io/illustrated-transformer/
Transformers之后的几个框架？

Mamba：https://arxiv.org/pdf/2312.00752

由卡内基梅隆大学和普林斯顿大学的研究人员2023年开发，Mamba通过引入状态空间模型(SSM)，实现更高的训练速度和更强的表示能力。
TTT：https://arxiv.org/pdf/2407.04620

TTT是斯坦福大学、UCSD、UC伯克利和Meta的研究团队2024.8联合推出的一种全新架构，旨在通过机器学习模型取代RNN的隐藏状态，从而优化语言模型方法。
1. Transformers概述

Transformers由6个encoder和6个decoder组成：



工作流程：

获取输入句子的每一个单词的表示向量X XX，由单词的embedding和位置编码相加得到：


将嵌入矩阵X ∈ R n × d X\in\R^{n\times d}X∈R 
n×d
 输入到Encoder中，经过6个encoder block后得到句子所有单词的编码信息矩阵C CC，其中n nn是句中单词数量，d dd是单词维度（论文中为d = 512 d=512d=512）

每一个encoderblock的输出矩阵与输入矩阵形状相同



（细节：这里会按照词根来划分token，比如doing会被分成do和ing来编码）

将Encoder输出的编码矩阵C CC传递到Decoder中，Decoder依次会根据当前翻译过的单词1 , 2 , . . . , i 1,2,...,i1,2,...,i来翻译下一个单词i + 1 i+1i+1

实际使用中，翻译到第i + 1 i+1i+1个单词时需要通过Mask来遮盖住i + 1 i+1i+1之后的单词：


Decoder接收了C CC然后输出一个翻译开始符<Begin>，预测第一个单词i ii
然后输入<Begin> i，预测单词have，以此类推
这是Transformer使用的大致流程
2. Transformer的输入部分具体是如何构成？
Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding 相加得到。

2.1 单词 Embedding
单词的 Embedding 有很多种方式可以获取,
例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。
2.2 位置 Embedding
Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。
因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。
所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。
位置 Embedding用 PE表示，PE的维度与单词 Embedding 是一样的。
PE 可以通过训练得到，也可以使用某种公式计算得到。在Transformer 中采用了后者，计算公式如下：
P E ( p o s , 2 i ) = sin ⁡ ( p o s / 1000 0 2 i / d ) P E ( p o s , 2 i + 1 ) = cos ⁡ ( p o s / 1000 0 2 i / d ) PE(pos, 2i) = \sin (pos / 10000^{2i/d})\\ PE(pos, 2i + 1) = \cos(pos / 10000^{2i/d})
PE(pos,2i)=sin(pos/10000 
2i/d
 )
PE(pos,2i+1)=cos(pos/10000 
2i/d
 )

pos 表示单词在句子中的位置，d表示 PE的维度(与词 Embedding 一样)
2i 表示偶数的维度，2i+1表示奇数维度 (即 2i < d, 2i + 1 < d)。
使用这种公式计算PE的好处：

使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以快速计算出第 21 位的 Embedding。
可以让模型容易地计算出相对位置，对于固定长度的间距k，PE(poS+k)可以用 PE(poS)计算得到。因为：
Sin(A+B)=Sin(A)Cos(B)+Cos(A)Sin(B),
Cos(A+B)=Cos(A)Cos(B)-Sin(A)Sin(B)
1
2
将单词的词 Embedding 和位置 Embedding相加，就可以得到单词的表示向量x，x就是 Transformer 的输入。
3 自注意力原理


红色圈忠的部分是多头注意力，是由多个自注意力组成，可以看到：

Encoder包含一个多头注意力
Decoder包含两个多头注意力（其中一个用到Mask）
多头注意力上方还包括一个AddNorm层，就是残差连接加层正则化（LayerNorm）

3.1 自注意力结构


输入：Q , K , V Q,K,VQ,K,V
实际操作忠，自注意力接收的是输入（单词的表示向量组成的矩阵X XX）或者上一个Encoder block的输出
Q , K , V Q,K,VQ,K,V正是通过自注意力的输入进行线性变换得到
3.2 QKV的计算
自注意力的输入用矩阵X XX表示，则可以使用线性变换矩阵W Q , W K , W V W_Q,W_K,W_VW 
Q
​
 ,W 
K
​
 ,W 
V
​
 计算得到Q , K , V Q,K,VQ,K,V，计算如下图所示，注意X , Q , K , V X,Q,K,VX,Q,K,V的每一行都表示一个单词：



3.3 自注意力的输出
得到矩阵Q , K , V Q,K,VQ,K,V之后就可以计算出自注意力的输出了：

A t t ( Q , K , V ) = s o f t m a x ( Q K ⊤ d ) V Att(Q,K,V)={\rm softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
Att(Q,K,V)=softmax( 
d
​
 
QK 
⊤
 
​
 )V

其中d k d_kd 
k
​
 是Q , K Q,KQ,K的列数，即向量维度，论文中d = 512 d=512d=512

公式中计算矩阵Q QQ和K KK每一行向量的内积，为了防止内积过大，因此除以d k d_kd 
k
​
 的平方根
Q QQ乘以K KK的转置后，得到的矩阵行列数都为n nn，n nn为句子单词数，这个矩阵可以表示单词之间的attention强度
下图为Q K ⊤ QK^\topQK 
⊤
 ，1234表示句子中的单词：


得到Q K ⊤ QK^\topQK 
⊤
 之后，使用softmax计算每一个单词对于其他单词的attention系数
公式中的softmax是对矩阵的每一行进行softmax，即每一行的和都变为1


得到softmax矩阵后可以和V VV相乘，得到最终输出Z ZZ


上图中Softmax矩阵的第一行表示单词1和其他所有单词的attention系数
最终单词1和输出Z 1 Z_1Z 
1
​
 等于所有单词i ii的值V i V_iV 
i
​
 根据attention系数的比例加在一起得到，如下图所示：


3.4 多头注意力


首先将输入X XX分别传递到h hh个不同的自注意力中，计算得到h hh个输出矩阵Z ZZ，论文中h = 8 h=8h=8，即得到8个输出矩阵Z ZZ


得到Z 1 Z_1Z 
1
​
 到Z 8 Z_8Z 
8
​
 之后，多头就是直接拼接，然后传入到Linear层，得到多头注意力最终输出Z \bf ZZ，这里Z \bf ZZ其实和那个是一个形状的。


4 Encoder结构
编码器由多头注意力，残差连接+正则（ADD&NORM），前馈和**残差连接+正则（ADD&NORM）**组成

4.1 AddNorm
L a y e r N o r m ( X + M u l t i H e a d A t t ( X ) ) L a y e r N o r m ( X + F e e d F o r w a r d ( X ) ) LayerNorm(X+MultiHeadAtt(X))\\ LayerNorm(X+FeedForward(X))
LayerNorm(X+MultiHeadAtt(X))
LayerNorm(X+FeedForward(X))

4.2 前馈
两层的全连接层，第一层激活用ReLU，第二层不用激活：

max ⁡ ( 0 , X W 1 + b 1 ) W 2 + b 2 \max(0, XW_1+b_1)W_2+b_2
max(0,XW 
1
​
 +b 
1
​
 )W 
2
​
 +b 
2
​
 

4.3 组成Encoder
Encoder block接收输入矩阵X ∈ R n × d X\in\R^{n\times d}X∈R 
n×d
 ，输出O ∈ R n × d O\in\R^{n\times d}O∈R 
n×d
 ，通过多个Encoder block叠加得到Encoder

第一个Encoder的输入是句子单词的表示向量矩阵
后续Encoder的输入是前一个Encoder的输出
最后一个Encoder的输出就是编码信息矩阵C CC，


5 Decoder结构


Decoder block结构，与 Encoder block 相似，但是存在一些区别:

包含两个 Multi-Head Attention 层。

第一个 Multi-Head Attention 层采用了 Masked 操作。
第二个 Multi-Head Attention 层的K,V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。
最后有一个 Softmax 层计算下一个翻译单词的概率。

5.1 第一个Multi-Head Attention
Decoder block 的第一个 Multi-Head Atention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第i个单词，才可以翻译第 i+1 个单词。

通过 Masked 操作可以防止第i个单词知道 i+1 个单词之后的信息

下面以 我有一只猫 翻译成" have a cat"为例，了解-下 Masked 操作。

下面的描述中使用了类似 Teacher Forcing 的概念。

Teacher Forcing 的概念

在处理像语言翻译这样的任务时，我们希望模型能够从一句话中逐词生成相应的翻译。为了训练模型能有效地做到这一点，我们经常使用一种叫做"Teacher Forcing"的方法。简单来说，这方法是一种告诉模型正确答案，以便它能更好学习的技巧。

想象一下，你在教小朋友用英语造句，在训练的时候，你不希望小朋友总是自由发挥，而是希望他们按照正确的例子来练习，这样才能尽快学会如何正确造句。对于模型来说也是类似的:

训练阶段
假设模型正在学习从英语到法语翻译句子。对于每个单词，模型都需要生成下一个单词
在"Teacher Forcing"中，每一步模型不是依据自己前一步猜测的单词、而是直接使用正确的下一个单词来帮它做出判断。这就像告诉小朋友“接下来应该说这个单词哦!
好处：
这样做加快了模型的学习速度，因为它总是“走在正确的路上”，不用一直被自己错误的猜测拖累。
帮助模型更快理解语言的结构和不同单词之间的关系。
使用时机：
在模型训练时，我们使用"Teacher Forcing"来快速让模型学习。
当模型真正独立工作时，比如实时翻译时，它就不再有正确答案可以参考，只能根据自己上一步的输出继续推断，这时候模0型就要展示自己的真实水平了。
在Decode时，是需要根据之前的翻译，求解当前最有可能的翻译

Decoder可以在训练过程忠使用 Teacher Forcing + 并行化训练，即将正确的单词序列（<Begin> I have a cat）和对应的输出（I have a cat <end>）传递刀Decoder

那么在预测第i ii个输出时，就要把i + 1 i+1i+1之后的单词掩盖，注意Mask操作是在Self-Attention的Softmax之前使用的，下面用0 1 2 3 4 5分别表示<Begin> I have a cat <end>

第一步：是 Decoder 的输入矩阵和 Mask矩阵，输入矩阵包含"<Begin>l have a cat ，即(0,1,2,3,4)五个单词的表示向量，Mask是一个 5x5 的矩阵。在 Mask 可以发现单词0只能使用单词0的信息，而单词1可以使用单词0,1的信息，即只能使用之前的信息。



第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算 Q 和K的乘积 Q K ⊤ QK^\topQK 
⊤
 



第三步：在得到 Q K ⊤ QK^\topQK 
⊤
 之后需要进行 Softmax，计算 attention score，我们在 Softmax之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下:



得到 Mask QK之后在 MaskK QK 上进行 Softmax,每一行的和都为1。但是单词0在单词1,2,3,4上的 attention score 都为 0.

第四步：使用 Mask QK 与矩阵 V 相乘，得到输出 Z,则单词1的输出向量 2,是只包含单词1信息的。



第五步：通过上述步骤就可以得到一个 Mask Self-Atention 的输出矩阵;，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出2; 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。

5.2 第二个MHA
Decoder block 第二个 MHA 变化不大，主要的区别在于其中 Self-Attention 的 K V 矩阵不是使用上一个 Decoder block 的输出计算的，而是Encoder 的编码信息矩阵 C 计算的

根据 Encoder 的输出 C 计算得到 K V，根据上一个 Decoder block 的输出 Z 计算 Q（如果是第一个 Decoder block 则使用 输入矩阵 X 进行计算），后续计算方式也是一样的

这样做的好处是在Decoder的时候，每一位单词都可以利用到Encoder所有单词的信息（这些信息无需Mask）

问题：transformer中，连接Encoder和Decoder的中间矩阵C会被用几次？

C会被用N次，N是解码器的层数，例如解码器6层，C就会被用6次

5.3 Softmax预测输出单词
Decoder最后部分是利用softmax预测下一个单词，在之前的网络层我们可以得到一个最终的输出Z，因为Mask的存在，使得单词 0 和 输出Z 0 Z_0Z 
0
​
  只包含单词 0的信息：



这就是Decoder block的定义，与Encoder一样，Decoder也是由多个block组合而成的

6 衍生问题：Decoder在训练过程中使用teacher forcing如何进行并行化训练呢？
在seq2seq模型的训练中，Decoder需要逐步生成输出序列。在不使用教师强制的情况下，解码器在每个timestep的输入都是前一个timestep的输出，这意味着每个时间步的计算都依赖于前一个时间步，因此无法并行化计算。

使用教师强制后，训练中，将decoder的每个timestep的输入替换为目标序列中对应的真实词语。这样，所有时间步的输入都是已知的，彼此之间没有依赖关系，因此可以并行化。

7 总结
Transformer与 RNN 不同，可以比较好地并行训练
Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则Transformer 就是一个词袋模型了。
Transformer 的重点是 Self-Attention 结构，其中用到的 Q,K,V矩阵通过输出进行线性变换得到
Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 atention score,
DeepSeek相关
1 训练管道
第一阶段：训练ds-r1-zero

引入ds-v3-base，并采用强化学习进行训练
使用GRPO调整奖励函数，并学习不同的序列化行为模式
训练出ds-r1-zero，出现的问题和优势：
训练过程早期不稳定（？，我记得v3的tech rep里说的是他们发现训练一直很顺利，loss稳步下降的）
但能自主诞生推理过程（aha moment）
第二阶段：训练ds-r1

再次使用ds-v3-base，并进一步微调
通过两轮SFT调整学习模式
使用两轮GRPO进行强化学习，基于奖励函数进一步调整
第三阶段：ds-r1模型蒸馏技术产出蒸馏后的小尺寸模型（拿到QWEN和LLAMA生成的SFT数据进行蒸馏）

2 deepseek-r1和deepseek-r1-zero有什么区别
2.1 DeepSeek-r1
训练方式：

冷启动数据引入：通过引入数千条高质量的冷启动数据进行初始微调，解决了ds-r1-zero的可读性和语言混杂问题，显著提升了模型的可读性和多语言处理能力。
两阶段强化学习：模型通过两轮强化学习不断优化推理模式，同时对齐人类偏好，提升了多任务的通用性
增强型SFT：在强化学习接近收敛时，结合拒绝采样和多领域的数据集，进一步强化了写作、问答和角色扮演等非推理能力，
主要功能：

高性能推理：

在数学和代码能力，以及自然语言推理等任务中表现出色，在AIME2024上取得了79.8%的成绩，略高于openai-o1-1217
在MATH-500取得了97.3%的成绩，和o1-1217相当
支持模型蒸馏：

支持用户利用ds-r1的输出进行模型蒸馏，训练更小的模型，如果使用Qwen和llama蒸馏出的32b和70b模型，在多项能力上对标o1-mini的性能
开源与灵活使用：遵循MIT License开源，支持商用和模型修改，学界业界都用广泛使用

2.2 DeepSeek-r1-zero
训练方式：

首个完全基于强化学习的推理模型，直接在基础模型上应用强化学习，跳过了监督问条阶段
训练中主要有两种奖励：
一种是只看最终答案是否正确，如数学题看最终结果，编程题看测试用例
另一种是格式奖励，要求模型将思考内容写出来，即<think>标签内，不要混杂思考内容和用户呈现的内容
主要功能：

推理能力：在AIME2024数学竞赛的表现接近o1-mini
自我进化：在训练过程中能自然涌现出反思，重新评估推理步骤等复杂行为，比如模型自己会进行反思，重新审视之前的步骤，以及探索解决问题的替代方案（aha moment）
开源与社区支持
区别：

r1-zero：更适合研究成精，验证纯RL训练的潜力，实际应用受限
r1：适用于高精度推理需求，如编程辅助、科学问题解答、教育工具等
启发：

对官方提供的其他llm，直接后面接个sft，就能用很大提升
3 DeepSeek-R1-Zero如何通过纯强化学习(RL)实现推理能力的突破?
DeepSeek-R1-Zero通过纯强化学习实现推理能力的突破，主要体现在以下几个方面：

3.1 纯强化学习训练范式
DeepSeek-R1-Zer0摒弃了传统大语言模型(LLM)训练中依赖监督微调(SFT)的步骤，完全通过强化学习进行训练。
传统方法认为：
大模型需先通过SFT获得基础能力，再通过RL优化性能
而DeepSeek-R1-Zero的实验证明，仅通过RL即可直接激励模型发展出强大的推理能力，例如在数学、编程等任务中生成长思维链(Chain-of-Thought,CoT)并自我验证。
3.2 创新的强化学习算法与奖励机制
GRPO算法：采用群体相对策略优化(Group Relative Policy Optimization,GRPO)，通过比较一组输出的奖励来估计优势函数，避免了传统方法中需要单独训练评论模型(Critic)的复杂性，显著降低了计算资源需求。
GRPO算法每组采样16条输出，强制模型探索多路径
双奖励系统:设计了基于规则的奖励机制，包括：
准确性奖励:评估答案正确性(如数学题答案验证或代码编译测试)
格式奖励:强制模型将推理过程置于特定标签(如和)之间，提升可读性
3.3 自我进化与aha moment
在RL训练过程中，DeepSeek-R1-Zero展现出自主进化的能力。例如，模型在解决数学问题时，会主动延长推理时间、重新评估错误步骤，并探索新的解题策略。
3.4 对传统训练范式的颠覆
DeepSeek-R1-Zero的成功挑战了监督数据依赖的固有模式，证明无需人工标注的SFT数据。模型也能通过RL自主探索并优化推理路径。
3.5 总结
DeepSeek-R1-Zero通过纯强化学习框架、创新的奖励设计及自我进化机制，实现了推理能力的显著提升。其核心贡献在于验证了RL在无监督场景下激发模型复杂推理能力的可行性。

4 训练deepseek-r1中，为什么要引入数千条样本进行冷启动，他们包含哪些内容？
冷启动数据用于解决DeepSeek-R1-Zero的可读性和语言混合问题。
具体来说，冷启动数据包含数千条高质量的长思维链(CoT)示例，通过人工标注和格式过滤(如使用<reasoning>和<summary>标签)，强制模型生成结构清晰、语言一致的内容。
数据质量 > 数据数量

其核心优势在于：

稳定性：为RL训练提供高质量的初始策略，避免早期探索阶段的输出混乱。

可读性：

通过模板化输出（如 summary 模块）提升生成内容的用户友好性。
加速收敛:

减少RL训练所需的步数，实验表明冷启动后AIMEPass@1进一步提升至79.8%(和 OpenAI-01-1217 能力差不多，其为79.2%，该描述见原文【1.2 Summary of Evaluation Results】部分）
5 冷启动数据怎么构造，为什么需要人工标注和格式过滤，冷启动数据中的“总结”（summary）模块如何提升可读性？
5.1 冷启动数据怎么构造
种子问题收集：从数学竟赛(如AIME)、编程题库(LeetCode)中选取代表性题目。
答案生成：使用DeepSeek-R1-Zero生成初始CoT，人工修正逻辑错误并统一格式
模板化:强制要求<think>和<answer>标签，并添加总结模块(如<summary>关键步骤:…</summary>)
5.2 为什么需要人工标注和格式过滤
可读性保障:自动生成的CoT可能含无关内容或语言混杂，需人工过滤，

格式一致性:确保后续RL训练中奖励信号稳定(如格式错误直接扣分)

5.3 冷启动数据中的“总结”(summary)模块如何提升可读性?
总结模块通过强制模型提炼推理过程的关键步骤，提升输出结构化：

信息压缩：要求模型用1-2句话概括最终结论，如解为x=2，关键步骤:平方消去根号。
用户友好：用户可直接阅读总结而无需解析长CoT，降低使用门槛。
奖励引导：总结的清晰度通过规则化评分（如关键词覆盖率）纳入奖励。
6. DeepSeek创新点有哪些
发布时间	技术创新	模型链接	paper	核心点
2024.1	DeepSeek MoE框架		2401.06066	
2024.4	GRPO，群体相对策略优化	DeepSeekMath	2402.03300	对PPO的优化
2024.6	MLA，多头隐式注意力	DeepSeek-V2	2405.04434	
2024.12	MTP，多token预测	DeepSeek-V3	2412.19437	
2025.1	通过强化学习显著提升模型推理能力，R1-Zero在AIME2024基准测试中打到openai-o1-0912的水平	DeepSeek-R1-Zero	2501.12948	
2025.1	将R1蒸馏为小模型	DeepSeek-R1-Distill-Qwen-32b	2501.12948	
7 MoE主要有哪些优化？


7.1 传统MoE模块
MoE模块包含N NN个前馈神经网络专家，每个专家在处理特定类型的数据上有独特的优势。
MoE模块通过路由机制，根据输入数据的特征动态选择最合适的K KK个专家进行处理，而不是激活所有的专家，在前向计算中，由于只激活了部分专家，实际参与计算的参数量被称为激活参数量
例如，Mixtral-8-7B模型包含8个专家，每次选择其中的2个专家进行计算，模型的总参数量为46.7B，而激活参数量为12B左右
7.2 细粒度专家划分
不同于传统MoE，DeepSeek把N个专家做更细粒度的划分，降低每个专家的参数量，增大专家数量
如上图(b)中，将N个专家拆分为m × N m\times Nm×N个，每个专家的隐层维度变为原来的1 / m 1/m1/m，相应地激活m × K m\times Km×K个专家
如此MoE模块的参数量和激活参数量都不变，并且可以更加灵活地组合多个专家
7.3 共享专家分离
把激活专家区分为Shared Experts和Routed Experts
如图©所示，共享专家和路由专家在数据处理上有显著区别
对于共享专家，输入数据无需通过路由模块计算，所有数据都会直接通过共享专家进行处理
相反，对于路由专家，输入数据会经过路由模块，该模块根据数据输入的特征选择最合适的专家进行计算。在这种架构中，路由模块通过计算输入数据和各个专家的匹配概率，选择概率最高的专家进行处理
最终，将路由专家和共享专家的计算结果相加，得到MoE的最终输出
这样，既能捕捉到输入数据的共性，也能关注到输入数据的差异性，这种设计能够提高模型的泛化性能和适应性
Model	# Total Params	# Activated Params
DeepSeek-R1-Zero	671B	37B
DeepSeek-R1	671B	37B
数据来自：https://github.com/deepseek-ai/DeepSeek-R1

8. MoE结构中负载不均衡是怎么解决的？
8.1 基本解释和特点介绍
DeepSeek-V3 针对 MoE 中常见的负载均衡问题，提出一种新的负载均衡策略。

在用于选择专家的 Gate 模块中引入一个可学习的偏置项

在计算路由得分时，这个偏置项会被动态地加到每个路由专家的得分上

s c o r e i ′ ( x ) = s c o r e i ( x ) + b i score'_i(x)=score_i(x)+b_i
score 
i
′
​
 (x)=score 
i
​
 (x)+b 
i
​
 

例如，若专家 A 原本的得分是 1，专家 B 的得分是 2，经过 softmax 后，专家 A 被选择的概率可能较低，但如果给专家 A 引入一个偏置项 3，那么新的得分就是 4，此时专家 A 被选择的概率就会大大增加。

该方式的主要特点在于：

动态调整路由倾向：
通过学习偏置项，模型可以动态地调整对不同路由专家的偏好。如果某个专家的负载过重，其对应的偏置项可能会被学习为负值，从而降低其被选择的概率
反之，负载较轻的专家，偏置为正
无额外损耗
该偏置项是直接通过模型的训练目标进行优化的，而不是通过一个独立的负载均衡损失
这意味着，模型在努力提高主要任务性能时，也会自然而然地学习到一种更均衡的路由策略，而不会因为额外地负载均衡损失而影响性能。
DeepSeek 通过这些 MoE 架构上的创新，直接促进了 V3 模型的整体效果提升

8.2 公式拆解
原论文中，负载均衡公式：
g i , t ′ = { s i , t if  s i , t + b i ∈ TopK ( { s j , t + b j ∣ 1 ≤ j ≤ N r } , K r ) 0 o.w. g_{i,t}'=\left\{
si,t0if si,t+bi∈TopK({sj,t+bj|1≤j≤Nr},Kr)o.w.
𝑠
𝑖
,
𝑡
if 
𝑠
𝑖
,
𝑡
+
𝑏
𝑖
∈
TopK
(
{
𝑠
𝑗
,
𝑡
+
𝑏
𝑗
|
1
≤
𝑗
≤
𝑁
𝑟
}
,
𝐾
𝑟
)
0
o.w.
\right.
g 
i,t
′
​
 ={ 
​
  
s 
i,t
​
 
0
​
  
​
  
if s 
i,t
​
 +b 
i
​
 ∈TopK({s 
j,t
​
 +b 
j
​
 ∣1≤j≤N 
r
​
 },K 
r
​
 )
o.w.
​
 

其中：

s i , t s_{i,t}s 
i,t
​
 ：第 i ii 个专家的得分
b i b_ib 
i
​
 ：偏置项
TopK ( ⋅ , K r ) \text{TopK}(\cdot, K_r)TopK(⋅,K 
r
​
 )，表示包含针对第 t tt 个 token 和所有路由专家计算出的分数中，K KK 个最高得分的集合
g i , t ′ g_{i,t}'g 
i,t
′
​
 ：是调整后的第 i ii 个专家的得分，用于决定该专家是否被激活
公式进一步解释：

对于每个专家，首先计算其加权得分 s i , t + b i s_{i,t} + b_is 
i,t
​
 +b 
i
​
 ，如果该加权得分在所有专家中排在 K r K_rK 
r
​
  名内，则该得分保留，说明该专家被激活，否则得分为 0，表示该专家不参与此输入的处理
通过引入偏置项b i b_ib 
i
​
 ，模型能灵活地调整每个专家的活跃度，实现负载均衡
9 GRPO和PPO的区别与联系
9.1 基本介绍
目前，大模型训练大体可以分为3种模式：

预训练(Pretraining)

有监督精调(Supervised Fine-Tuning,SFT)

强化学习(Reinforcement Learning)。

其中，SFT让模型通过学习训练数据数据分布的方式来提高模型在特定任务或指令上的表现与其不同的是，RL使用人类反馈来定义奖励函数，然后通过强化学习算法优化模型。让模型能生成符合人类喜好的回复。

主流的RL算法有PPO(Proximal Policy Optimization)、DPO(Direct Preference Optimization)以及本节重点介绍的GRPO(Group Relative Policy Optimization，群体相对策略优化)。在介绍GRPO之前，需要先了解PPO算法，因为GRPO可以算作是PPO的计算效率优化版本，在保持效果的同时，降低计算资源消耗。

9.2 PPO
在强化学习领域，PPO 算法被广泛认为是强化学习中的基准算法之一。PPO 采用了 Actor-Critic 架构，这一架构可以形象地理解为:

有一个演员(actor)在舞台上表演，而一个评论家(critic)在台下观看
演员的目标是通过不断调整自己的表演行为来获得观众的认可，并从观众那里获得及时反馈,
而评论家的任务则是评估演员的表演，并提供全面的建议。
在自然语言处理(NLP)生成模型的场景中，被训练的模型相当于演员，其表演即为生成的回复。相应地，会有评论家和观众模型来评价回复的质量。

具体来说，PPO使用了四个模型:

Policy 模型(又称 Actor):输入一段上文，输出下一个token的概率分布。该模型需要训练，是我们最终得到的模型。输出下一个token即为Policy模型的“行为”

Value 模型(又称 Critic):用于预估当前模型回复的总收益。该总收益不仅局限于当前token的质量，还需要衡量当前token对后续文本生成的影响。该模型需要训练。

Reward 模型:事先用偏好数据进行训练，用于对Policv模型的预测进行打分，评估模型对于当前输出的即时收益。

Reference 模型:与 Poicy 模型相同，但在训练过程中不进行优化更新，用于维持模型在训练中的表现，防止在更新过程中出现过大偏差。



为了更直观地理解 Value 模型的总收益和 Reward 模型的即时收益，可以用“磨刀不误砍柴工”来举例说明。

假设现在有一把钝刀，一分钟可以劈一根柴火;如果把刀磨锋利了，一分钟就可以劈两根柴火。
现在你可以选择直接用钝刀劈柴，或者先把刀磨锋利。
前者的当前收益比后者高，但未来的收益会低。也就是说，Value 模型会对后者“磨刀”这一行为更为推崇，而 Reward 模型会给前者“直接砍柴”一个更高的分数。
9.3 GRPO
PPO 在大模型的 RLHF 阶段被成功应用，不断提升模型回复表现的上限。
然而，PPO 在计算成本和训练稳定性方面仍然存在一定的挑战。GRPO 算法对此进行了优化，其核心目标是去除Value 模型，以此来减少训练的计算资源。


来源：https://arxiv.org/pdf/2402.03300

上图展示了 GRPO 相对于 PPO 的改进。

传统的 PPO 使用 Value 模型来估计模型回复的总收益，这实际上是对未来模型回复各种可能性的一个平均分值估计

而 GRPO 的方法是通过，大模型根据当前的上文输入进行多次采样，生成多个预测结果 ·，并分别使用 Reward 模型对这些预测结果进行评分得到 "，最后取这些评分的平均值来替代 Value 模型的预期总收益估计。通过这种方式，GRPO 在训练过程中可以减少一个模型的前向和反向传播计算，从而降低计算资源的消耗。

9.4 总结
下表针对 SFT 以及主流的一些强化学习方法做了对比和总结：

算法	特点
SFT	在标注的 SFT 数据上对预训练模型进行微调。
PPO	PPO 算法采用 Actor-Critic 架构，需要 Policy 模型、Value 模型、Reward 模型、1.
近端策略优化(PPO)Reference 模型
使用 Value 模型评估模型的预期总收益(模型回复的好坏)
GRPO	GRPO 算法采用 Actor-Critic 架构，需要 Reward 模型、Reference 模型，但是删掉了Value 模型。不使用 Value 模型，而是使用一组 LLM 生成的针对同一上文输入的多次采样结果来做预期总收益的估计。
10. MLA（多头隐式注意力）有什么作用
10.1 kv-cache
在标准的Transformer模型中，MHA机制通过并行计算多个注意力头来捕捉输入序列中的不同特征，每个注意力头都有自己的QKV矩阵，对于序列中的每一个token，都需要计算QKV，进而计算多头注意力
在推理过程中，当前LLM所采用的token by token递归生成的方式，上文token的KV计算不会受到后续生成token的影响，因此可以缓存下来，避免重复计算，提高推理效率，这就是KV cache的由来。
也就是说，当生成t + ! t+!t+!个token时，可以利用之前事先算好的上文t tt个token的KV值，t + 1 t+1t+1位置的token的KV值计算出来也将保存在cache中，如下：
def greedy_decode(model,
				  tokenizer,
				  input_text, 
				  max_length,
				  device = "cuda",
				  ):
	inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)	# Str => Long(1, n_tokens)
	past_key_values = None
	for i in range(max_length):
		outputs = model(inputs, past_key_values=past_key_values)
		logits = outputs.logits	# Float(1, n_tokens + i + 1, n_vocab), where `n_vocab` is 151936 in DeepSeek-R1-Distill-Qwen
		past_key_values = outputs.past_key_values	# Dictlike[key_cache: Float(1, 2, X, hidden_size), value_cache: Float(1, 2, X, hidden_size)], where X = (i + 1) * (n_tokens + i / 2)
		next_token_id = torch.argmax(logits[:, -1, :], dim=-1)	# Float(1, n_tokens + i + 1, n_vocab) => Float(1, n_vocab) => Long(1, )
		inputs = torch.cat([inputs, next_token_id.unsqueeze(-1)], dim=-1)	# Long(1, n_tokens + i) => Long(1, n_tokens + i + 1) 
	generated_text = tokenizer.decode(inputs[0], skip_special_tokens=True)	# Long(n_tokens + 1, ) => Str 
	return generated_text

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
KV cahce本质上就是在保存KV向量，以便于后续推理中快速计算注意力得分，但是随着模型增大，KV cache的显存占用也会相应增大

10.2 MLA对KV cache的优化
目前大模型对于注意力机制做的一些改进，包括MQA（多查询注意力），GQA（组查询）都是为了想办法减少KV cache，MLA亦如是

减少KV cache就可以实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batchsize更大，从而实现更快的推理速度或更大的吞吐量。最终都是实现耕地的推理成本



https://arxiv/org/pdf/2405.04434

如上图所示，GQA与MQA的办法时通过共享KV的注意力头，降低KV cache的数据维度（本质上，MHA时GQA的特殊形式，即Q的分支为1，MQA也是GQA的特殊形式，即Q的分支等于总头数）

MLA的方法本质上是对原本MHA的KV Cache作低秩分解，得到一个低维的隐向量（Latent Vector），在推理阶段，MLA只需要缓存该隐向量，由此大大降低需要缓存的数据量。

10.3 MLA的细节介绍
10.3.1 低秩分解
降维矩阵W D K V W^{DKV}W 
DKV
 和升维矩阵W U K , W U V W^{UK},W^{UV}W 
UK
 ,W 
UV
 
原始KV矩阵的维度是d × d d\times dd×d，而通过低秩分解后的降维和升维映射矩阵的维度分别为d × r d\times rd×r和r × d r\times dr×d
那么参数量就从O ( d 2 ) O(d^2)O(d 
2
 )降低为O ( d r ) O(dr)O(dr)
10.3.2 降维映射
原始表征：某一层的token的表征为高维向量h t h_th 
t
​
 ，假设维度为d dd
降维映射：使用降维矩阵W D K V W^{DKV}W 
DKV
 （维度r × d r\times dr×d，r ≪ d r\ll dr≪d），将h t h_th 
t
​
 压缩为低维隐向量
此时c t K V c_t^{KV}c 
t
KV
​
 的维度为r rr（如在DeepSeek-V3中r = 512 r=512r=512），远小于原始维度d dd（如在DeepSeek-V3中d = 7168 d=7168d=7168）
10.3.3 升维还原
还原KV，在前向计算时，通过升维矩阵W U K ∈ R d k × r W^{UK}\in\R^{d_k\times r}W 
UK
 ∈R 
d 
k
​
 ×r
 和W U V ∈ R d v × r W^{UV}\in\R^{d_v\times r}W 
UV
 ∈R 
d 
v
​
 ×r
 将低维隐向量还原为原始的KV：
k t C = W U K c t K V , v t C = W U V c t K V k_t^C=W^{UK}c_t^{KV},v_t^C=W^{UV}c_t^{KV}
k 
t
C
​
 =W 
UK
 c 
t
KV
​
 ,v 
t
C
​
 =W 
UV
 c 
t
KV
​
 

这样得到的k t C , v t C k_t^C,v_t^Ck 
t
C
​
 ,v 
t
C
​
 维度与原始KV一致（d k , d v d_k,d_vd 
k
​
 ,d 
v
​
 ）

10.4 实际应用中的设置和效果表现
在DeepSeek-V3中，低维隐向量的维度设置如下：

K和V的压缩维度d v d_vd 
v
​
 ：设置为512，原始嵌入维度d = 7168 d=7168d=7168，大约是14倍的差距
Q的压缩维度d c t d_{c^t}d 
c 
t
 
​
 ：设置为1536，大约是4.7倍
这种设置基于如下考虑：

K和V：由于键和值在推理时需要缓存，因此采用较大的压缩比例，减少内存开销
Q：Q在训练时需要频繁计算，因此采用较小的比例，保留更多特征，确保模型性能
10.5 总结
MLA提出的作用：

对原本MHA的KV Cache作低秩分解，得到一个低维隐向量
在推理阶段MLA只需要缓存该隐向量，由此大大降低需要缓存的数据量
11 上面介绍的降维矩阵和升维矩阵是怎么来的？
在MLA中，降维矩阵（W D K V W^{DKV}W 
DKV
 ）和升维矩阵（W U K , W U V W^{UK},W^{UV}W 
UK
 ,W 
UV
 ）是通过模型训练得到的，类似LoRa

11.1 本质：低秩分解的参数化实现
MLA将原始生成的K和V线性变换矩阵分解为两步：
k t C = W U K ⋅ W D K V h t ⏟ c t K V , v t C = W U V ⋅ W D K V h t ⏟ c t K V k_t^{C}=W^{UK}\cdot \underbrace{{W^{DKV}h_t}}_{c_t^{KV}},v_t^C=W^{UV}\cdot \underbrace{W^{DKV}h_t}_{c_t^{KV}}
k 
t
C
​
 =W 
UK
 ⋅ 
c 
t
KV
​
 
W 
DKV
 h 
t
​
 
​
 
​
 ,v 
t
C
​
 =W 
UV
 ⋅ 
c 
t
KV
​
 
W 
DKV
 h 
t
​
 
​
 
​
 

其中W D K V , W U K , W U V W^{DKV},W^{UK},W^{UV}W 
DKV
 ,W 
UK
 ,W 
UV
 都是可学习的矩阵

11.2 初始化与训练方法
11.2.1 初始化方法
随机初始化：正态分布或其他分布

基于预训练模型的分解（更为常见）

对原始W K , W V W^K,W^VW 
K
 ,W 
V
 进行奇异值分解（SVD）
W K = U k Σ K V K ⊤ ⇒ W D K V ≈ V K ⊤ , W U K ≈ U K Σ K W^K=U_k\Sigma_KV_K^\top\Rightarrow W^{DKV}\approx V_K^\top, W^{UK}\approx U_K\Sigma_K
W 
K
 =U 
k
​
 Σ 
K
​
 V 
K
⊤
​
 ⇒W 
DKV
 ≈V 
K
⊤
​
 ,W 
UK
 ≈U 
K
​
 Σ 
K
​
 
11.2.2 训练过程
端到端学习：通过梯度下降优化所有矩阵
参数共享：W D K V W^{DKV}W 
DKV
 被Key/Value共享，强制共享低维空间
11.3 与LoRa对比
LoRa：在原始权重旁边增加低秩旁路（相当于是两个通路，然后结果相加）
MLA：直接修改原始权重，类似稀疏训练
12 MLA中位置编码有何特殊性？
12.1 旋转编码（RoPE）
论文：https://arxiv.org/abs/2104.09864

旋转位置编码的精妙之处在于

旋转代替加法
相对位置自动显现：编码角度的差值体现了他们语义距离
数学上的优势：旋转操作保持了向量长度不变，只改变方向，避免了数值不稳定，通过旋转矩阵实现，计算高效，适合大模型
举例：

句子：“苹果 喜欢 阳光”
传统方法：给“苹果”加上位置1的编码，给“喜欢”加位置2，直接相加
RoPE：把“苹果”的向量旋转10度，“喜欢”旋转20度，“阳光”旋转30度，模型在计算“苹果”和“阳光”的关系时，会发现它们旋转了20度的差异（对应位置差）
为什么好用？

LLAMA，GLM均选择这种方法
长文本友好：旋转角度可以无限延申，适合处理长句子
兼容性强：直接嵌入刀注意力计算中，不需要修改模型结构
12.2 MLA中，针对位置编码的处理方式
MLA还针对RoPE进行了额外的处理
因为如果在隐向量h t h_th 
t
​
 中包含RoPE，经过升降维操作后，会对位置信息造成破坏，为了解决这一问题，MLA提出了解耦RoPE的方法
具体来说，对于隐向量c t K V c_t^{KV}c 
t
KV
​
 ，不将位置编码包含其中，而是专门为注意力头的Q和K新增向量维度，以添加RoPE的位置信息
具体如图：



DeepSeek通过对Q和K进行拆分为[ q t R , q t C ] [q_t^R,q_t^C][q 
t
R
​
 ,q 
t
C
​
 ]和[ k t R , k t C ] [k_t^R,k_t^C][k 
t
R
​
 ,k 
t
C
​
 ]
其中一部分做压缩( q t C , k t C ) (q_t^C,k_t^C)(q 
t
C
​
 ,k 
t
C
​
 )
一部分做RoPE编码( q t R , k t R ) (q_t^R,k_t^R)(q 
t
R
​
 ,k 
t
R
​
 )，其中R RR可以理解为RoPE的标识符
先对KV联合压缩（LoRa KV Joint Compression）后升维
再对Q压缩降维，后升维
之前提到了KV Cache中，Q的作用只发生在当下（预测下一个token时，其只能看到待预测token之前所有的token），但是在模型训练中，每个输入的token都会通过多头注意力机制生成对应的query, key和value，这些中间数据的维度往往非常高，占用很大
所以论文中也提到为了降低训练过程中激活内存（activation memory），DeepSeek-V2还对queries进行低秩压缩，即便这并不能降低KV Cache，而其对Q的压缩方式和K、V一致，依然是先降维后升维
12.3 总结
在隐向量h t h_th 
t
​
 zho能够包含RoPE，经过升降维操作后，会对位置信息造成破坏
为了解决这个问题，MLA提出解耦RoPE的方法
具体来说，对于隐向量c t K V c_t^{KV}c 
t
KV
​
 ，不将位置编码包含其中，而是专门为注意力头的Q和K新增向量为度，以添加RoPE的位置信息。
13 BatchNorm和LayerNorm的区别？
相同点：两者都是在对数据做规范化

13.1 为什么要做规范化
规范化本质是将非标准数据统一为指定格式的过程

一方面，随着网络深度的增加，各层特征值的分布会逐渐趋近于激活函数输出范围的上下限，导致激活函数饱和，持续如此就会导致梯度消失。归一化可以使特征值的分布重新回归到正态分布，保证特征值在激活函数对输入比较敏感的范围内，从而避免梯度消失，加快收敛速度

另一方面，在机器学习领域，有一个重要假设是IID（独立同分布），规范化也是保证训练数据上训练出来的模型能在测试集上有良好表现。可以防止不同数据分布对模型的影响。

13.2 基本介绍
BatchNorm对一批样本中的每个特征进行归一化
LayerNorm对每个样本中的所有特征进行归一化




图片来源：https://www.pinecone.io/learn/batch-layer-normalization

对于CV，特征依赖不同样本之间的统计参数，而BatchNorm更为有效，因为它消除了不同特征之间的大小关系，同时保留了不同样本之间的大小关系
在NLP领域，LayerNorm更加合适，因为单个样本的不同特征实际上是词语随时间的变化，并且样本内的特征关系非常紧密。
13.3 具体细节对比
13.3.1 BatchNorm
对于输入的小批量数据X = { x 1 , x 2 , . . . , x m } {\bf X}=\{x_1,x_2,...,x_m\}X={x 
1
​
 ,x 
2
​
 ,...,x 
m
​
 }，BatchNorm的计算过程如下：

计算小批量的均值和方差：
μ B = 1 m ∑ i = 1 m x i , σ B 2 = 1 m ∑ i = 1 m ( x i − μ B ) 2 \mu_B=\frac 1m\sum_{i=1}^mx_i,\sigma^2_B=\frac 1m\sum_{i=1}^m(x_i-\mu_B)^2
μ 
B
​
 = 
m
1
​
  
i=1
∑
m
​
 x 
i
​
 ,σ 
B
2
​
 = 
m
1
​
  
i=1
∑
m
​
 (x 
i
​
 −μ 
B
​
 ) 
2
 

对于每个元素x i x_ix 
i
​
 作归一化：
x ^ i = x i − μ B σ B 2 + ϵ \hat x_i=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}
x
^
  
i
​
 = 
σ 
B
2
​
 +ϵ
​
 
x 
i
​
 −μ 
B
​
 
​
 

引入可学习的缩放参数γ \gammaγ和偏移参数β \betaβ：
y i = γ x ^ i + β y_i=\gamma\hat x_i+\beta
y 
i
​
 =γ 
x
^
  
i
​
 +β

13.3.2 LayerNorm
对于输入向量x = ( x 1 , x 2 , . . . x d ) {\bf x}=(x_1,x_2,...x_d)x=(x 
1
​
 ,x 
2
​
 ,...x 
d
​
 )：

计算输入向量的均值方差：
μ L = 1 d ∑ i = 1 d x i , σ L 2 = 1 d ∑ i = 1 d ( x i − μ L ) 2 \mu_L=\frac 1d\sum_{i=1}^dx_i,\sigma^2_L=\frac 1d\sum_{i=1}^d(x_i-\mu_L)^2
μ 
L
​
 = 
d
1
​
  
i=1
∑
d
​
 x 
i
​
 ,σ 
L
2
​
 = 
d
1
​
  
i=1
∑
d
​
 (x 
i
​
 −μ 
L
​
 ) 
2
 

对于每个元素归一化：
x ^ i = x i − μ L σ L 2 + ϵ \hat x_i=\frac{x_i-\mu_L}{\sqrt{\sigma_L^2+\epsilon}}
x
^
  
i
​
 = 
σ 
L
2
​
 +ϵ
​
 
x 
i
​
 −μ 
L
​
 
​
 

引入可学习的缩放参数γ \gammaγ和偏差β \betaβ：
y i = γ x ^ i + β y_i=\gamma\hat x_i+\beta
y 
i
​
 =γ 
x
^
  
i
​
 +β

13.4 总结
特性	LN	BN
归一化繁为	在单个样本的特征维度上进行归一化，即对每个样本的所有特征都归一化	在整个batch维度上进行郭奕华，即对每个神经元的输出在当前小批量
依赖性	对batchsize不敏感，适用于小batch训练	依赖于batchsize，当batchsize较小时，均值方差估计不准确，可能导致效果不稳定
适用场景	适用于RNN，Transformers模型（NLP）	适用于CNN，深度网络（CV）
训练与推理	训练与推理时使用相同的估计量，即每个样本的均值和标准差是独立计算的	训练时使用当前batch的均值方差，推理时使用整个训练集的均值方差，通常通过移动平均估计
14 RMSNorm较于LayerNorm有什么特点？
RMSNorm是LayerNorm的一个简单变体，来自于2019年的论文（Root Mean Square Layer Normalization），被T5、LLAMA和当前流行的DeepSeek模型所使用。

其提出的动机是LayerNorm计算量太大，而RMSNorm性能与LayerNorm相当，但可以节省7%到64%的运算量。

RMSNorm和LayerNorm的主要区别在于：RMSNorm不需要同时计算均值和方差两个统计量，而只需要计算均方根（RMS）这一统计量

14.1 具体计算流程
对于输入向量x = ( x 1 , x 2 , . . . , x d ) {\bf x}=(x_1,x_2,...,x_d)x=(x 
1
​
 ,x 
2
​
 ,...,x 
d
​
 )，RMSNorm的计算过程如下：

计算输入向量的RMS：
R M S ( x ) = 1 d ∑ i = 1 d x i 2 RMS({\bf x})=\sqrt{\frac 1d\sum_{i=1}^d x_i^2}
RMS(x)= 
d
1
​
  
i=1
∑
d
​
 x 
i
2
​
 
​
 

对每个元素x i x_ix 
i
​
 进行归一化：
x ^ i = x i R M S ( x ) + ϵ \hat x_i=\frac{x_i}{RMS({\bf x})+\epsilon}
x
^
  
i
​
 = 
RMS(x)+ϵ
x 
i
​
 
​
 

引入可学习的缩放参数γ \gammaγ：
y i = γ x ^ i y_i=\gamma \hat x_i
y 
i
​
 =γ 
x
^
  
i
​
 

14.2 为什么RMSNorm仅保留缩放参数γ \gammaγ，去除了偏差参数β \betaβ，效果依然OK？（参考）
14.2.1 均值调整的必要性降低
残差连接的作用：在Transformer等能够架构中，残差连接（RC）直接将输入加到输出上，天然保留了输入向量的均值信息。即使归一化过程未显式调整均值（如RMSNorm），模型依然可以通过残差路经传递原始均值，降低了平移参数的必要性。
14.2.2 缩放参数主导分布调整
方向比绝对位置更重要，在自然语言等高维空间，向量方向（由缩放参数γ \gammaγ调整），往往比绝对位置（由平移参数β \betaβ调节）更能表征语义信息。RMSNorm通过调整向量模长，保留方向信息，可能更符合任务需求。
14.2.3 实验验证的适应性
论文实证表面，RMSNorm在多项任务中与LayerNorm性能相当，甚至更优，这说明平移参数可能被过参数化，或其对模型性能的影响被其他机制（注意力，前馈网络）补偿
14.3 对比
特性	LayerNorm	RMSNorm
归一化方式	均零方一的标准归一化	通过除以RMS进行归一化
是否引入偏差参数	是	否
计算复杂度	高（计算均值和方差）	低（仅计算均方根）
适用场景	广泛用于NLP和序列模型（Transformers），在需要较强表达能力的任务中表现优异	适用于计算量大的场景，尤其在LLM和低资源下
表达能力	强，引入了可学习的偏差参数，能够更好地拟合数据分布	较弱，没有偏差参数，但计算效率更高
15 Dynamic Tanh（DyT）是什么？和之前方法相比有何优化?
AI总结：

DyT 通过简单的动态缩放（α）和 tanh 非线性表示，替代了传统归一化层，解决了计算开销和架构复杂性问题，并在多任务中实现了相当或更优的性能。 其核心优势在于： 1. 高效性：减少计算耗时，适合资源敏感场景。 2. 普适性：覆盖视觉、语言、生成模型等多种任务。 3. 理论突破：重新理解归一化层的本质作用，推动无归一化网络的设计。 DyT 的提出不仅是一种工程优化，更挑战了深度学习中的传统假设，为未来网络架构创新提供了新方向。
何凯明最近发布的一篇paper（250314发布，被CVPR接收），Transformers without Normalization

arxiv：https://arxiv.org/abs/2503.10622

项目主页：https://jiachenzhu.github.io/DyT

GitHub：https://github.com/jiachenzhu/DyT

15.1 DyT是什么？
DyT是一种用于替代传统归一化层（如LayerNorm或RMSNorm）的简单元素级操作，其定义为：
DyT ( x ) = γ ⋅ tanh ⁡ ( α x ) + β \text{DyT}(x)=\gamma\cdot \tanh(\alpha x)+\beta
DyT(x)=γ⋅tanh(αx)+β

其中：α , γ , β \alpha,\gamma,\betaα,γ,β均为可学习的参数，tanh ⁡ ( x ) = e x − e − x e x + e − x \tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}tanh(x)= 
e 
x
 +e 
−x
 
e 
x
 −e 
−x
 
​
 ，是一个类似于逻辑回归的激活函数。

15.2 DyT解决了什么问题？
归一化层的基本公式可以表示为：
y = γ ⋅ x − μ σ 2 + ϵ + β y=\gamma\cdot\frac{x-\mu}{\sigma^2+\epsilon}+\beta
y=γ⋅ 
σ 
2
 +ϵ
x−μ
​
 +β

不同的归一化方法主要在如何计算这些统计量上有所区别

BN：主要用于CNN，计算跨批次和通道的均值和方差
LN：主要用于Transformer模型，计算每个样本中每个token的均值和方差
归一化层的作用

为了深入理解归一化层的作用，作者对多个训练好的网络进行了分析，包括视觉模型ViT，语音模型wav2vec2.0和扩散Transformer(DiT)

它们发现，深层LN层的输入输出关系呈现出明显的S型曲线，类似于Tanh函数，尽管LN在理论上是对每个token进行线性变换，但由于不同token的均值方差不同，整体上呈现出非线性特征。



图2显示了ViT，wav2vec2.0，DiT中选定层归一化（LN）层的输入与输出
我们对一个小批量样本进行采样，绘制了每个模型中4个LN层的输入/输出值，这里的输出是在LN中仿射变换之前的值，S型曲线与tanh函数的形状高度相似
此外，早期层中更接近线性的形状也可以用tanh曲线的中心部分来描述，这启发我们提出DyT作为替代解决方案，并引入一个可学习的缩放因子α \alphaα来适应不同的x xx轴尺度
DyT解决的问题：

传统归一化层被认为是训练深度网络（尤其是Transformers）的必要组件，用于稳定训练和加速收敛，然而，存在以下问题：

计算开销：需要计算输入的均值方差
架构复杂性：归一化层的统计计算限制了模型设计的灵活性
性能瓶颈：研究表明，归一化层的非线性特性可能未被充分替代（如极端值压缩）。DyT通过无需统计计算的非线性操作，解决上述问题，同时保持或提升模型性能。
15.3 DyT如何实现？


理论分析：

左图：原始的Transformer块
右图：采用DyT的Transformer块
采用DyT的Transformer在性能上能够匹配或超越其使用归一化层的对应模型
代码实现：

from torch import nn

class DyT(nn.Module):
    def __init__(self, dim, init_alpha=.5):
        super().__init__()
        self.alpha = nn.Parameter(torch.tensor(init_alpha)) # 标量参数
        self.gamma = nn.Parameter(torch.ones(dim)) # 通道级缩放
        self.beta = nn.Parameter(torch.zeros(dim)) # 通道级偏移
        
    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        return self.gamma * x + self.beta
1
2
3
4
5
6
7
8
9
10
11
12
15.4 DyT有何优势
计算效率的提升（表7）
推理速度：在LLAMA-7B中，归一化层的推理时间减少52.4%，整体模型时间减少7.8%
训练速度：训练中归一化层的耗时减少42.2%，整体模型耗时减少8.2%
性能相当或更优
视觉任务：ViT-L在ImageNet上准确率提升0.5%（表1），扩散模型DiT-B的性能指标降低1.0%（表3）
语言任务：在LLAMA-7B,13B,34B,70B的训练损失与RMSNorm持平或略优（表4）
自监督任务：MAE ViT-B和DINO ViT-B性能基本持平（表2）
简化架构和训练流程：
DyT是逐token操作，无需跨token或通道的聚合计算
超参鲁棒性，绝大多数任务中，无需调整原有训练参数（如学习率）即可使用，仅LLM微调需微调α \alphaα的初始化（表11）
15.5 总结
DyT 通过简单的动态缩放（α）和 tanh 非线性表示，替代了传统归一化层，解决了计算开销和架构复杂性问题，并在多任务中实现了相当或更优的性能。 其核心优势在于： 1. 高效性：减少计算耗时，适合资源敏感场景。 2. 普适性：覆盖视觉、语言、生成模型等多种任务。 3. 理论突破：重新理解归一化层的本质作用，推动无归一化网络的设计。 DyT 的提出不仅是一种工程优化，更挑战了深度学习中的传统假设，为未来网络架构创新提供了新方向。

16 怎么理解多token预测（MTP）？
AI总结：

怎么理解多令牌预测（Multi-Token Prediction，MTP） - MTP 的核心思想是让模型一次性预测多个 token，以提升模型的训练效率、生成质量和推理速度。 - 即模型不仅要学习预测下一个 token 的能力，还需要同时具备预测下
n n
n
个token的能力。
16.1 为什么要使用MTP
当前主流采用自回归的LLM都是单token预测，即根据上文预测下一个最可能的token
而MTP的核心思想是让模型一次性预测多个token，以提升模型的训练效率，生成质量和推理速度
16.2 MTP的优势
训练过程：在训练过程中，MTP的训练目标函数同时考虑了多个token的估计准确性，因此被认为可以捕捉token间的依赖关系，从而提升模型效果
推理维度（技术报告中未明确提及，实际上报告中说的是使得输出空间更加稠密）：减少了自回归生成的步数，达到推理加速的效果
缓解短视预测问题：
传统单步预测容易导致模型过度关注局部模式（如语法而非语义）
MTP通过强制模型同时优化多个位置的预测，鼓励其建立全局性的文本理解，这在技术报告提及的性能超越其他开源模型中起到关键作用
16.3 MTP实现细节


如上图所示，用D DD个顺序的模块，预测D DD个tokens，每个MTP模块的具体结构：

输入token首先接入一层共享的embedding层
对于第i ii个token t i t_it 
i
​
 和第k kk个预测深度：
我们首先将第k − 1 k-1k−1层的隐层输出h i k − 1 ∈ R d h_i^{k-1}\in\mathbb{R}^dh 
i
k−1
​
 ∈R 
d
 做归一化处理R M S N o r m ( h i k − 1 ) RMSNorm(h_i^{k-1})RMSNorm(h 
i
k−1
​
 )
在对第i + k i+ki+k位置的token embdding：E m b ( t i + k ) ∈ R d Emb(t_{i+k})\in\R^dEmb(t 
i+k
​
 )∈R 
d
 做归一化处理，即h i k = R M S N o r m ( E m b ( t i + k ) ) h_i^{k}=RMSNorm(Emb(t_{i+k}))h 
i
k
​
 =RMSNorm(Emb(t 
i+k
​
 ))
将上述两个结果concat之后，通过注意力矩阵M k ∈ R d × 2 d M_k\in\mathbb{R}^{d\times 2d}M 
k
​
 ∈R 
d×2d
 做一层线性变换得到h i ′ k ∈ R d h_i^{'k}\in\mathbb{R}^dh 
i
′
 k
​
 ∈R 
d
 
再将h i ′ k h_i^{'k}h 
i
′
 k
​
 输入Transformer层，获得第k kk个预测深度的输出：h i k h_i^kh 
i
k
​
 
最后将h i k h_i^kh 
i
k
​
 通过一个各Module共享的映射O u t H e a d ∈ R V × d OutHead\in\mathbb{R}^{V\times d}OutHead∈R 
V×d
 变换，再经过s o f t m a x softmaxsoftmax处理，计算得到词汇V VV维度的输出概率
16.4 总结
DeepSeek-V3论文中报告了MTP模块的效果，他们在推理过程中，不使用MTP模块，只在训练中使用，以达到提速的效果，使用MTP训练，能够提升模型的回复质量，在MMLU，GSM8K等公开基准上的性能均有提升。

17 详细介绍DeepSeek-R1训练过程的四阶段
尽管DeepSeek-R1-Zero展示了强大的推理能力，并能够自主发展出意想不到的强大推理行为，但它同样面临一些问题，比如可读性差和语言混杂等问题。R1旨在成为一个更易用的模型。

因此，R1并不像R1-Zero那样完全依赖于强化学习过程。而是通过多个阶段完成。

17.1 具体流程
训练分为四个阶段：（SFT->RL->SFT->RL）



冷启动（SFT）：

为了避免RL训练从基础模型开始的早期不稳定的冷启动阶段，构建并收集少量长的CoT数据来微调DeepSeek-V3-Base作为RL的起点。
推理导向的强化学习（RL）：

在冷启动数据上微调DeepSeek-V3-Base后，应用与DeepSeek-R1-Zero中相同的RL方法训练
本阶段侧重于增强模型的推理能力，尤其是在编码、数学、科学和逻辑推理等推理密集型任务中，这些任务涉及具有明确解决方案的明确定义的问题
当RL提示涉及多种语言时，CoT经常表现出语言混合现象，为了减少语言混合问题，在RL训练过程中引入了一种语言的一致性奖励。
拒绝采样与监督微调（SFT）：

当RL过程趋于收敛时，利用训练出的临时模型生产用于下一轮训练的SFT数据（60W推理数据）
与冷启动数据区别在于，此阶段既包含用于推理能力提升的60W数据，也包含20W推理无关的数据。使用这80W样本的精选数据集对DeepSeek-V3-Base进行了两个epoch的微调
全场景强化学习（RL）：

在微调模型的基础上，使用全场景的强化学习数据提升模型回复的有用性和无害性。
对于推理数据，遵循DeepSeek-R1-Zero的方法，利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程
对于通用数据，采用基于模型的奖励来捕捉复杂和细微场景中的人类偏好。
17.2 总结


使用SFT冷启动 ==> 推理导向的强化学习 ==> SFT拒绝采样与监督微调 ==> RL全场景强化学习
四阶段训练，R1达到o1-1217的水平
18 为何在推理任务中强调“规则化奖励”，而非“神经奖励模型”
指的是arXiv.2501.12948（deepseek-r1论文）中2.2.2中指出的一个问题

规则化奖励就像“客观考试评分”，答案对错一目了然
神经奖励模型类似“老师主观打分”，模型可能学会讨好老师却答错题
用规则化奖励更公平、更直接
具体而言，在推理任务中强调规则化奖励而非神经奖励模型的原因如下：

避免奖励黑客问题（reward hacking）：

原文指出：神经奖励模型在大规模强化学习过程中可能出现奖励黑客（the neuralreward model may suffer from reward hacking in the large-scale reinforcement learning process，章节2.2.2）
降低训练复杂性和资源消耗：

显然训练一个神经奖励模型是有额外消耗的，规则化则相当于人为的减少了这部分资源消耗
奖励信号更加清晰可靠：

规则化奖励基于确定性逻辑（数学答案验证、代码编译测试）
for math problems with deterministic results, the model is required to provide the final answer in a specified format … enabling reliable rule-based verification（Section 2.2.2），这种奖励机制直接关联任务目标，避免了神经奖励模型可能引入的评估偏差
总之，章节2.2.2 Reward Modeling明确对比了两种奖励机制的取舍，并解释了选择规则化奖励的核心原因。

19 论文提到的“自我认知”
指的是arXiv.2501.12948（deepseek-r1论文）中2.3.3中指出的一个问题

自我认知（self-cognition）数据具体指的是用于训练模型理解并回答与自身属性、能力边界相关的查询数据。例如：

关于模型身份的问答（你是什么类型的AI）

能力范围的说明（你能处理哪些任务）

训练数据相关询问（你的知识截止到什么时候）

伦理限制声明（为什么有些问题不能回答）

这类数据属于非推理数据（Non-reasoning data），与写作、事实问答、翻译等任务并列，在监督微调阶段用于塑造模型的自我认知能力

文档特别指出，对于这类简单查询（hello!），模型不需要生成思维链，直接给出简洁回应即可。

Section 2.3.3
For simpler queries, such as ‘hello’ we do not provide a CoT in response
20 如何避免模型在RL训练中过度拟合评测任务？
防止模型成为考试机器，除了模拟考（评测任务），还要定期抽查其他科目（多样化任务），确保全面发展

避免方式：（根据arXiv.2501.12948，即deepseek-r1论文）

采用多样化的训练数据分布：
混合推理与非推理数据
在SFT阶段，通过收集涵盖推理任务（如数学、代码）和通用任务（写作、事实问答等）的多样化数据
论文提到：In total, we collect about 600k reasoning related training samples… and 200k training samples that are unrelated，即结合了60万推理相关样本和20万非推理样本（Section 2.3.3）
这种数据多样性迫使模型适应不同场景，降低对单一评测任务的依赖
多阶段训练流程：
冷启动与多阶段RL训练
使用SFT冷启动=>RL推理导向的强化学习==>SFT拒绝采样与监督微调==>RL全场景强化学习四阶段训练
论文指出：在接近RL收敛时，通过拒绝采样生成新SFT数据，并结合通用数据重新微调模型，最后进行二次RL训练（Upon nearing convergence in the RL process, we create new SFT data through rejection sample … then retrain the model. After fine-tuning, the checkpoint undergoes an additional RL process，Section 1）
分阶段训练逐步扩展模型能力，避免过早过拟合
组合多类型奖励信号：
规则化奖励与人类偏好奖励结合
在最终RL阶段，对推理任务使用规则化奖励（如答案准确性、格式要求），对通用任务引入人类偏好奖励模型
论文强调：对于推理数据采用规则奖励，通用数据使用奖励模型捕获人类偏好（For reasoning data, we adhere to rule-based rewards … For general data, we resort to reward models to capture human preferences，Section 2.3.4）
这种混合奖励机制平衡了任务目标与泛化性
拒绝采样筛选高质量响应：
过滤低质量与重复内容
在生成SFT数据时，通过拒绝采样排除语言混杂、冗长或重复的推理过程
论文提到：过滤掉语言混杂的CoT，长段落和代码块（filter out chain-of-thought with mixed languages, long paragraphs, and code blocks，Section 2.3.3），确保训练数据的多样性和可读性，减少模型对噪声或特定模式的依赖
全场景提示分布训练：
覆盖广泛用户需求场景
在最终RL阶段，使用涵盖数学、代码、写作、问答等多场景的提示分布
论文在Section 2.3.4中提到相关内容，这种设计防止模型过度适配单一任务
21 DeepSeek的蒸馏是如何实现的
AI总结：为了使参数规模较小的模型也能具备像 DeepSeek-R1 这样的推理能力，首先通过 DeepSeek-R1推理得到的800k个样本。然后对 6 个不同参数量的开源模型进行了直接有监督微调。 这种方式也就是直接的数据蒸馏。

利用DeepSeek-R1生成的80W数据对Qwen和Llama系列的多个小模型进行了微调，以坍缩将R1的推理能力蒸馏到小模型中的潜力，发布了DeepSeek-R1-Distill系列模型

21.1 蒸馏的基本流程
具体流程：

数据准备：DeepSeek-R1生成80W高质量训练数据，这些数据包含了丰富的推理链和多种任务类型
模型选择：选择Qwen和Llama系列的多个小模型作为学生模型，这些模型的参数规模分别为1.5B, 7B, 8B, 14B, 32B和70B
蒸馏训练：使用DeepSeek-R1生成的数据对这些小模型进行微调，通过优化蒸馏损失函数，使小模型输出尽可能接近DeepSeek-R1的输出（其实就是SFT）
性能评估：对蒸馏后的小模型进行性能评估，验证其推理能力的提升。
21.2 性能表现
R1论文中的table5



几个数据集：

AIME 2024：数学竞赛
MATH-500：OpenAI发布的一个500道数学题
GPQA Diamond：专家设计的198道高难度STEM领域问题
LiveCodeBench：真实世界代码工程任务的评测集
CodeForces：跟力扣差不多的一个算法题库
22 为何在蒸馏过程中仅使用SFT而非RL
在R1论文的2.4节，指出：

For distilled models, we apply only SFT and do not include an RL stage, even though incorporation RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.

效率考量：

成本限制：小模型RL需大量计算资源，SFT仅需单轮微调
知识保留：SFT直接模仿大模型输出，避免RL探索中的知识遗忘
思考：结合SFT和轻量RL能否进一步突破？

23 蒸馏过程中是否存在知识损失？如何量化？
知识损失像压缩图片——高分辨率原图（大模型）缩成小图（小模型）。

蒸馏过程中存在知识损失，且通过以下方式量化（依据原文2.4，3.2节及表格）

23.1 知识损失的存在性
蒸馏模型的性能（如32B模型AIME 72.6%）明显低于原模型DeepSeek-R1（AIME 79.8%）
文档明确指出蒸馏模型仅接近o1-mini，而非匹配，进一步佐证性能差距
23.2 量化方法
具体量化指标：

23.2.1 标准基准测试分数对比（table 4 & 5）
数学推理：AIME 2024 pass@1（32B 72.6% VS 原模型 79.8%）
代码能力：Codeforces Rating（32B 1691 VS 原模型 2029）
综合知识：GPQA Diamond（32B 62.1% VS 原模型 71.5%）
23.2.2 任务类型敏感性分析
需要长链推理的任务（如LiveCodeBench）蒸馏模型性能下降更明显（57.5% VS 65.9%），而结构化任务（如MATH-500）损失较小（94.5% VS 97.3%）
23.3 知识损失的关键因素
规模效应：蒸馏1.5B模型AIME仅28.9%，而32B模型达72.6%，说明小模型因容量限制损失更多知识（表5）
推理深度依赖：深层推理行为（反思与验证）难以被小模型完全复现，导致CodeForces等复杂任务评分差距大。
强化学习专题
参考资料：

西湖大学WINDYLab赵世钰课程：

课件：https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning
视频：https://www.bilibili.com/video/BV1sd4y167NS
刘建平的博客：https://www.cnblogs.com/pinard/p/9385570.html

1 什么是RL？
RL讨论的问题是智能体怎么在复杂、不确定的环境里面去最大化它能获得的奖励
强化学习由两部分构成：智能体和环境
在强化学习过程中，智能体和环境一直在交互。
智能体在环境里面获取某个状态（state）后，它会利用该状态输出一个动作（action），这个动作也被称为决策（decision）
然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励（reward）
智能体的目的就是尽可能多地从环境中获取奖励
一句话总结：Reinforcement learning is learning what to do – how to map situation to actions – so as to maximize a numerical reward signal
《Reinforcement Learning: An Introduction》
强化学习就是学习做什么（即如何把当前地情境映射成动作）才能使得数值化地收益信号最大。
2 RL和监督、非监督、深度学习的区别
AI总结：强化学习、监督学习和无监督学习三者有什么区别呢?

强化学习和监督学习最大的区别是它没有监督学习已经准备好的训练数据输出值的。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的。比如下面的例子中，走路摔倒了才得到大脑的奖励值。
强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。
强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢?
深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小。
强化学习中的损失函数的目的是使总奖励的期望尽可能大。
RL、SFT、USFT并列为三种机器学习方法

RL和SFT

RL和SFT最大的区别是RL没有SFL的标注好的标签，RL只有reward，但是这个reward和SFT的标签是不一样的，它不是事先给出的，而是延后给出的（看到结果才给出）。
RL的每一步与时间顺序先后关系紧密，而SFT的训练数据一般是独立的，没有前后的依赖关系
RL和USFT

RL和USFT的区别，也是在奖励值这个地方，非监督学习是没有输出值也没有奖励的，只有数据特征，同时和SFT一样，数据也是独立的。
维度	SFT	USFT	RL
数据	带标签的静态数据	无标签的静态数据	动态交互生成数据
反馈	即时标签反馈	无显式反馈	延迟奖励信号
目标	预测准确率最大化	数据结构发现	累积奖励最大化
应用	分类、归回	聚类、降维	决策优化、控制
复杂度	中（依赖标注质量）	低（无需标注）	高（需处理长期依赖）
3 RL中所谓的损失函数与深度学习中的损失函数有何区别？
DL中的损失函数的目的是使得预测值和真实值之间的差距尽可能小
RL中的损失函数的目的是使得奖励的期望尽可能大
维度	DL	RL
核心目标	最小化预测误差	最大化累积奖励
数据性质	静态、独立同分布	动态生成、时序相关
动态性	固定（如交叉熵）	随策略或环境变化（如Bellman误差动态更新）
优化对象	模型输出（如分类概率）	策略、价值函数或其组合
依赖环境	无需交互、依赖静态数据	需与韩晶交互获取奖励信号
4 RL历史
传统强化学习阶段（1950s-2000s）：
动态规划（DP）：
原理：通过递归分解问题，计算每个状态的最优价值函数（如价值迭代）或直接优化策略（如策略迭代）
改进点：首次将数学规划引入决策过程，但需要完整的环境模型（即状态转移概率矩阵），且计算复杂度高，仅适用于小规模问题。
蒙特卡罗（MC）：
原理：通过随机采样轨迹（如玩完一局游戏）来估计价值函数。无需环境模型
改进点：解决了DP依赖模型的问题，但需要完整轨迹且方差大、收敛慢。例如，MC策略迭代通过经验平均更新策略，但数据利用率低。
时序差分（TD）：
代表算法：Q学习（1989）和SARSA（1994）
原理：结合DP和MC，通过单步更新，在线学习
改进点：Q学习是免模型的，且支持在线学习，但需离散状态动作空间，难以处理高维问题
策略优化和深度学习阶段（2000s-2010s）：
策略梯度：
原理：直接优化策略（如动作概率分布），通过梯度上升最大化期望奖励
改进点：适用于连续动作空间（如机器人控制），但梯度估计方差大、训练不稳定。REINFOCEMENT算法（策略梯度的早期代表，由Williams于1992年提出），通过整条轨迹更新策略，但样本效率低
AC系列方法：
原理：结合策略梯度（Actor）和值函数（Critic），Actor负责生成动作，Critic负责评估动作价值
改进点：通过Critic减少梯度方差，提升训练效率，例如A3C（Asynchronous Advantage Actor-Critic）支持并行训练
深度Q网络（DQN，2013）：
原理：利用DL近似Q值函数，结合经验回放（存储历史数据）和固定目标网络（稳定训练）
改进点：首次在Atari游戏中超越人类标新啊，解决了高维状态（如图像输入）的表示问题，但动作空间仍需离散。
深度强化学习扩展（2010s-至今）：
改进型DQN（解决Q值高估问题）：
代表算法:Double DQN（解决Q值高估问题），Dueling DQN（分离状态价值和动作优势）
改进点：通过结构优化提升稳定性和泛化性
策略优化进阶：
代表算法：TRPO（Trust Region Policy Optimization，信任域策略优化）、PPO（Proximal Policy Optimization，近端策略优化）、GRPO（Group Relative Policy Optimization）
模仿学习与逆强化学习
原理：模仿专家行为（如自动驾驶），或从数据反推奖励函数
改进点：减少探索成本，提升安全性和可解释性。
算法改进的核心逻辑：

从依赖模型到免模型：DP依赖环境，Q学习免模型
从离散到连续空间：Q学习离散，PG连续
从低效到高效采样：MC需完整轨迹，TD和AC实现单步更新，经验回放提升数据利用率
从单一到混合：AC结合值函数与PG，深度RL结合DL的特征挖掘能力
5 RL分类
5.1 分类图示


5.2 根据智能体动作选取方式分类
根据智能体动作选取方式（学习目标不同，目前最主流的方法）

分类：
value-based：Q-learning
policy-based：TRPO、PPO
actor-critic：A3C
根据是否构建模型（环境是否已知）：

model-free：绝大多数主流算法，如DQN，PPO
model-based：动态规划
根据执行策略与评估策略是否一致（学习方式不同）

on-policy：SARSA
off-policy：Q-learning
根据算法更新机制

单步更新的时序差分（TD）：Q-learning
回合更新的蒙特卡洛（MC）：REINFORCEMENT
5.2.1 基于价值的方法
核心思想：通过优化价值函数（如状态值函数V ( s ) V(s)V(s)或动作值函数Q ( s , a ) Q(s,a)Q(s,a)）来间接推导最优策略。智能体选择动作时倾向于最大化未来的累积奖励
代表算法：
Q学习（通过贝尔曼方程迭代更新Q表、适用于离散状态动作空间）
DQN（用神经网络拟合Q值函数，引入经验回放和目标网络解决训练不稳定性，适用于高维状态空间）
特点与局限性：
优点：采样效率高，收敛稳定，适合离散动作场景
缺点：无法直接处理连续动作空间，策略依赖价值函数估计精度
数学基础：贝尔曼方程驱动价值迭代
5.2.2 基于策略的方法
核心思想：直接优化策略函数π ( a ∣ s ) \pi(a|s)π(a∣s)，即状态到动作的概率分布，无需显式估计价值函数。通过策略梯度上升最大化长期回报
代表算法：
REINFORCEMENT：蒙特卡洛采样估计梯度，但高方差导致收敛慢。
PPO：通过剪切目标函数限制策略更新幅度，平衡探索与利用，称为工业界主流（如ChatGPT的训练）
TRPO：引入KL三度约束策略更新，确保训练稳定性
特点与局限：
优点：支持连续动作空间，策略表达灵活（如概率分布）
缺点：高方差导致样本效率低，易陷入局部最优
数学基础：策略梯度定理
5.2.3 结合价值和策略的方法（AC）
核心思想：融合价值函数和策略函数的优势，通过Actor生成策略，Critic评估动作价值，协同优化策略
代表算法：
A2C/A3C：多线程异步更新加速训练，Critic计算优势函数，指导Actor优化
SAC（soft AC）：引入熵正则化鼓励坍缩，适合复杂连续控制任务（如机器人行走）
特点与局限：
优点：平衡探索与利用，训练效率高，适合复杂任务
缺点：结构复杂，要同时调优A和C网络
数学基础：TD误差联合优化策略与价值函数
5.3 总结
参考：强化学习算法与应用综述-李茹杨.pdf

维度	基于价值	基于策略	A-C方法
策略生成方式	间接（贪心选择Q值最大）	直接（输出动作概率）	策略与价值函数联合优化
动作空间适用性	离散	连续/离散	连续/离散
训练稳定性	高（低方差）	低（高方差）	中等（需平衡两者）
典型算法	Q-learning，DQN	REINFORCEMENT，PPO	A2C，SAC
6 马尔可夫决策过程（MDP）的理解
RL中智能体与环境之间的交互，智能体得到环境的状态后，它会采取动作，并把这个采取的动作返还给环境。环境得到智能体的动作后，它会进入下一个状态，把下一个状态传给智能体。
智能体与环境就是这样进行交互，这个交互过程可以通过MDP表示，所以MDP是RL的基本框架
6.1 什么是马尔可夫？
马尔可夫指的是一种无记忆性（Memoryless Property），即未来状态仅依赖于当前状态，而与过去的历史状态无关。
6.2 马尔可夫最重要的性质是什么？
无记忆性是核心性质
这一性质使得建模和计算复杂度大幅降低，无需维护完整历史状态，只需关注当前状态
P ( S t + 1 ∣ S t , S t − 1 , . . . S 0 ) = P ( S t + 1 ∣ S t ) P(S_{t+1}|S_t,S_{t-1},...S_0)=P(S_{t+1}|S_t)P(S 
t+1
​
 ∣S 
t
​
 ,S 
t−1
​
 ,...S 
0
​
 )=P(S 
t+1
​
 ∣S 
t
​
 )
6.3 马尔可夫过程是什么？
MP是一个满足马尔可夫性的随机过程，由以下两部分构成：
状态空间
状态转移矩阵
6.4 马尔可夫决策过程是什么？
MDP是MP的扩展，引入了智能体动作和奖励机制，用于建模序贯决策问题，核心元素包括
< S , A , P , R , γ > \left<S,A,P,R,\gamma\right>
⟨S,A,P,R,γ⟩

S SS：状态空间，{ s 0 , . . . , s n } \{s_0,...,s_n\}{s 
0
​
 ,...,s 
n
​
 }
A AA：动作空间，{ a 0 , . . . , a m } \{a_0,...,a_m\}{a 
0
​
 ,...,a 
m
​
 }，实践中动作可能是绑定状态的，不一定每个状态都能采取所有的动作。
P PP：转移矩阵，P ( s ′ ∣ s , a ) P(s'|s,a)P(s 
′
 ∣s,a)，表示在状态s ss执行动作a aa后转移到状态s ′ s's 
′
 的概率
R RR：奖励函数，R ( s , a , s ′ ) R(s,a,s')R(s,a,s 
′
 )，表示上述转移的即时奖励
γ \gammaγ：折现因子，未来奖励折现到当前的一个衰减因子
MDP的目标是找到最优策略，即从状态到动作的映射，以最大化长期累积奖励，它是RL的理论基础，通过DP、MC、TD等方法求解

8 贝尔曼方程概述
贝尔曼方程是强化学习中的核心数学工具，用于描述状态或状态或状态—动作对的价值与其后续状态价值之间的递归关系。

8.1 核心思想：DP与马尔可夫性
递归分解：将长期回报分解为即时奖励（当前动作的收益）和折现后的未来回报（后续状态的期望值），体现一步前瞻的思想
马尔可夫性：未来状态仅依赖当前状态和动作，与历史无关，使得贝尔曼方程成立
8.2 贝尔曼方程表达
V ( s ) = R ( s ) + γ ∑ s ′ ∈ S p ( s ′ ∣ s ) V ( s ′ ) V(s)=R(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')
V(s)=R(s)+γ 
s 
′
 ∈S
∑
​
 p(s 
′
 ∣s)V(s 
′
 )

贝尔曼方程定义了当前状态与伟来状态之间的关系。未来奖励的折扣总和和加上即时奖励，就组成了贝尔曼方程。
8.3 贝尔曼方程的推导
V ( s ) = E [ G t ∣ s t = s ] = E [ r t + 1 + γ r t + 2 + γ 2 r t + 3 + . . . ∣ s t = s ] = E [ r t + 1 ∣ s t = s ] + γ E [ r t + 2 + γ r t + 3 + γ 2 r t + 4 + . . . ∣ s t = s ] = R ( s ) + γ E [ G t + 1 ∣ s t = s ] = R ( s ) + γ E [ V ( s t + 1 ∣ s t = s ) ] = R ( s ) + γ ∑ s ′ ∈ S p ( s ′ ∣ s ) V ( s ′ )
V(s)=𝔼[Gt|st=s]=𝔼[rt+1+γrt+2+γ2rt+3+...|st=s]=𝔼[rt+1|st=s]+γ𝔼[rt+2+γrt+3+γ2rt+4+...|st=s]=R(s)+γ𝔼[Gt+1|st=s]=R(s)+γ𝔼[V(st+1|st=s)]=R(s)+γ∑s′∈Sp(s′|s)V(s′)
𝑉
(
𝑠
)
=
𝐸
[
𝐺
𝑡
|
𝑠
𝑡
=
𝑠
]
=
𝐸
[
𝑟
𝑡
+
1
+
𝛾
𝑟
𝑡
+
2
+
𝛾
2
𝑟
𝑡
+
3
+
.
.
.
|
𝑠
𝑡
=
𝑠
]
=
𝐸
[
𝑟
𝑡
+
1
|
𝑠
𝑡
=
𝑠
]
+
𝛾
𝐸
[
𝑟
𝑡
+
2
+
𝛾
𝑟
𝑡
+
3
+
𝛾
2
𝑟
𝑡
+
4
+
.
.
.
|
𝑠
𝑡
=
𝑠
]
=
𝑅
(
𝑠
)
+
𝛾
𝐸
[
𝐺
𝑡
+
1
|
𝑠
𝑡
=
𝑠
]
=
𝑅
(
𝑠
)
+
𝛾
𝐸
[
𝑉
(
𝑠
𝑡
+
1
|
𝑠
𝑡
=
𝑠
)
]
=
𝑅
(
𝑠
)
+
𝛾
∑
𝑠
′
∈
𝑆
𝑝
(
𝑠
′
|
𝑠
)
𝑉
(
𝑠
′
)
V(s)
​
  
=E[G 
t
​
 ∣s 
t
​
 =s]
=E[r 
t+1
​
 +γr 
t+2
​
 +γ 
2
 r 
t+3
​
 +...∣s 
t
​
 =s]
=E[r 
t+1
​
 ∣s 
t
​
 =s]+γE[r 
t+2
​
 +γr 
t+3
​
 +γ 
2
 r 
t+4
​
 +...∣s 
t
​
 =s]
=R(s)+γE[G 
t+1
​
 ∣s 
t
​
 =s]
=R(s)+γE[V(s 
t+1
​
 ∣s 
t
​
 =s)]
=R(s)+γ 
s 
′
 ∈S
∑
​
 p(s 
′
 ∣s)V(s 
′
 )
​
  
​
 

分步解析：

价值函数定义
展开期望表达式
分解期望（关键步骤）
引入即时奖励函数
递归表达价值函数
展开状态转移概率
9 矩阵形式贝尔曼方程的解析解难以求解的原因
矩阵形式的贝尔曼方程：
V = R + γ P V V=R+\gamma PV
V=R+γPV

贝尔曼方程的矩阵形式可以写成：
V = ( I − γ P ) − 1 R V=(I-\gamma P)^{-1}R
V=(I−γP) 
−1
 R
9.1 原因一：计算复杂度高
贝尔曼方程的解析涉及矩阵求逆，这是最主要的原因
矩阵求逆的时间复杂度为O ( n 3 ) O(n^3)O(n 
3
 )，其中n nn是状态空间的维度。当状态数量庞大时，矩阵求逆在计算上变得不可行。
9.2 无折现情况下的不可逆性
当γ = 1 \gamma=1γ=1时（无折现），矩阵I − P I-PI−P的秩不满（存在平稳分布），导致矩阵不可逆。
此时解析解不存在，必须引入额外约束（如固定某个状态的值）才能求解，但这已超出标准解析解的范畴。
10 计算贝尔曼方程的常见方法之间的区别
计算方法主要有：动态规划、蒙特卡罗法、时间差分学习、直接解析求解
它们的核心qu别在于是否依赖环境、更新方式、计算效率以及适用场景
10.1 动态规划（DP）
核心思想：基于模型（已知状态转移概率和奖励函数），通过迭代求解贝尔曼方程。
典型算法：
策略迭代：交替进行策略评估（通过贝尔曼期望方程迭代计算值函数）和策略改进（选择更优动作）
值迭代：直接迭代最优方程，逐步逼近最优值函数
特点：
需要完整的模型信息（model-based）
每次更新考虑所有可能的后续状态（全备份），计算精确但开销大。
适用于小规模状态空间，理论保证收敛
区别：策略迭代显式维护策略并逐步优化，值迭代隐式优化策略。
10.2 蒙特卡罗法（MC）
核心思想：通过无模型的采样轨迹（完整episode），用经验平均回报估计值函数。
实现：基于贝尔曼方程的期望形式，通过采样替代期望计算。
特点：
无需环境模型（model-free）
需要完整轨迹，更新仅在episode结束时执行。
估计无偏但方差高，收敛速度较慢。
适合回合制任务（如游戏通关）。
10.3 时间差分学习（TD）
核心思想：结合动态规划的自举（bootstrapping）和蒙特卡洛的采样，实现在线学习。
典型算法：
TD(0)：单步更新，用当前奖励和下一状态估计值调整当前值（贝尔曼方程的增量形式）
SARSA/Q-learning：基于贝尔曼方程的动作值函数更新（SARSA用贝尔曼期望方程，Q-learning用贝尔曼最优方程）。
特点：
无模型（model-free）：无需完整轨迹。
更新方差低但可能有偏（因自举引入误差）。
适用于连续任务和在线学习。
10.4 直接解析求解
核心思想：将贝尔曼方程转化为线性方程组，通过矩阵求逆求解
公式：V = ( I − γ P ) − 1 R V=(I-\gamma P)^{-1}RV=(I−γP) 
−1
 R
特点：精确，但复杂度高，实际应用受限，多用于理论分析。
10.5 总结
方法	依赖模型	更新方式	计算效率	适用场景
DP	是	全备份，批量更新	低（小规模）	小状态空间，精确计算
MC	否	完整轨迹，延迟更新	中等	回合制任务，无模型场景
TD	否	单步/多步在线更新	高	连续任务，在线学习
解析解	是	矩阵运算	极低（极小）	理论验证
11 贝尔曼期望方程与贝尔曼最优方程
通俗理解贝尔曼期望方程和最优方程的用处（假设在一个网格世界中）：

期望方程：计算某个固定移动策略（如随机移动）下每个格子的得分。
最优方程：直接找到最快到达终点的路经对应的格子得分。
11.1 期望方程
状态值函数：
V π ( s ) = E π [ R t + 1 + γ V π ( s t + 1 ) ∣ s t = s ] V_{\pi}(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma V_{\pi}(s_{t+1})|s_t=s]
V 
π
​
 (s)=E 
π
​
 [R 
t+1
​
 +γV 
π
​
 (s 
t+1
​
 )∣s 
t
​
 =s]

动作值函数：
Q π ( s , a ) = E π [ R t + 1 + γ Q π ( s t + 1 , a t + 1 ) ∣ s t = s , a t = a ] Q_{\pi}(s,a)=\mathbb{E}_{\pi}[R_{t+1}+\gamma Q_{\pi}(s_{t+1},a_{t+1})|s_t=s,a_t=a]
Q 
π
​
 (s,a)=E 
π
​
 [R 
t+1
​
 +γQ 
π
​
 (s 
t+1
​
 ,a 
t+1
​
 )∣s 
t
​
 =s,a 
t
​
 =a]

解释：

目的：评估固定策略π \piπ下的状态或动作的长期期望回报
核心思想：
V π ( s ) V_{\pi}(s)V 
π
​
 (s)：在状态s ss下遵循策略π \piπ的期望回报
Q π ( s , a ) Q_{\pi}(s,a)Q 
π
​
 (s,a)：在状态s ss下执行动作a aa后继续遵循π \piπ的期望回报
递归分解回报：即时奖励 + 未来折现回报的期望
关键点：适用于策略评估（如策略迭代的第一步）
11.2 最优方程
最优状态值函数：
V ∗ ( s ) = max ⁡ ( R ( s , a ) + γ ∑ s ′ ∈ S p ( s ′ ∣ s , a ) V ∗ ( s ′ ) ) V^{*}(s)=\max\left(R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V^{*}(s')\right)
V 
∗
 (s)=max(R(s,a)+γ 
s 
′
 ∈S
∑
​
 p(s 
′
 ∣s,a)V 
∗
 (s 
′
 ))

V ∗ ( s ) V^*(s)V 
∗
 (s)：在状态s ss下，遵循最优策略时能获得的长期累积奖励的最大期望值
R ( s , a ) R(s,a)R(s,a)：在状态s ss下执行动作a aa后获得的即时奖励
p ( s ′ ∣ s , a ) p(s'|s,a)p(s 
′
 ∣s,a)：从状态s ss执行动作a aa后，转移到新状态s ′ s's 
′
 的概率
V ∗ ( s ′ ) V^*(s')V 
∗
 (s 
′
 )：在转移后的状态s ′ s's 
′
 下继续遵循最优策略的累积奖励
最优动作值函数：
Q ∗ ( s , a ) = R ( s , a ) + γ ∑ s ′ ∈ S p ( s ′ ∣ s , a ) max ⁡ a ′ Q ∗ ( s ′ , a ′ ) Q^*(s,a)=R(s,a)+\gamma \sum_{s'\in S}p(s'|s,a)\max_{a'}Q^*(s',a')
Q 
∗
 (s,a)=R(s,a)+γ 
s 
′
 ∈S
∑
​
 p(s 
′
 ∣s,a) 
a 
′
 
max
​
 Q 
∗
 (s 
′
 ,a 
′
 )

Q ∗ ( s , a ) Q^*(s,a)Q 
∗
 (s,a)：在状态s ss下执行动作a aa，并从此之后遵循最优策略时，能获得的长期累积奖励的最大期望值。
max ⁡ a ′ Q ∗ ( s ′ , a ′ ) \max_{a'}Q^*(s',a')max 
a 
′
 
​
 Q 
∗
 (s 
′
 ,a 
′
 )：在转移后的状态s ′ s's 
′
 下，选择最优动作a ′ a'a 
′
 以最大化后续累积奖励。
解释：

目的：直接找到最优策略π ∗ \pi^*π 
∗
 ，最大化长期回报。

核心思想：通过max ⁡ \maxmax操作直接选择最优动作

关键点：

无需显式策略：通过最大化隐式策略包含最优策略
适用于策略优化，如值迭代，Q-learning
11.3 核心区别
特性	期望方程	最优方程
目标	评估给定策略π \piπ的表现	找到最优策略π ∗ \pi^*π 
∗
 
操作符	对策略和环境求期望（∑ \sum∑）	对动作取最大值（max ⁡ \maxmax）
是否依赖策略	是	否（隐式包含最优策略）
应用场景	策略评估（如策略迭代的第一步）	策略优化（如值迭代、Q-Learning）
12 如果数据流不具备马尔可夫性质时，应当如何处理强化学习任务？
常见的解决方案：

状态表征增强：通过引入历史信息或时序特征，将非马尔可夫数据转换为近似马尔可夫的状态表征

循环神经网络（RNN，LSTM）：利用RNN类模型对历史序列建模，生成包含时序依赖关系的状态表征。例如，用LSTM网络将过去多个时间步的状态压缩为当前状态的补充信息。
注意力机制：动态关注关键历史状态，例如Transformer架构通过自注意力机制捕捉长距离依赖关系，筛选对当前决策重要的历史片段。
滑动窗口拼接：将最近k个时刻的状态、动作、奖励拼接为新的状态向量，适用于短期依赖场景。
模型结构改进：设计适应非马尔可夫性的强化学习算法

记忆增强网络：引入外部记忆模块（如神经图灵机），使智能体能够存储和检索长期历史信息。
分层强化学习：将任务分解为高层策略（规划长期目标）和底层策略（执行具体动作），高层策略可处理非马尔要可夫性。
转换马尔可夫决策过程：若环境本身具有马尔可夫性质但数据观测不完整，可通过以下方式重建MDP

延迟奖励分配：将多步动作的延迟奖励拆解到相关状态，例如蒙特卡罗法回溯完整轨迹的奖励影响。
状态聚合：将相似历史轨迹归类为同一抽象状态，例如使用聚类算法合并具有相同转移规律的非马尔可夫状态。
13 手写第n步的价值函数的更新公式，当n越来越大时，价值函数的期望和方差分别变大或是变小？
13.1 第n步价值函数更新公式
在n nn步TD学习中，价值函数的更新公式为：
V ( S t ) ← V ( S t ) + α [ G t ( n ) − V ( S t ) ] V(S_t)\leftarrow V(S_t)+\alpha[G_t^{(n)}-V(S_t)]
V(S 
t
​
 )←V(S 
t
​
 )+α[G 
t
(n)
​
 −V(S 
t
​
 )]

其中：

V ( S ) V(S)V(S)是状态价值函数，t tt表示从时刻t tt开始

α \alphaα是学习率

G t ( n ) G^{(n)}_tG 
t
(n)
​
 是n nn步回报，结合了前n nn步的实际奖励和后n nn步的状态价值估计

n nn步回报G t ( n ) G_t^{(n)}G 
t
(n)
​
 定义为：
G t ( n ) = R t + 1 + γ R t + 2 + ⋅ ⋅ ⋅ + γ n − 1 R t + n + γ n V ( S t + n ) G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\cdot\cdot\cdot+\gamma^{n-1}R_{t+n}+\gamma^n V(S_{t+n})
G 
t
(n)
​
 =R 
t+1
​
 +γR 
t+2
​
 +⋅⋅⋅+γ 
n−1
 R 
t+n
​
 +γ 
n
 V(S 
t+n
​
 )

γ \gammaγ是折现因子
该公式表示从时刻t tt开始的n nn步时序差分（n-step TD）回报，由两部分组成：
实际获得的累积奖励：从t + 1 t+1t+1到t + n t+nt+n步的即时奖励
未来状态的估计价值：以γ n \gamma^nγ 
n
 折现
13.2 当n越来越大时，价值函数的期望和方差分别变大或是变小？
期望：

n nn增大时，价值函数的期望会更接近真实值（偏差减小）
因为n nn步回报中包含了更多实际奖励的累积，减少了对后续状态估计值的依赖，从而降低了估计的偏差。
方差：

n nn增大时，价值函数的方差会变大
因为n nn步回报需要累加更多随机变量（即每一步的奖励），即使有折现因子衰减，随机性的累积仍会导致方差增大
偏差—方差权衡，这是RL中的典型表现：

MC无偏但高方差
TD(0)（n = 1 n=1n=1）有偏但低方差
14 on-policy和off-policy的区别
同策略 v.s. 异策略

同轨策略 v.s. 离轨策略‘

在这之前，先搞清楚一个问题：

什么是行为策略（Behavior Policy）和目标策略（Target Policy）
行为策略是用来与环境互动产生数据的策略，即在训练过程中做决策
目标策略在行为策略产生的数据中不断学习、优化，即学习训练晚比后拿去应用的策略。
14.1 为什么需要两个策略?
因为从人类的直观上来看，为了解决强化学习中的探索与利用（exploration and exploitation），我们可以利用一个策略（行为策略）来保持探索性，提供多样化的数据，而不断地优化另一个策略（目标策略）。

14.2 On-Policy
行为策略与目标策略完全相同，算法通过当前策略生成数据，并直接利用这些数据更新该策略。
特点：
数据生成和策略更新高度耦合，需不断用最新策略采样新数据。
好处是简单粗暴，直接利用数据就可以优化其策略，但这样地处理会导致策略其实是在学习一个局部最优，因为On-policy没办法很好地同时保持既探索又利用。
14.3 Off-Policy
行为策略与目标策略相互独立，算法可利用其他策略（如历史策略、随机策略）生成的数据来优化目标策略。
特点：
数据可复用，允许更高效地探索和样本利用。
而Off-Policy将目标策略和行为策略分开，可以在保持探索的同时，更能求到全局最优值。
难点在于，怎么在一个策略下产生的数据优化另一个策略（解决方案：重要性采样）
14.3 总结
维度	On-Policy	Off-Policy
策略一致性	行为策略与目标策略一致	行为策略与目标策略分离
数据生成	必须通过当前策略实时生成新数据	可复用历史数据或其他策略生成的数据
探索与利用	通常结合探索（如ε贪心）直接优化目标策略	行为策略负责探索（如随机动作）、目标策略专注利用
样本效率	较低（需持续采样新数据）	较高（支持经验回放、数据复用）
算法复杂度	较简单（无需处理策略差异）	较复杂（需处理分布偏移，如重要性采样）
收敛稳定性	通常更稳定	可能因策略差异导致方差较大
15 RL中策略随机探索怎样实现？
15.1 写在开始前
抛开RL算法的细节，几乎所有RL算法都可以抽象成如下的形式：
收集数据：与环境交互，收集学习样本
学习样本：学习收集到的样本中的信息，提升策略
RL算法的最终目的是学习每种状态下的最优动作，而在训练过程中，收敛到最优策略π ∗ \pi^*π 
∗
 前的当前策略π \piπ并非最优，所以它提供的动作并非最优
为了找到动作空间里潜在的最优动作，算法必须尝试当前策略π \piπ认为的非最优的动作，因此RL算法中的策略需要有随机探索的能力
15.2 随机探索怎样实现
主要分为确定性策略和随机性策略两种：

15.2.1 确定性策略
数学表达：
确定性策略函数表示为：π : S → A \pi:S\rightarrow Aπ:S→A，直接输出状态s ss对应的具体动作a aa，例如a = π ( s ) a=\pi(s)a=π(s)
这意味着对于同一状态，策略始终选择固定动作。
探索方式：由于缺乏内在随机性，需通过外部方法引入探索：
ε贪心：以ε概率随机选择动作，其余时间按确定性策略执行
动作噪声注入：在输出动作上迭加高斯噪声，或通过参数扰动
15.2.2 确定性策略
数学表达：
策略表示为条件概率分布π ( a ∣ s ) \pi(a|s)π(a∣s)，输出动作a aa的概率
它本身带有随机性，获取动作时只需对概率分布进行采样即可
探索方式：内在随机性支持自然探索，如：
概率采样：按分布直接采样动作（如PPO中的概率比裁剪）
15.3 Q函数构造的确定性策略及其增加随机性的方式
Q函数构造确定性策略是一种常见策略形式，即：
π ( s ) = arg ⁡ max ⁡ a ( Q ( s , a ) ) \pi(s)=\arg\max_a(Q(s,a))
π(s)=arg 
a
max
​
 (Q(s,a))

即选取Q值最大的动作作为最优动作
注意：一般只在动作空间离散的情况下采用这种策略，若动作空间连续则上式需要经过复杂优化求解过程。
可用ε贪心法将上述确定性策略改造成具有探索能力的策略，即以ε概率随机探索，其余实现选取Q值最大的动作。
16 SARSA概述
SARSA（state-action-reward-state-action）是一种基于时序差分（TD）的强化学习算法
其核心是通过与环境的交互迭代更新动作价值函数（Q值），最终学习到最优策略。
16.1 更新公式与算法流程
基于五元组( S t , A t , R t + 1 , S t + 1 , A t + 1 ) (S_t,A_t,R_{t+1},S_{t+1},A_{t+1})(S 
t
​
 ,A 
t
​
 ,R 
t+1
​
 ,S 
t+1
​
 ,A 
t+1
​
 )进行Q值更新，更新公式为：
Q ( S t , A t ) ← Q ( S t , A t ) + α [ R t + 1 + γ Q ( S t + 1 , A t + 1 ) − Q ( S t , A t ) ] Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
Q(S 
t
​
 ,A 
t
​
 )←Q(S 
t
​
 ,A 
t
​
 )+α[R 
t+1
​
 +γQ(S 
t+1
​
 ,A 
t+1
​
 )−Q(S 
t
​
 ,A 
t
​
 )]

其中R t + 1 + γ Q ( S t + 1 , A t + 1 ) R_{t+1}+\gamma Q(S_{t+1},A_{t+1})R 
t+1
​
 +γQ(S 
t+1
​
 ,A 
t+1
​
 )表示当前奖励与下一动作状态对的Q值加权和，α \alphaα是学习率。

具体更新流程：

选择动作：在当前状态S t S_tS 
t
​
 ，根据策略（如ε贪心）选择动作A t A_tA 
t
​
 
执行动作：执行动作A t A_tA 
t
​
 ，获得奖励R t + 1 R_{t+1}R 
t+1
​
 并转移到新状态S t + 1 S_{t+1}S 
t+1
​
 
选择下一动作：在状态S t + 1 S_{t+1}S 
t+1
​
 中，根据相同策略选择下一动作A t + 1 A_{t+1}A 
t+1
​
 
更新Q值：使用五元组( S t , A t , R t + 1 , S t + 1 , A t + 1 ) (S_t,A_t,R_{t+1},S_{t+1},A_{t+1})(S 
t
​
 ,A 
t
​
 ,R 
t+1
​
 ,S 
t+1
​
 ,A 
t+1
​
 )更新Q值
16.2 SARSA的on-policy性质
SARSA是一种on-policy算法，原因在于：

行为策略与目标策略一致：
算法在更新Q值时使用的下一动作A t + 1 A_{t+1}A 
t+1
​
 必须基于当前策略（如ε贪心）选择，而非最优动作。
这意味着SARSA的探索（如随机动作）与利用（如选择最大Q值动作）均受同一策略控制。
16.3 SARSA的特点
优点：适应随机环境（因更新依赖实际动作，对随机环境更鲁棒），在线学习（每一步交互后即时更新，无需完整轨迹），安全性（探索与利用的平衡使其在路经规划等任务中更保守可靠）

缺点：收敛速度慢（因兼顾策略的随机性），学习过程较Q-learning保守；且无法应对高维状态（Q表）。

17 Q-learning概述
Q-learning是经典的时序差分算法，与SARSA几乎完全相同，只在更新公式上有所区别。
但它具有根本的差异，Q-learning是off-policy算法
17.1 更新公式和算法流程
更新公式为：
Q ( S t , A t ) ← Q ( S t , A t ) + α [ R t + 1 + γ max ⁡ A t + 1 Q ( S t + 1 , A t + 1 ) − Q ( S t , A t ) ] Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma \max_{A_{t+1}} Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
Q(S 
t
​
 ,A 
t
​
 )←Q(S 
t
​
 ,A 
t
​
 )+α[R 
t+1
​
 +γ 
A 
t+1
​
 
max
​
 Q(S 
t+1
​
 ,A 
t+1
​
 )−Q(S 
t
​
 ,A 
t
​
 )]

这里跟16节中的SARSA更新公式的关键差异就是这里选取的下一步动作是严格使得Q值最大的Q值，这就是完全

具体更新流程：

选择动作：在当前状态S t S_tS 
t
​
 ，根据策略（如ε贪心）选择动作A t A_tA 
t
​
 
执行动作：执行动作A t A_tA 
t
​
 ，获得奖励R t + 1 R_{t+1}R 
t+1
​
 并转移到新状态S t + 1 S_{t+1}S 
t+1
​
 
选择下一动作：在状态S t + 1 S_{t+1}S 
t+1
​
 中，根据相同策略选择下一动作A t + 1 A_{t+1}A 
t+1
​
 
更新Q值：使用四元组( S t , A t , R t + 1 , S t + 1 ) (S_t,A_t,R_{t+1},S_{t+1})(S 
t
​
 ,A 
t
​
 ,R 
t+1
​
 ,S 
t+1
​
 )更新Q值
和SARSA的区别在于：

步骤3中，目标策略直接选取下一状态S t + 1 S_{t+1}S 
t+1
​
 中所有动作的Q值最大值（贪心），无需实际执行该动作
步骤4中，无需下一动作A t + 1 A_{t+1}A 
t+1
​
 ，直接基于理论最优动作更新Q值。
17.2 Q-learning的off-policy性质
Q-learning是典型的off-policy算法，原因如下：

目标策略与行为策略分离：
目标策略：采用贪心策略（最大Q值），即π ( s ) = arg ⁡ max ⁡ a ( Q ( s , a ) ) \pi(s)=\arg\max_a(Q(s,a))π(s)=argmax 
a
​
 (Q(s,a))
行为策略：通常采用以概率ϵ \epsilonϵ随机探索，1 − ϵ 1-\epsilon1−ϵ概率选择最优动作
17.3 Q-learning的特点
优点：

理论收敛性：在满足条件下可保证收敛到最优策略
高效利用探索数据：off-policy允许重用历史经验
缺点：

高估风险：max操作可能导致Q值高估
高维挑战：同SARSA，适合处理离散状态
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/CY19980216/article/details/144133212