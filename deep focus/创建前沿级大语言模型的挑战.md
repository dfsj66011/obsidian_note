
Llama 4 背后的完整故事以及 Meta 在研究策略上的重大转变……

最近发布的Llama 4 [1]远非完美，但从这一代新模型中可以学到很多东西。简单来说，Llama 4标志着Meta研究方向的重大转变。面对日益激烈的竞争，Meta正在重塑Llama系列，并明确致力于打造前沿级别的大语言模型（LLM）。鉴于大语言模型的开发是一个迭代过程，如此重大的变革伴随着巨大风险——这些模型最初表现不佳的可能性很大。目前来看，Llama 4被视为一次失败，但Llama系列的长期成功将取决于Meta快速迭代并改进这些模型的能力。

开放的大型语言模型（LLM）研究最美好——或者对模型开发者来说最可怕——的一点在于，这些研究进展是公开进行的。我们能够研究Meta为实现与领域内顶尖模型持平而做出的关键改进。通过研究这些改进，我们能更好地理解现代前沿级LLM是如何开发的。在本概述中，我们将通过深入了解LLama 4及相关模型来做到这一点。然后，我们会运用这一理解来分析LLM研究的关键趋势、LLama的未来，以及Meta在LLama 4之后取得成功必须做出的改变。

## Llama 4 模型架构

我们首先将概述Llama 4模型的架构，重点介绍与前几代Llama模型相比的关键变化。正如我们将看到的，新的Llama模型采用了截然不同的架构，这标志着研究方向和策略的明显转变。先前的Llama变体强调简单性和可用性，而Llama 4则显然致力于与前沿级别的闭源和开源大型语言模型实验室（LLM labs）看齐，通过采用提高性能和效率但在复杂性和规模上付出更高代价的技术来实现这一目标。
#### 专家混合模型（MoE）

> _“我们做出的设计选择旨在最大化模型开发过程的可扩展性。例如，我们选择采用标准密集型Transformer模型架构并进行少量调整，而非混合专家模型，以最大化训练稳定性。” —— 出自Llama 3论文[2]

与其使用一个密集型仅解码器Transformer（如下所示），Llama 4是首批采用混合专家（MoE）架构的Llama模型。出于稳定性和简洁性的考虑，Llama 3未采用MoE——更大的MoE模型会给训练和推理带来额外的复杂性。在Llama 4中，Meta与领先的开源（例如DeepSeek-v3 [4]）和专有模型（例如GPT-4）保持一致，这些模型已成功采用MoE架构。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F379c5d72-9aca-4b50-bd9c-d5ad9454f477_1622x798.png)

简单来说，尽管密集模型简单高效，但扩展起来却颇具难度。通过采用混合专家（MoE）架构，我们可以大幅提高超大型模型的训练（和推理）效率，从而实现更大规模的扩展。

什么是MoE？大多数读者可能熟悉使用MoE的动机——它是一种经过改进的仅解码器Transformer架构，能使大型模型在计算上更高效。以下三篇论文提出了MoE背后的大部分关键理念，我们将在这里对这些理念进行概述。

- [The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
- [Switch Transformers](http://switch%20transformers/)
- [Stable and Transferable Mixture-of-Experts (ST-MoE)](https://arxiv.org/abs/2202.08906)

与仅解码器的Transformer相比，混合专家模型（MoEs）对Transformer模块的前馈组件进行了修改。每个模块中不再只有一个前馈网络，而是有多个前馈网络，每个网络都有各自独立的权重。我们将这些网络中的每一个称为“专家”；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbb9a24-440d-4d26-8092-b6d72dafb55e_1482x858.png)

要创建一个混合专家（MoE）架构，我们将Transformer的前馈层转换为MoE层或专家层。MoE中的每个专家在结构上与该层原始的前馈网络相同，我们通常只将Transformer层的一个子集转换为MoE层；例如，Llama 4采用交错式MoE层，其中Transformer的每两层中就有一层变为专家层。

> _“我们的新款Llama 4模型是我们首批采用混合专家（MoE）架构的模型……MoE架构在训练和推理方面具有更高的计算效率，并且在固定的训练浮点运算（FLOPs）预算下，与密集模型相比能提供更高质量的输出。” —— 摘自Llama 4博客[1]

路由机制。显然，在Transformer中为每个前馈网络制作多个副本并不能提高计算效率。为了获得效率提升，我们需要引入稀疏性。换句话说，我们不会在每个MoE层中使用每个专家。相反，我们选择一部分专家（例如，一到两个专家）——称为“活跃”专家或参数——用于处理每个标记。这种选择是通过将每个标记向量传递到一个线性层来完成的，该线性层输出一组专家上的概率分布；见下文。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1189a50c-ad49-4e09-8fca-b800532e101a_1156x856.png)

从这里开始，我们可以仅使用获得最高概率的专家来处理每个标记。通过这样做，我们每次仅使用模型总参数的一部分来处理每个标记——活跃参数的数量远小于模型的总参数数量。因此，我们能够在仅消耗总计算成本的一小部分的情况下，训练具有大量总参数的模型。

> 门控网络倾向于收敛到一种状态，即它总是为相同的少数专家分配较大的权重。这种不平衡是自我强化的，因为受青睐的专家训练速度更快，因此门控网络会选择它们更多。 —— 来源

负载均衡与训练稳定性。如果我们像训练标准密集模型那样训练MoE，可能会出现几个问题。首先，模型会迅速学会将所有标记路由到单个专家——这种现象被称为“路由崩溃”。此外，在训练过程中，MoE 更容易出现数值不稳定，可能导致训练损失发散；见下文。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F213eacf6-6f4c-48ac-9fec-b81a24580b4b_1370x804.png)

为避免这些问题并确保训练的稳定性，大多数混合专家（MoE）模型在训练过程中采用了负载均衡损失（load - balancing loss），该损失会对将专家分配概率相等且均匀路由令牌的混合专家模型给予奖励。负载均衡损失通过在标准的下一令牌预测损失基础上增加一个额外的损失项来修改大语言模型的底层训练目标，具体如下。因此，这些辅助损失会影响模型的性能，这也导致一些基于混合专家的热门大语言模型（如DeepSeek - v3）完全避免使用它们。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa69f7cc-41ac-4b4f-9a13-c7b791a31430_1836x480.png)
DeepSeek-v3 所采用的无辅助损失负载均衡策略

[1]中并未明确说明用于训练Llama 4模型（如果有的话）的具体辅助损失是什么。为避免训练不稳定，我们可以采用类似于DeepSeek-v3的无辅助损失负载均衡策略，并采用各种额外技巧；例如，更好的权重初始化或选择性精度。

我们应该从这些信息中得到的主要结论是，尽管混合专家模型（MoEs）有许多优势，但与标准密集模型相比，它们的训练难度要大得多。这是简单性和性能之间的经典权衡！这些架构更为复杂，因此需要考虑的因素更多，在训练过程中可能出现的问题也更多。

Llama 4架构。文中介绍了三种Llama 4模型：

* Scout：总参数量为1090亿，活跃参数量为170亿，每层有16个专家。
- Maverick：总参数量为4000亿，活跃参数量为170亿，每层有128个专家。
- Behemoth：总参数量为2万亿，活跃参数量为2880亿，每层有128个专家。

Llama 4 Scout 和 Maverick 模型均已在[1]中根据Llama 4社区许可协议公开发布，而Behemoth模型仅进行了预览（即尚未发布）。与DeepSeek-v3类似，Llama 4模型同时使用了共享专家和路由专家。例如，Llama 4 Maverick有一个共享专家——这意味着所有token都会以100%的概率传递给该专家——并通过路由机制为每个token选择一个活跃的路由专家；

![|450](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ec49e67-8f67-4eea-8759-c27231ffacf5_1212x628.png)

与其他流行的混合专家（MoE）模型相比，Llama 4 模型活跃参数的数量非常少。然而，与顶级行业实验室的成果相比，这些架构设置并不罕见：

- Scout 优化了推理效率，让人联想到像双子座闪电（Gemini Flash）或 GPT - 4o - mini 这样的模型。
- Maverick 的架构与 DeepSeek - v3 相对类似（即拥有大量专家的稀疏模型）。
- Behemoth——该系列中最强大的模型——是一个类似 GPT - 4 的多万亿参数基础模型。

然而，Llama 4模型与其他流行的LLM之间仍存在差异。在Llama 4中，每层仅选择一个路由专家，而DeepSeek每层有多个共享专家和八个活跃的路由专家（即370亿活跃参数和6710亿总参数）。较少的活跃参数数量提高了Llama 4的训练和推理效率。事实上，据报道，尽管数据和模型规模大幅增加，但Llama 4模型在训练期间使用的计算量比Llama 3更少。

细粒度专家。一些现代基于MoE（混合专家）的大语言模型（如DeepSeek - v3和DBRX）所采用的一种流行的设计选择是使用细粒度专家。要使用细粒度专家，我们只需：

1. 增加每个MoE层中专家的数量。
2. 减小每个单独专家的规模（参数数量）。

通常，在细粒度MoE模型中，我们也会在每一层选择较多数量的活跃专家，以保持活跃参数的数量（相对）固定。在Llama 4系列中可以看到细粒度和粗粒度专家的使用——Scout模型共有16个专家，而Maverick共有128个专家。考虑到Maverick的专家数量是较小的Scout模型的16倍，但总参数数量仅为后者的4倍，它一定是使用了细粒度专家。

相比之下，Scout 和 Behemoth 模型都使用标准（粗粒度）专家。Meta 做出这一选择可能有几个不同的原因。一般来说，使用细粒度专家可以让专家之间实现更高的专业化分工，从而提高性能和效率。然而，细粒度专家也会给分布式训练过程带来额外的复杂性。

![tensor parallel vs expert parallel|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3c4895-4eb4-4f7a-974d-e53b1423af84_1136x676.png "tensor parallel vs expert parallel")

在训练过程中，专家通常分布在多个GPU上（即专家并行）；详见上文。使用粗粒度专家时，每个GPU通常存储一个专家1。然而，我们通常可以将多个细粒度专家装入单个GPU的内存中。此外，由于使用细粒度专家时通常会选择更多的专家，可能会出现每个token都需要路由到集群中多个不同GPU的情况，从而导致GPU之间的通信成本大幅增加。

>我们确保每个令牌最多被发送到𝑀个节点，这些节点是根据分布在每个节点上的专家的最高𝐾 / 𝑀亲和力分数之和选定的。在此约束下，我们的MoE训练框架几乎可以实现完全的计算-通信重叠。——引自DeepSeek-v3论文[4]

因此，我们必须采取一些策略来限制通信成本并提高训练效率。例如，DeepSeek-v3采用了上述的节点限制路由方案，该方案限制了单个令牌可以路由到的设备数量。我们可以通过不使用细粒度专家来避免这种额外的复杂性。然而，同时训练细粒度和粗粒度专家模型也为模型用户提供了更多的可配置性和选择。

对开放大语言模型的影响。混合专家模型（MoE）在推理过程中不会使用其全部参数，但我们仍需将模型的参数适配到GPU内存中。因此，与密集模型相比，基于MoE的大语言模型具有更高的内存占用，因而需要访问更多、性能更优的GPU。Llama 4 Scout“在采用Int4量化后能适配单个H100 GPU”，而Maverick则需要“单个H100主机”。换句话说，我们无法使用单个GPU对更大的Maverick模型进行推理，必须在多GPU主机上进行分布式推理。

综合考虑以上所有因素，我们可能会开始意识到，将Llama迁移到MoE架构是一把双刃剑：

- Llama项目向与最强大的（专有）大语言模型持平迈出了一步，并为创建更好的模型解锁了潜力。
- 使用Llama模型的门槛提高了。

这一困境对开放的大型语言模型（LLM）研究具有重大影响。提高开放的大型语言模型的准入门槛会产生显著的副作用，并将阻碍那些没有强大GPU资源的人开展有意义的研究。随着模型不断进步，如果开放的大型语言模型社区贡献者因成本问题逐渐被挤出研究领域，那么该社区将无法继续蓬勃发展。

> “成为开放标准的模型不一定是整体上最好的模型，而是一系列在多种形态和规模上都很稳健、适用于多种不同部署环境的模型……像稀疏混合专家（MoE）这类内存密集型模型会让开放社区中的更多参与者望而却步。” —— 内森·兰伯特

为了避免混合专家（MoE）架构的这一负面特性，我们可以将较大的MoE模型蒸馏成较小的密集模型，从而提供一系列更用户友好且性能依然良好的大语言模型（LLM）。这一方法被DeepSeek - R1采用并推广，这是一个拥有6710亿参数的基于MoE的推理模型，它被蒸馏成了多个规模从15亿到700亿参数不等的密集LLM。文献[5]中的一个关键发现是，当使用一个非常庞大且强大的模型作为教师模型时，蒸馏效果最佳。正如我们稍后在概述中看到的，从Llama 4模型进行蒸馏的工作已经在大力开展。

#### 原生多模态与早期融合

过去已发布过多模态Llama模型。最初的Llama 3发布文献[2]包含了对多模态的初步实验，这些实验在Llama 3.2 Vision发布后实现了产品化。多模态Llama 3模型的关键细节在下方链接的概述中有详细说明。与之前几代模型类似，Llama 4模型支持视觉输入——包括图像和视频。然而，正如我们将在本节中看到的，Llama 4在多模态方面采取了截然不同的方法。

多模态架构。多模态大语言模型有两个主要组成部分：一个大语言模型骨干网络和一个视觉编码器。大语言模型骨干网络只是一个标准的仅解码器架构的Transformer，而视觉编码器通常是一个CLIP或ViT模型，用于将图像转换为一组相应的嵌入表示；

![|450](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd022205-f5bc-4580-a4b3-3a03648d37d1_1288x1066.png)

鉴于这两个组件，视觉大语言模型（简称 vLLM）必须学会如何正确融合视觉和文本信息。换句话说，大语言模型必须以某种方式：i) 摄入图像嵌入；ii) 将这些嵌入作为额外上下文用于生成文本。有两种主要的模型架构可用于此目的（如下所示）：

1. 统一嵌入：在输入层将图像标记和文本标记连接起来，形成一个由大语言模型处理的单一输入序列。
2. 跨模态注意力：仅将文本标记作为输入传递给大语言模型，并通过额外的交叉注意力层将视觉信息融合到模型中。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa676e40-5e09-4315-9fd1-90275964685e_2372x938.png)


这些架构都有其优势。例如，跨模态注意力往往更高效，因为我们无需将图像嵌入传递通过整个大语言模型（LLM）主干网络。然而，统一嵌入方法却有可能因为同样的原因而取得更好的性能！

多模态训练。鉴于视觉语言大模型（vLLMs）以文本作为输出，我们仍然采用下一token预测的方式对其进行训练。然而，除了训练目标之外，对于这类模型还有几种不同的训练策略可供选择：

1. 原生多模态：从一开始就使用多模态数据对视觉语言大模型进行训练。
2. 组合式多模态：先分别训练一个独立的大语言模型骨干网络和视觉编码器，然后进行额外训练以将它们融合在一起。

客观来说，原生多模态会给训练过程引入额外的复杂性（例如，模态之间的不平衡）。然而，假设我们能避开这些陷阱，原生多模态训练具有巨大的潜力——它能扩大模型可接触的数据范围和数据量。正因如此，许多顶尖实验室——尤其是谷歌和OpenAI——都采用了这种方法，这很可能是Llama 4设计的动机之一。

> _“Llama 4模型采用原生多模态设计，通过早期融合技术将文本和视觉标记无缝整合到统一的模型架构中。早期融合是一项重大突破，因为它使我们能够利用大量未标注的文本、图像和视频数据对模型进行联合预训练。” —— 摘自Llama 4博客[1]

先前的Llama变体（例如Llama 3.2 Vision）采用跨模态注意力架构，并通过组合方法进行训练。相比之下，Llama 4模型原生支持多模态，并从零开始使用文本、图像和视频数据进行预训练。转向原生多模态使得Llama 4模型在构建其庞大的30T标记预训练数据集时能够利用多种模态的数据——该数据集比Llama 3的大两倍以上。

早期融合。正如上述引用所言，Llama 4也采用了统一的嵌入架构，而非Llama 3所使用的跨模态注意力架构。在文献[1]中，“早期融合”这一术语用于描述Llama 4模型的架构，其含义是在大语言模型（LLM）的输入层将图像和文本进行结合。另外，“后期融合”架构（例如跨模态注意力）则在大语言模型的后续层对图像和文本数据进行结合。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6347a27e-6a17-484b-aaae-69278a3dda75_1408x758.png)

尽管作者在[1]中并未提供关于Llama 4架构的过多细节，但我们可以参考Meta近期发表的一篇关于原生多模态和早期融合的论文《变色龙》[6]，以推测Llama 4可能采用的方法。如上所述，《变色龙》架构将交错的图像和文本标记作为单一序列传递给统一的LLM主干网络。该模型采用原生多模态方法进行训练，甚至能够生成图像作为输出。尽管[1]中并未展示Llama 4的图像生成功能，但基于Llama 4采用类似《变色龙》的早期融合架构，以及OpenAI在原生多模态模型图像生成方面近期的成功，我们或许可以期待Llama 4在不久的将来具备这一能力。

>“这种早期融合方法从一开始就将所有模态投影到一个共享的表示空间中，从而实现了跨模态的无缝推理和生成。然而，这也带来了重大的技术挑战，特别是在优化稳定性和扩展性方面。” ——引自[6]

在[6]中，作者提到，由于Chameleon模型本身具有多模态特性，在训练该模型时他们遇到了各种独特的困难。具体来说，与标准的基于文本的大型语言模型（LLM）相比，Chameleon模型训练时的不稳定性更频繁出现，并且更难扩展规模。为了解决这些问题，研究人员对底层Transformer架构进行了一些重要修改：

- 在注意力计算过程中，对查询向量和键向量应用层归一化。
- 在Transformer的每个注意力层和前馈层之后添加一个额外的Dropout模块。
- 修改Transformer块中层归一化的位置（即采用后归一化结构，而非更标准的前归一化结构[8]）。

[6]中概述的困难清楚地表明了原生多模态训练的技术复杂性。尽管尚未证实Llama 4使用了Chameleon中的任何架构技巧，但这些经验教训对于任何采用原生多模态方法训练的模型都具有普遍的借鉴意义。

视觉编码器。尽管变色龙架构在很大程度上与上述统一嵌入模型的结构相匹配，但细心的读者可能会注意到，变色龙并没有图像编码器！相反，我们直接将图像量化为离散的令牌嵌入，如本文所述。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5291bb46-b63b-4217-9dba-c21fbca3ed57_2584x940.png)

变色龙并非首个放弃图像编码器而直接将图像信息作为输入传递给大语言模型的模型。Fuyu [7] 将图像分割成图像块——类似于标准的视觉Transformer（ViT）——并线性投影这些图像块，使其与文本标记向量的大小相同。然后，大语言模型可以直接将这些图像块嵌入作为输入进行处理。这种方法的主要动机在于，当我们将图像通过视觉编码器时，图像中的相关信息可能会丢失。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563cde53-5b66-4b9b-822d-cdaf4d34336c_1952x836.png)

与Chameleon不同，作者在[1]中证实，Llama 4使用的是基于MetaCLIP [9]的视觉编码器——MetaCLIP是对CLIP的开源复现版本，强调训练数据的透明度。Llama 3的视觉编码器采用了相同的架构。然而，Llama 4的视觉编码器是与大语言模型（LLM）联合训练的，目的是：i）提高其嵌入向量的质量；ii）使视觉嵌入向量与大语言模型的文本嵌入向量更好地对齐。

> _“我们还改进了Llama 4中的视觉编码器。该编码器基于MetaCLIP，但与冻结的Llama模型分开训练，以更好地使编码器适配大语言模型。” —— 来自Llama 4博客[1]

#### 10M Token Context Window

长上下文理解很重要，无论是对于解决自然需要长上下文的任务（如多文档摘要），还是基于推理的应用场景。许多顶尖实验室已经发布了具有超大上下文窗口的模型，以实现更多长上下文应用。Llama 4 的发布顺应了更长上下文的发展趋势，并试图在这一领域树立新的标杆。然而，正如我们将了解到的，实现长上下文非常复杂，通常需要将众多相互关联的技术正确地集成到大语言模型中。

10M 令牌上下文。Llama 4 Scout 将 Llama 3 的 128K 令牌上下文长度进行了扩展，拥有行业领先的 10M 令牌上下文长度。该模型预训练时的上下文长度为 256K 令牌，但通过一系列技巧实现了 10M 令牌上下文，这些技巧包括修改位置嵌入、缩放 softmax 以及聚焦长上下文的训练流程。让我们深入探究这些技术的细节，以确切了解它们的工作原理。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8fec4d1-3b72-4e17-8a01-2e7c4f3b7a5c_1949x930.png)

位置嵌入有助于Transformer理解序列中标记的顺序，例如，哪个标记排在第一、第二、第三等等。显式的位置信息是必要的，因为自注意力机制本身并不会自然地考虑序列的顺序。相反，在计算标记之间的注意力得分时，序列中的所有标记会被同时考虑，而不考虑其位置（如上所述）。通过使用位置嵌入，我们可以将位置信息直接注入到每个标记的嵌入中，从而使自注意力机制能够利用这些信息，并学习标记顺序中的模式。存在许多位置编码方案，例如标准的绝对位置嵌入（APE）、旋转位置嵌入（RoPE）[11]、带线性偏差的注意力（ALiBi）等等。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02efc279-6e0a-43ab-b166-5d8c08d5cca9_1644x976.png)

RoPE 解析。原始的 Transformer 架构采用绝对位置嵌入方案，在模型输入层根据令牌在序列中的绝对位置为每个令牌向量添加固定的位置嵌入；见上文。如今，大型语言模型（LLM）更频繁地使用相对位置嵌入，该嵌入考虑的是令牌之间的距离而非绝对位置。通过使用相对位置嵌入，我们可以实现更好的性能7，并使注意力机制更具泛化性，适用于不同长度的序列。大型语言模型最常用的位置编码方案是 RoPE [11]（如下所示），Llama 3 [2] 和 Llama 4 [1] 均采用了该方案。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d51b4b1-deb9-4a2f-8b5f-9f7b683c9866_1566x984.png)

RoPE是一种将绝对位置嵌入和相对位置嵌入相结合的方法，其通过修改自注意力机制中的查询向量和键向量来发挥作用。与绝对位置嵌入不同，RoPE作用于每一个Transformer层，而不仅仅是输入层。在标准的Transformer架构中，我们通过对给定层的令牌向量序列进行线性投影来生成键向量和查询向量。对于输入序列中的单个令牌，我们可以如下表述这一操作，即对单个令牌嵌入进行线性投影。下图展示了键向量的创建过程，但我们采用完全相同的方法——只是使用权重矩阵不同——来生成查询向量和值向量。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66db2a1d-b210-4464-a0aa-278c522601fe_974x562.png)

RoPE通过将上述操作中使用的权重矩阵与一个独特的旋转矩阵相乘，将位置信息融入到键向量和查询向量的创建中。在这里，这个旋转矩阵是基于序列中一个标记的绝对位置计算得出的——给定向量的旋转量取决于它在序列中的位置。下面展示了修改后的操作，我们再次描述了键向量的创建过程。同样的策略也应用于查询向量的创建，但我们不修改值向量的创建。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F660d68d7-f108-493e-b010-1e1c7205a1a6_1466x624.png)

在这里，θ 被称为旋转（或频率）基的向量。我们有一个函数 R，它以旋转基 θ 和序列中标记的位置 i 作为输入，并输出一个旋转矩阵。该旋转矩阵是一个块对角矩阵，其构造方式如下方程所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9963ed9d-67e5-4587-ac67-f1cdea075570_1670x646.png)

该矩阵是块对角矩阵，矩阵中的每个块都是一个 2×2 的旋转矩阵。这些块中的每一个都对输出键（或查询）嵌入中的两个维度对进行旋转。因此，结果嵌入中的每一对维度都会根据序列中标记的绝对位置 i 以及与该维度对相对应的旋转基 θ 的条目进行旋转。我们在每个Transformer层的自注意力机制生成键向量和查询向量时都应用这个旋转矩阵。这些修改产生了下面所示的注意力操作，其中每个键向量和查询向量都根据其在序列中的绝对位置进行旋转。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab69c09a-f632-4e18-9cc3-16cd92cd8fb2_1548x936.png)

然而，当我们对旋转后的键和查询取这个标准的外积时，有趣的事情发生了。分别用于旋转键和查询的两个旋转矩阵结合形成了一个单一的旋转矩阵R(θ, n - m)。换句话说，在自注意力机制中同时旋转键向量和查询向量，捕捉到了序列中标记之间的相对距离。这就是RoPE的核心！虽然我们一开始可能难以理解这些旋转矩阵的作用，但现在我们看到，它们将每对标记的相对位置直接注入到了自注意力机制中！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1c2937-a1c7-4cda-b7a9-c078731694c9_2186x1006.png)

长度泛化能力。如果我们向大语言模型（LLM）提供一个远长于其训练数据中序列长度的输入序列，模型的性能将会急剧下降。位置嵌入在大语言模型对更长上下文长度的泛化能力中起着关键作用。理想情况下，我们希望采用一种位置编码方案，使模型能够更轻松地泛化到训练期间未见过的更长上下文长度！

> “长度泛化能力，即从较小的训练上下文大小泛化到更大的上下文大小的能力，是基于Transformer的语言模型开发中的一个关键挑战。位置编码已被确定为影响长度泛化的一个主要因素。” —— 来自[12]

最近，研究人员表明，包括RoPE在内的最常用的LLM位置编码方案在泛化到长上下文长度时表现不佳[12]；详见下文。尽管RoPE通常被认为是一种相对位置编码方案，但在泛化到长上下文长度时，其表现与绝对位置编码相似。然而，在[12]中提出的无位置嵌入（NoPE）方案，即简单地从模型中移除位置嵌入，却出人意料地能够泛化到更长的上下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44d1db51-b8a6-43c7-a998-50fa94d5f05e_1674x864.png)

“NoPE表现良好这一事实令人惊讶，但[12]中的实证（和理论）分析表明，Transformer可以在不使用显式位置嵌入的情况下表示相对位置编码和绝对位置编码。实际上，[12]显示NoPE学习到的注意力模式类似于相对位置编码；详见上文。基于这些结果，Llama 4模型将使用RoPE的标准Transformer层与使用NoPE的层交替堆叠。这种方法被称为交错式RoPE（iRoPE），提升了长上下文处理能力。”

> Llama 4架构的一项关键创新在于使用了无位置嵌入的交错注意力层。此外，我们在推理时采用注意力温度缩放来增强长度泛化能力。——引自Llama 4博客[1]

温度缩放。每个Transformer层在其注意力机制中都有一个softmax变换。对于N维向量的元素i，softmax按如下方式计算：$$\text{Softmax}(x_i)=\frac{e^{x_i}}{\sum^N_{j=1}\,e^{x_j}}$$
这个表达式的分母——序列中所有标记对的原始注意力得分之和——会随着上下文长度的增加而变大，但分子与上下文长度解耦且大小固定。这两个事实在长上下文的注意力得分中产生了一种有趣的现象：随着上下文长度的增加，注意力得分会变小。为了缓解这一问题，[13] 中的作者提出了可扩展的 Softmax（Scalable-Softmax），其公式如下：$$\text{Scalable-Softmax}(x_i)=\frac{N^{sx_i}}{\sum^N_{j=1}N^{sx_j}}$$
与标准softmax类似，可扩展softmax只是一个将值向量转换为有效概率分布的函数。然而，这种softmax变体引入了两个新的重要因素：

- s：一个缩放参数，可以调整以改变函数的形状。
- N：输入向量的长度。

通过在可扩展Softmax中加入输入向量的长度，我们可以平衡分子和分母的规模，防止长上下文注意力分数衰减，并提升长上下文能力；

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3f99960-52bf-4e47-a39b-d41d92cd2d29_760x1556.png)

正如[1]中提到的，Llama 4模型采用了一种类似的方法，在推理时缩放softmax函数的温度，以避免在非常大的上下文长度下注意力分数衰减。降低softmax温度会使结果分布更加尖锐，而提高温度会使分布更加均匀。我们可以简单地在长上下文长度下降低softmax的温度，以平衡注意力分数。这种推理时的技巧很有用，但它们也使Llama 4的推理过程复杂化，从而增加了有害错误和实现差异的可能性。

上下文扩展。最后，除了迄今为止概述的策略外，我们还需要训练大语言模型以支持长上下文。通常，我们不会仅仅对大语言模型进行长上下文的预训练。这种方法并不是最优的，因为在长序列上进行训练的内存需求非常高。相反，我们可以分两个阶段对模型进行训练：

1. 采用较低上下文长度的标准预训练。
2. 在长上下文数据集上进行微调，也称为“上下文扩展”。

例如，Llama 4 Scout在训练的后期阶段扩展上下文长度之前，已经进行了256K上下文长度的预训练。

> “我们在[中期训练阶段]继续对模型进行训练，通过采用新的训练方法（包括利用专业数据集扩展长上下文）来提升核心能力。这使得我们能够在提高模型质量的同时，为Llama 4 Scout解锁同类领先的1000万输入上下文长度。” —— 摘自Llama 4博客[1]

通过专门设置一个微调阶段用于上下文扩展，我们可以限制使用超长序列进行的训练量。在大多数情况下，用于上下文扩展的训练数据是合成的——要么是通过启发式方法创建，要么是由大语言模型生成——因为收集真实的长上下文数据难度较大。正如我们将看到的，用于上下文扩展的合成数据的质量会极大地影响模型的能力。这些数据必须准确模拟并捕捉模型在实践中将要解决的任务类型。正如我们将看到的，Llama 4 模型在实际应用中的长上下文能力会失效，这可能是由于这一问题所致。

在[1]中，作者并未提及用于扩展Llama 4上下文的具体方法。然而，我们可以概述文献中一些常用的技术，以便为Llama 4可能使用的上下文扩展技术提供灵感。正如上述视频中完美描述的那样，用于扩展大型语言模型（LLM）上下文的方法主要有两大类：

* 位置插值：这些技术调整RoPE8的频率基础，使得较大的位置仍能适应模型的“已知”上下文长度；例如，位置插值（Position Interpolation）、NTK - RoPE、YaRN和CLEX。
* 近似注意力：这些技术通过修改注意力的结构，在计算注意力分数时仅考虑某些特定的标记组（例如，基于块、地标或滑动窗口）。

[14]中对这些方法进行了广泛分析，我们发现位置插值类方法往往表现最佳。特别是，NTK - RoPE由于其能够动态调整RoPE中的频率，使得相邻标记的频率不会变化过大，从而取得了非常令人瞩目的性能。这些技术在进行大语言模型训练时非常常用。举个具体的例子，在《Qwen - 2.5报告》的第4页，作者描述了在进行长上下文训练之前提高了RoPE的基础频率。

## 训练 Llama 4

除了完全重新设计的架构外，Llama 4 还采用了新的训练流程，在预训练和后训练阶段都进行了重大改进。同样，许多这些变化为了更好的性能而引入了额外的复杂性，并且借鉴了在前沿研究实验室中成功采用的技术。有趣的是，较小的 Llama 4 Maverick 和 Scout 模型的训练过程也大量利用了来自更大的 Behemoth 模型的知识蒸馏。

#### **Pretraining**


![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15fbad1d-acda-4c67-96c3-d48444e083ae_1920x1308.png)

对大型语言模型（LLM）的预训练过程进行大幅调整既存在风险又极为罕见，原因在于：其一，预训练成本极高；其二，预训练及扩展技术已得到深入研究且（相对）成熟稳定。然而，Llama 4 原生的多模态特性以及基于混合专家（MoE）的架构使得其预训练过程有必要做出一些改变，我们将在本节中快速概述这些改变。

原生多模态。如前文所述，Llama 4 模型是在一个包含文本、图像和视频的海量 30T 令牌数据集上进行预训练的。然而，这个数据集不仅具有多模态特性，还具有高度的多语言性，涵盖了来自 200 种语言的数据。其中超过 100 种语言至少有 10 亿个训练令牌与之相关联，与 Llama 3 相比，多语言数据增加了 10 倍。鉴于 Meta 之前在机器翻译研究方面的投入，尤其是支持 200 种语言的“不让任何语言掉队”（NLLB）模型，这种对多语言的重视也就不足为奇了。


> 在预训练的最后阶段，我们在长序列上进行训练，以支持最长可达128K标记的上下文窗口。我们之前不在长序列上进行训练，是因为自注意力层中的计算量会随序列长度呈二次方增长。 —— 摘自Llama 3论文[2]

Llama 4模型使用256K个标记的上下文长度进行预训练，这比之前的模型要大得多。例如，Llama 3最初是以8K的上下文长度进行预训练的，后来通过六阶段的上下文扩展过程增加到128K。这种扩展的上下文长度体现了Llama 4新MoE架构在预训练过程中的高效性，并且对于多模态预训练是必要的。具体来说，在预训练期间，Llama 4在其输入序列中接收多达48张图像——无论是单独的图像还是视频中的静态帧——并在测试期间处理多达八张图像时取得了良好的效果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ba82b9e-e187-48ec-ab5d-7edb999fcdb1_1488x1126.png)

鉴于Llama 4采用了（变色龙风格的）统一嵌入架构，图像和视频截图可以在模型的输入序列中任意交错排列；详见上文。在这里，视觉标记只是模型输入序列中的另一个标记，并且与标准文本标记的处理方式类似。与Llama 3不同的是，Llama 4的博客[1]并未明确提及使用感知器重采样器来处理视频数据。相反，从博客文章的措辞来看，该模型似乎只是摄取静态视频帧，并从输入中每个标记的位置学习时间模式。

> 与BF16基线相比，我们的FP8训练模型的相对损失误差始终保持在0.25%以下，这一水平完全在训练随机性的可接受范围内。——引自[4]

低精度训练。文献[1]中提到，Llama 4模型采用FP8精度进行训练。DeepSeek - v3 [4]是首个成功在大规模预训练中使用FP8精度的开源模型。混合精度训练较为常见，但FP8是一种激进的精度设置——大多数训练采用的是更高精度，如bfloat16。此外，由于混合专家模型（MoEs）在训练过程中更容易出现不稳定的情况，因此它们对混合精度训练尤为敏感。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26c333a4-8f03-4704-b8d3-83f9f2b73cc8_1234x347.png)

关于用于训练Llama 4的FP8方案，提供的细节很少，但其实现可能与DeepSeek-v3类似；详见上文。FP8训练的主要问题是大型语言模型（LLM）的激活值、权重和梯度中存在异常值——对大数截断精度会导致舍入误差，从而在训练过程中引发不稳定。为避免这一问题，DeepSeek-v3提出了一种新颖的FP8量化方案，该方案对模型内的一维瓦片或二维块中的值进行细粒度量化。通过对更细粒度的组进行量化，我们最大限度地减少了舍入误差。

课程学习。最后，Llama 4 也经过多阶段预训练，包括标准预训练阶段和一个额外的训练阶段——在[1]中被称为“中期训练”（mid - training），该阶段采用了不同的数据混合方式，重点关注关键领域和特定模型能力（例如长上下文理解）。在预训练接近尾声时，对所使用的数据混合方式进行逐步调整的策略在大语言模型（LLMs）中很常见。例如，Llama 3 采用了类似的策略，使用了高质量逐步调整数据集（见论文第56页），甚至有整篇论文专门探讨这一主题[10]！

#### Post-Training

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18dc7ee3-961e-4fd6-b1dc-5fe7d91826c5_2660x1064.png)

Llama 3 最引人入胜的方面之一在于其训练后流程的简洁性，该流程包括多轮监督微调（SFT）和直接偏好优化（DPO）[18]；详见上文。由于 DPO 不像基于近端策略优化算法（PPO）的人类反馈强化学习（RLHF）那样需要训练单独的奖励模型，因此从所需 GPU 资源的角度来看，这种策略更易于上手。然而，从 Llama 4 我们可以看出，这种基础的对齐策略是以牺牲模型性能为代价的。训练后处理是大语言模型研究领域中发展最快的方向之一，若要与顶尖模型相媲美，就需要一种更复杂的方法。有关大语言模型训练后处理的更全面概述，请观看下方视频。

Llama 4 的训练后优化。Llama 4 的训练后优化包含三个关键阶段：

1. 轻量化监督微调（SFT）：针对困难提示，在一小部分（经过高度筛选的）补全样本上进行监督训练。
2. 在线强化学习（RL）：大规模的强化学习训练，重点提升模型在多个领域（如多模态、推理、对话等）的能力。
3. 轻量化直接偏好优化（DPO）：一个简短的额外训练阶段，用于修复模型响应质量方面的小问题和极端情况。

简单来说，Llama 4 在强化学习（RL）训练方面投入更多，采用了一种更复杂的训练后策略，该策略依赖大规模强化学习来发展关键模型能力，如推理和对话。然而，[1]中省略了关于 Llama 4 所使用的具体强化学习设置的大部分细节。同样，我们将不得不依靠近期的研究来提供有关 Llama 4 方法的线索。


> 我们发现，先进行轻量级监督微调（SFT），再进行大规模强化学习（RL），可以显著提升推理和编码能力。——引自Llama 4博客[1]

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e3b5bc-a219-4219-83e0-80075efa1f01_640x480.gif)

在线强化学习（Online RL）与离线强化学习（Offline RL）。在文献[1]中，作者强调了使用在线强化学习来训练Llama 4，但这意味着什么呢？正如这篇博客所详细介绍的，我们在使用强化学习训练大语言模型（LLM）或其他模型时，可以采用在线或离线的方法。这两种策略的区别在于我们收集训练数据的方式：

- 在线强化学习使用当前模型收集的数据来训练大语言模型——训练数据来自大语言模型本身。
- 离线强化学习使用历史数据来训练大语言模型，例如来自大语言模型先前版本或其他大语言模型的数据。

在线强化学习的关键区别特征是存在同策略采样（即直接从当前的LLM中采样训练数据）。通常，离线强化学习被认为既更经济又更容易实施。然而，近期的研究表明，在线强化学习具有明显的性能优势[18]。

与推理研究的关系。有趣的是，[1]中的作者发现，仅使用SFT和DPO可能会“过度限制”大语言模型的性能——尤其是在数学和代码等需要复杂推理的领域——因为在强化学习（RL）训练阶段允许的探索较少。最近的推理研究（例如DeepSeek - R1和Kimi - 1.5）得出了非常相似的结论。最近模型令人印象深刻的推理能力得益于大规模的强化学习训练，并且不太强调监督训练；例如，最初的DeepSeek - R1 - Zero模型实际上是在没有SFT的情况下仅通过纯强化学习进行后训练的！

>“DeepSeek - R1 - Zero的自我进化是一个令人着迷的展示，它表明强化学习能够让模型自主提升其推理能力。” —— 引自[1]

近期的一些推理模型大量使用了基于可验证奖励的强化学习（RLVR）；详见下文。与从基于大型语言模型（LLM）的奖励模型中获取奖励信号的标准人类反馈强化学习（RLHF）不同，该奖励模型是通过人类偏好数据进行训练的，而RLVR使用的奖励信号是确定性的。例如，对于一道数学题的奖励可以简单地检查大型语言模型的答案是否与标准答案一致。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7334cdb5-5398-47d2-98bb-01ca41a58879_1854x726.png)

数据混合与整理。除了运用新算法外，作者还强调了在Llama 4的训练后过程中数据整理和课程学习的重要性。通过使用大型语言模型评判器（即之前的Llama模型）来识别并移除简单示例，超过50%可用于监督微调的数据被从训练过程中剔除，从而使训练后过程更专注于更难的数据。对于Behemoth模型，这一数据的比例甚至更大，高达95%的数据被移除。

> 我们还发现，在训练过程中动态过滤掉优势为零的提示，并构建包含多种能力混合提示的训练批次，对提升数学、推理和编码方面的性能起到了重要作用。——摘自Llama 4博客[1]

在在线强化学习（RL）中也采用了类似的策略，即交替进行模型训练和使用该模型来识别困难的训练提示。具体来说，通过 pass@k 分析来评估提示难度，该分析会使用大语言模型（LLM）生成 k 个完成结果，并检查其中有多少是正确的。值得注意的是，Kimi - 1.5（参见本文第二部分）采用了几乎相同的技术来评估提示难度并制定课程学习策略。正如上述引文详细介绍的那样，Llama 4 还采用了一些额外的技巧来识别困难提示，并在每个训练批次中混合多个领域的数据，以实现模型能力（例如对话、推理、编码等）的良好平衡。

#### Model Distillation

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc65edf3f-2618-4712-8339-3e37ded9e142_1920x729.png)

除了在[1]中发布Llama 4 Scout和Maverick模型外，作者还预览了Llama 4 Behemoth——一种更大的原生多模态MoE模型，拥有2880亿个活跃参数、16个专家和2万亿个总参数。Llama 4 Behemoth模型的关键性能指标如上表所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c601ca0-a535-4d7f-874d-6b94c8e7763e_2488x1050.png)

尽管Llama 4 Behemoth表现令人印象深刻，但这个模型主要用于知识蒸馏[15]。换句话说，在训练其他Llama 4模型时，我们将其用作教师模型。

> “得益于从拥有16个专家、2880亿活跃参数的Llama 4巨兽模型（这是我们目前最强大的模型，也是全球最智能的大型语言模型之一）中获得的蒸馏知识，这些模型成为了我们迄今为止的最佳成果。” —— 摘自Llama 4博客[1]

什么是蒸馏？给定一个输入的标记向量序列，大语言模型（LLM）会输出一组大小相同的（经过转换的）标记向量。我们可以将这些输出向量中的每一个都通过大语言模型的基于分类的下一个标记预测头——这通常只是一个额外的线性层——并应用softmax函数，以获得潜在下一个标记的概率分布。因此，大语言模型的最终输出是一个向量列表，表示输入序列中每个位置下一个标记的概率分布；见下文。

```python
import torch
import torch.nn.functional as F

seq_len = 128
d = 768  # size of token embeddings
vocab_size = 32678

# classification head for next token prediction
ntp_head = torch.nn.Linear(in_features=d, out_features=vocab_size)

# construct LLM output and next token probabilities
llm_output = torch.rand((seq_len, d))
logits = ntp_head(llm_output)
ntp_probs = F.softmax(logits, dim=-1)
```

在训练过程中，我们知道序列中实际的下一个标记是什么。因此，我们可以使用交叉熵损失来训练我们的模型，该损失应用于序列中正确下一个标记的概率。这种训练损失在下文中实现，其中每个位置的真实下一个标记存储在目标向量中。这里，我们提供逻辑斯蒂分数作为输入，因为 PyTorch 在其交叉熵实现中已经在内部应用了 softmax。

```python
# next token prediction (cross-entropy) loss
targets = torch.randint(0, vocab_size, (seq_len,))
loss = F.cross_entropy(logits, targets)
```

知识蒸馏背后的关键思想是从另一个大语言模型（LLM）而非真实标签中推导出我们的目标。在保持其他所有条件不变的情况下，我们可以使用两个大语言模型——学生模型和教师模型——来生成输出，并将教师模型的输出作为训练学生模型的目标，而非真实标签。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F977c55cd-7f51-46be-bf3d-221b5fb7915f_1620x1166.png)

蒸馏的实际应用示例。知识蒸馏最初是在深度学习领域提出的[15]，自那时起便得到了广泛应用；例如，在ChatGPT出现之前的蒸馏案例包括DistilBERT和TinyViT。蒸馏在大型语言模型（LLMs）领域也被广泛使用。例如，DeepSeek-v3 [16] 在预训练阶段使用了DeepSeek-R1推理模型[17]作为教师模型。此外，还利用基于混合专家（MoE）架构的庞大DeepSeek-R1模型作为教师模型，通过知识蒸馏创建了一系列不同规模的密集推理模型。除了这些公开的案例外，类似的蒸馏策略以及合成数据方法几乎可以肯定也被用于训练顶尖的闭源大语言模型。这类趋势很可能促使Meta在Llama 4的训练中采用了类似的方法。

硬蒸馏与软蒸馏。知识蒸馏主要有两种变体：硬蒸馏和软蒸馏。硬蒸馏与我们的原始训练目标非常相似。我们只需执行以下步骤：i) 从教师大语言模型的输出中通过选择概率最高的标记来派生出一个独热标签；ii) 将这个独热标签视为真实目标；iii) 应用相同的交叉熵损失；详见下文。

```python
temperature = 1.0  # softmax temperature
scaling_factor = 1.0

# student forward pass
llm_output = torch.rand((seq_len, d))
logits = ntp_head(llm_output)

# teacher forward pass
teacher_output = torch.rand((seq_len, d))
teacher_logits = ntp_head(teacher_output)
teacher_ntp_probs = F.softmax(teacher_logits / temperature, dim=1)

# different distillation losses
teacher_one_hot = torch.argmax(teacher_logits, dim=1)
hard_loss = F.cross_entropy(logits, teacher_one_hot)
soft_loss = F.cross_entropy(logits, teacher_ntp_probs)
hybrid_loss = hard_loss + scaling_factor * soft_loss
```

然而，在教师模型预测的完整概率分布中包含大量潜在有用的信息，而我们在创建硬蒸馏目标时丢失了这些信息。相反，我们可以将教师模型的整个分布用作训练信号——这被称为软（或密集）蒸馏。如上所示，可以实现这样的软蒸馏损失。在软蒸馏中，我们还可以调整用于创建教师预测的标记概率分布的 softmax 温度作为训练超参数。

使用硬蒸馏还是软蒸馏取决于多种因素。例如，如果我们使用封闭式大语言模型作为教师模型，可能无法获取该模型的对数概率，这就无法进行软蒸馏。然而，假设教师模型能力强大，软蒸馏通常能为学生模型提供更密集或更丰富的信号，这能加快训练速度，并使学生模型更加稳健[15]。我们也可以同时使用这两种方法，将它们合并为一个损失函数。

蒸馏 Llama 4。Llama 4 模型采用了一种协同蒸馏方法。这里的“协同蒸馏”指的是 Llama 4 Maverick 和 Scout 都使用 Behemoth 模型作为教师模型进行训练。通过从更大的 Behemoth 模型中蒸馏出多个模型，我们可以分摊在训练期间计算蒸馏目标的前向传播成本，这一成本很高——这可是一个大型模型！作者在[1]中提到，这种结合了硬目标和软目标的协同蒸馏策略提升了两个模型的性能。

> 我们从Llama 4 Behemoth中蒸馏出了Llama 4 Maverick模型作为教师模型，显著提高了最终任务评估指标的质量。我们开发了一种新颖的蒸馏损失函数，该函数在训练过程中动态地对软目标和硬目标进行加权。——引自Llama 4博客[1]

正如上文所述，Llama 4所采用的蒸馏策略是动态的——在训练过程中，硬目标和软目标之间的平衡会发生变化。实际上，我们可以通过修改上述代码中的scaling_factor来实现这一点。尽管[1]中并未透露确切的策略，但很可能训练过程开始时使用硬目标，而在训练后期强调软目标，从而缓慢增加大型语言模型接触到的信息密度。这是一种常见的课程学习形式，即大型语言模型首先从较容易的数据中学习，并随着时间的推移逐渐接触到更难的数据；例如，可参见此处。

## Llama 4 的性能与能力

大型语言模型（LLM）的开发是一个以实证为驱动且不断迭代的过程。为了开发出强大的大型语言模型，我们会调整模型并构建稳健的评估体系，以便能够检测到有意义的改进。随着时间推移进行足够多的积极改进，模型性能就会得到提升。相比之下，Llama 4 一次性对模型做出了许多重大改变——这是研究方向上的一次彻底（且有风险）的转变。正如我们在本节中将看到的，Llama 4 模型并非处于行业领先水平，其性能也饱受诟病。然而，这并不意味着 Llama 4 所做的改变是错误的。事实上，Llama 4 所采取的方法借鉴了许多成功且受欢迎的大型语言模型。Llama 的长期成功将取决于该团队迭代和改进现有状态的能力。

#### 报告的性能

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd1a524f-fcf9-4a8b-b1c3-cef397b0e9c8_2242x1220.png)


![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2e4ca8a-6416-4b31-b93f-0c2076c7e8e3_1704x1298.png)

[1]中报告的Llama 4模型的基准测试细节在上面的表格中进行了总结，在这些表格中，Llama 4 Maverick和Scout与其他类似的模型（包括开源和闭源模型）在各种感兴趣的任务上进行了比较。从这些指标中，我们可以看出Llama 4模型：

- 在基于图像的文档理解任务上表现良好，这可能是因为在其训练过程中包含了合成的结构化图像（例如，图表、图形和文档）。
- 由于其原生多模态训练过程和早期融合架构，具有强大的图像理解能力。
- 比之前的Llama模型迭代以及一些闭源模型（如GPT - 4o）更具多语言性，这意味着支持的语言更多，并且在支持的语言上的性能更好。
- 具有有前途的长上下文能力，要么与Gemini 2.0 Flash（1M标记上下文长度）等行业领先模型相匹配，要么超过它们。

Llama 4 Maverick模型在LMArena上也取得了令人印象深刻的Elo评分1417分，在撰写本文时，这一成绩使其位居排行榜前列。然而，这些结果是用该模型的“实验性聊天版本”测得的，此版本与实际用于评估的模型有所不同。

![Image|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87bf9d8b-945e-40c8-8989-d8dcfa5b8ec0_1830x921.png "Image")

这一变化在网上引发了大量困惑和讨论。LMArena的结果是Llama 4发布的关键部分，因此使用专门模型进行这一单项评估被认为是误导性的（甚至有点虚伪）。

>“Meta对我们政策的解读与我们期望的模型提供商的表现不符。Meta本应更明确地说明Llama - 4 - Maverick - 03 - 26 - Experimental是一款为优化人类偏好而定制的模型。我们正在更新我们的排行榜政策，以强化我们对公平、可复现评估的承诺，避免未来再出现这种混淆。” —— LMArena声明

为了进一步分析Llama 4模型的长上下文能力，[1]中的作者还展示了每个模型的“大海捞针”测试结果，发现Llama 4模型能够从多达100万个标记（针对Maverick）和1000万个标记（针对Scout）的上下文中检索信息；见下文。然而，这种长上下文测试方式仅衡量检索能力，并不能保证模型能够利用其全部上下文来解决问题。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdc27709-6f00-4b08-a382-c06ad21d29ba_2096x929.png)

目前没有使用任何现代长上下文基准测试（例如 BABILong、RULER 或 NoLiMa）来评估 Llama 4，这使得这些模型的长上下文能力——它们的一项关键区别特征——显得有些存疑。从这些指标中我们还可以看出，Llama 4 模型在编码任务上并不特别出色，并且尽管它们是强大的“非推理”模型，但并未与像 DeepSeek-R1 或 OpenAI 的 o 系列模型这样的推理模型进行比较。正如我们将看到的，问题还不止于此。Llama 4 模型在发布后受到了严厉批评，公开评估揭示了它们性能上的诸多不足。

#### 公众反应与批评

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c212390-6df2-4b3d-b210-cf75d80d25af_1158x1158.png)
公众评价。在Llama 4发布后不久，研究人员便开始独立评估这些模型，但评估结果差异很大。在编码任务方面，Llama 4模型明显不尽如人意：

- 在BigCodeBench排行榜上链接，两款Llama 4模型均未进入前40名。
- 在Aider Polyglot基准测试中，Llama 4 Maverick的代码完成准确率仅为16%（当前最优水平约为80%）链接。
- 一些用户根据自身使用体验对Llama 4模型的编码能力提出了非常严厉的批评，似乎表明在此次模型发布中，编码能力几乎被完全忽视了链接。

鉴于Llama 4模型并非在所有编程基准测试中都表现不佳，这些结果尤其难以分析；例如，[1]中LiveCodeBench的指标似乎表明其编程性能尚可。

此外，在公开评估中，Llama 4 模型在长上下文能力方面的表现并不出色；例如，在数据污染最小的数据集 LiveBench 的长上下文部分的性能较差。这些结果凸显了基于检索的长上下文评估（例如大海捞针）存在更深层次的问题。仅仅因为模型能够在上下文中检索信息，并不意味着它能够真正利用其整个上下文来解决问题。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feff7f0f4-1134-4d4b-a039-8bcc29faca67_1108x1560.png)

研究人员还指出，在大多数关键基准测试中，“双子座 - 2.5 Pro”的表现明显优于最大的Llama 4巨兽模型；详情见上文。

公众对Llama 4的看法。Llama 4报告的指标与公众评估结果之间的差异在人工智能研究界引发了诸多猜测和不满，甚至有人声称测试数据被故意纳入Llama 4的训练数据集以提高基准测试分数。Meta高管迅速否认了这些说法，并强调模型性能的波动是由于模型本身实现方式的差异、推理的量化策略等多种因素造成的。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc80fa15-7c41-484b-bbbc-4a11a01ce436_1971x770.jpeg)

尽管如此，围绕Llama 4发布的混乱氛围依然存在。此次发布在许多方面似乎都很仓促，从随机决定在周六而非随后的周一发布模型开始就可见一斑。

## Llama 的未来

Llama 4的发布在人工智能研究界反响不佳。然而，如今我们对这些模型有了深入的了解后，发现Llama 4背后的故事其实很复杂。与Llama 3相比，这些新的Llama模型修改——或者完全重新设计了几乎每一个组件：

- 基于混合专家（MoE）的模型架构。
- 不同的多模态处理方法（早期融合）。
- 原生多模态预训练。
- 在预训练过程中强调模型蒸馏。
- 完全不同的训练后处理流程。
- 注重长上下文能力。

随着DeepSeek-v3、Qwen-2.5等的成功，开放的大语言模型领域竞争愈发激烈。随着Llama 4的发布，Meta既回应了这一竞争态势，也明确了其打造前沿级Llama模型的目标。Llama 4并未实现这一目标，但这并不令人意外。Meta采取了一项（显而易见的）冒险举措——从长远来看，这一举措或许仍会被证明是正确的选择——即转变其研究策略。

前沿级Llama模型。鉴于大型语言模型（LLM）研究进展迅猛，Llama能否取得成功远非板上钉钉之事，而且在Llama 4不尽如人意之后，Meta还有很多工作要做。若要打造一款前沿级的大型语言模型，Meta需要更快速地迭代并改进其模型。那些密切使用Llama模型的人可能已经注意到，主要版本发布的时间间隔正在逐渐拉长：

- Llama于2023年2月发布。
- Llama 2于2023年7月发布。
- Llama 3于2024年4月发布。
- Llama 4于2025年4月发布。

这种不断扩大的差距令人担忧，且落后于顶尖实验室的水平；例如，自2024年1月以来，DeepSeek已经发布了DeepSeek-v1、v2、v3和R1。即使下一个Llama模型是最先进的，新模型也会很快随之发布。模型的发展和改进将以令人不安的速度持续下去。唯一的前进道路是快速迭代，并解决导致Llama 4内部和外部评估之间巨大脱节的评估能力差距。

开放的大型语言模型（LLM）格局。即使Llama模型并非最先进的，它们在开放的LLM格局中仍可能取得成功，在这一格局中，许多其他因素——如进入门槛和使用便捷性——都很重要。为了实现最大程度的成功，Meta必须竭尽所能避免限制开放LLM的使用场景。尤为值得注意的是，Llama 4模型需要被提炼成一系列更小、更密集的模型——类似于DeepSeek - R1和Qwen - 2.5——以避免大规模混合专家（MoE）模型对硬件的要求。创建前沿级别的Llama模型是一个重要目标，但不应以削弱Meta在开放LLM格局中的地位为代价。毕竟，Llama从来都不是性能最优的大型语言模型。对开放性的重视正是Llama最初取得成功的原因。