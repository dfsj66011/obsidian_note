
大型语言模型（LLMs）的普及彻底改变了我们人类解决问题的方式。在过去，使用计算机完成任何任务（例如，重新排版文档或对句子进行分类）都需要创建一个程序（即根据某种编程语言精确编写的一组指令）。而借助大型语言模型，解决此类问题只需一个文本提示即可。例如，我们可以通过类似下面所示的提示来引导大型语言模型重新排版任意文档。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3ce615-2714-44a4-975f-540806dc33e4_1376x1294.png)

正如上面的例子所示，大型语言模型（LLMs）的通用文本 - 文本格式使我们能够轻松解决各种各样的问题。我们在GPT - 3 [18]的提出中首次瞥见了这种潜力，表明足够大的语言模型可以利用小样本学习以惊人的准确性解决许多任务。然而，随着围绕LLMs的研究不断深入，我们开始超越这些基本（但仍然非常有效！）的提示技术，如零样本/小样本学习。

遵循指令的大语言模型（如InstructGPT和ChatGPT）促使我们探索语言模型是否能够解决真正困难的任务。也就是说，我们希望将大语言模型用于解决不仅仅是简单的玩具问题。为了具有实际用途，大语言模型需要能够遵循复杂的指令，并进行多步推理以正确回答人类提出的难题。不幸的是，这类问题通常无法通过基本的提示技术来解决。为了从大语言模型中激发复杂的问题解决行为，我们需要更复杂的方法。

### 拓展可能性的边界……

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e098a6a-3681-44e7-a3d8-b04555d8492f_2080x784.png)

在之前的文章中，我们学习了针对大语言模型更基础的提示方法，如零/少样本学习和指令提示。理解这些实用的提示技巧对于掌握本文将介绍的更高级的提示流程至关重要。如需了解这些技术的更多详情，请查看下方链接中的概述！

更好的提示 → 更好的结果。这类技术（假设应用得当）可以借助大语言模型完成很多任务。然而，由于种种原因，它们可能会力有不逮。少样本学习需要占用大多数大语言模型有限的上下文窗口来展示示例；如果未设置防护措施，大语言模型可能会被诱导输出有害内容；而且大多数模型不擅长解决推理任务或遵循多步骤指令。鉴于这些局限性，我们该如何推进以尝试用大语言模型解决难题呢？

一种方法是创建能力更强的LLM，可以是从头开始创建，也可以通过更优化的微调流程来实现。然而，这需要付出大量努力！要是我们能让现有的模型在解决问题方面表现得更好，那会怎样呢？在本文中，我们将探讨更高级的提示工程技术（例如，思维链提示、自动提示工程、信息检索等），这些技术能让我们提高LLM的性能，并激发更复杂的问题解决行为。学习这些理念十分重要，因为它们拓宽了LLM的应用可能性。例如，运用这些技术，我们可以：

- 让LLM访问外部知识数据库。
- 使复杂的、基于推理的问题得以解决。
- 通过让模型存储并获取对话中的先前信息，为LLM提供无限的记忆能力。

提示工程正在不断发展。本概述将侧重于对提示工程近期进展提供高层次的概览。我们将不会深入探究个别方法，而是聚焦于广泛了解可能有用的一系列提示技巧。然而，需要指出的是，提示工程这一主题既新颖又在迅速发展。几乎每天都有新的研究成果发布，许多前沿理念仅在网络上分享，而并未正式发表。因此，在未来几个月里，这一主题很可能会发生重大变化，从而拓展大语言模型可解决问题的范围。

### 高级提示技巧

我们现在将介绍提示工程领域的三个有影响力的主题。首先，我们将学习如何通过思维链提示（包括几种值得注意的扩展和变体）来提高大语言模型的推理能力。接下来，我们将讨论将大语言模型与外部数据库集成，从而使每个提示都能注入相关且准确的信息。最后，我们将学习如何利用自动提示工程方法从数据中发现更好的提示。

### 思维链提示及其拓展

在之前的文章中，我们介绍了思维链（CoT）提示 [1] 及其一些流行变体的主要思想。

什么是 CoT 提示法？CoT提示法是一种简单的技术，用于提高大语言模型在常识推理或符号推理等推理任务中的表现。CoT提示法利用少样本学习，在提示中插入几个已解决的推理问题的示例。每个示例都配有一个思维链（或推理依据），通过逐步解释问题的解决过程来增强问题的答案；具体内容见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

由于具备少样本学习能力，大语言模型（LLMs）可以通过观察思维链（CoT）提示中的示例，学会在给出答案的同时生成推理依据。先前的研究表明，以这种方式生成准确的推理依据能够提升推理性能[10, 11]，我们在使用思维链提示的实验中恰好观察到了这种效果。具体而言，教会大语言模型输出一条能解释其最终答案的相关思维链，可以大幅提高其在算术、符号和常识推理等任务上的表现；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb59f1adb-72c2-4b9b-8dd6-ee2da723f544_2272x1138.png)

流行的思维链（CoT）变体。除了基本的思维链提示之外，还探索了该技术的几种变体，例如：

- 零样本思维链提示（Zero - shot CoT prompting）¹³：替换所有示例推理过程，并在提示末尾插入语句“让我们一步一步地思考”。
- 自洽性（Self - consistency）¹⁴：使用大语言模型生成多条思维链，并将这些多个输出中的多数票作为最终答案。
- 从少到多提示（Least - to - most prompting）¹⁵：将推理问题分解为较小的步骤，一次解决一个步骤，其中每个子问题的输出用作下一个步骤的输入。

这些技术（如下图所示）与思维链（CoT）提示法相似，并能产生相近的结果，但它们各有独特优势。例如，零样本思维链提示法极其简单！我们只需在提示中插入一个陈述，而无需手写或精心策划多个相关的思维链示例。另一方面，从易到难提示法比普通思维链提示法略复杂一些，但这种技术也更有能力解决需要多步骤的推理问题。因此，我们可以使用从易到难提示法来解决思维链提示法难以应对的最艰巨任务。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfb3bd5-b081-48e7-906f-973eec16ea73_2332x1274.png)

在这些技术中，自我一致性是我个人最喜欢的。为什么？因为它是一种简单、应用广泛且非常有效的技术。事实上，这个想法甚至不特定于思维链（CoT）提示！自我一致性可以在许多情况下提高大语言模型（LLM）应用的性能。我们不是用大语言模型生成单一输出，而是生成多个输出，并取它们的平均值作为最终答案，从而提高可靠性和准确性。

这个想法让我想到了深度学习中的模型集成，在深度学习里，我们 i) 独立训练多个模型来解决某个任务，ii) 在推理时取每个模型输出的均值。尽管自洽性仅使用单个模型而非集成模型，但在更广泛的深度学习文献中已经应用了类似的技术；例如，为了模拟一个集成模型，可以从包含像dropout这类非确定性模块的神经网络中生成多个输出并取其均值 [19, 20]。

扩展思维链（CoT）提示法。思维链提示法是否真的教会了大型语言模型（LLMs）如何进行“推理”尚不清楚。然而，思维链提示法具有重大的实际意义，因为它可以用于利用大型语言模型解决复杂的多步骤问题。因此，最近围绕思维链提示法探索了许多有趣的想法。在文献[16]中，研究人员探索了多模态思维链提示法，其中同时使用图像和文本模态来执行不同的推理任务；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3d565bf-4e8b-4f51-8543-ede33f87d462_976x712.png)

除了探索多种数据模态（即图像和文本）外，[16]中的作者还对思维链（CoT）设置进行了微调，将多步推理过程生成和答案推断视为解决基于推理的任务中的两个不同步骤；具体内容见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7fdcae3-ac65-45b1-984f-fff0d3525da7_1314x556.png)

通过清晰地分离这些组件，我们可以更轻松地分析思维链（CoT）提示中的错误来源。因此，[16] 中的作者发现：i) 错误答案通常可能是由生成的理由中的幻觉导致的；ii) 使用多模态数据可以生成更有效的理由。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd952604f-789e-444c-bed1-c96fe18009b2_2148x1404.png)

进一步地，文献[17]中的作者将思维链（CoT）提示与主动学习（即利用模型本身来识别应纳入训练集的数据）的思想相结合。大语言模型首先使用思维链提示回答几个问题。在此基础上，通过同一大语言模型生成的多个答案之间的分歧来衡量“不确定性”，并据此识别模型理解较差的问题。随后，这一组问题会被人工标注上正确的思维链，并作为解决后续问题的示例。

在实践中应用思维链（CoT）提示可能遇到的最大问题之一是缺乏与我们试图解决的任务高度契合的少样本示例。也许我们能获取多个高质量的思维链添加到提示中，但如果我们试图解决的问题与这些示例所解决的问题略有不同，那该怎么办呢？尽管这样的问题可能会导致性能下降，但文献[17]中提出的方法旨在解决这一问题。具体来说，我们可以利用主动学习来动态识别用于思维链提示的现有示例是否不足以解决某个特定问题。

### 知识增强

尽管大语言模型（LLMs）在预训练期间学习了很多信息，但在其提示词中添加额外的相关信息通常会有所帮助。这种方法可以通过在生成输出时，为大语言模型的提示词提供准确的信息来源作为上下文，来帮助解决诸如幻觉（即生成错误事实）等问题。虽然实现这一点有多种方法，但我们重点关注基于信息检索和生成知识的技术。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e1cebd-b824-4e13-af98-4c226cb6a7e9_992x978.png)

信息检索。由于在信息检索中的作用，大语言模型（LLM）社区近期格外重视向量数据库技术（例如 Pinecone、Milvus、Weaviate 等），详见上文。从宏观层面来看，信息检索的目标是让大语言模型能够访问大量超出最大上下文窗口的文本信息库，具体通过以下方式实现：

1. 将文本分割成小部分。
2. 为每个文本块生成嵌入向量。
3. 将这些嵌入向量存储在向量数据库中。
4. 基于这些嵌入向量进行向量相似性搜索，以找出相关文本块并纳入提示中。

最终的结果是，我们可以快速找到相关的文本信息，作为额外上下文提供给大型语言模型（LLM）的提示词中。这种方法甚至可以与思维链（CoT）提示相结合，引导检索过程获取新的、有用的信息[2]。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c2223ce-8aa1-4a40-92e7-d2a91ee705ac_900x924.png)

生成式知识。信息检索功能强大（即它能够访问几乎无限量的信息！），但我们可能会想：外部向量数据库是否完全必要？有趣的是，近期研究[1]表明答案或许是否定的！我们可以仅通过提示另一个大语言模型生成信息来提升大语言模型的性能，而非存储和检索外部知识；详见上文。具体而言，我们可以通过少样本学习来实现这一点，即向一个大语言模型提供关于各类主题的知识生成示例，并以要求其生成关于目标主题的有用上下文作为结尾；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12354238-5515-445b-82ea-a4660e3505f3_1226x406.png)

从这里开始，我们可以在生成预测时将生成的信息作为额外的上下文输入。尽管不依赖任何外部数据库，这种方法仍能显著提升大语言模型在多个常识推理任务上的表现；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc50943e-9982-49f2-a5b9-a379669b2a40_1218x726.png)

生成式知识对于那些假设理解世界常识性知识的任务（如常识推理）最有帮助。简而言之，只要谨慎使用并用于合适的任务类型，大语言模型就是良好的信息来源。

> 生成式知识提示凸显了大型语言模型作为灵活的外部知识来源，可提升常识推理能力 —— 引自[1]


### 自动提示

提示工程的目标是对我们的语言模型输入进行微调，以使模型提供正确结果的可能性最大化。基于此，我们甚至可以将提示视为可调整的一组参数（例如，通过梯度下降或其他数据驱动的标准进行更新），以生成正确答案。基于数据自动更新提示的想法非常通用，但近年来已有几种此类技术在研究中得到了成功探索。

自动提示工程师（APE）[4] 提出了一种自动生成提示指令的简单方法。首先，利用带有若干指令示例的少样本提示，让大语言模型提出一组潜在指令。文中探索了几种用于生成指令的提示模板；具体内容见下文。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c6f6b60-13b6-4d73-bd5d-45e0b314603a_550x1314.png)

然后，我们通过对使用每条指令的大语言模型（LLM）的零样本性能（即正确结果的准确率或对数概率）进行评估，在这组指令“候选项”中进行搜索。换句话说，每条提示词下大语言模型的性能被用作评估指令质量的指标。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1dbee5f5-5569-4cb5-8ff7-a37293c73d04_1880x1114.png)

进一步研究发现，在[4]中可以看到，通过重复这一过程，指令可以迭代优化。具体而言，我们可以：i) 提出一组候选指令；ii) 根据性能对这些候选指令进行评估；iii) 筛选出表现最优的候选指令；iv) 通过提示大语言模型生成相似指令（即重新采样），为表现最优的候选指令生成新的变体。该过程（及相关提示）如下图所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3d6092-7445-4c4d-97cd-d87491e9106a_1522x406.png)

**基于梯度的搜索。​**除了搜索更优文本提示的技术外，还有一系列实用的提示工程工作探索了对提示嵌入的连续更新。首先，我们应该回顾一下语言模型中的提示嵌入是什么。给定一个文本提示，我们通常会对这个提示进行分词（即将其拆分为单词或子词），然后查找每个分词结果的嵌入。这个过程会得到一个分词嵌入列表（即一个提示嵌入！），我们将其作为输入传递给语言模型；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4106638-f995-42c9-8305-4db9f6adb219_1852x1008.png)

多项研究探索了直接修改提示嵌入（即每个标记的嵌入列表）的提示工程策略。换句话说，这些研究并不直接修改提示的词语，而是使用类似梯度下降的规则更新提示嵌入。该领域的主要研究如下：

- AutoPrompt [5] 将原始提示输入与一组共享的（适用于所有输入数据）“触发标记”相结合，这些触发标记通过基于梯度的搜索选出以提高性能。
- 前缀调整（Prefix Tuning）[6] 在输入层和隐藏层的提示嵌入中添加几个“前缀”标记，然后使用梯度下降法训练这些前缀的参数（固定模型参数），作为一种参数高效的微调策略。
- 提示调整（Prompt Tuning）[7] 与前缀调整类似，但前缀标记仅添加到输入层。这些标记会在语言模型解决的每个任务上进行微调，使前缀标记能够针对特定任务对模型进行调节。
- P - 调整（P - Tuning）[8] 向模型的输入层添加特定任务的锚定标记并进行微调，但允许将这些标记放置在任意位置（例如，提示的中间），这使得该方法比前缀调整更灵活。

我们应该使用哪一种方法呢？所有这些方法（如下所示）都探索了向语言模型中添加“软”标记，并在目标数据集上进行监督微调。值得注意的是，这些技术不能用于只能通过付费 API 访问的语言模型（例如 OpenAI API）。这是因为我们需要能够访问和修改提示嵌入，而大多数 API 仅提供模型的文本输入和输出。目前，如果我们使用的是自己托管的大语言模型，就只能使用基于梯度的自动提示技术。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08f7a86f-3063-4495-b356-06624984ac1d_2770x1320.png)

在这些方法中，提示调整（Prompt Tuning）最为简单，并且能带来显著的性能提升。通过提示调整，我们只需 i) 在输入中添加一些前缀标记嵌入，以及 ii) 针对各个下游任务对这些嵌入进行参数高效的微调。[7] 中的方法通过对每次更新混合多个不同任务并为每个任务分配一个独特的学习前缀来执行多任务微调；详见下文。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc082817-d331-4073-9c88-a0964178c88f_612x390.png)
通常，微调语言模型意味着我们必须为每个任务存储一份单独的模型参数副本。相比之下，提示调优仅对一小部分前缀标记嵌入进行微调，并保持其余模型参数不变。如下图所示，尽管只微调了一小组参数，提示调优在性能上却非常接近端到端微调的效果。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4a3cb86-d21f-456a-8910-f7373f6a0cb1_746x932.png)


### 总结

> “随着模型规模的扩大，我们还能期待推理能力有多大程度的提升？还有哪些其他的提示方法可能会扩大语言模型所能解决的任务范围？” —— 引自[9]

本概述的主要目的是探索一些在实际应用中可能对利用大型语言模型（LLMs）解决难题有用的提示技术。如果运用得当，零/少样本学习和指令提示等基础技术是有用且有效的。然而，若要让大型语言模型能够解决基于推理的任务或遵循复杂的多步骤指令，可能需要更复杂的技术。尽管随着时间的推移，模型的质量可能会提高，并且能更轻松地处理这类难题，但本概述所涵盖的技术可用于拓展当前可用的大型语言模型的应用范围。以下是这些技术的一些基本要点。

解决难题。对思维链（CoT）提示的分析表明，大语言模型（LLMs）能够解决复杂的多步骤问题。然而，要做到这一点，需要将问题分解成较小的部分，无论是由模型自身完成还是由外部协助完成。我们可以通过鼓励模型在给出答案之前先生成问题解决的推理过程来隐式地实现这一点，也可以通过使用从易到难的提示方法将问题分解成小部分，再由大语言模型分别解决这些问题部分来实现。无论采用哪种方式，我们通常都会发现，鼓励大语言模型逐步解决问题，而非整体一次性解决，会带来更好的效果。

学习提示工程。如果听到“提示工程”这个词，我们大多数人可能会想到调整提示的措辞或结构，看看哪种效果最好。然而，这并不是提示工程的唯一方法！具体来说，我们可以采用一种自动提示工程的方法，通过梯度下降从数据中学习最优提示。为此，我们将提示嵌入（即提示中每个标记的嵌入列表）设置为可训练，并进行微调。尽管这种方法有趣且有用，但有几点需要注意：

1. 学习到的提示嵌入无法映射回文本提示，因为模型词汇表中每个标记的嵌入是离散的。
2. 只有在能够访问语言模型的嵌入层时，才能使用这些方法。而通过付费API（例如OpenAI提供的API）是无法获得这种访问权限的。
    

简单却强大。尽管本概述聚焦于高级提示工程技巧，但有许多简单的诀窍可以轻松应用，以提升大语言模型（LLM）应用程序的性能。例如，自洽性可以通过生成多个答案并取其平均值来提高大语言模型的可靠性。零样本思维链（Zero - shot CoT）提示只需在提示末尾添加一个陈述，就能轻松提升大语言模型的推理能力。最后，生成式知识只需让模型在生成最终答案之前列出有关某个主题的有用信息，就能提高大语言模型的性能。在许多情况下，向我们的提示工程工具包中添加简单诀窍能产生显著效果！
