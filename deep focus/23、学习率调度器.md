
> https://cameronrwolfe.substack.com/p/the-best-learning-rate-schedules


凡训练过神经网络的人都知道，学习率的合理设置是决定模型性能的关键因素。不仅如此，学习率通常还需要根据训练进程动态调整，这种调整策略（即学习率调度）对训练质量有着举足轻重的影响。

当前业界普遍采用几种经典的学习率调度策略，例如分段衰减（step decay）或余弦退火（cosine annealing）。这些策略往往针对特定基准测试场景开发，经过多年实证研究才能确定其最优参数。但令人困扰的是，这些精心调校的策略在其他实验环境中常常失效，这引出一个核心问题：_是否存在普适性强且效果稳定的学习率调度方法？_

本文将系统梳理该领域的最新研究成果，这些研究揭示了许多高效易用的学习率调度策略，例如循环学习率（cyclical）和三角学习率（triangular）等。通过分析这些方法，我们将提炼出可直接应用于实际训练的实用技巧。

为便于实践，笔者已将本文涉及的主要学习率调度策略实现并开源（[代码仓库](https://github.com/wolfecameron/LRSchedules)）。这些代码示例虽然精简，但足以轻松复现本文讨论的所有调度策略。

### 1、NN 训练和学习率

在监督学习框架下，神经网络训练的核心目标是构建一个能够根据输入数据预测其对应真实标签的模型。典型范例是：基于已标注的大型猫狗图像数据集，训练神经网络准确判断图像内容属于猫或狗。  

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F894b2d37-dc26-4be8-bf90-a0cc8a2bfa12_692x628.png)

神经网络训练的基本组件如下图所示，其核心构成要素包括：

* 神经网络架构：接收输入数据后，通过内部参数/权重对数据进行非线性变换以产生输出
* 训练数据集：由大量输入-输出数据对组成的集合（如图像及其对应分类标签）
* 优化器：动态调整神经网络内部参数，使预测结果逐渐逼近真实值
* 超参数体系：由深度学习工程师设定的外部参数，用于控制训练过程的关键细节

典型训练流程始于随机初始化所有网络参数。为学习有效特征表征，网络会逐步处理数据集中的样本——对每个样本尝试预测正确输出后，优化器立即根据预测误差更新网络参数。这种通过参数迭代使网络输出逼近已知标签的过程称为模型训练，整个数据集通常需要被反复遍历多次（每个完整遍历称为一个训练周期 / epoch）。

#### 1.1 超参数

模型参数由优化器在训练中自动调整，而超参数则是工程师可控的外部调节旋钮。其中与本文最相关的关键超参数是学习率——它本质控制着优化器每次参数更新的步长幅度。学习率设置需要权衡：

- **过大**会导致训练发散
- **过小**则可能引发收敛缓慢与性能瓶颈

理想值应在保证训练稳定的前提下，尽可能提升收敛速度与正则化效果。

#### 1.2 超参数优化方法论

常规做法是采用网格搜索：

1. 为每个超参数定义取值范围
2. 在范围内选取离散测试点
3. 穷举所有参数组合进行验证
4. 根据验证集表现选择最优配置

如下图所示，这种系统化搜索策略虽然计算量较大，但能可靠定位较优的超参数组合。更先进的贝叶斯优化等方法可进一步提升搜索效率。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fee713a86-ed9a-424e-89e1-059b55a8abc2_1432x884.png)

这种网格搜索方法可扩展应用于多组超参数的联合优化——通过测试所有可能的超参数组合来寻找最优配置。然而其计算效率较低，因为每个超参数组合都需要重新训练神经网络。为降低计算成本，许多从业者会采用"猜测-验证"的启发式方法：在合理范围内尝试若干组超参数，通过观察实际效果来确定最佳配置。虽然已有更先进的超参数优化方法被提出，但由于其简单易用性，网格搜索和猜测验证仍是当前主流方案。

#### 1.3 学习率动态调度机制

选定初始学习率后，通常不应在整个训练过程中保持固定值。业界普遍遵循的准则是：

1. 设置初始学习率
2. 在训练过程中逐步衰减学习率

这种衰减策略的函数表达即为学习率调度。多年来研究者提出了多种调度方案，主要包括：

- 阶梯衰减（Step decay）：在训练关键节点将学习率降低 10 倍
- 余弦退火（Cosine annealing）：按余弦曲线平滑调整学习率

本文将重点解析若干新近提出的高效调度策略，这些方法在实验中都展现出卓越的性能表现。通过系统比较不同调度算法的收敛特性与泛化能力，我们将为读者提供切实可行的调参指南。值得注意的是，最新研究表明：动态学习率不仅能加速收敛，还具有隐式正则化效果，这解释了为何精心设计的调度策略往往能获得更优的模型性能。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6a6df48-d0fd-4c5d-a0f4-bbc1015f9cb6_800x600.png)

**自适应优化技术解析**   传统 SGD 采用全局统一的学习率更新所有模型参数，而自适应优化算法（如RMSProp、Adam）通过训练统计量动态调整每个参数的学习率。值得注意的是，本文讨论的多数学习率调度策略可同时适用于 SGD 和自适应优化器。


### 2、前沿学习率调度方案  

本节将分析几种新型学习率调度策略，包括循环学习率、三角学习率等动态衰减模式。虽然最优策略高度依赖具体任务场景，但通过多组实验数据的横向对比，我们仍能提炼出普适性规律。

#### 2.1 循环学习率

该研究突破性地提出：在训练过程中，让学习率按照平滑曲线在预设范围内周期性波动。这颠覆了传统 "先大后小" 的单调衰减范式——尽管学习率上升阶段会暂时降低模型表现，但整个训练周期结束后反而能获得显著收益。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f4b9a3-56c7-4889-b53b-5efed17bb8c0_1132x734.png)

**循环学习率引入三个新超参数**：步长（stepsize）、最小学习率和最大学习率。由此产生的调度呈"三角形"特征——即相邻周期内学习率递增/递减（见上文示意图）。步长通常设置为 2-10 个训练周期，而学习率范围则通过学习率范围测试确定。

提高学习率会暂时降低模型性能，但当学习率再次衰减后，模型性能不仅会恢复还将进一步提升。实验结果显示：采用循环学习率的模型性能呈现周期性波动规律——每个周期结束时（即学习率衰减至最小值时）性能达到峰值，而在周期中期阶段（即学习率上升期间）性能会出现暂时性回落。
![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4f94309-db71-41c4-ab26-f822feaef60e_1136x754.png)

结果表明，周期性学习率有利于在训练过程中提高模型性能。通过周期性学习率训练的模型比使用其他学习率策略训练的模型更快地达到更高的性能水平。换句话说，使用周期性学习率训练的模型的任何时候的性能都非常好！
![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F81b62007-11a0-4e01-b90d-b7a096b31327_1134x812.png)

在 ImageNet 上进行更大规模的实验中，周期性学习率仍然有好处，尽管好处不那么明显。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8023b8cb-4508-4680-84fb-fccddc124dff_1766x664.png)



#### 2.2 SGDR: Stochastic Gradient Descent with Warm Restarts

提出了一种简单的学习率重启技术，称为带重启的随机梯度下降 (SGDR)，其中学习率会定期重置为其原始值并按计划降低。该技术采用以下步骤：

1. 根据固定的时间表衰减学习率
2. 衰减计划结束后将学习率重置为原始值
3. 返回步骤＃1（即再次降低学习率）

下面描述了遵循此策略的不同时间表。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0f83c919-eb1f-4fa5-94e9-6e9fe2cbc51b_1356x892.png)

通过观察上述调度策略，我们可以注意到几个关键特征：首先，始终采用余弦衰减调度（图表纵轴为对数刻度）。此外，随着训练推进，每个衰减周期的持续时间会逐步延长。具体而言，作者将首个衰减周期长度定义为 $T_0$，后续每个周期的长度会乘以系数 $T_\text{mult}$ 进行扩展。
![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff44786f7-9a31-4f85-a8c2-285715e67124_800x600.png)

SGDR 的步长可能会在每个循环后增加。然而，SGDR 不是三角形的（即，每个循环只会衰减学习率）。 在 CIFAR10/100 上的实验中，我们可以看到 SGDR 学习率计划比步进衰减计划更快地产生良好的模型性能——SGDR 具有良好的随时性能。每个衰减周期后获得的模型表现良好，并在连续的衰减周期中继续变得更好。
![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1ada447d-64d7-4cf3-ba6f-a9489a318b3a_980x1440.png)

除了这些初步结果之外，我们还可以研究通过在每个衰减周期结束时拍摄“快照”而形成的模型集合。具体来说，我们可以在 SGDR 计划中保存每个衰减周期后模型状态的副本。然后，在训练完成后，我们可以在推理时平均每个模型的预测，形成一个模型集合/组；有关集合概念的更多详细信息，请参阅下面的链接。

[More on Ensembles](https://towardsdatascience.com/neural-networks-ensemble-33f33bea7df3)

通过以这种方式形成模型集成，我们可以显著减少 CIFAR10 上的测试误差。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4c8dd498-c586-421a-85aa-3b25aa42d789_1348x792.png)

此外，SGDR 的快照似乎提供了一组具有不同预测的模型。以这种方式形成集成实际上优于将独立的、经过充分训练的模型添加到集成中的常规方法。

#### 2.3 Super-Convergence

作者研究了一种有趣的神经网络训练方法，这种方法可以将训练速度提高一个数量级。基本方法是执行一个具有较大最大学习率的单一三角学习率循环，然后让学习率在训练结束时衰减到该循环的最小值以下
![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e523e0-73a0-4e29-a903-423e2970fecd_1100x600.png)

此外，动量以与学习率相反的方向循环（通常在 [0.85, 0.95] 范围内）。这种共同循环学习率和动量的方法称为 “1cycle”。作者表明，它可用于实现“超收敛”（即极快收敛到高性能解决方案）。 

例如，我们在 CIFAR10 上的实验中发现，1cycle 可以在训练迭代次数减少 8 倍的情况下，实现比基线学习率策略更好的性能。使用不同的 1cycle 步长可以进一步加快训练速度，但准确度水平会因步长而异。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3a3a8779-7c01-4960-a125-041b80f046df_1880x760.png)

我们可以在几个不同的架构和数据集上观察到类似的结果。见下表，其中 1cycle 再次在极少的训练周期内产生了良好的性能。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faab7d86b-7f98-425f-9059-b5d19870ce24_1188x1132.png)

目前，尚不清楚超收敛是否可以在大量实验环境中实现，因为它提供的实验在规模和种类上都有些有限。尽管如此，我们可能都同意超收敛现象相当有趣。事实上，结果非常有趣，甚至被 [fast.ai](http://fast.ai/) 社区推广和深入研究。

#### 2.4 REX

作者（包括我自己）考虑了在不同预算制度（即训练周期数少、中或多）下合理安排学习率的问题。你可能会想：_我们为什么要考虑这种设置？_ 因为通常情况下，最佳训练周期数是无法提前知道的。另外，我们可能使用固定的货币预算，这会限制我们可以进行的训练周期数。

为了找到最佳的与预算无关的学习率计划，我们必须首先定义将要考虑的可能的学习率计划空间。我们通过将学习率计划分解为两个部分来实现这一点：

1. 概况：学习率在整个训练过程中变化的函数。
2. 采样率：根据所选配置文件更新学习率的频率。

这种分解可用于描述几乎所有固定结构学习率计划。下面描述了不同的配置文件和采样率组合。更高的采样率使计划与底层配置文件更紧密地匹配。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F958723af-9979-4f48-ae53-07e85f2ac095_1996x476.png)

作者考虑了由不同的采样率和三个函数配置文件——指数（即产生步进时间表）、线性和 REX 形成的学习率时间表。

从这里开始，作者在 CIFAR10 上使用不同的采样率和配置文件组合训练 Resnet20/38。在这些实验中，我们看到步进衰减计划（即具有低采样率的指数配置文件）仅在低采样率和许多训练时期的情况下表现良好。每次迭代采样的 REX 计划在所有不同的时期设置中都表现良好。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feaaf0ac7-53fc-457c-9fa9-4e3fecfb8a3e_2046x1154.png)

先前的研究表明，线性衰减方案最适合低预算训练设置（即训练次数较少）。我们可以看到 REX 实际上是更好的选择，因为它避免在训练期间过早衰减学习率。

从这里开始，作者考虑了各种流行的学习率方案：

![|200](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbf45c03-53c5-40de-85e0-2b6778d704ed_794x540.png)

这些计划在各种领域和训练周期预算中进行了测试。当汇总所有实验的性能时，我们得到了如下所示的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8f9e817-271a-4bfb-964a-33dd0be99848_2042x702.png)

我们立即发现，REX 在不同的预算制度和实验领域中实现了惊人的一致表现。在实验中，没有其他学习率方案能达到接近相同的前 1/3 完成率，这表明 REX 是一种与领域/预算无关的良好学习率方案。

除了 REX 的一致性之外，这些结果还告诉我们一些更普遍的东西：_常用的学习率策略在实验设置中不能很好地推广。_ 每个计划（甚至是 REX，尽管程度较小）仅在少数情况下表现最佳，这表明为任何特定设置选择合适的学习率策略非常重要。

### 总结

正确处理学习率可以说是神经网络训练中最重要的方面。在本概述中，我们了解了几种用于训练深度网络的实用学习率方案。研究这项工作提供了简单易懂、易于实施且高效的要点。下面概述了其中一些基本要点。

**选择合适的学习率。** 正确设置学习率是训练高性能神经网络的最重要方面之一。选择较差的初始学习率或使用错误的学习率计划会严重降低模型性能。

**“默认”时间表并不总是最好的。** 许多实验设置都有一个“默认”学习率时间表，我们倾向于不假思索地采用它；例如，用于训练 CNN 进行图像分类的步进衰减时间表。我们应该意识到，随着实验设置的变化，这些时间表的性能可能会急剧下降；例如，对于预算设置，基于 REX 的时间表明显优于步进衰减。作为从业者，我们应该始终注意我们选择的学习率时间表，以真正最大化我们模型的性能。

**周期性学习计划非常棒。** 周期性或三角学习率计划非常有用，因为：

- 它们通常达到或超过最先进的性能
- 他们随时都有良好的表现
    

使用循环学习率策略，模型在每个衰减周期结束时达到最佳性能。我们可以简单地继续训练任意数量的周期，直到我们对网络的性能感到满意为止。最佳训练量不需要事先知道，这在实践中通常很有用。

**还有很多东西需要探索。** 虽然学习率策略已经被广泛研究，但似乎还有更多的东西有待发现。例如，我们已经看到采用替代衰减曲线有利于预算设置，在某些情况下，循环策略甚至可能用于实现超收敛。我的问题是：_还有什么可以发现的？_ 似乎有一些非常有趣的策略（例如分形学习率）尚待探索。 

#### 软件资源

作为本概述的补充，我创建了一个轻量级代码库来重现一些不同的学习率计划，其中包括：

- 生成不同衰减曲线的函数
- 用于调整 PyTorch 优化器中的学习率/动量的函数
- 我们在本概述中看到的常见学习率方案的工作示例

[View the Repo](https://github.com/wolfecameron/LRSchedules)


