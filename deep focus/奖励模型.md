
在推理模型时代下对人类偏好的 LLM 建模...


奖励模型（RMs）是大语言模型（LLM）研究的基石，通过将人类偏好融入训练过程实现了重大突破。尽管作用关键，这类模型却常被忽视。关于如何有效训练和使用它们的实用指导仍然匮乏——尤其是在*基于可验证奖励的强化学习等无 RM 技术日益流行之际*。但采用 PPO 强化学习训练 LLM，依然是开发顶级基础模型的关键要素。本综述将系统性地深入解析奖励模型，阐明其在快速演进的 LLM 生态系统中持续发挥的历史与现实意义。

## 什么是奖励模型？
 
![|700](https://substackcdn.com/image/fetch/$s_!UkPk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94ec9186-9bf4-4b06-a6b7-7eb119b91e1a_2020x650.png)

> _“奖励模型在强化学习研究中被广泛用作环境奖励的替代品……最常见的奖励模型会预测一段文本与训练比较中偏好文本的接近概率。”_ - [RLHF book](https://rlhfbook.com/c/07-reward-models.html)

奖励模型（RMs）是专门化的大语言模型——*通常源自我们正在训练的大语言模型*——其训练目的是根据输入的提示和候选补全内容预测人类偏好分数（参见上文）。RM 给出的分数越高，表明该补全内容越可能受到人类青睐。

作为第一步，我们必须建立对奖励模型（RMs）的基本理解，包括它们是如何创建的，以及我们如何在大型语言模型（LLMs）的背景下使用它们。在本节中，我们将重点理解以下内容：

* 正如从偏好统计模型中得出的那样，RMs 的动机。
* 大多数关系型数据库采用的架构与结构
* RM 的训练过程

要理解奖励模型（RMs）的运用方式，我们需要先了解强化学习（RL）和大语言模型（LLM）微调的相关背景知识，这些内容将在下一章节详细阐述。

#### Bradley-Terry 偏好模型

RM 的标准实现源自 [Bradley-Terry 偏好模型](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)——这是一种统计模型，*用于根据配对比较数据中项目的相对强度或表现进行排名*。给定从同一分布中抽取的两个事件 $i$ 和 $j$，Bradley-Terry 模型将项目 $i$ 相对于项目 $j$ 获胜（或 *被偏好*）的概率定义如下：
![|300](https://substackcdn.com/image/fetch/$s_!zgW4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bb1622e-98a9-4e69-9220-433c8085cd93_884x612.png)
在大型语言模型（LLM）的背景下，项目 $i$ 和 $j$ 是由同一 LLM 基于相同提示生成的两个补全（即这些补全是从同一分布中采样的）。奖励模型（RM）为每个补全分配一个分数，然后我们使用上述 Bradley-Terry 模型中的表达式来推导补全 $i$ 优于补全 $j$ 的概率。简而言之，*我们使用 Bradley-Terry 模型来表达两个补全之间成对比较的概率。*

![|300](https://substackcdn.com/image/fetch/$s_!rKGp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F609d472d-1a82-4fd4-8c25-fe4e7253ee13_610x1118.png)

**偏好数据**    成对偏好数据已被广泛用于大语言模型的后训练中，且*已使用相当长的时间*。这类数据由众多不同的提示组成，我们力求最大化提示的多样性。提示的分布应能代表模型在实际应用中可能遇到的提示类型。每个提示对应一对候选补全项，其中一项（*通常由人类标注，有时也由模型判定*）被认为优于另一项；详见上文。这种包含提示及其对应选中与被拒补全项的数据集被称为（人类）偏好数据集。

#### RM 是如何运作的？

我们知道奖励模型（RM）基于 Bradley-Terry 偏好模型，但在实际应用中可以采用多种方式实现这种统计模型。在大型语言模型（LLM）领域，这些模型的实现方式——*或许并不令人意外*——正是通过 LLM 本身完成的。然而与标准的（生成式）纯解码器架构 LLM 相比，奖励模型对底层架构和训练目标都进行了调整。

![|600](https://substackcdn.com/image/fetch/$s_!M_zU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0757f3b6-d8a3-49da-80dc-74b9bcb9a1aa_1716x890.png)


**RM 架构**    RM 以 LLM 生成的提示-补全对作为输入，输出一个（标量）偏好分数。实践中，RM 是通过在仅解码器架构末端添加线性头部实现的 LLM；具体如上所述。具体来说，LLM 输出一个标记向量列表——每个输入标记向量对应一个——我们将列表中的最后一个向量通过线性头部传递，生成一个单一的标量分数。*我们可以将 RM 视为带有额外分类头部的 LLM，用于将给定的补全分类为偏好或非偏好。*

**训练过程**    奖励模型（RM）的参数通常使用现有策略进行初始化，我们将其称为 RM 的“基础”模型。初始化 RM 的策略有多种选择；例如，正在训练的大语言模型（LLM）或其早期版本，如预训练的基础模型或监督微调（SFT）模型。RM 初始化完成后，我们会为其添加一个线性头部，并在偏好数据集（即针对同一提示的模型优选和弃用响应配对）上进行训练。

![|500](https://substackcdn.com/image/fetch/$s_!RqTs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd836bca0-f804-4052-8b7f-4eb2b1e2356b_1392x530.png)

给定一个偏好对，我们希望我们的奖励模型（RM）能够为被选中的回答分配比被拒绝的回答更高的分数。换句话说，最优的奖励模型应最大化被选中回答优于被拒绝回答的概率。正如我们之前所学，可以使用 Bradley-Terry 模型来表达这一概率（见上文）。通过重新排列这一概率表达式，我们可以推导出如下所示的损失函数，这是一种[成对排序损失](https://gombru.github.io/2019/04/03/ranking_loss/)，其目的仅仅是鼓励模型为被选中的回答分配更高的分数。

![|500](https://substackcdn.com/image/fetch/$s_!iPQn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc84db389-0e57-4a3c-808b-d48b28a192d6_1204x392.png)

我们可以将其视为负对数似然（NLL）损失函数，其中 NLL 的概率由 Bradley-Terry 模型给出。下图展示了该损失函数的可视化情况，*从中可见当选取得分最大化且被拒得分最小化时，损失函数达到最小值*。通过在大型偏好数据集上对该损失函数进行经验性最小化，我们能够（近似地）最大化选定回答优于被拒回答的期望概率。

![|400](https://substackcdn.com/image/fetch/$s_!qlGB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9004e549-5346-4e1b-8c00-faa76fa72bf6_1568x1288.png)


**对奖励进行归一化处理**    训练完成后，奖励模型会输出未经归一化的标量值。为了降低奖励函数的方差（即确保奖励模型的输出落在标准范围内），我们可以对奖励模型的输出进行归一化，使其在用于训练的偏好数据集上平均奖励为零。

#### 实现一个 RM

为了让讨论更具实践性，让我们学习如何利用常见的深度学习框架实现奖励模型（RM）——*包括其架构和损失函数*。奖励模型本质上就是一个分类模型，*它对文本序列进行文本分类*。当输入提示词和对应回复时，奖励模型会预测这个提示-回复组合被优先选择的概率（即输出单个标量分数）。

**Toy 示例**：我们可以通过类似HuggingFace的AutoModelForSequenceClassification这样的抽象来实现这一点。下面提供了一个可以在本地运行的小型（基于BERT的）RM实现，其中我们：

**Toy example.** We can implement this via an abstraction like HuggingFace’s `AutoModelForSequenceClassification`. An implementation of a small ([BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)-based) RM that can be run locally is provided below, where we:

- Create the RM using `AutoModelForSequenceClassification`.
    
- Compute the RM’s output—_in the form of a single logit_—for all chosen and rejected sequences[3](https://cameronrwolfe.substack.com/p/reward-models#footnote-3-166169560).
    
- Compute the RM’s loss as described above.
    

```
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
import torch

# Load a tiny model for sequence classification
model_name = "google-bert/bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    trust_remote_code=True,
)

# Chosen prompt-response sequences
chosen_seqs = [
    "I love deep (learning) focus!",
    "Cameron is great at explaining stuff",
    "AGI is coming very soon...",
]

# Rejected prompt-response sequences
rejected_seqs = [
    "I'm not a fan of deep (learning) focus",
    "Cameron doesn't know what he's talking about",
    "AGI is fake and LLMs can't reason!",
]

# Tokenize the chosen / rejected sequences
chosen_inps = tokenizer(
    chosen_seqs,
    return_tensors="pt",
    padding=True,
)
rejected_inps = tokenizer(
    rejected_seqs,
    return_tensors="pt",
    padding=True,
)

# Compute the RM's output
rewards_chosen = model(**chosen_inps).logits[:, 0]
rewards_rejected = model(**rejected_inps).logits[:, 0]

# Compute the RM's loss
loss = -torch.nn.functional.logsigmoid(
    rewards_chosen - rewards_rejected
).mean()
print(loss)
```

From here, we train the RM [similarly to any other model](https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html); i.e., by _i)_ looping over a preference dataset, _ii)_ computing the loss as outlined above, _iii)_ obtaining a gradient via [backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U), _iv)_ performing a gradient update and _v)_ repeating.

**Real RM training example.** For a more practical view of what training an RM looks like at an LLM research lab, we can look at the [RM training script](https://github.com/allenai/open-instruct/blob/main/open_instruct/reward_modeling.py) in AI2’s [OpenInstruct](https://github.com/allenai/open-instruct). This script implements distributed training of an RM—_based upon [OLMo-2](https://arxiv.org/abs/2501.00656) or [OLMoE](https://arxiv.org/abs/2409.02060)_—using [accelerate](https://huggingface.co/docs/accelerate/en/index). The script is quite simple, and most of the code is actually just configuring the training process. We can parse through this training script to find the core RM training loop, copied below for reference.

```
for _ in range(args.num_train_epochs):
    for data in dataloader:
        training_step += 1

        # Concat the chosen / rejected sequences
        query_responses = torch.cat(
            (
                data[CHOSEN_INPUT_IDS_KEY],
                data[REJECTED_INPUT_IDS_KEY]
            ),
            dim=0,
        )
        with accelerator.accumulate(model):
            # Predict reward for each sequence with RM
            _, predicted_reward, _ = get_reward(
                model,
                query_responses,
                tokenizer.pad_token_id,
                0,
            )

            # Parse chosen / rejected rewards from output
            chosen_reward = predicted_reward[
                :data[CHOSEN_INPUT_IDS_KEY].shape[0]
            ]
            rejected_reward = predicted_reward[
                data[CHOSEN_INPUT_IDS_KEY].shape[0] :
            ]

            # Compute loss and gradient for RM
            loss = -F.logsigmoid(chosen_reward - rejected_reward).mean()
            accelerator.backward(loss)

            # Perform parameter update for RM
            optimizer.step()
            optimizer.zero_grad()
```

As we can see, this code, which is used for training large-scale RMs at a top research lab, is not much different from our toy example! Of course, the training loop is largely made simple by abstractions provided by modern deep learning packages like HuggingFace. However, _the key takeaway here is that the concepts we have learned so far directly translate to practical training and usage of RMs_.

#### Different Types of RMs

So far, we have focused on the standard form of an RM, typically referred to as a classifier-based RM. However, RMs are just models that predict a preference score given a prompt and response, which we can implement in many ways. For example, we can train a custom classifier like [ArmoRM](http://arxiv.org/abs/2406.12845) to serve as an RM.

[

![](https://substackcdn.com/image/fetch/$s_!MLb6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15fe250b-0749-4303-8052-2641bc1dff20_1086x452.png)



](https://substackcdn.com/image/fetch/$s_!MLb6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15fe250b-0749-4303-8052-2641bc1dff20_1086x452.png)

(from [9])

**LLM-as-a-Judge** models can also be used as an RM by simply prompting an LLM judge to provide a preference score; see above. These preference scores can then be taken as the reward signal during training with RL. For a more in-depth overview of LLM-as-a-Judge, please see the article linked below.

[](https://cameronrwolfe.substack.com/p/llm-as-a-judge)

[

![Using LLMs for Evaluation](https://substackcdn.com/image/fetch/$s_!RWz4!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cca744e-8ad5-4266-9680-7da4fe94f497_1878x1052.png)

](https://cameronrwolfe.substack.com/p/llm-as-a-judge)

[

#### Using LLMs for Evaluation

](https://cameronrwolfe.substack.com/p/llm-as-a-judge)

[](https://cameronrwolfe.substack.com/p/llm-as-a-judge)

[](https://cameronrwolfe.substack.com/p/llm-as-a-judge)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

2024年7月22日

[

Read full story

](https://cameronrwolfe.substack.com/p/llm-as-a-judge)

Alternatively, we can use LLM judges to collect synthetic preference data—_using prompts like the one shown below from [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)_—and train an RM normally over this synthetic data, as is done by [Constitutional AI](https://cameronrwolfe.substack.com/i/136751520/constitutional-ai-harmlessness-from-ai-feedback) [10] and [RLAIF](https://cameronrwolfe.substack.com/i/136751520/rlaif-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback) [11].

[

![](https://substackcdn.com/image/fetch/$s_!tms4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda52e00b-d851-46f4-89a8-0d46e8badbe9_2650x1444.png)



](https://substackcdn.com/image/fetch/$s_!tms4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda52e00b-d851-46f4-89a8-0d46e8badbe9_2650x1444.png)

([source](https://github.com/tatsu-lab/alpaca_eval))

**Outcome Reward Models (ORMs)** [12] and **Process Reward Models (PRMs)** [11] are two other commonly-used variants of RMs in the literature. ORMs, which are mostly used for reasoning tasks, predict the probability that a completion is the correct answer to a task. To train an ORM, we collect a preference dataset similarly to before, but each preference pair contains both an incorrect and a correct answer to a given question. Unlike a standard RM that predicts the reward at a sequence level, the ORM predicts correctness on a per-token basis.

> _“Our verifiers are language models, with a small scalar head that outputs predictions on a per-token basis.”_ - from [12]

[

![](https://substackcdn.com/image/fetch/$s_!fqNz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf5d0e0c-de50-4f50-9b4a-b11d3a2c45fb_1964x960.png)



](https://substackcdn.com/image/fetch/$s_!fqNz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf5d0e0c-de50-4f50-9b4a-b11d3a2c45fb_1964x960.png)

(from [12])

Like ORMs, PRMs are used primarily for reasoning tasks and predict more granular outputs, but PRMs make predictions after every step of the reasoning process rather than after every token. Although PRMs have been used in a [variety of papers](https://arxiv.org/abs/2501.07301), collecting training data for PRMs is difficult, as they require granular supervision (i.e., a correctness signal at each step of the reasoning process).

> _“PRMs are reward models trained to output scores at every step in a chain of thought reasoning process. These differ from a standard RM that outputs a score only at an EOS token or a ORM that outputs a score at every token. Process Reward Models require supervision at the end of each reasoning step.”_ - [source](https://rlhfbook.com/c/07-reward-models.html)

## The Role of Reward Models in Post-Training

[

![](https://substackcdn.com/image/fetch/$s_!Dtl3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bde3170-7f57-4f2f-aebb-3af9eb7b6a62_1556x948.png)



](https://substackcdn.com/image/fetch/$s_!Dtl3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bde3170-7f57-4f2f-aebb-3af9eb7b6a62_1556x948.png)

(from [3])

Early post-ChatGPT LLMs were almost always post-trained using the three-step alignment procedure (shown above) proposed by InstructGPT [3]. This procedure is comprised of the following three steps:

1. [Supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)—_a.k.a. instruction finetuning (IFT)_—trains the model using [next-token prediction](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) over examples of good completions.
    
2. A reward model (RM) is trained over a [human preference dataset](https://rlhfbook.com/c/05-preferences.html).
    
3. Reinforcement learning (RL) is used to finetune the LLM by using the output of the RM as a training signal.
    

Collectively, steps two and three in this procedure are called [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)—_we use a reinforcement learning (RL) optimizer to finetune the LLM and incorporate human feedback via preference labels_.

[

![](https://substackcdn.com/image/fetch/$s_!QTAv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2914a412-f20e-40d5-8d44-abdb4d77f1be_1754x746.png)



](https://substackcdn.com/image/fetch/$s_!QTAv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2914a412-f20e-40d5-8d44-abdb4d77f1be_1754x746.png)

(from [4])

Today, the story is a bit more complicated; an example of a more modern post-training pipeline (used for [Tulu-3](https://arxiv.org/abs/2411.15124) [4]) is provided above. Key differences from the original three step alignment procedure include:

- The SFT phase—_although still very common_— is not always used, especially for [recent reasoning models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models); e.g., some variants of [DeepSeek-R1](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models) forgo SFT and apply RL directly to the pretrained model.
    
- RL training is usually performed in several rounds, where fresh data is collected for each round to further improve the LLM’s capabilities.
    
- Several variants of RL (and non-RL-based alternatives) are used—_potentially in tandem_—that may or may not require an RM.
    

Despite the extra complexity, data quality remains the key determinant of successful post-training even today. In this section, we will cover RL training frameworks at a high level, _focusing on the role (if any) of RMs in each of them_.

#### RL Training Strategies for LLMs

For those who are unfamiliar with the high-level setup used for training LLMs with RL, please see the overview below. A basic understanding of RL in the context of LLMs is a necessary prerequisite for this discussion.

[](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

[

![Basics of Reinforcement Learning for LLMs](https://substackcdn.com/image/fetch/$s_!Uo4_!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef02a687-cf34-4407-ad59-1527571e1a65_2410x1354.png)

](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

[

#### Basics of Reinforcement Learning for LLMs

](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

[](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

[](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

2023年9月25日

[

Read full story

](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

**RL for LLMs.** There are two broad categories of reinforcement learning (RL) training that are heavily leveraged by LLMs: RLHF (i.e., steps two and three of the post-training setup that we outlined above) and [reinforcement learning with verifiable rewards (RLVR)](https://cameronrwolfe.substack.com/i/153722335/reinforcement-learning-with-verifiable-rewards). These two variants of RL are depicted below.

[

![](https://substackcdn.com/image/fetch/$s_!CJn6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fd3791-df29-4a92-b185-21f6be4f2ddc_2176x642.png)



](https://substackcdn.com/image/fetch/$s_!CJn6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fd3791-df29-4a92-b185-21f6be4f2ddc_2176x642.png)

(from [4])

From an RL perspective, these techniques are similar. They follow the same high-level training setup and both use RL optimizers based upon [policy gradient algorithms](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)[4](https://cameronrwolfe.substack.com/p/reward-models#footnote-4-166169560) to derive parameter updates. The primary difference between these techniques lies in how we define the reward:

- In RLHF, the reward comes from the RM, which provides a human preference score for each of the completions provided by the LLM.
    
- RLVR uses [deterministic (or verifiable) rewards](https://cameronrwolfe.substack.com/i/153722335/reinforcement-learning-with-verifiable-rewards), where the answer provided by the LLM is marked as either correct or incorrect.
    

Notably, the deterministic—_usually rules-based_—rewards in RLVR eliminate the need for an RM! Usually, rewards are derived by extracting the LLM’s final answer from its generated output and comparing this answer (e.g., via exact string match or some form of fuzzy matching) to a known, ground truth answer. From this comparison, we can determine whether the LLM’s output is correct or not and use this binary signal as a reward for training with RL.

**RLHF vs RLVR.** In [more recent frontier models](https://www.interconnects.ai/p/the-state-of-post-training-2025), both styles of RL play a role in the post-training process. We still perform the three step post-training procedure (SFT → RLHF), which teaches the LLM correct formatting and aligns it to human preferences. However, we now have an additional RLVR step that boosts reasoning capabilities and performance on verifiable tasks; see below.

[

![](https://substackcdn.com/image/fetch/$s_!qyUt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabaeef87-7ac7-4442-babd-1d4741b4255d_2608x1184.png)



](https://substackcdn.com/image/fetch/$s_!qyUt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabaeef87-7ac7-4442-babd-1d4741b4255d_2608x1184.png)

([source](https://docs.google.com/presentation/d/1FL6pzRT3tjCfJ985emS_2YfujCe_iz6dsyRcDIUFPqs/edit?usp=sharing))

More generally, the amount of compute being invested into RL finetuning—_and RLVR in particular_—is also rapidly increasing. This change is motivated by recent results on reasoning models that show clear [scaling laws](https://cameronrwolfe.substack.com/p/llm-scaling-laws) of model performance with respect to the amount of compute used for RL training; see below.

[

![](https://substackcdn.com/image/fetch/$s_!JEX5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a4cd97-6c90-456b-9b51-65eaaa4fe677_608x636.png)



](https://substackcdn.com/image/fetch/$s_!JEX5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a4cd97-6c90-456b-9b51-65eaaa4fe677_608x636.png)

(from [5])

> _“RLHF is a complex and often unstable procedure… we introduce a new parameterization of the reward model in RLHF that [allows] us to solve the standard RLHF problem with only a simple classification loss.”_ - from [6]

**Direct alignment.** RLVR is not the only way to avoid using an RM. In fact, we can still align a model to human preferences—_similarly to RLHF_—while foregoing the RM completely. Such techniques are referred to as [direct alignment algorithms](https://rlhfbook.com/c/12-direct-alignment.html), and the most widely-used algorithm in this class is direct preference optimization (DPO) [6]. Not only do direct alignment algorithms like DPO forego the RM while optimizing the same training objective as RLHF, but they avoid RL training altogether. A comparison between RLHF and DPO is provided below.

[

![](https://substackcdn.com/image/fetch/$s_!vj3B!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cae8395-d481-49f9-b630-f5120b9abe7e_1642x576.png)



](https://substackcdn.com/image/fetch/$s_!vj3B!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cae8395-d481-49f9-b630-f5120b9abe7e_1642x576.png)

(from [6])

The loss function used for training in DPO is presented below. As we can see, this loss function looks very similar to the loss function used by an RM. However, we are no longer predicting the reward with an RM. Instead, we directly use our policy to estimate a reward implicitly by using the probabilities of chosen and rejected completions assigned by the current policy and a reference policy. Intuitively, this loss is minimized when the log-ratio of the chosen completion is larger than that of the rejected completion. _DPO trains the current policy to assign higher (implicit) rewards to chosen responses relative to rejected responses._

[

![](https://substackcdn.com/image/fetch/$s_!yQz2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7107abbb-358e-48d4-a200-64ca6b5d1d72_2050x1092.png)



](https://substackcdn.com/image/fetch/$s_!yQz2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7107abbb-358e-48d4-a200-64ca6b5d1d72_2050x1092.png)

DPO training loss (from [6])

DPO does not require the creation of an intermediate RM. However, the loss function is still derived from the Bradley-Terry model, and we are still learning an RM. The key distinction here is that the RM is learned implicitly rather than explicitly; hence the title of the DPO paper [6] _“Your Language Model is Secretly a Reward Model”._ We can directly obtain this implicit reward estimate from a DPO model similarly to an RM. For a full derivation and analysis of DPO; see [here](https://rlhfbook.com/c/12-direct-alignment.html).

#### Why are RMs useful?

[

![](https://substackcdn.com/image/fetch/$s_!qAWR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50c593a0-6b23-4033-806e-0407afaebc6c_1552x636.png)



](https://substackcdn.com/image/fetch/$s_!qAWR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50c593a0-6b23-4033-806e-0407afaebc6c_1552x636.png)

Orchestrating reference, reward, value and policy models in RL ([source](https://arxiv.org/abs/2409.19256v2))

Without a doubt, using an RM adds extra complexity to the LLM training process. First, we need to train a separate model over a large preference dataset, which already introduces added costs and complexity. From here, this model is used in an online fashion during RL training—_the RM scores completions generated by the current policy during training_. Given that the RM is also an LLM, this means that we have to separately host and run inference for a separate LLM during training, which can be difficult to efficiently orchestrate; see above.

> _“We find that the neural RMs may suffer from reward hacking in the large-scale reinforcement learning process. Retraining the reward model needs additional training resources and it complicates the whole training pipeline.”_ - from [7]

**Reward hacking.** Going further, RMs are subject to [reward hacking](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/). The RM may spuriously assign high rewards to low quality completions or—_more generally_—be exploited in a way that allows the policy to receive high rewards without actually solving the desired task. Interestingly, reward hacking is a key limitation that prevents scaling up training with RLHF—_our policy will eventually find an exploit for the RM if we continue to train it for long enough_. In contrast, verifiable rewards are more difficult (though [not impossible](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/#reward-hacking-examples-in-llm-tasks)) to hack, allowing reasoning models to be trained more extensively (i.e., for more iterations) when using RLVR.

**Should we avoid RMs?** Given the added costs and complexity of RMs, we might wonder: _Should we just avoid RMs altogether?_ There is no definitive answer to this question. Impressive results have been achieved via RLVR, and we can still align models to human preferences with techniques like DPO that avoid an RM. [Many works](https://www.interconnects.ai/p/the-dpo-debate) have argued whether there is (or is not) a performance gap between RLHF and DPO with differing results. Whether DPO is an effective RM-free preference tuning alternative is dependent on the use case, but the fact that there is a gap in performance between these techniques is generally accepted to be true.

> _“The prevalence of RLHF stems from its efficacy at circumventing one of the greatest difficulties in integrating human values and preferences into language models: specifying an explicit reward”_ - from [1]

**The utility of RMs.** Despite these findings, we should not lose sight of the fact that RMs are an incredibly important and powerful concept. One of the most difficult tasks in any form of RL training is specifying a reward. For LLMs, this task is especially difficult—_how do we explicitly define what constitutes a “good” response from an LLM?_ Unfortunately, there is no single property or quality that can be used. The scope of valid model responses is nearly infinite.

With RMs, we circumvent the problem of specifying an explicit reward by distilling this process into a simpler task of asking humans to provide preference feedback (i.e., choosing among pairs of model responses); see below.

[

![](https://substackcdn.com/image/fetch/$s_!JBCh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9510abc4-232c-446b-a0b0-cf949efd9045_2046x1540.png)



](https://substackcdn.com/image/fetch/$s_!JBCh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9510abc4-232c-446b-a0b0-cf949efd9045_2046x1540.png)

Interface for collecting human preference data (from [8])

Choosing the better model in a pair is a much simpler task compared to manually writing or evaluating individual model responses—_the human just has to provide a binary preference_. We can train an RM over this preference feedback, allowing us to derive a reward for RL training without ever making an explicit specification of the reward. Such an approach provides us with a flexible and effective approach for training LLMs with generic human feedback, which is transformational.

[

![](https://substackcdn.com/image/fetch/$s_!3gHA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb649fc49-a1df-4b76-9f6f-6408c1838ed9_2484x576.png)



](https://substackcdn.com/image/fetch/$s_!3gHA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb649fc49-a1df-4b76-9f6f-6408c1838ed9_2484x576.png)

Using an RM to perform Best-of-N sampling

**Other use cases for RMs.** Beyond their use in RL training, RMs have a variety of other use cases. For example, RMs are commonly used for _i)_ Best-of-N sampling and inference-time scaling (see above), _ii)_ evaluation, _iii)_ [rejection sampling](https://rlhfbook.com/c/10-rejection-sampling.html), _iv)_ data filtering and much more! Despite these many use cases, we usually evaluate the performance of an RM based upon:

- _Accuracy_: an RM’s ability to correctly identify the chosen response in a pair.
    
- _Downstream performance_: the performance of an LLM that is RL finetuned with a particular RM.
    
- _Inference-time scaling_: the performance boost achieved by using a particular RM in a Best-of-N sampling pipeline.
    

## Reward Models in Practice

Now that we have an understanding of RMs, we will study some recent papers on this topic. Specifically, we will focus on RewardBench [1], which is a benchmark for evaluating the effectiveness of RMs. This benchmark has been used to evaluate hundreds of different RMs across a variety of use cases, allowing us to derive useful takeaways for effectively training and using RMs in practice. Recently, a new version of RewardBench—_called RewardBench 2 [2]_—was also proposed, which modernized and expanded upon these findings.

#### **[RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787) [1]**

[

![](https://substackcdn.com/image/fetch/$s_!Rmwn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6539e2fa-ef3b-404e-bb3a-934b3d51de05_2232x502.png)



](https://substackcdn.com/image/fetch/$s_!Rmwn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6539e2fa-ef3b-404e-bb3a-934b3d51de05_2232x502.png)

(from [1])

Many practical choices are involved in training an RM; e.g., selecting the type of reward model to be used, choosing a policy to initialize the RM, setting the number of training epochs and more. However, most practical details of creating RMs are poorly documented. In [1], authors solve this issue by creating a standard benchmark—_called RewardBench_—for evaluating RMs. By evaluating a wide range of RMs on RewardBench, we can determine the impact of various practical choices on both RM performance and the performance of downstream LLMs trained with a given RM. From this analysis, we emerge with a better grasp of how RMs work and a set of best practices for creating high-quality RMs.

> _“Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models.”_ - from [8]

**What is RewardBench?** RewardBench is a framework and dataset for evaluating RMs. This open (i.e., [data](https://huggingface.co/datasets/allenai/reward-bench) and [evaluation code](https://github.com/allenai/reward-bench) are released) benchmark is used in [1] to chart the landscape of publicly-available RMs; see [here](https://huggingface.co/spaces/allenai/reward-bench) for a leaderboard. By providing structured evaluations of RMs across many capabilities, RewardBench helps us to better understand how and why certain types of RMs work.

**Quantifying RM performance.** RewardBench is comprised of prompts paired with two responses—_one chosen (preferred) and one rejected_. To evaluate an RM, we can simply test whether the RM is capable of identifying the preferred response. Specifically, this is done by computing the RM’s output for both the chosen and rejected responses, then comparing their scores. The “correct” behavior from an RM would be to assign a higher score to the chosen response; see below. _We can also evaluate DPO models as an RM in this way using their implicit reward estimate._

[

![](https://substackcdn.com/image/fetch/$s_!RDRA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de7117d-4544-4294-b907-8ce711fe597b_1610x704.png)



](https://substackcdn.com/image/fetch/$s_!RDRA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de7117d-4544-4294-b907-8ce711fe597b_1610x704.png)

Scoring technique used by RewardBench (from [1])

This ability to correctly identify the preferred response can be easily captured via an accuracy metric that counts the number of correct RM outputs across a dataset of prompts with chosen and rejected responses. To compare different RMs, we can just compute this accuracy metric over a fixed dataset.

[

![](https://substackcdn.com/image/fetch/$s_!ZOMs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ee8aee-08da-4942-bb7f-4d0d34f83d9d_1652x1328.png)



](https://substackcdn.com/image/fetch/$s_!ZOMs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ee8aee-08da-4942-bb7f-4d0d34f83d9d_1652x1328.png)

(from [1])

**Data composition.** Depending on the application, RMs are expected to capture a wide scope of different capabilities. To provide a comprehensive view of RM performance, RewardBench chooses to measure RM quality in several different domains (summarized in the table above):

1. _Chat_: tests the RM’s ability to distinguish correct chat responses.
    
2. _Chat Hard_: tests the RM’s ability to identify trick questions and subtle differences between responses.
    
3. _Safety_: tests refusals of unsafe prompts and the ability to avoid false refusals.
    
4. _Reasoning_: tests ability to distinguish good coding and reasoning responses.
    
5. _Prior datasets_: existing preference datasets (e.g., [Anthropic’s HH dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf), [Stanford Human Preferences dataset](https://huggingface.co/datasets/stanfordnlp/SHP), and [OpenAI’s learning to summarize dataset](https://huggingface.co/datasets/openai/summarize_from_feedback)) are also included for consistency with prior work.
    

Within each category of RewardBench, models are evaluated in terms of their accuracy. To generate an aggregate score per category, we take a weighted average of examples within that category. By evaluating RMs across several domains, we gain a more granular view of their performance—_certain categories of RMs oftentimes perform well in some domains but not others_.

[

![](https://substackcdn.com/image/fetch/$s_!SJFG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c87f9e9-d002-4b7c-9621-20fc42387b1d_1056x710.png)



](https://substackcdn.com/image/fetch/$s_!SJFG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c87f9e9-d002-4b7c-9621-20fc42387b1d_1056x710.png)

(from [1])

To study the ability of RMs to capture subtle differences in response quality, authors also create difficult preference examples with small differences between chosen and rejected responses; see above for an example. Ideally, the RM should capture these subtle differences and assign credit to the preferable response in a stable manner. To ensure that [length bias](https://arxiv.org/abs/2404.04475) does not skew results, authors ensure that all response pairs within RewardBench are of similar length.

[

![](https://substackcdn.com/image/fetch/$s_!N3hz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a2c6e1-49ff-470c-b567-815aa23fac19_1622x1272.png)



](https://substackcdn.com/image/fetch/$s_!N3hz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a2c6e1-49ff-470c-b567-815aa23fac19_1622x1272.png)

(from [8])

**Analysis of RMs.** The empirical performance the top-20 RMs—_of 50+ total RMs considered in [1]_—on RewardBench is outlined above. These RMs range from 400M to 70B parameters in size and are separated into small, medium, and large groups. We can summarize the key results for these models as follows:

- Performance is generally lower on Chat Hard and Reasoning subsets for all RMs, revealing a potential area of improvement. Only larger RMs perform consistently well on Chat Hard and Reasoning subsets.
    
- Using a more powerful base model for the RM is helpful; e.g., Llama-3-based[5](https://cameronrwolfe.substack.com/p/reward-models#footnote-5-166169560) RMs do well on RewardBench. Even subtle changes to the RM’s base model (e.g., tweaking the training data or strategy) can impact the RM.
    
- Model size benefits performance for LLM-as-a-Judge-style RMs, but classifier-based RMs still perform noticeably better.
    
- The scaling properties of RMs depend on the style of RM (e.g., classifier-based vs. DPO vs. LLM-as-a-Judge) and choice of base model. For example, the table below shows an example where LLaMA-2 DPO models improve in RM performance with scale, while classifier-based Qwen-1.5 RMs do not.
    
- Results on prior evaluation datasets are not consistent with RewardBench, _revealing that results on these benchmarks may fail to comprehensively measure performance_. For example, DPO models—_when evaluated as an RM_—perform well on RewardBench but struggle on legacy benchmarks.
    

> _“Llama 2 shows a clear improvement with scaling across all sections of RewardBench, but Qwen 1.5 shows less monotonic improvement, likely due to out of distribution generalization challenges.”_ - from [1]

[

![](https://substackcdn.com/image/fetch/$s_!-ReQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b65081a-c324-4961-9abe-fc368009ed97_2256x496.png)



](https://substackcdn.com/image/fetch/$s_!-ReQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b65081a-c324-4961-9abe-fc368009ed97_2256x496.png)

(from [1])

#### [Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback](https://arxiv.org/abs/2406.09279) [13]

The next step after learning best practices for RMs is using these ideas to train a better LLM. In [13], authors apply lessons learned from RewardBench to deeply study RL finetuning. In particular, this paper focuses on making a comparison between the performance of DPO and PPO. We will not focus on the comparison between these techniques in this overview. However, this analysis also contains numerous practical lessons for creating RMs that maximize the downstream performance of the LLMs that they are used to train.

[

![](https://substackcdn.com/image/fetch/$s_!EQMc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad6659b-69df-41e3-b09b-80d4ab854cb8_1302x828.png)



](https://substackcdn.com/image/fetch/$s_!EQMc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad6659b-69df-41e3-b09b-80d4ab854cb8_1302x828.png)

(from [13])

**Data quality.** The key experimental results presented in [13] are summarized above. The experiments begin by training a DPO model over the [HH RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset from Anthropic, which is known to be an older and noisier dataset. This data boosts model performance, but a much bigger boost is seen from training on [UltraFeedback](https://arxiv.org/abs/2310.01377)—_a modern, high-quality preference dataset_. When we switch to training with [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) (i.e., meaning that an RM is used) over the same data, we see a clear performance improvement, indicating that there is a downstream benefit in performance from using PPO with an explicit RM. However, we should notice that this benefit is much smaller relative to the impact of using better data!

**Larger RMs.** Given the clear benefit of training with PPO, we might wonder if the LLM would also benefit from using a larger RM. This makes intuitive sense given [LLM scaling laws](https://cameronrwolfe.substack.com/p/llm-scaling-laws), but observations in [13] are not this straightforward.

When scaling the RM from 13B to 70B parameters, downstream LLM performance remains stagnant, even for models that are initialized from the same SFT checkpoint. The only observable performance benefit occurs in the reasoning domain, indicating that the benefit of larger RMs is only clear in scenarios where the superior capabilities of a bigger model are useful or necessary. In other words, we need harder data for these larger RMs to be useful!

> _“If we’re using a bigger reward model, we need to have data that is actually challenging the reward model.”_ - [source](https://www.youtube.com/watch?v=rDF7eFPeVto)

**Better data + bigger RM.** Combining the lessons outlined above, authors in [13] collect a larger set of more difficult prompts—_emphasizing coding and reasoning tasks_—for RM training and test again whether larger RMs are beneficial. From these experiments, we see clear signals of improving RM quality. For example, these larger and better RMs yield a noticeable boost in performance when used for Best-of-N sampling as shown below. However, this improvement is much less clear when we look at both RewardBench and downstream performance.

[

![](https://substackcdn.com/image/fetch/$s_!ZfRv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96a83a07-9d28-406b-b1eb-16b99ce61594_920x398.png)



](https://substackcdn.com/image/fetch/$s_!ZfRv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96a83a07-9d28-406b-b1eb-16b99ce61594_920x398.png)

(from [13])

Put simply, using a bigger and better RM does not directly imply that our LLM will be better when this RM is used for RL finetuning. In fact, we even see a performance _regression_ in some domains when using larger RMs in [13]. Such findings make the evaluation of RMs very complicated—_just measuring the accuracy of an RM does not help us to understand how useful it will be_.

#### **[RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937) [2]**

[

![](https://substackcdn.com/image/fetch/$s_!QC2H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f2e98d3-b5b4-43cb-9824-63dd79b78244_1938x820.png)



](https://substackcdn.com/image/fetch/$s_!QC2H!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f2e98d3-b5b4-43cb-9824-63dd79b78244_1938x820.png)

(from [2])

The recently-proposed RewardBench 2 [2][6](https://cameronrwolfe.substack.com/p/reward-models#footnote-6-166169560) aims to make improvements over the initial RewardBench so that evaluating RMs is more useful and informative. This benchmark contains new data that covers a wider scope of skills that LLMs may possess, and RMs score ~20 points lower on this benchmark on average—_it is a much more challenging benchmark._ Despite still using an accuracy-based approach for evaluating RMs, RewardBench 2 has a clear correlation with downstream RM usage (e.g., for Best-of-N sampling) and provides useful lessons for determining whether a given RM will be effective when used for RL finetuning.

**Measuring RM performance.** Instead of measuring the accuracy of the RM in differentiating between a chosen and rejected response, RewardBench 2 has four possible responses for each prompt—_one chosen and three rejected_. Among these responses, the RM must score the chosen response higher than all rejected responses; see below. This best-of-4 approach, which is still accuracy-based like the initial RewardBench, is more challenging and brings the performance of even strong RMs closer to that of the random baseline (i.e., 25% accuracy).

[

![](https://substackcdn.com/image/fetch/$s_!MobT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb44b1dcf-1e55-4544-98b9-1c075aa60486_951x367.png)



](https://substackcdn.com/image/fetch/$s_!MobT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb44b1dcf-1e55-4544-98b9-1c075aa60486_951x367.png)

(from [2])

Additionally, RewardBench 2 goes beyond accuracy-based evaluation by measuring LLM performance when:

1. A certain RM is used for Best-of-N sampling.
    
2. A certain RM is used for RL training.
    

As a result of this extended evaluation, we can both understand the quality of an RM, as well as observe the impact of this quality on downstream performance when used for inference-time scaling and RL training. Compared to alternative benchmarks, this evaluation process is quite comprehensive; see below.

[

![](https://substackcdn.com/image/fetch/$s_!hGEQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73c28087-a6c7-49d6-9682-39b53980caad_1818x948.png)



](https://substackcdn.com/image/fetch/$s_!hGEQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73c28087-a6c7-49d6-9682-39b53980caad_1818x948.png)

(from [2])

**Data composition.** RewardBench 2 focuses upon six different domains or capabilities when evaluating an RM. Three of these domains—_focus, math and safety_—overlap with existing benchmarks, while the three others—_factuality, precise instruction following and ties (i.e., testing the RM’s ability to handle equally-valid answers)_—present completely new challenges for RMs.

> _“The benchmark was created with a majority of previously unused human prompts from the WildChat pipeline with extensive manual, programmatic, and LM-based filtering techniques.”_ - from [2]

RewardBench 2 uses unseen and human-written prompts, largely sampled from [WildChat](https://arxiv.org/abs/2405.01470)—_a dataset of ChatGPT logs collected from real-world users_. Using unseen prompts is important due to the risk of data contamination. If our data is contaminated[7](https://cameronrwolfe.substack.com/p/reward-models#footnote-7-166169560), the RM benchmark will be highly correlated with downstream performance due to the same data being used for both evaluations. To ensure correlation is legitimate, we must decontaminate the data and avoid leakage.

To accomplish this goal, authors in [2] adopt a multi-stage data curation pipeline that involves:

- Sourcing unseen, human-written prompts from WildChat.
    
- Identifying the domain and quality of each prompt using manual inspection and classifiers; e.g., [QuRater](https://huggingface.co/princeton-nlp/QuRater-1.3B) and [domain classifiers](https://huggingface.co/valpy/prompt-classification).
    
- Performing extensive [data decontamination](https://github.com/allenai/open-instruct/tree/main/decontamination) to ensure virtually zero overlap with downstream evaluation datasets.
    
- Manually selecting the best prompts from those remaining.
    
- Sampling completions for each of the prompts from diverse sources that accurately reflect the capabilities of recent LLMs.
    
- Filtering completions based on correctness using a variety of signals; e.g., [LLM-as-a-Judge](https://cameronrwolfe.substack.com/p/llm-as-a-judge), automatic verifiers, [majority voting](https://cameronrwolfe.substack.com/i/120285767/solving-tough-problems-with-llms) and more.
    

Details of the final dataset created for RewardBench 2 and how each component of this dataset is created are summarized below. To derive the final benchmark score, we take an unweighted average of an RM’s performance[8](https://cameronrwolfe.substack.com/p/reward-models#footnote-8-166169560) in each domain.

[

![](https://substackcdn.com/image/fetch/$s_!R9d7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7137cb51-ed3e-4c08-9ec2-d714c9c39c87_1812x812.png)



](https://substackcdn.com/image/fetch/$s_!R9d7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7137cb51-ed3e-4c08-9ec2-d714c9c39c87_1812x812.png)

(from [2])

**RewardBench 2 performance.** RewardBench 2 is used to evaluate >100 different RMs in [2]. The performance of the top-20 models is provided below. In addition to scores being lower on this new benchmark, we see that foundation model-based (e.g., Gemini and Claude) LLM-as-a-Judge models perform very well. This observation—_though in line with the improving capabilities of foundation models_—is in stark contrast to observations on the initial RewardBench, where LLM-as-a-Judge models performed consistently worse than classifier-based RMs.

[

![](https://substackcdn.com/image/fetch/$s_!tLkg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf904de-02a0-4f50-bb61-2ec5e8995daa_1034x916.png)



](https://substackcdn.com/image/fetch/$s_!tLkg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf904de-02a0-4f50-bb61-2ec5e8995daa_1034x916.png)

(from [2])

Authors in [2] also train a variety of their own RMs using various base models and hyperparameter settings, finding that the base model used to initialize the RM clearly impacts the RM—_skills present in the base model carry over to the RM_. Factors like the model family, training data mixture, style of training used or the stage of post-training from which the RM is initialized clearly influence the performance of the RM across domains. Additionally, authors in [2] find that training the RM for two epochs—_instead of the usual one epoch_—can be beneficial.

**Downstream performance.** Finally, the analysis of RMs in [2] is extended to consider inference-time scaling and RL training. Unsurprisingly, performance on RewardBench 2 is highly correlated with Best-of-N sampling—_accurate reward models are capable of identifying the best completions within a candidate set_.

Although correlation of RewardBench 2 with downstream performance is less clear, authors in [2] do identify one key factor that influences the success of an RM when used for RL training: _whether the RM and the policy being trained are derived from the same model lineage_. In other words, we see the following:

- High scores on RM benchmarks are necessary (but not sufficient) for high downstream performance with RL training—_downstream performance quickly saturates with improving RM quality_[9](https://cameronrwolfe.substack.com/p/reward-models#footnote-9-166169560)_._
    
- A misalignment between the policy model for RL training and the RM’s base model—_or between the distribution of prompts used for RL training versus training the RM_—causes a huge drop in downstream performance.
    

As a result of these findings, authors in [2] conclude their work by leaving us with a final recommendation for training RMs summarized in the below quote.

_“These findings warrant caution when using reward model evaluation benchmarks: While the benchmark can be used as a guide for picking a reward model off-the-shelf to be used in some settings like best-of-N sampling… for policy-gradient algorithms like PPO, the results of the benchmark should be considered in the context of one’s training setup. Instead of simply taking the top model on RewardBench 2, we show that one should take the recipe for that model and integrate it into their specific workflow rather than the checkpoint itself.”_ - from [2]

## Conclusion

Reward models are among the most powerful and flexible tools in LLM research. As we have learned, various styles of RMs exist beyond the standard classifier-based RM, and creating an effective RM is the result of countless practical considerations. Additionally, the correct choices for creating an RM are application-dependent; e.g., Best-of-N sampling versus RL finetuning. In this overview, we have built a foundational understanding of RMs, ranging from basic statistical models like Bradley-Terry to training large-scale LLM-based RMs. As more focus is dedicated to large-scale RL training for LLMs, research on RMs will rapidly advance and play an increasingly pivotal role in AI.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Senior Research Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. The newsletter will always be free and open to read. If you like the newsletter, please subscribe, consider a paid subscription, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Lambert, Nathan, et al. "Rewardbench: Evaluating reward models for language modeling." _arXiv preprint arXiv:2403.13787_ (2024).

[2] Malik, Saumya, et al. "RewardBench 2: Advancing Reward Model Evaluation." _arXiv preprint arXiv:2506.01937_ (2025).

[3] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[4] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." _arXiv preprint arXiv:2411.15124_ (2024).

[5] OpenAI et al. “Learning to Reason with LLMs.” _https://openai.com/index/learning-to-reason-with-llms/_ (2024).

[6] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." _Advances in Neural Information Processing Systems_ 36 (2023): 53728-53741.

[7] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." _arXiv preprint arXiv:2501.12948_ (2025).

[8] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).

[9] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." _Advances in Neural Information Processing Systems_ 36 (2023): 46595-46623.

[10] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[11] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." _Advances in Neural Information Processing Systems_ 36 (2023): 46595-46623.

[12] Cobbe, Karl, et al. "Training verifiers to solve math word problems." _arXiv preprint arXiv:2110.14168_ (2021).

[13] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." _Advances in neural information processing systems_ 37 (2024): 36602-36633.

[14] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in neural information processing systems_ 33 (2020): 3008-3021.

[1](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-1-166169560)

Sometimes, we may have more than two candidate completions per prompt. In this case, preferences are captured by ranking completions in terms of their preference. However, binary preference data is more commonly used in recent research.

[2](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-2-166169560)

Here, we use the term policy to refer to an LLM that we are currently training. This is standard terminology used within reinforcement learning; see [here](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning).

[3](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-3-166169560)

In practice, these sequences will be both the prompt and the completion for all chosen and rejected sequences. Here, we just have flat textual sequences with no clear prompt or completion structure for simplicity.

[4](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-4-166169560)

However, there are many variants of policy gradient algorithms used for training LLMs (e.g., [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo), [REINFORCE](https://arxiv.org/abs/2402.14740), [GRPO](https://arxiv.org/abs/2402.03300) and many more), each of which have their benefits.

[5](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-5-166169560)

At the time of writing, Llama-3 was the best open-source model that was available.

[6](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-6-166169560)

This benchmarks comes with [data](https://huggingface.co/datasets/allenai/reward-bench-2), a [leaderboard](https://huggingface.co/spaces/allenai/reward-bench), and an extensive technical report!

[7](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-7-166169560)

Data contamination refers to the idea of data being present in our training set that will later be used to evaluate the same model.

[8](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-8-166169560)

Performance is measured in terms of accuracy for all domains except ties, where we check for the correct margin between correct and incorrect examples.

[9](https://cameronrwolfe.substack.com/p/reward-models#footnote-anchor-9-166169560)

This is in line with results in [13], where we see that RMs of various strengths all perform relatively well when used for RL training.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Nader Essam's avatar](https://substackcdn.com/image/fetch/$s_!vCNc!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7ab55db-009a-471e-ab84-1043141896eb_674x678.jpeg)



](https://substack.com/profile/208322586-nader-essam)

[

![Sarama's avatar](https://substackcdn.com/image/fetch/$s_!CTCq!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff000d7b9-32dd-455f-9d1e-b9a47b8bde49_144x144.png)



](https://substack.com/profile/16325550-sarama)

[

![Ash's avatar](https://substackcdn.com/image/fetch/$s_!27dJ!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F642a63bc-398b-413f-a8f0-0e98c852c7b2_1080x1920.jpeg)



](https://substack.com/profile/358345086-ash)

[

![Emmanuel Maminta's avatar](https://substackcdn.com/image/fetch/$s_!VpnG!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4f1db49-fcdd-4643-ac7a-9cfa347cdb71_144x144.png)



](https://substack.com/profile/174228865-emmanuel-maminta)

[

![Lorenzo Bradanini's avatar](https://substackcdn.com/image/fetch/$s_!ACM6!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18342bf1-31cb-404a-b9e1-998a38d299bf_1200x1200.jpeg)



](https://substack.com/profile/201922174-lorenzo-bradanini)

40 Likes∙

[6 Restacks](https://substack.com/note/p-166169560/restacks?utm_source=substack&utm_content=facepile-restacks)

40

[

6

](https://cameronrwolfe.substack.com/p/reward-models/comments)

6

Share

#### Discussion about this post

CommentsRestacks

![User's avatar](https://substackcdn.com/image/fetch/$s_!TnFC!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)

[

![Devansh's avatar](https://substackcdn.com/image/fetch/$s_!0X66!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48081c70-8afa-41e3-a44e-b0f917bc7577_1200x1600.jpeg)



](https://substack.com/profile/8101724-devansh?utm_source=comment)

[Devansh](https://substack.com/profile/8101724-devansh?utm_source=substack-feed-item)

[14h](https://cameronrwolfe.substack.com/p/reward-models/comment/130855147 "2025年7月1日 05:42")

Liked by Cameron R. Wolfe, Ph.D.

A little touch but seeing you do thjis--

chosen_seqs = [

"I love deep (learning) focus!",

"Cameron is great at explaining stuff",

"AGI is coming very soon...",

]

# Rejected prompt-response sequences

rejected_seqs = [

"I'm not a fan of deep (learning) focus",

"Cameron doesn't know what he's talking about",

"AGI is fake and LLMs can't reason!",

]

made me so happy. t's very cool to see you inject more personality here

Like (2)

Reply

Share

[3 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/reward-models/comment/130855147)

[

![Dr. Ashish Bamania's avatar](https://substackcdn.com/image/fetch/$s_!1rS7!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff41b7f65-55d7-4099-969a-931c2ddd2f5f_612x612.png)



](https://substack.com/profile/155457308-dr-ashish-bamania?utm_source=comment)

[Dr. Ashish Bamania](https://substack.com/profile/155457308-dr-ashish-bamania?utm_source=substack-feed-item)

[1d](https://cameronrwolfe.substack.com/p/reward-models/comment/130672964 "2025年6月30日 18:50")

Liked by Cameron R. Wolfe, Ph.D.

Always love to read your content! Great work!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/reward-models/comment/130672964)

[4 more comments...](https://cameronrwolfe.substack.com/p/reward-models/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

129

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/$s_!-1vf!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

227

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/$s_!mk9r!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

[Understanding how SFT works from the idea to a working implementation...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Sep 11, 2023 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

59

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

![](https://substackcdn.com/image/fetch/$s_!2Tj5!,w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68686a01-2b31-4694-8c04-a562ffd725ad_2210x1244.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture