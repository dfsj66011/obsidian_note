
大型语言模型（LLMs）的成功源于我们能够（使用语言建模目标）在海量文本语料库上预训练仅解码器的Transformer模型。鉴于我们预训练了足够大的模型，LLMs是非常强大的小样本学习者。换句话说，这意味着我们只需制定一个文本提示（可能包含几个正确输出的示例），并让LLM生成正确答案，就可以解决各种不同的问题（例如，翻译、句子分类、摘要等）。

尽管大型语言模型（LLMs）功能强大，但这些模型始终难以解决一些问题。特别是推理问题（例如算术推理或常识推理）极其困难。最初为解决这一问题所做的尝试是，在各种推理问题的解决方案和解释的监督数据集上对大型语言模型进行微调，并采用特定任务的验证模块[3, 4]。然而，近期的研究发现，可以利用少样本学习来更轻松地解决这一问题 。

> “本文的目标是赋予语言模型生成思维链的能力——一系列连贯的中间推理步骤，从而得出问题的最终答案。” —— 引自[1]

特别是，思维链（CoT）提示[1]是一种最近提出的技术，它通过少样本学习来提高大型语言模型（LLM）在基于推理的任务上的性能。与标准提示技术类似，思维链提示将几个推理问题的示例解决方案插入到大型语言模型的提示中。然后，每个示例都与一个思维链配对，即解决问题的一系列中间推理步骤。然后，大型语言模型（以少样本的方式）学习在解决推理问题时生成类似的思维链。这种方法使用最少的数据（即仅几个用于提示的示例），不需要针对特定任务进行微调，并显著提高了大型语言模型在基于推理的基准测试中的性能，尤其是对于较大的模型。

### 核心概念

我们不会在本节中介绍大型语言模型（LLMs）的基础知识。相反，我们将着重加深对大型语言模型的提示和少样本学习的理解，并探讨如何利用这些技术来解决这些模型的一个核心局限性：它们无法解决推理任务。


### Prompting 和 Few-Shot Learning

在像GPT和GPT - 2这样的语言模型被提出之后，我们知道通过自监督的下一词预测（或语言建模）目标进行预训练是非常强大的。然而，如何适配这些通用的基础模型来解决特定的下游任务并不那么清晰。例如，GPT通过在下游任务上微调模型，而GPT - 2则以零样本的方式解决问题；具体见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46457a6f-577b-43ed-bd40-c5ad188a34f4_1818x1130.png)

在GPT - 3[2]被提出之后，我们发现规模足够大的大型语言模型（LLMs）能够很好地进行少样本学习。通过语言建模目标进行预训练后，GPT-3（一个具有1750亿参数的大型语言模型）被发现无需任何微调就能准确解决各种不同的语言任务。我们可以用提示方法来代替微调。  
更具体地说，提示利用语言模型的文本到文本结构，提供如下输入：

- “将这个句子翻译成英语：`<sentence> =>`”
- “总结以下文档： `<document> =>`”

这些解决任务的“提示”使得语言模型能够进行零样本（即，未见过正确输出的示例；见上文）或少样本（即，在提示中插入少量正确输出的示例；见下文）推理。语言模型给出的最恰当的输出应该能够解决任务（例如，总结文档或完成推理任务），这意味着我们可以通过准确的下一标记预测来解决各种问题！
![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5762db-6c25-45fb-8854-d64c760989f6_1342x1216.png)

我们可以通过提示来完成很多工作。事实上，最近专门创建了一个提示工程领域，以研究如何优化提示的措辞或结构以提高大型语言模型（LLM）的性能。但是，在这个不断发展的领域中，敏感性是一个需要重点考虑的因素。输入提示稍有扰动，大型语言模型的性能就可能发生巨大变化（例如，在SST - 2数据集上，对少量示例进行排列会使GPT - 3的准确率从93.4%降至54.3% [13]）。因此，在我们对提示方法的研究中，我们的目标是找到既i) 性能良好又ii) 不受敏感性影响的技术。

### 我们能否通过规模化来解决推理问题？

如上所述，大型语言模型（LLM）的少样本学习性能会随着规模扩大而提升，但大型模型并非我们所需要的一切。强大的大型语言模型需要将大型模型与大规模预训练数据集相结合[14]。考虑到这一点，我们可能会问自己：大型语言模型在基于推理的数据集上的性能如何？随着规模扩大，大型语言模型在推理方面是否会变得更好？

> “仅仅扩大模型规模对于在算术、常识和符号推理等具有挑战性的任务上实现高性能还不够” —— 引自[1]

有趣的是，我们发现使用更大的模型和预训练数据集并不能提高大型语言模型（LLM）的推理能力（例如，可参考对Gopher[15]的分析）。事实上，这些模型因无法解决基本推理任务而饱受诟病。因此，许多研究人员声称，大型语言模型只是在“反刍”训练数据，而非进行任何复杂的推理或分析。无论如何，本综述将重点关注旨在解决这一问题并使大型语言模型能更轻松地解决基本推理任务的提示技术。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725341ae-60c3-4074-946f-eb16ece5ab87_1622x808.png)

**先前的方法**  在进一步了解我们如何帮助大型语言模型（LLMs）解决推理问题之前，了解这一领域先前的方法是有用的。针对算术、常识和符号推理任务的基础技术会进行特定任务的微调，也就是说，模型会在每个推理问题的监督示例上进行训练。更进一步说，最佳方法会训练一个补充性的“验证”模块，该模块能够判断大型语言模型在推理任务上的输出是否正确[4]。在测试时，这个验证器可以在为一个问题生成多个答案后推导出最有可能的输出；见上文。

尽管这些技术在某些情况下可能效果相对不错，但由于以下几个原因，它们存在一定局限性：

1. 需要进行特定任务的微调。
2. 必须针对每个任务对模型架构进行调整（即通过验证模型进行调整）。
3. 必须收集大量的有监督数据 。

考虑到这些限制因素，使用仅提示的方法（例如思维链提示）来解决推理任务会简单得多。我们可以避免微调，保持相同的模型架构，收集更少的数据，并使用单个预训练模型检查点解决许多任务。

### CoT Prompting

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

虽然我们可能总体上理解提示（prompting）的概念，但什么是思维链（CoT）提示呢？思维链提示只是指一种特定的提示技术，它在大型语言模型（LLM）的提示中插入一个思维链（即一系列中间推理步骤）；见上文。对于足够大的模型（参数超过1000亿），这种方法显著提高了大型语言模型在算术、常识和符号推理任务上的复杂推理能力。

思维链提示从何而来？在提出思维链提示之前，我们已经知道少样本学习对大型语言模型（LLMs）非常有效；见下文。我们不是微调大型语言模型来执行任务，而是在生成最终答案之前，仅用少量正确输出的示例“提示”一个通用模型。这种方法对一系列任务都非常成功。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F516ed49c-c4a2-4087-a1d8-ffce624f4f9f_2518x842.png)

此外，我们从相关工作中了解到，生成解释如何得出最终答案的自然语言理由对于算术推理任务是有益的。我们可以训练或微调模型来生成这些理由[3, 4]，但这需要为不同的推理任务创建一个高质量的理 由数据集，这是昂贵且耗时的！


> “仅采用提示方法很重要，因为它不需要大量的训练数据集，并且单个模型可以在不失一般性的情况下执行许多任务。” - 引自[1]

思维链提示结合了少样本提示的优势和生成自然语言推理的好处。我们无需进行额外的训练或微调，只需在提示中插入一些推理示例（即思维链），就能让大型语言模型通过少样本学习生成类似的推理。

### 思维链提示是如何工作的？

当我们像人类一样解决推理任务时，通常会将问题分解成更小的任务。例如，在计算餐厅给多少小费时，我通常会执行以下步骤：

- 取账单的总金额：56.00美元
- 计算总金额的10%：5.60美元
- 将该值乘以2（得出20%的小费）：11.20美元

尽管这个例子很简单，但这个思路可以延伸到我们人类所解决的各种心智推理任务中。我们通过生成思维链（在[1]中定义为“一系列连贯的中间推理步骤，最终得出问题的最终答案”）来解决这类任务。简单来说，思维链提示使大型语言模型具备了生成类似思维链的能力。

> “我们探究语言模型针对推理任务执行少样本提示的能力，所给提示由三元组构成：[输入，思维链，输出]。” —— 引自[1]

下面展示了结合推理任务和针对几个不同问题的思维链的解决方案示例。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d770953-ca52-4025-9945-683b5195c67d_1604x1394.png)

学习思维链。要教会大型语言模型生成解决问题的推理过程，我们只需将此类推理过程的示例插入到它们的提示中。然后，大型语言模型可以利用其小样本学习能力，在解决任何推理问题时生成类似的思维链。如下所示，提示通常包含几个思维链示例。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff291e2c5-0a45-4dc5-86f1-34df1552c8d0_1670x692.png)

文献[1]中的作者发现，这种提示方法使大型语言模型（LLMs）在解决问题时生成相似的思维链，这有助于推理能力，并具有几个显著的好处：

- 可解释性：大型语言模型生成的思维链可用于更好地理解模型的最终答案。
- 适用性：思维链提示可用于任何可以通过语言由人类解决的任务。
- 提示：不需要对任何大型语言模型进行训练或微调。我们只需在提示中插入几个思维链示例！  
    此外，大型语言模型甚至可以通过生成步骤更多的思维链，为复杂的推理问题分配更多的计算资源。这模仿了我们人类通常会做的事情！

此外，大型语言模型（LLMs）甚至可以通过生成步骤更多的思维链，为复杂的推理问题分配更多的计算资源。这与我们人类通常的做法很相似！

### CoT Prompting is Massively Beneficial

为了评估其对大型语言模型（LLMs）解决推理问题能力的影响，在算术、常识和符号推理基准上对思维链（CoT）提示进行了测试。使用几种不同的预训练大型语言模型进行评估，包括GPT - 3[2]、LaMDA[5]、PaLM[6]、Codex[7]和UL2[8]。作为基线，在[1]中作者使用了GPT - 3提出的标准少样本提示。所有模型在评估期间都使用贪婪解码，但通过对多个样本进行多数投票可以获得更好的结果[9]。

算术推理。算术推理任务包括数学应用题。这类问题对人类来说很容易解决，但已知大型语言模型（LLMs）在处理此类问题时会遇到困难。下面提供了[1]中使用的算术推理数据集的概述。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6487d8fa-0fe8-46d0-9a75-e441c912c9cc_1110x852.png)

对于思维链（CoT）提示，手动编写了一组八个少样本示例（无需进行大量的提示工程），并应用于除AQuA之外的所有数据集（AQuA具有多项选择结构）。下面展示了使用思维链提示对多个大型语言模型（LLMs）在算术推理数据集上进行实验的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F303b4687-7860-4d71-864d-aec0b090c6f2_1846x1210.png)

通过这些实验，我们发现了思维链提示的几个显著特性。首先，思维链提示对较大的大型语言模型（即参数超过1000亿）效果要好得多。研究发现，较小的模型会产生不合逻辑的思维链，从而降低性能，不如标准提示。此外，更复杂的问题（例如GSM8K）从思维链提示中受益更多。与先前的最先进方法（进行特定任务的微调）相比，使用GPT-3和PaLM-540B的思维链提示在所有情况下都取得了相当或更好的性能。

当我们对思维链提示生成的正确答案和错误答案进行定性分析时，我们了解到以下情况：

- 大多数正确答案是逻辑思维链的结果，只有少数情况是碰巧从不正确的思维链预测出正确答案。
- 46% 的错误答案几乎是正确的，这意味着它们包含带有小错误的思维链。
- 56% 的错误答案是由于在理解或推理方面存在重大问题的思维链导致的。

[1] 中的作者还分析了思维链提示对不同结构（例如，打乱少样本示例）的鲁棒性，并对思维链提示的各个方面进行了消融实验，发现思维链对模型性能有一致且独特的提升。有趣的是，思维链提示对小的提示扰动并不十分敏感。

常识推理。常识推理问题假定具备一般背景知识，并要求对物理和人类互动进行推理。采用与算术推理实验类似的设置（除了一些需要手动策划少样本示例的数据集外），作者对各种预训练的大型语言模型（LLMs）在常识推理任务上进行了评估，得出如下图所示的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f685d54-6a15-49fe-b6c3-46440f0f6999_1076x470.png)

简而言之，思维链（CoT）提示在常识推理问题上也被发现能带来巨大的好处。同样，我们看到更大的模型从思维链提示中受益更多。然而，标准提示和思维链提示的性能都随着模型规模的增长而提高，其中思维链提示往往能取得稍好一些的性能。

符号推理。[1]中的作者还对思维链提示在符号推理任务上进行了评估，例如：

- 最后字母串联：要求模型对序列中每个单词的最后字母进行串联并输出。
- 硬币翻转：要求模型确定在一系列硬币翻转后硬币是否仍然正面朝上。

进一步地，考虑了领域内和领域外的符号推理测试，其中领域外的示例是指那些需要比训练期间或在少样本提示中看到的示例更多的推理步骤（例如，字母接龙中单词更多或者抛硬币序列更长）的示例。领域内和领域外评估的结果如下所示。

![|200](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64d4e762-6720-4b73-9f7b-0b20ad2a73d7_446x824.png)

尽管这些任务较为简单，但我们从上文可以看出，思维链（CoT）提示法 i) 提升了符号推理任务的表现；ii) 能够更好地泛化到需要更多推理步骤的跨领域问题。此外，我们再次观察到，较小的模型无论是否有思维链提示都无法解决符号推理任务。因此，思维链提示似乎是符号推理的一种非常有益的方法 。

### 思维链提示的变体

在[1]中提出思维链（CoT）提示方法后，人们又提出了几种变体，这些变体能够提升大型语言模型（LLMs）的推理能力。这些不同的变体为激发大型语言模型中的“推理”行为提供了多种有趣且实用的方法。下面列出了一些值得关注的思维链提示变体。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89900c8b-d33a-4cbf-bffc-bf0729db0353_1618x1128.png)

**零样本思维链（zero-shot CoT）。​**​ 零样本思维链提示[10] 是对思维链提示[1] 的一个简单后续改进。为了鼓励大型语言模型（LLM）生成思维链，零样本思维链只需在所提问题的末尾添加“让我们一步步思考。” 这样简单的表述。通过对大型语言模型的提示进行这一简单添加，我们在[10] 中看到，即使没有观察到此类行为的任何明确示例，大型语言模型也能够生成思维链，从而使它们在推理任务中得出更准确的答案。有关零样本思维链与其他提示方法的比较，请参见上文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8ae4c1d-cdca-4f7b-a792-714e74c65885_1612x950.png)

自我一致性。自我一致性是思维链提示的一种变体，它利用大型语言模型（LLM）生成多个思维链，然后从这些生成结果中进行多数投票，将得票最多的作为最终答案；详见上文。在思维链提示无效的情况下，使用自我一致性通常能改善结果。简单来说，自我一致性只是用一个通过大型语言模型生成多个答案并选取其中最常见答案的流程，取代了[1]中使用的贪婪解码程序。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4141d1fc-1665-408e-9c55-1ea64970026a_1082x918.png)

*最少到最多提示法*。最少到最多提示法通过首先将一个问题分解为较小的子问题，然后分别解决这些子问题，从而超越了思维链提示法。随着每个子问题的解决，其答案会被包含在用于解决下一个子问题的提示中。与思维链提示法相比，最少到最多提示法在一些任务（例如最后一个字母连接）上提高了准确性，并改善了对需要更多推理步骤的域外问题的泛化能力。  

*提示工程*。正如上面的示例（以及思维链提示法的总体思想）所示，为大型语言模型精心策划一个有用的提示是一门艺术。为了了解更多关于如何设计更有效的提示，我强烈推荐下面链接的Learn Prompting网站上提供的课程。

### 总结

在此概述中，我们看到标准提示不足以充分发挥大型语言模型（LLMs）的能力。相反，它似乎为大型语言模型的性能提供了一种“下限”，尤其是在更困难的基于推理的任务中。思维链（CoT）提示通过利用大型语言模型的少样本学习能力，在解决基于推理的问题时引导生成连贯的多步推理过程，从而超越了标准提示技术。这种方法对大型语言模型的性能极为有益，对于较大的模型更是如此。以下是一些要点。

思维链提示的效用。大型语言模型（LLMs）不擅长处理常识、算术和符号推理等任务。然而，思维链提示能极大提高在这些任务上的表现。此外，这种方法无需进行微调，并且只需要极少的额外数据（即，仅需一组用于少样本学习的示例）。因此，这是一种易于使用的技术，只要在提示工程和精心挑选少量示例方面下些功夫，就能帮助预训练的大型语言模型解决它们通常难以处理的任务 。

推理能力随规模而显现。并非所有模型都能从思维链（CoT）提示中受益。事实上，与较小的模型相比，较大的大型语言模型（LLMs）从思维链提示中获得了不成比例的更大益处。在文献[1]中，作者观察到，在参数超过1000亿的模型中，思维链提示的益处开始显现。这个确切的数字很可能在很大程度上取决于实验设置，但总体思路很明确：思维链提示的益处在较大的模型中最为明显。


> “思维链模拟了人类推理者的思维过程。这并没有回答神经网络实际上是否在进行推理。”——摘自[1]

大型语言模型（LLMs）真的懂得如何推理吗？思维链（CoT）提示有助于大型语言模型解决某些推理任务，但这并不一定意味着大型语言模型具备复杂的推理能力。[1]中的作者甚至明确指出，对思维链提示的分析并未解答大型语言模型是否真的在推理这一问题。相反，思维链提示是一种经验性技术，可用于更准确地解决算术、常识和符号推理等通常令大型语言模型感到棘手的任务。无论我们是否相信大型语言模型具备推理能力，我们都可以认同这种技术在实践中是有用的。
