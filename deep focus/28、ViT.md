
## 教程一

> https://cameronrwolfe.substack.com/p/vision-transformers-from-idea-to

鉴于 Transformer 在自然语言领域（甚至更广泛）取得了如此大的成功，我们可能会开始怀疑：_我们能否将相同的架构用于计算机视觉？_[1] 中提出了 Vision Transformer (ViT) 架构，该架构是图像分类常用卷积神经网络 (CNN) 架构（例如 ResNets）的替代方案，对这个问题给出了肯定的回答。

### 1、仅编码器架构

![|200](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9ec3e8bd-88f4-43fd-a5c4-14c120dd31ee_330x384.png)

**我们应该使用多大尺寸的模型？** 我们可以通过增加模型的宽度、深度或注意力头的数量来增加网络的参数数量。可以使用几种不同大小的 ViT 架构。但是，由于该模型与 BERT 共享相同的架构，因此我们通常继承 BERT 中流行的相同大小的模型（例如 BERT-base 和 BERT-large）。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566d7d78-9964-49d0-89f6-3f17218f8c4b_1280x348.png)

### 2、构建输入序列

相较于文本 token，_如果我们的输入是图像，我们如何形成这样的 token 序列？_

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2760f080-c901-4ca4-ba88-3988e79cdfd3_1328x442.png)

采用的方法实际上非常简单，我们只需：

1. 将图像分解为几个不重叠的块；
2. 线性投影每个块以形成一个向量。

我们必须做更多的事情来构建 ViT 的最终输入序列，例如：

1. 向每个 patch 嵌入添加位置嵌入
2. 在序列开头添加一个额外的 token `[class]`

这些都是是必要的，因为除非我们直接注入位置信息，否则自注意力无法理解 patch 在序列中的位置。_自注意力只是一堆矩阵乘法和点积，不考虑每个 token 的位置_ ！

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F317246b2-8221-4e04-b520-68293266bbb7_1360x426.png)

我们在序列开头添加的 `[class]` 标记通常用作整个序列的聚合表示。我们可以使用 ViT 最终输出的这个嵌入进行分类及其他操作。`

### 3、完整架构

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52e64916-62a3-4c07-a791-2a821153467e_1872x1134.png)

（这里的正则化层在子层之前）

### 4、图像分类效果
![|450](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9644bfb-cd9e-4b97-ad8e-29ab9ff9aa09_1342x652.png)


**训练流程。** 在这些实验中，ViT 遵循两步训练程序。首先在初始数据集上对模型进行预训练，然后在下游数据集（即进行评估的目标数据集）上进行微调。预训练使用了几种不同的数据集，包括 JFT-300M（来自 Google 的大型内部数据集）、ImageNet-21K（ImageNet 的较大版本）和 ImageNet。*整个预训练过程都是有监督的*。但是，通过对非常大的数据集进行监督预训练，我们在下游数据集上进行微调后提高了模型的质量。

**更大 = 预训练效果更好。** 从上面的结果中我们可以立即看到一个主要发现，即只有在预训练数据集足够大的情况下，ViT 才能表现良好（即优于最先进的 CNN）。

这种趋势可能是由于以下事实造成的：虽然 CNN 自然地对图像中的平移和局部性等模式具有不变性，但 ViT 架构（以及一般的 Transformer 架构）没有这种归纳偏差，必须直接从数据中学习这些不变性。因此，对大型数据集进行预训练可以学习 CNN 中自然存在的此类有价值的模式。

### 5、扩展性

虽然 ViT 表现良好，但它们需要一些额外的东西（即大量的预训练）才能正常工作，而 CNN 无需任何预训练就能很好地工作。考虑到这一点，_我们可能会开始怀疑  ViT 是否值得使用_。幸运的是，这种预训练要求在后来的工作中基本被取消，使 ViT 成为一种更实用的计算机视觉架构。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7eff212b-04c2-4e6e-aa76-fa78d2726068_1400x1410.png)


**减轻预训练要求。** 在 DeiT 中，作者提出了一种 ViT 变体，它为模型的输入添加了一个额外的特殊标记。我们可以使用此标记将蒸馏组件应用于模型的损失。具体来说，采用硬蒸馏（与软蒸馏相反）损失。

_这是什么意思？_ 基本上，这只是意味着我们可以在整个训练过程中使用已经表现良好的教师模型（通常是 CNN）。然后，我们使用该教师网络的 argmax 输出（即预测分类）作为训练 ViT 的附加目标。有趣的是，这种方法实现了令人印象深刻的性能水平，并且不需要对 ViT 进行广泛的预训练。

**相对位置嵌入。** 如前所述，Transformer 架构的一个缺点是缺乏平移不变性。如果我们将同一个物体放在图像中的两个不同空间位置，那么我们会得到不同的输出。直观地讲，这是不可取的，因为物体的位置不会从根本上改变其语义含义。

平移不变性通过使用卷积被硬编码到 CNN 中。然而，对于 ViT，我们必须从数据中学习这一特性。为了缓解这个问题，我们可以只使用相对位置嵌入。在原始形式中，Transformer 使用全局位置嵌入，它们不是平移不变的。简而言之，这意味着一个唯一的、附加的位置嵌入与模型输入序列中的每个位置相关联。


## 教程二（朱毅视频讲解）

这一个 patch相当于是一个元素呢那有效的长宽又会是多少呢那比如说现在这个宽度现在这个宽度就变成了24除以16其实就相当是14了同样的高度呢也就是14所以说这样的话呢最后的这个序列长度呢就变成了14x14x196意思就是说现在这张图片呢就只有196个元素那196对于Transformer来说呢还是可以接受的这个序列程度然后呢我们把这里的每一个 patch 呢当做一个元素然后通过一个这个 fc layer就会得到一个 linear embedding然后这些呢就会当做输入传给Transformer这个时候呢你其实可以很容易的看出来一个图片呢就已经变成一个一个的这样的图片块了然后你可以把这些的图片块呢当成是像NLP里的那些单词这一个句子有多少词那就相当是一个图片现在有多少个 patch这就是他题目的意义一个图片等价于很多16乘16的单词然后最后一句话呢他说他说我们训练这个Vision Transformer呢是用的有监督的训练啊为什么要突出啊有监督呢因为对于 nlp 那边来说的话Transformer基本上都是用无监督的方式去训练的要么是用language modeling要么是用mask language modeling总之都是无监督的训练方式但是对于视觉来说呢大部分的基线(baseline)网络还都是用这个有监督的训练方式去训练走到这呢我们可以看出来这个Vision Transformer这篇 paper 呢真的是把视觉当成自然语言处理的任务去做尤其是中间的这个模型呢他就是用了一个Transformer encoder跟BERT是完全一样那如果你已经听过沐神讲解BERT这篇论文的那其实Vision Transformer这篇论文对你而言没有任何技术难点这篇论文其实就是想告诉你用这么简洁的一套框架啊Transformer也能在视觉里起到很好的效果那其实大家就会很好奇啊这么简单的做法之前难道就没有人想过这么做吗那其实肯定是有人这么做了那其实Vision Transformer这篇论文在相关工作里已经介绍过他说跟他们这篇工作最像的呢其实是一篇 ICLR 2020的paper那篇论文呢是从这个输入图片里去抽这个2x2的图片patch 为什么是2x2呢因为那篇论文的作者呢只在CIFAR-10这个数据集上去做了实验那CIFAR-10的那些图片呢都是32x32的所以说你只需要抽2x2的patch就够了如果你抽16x16那就太大一旦抽好这个patch以后呢然后他们就在上面做 self-attention所以说你你一听那其实这个不就是Vision Transformer吗那确实是 从技术上而言他就是Vision Transformer但是呢Vision Transformer的作者告诉你说我们的区别在哪呢我们的工作就是证明了如果你在大规模的数据集上去做这个预训练的话就跟NLP那边一样在大规模的语料库上去做预训练那么我们就能让一个标准的Transformer就是不用在视觉上做任何的更改或者特殊的改动就能取得比现在最好的卷积神经网络还好或者差不多的结果啊同时他们还指出说之前的这个ICLR的这篇论文呢他们用的是很小的patch是2x2的所以让他们的模型呢只能处理那些小的图片而Vision Transformer呢是可以handle medium-resolution也就是224这种的图片所以这篇文章的主要目的呢就是来告诉你Transformer在Vision 这边能够扩展的有多好这是在超级大数据集和超级大模型的两方加持下 Transformer到底能不能取代卷积神经网络的地位那我们回到引言啊一般引言的最后呢就是把你最想说的结论或者最想表示的结果放出来这样大家不用看完整篇论文就能知道你这篇论文的贡献到底有多大他说呢当在中型大小的数据集上比如说ImageNet去训练的时候呢如果不加比较强的约束ViT的模型其实跟同等大小的残差网络相比呢其实是要弱的就是要弱几个点那作者就得解释啊那他为什么弱啊所以论文紧接着说这个看起来不太好的结果呢其实是可以预期的为什么呢因为 transformer跟卷积神经网络相比他缺少一些卷积神经网络有的归纳偏置这个归纳偏置这个概念很有意思他其实说的呢就是指一种先验知识或者一种我们提前做好的假设比如说呢对于卷积神经网络来说我们常说的就有两个inductive bias两个归纳偏置一个呢叫做 locality因为卷积神经网络是以滑动窗口这种形式是一点一点在图片上进行卷积的所以他假设图片上相邻的区域会有相邻的特征那这个假设当然是非常合理的比如说桌子和椅子呢就大概率他经常就会在一起靠的越近的东西呢他的相关性就会越强另外一个归纳偏置呢叫做平移等变性也就是我们常说的 translation equivariance(平移同变性)他的意思呢就是如果你把它写成公式的话呢其实是这样的f(g(x)) = g(f(x))就是不论你先做 g 这个函数还是先做 f这个函数最后的结果呢是不变的这里你可以把 f 理解成是卷积然后 g 呢可以理解成是平移这个操作那意思就是说无论你先做平移还是先做卷积最后的结果是一样的因为卷积神经网络里这个卷积核就相当于是一个模板一样像一个 template 一样不论你这个图片同样的物体移到哪里那只要是同样的输入进来然后遇到了同样的卷积核那他的输出永远是一样的一旦卷积神经网络有了这两个归纳偏置以后呢他其实就有了很多先验信息所以他就需要相对少的数据去学一个比较好的模型但是对于Transformer来说呢他没有这些先验信息所以说他所有的这些能力对视觉世界的感知呢全都需要从这些数据里自己学为了验证这个假设呢作者就在更大的数据集上去做了预训练这里的这个14M就是ImageNet 22k数据集这里的300M数据集呢就是google他们自己的JFT 300M数据上了大数据以后呢果然这效果拔群他们立马就发现这个大规模的预训练呢就比这个归纳偏置要好Vision Transformer只要在有这个足够的数据去预训练的情况下呢就能在下游任务上获得很好的迁移学习效果具体来说呢就是当在ImageNet 21 k 上去训练或者说在这个 JFT 300M上去训练的时候呢vit 就能获得跟现在最好的残差网络相近或者说更好的结果啊最后他还罗列了一些结果啊比如说ImageNetImageNet-ReaLCIFAR-100 还有 VTAB 这些数据集VTAB其实也是这个作者团队他们提出来的一个数据集是融合了19个数据集主要是用来检测一个模型这个稳健性好不好的所以从侧面也可以反映出呢Vision Transformer的稳健性也是相当不错的所以读到这样呢引言算是读完了那引言还是写的非常简洁明了的第一段先上来说因为Transformer在 NLP那边扩展的很好越大的数据或者越大的模型最后的performance就会一直上升没有饱和的现象那自然而然就会有一个问题那如果把Transformer用到视觉里来是不是视觉的问题也能获得大幅度的提升呢第二段就开始讲前人的工作因为这么好的方向不可能没人做过所以一定要讲清楚自己的工作和前人的工作的区别在哪里他说啊之前的工作呢要么就是把卷积神经网络和自注意力合起来要么就是用自注意力去取代卷积神经网络但是呢从来都没有工作直接把Transformer用到视觉领域里来而且也都没有获得很好的扩展效果所以第三段他就讲Vision Transformer就是用了一个标准的Transformer模型啊只需要对图片做一下预处理就是把图片打成块然后送到Transforner里就可以了别的什么改动都不需要这样彻底可以把一个视觉问题理解成是一个NLP问题这样就CV和 NLP这两个领域就大一统了最后两段当然是卖一下结果了只要在有足够多数据区域训练的情况下Vision Transformer在很多数据集上取得很好的效果那接下来我们按照沐神读论文的方式先来看一下结论结论里呢他上来没有任何花里胡哨直接一句话就点明了这篇 paper 在到底在干什么直接拿NLP领域里标准Transformer来做计算机视觉问题然后他说跟之前的那些工作就是用自注意力的那些工作不一样的呢除了在刚开始的抽图像块的时候还有这个位置编码用了一些图像特有的这个归纳偏置如此之外呢就再也没有引入任何图像特有的归纳偏置了这样的好处呢就是我们不需要对Vision领域有什么了解或者有什么domain knowledge(领域知识)我们可以直接把这个图片呢理解成为是一个序列的图像块就跟一个句子里有很多单词一样然后就可以直接呢NLP里面一个标准的Transformer来做图像分类这个简单然后呢扩展性很好的这个策略呢当你跟大规模预训练结合起来的时候工作得出奇的好怎么个好法呢就是说Vision Transformer在很多这个图像分类的这个benchmark 上呢超过了之前最好的方法而且还相对便宜就训练起来还相对便宜然后接下来第二段呢他又写了一些目前还没有解决的问题或者是对未来的一个展望吧为什么要写这一段呢其实是因为Vision Transformer这篇论文呢属于是一个挖坑的论文这挖坑呢你可以是挖一个新问题的坑或者是挖一个新数据集的坑那这篇论文呢其实是挖了一个新模型的坑又说如何能Transformer来做 cv那所以很自然的第一个问题就是说那Vision Transformer不能只做分类啊那这个Vision Transformer肯定还得去做这个分割和检测对吧另外两个最主流的视觉任务啊就是之前另外一篇很popular的工作就是这个DETRDETR是去年啊目标检测了一个力作他现在是改变了整个目标检测之前的框架就是出框的这种方式啊鉴于ViT和DETR而良好的表现呢所以说他是拿Vision Transformer做视觉的其他问题呢应该是没有问题的而事实上呢也正是如此就在 ViT 出来短短的一个半月之后呢就在2020年12月检测这块呢就出来了一个工作叫做ViT-FRCNN就已经把ViT用到detection上了seqmentation 呢也一样 同年12月只有一篇SETRSETR的论文把Vision Transformer用到分割里了但这里我想说的是大家的手速真的是太快了因为虽然表面看起来SERT这篇论文是12月份才传到arXiv上的但是他是CVPR2020的中稿论文也就意味着他在11月15号的时候就已经完成了论文的写作并且投稿了所以说啊 cv 圈这个卷的程度啊已经不能用月来计算了一定要用天来计算而且紧接着三个月之后Swin Transformer 横空出世他把多尺度的这个设计呢融合到了Transformer里面所以说更加适合做视觉的问题了真正证明了Transformer是能够当成一个视觉领域一个通用的骨干网络的另外一个未来工作方向呢他就是说要去探索一下这个自监督的预训练方案那是因为在NLP领域所有的这些大的Transformer全都是用自监督的方式训练一会我们在看实验的时候呢也可以一起讨论一下之前NLP包括Vision里边是怎么做自监督预训练的所以说呢Vision Transformer这篇论文呢他也做了一些初始实验他们证明了就是说用这种自监督的训练方式呢也是 ok 的但是呢跟这个有监督的训练比起来呢还是有不小的差距的最后呢作者还说就说继续把这个Vision Transformer变得更大有可能会带来更好的结果这个坑呢作者团队没有等别人去填他想着反正别人可能也没有这种计算资源那我就来把这个坑填一填吧所以说过了半年同样的作者团队又出了一篇论文叫 Scaling Vision Transformer就是把Transformer变得很大提出了一个ViT-G然后就把ImageNet分类的准确率刷到90以上了所以说这篇论文真的是一个很好的挖坑之作他不光是给视觉挖了一个坑就是从此以后你可以用传送门来做所有视觉的任务了他同时还挖了一个更大的坑就是在CV和NLP能够大一统之后那是不是多模态的任务就可以用一个Transformer去解决了呢而事实上多模态那边的工作呢最近一年也是井喷式的增长由此可以看来Vision Transformer这篇论文的影响力还是非常巨大的那我们现在回到相关工作大家说了一下Transformer在NLP领域的应用他说啊自从2017年这个Transformer提出来去做这个机器翻译以后呢基本上Transformer就是很多NLP任务里表现最好的方法现在大规模的这个Transformer模型呢一般都是先在一个大规模语料库上去做预训练然后再在这个目标任务上呢去做一些细小的微调这里面有两系列比较出名的工作一个就是BERT一个就是GPTBERT呢是用了一个 denoising自监督的方式其实就是完形填空就是你有一个句子然后你把其中某些词划掉然后你先要把这些词再 predict 出来这个GPT呢用的是 language modeling去做自监督language modeling 呢是你已经有一个句子然后我要去预测下一个词是什么也就是 next word prediction预测下一个词因为这两个任务呢其实都是我们人为定的就说语料都在哪句子就在哪是完整的我们只是人为的去划掉其中某些部分或者把最后的词拿掉然后去做这种完形填空或者预测下一个词所以这叫自监督的训练方式这下就开始讲自注意力在视觉里的应用视觉里呢就是说如果你想简单的使用一个自注意力还用到这个图片上最简单的方式就是说你把每一个像素点当成是一个元素啊你就去让他们两两去做自注意力就好了但是呢我们在引言里也说过这个是平方复杂度所以说是很难应用到真实的这个图片输入尺寸上的那像现在分类任务的224x224一个Transformer都很难处理那更别提就我们人眼看的比较清晰的图一般都是1 k 或者4 k 这种画质了那序列长度都是上百万直接在像素层面使用Transformer肯定是不现实的所以说呢如果你想用Transformer那就一定得做一些这个近似这里他就罗列一些例子啊比如说这些工作呢他们是说你的复杂度高是因为你用了整张图所以你的序列长度长那我现在不用整张图我就用这个 local neighborhood 啊我就用一个小窗口制作这个自注意力那序列长度不就大大降低了吗那最后的这个计算复杂度不也就降低了吗那另外一条路呢就是用 SparseTransformer这个顾名思义啊就是说我去只对一些稀疏的点去做自注意力所以只是全局注意力的一个近似还有一系列方法呢就是说把这个自注意力用到不同大小的这种 block 上或者说在极端的情况下呢就是直接走轴了也就是我们刚才讲的轴注意力就先在横轴上去做自注意力然后再在纵轴上做自注意力那这个序列长度也是大大减小的然后他说啊 这些特制的自注意力结构其实在计算机视觉上的结果都不错就是表现是没问题但是他需要很复杂的工程呢去加速这个算子虽然在 cpu 或者 gpu 上跑的很快或者说让训练一个大模型成为可能接下来这段我们其实已经在引言里读过了就是说跟他们工作最相似的呢是一篇ICLR2020论文Vision Transformer呢就是在用了更大的 patch和更多的数据集去训练一个更好的 transformer然后他说啊 在计算机视觉领域肯定还有很多工作呢是把这个卷积神经网络和这个自注意力结合起来的而这类工作呢是相当多的而且基本涵盖了视觉里的很多任务比如说这个检测啊分类啊然后这个视频 还有多模态啊然后呢作者又说还有一个工作跟他们的工作很相近就叫image GPT我们都知道啊GPT呢是用在NLP里呢是一个生成性的模型那image GPT呢同样也是一个生成性模型也是用无监督的方式去训练的他跟Vision Transformer有什么相近的地方呢是因为他也用了Transformer最后这个 iGPT 能达到一个什么效果如果我们拿训练好的模型去做一个微调或者说就把他当成一个特征提取器呢我们会发现他ImageNet的上最高这个分类准确率也只能到72那像这篇 Vision Transformer最后的结果已经有88.5了所以说是远高于这个72的结果但这个结果也是最近一篇论文 MAE(何凯明)Masked Autoencoders Are Scalable Vision Learners爆火的原因因为在BEiTBERT Pre-Training of Image Transformer或者MAE这类工作之前生成是网络在视觉领域很多任务上是没法跟判别式网络比的判别式网络往往要比生成式网络的结果要高很多但是 MAE 就做到了就在ImageNet-1k 这个数据集上去做训练啊就用一个生成式的模型比之前判别式模型的效果要好而且不光是在分类任务上最近他们还研究了一下迁移学习的效果就是在目标检测上效果也非常好最后呢作者又说Vision Transformer其实还跟另外一系列工作是有关系的就是用比ImagNet还大的数据集去做这个预训练这种使用额外数据的方式呢一般能够帮助你达到特别好的效果啊比如说之前2017的这篇论文其实也就是介绍 JFT300M的这个数据集的论文呢研究了就是卷积神经网络的效果是怎么随着数据集的增大而提高的还有一些论文呢是研究了当你在更大的数据集上比如说ImageNet-21 k和 JFT300M去做预训练的时候这个迁移学习的效果会怎么样就是当你迁移到ImageNet或者CIFAR-100上的效果会如何在这篇论文里呢Vision Transformer上说我们也是聚焦于最后两个数据集就是这个ImageNet-21k和 JFT300M但是呢 我们并不是训一个残差网络我们是去训Transformer那其实这个相关工作写的非常彻底的而且他列举了很多跟他们工作最相近的啊比如说这篇ICLR2020的论文还有这个 iGPT还有之前研究大数据集的比如说BIT其实写相关工作这个章节呢就是要让读者知道在你的工作之前别人做了哪些工作你跟他们的区别在哪里这个只要写清楚了其实是对你非常有利的并不会因此降低论文的这个创新性反而会让整个文章变得更加简单易懂接下来我们就一起来读一下这个文章的第三节这个主体部分那么说啊在模型的设计上是尽可能的按照这个最原始的这个Transformer里来做的 这样一个好处就是说呢我们可以直接把 NLP那边已经成功的 Transformer的架构直接拿过来用我们就不用自己再去魔改模型了而且因为Transformer已经在NLP领域火了这么多年了他有一些写得非常高效的一些实现同样Vision Transformer可以直接把他拿过来用那接下来看3.1  他说模型的一个总览图呢 画在图一里了模型的总览图啊其实对很多论文来说是非常重要的画得好的模型总览图呢能够让读者在不读论文的情况下光看这张图就能大概知道你整篇论文在讲什么内容Vision Transformer这张图呢 其实画得就很好因为我们也可以看到啊当别人在讲解Vision Transformer这篇论文或者跟他对比的时候就是直接把这个图1直接就复制粘贴过去了而不是说自己为了讲解的需要还要再费尽心思再从头再画一个图接下来我们先大概过一下这个整体的流程然后我们再具体的过一遍整个网络的这个前向操作这样对Vision Transformer的理解就更深刻了首先呢给定一张图他是先把这张图呢打成了这种 patch比如说这里是把这个图打成了九宫格然后他把这些 patch 呢变成了一个序列每个 patch会通过一个叫线性的投射层的操作得到一个特征也就是这篇论文里说的 patch embedding但我们大家都知道啊这个自注意力是所有的元素之间两两去做这个交互所以说他本身并不存在一个顺序问题但是对于图片来说呢他是一个整体这个九宫格是有自己的顺序的比如说这图片就是 1 2 3一直到第九张图片还是有顺序的如果这个顺序颠倒了呢其实就不是原来这张图片了那同样类似NLP那边呢我们给这个patch embedding加上了一个position embedding就加上一个位置编码一旦加上这个位置编码信息以后呢这个整体的 token就既包含了这个图片块原本有的图像信息又包含了这个图像块的所在位置信息那一旦我们得到这些一个一个的 token那其实接下来就可能NLP那边是完全一样了我们直接把它给这么一个Transformer encoder然后Transformer encoder呢就会同样的反馈给我们很多输出那这里问题又来了那这么多输出我们该拿哪个输出去做最后的分类呢所以说再次借鉴BERTBERT那边呢有这个叫 extra learnable embedding也就是一个特殊字符叫 cls叫分类字符同样的我们在这里也加了这么一个特殊的字符这里用新号代替而且他也是 position embedding他有位置信息的他永远是0因为所有的token都在跟所有的token做交互信息所以他们相信呢这个class embedding能够从别的这些embedding里头去学到有用的信息从而我们只需要根据他的输出做一个最后的判断就可以那最后这个 MLP Head其实就是一个通用的分类头了最后用交叉熵函数去进行模型的训练至于这个Transformer encoder也是一个标准transformer这篇论文也把具体的结构列在右边了比如说当你有这些patch的时候呢你先进来做一次 layer norm然后再做 multi-head self-attention然后再 layer norm然后再做一个 MLP这就是一个Transformer block然后你可以把它叠加 L 次啊就得到了你的Transformer Encoder所以说整体上来看Vision Transformer的架构还是相当简洁它的特殊之处就在于如何把一个图片变成这里的一系列的 token我们接下来呢可以具体按照论文中的文字再把整个模型的前向过程走一遍假如说啊我们有个图片 X然后他的dimension呢是224x224x3如果我们这里使用16x16的这个patch size 大小呢那我们就会得到多少 token 呢等于24平方除以16的平方也就是196 就是14的平方就我们一共会得到196个图像块那每一个图像块的维度如何呢其实就是16x16x3也就是768所以呢我们就把原来这个图片变成了一个有196个patch每个patch的维度是768那接下来这个线性投射层是什么呢其实这个线性投射层呢就是一个全连接层文章后面呢是用大E这个符号呢就代表这个全连接层的这个全连接层的维度是多少呢其实这个全连接层的维度呢是768x768这个768呢也就是文章中一直说的那个D当然这个D是可以变的就如果你的Transformer变得更大了那这个D也可以变得相应的更大但是前面这个768是从图像patch算来的是16x16x3算来的所以这个是不变的那经过了这个线性投射我们就得到我们的这个 patching embedding那具体来说呢就是如果 XE =1那在维度上呢其实就可以理解成是196x768然后又乘了个768x768的矩阵然后这两个维度抵消掉所以最后你得到的还是196x768意思就是我现在有196个 token每个 token 向量的维度呢是768那到现在呢就其实已经成功的把一个 Vision 的问题变成了一个NLP 的问题我的输入呢就是一系列而不再是一张2d的图了那除了图像本身带来的这些 token 以外呢我们之前也说过这里面要加一个额外的 cls token就是一个特殊的字符那这个特殊字符他只有一个token他的维度也是768这样方便你可以和后面图像的信息呢直接拼接起来所以最后序列的长度就是整体进入Transformer中的这个序列长度呢其实是196 +1就是197乘以768了这197呢就是有196个图片对应的 token和一个那个特殊字符 cls token最后呢我们还要加上这些位置编码信息那位置编码呢刚才我们说 1 2 3 一直到9其实这只是一个序号而并不是我们真正使用的位置编码因为我们不可能把这个1 2 3 4 5 6这些数字然后传给一个Transformer去学具体的做法呢是我们有一个表啊这个表的每一行呢其实就代表了这里面的1 2 3 4这个序号然后每一行呢就是一个向量这个向量的维度是多少呢跟这边是一样的 跟这边的D是一样的是768 这个向量也是可以学的然后我们把这个位置信息呢加到这所有的这些的token里面而注意这里面是加而不是拼接就不是 concatenation 是直接sum了所以加完位置编码信息以后呢这个序列还是197x768那到此呢我们就做完了整个对图片的这个预处理包括加上特殊的字符 c l s和加上这个位置编码信息啊也就是说我们对于Transformer输入这块的embedded patches 呢就是197x768的一个tensor这个tensor呢先过一个 layer norm 出来还是197x768然后我们就要做这个多头注意力了那多头注意力呢这里就变成了三份 k q v每一个都是197x768这样的那这里呢因为我们做的是多头自注意力所以其实最后的这个维度呢并不是768假设说我们现在用的是Vision Transformer的base版本也就是说他用多头呢是用了12个头那这个维度呢就变成了768除以12也就等于64的维度也就是说这里的 k q v呢变成了197x64但是有12个头去做这个自注意力操作最后呢再把这些这样64拼接出来以后呢又变成了768所以多头注意力出来的结果经过拼接呢到这块还是197x768然后再过一层 layer norm 还是197x768然后再过一层 MLPMLP 这里呢会把维度相对应的放大一般呢是放大四倍所以说就是197乘以3072然后再把它缩小投射回去然后再变成197乘768就输出了这个呢就是一个Transformer block的前向传播的过程所以说进去呢是197x768出来呢还是197x768这个序列的长度和每个token对应的维度大小都是一样的所以说你就可以在一个Transformer block上呢不停的往上叠Transformer block想加多少加多少那最后呢有 L 层这个Transformer block的模型呢就是这个Transformer Encoder也就是这里面这个Transformer Encoder的其实到这里呢就已经把Vision Transformer这整个的模型讲完了我们再回到正文再快速的把3.1过一遍看有没有什么遗漏的他说啊标准的Transformer是需要一系列这个1D的token呢当做输入那但是对于2 d 的图片怎么办呢那我们就把这个图片呢打成这么多的小 patch具体有多少个 patch 呢就是 N 等于这个 HW/P^2那其实也就是我们之前算过的除以16的平方也就是196当然这个是对于patch size =16 而言了如果你patch size变了就变成8啊或者变成14那对应在这里的这个序列长度呢也会相应改变他说啊这个Transformer从头到尾都是用了这个 D 当做向量的长度的也就是我们刚才说的那个768啊所以就是从头到尾他的向量长度都是那为了和Transformer这个维度匹配上呢所以说我们的这个图像的 patch 的维度呢也得是这个 Ddimension也就是768那具体怎么做呢就是用一个可以训练的 linear projection也就是一个全连接层从全连接层出来的这个东西呢叫做 patch embedding然后接着说为了最后的分类呢我们借鉴了BERT 里的这个 class token一个特殊的 cls token这个 token 是什么呢是一个可以学习的特征他呢跟图像的特征有同样的维度就都是768但他只有一个token就是经过很多层的这个Transformer block以后呢 我们把他这个输出当成是整个Transformer的模型的输出也就是当做整个图片的这个特征那一旦有了这个图像特征了后面就好做了因为你卷积神经网络到这块也是有一个图像的整体的特征我们就在后面加一个这个MLP的这个分类图就可以了然后对于位置编码信息呢这篇文章就用的是标准的这个可以学习的也就是BERT里用的位置编码那当然作者也尝试了别的编码形式了就比如说因为你是做图片嘛所以你对空间上的位置可能更敏感你可能需要一个2D aware 就是能处理2D信息的一个这个位置编码那最后发现了这个结果其实都差不多没什么区别其实针对这个特殊的 class token还有这个位置编码呢作者们还做了详细的消融实验因为对于 Vision Transformer 来说怎么对图片进行预处理还有怎么对图片最后的输出进行后处理是很关键的因为毕竟中间的这个模型就是一个标准的transformer没什么好讲的所以我们现在就来看一下这些消融实验 首先我们说一下这个 class token他说 因为在这篇论文中呢我们想跟原始的这个transformer保持尽可能的一致所以说呢我们也用了这个class token因为这个class token呢在NLP那边的分类任务里也是用的在那边呢也是当做一个全局的对句子理解的一个特征那在这里呢我们就是把它当做一个图像的整体特征那拿到这个token的输出以后呢我们就在后面接一个这个 mlpmlp 里面呢是用 tanh 当做一个非线性的激活函数 去做这个分类的预测 但是这个 class token 的设计呢是完全从 nlp 里边借鉴过来的之前在视觉领域呢我们其实不是这么做的那比如说我们现在有一个残差网络一个 Res50 我们现在有这个前面的几个 blockres 2 res 3 res 4 res 5对吧假如说是2345 那在最后这个 stage 出来的呢是一个 feature map这个 feature map 呢是14乘14这么大然后在这个 feature map 之上呢我们其实是做了一步这个 gap 的操作也就他这里说的 global average pulling 就是一个全局的平均池化池化以后的特征呢其实就已经拉直了 就是一个向量了那这个时候我们就可以把这个向量理解成是一个全局的对于这个图片的一个特征然后我们就拿这个特征去做分类那现在对于transformer来说 如果你有一个transformer的model然后你进去呢有这么多 n 个元素而你出来呢也是有这么多 n 个元素我们为什么不能直接在这 n 个输出上去做一个这个 globel average pulling然后得到一个最后的特征呢我们为什么非得在这个前面呢加一个 class token然后最后用 class token 去做输出然后去做分类呢那其实通过实验呢作者最后的结论是说其实这两种方式都可以就你也可以去做这个 global average pulling那得到一个全局的特征然后去做分类或者你用什么一个 class token去做vision transformer这篇论文所有的实验都是用 class token 去做的他主要的目的呢就是跟原始的这个transformer而保持尽可能的一致 就像他这里说的stay as close as possible我不想让你觉得他这个效果好 有可能是因为某些 trick 带来的或者是某些针对 cv 的改动而带来的那就是想告诉你一个标准的transformer照样能做视觉那我们往下拉看这张图 这张图里呢其实这个绿线呢就代表这个全局平均池化就是原来vision 领域里怎么做的然后这个 class token 呢就代表是 nlp 那边怎么做就这条蓝线那我们可以看到呢其实最后呢这个绿线和蓝线的效果是一样的但是作者指出呢这个绿线和蓝线用的这个学习率是不一样的 就是你得好好调参如果你不好好调餐呢比如说直接把这个 class token用的这个 learning rate直接拿过来去做这个全局平均池化呢那他效果是非常差的只有这么多可能大概低了五六个点吧所以说呢有的时候也不是你的 idea不work 可能主要还是参没调好炼丹技术还是要过硬 我们再来看一下位置编码 他这里也做了很多这个消融实验 主要的呢其实就三种 1D的 或者是这个 relative positional embedding其实也就是 nlp 那边常用的一个位置编码也就是这篇文章从头到尾都在使用的位置编码他是这个意思就是原来你1D那边呢比如说你把一个图片打成九宫格我们之前有的是123直到9那我们现在与其用这种方式去表示那些图像块呢我们用这种方式 就是11 去表示这么一个九宫格的图片那这样是不是跟视觉问题就更贴近呢因为他有了这个整体的这个 structure 信息那具体是怎么做呢就是说假如说原来的这种1D的这种位置编码他的这个维度是d那现因为横坐标纵坐标你都得需要去表示那对于横坐标来说呢你有一个d/2的维度那对于纵坐标来说一样你也有一个d/2的维度就是你分别有这么一个d/2的这个向量  去表述这个横坐标然后还有一个d/2的向量呢去描述这个纵坐标最后你把这两个d/2的向量拼接在一起 concat在一起就又得到了一个长度为 d 的这个向量我们把这个向量呢叫做2d 的位置编码那第三种这个相对位置编码是什么意思呢就是说假如我们讲还是1 d 的情况那这个patch和这个patch的距离既可以用绝对距离来表示又可以用他俩之间的这个相对距离来表示那比如说他们现在之间差了7个单位距离也就是他这里说的这个 offset这样呢也可以认为是一种方式就表示各个这个图像块之间的这个位置信息但是呢这个消融实验最后的结果也是 无所谓那我们现在来看他这个表8表8里说如果你不加任何的这个位置编码呢那效果肯定是很差的 这个61其实也不算很差transformer根本没有感知这个图片位置的能力在没有位置编码的情况下他还能达到这个61的效果其实已经相当不错了然后如果你来对比这个1D三种位置编码的形式呢你会发现所有的这个 performance 呢都是64 没有任何区别这个现象其实还比较奇怪这作者在后面呢也给出了一个他们认为比较合理的解释就是说呢他们这个vision transformer呢是直接在这个图形块上去做的而不是在原来的这个像素块上去做因为这个图像块很小 比如说14x14而不是全局的那种224x224所以你去排列组合这种小块或者你想知道这些小块之间相对的位置信息呢还是比较容易的所以你用什么位置编码都无所谓那么现在从附录回到正文 刚才看了那些消融实验以后呢我们也可以看出来其实这个 class token 呢也可以用这个 global average pooling去替换包括最后的这个 1 d 的positional embedding 呢也可以用2 d 的或者相对的这种位置编码去替换但是为了尽可能的对这个标准的transformer不做太多改动所以说本文中vision transformer还是用了这个 class token而且还是用了这个 1d 的位置编码然后接下来就开始说这个transformer encoder了那其实transformer在现在来看呢已经是一个非常标准的操作了那他这里对transformer或者说这个多头自注意力的解释呢都没有放在正文里 直接就放在这个附录里了 就跟你看现在卷积神经网络的论文也不会有人把卷积写在正文里然后作者呢用公式把整体的这个过程呢总结了一下就说 刚开始你这个 x 1 px 2 px n p 呢其实就是这些图像块的 patch 然后一共有 n 个 patch所以说123一直到 n那每个 patch 呢先去跟这个 linear projection也就是那个全连接层去做转换 从而得到这个 patching embedding 得到这些 patching embedding之后呢我们在它前面拼接一个这个 class embedding因为我们需要用它去做最后的输出而一旦得到所有的这个 tokens 呢我们需要对这些 token进行位置编码所以说我们把这个位置编码信息加进去 这个呢就是对于transformer的输入了z 0就是这个输入 接下来呢这块就是一个循环 对于每一个transformer block来说呢那里面都有两个操作一个是多头自注意力一个是 mlp然后在做这两个操作之前呢我们都要先过layer norm然后每一层出来的结果呢都要再去有一个残差连接所以说这个 zl'呢就是每一个多头自注意力出来的结果 这个 zl 呢就是每一个transformer block整体做完以后出来的结果然后我们这个 l 层循环完了之后呢我们把 zl就是最后一层的那个输出的zl 0也就是这个 class token所对应的输出呢当做整体图像的一个特征从而去做最后的这个分类任务那所以到这呢对vision transformer的文字描述也结束了所以他就补了一些分析比如说这个归纳偏置然而又提出了一个模型的变体 叫做这个混合模型 最后在3.2里又讲了讲当你遇到更大尺寸图片的时候你该怎么去做微调那么先来看归纳偏置 他说vision transformer比 cnn 而言呢要少很多这种图像特有的归纳偏置那比如说在 cnn 里呢这个 locality还有这个 translation equivariance就是这个局部性这个平移等变性是在模型的每一层都有所体现的这个先验知识呢相当于贯穿整个模型始终但是对于vit 来说呢这个 mlp 这个类型呢是局部而且平移等变性的但是自注意力层呢绝对是全局的这种图片2 d 的信息呢基本 vit没怎么用其实也就是刚开始把图片切成 patch 的时候还有就是加这个位置编码的时候用了而除此之外就再也没有用任何针对视觉问题就针对图像的这个归纳偏置了而且这个位置编码呢说白了其实也是刚开始随机初始化的并没有携带任何2D的信息所有的关于这个图像块之间的这个距离信息呢这个场景信息呢都得重头学所以他这里呢也就是先给后面的结果做了个铺垫就是说我vision transformer没有用太多的这个归纳偏置所以说在中小数据集上进行一训练的时候呢效果不如卷积神经网络是可以理解那讲完这个呢很自然呢大家就会想今天transformer这个全局建模的能力比较强然后卷积神经网络呢又比较 data efficient就不用那么多的训练数据那我们是不是可以搞一个这个混合的网络呢就是说前面是卷积神经网络后面是transformer了在他这里呢也做了对应的实验具体来说呢就是原来你有一个图片 你把它打成 patch比如说打成16乘16的patch然后你得到了196个元素然后这196个元素呢去和这个全连接层去做一次操作 最后得到这个patch embedding那现在呢我们不把图片打成块了我们去按照卷积神经网络那样去处理图片就是一整张图进来然后我们把它给一个 cnn比如说一个残差网络 res 50那他最后的那个特征图出来呢是一个14*14的特征图那这个特征图呢刚好你把它拉直了以后呢他也是196个元素然后你用这新得到的这196个元素呢去跟这个全连接层去做操作 得到新的这种 patch embedding那其实呢这里就是两种不同的对图片进行预处理的方式那一种呢就是直接把它打成 patch 很简单直接过全连接层另外一种呢就是还要再过一个 cnn 得到这么196个序列长度但是呢这得到的这个序列长度 都是196所以说后续的操作全都是一样的了都是直接扔给一个transformer然后最后做分类然后我们再来看3.2为什么要把这个微调放在这个3乘3呢因为之前有工作说呢如果你在微调的时候呢能用比较大的这个图像尺寸不是用224乘224了而是用256乘256甚至更大320乘320那你就会得到更好的结果那vision transformer肯定也不会放弃这个刷脸的机会然后他也想在更大的这个尺寸上去做微调但是呢用一个预训练好的vision transformer其实是不太好去调整这个输入尺寸的他这里说当你用更大尺寸的图片的时候呢如果我们把 patch size 保持一样就还用16*16但是你图片扩大了那很显然你这个序列长度就增长了对吧你原来的这个序列长度呢是n是假如说你现在换成了320的图片那就是320的平方除以16的平方那对 序列长度肯定是增加了所以说transformer呢从理论上来说呢他是可以处理任意长度的要只要硬件允许 任意长度都可以但是呢你提前预训练好的这些位置编码呢有可能就没用了因为你原来的位置编码比如说九宫格的话你是1-9他是有明确的这个位置信息的意义在里面的你现在图片变大了如果保持 patch size 不变的话你有可能得到十六宫格二十五宫格你的patch增多了那你现在的位置信息呢就从1到25而不是从1到9了那这个时候位置编码该如何使用呢那他这里发现呢其实简单的做一个2 d 的插值就可以了那在这里呢他们这个操作其实就是用torch官方自带的这个 interpolate的函数就能够完成但是呢这里的插值也不是说可以想插多长就插多长当你从一个很短的序列想要变到一个很长的序列的时候比如说你从256到512甚至变到768更长的时候呢直接这样一个简单的插值操作呢是会让最后的效果掉点的所以说这里的插值呢其实只能说是一种临时的解决方案他算是vision transformer在微调的时候的一个局限性最后他说因为你用了这个图片的位置信息去做这个插值所以说这块的这个尺寸改变还有这个抽图像块 是vision transformer里唯一的地方用了2 d 信息的这个归纳偏置接下来我们一起来读实验部分他说 在这个章节主要是对比的残差网络  vit 他们这个混合模型的这个表征学习能力为了了解到底训练好每个模型需要多少数据呢他们在不同大小的数据集上去做预训练然后在很多的数据集上去做测试当考虑到预训练的这个计算代价的时候就是预训练的时间长短的时候呢vision transformer表现的非常好能在大多数数据之上取得最好的结果 同时需要更少的时间去训练最后作者呢还做了一个小小的实验就做了一个自监督的实验他说 自监督说的实验的结果呢还可以 虽然没有最好但他说这个呢还是比较有潜力的而事实上确实如此 时隔一年之后最近大火的 mae就证明了自监督的方式去训练 vit 确实效果很好数据集的使用方面呢主要就是说用了这个image net的数据集他把这个有1,000个类就是大家最普遍使用的这个1,000类的image net数据集呢叫做image net或者在很多别的论文里呢叫image net-1 k然后更大的那个集合imagenet-21 k就是有21,000个类别而且有1400万图片的那个数据集呢 叫做imagenet-21 kjft 数据集呢就是 google 自己的数据集 就是有那个三个亿的图片的数据集他的任务呢全都做的是分类也用的都是比较popular的数据集比如说 cifar oxford petoxford flower 这些数据集接下来我们看一下模型的变体 它呢一共是三种模型其实就是跟transformer 那边对应了就是有 baselarge 和 huge 就是说对应的这些参数 全都逐渐的变大比如说用了更多的 transformer block 或者用了更长的这个向量维度或者 mlp 的 size 变大或者说多头自注意力的时候用了更多的头然后他接着正文里又补充了一些内容因为他的模型呢不光跟这个 transformer 本身有关系他还跟你这个输入有关系当然这个patch size大小变化的时候比如说从16乘16变大变成32乘32或者变小变成8乘8的时候呢那这个模型的位置编码就不一样所以patch size呢也要考虑到这个模型命名里面 于是他们模型命名的方式就是比如 vit-l 16意思就是说呢他用的是一个 vit large 的模型然后他的输入patch size 呢是16*16接着他又说 说这个transformer的这个序列长度呢其实是跟你这个patch size成反比的因为你patch size越小那你切成的块就越多 你patch size越大切成的块就越小所以当模型用了更小的patch size的时候呢计算起来就会更贵也就意味着比如说你这个 vit-l 16你要换成 vit-l 8的话呢那那个 vit-l 8的模型就要比这个16要贵很多因为他的序列程度增加了好讲了这么多终于到可以看结果的时候了这个表2呢是说当他已经在大规模的数据及上进行预训练了然后在这么多数据集上去做fine-tune的时候得到的表现他这里呢先对比了几个 vit 的变体 比如说 vit 的 hugemodel就是他最大的那个模型 放在这里是用来秀肌肉的因为全都取得了最好的结果然后跟卷积神经网络那边呢他就跟这个bit 和这个 noisy student做对比了跟 bit 做对比的原因呢是因为 bit 确实是之前卷积神经网络里做的比较大的而且也是因为是这个作者团队自己本身的工作所以正好拿来可以比所有之前做过的这些数据集呢bit 也都做过那至于这个noisy student 呢是因为他是 image net 之前表现最好的方法他用的方式呢是用 pseudo-label(伪标签)去进行self training也就是我们常说的用这个伪标签也取得了很好的效果首先我们可以从这个表里看出来呢vit huge 这个模型而且用比较小的patch在14 就是相对更贵一点的模型他呢能取得所有数据集上最好的结果 虽然说这里这个99.74被黑体了大概99.68也没差多少但是呢因为这些数值 比如说这里这里这些数值 都太接近了都是差零点几个点或者一点几个点没有特别大的差距作者就觉得没有展示出vision transformer的威力作者就得从另外一个角度来凸显vit 的优点那他找了一个什么角度呢他说因为我们训练起来更便宜 所以我们先来看他说的训练更便宜呢是说他们最大的这个 vithuge 的模型呢也只需要训练2,500天tpuv 3天数 正好呢这个 bit 和 noisy student 呢也都是 google 的工作 也都是用 tpu v3训练的所以刚好可以拿来比那 bit 呢是用了9,900天然而 noises student 就更不得了了那用了一万多天所以他说从这个角度上来说呢  vit不仅比你之前的这个 bit 和 nosy student要训练的快而且效果还比你好那这一下双重卖点大家就会觉得 vision transformer真的是比卷积神经网络要好那在众多数据集上秀完肌肉之后呢接下来就该做一些分析那首先第一个分析也就是最重要的vision transformer到底需要多少数据才能训练的比较好那我们来看图三 这个图3应该是整个 vision transformer论文最重要的 take home message就是他最想让你知道的其实这一张图基本就把所有的实验都快概括了这张图的意思呢就是说当你使用不同大小的数据集的时候比如说 image net 呢是1.2million而这个21 k 呢是14million 一直到最后 jft 呢是300million就是当你的数据集不断增大的时候那 resnet 和 vit 到底在image net 的 fine-tune 的时候效果如何他这个图上点有点多 所以不太好看但他的主要意思是说这个灰色 代表 bit也就是各种大小的这个 resnet  比如说这个下面呢应该就是个 res 50 上面呢应该就是152 就是从最小的然后到最大的这有这么一个范围那同样呢中间这样应该也是这是个五十 这是个152他这里想展示呢就是说在中间的这个灰色的区域就是resnet 能达到的这个效果范围那剩下的这些圆圈点了就是各种大小不一的 vision transformer我们首先来看 在最小的image net上做预训练resnet 和vision transformer 的比较如何 vision transformer呢是全面不如 resnet因为 resnet 这个效果范围在这可是 vision transformer基本所有的点都在这个范围之下就是 vision transformer在中小型数据集上去做预训练的时候呢的效果是远不如残差网络的原因呢就是因为 vision transformer没有用那些先验知识没有用那些归纳偏置所以他需要更多的数据去让这网络学的更好那接下来我们换到这个image net的21 k  用中间这一档的数据集去预训练效果如何呢我们会发现呢resnet 和 vision transformer就差不多了他们的效果呢都是在这个范围里面只有当用特别大的数据集用三个亿的图片数据集去做预训练的时候呢我们可以发现Vision Transformer 基本是全面超过 resnet 了因为你看 vision transformer范围在这但是 resnet 的范围在这首先呢就是即使是最小的这个 vit-b 32就这个蓝点 也比这个 res 50高其次呢就是当你用最大的模型 就是 vit h 14的时候呢它是比 bit 对应的 res 152还要高的那总之呢这个图其实只有两个信息一个信息呢就是说如果你想用 vision transformer那你至少得准备差不多这么大的数据集如果你只有很小的数据集呢那还是用卷积神经网络比较好第二个点呢就是当你有规模比这个数据集更大的时候呢用 vision transformer应该能给你更好的结果他的扩展性更好一些其实整篇论文讲的呢就是这个 scaling接下来呢作者又做了图四原因呢是因为在图三的时候他要用 vision transformer 去跟这个resnet 的比所以他还在训练的时候呢用了一些强约束比如说用了dropout还用了 weight decay用了 label smoothing所以就不太好分析  vision transformer这个模型本身的一些特性所以说在图四里呢他做的是这个 linear fewshotevaluation这个 linear evaluation 呢就是我一旦拿到这个预训练的模型呢我是直接把它当一个特征提取器我不去fine tune而是拿这些特征直接来做一个了just take a regression 就行了同时呢他选了 few shot然后在图里有没有看到了是 fiveshot 就是在image net上你去做这个 linear evaluation的时候呢每一类呢就随机选了5个 sample所以这个evaluation做起来是很快的作者呢也就是用这种方式去做大量的消融实验那在图四里呢这个横轴还是表示的这个预训练数据集的大小这里呢他没有用别的数据集他就用的是 jft但是呢他取了一些 jft 的子集比如说这个10million30million100million这样呢因为所有的数据都是从一个数据集里来的他就没有那么大的 distributiongap 那这样比较起来模型的效果呢就更加能体现出模型本身的特质至于结果呢其实跟图三也是差不多的说这条浅灰色的线呢是 res 50然后这条深灰色的线呢是 res 152当用很小的预训练数据集的时候呢 vision transformer呢是完全比不过 resnet 的文章里给出的解释呢因为缺少归纳偏置和那些约束的方法比如说之前说的 weight decaylabel smoothing 这些所以就导致这里的 vision transformer呢容易过拟合导致最后学到的特征呢不适合做其他任务但是随着预训练数据集的增大  vision transformer的稳健性就上来了但是因为这里的提升 也不是很明显所以说作者其实在这一段的最后也写了如何用  vision transformer 去做这种小样本的学习是一个非常有前途的方向那么接下来看图五 由于  vision transformer 这篇论文之前说了他的预训练呢比用卷积神经网络要便宜他这里呢就得做更多的实验去支持他的这个论断因为毕竟大家对transformer的印象都是说又大又贵很难训练那在图五里呢画了两个表一个表这个 average-5意思呢就是他是在五个数据集上去做了evaluation然后把这个数字呢平均了那这五个数据集呢分别是 image net real pets flowerscifar 10和 cifar 100然后因为 imagenet 太重要了所以说他把 imagenet 单独摘出来然后又画了一张表但其实这两张表的结果呢都差不多首先我们看图例 图例说一下这蓝色的这些圈呢都是 vit 然后这个灰色呢就是 resnet然后这个橙色的加号呢就指的是混合模型就是前面是卷积神经网络 后面是  transformer但是图里各种大大小小的点呢其实就是各种配置下大小不一样的这种 vision transformer 的变体 或者说是 reset 的变体这两张图里所有的模型都是在 jft 300million这个数据集上去训练的他的意思就是说呢他不想让模型的能力受限于数据集的大小所以说呢所有的这些模型都在最大的数据集上去做预训练好了我们可以看到有几个比较有意思的现象 首先第一个现象就是说如果你拿蓝色这些圈 叫 vit去跟灰色这些 resnet 的比呢你会发现在同等的计算复杂度的情况下一般transformer都是比 resnet 要好的这个呢就证明了他们所说的就是训练一个 vision transformer是要比训练一个卷积神经网络要便宜的第二个比较有意思的点呢我们可以发现在比较小的模型上呢这个混合模型的精度是非常高的他比 vision transformer和对应的 resnet 都要高这个呢倒也没什么惊讶了按道来说这个混合模型呢就应该吸收了双方的优点就是说既不需要很多的数据去预训练同时呢又能达到跟 vision transformer一样的效果但是比较好玩的就是说当随着这个模型越来越大的时候呢这个混合模型慢慢的就跟 vision transformer差不多了而且甚至还不如在同等计算条件下的一个 vision transformer这个还是比较有意思的就为什么卷积神经网络抽出来的特征没有帮助 vision transformer去更好的学习呢当然这里作者也没有做过多的解释其实怎么预处理一个图像 怎么做这个 tokenization是个非常重要的点之后有很多论文都去研究了这个问题最后呢如果我们看这个整体趋势的话呢就是随着这个模型不断的增加呢这个 vision transformer的效果也在不停增加并没有一个饱和的现象饱和的话就是一般就增加到一个平台就不增加了但是现在的这个趋势呢还是在不停的往上走的当然如果只从这个图里来看呢其实除了这个混合模型有点饱和之外呢其实卷积神经网络的这个效果呢好像也没有饱和那分析完这个训练成本以后呢作者就继续做了一些可视化他希望通过这个可视化呢能够分析一下 vit 内部的表征那如果要类似卷积神经网络那边呢那第一个要看了呢那就是第一层到底学到了什么样的特征那 vision transformer的第一层呢就是那个 linear projection layer也就是那个我们说的E那么现在来看图7 图七呢就是展示一下那个E是如何embed rgb value  他这里呢只是展示了头28个主成分其实这个 vision transformer而学到的跟卷积神经网络也很像都是这种看起来像 gabor filter 的这种有一些颜色 然后还有一些纹理 所以说呢作者说 这些成分是可以当做这个基函数的也就是说 可以去描述每一个图像块的这种底层的结构那看完了 patch embedding呢那接下来就要看一下这个positional embedding 这个位置编码是怎样工作的那么再回来看图7中间的这个图 这个图呢就画的是这个位置编码的这个相似性 相似性越高呢就是1相似性越低呢就是-1因为他做的是个cosine 的 similarity他的横纵坐标呢就分别是对应的 patch首先我们来看 如果是同一个 patch自己跟自己比那这个相似性肯定是最高的嘛所以说对于11这个 position 来说呢那这个肯定黄色点是在这呢那接下来呢我们能观察到这个学到的位置编码真的是可以表示一些这个距离的信息的 比如说假如说我们看这个 那就说离中心点越近的地方呢他就会越黄然后离中心点越远的地方呢他就会越蓝也就意味着他的相似度越低所以他已经学到了这个距离的概念同时呢我们还可以看到他还学到一些行和列之间的这个规则比如说你看这个每一个都是同行同列的这个相似度更高同行同列 同行同列也就意味着虽然他是一个1D的位置编码但他呢已经学到了2D图像的这种距离的概念所以作者在文中也说了就这也可以解释为什么他们换成2 d 的位置编码以后呢并没有得到效果上的提升是因为1D已经够用了最后呢作者就想看一下这个自注意力呢到底有没有起作用因为之所以大家想用  transformer吗就是因为自注意力这个操作呢能够模拟长距离的关系那在 nlp 那边呢就是说一个很长的句子里 抬头的一个词和结尾的一个词也能互相有关联那在图像里呢就是很远的两个像素点之间也能做自注意力这作者呢在这里就想看一下自注意力到底是不是像期待的一样去工作的那我们回到图7 看最右边的这张图这张图呢画的是这个 vit largeviti large 只有24层 所以说这个横坐标呢这个网络深度这就是从0到24那这些五颜六色的点呢就是每一层的transformer block里面的那个多头自注意力的头那对于 vit large来说呢我们一共有16个头所以其实就是说每一列呢我们其实是有16个点的他这里纵轴画的是一个叫 meanattention distance 的东西就是平均注意力的距离这个他是怎么定义的呢 假如说你图像上有两个点那 a   b那这个 mean tension distance 呢假如说我们用的 d 来表示 d a b 呢就相当于是 l a b l a b就是这两个点之间真正的 pixel 之间差的距离乘以他们之间的 attention weights因为自注意力是全局在做所以说每个像素点跟每个像素点都会有一个这个自注意力权重那这个 d ab 的大小呢就能反映这个模型到底能不能注意到很远的两个 pixel那这个图里的规律呢还是比较明显的这假如说我们先看头几层 那头几层呢就说有的自注意力头注意的距离还是挺近的这可能20个 pixel但是有的头呢能到120个 pixel所以这也就证明了自注意力真的能在网络最底层就最刚开始的时候就已经能注意到全局上的信息了而不是像卷积神经网络一样刚开始第一层的是 receptive field (感受野)非常小就只能看到附近的这些 pixel那随着网络越来越深呢网络学到的这个特征呢也会变得越来越 high level就是越来越具有语义信息所以说我们别人看到了大概就是网络的后半部分呢他的这个自注意力与距离呢都非常远了也就意味着他已经学到这个带有语义性的概念而不是靠临近的这个像素点去进行判断那为了验证这一点呢作者又画另外一个图 就是图6他这里面呢是用网络最后一层那个output token去做的这些图他发现如果用输出的 token这个自注意力折射回原来的这个输入图片呢我们其实可以看到他真的学到了这些概念 比如说第一个里面这个狗 他其实已经注意点这个狗了 这个飞机 飞机那作者最后一句话写 对于全局来说因为你这个输出token是融合了所有的信息这个全局的特征他们就发现模型已经可以关注到那些图像区域哪些区域呢就是跟最后分类有关的区域在文章到最后 作者还做了一个尝试就是如何用自监督的方式去训练一个 vision transformer这篇论文 其实如果算上附录的话呢一共有22页那在这么多结果中呢作者把别的结果都放到了附录里而把这个自监督放到了正文里可见他的重要性他之所以重要是因为在 nlp 领域呢 transformer这个模型确实起到了很大的推动作用但另外一个真正让 transformer能火起来另外一个成功的因素呢其实是大规模的自监督训练这两个是缺一不可的那之前我们说过nlp 那边的这个自监督呢无非就是完形填空或者预测下一个词因为这篇论文整体都仿照的是 bert所以他就想说我能不能也借鉴 bert是这个目标函数去创建一个专属于 vision 的目标函数那 bert 呢用的其实就是完形填空了也就叫 mask language modeling就是一个句子给你然后你把一些词 mask 掉然后你通过一个模型最后再去把它预测出来那同样的道理呢这篇论文就搞了个 maskedpatch prediction意思就是说我有一张图片然后已经画成 patch 了而这时候我把某些 patch 随机抹掉通过这个模型以后呢你再把这个patch给我重建出来这其实真的是很激动人心的因为他不论是从模型上还是从目标函数上cv 和 nlp 真的就做到大一统但是呢他们这个模型他说最后 比如说 vit base一个patch size16的模型呢 在imagenet上只能达到大概80的准确率虽然呢相对于从头来训练这个 vision transformer呢他已经提高了两个点但是跟最好的这种就是有监督的训练方式比呢差了四个点然后作者说 他把跟对比学习的结合呢当做是future work 了因为对比学习其实去年 cv 圈最火的一个 topic是所有自监督学习方法里表现最好的所以紧接着 vit 呢MoCo v3就出来了 还有DINO 也出来 这两篇论文都是用 Contrastive去训练了一个 vision transformer最后 我们来回顾总结一下这篇 vision transformer的论文这篇论文写的还是相当简洁明了的他在有这么多内容和结果的情况下呢做到了有轻有重把最重要的结果都放在了正文里图和表做的也都一目了然那从内容上来说呢 vision transformer真的是挖了一个很大的坑你可以从各个角度去分析他或者提高他推广他比如说如果从任务角度来说呢 vision transformer只做了分类那其实还很自然那你就可以拿他去做检测分割和甚至别的领域的任务那如果从改变结构的角度来说呢那你可以去改刚开始的tokenization你也可以改中间的这个 transformer block因为后来就已经有人把这个 self attention 呢换成 mlp而且还是可以工作的很好然后就在几天前呢甚至有一篇论文叫 mataformer他觉得 transformer真正 work 的原因呢是因为 transformer这个架构而不是某些特殊的算子所以他就把这个self attention上呢直接换成了一个池化操作然后他发现呢用一个甚至不能学习的池化操作也就是他提出这个poll former的这个模型呢也能在视觉领域取得很好的效果所以说在模型的改进上呢也大有可为而如果从目标函数来讲呢那就更是了你可以继续走有监督然后也可以尝试很多不同的这个自监督训练的方式最后更重要的呢是他打通了 cv 和 nlp 之间的鸿沟挖了一个更大的多模态的坑那接下来大家还可以用它去做视频去做音频甚至还可以去做一些基于touch的这种信号就各种modality的信号都可以拿来用在我看来呢卷积 自注意力或者 mlp究竟鹿死谁手还犹未可知我是很期待下一个即将出现的一个改进版的 vision transformer还有可能真的就是一个简洁高效通用的视觉骨干网络而且可以完全不用任何标注信息