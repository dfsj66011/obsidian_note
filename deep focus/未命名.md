## 教会 LM 适应工具

随着我们对大型语言模型（LLMs）了解的加深，它们变得越来越有趣。这些模型能够准确地解决各种复杂的任务。然而，与此同时，它们却在某些我们认为人类理所当然具备的功能上表现挣扎！例如，LLMs 常常在算术运算中出错，无法获取最新信息，甚至难以理解时间的推移。鉴于这些局限性，我们可能会想，有什么办法可以让 LLMs 变得更加能干呢？LLMs 是否注定要永远受这些局限性的困扰？

人类的许多进步都是由接触新的创新工具（例如印刷机或计算机）所推动的。同样的情况是否也适用于大型语言模型（LLMs）呢？在这篇概述中，我们将研究一种旨在教会LLMs如何使用外部工具的最新研究方向，这些工具通过简单的文本到文本API提供。借助这些工具，LLMs可以将执行算术运算或查找最新信息等任务委托给专门的工具。然后，该工具返回的信息可以在生成输出时作为上下文供LLM使用，从而产生更准确和可靠的响应。

### 让大语言模型更强大

让大语言模型（LLM）访问外部工具是解决这些模型所面临部分局限性的可靠方法。然而，大语言模型不会自然地知道如何使用工具，这就引出了一个问题：我们如何教会模型利用外部工具？在本节中，我们将探讨一些可用的选项，并列举各种对构建大语言模型应用程序有用的工具。

### 不同类型的学习

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png)

教大语言模型（LLM）利用工具与学习如何用大语言模型解决其他任务并无不同。由于这些模型的学习方式有多种，我们将在这里介绍利用大语言模型学习的主要形式。除了本文之外，网上还有详细的解释可供参考。

**预训练**    大语言模型（LLMs）最基础且首要的学习形式是预训练。在预训练阶段，模型会使用语言建模目标，在大量未标注的文本语料上进行训练。预训练过程从随机初始化开始，计算成本相当高。通常，由于计算成本高昂，预训练只需进行一次——我们不想频繁重复预训练过程！值得注意的是，预训练的高计算成本解释了像 ChatGPT 这类大语言模型存在知识截止日期的原因。这些模型的所有知识都是在预训练阶段习得的，因此知识截止日期仅与最近一次预训练运行时所使用的数据相关。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a47baaf-62c9-4768-b333-22365389cb48_1762x914.png)


**微调**    在预训练之后，大语言模型（LLMs）可以准确地执行下一个词元预测，但这并不总是意味着它们实际上是有用的。例如，如果我们试用一下 GPT-2 的演示版本两分钟，我们马上就会发现，准确预测下一个词元可能会产生一些相当乏味且无用的输出！因此，我们通常会在预训练之后对大语言模型进行微调，可以通过有监督微调（SFT）或基于人类反馈的强化学习（RLHF）来实现；有关详细信息，请参见上图和此处。尽管这些技术的细节超出了本文的范围，但基本思路是：

1. 策划更多的训练数据（例如，针对我们想要解决的任务的领域内数据、正确对话的示例、对大语言模型输出的人工反馈等）。
2. 使用强化学习或带有（自）监督目标的梯度下降法，在这些新数据上训练模型的参数。

通过这样做，我们能取得相当大的成果！例如，利用基于人类反馈的强化学习（RLHF）[11] 对大语言模型（LLM）进行微调已被证明能让大语言模型更有趣、更贴合事实且更有帮助。更进一步，Meta 最近发表的 LIMA 论文表明，仅对 1000 条高质量的对话示例进行监督微调（SFT），就能训练出一个与 GPT-4 [12] 质量相媲美的模型。简单来说，微调能让通用的大语言模型变得真正独特且有实用价值。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abadc37-6ed1-482f-8ea2-fbf8bfa60615_1924x962.png)

**上下文学习**    我们应该了解的最后一种学习形式是上下文学习；见上文。上下文学习与预训练和微调不同，它实际上并不修改底层模型的参数。相反，我们通过修改提示语来教会大语言模型更有效地解决问题！特别是，我们可以通过使用特定的提示技巧改写提示语，甚至将数据插入提示语中进行少样本学习。下图展示了微调与上下文学习之间的区别。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F123f7cf3-40de-41ad-9a1c-157821bd0b7c_1528x1348.png)

上下文学习能力极其强大，因为它使我们能够使用单一模型解决各种不同的任务。我们无需对模型进行微调或修改其底层参数，只需将有用数据插入到大型语言模型的提示中。该模型可以从这些数据中学习，并在无需修改模型本身的情况下更准确地完成任务！此外，我们既可以对预训练模型也可以对微调后的模型进行上下文学习。要了解可用于大型语言模型的提示技巧，请查看以下概述。

### 哪些工具对大语言模型有用？

虽然将大语言模型（LLMs）与外部工具连接起来的想法似乎很有吸引力，但我们可能会想：_哪些工具最有用呢？_为了回答这个问题，我们应该着眼于大语言模型的常见局限性，比如：

- 无法获取最新信息[2]
- 倾向于产生幻觉（即输出错误信息）
- 评估数学表达式的困难
- 对低资源语言理解不充分
- 无法理解时间的推移 [8]

如果我们想要解决这些问题，我们有几个选择。我们可以通过监督微调（SFT）或基于人类反馈的强化学习（RLHF）来专注于对模型进行微调和优化——只需对模型进行广泛微调，以避免上述行为。事实上，人们已经投入了大量资源，通过有针对性的人类反馈来优化像 GPT-4 这样的模型，并且取得了相当令人印象深刻的结果。然而，我们不必在模型本身内部解决这些问题，而是可以专注于对模型进行微调，使其采用一种间接的方法，但这种方法往往更可靠。具体来说，我们可以教模型如何使用外部工具来辅助回答问题！

工具如何提供帮助？在努力解决问题时，大语言模型（LLM）查询能够提供更多背景信息的外部工具往往很有用。一些实用工具的显著例子包括但不限于：

- 能够返回当前日期的日历应用程序
- 能够计算数学表达式的计算器
- 存储（可能）相关信息但因其过大而无法直接存储在提示中的向量数据库
- 翻译模块用于将数据转换为不同的语言

总体而言，只要提供额外信息或背景知识有助于大语言模型解决问题，工具就极其有用。除了这些简单的工具之外，我们甚至可以将大语言模型与外部代码解释器相连，使其具备编写和执行任意程序的能力。当与支持代码的大语言模型（例如 Codex[10]）相结合时，这种方法实际上会非常强大！

### 工具超级受欢迎！

尽管本概述主要聚焦于研究工具与大型语言模型集成的最新研究，但利用外部工具增强像 GPT-4 这样的模型近期已成为一个受关注的话题。例如，OpenAI 最近为其模型推出了插件扩展功能，使这些强大的大型语言模型能够利用大量外部工具；详见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3211fdac-97bc-4bcf-9f1e-32ce3758ad44_2108x1052.png)

截至撰写本文时，GPT-4 已有近 130 款不同的插件可供使用，这充分表明了人们将各类工具与强大的大型语言模型集成的浓厚兴趣。除了第三方插件之外，OpenAI 最近还为 GPT-4 推出了代码解释器和互联网搜索工具。互联网搜索工具对于减少大型语言模型中的幻觉现象极为有用，因为模型给出的答案可以通过从互联网获取的相关且最新的信息进行情境化处理。除了让大型语言模型更具事实性和可靠性之外，代码解释器工具还能够处理大量的代码和数据文件，并对这些数据进行精准分析，从而得出有价值的见解。

**TL;DR：** 这里的主要结论是，工具正成为大语言模型（LLM）的常见特性。除了 OpenAI 的产品外，我们甚至看到像 Bard 这样的模型也在添加类似功能，而像 LangChain 这样的开源库可以轻松地为现有的大语言模型构建各种类似工具的功能。

---

### 教大语言模型使用工具

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb430281-e4d5-4b7e-b6ea-5b76ecba8992_758x1054.png)

在[1]中，作者探索了一种名为 Toolformer 的方法，该方法 i) 教会大语言模型（LLM）如何利用外部工具，以及 ii) 在此过程中保持 LLM 的通用性。这些工具通过一组简单的文本到文本应用程序编程接口（API）提供给 LLM（即模型提供文本作为输入，API 返回文本输出）。有趣的是，我们在[1]中发现，LLM 能够以完全端到端的方式学会利用这些工具。该模型自行决定调用哪些 API、向这些 API 传递哪些参数，以及如何最好地使用返回的信息，而无需任何硬编码的控制流程。

> “语言模型可以学会控制各种工具，并自行选择何时以及如何使用哪种工具。” —— 引自[1]

为此，我们整理了一个训练数据集，以展示这些工具的正确使用方法。在文献[1]中，这个数据集是通过一种自监督启发式方法自动创建的——这意味着不需要人工干预——并且只需要每个工具的几个使用示例。然后，我们在这个数据集上对大型语言模型（LLM）进行微调，使其学会正确使用每个工具。最终得到的是一个高性能的大型语言模型，它可以将简单但困难的小任务（例如，语言翻译、算术运算、获取当前信息等）委托给专门的、外部工具，这些工具会返回相关且准确的数据，供大型语言模型用于生成输出。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9396abd-34ec-4a58-bdca-4e2548476f03_2144x700.png)

使用了哪些工具？在[1]中，Toolformer 使用以下固定的工具集：

- 问答工具：基于Atlas [13]，这是一个经过微调的大型语言模型，用于回答简单的、基于事实的问题。
- 计算器：一个用于数值运算的基本计算器。
- 维基百科搜索工具：一种搜索引擎，可根据给定的搜索词返回来自维基百科的简短文本片段。
- 翻译器：一种语言翻译系统，能将任何语言的文本翻译成英语（但不能反向翻译！）
- 日历：一种工具，在被查询时仅返回当前日期。

这些工具均通过一个简单的文本 - 文本结构的API提供使用，详见上文。要使用这些工具，大语言模型必须学会：i) 识别需要使用工具的场景；ii) 明确指定使用哪种工具；iii) 向工具的API提供相关的文本输入；iv) 利用API返回的文本来构建回应。值得注意的是，这些API简单的文本 - 文本结构使我们能够轻松地将工具使用的示例直接插入到文本序列中，详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb823eb83-0440-4e84-9cab-42aa049bd722_1368x366.png)
对外部 API 的调用被格式化为文本，并与现有的文本序列内联放置。

与先前工作相比的改进。让大语言模型（LLMs）使用外部工具并非新想法。举个简单的例子，许多研究人员尝试通过让大语言模型使用外部计算器来提高其算术能力，尤其是处理大数字时的能力（详见[4]的附录B）。然而，主要问题在于：我们应该如何教大语言模型使用这类工具？先前的方法在很大程度上依赖于人工标注的数据集。例如，LaMDA [3] 使用外部搜索工具来减少幻觉现象；详见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1dcf49f-3885-45ef-a422-cd91bd648f26_1276x1288.png)

然而，在[3]中我们看到，要教会 LaMDA 利用外部工具——在这个例子中是一个外部信息检索系统——需要大量人工标注的数据。更具体地说，[3]的作者让大量众包工作者手写对话，在这些对话中他们使用与LLM相同的搜索工具，从而提供了LLM应如何表现和回应的示例。相关出版物往往依赖于类似以人为中心的方法[2]。创建这样一个数据集既困难又昂贵且耗时，这促使[1]的作者开发了一种更高效的解决方案。

学习自动使用工具。在文献[1]中，我们看到可以通过一种提示方法（该方法利用现有的预训练大语言模型）自动生成用于教导大语言模型如何利用外部工具的数据集——为简便起见，我们将这种数据集称为“工具遵循数据集”。我们从初始（常规）数据集开始，例如用于预训练的文本语料库。然后，我们提示一个预训练的大语言模型用外部API调用扩充这些数据。在这里，我们依靠通用预训练大语言模型的上下文学习能力来整理一组API调用，这些调用展示了如何正确使用可用工具。下面展示了一个生成对问答工具API请求的示例提示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe24f1112-591c-4cc5-845e-2b01391194b1_752x1076.png)

在我们用每个工具的示例用法扩充了数据集之后，我们需要执行一个过滤步骤。这一步是必要的，因为我们只有在某个外部工具确实对大语言模型有帮助时才想使用它！即使不需要外部工具，我们也不应总是依赖它们——使用工具通常会产生延迟（甚至是金钱）成本。为了体现这一理念，我们可以直接这样做：

1. 使用该工具测量大语言模型的性能（即，API调用后出现的标记的交叉熵损失）。
2. 在不使用工具的情况下评估大语言模型的性能。
3. 丢弃那些使用工具后大语言模型的性能提升未超过一定阈值的示例。

在这里，我们假设可以访问一个数据集，该数据集展示了大型语言模型（LLM）应产生的正确输出。通过采用这种方法，我们可以自动构建一个包含示例的数据集，这些示例展示了何时以及如何利用工具来切实改进LLM的输出。在实际操作中，具体流程要复杂一些。具体来说，为了衡量不使用工具时LLM的性能，我们在两种不同情况下观察其表现——一种是完全不使用工具的情况，另一种是进行API调用但未提供响应的情况。这种方法确保了工具及其数据对LLM是有用的。

> “如果提供某个 API 调用的输入和输出能让（大语言模型）更容易预测后续的标记，那么这个API调用就是有帮助的。” ——引自[1]

此外，我们没有将API调用直接插入到文本序列中，而是将其作为前缀添加，这避免了大型语言模型（LLM）损失出现峰值。请记住，像这样的API调用在LLM的原始预训练语料库中并不存在，这意味着直接将API调用插入到文本序列中可能会使用于过滤的结果产生偏差。模型并不期望在数据中看到这样的API调用！此外，在衡量性能时，我们会为与API调用在空间上接近的标记分配更高的权重，以确保API调用在需要的地方附近进行，而不是在生成输出时的随机时间进行。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37240b64-3d51-4e81-b581-c94d03d4132f_1232x414.png)

上述过程展示了构建[1]中所用工具跟随数据集的完整流程。与以往的研究不同，这一过程无需人工参与。相反，我们利用大型语言模型的上下文学习能力以及一些巧妙的启发式方法来自动构建数据集。尽管这一过程并非完美无缺（即，一些无用的 API 调用可能会避开过滤），但在实践中效果相当不错！

学习使用工具。一旦我们构建了一个数据集，教大型语言模型（LLM）如何利用外部工具就变得简单了——我们只需使用标准的 language modeling objective 对该数据集进行微调。在文献[1]中，遵循工具使用的数据集是从预训练语料库中派生出来的。因此，经过微调的 LLM 仍然是一个通用模型，尽管它具备了利用外部工具的能力。此外，由于文献[1]中的过滤过程移除了那些对性能没有提升的 API 调用，LLM 会隐式地学习何时以及如何使用每个工具来改善其自身的输出。对于这样一种简单的方法来说，结果相当不错！

### 工具会有影响吗？

[1] 中分析的模型基于 [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b)，这是一个拥有 60 亿参数的语言模型，采用 [CCNet](https://github.com/facebookresearch/cc_net) 作为训练数据集。Toolformer 与几个基准进行比较，包括禁用 API 调用的 Toolformer 模型、原始 GPT-J 模型、在 CCNet 上微调的 GPT-J 版本，以及其他一些大型语言模型，如 [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15) 和 [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)。与以往研究少样本学习的方法不同，这些模型使用[零样本方法](https://cameronrwolfe.substack.com/i/117151147/zero-shot-learning)进行评估，即仅向模型描述任务而不提供任何示例，并采用[贪婪解码](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20)策略。在 Toolformer 中，每当 `<API>`（即 API 调用的起始标记）出现在模型最可能的 `k` 个标记之一时，就会利用工具。

Toolformer 在多个不同领域进行了评估。在基于事实的数据集上，我们发现问答工具得到了充分利用，使得准确率相较于基线模型有了大幅提升。同样地，在数学推理数据集上，计算器工具也被证明非常有用；详见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25d8534e-8d0c-469c-80c7-ceebbe066038_2020x676.png)

在（多语言）问答基准测试中，该模型的表现并不十分出色（即Toolformer在某些情况下不如GPT - 3或GPT - J的表现）。然而，某些工具（如日历工具）被发现对提高大型语言模型在时间推理等任务上的性能非常有用。有趣的是，作者还进行了一些分析，这些分析改变了大型语言模型解码策略中API调用的概率。通过这一分析，我们了解到更频繁地利用外部工具并不总是好事——如果工具使用过于频繁，性能就会下降；具体情况见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43dbbce7-98f6-41e0-b34c-835c0fa5e729_1384x896.png)

这一发现凸显了[1]中所采用的过滤策略的重要性。工具的使用不仅伴随着成本，还可能会降低性能。大语言模型必须学会理解在哪些场景下调用工具最为重要。[1]所采用的方法明确引导大语言模型仅在调用外部工具能显著提升模型性能时才加以利用。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff33b5c92-773a-4f9a-aaf7-05965d372eb2_1048x548.png)

保持通用性。除上述下游评估外，[1]中的作者在工具遵循数据集上进行微调后，对预训练数据集的一个保留部分评估了 Toolformer，发现该模型在微调前后的困惑度相当；详见上文。换句话说，当Toolformer 学会利用外部工具时，它作为通用语言模型的能力并未丧失，这意味着——与之前以特定任务方式处理工具遵循问题的研究[8]不同——该模型仍然是一个基础模型，可以解决各种不同的任务。

### Using Tools is Getting Easier

Although the approach proposed in [1] is groundbreaking and incredibly informative, it still requires an extensive fine-tuning process. Compared to most recent applications of LLMs, this is quite a hassle! _Is it possible that we could leverage a prompting-only approach to teach an LLM to leverage external tools?_ Recent developments surrounding GPT-4 suggest that this problem might be solved by improving the [instruction following capabilities](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting) of LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cdade12-c78e-4203-bc6c-51dc10b7f4d9_1652x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cdade12-c78e-4203-bc6c-51dc10b7f4d9_1652x596.png)

A simple recipe for leveraging search tools with GPT-4

**GPT-4 plugin workflow.** As an example, GPT-4 has access to a variety of different tools via the [plugin store](https://openai.com/blog/chatgpt-plugins). However, the model is not explicitly fine-tuned to learn about each plugin within the store. Rather, it just uses in-context learning. In particular, OpenAI has invested heavily ubto improve GPT-4’s [steerability](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20), which has made the model surprisingly capable of following very detailed instructions and prompts. As a result, teaching GPT-4 how to use a plugin only requires:

1. A textual description describing the plugin’s purpose
    
2. A schema describing the input/output format for the plugin’s API
    

Using this information, the model can determine when to use a plugin on its own, make properly-formatted API calls, and integrate the resulting information into its dialogue. All of this is done purely via textual descriptions without any explicit fine-tuning, _revealing that teaching LLMs to leverage external tools is likely to become easier over time_. To understand this process in more detail, we can look at [open-source plugin implementations](https://github.com/openai/chatgpt-retrieval-plugin) or [developer documentation](https://platform.openai.com/docs/plugins/introduction/plugin-flow) for OpenAI plugins.

# Closing Remarks

Similar to how humans become better with access to tools (e.g., hammers, computers, planes, etc.), LLMs become more capable when given access to a set of simple APIs that can provide useful information or perform simple tasks for them. _Why would we rely 100% on an LLM to solve everything, when we can delegate difficult tasks to a more accurate and specialized tool?_ We can use such an approach to mitigate problems that are constantly encountered with these models, such as incorrect information within the output or a lack of temporal reasoning skills. With Toolformer [1], we see than LLMs can be taught to leverage external tools via fine-tuning over a dataset of tool-following exemplars. But, recent trends suggest that teaching LLMs to use external tools might be possible via in-context learning alone. There is a lot to be uncovered in this area, and it will be interesting to watch these topics and related applications evolve over time!

### New to the newsletter?

Hello! I am [Cameron R. Wolfe](https://cameronrwolfe.me/), Director of AI at [Rebuy](https://www.rebuyengine.com/) and PhD student at Rice University. I study the empirical and theoretical foundations of deep learning. This is the Deep (Learning) Focus newsletter, where I help readers build a better understanding of deep learning research via understandable overviews that explain relevant topics from the ground up. If you like this newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

### Bibliography

[1] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." _arXiv preprint arXiv:2302.04761_ (2023).

[2] Komeili, Mojtaba, Kurt Shuster, and Jason Weston. "Internet-augmented dialogue generation." _arXiv preprint arXiv:2107.07566_ (2021).

[3] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." _arXiv preprint arXiv:2201.08239_ (2022).

[4] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[5] Wang, Ben, and Aran Komatsuzaki. "GPT-J-6B: A 6 billion parameter autoregressive language model." (2021).

[6] Zhang, Susan, et al. "Opt: Open pre-trained transformer language models." _arXiv preprint arXiv:2205.01068_ (2022).

[7] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[8] Parisi, Aaron, Yao Zhao, and Noah Fiedel. "Talm: Tool augmented language models." _arXiv preprint arXiv:2205.12255_ (2022).

[9] Dhingra, Bhuwan, et al. "Time-aware language models as temporal knowledge bases." _Transactions of the Association for Computational Linguistics_ 10 (2022): 257-273.

[10] Chen, Mark, et al. "Evaluating large language models trained on code." _arXiv preprint arXiv:2107.03374_ (2021).

[11] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[12] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[13] Izacard, Gautier, et al. "Atlas: Few-shot learning with retrieval augmented language models." _arXiv preprint arXiv_ 2208 (2022).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![ASYMODO's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F13d1dfb4-535c-4192-b1bf-f6d40aac5f4a_636x276.jpeg)



](https://substack.com/profile/93198136-asymodo)

[

![Web Raccoon's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8951259b-32ab-4a51-8b4b-cd6712c0b227_400x400.jpeg)



](https://substack.com/profile/33177685-web-raccoon)

[

![Lazaro  Hurtado's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd090f3a8-f320-4a21-aa62-dd22ba4b707b_144x144.png)



](https://substack.com/profile/100785516-lazaro-hurtado)

[

![darlin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29e363b8-0457-4b09-b998-d7799cc67d0d_2265x2265.png)



](https://substack.com/profile/127381235-darlin)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

15 Likes∙

[2 Restacks](https://substack.com/note/p-123558334/restacks?utm_source=substack&utm_content=facepile-restacks)

15

- 

[

2

](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Elton's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Forange.png)



](https://substack.com/profile/99153282-elton?utm_source=comment)

[Elton](https://substack.com/profile/99153282-elton?utm_source=substack-feed-item)

[2023年5月29日](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools/comment/16665253 "2023年5月29日 18:03")

Liked by Cameron R. Wolfe, Ph.D.

I really love your articles. You have an ability to break big complex topic with very digestible language. Keep it up

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools/comment/16665253)

[1 more comment...](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


------------


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Language Models and Friends: Gorilla, HuggingGPT, TaskMatrix, and More

### What happens when we give LLMs access to thousands of deep learning models?

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jun 05, 2023

12

- 

[](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla/comments)

1

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)

This newsletter is presented by [Cerebrium](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023). Cerebrum allows you to deploy and fine-tune ML models seamlessly, without worrying about infrastructure. Notable features include serverless GPU deployment (<1 second cold start), access to 15+ pre-trained models (e.g., FLAN-T5, GPT-Neo, Stable Diffusion and more), and support for all major ML frameworks. [Try it](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023) for free today! 

Enjoy deep learning? Find current research topics difficult to parse? Join subscribers from Microsoft, Tesla, Google, Meta, Salesforce and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [My contact info](http://cameronrwolfe.me/)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24353a75-cad3-4e65-947d-374db36d9255_1942x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24353a75-cad3-4e65-947d-374db36d9255_1942x1182.png)

(from [2, 3])

Recently, we have witnessed a rise of foundation models to popularity within deep learning research. Pre-trained large language models (LLMs) have led to a new paradigm, in which a single model can be used—with surprising success—to solve many different problems. Despite the popularity of generic LLMs, however, fine-tuning models in a task-specific manner tends to outperform approaches that leverage foundation models. Put simply, _specialized models are still very hard to beat_! With this being said, we might start to wonder whether the powers of foundation models and specialized deep learning models can be combined. Within this overview, we will study recent research that integrates LLMs with other, specialized deep learning models by learning to call their associated APIs. The resulting framework uses the language model as a centralized controller that forms a plan for solving a complex, AI-related tasks and delegates specialized portions of the solution process to more appropriate models.

> _“By providing only the model descriptions, HuggingGPT can continuously and conveniently integrate diverse expert models from AI communities, without altering any structure or prompt settings. This open and continuous manner brings us one step closer to realizing artificial general intelligence.”_ - from [2]

# Background

Before exploring how language models can be integrated with other deep learning models, we need to cover a few background ideas, such as LLM tools, information retrieval, and self-instruct [11]. For more generic background information on language models, check out the following resources.

- Language Modeling Basics (GPT and GPT-2) [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
    
- The Importance of Scale for Language Models (GPT-3) [[link](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]
    
- Modern [[link](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)] and Specialized [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)] LLMs
    
- Basic [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)] and Advanced [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)] Prompt Engineering
    

### Using tools with LLMs

> _“By empowering LLMs to use tools, we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.”_ - from [3]

Although language models have a large number of impressive capabilities, they are not perfect and can’t accomplish all tasks on their own. In many cases, combining existing models with tools (e.g., search engines, calculators, calendars, etc.) can drastically expand the scope of their capabilities. In a prior overview, we explored the Toolformer [1]—a fine-tuning technique for teaching LLMs to use a small set of simple, textual APIs—and how tools can be used to improve the performance of LLMs without too much effort; see below for more details.

[The Toolformer](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools)

Although models like the Toolformer are effective, they only consider a small set of very simple APIs. These APIs barely scratch the surface of the total number of tools that can be made available to LLMs. Imagine, for example, if we were able to integrate an LLM with any API that is available via the internet—_we could unlock an entire realm of new applications and possibilities_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3211fdac-97bc-4bcf-9f1e-32ce3758ad44_2108x1052.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3211fdac-97bc-4bcf-9f1e-32ce3758ad44_2108x1052.png)

Popular apps on the ChatGPT Plus Plugin store

**(almost) anything is possible!** This trend towards providing language models with widespread access to a variety of APIs online is being explored via the ChatGPT plugins store; see above. By leveraging these APIs, we can do much more than just provide LLMs access to simple tools like calculators! We can easily think of several high-impact applications that become possible with this approach. For example, we could use language models with tool integrations for:

- Forming a vacation itinerary and booking all needed tickets and reservations
    
- Curating and purchasing the week’s grocery list for curbside pickup
    
- Finding and reserving a table at a restaurant for the upcoming weekend
    
- Discovering and purchasing relevant products on any e-commerce store
    

The scope of possibilities is nearly endless! By using language as a standardized medium of communication, we can work with foundation models like ChatGPT to accomplish surprisingly complex tasks. All we have to do is [prompt the model](https://cameronrwolfe.substack.com/i/123558334/using-tools-is-getting-easier) to produce the API calls that are relevant to our request.

**deep learning APIs.** In this overview, we will consider integrating LLMs with a particular kind of API—those that provide access to open-source deep learning models on platforms like [HuggingFace](https://huggingface.co/). The AI/ML community places a heavy emphasis on open-source software, meaning that many of the most powerful deep learning models are freely available online. Usually, these models come with well-written descriptions, called [model cards](https://huggingface.co/docs/hub/model-cards), that can be used to provide all needed information about any model to an LLM. Then, these models can be easily integrated with an LLM via basic prompting techniques.

### The Self-Instruct Framework

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)

(from [11])

The self-instruct framework, proposed in [11], pioneers the idea of using LLMs to train themselves by generating synthetic [instruction tuning](https://twitter.com/cwolferesearch/status/1652064977493057545?s=20) data that can be used for fine-tuning. Beginning with a single input-output pair associated with a certain task or instruction, self-instruct prompts an off-the-shelf LLM to generate new tasks, as well as valid instructions and responses to go with each of them. After filtering is performed to remove low-quality data, we can fine-tune any language model on the resulting instruction tuning dataset. Interestingly, we find that models fine-tuned over this data tend to match the performance of those trained over human-curated datasets. Although self-instruct works well, several improvements to the overall framework were also proposed by [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) [12].

[Self-Instruct 2.0](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)

### Information Retrieval

As we have seen in prior overviews, the quality of foundation language models tends to [improve with scale](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)—large pre-training datasets and models lead to the best results. However, we can only store so much information within the fixed set of weights contained within a language model. _Even massive models have a finite number of parameters_. Additionally, the limited context window of modern LLMs limits us to injecting only a small amount of information into the model’s prompt.[1](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla#footnote-1-125726849) So, _what should we do if we want to provide our LLM access to a large bank of information_? We need to adopt some form of information retrieval.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e1cebd-b824-4e13-af98-4c226cb6a7e9_992x978.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e1cebd-b824-4e13-af98-4c226cb6a7e9_992x978.png)

(from [13])

**vector databases.** One popular form of information retrieval can be performed by integrating an LLM with a vector database that stores large amounts of textual information. At a high level, this integration with a vector database (e.g., [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [Weaviate](https://weaviate.io/), [Redis](https://redis.io/docs/stack/search/reference/vectors/), etc.) is formed by:

1. Chunking the text into small parts.
    
2. Producing an [embedding](https://platform.openai.com/docs/guides/embeddings) for each chunk of text.
    
3. Storing these embeddings in a vector database.
    
4. Performing [vector similarity search](https://www.pinecone.io/learn/what-is-similarity-search/) (based on these embeddings) to find relevant chunks of text to include in a prompt.
    

The net result is that we can quickly find relevant textual information to provide as extra context within a prompt, allowing the LLM to draw upon information beyond the maximum size of its [context window](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering). Even though we still cannot provide all of the information we have to the model, we can use vector similarity search to quickly identify the most relevant parts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c2223ce-8aa1-4a40-92e7-d2a91ee705ac_900x924.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c2223ce-8aa1-4a40-92e7-d2a91ee705ac_900x924.png)

(from [14])

**is this the only way?** Many other methods have been proposed for information retrieval—there is an entire (incredibly active) area of research dedicated to these techniques. We can even use LLMs to generate relevant information (instead of retrieving it) via [generated knowledge prompting](https://cameronrwolfe.substack.com/i/118401596/knowledge-augmentation); see above. The article below does a great job of summarizing existing techniques for information retrieval.

[Info Retrieval Survey](https://jxmo.io/posts/retrieval)

Overall, many different techniques exist, and we have a lot of options for choosing how an LLM could be augmented with external sources of information.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a9a9574-1724-4230-8370-e4dc0c51d2cf_2110x652.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a9a9574-1724-4230-8370-e4dc0c51d2cf_2110x652.png)

(from [3])

**why should we care?** Information retrieval is great, but we might be wondering why this is relevant to the topic of integrating LLMs with other deep learning models. Well, _what if we want to provide access to any model available online?_ There are thousands of models available on ML communities like HuggingFace! As such, we can’t provide a description for all of these models to the LLM. Rather, we need to use some form of information retrieval to determine the most relevant subset of models that we should include in the LLM’s prompt; see above.

# Integrating LLMs with Other Models

Now that we have some relevant background information under our belt, we will take a look at recent publications that augment LLMs with other deep learning models. Although each approach is different [2, 3], all of such techniques aim to teach an LLM how to call APIs associated with other, specialized models. Then, the LLM can act as a controller (or brain) that coordinates the solution of a problem by planning and delegating subtasks to different APIs.

### [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580) [2]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F469e1fa3-1ff3-48f3-a459-00210aa6fba6_1186x1414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F469e1fa3-1ff3-48f3-a459-00210aa6fba6_1186x1414.png)

(from [2])

LLMs have become popular recently, but research in deep learning has produced a variety of incredibly useful models in recent years for solving specific tasks like image recognition, video action detection, text classification, and much more. Unlike language models[2](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla#footnote-2-125726849), these models are narrow experts, meaning that they can accurately solve a specific task given a fixed input-output format. But, they are not useful for anything beyond the specific task that they are trained to solve. _What if we want to repurpose these models as components for solving more open-ended AI-related tasks?_

> _“LLMs [can] act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this.”_ - from [2]

In [2], authors explore using LLMs as a generic interface for connecting deep learning models together. HuggingGPT [2] is inspired by the idea of allowing an LLM to coordinate with external models with a variety of different specialized capabilities. Put simply, the LLM serves as the “brain” of a problem solving system, which plans how to solve a problem and coordinates efforts between different deep learning models that solve necessary subtasks for this problem. To teach an LLM how to do this, we need high-quality descriptions of each model. Luckily, we don’t need to perform any prompt engineering to create these descriptions—they are widely available via ML communities like HuggingFace!

[HuggingFace Model Cards](https://huggingface.co/docs/hub/model-cards#what-are-model-cards)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b6ea81c-e5fa-4b26-a1cb-76e2455a0085_1872x1124.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b6ea81c-e5fa-4b26-a1cb-76e2455a0085_1872x1124.png)

(from [2])

**how does this work?** HuggingGPT [2] decomposes problems into four parts:

- _Task planning:_ use the LLM to decompose a user’s request into solvable tasks.
    
- _Model selection_: select models from HuggingFace to use for solving tasks.
    
- _Task execution_: run each selected model and return results to the LLM.
    
- _Response generation_: use the LLM to generate a final response for the user.
    

As we might expect, leveraging the capabilities of models available online gives HuggingGPT the ability to solve a wide variety of complex problems!

> _“HuggingGPT can automatically generate plans from user requests and use external models, and thus can integrate multimodal perceptual capabilities and handle multiple complex AI tasks.”_ - from [2]

Quite impressively, HuggingGPT does not need to be fine-tuned at all to learn how to coordinate and use these models! Rather, it leverages [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) and [instruction prompting](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting) to perform each of its required tasks for solving a problem; see below. For these prompts to work well, we need a [steerable](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20) LLM (e.g., ChatGPT or GPT-4) that can follow directions closely and obey strict output formats (e.g., decomposing a problem into json-formatted tasks).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f6466c8-645a-4bcb-a2c0-374ce1d7f900_1182x1554.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f6466c8-645a-4bcb-a2c0-374ce1d7f900_1182x1554.png)

(from [2])

Notably, we should observe that the set of available models to be used is directly injected into the task planning prompt provided to HuggingGPT; see the example above. Obviously, there are too many models available online to include them all in the prompt. To decide which models should be included as an option in the prompt, HuggingGPT selects a group of models based on their task type (i.e., do they solve a task that’s relevant to the current problem), ranks them according to downloads (i.e., the number of users that downloaded or used this model on HuggingFace), then provides the top-`K` models as options to the LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed20ca8a-4260-419e-816c-40507d3c8bec_1180x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed20ca8a-4260-419e-816c-40507d3c8bec_1180x446.png)

(from [2])

**resource dependencies.** After HuggingGPT decomposes a user’s request into several problem-solving steps, we need to execute each model in the specified plan. However, when we execute each model specified by HuggingGPT, some models may be dependent upon the output of others, which is referred to as a “resource dependency” in [2]. To handle these cases, models with dependencies await the output of models upon which they are dependent before executing. Models with no dependencies can execute in parallel to make the task execution step of HuggingGPT faster. Interestingly, the task-planning structure produced by the LLM not only changes the order of execution for each model, but also the manner in which we evaluate HuggingGPT’s output. For example, authors in [2] use GPT-4 to evaluate the quality of more complex task plans; see above.

**does it perform well?** The evaluation of HuggingGPT performed in [2] focuses solely upon assessing the task-planning capabilities of a few different LLMs, as the quality of task planning largely determines the success of the overall problem-solving framework for HuggingGPT. As shown in the figures below, LLMs like GPT-3.5 (and less powerful models to a certain extent) seem to be capable of effectively decomposing user requests into a valid task plan.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a04b57a-6994-4281-bdc0-760eca720b8b_1674x234.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a04b57a-6994-4281-bdc0-760eca720b8b_1674x234.png)

(from [2])

Much work is required to improve the evaluation of such techniques—the analysis provided in [2] is far from comprehensive. Additionally, although HuggingGPT works well, it only considers a small, well-documented set of model APIs that are directly injected into the LLM’s prompt. Compared to fine-tuning, this framework requires a lot of prompt engineering to work well. Although we avoid the need for a fine-tuning dataset, the framework is highly-dependent upon the capabilities of the underlying model. As such, we might wonder: _how can we generalize this approach to work more reliably for a larger number of models?_

### [Gorilla: Large Language Models Connected with Massive APIs](https://arxiv.org/abs/2305.15334) [3]

> _“Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.”_ - from [3]

Integrating LLMs with a smaller, fixed set of other models is cool, but _what if we could teach LLMs to use any model API that is available online?_ To do this, we could just use retrieval techniques that _i)_ identify relevant model APIs and _ii)_ provide their documentation to the LLM. With this approach, LLMs would have access to much more than a small set of curated tools! Rather, models could access the vast suite of changing APIs in the cloud. Unfortunately, however, even powerful LLMs (e.g., [GPT-4](https://openai.com/research/gpt-4) or [Claude](https://www.anthropic.com/index/introducing-claude)) struggle to leverage APIs in this way due a tendency to pass incorrect arguments or hallucinate calls to APIs that do not exist; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F551d868f-9e9b-40d0-8d4f-fdc85fdfbe8b_2138x1120.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F551d868f-9e9b-40d0-8d4f-fdc85fdfbe8b_2138x1120.png)

(from [3])

In [3], authors adopt fine-tuning approach, based upon the self-instruct [11] framework, to make LLMs more capable of leveraging a large group of external deep learning APIs. Going beyond proposals like HuggingGPT [2], authors in [3] consider over 1,600 different model APIs. This set of model APIs is much larger than those considered in prior work, has overlapping functionality (i.e., many models perform similar tasks), and even includes a variety of models with less-than-perfect documentation. To learn how to leverage these APIs, work in [3] fine-tunes a [LLaMA-7B](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [5] model over a large dataset of valid API calls. The resulting model, which can effectively leverage many of these APIs, is called Gorilla.

**creating the dataset.** To fine-tune Gorilla, a massive corpus of API calls is created by leveraging [HuggingFace Hub](https://huggingface.co/docs/hub/index), [PyTorch Hub](https://pytorch.org/hub/), and [TensorFlow Hub](https://www.tensorflow.org/hub). Across all three hubs, 1,645 total model APIs are selected, spanning numerous domains from computer vision, to audio, to reinforcement learning and more. After storing the relevant information for each API in a json object (i.e., includes information like domain, framework, description of functionality, and API signature), we can follow a self-instruct approach by using GPT-4 to generate ten user question prompts and associated responses to go along with each API. After filtering incorrect API calls, the result is just a large dataset of real-world use cases that leverage each of the different model APIs to solve various questions. This dataset is perfect for fine-tuning an LLM; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b1150e5-8be1-478a-b997-5ecc9ada422d_1522x452.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b1150e5-8be1-478a-b997-5ecc9ada422d_1522x452.png)

(from [3])

**retrieval-aware fine-tuning.** Instead of performing “normal” supervised fine-tuning, Gorilla leverages a “retrieval-aware” fine-tuning variant. This might sound fancy, but the practical implementation is simple! For every call to a model API in the fine-tuning dataset, we add up-to-date documentation for this API within the data. Then, we follow a similar approach at test-time by appending API documentation to our prompt, which teaches the LLM to dynamically determine the proper usage of each API based on documentation; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750d1e03-4875-46cb-ad5d-5d4987414d41_3426x100.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750d1e03-4875-46cb-ad5d-5d4987414d41_3426x100.png)

(from [3])

Retrieval aware fine-tuning is a technique that teaches the LLM to better leverage API documentation when determining how to solve a problem. Such a dynamic approach allows the LLM to:

- Adapt to real-time changes in an API’s documentation at test time
    
- Develop improved [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) abilities for making API calls
    
- Hallucinate less by paying better attention to info in an API’s documentation
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)

(from [3])

Retrieval-aware fine-tuning makes Gorilla an incredibly capable interface for leveraging a variety of different deep learning models—the resulting LLM can use a massive number of different APIs to solve a problem. Plus, _the model can actually adapt to changes in documentation for any of its APIs!_ See the figure above for an example of adapting to changes in API documentation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60243e28-c792-4262-8656-f123196af0dc_1616x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60243e28-c792-4262-8656-f123196af0dc_1616x1192.png)

(from [3])

**using Gorilla.** Although we know which API to include in the prompt when constructing the fine-tuning dataset, we don’t know the proper API to use when we receive an arbitrary prompt from a user during inference. To determine the correct API to use, we can just adopt an information retrieval technique that i_)_ embeds the user’s prompt (or other relevant information) and _ii)_ performs vector similarity search to find the most relevant API documentation. This way, we can easily and efficiently identify the best API to use for solving a problem. Alternatively, we could use Gorilla in a zero-shot manner by passing a user’s prompt directly to the model without any information retrieval or extra information. Either way, the goal of Gorilla is to generate an accurate call to the most appropriate API for solving a user’s prompt; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67396d0f-c5bb-4e08-9abe-55ab179108ef_1232x1090.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67396d0f-c5bb-4e08-9abe-55ab179108ef_1232x1090.png)

(from [3])

As demonstrated by the experimental results above, Gorilla is an incredibly capable interface for deep learning APIs. Compared to larger and more powerful models (e.g., GPT-3.5, Claude, and GPT-4), we see that Gorilla is much more capable of generating accurate API calls, meaning that the model hallucinates calls to nonexistent API calls less and tends to pass correct input arguments!

### Other notable techniques…

HuggingGPT [2] and Gorilla [3] have garnered a lot of public recognition and discussion over the last few months, but many other techniques have been proposed that consider using LLMs to coordinate efforts of several different deep learning models. A brief list of other interesting techniques is outlined below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a55239-5dde-41aa-bca5-34806bf6da69_1618x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a55239-5dde-41aa-bca5-34806bf6da69_1618x1192.png)

(from [7])

**TaskMatrix [7]** is a position paper—meaning that it presents a position or outlook on a notable issue—that considers the integration of [foundation models](https://crfm.stanford.edu/) (e.g., LLMs like ChatGPT) with millions of different APIs. Notably, this work argues that foundation models lack domain knowledge needed to accurately solve specialized tasks, but many existing, task-specific models are available that can solve a specified task with impressive accuracy. Integrating these specialized/expert models with an LLM may be difficult due to compatibility issues, but authors in [7] extensively discuss and consider the idea. In many ways, HuggingGPT and Gorilla are practical realization of ideas discussed in [7].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30fb9d3e-96b2-451e-a446-be950222e0d2_1746x1062.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30fb9d3e-96b2-451e-a446-be950222e0d2_1746x1062.png)

(from [8])

**API Bank [8]** provides a better benchmark for evaluating tool-augmented LLMs. In particular, the benchmark considers over 50 APIs that are commonly integrated with LLMs and 264 annotated dialogues—including 568 API calls in total—to go along with these tools. The benchmark is designed to evaluate LLMs’ ability to create a task plan (i.e., step-by-step guide of which API calls to execute), determine the correct APIs to use, and execute API calls to answer a provided question. Unsurprisingly, preliminary experiments show that GPT-4 has the strongest capabilities in leveraging external tools to answer user-provided questions. Although this work does not consider deep learning model APIs in particular, the task framework used mirrors approaches seen in this overview.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927ec2e1-bc3b-4320-8550-1a83befcf7af_2132x1266.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F927ec2e1-bc3b-4320-8550-1a83befcf7af_2132x1266.png)

(from [9])

**ToolkenGPT [9]** attempts to mitigate fine-tuning requirements for tool-following foundation models by assigning a specific token—and associated [token embedding](https://twitter.com/cwolferesearch/status/1659608479089278978?s=20)—to each tool, allowing LLMs to generate tool requests in a similar manner to generating a normal word token. Such an approach is found to be quite flexible for leveraging a variety of external tools.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09d65ae7-ff99-4600-a9d1-f5f471ccf4b5_1622x568.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09d65ae7-ff99-4600-a9d1-f5f471ccf4b5_1622x568.png)

(from [10])

**Tool Manipulation with Open-Source LLMs [10].** In most cases, we see that closed-source LLMs (e.g., GPT-4) are more [steerable](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20) and can, therefore, better manipulate external tools. In [10], however, the authors analyze whether open-source LLMs can be fine-tuned to match the performance of powerful, closed-source LLMs in this particular skill. A variety of open-source LLMs are refined using human feedback and supervision to improve their tool-following capabilities. Interestingly, we see that several open-source models can achieve competitive performance with GPT-4 given sufficient fine-tuning. In this overview, we have seen with models like Gorilla that open-source LLMs (e.g., LLaMA) are incredibly powerful given the correct fine-tuning procedure.

# Closing Remarks

> _“There is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them.”_ - from [7]

Within this overview, we have seen that LLM are capable of integrating with other deep learning models via their APIs. In the case of HuggingGPT [2], this can be done using an in-context learning approach, in which we prompt the LLM with descriptions of existing models and their functionality. Notably, however, HuggingGPT works best with powerful, closed-source models like ChatGPT. If we want to teach open-source models (e.g., LLaMA) to call deep learning model APIs when solving complex problems, we need to adopt a fine-tuning approach, as proposed by Gorilla [3]. Either way, these techniques are incredibly powerful, as they strike a balance between the strengths of narrow expert and foundation models. We can draw upon the power of both by relying upon LLMs to perform high-level reasoning and form problem-solving plans, while delegating certain sub-tasks to specialized models that are more reliable and accurate.

### New to the newsletter?

Hello! I am [Cameron R. Wolfe](https://cameronrwolfe.me/), Director of AI at [Rebuy](https://www.rebuyengine.com/) and PhD student at Rice University. I study the empirical and theoretical foundations of deep learning. This is the Deep (Learning) Focus newsletter, where I help readers build a better understanding of deep learning research via understandable overviews that explain relevant topics from the ground up. If you like this newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

Subscribe

### Bibliography

[1] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." _arXiv preprint arXiv:2302.04761_ (2023).

[2] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." _arXiv preprint arXiv:2303.17580_ (2023).

[3] Patil, Shishir G., et al. "Gorilla: Large Language Model Connected with Massive APIs." _arXiv preprint arXiv:2305.15334_ (2023).

[4] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[5] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[6] Wang, Yizhong, et al. "Self-Instruct: Aligning Language Model with Self Generated Instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[7] Liang, Yaobo, et al. "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis." _arXiv preprint arXiv:2303.16434_ (2023).

[8] Li, Minghao, et al. "Api-bank: A benchmark for tool-augmented llms." _arXiv preprint arXiv:2304.08244_ (2023).

[9] Hao, Shibo, et al. "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings." _arXiv preprint arXiv:2305.11554_ (2023).

[10] Xu, Qiantong, et al. "On the Tool Manipulation Capability of Open-source Large Language Models." _arXiv preprint arXiv:2305.16504_ (2023).

[11] Wang, Yizhong, et al. "Self-Instruct: Aligning Language Model with Self Generated Instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[12] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[13] Trivedi, Harsh, et al. "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions." _arXiv preprint arXiv:2212.10509_ (2022).

[14] Liu, Jiacheng, et al. "Generated knowledge prompting for commonsense reasoning." _arXiv preprint arXiv:2110.08387_ (2021).

[1](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla#footnote-anchor-1-125726849)

Notably, models like Claude now have massive context windows (e.g., [100K tokens](https://www.anthropic.com/index/100k-context-windows)). However, this still doesn’t mean we should just cram massive amounts of information into the LLM’s context window! More details [here](https://twitter.com/cwolferesearch/status/1658192029409484811?s=20).

[2](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla#footnote-anchor-2-125726849)

Pre-trained (large) language models are not narrow experts. Rather, they are [foundation models](https://crfm.stanford.edu/), which means that we can solve a variety of different tasks using the same model (e.g., by constructing different prompts for each task).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Jim Letts's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe05ab05-38c1-4f9b-acd4-4592e05b7e76_1242x1242.jpeg)



](https://substack.com/profile/134408271-jim-letts)

[

![Ira's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb1c345f-e8b6-48b7-9da0-f87c440cc094_144x144.png)



](https://substack.com/profile/8757797-ira)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Vaibhav's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9440269-073f-490b-b534-b6afb32bb06f_680x680.jpeg)



](https://substack.com/profile/56469536-vaibhav)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

12 Likes∙

[1 Restack](https://substack.com/note/p-125726849/restacks?utm_source=substack&utm_content=facepile-restacks)

12

- 

[](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla/comments)

1

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---

[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Can language models make their own tools?

### LaTM, CREATOR, and other closed-loop frameworks for LLM tool usage...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jun 12, 2023

8

- 

[](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own/comments)

1

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)

This newsletter is presented by [Cerebrium](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023). Cerebrum allows you to deploy and fine-tune ML models seamlessly, without worrying about infrastructure. Notable features include serverless GPU deployment (<1 second cold start), access to 15+ pre-trained models (e.g., FLAN-T5, GPT-Neo, Stable Diffusion and more), and support for all major ML frameworks. [Try it](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023) for free today! 

Enjoy deep learning? Find current research topics difficult to parse? Join subscribers from Microsoft, Tesla, Google, Meta, Salesforce and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [My contact info](http://cameronrwolfe.me/)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F832b9ac7-f8a2-483e-8bad-7951b8d3cd85_2346x1330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F832b9ac7-f8a2-483e-8bad-7951b8d3cd85_2346x1330.png)

(from [1, 2])

In recent overviews, we have explored the utility of augmenting large language models (LLMs) with external tools. These models can be taught to leverage tools in a [variety of ways](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning). However, we should realize that existing [tool-following LLMs](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools) leverage only a limited set of potential tools [3], _whereas the range of problems we want to solve with LLMs is nearly endless!_ With this in mind, it becomes clear that such a paradigm is limiting—we will always be able to find scenarios that require tools that do not yet exist. In this overview, we will explore recent research that aims to solve this problem by providing LLMs with the skills to create their own tools. Such an approach draws an interesting analogy to human life, as the ability to fabricate tools led to major technological advancements[1](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own#footnote-1-125158392). Now, we explore the impact of similar techniques upon the evolution of LLMs.

> _“According to the lessons learned from the evolutionary milestones of humans, a crucial turning point was that humans got the ability to fabricate their own tools to address emerging challenges. We embark on an initial exploration to apply this evolutionary concept to the realm of LLMs.”_ - from [1]

# Background

Prior to learning more about tool-making LLMs, there are a few background concepts that we need to refresh. We have covered many of these ideas in recent overviews, but we’ll briefly go over them again now to make our discussion of recent publications more comprehensive and understandable.

### Why should we use tools?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34bfb8f-6cc1-48f5-933d-9c9281cc1905_1838x1238.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34bfb8f-6cc1-48f5-933d-9c9281cc1905_1838x1238.png)

(from [3, 8, 9]

In prior overviews, we have learned about a few different kinds of tools that can be integrated with LLMs to improve their performance, such as:

- Basic Tools (calculators, search engines, etc.) [[link](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools)]
    
- Deep Learning Model APIs [[link](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla)]
    

By giving LLMs access to certain tools, we can easily solve limitations that these models have, such as lacking up-to-date information, failing to perform simple arithmetic, hallucinating facts, or making errors in long chains of reasoning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20ae1c23-3c00-46bf-aa52-3bf17f95e172_754x306.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20ae1c23-3c00-46bf-aa52-3bf17f95e172_754x306.png)

(from [3])

**tools provide context.** For example, if an LLM is asked a question about a pop culture event in recent weeks, it is unlikely to have needed information to provide an accurate answer due to its [knowledge cutoff](https://twitter.com/cwolferesearch/status/1666197953965350926?s=20). In some cases, the LLM might hallucinate an incorrect answer that seems plausible and mislead the user with incorrect information—_this is a major problem because many (non-technical) users of LLMs like ChatGPT treat these model like a search engine_! To solve this issue, we can provide a tool that allows the LLM to perform search queries and retrieve up-to-date information from the internet as extra context; see above. This way, the LLM can memorize less information, relying instead on [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) to arrive at an accurate final answer based upon up-to-date information provided by a tool.

> _“By empowering LLMs to use tools, we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks.”_ - from [3]

One interesting tool that we will see used as a baseline within this overview is the Wolfram ChatGPT plugin. The [plugin ecosystem](https://cameronrwolfe.substack.com/i/123558334/tools-are-super-popular) for ChatGPT integrates LLMs with external tools via their APIs. Basically, we provide a description of the API to ChatGPT, and the model learns how to use this tool (i.e., make calls to its API) via a prompting approach; more details [here](https://cameronrwolfe.substack.com/i/123558334/using-tools-is-getting-easier). To learn more about the Wolfram plugin (it’s super useful!), check out the awesome overview below.

[Wolfram Plugin](https://www.wolfram.com/wolfram-plugin-chatgpt/)

**this overview.** A lot of different types or genres of tools exist, but we will focus upon a particular type of tool—_those that are actually created by an LLM_. Typically, these tools are formatted as standalone Python functions that accomplish some task or sub-task that is useful to the LLM. Tools are created by directly prompting a [code-enabled LLM](https://cameronrwolfe.substack.com/i/121554239/teaching-llms-how-to-code) to generate a needed function. By allowing LLMs to create their own tools, _problem-solving systems are no longer limited by the fixed set of tools that they have available_. We can identify needed functionality over time and enable the LLM to automatically create any tool that would be helpful!

### Prompting Techniques

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a25852-e696-4033-9887-1560baf4f37c_2456x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a25852-e696-4033-9887-1560baf4f37c_2456x840.png)

Language models solve many different tasks with a unified format (from [10])

The generic text-to-text structure of a language model is incredibly powerful, as it allows us to solve many different tasks by just _i)_ formatting the problem as a textual prompt and _ii)_ extracting relevant output information from the text returned by the model. However, using language models is not usually quite this simple. The wording and structure of the prompt that we provide to the model can drastically alter its effectiveness—[prompt engineering](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering) is a huge deal!

Recently, we have gone over a lot of different practical tricks and techniques for getting the most out of an LLM via prompt engineering.

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    

However, there are two particular prompting techniques that are especially relevant to this post—[chain of thought](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) (CoT) [6] and [program of thoughts](https://cameronrwolfe.substack.com/i/121554239/program-of-thoughts-pot-prompting) (PoT) [7] prompting. Both of these techniques aim to improve the ability of a language model to reliably solve complex, reasoning-based tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [6])

**chain of thought.** For a long time, LLMs were criticized for their inability to solve reasoning-based tasks. Although this issue has been mitigated with [recent model variants](https://twitter.com/cwolferesearch/status/1666879932796895248?s=20), techniques such as CoT prompting can elicit better reasoning abilities within these models nonetheless. _How is this possible?_ We simply need to provide the LLM with examples of reasoning-based problems being broken down into a step-by-step solution (i.e., a problem-solving rationale or “chain of thought”). These examples are inserted directly into the LLM’s prompt. Then, the model can use its [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) capabilities to generate a similar rationale when solving a problem posed by a user. Interestingly, generating such rationales drastically improves LLM performance on reasoning-based tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfb3bd5-b081-48e7-906f-973eec16ea73_2332x1274.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfb3bd5-b081-48e7-906f-973eec16ea73_2332x1274.png)

Popular CoT prompting variants (from [11, 12, 13])

Beyond vanilla CoT prompting, [numerous variants](https://cameronrwolfe.substack.com/i/118401596/chain-of-thought-prompting-and-beyond) have also been proposed; see above. The idea of enabling LLMs to solve difficult problems by breaking them into smaller parts and solving them step-by-step is incredibly powerful. But, we can do this in several different ways (some of which are actually quite a bit simpler than CoT prompting)—_CoT prompting is not always our best option_.

**program of thoughts.** Thought CoT prompting variants work well, they fail to model concepts like iteration and are subject to a [compositionality gap](https://cameronrwolfe.substack.com/i/121554239/decoupling-reasoning-and-computation-within-llms), meaning that the LLM may correctly solve every step of a problem but still generate an incorrect final answer. To mitigate these problems, recent research has explored [program-aided language models](https://cameronrwolfe.substack.com/p/program-aided-language-models). These techniques are similar to CoT prompting, but we use code-enabled LLMs (e.g., [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code) [14]) to generate a hybrid problem-solving rationale that contains both code and natural language components—_basically a program with informative comments_. Then, we can execute the program created by the LLM (using an external interpreter) to yield a final answer!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f486545-4d69-49e6-abb2-7f90049d7c77_2180x1530.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f486545-4d69-49e6-abb2-7f90049d7c77_2180x1530.png)

(from [7])

The basic idea behind PoT prompting is that there are certain ideas and concepts that are much easier to model within a program. Instead of using the LLM to both solve subtasks and generate a final answer from all of these solutions, we can delegate a portion of this process to a system that is more reliable. Namely, programs can more easily model and solve mathematical equations, perform iteration, and much more, thus lessening the compositionality gap for LLMs.

# A Turning Point in Tool Usage

> _“Instead of letting the LLMs act as the users of tools, we enable them to be the creators of tools and solve problems with higher accuracy and flexibility.”_ - from [2]

At this point, we should probably be convinced that tools are a useful addition to existing language models. But, _what becomes possible when we expand the scope of available tools to anything that an LLM can create?_ Put simply, such an approach forms a closed-loop framework, in which LLMs are given the ability to arbitrarily improve their own functionality. As we will see moving forward, existing models are surprisingly capable of making their own tools, which enables dynamic adaptation to solving new, difficult problems over time.

### Decoupling Tool Making and Tool Usage

We know that using external tools can greatly improve the problem solving capabilities of LLMs [3]. However, prior work in this area assumes that needed tools already exist and are available for the LLM. As such, there exists a dependence upon humans to craft and curate a set of needed tools that comprehensively addresses needed functionality for solving any task. But, _what if the LLM needs a tool that is not included in its tool belt?_ Existing tool-following approaches have no solution for a problem like this!

As an alternative approach, authors in [2] propose a “closed-loop” framework that uses the LLM itself to construct needed tools on-the-fly. This framework, called LLMs as Tool Makers (LaTM), allows LLMs to continually generate tools that are needed to solve different complex reasoning tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2714735-1129-419d-b14d-27ca47158846_1878x1258.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2714735-1129-419d-b14d-27ca47158846_1878x1258.png)

(from [2])

**two-phase tooling approach.** LaTM uses a two-phase framework of:

- _Tool making:_ crafting a tool (i.e., a Python function) for a given task
    
- _Tool Using:_ using an existing tool to solve a task
    

Notably, we do not have to use the same LLM for both of these phases. For example, we could apply a more powerful model (e.g., [GPT-4](https://platform.openai.com/docs/models/gpt-4)) to tool making, while using a lightweight and cost-effective model (e.g., [GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5)) for tool using. Given that tool making typically demands a more capable LLM relative to tool using, this approach allows LaTM to save costs in practice. We only use the most expensive and powerful models in the tool making phase, which we can execute once per tool and continually reuse for problem solving!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a05b2bc-e2cd-4dcb-a47b-fed6c203f6a9_2440x1080.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a05b2bc-e2cd-4dcb-a47b-fed6c203f6a9_2440x1080.png)

(from [2])

The goal of the tool making process is to generate a generic and reusable tool—implemented as a Python function—from a few examples of solving a task. In [2], this goal is accomplished by first “proposing” a tool via [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning). We provide several demonstrations of desired behavior and prompt the LLM to generate a program that reproduces the output of these demonstrations. If a program is generated that produces no errors, LaTM uses an LLM to generate several unit tests for this tool (i.e., based upon the provided demonstrations) and executes these tests to confirm that they pass. Finally, the tool is “wrapped”, or made available via a prompt that demonstrates its usage; see above.

Though tool making is complex and requires a powerful LLM to be successful, tool usage can be accomplished with a more cost effective model—_we are just using existing tools to solve a task_! We can prompt the LLM to use tools via the wrapped tools created during tool making, which provide demonstrations of converting tasks into relevant function calls. Here, LaTM relies on the in-context learning abilities of LLMs to determine the proper usage of each tool; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe311d0fe-c63d-4fd8-97fa-927c164353d2_1348x1394.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe311d0fe-c63d-4fd8-97fa-927c164353d2_1348x1394.png)

(from [2])

Notably, using a smaller model for tool using means that we only use more powerful models during a small portion of the LaTM pipeline—_tools are only created once and can be reused continually as more problems are solved_. The process of creating and using tools with LaTM might seem a bit complex, but it is actually quite simple. An end-to-end example of creating and using tools for solving a logical deduction problem is provided within the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F430d9373-6044-477a-92e5-bf48c344cd5d_1352x1146.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F430d9373-6044-477a-92e5-bf48c344cd5d_1352x1146.png)

(from [2])

**adding a dispatcher.** Beyond tool creation and usage, authors in [2] also propose a dispatcher module to handle on-the-fly creation and usage of new tools with LaTM; see below. Put simply, the dispatcher is an LLM that is used to determine whether an incoming task should create a new tool or just use existing tools. By using this extra module, we can easily identify new tasks that cannot be handled by existing tools and create any tools that are needed, allowing LaTM to better handle streaming scenarios in which new tasks arrive sequentially. Interestingly, authors in [2] show that a GPT-3.5-based dispatcher can identify correct tools to use—or the need for a new tool—with ~95% accuracy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf3d9c4d-5a76-4c7e-858d-e55351b75fdc_2144x1076.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf3d9c4d-5a76-4c7e-858d-e55351b75fdc_2144x1076.png)

(from [2])

**does this work?** LaTM is evaluated over a small set of complex reasoning tasks from [BigBench](https://github.com/google/BIG-bench), where [GPT-4](https://openai.com/research/gpt-4) is used as the tool maker and [GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) is used as the tool user. Somewhat unsurprisingly, we see that GPT-4 is capable of creating viable and useful tools in most test cases; see below. Less capable models (e.g., GPT-3.5-turbo) can be used for making tools on easier problems, but GPT-4 is needed in more complex domains. Going further, we see that longer [context lengths](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering) are necessary for tool making, as LaTM uses the full history (i.e., examples of generating all tools so far) when generating a tool to improve [reliability](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-reliability.md).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff978435-8cf0-48ba-9782-acbb18060499_1350x398.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff978435-8cf0-48ba-9782-acbb18060499_1350x398.png)

(from [2])

When the performance of LaTM is compared to techniques like [CoT prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [4], we see that the proposed approach makes existing LLMs way more capable! By using generated tools, models like GPT-3.5-turbo can perform similarly to GPT-4 and far surpass the performance of CoT prompting; see below. Plus, we see that using more lightweight models as the tool user is preferable and even outperforms using powerful models like GPT-4 in some cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f98e747-2981-486e-9cb2-477897776fa3_1350x584.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f98e747-2981-486e-9cb2-477897776fa3_1350x584.png)

(from [2])

LaTM is an interesting, closed-loop framework that enables LLMs to produce their own tools. Because it only uses large, expensive LLMs (e.g., GPT-4) for a small portion of the problem-solving process, LaTM is a cost-effective approach for improving LLM performance. _We can nearly match the performance of GPT-4 on complex reasoning tasks with smaller models and at a lower cost._

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)

### Now from our partners!

- [Rebuy Engine](https://www.rebuyengine.com/) is the Commerce AI company. They use cutting edge deep learning techniques to make any online shopping experience more personalized and enjoyable.
    
- [KUNGFU.AI](https://urldefense.com/v3/__http://KUNGFU.AI__;!!BuQPrrmRaQ!i4L4Oc-1VDW1AHrfWwPg9wcLgB7A4UgD2LsIn9-L7LvnJJbz2Sh6c3ee4MnN_sn04GFwufC-Elb0tnEnztEylFoQBdkEJgf7$) partners with clients to help them compete and lead in the age of AI. Their team of AI-native experts deliver strategy, engineering, and operations services to achieve AI-centric transformation.
    
- [MosaicML](https://www.mosaicml.com/) enables you to train and deploy large AI models on your data and in your secure environment. Try out their tools and platform [here](http://mosaicml.me/cameronrwolfe) or check out their [open-source, commercially-usable LLMs](https://www.mosaicml.com/blog/mpt-7b).
    

---

### Rectifying Mistakes in Tool Creation

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b59469-6eba-491b-a67d-57d05cc4ee3e_1538x1066.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b59469-6eba-491b-a67d-57d05cc4ee3e_1538x1066.png)

(from [2])

Expanding upon the idea of using LLMs to create their own tools, authors in [2] propose a novel framework that _i)_ uses LLMs to create their own, relevant tools via documentation and code creation, _ii)_ adopts a simpler approach for planning how to use tools to solve a problem, and _iii)_ adds a supplemental error-handling mechanism to the tool-using process to make the overall system more robust and accurate. The resulting technique, called CREATOR [2], was explored in parallel to research in [2][2](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own#footnote-2-125158392). _The goal of both publications is to create more intelligent and adaptable systems for solving complex problems by enabling the creation of needed tools._

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef4f7d29-f429-4caf-846c-abc64af7fa70_758x850.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef4f7d29-f429-4caf-846c-abc64af7fa70_758x850.png)

(from [2])

**problem-solving approach.** CREATOR approaches tool creation and usage via a four-step process (see above for an illustration):

1. _Creation_: create tools through documentation and code realization.
    
2. _Decision_: decide when and how to use existing tools to solve a problem.
    
3. _Execution_: execute the program that applies tools to solving a problem.
    
4. _Rectification_: Modify tools and decisions based on results of execution.
    

The rectification step is not present in prior work. This component acts as an automated error handling mechanism that improves the robustness of the system. Because LLMs in [2] (and in related publications [1]) use code as a medium for creating tools, we can detect and rectify errors (e.g., via a stack trace or something similar) that arise when creating or using tools without much effort.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d818711-7e08-44d1-9a37-65073b4f6b13_1330x924.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d818711-7e08-44d1-9a37-65073b4f6b13_1330x924.png)

(from [2])

In [2], tool creation follows an in-context learning approach that provides detailed instructions and [few-shot examples](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) to guide the LLM towards generating the correct tool. Tool creation has two main components:

- _Documentation_: outlines relevant information about a tool (e.g., function, purpose, signature, etc.).
    
- _Realization_: implements the tool in code (see above).
    

Similar to [1], tools created in [2] are captured within a function or method (in Python) that can be called upon by the LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa322932a-530c-4895-bdac-5572cbe5b09a_812x302.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa322932a-530c-4895-bdac-5572cbe5b09a_812x302.png)

(from [2])

During the decision stage, the LLM considers documentation of all tools and determines which tools to use and how to use them to solve the current problem. After a problem-solving plan as been created, we can then:

- Format the input for each tool.
    
- Execute each tool to get the associated output.
    
- Perform any needed operations on tool outputs to derive an answer.
    

If tool execution leads to the generation of any errors, we can simply record this information and iterate over the four-step process again, passing the error as an extra input for rectifying existing tools; see above. Otherwise, we can use the generated information to extract a final answer for the user’s question.

> _“Our research reveals that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to flexibly tackle diverse situations.”_ - from [2]

**improved mathematical reasoning.** The system proposed in [2] is evaluated over mathematical (and tabular) reasoning datasets [MATH](https://github.com/hendrycks/math) and [TabMWP](https://promptpg.github.io/) [4, 5]. In all experiments, ChatGPT (i.e., [GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5)) is used as the base model, due to its code generation and impressive reasoning capabilities. CREATOR is compared to baselines such as standard [CoT prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [6], [PoT prompting](https://cameronrwolfe.substack.com/i/121554239/program-of-thoughts-pot-prompting) [7], and the [Wolfram alpha ChatGPT plugin](https://www.wolfram.com/wolfram-plugin-chatgpt/). When all methods use ChatGPT as the base model, we see that CREATOR (combined with CoT prompting) yields improved overall accuracy compared to baselines, as well as an improved successful execution rate (i.e., meaning the system provides an answer with a valid format).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088fc921-934f-4d9d-a80a-ac9bc8b71594_1156x424.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088fc921-934f-4d9d-a80a-ac9bc8b71594_1156x424.png)

(from [2])

Beyond these evaluations, authors in [2] propose a new Creation challenge dataset that attempts to evaluate the tool creation abilities of an LLM by testing problem-solving capabilities in new scenarios for which no existing tool or code package exists. On this benchmark, CREATOR slightly outperforms existing baselines. However, this performance improvement becomes much larger when the LLM is given a textual hints about the utility of the tool that should be created; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a82e952-1cce-4da9-97f5-4100ea5397ef_750x796.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a82e952-1cce-4da9-97f5-4100ea5397ef_750x796.png)

(from [2])

Beyond its ability to match or exceed baseline performance in aggregate, the CREATOR framework performs more consistently as problems become increasingly difficult, while baselines tend to deteriorate. CREATOR experiences a similar deterioration on certain problem classes, but the framework seems to be more adaptable and capable of handling complex problems; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde822f9c-255f-4eb4-a763-671d205fe7bf_748x866.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde822f9c-255f-4eb4-a763-671d205fe7bf_748x866.png)

(from [2])

# Takeaways

Within this overview, we have explored a more dynamic and flexible approach for tool usage with LLMs. Instead of curating a fixed set of tools that can be used by the LLM, we can simply enable the LLM to create whatever tool that is needed. By following such an approach, we no longer deal with issues created by not having access to a tool that is needed by an LLM to solve a problem. We might initially question whether such a strategy would be successful (i.e., are LLMs powerful enough to create their own tools?), but recent research [1, 2] shows us that state-of-the-art LLMs like GPT-4 are more than capable of creating tools in the form of standalone Python functions, assuming that measures to rectify errors in tool creation are put in place. Then, these tools can be used and re-used (even by less powerful LLMs) to solve a variety of complex problems.

> _“Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks.”_ - from [1]

Using LLMs to create tools is great, _but how does this relate to [parallel efforts](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools) that have integrated LLMs with a variety of existing tools?_ It remains to be seen. However, I personally think that the optimal system will use a hybrid of existing techniques. There are many useful tools that already exist and are being integrated with popular LLMs every day (e.g., see the ChatGPT plugin store). As such, relying solely upon LLMs to create their own tools doesn’t make sense. Instead, we can leverage existing tools, but also give LLMs needed skills to create any tools that they lack. Over time, the suite of tools available to LLMs will continue to evolve and make AI-based problem solving systems more effective.

### New to the newsletter?

Hello! I am [Cameron R. Wolfe](https://cameronrwolfe.me/). Ph.D. in deep learning and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers build a better understanding of deep learning research via understandable overviews that explain relevant topics from the ground up. If you like this newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

[Share](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEyNTE1ODM5MiwiaWF0IjoxNzQ1NzQ1NjUzLCJleHAiOjE3NDgzMzc2NTMsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.MYIxm12tqigTLBPv7-u5zy0eHhyhg4n9KwYk8pvnWdc)

### Bibliography

[1] Cai, Tianle, et al. "Large Language Models as Tool Makers." _arXiv preprint arXiv:2305.17126_ (2023).

[2] Qian, Cheng, et al. "CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation." _arXiv preprint arXiv:2305.14318_ (2023).

[3] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." _arXiv preprint arXiv:2302.04761_ (2023).

[4] Lu, Pan, et al. "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning." _arXiv preprint arXiv:2209.14610_ (2022).

[5] Hendrycks, Dan, et al. "Measuring mathematical problem solving with the math dataset." _arXiv preprint arXiv:2103.03874_ (2021).

[6] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[7] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." _arXiv preprint arXiv:2211.12588_ (2022).

[8] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." _arXiv preprint arXiv:2303.17580_ (2023).

[9] Patil, Shishir G., et al. "Gorilla: Large Language Model Connected with Massive APIs." _arXiv preprint arXiv:2305.15334_ (2023).

[10] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." _arXiv preprint arXiv:2205.11916_ (2022).

[12] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." _arXiv preprint arXiv:2203.11171_ (2022).

[13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." _arXiv preprint arXiv:2205.10625_ (2022).

[14] Chen, Mark, et al. "Evaluating large language models trained on code." _arXiv preprint arXiv:2107.03374_ (2021).

[1](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own#footnote-anchor-1-125158392)

Check out [this interesting article](https://www.livescience.com/7968-human-evolution-origin-tool.html) for some more details on the creation and usage of tools by humans.

[2](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own#footnote-anchor-2-125158392)

The LaTM [1] and CREATOR [2] papers were actually released on Arxiv within 3 days of each other! CREATOR came first, but not by much.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Neil Dave's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5950036-8a6c-4209-86c8-0b57b4324e0b_906x906.jpeg)



](https://substack.com/profile/18680357-neil-dave)

[

![James Le's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd92c7fe-0109-44bd-a10f-19c8bd4840c0_3596x2514.jpeg)



](https://substack.com/profile/6009523-james-le)

[

![Patel Gbedjemaiho's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f56b0af-c6bf-4b54-9f28-a86c0ae25ff5_144x144.png)



](https://substack.com/profile/141603119-patel-gbedjemaiho)

[

![DAMIAN THONG's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2a69b9b-3271-417d-a986-0ede3e94482f_1116x1118.jpeg)



](https://substack.com/profile/29409869-damian-thong)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

8 Likes∙

[1 Restack](https://substack.com/note/p-125158392/restacks?utm_source=substack&utm_content=facepile-restacks)

8

- 

[](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own/comments)

1

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Imitation Models and the Open-Source LLM Revolution

### Are proprietary LLMs like ChatGPT and GPT-4 actually easy to replicate?

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jun 19, 2023

19

- 

[](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source/comments)

1

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)

This newsletter is presented by [Cerebrium](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023). Cerebrum allows you to deploy and fine-tune ML models seamlessly, without worrying about infrastructure. Notable features include serverless GPU deployment (<1 second cold start), access to 15+ pre-trained models (e.g., FLAN-T5, GPT-Neo, Stable Diffusion and more), and support for all major ML frameworks. [Try it](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023) for free today! 

Enjoy deep learning? Find current research topics difficult to parse? Join subscribers from Microsoft, Tesla, Google, Meta, Salesforce and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb6679a-c471-4adc-88f1-14b125f24391_2458x1274.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb6679a-c471-4adc-88f1-14b125f24391_2458x1274.png)

(from [1])

The proposal of the [LLaMA suite](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [2] of large language models (LLMs) led to a [surge in publications](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7alpaca-an-instruction-following-llama-model) on the topic of open-source LLMs. In many cases, the goal of these works was to cheaply produce smaller, opens-source LLMs (for research purposes) that have comparable quality to proprietary models like [ChatGPT](https://openai.com/blog/chatgpt) and [GPT-4](https://openai.com/research/gpt-4). These models adopt an imitation strategy, which fine-tunes a base LLM over synthetic dialogue data from a more powerful LLM. Despite being cheap to train, these models seemed to perform comparably to proprietary LLMs like ChatGPT. As a result, the deep learning research community quickly adopted the view that open-source LLMs will rule the future—_re-producing open-source variants of proprietary models was both easy and cost-effective_!

> _“Will the most powerful LLMs be closed-source or will they be freely distributed for anyone to use, modify, and extend?”_ - from [1]

Unfortunately, preliminary evaluations performed on these models, which relied upon ratings provided by other LLMs (e.g., GPT-4) or human crowd workers, were somewhat cursory. _Does the performance of imitation models actually match that of models like ChatGPT?_ To answer this question more rigorously, we will study recent research that analyzes whether imitation models truly remove the “moat” around proprietary LLMs. Interestingly, we will see that these cheap reproductions of powerful LLMs perform well in human evaluations due to their ability to learn the style of a powerful LLM. However, they lack factuality and perform poorly when subjected to more broad and targeted evaluations. In reality, _imitation models do not perform nearly as well as proprietary models like ChatGPT._

# Model Imitation

> _“The premise of model imitation is that once a proprietary LM is made available via API, one can collect a dataset of API outputs and use it to fine-tune an open-source LM.”_ - from [1]

The majority of models that we will see in this overview are trained via a model imitation strategy. This strategy, which is based upon the more generic idea of knowledge distillation, is a seemingly effective way to fine-tune less powerful LLMs to make them behave more similarly to powerful LLMs like ChatGPT and GPT-4. To do this, we just:

- Collect dialogue examples from the more powerful model (e.g., using the OpenAI API).
    
- Use them to fine-tune the smaller model using a [normal language modeling objective](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20).
    

This approach (although not [commercially viable](https://openai.com/policies/terms-of-use)) was heavily utilized by a variety of open-source LLMs—including Alpaca, Vicuna, Koala, and more [3, 4, 5]—to create language models much closer to the quality or ChatGPT or GPT-4.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b790fbc-a391-4a0d-b2c7-6ce77ad7df01_1562x674.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b790fbc-a391-4a0d-b2c7-6ce77ad7df01_1562x674.png)

(from [7])

**knowledge distillation.** The idea of knowledge distillation for deep neural networks was originally explored in [1][1](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-1-127874443). To put it simply, knowledge distillation uses a (large) fully-trained neural network as a training signal for another (small) neural network; see above. If we train a neural network using both _i)_ the normal training data and _ii)_ the output of a larger, more powerful neural network over that data, then we will typically arrive at a better result than training a neural network over the data alone. By using its output as a training target, we can distill some of the information from a larger “teacher” network into a smaller “student” network that is being trained. For more details, check out the link below.

[Knowledge Distillation](https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764)

Although many types of knowledge distillation exist, the variant considered in this overview is referred to as model imitation, where we use the output of a teacher LLM as a training target for [supervised fine-tuning](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior) of another LLM.

**types of model imitation.** There are a variety of high-quality LLMs available online, but many of them are only accessible via a [black-box API](https://openai.com/blog/openai-api). Instead of having access to the model itself, we can only provide input to the model and receive output (possibly with associated [log probabilities](https://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/)). Model imitation collects data from these APIs and uses it for fine-tuning, allowing any model to imitate the output of a proprietary LLM. There are two basic types of imitation:

- _Local Imitation_: learn to imitate a model’s behavior on a specific task, instead of imitating its behavior as a whole.
    
- _Broad Imitation_: learn to imitate a model’s behavior broadly, across a variety of different topics.
    

Broad imitation is (generally) more difficult than local imitation, as it aims to comprehensively capture a model’s behavior. Although imitating a specific task is not hard, replicating a model’s behavior as a whole requires a lot of data and can be quite difficult.

> _“Broad-coverage imitation is challenging because (1) one must collect an extremely diverse imitation dataset and (2) imitation models must capture this wide data distribution and generalize similarly to the target model on a myriad of held-out examples.”_ - from [1]

# The Wake of LLaMA

Model imitation was explored extensively by recent research on open-source LLMs. This line of work began with the proposal of [LLaMA](https://cameronrwolfe.substack.com/i/113386783/the-llama-suite) [2] and was quickly extended by follow-up models like [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model), [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality), [Koala](https://cameronrwolfe.substack.com/i/114077195/koala-a-dialogue-model-for-academic-research), and more [3, 4, 5]. We learned about most of these models within prior overviews:

- LLaMA: LLMs for Everyone! [[link](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)]
    
- Beyond LLaMA: The Power of Open LLMs [[link](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms)]
    

Here, we will quickly cover the basics of these models and provide relevant context that will make this overview more understandable.

### What is LLaMA?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)

LLaMA catalyzed an explosion of open-source LLMs (from [3, 4, 5, 16] and DreamStudio)

LLaMA is not a single language model, but rather a suite of LLMs with sizes ranging from 7 billion to 65 billion parameters. Taking inspiration from [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [13], these LLMs are a bit smaller than [their counterparts](https://cameronrwolfe.substack.com/i/91134599/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-b-a-large-scale-generative-language-model) but are pre-trained extensively (i.e., [smaller models, more tokens](https://twitter.com/cwolferesearch/status/1603837192346165248?s=20)). LLaMA models perform surprisingly well; e.g., the 13 billion parameter model is comparable to [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt) [14], while the 65 billion parameter model surpasses the performance of [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [15].

**fully open-source.** Unlike closed-source models that are trained on a combination of public and proprietary data, LLaMA uses only publicly available data for pre-training—_LLaMA models can be reproduced completely from online resources_! After being publicly released for research purposes, the weights of the model were “leaked” online. Even still, LLaMA is prohibited from being used in any commercial applications even if one has access to the model’s weights.

### Imitation Models: Alpaca, Vicuna, Koala, and More

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7043a6c9-e72f-460b-9dc0-ec410293a3dc_2380x1324.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7043a6c9-e72f-460b-9dc0-ec410293a3dc_2380x1324.png)

(from [3, 4, 5, 16])

Interestingly, LLaMA weights being leaked online led to a massive explosion in the model’s popularity. Researchers quickly began to release a variety of interesting, open-source derivatives. Primarily, LLaMA was used to create imitation models based on data derived from dialogues with powerful LLMs like ChatGPT. Let’s take a look at some of the popular LLMs derived from LLaMA.

**Alpaca [3]** is a fine-tuned version of the LLaMA-7B LLM. The fine-tuning process is based on [self-instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework) [17], in which instruction-following data is collected from a higher-performing LLM (i.e., `text-davinci-003`) and used for [supervised fine-tuning](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior). The entire fine-tuning process of Alpaca costs only $600 (including both data collection and fine-tuning). Read more about Alpaca [here](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model).

**Vicuna [4]** is an open-source chatbot that is created by fine-tuning LLaMA-13B (i.e., comparable performance to [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)). Vicuna is fine-tuned using examples of user conversations with ChatGPT, and the entire fine-tuning process can be replicated for $300. Compared to Alpaca, Vicuna is more comparable to ChatGPT and generates answers with detail and structure. Read more about Vicuna [here](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality).

**Koala [5]** is a version of LLaMA-13B that has been fine-tuned on dialogue data from a variety of sources, ranging from public datasets to dialogues with other high-quality LLMs that are available on the internet. Compared to Alpaca, Koala is fine-tuned over more dialogue data and evaluated more extensively (using a larger number of crown workers). Read more about Koala [here](https://cameronrwolfe.substack.com/i/114077195/koala-a-dialogue-model-for-academic-research).

**GPT4ALL [16]** is a fine-tuned LLaMA-7B model that has been trained on over 800K chat completions from `GPT-3.5-turbo`. Along with releasing the code and model, authors of GPT4ALL release the 4-bit quantized weights of the model, which can be used to run model inference on CPUs. As a result, we can use this model on a normal laptop! More details are provided [here](https://gpt4all.io/index.html).

> _“Open-source models are faster, more customizable, more private, and … more capable. They are doing things with $100 and 13B params that [Google] struggles with at $10M and 540B. And they are doing so in weeks, not months.”_ - from [9]

**the massive potential of imitation models.** The models mentioned above were published in close succession and (in most cases) claimed to achieve results that were comparable to top proprietary models like ChatGPT or GPT-4. As such, the research community quickly adopted the opinion that open-source model will soon dominate the LLM landscape. But, _is this actually the case?_

### Are we missing something?

Open-source, LLaMA-based imitation models seem to perform well, as they are much better at [instruction following](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting) compared to the base LLM (i.e., the model that has been pre-trained but not fine-tuned) and have a comparable style to [ChatGPT](https://openai.com/blog/chatgpt). In fact, crowd workers initially rate the outputs of a LLaMA-13B model that has been trained to imitate ChatGPT as better 70% of the time; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eca7313-697b-44e6-9be7-cb6b4de08fc9_808x664.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eca7313-697b-44e6-9be7-cb6b4de08fc9_808x664.png)

(from [1])

With these results in mind, it seems like model imitation provides an easy way to distill the capabilities of any proprietary model into a smaller, open-source LLM. If this is the case, we can match the performance of the best proprietary models via an open-source LLM by just using [fine-tuning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) and imitation data, leaving closed-source models like GPT-4 with [no true advantage](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).

**the (unfortunate) truth.** Although the ability to easily re-create open-source variants of proprietary models for research purposes is enticing, evaluation using crowd workers can be misleading. A model can score well simply by outputting answers with the correct style and structure, even if an answer is factually weak or incorrect. _Why is this the case?_ Verifying factual correctness requires a larger time investment (or existing knowledge) from the crowd worker.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65ce17b8-0584-4872-bcf7-359beb85bf16_2542x1494.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65ce17b8-0584-4872-bcf7-359beb85bf16_2542x1494.png)

(from [3, 4, 5])

**how are open-source LLMs evaluated?** With this in mind, we might start to question whether [post-LLaMA LLMs](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) are _actually_ closing the gap between paid and open-source LLMs. These models are definitely exciting and impressive, but when we look at how they are evaluated, we typically see that the evaluation is:

1. Not very comprehensive
    
2. Primarily based upon human (or LLM) evaluation
    

As such, it’s easy to be misled regarding the true quality of these models given the limitations of human evaluation. Put simply, _these models are not evaluated rigorously enough to gain an accurate picture of their quality_.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)

### Now from our partners!

- [Rebuy Engine](https://www.rebuyengine.com/) is the Commerce AI company. They use cutting edge deep learning techniques to make any online shopping experience more personalized and enjoyable.
    
- [KUNGFU.AI](https://urldefense.com/v3/__http://KUNGFU.AI__;!!BuQPrrmRaQ!i4L4Oc-1VDW1AHrfWwPg9wcLgB7A4UgD2LsIn9-L7LvnJJbz2Sh6c3ee4MnN_sn04GFwufC-Elb0tnEnztEylFoQBdkEJgf7$) partners with clients to help them compete and lead in the age of AI. Their team of AI-native experts deliver strategy, engineering, and operations services to achieve AI-centric transformation.
    
- [MosaicML](https://www.mosaicml.com/) enables you to train and deploy large AI models on your data and in your secure environment. Try out their tools and platform [here](http://mosaicml.me/cameronrwolfe) or check out their [open-source, commercially-usable LLMs](https://www.mosaicml.com/blog/mpt-7b).
    

---

# [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717) [1]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)

(from [1])

Authors in [1] aim to comprehensively analyze the performance of model imitation, thus answering the question: _can we really imitate proprietary LLMs with weaker, open-source models?_ A variety of models are fine-tuned over different sets of imitation data, then extensively evaluated using both crowd workers and a variety of different natural language benchmarks. Initially, LLMs produced via model imitation of ChatGPT seem to perform well, but targeted evaluations reveal that they do far less to close the gap between the base LLM (i.e., LLaMA [2]) and [ChatGPT](https://openai.com/blog/chatgpt) than it seems. These models are less factual and only improve performance on tasks that are heavily represented in the fine-tuning set. The model oftentimes _declines_ in accuracy on tasks that aren’t seen during fine-tuning!

### Experimental Setup

Analysis in [1] critically evaluates recent work on model imitation by exploring a variety of experimental setups. All models used are [decoder-only transformers](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20), including [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) [6], [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)-7B, and LLaMA-13B [2]. Evaluation is performed using [GPT-4](https://openai.com/research/gpt-4), crowd workers, and widely-used natural language benchmarks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4577a923-8d8b-4bb0-86f8-9cae1dea1aab_1014x504.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4577a923-8d8b-4bb0-86f8-9cae1dea1aab_1014x504.png)

**building the dataset.** Fine-tuning datasets are created using a combination of human and LLM-provided examples for both local and broad imitation. For local imitation, a task-specific fine-tuning dataset is created by bootstrapping the [Natural Questions dataset](https://ai.google.com/research/NaturalQuestions) (i.e., based on factual knowledge of Wikipedia). In particular, authors in [1] take a small set of QA pairs from Natural Questions, then prompt ChatGPT to curate 6,000 more examples of similar questions; see above.

Curating a broad imitation dataset is more difficult, as the data needs to comprehensively cover desired LLM behavior. To create such a dataset, authors in [1] rely upon public, high-quality dialogues from sources like [ShareGPT](https://sharegpt.com/), ChatGPT-focused discord servers (e.g., [TuringAI](https://top.gg/servers/899761438996963349)), and even `r/ChatGPT` on Reddit. The result is ~130K examples of freely-collected dialogue examples—referred to as ShareGPT-Mix—that are used for imitation fine-tuning. The quality of this data is high, and there is a large diversity in instructions—the most similar user queries have a [BLEU score](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b) similarity of only 8%[2](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-2-127874443). Each dialogue example from ShareGPT-Mix is post-processed by adding special tokens that mark the beginning of each user query and model output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc99bf80d-58f4-421c-87b4-fc3fead9b22b_1888x1210.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc99bf80d-58f4-421c-87b4-fc3fead9b22b_1888x1210.png)

(from [1])

**fine-tuning approach.** Models are fine-tuned using a [standard language modeling loss](https://cameronrwolfe.substack.com/i/85568430/language-modeling). However, this loss is only applied over the portion of tokens corresponding to model output. In other words, the fine-tuning loss is only applied over the blue portions of each dialogue example within the above figure. Several fine-tuning runs are performed with dataset sizes ranging from 0.3M to 150M tokens[3](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-3-127874443).

### Are imitation models actually useful?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d7ea1a-7a27-4a1a-9e90-ff750277a5d5_1896x960.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d7ea1a-7a27-4a1a-9e90-ff750277a5d5_1896x960.png)

(from [1])

At initial glance, the quality of models trained via ShareGPT-mix imitation data seems to be quite high. While base models fail to follow instructions, the imitation fine-tuned variants stay on task and are capable of solving problems in a similar manner to ChatGPT. Plus, increasing the size of the model leads to consistent improvements in performance, and these models are rated positively when evaluated with GPT-4; see above.

However, more detailed analysis seems to indicate that these results might be slightly misleading. For example, human evaluation scores saturate quickly (and even degrade) as more imitation data is used; see below. Such a surprising result indicates that there is something we might be missing within the evaluation of these models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F254acedd-cb9e-4c4c-98a7-ea149742c9c9_1888x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F254acedd-cb9e-4c4c-98a7-ea149742c9c9_1888x882.png)

(from [1])

**targeted evaluations.** When imitation models are evaluated across a wider variety of natural language benchmarks, we see that their performance is comparable to or below that of the corresponding base LLM. In other words, _fine-tuning over imitation does not improve the performance across a wider variety of tasks_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31ad7cd5-7d0a-4758-a226-12736a503825_1888x1352.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31ad7cd5-7d0a-4758-a226-12736a503825_1888x1352.png)

(from [1])

Such lackluster performance on benchmarks like [MMLU](https://huggingface.co/datasets/lukaemon/mmlu) [10], [HumanEval](https://huggingface.co/datasets/openai_humaneval) [11], and [Natural Questions](https://ai.google.com/research/NaturalQuestions) [12] reveals that imitation models do not have improved factuality, coding abilities, or problem-solving capabilities compared to base LLMs. Given that [most of an LLM’s knowledge](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20) is learned during pre-training, such a trend makes sense. We see in [1] that the imitation models can match the style of powerful LLMs like ChatGPT (see below), _but they lack the same knowledge base_. These models hallucinate more frequently, which is difficult to detect in basic human evaluations without extensive research or time investment.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06f75e02-8587-4639-a634-08d71a457391_1884x630.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06f75e02-8587-4639-a634-08d71a457391_1884x630.png)

(from [1])

**local imitation works well.** Despite the limitations of imitation models when evaluated on a broader set of tasks, we see that local imitation is actually quite effective. Learning specific behaviors of ChatGPT is possible via imitation, but we run into roadblocks when mimicking behavior more broadly; see below. Local imitation can be a useful point solution for adapting open-source LLMs to solve specific tasks or mimic proprietary models in particular scenarios.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F947cf330-086f-4803-838e-7d21ce2e84f4_948x1106.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F947cf330-086f-4803-838e-7d21ce2e84f4_948x1106.png)

(from [1])

To broadly imitate the behavior of a model like ChatGPT, we need a significantly larger and more diverse source of imitation data. However, curating this dataset might not be the best approach—_we see a much larger performance benefit from simply increasing the size of the base model_. As such, creating more powerful base LLMs might be a more promising direction for open-source LLM research compared to creating cheap imitation models.

> _“We argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.”_ - from [1]

# Final Thoughts

Although the deep learning community has embraced openness and transparency for years, the explosion in popularity of LLMs has given birth to an alternative paradigm in which development is performed with proprietary APIs that provide no access to the actual model itself. To combat this shift away from open-source, researchers have developed open-source LLM alternatives. The creation of imitation models made this area of research seem to progress incredibly fast, leading many to assume that proprietary LLMs would quickly fall out of favor. Within this overview, we have seen that such imitation LLMs have major limitations. However, the development of powerful, open-source LLMs continues to progress. Some major takeaways from this work are outlined below.

**the importance of rigorous evaluation.** Imitation models seem to perform well when qualitatively evaluated by humans. When subjected to more rigorous quantitative evaluation, however, the performance of such models is found to be somewhat lackluster (and even worse than base models in some cases)! The findings from this work highlight the importance of rigorous evaluation in research. For a field to progress, we need to be sure that techniques and models being proposed are actually improving upon those that exist.

**local imitation is still very useful.** Although imitation models are found to perform poorly when evaluated broadly, they perform quite well for any task that is included in their fine-tuning dataset. As such, _local imitation is still a useful and effective technique_. We can easily teach a smaller, open-source LLM to match the performance and behavior of a popular model like ChatGPT in a specific domain via imitation. However, we run into problems when trying to replicate the behavior of proprietary LLMs as a whole. This would require curating a massive dataset of dialogue examples for imitation fine-tuning.

**implications for open-source LLMs.** As we have seen, imitation models (although useful for local imitation and specific use-cases) are not a general-purpose solution for producing high-quality, open-source foundation models. However, we see within [1] that LLM performance continues to improve with the size and quality of the underlying base model. Such a finding indicates that the creation of larger and more powerful base models is necessary for further advancements in open-source LLMs to occur.

### New to the newsletter?

Hello! I am [Cameron R. Wolfe](https://cameronrwolfe.me/). Ph.D. in deep learning and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers build a better understanding of deep learning research via understandable overviews that explain relevant topics from the ground up. If you like this newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

[Share](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEyNzg3NDQ0MywiaWF0IjoxNzQ1NzQ1NjcxLCJleHAiOjE3NDgzMzc2NzEsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.A2fCLKas3sBYkluSLcUoWoqO0P-IjJ2-RGZLI6OIGEg)

### Bibliography

[1] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[2] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[3] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[4] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[5] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[6] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[7] Gou, Jianping, et al. "Knowledge distillation: A survey." _International Journal of Computer Vision_ 129 (2021): 1789-1819.

[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." _arXiv preprint arXiv:1503.02531_ (2015).

[9] Dylan Patel and Afzal Ahmad. Google “we have no moat, and neither does OpenAI”, 2023.

[10] Hendrycks, Dan, et al. "Measuring massive multitask language understanding." _arXiv preprint arXiv:2009.03300_ (2020).

[11] Chen, Mark, et al. "Evaluating large language models trained on code." _arXiv preprint arXiv:2107.03374_ (2021).

[12] Kwiatkowski, Tom, et al. "Natural questions: a benchmark for question answering research." _Transactions of the Association for Computational Linguistics_ 7 (2019): 453-466.

[13] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[14] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[15] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[16] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.

[17] Wang, Yizhong, et al. "Self-Instruct: Aligning Language Model with Self Generated Instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[1](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-anchor-1-127874443)

This paper is actually one of the first deep learning papers that made me interested in the field. It’s an awesome read that I would recommend to anyone!

[2](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-anchor-2-127874443)

If we similarly compute BLEU score similarity on the widely-used [SuperNaturalInstructions](https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m) dataset, we get a similarity of 61%, which is significantly higher than the broad imitation dataset created in [1].

[3](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source#footnote-anchor-3-127874443)

The number of tokens is a common way to express the size of a textual corpus. Tokenization refers to breaking text down into words or sub-words, which are then embedded and provided to a model as input; see [here](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20) for details.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Antoan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042caf1d-47da-442b-a063-c4c3b0300bc6_144x144.png)



](https://substack.com/profile/4357647-antoan)

[

![Ron Wolfe's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b48bd4-35d7-428d-8c1d-2282e78509b2_144x144.png)



](https://substack.com/profile/104601479-ron-wolfe)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Matt Gruner's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd9b6c1c-cdea-4b58-b1ad-dddb91fad763_1080x1080.jpeg)



](https://substack.com/profile/17923793-matt-gruner)

[

![Parvaz Cazi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3ab68ace-dfcb-4402-b4e9-2f9ebccf0f70_1024x1024.png)



](https://substack.com/profile/64551444-parvaz-cazi)

19 Likes∙

[1 Restack](https://substack.com/note/p-127874443/restacks?utm_source=substack&utm_content=facepile-restacks)

19

- 

[](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source/comments)

1

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Orca: Properly Imitating Proprietary LLMs

### Leveraging imitation to create high-quality, open-source LLMs...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jun 26, 2023

11

- 

[](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary/comments)

1

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07e22103-8ed0-49c8-91e1-8a3959e0f1e6_2654x500.png)

This newsletter is presented by [Cerebrium](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023). Cerebrium allows you to deploy and fine-tune ML models seamlessly, without worrying about infrastructure. Notable features include serverless GPU deployment (<1 second cold start), access to 15+ pre-trained models (e.g., FLAN-T5, GPT-Neo, Stable Diffusion and more), and support for all major ML frameworks. [Try it](https://www.cerebrium.ai/?utm_source=newsletter&utm_medium=email&utm_campaign=cameronwolfe&utm_term=main_ad&utm_content=2023) for free today! 

Enjoy deep learning? Find current research topics difficult to parse? Join subscribers from Microsoft, Tesla, Google, Meta, Salesforce and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc54c358-f355-4e51-8624-f6256ae05077_2558x1278.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc54c358-f355-4e51-8624-f6256ae05077_2558x1278.png)

(from [1])

As research progresses on large language models (LLMs), one key question that remains unanswered is whether an existing, high-quality LLM can be used to effectively train another LLM. Currently, there is a lot of debate and contention around this topic. The recent [explosion of open-source imitation models](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) initially indicated that proprietary LLMs like ChatGPT could be easily replicated at a low cost. However, [subsequent research](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source) concluded that the evaluation of such models was incomplete and misleading, finding that these models actually have large gaps in their comprehension. In this overview, we will study work [1] that aims to solve the limitations of open-source replicas of proprietary LLMs via a more robust approach. In particular, we will see that imitation learning can be made more effective by curating a larger dataset with more detailed information.

> _“As these models continue to evolve and become more powerful, an intriguing question arises: Can we use the model itself to supervise its own behavior or that of other AI models?”_ - from [1]

# Background Information

Before diving into the overview, we will cover a few ideas related to both LLMs and deep learning in general. These concepts might not be explicitly described in papers that we read. Rather, they are oftentimes referenced via a citation or assumed to be common knowledge. So, getting a basic grasp of these concepts will make this overview, and the papers it considers, easier to understand.

### Instruction Tuning

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaabd416-6722-477a-b66b-8ba6c77576af_1352x558.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaabd416-6722-477a-b66b-8ba6c77576af_1352x558.png)

(from [12])

Instruction tuning was originally proposed by [FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html) [12] and aimed to provide a form of training that teaches LLMs to solve language-based tasks in general, rather than a specific task. In particular, this is done by fine-tuning an LLM over sets of “instructions”, or input prompts—including a description of the task being solved—combined with the desired model output; see above. Recent LLMs mostly use a specific variant of instruction tuning that fine-tunes the LLM over examples of dialogue sessions, either from humans or another LLM. Usually, instruction tuning is a fine-tuning step that occurs after pre-training; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ae8df59-0e02-4669-be09-954442a42786_1492x504.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ae8df59-0e02-4669-be09-954442a42786_1492x504.png)

Instruction tuning versus other common training paradigms (from [12])

**synthetic instruction tuning.** Although humans can manually create data for instruction tuning, we can also synthetically generate this data using an LLM. There are two basic approaches for this:

- Obtain example dialogue sessions from another model (e.g., from [ShareGPT](https://sharegpt.com/)).
    
- Use a prompting framework (e.g., [self-instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework) [9]) to generate and refine high-quality dialogue examples with an LLM.
    

Both of these approaches are valid, but they have their limitations. For example, public examples of LLM dialogues tend to be biased towards certain tasks, such as creative content generation or information-seeking dialogue. Additionally, dialogues that are generated via self-instruct [9] tend to lack complexity, though this issue was mitigated by the Evol-Instruct [2] strategy that explicitly instructs and guides the LLM towards more complex generations; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc96f5288-86ba-4e47-903a-19480d338fac_1340x1286.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc96f5288-86ba-4e47-903a-19480d338fac_1340x1286.png)

(from [2])

### The System Message

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fcef28-d1f4-4f2c-9ad0-ef9caaae003c_2132x1160.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fcef28-d1f4-4f2c-9ad0-ef9caaae003c_2132x1160.png)

(from OpenAI API documentation)

Most chat-based LLMs that we interact with allow us to provide a system message; see above. This message is basically an instruction to the model that describes how it is expected to behave or respond to the user. In the [chat markup language](https://github.com/openai/openai-python/blob/main/chatml.md) used by ChatGPT and GPT-4 APIs, this system message is given the “system” role—as opposed to “user” or “assistant”—within a chat history. Generally, the system message is where we should place any instructions that should be followed by the LLM throughout the dialogue session with a user.

**modern LLMs are steerable.** Although prior LLMs (e.g., early versions of GPT-3.5-turbo) did not pay much attention to the system message, current models (e.g., GPT-4) are much more [steerable](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20). This means that we can provide detailed instructions in the system message for the LLM to follow. In practice, this property of modern LLMs can be leveraged to tweak their style or format (via the system message) to exactly match the application or task we are solving.

### Other Useful Ideas

- _Knowledge distillation and model imitation_: we have explained this idea extensively in prior overviews, but it is incredibly relevant to the analysis presented within this overview. [[link](https://cameronrwolfe.substack.com/i/127874443/model-imitation)]
    
- _Packing technique_: This is a trick used in [1] that simply concatenates multiple text sequences into a single example during training to avoid excessive padding after each sequence and improve efficiency. [[link]](https://arxiv.org/abs/2107.02027)
    
- _Chain of Thought Prompting [13]_: We have seen that encouraging an LLM to produce a problem-solving rationale along with its answer to a problem improves reasoning capabilities. Explanation tuning (more details to come) in [1] has a lot of fundamental similarities to this technique. [[link](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)]
    
- _Curriculum or progressive learning_: Instead of just training a model over all of the data that we have, we could form a particular strategy or curriculum for exposing this data to our model[1](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-1-130195416). In the case of [1], this curriculum involves first training the model over dialogue examples from ChatGPT, then performing further training over GPT-4 dialogue. This term is quite generic, as many different types of curriculums may exist. [[link](https://ronan.collobert.com/pub/2009_curriculum_icml.pdf)]
    

# The Explosion of Open-Source LLMs

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8669e7ad-7483-45b9-8f1a-f46b7146135b_512x512.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8669e7ad-7483-45b9-8f1a-f46b7146135b_512x512.png)

The future for open-source LLMs is *almost* as bright as this photo (from DreamStudio)

As LLMs began to increase in popularity, the most impactful models (e.g., GPT-3 and ChatGPT) were initially available only via paid, proprietary APIs. As we have learned in recent overviews, however, there is a [burgeoning movement](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) in the LLM community to create powerful open-source models! Many open-source models have been proposed, but this movement was especially catalyzed by the recent proposal of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [4], a suite of high-performing base models of various sizes that are trained solely on publicly-available data[2](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-2-130195416).

[Learn about LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)

### LLaMA and Imitation Models

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)

Derivative models created from LLaMA (from [5, 6, 7, 8])

The weights of LLMs within the LLaMA suite were openly released (for research purposes) and then subsequently leaked online for anyone to access. Following this leak, LLaMA quickly gained in popularity and was used to create a [variety of open-source derivative models](https://cameronrwolfe.substack.com/i/127874443/imitation-models-alpaca-vicuna-koala-and-more), which we have explored in prior overviews.

- Beyond LLaMA: The Power of Open LLMs [[link](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms)]
    
- Imitation Models and the Open-Source LLM Revolution [[link](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source)]
    

Such LLaMA derivatives were primarily created using an [imitation approach](https://cameronrwolfe.substack.com/i/127874443/model-imitation) that instruction tunes LLaMA over dialogue examples from a more powerful model (e.g., ChatGPT). These imitation models were proposed in quick succession and seemed to perform really well—_even on par with powerful models like ChatGPT in certain cases_ [6]. This led the LLM community to believe that proprietary LLMs could be easily replicated, but there’s a bit more to the story than that.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)

(from [3])

**imitation or limitation?** Although imitation models seem to perform well, we see in [prior work](https://cameronrwolfe.substack.com/i/127874443/the-false-promise-of-imitating-proprietary-llms) [3] that this is only the case on the small subset of tasks that are observed during fine-tuning. Namely, most imitation models capture the style of proprietary LLMs like ChatGPT, but they fail to capture the knowledge, reasoning capabilities, and comprehension of these models. Such limitations can easily be missed within [human evaluations](https://cameronrwolfe.substack.com/i/127874443/are-we-missing-something) of a model, as verifying whether a model’s information is factually correct requires a significant time investment.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b9de6d4-3d12-424d-8423-86c9e1bf1c1a_1790x416.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b9de6d4-3d12-424d-8423-86c9e1bf1c1a_1790x416.png)

(from [1])

Because modern LLMs are so good at generating coherent text, the differences between them can be difficult to measure, especially when the models being compared have a similar style and fluency. When imitation models are more rigorously evaluated using a broad set of quantitative benchmarks, we begin to clearly see their shortcomings. For example, the performance of [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality) [6]—one of the higher-performing imitation models created with LLaMA—is seen to fall far short of ChatGPT on more difficult and complex benchmarks; see above.

### **Why is imitation not working?**

When we study existing attempts at creating open-source replicas of proprietary LLMs via an imitation approach, most issues that we see are caused by the same problem: _we don’t have enough high-quality data for instruction tuning_. There are three basic ways in which we can generate this data:

1. Ask humans to generate the data
    
2. Use a prompting framework (e.g., self-instruct [9]) to generate synthetic data
    
3. Directly train on the outputs of existing LLMs
    

Popular LLMs like GPT-4 are trained over extensive amounts of human feedback, but generating data from humans is expensive and time consuming. To automate the collection of data, recent imitation models rely upon some variant of [self-instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework) [9] to generate a synthetic—_meaning the data is generated by an LLM rather than a human_—fine-tuning dataset. Unfortunately, datasets generated in this manner tend to lack in diversity and complexity. Plus, we run into similar problems when directly fine-tuning on LLM dialogues obtained from public APIs or [ShareGPT](https://sharegpt.com/). These datasets tend to be small-scale and homogenous, which is insufficient for creating a powerful imitation model.

> _“We conclude that broadly matching ChatGPT using purely imitation would require a concerted effort to collect enormous imitation datasets and far more diverse and higher quality imitation data than is currently available.”_ - from [3]

**the path forward.** Although existing attempts at using imitation have fallen short, there are a few different ways we can move forward. As proposed in [3], we could begin by creating more powerful, open-source base LLMs that could serve as a better “starting point” for instruction tuning. Prior work shows that using a better underlying base LLM [drastically improves the performance](https://cameronrwolfe.substack.com/i/127874443/are-imitation-models-actually-useful) of resulting imitation models. This area is already being extensively explored, as we see with the proposal of awesome, open-source foundation models like [Falcon](https://falconllm.tii.ae/) or [MPT](https://www.mosaicml.com/blog/mpt-7b)[3](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-3-130195416).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec485bd5-6b09-4955-aa44-08f433533f73_2134x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec485bd5-6b09-4955-aa44-08f433533f73_2134x838.png)

(from [1])

Alternatively, we could consider ways to improve or expand existing datasets used for imitation learning. Current work relies solely upon prompt and response pairs generated from an LLM; see above. Within this overview, we will refer to such dialogues as “shallow” imitation examples, as they only contain information about the proprietary LLM’s response to a prompt. Going beyond shallow imitation, this overview will explore the idea of augmenting synthetic instruction tuning datasets with more detailed outputs from the proprietary LLM, such as:

- Explanation traces
    
- Step-by-step thought processes
    
- Complex instructions
    

The imitation model can learn from extra information produced by a proprietary model during fine-tuning. We want imitation datasets that are large and diverse. In this overview, however, _we will see that the type and granularity of data used can make a huge difference too_. Such extra information can allow smaller, open-source LLMs to learn the reasoning process followed by a more powerful model.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2823ed58-0330-49a9-aaf3-060525eae629_2124x342.png)

### Now from our partners!

- [Rebuy Engine](https://www.rebuyengine.com/) is the Commerce AI company. They use cutting edge deep learning techniques to make any online shopping experience more personalized and enjoyable.
    
- [KUNGFU.AI](https://urldefense.com/v3/__http://KUNGFU.AI__;!!BuQPrrmRaQ!i4L4Oc-1VDW1AHrfWwPg9wcLgB7A4UgD2LsIn9-L7LvnJJbz2Sh6c3ee4MnN_sn04GFwufC-Elb0tnEnztEylFoQBdkEJgf7$) partners with clients to help them compete and lead in the age of AI. Their team of AI-native experts deliver strategy, engineering, and operations services to achieve AI-centric transformation.
    
- [MosaicML](https://www.mosaicml.com/) enables you to train and deploy large AI models on your data and in your secure environment. Try out their tools and platform [here](http://mosaicml.me/cameronrwolfe) or check out their [open-source, commercially-usable LLMs](https://www.mosaicml.com/blog/mpt-7b).
    

---

# Properly Learning to Imitate

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d8e0c9-5abb-498f-8253-e2620de2ab73_1878x1322.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d8e0c9-5abb-498f-8253-e2620de2ab73_1878x1322.png)

(from [1])

Aiming to mitigate issues with existing imitation models, authors in [1] propose a 13 billion parameter imitation LLM, referred to as Orca. Like prior imitation models, Orca is based upon the LLaMA suite of LLMs, but it is fine-tuned using more than just a small set of “shallow” imitation examples. More specifically, Orca differentiates itself from prior work in two main ways:

- A much larger and more comprehensive imitation dataset
    
- Injecting detailed explanation traces into each instruction tuning example
    

The resulting model performs quite well across a variety of benchmarks, allowing the gap between imitation models and proprietary LLMs (e.g., ChatGPT or GPT-4) to be narrowed; see below. As we will see, however, GPT-4 is still much better.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fd3ba2e-0567-4c27-adba-8df8ca608c47_2138x872.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fd3ba2e-0567-4c27-adba-8df8ca608c47_2138x872.png)

(from [1])

**bigger and better data.** Orca selectively samples tasks from the [FLAN collection](https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html) [10]—a massive data source for instruction tuning—and acquires millions of responses from both ChatGPT and GPT-4 over complex prompts from each of these tasks. Using the system message, authors encourage these models to explain their response with added details, thus providing an “explanation trace” for each output generated by an LLM. Such an approach has a massive impact on model quality, as it provides a richer source of information from which the imitation model can learn. We will refer to this approach as “explanation tuning”—it’s just instruction tuning over data that contains explanation traces!

> _“Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.”_ - from [1]

**relation to prior work.** In a prior overview, we saw that LLaMA-based imitation models [fall far short](https://cameronrwolfe.substack.com/i/127874443/the-false-promise-of-imitating-proprietary-llms) of imitating proprietary LLMs. To close the capability gap between imitation models and proprietary LLMs, we would need an imitation dataset that is significantly larger and more diverse. Prior work in [3] claims that obtaining such a dataset is too difficult, indicating that imitation models are a dead end. However, authors in [1] do exactly this (i.e., generate a massive and complex imitation dataset) to achieve a breakthrough in imitation model quality.

### A better approach for imitation learning…

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb48c6e83-f389-44f0-8b9c-447b985a044c_1074x292.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb48c6e83-f389-44f0-8b9c-447b985a044c_1074x292.png)

(from [1])

Orca’s breakthrough in imitation model quality can be attributed to its much larger, more detailed, and more complex imitation dataset; see above. Let’s explore the details of this dataset, focusing upon how proprietary models can be prompted to output step-by-step problem solving explanations that are a much more powerful learning signal for open-source imitation models.

**explanation tuning.** Prior imitation models are trained over pairs of prompts and associated responses generated from an LLM. Although this approach can teach the imitation model to replicate or memorize the teacher model’s output, there is not much else that can be learned from the model’s response alone—_this information is shallow and lacks information regarding how or why a response was produced_. In [1], authors explore an alternative approach that trains an imitation model how to replicate the reasoning process of the teacher model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F128b20f5-1e53-46b2-8bc4-cd2facefca8a_1174x1334.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F128b20f5-1e53-46b2-8bc4-cd2facefca8a_1174x1334.png)

(from [1])

To do this, we just need to prompt the teacher model to output a detailed explanation along with its normal response. Drawing upon ideas like [zero-shot CoT prompting](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [11], we can encourage the model to produce detailed explanations with each of its responses by just tweaking the system message; see above. Then, we can fine-tune the imitation model using both the response and the explanation as a training signal. As we have seen in prior work [11], teaching an LLM to output such detailed explanation traces with each of its answers can lead to large improvements on reasoning tasks and complex instruction following.

**creating the dataset.** To fine-tune Orca, a massive imitation dataset is created by sampling from the millions of instructions contained in the [FLAN collection](https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html) as shown in the table below. The resulting set of instructions is called FLAN-5M.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88afe067-3be1-4a87-ab3d-3fb76433dc26_976x274.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88afe067-3be1-4a87-ab3d-3fb76433dc26_976x274.png)

(from [1])

The FLAN-5M instruction set is augmented with responses and explanations from ChatGPT obtained with the OpenAI API. Similarly, using a smaller set of sampled instructions called FLAN-1M (i.e., we basically just sub-sample the original set of 5M instructions), a similar procedure is performed using GPT-4, producing a dataset of 6 million total instruction examples paired with a response and explanation from a proprietary teacher model. Interestingly, authors in [1] note that collecting data from each of these models takes several weeks—even using the [Azure OpenAI service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)—due to rate limits; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d686965-2723-4267-b72c-b9b8442df33b_2132x560.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d686965-2723-4267-b72c-b9b8442df33b_2132x560.png)

(from [1])

**progressive learning.** Instead of training on all of this data together, we can achieve improved performance by fine-tuning Orca over ChatGPT-based explanations first, then fine-tuning on explanations from GPT-4 afterwards. Given that Orca is based upon a smaller LLaMA model that is significantly less powerful than proprietary LLMs, this progressive learning approach allows the imitation model to first learn from “easier” examples, prior to learning from the more detailed explanations of a powerful model like GPT-4. The positive impact of this approach is likely due to the fact that GPT-4 tends to produce longer and more intricate explanations that are harder to learn from; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf810a2-4200-41d9-9241-7817195dabbc_1878x1018.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf810a2-4200-41d9-9241-7817195dabbc_1878x1018.png)

(from [1])

### Explanation-based imitation learning is effective!

Orca is compared to a variety of different baselines, including [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality) [6], text-davinci-003 (i.e., GPT-3.5), ChatGPT, and GPT-4. Authors in [1] consider a suite of different benchmarks that include writing, comprehension, and reasoning tasks; see below. Orca’s evaluation strategy is made quite comprehensive as to avoid the issues with misleading or incomplete evaluation results that plagued prior imitation models. Notably, we see in [1] that benchmarks composed of standardized tests offer a surprisingly robust evaluation framework.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750f60aa-f79b-4b97-8000-c533509ba924_1074x292.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750f60aa-f79b-4b97-8000-c533509ba924_1074x292.png)

(from [1])

**open-ended generation.** When evaluated on open-ended generation tasks, Orca outperforms Vicuna by a large margin in all experimental settings; see below. Here, performance is measured by considering a reference model (e.g., ChatGPT or GPT-4) and prompting GPT-4 to determine whether the output produced by the candidate model is better than the reference model’s output.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f876463-d80d-48b0-a9e4-d588900ea796_1178x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f876463-d80d-48b0-a9e4-d588900ea796_1178x606.png)

(from [1])

We see in these experiments that Orca maintains 95% of ChatGPT quality and 85% of GPT-4 quality across datasets. Although these metrics indicate a significant improvement in performance compared to prior imitation models, we should keep in mind that [LLM-based evaluations are imperfect](https://ehudreiter.com/2023/05/22/future-of-nlg-evaluation/) and still being explored. As such, these results, although positive, could be misleading.

**reasoning.** Orca continues to perform similarly to ChatGPT across reasoning benchmarks. For example, across (nearly) all topics on the [AGIEval](https://github.com/microsoft/AGIEval) and [BigBench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard) datasets, Orca comes near or exceeds the performance of ChatGPT!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ba3b36-5320-456f-b589-ff704fc2a4d5_1918x668.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ba3b36-5320-456f-b589-ff704fc2a4d5_1918x668.png)

(from [1])

Although Orca’s performance on standardized exams still falls below that of ChatGPT in certain cases, we see that work in [1] makes significant progress towards bridging the gap with proprietary LLMs compared to prior imitation models; see below. Although it might not come as a surprise, GPT-4 is still a clear frontrunner in performance across nearly all tasks that are considered[4](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-4-130195416).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F710e343f-ebce-4383-b538-69626db538c4_1180x516.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F710e343f-ebce-4383-b538-69626db538c4_1180x516.png)

**other findings.** Beyond the main empirical results presented in [1], we see that the proposed curriculum (or progressive) learning approach—where the model is first fine-tuned over 5M ChatGPT dialogue examples then 1M examples from GPT-4—has a large and positive impact on Orca’s performance. Additionally, Orca consistently underperforms ChatGPT in modeling long sequences.

# Closing Remarks

> _“Our findings indicate that Orca significantly outperforms other open-source smaller models. Moreover, in some settings, it can match or even surpass the quality of ChatGPT, although a substantial gap with GPT-4 still remains.”_ - from [1]

Research on open-source LLMs is constantly evolving. One week, we think that proprietary LLMs have [completely lost their moat](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), and the next week we find out that open-source (imitation) models are far worse than originally claimed. Although it seemed like [imitation models were a dead end](https://cameronrwolfe.substack.com/i/127874443/the-false-promise-of-imitating-proprietary-llms) only a few weeks ago, we see in this overview that imitation is a valid approach! All we need is a bigger and better dataset. The major takeaways from this work are outlined below.

**learning from step-by-step instructions.** Prior work on imitation models relied upon simple prompt-response pairs for training. We see here that augmenting such data with detailed explanation traces allows the resulting model to learn from a much richer source of information. Rather than memorizing a model’s response for a small set of examples, we can allow the proprietary LLM’s problem-solving process to be replicated. As a result, the approach in [1] enables imitation model performance to generalize beyond data seen during fine-tuning.

> _“We emphasize the crucial role of data size and coverage when it comes to aligning smaller models to their more powerful counterparts, like GPT-4.”_ - from [1]

**lots of imitation data.** One of the main problems with prior imitation models is that they only performed well on tasks that were similar to data seen in their fine-tuning datasets. Given this particular limitation, we clearly need larger imitation datasets with more coverage. Although prior work indicated that producing such a dataset would be too difficult, we see in [1] that it is possible. Given a much larger and more comprehensive dataset (i.e., millions of examples), we can make imitation models perform much better than before.

**remaining work.** Orca’s performance, although impressive, still falls short of the best proprietary LLMs—_more work has to be done to make open-source LLMs truly competitive_. Closing this gap will mostly likely be a product of multiple ongoing initiatives, such as imitation learning, creating better foundation models, and curating better publicly-available datasets for instruction tuning and LLM refinement. However, open-source offerings should not be underestimated, as they will continue to improve alongside their proprietary counterparts.

### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Ph.D. in deep learning and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that clearly explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

[Share](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoyOTczNjUyMSwicG9zdF9pZCI6MTI3ODc0NDQzLCJpYXQiOjE2ODc0NDQ1NDEsImV4cCI6MTY5MDAzNjU0MSwiaXNzIjoicHViLTEwOTI2NTkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.qXNphO4RV2BVkpCuqpHfjRGuku3gz9-XYHE24S-d568)

### Bibliography

[1] Mukherjee, Subhabrata, et al. "Orca: Progressive Learning from Complex Explanation Traces of GPT-4." _arXiv preprint arXiv:2306.02707_ (2023).

[2] Xu, Can, et al. "Wizardlm: Empowering large language models to follow complex instructions." _arXiv preprint arXiv:2304.12244_ (2023).

[3] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[4] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[5] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[6] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[7] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[8] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.

[9] Wang, Yizhong, et al. "Self-Instruct: Aligning Language Model with Self Generated Instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[10] Longpre, Shayne, et al. "The flan collection: Designing data and methods for effective instruction tuning." _arXiv preprint arXiv:2301.13688_ (2023).

[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." _arXiv preprint arXiv:2205.11916_ (2022).

[12] Wei, Jason, et al. "Finetuned language models are zero-shot learners." _arXiv preprint arXiv:2109.01652_ (2021).

[13] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[1](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-anchor-1-130195416)

Interestingly, the generic idea of curriculum learning was proposed in 2009 by Yoshua Bengio. This idea actually predates a lot of concepts from modern deep learning!

[2](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-anchor-2-130195416)

This is important simply because anyone can replicate the training process. Every component of LLaMA can be recreated by anyone with sufficient compute resources.

[3](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-anchor-3-130195416)

In fact, [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)—including [instruction tuned versions](https://huggingface.co/mosaicml/mpt-30b-chat)—was released this week! Previously, we only had a 7 billion parameter version.

[4](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary#footnote-anchor-4-130195416)

However, I’m increasingly thinking that these comparisons to GPT-4 might be unfair. Recently, [rumors](https://twitter.com/soumithchintala/status/1671267150101721090?s=20) have circulated that GPT-4 is not a single model, but rather an ensemble of several, independently-trained LLMS that work together.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![James Le's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd92c7fe-0109-44bd-a10f-19c8bd4840c0_3596x2514.jpeg)



](https://substack.com/profile/6009523-james-le)

[

![林煒清's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc965b698-e736-43c7-8d77-6d9a1eee99d1_96x96.jpeg)



](https://substack.com/profile/3322218-679771526e05)

[

![Knowing less's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0a0cf901-1e3c-4f87-b7c6-ad7be4e8720e_144x144.png)



](https://substack.com/profile/41949021-knowing-less)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

11 Likes∙

[1 Restack](https://substack.com/note/p-130195416/restacks?utm_source=substack&utm_content=facepile-restacks)

11

- 

[](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary/comments)

1

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Democratizing AI: MosaicML's Impact on the Open-Source LLM Movement

### How high-quality base models unlock new possibilities for an entire industry...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 03, 2023

23

- 

[](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact/comments)

3

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b60ffa-37a0-449f-aa55-92527f4982fa_512x132.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b60ffa-37a0-449f-aa55-92527f4982fa_512x132.png)

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F248521f9-d6ad-4826-b806-ace0848d46cf_2670x1510.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F248521f9-d6ad-4826-b806-ace0848d46cf_2670x1510.png)

(from [1, 2])

Recently, we have overviewed a lot of current research on the creation of open-source large language models (LLMs). Across all of this work, models are created using a common framework with a few simple components; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3f6474-3f89-4178-9583-ec53adf92d74_2738x522.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3f6474-3f89-4178-9583-ec53adf92d74_2738x522.png)

Multi-step process for creating and refining an LLM (from [12, 13])

Although this framework has several steps, the first step is arguably the most important. Creating a more powerful base model via extensive, high-quality [pre-training](https://cameronrwolfe.substack.com/i/85568430/language-modeling) enables better results when the LLM is [refined](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior) via supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Then, downstream applications are better due to the use of an improved model. The pre-trained (base) model is the common starting point for any LLM application.

Until recently, open-source base models either [performed poorly](https://cameronrwolfe.substack.com/i/113386783/llama-vs-sota-llms) compared to their proprietary counterparts or could only be used for [research](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone). However, this changed with the release of MPT-7B and MPT-30B [1, 2] by MosaicML. These open-source base models achieve impressive levels of performance, are free for commercial use, and come with an entire suite of efficient software for training, fine-tuning, and evaluating LLMs. These open-source tools enable a wide variety of specialized use cases for LLMs to be explored at a significantly reduced cost, making them a powerful resource for practitioners in AI.

## Faster LLMs and Longer Context Lengths

The MPT-7B/30B models are based upon a typical, [decoder-only transformer](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20) architecture. However, a few key modifications are made, including:

- [ALiBi](https://docs.mosaicml.com/projects/composer/en/stable/method_cards/alibi.html) [6] (instead of normal position embeddings)
    
- [Low precision layer norm](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html)
    
- [Flash Attention](https://github.com/HazyResearch/flash-attention) [7]
    

Within this section, we will learn about each of these components, how they work, and their impact on LLMs. To fully understand the details of this section, it might be useful to review the following concepts:

- Self-Attention [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
    
- Causal Self-Attention (used by decoder-only LLMs) [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
    

#### ALiBi enables context length extrapolation

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc541cc8-a513-4bcc-896e-97c5e8a37dde_832x800.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc541cc8-a513-4bcc-896e-97c5e8a37dde_832x800.png)

Embedding a sequence of tokens within an LLM

In a vanilla transformer architecture, we create an input sequence of tokens by first [tokenizing](https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained) the raw text and looking up the embedding (each token in the tokenizer’s vocabulary has a unique embedding) for each token. Then, we add a position embedding to each token embedding, thus injecting positional info into the embedding of each token in the sequence; see above. This is necessary because the [self-attention operation](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20) is agnostic to the position of each token in the sequence. Although position embeddings work well, there’s one big problem: _they struggle to generalize to sequences longer than those seen during training_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0901a03-d011-47b5-b110-72ea9864b814_1878x756.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0901a03-d011-47b5-b110-72ea9864b814_1878x756.png)

(from [6])

**the solution.** Attention with Linear Biases **(**ALiBi) [6] solves this problem by getting rid of position embeddings altogether. Instead, positional information is injected into the transformer as part of the self-attention operation by adding an additive penalty to the key-query attention score; see above. We should recall that self-attention computes an attention score between each pair of tokens within a sequence. ALiBi operates by adding a static, non-learned bias (or penalty) to this score that is proportional to the distance between the pair of tokens; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F319a920b-4f24-48f4-8c8d-c49bec6be67e_2370x1412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F319a920b-4f24-48f4-8c8d-c49bec6be67e_2370x1412.png)

Computing the key-query attention score for a particular pair of tokens

Such an approach is impactful because it depends on the pairwise distance between tokens rather than the absolute positions of tokens within a sequence. This quantity is less dependent upon the length of the underlying sequence and allows ALiBi to generalize much better to sequences that are longer than those seen during training; see below. As we will see, MPT models (which use ALiBi) can be trained to support larger context lengths compared to most open-source alternatives _and can even extrapolate to sequences as long as 84K tokens_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffefeb19c-9cee-412b-b0e0-f8ade93ae581_1874x1030.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffefeb19c-9cee-412b-b0e0-f8ade93ae581_1874x1030.png)

(from [6])

#### Faster Inference

Due to their use of low precision layer norm and FlashAttention [7], the MPT models have very fast training and inference speeds (i.e., `1.5-2X` faster than similarly-sized [LLaMA models](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [3] using standard [HuggingFace inference pipelines](https://huggingface.co/blog/inference-update)). Going further, the weights of these models can be ported to optimized modules like [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) or [ONNX](https://github.com/onnx/models) to enable even faster inference.

**low precision layer norm.** Put simply, low precision layer norm performs the operations from a [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) module in 16-bit precision. Although such an approach can cause loss spikes in some cases, it improves hardware utilization and, in turn, speeds up both training and inference. Using low precision layer norm also has minimal impact on the model’s final performance.

**flash attention.** In its canonical form, self-attention is an `O(N^2)` operation, where `N` is the length of the input sequence. In order to improve the efficiency of this operation, many approximate attention variants have been proposed, such as:

- Reformer [[link](https://arxiv.org/abs/2001.04451)]
    
- SMYRF [[link](https://arxiv.org/abs/2010.05315)]
    
- Performer [[link](https://arxiv.org/abs/2009.14794)]
    

The goal of most of these techniques is to derive a “linear” variation of attention—a similar/approximate operation with a complexity of `O(N)`. Although these variants achieve a theoretical reduction in [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning), _many of them do not achieve any wall-clock speedup in practical scenarios_! Flash attention solves this problem by reformulating the attention operation in an IO-aware manner; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55a6c128-6a6a-4a17-b587-06d5bb90f83a_1896x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55a6c128-6a6a-4a17-b587-06d5bb90f83a_1896x1008.png)

(from [7])

The hardware-related details of how FlashAttention is implemented are beyond the scope of this post. However, the resulting efficient attention implementation has a variety of positive benefits. For example, FlashAttention can:

- Speed up BERT-large [10] training time by 15%
    
- Improve training speed by `3X` for [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) [11]
    
- Enable longer [context lengths](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering) for LLMs (due to better memory efficiency)
    

For more details on FlashAttention, check out the writeup below.

[FlashAttention](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)

## MPT-7B: A Commercially-Usable LLaMA-7B

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74aa91e1-1670-4358-ae58-ce9d2a0d2a40_1774x788.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74aa91e1-1670-4358-ae58-ce9d2a0d2a40_1774x788.png)

The total compute cost of training the MPT-7B model and fine-tuning various derivatives of this model (from [1])

Proposed in [1], MPT-7B is an open-source, commercially-usable language foundation model that broadly matches the performance of similarly-sized, open-source base models like [LLaMA-7B](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [3] (which is _not_ commercially-usable!). Following the lessons of [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [4], MPT-7B is pre-trained over a large corpus—one trillion tokens in total—of diverse, publicly-available text. The code used to train, fine-tune, and evaluate MPT-7B is completely open-source, _making this model a great resource or starting point for practitioners looking to fine-tune their own specialized LLM for solving a variety of different downstream applications_!

#### Creating the Base Model

Due to its modified architecture, MPT-7B has several desirable properties, such as the ability to generalize to much longer context lengths and faster inference speeds. Additionally, we see in [1] that this modified architecture leads to the elimination of [loss spikes](https://cameronrwolfe.substack.com/i/73746319/understanding-opt) during pre-training of MPT-7B, allowing the model to be pre-trained without any human intervention[1](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-1-131642185) (assuming that any [hardware failures](https://twitter.com/jefrankle/status/1654495486919729152?s=20) are handled automatically within the LLM’s training code)!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ef7a122-256a-4d11-a424-434328c36d50_1684x980.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ef7a122-256a-4d11-a424-434328c36d50_1684x980.png)

MPT-7B only experiences hardware failures, which can be automatically resolved, during its pre-training process (from [1])

**training process.** Although most LLMs are trained using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html), MPT adopts the Lion optimizer [8], which improves the stability of the training process. The entire training framework is based upon PyTorch’s [Fully Sharded Data Parallel (FSDP)](https://github.com/huggingface/blog/blob/main/pytorch-fsdp.md) package and uses no pipeline or tensor parallelism (see “training system” section [here](https://cameronrwolfe.substack.com/i/104244919/how-does-palm-work)). To put it simply, the training framework for MPT-7B, which is [completely open-sourced](https://github.com/mosaicml/llm-foundry), uses popular/common components, but makes a few useful changes that are found to improve the stability of training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6cae9d-b4ec-4e62-a076-9d07bb05667c_1690x1566.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc6cae9d-b4ec-4e62-a076-9d07bb05667c_1690x1566.png)

(from [1])

**the data.** The textual corpus used to train MPT-7B is a custom mixture of publicly-available datasets (mostly English language); see above. In [1], we see that the amount of data used to train MPT-7B is quite large—1T tokens in total. For comparison, open-sources models like [Pythia](https://huggingface.co/EleutherAI/pythia-1.4b) and [StableLM](https://github.com/Stability-AI/StableLM) pre-train on 300B and 800B tokens, respectively. Interestingly, we see that the authors of [1] adopt a very particular tokenizer—the [GPT-NeoX-20B](https://huggingface.co/docs/transformers/model_doc/gpt_neox) BPE tokenizer[2](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-2-131642185)—for their model. This tokenizer is desirable because it is trained on a large, diverse dataset and handles spaces more consistently than other popular tokenizers.

> _“This tokenizer has a number of desirable characteristics, most of which are relevant for tokenizing code: trained on a diverse mix of data that includes code, applies consistent space delimitation (unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces), and contains tokens for repeated space characters.”_ - from [1]

As practitioners, we should always be aware of the tokenizer being used by our model. This choice—_although typically ignored or overlooked_—can drastically impact our results. For example, [code-based language models](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code) need a tokenizer that handles whitespace in a particular manner, while [multilingual language models](https://cameronrwolfe.substack.com/p/many-languages-one-deep-learning) have a variety of unique tokenization considerations.

#### How does it perform?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F765c9507-34d8-4caa-b20a-afbe2090c10f_1822x1160.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F765c9507-34d8-4caa-b20a-afbe2090c10f_1822x1160.png)

(from [1])

MPT-7B is compared to a variety of open-source models (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone), [StableLM](https://github.com/Stability-AI/StableLM), [Pythia](https://github.com/EleutherAI/pythia), [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox), [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15), and [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)) on standard benchmarks. As shown above, LLaMA-7B achieves drastic improvements over open-source alternatives, while MPT-7B matches or exceed LLaMA’s performance. Recent open-source LLMs are much better than their predecessors! _LLaMA-7B and MPT-7B are both incredibly high-performing base models compared to other open-source models_. However, MPT-7B can be used commercially, while LLaMA can only be used for research.

#### Derivatives of MPT-7B

In addition to releasing the MPT-7B base model, authors in [1] leverage the open-source training code for MPT to fine-tune several different derivatives of the base model (outlined below). Fine-tuning is very cheap compared to pre-training an LLM from scratch (i.e., _10-100X reduction in time and cost, if not more_). As such, most of the time and effort in developing MPT-7B went into creating the base model, which serves as a starting point for fine-tuning the models below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea7cf38d-e111-4119-be35-750d2311465d_1506x1606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea7cf38d-e111-4119-be35-750d2311465d_1506x1606.png)

(from [1])

**MPT-StoryWriter-65K (commercial)** is a version of MPT-7B that has been fine-tuned on data with very long [context lengths](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering). In particular, authors in [1] leverage the [books3 dataset](https://huggingface.co/datasets/the_pile_books3), which contains excerpts from fiction books, to create a dataset for fine-tuning (i.e., just using the [next-token prediction objective](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20)) with a 65K token context length. Due to the use of ALiBi [6] and FlashAttention [7], MPT-StoryWriter-65K can be feasibly trained over such large inputs, used to consume the entirety of The Great Gatsby (68K tokens) to write an epilogue (see above), and even generalized to process sequences lengths as long as 84K tokens.

> _“We expect LLMs to treat the input as instructions to follow. Instruction finetuning is the process of training LLMs to perform instruction-following in this way. By reducing the reliance on clever prompt engineering, instruction finetuning makes LLMs more accessible, intuitive, and immediately usable.”_ - from [1]

**MPT-7B-Instruct (commercial)** and **MPT-7B-Chat (non-commercial)** are [instruction tuned](https://cameronrwolfe.substack.com/i/130195416/instruction-tuning) versions of MPT-7B. The instruct variant is fine-tuned over data from [Dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and the [Helpful and Harmless dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf), while the chat model is trained with data from sources like [ShareGPT](https://sharegpt.com/), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), and [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k). As outlined by the quote above, instruction tuning takes a pre-trained language model and modifies its style or behavior to be more intuitive and accessible, usually with an emphasis upon instruction following or problem solving.

## MPT-30B: An Open-Source GPT-3 Alternative

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e84aa1d-81d3-4d4c-8002-1ef03cf80983_1478x898.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e84aa1d-81d3-4d4c-8002-1ef03cf80983_1478x898.png)

MPT-30B improves upon MPT-7B in all performance categories (from [2])

Shortly after its proposal, the MPT-7B model gained significant recognition in the AI research community—_it even amassed over 3M downloads on HuggingFace_! The success of MPT-7B was no surprise, as it provided a commercially-usable alternative to the [incredibly popular](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) LLaMA-7B model. Riding this momentum, researchers at MosaicML followed MPT-7B with a slightly larger model, called MPT-30B [2], that was found to match or exceed the performance of [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [9]. As such, the proposal of MPT-30B continues the trend of making commercially-usable versions of powerful base LLMs available to anyone.

#### Diving Deeper into MPT-30B

MPT-30B shares the same, modified decoder-only architecture as MPT-7B, which uses FlashAttention and low precision layer norm for improved efficiency. Overall, the models are quite similar aside from MPT-30B being larger. Interestingly, the size of MPT-30B was chosen very specifically. A model of this size is feasible to deploy on a single GPU using 8 or 16-bit precision, while alternatives like [Falcon-40B](https://falconllm.tii.ae/) are slightly too large to be deployed in this manner.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c8bbe8d-ec2d-41b7-9d76-0f624ccda990_926x1424.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c8bbe8d-ec2d-41b7-9d76-0f624ccda990_926x1424.png)

(from [2])

**what’s different?** MPT-30B is different from MPT-7B in two main ways:

- Pre-training data mixture
    
- Context length
    

The pre-training dataset for MPT-30B is similar to that of MPT-7B, but the mixture of data is slightly different; see above. Additionally, MPT-30B is (partially) trained using a 8K context length, whereas most other open-source models (e.g., LLaMA, Falcon, and MPT-7B) are trained using a shorter context length of 2K tokens[3](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-3-131642185). More specifically, we see that MPT-30B uses a [training curriculum](https://cameronrwolfe.substack.com/i/130195416/other-useful-ideas) in which the model is first trained with a 2K context length, then switches to an 8K context length later in training. During this second phase, the proportion of code in the dataset is increased by `2.5X`, leading the resulting model to have improved coding abilities compared to other open-source LLMs.

**model variants.** In addition to the MPT-30B base model, authors in [2] release chat and instruct variants of MPT-30B. These models follow a similar training strategy as MPT-7B-Instruct and MPT-7B-Chat. However, the data used for instruction tuning is significantly expanded for both of these models. Interestingly, MPT-30B-Chat is found to have impressive programing skills!

#### Does it perform well?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee9b9d24-6beb-4ff1-adbe-634478d47acd_1768x614.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee9b9d24-6beb-4ff1-adbe-634478d47acd_1768x614.png)

(from [2])

In addition to outperforming MPT-7B across a variety of categories, MPT-30B achieves comparable performance to top open-source alternatives like LLaMA-30B and Falcon-40B; see above. In general, we see that MPT-30B lags behind Falcon and LLaMA in solving text-based tasks, but tends to outperform these models on programming-related problems (likely due to the higher ratio of code in the pre-training dataset!). Notably, we see that MPT-30B outperforms GPT-3 on a variety of [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe34cef96-85ff-41a2-99c0-1fc1b18c7b59_1816x424.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe34cef96-85ff-41a2-99c0-1fc1b18c7b59_1816x424.png)

(from [2])

With this result in mind, it seems that models like MPT-30B could potentially lay the foundation for open-source LLM applications that rival the quality of proprietary systems. All we need is sufficient refinement and fine-tuning!

## Final Remarks

> _“You can train, finetune, and deploy your own private MPT models, either starting from one of our checkpoints or training from scratch”_ - from [2]

The foundation models provided by MosaicML are a huge step forward for the open-source LLM community, as they provide commercially-usable LLMs that are comparable to popular base models like LLaMA and GPT-3. However, this open-source offering goes beyond the MPT models themselves—it includes an [open-source codebase for training LLMs](https://github.com/mosaicml/llm-foundry), a variety of [online demos](https://huggingface.co/mosaicml)[4](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-4-131642185), and more.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d9b5a5-2f9e-46c9-8bd5-7f4c1dc0b362_1818x1230.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d9b5a5-2f9e-46c9-8bd5-7f4c1dc0b362_1818x1230.png)

(from [2])

The MPT-7B and 30B models come with an entire ecosystem of open-source tools that can be used to create specialized/personalized LLMs. Given that creating the base model is the most expensive aspect of any LLM-based system (see above), these tools significantly lower the barrier to entry for working with LLMs and provide a starting point for solving a variety of downstream applications. Remember, [fine-tuning is extremely effective](https://magazine.sebastianraschka.com/i/125373356/finetuning-task-specific-llms-for-your-business-needs) (i.e., hard to beat by just prompting a more generic LLM) when we have a particular task that we are trying to solve!

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that clearly explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

[Share Deep (Learning) Focus](https://cameronrwolfe.substack.com/?utm_source=substack&utm_medium=email&utm_content=share&action=share)

#### Bibliography

[1] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _MosaicML_, 5 May 2023, www.mosaicml.com/blog/mpt-7b.

[2] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” _MosaicML_, 22 June 2023, www.mosaicml.com/blog/mpt-30b.

[3] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[4] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[5] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.” _arXiv preprint arXiv:2205.01068_ (2022).

[6] Press, Ofir, Noah A. Smith, and Mike Lewis. "Train short, test long: Attention with linear biases enables input length extrapolation." _arXiv preprint arXiv:2108.12409_ (2021).

[7] Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." _Advances in Neural Information Processing Systems_ 35 (2022): 16344-16359.

[8] Chen, Xiangning, et al. "Symbolic discovery of optimization algorithms." _arXiv preprint arXiv:2302.06675_ (2023).

[9] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[10] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[11] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[12] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[13] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-anchor-1-131642185)

The amount of human intervention required to resolve loss spikes and various hardware failures during LLM pre-training was massive in prior work. As an example, one can look at the [pre-training logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf?fbclid=IwAR1gSseT67AGnNprJRdiW91Pf7eW1b82Z3pYshE4CYGT_-AKVnCUdaIdmm8) from [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15) [5].

[2](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-anchor-2-131642185)

For more on BPE tokenization, check out [this article](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt). As we’ve [seen previously](https://twitter.com/cwolferesearch/status/1659608479089278978?s=20), tokenizers are used to convert raw text into a sequence of individual tokens that can be fed to a language model. Each token, which resides in the model’s vocabulary of viable tokens, has its own embedding that is stored and updated by the language model.

[3](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-anchor-3-131642185)

Due to the use of ALiBi, MPT-7B can generalize beyond a 2K context length. However, the model was pre-trained using a 2K context length and can only extrapolate a reasonable amount. To enable inference with longer context lengths (e.g., as with the StoryWriter LLM), we must first fine-tune the model with longer context lengths.

[4](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact#footnote-anchor-4-131642185)

All MPT models are compatible with HuggingFace. They are subclasses of HuggingFace’s PreTrainedModel class.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![John Kalafut's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9d9de29f-9ee8-4229-925c-1a4426bd96bb_144x144.png)



](https://substack.com/profile/48402682-john-kalafut)

[

![LaSalle Browne's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7bc42b17-ab95-461d-b843-b9cea7a630b3_144x144.png)



](https://substack.com/profile/87305971-lasalle-browne)

[

![Suyash Harlalka's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39df6efd-bba0-4c96-b81b-1215b880c53f_144x144.png)



](https://substack.com/profile/38321012-suyash-harlalka)

[

![CarolG's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0ce15ba5-d6b5-481f-bc45-6850d98197e0_144x144.png)



](https://substack.com/profile/4041140-carolg)

[

![Rupak Ganguly's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54de30b2-7d34-4326-925d-357c2ccc4492_265x266.png)



](https://substack.com/profile/4066279-rupak-ganguly)

23 Likes∙

[3 Restacks](https://substack.com/note/p-131642185/restacks?utm_source=substack&utm_content=facepile-restacks)

23

- 

[](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Falcon: The Pinnacle of Open-Source LLMs

### The gap between open-source and proprietary LLMs continues to shrink...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 10, 2023

18

- 

[](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source/comments)

1

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0a86e53-1869-433a-bf3a-096b36afe07b_1667x729.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0a86e53-1869-433a-bf3a-096b36afe07b_1667x729.png)

This newsletter is presented by [Lightly](https://www.lightly.ai/?utm_source=Newsletter&utm_medium=Email&utm_campaign=Lightly). By using self-supervised and active learning, Lightly can make data annotation pipelines more efficient and quickly identify the best subsets of data for training your model. Check them out [here](https://www.lightly.ai/?utm_source=Newsletter&utm_medium=Email&utm_campaign=Lightly) or start using their [GitHub repo](https://github.com/lightly-ai/lightly) that already has over 2000 stars.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F486ee1cd-12e6-417e-9b5a-2ae7f8a6faa2_2206x1126.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F486ee1cd-12e6-417e-9b5a-2ae7f8a6faa2_2206x1126.png)

(from [1, 2])

Recent research in open-source large language models (LLMs) has mostly focused upon two areas: [imitation learning](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) and [pre-training open-source base models](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)[1](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-1-131393593). Though both approaches are viable, the creation of high-quality, open-source base models is especially enticing, as these models can be further fine-tuned (at a lower cost) and used in a variety of different downstream applications. [Initial attempts](https://cameronrwolfe.substack.com/i/113386783/llama-vs-sota-llms) at creating these models failed. Although later models (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) and [MPT-7B](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact)) perform much better, these models have struggled to match the quality of their proprietary counterparts (e.g., GPT-3.5 or GPT-4) until recently.

With the release of the Falcon-7B and Falcon-40B LLMs [1], we see—_for the first time_—open-source base LLMs that begin to rival the quality of the most popular paid models. Trained over a massive textual corpus[2](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-2-131393593) obtained via a novel data pipeline, these models achieve (by a decent margin) new state-of-the-art performance among open-source LLMs and are free to use in commercial applications. To make things better, the Falcon models adopt several modifications to their underlying transformer architecture that significantly accelerate inference and can even improve the efficiency of pre-training.

**the big picture.** The process of creating an LLM is comprised of several steps; see below. The first step of this process (i.e., obtaining a pre-trained base model) is widely known to be the most expensive, both in terms of money and time.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F212105c5-8a85-4d1b-9cbe-039cb9f81ce3_2514x486.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F212105c5-8a85-4d1b-9cbe-039cb9f81ce3_2514x486.png)

Multi-step process for creating and refining an LLM (from [16, 17])

Such models were previously kept behind proprietary APIs, but advancements in open-source LLMs have made high-performing base LLMs more publicly available. Falcon is another model in this category, and it achieves unprecedented levels of performance in comparison to other open-source alternatives.

## Using Web Data for LLM Pre-Training

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab07a276-a12c-4901-afc2-86bc61b818dc_1118x410.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab07a276-a12c-4901-afc2-86bc61b818dc_1118x410.png)

(from [3])

When we look at the major differences between pre-training and fine-tuning (i.e., [SFT and RLHF](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)) a language model, we will notice that pre-training is much harder (and more expensive) compared to fine-tuning; see above. There are two fundamental properties of pre-training that make it so difficult:

1. The model is being trained from scratch, so it requires a greater number of training iterations.
    
2. The pre-training dataset must be large and diverse (i.e., provide as much “coverage” as possible) so the resulting LLM has a sizable knowledge base.
    

Put simply, pre-training datasets are massive (e.g., [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [6] was trained on 1.4 trillion textual tokens), which means that the extent of pre-training is unavoidably large. _We must run many training iterations to traverse all of this data!_

**creating a pre-training dataset.** However, it’s not just the size of the dataset that makes pre-training such a massive undertaking. Just curating the dataset is an intricate process that involves both retrieving the data and executing an entire pipeline of filtering (e.g., based on data quality, [contaminated data](https://arxiv.org/abs/2203.08242), [PII](https://www.ibm.com/topics/pii), and more) and de-duplication steps. A variety of different processing steps have been proposed and explored for curating LLM pre-training data; see below.

[Processing Data for LLMs](https://wandb.ai/wandb_gen/llm-data-processing/reports/Processing-Data-for-Large-Language-Models--VmlldzozMDg4MTM2)

Although one might initially think that such processing steps could be simplified or avoided, research on LLMs has showed us time and time again that the quality of data on which these models are trained is incredibly important. For evidence of this, we can see, for example, LIMA [7] or [Galactica](https://cameronrwolfe.substack.com/i/93578656/galactica-a-large-language-model-for-science) [8], both of which are trained over smaller (but higher-quality) text corpora and found to match or exceed the performance of identical models trained over noisier datasets at a larger scale.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb1c6ee2-aa1d-4ba9-aec7-0812771e165e_1346x280.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb1c6ee2-aa1d-4ba9-aec7-0812771e165e_1346x280.png)

(from [4, 5])

**current LLMs.** Due to the impact of data quality on model quality, pre-training data used for most LLMs are obtained from highly curated sources, such as filtered textual content, books, code, or technical reports; see above [4, 5]. In fact, numerous public sources of curated pre-training data are readily available online (e.g., [the Pile](https://huggingface.co/datasets/EleutherAI/pile) or [C4](https://huggingface.co/datasets/c4)) and have already been used extensively by existing models.

> _“Curation is believed to be necessary to produce performant models ... However, as models requiring pretraining on trillions of tokens are considered, it is unclear whether curation is scalable and if we will run out of unique high-quality data soon.”_ - from [2]

However, it is questionable whether the use of curated data sources is actually scalable—_fine-grained filtering and curation becomes more difficult with the scale of the pre-training dataset_. As such, extensive curation may need to become less necessary for larger models and datasets, especially given that locating sufficient amounts of data for LLM pre-training is becoming [increasingly difficult](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset).

#### RefinedWeb: Scalable Curation of Text from the Web

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ec6621-b166-46b9-b3cd-31c3c5b5099b_1644x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ec6621-b166-46b9-b3cd-31c3c5b5099b_1644x784.png)

(from [2])

Given these limitations, the authors of Falcon [1] explore scalable and efficient methods of data curation that generalize to massive amounts of data. The full data curation pipeline, which is used to create the [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) pre-training dataset over which Falcon-7B [10] and Falcon-40B [11] are pre-trained, is detailed in [2]. RefinedWeb is comprised of web-based data that undergoes a simplified filtering pipeline and can be used to train models that outperform similar models trained over curated sources of data; see above. Such a finding indicates that large-scale training corpora can be efficiently created from data obtained exclusively from the internet (as opposed to curated sources).

> _“Challenging existing beliefs on data quality and LLMs, models trained on adequately filtered and deduplicated web data alone can match the performance of models trained on curated data.”_ - from [2]

**simplified curation pipeline.** The pre-training dataset used for Falcon is based upon [Common Crawl](https://commoncrawl.org/). Compared to prior work, authors in [2] differentiate their data curation pipeline by placing an emphasis upon scale, _aiming to produce a pre-training corpus with 3-6 trillion tokens of data from the web_. This is much larger than datasets explored in prior work—even MassiveText (i.e., the corpus used to pre-train [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher) [9] and [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [6]) contains only 2.3 trillion tokens of text in total. Plus, existing models only use a subset of this data during pre-training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa077ad-078a-441a-a5ae-fe6599b240fe_1332x622.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa077ad-078a-441a-a5ae-fe6599b240fe_1332x622.png)

(from [2])

The corpus constructed in [2] contains roughly 5 trillion tokens of English-only data, obtained exclusively from the web; see above. Despite the scale of this data, authors adopt a stringent deduplication policy during the construction process, which removes both exact and [fuzzy duplicates](https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7) at a high rate. Minimal extra filtering is done on top of such deduplication. In fact, no machine learning-based filtering is performed aside from language identification with a [FastText classifier](https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02f82fdc-93a6-45c0-a8cd-0089ea2ef879_1278x526.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02f82fdc-93a6-45c0-a8cd-0089ea2ef879_1278x526.png)

(from [9])

In addition to filtering out non-English documents, authors in [2] use several simple heuristics to filter out unwanted content, such as:

- Filtering content from URLs associated with a blocklist
    
- Using [trafilatura](https://trafilatura.readthedocs.io/en/latest/) to extract content from web pages
    
- Defining simple rules to identify and filter PII
    

Additionally, several steps from the filtering pipeline of MassiveText [9] are adopted; see above. RefinedWeb’s full curation pipeline removes nearly 90% of the data that was originally obtained from CommonCrawl, but over 5 trillion tokens of textual data are still present in the fully-filtered corpus; see below. Plus, this dataset is “multimodal friendly”, as it contains links to images and alt text.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F648bf869-bce5-46c9-a0a4-78b794f6d0c8_1308x714.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F648bf869-bce5-46c9-a0a4-78b794f6d0c8_1308x714.png)

(from [2])

**open-source pre-training data.** The resulting RefinedWeb dataset is quite large, indicating that sufficient data is available on the internet to conduct LLM pre-training at an unprecedented scale. In other words, _we aren’t quite running out of data (at least yet)_. However, authors in [2] only open-source a small, 600B token subset of this corpus (i.e., 12% of the data). Despite its size, this public subset of RefinedWeb is a useful source of pre-training data for any practitioner working on LLMs. In fact, smaller variants of Falcon trained on this data in [2] are found to perform favorably compared to models trained on curated corpora; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96da3508-2949-441b-873f-81f1fbf88558_1310x378.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96da3508-2949-441b-873f-81f1fbf88558_1310x378.png)

(from [2])

## The Falcon Suite of LLMs

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52b8a290-0d40-4935-8aa5-38717e13c9cf_2126x1128.jpeg "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52b8a290-0d40-4935-8aa5-38717e13c9cf_2126x1128.jpeg)

(from the TII Falcon webpage [1])

The Falcon suite of LLMs [1], including Falcon-7B [10] and Falcon-40B [11], achieves state-of-the-art performance among open-source models. Furthermore, instruction tuned variants of these models, such as [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) (i.e., the top model on HuggingFace’s [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)), perform even better on public benchmarks. As we will see, these models have several key traits (e.g., data, performance, and inference speed) that make them unique and practically useful.

**commercial license.** Originally, the Falcon models were released under a [peculiar license](https://twitter.com/cwolferesearch/status/1662504834916929536?s=20) that required royalty payments when the models (or any derivatives of them) were used in commercial applications. Shortly after this initial release, however, the license was modified to a normal [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license, which means that the Falcon base models are now free to use in commercial applications! Compared to other open-source and commercially-usable LLMs (even [MPT-30B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-an-open-source-gpt-alternative)), Falcon-40B achieves uniquely impressive performance.

#### Falcon-7B and 40B Datasets

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b772bc6-d0da-477f-be72-ef7f962d6719_1206x454.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b772bc6-d0da-477f-be72-ef7f962d6719_1206x454.png)

(from [10, 11])

As mentioned previously, the dataset created in [2] is used to pre-train the open-source Falcon-7B/40B models. However, these models only pre-train over a subset of the full 5 trillion token corpus (i.e., 1.5 trillion tokens for Falcon-7B and 1 trillion tokens for Falcon-40B). However, this subset of the corpus is augmented with extra, curated data, such as books, code, and technical content[3](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-3-131393593); see above. Due to its larger size, Falcon-40B is trained over less data than Falcon-7B. Nonetheless, the larger model still performs much better and takes over two months to train, as opposed to only two weeks for Falcon-7B.

**multi-lingual LLMs.** The RefinedWeb corpus is English-only and Falcon-7B is trained using only English text. Falcon-40B, however, has the RefinedWeb-Europe corpus added to its pre-training set, which contains textual data from a variety of common European languages. Although this data only accounts for 7% of the pre-training corpus, it injects a small amount of multilingual data into the model’s knowledge base, enabling higher levels of performance on public benchmarks that require basic multilingual understanding.

#### Falcon Architecture

Both Falcon-7B and Falcon-40B models use a modified [decoder-only transformer architecture](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20). Modifications made to this architecture include:

- Flash Attention [[link](https://cameronrwolfe.substack.com/i/131642185/faster-inference)]
    
- RoPE embeddings
    
- Multi-Query Attention
    
- Parallel Attention and Feed-Forward Layers
    

These modifications (some of which are shared with the [MPT suite of LLMs](https://cameronrwolfe.substack.com/i/131642185/faster-inference)) drastically improve Falcon’s inference speed. In fact, Falcon-40B is `5X` faster than GPT-3 at inference time. Plus, pre-training Falcon-40B is less costly due to these modifications; e.g., Falcon-40B requires 75% of [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)’s [4] compute budget, 40% of [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)’s [6] compute budget, and 80% of [PaLM-62B](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)’s [5] compute budget. Both Falcon-7B and Falcon-40B are trained with a sequence length of 2K tokens, which is arguably small compared to recent LLMs (e.g., [StoryWriter](https://www.mosaicml.com/blog/mpt-7b) and [Claude](https://www.anthropic.com/index/100k-context-windows)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c039008-cdd5-4421-84ab-e3139462c94c_1288x688.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c039008-cdd5-4421-84ab-e3139462c94c_1288x688.png)

(from [10, 11])

Falcon-7B/40B share the same model architecture, but the 40B variant is slightly deeper (i.e., 60 layers vs. 32 layers) and has higher-dimensional hidden layers; see above. Using Falcon-40B requires ~90Gb of memory, which is less overhead than comparable models like LLaMA-65B. However, Falcon-40B still cannot be hosted on a single GPU like [MPT-30B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-an-open-source-gpt-alternative)[4](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-4-131393593). Given its smaller size, Falcon-7B only requires ~15Gb of memory, making it more accessible for both inference and fine-tuning.

**RoPE embeddings.** As we have seen in prior overviews, the [self-attention operation](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20), which is implemented in every layer of a language model’s [decoder-only transformer](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20) architecture, does not naturally consider the position of each token within a sequence. As such, we must inject position information (e.g., via an additive positional embedding) into this operation for each token; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00e3c612-5448-4b83-8490-4c45bb5de320_1468x1392.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00e3c612-5448-4b83-8490-4c45bb5de320_1468x1392.png)

Additive positional embeddings for a transformer

Several position embedding variants—which may or may not learn each embedding during training—have been proposed, including [absolute](https://cameronrwolfe.substack.com/i/76273144/berts-architecture) and [relative embeddings](https://jaketae.github.io/study/relative-positional-encoding/). Rotary positional embeddings (RoPE) [15], however, are an alternative that incorporates both the absolute (i.e., global position of a token in the sequence) and relative (i.e., defines position based on distances between tokens) position of each token into self-attention by:

1. Encoding absolute position with a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix)
    
2. Adding relative position information directly into the self-attention operation
    

Such an approach is found to yield a balance between absolute and relative position information, which benefits performance especially on tasks that require longer sequence lengths. As such, RoPE embeddings have recently gained in popularity, leading to their use in models like [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [5]. For a more detailed overview, check out the outline of RoPE embeddings below.

[More on RoPE](https://blog.eleuther.ai/rotary-embeddings/)

**multi-query attention.** Falcon models replace the typical, [multi-headed self-attention operation](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20) with an alternative structure called multi-query attention. Multi-query attention just shares key and value vectors (highlighted in red below) between each of a layer’s attention heads. Instead of performing a separate projection for each head, all heads share the same projection matrix for keys and the same projection layer for values. Although this change does not make training any faster, it significantly improves the inference speed of the resulting LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c6fdc1-8f5f-4ce2-89b4-3ee33123f207_2004x1162.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c6fdc1-8f5f-4ce2-89b4-3ee33123f207_2004x1162.png)

Multi-query attention in LLMs

**parallel attention layers.** Finally, the Falcon models make one fundamental change to the structure of each layer in their architecture. In contrast to the “serial” formulation of the decoder-only transformer layer, each layer of the Falcon models performs self-attention and a feed-forward transformation in parallel, then follows these operations with a single layer norm operation. The difference between this formulation and a standard transformer block is depicted below. Interestingly, this parallel formulation does not deteriorate the model’s performance. However, it can yield benefits in inference speed due to the fact that both major operations of a transformer layer happen in parallel.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc92bf943-91f4-4537-bf8d-c77365ddad95_2524x936.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc92bf943-91f4-4537-bf8d-c77365ddad95_2524x936.png)

Different variants of decoder-only transformer layers

#### A New Standard for Open-Source LLMs!

At the time of writing, no manuscript has yet been published about the Falcon models. However, Falcon-7B and Falcon-40B (along with their instruction tuned variants) have been evaluated via the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), which includes several benchmarks such as:

- ARC [[link](https://huggingface.co/datasets/ai2_arc)]
    
- HellaSwag [[link](https://huggingface.co/datasets/hellaswag)]
    
- MMLU [[link](https://huggingface.co/datasets/lukaemon/mmlu)]
    
- TruthfulQA [[link](https://huggingface.co/datasets/truthful_qa)]
    

Evaluations conducted via this leaderboard are incomplete and preliminary. However, these evaluations, which do capture model performance to a reasonable extent, clearly show that Falcon-40B is the current state-of-the-art for open-source language models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45f63026-86ee-4eb5-8a93-86985a4a670f_2680x686.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45f63026-86ee-4eb5-8a93-86985a4a670f_2680x686.png)

(from Open LLM Leaderboard)

The instruct variant of Falcon-40B (i.e., [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)), which has been [instruction tuned](https://cameronrwolfe.substack.com/i/130195416/instruction-tuning) on a mixture of data from [Baize](https://github.com/project-baize/baize-chatbot), far outperforms a variety of other open-sourced models. Additionally, the pre-trained Falcon-40B model performs quite well, even better than notable base models like LLaMA-65B and [MPT-30B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-an-open-source-gpt-alternative). Going further, Falcon-40B is also commercially-usable, whereas many comparable models on the leaderboard (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [13], [Guanaco](https://huggingface.co/timdettmers/guanaco-65b) [14], and [Lazarus](https://huggingface.co/CalderaAI/30B-Lazarus)) are only available for research purposes.

**practical usage of Falcon.** Given that they perform incredibly well, are lightweight to host compared to other LLMs (due to improved inference speed), and can be used freely in commercial applications, the Falcon LLMs are an impactful open-source tool for any practitioner working on LLMs. Luckily, several detailed overviews have been written that outline useful frameworks for both fine-tuning and hosting/deploying these models in practice.

- Deploy Falcon-40B on AWS Sagemaker [[link](https://aws.amazon.com/blogs/machine-learning/deploy-falcon-40b-with-large-model-inference-dlcs-on-amazon-sagemaker/)]
    
- Inference and Parameter-Efficient Fine-Tuning with Falcon [[link](https://huggingface.co/blog/falcon)]
    
- Fine-Tune Falcon-40B with PyTorch Lightning [[link](https://lightning.ai/blog/falcon-a-guide-to-finetune-and-inference/)]
    

Given that Falcon was trained using AWS, there are currently a decent number of explainer articles for deploying and training these models on similar hardware. These articles provide a good starting point for anyone looking to leverage Falcon in a use case of their own.

## Final Thoughts

The release of Falcon was a major breakthrough for the research and application of open-source LLMs. When we examine the contributions that are unique to these models, we immediately see a few key components that lead to success:

1. A unique mixture of large-scale pre-training data
    
2. An architecture that is optimized for efficiency
    

The RefinedWeb dataset shows us that textual corpora can be created at a much larger scale than was previously explored. To do this, we just download a large amount of data from the web and adopt strict deduplication rules along with simpler, efficient filtering heuristics. Then, by combining this massive source of data with a smaller amount of curated text, we can pre-train an incredibly performant open-source LLM. Finally, the modified architecture of the Falcon models makes both training and inference more efficient, resulting in a model that both performs incredibly well and can quickly generate text when deployed.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch)!

[Share Deep (Learning) Focus](https://cameronrwolfe.substack.com/?utm_source=substack&utm_medium=email&utm_content=share&action=share)

#### Bibliography

[1] “Introducing Falcon LLM”, _Technology Innovation Institute_, 7 June 2023, https://falconllm.tii.ae/.

[2] Penedo, Guilherme, et al. "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only." _arXiv preprint arXiv:2306.01116_ (2023).

[3] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _MosaicML_, 5 May 2023, www.mosaicml.com/blog/mpt-7b.

[4] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[5] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[6] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[7] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[8] Taylor, Ross, et al. "Galactica: A large language model for science." _arXiv preprint arXiv:2211.09085_ (2022).

[9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).

[10] “Falcon-7B”, _Technology Innovation Institute_, HuggingFace Page, https://huggingface.co/tiiuae/falcon-7b.

[11] “Falcon-40B”, _Technology Innovation Institute_, HuggingFace Page, [https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b).

[12] Gao, Leo, et al. "The pile: An 800gb dataset of diverse text for language modeling." _arXiv preprint arXiv:2101.00027_ (2020).

[13] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[14] Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." _arXiv preprint arXiv:2305.14314_ (2023).

[15] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[16] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[17] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-anchor-1-131393593)

Here, the term “base model” is used to refer to an LLM that has been extensively pre-trained but has not yet undergone any refinement or fine-tuning.

[2](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-anchor-2-131393593)

The corpus has 5 trillion tokens in total, but not all of this data is actually used during the pre-training process.

[3](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-anchor-3-131393593)

We might notice that many of these sources are similar to those used by the [Pile](https://huggingface.co/datasets/EleutherAI/pile) [12]. The creators of Falcon were inspired by this dataset and adopted similar sources of curated data.

[4](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source#footnote-anchor-4-131393593)

MPT-30B can be hosted on either an NVIDIA A100-80GB GPU in 16-bit precision or an NVIDIA A100-40GB GPU in 8-bit precision.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![James Le's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd92c7fe-0109-44bd-a10f-19c8bd4840c0_3596x2514.jpeg)



](https://substack.com/profile/6009523-james-le)

[

![Richard Hess's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0877cf45-879a-4110-90fa-b68b41ad3951_1024x1022.jpeg)



](https://substack.com/profile/132806202-richard-hess)

[

![A. P.'s avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fbb77b6-b665-4627-83a4-1f12cf0ec553_144x144.png)



](https://substack.com/profile/1499230-a-p)

[

![darlin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29e363b8-0457-4b09-b998-d7799cc67d0d_2265x2265.png)



](https://substack.com/profile/127381235-darlin)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

18 Likes∙

[1 Restack](https://substack.com/note/p-131393593/restacks?utm_source=substack&utm_content=facepile-restacks)

18

- 

[](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source/comments)

1

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Data is the Foundation of Language Models

### How high-quality data impacts every aspect of the LLM training pipeline...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 17, 2023

18

- 

[](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language/comments)

2

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b60ffa-37a0-449f-aa55-92527f4982fa_512x132.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07b60ffa-37a0-449f-aa55-92527f4982fa_512x132.png)

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58c75d94-605f-4560-a5b2-6901bba93791_2066x1408.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58c75d94-605f-4560-a5b2-6901bba93791_2066x1408.png)

(from [1])

Large Language Models (LLMs) have been around for [quite some time](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2), but only recently has their impressive performance warranted significant attention from the broader AI community. With this in mind, we might begin to question the origin of the current LLM movement. _What was it that actually made recent models so impressive compared to their predecessors?_ Although some may argue a variety of different factors, one especially impactful advancement was the ability to perform [alignment](https://cameronrwolfe.substack.com/i/93578656/where-do-generic-llms-fall-short). In other words, we figured out how to train LLMs to not just output the most likely next word, but to output text will satisfy the goals of a human, whether it be by following an instruction or retrieving important information.

> _“We hypothesize that alignment can be a simple process where the model learns the style or format for interacting with users, to expose the knowledge and capabilities that were already acquired during pretraining”_ - from [1]

This overview will study the role and impact of alignment, as well as the interplay between alignment and pre-training. Interestingly, these ideas were explored by the recent LIMA model [1], which performs alignment by simply fine-tuning a pre-trained LLM over a semi-manually curated corpus of only 1,000 high-quality response examples. We will learn that the alignment process, although critical, primarily teaches an LLM [steerability](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20) and correct behavior or style, while most knowledge is gained during pre-training. As such, alignment can be performed successfully even with minimal training data. However, we will see that the impact of data quality and diversity on both alignment and other avenues of LLM training (e.g., pre-training, fine-tuning, etc.) is absolutely massive.

## The LLM Training Pipeline

> _“LLMs are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences.”_ - from [1]

Although language models have been studied from a variety of different perspectives in recent months, the creation of these models tends to follow a standardized process with a few common components; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3f6474-3f89-4178-9583-ec53adf92d74_2738x522.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3f6474-3f89-4178-9583-ec53adf92d74_2738x522.png)

Multi-step process for creating and refining an LLM (from [11, 12])

The first step in this process—pre-training—trains the model over a large corpus of unlabeled text using a [language modeling objective](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20) and is typically the most expensive. After this, the model undergoes an [alignment](https://cameronrwolfe.substack.com/i/93578656/where-do-generic-llms-fall-short) process, comprised of supervised fine-tuning (SFT) and/or reinforcement learning from human feedback (RLHF). After the model has been trained, it can be deployed to a downstream application, where further fine-tuning or in-context learning can be leveraged to improve performance. Within this section, we will overview each of these components to better understand their impact on an LLM’s behavior.

#### Language Model Pre-Training

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F89687ad1-ab5d-4c72-840c-343d7fa26ab2_1854x1030.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F89687ad1-ab5d-4c72-840c-343d7fa26ab2_1854x1030.png)

The pre-training process—shown above—is the most computationally expensive step in the creation of an LLM. During this process, language models are exposed to a corpus of unlabeled textual data and trained using a standard [language modeling objective](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20). Put simply, this means that we train the model by _i)_ sampling some text from the dataset and _ii)_ training the model to predict the next word. This pre-training procedure is a form of [self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning), as the correct “next” word can be determined by simply looking at the next word in the dataset. The pre-training process is extensive given that the dataset is large (e.g., ~0.5-2 trillion tokens [13]) and the model must be trained from scratch.

#### The Alignment Process

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1367bb80-3904-4f0d-a779-275da06a6894_1176x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1367bb80-3904-4f0d-a779-275da06a6894_1176x638.png)

(from [11])

After pre-training is complete, we have a “base model”, or a generic LLM that does not yet possess any specialized abilities. To endow this model with the ability to conduct interesting conversations, follow instructions, and more, we must align this model, or train it to replicate behavior desired by a human user. In most cases, the alignment procedure is based upon two primary techniques:

- Supervised Fine-Tuning (SFT)
    
- Reinforcement Learning from Human Feedback (RLHF)
    

These techniques can either be used individually or combined together by performing one after the other, as was originally proposed by [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) [11] (i.e., the predecessor to ChatGPT).

**SFT** is a simple alignment approach—we just obtain examples of desired behavior and directly fine-tune the LLM (using a language modeling objective) on this data. For example, if we want to teach a base LLM to follow directions, we can obtain many examples of accurate responses to instruction-based prompts, and train the base model over these examples. This technique, which we will focus upon in this overview, is both simple and powerful. As we will see, however, _getting good results with SFT is dependent upon curating a high-quality dataset_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4ddb9d0-e7d7-4fe4-8a2e-3c99dc9f7770_620x1158.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4ddb9d0-e7d7-4fe4-8a2e-3c99dc9f7770_620x1158.png)

(from [11])

**RLHF** provides us with the ability to optimize an LLM’s parameters based on feedback provided by humans. Starting with a set of prompts, we first use the LLM to generate several potential outputs for each prompt. Given these outputs, when can ask human annotators to rank the quality of these responses (i.e., which output is the “best”), then use these rankings to train a reward model—this is just a smaller LLM that predicts human preference given a model’s response[1](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-1-134561977); see above. We can use the reward model’s output as a scalar reward and optimize the LLM to maximize this reward via the PPO algorithm; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb49427e7-5b35-457d-959a-4af687f2b9e2_674x1206.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb49427e7-5b35-457d-959a-4af687f2b9e2_674x1206.png)

(from [11])

The beauty of the RLHF process described above is that we are directly optimizing the model based on human preference, but “preferences” can capture a variety of different properties! For example, maybe we want the model to follow instructions better, output more interesting content, or even stop hallucinating (i.e., making up false information). All of these behaviors can be optimized with the use of RLHF, making it a very robust alignment tool; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5e5e7f0d-36c8-4a0d-b5b7-af37f3719673_1600x747.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5e5e7f0d-36c8-4a0d-b5b7-af37f3719673_1600x747.png)

(from [11])

For those not familiar with reinforcement learning (RL), RLHF may be a difficult framework to understand without some additional background reading. To gain a better background in RL, I’d recommend checking out the resources below.

- Basics of Reinforcement Learning [[link](https://www.synopsys.com/ai/what-is-reinforcement-learning.html)]
    
- Proximal Policy Optimization [[link](https://openai.com/research/openai-baselines-ppo)]
    
- Overview of RLHF [[link](https://huggingface.co/blog/rlhf)]
    

> _“The model’s capabilities seem to come primarily from the pre-training process—RLHF does not improve exam performance (without active effort, it actually degrades it). But steering of the model comes from the post-training process—the base model requires prompt engineering to even know that it should answer the questions.”_ - from GPT-4 blog

**What is the purpose of alignment?** Alignment is an incredibly active area of research. Currently, there is a discussion within the research community around better understanding the role/purposes of alignment. In the analysis of GPT-4, we see that the role of alignment techniques like RLHF is to make the LLM more [steerable](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20) and interesting, rather than to teach the model new information. In fact, most knowledge possessed by the model seems to come from pre-training. A similar argument is made in [1], where we see that high-quality alignment can be achieved via a small, curated dataset for SFT. Such a result is especially interesting given the [massive amount](https://openai.com/research/gpt-4) of human and computational resources that have been invested into aligning popular proprietary models like GPT-4.

#### Applying the LLM

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56876166-d543-43c9-a5db-d519679ad4fa_1614x906.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56876166-d543-43c9-a5db-d519679ad4fa_1614x906.png)

Different types of learning with an LLM

Once an LLM has underwent pre-training and alignment, it’s (more or less) ready to be used in downstream applications. However, we must take some measures to ensure that the LLM solves a particular task accurately. Typically, this is done by either _i)_ further fine-tuning the model or _ii)_ using in-context learning; see above.

**Domain specific fine-tuning.** If we are deploying an LLM into a specialized domain (e.g., medical, legal, software, etc.), then it might make sense to further fine-tune the model over the types of data that it will see in this domain. This process is quite simple. We just continue to train the model using a language modeling objective, but we use a domain-specific corpus (i.e., data that is similar to what will be seen in the desired application) instead of the pre-training dataset.

**In-context learning.** Once we are ready to deploy the model (with or without domain specific fine-tuning), we should leverage [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning), which uses textual prompts that instruct/guide the model towards desired behavior, to more accurately solve downstream tasks. These prompts may include examples of correct solutions (i.e., [few-shot exemplars](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)), but this data is only used by the model as context when generating output (i.e., we do not use it for training). We have explored prompting approaches extensively within prior overviews.

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    

## LIMA: Less is More for Alignment

In [1], authors study the relative importance of pre-training versus alignment by training LIMA, a derivative of LLaMA-65B [2] that undergoes SFT (without RLHF) over a curated alignment dataset. In particular, the fine-tuning set used by LIMA is a small set of 1,000 carefully curated prompt-and-response examples with a similar output style and diverse inputs; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cfe7b4d-b3d1-403c-b97c-7ae18c59c68e_964x1542.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cfe7b4d-b3d1-403c-b97c-7ae18c59c68e_964x1542.png)

(from [1])

When trained on these examples, we see that LIMA performs quite well and even approaches the performance of state-of-the-art proprietary models like GPT-4 and [Claude](https://www.anthropic.com/index/introducing-claude)[2](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-2-134561977). Such a result reveals that language models can be effectively aligned via a small number of carefully chosen examples, _thus emphasizing the role of data quality and diversity in training and aligning powerful language models_.

> _“A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.”_ - from [1]

**The Superficial Alignment Hypothesis.** Along these lines, authors in [1] propose the Superficial Alignment Hypothesis (SAH), which is summarized in the quote above. Most of an LLM’s core knowledge is [learned during pre-training](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20), while alignment searches for the proper format or style for surfacing this knowledge. The SAH simply states that alignment can be learned in a data efficient manner given a set of examples with sufficient quality and diversity.

#### Curating Data for Alignment

The dataset used for alignment in [1] is constructed from a combination of community QA forums (e.g., StackExchange, wikiHow, and Reddit) and manually authored examples. Unlike recent work that attempts to automate the curation of data for SFT (e.g., [Self-Instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework)), we see in [1] that data from both of these sources is carefully (and oftentimes manually) filtered for both quality and diversity. Although manual curation takes time, it boosts the quality of the resulting dataset, which is found to be highly beneficial.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c840c3f-b940-4e02-a4a8-4d8e5a7f9ac1_1600x714.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c840c3f-b940-4e02-a4a8-4d8e5a7f9ac1_1600x714.png)

(from [1])

**Sourcing the data.** The breakdown of LIMA’s training data is shown in the table above. In the training set, 750 examples are sourced from community QA forums. To ensure data quality, these examples are either filtered manually or via “up vote” metrics. The remaining 250 examples are manually written by the authors—200 from scratch and 50 from [SuperNaturalInstructions](https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m). When manually creating data, the authors maximize diversity and uniformity by making sure sure that:

- Responses are stylistically aligned to the behavior of a helpful AI agent
    
- Prompts are as diverse as possible
    

Put simply, we want to minimize the amount of noise in the alignment dataset (i.e., ensure a uniform style, tone, format, etc.), while making sure that the data observed by the LLM has as much diversity and coverage as possible. Notably, authors in [1] even include a few malevolent prompts in the alignment data to demonstrate how potentially harmful commands can be avoided.

**Can we automate this?** In recent work on [imitation learning](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) with open-source LLMs, we usually see that data used for fine-tuning is automatically curated. For example, data for SFT can be downloaded from online sources (e.g., [ShareGPT](https://sharegpt.com/)) or obtained directly from LLM APIs for ChatGPT or GPT-4. Such an approach is incredibly efficient compared to manual curation, and (in some cases) it even works well; e.g., [Orca](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) [3] is trained over a large number of dialogues obtained from ChatGPT/GPT-4 and performs quite well (even compared to top models). However, we see in [4] that models trained in this manner [typically have limitations](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source) and perform poorly when subjected to extensive analysis.

> _“Manually creating diverse prompts and authoring rich responses in a uniform style is laborious. While some recent works avoid manual labor via distillation and other automatic means, optimizing for quantity over quality, this work explores the effects of investing in diversity and quality instead.”_ - from [1]

In [1], we study an alternative approach to alignment that invests into the curation of high-quality data. Instead of automatically obtaining a large amount of data, we manually filter and select fewer examples. This smaller-scale (but labor-intensive) selection process allows the diversity and quality of data to be controlled, _which illustrates the impact of data scale and quality on LLM alignment_.

#### Is alignment actually superficial?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)

(from [1])

LIMA’s performance is compared to that of a variety of different language models. In particular, we see LIMA’s performance compared to [Alpaca-65B](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) [6], DaVinci003 (i.e., an RLHF-tuned version of [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)), Bard (i.e., based on [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [7]), Claude (i.e., 52B parameter LLM trained via AI feedback [8]), and [GPT-4](https://openai.com/research/gpt-4). Evaluation is performed using both crowd workers and automated feedback from GPT-4, as shown in the Figure above. Interestingly, LIMA consistently outperforms Alpaca (despite being fine-tuned on `52X` less data!) and even matches or exceeds the quality of Claude and GPT-4 in a non-trivial number of cases. LIMA’s competitive performance is impressive given that these other models are trained over millions of user prompts with feedback.

**Absolute performance.** Beyond the model comparison trials conducted above, authors in [1] manually evaluated the quality of 50 random responses generated by LIMA. Interestingly, we see that LIMA fails to answer only six of the 50 prompts and produces an excellent response in 50% of the trials; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1622d5b9-abc2-4635-a521-e2eeac2f442b_424x780.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1622d5b9-abc2-4635-a521-e2eeac2f442b_424x780.png)

(from [1])

When this manual evaluation is repeated on exclusively out-of-distribution prompts (i.e., those that are much different from examples included in the fine-tuning set), the results are not much different—20% of responses fail, 35% pass, and 45% are excellent. Such a result indicates that LIMA actually generalizes well and is not just memorizing or overfitting to the curated fine-tuning dataset[3](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-3-134561977).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98e42b77-51ce-4630-bd11-302f2a22e963_282x506.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98e42b77-51ce-4630-bd11-302f2a22e963_282x506.png)

(from [1])

For example, LIMA can perform multi-turn dialogue relatively well (but not great) despite having no such examples in its fine-tuning dataset. When just 30 multi-turn dialogue examples are exposed to the model, we see that LIMA quickly learns how to maintain a dialogue from these examples; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56f39948-b454-46eb-af39-f102405868dc_1618x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56f39948-b454-46eb-af39-f102405868dc_1618x840.png)

(from [1])

**Useful properties of data.** Beyond the main results outlined above, we see in the ablation experiments of [1] that the diversity and quality of examples used for alignment is incredibly important; see above. Notably, _just increasing the size of the fine-tuning dataset does not always improve the LLM’s performance_. As such, we learn from [1] that careful curation of high-quality data for alignment is valuable.

#### The Bigger Picture

In recent work on open-source language models, we have seen a variety of different LLMs (e.g., [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) [6], [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality) [9], [Koala](https://cameronrwolfe.substack.com/i/114077195/koala-a-dialogue-model-for-academic-research) [10] and more) that have adopted an automatic approach for curating data for SFT. In particular, these model use an imitation approach that _i)_ collects a large amount of dialogues from other LLMs and _ii)_ performs supervised fine-tuning over this data. Although these models initially seemed to perform quite well, we see in more targeted evaluations that the quality of their alignment is poor . With this in mind, we might reasonably ask: _What makes LIMA’s approach more effective?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31ad7cd5-7d0a-4758-a226-12736a503825_1888x1352.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31ad7cd5-7d0a-4758-a226-12736a503825_1888x1352.png)

(from [4])

**Quality > Quantity.** Even in studies on imitation models, we see that just increasing the amount of data in the fine-tuning set yields minimal impact on the underlying model’s performance; see above. We see in [1] that similar results are observed for LIMA. Given that increasing the amount of data alone yields no benefit, we a have a few different options for improving an LLM’s performance:

1. Create a more powerful base model
    
2. Improve the alignment dataset
    

While several works (e.g., [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact) and [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source)) have explored the creation of better base models, LIMA studies how better alignment datasets can be created[4](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-4-134561977). Put simply, we see in [1] that creating an alignment dataset that is both diverse and high-quality (even if it is small!) is extremely effective. LLMs can accurately learn to emulate certain behaviors based on minimal data, which supports the SAH.

> _“For the purpose of alignment, scaling up input diversity and output quality have measurable positive effects, while scaling up quantity alone might not.”_ - from [1]

**But, these results aren’t perfect!** Prior imitation models were initially thought to perform incredibly well, even comparably to top proprietary models like ChatGPT. However, we later discovered that such conclusions were a product of [human error](https://cameronrwolfe.substack.com/i/127874443/are-we-missing-something). These models mimicked the style of proprietary LLMS, but lacked their factuality and tended to generalize poorly beyond their training sets, which is more difficult for humans to deduce when evaluating these models. Given that LIMA is also primarily evaluated with crowd workers, the results in [1] are subject to similar limitations. However, we see that LIMA tends to generalize well and oftentimes outperforms imitation models like Alpaca, which indicates that high-quality alignment data is still incredibly beneficial to LLM performance.

## The Impact of Data Beyond Alignment

Within [1], we see that the quality of data is incredibly important for effectively aligning a language model. However, the importance of data quality and diversity goes beyond alignment alone—_the type and quality of data being used impacts every aspect of the LLM training pipeline_. Let’s look at a few examples for reference.

**Pre-training.** Across a variety of different models, we have seen that the quality of data used for pre-training is incredibly important. For example, within [Galactica](https://cameronrwolfe.substack.com/i/93578656/galactica-a-large-language-model-for-science) [14], authors find that training on a smaller, heavily-curated dataset of high-quality scientific information yields the best possible performance. Similarly, the [BioMedLM model](https://www.mosaicml.com/blog/introducing-pubmed-gpt) is pre-trained over a smaller, curated corpus of technical content. Finally, [Falcon-40B](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source)—_the current state-of-the-art for open-source language models_—places a notable emphasis on the quality of pre-training data, where we see that authors have invested significant effort into developing a novel and efficient pipeline for extracting high-quality pre-training data from the web.

**Alignment.** Beyond the approach explored in [1], the recently-proposed [Orca model](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) [3] heavily studies the role of data quality in solving the alignment problem. However, a slightly different approach is adopted. Namely, the authors train a model using an imitation approach but augment the data used for SFT (i.e., dialogue examples with other LLMs) with detailed information from the model about how each problem is solved. Including these extra details in the alignment dataset is found to produce imitation models that are much more robust compared to models like [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) [6] or [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality) [9].

**In-context learning.** Beyond training and fine-tuning the LLM, the data used for in-context/few-shot learning can massively impact performance. In particular, recent research on few-shot learning shows us that factors such as the ordering, distribution, or format of exemplars can impact a model’s performance. Going further, we see that the diversity of data is incredibly important, where models that are prompted with a diverse group few-shot exemplars tend to perform better. Check out the link below for a more in-depth discussion.

[Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)

## Closing Remarks

> _“These results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.”_ - from [1]

The major conclusions from work covered within this overview are twofold:

1. _The Superficial Alignment Hypothesis_: LLMs learn their knowledge during pre-training and alignment teaches them how to properly interact with users.
    
2. The quality and diversity of data is incredibly important to the alignment process (much more so that data scale).
    

Within [1], we have observed these major conclusions in the creation of LIMA, where high-quality alignment can be performed using a smaller, curated corpus. Not much data is needed for alignment (with SFT) if the data is of sufficient quality, meaning that input prompts are diverse and responses have a standardized structure and tone. However, the positive impact of high-quality data spans far beyond alignment—_all aspects of LLM training are positively benefitted by the use of higher-quality data_. Whether it be during pre-training or in-context learning, language models are still fundamentally subject to the same basic rule as all other machine learning models: “garbage in, garbage out”.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

[Share Deep (Learning) Focus](https://cameronrwolfe.substack.com/?utm_source=substack&utm_medium=email&utm_content=share&action=share)

#### Bibliography

[1] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[2] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023). 

[3] Mukherjee, Subhabrata, et al. "Orca: Progressive Learning from Complex Explanation Traces of GPT-4." _arXiv preprint arXiv:2306.02707_ (2023).

[4] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[5] Wang, Yizhong, et al. "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks." _arXiv preprint arXiv:2204.07705_ (2022).

[6]  Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[7] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[8] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[9] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[10] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[11] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[12] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[13] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[14] Taylor, Ross, et al. "Galactica: A large language model for science." _arXiv preprint arXiv:2211.09085_ (2022).

[1](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-anchor-1-134561977)

This reward model is trained over pairs of model responses, where one pair is “better” than the other. Using these pairs, we can derive a loss function that _(i)_ maximizes the reward of the preferred response and _(ii)_ minimizes the reward of the worse response.

[2](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-anchor-2-134561977)

P.S. [Claude 2](https://www.anthropic.com/index/claude-2) was released last week for anyone that is interested!

[3](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-anchor-3-134561977)

Notably, this results goes against most recent results on obtained from models that are fine-tuned on imitation data on proprietary LLMs. These models are found to [struggle with generalization](https://cameronrwolfe.substack.com/i/127874443/are-imitation-models-actually-useful) beyond their fine-tuning dataset. As such, LIMA’s ability to generalize is truly a testament to the impact of high-quality alignment data.

[4](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language#footnote-anchor-4-134561977)

Notably, the recently-proposed [Orca](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) [3] LLM also studies how to improve alignment datasets. Their solution, which also works well, is based upon augmenting imitation data used for fine-tuning with detailed explanation traces from ChatGPT and GPT-4.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Shyam Peri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa52e6507-3b1d-4353-be93-f409111bc4e2_96x96.jpeg)



](https://substack.com/profile/4926723-shyam-peri)

[

![Fabbri Paolo's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54d12824-9b91-4536-8696-59f5c1628689_766x1434.jpeg)



](https://substack.com/profile/1483954-fabbri-paolo)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Steve Bang's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F958c7444-e06c-4ff9-b3bc-840e317d3db7_407x407.png)



](https://substack.com/profile/248118-steve-bang)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

18 Likes∙

[2 Restacks](https://substack.com/note/p-134561977/restacks?utm_source=substack&utm_content=facepile-restacks)

18

- 

[](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# The History of Open-Source LLMs: Early Days (Part One)

### Understanding GPT-Neo, GPT-J, GLM, OPT, BLOOM, and more...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 24, 2023

27

- 

[

5

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comments)

2

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F643e9bc0-8c11-491e-bf1f-d9a4061a3f50_2526x1420.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F643e9bc0-8c11-491e-bf1f-d9a4061a3f50_2526x1420.png)

(from [12, 20])

Research on language modeling has a long history that dates back to models like [GTP and GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) or even RNN-based techniques (e.g., [ULMFit](https://arxiv.org/abs/1801.06146)) that predate modern, transformer-based language models. Despite this long history, however, language models have only become popular relatively recently. The first rise in popularity came with the proposal of [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [1], which showed that impressive few-shot learning performance could be achieved across many tasks via a combination of self-supervised pre-training and in-context learning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e797415-7032-4ab3-b2d3-7ad2f8385a27_2154x1368.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e797415-7032-4ab3-b2d3-7ad2f8385a27_2154x1368.png)

(from [1])

After this, the recognition garnered by GPT-3 led to the proposal of a [swath of large language models](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher) (LLMs). Shortly after, research on language model alignment led to the creation of even more impressive models like InstructGPT [19] and, most notably, its sister model [ChatGPT](https://openai.com/blog/chatgpt). The impressive performance of these models led to a flood of interest in language modeling and generative AI.

Despite being incredibly powerful, many early developments in LLM research have one common property—_they are closed source_. When language models first began to gain widespread recognition, many of the most powerful LLMs were only accessible via paid APIs (e.g., the [OpenAI API](https://openai.com/blog/openai-api)) and the ability to research and develop such models was restricted to select individuals or labs. Such an approach is markedly different from typical AI research practices, where openness and idea sharing is usually encouraged to promote forward progress.

> _“This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues such as bias and toxicity.”_ - from [4]

**This overview.** Despite the initial emphasis upon proprietary technology, the LLM research community slowly began to create open-source variants of popular language models like GPT-3. Although the first open-source language models lagged behind the best proprietary models, they laid the foundation for improved transparency within LLM research and catalyzed the development of many subsequent models that were more powerful (e.g., Falcon [10] and [LLaMA-2](https://ai.meta.com/llama/)[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-1-135273362)).

This overview is part of a three part series exploring the history of open-source language models. Here, we will learn about the beginning of this history, including several initial attempts at creating open-source language models. Although these models left something to be desired in terms of performance, they are incredibly important to understand, as the revolution of open-source LLMs that ensued was entirely based upon these models. In the following two parts of the series, we will learn more about recent open-source LLMs, as well as how imitation and alignment techniques have been used to improve their performance.

## The Mechanics of a Language Model

Open-source LLM research catalyzed transparency and idea sharing, creating an environment in which researchers could collaborate and innovate more quickly. Put simply, _the beauty of open-source LLM research is that it gives us the potential to study these incredible models and develop a deeper understanding of how they work_. There are no unknown tricks hidden behind a paid API or black box. Open-source LLMs allow us to look at the code, run experiments, and even try out our own ideas and modifications—we have full access to the underlying model!

> _“A much broader segment of the AI community needs access to these models in order to conduct reproducible research and collectively drive the field forward.”_ - from [4]

But, to build a deep understanding of such models, we first need to understand the basics behind how they work. Within this section, we will overview these ideas, attempting to provide a (relatively) comprehensive understanding of LLMs.

#### The Language Modeling Objective

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F89687ad1-ab5d-4c72-840c-343d7fa26ab2_1854x1030.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F89687ad1-ab5d-4c72-840c-343d7fa26ab2_1854x1030.png)

Pre-training with a language modeling objective

At the core of language modeling is next token prediction (also called the standard language modeling objective), which is used to train nearly all language models. To train a language model using next token prediction, we need a large corpus of raw text. Using this corpus, we train the model by _i)_ sampling some text from the dataset and _ii)_ training the model to predict the next word; see above. Because the ground truth next token can always be deduced from the raw text, next token prediction is a form of [self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning).

**What is a token?** One can roughly consider next token prediction to be predicting the next word in a sequence, given a few preceding words as context. However, this analogy is not perfect, as tokens and words are not exactly equal. When a language model receives text as input, the raw text is first tokenized[2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-2-135273362) (i.e., converted into a sequence of discrete words or sub-words); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)

Converting raw text into a sequence of tokens

The tokenizer associated with a language model typically has a fixed-size vocabulary, or set of viable tokens that can be created from a textual sequence.

**Predicting next tokens.** Once a sequence of tokens has been created, the language model has an embedding layer that stores a unique and learnable vector embedding for every token within the tokenizer’s vocabulary. Using this embedding layer, we can convert each token within the input sequence into a corresponding vector embedding, forming a sequence of token vectors; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)

Tokenizing and embedding raw text data

After adding [positional embeddings](https://cameronrwolfe.substack.com/i/131642185/alibi-enables-context-length-extrapolation) to each token, we can pass this sequence of token vectors into a decoder-only transformer (more explanation will follow), which transforms (no pun intended) each of these token vectors and produces a corresponding output vector for each token. Notably, the number of output vectors is the same as the number of input vectors; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7721d1fa-c9ef-47e0-96cf-483bbde4967f_1008x792.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7721d1fa-c9ef-47e0-96cf-483bbde4967f_1008x792.png)

Processing tokens with a decoder-only transformer

Now that we have an output representation for each token, we are ready to perform next-token prediction! For each token in the sequence, we simply take its output token vector and use this to predict the token that comes next in the sequence! An illustration of this process is shown below. In practice, this next token prediction objective is simultaneously computed over all tokens in the sequence (and over all sequences in a mini-batch!) to maximize efficiency.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)

Computing the next token prediction training objective

Due to the use of causal (or masked) self-attention, each output token vector only considers the current token and those that is come before it in the sequence when computing its representation. If we were to use bidirectional self-attention, each output token vector would be computed by looking at the entire sequence of vectors, which would allow the model to cheat and solve next token prediction by just copying the token that comes next in the sequence. As such, _masked self-attention is needed for next-token prediction_. But, what is self-attention and—more fundamentally—what is a transformer? Let’s dive into this next.

**A quick note.** The phrase “language model” may sometimes be used to refer to models beyond those that specialize in performing next token prediction. For example, [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [18] is considered by some to be a “language model”, but it is trained using a [Cloze](https://en.wikipedia.org/wiki/Cloze_test)-style objective and is not a generative model. As such, language models that specialize in next token prediction are oftentimes distinguished as “causal” language models. Here, we will use both of these terms interchangeably to refer to models that specialize in next token prediction.

#### The Transformer Architecture and its Variants

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82a9585b-9f04-429c-a59b-8b1afc8e9a0f_1600x1462.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82a9585b-9f04-429c-a59b-8b1afc8e9a0f_1600x1462.png)

(from [17])

All language models use some variant of the [transformer architecture](https://cameronrwolfe.substack.com/i/74325854/the-transformer-architecture). This architecture (shown above) was originally proposed in [17] for solving sequence-to-sequence tasks[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-3-135273362). However, it was subsequently extended to solve a variety of different problems, from [assessing the semantic similarity of text](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) to [classifying images](https://cameronrwolfe.substack.com/p/vision-transformers-from-idea-to). In its original form, the transformer architecture has two components:

- _Encoder_: each block performs [bidirectional self-attention](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20) and a pointwise feed-forward transformation[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-4-135273362), which are separated with a residual connection[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-5-135273362) and LayerNorm.
    
- _Decoder_: each block performs [causal self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20), [cross attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention) (i.e., self-attention across encoder and decoder tokens), and a pointwise feed-forward transformation, each separated by a residual connection and LayerNorm.
    

When both components of the architecture are present, the encoder processes the input sequence and produces an output sequence. Then, the decoder generates its own output sequence, given the encoder’s output sequence as input. In other words, the encoder processes the entire input sequence to form a representation that the decoder uses as context when generating output. As a whole, the transformer takes a sequence as input and produces a new sequence as output.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1664ad-3866-4438-91cb-06fec1d77174_864x1336.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf1664ad-3866-4438-91cb-06fec1d77174_864x1336.png)

(from [17])

**Decoder-only and encoder-only transformers.** Nearly all causal language models use a decoder-only transformer as their underlying architecture, which is just a normal transformer with the encoder-portion of the architecture removed; see above. Additionally, the cross attention portion of each decoder block is removed due to the lack of an encoder (i.e., we can’t attend to an encoder that doesn’t exist)! Alternatively, one could form an encoder-only architecture by just using the encoder portion of the architecture. Encoder-only architectures (e.g., [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [18]) excel at solving a variety of discriminative natural language tasks, but they are not used for generating text. To learn more, check out the link below.

[BERT Overview](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)

**Why the decoder?** The choice of using the decoder-only architecture (as opposed to encoder-only or the full encoder-decoder transformer) for LLMs is not arbitrary. Rather, this choice is driven by the use of next-token prediction for training language models. The use of masked self-attention within the decoder ensures that the model cannot look forward in the sequence when predicting the next token. Otherwise, next-token prediction would be trivial, as the model could simply copy the next token; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcff8e70c-5a14-4abb-a53a-14393b23bee4_1500x1006.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcff8e70c-5a14-4abb-a53a-14393b23bee4_1500x1006.png)

Causal self-attention is used for next-token prediction

To perform next token prediction without cheating, both encoder-only and encoder-decoder transformers would have to avoid including any ground truth next token in their input sequence. To do this, we could _i)_ ingest a prefix and _ii)_ predict the token that follows this prefix. However, this approach is a bit inefficient because we can only predict a single next token at a time. In contrast, decoder-only models, due to their use of masked self-attention, can ingest an entire sequence of tokens and apply a language modeling objective to every token within the sequence. Plus, several papers [12] have shown practically that decoder-only architectures yield the best performance for next token prediction.

**How do we generate text?** Given the decoder-only architecture outlined above, generating text follows a simple autoregressive[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-6-135273362) process. We just continually predict the next token, add this token to our input, and repeat; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)

Generating text with a language model

#### Training and Using Language Models

To complete our understanding of language models, we need to quickly explore how these models are typically trained and used in practice. Although a lot of research has been done in this area, most language models are trained according to a few standard techniques, as proposed in [19]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4edd5ee-f6d0-4f02-ad24-5b0499b16609_2028x738.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4edd5ee-f6d0-4f02-ad24-5b0499b16609_2028x738.png)

LLM training components (from [19])

Language models can learn in a [variety of different ways](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning). Here, we will focus on pre-training, alignment, and in-context learning, which collectively encompass most of what’s required to train an LLM and use it in a practical application.

**Pre-training.** The pre-training process is the initial and most computationally expensive step of creating an LLM. Beginning with a randomly-initialized LLM, we must train this model—using a language modeling objective—over a massive corpus of raw text that is curated from a [variety of different sources](https://cameronrwolfe.substack.com/i/131393593/using-web-data-for-llm-pre-training). Prior research [1] has shown us that by pre-training a [very large model](https://cameronrwolfe.substack.com/i/88082618/scaling-laws-for-neural-language-models) (i.e., lots of parameters) over [a large dataset](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms), we can obtain a [foundation model](https://crfm.stanford.edu/) that can accurately solve a variety of different tasks by performing next token prediction. To get the best results, we need scale in terms of both [data and model size](https://twitter.com/cwolferesearch/status/1603837192346165248?s=20).

**What else do we need?** Language models that solely undergo pre-training can be powerful. Look at [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [1] and [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [15] for a few examples. However, there is a reason that LLMs did not explode in popularity until the proposal of models like [ChatGPT](https://openai.com/blog/chatgpt)—_just performing next token prediction is not very interesting_. Oftentimes, predicting the statistically-correct next token, although it leads to reasonable text being generated, produces output that is repetitive, simple, and generally not helpful. We needed some way to make LLMs craft outputs that are more helpful and interesting to us as humans!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5e5e7f0d-36c8-4a0d-b5b7-af37f3719673_1600x747.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5e5e7f0d-36c8-4a0d-b5b7-af37f3719673_1600x747.png)

(from [19])

[Alignment](https://cameronrwolfe.substack.com/i/134561977/the-alignment-process) refers to the process of fine-tuning an LLM to better align with the desires of human users. This is accomplished primarily via two techniques: supervised fine-tuning (SFT) and/or reinforcement learning from human feedback (RLHF). The desired behavior of an LLM depends a lot on the context or application in which it is deployed. However, alignment is a generic tool that can be used to arbitrarily fine-tune an LLM to behave in a certain way; see above. Recent research indicates that models do not learn new information during alignment. Rather, this process simply teaches the model how to properly format or present the knowledge that it has already gained from the pre-training process.

**Using LLMs in practice.** After we have pre-trained and fine-tuned (or aligned) our language model, the final step is to specialize the model to our desired application. This process may require extra fine-tuning over domain-specific data. More training is not always necessary, however, as we can accomplish a lot by just using in-context learning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38366b6b-9816-408d-9d67-5ca9066df293_1912x1108.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38366b6b-9816-408d-9d67-5ca9066df293_1912x1108.png)

(from [1])

Put simply, in-context learning refers to the idea of solving a variety of different problems using a single, general-purpose foundation model (e.g., a pre-trained LLM). Given the generic text-to-text structure of a language model, this can actually be done quite easily. We just need to construct a textual problem-solving prompt that can be provided as input to the LLM; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F037dd16c-bb64-4cac-9af3-f7264499bd68_2032x432.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F037dd16c-bb64-4cac-9af3-f7264499bd68_2032x432.png)

Different prompt variants for solving an arithmetic problem

Then, the LLM should generate the answer to our problem as output. As such, we can solve many different problems by just modifying the input prompt! The process of constructing good prompts for solving problems is referred to as prompt engineering, and we have explored this idea extensively in previous posts:

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    

## Initial Attempts at Open-Source LLMs

Given the [expense of pre-training](https://www.mosaicml.com/blog/gpt-3-quality-for-500k), it took some time for the research community to pursue the creation of an open-source LLM, causing proprietary models like GPT-3 to become the standard. However, once the first few models were proposed, the floodgates opened and research on open-source LLM progressed rapidly (almost _[too](https://cameronrwolfe.substack.com/i/127874443/the-wake-of-llama)_ [rapidly](https://cameronrwolfe.substack.com/i/127874443/the-wake-of-llama)). We will learn about a few of the early models here, while more recent open-source LLMs will be covered in future parts of the series.

#### [GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) [6]

One of the first open-source LLMs—a 20 billion parameter model called GPT-NeoX-20B [6]—was created by [EleutherAI](https://www.eleuther.ai/). GPT-NeoX-20B was created after the initial GPT-Neo model (2.7 billion parameters) [22], was pre-trained over [the Pile](https://huggingface.co/datasets/EleutherAI/pile), and achieves impressive [few-show learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) performance (comparable to GPT-3) on a variety of natural language benchmarks. Although this model is somewhat small compared to GPT-3 (i.e., 20 billion parameters vs. 175 billion parameters), it was the largest open-source language model to be released at the time. Plus, all of code for training and evaluating the model was released alongside its weights under an [Apache 2.0 license](https://www.planetcrust.com/what-does-apache-2-0-license-mean), which permits commercial use.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7577fb59-58ab-4f1d-a4c2-1c6f49f18d1f_1574x990.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7577fb59-58ab-4f1d-a4c2-1c6f49f18d1f_1574x990.png)

(from [8])

**The model.** GPT-NeoX-20B [6] uses a standard decoder-only transformer architecture, but makes the following two changes:

- RoPE Embeddings
    
- Parallel Attention and Feed Forward Layers
    

Improving upon standard position embeddings, [RoPE embeddings](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications) (shown above) provide a new methodology for injecting positional information into the self-attention operation. This approach finds a better balance between absolute and relative position information and is used in a variety of other models (e.g., [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [9] and [Falcon-40B](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [10]) due to its ability to improve performance on tasks with long sequence lengths. Additionally, the use of parallel attention and feed forward layers (see below) leads to a 15% improvement in training throughput with minimal performance degradation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc92bf943-91f4-4537-bf8d-c77365ddad95_2524x936.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc92bf943-91f4-4537-bf8d-c77365ddad95_2524x936.png)

Performing attention and feed forward layers in parallel

Interestingly, a custom tokenizer is created for GPT-NeoX-20B. This tokenizer is comparable to that of [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) [11], but it is trained from scratch on the Pile—a large and diverse corpus of text—and is modified to more consistently tokenize whitespace characters. As such, the resulting tokenizer, in addition to being trained on a high-quality corpus, is especially effective at tokenizing code (i.e., there are a lot of whitespace characters in code!). As a result, several open-source models (e.g., [MPT-7B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-a-commercially-usable-llama-b) [5]) adopt this tokenizer even today.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2ecbd39-14fb-4678-877f-cb097f58ffc2_1540x1402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2ecbd39-14fb-4678-877f-cb097f58ffc2_1540x1402.png)

(from [6])

**The performance.** GPT-NeoX-20B was compared to both GPT-3 and other open-source models, such as [GPT-J](https://www.eleuther.ai/artifacts/gpt-j). In these evaluations, we see that GPT-NeoX-20B performs quite well (even when compared to proprietary models) on common language modeling tasks; see above. Notably, GPT-3 tends to achieve the best performance. However, GPT-NeoX-20B performs quite well relative to its size and even outperforms proprietary models with a similar number of parameters.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa8d375-1dcc-4419-a69c-9cc242734d87_2478x1156.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa8d375-1dcc-4419-a69c-9cc242734d87_2478x1156.png)

(from [6[)

The performance of GPT-NeoX-20B is not quite state-of-the-art, but the model performs surprisingly well for its size, even when compared to recent models!

#### [Open Pre-Trained Transformers (OPT) Language Models](https://arxiv.org/abs/2205.01068) [4]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7c2f92f2-fdd7-4e1e-804d-b9a3aea18aa6_800x365.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7c2f92f2-fdd7-4e1e-804d-b9a3aea18aa6_800x365.png)

Components of the OPT release

In a previous overview, we have discussed the details of the Open Pre-trained Transformers (OPT) library in depth. See below for a link.

[OPT Overview](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)

OPT, which was proposed by [Meta AI](https://ai.meta.com/)[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-7-135273362), was created as an initiative to democratize access of powerful LLMs to the public and is comprised of several different LLMs with sizes ranging from 125 million to 175 billion parameters. These models are pre-trained over a curated dataset compiled from sources like [Reddit](https://arxiv.org/abs/2001.08435), [the Pile](https://arxiv.org/abs/2101.00027), and [BooksCorpus](https://yknzhu.wixsite.com/mbweb), and the largest model in this suite—OPT-175B—was one of the first truly _large_ language models to be open-sourced. Going further, the models are accompanied by a [code repository](https://github.com/facebookresearch/metaseq) and even a logbook that details the pre-training process of all models. Although OPT models are [not commercially-usable](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md), they are an incredibly resource that heavily influenced the open availability of LLMs for research.

**The impact.** The OPT language models were the first large-scale effort to make massive language models accessible to the research community—_LLMs were now fully-available to anyone_, rather than being hidden behind an API. Additionally, OPT’s open-source training code makes a highly efficient training framework, using common techniques like [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) and [tensor parallelism](https://github.com/NVIDIA/Megatron-LM?fbclid=IwAR3SvXpTaLseZacJv_Bntwg0czNNYj8hEhcho3R_mo8ABDS8zmszw4mdZ3E), readily available. This code achieves resource utilization that is 17% better than research published directly by NVIDIA [3], making it a great resource for training LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f6766a-7780-4747-bf51-3ebb940741e1_1978x1298.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f6766a-7780-4747-bf51-3ebb940741e1_1978x1298.png)

(from [5])

The training [notes](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles?fbclid=IwAR3qONxU4mENL_HAVcf9LJCwwqijGCVMk87C8Sm9_q3y6TZS3kZiY6Fd5dY) and [logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf?fbclid=IwAR1gSseT67AGnNprJRdiW91Pf7eW1b82Z3pYshE4CYGT_-AKVnCUdaIdmm8) associated with OPT provide a massive amount of (previously unknown) insight into the LLM training process. From these resources, we can better understand the full cost of training an LLM and the many struggles that may occur in this process (e.g., loss spikes, hardware failures, and other “mid flight” training adjustments that are required). Such difficulties with training LLMs became a topic of conversation and have since been (mostly) resolved by subsequent work on open-source LLMs; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b2815ae-3504-4978-bd4a-c4f8b54f5b14_1658x762.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b2815ae-3504-4978-bd4a-c4f8b54f5b14_1658x762.png)

(from [4])

**Does it perform well?** OPT-175B was extensively compared to popular models at the time of its proposal and found to achieve comparable performance to GPT-3 in zero and few-shot learning settings; see above. Overall, OPT’s performance is not notable—_the model is widely considered to lag behind proprietary models in terms of quality_. Despite its lackluster performance, however, OPT was a massive step forward for AI research and significantly boosted the level of interest in open-source LLMs. This impact should not be understated, as it came at a time when the dominance of proprietary models had been accepted as a new standard.

#### [BLOOM: An Open, Multilingual Language Model](https://bigscience.huggingface.co/blog/bloom) [12]

> _“Academia, nonprofits and smaller companies' research labs find it difficult to create, study, or even use LLMs as only a few industrial labs with the necessary resources and exclusive rights can fully access them.”_ - from [12]

Proposed in [12], BLOOM is an 176 billion parameter LLM that was trained as part of a massive, open collaboration of AI researchers (i.e., over 1000 researchers participated!), called the [Big Science Research Workshop](https://bigscience.huggingface.co/). Running over the timespan of one year (May 2021 to May 2022), the goal of this workshop was to create _i)_ a massive multilingual text dataset and _ii)_ a large multilingual language model that is trained on this dataset. The resulting model, which is slightly larger than GPT-3 and is open-sourced under the [Responsible AI License](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) (RAIL), can generate text in 46 different languages[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-8-135273362) and 13 programming languages.

**The dataset** developed for training BLOOM, called the [ROOTS corpus](https://arxiv.org/abs/2303.03915), is comprised of 498 HuggingFace datasets and contains over 1.6 terabytes of text that spans 46 natural languages and 13 programming languages. The distribution of this dataset across the different languages is shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F001280d5-6e00-40e8-bcef-80c450309a5f_1754x1044.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F001280d5-6e00-40e8-bcef-80c450309a5f_1754x1044.png)

(from [12])

After obtaining the raw data, the authors apply a pipeline of different quality filters to remove text that is not natural language. The exact filtering components that are used, which are further outlined in Section 3.1.3 of [12], change depending on the source of the data. However, the overall pipeline shares a common goal of filtering out as much low-quality text as possible.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F937fbfdd-03ec-4a30-8b6b-eeb26e5bc9b5_1292x684.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F937fbfdd-03ec-4a30-8b6b-eeb26e5bc9b5_1292x684.png)

(from [12])

**The architecture** used by BLOOM is a standard decoder-only transformer. As shown above, however, a few modifications are made to this architecture, such as:

- _ALiBi_ [13]: This aids the model in generalizing to longer context lengths than those seen during training. [[link](https://cameronrwolfe.substack.com/i/131642185/alibi-enables-context-length-extrapolation)]
    
- _Embedding Layer Norm_: An extra [layer norm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) is placed after the model’s embedding layer, which is empirically found to improve training stability.
    

Overall, this model is not much different than most LLMs. Interestingly, authors in [12] perform an extensive analysis between [different types of transformer architectures](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures) (e.g., encoder-only models, encoder-decoder models, and decoder-only models), finding that the decoder-only model (used by nearly all causal language models) achieves the best performance after pre-training.

> _“Our results show that immediately after pre-training, causal decoder-only models performed best – validating the choice of state-of-the-art LLMs.”_ - from [12]

**Does it perform well?** Compared to other open-source LLMs, BLOOM performs relatively well. It achieves comparable, or improved, results relative to OPT in natural language benchmarks and tends to excel at machine translation tasks given that it was trained on a multilingual corpus; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6c6115c-2f79-48ab-b246-ae3caceffde5_2362x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6c6115c-2f79-48ab-b246-ae3caceffde5_2362x804.png)

(from [12])

However, BLOOM’s performance falls below that of top proprietary models. For example, we see in results on the HumanEval benchmark (shown below) that the model’s coding abilities fall far short of alternatives like [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code) [14]. Additionally, when we compare the performance of BLOOM to models like [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [15] and [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [9], we quickly see that the performance of open-source models falls short of their proprietary counterparts. In other words, _research in open-source LLMs was still lagging at the time when BLOOM was proposed_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f716b40-d8c9-4c2d-8319-ca43c56d7981_1292x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f716b40-d8c9-4c2d-8319-ca43c56d7981_1292x1192.png)

(from [12])

#### Other Notable Models

We tried to cover several notable models that were proposed during the early days of open-source LLM research. But, there are still a few models not covered in this overview that are worth mentioning. Let’s take a quick look at a few of them.

**GPT-J [21]** is a 6 billion parameter, English-only causal language model that was proposed prior to GPT-NeoX-20B [6]. Similar to GPT-NeoX-20B, this model was pre-trained on the Pile. At the time of its release, GPT-J-6B was the largest publicly-available GPT-3-style language model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd717af76-0028-46b3-9cb5-2527ea8966b2_1062x618.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd717af76-0028-46b3-9cb5-2527ea8966b2_1062x618.png)

(from [20])

**GLM [20]** is more of a pre-training objective rather than a language model. This work explores the idea of unifying different pre-training techniques (e.g., from [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert), [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part), and [GPT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)) by proposing a autoregressive blank infilling objective. In other words, we predict masked words in a sentence in an autoregressive manner, similar to a language model; see above. The resulting model, which is quite small (<1 billion parameters), is found to outperform BERT, T5 and GPT on several popular natural language processing benchmarks.

## Where do we go from here?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c71b781-5d41-4848-a113-557fcc11efdc_2014x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c71b781-5d41-4848-a113-557fcc11efdc_2014x1008.png)

The evolution of open-source LLM research

Given that initial attempts at open-source LLMs yielded models that did not perform nearly as well as proprietary counterparts, we might reasonably wonder: _What should we do to make these models better?_ As this research area has evolved, we have seen effort invested into two primary areas:

1. Creating better base LLMs[9](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-9-135273362)
    
2. Fine-tuning open-source LLMs (i.e., alignment and imitation)
    

Given that open-source LLMs are accessible to everyone, research in these areas progressed at a shocking pace—_we went from OPT to near state-of-the-art models (e.g., LLaMA-2 or [Falcon-40B](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [10]) in less than a year_!

> _“We argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs”_ - from [16]

Both of the research directions outlined above were explored in parallel during this time, and each resulted in the development of useful techniques for AI practitioners. Within the next two parts of this survey, we will overview each of these areas and the key contributions of each, exploring how initial attempts at open-source LLMs evolved into incredibly-capable models such as LLaMA-2.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

[Share](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEzNTI3MzM2MiwiaWF0IjoxNzQ1NzQ1Njg2LCJleHAiOjE3NDgzMzc2ODYsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.N1HFwxMy8cjSN2PqlrZIWCcI6-D8WMmvM1gKGdyJ3v0)

#### Bibliography

[1] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[2] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." _arXiv preprint arXiv:2112.11446_ (2021).

[3] Smith, Shaden, et al. "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model." _arXiv preprint arXiv:2201.11990_ (2022).

[4] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.” _arXiv preprint arXiv:2205.01068_ (2022).

[5] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _MosaicML_, 5 May 2023, www.mosaicml.com/blog/mpt-7b.

[6] Black, Sid, et al. "Gpt-neox-20b: An open-source autoregressive language model." _arXiv preprint arXiv:2204.06745_ (2022).

[7] Gao, Leo, et al. "The pile: An 800gb dataset of diverse text for language modeling." _arXiv preprint arXiv:2101.00027_ (2020).

[8] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[9] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[10] “Introducing Falcon LLM”, _Technology Innovation Institute_, 7 June 2023, https://falconllm.tii.ae/.

[11] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[12] Scao, Teven Le, et al. "Bloom: A 176b-parameter open-access multilingual language model." _arXiv preprint arXiv:2211.05100_ (2022).

[13] Press, Ofir, Noah A. Smith, and Mike Lewis. "Train short, test long: Attention with linear biases enables input length extrapolation." _arXiv preprint arXiv:2108.12409_ (2021).

[14] Chen, Mark, et al. "Evaluating large language models trained on code." _arXiv preprint arXiv:2107.03374_ (2021).

[15] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[16] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[17] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[18] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[19] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[20] Du, Zhengxiao, et al. "Glm: General language model pretraining with autoregressive blank infilling." _arXiv preprint arXiv:2103.10360_ (2021).

[21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.

[22] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large scale autoregressive language modeling with MeshTensorflow.

[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-1-135273362)

[LLaMA-2](http://llama-2%20was%20proposed%20just%20last%20week,%20and%20it%20has%20officially%20dethroned%20falcon-40b%20as%20the%20state-of-the-art%20for%20open-source%20llms.%20more%20to%20come%20in%20part%20two!/) was proposed just last week, and it has officially dethroned Falcon-40B as the state-of-the-art for open-source LLMs. More to come in part two of this series!

[2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-2-135273362)

The most common tokenization technique used for LLMs currently is Byte-Pair Encoding tokenization. Read more about how it works [here](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).

[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-3-135273362)

These are tasks that take a sequence as input and produce a sequence as output, such as language translation or text summarization.

[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-4-135273362)

This just means that the same [feed-forward transformation](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks) is separately applied to the embedding of every token vector within the input sequence.

[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-5-135273362)

A residual connection just means that we add a module’s input value to its output. In other words, if a module performs an operation given by the function f(x), this same operation with a residual connection would have the form g(x) = f(x) + x.

[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-6-135273362)

This words just means that, given a starting input sequence, we sequentially _i)_ generate an output, _ii)_ add this output to our input sequence, and _iii)_ repeat.

[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-7-135273362)

Following the proposal of OPT, Meta AI has continued to be a major contributor to open-source LLM research. Their research has led to a variety of models like [OPT-IML](https://arxiv.org/abs/2212.12017), [LLaMa](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone), [LIMA](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language), [LLaMA-2](https://ai.meta.com/llama/), and more.

[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-8-135273362)

For nearly all of these languages (e.g., Spanish, French and Arabic), BLOOM is the first language model with >100B parameters to be trained on the language.

[9](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early#footnote-anchor-9-135273362)

Even work on fine-tuning open-source LLMs heavily emphasizes [the value of creating better base LLMs](https://cameronrwolfe.substack.com/i/127874443/are-imitation-models-actually-useful). Improvements to the base LLM yield benefits after fine-tuning too!

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Mortal's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F085f02a0-d7cc-47b0-9735-99e5615ed95d_144x144.png)



](https://substack.com/profile/117183132-mortal)

[

![KB Lee's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feec3b34a-8e51-4f62-b62f-afe962f57003_900x960.jpeg)



](https://substack.com/profile/101678808-kb-lee)

[

![Padma's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcafc71f9-3713-4592-86b1-1198a9438fd9_144x144.png)



](https://substack.com/profile/24073467-padma)

[

![Andreas's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7c8fcc1-b64e-430f-b9e8-03a0a01b7275_1776x1184.jpeg)



](https://substack.com/profile/17174275-andreas)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

27 Likes∙

[2 Restacks](https://substack.com/note/p-135273362/restacks?utm_source=substack&utm_content=facepile-restacks)

27

- 

[

5

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sam's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d2c2a42-ebd9-437b-a596-36f04e639689_144x144.png)



](https://substack.com/profile/12042942-sam?utm_source=comment)

[Sam](https://substack.com/profile/12042942-sam?utm_source=substack-feed-item)

[2024年2月14日](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comment/49499820 "2024年2月14日 08:55")

Liked by Cameron R. Wolfe, Ph.D.

I've been absolutely devouring your articles for the last week -- you're doing a great job! Thank you very much for writing, please continue 💪

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comment/49499820)

[

![Lei's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Forange.png)



](https://substack.com/profile/161583467-lei?utm_source=comment)

[Lei](https://substack.com/profile/161583467-lei?utm_source=substack-feed-item)

[2023年8月8日](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comment/22021546 "2023年8月8日 06:04")

Liked by Cameron R. Wolfe, Ph.D.

Galactica?

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comment/22021546)

[3 more comments...](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# The History of Open-Source LLMs: Better Base Models (Part Two)

### How LLaMA, MPT, Falcon, and LLaMA-2 put open-source LLMs on the map...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 31, 2023

32

- 

[

3

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comments)

4

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a1db71-1ff9-40fd-8ee4-b4ca2113eeca_2512x1412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a1db71-1ff9-40fd-8ee4-b4ca2113eeca_2512x1412.png)

(from [10, 12, 14, 15]

Open-source research on large language models (LLMs) is incredibly valuable, as it aims to democratize a powerful and influential technology. Although open-source LLMs are now commonly used and widely studied, this area of research saw some initial struggles that were difficult to overcome. Namely, open-source LLMs performed poorly at first and were heavily criticized. Within this overview, we will study a line of research that changed this narrative by making high-performing pre-trained LLMs available to everyone. Given that pre-training a language model is so expensive, the models we will study here are especially impactful. After these high-performing base models were created and released, many people could conduct research using these models at marginal added cost.

> _“The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology.”_ - from [14]

**The current series.** This overview is part two of a three part series on the history of open-source LLMs. The [first part](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early) in the series overviewed initial attempts at creating open-source LLMs. Here, we will study the most popular open-source base models (i.e., language models that have been pre-trained but not fine-tuned or aligned) that are currently available. Next time, we will go over how these models can be fine-tuned or aligned to create a variety of useful applications.

## Early Days of Open-Source LLMs

In [part one](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early) of this series, we saw that the early days of research on open-source LLMs resulted in the proposal of several important base models, such as [OPT](https://cameronrwolfe.substack.com/i/135273362/open-pre-trained-transformers-opt-language-models) and [BLOOM](https://cameronrwolfe.substack.com/i/135273362/bloom-an-open-multilingual-language-model). However, these models were widely considered to perform quite poorly compared to closed-source pre-trained models (e.g., [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)). _How do we solve this?_ First, we need to take a deeper look at the LLM training process.

**Training pipeline.** LLMs are trained in several steps, as shown in the figure below. First, we [pre-train](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective) the model over a lot of raw text. Then, we perform [alignment](https://cameronrwolfe.substack.com/i/135273362/training-and-using-language-models) with techniques like [SFT and RLHF](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior). Finally, we can perform further [fine-tuning or in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning) to specialize the LLM to a particular task.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc9d43f-8fce-4db1-acf9-e2252491f706_2356x632.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc9d43f-8fce-4db1-acf9-e2252491f706_2356x632.png)

Recently, we have seen strong empirical evidence that most of a language model’s knowledge is gained during pre-training[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-1-135439692). The alignment process simply teaches the model to properly format or surface this knowledge gained during pre-training. As coined by [LIMA](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) [3], this idea is known as the “Superficial Alignment Hypothesis”. Although this hypothesis might not seem entirely relevant to the topic of this overview, we learn from it something important—_a model that undergoes insufficient pre-training is unlikely to be “fixed” by fine-tuning or alignment_.

> _“A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.”_ - from [3]

**What’s the solution?** Given the poor performance of initial open-source LLMs, it quickly became clear that the community needed to re-create higher-quality base models from scratch if any forward progress was to be made. Additionally, these models needed to be pre-trained over much more data so that their performance could be improved. Given that pre-training is incredibly expensive (especially when executed over a lot of data), such an effort is not trivial. The creation of better open-source base models had to be an undertaking of organizations with sufficient funding (e.g., [Meta](https://ai.meta.com/) or [MosaicML](https://www.databricks.com/company/newsroom/press-releases/databricks-completes-acquisition-mosaicml)) that could pay the cost of training these models and make them freely available to others in the community.

## Towards Better Base Models

The performance of open-source LLMs was initially too poor to warrant significant usage and exploration, but this problem was quickly solved. Here, we will review several models that changed this narrative by making powerful pre-trained LLMs available to all.

#### [LLaMA: A Leap in Open-Source Quality](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)

LLaMA [1] was one of the first pre-trained LLMs to be released that was both high-performing and open-source. However, LLaMA is not just a single model, but rather a suite of different LLMs with sizes ranging from 7 billion to 65 billion parameters. These models each achieve a different tradeoff between performance and inference efficiency. Although LLaMA cannot be used commercially (i.e., only for research), it is nonetheless an impactful proposal that served to catalyze several directions of open-source research with LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c9131-98ec-40cb-9bde-51b3772e832c_610x534.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c9131-98ec-40cb-9bde-51b3772e832c_610x534.png)

(from [1])

**The data.** Inspired by lessons from [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [2][2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-2-135439692), LLaMA models are pre-trained over a corpus that contains over 1.4 trillion tokens of text. This pre-training dataset was significantly larger than that of any prior open-source LLM. The sources and distribution of data are depicted above. Interestingly, LLaMA is pre-trained solely using publicly-available data sources, meaning that the entire pre-training process can be replicated by anyone with sufficient compute.

> _“GPT-4 has learned from a variety of licensed, created, and publicly available data sources, which may include publicly available personal information.”_ - from GPT-4 blog

Such a property is especially desirable given that many proprietary LLMs are trained using internal data that is not openly available. Put simply, LLaMA was a step towards improved transparency and openness in more ways than one.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5926683-07ef-4569-9025-1dce298a60b5_1164x760.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5926683-07ef-4569-9025-1dce298a60b5_1164x760.png)

(from [1])

**Improved performance.** Compared to its predecessors, _LLaMA is a huge leap forward in the performance of open-source LLMs_. Still, the quality lagged behind that of top proprietary LLMs (e.g., [ChatGPT](https://openai.com/blog/chatgpt) or [GPT-4](https://openai.com/research/gpt-4)), but we should recall that LLaMA models have not undergone [alignment](https://cameronrwolfe.substack.com/i/135273362/training-and-using-language-models). Notably, LLaMA-13B performs comparably to [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [3], while LLaMA-65B outperforms [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [4] in several cases, indicating that the LLaMA suite performs comparably to other widely-used base models. Detailed metrics are provided in the tables above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)

(from [5, 6, 7, 8])

**The open-source explosion.** One of the most interesting aspects of LLaMA’s proposal was the [wake of open-source LLM research](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) that followed it; see above. After the weights of LLaMA models were made publicly available, the open-source research community quickly began to release a variety of different model variants and software packages. These developments included anything from [fine-tuned versions of LLaMA](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) to a [C++ library](https://cameronrwolfe.substack.com/i/114077195/going-further) for efficiently running inference with any of the LLaMA models from a laptop. Such developments truly demonstrate the beauty of openness in research. _We went from interacting with these powerful models solely via an API to running them on our laptop in only a few weeks!_

#### [MPT: LLMs that are High-Quality, Commercial, and Open-Source](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c6d97e0-f9f6-43db-b809-150c874bb58a_1094x728.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c6d97e0-f9f6-43db-b809-150c874bb58a_1094x728.png)

(from [10])

Although LLaMA was impressive, none of the models within this suite could be used in commercial applications—_they were valuable solely from a research perspective_. Luckily, the proposal of LLaMA was quickly followed by the development and release of the commercially-usable (i.e., released under an [Apache 2.0 license](https://www.planetcrust.com/what-does-apache-2-0-license-mean)) MPT suite by MosaicML. MPT-7B [9] was released first, which garnered a lot of interest (i.e., it was basically a commercially-usable alternative for LLaMA-7B!). In fact, MPT-7B was downloaded over 3M times on [HuggingFace](https://huggingface.co/mosaicml/mpt-7b) before the larger MPT-30B [10] model was made available!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd56ab4ab-e918-46cb-9f65-69b54f93e528_1270x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd56ab4ab-e918-46cb-9f65-69b54f93e528_1270x746.png)

(from [9, 10])

The main differences between these two models are:

1. They are pre-trained using slightly different mixes of data; see above.
    
2. MPT-30B is trained using a longer context length of 8K tokens[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-3-135439692).
    

However, these models both perform well and can be used in commercial applications, which led them to become popular in the AI community.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae37f2dd-c587-476a-a849-bd758cca5785_1338x624.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae37f2dd-c587-476a-a849-bd758cca5785_1338x624.png)

(from [9])

**Does MPT live up to the hype?** Although LLaMA drastically improved state-of-the-art performance for open-source LLMs, the MPT suite rivaled this performance. In particular, MPT-7B matches the performance of LLaMA-7B across a variety of standard benchmarks; see above. Going further, MPT-30B tends to match the performance of GPT-3. Compared to similarly-sized open-source models (e.g., LLaMA-30B and Falcon-40B), MPT-30B tends to perform slightly worse; see below. However, it is better than these models on coding-related tasks and can be hosted on a single GPU (with quantization).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51942512-2bc4-4a95-957a-85402ccf0069_1478x432.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51942512-2bc4-4a95-957a-85402ccf0069_1478x432.png)

(from [10])

**MPT variants.** In addition to the pre-trained MPT-7B and MPT-30B models, a variety of fine-tuned MPT models were released, such as [instruct](https://huggingface.co/mosaicml/mpt-30b-instruct) and [chat](https://huggingface.co/mosaicml/mpt-30b-chat)[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-4-135439692) versions of both MPT models. Additionally, a “StoryWriter” version of MPT-7B was created by fine-tuning on data with a 64K token context length. Given that pre-training an LLM is significantly more expensive than fine-tuning, a variety of different fine-tuned MPT variants could be created at marginal cost; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07820e2f-a0cb-4e4c-bb26-616ac8d71913_1566x674.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07820e2f-a0cb-4e4c-bb26-616ac8d71913_1566x674.png)

**But wait… there’s more!** MPT models are useful (especially for those working on commercial applications), but the models are also accompanied by an entire suite of software (i.e., the [LLM foundry](https://github.com/mosaicml/llm-foundry)) released by MosaicML. This open-source code can be used to pre-train and fine-tune MPT models, making the MPT suite an incredibly valuable tool for exploring specialized use cases with LLMs.

#### [Falcon: Reaching New Heights in Open-Source Performance](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source)

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52b8a290-0d40-4935-8aa5-38717e13c9cf_2126x1128.jpeg "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52b8a290-0d40-4935-8aa5-38717e13c9cf_2126x1128.jpeg)

(from [1])

Although many advances had been made in the space of open-source LLMs, available models still lagged behind proprietary LLMs in terms of performance for quite some time. The proposal of the Falcon suite of LLMs [11] was the first time that the quality of proprietary LLMs was truly rivaled by an open-source alternative. Two variants of Falcon are available—Falcon-7B and Falcon-40B. In addition to being commercially licensed, these Falcon models perform incredibly well due to being pre-trained on a massive, custom-curated corpus. Notably, the instruct variant of Falcon-40B was the top-performing model on the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) (by a significant margin) for several months[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-5-135439692).

> _“Challenging existing beliefs on data quality and LLMs, models trained on adequately filtered and deduplicated web data alone can match the performance of models trained on curated data.”_ - from [12]

**Curating data from the web.** The Falcon models are trained over a massive textual corpus called RefinedWeb [12] that contains over 5 trillion tokens of text. Only 1.5 trillion tokens and 1 trillion tokens of RefinedWeb are actually used for pre-training Falcon-7B and Falcon-40B, respectively. Although a majority of LLMs are pre-trained over public sources of curated data, the authors of Falcon choose instead to construct their own pre-training dataset exclusively using data from the web (i.e., [CommonCrawl](https://commoncrawl.org/)). To filter this data, a novel pipeline is created that emphasizes simple, but effective, components; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fea4b2-cc9b-4694-aade-c3063d9dc006_698x666.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52fea4b2-cc9b-4694-aade-c3063d9dc006_698x666.png)

(from [12, 13])

The RefinedWeb corpus shows that a massive amount of high-quality text data—_beyond the scale of datasets explored previously_—can be efficiently curated from the web. After filtering is applied, models trained on this data can even outperform comparable models trained over curated sources of data.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b772bc6-d0da-477f-be72-ef7f962d6719_1206x454.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b772bc6-d0da-477f-be72-ef7f962d6719_1206x454.png)

(from [12])

The exact datasets used to train Falcon-7B and Falcon-40B are shown above. Notably, Falcon-7B is trained over English-only data, while Falcon-40B has data from a variety of European languages inserted into its pre-training set.

**A new SOTA.** Currently, no publication for the Falcon LLMs has been released. As such, the only formal evaluation of these models was performed via the OpenLLM leaderboard, where the Falcon-40B model fared quite well. In particular, [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) was the state-of-the-art model for some time, outperforming other models by a significant margin; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45f63026-86ee-4eb5-8a93-86985a4a670f_2680x686.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45f63026-86ee-4eb5-8a93-86985a4a670f_2680x686.png)

(from the Open LLM Leaderboard)

Qualitatively, some practitioners have [claimed](https://twitter.com/cwolferesearch/status/1678451643405279262?s=20) that Falcon-40B seems to underperform LLaMA-based models. Although an awareness of these remarks is useful, such evidence is anecdotal and subjective. In standardized natural language benchmarks, Falcon LLMs perform incredibly well, leading them to retain state-of-the-art performance among open-source models for a long time.

#### [LLaMA-2: Current State-of-the-Art](https://ai.meta.com/llama/)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F083b01d7-6fe7-4c90-b22e-6b008e9f15be_1262x760.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F083b01d7-6fe7-4c90-b22e-6b008e9f15be_1262x760.png)

(from [14])

Although Falcon-40B was the state-of-the-art open-source LLM for some time, the recent release of the LLaMA-2 model suite dethroned this model. Similarly to LLAMA-1, LLaMA-2 [14] is comprised of several different LLMs with sizes ranging from 7 billion to 70 billion parameters and uses only publicly available data for pre-training. Both pre-trained and fine-tuned[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-6-135439692) versions of LLAMA-2 models are released, though we will only cover the pre-trained models within this overview due to our focus upon open-source base models.

> _“There have been public releases of pre-trained LLMs (such as BLOOM that match the performance of closed pre-trained competitors like GPT-3 and Chinchilla, but none of these models are suitable substitutes for closed product LLMs, such as ChatGPT, BARD, and Claude.”_ - from [14]

LLaMA-2 continues to narrow the gap in performance between open and closed-source language models by releasing a suite of higher-performing base models that are pre-trained over a massive dataset. As we will see, these models still fall short of matching the quality of proprietary models, but they come much closer than any open-source model before them.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba0fd31-9d39-4594-92c1-054d4c9e1541_872x356.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba0fd31-9d39-4594-92c1-054d4c9e1541_872x356.png)

(from [14])

**How is it different?** LLaMA-2 adopts an approach that is quite similar to its predecessor, aside from a few minor (but impactful) differences. First, LLaMA-2 models are pre-trained over 40% more data—2 trillion tokens in total, compared to 1.4 trillion tokens for LLaMA-1. Additionally, LLaMA-2 models are trained with a slightly longer context length, and the larger models use grouped query attention (GQA) within their underlying architecture. Interestingly, authors in [14] note that LLaMA-2’s pre-training set up-samples sources of data that are known to be more knowledgeable. Such a change is made in an attempt to emphasize factual sources, increase knowledge, and reduce hallucinations.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)

(from [15])

**What is GQA?** As proposed in [15], GQA is a modification to [multi-headed self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20) that can improve inference efficiency in LLMs. A typical multi-headed self-attention mechanism has `N` total query, key, and value heads, creating `N` self-attention heads in total. In GQA, we divide these `N` total heads into groups, where key and value heads are shared within each group; see above. Such an approach is an interpolation between vanilla multi-headed self-attention and multi-query attention, which uses a shared key and value projection across all `N` heads[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-7-135439692). GQA is found in [15] to improve inference speed comparably to multi-query attention, while maintaining the performance of vanilla multi-headed attention.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a42b8e-34ba-48f4-b9c5-1f36ba909f64_1260x490.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a42b8e-34ba-48f4-b9c5-1f36ba909f64_1260x490.png)

(from [14])

**LLaMA-2 is really good.** Compared to popular open-source models (e.g., MPT, Falcon, and LLaMA-1), the LLaMA-2 base LLMs perform quite well. In fact, LLaMA-2-70B sets a new state-of-the-art among open-source LLMs on all tasks considered; see above. Notably, however, LLaMA-2 was somewhat [criticized](https://twitter.com/amasad/status/1681383736032493580?s=20) for its (relatively) poor performance on coding-based tasks (e.g., [HumanEval](https://arxiv.org/abs/2107.03374)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7984d418-3031-480a-8438-a7e992c82023_1262x356.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7984d418-3031-480a-8438-a7e992c82023_1262x356.png)

When compared to proprietary models, LLaMA-2 base models perform worse; see above. However, we should keep in mind that this comparison is made between a base LLM and aligned models like GPT-3.5 and GPT-4. When compared to other popular base LLMs (e.g., [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [4]), LLaMA-2 performs favorably!

**Commercial license.** While LLaMA-1 could only be used for research, LLaMA-2 is released under a [commercial license](https://github.com/facebookresearch/llama/blob/main/LICENSE), meaning that—_like MPT and Falcon_—the models can be used in commercial applications. However, the license used for LLaMA-2 is not a standard Apache 2.0 license—it has a few caveats that should be considered by practitioners. Most notably, any entity/application powered by LLaMA-2 with over 700 million monthly active users must obtain a license from Meta to use LLaMA-2. Read more about LLaMA-2’s license below.

[LLaMA-2 License](https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/)

## Trends in Open-Source LLMs

Given that LLaMA, MPT, Falcon, and LLaMA-2 perform so much better than their predecessors, we might reasonably ask: _what led the current generation of open-source LLMs to perform so well?_ Here, we will quickly look at a few key properties of these models that were especially valuable in catalyzing their impressive performance and quick rise to popularity. In particular, these models _i)_ were pre-trained over a massive amount of data and _ii)_ emphasize inference efficiency.

#### Better Data = Better Performance!

The key difference between current open-source LLMs and those that came before them is the dataset used for pre-training. While models like OPT and BLOOM are trained on 180 billion and 341 billion tokens, respectively, current open-source models are pre-trained over significantly larger datasets:

- _LLaMA_: 1.4 trillion tokens
    
- _MPT_: 1 trillion token
    
- _Falcon_: 1-1.5 trillion token
    
- _LLaMA-2_: 2 trillion tokens
    

Current open-source LLMs increase the amount of data used for pre-training by (nearly) an order of magnitude! In fact, these pre-training datasets are similarly-sized to those used for proprietary LLMs. For example, MassiveText (i.e., used to train [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher) [13] and [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [2]) contains roughly 2.3 trillion tokens, though only a subset is actually used for pre-training; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2076e2d3-4290-41eb-8f84-295ccae8f64c_1266x438.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2076e2d3-4290-41eb-8f84-295ccae8f64c_1266x438.png)

**Size isn’t everything!** In addition to increasing the amount of pre-training data significantly, current open-source LLMs pay close attention to the composition and quality of data. For example, the proportion of code is increased within the datasets used for training MPT, allowing the resulting models to perform much better on coding-based tasks. Additionally, Falcon-40B proposes an entirely new pipeline for constructing high-quality corpora of text from the web, while LLaMA-2 claims to use an updated data pipeline and mix for pre-training. Overall, focusing on the quality and composition of the pre-training dataset seems to be a common trend within recent research on open-source LLMs.

> _“We performed more robust data cleaning, updated our data mixes, trained on 40% more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models.”_ - from [14]

#### Optimizing for Faster Inference

In making the decision between using an open or closed-source LLM, practitioners have to consider more than just performance. Paid language model APIs might achieve impressive performance across a wide scope of tasks, but they oftentimes cannot be fine-tuned on domain-specific data. On the other hand, however, a major consideration when building applications with open-source LLMs is the cost of deploying the model. Given the difficulty of hosting LLMs, recent open-source models are oftentimes optimized for fast and easy inference. In fact, MPT-30B [10] is specifically sized so that it can be hosted on a single GPU!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25fa9afd-e9d2-4c08-b147-8c55870964bd_1754x1124.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25fa9afd-e9d2-4c08-b147-8c55870964bd_1754x1124.png)

(from [15, 16, 17])

**Modified architecture.** Beyond being slightly smaller than most proprietary models, current open-source LLMs adopt a variety of architectural tricks—shown in the figure above—to speed up the inference process, such as:

- Low Precision Layer Norm [[link](https://cameronrwolfe.substack.com/i/131642185/faster-inference)]
    
- Flash Attention[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-8-135439692) [[link](https://cameronrwolfe.substack.com/i/131642185/faster-inference)]
    
- Multi-Query Attention [[link](https://cameronrwolfe.substack.com/i/131393593/falcon-architecture)]
    
- Parallel Transformer [[link](https://cameronrwolfe.substack.com/i/131393593/falcon-architecture)]
    
- Group-Query Attention
    

Additionally, several other architecture modifications—e.g., [RoPE embeddings](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications), [ALiBi](https://cameronrwolfe.substack.com/i/131642185/alibi-enables-context-length-extrapolation), [SwiGLU activations](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications), and more—are adopted to improve performance. Current open-source LLMs apply simple modifications to the decoder-only transformer architecture to improve performance and inference speed.

## Final Thoughts

Within this overview, we have studied the evolution of open-source LLMs from initial, lower-quality models (e.g., BLOOM and OPT) to the more recent, powerful base models (e.g., LLaMA and MPT). To improve upon the performance of their predecessors, these recent models primarily focused upon curating larger, higher-quality datasets for pre-training, which resulted in a drastic improvement in quality. Given that a high-quality base model is a prerequisite for any LLM application, these models had a significant impact upon the raise in popularity of open-source LLMs. Instead of having to invest significant funds into pre-training a model from scratch, any practitioner can now leverage powerful base LLMs whether is be for research purposes or commercial applications.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

[Share](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEzNTQzOTY5MiwiaWF0IjoxNzQ1NzQ1Njg4LCJleHAiOjE3NDgzMzc2ODgsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.5O4x_UJmc_GjfEi-vKdPjZL2Of4X3pyH4pRVgZCsxy4)

#### Bibliography

[1] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[2] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[3] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[4] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[5] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[6] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[7] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[8] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.

[9] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _MosaicML_, 5 May 2023, www.mosaicml.com/blog/mpt-7b.

[10] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” _MosaicML_, 22 June 2023, www.mosaicml.com/blog/mpt-30b.

[11] “Introducing Falcon LLM”, _Technology Innovation Institute_, 7 June 2023, https://falconllm.tii.ae/.

[12] Penedo, Guilherme, et al. "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only." _arXiv preprint arXiv:2306.01116_ (2023).

[13] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." _arXiv preprint arXiv:2112.11446_ (2021).

[14] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." _arXiv preprint arXiv:2307.09288_ (2023).

[15] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." _arXiv preprint arXiv:2305.13245_ (2023).

[16] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[17] Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." _Advances in Neural Information Processing Systems_ 35 (2022): 16344-16359.

[18] Dao, Tri. "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning." _arXiv preprint arXiv:2307.08691_ (2023).

[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-1-135439692)

Even the [blog post](https://openai.com/research/gpt-4) for GPT-4 mentions that this seems to be the case!

[2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-2-135439692)

The Chinchilla paper [2], which provides several valuable insights, shows us that increasing the size (i.e., number of parameters) of a language model is most effective when we also increase the amount of data over which the model is pre-trained.

[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-3-135439692)

Despite the trend in LLM applications toward [longer context lengths](https://www.anthropic.com/index/100k-context-windows), most open-source LLMs (e.g., LLaMA, Falcon, and MPT-7B) are trained using a relatively short context length of only 2K tokens.

[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-4-135439692)

Chat versions of the MPT models cannot be used commercially, as they are trained on data that cannot be used commercially (e.g., [ShareGPT](https://sharegpt.com/)).

[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-5-135439692)

This model was recently dethroned on the OpenLLM leaderboard by various fine-tuned versions of LLaMA-2-70B.

[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-6-135439692)

Fine-tuned versions of LLaMA-2, called [LLaMa-2-Chat](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf), are optimized for chat use cases using both supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF).

[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-7-135439692)

Multi-query attention is used by a variety of different LLMs, even including Falcon-40B, to improve inference speed.

[8](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better#footnote-anchor-8-135439692)

By the way, FlashAttention was recently made faster with the proposal of FlashAttention-2 [18]. Read more about it [here](https://tridao.me/publications/flash2/flash2.pdf).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Sasa Zelenovic's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03681be7-d6d9-400a-99aa-89a8ac285970_144x144.png)



](https://substack.com/profile/24783940-sasa-zelenovic)

[

![Amed Mesa's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd752e93-b6a1-4ed8-98f7-75e2f8cbac85_144x144.png)



](https://substack.com/profile/33220885-amed-mesa)

[

![coddiwomple's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1af537f-8e05-43a9-b121-e90734e5a790_222x222.jpeg)



](https://substack.com/profile/977582-coddiwomple)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

32 Likes∙

[4 Restacks](https://substack.com/note/p-135439692/restacks?utm_source=substack&utm_content=facepile-restacks)

32

- 

[

3

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sasa Zelenovic's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03681be7-d6d9-400a-99aa-89a8ac285970_144x144.png)



](https://substack.com/profile/24783940-sasa-zelenovic?utm_source=comment)

[Sasa Zelenovic](https://substack.com/profile/24783940-sasa-zelenovic?utm_source=substack-feed-item)

[2023年8月1日](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comment/21685647 "2023年8月1日 21:33")

Liked by Cameron R. Wolfe, Ph.D.

Great read! Thanks for doing this writeup. My company Neural Magic is working on making open-source LLMs even more efficient with sparsity, so they can be deployed on ordinary CPUs without GPUs. We'll make sure to share our progress with you!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comment/21685647)

[

![Adam Mackay's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec477f7-c78c-41a8-98a4-e866c876c911_500x500.jpeg)



](https://substack.com/profile/3093828-adam-mackay?utm_source=comment)

[Adam Mackay](https://substack.com/profile/3093828-adam-mackay?utm_source=substack-feed-item)

[Present Tense/Future Perfect](https://adammackay.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年8月1日](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comment/21680568 "2023年8月1日 19:27")

Liked by Cameron R. Wolfe, Ph.D.

It's impressive to see how LLaMA and other models have improved the performance of these language models and paved the way for more open-source research. Looking forward to your next installment on fine-tuning applications.

Like (1)

Reply

Share

[1 more comment...](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



-----



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# The History of Open-Source LLMs: Imitation and Alignment (Part Three)

### Open-source LLMs need alignment to become truly remarkable...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Aug 07, 2023

19

- 

[

2

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation/comments)

2

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2810c19f-dc44-464f-bb28-10c85d33c42d_2388x1402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2810c19f-dc44-464f-bb28-10c85d33c42d_2388x1402.png)

(from [1, 2, 12])

A majority of prior research on open-source large language models (LLMs) focused heavily upon creating [pre-trained base models](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better). However, these models have not undergone any fine-tuning, so they fail to match the quality of top closed-source LLMs (e.g., ChatGPT or Claude) due to their lack of [alignment](https://cameronrwolfe.substack.com/i/135273362/training-and-using-language-models). Paid models are aligned extensively using techniques like [SFT and RLHF](https://cameronrwolfe.substack.com/i/93578656/where-do-generic-llms-fall-short), which greatly enhances their usability. In comparison, open-source models are typically fine-tuned to a lesser extent using smaller, public datasets. Within this overview, however, we will take a look at recent research that aims to improve the quality of open-source LLMs via more extensive fine-tuning and alignment.

This overview is the third (and final[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-1-135626389)) part of my series on the history of open-source LLMs. In the [first part](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early) of the series, we looked at initial attempts at creating open-source language models. Although these initial pre-trained LLMs performed poorly, they were quickly followed up by much better open-source base models, which we covered in [part two](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better) of this series. Now, we will cover how these better open-source models can be fine-tuned/aligned to improve their quality and close the gap in performance between open-source and proprietary LLMs, completing the journey from initial models like OPT to the incredibly high-performing open-source LLMs that we have today (e.g., [LLaMA-2-Chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa518db48-b878-43a8-8941-cb3580a78c5f_2512x662.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa518db48-b878-43a8-8941-cb3580a78c5f_2512x662.png)

(from [17, 18])

**The alignment process.** This overview will study the fine-tuning and alignment process for open-source LLMs. Prior to studying research in this area, however, we need to understand what alignment is and how it is accomplished. We should recall that the training process for language models proceeds in several parts. As shown above, we begin with pre-training, which is followed by several fine-tuning steps. After pre-training, the LLM can accurately perform [next token prediction](https://cameronrwolfe.substack.com/i/85568430/language-modeling), but its output may be repetitive and uninteresting. Thus, the model needs to be fine-tuned to improve its _alignment_, or its ability to generate text that aligns with the desires of a human user (e.g., follow instructions, avoid harmful output, avoid lying, produce interesting or creative output, etc.).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)

(from [17])

**SFT.** Alignment is accomplished via two fine-tuning techniques: supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF); see above for a depiction and [here](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior) for more details. SFT simply fine-tunes the model, using a standard language modeling objective, over examples of high-quality prompt and response pairs. The LLM is allowed to see examples of how it should respond and learn from these responses! SFT is incredibly simple and effective, but it requires carefully curating a dataset that captures “correct” behavior.

**RLHF** trains the LLM directly on feedback from human annotators—_humans identify outputs that they like, and the LLM learns how to produce more outputs like this_. To do this, we first obtain a set of prompts and generate several different outputs from the LLM on each prompt. Using a group of human annotators, we score each of these responses based on their quality. These scores can then be used to train a reward model (i.e., just a fine-tuned version of our LLM with an added regression head) to predict the score of a response. From here, RLHF fine-tunes the model to maximize this score using a reinforcement learning algorithm called [PPO](https://openai.com/research/openai-baselines-ppo). Typically, the highest-performing LLMs are aligned by performing both SFT and RLHF[2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-2-135626389) (with lots of human feedback) in sequence.

## Imitation Learning

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b790fbc-a391-4a0d-b2c7-6ce77ad7df01_1562x674.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b790fbc-a391-4a0d-b2c7-6ce77ad7df01_1562x674.png)

(from [16])

With the release of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [3], the open-source research community finally had access to powerful base LLMs that could be fine-tuned or aligned for a variety of different applications. As such, LLaMa catalyzed an [explosion](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) of open-source LLM research, as practitioners rushed to fine-tune LLaMA models on their task of choice. Interestingly, one of the most common directions of research during this time was _imitation learning_. Imitation learning, which is (arguably) a form of alignment[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-3-135626389), fine-tunes an LLM over outputs from another, more powerful LLM. Such an approach is inspired by the idea of [knowledge distillation](https://cameronrwolfe.substack.com/i/114077195/knowledge-distillation); see above.

> _“The premise of model imitation is that once a proprietary LM is made available via API, one can collect a dataset of API outputs and use it to fine-tune an open-source LM.”_ - from [6]

The question posed by open-source imitation learning research was simple: _can we create a model that is as powerful as ChatGPT or GPT-4 by just fine-tuning on responses from these models?_ To test this out, we can follow a simple approach:

- Collect dialogue examples from these models (e.g., using the OpenAI API).
    
- Perform (supervised) fine-tuning on this data (i.e., using a [normal language modeling objective](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20)).
    

As we will see, the research community hotly debated whether imitation learning was a valuable approach for quite some time! In the end, we found out that the approach is practically useful, but it only works well under certain conditions.

#### Initial Efforts in Imitation Learning

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)

LLaMA catalyzed the creation of numerous imitation models (from [7, 8, 9, 10])

After the release of LLaMA, researchers quickly began to release a variety of imitation models using dialogue derived from ChatGPT. Typically, the data used for training—_which prohibits the resulting model from being used commercially_—is obtained either from the OpenAI API or sources like [ShareGPT](https://sharegpt.com/). A few of the most widely-known imitation models are outlined below (in chronological order).

**Alpaca [7]** fine-tunes LLaMA-7B by using the [self-instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework) [11] framework to automatically collect a fine-tuning dataset from [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5) (i.e., `text-davinci-003`). Collecting data and fine-tuning Alpaca costs only $600; see [here](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model).

**Vicuna [8]** fine-tunes LLaMA-13B over 70K dialogue examples from ChatGPT (i.e., derived from ShareGPT). Interestingly, the entire fine-tuning process for Vicuna costs only $300; more details are available [here](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality).

**Koala [9]** fine-tunes LLaMA-13B on a large dataset of dialogue examples from both the Alpaca fine-tuning set and a variety of other sources like [ShareGPT](https://sharegpt.com/), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [OIG](https://laion.ai/blog/oig-dataset/), [Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf), and OpenAI [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)/[Summarization](https://huggingface.co/datasets/openai/summarize_from_feedback). Compared to prior imitation models, Koala is fine-tuned over a larger dataset and evaluated more extensively; read more about the model [here](https://cameronrwolfe.substack.com/i/114077195/koala-a-dialogue-model-for-academic-research).

**GPT4ALL [16]** fine-tunes LLaMA-7B on over 800K chat completions from `GPT-3.5-turbo`. Along with the model, authors release both training/inference code and quantized model weights that can be used to perform inference with minimal compute resources (e.g., a laptop); see [here](https://gpt4all.io/index.html) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72f27ee3-9958-4370-8f47-210f51281187_1294x1274.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72f27ee3-9958-4370-8f47-210f51281187_1294x1274.png)

(from [8, 9])

**The impact of imitation.** These models were published in close succession and claimed to achieve comparably quality to top proprietary models like ChatGPT and GPT-4. For example, Vicuna is found to maintain 92% of the quality of GPT-4[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-4-135626389), while Koala is found to match or exceed the quality of ChatGPT in many cases; see above. Such findings seemed to indicate that model imitation could be used to distill the capabilities of any proprietary model into a smaller, open-source LLM. If this were true, the quality of even the best proprietary LLMs could be easily replicated and these models would be left with [no true advantage](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).

> _“Open-source models are faster, more customizable, more private, and … more capable. They are doing things with $100 and 13B params that [Google] struggles with at $10M and 540B. And they are doing so in weeks, not months.”_ - from [9]

The explosion of imitation models was one of the first instances in which open-source models were truly seen as a potential alternative to the closed-source LLMs that had dominated the LLM landscape [since the proposal of GPT-3](https://openai.com/blog/openai-api). Despite the use of paid APIs becoming standard, the impressive performance of imitation models fostered a feeling of promise for open-source LLMs.

#### Are imitation models a false promise?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc80eb65d-a2c8-4e36-8f6b-a9db6b799092_1612x1310.png)

(from [6])

Despite the promise of imitation models’ impressive performance, we see in [6] that we are missing something important. Namely, more targeted evaluations of these models reveal that they do not perform nearly as well as top proprietary LLMs like ChatGPT and GPT-4. In fact, we see that fine-tuning a base model via imitation actually does very little to close the gap in performance between open-source and proprietary models in most cases. Rather, the resulting model tends to only improve in performance on tasks that are heavily represented in the fine-tuning set and may even have a more pronounced tendency for hallucination.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31721f8a-beec-4a06-8776-c269d4c656c0_992x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31721f8a-beec-4a06-8776-c269d4c656c0_992x1182.png)

(from [6])

**Experimental setup.** To determine the utility of imitation learning, authors in [6] curate a dataset of ~130K diverse dialogue examples from ChatGPT. Then, several different sizes of language models are fine-tuned over various amounts of imitation data before having their performance measured. As shown above, there are a few interesting observations that we can make from these experiments:

- The amount of imitation data used for fine-tuning does not improve model quality in human evaluation trials.
    
- Imitation models’ performance on standardized benchmarks is often worse than that of the base model (and deteriorates as more imitation data is used).
    
- Increasing the size of the base model consistently improves the quality of the resulting imitation models.
    

**What is going on here?** When imitation models are evaluated across a wider variety of natural language benchmarks, we see that their performance is comparable to or below that of the corresponding base LLM. In other words, _imitation models do not actually match the quality of models like ChatGPT_. Compared to proprietary LLMs, these models have a less extensive knowledge base, as revealed by the performance improvement observed with larger base models.

> _“We argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.”_ - from [6]

With this in mind, the first question we might have is: _why did it seem like these models performed so well?_ We see in [6] that imitation models learn to mimic the style of a model like ChatGPT. As such, human workers [can be tricked](https://cameronrwolfe.substack.com/i/127874443/are-imitation-models-actually-useful) into perceiving the model as high-quality even if it generates factually incorrect information more frequently (i.e., this is harder to easily check or verify).

#### Is imitation learning actually useful?

> _“Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.”_ - from [1]

After work in [6] revealed that imitation models did not perform nearly as well as initially thought, the research community was unclear whether imitation models actually had any value. Notably, analysis in [6] indicates that local imitation—_or learning to imitate a model’s behavior on a specific task, instead of imitating its behavior as a whole—_is quite effective. However, this does not mean the imitation model matches the quality of proprietary models more generally. To make imitation models better in general, authors in [6] pose two paths forward:

- Generating a much bigger and more comprehensive imitation dataset
    
- Creating a better base model to use for imitation learning
    

Interestingly, both of these recommendations were explored extensively by subsequent research and found to yield positive results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec485bd5-6b09-4955-aa44-08f433533f73_2134x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec485bd5-6b09-4955-aa44-08f433533f73_2134x838.png)

(from [12])

**Orca [12]** is an imitation model based upon LLaMA-13B; see [here](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) for more details. Compared to prior work on imitation learning, however, Orca is trained over a higher-quality, more detailed, and more comprehensive dataset collected from ChatGPT and GPT-4. In particular, prior datasets collected for imitation learning can be considered “shallow”—they are simply examples of prompt and response pairs generated by a model like ChatGPT; see above.

> _“We conclude that broadly matching ChatGPT using purely imitation would require a concerted effort to collect enormous imitation datasets and far more diverse and higher quality imitation data than is currently available.”_ - from [6]

Improving upon shallow imitation, Orca attempts to augment imitation datasets generated by models like ChatGPT or GPT-4 with:

- Explanation traces
    
- Step-by-step thought processes
    
- Complex instructions
    

To do this, the model being imitated is prompted to provide detailed explanations of its response via an instruction or system message. Such an approach goes beyond simple prompt-response pairs by adding extra, useful information to the data seen by an imitation model. When learning from powerful LLMs like ChatGPT, Orca sees more than just the model’s response. Namely, it can learn from detailed explanations and thought processes generated along with the model’s response on complex prompts! See below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d8e0c9-5abb-498f-8253-e2620de2ab73_1878x1322.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d8e0c9-5abb-498f-8253-e2620de2ab73_1878x1322.png)

(from [12])

After being fine-tuned over a massive dataset of such detailed imitation data (i.e., 5M examples from ChatGPT and 1M examples from GPT-4[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-5-135626389)), we see that Orca performs incredibly well compared to prior imitation models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fd3ba2e-0567-4c27-adba-8df8ca608c47_2138x872.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fd3ba2e-0567-4c27-adba-8df8ca608c47_2138x872.png)

Although Orca significantly narrows the gap between open-source imitation models and proprietary LLMs, we still see in the table below that the model is outperformed consistently by GPT-4. Unfortunately, even an improved imitation approach is not enough to fully match the quality of top proprietary models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9de8199c-f89b-4627-a4eb-b363e23aa8b1_1080x558.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9de8199c-f89b-4627-a4eb-b363e23aa8b1_1080x558.png)

Nonetheless, Orca’s impressive performance reveals that imitation learning is a valuable fine-tuning strategy that can drastically improve the performance of any high-quality base LLM. Going further, we learn in [12] that leveraging imitation learning successfully has two main requirements:

- A large, comprehensive imitation dataset
    
- Detailed explanation traces within each response
    

**Better base LLMs.** Although authors in [6] argue that collecting a sufficiently large and diverse imitation learning dataset is incredibly difficult, we see with Orca that such a feat is at least possible. Additionally, later work extensively explores the alternative suggestion in [6]: _creating more powerful (open-source) base models_. Although open-source pre-trained LLMs [performed poorly at first](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early), we have recently seen the proposal of a variety of powerful pre-trained LLMs; e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [3], [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact) [14, 15], and [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [13]. Given that model pre-training is a starting point for any fine-tuning that follows (e.g., imitation learning, SFT, RLHF, etc.), starting with a better base model improves the downstream imitation model as well! Luckily, we covered all of the best open-source, pre-trained language models in part two of this series. See below for more details.

[Better Base Models](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better)

## Aligning Open-Source LLMs

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a4c31-43b9-477e-925f-bcefdc97fdbb_1278x778.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F023a4c31-43b9-477e-925f-bcefdc97fdbb_1278x778.png)

(from [5])

Imitation learning attempted to improve the quality of open-source base models by training over the responses (and explanation traces) of proprietary LLMs. Although this approach is successful in some cases, this is (obviously) not the manner in which the top proprietary models are trained—_imitation is a short cut for creating powerful open-source models_. If we want open-source LLMs that rival the quality of proprietary models, we need to invest significantly into alignment.

> _“These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible.”_ - from [1]

**What’s the hold up?** The idea of aligning open-source imitation models seems easy enough. We have really great base models, _why not just replicate the alignment process used by models like GPT-4?_ The alignment process requires extensive compute and human annotation resources. Plus, it is heavily dependent upon proprietary data, which limits transparency and makes reproducing results quite difficult. As such, open-source models have lagged behind their proprietary counterparts in alignment research for quite some time. Within this section, however, we will explore two recent works—LIMA [2] and LLaMA-2 [1]—that drastically improve the quality of open-source LLMs via better alignment.

#### Prior Work on Open-Source Alignment

Before covering LIMA and LLaMA-2, it is important to note that the open-source research community has not avoided aligning pre-trained models altogether. For example, [Falcon-40B](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source)-Instruct [13] undergoes SFT over 150M token of data from [Baize](https://github.com/project-baize/baize-chatbot). Similarly, numerous fine-tuned variants of [MPT-7B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-a-commercially-usable-llama-b) [14] and [MPT-30B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-an-open-source-gpt-alternative) [15] have been released, including both chat/instruct variants that undergo SFT on public datasets and a StoryWriter variant that is fine-tuned over data with a much longer context length.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a116e7d-e0b1-414c-ae57-2327fdf47fd1_1424x1072.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a116e7d-e0b1-414c-ae57-2327fdf47fd1_1424x1072.png)

(from the Open LLM leaderboard)

Plus, if we take a simple look at the Open LLM Leaderboard (see above), we see a variety of different models that have underwent fine-tuning via SFT on all types of different datasets. Open-source LLMs have not avoided alignment altogether. However, top proprietary models undergo both SFT and RLHF over massive datasets of high-quality dialogue and human feedback. In comparison, most open-source models have been aligned using solely SFT over public datasets that lack in quality and diversity. To truly match the quality of proprietary models, _open-source LLMs needed to make an attempt at replicating their alignment process._

#### [LIMA: Data-Efficient Alignment](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) [2]

> _“A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.”_ - from [2]

As mentioned above, open-source LLMs—for quite some time—mostly performed alignment via SFT on public datasets. Given this heavy emphasis upon SFT, authors in [2] studied extensively the impact impact of SFT on pre-trained LLMs. The goal of this analysis was to uncover the relative importance of pre-training and alignment via SFT in creating a high-performing LLM, as well as to reveal best practices for maximizing a model’s performance after undergoing SFT.

**The dataset.** To do this, authors in [2] construct a small dataset of 1,000 dialogue examples to use for SFT. Although this might not seem like enough data, the examples included in this dataset are [carefully curated](https://cameronrwolfe.substack.com/i/134561977/curating-data-for-alignment) to ensure quality by using diverse prompts and a uniform output style or tone; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cfe7b4d-b3d1-403c-b97c-7ae18c59c68e_964x1542.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cfe7b4d-b3d1-403c-b97c-7ae18c59c68e_964x1542.png)

(from [2])

The SFT dataset used to train LIMA is small but of incredibly high quality. Interestingly, we see in [2] that LIMA performs surprisingly well when fine-tuned over this dataset, even approaching the performance of state-of-the-art LLMs like GPT-4 or Claude; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)

(from [2])

Such a result reveals that language models can be effectively aligned via a small number of carefully chosen examples. Although LIMA still falls short of GPT-4’s performance, the ability to perform such high-quality alignment with such little data is both unexpected and impressive. Such a result shows us that data quality is seemingly the most important factor in performing alignment via SFT.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56f39948-b454-46eb-af39-f102405868dc_1618x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56f39948-b454-46eb-af39-f102405868dc_1618x840.png)

**What do we learn?** We learn a variety of useful lessons from LIMA. First, the quality of data is incredibly important for SFT. Just using more data is not enough—_the data also needs to be of high quality_; see above. Additionally, the results in [2] lead to the proposal of the “Superficial Alignment Hypothesis”, which offers a new and unique perspective of alignment. Put simply, this hypothesis posits that most of an LLM’s core knowledge is [learned during pre-training](https://twitter.com/cwolferesearch/status/1660744247123890179?s=20), while alignment searches for the proper format or style for surfacing this knowledge. As such, alignment can be learned in a data efficient manner.

#### LLaMA-2: Improving Transparency in Alignment Research [1]

> _“Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.”_ - from [1]

The recently-released LLaMA-2 [1] suite of LLMs is comprised of several open-source models with sizes ranging from 7-70 billion parameters. Compared to their predecessors (i.e., [LLaMA-1](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [3]), LLaMA-2 models differentiate themselves by pre-training over 40% more data (i.e., 2 trillion tokens instead of 1.4 trillion), having a longer context length, and using an architecture that is optimized for fast inference (i.e., by using [grouped query attention](https://twitter.com/_philschmid/status/1673335690912825347?s=20) [4]). LLaMA-2 achieves state-of-the-art performance among open-source models; read more below.

[LLaMA-2 Base Models](https://cameronrwolfe.substack.com/i/135439692/llama-current-state-of-the-art)

However, the LLaMA-2 suite contains more than just pre-trained LLMs. Authors invest heavily into the alignment process by fine-tuning each model—using both SFT and RLHF—over a massive amount of dialogue data and human feedback; see below. The resulting models are referred to as the LLaMA-2-Chat models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73ad11af-3349-4e29-9522-86a64af4c78b_2202x1338.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73ad11af-3349-4e29-9522-86a64af4c78b_2202x1338.png)

(from [5])

These refined versions of LLaMA-2 perform incredibly well and take a major step towards closing the gap in alignment between open-source and proprietary LLMs. LLaMA-2’s alignment process emphasizes two key behavioral properties:

1. _Helpfulness_: the model fulfills users’ requests and provides requested information.
    
2. Safety: the model avoids responses that are “unsafe”
    

To ensure that the aligned model is both helpful and safe, data curated for both SFT and RLHF is filtered, collected, and annotated according to these principles.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F159a4ee7-73e4-490a-8472-377c871bd55a_1898x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F159a4ee7-73e4-490a-8472-377c871bd55a_1898x1004.png)

(from [1])

**SFT.** The first step in LLaMA-2’s alignment process is fine-tuning with SFT. Similar to other open-source LLMs, LLaMA-2 is first fine-tuned over publicly-available [instruction tuning](https://cameronrwolfe.substack.com/i/130195416/instruction-tuning) data. However, such data tends to lack in diversity and quality, which—_as demonstrated by LIMA [2]_—massively impacts performance. As such, authors in [1] focus upon collecting a smaller set of high-quality data for SFT. This data comes from a variety of sources, including both manually created or annotated examples and data from public sources that is filtered for quality. Ultimately, LLaMA-2 undergoes a second stage of fine-tuning with 27,540 high-quality dialogue examples; see above for samples.

> _“Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF.”_ - from [1]

Interestingly, authors in [1] observe that collecting more data (i.e., beyond the 27K high-quality examples) for SFT provides diminishing benefits. These findings align with the empirical analysis from LIMA [2]. We don’t need a ton of data for SFT, but the data should be of high-quality! Interestingly, authors in [1] also note that LLaMA-2 models that have underwent SFT seem to be capable of generating their own data for SFT anyways.

**RLHF.** LLaMA-2 is further fine-tuned using RLHF[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-6-135626389) over a dataset of >1M examples of human feedback. To collect this feedback, a binary protocol is adopted, in which human annotators are asked to write a prompt and choose the better of two generated responses from the LLM. Here, human preference data is collected according to both helpfulness and safety standards. For example, human preference annotations focused upon safety may encourage the annotator to craft an adversarial prompt that is likely to elicit an unsafe response. Then, the human annotator can label which of the responses—if any—is preferable and safe.

> _“Everything else being equal, an improvement of the reward model can be directly translated into an improvement for Llama 2-Chat.”_ - from [1]

Human feedback data is collected in batches, and LLaMA-2 is fine-tuned via RLHF between each batch. As such, several versions of each LLaMA-2-Chat model—five in total—are iteratively created after each trial of RLHF. In [1], we see that a new reward model is trained for use in RLHF each time fresh human preference data is collected, ensuring the reward model accurately captures human preferences of the latest model. Additionally, we see that the quality of the resulting reward model is surprisingly predictive of LLaMA-2-Chat model quality overall. In total, LLaMA-2 is fine-tuned on over 1M instances of human feedback throughout the entirety of the iterative RLHF process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F701fb253-6bbf-4c36-9894-e243c3ef105a_2222x1058.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F701fb253-6bbf-4c36-9894-e243c3ef105a_2222x1058.png)

(from [1])

As shown in the figure above, the quality of LLaMA-2-Chat—in terms of both helpfulness and safety—improves smoothly throughout the several iterations of alignment with both SFT and RLHF. This visualization clearly depicts the level of impact of each technique on the resulting model’s quality. Namely, performing SFT alone only gets us so far! The model’s alignment improves drastically with each phase of RLHF that is performed even after SFT is applied.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d03acc6-4ded-4a8d-9d22-8a2d5f0b5143_1550x392.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d03acc6-4ded-4a8d-9d22-8a2d5f0b5143_1550x392.png)

All top-5 models on the Open LLM leaderboard are based upon LLaMA-2 (from Open LLM leaderboard)

**Performance.** The LLaMA-2-Chat models are currently state-of-the-art for open-source LLMs, as shown by the [Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) above. When LLaMA-2-Chat models are compared to other popular LLMs in [1], we see that they far exceed other open-source models in terms of helpfulness and safety; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F280709af-60eb-49d9-bbd2-d6bc26ebb2bd_1186x1290.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F280709af-60eb-49d9-bbd2-d6bc26ebb2bd_1186x1290.png)

(from [1])

Furthermore, LLaMA-2 is even found to perform comparably to top proprietary models like ChatGPT when evaluated in terms of helpfulness and safety. Put simply, these results heavily indicate that the quality of alignment performed for the LLaMA-2-Chat models is high. The resulting models tend to accurately capture and adhere to desired helpfulness and safety standards.

> _“[Alignment] can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.”_ - from [1]

**The importance of LLaMA-2.** The impact of LLaMA-2 on open-source LLM research goes beyond simply setting a new state-of-the-art in terms of performance. _Why?_ We see in [2] that LLaMA-2 adopts a fundamentally different approached compared to prior work. Due to the fact that closed-source LLMs are typically aligned with extensive amount of proprietary, human-annotated data, this process has been more difficult to replicate within open-source research. Although prior open-source models mostly leverage SFT and public sources of dialogue data[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-7-135626389), LLaMA-2 is one of the first open-source LLMs to invest extensively into the alignment process, curating a great deal of high-quality dialogues and human preferences for both SFT and RLHF.

## Closing Remarks

We have now studied the entire journey of open-source language models from OPT to LLAMA-2. Despite the incredible amount of research that occurred between these two models, their proposal was only a year apart! The open-source AI research community moves very quickly, and keeping up with research in this area is incredibly exciting, interesting, and rewarding. Having access to powerful models like LLaMA-2-Chat is humbling. As both practitioners and researchers, we have the ability to use these models, learn from them, and truly gain a deeper understanding of how they work. Such an opportunity is unique and should not be taken for granted. Especially for LLMs, open-source is pretty cool!

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

[Share](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEzNTYyNjM4OSwiaWF0IjoxNzQ1NzQ1NjkwLCJleHAiOjE3NDgzMzc2OTAsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.PCwuuqZhUWWpEJMOdiKNwczqasc6sGnAAdqJv9N1Bi4)

#### Bibliography

[1] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." _arXiv preprint arXiv:2307.09288_ (2023). 

[2] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[3] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[4] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." _arXiv preprint arXiv:2305.13245_ (2023).

[5] “Introducing Llama2: The next generation of our open source large language model”, _Meta_, https://ai.meta.com/llama/.

[6] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[7] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[8] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[9] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.

[11] Wang, Yizhong, et al. "Self-instruct: Aligning language model with self generated instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[12] Mukherjee, Subhabrata, et al. "Orca: Progressive Learning from Complex Explanation Traces of GPT-4." _arXiv preprint arXiv:2306.02707_ (2023).

[13] “Introducing Falcon LLM”, _Technology Innovation Institute_, https://falconllm.tii.ae/.

[14] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _MosaicML_, www.mosaicml.com/blog/mpt-7b.

[15] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” _MosaicML_, www.mosaicml.com/blog/mpt-30b.

[16] Gou, Jianping, et al. "Knowledge distillation: A survey." _International Journal of Computer Vision_ 129 (2021): 1789-1819.

[17] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[18] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-1-135626389)

For now! I’m sure that I will write another post in this series after research on open-source LLMs continues to develop.

[2](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-2-135626389)

This “recipe”—commonly called the three-step technique—was proposed by [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) (the sister model to ChatGPT) and has been heavily used by a variety of powerful LLMs ever since!

[3](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-3-135626389)

I’m not 100% sure whether imitation learning would be considered alignment. It is quite similar to SFT, where we choose dialogue examples for SFT from existing powerful LLMs (e.g., GPT-4). One could also consider imitation learning a form of generic fine-tuning or even an [instruction tuning](https://cameronrwolfe.substack.com/i/114077195/instruction-fine-tuning) variant.

[4](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-4-135626389)

This metric is obtained via automatic evaluations that use GPT-4 as a judge.

[5](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-5-135626389)

Orca uses prompts from the [FLAN collection](https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html) to generate its imitation dataset, which takes several weeks to collect due to rate/token limits on the OpenAI API.

[6](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-6-135626389)

Interestingly, authors in [1] adopt two different approaches for RLHF, including the typical [PPO](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b) variant of RLHF and a rejection sampling fine-tuning variant that _i)_ samples K outputs from the model, _ii)_ selects the best one, and _iii)_ fine-tunes on this example. Notably, both methods are based upon reinforcement learning.

[7](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation#footnote-anchor-7-135626389)

This public data may even come from other, powerful LLMs, as in the case of imitation learning. See, for example, dialogues available via [ShareGPT](https://sharegpt.com/).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![James Le's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd92c7fe-0109-44bd-a10f-19c8bd4840c0_3596x2514.jpeg)



](https://substack.com/profile/6009523-james-le)

[

![Jeny's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1087b596-5d71-4961-a059-07f55c307901_144x144.png)



](https://substack.com/profile/42336258-jeny)

[

![Wayne's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7262dda0-aecc-4613-bce8-0218f23381b2_144x144.png)



](https://substack.com/profile/2157901-wayne)

[

![Matt Gruner's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd9b6c1c-cdea-4b58-b1ad-dddb91fad763_1080x1080.jpeg)



](https://substack.com/profile/17923793-matt-gruner)

[

![Benjamin Marie's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcad63296-e403-4e10-b54f-a1dc5602f881_1280x1280.png)



](https://substack.com/profile/155699076-benjamin-marie)

19 Likes∙

[2 Restacks](https://substack.com/note/p-135626389/restacks?utm_source=substack&utm_content=facepile-restacks)

19

- 

[

2

](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Jinxu's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60aba3ce-90e7-409a-898f-10b771820a12_144x144.png)



](https://substack.com/profile/101857268-jinxu?utm_source=comment)

[Jinxu](https://substack.com/profile/101857268-jinxu?utm_source=substack-feed-item)

[2023年8月22日](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation/comment/36900245 "2023年8月22日 15:21")

Liked by Cameron R. Wolfe, Ph.D.

Cameron, great article! I’m wondering if we can translate your blog into Chinese and post it on AI community in China. We will highlight your name and keep the original link on the top of the translation version. Thank you.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation/comment/36900245)

[1 more comment...](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



---



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# LLaMA-2 from the Ground Up

### Everything you need to know about the best open-source LLM on the market...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Aug 14, 2023

43

- 

[

6

](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comments)

6

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3464668-f880-4091-a4f0-0c26bfe9f4c9_2134x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3464668-f880-4091-a4f0-0c26bfe9f4c9_2134x1182.png)

(from [1])

The opeurce AI research community has heavily explored the creation of open-source, commercially-usable large language models (LLMs). Although a variety of such models perform well, none stand out quite like LLaMA-2 [1], a suite of recently-proposed open-source language models with sizes ranging from 7 to 70 billion parameters. Compared to their predecessors (i.e., [LLaMA-1](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [2]), LLaMA-2 models differentiate themselves by pre-training over more data, using a longer [context length](https://cameronrwolfe.substack.com/i/117151147/the-context-window), and adopting an architecture that is optimized for faster inference. Additionally, LLaMA-2 goes beyond prior research in open-source LLMs by investing heavily into the models’ alignment process, which is used to create LLaMA-2-Chat models that are optimized for dialogue applications and nearly match the quality of top proprietary LLMs (e.g., ChatGPT and GPT-4) in certain areas. Within this overview, we will explore the details of LLaMA-2 and build an in-depth understanding of these models starting from basic concepts.

> _“There have been public releases of pre-trained LLMs (such as BLOOM) that match the performance of closed pre-trained competitors like GPT-3 and Chinchilla, but none of these models are suitable substitutes for closed product LLMs, such as ChatGPT, BARD, and Claude.”_ - from [14]

**Language model fundamentals.** Throughout this overview, we will try to build an understanding of relevant concepts from the ground up (sometimes providing links for further reading for the purpose of brevity). However, we will not cover fundamentals with respect to how language models work in general. For these details, I highly recommend reading the resources below:

- The transformer architecture [[link](http://jalammar.github.io/illustrated-transformer/)]
    
- Decoder-only transformer architecture [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]
    
- Language modeling (next-token prediction) objective [[link](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective)]
    
- Prompting language models [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Specialized language models [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)]
    
- Decoding (or inference) with a language model [[link](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20)]
    

Gaining an in-depth understanding of fundamental concepts related to language models is important for understanding this overview. Leveraging this knowledge, we will now build upon these basic concepts to better understand how LLaMA-2 approaches and improves the pre-training and alignment process for LLMs.

## Model Architecture and Pre-Training

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd8bc0b7-e7b7-4a96-9e00-627ad2ecda20_2232x1362.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd8bc0b7-e7b7-4a96-9e00-627ad2ecda20_2232x1362.png)

(from [3])

There are many factors that contribute to the utility of LLaMA-2. To begin with, however, the models use a modified (and improved) model architecture and pre-training procedure. Compared to its predecessor, LLaMA-2 has an architecture that is optimized for faster inference and is pre-trained over more data, allowing a broader knowledge base to be formed; see above.

#### Optimized Architecture with Faster Inference

LLaMa-2 adopts the model architecture of LLaMA-1 with a few modifications. To understand LLaMA-2’s architecture, we need a working understanding of the transformer architecture in general; see [here](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers) for more details on this topic. More specifically, nearly all causal language models adopt the [decoder-only variant](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20) of the transformer architecture. Now, we will study some of the modifications to this architecture that are made by both [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [2] and LLaMA-2 [1]. All of these changes apply to both LLaMA and LLaMA-2 unless otherwise specified.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6fab4d-7825-47ec-a59e-979d9dfa0384_1256x938.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b6fab4d-7825-47ec-a59e-979d9dfa0384_1256x938.png)

LLaMA-2 uses a pre-normalization variant of the normal transformer block

**Normalization.** Most transformer architectures adopt [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), which is applied after each layer within the transformer block; see above. LLaMA, however, replaces this with a variant called Root Mean Square Layer Normalization (or [RMSNorm](https://github.com/bzhangGo/rmsnorm) for short!), which is a simplified version of layer normalization that has been shown to improve training stability and generalization[1](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-1-135824233) [4]. RMSNorm is formulated as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaeeee5e-8cba-426f-9fa4-8666a7e76155_1080x138.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaeeee5e-8cba-426f-9fa4-8666a7e76155_1080x138.png)

For LLaMA, a pre-normalization variant of RMSNorm is adopted, meaning that normalization is applied prior to the major layers in a transformer block, rather than after, as shown in the figure above. RMSNorm achieves comparable levels of performance compared to layer norm with a 10-50% improvement in efficiency.

**Activation functions.** LLaMA models adopt the SwiGLU activation function—as opposed to the standard [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) function adopted by most neural networks—within their feed-forward layers. The SwiGLU activation function can be formulated as follows.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9aec7e3e-45bc-42f1-9e0c-0248bee25263_982x244.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9aec7e3e-45bc-42f1-9e0c-0248bee25263_982x244.png)

SwiGLU is an element-wise product of two [linear transformations](https://mathworld.wolfram.com/LinearTransformation.html) of the input `x`, one of which has had a Swish activation applied to it. This activation function requires three matrix multiplications (i.e., it is more computationally expensive than a normal activation function such as ReLU), but it has been found to yield improvements in performance relative to other activation functions, _even when the amount of compute being used is held constant_. A more in-depth analysis of potential activation functions for LLMs can be found in [5].

**RoPE.** Instead of using [absolute](https://cameronrwolfe.substack.com/p/language-understanding-with-bert#%C2%A7berts-architecture) or [relative](https://jaketae.github.io/study/relative-positional-encoding/) positional embeddings, LLaMA models adopt a RoPE [6] scheme, which finds a balance between the absolute and relative position of each token in a sequence. This position embedding approach encodes absolute position with a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix) and adds relative position information directly into the [self-attention operation](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20). The benefit of RoPE embeddings on tasks with longer sequence lengths has led this approach to be adopted by a variety of LLMs (e.g., [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [7] and [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [8]); read more below.

[Learn about RoPE](https://blog.eleuther.ai/rotary-embeddings/)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9b56d4a-69cc-446d-83e1-5359b87ebf8e_2212x816.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9b56d4a-69cc-446d-83e1-5359b87ebf8e_2212x816.png)

(from [1])

**What is different?** So far, all of the modifications to the normal decoder-only transformer that we have discussed are adopted by both LLaMA and LLaMA-2. However, LLaMA-2 does have a few notable architectural changes compared to its predecessor. First, LLaMA-2 is trained with a longer [context length](https://cameronrwolfe.substack.com/i/117151147/the-context-window) of 4K tokens (i.e., LLaMA was trained with a 2K context length). Additionally, LLaMA-2 adopts grouped query attention (GQA) [9] within each of its layers; see below. This modification is made to speed up the inference process of LLaMA-2 models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)

(from [9])

GQA is a modified version of the standard multi-head [causal self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20). In GQA, we divide the `N` total self-attention heads into groups, where key and value heads are shared within each group; see above. Such an approach is an interpolation between vanilla multi-headed self-attention and [multi-query attention](https://cameronrwolfe.substack.com/i/104244919/architectural-modifications), which uses a shared key and value projection across all `N` heads. Interestingly, _GQA maintains the performance of vanilla multi-headed causal self-attention and achieves comparable efficiency compared to multi-query attention_.

#### More Data = Better Model

All LLMs[2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-2-135824233) follow a (relatively) standard and simple pre-training process, based upon an unlabeled textual corpus and the [next token prediction](https://cameronrwolfe.substack.com/i/85568430/language-modeling) objective. Given that the pre-training methodology for LLMs is standardized, model performance is largely correlated with the quality and amount of data used for pre-training. For pre-trained base models, using [more and better data](https://twitter.com/cwolferesearch/status/1688662318522007552?s=20) for pre-training the LLM will generally improve the resulting model’s performance!

> _“The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology.”_ - from [1]

**The LLaMA approach.** Both LLaMA and LLaMA-2 use solely public sources of data for pre-training. Such a choice is made deliberately to ensure that the pre-training process can be openly replicated by anyone with sufficient compute resources. Compared to LLaMA, however, LLaMA-2 adopts a new mixture of pre-training data (i.e., sources that are known to be high-quality and factual are sampled more heavily) and increases the size of the pre-training dataset by 40%. Such a change allows the model to learn from more data during pre-training and, as a result, improves the knowledge base possessed by LLAMA-2.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0e47f39-c9ce-4fbe-89d3-96f9baa2cd5f_1270x1362.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0e47f39-c9ce-4fbe-89d3-96f9baa2cd5f_1270x1362.png)

(from [1])

**Pre-training on the right data.** As we will see, LLaMA-2 places an emphasis on key properties like helpfulness and safety that should be encouraged within the resulting model. Interestingly, LLaMA-2’s focus upon safety metrics and data quality begins as early as the pre-training process. Data from sources that are known to contain high levels of personal information are excluded from the pre-training set, while data coming from factual or respected sources is emphasized more heavily during pre-training. Additionally, authors measure various factors, such as representation of different demographic groups and levels of toxicity, to minimize the amount of bias within LLaMA-2; see above. Put simply, great care is placed into the contents and composition of LLAMA-2’s pre-training dataset.

## Fine-Tuning Process and LLaMA-2-Chat

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)

(from [1])

Prior to LLaMA-2, most open-source models were aligned exclusively by performing supervised fine-tuning (SFT) over publicly-available datasets. However, proprietary models (e.g., ChatGPT or GPT-4) tend to follow a more thorough approach that fine-tunes the LLM over extensive amounts of dialogue and human feedback. Attempting to close this gap, LLaMA-2 is fine-tuned using a large dataset in a similar manner to proprietary models, producing the LLaMA-2-Chat model that is optimized for dialogue-based applications; see above. For most closed-source models, the alignment process is highly proprietary, but LLaMA-2 attempts to improve transparency and make high-quality alignment more understandable for open-source research efforts.

> _“Closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.”_ - from [1]

**What is alignment?** Alignment refers to the idea of teaching an LLM to produce an output that _aligns_ with human desires. While the pre-training process focuses upon accurately performing next token prediction, alignment fine-tunes—typically using [supervised fine-tuning (SFT) and/or reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)—the model to accomplish a variety of different goals; see below. Notably, LLMs gain [most of their knowledge base](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) during pre-training, while alignment serves to teach them the correct output style or format.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9295576d-46db-45db-b156-ccaae2ebaa99_2428x588.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9295576d-46db-45db-b156-ccaae2ebaa99_2428x588.png)

(from [10, 11])

For example, alignment can be performed with a goal of reducing hallucinations, avoiding unsafe questions[3](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-3-135824233), following detailed instructions, and more. Multiple goals can be aligned for at once. In the case of LLaMA-2, authors focus upon improving the following criteria during alignment:

- _Helpfulness_: the model fulfills users’ requests and provides requested information.
    
- _Safety_: the model avoids responses that are “unsafe”
    

Performing alignment in pursuit of these goals produces the LLaMA-2-Chat model that is specialized for dialogue-based use cases. Given that LLaMA-2-Chat is fine-tuned/aligned using both SFT and RLHF, we will now consider and explore each of these phases of fine-tuning individually.

#### Supervised Fine-Tuning (SFT)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1368769-40db-440b-b13f-5299b62dd53a_632x1070.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1368769-40db-440b-b13f-5299b62dd53a_632x1070.png)

(from [10])

The first step in the fine-tuning process for LLaMA-2 is SFT. This technique is simple to understand, as it relies upon a (supervised) next-token prediction objective that is nearly identical to pre-training; see above. Put simply, we just collect a dataset of dialogue examples (i.e., prompt and response pairs; see below)[4](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-4-135824233) and train the model using a [next-token prediction objective](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective) applied to the responses within each of these examples. By fine-tuning the model in this way, the LLM learns to replicate behavior emulated within the responses of the fine-tuning dataset. Thus, any desirable properties—or alignment objectives—present within this data, in turn, become present within the model itself.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c6d0e01-1cb0-4acf-937a-52b05d96591a_1272x672.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c6d0e01-1cb0-4acf-937a-52b05d96591a_1272x672.png)

(from [1])

Notably, LLaMA-2-Chat performs two separate phases of SFT. The first phrase trains over a larger amount of public data, while the second phase trains over a smaller curated dataset with much higher quality. In total, roughly 100K dialogue examples are observed by LLaMA-2-Chat while performing SFT.

**The data.** Many datasets for dialogue-based fine-tuning are available via sites like HuggingFace. LLaMA-2 begins its alignment process by fine-tuning over a [publicly available dataset for SFT](https://huggingface.co/datasets/lucasmccabe-lmi/FLAN_CoT_alpaca_style). However, such public data lacks in quality and diversity, which—as revealed by LIMA [12]—is problematic. Authors in [1] collect extra data via a combination of 3rd party vendors and filtering efforts on public data. The resulting dataset, although of moderate size (i.e., 27,540 examples in total), is manually scrutinized and validated to ensure quality criteria are met.

> _“We found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF.”_ - from [1]

The limited set of clean instructions used for the second phase of SFT in [1] are quite effective. Interestingly, authors mention that collecting more data for SFT had diminishing returns because _i)_ the fine-tuned model is capable of generating its own data for SFT and _ii)_ investing more annotation into RLHF yields a clear benefit. Overall, we see in [1] that an effective strategy for SFT is to curate a small dataset with higher quality standards, which aligns with findings from [12]. 

#### Reinforcement Learning from Human Feedback (RLHF)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbc95a9-a0cc-4ef6-908a-7154a2e6517d_1356x1218.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbc95a9-a0cc-4ef6-908a-7154a2e6517d_1356x1218.png)

(from [10])

Although SFT is quite effective, it can be difficult to curate and manage a dataset that accurately captures the goals of alignment. _How do we know that the data present within our SFT dataset is consistently helpful and safe?_ On the other hand, RLHF (shown above) takes the more direct approach of fine-tuning the LLM directly from human feedback on the model’s output. First, human annotators are asked to write prompts for the LLMs, which can be selected based on the alignment criteria (e.g., helpfulness and safety). From here, the LLM is used to generate multiple (i.e., at least two) responses for each prompt, and the human annotators rank these responses based on their quality. Here, “quality” is typically judged according to how well the response matches the alignment criteria. Then, RLHF can be used to optimize the LLM based on these human preference scores.

**The data.** LLAMA-2 follows a binary protocol for collecting human preference data. For each prompt written by an annotator, two model responses are generated, and the annotator chooses the desired response based on helpfulness and safety criteria. Notably, each instance of human preference collection focuses upon a single alignment criteria[5](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-5-135824233). For example, an annotator could write an adversarial prompt that tries to get the LLM to generate something unsafe, then rate the responses specifically based upon whether safety criteria are satisfied. Similarly, one could prompt the LLM with a detailed instruction and evaluate whether either of the model’s responses are more helpful.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0cbcce0-923b-4a73-bd93-8ef0a7f00434_1266x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0cbcce0-923b-4a73-bd93-8ef0a7f00434_1266x646.png)

(from [1])

Going a bit further, annotators are also asked to label the degree to which one response is better than the other (i.e., _significantly better_, _better_, _slightly better_, or _negligibly better_), as well as explicitly label whether either of the LLM’s responses are considered unsafe. We see in [1] that new batches of human preference data are collected each week, allowing multiple phases of RLHF to be performed iteratively. Statistics of the full human preference dataset that is collected compared to public human preference datasets are provided in the table above.

> _“Human feedback is … used to train a reward model, which learns patterns in the preferences of the human annotators and can then automate preference decisions.”_ - from [1]

Data for RLHF is collected in several batches. A new batch of data is collected each week, and authors of [1] iteratively fine-tune the LLaMA-2-Chat model (using RLHF) as each of these batches become available. As such, multiple successive “rounds” of RLHF are performed throughout the alignment process.

**Safety-based data collection.** To collect fine-tuning data for improved safety during RLHF, authors in [1] design instructions using two techniques:

1. _Risk Categories_: a topic about which the LLM could potentially product unsafe content.
    
2. _Attack Vectors_: question styles that cover a broad variety of prompts that could elicit negative behavior.
    

Three different risk categories are considered, including illicit and criminal activities, hateful and harmful activities, and unqualified advice. Additionally, several different attack vectors are defined, such as psychological manipulation (authority manipulation), logic manipulation (false premises), syntactic manipulation (misspelling), semantic manipulation (metaphor), and perspective manipulation (role playing). Using combinations of these different techniques, human annotators craft prompts that are more likely to elicit unsafe behavior, allowing such behavior to be discouraged during the fine-tuning process.

**Training the reward model.** After collecting human feedback, a reward model is trained on this data. The reward model takes a prompt—_with the full chat history_—and response as input and predicts a preference score. The reward model usually shares the same architecture and weights as the LLM, but its classification head (for next token prediction) is replaced with a regression head (for preference estimation) and the model is fine-tuned on preference data; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0961ffa-b040-419a-ab10-1a16a2a84f87_2212x1244.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0961ffa-b040-419a-ab10-1a16a2a84f87_2212x1244.png)

Normal LLM structure for next-token prediction compared to the structure of an LLM-based reward model

The reward model automates obtaining an accurate preference score for prompt-response pairs, which allows the LLM to be fine-tuned using [reinforcement learning (RL)](https://www.geeksforgeeks.org/what-is-reinforcement-learning/) to maximize (automatically generated) human preference scores across a large dataset. To train the reward model, we take binary preference data and form a training objective that forces the preferred example to have a higher score than its counterpart[6](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-6-135824233)! An example of such an objective is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f29a20d-c865-4a01-9fec-61590e4e7d79_1394x498.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f29a20d-c865-4a01-9fec-61590e4e7d79_1394x498.png)

(from [1])

However, we should recall that the preference data collected in [1] is not just binary. We have more granular information that identifies responses that are significantly or moderately better than others. To capture this more detailed information, we can simply add a margin—_or a fixed value assigned to each of the significantly better, better, slightly better, or negligibly better categories_—to the loss shown above. Such a margin pushes the reward model to assign larger gaps in score between responses with big differences in preference; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7ac7d1c-cfd3-4e65-9db3-1163349c2717_1376x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7ac7d1c-cfd3-4e65-9db3-1163349c2717_1376x282.png)

(from [1])

Interestingly, we see in [1] that authors combine their custom-curated human preference data with other public sources of data when training reward models. To explain this decision, authors claim that injecting such extra sources of data does not seem to have a negative impact on reward model performance. Plus, this data aids the reward models’ generalization and can protect against issues like reward hacking, in which RLHF takes advantage of weaknesses in the reward function to arbitrarily inflate preference scores without improving the model.

> _“We do not observe negative transfer from the open-source preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of some weaknesses of our reward, and so artificially inflating the score despite performing less well.”_ - from [1]

**Alignment is oftentimes a tradeoff!** Interestingly, we see in [1] that separate reward models are trained for each of the alignment criteria. More specifically, we have a helpfulness reward model and a separately safety reward model. This might seem unnecessary at first, but we should realize that properties such as helpfulness and safety—_or any alignment criteria in general_—are oftentimes a tradeoff. Making an LLM more safe might make it less helpful; e.g., the model could refuse to answer a question that is unsafe, which is (arguably) not helpful. Given this tradeoff, authors in [2] observe that predicting helpfulness and safety preference scores with separate reward models is the best way to accurately model the nuances in human preference according to different alignment criteria.

> _“In order for a single model to perform well on both dimensions, it needs to not only learn to select the better response given a prompt but also to distinguish adversarial prompts from safe ones. As a result, optimizing two separate models eases the reward modeling task.”_ - from [1]

**Optimization via RL.** Even after multiple rounds of SFT are performed, five successive versions of RLHF models are created in [1]—_one for each batch of human preference data_. Although the [PPO algorithm](https://openai.com/research/openai-baselines-ppo) has typically been a standard for performing RLHF, we see in [1] that two separate algorithms are considered:

- _PPO_: standard algorithm used for RLHF.
    
- _Rejection Sampling_: samples `K` responses from the LLM for each prompt, scores each response using the reward model, selects the best response, and fine-tunes on this example.
    

While PPO only takes one sample from the model per iteration, rejection sampling takes numerous samples. Within rejection sampling, the highest-reward sample that is generated is considered the new “gold standard”. As shown in the figure below, generating multiple samples in this manner can drastically increase the maximum reward of samples observed during fine-tuning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b3e108b-443b-481c-8b76-3c0a12d4a165_1594x604.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b3e108b-443b-481c-8b76-3c0a12d4a165_1594x604.png)

(from [1])

In [1], rejection sampling is performed with the largest model (i.e., LLaMA-70B-Chat) and used to train all other (smaller) models[7](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-7-135824233). While PPO performs iterative updates after each sample, rejection sampling fine-tuning uses the same model (i.e., at the beginning of the RLHF round) to generate an entire dataset of high-reward samples that are used for fine-tuning in a similar manner to SFT.

> _“We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output.”_ - from [1]

To ensure the best possible performance, authors include best samples from all RLHF iterations—_not just the current iteration_—when performing rejection sampling fine-tuning. Without including samples from prior iterations, we see that regressions in performance may occur when using rejection sampling fine-tuning. In [1], a rejection sampling approach is adopted for the first four rounds of RLHF, then the final round combines both rejection sampling fine-tuning and PPO sequentially (i.e., PPO is applied after rejection sampling fine-tuning).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b0ef016-bb81-4a02-9a41-4f685609aea1_1584x568.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b0ef016-bb81-4a02-9a41-4f685609aea1_1584x568.png)

(from [1])

**Tweaking the temperature.** Interestingly, we also observe in [1] that the optimal [temperature](https://twitter.com/cwolferesearch/status/1671628210180698112?s=20) to use when generating samples for RLHF changes with iterative updates to the model; see above. As such, the temperature must be readjusted after each round of RLHF to achieve the best results. Furthermore, authors in [1] mention that the optimal temperature seems to be dependent upon context. For example, different shifts in the optimal temperature are observed for certain types of prompts, such as creative prompts or those that are considered more factual.

#### Improving Multi-Turn Dialogue with Ghost Attention

In many cases, dialogue agents, such as LLaMA-2-Chat, conduct conversations with users over multiple turns. Sometimes, a system message or instruction is provided to the model at the beginning of this back-and-forth conversation that should be obeyed throughout. However, we see in [1] that the LLM may quickly forget about this instruction, leading it to only apply to initial turns; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85cc8dd2-4fbc-4aba-8767-ecf72474ff5e_1928x1106.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85cc8dd2-4fbc-4aba-8767-ecf72474ff5e_1928x1106.png)

(from [1])

To solve this issue, authors in [1] explore a fine-tuning approach with synthetic data, called Ghost Attention (GAtt). Given a dialogue session, GAtt _i)_ samples an instruction that should be followed in a conversation, _ii)_ concatenates this instruction to every user message within the conversation, and _iii)_ samples responses to each message using the model from the latest round of RLHF. After removing the concatenated instruction from all but the first user message, such an approach can generate synthetic multi-turn dialogue data that consistently abides by an instruction. Then, we can just fine-tune over this data using SFT, which—_as shown above_—improves instruction following over long dialogues!

## Evaluations and Key Findings

LLaMA-2 and LLaMA-2-Chat are evaluated extensively in comparison to both other open-source LLMs and proprietary models. The base and fine-tuned models set a new state-of-the-art among open-source LLMs. As we will see, these models also have a variety of different emergent capabilities[8](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-8-135824233) and interesting properties that are analyzed extensively within [1] to provide useful insights.

#### LLaMA-2 (Base Model) Performance

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a42b8e-34ba-48f4-b9c5-1f36ba909f64_1260x490.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a42b8e-34ba-48f4-b9c5-1f36ba909f64_1260x490.png)

(from [1])

Compared to popular open-source base LLMs (e.g., [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source), and [LLaMA-1](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)), LLaMA-2 models set a new state-of-the-art on all tasks considered; see above. Notably, however, LLaMA-2 was somewhat [criticized](https://twitter.com/amasad/status/1681383736032493580?s=20) for its (relatively) poor performance on coding-based tasks (e.g., [HumanEval](https://arxiv.org/abs/2107.03374)). Some have speculated that this may be due to a lack of sufficient code within LLaMA-2’s pre-training corpus, though the exact explanation is not completely clear as of now.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7984d418-3031-480a-8438-a7e992c82023_1262x356.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7984d418-3031-480a-8438-a7e992c82023_1262x356.png)

(from [1])

When compared to proprietary models, LLaMA-2 base models perform worse; see above. However, we should keep in mind that this comparison is made between a base LLM and aligned models like GPT-3.5 and GPT-4. When compared to proprietary base LLMs (e.g., [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [4]), LLaMA-2 performs favorably!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13124aad-2af1-43ed-8ac1-291c1cd49072_1590x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13124aad-2af1-43ed-8ac1-291c1cd49072_1590x732.png)

(from [1])

**Safety results.** As we have mentioned previously, the authors of LLaMA-2 place an emphasis upon safety when crafting the model’s pre-training dataset, leading them to measure and even exclude data according to a variety of different factors. Despite these efforts to minimize bias in the pre-training dataset, results on automatic safety benchmarks are mixed as shown above. While LLaMA-2-7B is less toxic and more truthful and informative compared to prior models, larger models (e.g., LLaMA-2-34B and LLaMA-2-70B) seem to have an increase in toxicity, which the authors claim may come from their use of a larger pre-training and lack of aggressive filtering of toxic information from this dataset.

#### Fine-Tuning Analysis for LLaMA-2-Chat

> _“Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can be complicated by various HCI considerations, and is not always scalable.”_ - from [1]

LLaMA-2-Chat is evaluated according to its levels of helpfulness and safety using a variety of different techniques. First, evaluation of model quality is automated by using reward model output as a proxy for performance. The results of these evaluations—and alternative evaluations that use GPT-4 as a judge—are shown below, where we see that LLaMA-2-Chat slowly becomes more helpful and safe throughout each round of the fine-tuning process with both SFT and RLHF.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97eb6108-8b95-47a1-965e-85e8390d70b4_1892x672.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97eb6108-8b95-47a1-965e-85e8390d70b4_1892x672.png)

(from [1])

Although such automated metrics are informative, they can be biased or incorrect. As such, LLaMA-2-Chat is subjected to extensive human evaluations. In the figure below, we see that humans find LLaMA-2-Chat to be more helpful than a variety of powerful open-source and proprietary language models. Similarly, LLaMA-2-Chat exceeds the same models in human safety evaluations. As such, the LLaMA-2-Chat model is generally found to be more helpful and safe than a variety of other powerful LLMs based on extensive human evaluation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F065228d3-36b5-4837-9901-84fd88d02ec4_2040x792.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F065228d3-36b5-4837-9901-84fd88d02ec4_2040x792.png)

(from [1])

Despite these results, we should recall that human evaluations can be limited and are only conducted over a (relatively) small set of prompts[9](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-9-135824233). In general, human evaluation can be subjective and noise is present in the evaluation process, which we should keep in mind when studying and interpreting these results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bc94cc6-4876-4a05-beb9-12ec1727926c_1274x806.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bc94cc6-4876-4a05-beb9-12ec1727926c_1274x806.png)

(from [1])

**Reward model scaling.** To assess the quality of helpfulness and safety reward models created in [1], authors compare and evaluate these models against a variety of other reward models; see above. As expected, the reward models used to fine-tune LLaMA-2-Chat seem to perform the best. Additionally, we see that larger reward models perform better when trained over the same amount of data and that the performance of reward models has yet to reach a plateau; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92dac79e-851f-4bd1-ab42-07f0c48b237f_1266x528.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92dac79e-851f-4bd1-ab42-07f0c48b237f_1266x528.png)

(from [1])

_Why does this matter?_ As authors note in [1], the performance of the reward model, given that it is used to automate preference feedback during RLHF, is directly related to the performance of the fine-tuned chat model. In other words, an improvement in the quality of the reward model tends to produce a relative improvement in the quality of the fine-tuned LLM. The results above seem to indicate that using larger reward models trained over more data is helpful.

**Other findings.** In addition to the helpfulness and safety analysis performed in [1], a variety of interesting observations are made about the behavior of LLaMA-2-Chat. Most LLMs struggle with temporal reasoning (i.e., reasoning about when in time certain events occurred), but LLaMA-2 is fine-tuned to better process temporal information by performing SFT over time-focused prompt-response examples. As such, the model is surprisingly capable of organizing knowledge in a temporal manner; see above. Going further, LLaMA-2-Chat is not specifically trained to [leverage tools](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools), but the model seems to be able to generalize to tool usage in a [zero-shot](https://cameronrwolfe.substack.com/i/117151147/zero-shot-learning) manner as shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1625468-04e3-4875-a760-f25c6da9187a_1578x614.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1625468-04e3-4875-a760-f25c6da9187a_1578x614.png)

(from [1])

## Closing Thoughts

The LLaMA-2 (and LLaMA-2-Chat) suite of LLMs mark the beginning of a new era for open-source LLMs, as their performance is now closer than ever to that of top proprietary models like ChatGPT and GPT-4. When we examine the key insights from [1] that make these models so impressive, we will notice that simple changes can have a big impact on model performance. However, a variety of in-depth technical insights are provided by LLaMA-2 as well. Some of the major takeaways from LLaMA-2 are itemized and elaborated below.

**Garbage in = garbage out.** As we have [discussed previously](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language), the amount and quality of data used to train an LLM—both during pre-training and fine-tuning—is incredibly important. We see in [1] that part of what makes LLaMA-2 so great is an emphasis upon data. During pre-training, the model observes 40% more data than its predecessor and adopts a modified mixture that emphasizes factual sources. Additionally, LLaMA-2 adopts an approach similar to [LIMA](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) during fine-tuning by curating a smaller, high-quality corpus of dialogues for SFT.

**Alignment is important.** One of the largest differences between LLaMA-2 and prior work on open-source LLMs is the emphasis upon high-quality alignment. Rather than simply performing SFT with public data, authors in [1] truly try to replicate the complex alignment process that is used for proprietary models by performing multiple rounds of both SFT and RLHF. Although the resulting LLaMA-2-Chat model isn’t the current leader on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)[10](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-10-135824233), such an emphasis upon extensive alignment provides invaluable technical insight that drastically improves the transparency of research in this area.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e903beb-ebf5-44b2-9731-55afc4c02584_1584x432.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e903beb-ebf5-44b2-9731-55afc4c02584_1584x432.png)

(from [1])

**RLHF is powerful.** Interestingly, authors in [1] mentioned an aversion to using reinforcement learning-based alignment approaches at the outset of this project. However, we see in [1] that reinforcement learning is incredibly effective! RLHF seems to be a natural approach for alignment, as it fosters a synergy between human and LLM via iterative feedback and fine-tuning. Although SFT is useful, human writing quality has high variance, and the model must learn (at least in part) from the tail end of this variance (i.e., bad examples) when using SFT. In contrast, the annotation process for RLHF only requires humans to select the better response, which is less noisy and allows poor responses to be eliminated over time; see above. Additionally, we can observe during RLHF responses that even go beyond human writing comprehension. Despite not being able to write such responses, humans can easily rate them as preferable.

> _“Many among us expressed a preference for supervised annotation, attracted by its denser signal… However, reinforcement learning proved highly effective, particularly given its cost and time effectiveness. Our findings underscore that the crucial determinant of RLHF’s success lies in the synergy it fosters between humans and LLMs throughout the annotation process.”_ - from [1]

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

[Share](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up?utm_source=substack&utm_medium=email&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxMTAxMDcwNzksInBvc3RfaWQiOjEzNTgyNDIzMywiaWF0IjoxNzQ1NzQ1NjkzLCJleHAiOjE3NDgzMzc2OTMsImlzcyI6InB1Yi0xMDkyNjU5Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.cpNN30DRDlENdpCddyx8oHWVDLUtfQPpqmNwbihYtZs)

#### Bibliography

[1] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." _arXiv preprint arXiv:2307.09288_ (2023). 

[2] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[3] “Introducing Llama2: The next generation of our open source large language model”, _Meta_, https://ai.meta.com/llama/.

[4] Zhang, Biao, and Rico Sennrich. "Root mean square layer normalization." _Advances in Neural Information Processing Systems_ 32 (2019).

[5] Shazeer, Noam. "Glu variants improve transformer." _arXiv preprint arXiv:2002.05202_ (2020).

[6] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[7] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[8] “Introducing Falcon LLM”, _Technology Innovation Institute_, 7 June 2023, https://falconllm.tii.ae/.

[9] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." _arXiv preprint arXiv:2305.13245_ (2023).

[10] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[11] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[12] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[1](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-1-135824233)

“Generalization” is a term in deep/machine learning that refers to a model’s ability to _generalize_ beyond training data and also perform well on data it has not seen during training, either in the wild or as part of a hold-out test set.

[2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-2-135824233)

Many people have disagreements over the definition of an LLM. Here, I am referring to causal language models (i.e., trained with a language modeling objective) that are based upon a decoder-only transformer architecture.

[3](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-3-135824233)

Notably, many researchers/practitioners have heavily criticized popular LLMs—especially recently—for their refusal to answer questions that are “unsafe”. In some cases, alignment efforts can even be perceived by some as reducing the quality of the underlying LLM for this exact reason.

[4](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-4-135824233)

LLaMA-2 uses dialogue examples for fine-tuning because it is explicitly optimized for dialogue or chat-based conversations during alignment.

[5](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-5-135824233)

Maximizing the quality of human annotation is largely related to minimizing cognitive load. So, focusing upon a single alignment criteria at a time likely leads to higher quality, detailed annotations that more accurately reflect alignment criteria.

[6](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-6-135824233)

This style of loss function is called a “ranking” loss. We don’t directly optimize the model to output a certain score for any input. Rather, we just have pairs of inputs where we know that one input should have a higher score within this pair. So, we train the model to push these scores apart, where the preferred input has a higher score.

[7](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-7-135824233)

Interestingly, this is a form of [knowledge distillation](https://cameronrwolfe.substack.com/i/114077195/knowledge-distillation), as the best samples from the larger model are used to fine-tune all smaller LLaMa-2-Chat models via a fine-tuning approach that is similar to SFT.

[8](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-8-135824233)

An emergent capability simply refers to a capability or skill possessed by a model that is not explicitly encouraged during training. For example, the ability of pre-trained LLMs to perform in-context learning can be considered “emergent” given that we never directly train the model to develop this skill via next-token prediction.

[9](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-9-135824233)

Most evaluations are performed over a set of ~4,000 prompts, which is quite large by academic research standards.

[10](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up#footnote-anchor-10-135824233)

We should keep in mind here that whether one LLM is better than another is subjective and highly dependent upon how we define “better”. LLaMA-2-Chat is specifically aligned based on helpfulness and safety, and the model seems to excel in these categories as shown by extensive evaluation in [1].

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Eswar Divi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd8cbd5e-7e5a-41b0-99bd-96903aabd88a_144x144.png)



](https://substack.com/profile/106254331-eswar-divi)

[

![Kiran Adimatyam's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F914ce083-db2b-4f5b-891f-38c5abdf007e_1024x1024.png)



](https://substack.com/profile/119560333-kiran-adimatyam)

[

![Lazaro  Hurtado's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd090f3a8-f320-4a21-aa62-dd22ba4b707b_144x144.png)



](https://substack.com/profile/100785516-lazaro-hurtado)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

43 Likes∙

[6 Restacks](https://substack.com/note/p-135824233/restacks?utm_source=substack&utm_content=facepile-restacks)

43

- 

[

6

](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comments)

6

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Oleksandr Rechynskyi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F484cd279-07e6-417d-82a9-65fa67c93d7e_144x144.png)



](https://substack.com/profile/134301190-oleksandr-rechynskyi?utm_source=comment)

[Oleksandr Rechynskyi](https://substack.com/profile/134301190-oleksandr-rechynskyi?utm_source=substack-feed-item)

[Oleksandr’s Substack](https://oleksandrrechynskyi.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[6月10日](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comment/58676835 "2024年6月10日 21:50")

Liked by Cameron R. Wolfe, Ph.D.

wow, that's a really deep and comprehensive explanation. Thanks a lot!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comment/58676835)

[

![Rahul's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fe4a66c-ed97-4468-a7e0-69396ad9233b_144x144.png)



](https://substack.com/profile/142072296-rahul?utm_source=comment)

[Rahul](https://substack.com/profile/142072296-rahul?utm_source=substack-feed-item)

[2024年3月2日](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comment/50759769 "2024年3月2日 20:03")

Liked by Cameron R. Wolfe, Ph.D.

What a great article. Saved me tens of hours of going through multiple sources. Thank you Dr. Cameron!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comment/50759769)

[4 more comments...](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



-----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Tree of Thoughts Prompting

### Solving multi-step problems with LLMs via deliberate planning and exploration...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Aug 21, 2023

31

- 

[](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting/comments)

3

Share

[

![Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png "Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png)

This newsletter is presented by [Deci AI](https://deci.ai/). Deci does a ton of interesting AI research. Most recently, they released DeciCoder-1B, an open-source code generation model. Read about it [here](https://twitter.com/cwolferesearch/status/1691929174175264858?s=20) or [download it](https://huggingface.co/Deci/DeciCoder-1b) on HuggingFace.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

[Sponsor the newsletter](https://forms.gle/vF8JHjd2gAMwLtpk8) | [Follow me on Twitter](https://twitter.com/cwolferesearch) | [Get in touch](http://cameronrwolfe.me/) | [Suggest a topic](https://forms.gle/BkJykjDbqX6ZTXFF7)

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fcd3472-8428-4774-8d58-e6315b749fc9_2106x1308.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fcd3472-8428-4774-8d58-e6315b749fc9_2106x1308.png)

(from [1, 2])

As large language models (LLMs) first started to gain in popularity, they were criticized for their shortcomings in solving complex, reasoning-based problems. Although scaling up these models (i.e., more parameters and more data) provided a near-uniform performance improvement across tasks, we saw [virtually no boost](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher#%C2%A7scaling-language-models-methods-analysis-and-insights-from-training-gopher) in performance on reasoning-based tasks with modern LLMs. This changed with the proposal of advanced prompting techniques, such as [chain of thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [2] and [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [3]. Such methods showed us that LLMs are more than capable of reasoning and solving complex, multi-step problems. They just have to be properly prompted to fully leverage these abilities.

> _“It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion.”_ - from [1]

Even if proper prompting can enable LLMs to solve complex problems, these techniques are lacking. Namely, we typically _i)_ provide a prompt to the LLM and _ii)_ expect the model to use [next token prediction](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective) to generate a full solution. Certain approaches may generate solutions in a step-by-step fashion (e.g., [least-to-most prompting](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [8]), but the LLM still follows a single reasoning path instead of exploring several potential solutions. For complex problems in which initial decisions can completely derail a solution, such an approach will fall short, which is an issue given that LLMs are now commonly used as general purpose problem solvers in a variety of practical applications. Put simply, _we need a prompting approach that performs more deliberate planning and exploration when solving problems_.

In [1], authors propose such an approach—called Tree of Thoughts prompting—that solves problems by explicitly decomposing them into a series of _thoughts_, or intermediate steps. Similar to chain of thoughts prompting, tree of thoughts prompting generates a solution that is simply a sequence of individual thoughts. However, this approach goes further by allowing multiple reasoning path to be considered at once—_forming a tree of potential thoughts or reasoning paths_—and exploring this entire solution space via LLM-powered self-evaluation. With tree of thoughts prompting, the LLM can deliberately plan its solution, test various intermediate reasoning paths, and even perform backtracking, allowing the model to explore the solution space and eventually generate the correct output.

#### Connections to Research in Other Fields and Generations

> _“A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered.”_ - from [12]

**Analogy to humans.** To explain their technique, authors in [1] draw upon analysis of the human decision making process. In particular, humans seems to have two separate modes of making decisions:

- A fast, automatic, and unconscious mode
    
- A slow, deliberate, conscious mode
    

Authors in [1] argue that techniques like chain of thought prompting seem to mimic the first mode outlined above, as the LLM just generates text in a left-to-right manner without deliberate planning or deconstruction of the problem. The main motivation behind tree of thoughts prompting is to inject deliberate planning and exploration into the problem-solving process by deconstructing each problem into a tree of smaller steps that are individually explored.

**Inspired by early work in AI.** The planning process followed by tree of thoughts prompting is inspired by AI research from the mid 20th century [12, 13]! This work argues that problem solving can be formulated as searching through a combinatorial space represented as a tree. Within this space, multiple active chains of thought are maintained, each representing an individual path within the larger tree. As we will see, this formulation allows us to explicitly decompose a solution to a complex problem, as well as leverage established graph algorithms (e.g., breadth-first and depth-first search) to find a viable solution.

## The Basics of Prompting

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd839a4b-c2f2-4d76-9891-0129b97104c7_2144x684.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd839a4b-c2f2-4d76-9891-0129b97104c7_2144x684.png)

(from [10])

The generic text-to-text format of LLMs is incredibly powerful. To solve any problem, we can simply _i)_ write a textual prompt that describes the problem and _ii)_ generate a relevant output/solution with the language model. For this reason, LLMs are considered to be [foundation models](https://crfm.stanford.edu/), or models that can singlehandedly be adapted to solve a wide variety of tasks. Such an ability is largely due to in-context learning. Namely, pre-trained LLMs have the ability to use data injected into their prompt as context to produce a more accurate output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6c7afe7-5d65-49cb-9fbd-5391d5be91fd_2236x1292.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6c7afe7-5d65-49cb-9fbd-5391d5be91fd_2236x1292.png)

(from [5])

However, the effectiveness of in-context learning is highly related to the prompt that is used to solve a problem. Many different prompting approaches—including tree of thoughts prompting [1]—exist, but the process of choosing the correct prompting technique or writing the correct prompt can be quite difficult. For this reason, we will now take a brief look at the basics of prompt engineering, providing useful context that will make the various prompting techniques explored within this overview more understandable.

#### What is prompt engineering?

> _“Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use LMs for a wide variety of applications and research topics.”_ - from [2]

Prompt engineering refers to the process of iteratively tweaking a prompt for a language model with the goal of discovering a prompt that accurately solves a desired task. Typically, the prompt engineering process is empirical, meaning that we discover the best prompts by measuring their performance on related tasks. Prompting is a new field that is full of heuristics and a variety of different techniques. As such, we can maximizes our chances of success by following an approach similar to any other engineering problem:

- Track and version different prompts
    
- Setup extensive benchmarks to measure prompt performance
    
- Test different ideas to see what yields the best results
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3f8a12-a893-4508-95d7-312d37a77ea2_1792x1112.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3f8a12-a893-4508-95d7-312d37a77ea2_1792x1112.png)

**Context window.** One major consideration when writing a prompt is the size of the underlying LLM’s context window. As shown in the figure above, all LLMs are trained using inputs of a certain size (i.e., referred to as the size of the context window or context length), which—_along with memory constraints_—limits the total amount of data that can be provided to an LLM within a prompt. Practically, this means that we must be selective about the data that is included in a prompt[1](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-1-136223454). Next, we will overview the components of a prompt, as well as the kinds of information that might be provided to guide an LLM towards a correct solution.

#### Structure of a Prompt

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7c6306b-7ab5-419e-9b23-254613e41727_1290x1346.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7c6306b-7ab5-419e-9b23-254613e41727_1290x1346.png)

Example prompt that uses all structural components (indicators are bolded for visibility)

A variety of prompting techniques exist, but each of these techniques utilize a (relatively) common structure. The various components of prompts that one might encounter are depicted within the figure above and outlined below.

- _Input Data_: the input data being processed by the LLM.
    
- _Exemplars_: input-output examples that demonstrate correctly solving a desired problem.
    
- _Instruction_: a detailed, textual description of the LLM’s expected behavior.
    
- _Indicators_: textual tags that are used to organize and structure the different components of a prompt.
    
- _Context_: any extra context that may be useful to provide to the LLM (e.g., chunks of information [retrieved from a vector database](https://cameronrwolfe.substack.com/i/118401596/knowledge-augmentation)).
    

Notably, not all of these components are necessary when writing a prompt. Several of the techniques explored in this overview will only use a subset of the above components, but each of them can be leveraged to provide extra, useful information to the LLM when necessary.

## Hierarchy of Prompting Techniques

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb7772f-305e-4d12-9ca3-68ec34dcf714_2096x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb7772f-305e-4d12-9ca3-68ec34dcf714_2096x636.png)

Writing prompts is an iterative process that should start simple and only add complexity when needed

Now that we have a basic understanding of in-context learning and prompt engineering, we need to take a deeper dive into some common techniques for prompting a language model. We will start with simpler techniques, such as zero and few-shot prompting, then move on to more complex techniques like [chain of thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [2] and [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [3]. As always, we should remember that simplicity is best when writing prompts—_we should start as simple as possible, then use test-driven development to decide when extra complexity is necessary_. In other words, we can create a large benchmark of prompt examples based upon our desired application, then measure performance on this benchmark as we iterate and test different prompting variants. For a more extensive (and practical) guide on prompting, check out the article below.

[Practical Prompt Engineering](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)

#### Zero and Few-Shot Prompting

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60b80352-f1c8-4013-89b3-141b04646525_806x346.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60b80352-f1c8-4013-89b3-141b04646525_806x346.png)

(from [5])

**Zero-shot prompting** is one of the simplest techniques that we can use for prompting a language model and originally became popular when leveraged by [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) to achieve impressive performance across a wide variety of different natural language benchmarks. To form a zero-shot prompt, we need to provide two pieces of information (see above):

1. A task description
    
2. Our input data
    

Here, the language model is expected to leverage its knowledge base and the provided task description to solve a problem without any explicit exemplars or detailed instructions. Although many language models can perform relatively well when prompted in a zero-shot manner, we typically need to provide extra details to the model to get more reliable and accurate results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f14aec-ebcb-4a17-acc0-18603242314b_826x520.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f14aec-ebcb-4a17-acc0-18603242314b_826x520.png)

(from [5])

**Few-shot prompting** goes beyond zero-shot prompting by adding “exemplars” of the model’s desired output to the prompt; see above. In addition to a task description, we provide several examples of correct input-output pairs. By adding this context to the prompt, we can provide the language model with more concrete details about the output that is expected. This technique was popularized with GPT-3 [5], where language models were first shown to be highly capable in-context learners. Put simply, the model can learn from these exemplars and improve in accuracy as more are provided.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5474ad5-00ff-4ed5-ae12-e8a472d0e3b5_1910x1136.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5474ad5-00ff-4ed5-ae12-e8a472d0e3b5_1910x1136.png)

#### Instruction Prompting

Zero and few-shot learning are simple techniques that work surprisingly well, but sometimes they will not yield sufficient levels of performance. Plus, few-shot learning has the added limitation of increasing the size of the prompt[2](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-2-136223454). If these techniques are not working for our use case, the next technique we can try is [instruction prompting](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting). Instead of demonstrating correct behavior via a task description and several examples of correct output, instruction prompting includes a detailed instruction—_or explanation of the correct behavior_—within the prompt that is given to the language model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F162ec2a6-ba57-486d-8f67-b95a1126d8a6_1628x1214.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F162ec2a6-ba57-486d-8f67-b95a1126d8a6_1628x1214.png)

Examples of instruction prompts with indicators

Plus, instruction prompting and few-shot prompting are not mutually exclusive! We can easily combine an instruction prompt with several few-shot exemplars to yield improved performance. In fact, the task descriptions used by zero and few-shot prompting techniques are actually quite similar to an instruction anyways.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc24a789f-8390-4c17-b8a6-55ee5c9ff4b5_1854x1396.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc24a789f-8390-4c17-b8a6-55ee5c9ff4b5_1854x1396.png)

(from [6])

**Alignment is necessary.** Crafting thoughtful instructions is a highly effective and token-efficient prompting technique. However, not all language models are good instruction followers. For example, pre-trained (base) LLMs do not naturally have the ability to follow detailed instructions. This ability is typically developed via an alignment process that fine-tunes the LLM’s ability to follow instructions; see above. Many modern LLMs (e.g., GPT-4) are incredibly [steerable](https://twitter.com/cwolferesearch/status/1645535868021805056?s=20) (i.e., adept at following detailed instructions), making instruction prompting one of the most effective techniques for working with such models, as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cdade12-c78e-4203-bc6c-51dc10b7f4d9_1652x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cdade12-c78e-4203-bc6c-51dc10b7f4d9_1652x596.png)

GPT-4 is steerable and can easily follow complex instructions within a prompt

#### Advanced Prompting Techniques

Sometimes, few-shot and instruction prompting are not enough to accomplish a desired task. In particular, language models tend to struggle with complex reasoning problems, such as commonsense reasoning problems that require multiple steps or mathematical puzzles. However, numerous [advanced prompting techniques](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)—_including tree of thoughts prompting_—have been developed to expand the scope of difficult problems that can be solved with an LLM. In this section, we will focus on one technique—[chain of thought (CoT) prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [2] (and its several variants)—that is especially effective in practice and forms the basis of the methodology used for tree of thoughts prompting.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [2])

**What is CoT prompting?** By leveraging in-context learning abilities, CoT prompting encourages a language model to more effectively solve complex problems by outputting along with its solution a corresponding “chain of thought” (i.e., a step-by-step explanation for how the problem was solved). The model can be prompted to generate a chain of thought via a few-shot learning approach that provides several chain of thought exemplars; see above. The CoT technique is most effective when the map from input to output is highly non-trivial; e.g., math or multi-step reasoning problems. In such cases, introducing a chain of thought allows the model to follow smaller, intermediate steps towards the correct final solution. In practice, CoT prompting was found to drastically improve LLM performance on various styles of reasoning tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1cda6fc-044c-4ccd-8e95-4ba5dcb7d2f1_2268x1172.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1cda6fc-044c-4ccd-8e95-4ba5dcb7d2f1_2268x1172.png)

(from [2])

**Variants of CoT prompting.** Given the practical utility of CoT prompting (i.e., it can be used to solve complex, multi-step problems with which LLMs typically struggle!), [several variants of this technique](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) were developed shortly after its proposal, such as zero-shot CoT [7] and least-to-most prompting [8]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2de87cf5-e598-494d-9f49-0ae17df37838_2320x940.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2de87cf5-e598-494d-9f49-0ae17df37838_2320x940.png)

(from [7, 8])

The CoT prompting variant that is most relevant to tree of thought prompting is self-consistency [3]. This approach leverages an approach that is similar to that of CoT prompting. A single model is prompted—using the same (CoT) prompt—to generate output several different times. Then, the final answer is generated by taking a majority vote of the model’s outputs as shown in the figure below. Such an approach is found to yield similar benefits to CoT prompting, as well as improve performance and reliability on more difficult problems.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8ae4c1d-cdca-4f7b-a792-714e74c65885_1612x950.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8ae4c1d-cdca-4f7b-a792-714e74c65885_1612x950.png)

(from [3])

**Existing limitations.** Techniques like CoT prompting and self-consistency do a lot to expand the scope of problems that are solvable with LLMs. Without them, solving multi-step reasoning problems would be quite difficult. However, these prompting techniques do not come without limitations. For example, not all problems have a solution space that is amenable to a majority vote, and a majority vote has been [shown in prior work](https://cameronrwolfe.substack.com/i/120285767/research-on-prompt-ensembles) to be a poor heuristic for improving LLM accuracy even for problems that can be formulated in this manner.

> _“We see 9.5% average variation in accuracy and that the Jaccard index over errors is 69% higher than if prompt errors were i.i.d. Majority vote (MV) is the primary unsupervised aggregation strategy in prior work but it does not account for either property, making it unreliable.”_ - from [9]

More broadly, solving a complex task may require extensive planning, strategic lookahead, backtracking, and even exploration of numerous viable solutions in parallel. Techniques like CoT prompting follow a left-to-right, continuous generation approach that uses [next-token prediction](https://twitter.com/cwolferesearch/status/1669811217148289026?s=20) to output a solution in a single attempt. Such an approach, although highly effective in certain scenarios, fails to solve tasks that require strategic planning and exploration. But, this is where tree of thoughts prompting comes in! Similar to CoT prompting, tree of thoughts prompting breaks problems down into smaller parts (i.e., a chain of thought) but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain!

## Understanding Tree of Thoughts Prompting

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)

(from [1])

The effectiveness of CoT prompting comes from its ability to break a complex problem solution into a sequence of smaller and simpler steps. Tree of thoughts (ToT) prompting similarly breaks a problem into a sequence of smaller steps—or _thoughts_—that are solved individually. However, the approach does not constrain the model to output these steps all at once. Rather, each thought is generated or solved independently and passed to the next step for solving the problem, which allows the model to:

- Explore multiple choices for each problem-solving thought.
    
- Evaluate whether certain thoughts bring the model closer to a final solution.
    
- Perform backtracking when certain thoughts are found to be a dead end.
    
- Search over a combinatorial space of possible problem-solving steps to find the best final solution.
    

With ToT prompting, an entire tree of thoughts (shown above) is formed by allowing exploration of different thoughts throughout the problem-solving process. During exploration, the LLM can evaluate progress made by each thought towards a final solution via a language-based process[3](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-3-136223454). Then, by leveraging widely-used search algorithms (e.g., [breadth-first search or depth-first search](https://www.geeksforgeeks.org/difference-between-bfs-and-dfs/)), ToT prompting can be augmented with lookahead and backtracking techniques, allowing the solution space of any problem to be thoroughly explored.

> _“While existing methods sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving.”_ - from [1]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c131f05-b333-4d1c-8939-e4422cbc3485_1552x1058.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c131f05-b333-4d1c-8939-e4422cbc3485_1552x1058.png)

(from [1])

**What does the tree represent?** When using ToT prompting, we explore several _paths_—each comprised of individual _thoughts_—that represent potential solutions to a problem. Together, each of these paths and their individual thoughts form a tree that explores a problem’s solution space; see above. In this tree, each node is simply a partial solution (or thought) to our problem, while each connection is a operator that modifies this partial solution to yield the next thought within a problem-solving path. An example of decomposing a problem-solving chain of thought (i.e., a single path in a tree of thoughts) in this way is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb33e72-2c16-4297-8403-9b29e38f1050_1834x954.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb33e72-2c16-4297-8403-9b29e38f1050_1834x954.png)

Using operators to iteratively modify a partial solution until a final solution is found (from [2])

#### Tree of Thoughts Problem Solving Framework

At this point, we have talked about the generic idea behind ToT prompting, but _how do we use this technique in a practical application_? The implementation of ToT prompting looks a bit different depending upon the problem we are trying to solve, but any instantiation of ToT prompting has to concretely define four standard problem-solving components, which are outlined below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a9bad7f-ac13-4bd8-9cc2-138e390f7ac6_1342x434.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a9bad7f-ac13-4bd8-9cc2-138e390f7ac6_1342x434.png)

(from [1])

**Thought decomposition.** Unlike CoT prompting, ToT explicitly decomposes a problem into intermediate steps or thoughts, which are combined together to form a solution to the underlying problem. Depending on the problem, this decomposition can take a variety of different forms, such as outputting a few words or a single line of an equation. As shown above, the definition of a thought is different for each of the three separate tasks that are considered in [1].

> _“A thought should be small enough so that LMs can generate promising and diverse samples (e.g. generating a whole book is usually too big to be coherent), yet big enough so that LMs can evaluate its prospect toward problem solving (e.g. generating one token is usually too small to evaluate).”_ - from [1]

**Thought generation.** Once we have decided what will constitute a thought, we need to determine how thoughts should be generated during ToT prompting. In [1], authors propose two basic techniques for thought generation:

- _Sampling_: generating several thoughts independently with the same prompt
    
- _Proposing_: generating several thoughts sequentially with a “propose prompt”
    

The sampling approach works best when the thought space is rich, as several independently-generated thoughts are unlikely to be duplicates. If the thought space is more constrained, then the proposing technique can be used to generate several thoughts while avoiding duplicates.

**State evaluation.** Once we have defined our thoughts and chosen how they will be generated, we need to define a heuristic for evaluating the quality of certain chains of thought. Otherwise, there is no way to know whether we are making progress towards a final solution. Given several thoughts that have been generated, authors in [1] use an LLM to reason about the quality of each thought. In particular, two different strategies are followed:

- _Value_: independently assign a scalar value (i.e., rating from 1-10) or classification (i.e., sure, likely, or impossible to reach a solution) to each state.
    
- _Vote_: compare different solutions and select the one that is most promising.
    

Although both approaches can work well, voting is best when a successful solution to a problem is hard to directly value (e.g., creative writing tasks). In both cases, the LLM can be prompted multiple times in a manner similar to self-consistency to achieve more reliable evaluations of each state.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6237d7b-d53b-413d-9877-a0b475fe690b_1334x452.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6237d7b-d53b-413d-9877-a0b475fe690b_1334x452.png)

(from [1])

**Search algorithm.** The final component of ToT prompting is the search algorithm that is used to explore the solution space. Although many potential search algorithms can be used, we see in [1] that authors focus on two basic algorithms—_BFS and DFS_—with formulations shown above.

## Experimental Analysis

Authors in [1] propose three new tasks that are used to evaluate the ToT prompting technique: Game of 24, Creative Writing, and 5x5 Crosswords. For each of these tasks, we will first overview the implementation of ToT prompting, which follows the four-part framework outlined above. Then, we will outline the experimental results, highlighting the effectiveness of ToT prompting on problems that require extensive planning or search. Notably, alternatives like CoT prompting and self-consistency tend to fall short of solving such complex tasks.

#### Game of 24

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce672d1-571c-4520-a63b-3b13b864249b_1318x416.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce672d1-571c-4520-a63b-3b13b864249b_1318x416.png)

(from [1])

The implementation of ToT prompting for the Game of 24 task is shown above. In this task, the LLM is given four numbers and expected to generate a sequence of arithmetic operations—_where each number is only used once_—that result in the number 24. This task is always decomposed into three thoughts[4](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-4-136223454), each of which is an intermediate equation. The same prompt is used to generate each of the three thoughts in a candidate solution, and we evaluate states with a value prompt that classifies intermediate solutions as _sure_, _likely_, or _impossible_ to reach a correct final solution. Then, a BFS algorithm is applied to find the resulting solution, keeping the best five (i.e., `b=5`) candidate solutions at each step.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13b0f4a4-9ef6-4a15-8605-0f4e7292297e_1530x552.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13b0f4a4-9ef6-4a15-8605-0f4e7292297e_1530x552.png)

(from [1])

**Performance.** ToT prompting is compared to several baselines, including standard few-shot prompting (IO), CoT prompting, and CoT-based self-consistency (CoT-SC). As shown above, all baselines perform quite poorly on this task (i.e., <10% success rate), while ToT prompting achieves up to a 74% success rate. Interestingly, the success rate improves with higher settings of `b` for BFS. Furthermore, we see based on error analysis in [1] that most solutions with CoT prompting fail after the first step, while failures with ToT prompting are distributed uniformly among intermediate steps. Such a finding demonstrates the benefit of ToT prompting, as it can evaluate intermediate states prior to producing a final output, allowing multiple viable solution paths to be explored.

#### Creative Writing

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48bc443b-7a7a-4c61-ade6-4417c7ed40f9_1340x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48bc443b-7a7a-4c61-ade6-4417c7ed40f9_1340x606.png)

(from [1])

The creative writing task explored in [1] provides four random sentences as input to the LLM and expects a passage containing four paragraphs that each end in the four input sentences to be outputted. The quality of outputs is judged either using GPT-4 (with a zero-shot prompt) or human evaluations. For this task, ToT prompting requires two intermediate thoughts. First, the LLM generates five different writing plans and uses a zero-shot voting prompt to select the best one. Then, five different passages are generated based upon the selected plan, and the best passage is identified by using (again) a zero-shot voting prompt; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdb0d4a0-c8b8-4b18-a4ab-762b450503d9_800x418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdb0d4a0-c8b8-4b18-a4ab-762b450503d9_800x418.png)

(from [1])

**Performance.** As shown in the figure above, ToT generates more coherent passages than few-shot and CoT prompting on average, as judged by both GPT-4 and human evaluators. When an iterative refinement procedure is applied to improve result quality, we see that both few-shot and ToT prompting perform comparably. Such a procedure can be thought of as a new approach to thought generation, where new thoughts are generated by prompting the LLM to refine old thoughts instead of generating them from scratch.

#### 5x5 Crosswords

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4acf1a5-0702-47fb-ab62-584cd2f24179_1344x630.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4acf1a5-0702-47fb-ab62-584cd2f24179_1344x630.png)

(from [1])

The final task considered in [1] is a mini crossword puzzle, which explores the ability of ToT prompting to discover solutions to problems that require a greater number of intermediate steps. The input for this task provides five horizontal clues and five vertical clues, while the output should be a 5x5 grid of letters that solve the crossword puzzle. Success is judged based upon the portion of correct letters, words, and games achieved by each prompting technique.

**ToT setup.** For mini crossword, ToT prompting is implemented with DFS. Each thought considers a single word clue. Thoughts are generated sequentially and cannot change any currently-filled words or letters. To find new candidates, the LLM takes all existing thoughts as input, generates letter constraints for remaining word clues based on these thoughts, and uses a proposal prompt to come up with candidates for the word should be filled next and where it should go. Interestingly, the LLM is also prompted to provide a confidence level for each thought, allowing thoughts to be explored in order of confidence. Intermediate states are evaluated based upon whether it is possible to arrive at a viable final solution. If not, DFS backtracks to the parent node within the tree of thoughts and continues to explore. This entire process is depicted in the figure above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F642a5cb1-34bc-4de3-99b4-7a5c36e42058_488x414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F642a5cb1-34bc-4de3-99b4-7a5c36e42058_488x414.png)

(from [1])

**Performance.** As shown in the table above, ToT prompting drastically outperforms few-shot and CoT prompting in terms of success rate on the mini crosswords benchmark. Even still, ToT prompting only globally solves 4 out of 20 total puzzles that were tested, revealing that there is significant room for improvement on such tasks. However, the ability of ToT prompting to backtrack and explore different solutions via DFS is a massive differentiator.

> _“Such an improvement is not surprising, given IO and CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.”_ - from [1]

## Closing Remarks

Recent research on prompting techniques has drastically expanded the scope of problems that are solvable via an LLM. However, most prompting techniques are constrained by the autoregressive nature of language generation—_they tend to follow a left-to-right approach that lacks deliberate planning and opportunities for exploring alternative solutions to a problem_. ToT prompting solves this issue by abstracting the solution to a problem as a tree of intermediate steps that can be independently explored and evaluated using known algorithms and heuristics. The idea behind ToT prompting is incredibly generic and can be instantiated differently depending on the underlying problem. We see several examples of this in [1], where ToT prompting is shown to more effectively solve multi-step reasoning problems compared to CoT prompting and related variants.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews that explain relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [twitter](https://twitter.com/cwolferesearch) or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." _arXiv preprint arXiv:2305.10601_ (2023).

[2] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." _Advances in Neural Information Processing Systems_ 35 (2022): 24824-24837.

[3] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." _arXiv preprint arXiv:2203.11171_ (2022).

[4] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[5] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[7] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." _Advances in neural information processing systems_ 35 (2022): 22199-22213.

[8] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." _arXiv preprint arXiv:2205.10625_ (2022).

[9] Arora, Simran, et al. "Ask me anything: A simple strategy for prompting language models." _arXiv preprint arXiv:2210.02441_ (2022).

[10] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[11] Saravia, Elvis, et al. “Prompt Engineering Guide”, [https://github.com/dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) (2022).

[12] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In IFIP congress, volume 256, page 64. Pittsburgh, PA, 1959.

[13] A. Newell, H. A. Simon, et al. Human problem solving. Prentice-Hall, 1972.

[1](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-anchor-1-136223454)

Plus, using a longer prompt results in higher usage on LLM APIs due to the increased number of tokens. Even when using open-source LLMs, longer prompts are likely to lead to heightened inference times.

[2](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-anchor-2-136223454)

If we are using an API, then we have to pay for usage based on the number of tokens or characters that the model ingests and generates. As such, using many few-shot exemplars in our prompt can drastically increase the number of input tokens and, in turn, increase the price of using the API.

[3](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-anchor-3-136223454)

In other words, we can just use a prompting approach that asks the language model—using a standard language-based/textual prompt—to assess its progress towards a final solution for any given problem.

[4](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#footnote-anchor-4-136223454)

Given that the solution is always an arithmetic equation with four numbers, we know that only three intermediate equations or arithmetic operations always need to be outputted before reaching a final solution.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Jared Kirby's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F400d64cd-b630-4cc0-b4ea-489892550d99_1287x1284.jpeg)



](https://substack.com/profile/32625339-jared-kirby)

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92491652-7333-4422-8f66-980b73e409fc_3024x4032.jpeg)



](https://substack.com/profile/154511708-cameron-r-wolfe-phd)

[

![darlin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29e363b8-0457-4b09-b998-d7799cc67d0d_2265x2265.png)



](https://substack.com/profile/127381235-darlin)

[

![米斯特's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F867e9bd3-3d48-433e-b41d-3ceeabb17f5c_48x48.png)



](https://substack.com/profile/119205644-7c7365af7279)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

31 Likes∙

[3 Restacks](https://substack.com/note/p-136223454/restacks?utm_source=substack&utm_content=facepile-restacks)

31

- 

[](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture




----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Graph-Based Prompting and Reasoning with Language Models

### Understanding graph of thoughts prompting and several variants...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Aug 28, 2023

33

- 

[

2

](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning/comments)

3

Share

[

![Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png "Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png)

This newsletter is presented by [Deci AI](https://deci.ai/). Deci does a ton of interesting AI research. Most recently, they released DeciCoder-1B, an open-source code generation model. Read about it [here](https://twitter.com/cwolferesearch/status/1691929174175264858?s=20) or [download it](https://huggingface.co/Deci/DeciCoder-1b) on HuggingFace.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F553be3b4-3c80-435d-88c5-c7079bff9cbb_1940x1090.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F553be3b4-3c80-435d-88c5-c7079bff9cbb_1940x1090.png)

(from [1, 2])

Advanced prompting techniques like [chain of thought](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [8] and [tree of thought](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) [9] prompting have drastically improved the ability of large language models (LLMs) to solve complex, reasoning-based tasks. At a high level, forcing the LLM to construct a step-by-step response to a problem drastically improves its problem-solving capabilities. However, all of such techniques assume that the reasoning process should follow a linear patterns that progresses from one thought to the next. Notably, the reasoning process followed by humans tends to be quite different, following multiple different chains of thought and even combining insights from different thoughts to arrive at a final solution. Within this overview, we will studying several prompting techniques that model the reasoning process as a graph structure—rather than a chain or tree—that better captures the various types of non-linear patterns that may occur when reasoning over a problem.

> _“Human thinking is often characterized by its ability to make sudden leaps and connections between seemingly unrelated ideas, which can lead to novel insights and solutions. This non-linear, jumping thought process is a hallmark of human creativity, reasoning, and problem-solving abilities.”_ - from [1]

## Background Information

Within this overview, we will explore several advanced prompting techniques for LLMs that can be used to solve difficult multi-step reasoning problems. Luckily, we have recently overviewed the basic ideas behind prompting, including:

- Prompting basics (i.e., prompt engineering, context windows, structure of a prompt, etc.) [[link](https://cameronrwolfe.substack.com/i/136223454/the-basics-of-prompting)]
    
- Simple prompting techniques (e.g., zero/few-shot learning and instruction prompting) [[link](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting#%C2%A7hierarchy-of-prompting-techniques)]
    
- Advanced prompting techniques (e.g., chain of thought, self-consistency, and least-to-most prompting) [[link](https://cameronrwolfe.substack.com/i/136223454/advanced-prompting-techniques)]
    

We have covered both [practical](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part) and [advanced](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering) prompting techniques in the past. All of these concepts—especially [chain of thought (CoT) prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [8], [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [10], and [tree of thought (ToT) prompting](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) [9]—will be relevant for gaining an understanding of this overview. Beyond these ideas, we need to understand the transformer architecture and the graph convolutional network (GCN) [13], which is applicable to machine learning on graph-structured data.

#### The Transformer from Top to Bottom

The transformer architecture, proposed in [11], was originally applied to Seq2Seq[1](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-1-136366740) tasks (e.g., language translation). However, this model—and several of its variants—has since evolved to capture a variety of different use cases, such as:

- Vision transformers [4] for object [detection](https://github.com/facebookresearch/detr) and [classification](https://cameronrwolfe.substack.com/p/vision-transformers-from-idea-to) in images
    
- [Encoder-only transformers](https://cameronrwolfe.substack.com/i/76273144/berts-architecture) for [discriminative language tasks](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    
- [Decoder-only transformers](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20) for [language modeling](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    

Many deep learning architectures are used in practice, but the transformer is unique in its scope—_it is a single architecture that can be applied to a massive variety of task_s. First, we will learn about the standard, encoder-decoder transformer architecture, then we will extend this discussion to other notable variants.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ff80936-f5fd-410f-8dc2-f09a16f29bfb_742x1166.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ff80936-f5fd-410f-8dc2-f09a16f29bfb_742x1166.png)

**Encoder-decoder transformers.** The transformer has two components:

- _Encoder_: each block contains [bidirectional, multi-headed self-attention](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20) and a [feed-forward](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks) transformation (usually a two-layer feed-forward network).
    
- _Decoder_: each block contains [masked self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20), [cross-attention](https://vaclavkosar.com/ml/cross-attention-in-transformer-architecture), and a feed-forward transformation.
    

In prior overviews, we have discussed how [raw text is processed](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective) before being ingested by a transformer. This processing converts text into a sequence of vectors—_with added positional information_—corresponding to each token in the input. This sequence of vectors is then ingested by the transformer’s encoder, which performs the operations described above. See below for a depiction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cac73ca-fa2b-471c-b1bd-a023af6264e8_1754x1166.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cac73ca-fa2b-471c-b1bd-a023af6264e8_1754x1166.png)

Depiction of a basic transformer encoder block

The output of this block is simply a sequence of token vectors that have been transformed via bidirectional self-attention and a feed-forward neural network. We can then take the resulting sequence that has been passed though all blocks of the encoder and use it as input for the decoder component. Put simply, _the encoder forms a representation of the entire input sequence using bidirectional self-attention_, meaning that every token within the input sequence considers all other tokens in the sequence when crafting the encoder’s output sequence.

The decoder then ingests the encoder’s output and uses this representation of the input sequence as context when generating output. The decoder portion of the transformer is similar to the encoder with two main differences:

1. It uses [masked self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20).
    
2. It has an added cross-attention mechanism.
    

Masked self-attention restricts the multi-headed self-attention operation in the decoder from “looking forward” in the sequence. In other words, each token’s representation only depends on the tokens that come before it; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31acf19b-f698-4bf3-a992-821c3f000d58_1160x408.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31acf19b-f698-4bf3-a992-821c3f000d58_1160x408.png)

Such a modification is necessary because the decoder is expected to generate a textual sequence as output. If the decoder used bidirectional self-attention, the model would be able to “cheat” during training by looking at the correct next token within the target sequence and copying it when predicting the next token. Masked self-attention avoids this issue and can be efficiently trained to generate coherent text via [next token prediction](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective). Cross attention is similar to any other attention operation, but it fuses two separate sequences—from the encoder and the decoder—with a single attention operation. See [here](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) for more details.

**Example of the encoder-decoder architecture.** One notable and widely-used example of a standard, encoder-decoder transformer architecture is the text-to-text transformer (T5) model [5]. This model is heavily used for transfer learning tasks with natural language and is even used by one of the prompting techniques that we will learn about in this overview. Analysis of T5 shows us that encoder-decoder transformers are useful for Seq2Seq tasks and prefix language modeling tasks[2](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-2-136366740), which are both common practical problems. To learn more about the T5 architecture, feel free to check out the prior overview on this topic linked below.

- T5 Architecture (Part One) [[link](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)]
    
- T5 Architecture (Part Two) [[link](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part-354)]
    

**Encoder-only and decoder-only variants.** Within the explanation of T5 linked above, we learn extensively about the [different variants](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures) of the transformer architecture. The two most notable variants are _encoder-only_ and _decoder-only_ models, both of which are relatively self-explanatory. Encoder-only architectures use the encoder portion of the transformer and completely eliminate the decoder. Such an architecture was popularized by [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [12] and is incredibly effective when fine-tuned on a variety of different discriminative language tasks (e.g., sentence classification, named entity recognition, question answering, etc.).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe22dcc71-2a18-4d31-b811-9fae5d6c2889_1586x750.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe22dcc71-2a18-4d31-b811-9fae5d6c2889_1586x750.png)

The decoder-only transformer architecture (from [11])

Similarly, decoder-only architectures just eliminate the encoder portion of the transformer. However, this means that we also must get rid of any cross-attention modules due to the lack of an encoder; see above. As such, each block of a decoder-only transformer just performs masked self-attention and a feed-forward transformation. These architectures have exploded in popularity recently due to their heavy use in large, causal language models. Most of the generative LLMs that are widely studied today—[GPT variants](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2), [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive), [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source), [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up), etc.—rely upon a decoder-only transformer architecture.

#### AI with Graph-Structured Data

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9692ad0-7227-4541-8b40-ddf2e59d94b2_1190x434.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9692ad0-7227-4541-8b40-ddf2e59d94b2_1190x434.png)

Basic examples of Euclidean and non-Euclidean data

Within this overview, we will learn about prompting techniques that leverage a graph data structure to model the reasoning processes. With this in mind, we need to learn about how graph-structured data is usually handled within machine learning applications. Namely, most model architectures (e.g., transformers or [convolutional neural networks](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939)) are meant for handling [Euclidean data](https://ai.stackexchange.com/questions/11226/what-is-non-euclidean-data) (e.g., images or text) that can easily be represented as a matrix. However, not all data is Euclidean. In fact, many sources of real world data are more appropriately modeled as a [graph](https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/) (e.g., social networks, molecules, etc.). For such data, we use a special model architecture called a [graph convolutional network (GCN)](https://distill.pub/2021/gnn-intro/) [13].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5ddfb0a-863e-4e3b-90d0-20d212bf0ef0_1236x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5ddfb0a-863e-4e3b-90d0-20d212bf0ef0_1236x882.png)

Illustration of a layer in a graph convolutional network

**Understanding the GCN.** At their core, GCNs are not much different from your typical feed-forward neural network. Given a graph, we associate each node in this graph with an input embedding, which can come from a variety of sources (e.g., embedding of a document, features corresponding to a user, etc.). Then, in each layer of the GCN, we first apply a feed-forward transformation to each node embedding (and normalization). Then, we incorporate the structure of the underlying graph by aggregating neighboring features for each node, such as by taking an average of all neighboring node embeddings; see above. By adding multiple layers to the GCN, we can learn rich node representations that capture both the properties of each node and the structure of the graph; read more below.

[More on GCNs](https://distill.pub/2021/gnn-intro/)

**Other architectures.** The GCN architecture has gained a massive amount of popularity and is widely-used in a variety of impressive, large-scale applications (e.g., [Google Maps](https://arxiv.org/abs/2108.11482)). Given this popularity, several extensions to the GCN have been proposed, including new architectural variants.. One notable example, which we will see used in this overview, is the [Graph Attention Network (GAT)](https://petar-v.com/GAT/).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd1bd461-74e7-43e4-8151-e24ea4262071_1348x884.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd1bd461-74e7-43e4-8151-e24ea4262071_1348x884.png)

(from [14])

The GAT [14] architecture is somewhat similar to the GCN, but it doesn’t just perform a simple average to aggregate features from neighboring nodes. Rather, a weighted average is taken over neighboring node features, where the weights are computed using an attention mechanism. The attention mechanism used in [14] is simple—it just takes two concatenated node embeddings as input and performs a feed-forward transformation to compute a score; see above. Such an approach allows more general aggregations of neighboring features to be learned.

#### Multi-Modal CoT Reasoning

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6aeb7fb-10a7-4d6f-bc41-efb7e556b6ef_664x476.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6aeb7fb-10a7-4d6f-bc41-efb7e556b6ef_664x476.png)

(from [3])

Finally, before moving on to graph-based prompting techniques, there is one final prompting technique of which we will want to be aware—[multi-modal chain of thought prompting](https://towardsdatascience.com/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa) [3]. This method, depicted above, proposes a two-stage approach for solving reasoning problems with both textual and visual inputs. In the first stage, the model takes both text and images as inputs and uses them to generate a problem-solving rationale similar to a chain of thought [8]. Then, this rationale is concatenated with the input and passed though the model—_along with the image_s—once again to produce a final answer. Notably, this approach uses a T5 architecture [5] and is fine-tuned on the tasks that it solves. As we will see, the approach used by graph of thought reasoning [1] is quite similar.

## Going Beyond Chain (or Tree) of Thought

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)

(from [4])

Although CoT prompting is incredibly impactful, we have seen that it has [important limitations](https://cameronrwolfe.substack.com/i/136223454/advanced-prompting-techniques). Most notably, it generates problem solving rationales in a left-to-right fashion using [next token prediction](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective), _which prevents the model from recovering from early mistakes in the reasoning process_. One solution to this problem is [Tree of Thoughts (ToT)](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) prompting (shown above), which enables backtracking and strategic lookahead over intermediate reasoning steps modeled as a tree. Despite its utility, ToT prompting still models reasoning and problem solving as a linear process that progresses over a single path of nodes within a tree, which fundamentally limits the capabilities of this prompting technique.

> _“By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes.”_ - from [1]

As we have discussed, humans do not reason linearly. Rather, we make leaps and connections between ideas that lead to novel insights. Inspired by this idea, researchers have recently extended chain and tree of thoughts prompting to graph-structured data. In other words, we model the reasoning process as a graph, rather than as a chain or tree. In this section, we will overview this work and how it can be used to improve the reasoning capabilities of LLMs.

#### Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models [1]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ba9398f-32d5-4e02-bf8b-05561a51e6b6_1608x884.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ba9398f-32d5-4e02-bf8b-05561a51e6b6_1608x884.png)

(from [1])

In [1], authors propose a two-stage reasoning framework[3](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-3-136366740), called graph-of-thought reasoning (we’ll call it GOTR for short), for solving reasoning tasks with textual (and potentially visual) inputs. In the first stage, the language model is used to generate a problem-solving rationale. Then, the second stage uses this generated rationale to arrive at a final answer. This two-stage process is inspired by multi-modal CoT [3]. The GOTR framework relies upon three different kinds of inputs during the reasoning process:

- _Text_: This is just the normal, textual input that we get for any prompt-based reasoning task.
    
- _Image_: We can (optionally) ingest an image that is associated with the reasoning task.
    
- _Thought Graph_: We generate a graph of all named entities within the textual input and their relationships to use as input.
    

As we will see, each of these inputs are given separate encoders within [1]. Then, the representations generated by each of these encoders is fused and passed to a decoder module that can generate output—_either a rationale or final answer_.

> _“By representing thought units as nodes and connections between thoughts as edges, the Graph-of-Thought captures the rich, non-sequential nature of human thinking and allows for a more realistic and logical modeling of reasoning processes.”_ - from [1]

**Two-stage framework.** As mentioned above, GOTR operates in a two-stage framework. In the first stage, the model is given the input text for the problem being solved and is expected to generate a problem-solving rationale—_similar to a chain of thought_. Then, the rationale that is generated during the first phase is just concatenated with the input text, and we generate output once again. The only difference between the first and second stages is that:

- The second stage has a longer input (i.e., both input text and the rationale).
    
- The second stage generates a final answer rather than a rationale.
    

However, the structure of both stages is identical other than the modified inputs and outputs. This two-stage process followed by GOTR is depicted below, where we see that two different kinds of outputs are generated in each stage.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c037be3-7b93-4815-a15c-7ea3165e8a4b_1548x952.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c037be3-7b93-4815-a15c-7ea3165e8a4b_1548x952.png)

Two-stage framework for GOTR (from [1])

**Generating the thought graph.** As mentioned before, GOTR takes three sources of data as input: text, images, and a thought graph. The image data is completely optional—_GOTR works perfectly fine without it_. But, we might be wondering: _Where does the thought graph come from?_ Interestingly, we see in [1] that the thought graph is constructed based on the input text. A depiction of this is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0896987-3aef-4fa8-bea7-2c85c4116967_1444x522.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0896987-3aef-4fa8-bea7-2c85c4116967_1444x522.png)

(from [1])

More specifically, the thought graph in GOTR is used to represent the named entities within the input text and their relationships. To generate this graph, we just use off-the-shelf tools (i.e., from the [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) framework) to extract subject-verb-object triplets from the text and perform coreference resolution[4](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-4-136366740) to unify duplicate entities, thus forming a graph representation of our input text.

**Encoding the inputs.** To ingest data from the different input modalities (i.e., text, image, and graph), we use separate encoders for each. For the image and text data, we can just use a transformer encoder! In [1], images are encoded using a [vision transformer](https://cameronrwolfe.substack.com/p/vision-transformers-from-idea-to) [4], while text is encoded using the encoder of the [T5 model](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [5].

Notably, we should realize here that the GOTR framework is using a different model architecture compared to most [causal LLMs](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)[5](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-5-136366740). Rather than the typical decoder-only architecture used by causal language models, GOTR uses a prefix-based language modeling approach that ingests input with multiple encoder models, then passes the output of these encoders to a decoder to generate output. This is similar to the encoder-decoder transformer architecture, but there are multiple different types of encoders that are used! To handle the multiple encoders, we have a few learnable layers prior to the decoder that fuse their outputs into a single sequence that is then passed to the decoder. The full encoder-decoder setup used by GOTR is fine-tuned on the desired task.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c69c15a-9512-4400-9a7a-b835750616cf_2108x820.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c69c15a-9512-4400-9a7a-b835750616cf_2108x820.png)

Formulation and depiction of the GAT-based encoder architecture used to encode the thought graph within GOTR (from [1])

**Using the GAT.** The encoder used for the thought graph is based upon a GAT architecture. As mentioned previously, the GAT is a style of GCN architecture. Instead of aggregating features of neighboring nodes via a simple sum or average operation, GATs use an attention mechanism to aggregate information between neighboring nodes. The exact GAT architecture used in [1] is depicted above.

[GAT Model](https://docs.dgl.ai/en/0.8.x/tutorials/models/1_gnn/9_gat.html)

**Fusing representations.** Once we have encoded the data from our text, image, and thought graph inputs, we need to fuse these features together prior to passing them to the decoder to generate an output. To do this, we can first use a simple [cross-attention mechanism](https://vaclavkosar.com/ml/cross-attention-in-transformer-architecture)! This initial feature fusion operation is shown below, where textual features are fused with both image and thought graph features using cross attention. Here, we should recall that image features are optional and can be completely excluded from the GOTR framework.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffefca346-bc7e-4082-9407-012ad612371d_1272x150.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffefca346-bc7e-4082-9407-012ad612371d_1272x150.png)

(from [1])

After cross-attention, we still have image features, text features, and (potentially) image features. We still need to combine these features together into a single feature representation that can be passed to the decoder. This is done via a gated fusion layer. Although this might sound complicated, all it means is that we _i)_ take our input features, _ii)_ multiply them by some learnable weight matrices, and _iii)_ produce “masks” (i.e., matrices with values between zero and one at each entry) that tell use which portions of each feature to keep or get rid of as we combine all image, text, and graph features together; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86d993f1-7c49-4a86-b159-8f491748a7f1_1636x362.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86d993f1-7c49-4a86-b159-8f491748a7f1_1636x362.png)

Learnable fusion layer for GOTR (from [1])

**Experimental results.** The GOTR framework is evaluated on two tasks: text-only [GSM8K](https://huggingface.co/datasets/gsm8k) (i.e., grade-school math problems) and multi-modal [ScienceQA](https://scienceqa.github.io/) (i.e., multiple choice science questions). GOTR uses a pre-trained T5 model [5] as its backbone and [DETR](https://github.com/facebookresearch/detr) [6] as its image encoder. Notably, GOTR is fine-tuned on each of the experimental benchmarks. As such, all baselines that are used for comparison—_including few-shot learning and CoT prompting with multiple LLMs, as well as a few task-specific methods_—undergo similar fine-tuning prior to evaluation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad21d3d-6ff8-458c-8674-7e0a390a2572_1612x584.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ad21d3d-6ff8-458c-8674-7e0a390a2572_1612x584.png)

(from [1])

When we solely look at the quality of problem-solving rationales generated by GOTR in comparison to other frameworks, we immediately learn that GOTR generates higher-quality rationales in terms of [ROUGE](https://eugeneyan.com/writing/llm-patterns/#more-about-evals) score; see above. Most notably, we see a slight increase in quality compared to multi-modal CoT and [UnifiedQA](https://arxiv.org/abs/2005.00700) approaches, which seem to indicate that incorporating a thought graph into the problem-solving process can be helpful.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c16f38a-f6f0-42fa-baa7-d274dad8b502_1372x516.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c16f38a-f6f0-42fa-baa7-d274dad8b502_1372x516.png)

(from [1])

When we examine the accuracy of GOTR’s final solutions, we see that the framework in [1] outperforms a variety of other alternatives on the GSM8K dataset; see above. Notably, [GPT-4](https://openai.com/research/gpt-4) far outperforms all other techniques. However, we should keep in mind that such a comparison is likely unfair given that GPT-4 is [rumored](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) to be an ensemble (or [mixture](https://arxiv.org/abs/1701.06538)) of several large models. Plus, GOTR makes significant progress towards closing the gap in performance with GPT-4 and outperforms strong baselines like [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [7] and [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2864d797-1fa9-4a3e-942b-6228cb60c280_1626x1064.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2864d797-1fa9-4a3e-942b-6228cb60c280_1626x1064.png)

(from [1])

On the ScienceQA dataset, GOTR achieves state-of-the-art performance among all techniques, even outperforming GPT-4 with CoT prompting in several cases. Such results indicate that GOTR is a useful framework for integrating multiple modalities of data into the problem-solving process. Although we see a slight improvement in performance on the GSM8K dataset, GOTR’s value is most evident when used for ScienceQA given its ability to leverage all input data modalities—_text, image, and graph_—when solving a reasoning task.

#### **Graph of Thoughts: Solving Elaborate Problems with Large Language Models [2]**

> _“This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.”_ - from [2]

Although GOTR is an interesting framework, one may argue that it is not truly a prompting technique, as it must be fine-tuned or trained to solve any reasoning problem. In [2], authors again explore a graph-inspired framework for reasoning with LLMs. However, a pure prompting approach—similar to [CoT](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [8] or [ToT prompting](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) [9]—is taken that _i)_ uses a casual pre-trained LLM and _ii)_ does not require any fine-tuning. The method, called Graph of Thought (GoT) prompting, models each thought generated by an LLM as a node within a graph, then uses vertices that connect these nodes to represent dependencies; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a6e06d-3c31-40b4-b96d-d7957898cfaa_822x666.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a6e06d-3c31-40b4-b96d-d7957898cfaa_822x666.png)

(from [2])

As previously mentioned, humans may not follow a strict chain of thought when solving a problem. Instead, they are likely to:

1. Try multiple chains of thought.
    
2. Combine insights from multiple chains of thought together.
    

The first case outlined above can be handled by ToT prompting, but combining different chains of thought is not amenable to a tree-structured thought pattern. For this, _we need a graph structure in which multiple paths of reasoning can be merged together_. Furthermore, such a structure enables patterns like recurrence to be captured, which may be valuable for solving a variety of different problems.

The GoT approach in [1] enables us to model individual thoughts from an LLM and arbitrarily combine these thoughts—_e.g., by distilling an entire network of thoughts, enhancing thoughts with feedback loops, and more_—to form an accurate output. Plus, the framework is extensible to other models and thought patterns, making plug-and-play with different LLMs and prompting techniques easy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc113cb01-ac85-4e0e-a418-a21c5dd9a4e0_1728x718.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc113cb01-ac85-4e0e-a418-a21c5dd9a4e0_1728x718.png)

(from [2])

**GoT framework.** The approach employed by GoT is shown in the figure above. The LLM’s reasoning process is represented as a (directed) graph. Each node in this graph corresponds to an individual thought generated by an LLM, and edges represent relationships between thoughts. Namely, an edge from thought `a` to `b`—or directed edge (`a`, `b`) in the graph—simply tells us that thought `b` was generated using thought `a` as input. Similarly to ToT prompting, the exact definition of a thought depends on the problem being solved. Going further, each node represents a (potentially intermediate) solution to a problem, but we can have different types of nodes within the graph that represent different aspects of the reasoning process (e.g., planning versus execution).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9619ac8-6a8d-4394-b9bd-d9a530667aa5_824x656.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9619ac8-6a8d-4394-b9bd-d9a530667aa5_824x656.png)

(from [2])

**Thought transformations.** Given that we use a graph to represent the reasoning process executed by the LLM, any modification to this graph represents a modification to the underlying reasoning process. Authors in [2] refer to these modifications as _thought transformations_, which are concretely defined as adding new vertices or edges to the graph. As shown in the figure above, various kinds of thought transformations exist (e.g., merging or splitting numbers of an array, summarizing a set of articles, generating multiple summaries of a single article, and so on). We consider three primary types of thought transformations in [2]:

- _Aggregation_: aggregate arbitrary thoughts into a new thought.
    
- _Refinement_: refining the content in a thought via a self-connection.
    
- _Generation_: generate multiple new thoughts based on a single thought.
    

Each of these transformations can modify and advance an LLM’s reasoning process arbitrarily. For example, aggregation can merge the results of multiple different chains of thought together, while refinement can recursively update a thought until arriving at a final answer. Such functionality strictly extends CoT and ToT prompting—_it can do everything these techniques can do and more_!

> _“When working on a novel idea, a human would not only follow a chain of thoughts (as in CoT) or try different separate ones (as in ToT), but would actually form a more complex network of thoughts.”_ - from [2]

**Scoring and ranking.** Finally, GoT prompting uses evaluator functions to assign scores to certain thoughts, as well as a ranking function to select the most relevant thoughts. Notably, both ranking and scoring consider the entire graph. This is necessary because, for scoring, the quality of a thought might depend on other thoughts. Ranking typically just returns thoughts with the highest scores.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5134c71-394b-4157-a9d4-4ad992a6a8ff_1102x1448.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5134c71-394b-4157-a9d4-4ad992a6a8ff_1102x1448.png)

(from [2])

**Actual implementation.** So far, the discussion of GoT prompting has been relatively high-level, but _how do we actually implement this?_ In [2], authors do this via a series of different LLM-powered modules. The modules, which are detailed in depth in the figure above, are as follows:

- _Prompter_: prepares messages or prompts for the LLM. The prompt is expected to contain an encoding of the graph structure.
    
- _Parser_: extracts relevant information from LLM outputs, thus forming the state stored within each thought.
    
- _Scorer_: verifies that thought states satisfy correctness conditions and assigns them a score (derived either from an LLM or a human annotator).
    
- _Controller_: coordinates the reasoning process and decides how to progress.
    

Notably, the controller selects the thought transformations that should be applied to the underlying graph, communicates this information to the prompter, and decides whether the reasoning process has finished or should continue forward based on the output of the scorer on generated thought states. Throughout this process, the controller maintains two pieces of information:

- _Graph of Operations_: a user-defined, static structure that is created prior to the reasoning process and captures the execution plan for thought operations.
    
- _Graph Reasoning State_: a dynamic structure that tracks the state of the LLM reasoning process, including all thoughts and their states.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95ca135-cf15-4e66-a1df-15fa49e865c8_532x1374.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95ca135-cf15-4e66-a1df-15fa49e865c8_532x1374.png)

(from [2])

**Use cases.** Several applications of GoT prompting are explored in [2]. The first use case is sorting a list of digits (with duplicates) using a merge-based approach[6](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-6-136366740). Here, a thought is defined as a sequence of sorted numbers and thoughts are scored based on the number of errors in the sorting. The full GoT framework for sorting is depicted above. Beyond sorting, authors consider computing the intersection of two sets using GoT prompting, which is also implemented using a merge-based approach. The score of each thought is computed as the number of missing elements from the set intersection. Finally, a few practical use cases, such as keyword counting and document merging (i.e., generating a new output document based on several similar input examples), are considered.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b1c5787-c840-45b5-b842-e607d16632df_982x420.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b1c5787-c840-45b5-b842-e607d16632df_982x420.png)

(from [2])

**Is GoT effective?** To begin their evaluations, authors theoretically analyze two properties of all prompting approaches considered in [2]:

- _Latency_: How many thoughts does it take to reach a solution?
    
- _Volume_: How many preceding thoughts can impact the current thought?
    

Interestingly, simple analysis can be used to show that GoT prompting has _i)_ less latency and _ii)_ greater volume compared to prior techniques; see above. When evaluated on sorting tasks, we see that GoT prompting consistently produces fewer errors compared to techniques like [CoT](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [8], [CoT with self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [10], or [ToT prompting](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) [9]. These results are outlined in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbce3db99-ae73-46d5-9e78-5a4aa5c84bb3_2044x852.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbce3db99-ae73-46d5-9e78-5a4aa5c84bb3_2044x852.png)

(from [2])

One downside of GoT prompting that we see above is that the total cost of deriving a solution is higher than more straightforward approaches like basic [few-shot prompting](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) (IO) or CoT. On other tasks, findings are similar; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf6bfcd2-78a6-4d51-9a1c-3f87db8f3cf9_1514x1372.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf6bfcd2-78a6-4d51-9a1c-3f87db8f3cf9_1514x1372.png)

(from [2])

However, one key takeaway to notice here is that the difference in performance between GoT and other [advanced prompting techniques](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering) seems to be less pronounced on real-world tasks. For example, GoT prompting provides a less noticeable improvement on the document merging task. Similarly, GoT provides a benefit in terms of performance on the keyword counting task, but baseline techniques—_especially ToT prompting_—are quite competitive.

**When does GoT work well?** Within [2], we see that GoT works quite well for certain tasks, but provides less of a benefit on others. When considering whether to use GoT in practice, there are a few questions we should ask:

- Can the problem we are trying to solve be easily broken in to smaller, solvable sub-problems and merged for a final solution? For these kinds of (merge-based) problems, GoT prompting works incredibly well.
    
- Is the increased cost of GoT prompting going to be a problem? Can we get a reasonable solution with a cheaper technique (e.g., CoT prompting)?
    

The answers to these questions will determine whether it makes sense to use GoT prompting. We see in [2] that this method works well, but it also _i)_ is more costly and _ii)_ only has a noticeable impact on performance for certain types of problems.

## Takeaways

> _“Graph-enabled transformations bring a promise of more powerful prompting when applied to LLM thoughts, but they are not naturally expressible with chain of thought or tree of thought prompting.”_ - from [2]

The techniques we have learned about in this overview are inspired by a simple idea—_allowing language models to structure their reasoning process as a graph_. Although this idea might seem natural, prior techniques have achieved massive success without it. Despite this success, we see in this overview that moving away from a linear reasoning process towards a more flexible graph-based structure is beneficial on certain tasks. Interestingly, multiple different approaches for “graph of thought” prompting have been explored, including both GOTR—_a two-stage reasoning framework that uses an encoder-decoder structure with fine-tuning_—and GoT [2]—_a more traditional prompting approach that leverages a system of language foundation models with prompts_. For both of these techniques, we see that a graph structure can benefit the reasoning process. However, the resulting reasoning process can be more complex and costly, revealing that a GoT-style approach may only be necessary for problems that cannot be solved via standard CoT.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." _arXiv preprint arXiv:2305.16582_ (2023).

[2] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." _arXiv preprint arXiv:2308.09687_ (2023).

[3] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." _arXiv preprint arXiv:2302.00923_ (2023).

[4] Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." _arXiv preprint arXiv:2010.11929_ (2020).

[5] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[6] Carion, Nicolas, et al. "End-to-end object detection with transformers." _European conference on computer vision_. Cham: Springer International Publishing, 2020.

[7] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[8] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." _Advances in Neural Information Processing Systems_ 35 (2022): 24824-24837.

[9] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." _arXiv preprint arXiv:2305.10601_ (2023).

[10] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." _arXiv preprint arXiv:2203.11171_ (2022).

[11] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[12] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[13] Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." _arXiv preprint arXiv:1609.02907_ (2016).

[14] Veličković, Petar, et al. "Graph attention networks." _arXiv preprint arXiv:1710.10903_ (2017).

[1](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-1-136366740)

This stands for sequence-to-sequence and refers to tasks that take a sequence as input and produce another sequence as output. For example, language translation tasks take a sequence of tokens in one language and produce the corresponding/translated sequence of tokens in another language as output.

[2](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-2-136366740)

These tasks take a textual prefix as input, then generate a completion. Encoder-decoder models work well for these tasks because the bidirectional self-attention used in the encoder allows a more comprehensive representation of the prefix to be formed before generating output, compared to causal language models that use solely masked self-attention to ingest the prefix.

[3](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-3-136366740)

Notable, the two-stage framework adopted in [1] is inspired by the technique used by multi-modal CoT prompting [3].

[4](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-4-136366740)

Put simply, coreference resolution refers to the problem of finding all noun phrases that refer to the same real-world entity. In [1], we need this to ensure that all nodes in our thought graph are unique! Read more about this idea [here](https://www.cs.cmu.edu/~yimengz/papers/Coreference_survey.pdf).

[5](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-5-136366740)

Here, we use the term “causal LLM” to refer to large language models that use a decoder-only transformer architecture and are trained using a next token prediction (or language modeling) objective.

[6](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning#footnote-anchor-6-136366740)

This means that our approach will divide an array of digits into sub-arrays, sort these sub-arrays, then combine them back together.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Camille's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589b16b4-1509-448a-8baf-374b6e89daec_144x144.png)



](https://substack.com/profile/6504664-camille)

[

![Web Raccoon's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8951259b-32ab-4a51-8b4b-cd6712c0b227_400x400.jpeg)



](https://substack.com/profile/33177685-web-raccoon)

[

![fkx's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F79b72bdb-0765-4b85-8ad8-c94ce76f3382_144x144.png)



](https://substack.com/profile/1873242-fkx)

[

![Harpreet Sahota's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F959a8536-f0f0-4574-8236-e24cfc941456_900x600.jpeg)



](https://substack.com/profile/23215235-harpreet-sahota)

[

![Knowing less's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0a0cf901-1e3c-4f87-b7c6-ad7be4e8720e_144x144.png)



](https://substack.com/profile/41949021-knowing-less)

33 Likes∙

[3 Restacks](https://substack.com/note/p-136366740/restacks?utm_source=substack&utm_content=facepile-restacks)

33

- 

[

2

](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![A Pox on Both Your Houses.'s avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fblack.png)



](https://substack.com/profile/18429479-a-pox-on-both-your-houses?utm_source=comment)

[A Pox on Both Your Houses.](https://substack.com/profile/18429479-a-pox-on-both-your-houses?utm_source=substack-feed-item)

[2023年10月3日](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning/comment/41162811 "2023年10月3日 01:47")

Liked by Cameron R. Wolfe, Ph.D.

How hard and expensive would it be for a small business to apply what you have discussed here to improve predictions and decision making? Are there open source solutions? Could you be hired to help somebody?

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning/comment/41162811)

[1 more comment...](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Language Model Training and Inference: From Concept to Code

### Learning and implementing next token prediction with a casual language model...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Sep 04, 2023

35

- 

[

5

](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comments)

6

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85dbb7d4-7acd-459f-8d02-8ffdd042ecbf_1938x1092.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85dbb7d4-7acd-459f-8d02-8ffdd042ecbf_1938x1092.png)

Despite all that has been accomplished with large language models (LLMs), the underlying concept that powers all of these models is simple—_we just need to accurately predict the next token_! Though some may (reasonably) argue that recent research on LLMs goes beyond this basic idea, next token prediction still underlies the pre-training, fine-tuning (depending on the variant), and inference process of all causal language models, making it a fundamental and important concept for any LLM practitioner to understand.

> _“It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion.”_ - from [10]

Within this overview, we will take a deep and practical dive into the concept of next token prediction to understand how it is used by language models both during training and inference. First, we will learn these ideas at a conceptual level. Then, we will walk through an actual implementation (in PyTorch) of the language model pretraining and inference processes to make the idea of next token prediction more concrete.

## Relevant Background Concepts

Prior to diving into the topic of this overview, there are a few fundamental ideas that we need to understand. Within this section, we will quickly overview these important concepts and provide links to further reading for each.

**The transformer architecture.** First, we need to have a working understanding of the transformer architecture [5], especially the [decoder-only variant](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20). Luckily, we have covered these ideas extensively in the past:

- The Transformer Architecture [[link](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom)]
    
- Decoder-Only Transformers [[link](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)]
    

More fundamentally, we also need to understand the idea of [self-attention](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20) and the role that it plays in the transformer architecture. More specifically, large causal language models—_the kind that we will study in this overview_—use a particular variant of self-attention called [multi-headed causal self-attention](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20).

**Training neural nets with PyTorch.** The code we will look at in this overview is written in [PyTorch](https://pytorch.org/) and heavily relies upon distributed training techniques, such as distributed data parallel (DDP) training. To understand the basics of PyTorch and distributed training, check out the following articles:

- Neural Nets in PyTorch [[link](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)]
    
- PyTorch Distributed Overview [[link](https://pytorch.org/tutorials/beginner/dist_overview.html)]
    
- Distributed Data Parallel in PyTorch [[link](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)]
    

Beyond basic (and distributed) neural network training in PyTorch, we will also see [automatic mixed precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision) training being used, which selectively adjusts the precision—between full precision (`float32`) and half precision (`float16` or [bfloat16](https://cloud.google.com/tpu/docs/bfloat16))—within the neural net during training to improve efficiency. Put simply, we perform a lot of matrix multiplications within the neural net, and _training is a lot faster if we can run some of these multiplications in lower precision_. See [here](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html) for a more extensive (and practical) overview of AMP.

**Deep learning basics.** This overview also requires a baseline understanding of neural networks, including how they are trained and used. To gain this knowledge, I highly recommend the _[Practical Deep Learning for Coders](https://course.fast.ai/)_ course from [fast.ai](https://www.fast.ai/), which is updated frequently and remains (in my opinion) the best practical introduction to deep learning that anyone can get[1](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-1-136638774).

## Understanding Next Token Prediction

We will now learn about next token prediction (also known as the standard language modeling objective)—_the workhorse behind all causal language models_. Within this section, we will first cover a few fundamental concepts related to tokenization, then we will overview the pretraining and inference processes for language models, as well as their relation to the concept of next token prediction.

#### Tokens and Vocabularies

In trying to understand next token prediction, the first question we might have is: _What is a token?_ Put simply, a token is just a word or sub-word within a sequence of text. Given a sequence of raw text as input, the first step we take in using a language model is to tokenize this raw text, or break it into a sequence of discrete tokens; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)

Tokenizing a sequence of raw text

To perform this tokenization, we rely upon a [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). The tokenizer is trained over an unlabeled textual corpus to learn a fixed-size, unique set of tokens that exist. This fixed-size set of tokens is referred to as our vocabulary, and the vocabulary contains all tokens that are known by the language model. Usually, we should try to make sure that the data used to train the tokenizer accurately reflects the kind of data our model will see during training and inference. Given that the vocabulary has a fixed size, this ensures that the tokens we see in the wild are present within the language model’s vocabulary more often than not.

**Tokenization techniques.** Numerous different tokenization techniques exist; see [here](https://huggingface.co/docs/transformers/tokenizer_summary) for an overview. For details on training and using popular tokenizers for LLMs, see [this article](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) that details the byte pair encoding (BPE) tokenizer—_the most commonly-used tokenizer for LLMs_. Another tokenization technique that has become recently popular is [byte-level BPE (BBPE)](https://medium.com/@pierre_guillou/byte-level-bpe-an-universal-tokenizer-but-aff932332ffe), which relies upon bytes (instead of textual characters) as the basic unit of tokenization.

**Token embeddings.** Once we have tokenized our text, we look up the embedding for each token within an embedding layer that is stored as part of the language model’s parameters[2](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-2-136638774). After this, the sequence of textual tokens constructed from our input becomes a sequence of token embedding vectors; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)

Generating token embeddings from raw text

There is one final step required to construct the input that is actually passed to our decoder-only transformer architecture—_we need to add positional embeddings_. Positional embeddings are the same size as token embeddings and treated similarly (i.e., they are stored as part of the language model and trained along with other model parameters). Instead of associating an embedding with each unique token, however, we associate an embedding with each unique position that can exist within a tokenized input; see below for a depiction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3510038d-21bf-4a65-b4a2-20c5676b0fb1_1468x1392.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3510038d-21bf-4a65-b4a2-20c5676b0fb1_1468x1392.png)

Positional embeddings within a language model

We add these embeddings to the token embeddings at the corresponding position. Such additive positional embeddings are necessary because the self-attention operation does not have any way of representing the position of each token. By adding positional embeddings, we allow the self-attention layers within the transformer to use the position of each token as a relevant feature during the learning process. Recent research has explored novel techniques for injecting positional information into self-attention, resulting in techniques like [RoPE](https://blog.eleuther.ai/rotary-embeddings/) [6].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3f8a12-a893-4508-95d7-312d37a77ea2_1792x1112.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3f8a12-a893-4508-95d7-312d37a77ea2_1792x1112.png)

Context window for language models

**Context window.** Language models are pretrained with token sequences of a particular size, which is referred to as the size of the context window or the context length. This size—_typically somewhere in the range of 1K to 8K tokens_ (though [some models](https://www.anthropic.com/index/100k-context-windows) are much larger!)—is (usually) selected based hardware and memory constraints[3](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-3-136638774). Given that we only learn positional embeddings for input of this length, the context window limits the amount of input data that an LLM can process. However, recent techniques like [ALiBi](https://paperswithcode.com/method/alibi) [7] have been developed to enable extrapolation to inputs longer than those seen during training.

#### Language Model Pretraining

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6ac34b2-b4c4-4ae8-87eb-dd46417adba4_1932x374.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6ac34b2-b4c4-4ae8-87eb-dd46417adba4_1932x374.png)

(from [8, 9])

Language models are trained in several steps, as shown above. The first (and most computationally expensive) step is pretraining, which we will focus on within this overview. During pretraining, we get a large corpus of unlabeled text and train the model by _i)_ sampling some text from the dataset and _ii)_ training the model to predict the next word. This is a [self-supervised](https://cameronrwolfe.substack.com/p/language-understanding-with-bert#%C2%A7self-supervised-learning) objective due to the fact that no labels are required. Rather, the ground truth next token is already present within the corpus itself—_the source of supervision is implici_t. Such a training objective is referred to as next token prediction, or the standard language modeling objective.

**Predicting the next token.** After we have our token embeddings (with position embeddings), we pass these vectors into a decoder-only transformer, which produces a corresponding output vector for each token embedding; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7721d1fa-c9ef-47e0-96cf-483bbde4967f_1008x792.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7721d1fa-c9ef-47e0-96cf-483bbde4967f_1008x792.png)

Input and output of a decoder-only transformer

Given an output vector for each token, we can perform next token prediction by _i)_ taking the output vector for a token and _ii)_ using this to predict the token that comes next in the sequence. See below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)

Visualizing next token prediction on a single token

As we can see above, the next token is predicted by passing a token’s output vector as input to a linear layer, which outputs a vector with the same size as our vocabulary. After a softmax transformation is applied, a probability distribution over the token vocabulary is formed, and we can either _i)_ sample the next token from this distribution during inference or _ii)_ train the model to maximize the probability of the correct next token during pretraining.

**Predicting tokens across a sequence.** During pretraining, we don’t predict only a single next token. Rather, we perform next token prediction for every token in a sequence and aggregate the loss over them all. Due to the use of causal self-attention, each output token vector only considers the current token and those that come before it in the sequence. As such, next token prediction can be performed across an entire sequence using a single forward pass of the decoder-only transformer, as each token has no knowledge of tokens that come after it.

#### Autoregressive Inference Process

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)

Generating text with a language model

Now, we understand how to pretrain a language model, but next token prediction is also used when we are performing inference! _Next token prediction underlies all aspects of training and using LLMs._ Starting with an initial (possibly empty) input sequence or prefix, language models generate text by following an autoregressive next token prediction process (see above) with the following steps:

1. Predict the next token
    
2. Add the predicted token to the current input sequence
    
3. Repeat
    

**Choosing next token.** In the prior section, we’ve seen how a probability distribution over tokens is created. But, _how do we actually choose the next token from this distribution?_ Typically, we just sample the next token from this distribution. However, numerous sampling strategies exist that add slight variations to this approach by modifying the probability distribution over tokens. The exact decoding approach varies depending upon the application, but the main concepts and strategies that we need to be familiar with are outlined below:

- Temperature [[link](https://twitter.com/cwolferesearch/status/1671628210180698112?s=20)]
    
- Greedy Decoding [[link](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20)]
    
- Nucleus Sampling [[link](https://twitter.com/cwolferesearch/status/1692617211205022064?s=20)]
    
- Top-K Sampling [[link](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p)]
    

## Creating a Minimal Implementation

Now that we understand the concept of next token prediction, we need to take the ideas we have learned and make them a bit more concrete. Within this section, we will examine an implementation—written in PyTorch—of pretraining and inference (using next token prediction) with an LLM. This implementation is derived from [NanoGPT](https://github.com/karpathy/nanoGPT) by [Andrej Karpathy](https://twitter.com/karpathy), which matches the specs of [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) [1]. In addition to the implementation of NanoGPT provided on GitHub (linked above), there’s an awesome tutorial video to go with it; see below.

[NanoGPT Tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)

Although this model is small compared to most modern LLMs[4](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-4-136638774), it serves as a great example of what language models look like in code. Here, we will study the implementation of NanoGPT and connect it to our discussion of next token prediction from previous sections.

#### The Decoder-Only Transformer

First, we will detail the implementation of our language model architecture, which is based upon a decoder-only transformer. First, we will overview the components of this architecture, moving from a single block of the model to the full, multi-layer architecture. Then, we will study how this model architecture can be used during pretraining and inference with next token prediction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9b00af0-840d-4079-93e6-4a976c648b68_1666x708.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9b00af0-840d-4079-93e6-4a976c648b68_1666x708.png)

**Model configuration.** The first thing we need to look at is the configuration of our model architecture; see above. As we can see, the configuration is just a [data class](https://www.dataquest.io/blog/how-to-use-python-data-classes/) in Python that specifies the various hyperparameters of our architecture. The settings shown above correspond to those of the smallest model architecture explored within the GPT-2 paper [1], as shown in the table below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80a786e0-35eb-4216-be14-7e32b88f5ff8_1186x460.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80a786e0-35eb-4216-be14-7e32b88f5ff8_1186x460.png)

(from [1])

This model contains only 117M parameters and is actually identical to the base transformer architecture used within the original [GPT publication](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt) [2].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e74de2d-90f4-4749-ba1b-7c2426a89ea9_1402x1080.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e74de2d-90f4-4749-ba1b-7c2426a89ea9_1402x1080.png)

**A single block.** Next, we can look at the implementation of a single block within the decoder-only transformer architecture; see above. Here, we see that a decoder-only transformer block has two components:

1. Multi-headed Causal Self-Attention [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
    
2. Feed-forward Neural Network [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
    

For most language models (including NanoGPT), the feed-forward network is a two-layer model, where the hidden layer is slightly wider[5](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-5-136638774) than the input layer. The block’s input is normalized prior to each of the two layers, and a residual connection is added between the layers. See below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c924527-cc5b-463a-96a3-c2969e600883_334x582.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c924527-cc5b-463a-96a3-c2969e600883_334x582.png)

Schematic of a decoder-only transformer block

**Model definition.** Now that we understand the structure of a decoder-only transformer block, we can look at NanoGPT’s full model definition. This definition is provided below, where we see the [constructor](https://www.geeksforgeeks.org/constructors-in-python/) for the model class.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a908daa-d345-4e9e-b4f4-d4af5de0f1e0_2114x1526.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a908daa-d345-4e9e-b4f4-d4af5de0f1e0_2114x1526.png)

As shown above, the LLM contains two different embedding layers—one to store token embeddings and one to store positional embeddings. There are 1024 positional embeddings, corresponding to the context length used to train NanoGPT (i.e., `block_size` setting in the configuration). The language model has 12 transformer blocks in total. The weights of the model are initialized [normally](https://github.com/karpathy/nanoGPT/blob/master/model.py#L162), aside from a few special techniques adopted from GPT-2 [1].

Beyond the basic transformer architecture, there are extra [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) modules that are used during the forward pass at the first/final layer of the LLM. Plus, we have a linear classification head that is used for next token prediction and shares weights with the token embedding layer. This weight sharing method, called [weight tying](https://paperswithcode.com/method/weight-tying) [3], can improve performance while drastically decreasing the total number of parameters in the model[6](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-6-136638774).

#### Implementing Next Token Prediction

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0695ab4-3547-436a-a00b-f88245869f28_1820x1024.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0695ab4-3547-436a-a00b-f88245869f28_1820x1024.png)

Pretraining a language model with next token prediction

Now that we understand the implementation of an LLM’s model architecture, we will take a look at a pretraining and inference implementation with the same architecture. Both pretraining (shown above) and inference rely upon a next token prediction strategy, and we will overview the implementation of next token prediction for each of these processes within this section.

**Forward pass.** To understand how to train NanoGPT, we need to understand the model’s forward pass. There are two different types of forward passes that we can consider—_one for training and one for inference_. The code for NanoGPT’s forward pass (i.e., this method is part of the GPT model class provided previously) is shown below. First, we will consider how this forward pass is used during pretraining, then will return to the inference process later.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83f92e86-d0c9-4c10-a1e9-70a9f69283c9_2168x1564.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83f92e86-d0c9-4c10-a1e9-70a9f69283c9_2168x1564.png)

The forward pass operates as we might expect. We take two tensors as input:

- _Input tensor_ (`idx`): a matrix where each row contains a sequence of token ids, representing a textual sequence to use for pretraining (or inference).
    
- _Target tensor_ (`targets`): similar to the input tensor, but each entry contains the ground truth next token id for each token in the input tensor.
    

Each of these tensors store an entire [mini-batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) that contains multiple sequences of text over which a training iteration is parallelized. Here, we will assume the target tensor is not `None`. This is always true during pretraining, while during inference we have no target and are just freely generating next tokens.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea151b2-0d4d-4a50-b701-22a232342a79_724x366.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea151b2-0d4d-4a50-b701-22a232342a79_724x366.png)

The first step in the forward pass it to construct a matrix corresponding to our positional and token embeddings; see above. The `idx` tensor contains token ids that can be directly used for lookup within the token embedding matrix. We have to manually construct index values to look up positional embeddings. Positional and token embeddings are added together, passed through a dropout layer, and passed through all transformer blocks. Then, a final LayerNorm operation is performed before computing the loss with the next token prediction objective.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc60ee97-77ad-40b9-af54-830b71dc1882_1156x214.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc60ee97-77ad-40b9-af54-830b71dc1882_1156x214.png)

The next token prediction process outputs a distribution over potential next tokens—using the linear `lm_head` module, where the transformer’s output vector for each token is used as input—for every token within the input sequence. Then, we apply a [CrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) loss to this result, thus training the model to correctly predict the next token at every position within the entire input sequence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8f6830-66bf-48f5-888b-4ceaac2a11c0_2082x1414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8f6830-66bf-48f5-888b-4ceaac2a11c0_2082x1414.png)

**Performing inference.** Beyond pretraining, we can can generate text with next token prediction. As explained previously, generating text with a language model is an autoregressive process that iteratively predicts each next token. To predict a token, NanoGPT follows the steps outlined below:

1. Perform a forward pass with the current input sequence
    
2. Scale the outputted [logits](https://wandb.ai/amanarora/Written-Reports/reports/Understanding-Logits-Sigmoid-Softmax-and-Cross-Entropy-Loss-in-Deep-Learning--Vmlldzo0NDMzNTU3) according to the specified [temperature](https://twitter.com/cwolferesearch/status/1671628210180698112?s=20)
    
3. [Optional] Remove all but the `k` most likely tokens (i.e., [Top-K sampling](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p))
    
4. Apply the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function
    
5. Sample the next token from the resulting distribution
    

Notably, the forward pass within the code above uses the same exact forward pass we defined previously, but no target tensor is specified within the input!

#### NanoGPT Training

Although distributed training is a complex topic that we will not be able to cover thoroughly in this overview, we will cover the practical highlights of NanoGPT’s pretraining process for the purpose of completeness. We typically distribute LLM training across multiple compute devices (e.g., GPUs or [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)s). At a high level, there are a few reasons that distributed training is desirable and/or necessary:

- Pretraining is computationally expensive and we want to speed it up.
    
- The size of the model might be too big to store on a single device.
    

The second case outlined above is especially applicable to the current generation of language models, which are quite large and typically cannot be stored on a single device. A variety of distributed training techniques exist that can handle these cases and speed up the training process; see [here](https://twitter.com/rasbt/status/1625494398778892292?s=20) for a summary.

**Distributed training setup.** The full pretraining implementation is provided within the [train.py file](https://github.com/karpathy/nanoGPT/blob/master/train.py) within NanoGPT’s repository. The model is trained using either using a single GPU or with a [distributed data parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) approach. The setup of this training framework is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cf5757b-34ae-4f5f-b0bc-2b9bcba54b67_2168x1600.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cf5757b-34ae-4f5f-b0bc-2b9bcba54b67_2168x1600.png)

As we can see, training with DDP requires that we simultaneously run multiple training processes[7](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-7-136638774) that will communicate together. The number of processes is equal to the total number of GPUs that we have available (either on the same machine or across multiple nodes) for training. Using DDP, we can parallelize the training process across these GPUs. To coordinate the multiple processes that are running, we must specify a rank for each process. For example, if there are four total processes running training across four GPUs, these processes will each have a unique rank within the range [0, 3][8](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-8-136638774). In the code above, all rank information is stored within an [environment variable](https://towardsdatascience.com/environment-variables-python-aecb9bf01b85) that can be accessed by the process.

**Gradient accumulation.** Within the NanoGPT implementation, you might see the term gradient accumulation mentioned a few times. Typically, we train a neural network by:

- Computing the loss over a mini-batch of data
    
- Backpropagating this loss to derive a gradient
    
- Updating the model’s weights based on this gradient
    

Gradient accumulation removes the last step shown above. Instead, the gradient is _accumulated_ (i.e., by just taking an average) across multiple “micro-batches” of data that simulate a single, larger mini-batch. Once we have accumulated gradients across a sufficient amount of data, we update the weights. Such a process is useful when our desired batch size is too large for the hardware being used. We can simply compute the gradient over several smaller batches and use gradient accumulation to simulate the larger batch. See [here](https://kozodoi.me/blog/20210219/gradient-accumulation) for more details.

**What if we have a larger model?** With DDP, a copy of the model is sent to each device, and we train these copies of the model in parallel by _i)_ computing gradients over data that is randomly sampled on each device and _ii)_ getting an aggregated model update by synchronizing the gradients on each device after a mini-batch. For many modern LLMs, we might not be able to store the full model within the memory of a single device, so we need a different training approach. One of the most popular distributed training algorithms that is compatible with such large models is [fully sharded data parallel (FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) training [4]. This approach, as opposed to DDP, is more commonly used for training modern LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a2d7353-385e-4266-a667-5bdc345cf20e_2168x894.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a2d7353-385e-4266-a667-5bdc345cf20e_2168x894.png)

**Loading the data.** There are many ways in which we can create a [data loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for training a language model. One (simplified) example is shown within the code above. Here, the data is stored within a single file, and we have separate files for training and validation data. This data is loaded during training by simply taking random chunks with the size of the context window. We can optionally put this data onto the GPU, but the overall process is simple enough!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b057707-9c67-4a13-89ac-c7d1b2c8f342_2168x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b057707-9c67-4a13-89ac-c7d1b2c8f342_2168x930.png)

**The learning rate.** One of the main hyperparameters that we need to think about while pretraining a language model is the learning rate. Typically, we will adopt a [schedule](https://cameronrwolfe.substack.com/p/the-best-learning-rate-schedules) for the learning rate during pretraining. An example implementation of a typical learning rate schedule for language model pretraining is shown above. Here, the schedule has a short (linear) warm-up period followed by a (cosine) decay period that lasts for a specified number of iterations; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c86d2c-9d19-4026-afbc-60b9d0d1c9d3_640x480.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c86d2c-9d19-4026-afbc-60b9d0d1c9d3_640x480.png)

Cosine decay learning rate schedule used for language model pretraining

**The training loop.** Now that we have done all of the necessary setup, we can finally implement the actual (pre)training loop for our language model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86475da5-84a5-42c9-929f-3ca87927dbfd_2082x1936.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86475da5-84a5-42c9-929f-3ca87927dbfd_2082x1936.png)

There may be a few unfamiliar components in the implementation above (e.g., gradient clipping and loss scaling). Most of these changes are related to [automatic mixed precision (AMP)](https://pytorch.org/docs/stable/notes/amp_examples.html) training, which is a supported (but not mandatory) component of NanoGPT. Aside from these added details, the above code matches our prior discussion of the pretraining process and uses standard PyTorch syntax.

## Closing Thoughts

Reading papers about LLMs is fun and informative, but we can only go so far by just reading. Eventually, we have to implement these ideas if we want to build anything tangible. In this overview, we first learned about the idea of next token prediction and its application to causal language models. Then, we explored a concrete implementation of next token prediction for pretraining and inference with an LLM in PyTorch. Although this implementation is simple compared to some of the massive language models that are explored by current research, it lays a practical foundation that gives us a more concrete understanding of LLMs.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[2] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[3] Press, Ofir, and Lior Wolf. "Using the output embedding to improve language models." _arXiv preprint arXiv:1608.05859_ (2016).

[4] Ott, Myle, et al. "Fully sharded data parallel: faster ai training with fewer gpus." (2021).

[5] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[6] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[7] Press, Ofir, Noah A. Smith, and Mike Lewis. "Train short, test long: Attention with linear biases enables input length extrapolation." _arXiv preprint arXiv:2108.12409_ (2021).

[8] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[9] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[10] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." _arXiv preprint arXiv:2305.10601_ (2023).

[1](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-1-136638774)

In fact, I watched the first version of this course during my undergrad, when I was first learning about neural networks. It advanced my understanding significantly and made me capable of implementing a lot of the ideas that I would see in books or papers.

[2](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-2-136638774)

As we will see later, the token embeddings are part of the language model and are trained normally along with the rest of the model’s parameters.

[3](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-3-136638774)

This isn’t always the case. For example, we might be able to support a longer context length but choose to use a shorter context length because a longer context is not necessary for a certain application.

[4](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-4-136638774)

The GPT-2 publication studies multiple sizes of models, the largest of which contains roughly 1.5 billion parameters.

[5](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-5-136638774)

See [here](https://github.com/karpathy/nanoGPT/blob/master/model.py#L78) for the exact feed-forward network implementation by NanoGPT. The input to the feed-forward model is of size 768 (i.e., size of a single token embedding), while the hidden layer is `4X` wider than this.

[6](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-6-136638774)

Notably, the token embedding layer is huge! If we have a vocabulary of `V` tokens and use `d` dimensional vectors for each token, this layer has `V x d` parameters that are learned throughout pretraining. The next token prediction layer has the same exact number of parameters, so tying their weights together is highly beneficial.

[7](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-7-136638774)

We can just think of this as running the training script from multiple terminals at the same time

[8](https://cameronrwolfe.substack.com/p/language-model-training-and-inference#footnote-anchor-8-136638774)

We specify both rank and local rank. Rank corresponds to a process’ rank among all other processes. Notably, however, we might be running training across several compute nodes (e.g., across several servers, each of which have eight GPUs). Local rank corresponds to the rank of a process on its individual node.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Shane Robinson's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6d12b9d4-d5b0-4698-ac76-b52f86955b76_144x144.png)



](https://substack.com/profile/6780513-shane-robinson)

[

![C K's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10e10540-4a04-4061-8581-06b42a312acb_96x96.png)



](https://substack.com/profile/165328210-c-k)

[

![林煒清's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc965b698-e736-43c7-8d77-6d9a1eee99d1_96x96.jpeg)



](https://substack.com/profile/3322218-679771526e05)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Unnat Bak (UB)'s avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F26b7e920-73dc-4cad-806e-5decf90daea0_1250x1250.png)



](https://substack.com/profile/100940110-unnat-bak-ub)

35 Likes∙

[6 Restacks](https://substack.com/note/p-136638774/restacks?utm_source=substack&utm_content=facepile-restacks)

35

- 

[

5

](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comments)

6

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Atul Deshpande's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fyellow.png)



](https://substack.com/profile/85836201-atul-deshpande?utm_source=comment)

[Atul Deshpande](https://substack.com/profile/85836201-atul-deshpande?utm_source=substack-feed-item)

[2023年10月14日](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comment/41804102 "2023年10月14日 02:09")

Liked by Cameron R. Wolfe, Ph.D.

Cameron, by all means, your blog on best LLM/DL blog written on entire Substack (I would rate higher than many paid AI newsletters).

Every blog written by you so neatly written, and anyone can understand complex topics so easily. I’m LLM and Deep Learning beginner but I refer your articles day in day out to learn more. Pls keep it up. Thanks.

Like (5)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comment/41804102)

[

![Phan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c7db268-b888-429a-83f7-34d97e710022_144x144.png)



](https://substack.com/profile/85885729-phan?utm_source=comment)

[Phan](https://substack.com/profile/85885729-phan?utm_source=substack-feed-item)

[2024年4月19日](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comment/54323870 "2024年4月19日 23:43")

Liked by Cameron R. Wolfe, Ph.D.

Your blog posts about AI and deep learning are very interesting and helpful. Thank you a lot.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comment/54323870)

[3 more comments...](https://cameronrwolfe.substack.com/p/language-model-training-and-inference/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Understanding and Using Supervised Fine-Tuning (SFT) for Language Models

### Understanding how SFT works from the idea to a working implementation...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Sep 11, 2023

55

- 

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

4

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a4b239-ce73-427a-9656-b6de09ea26cf_2206x1180.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02a4b239-ce73-427a-9656-b6de09ea26cf_2206x1180.png)

(from [5])

Large language models (LLMs) are typically trained in several stages, including pretraining and several fine-tuning stages; see below. Although [pretraining is expensive](https://www.mosaicml.com/blog/gpt-3-quality-for-500k) (i.e., several hundred thousand dollars in compute), fine-tuning an LLM (or performing in-context learning) is cheap in comparison (i.e., several hundred dollars, or less). Given that high-quality, pretrained LLMs (e.g., [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source), or [LLAMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)) are widely available and free to use (even commercially), we can build a variety of powerful applications by fine-tuning LLMs on relevant tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png)

Different stages of training for an LLM

One of the most widely-used forms of fine-tuning for LLMs within recent AI research is supervised fine-tuning (SFT). This approach curates a dataset of high-quality LLM outputs over which the model is directly fine-tuned using a standard language modeling objective. SFT is simple/cheap to use and a useful tool for aligning language models, which has made is popular within the open-source LLM research community and beyond. Within this overview, we will outline the idea behind SFT, look at relevant research on this topic, and provide examples of how practitioners can easily use SFT with only a few lines of Python code.

## Useful Background Information

To gain a deep understanding of SFT, we need to have a baseline understanding of language models (and deep learning in general). Let’s cover some relevant background information and briefly refresh a few ideas that will be important.

**AI Basics.** In my opinion, the best resource for learning about AI and deep learning fundamentals is the _[Practical Deep Learning for Coders](https://course.fast.ai/)_ course from [fast.ai](https://www.fast.ai/). This course is extremely practical and oriented in a top-down manner, meaning that you learn how to implement ideas in code and use all the relevant tools first, then dig deeper into the details afterwards to understand how everything works. If you’re new to the space and want to quickly get a working understanding of AI-related tools, how to use them, and how they work, start with these videos.

**Language models.** SFT is a popular fine-tuning technique for LLMs. As such, we need to have a baseline understanding of language models. The resources below can be used to gain a quick understanding of how these models work:

- _Transformer Architecture_ [[link](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom)]: Nearly all modern language models—_and many other deep learning models_—are based upon this architecture.
    
- _Decoder-only Transformers_ [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]: This is the specific variant of the transformer architecture that is used by most generative LLMs.
    
- _Brief History of LLMs_ [[link](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)]: LLMs have gone through several phases from the creation of [GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt) [1] to the release of ChatGPT.
    
- _Next token prediction_ [[link](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction)]: this [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) training objective underlies nearly all LLM functionality and is used by SFT!
    
- _Language Model Pretraining_ [[link](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining)]: language models are pretrained over a massive, unlabeled textual corpus.
    
- _Language Model Inference_ [[link](https://cameronrwolfe.substack.com/i/136638774/autoregressive-inference-process)]: language models can be used to generate coherent sequences of text via autoregressive next token prediction.
    

**Transformers library.** The code in this overview relies upon the [transformers library](https://huggingface.co/docs/transformers/index), which is one of the most powerful deep learning libraries out there. Plus, the library has a ton of tutorials and documentation that serve as a practical learning resource for any deep learning or LLM-related project.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb9d0144-3952-42db-8382-8e2eb37d917e_1670x640.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb9d0144-3952-42db-8382-8e2eb37d917e_1670x640.png)

(from [2])

**Training LLMs.** The training process for language models typically proceeds in three phases; see above. First, we pretrain the language model, which is (by far) the most computationally-expensive step of training. From here, we perform alignment, typically via the [three-step framework](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior) (see below) with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)[1](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-1-136815345).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)

(from [2])

The steps outlined above form the standardized training pipeline that is used for most state-of-the-art LLMs (e.g., ChatGPT or LLaMA-2 [3]). SFT and RLHF are computationally cheap compared to pretraining, but they require the curation of a dataset—either of high-quality LLM outputs or human feedback on LLM outputs_—_which can be difficult and time consuming.

Sometimes we have to do a bit more when applying an LLM to solve a downstream task. In particular, we can further specialize a language model (if needed) either via domain-specific fine-tuning or [in-context learning](https://cameronrwolfe.substack.com/i/123558334/different-types-of-learning); see below. Domain-specific fine-tuning simply trains the model further—_usually via a [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling), similarly to pretraining/SFT_—on data that is relevant to the downstream task, while in-context learning adds extra context or examples into the language model’s prompt to be used as context for solving a problem.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5dd2fe9-0f4d-40d7-bc83-f00da6592de9_2396x466.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5dd2fe9-0f4d-40d7-bc83-f00da6592de9_2396x466.png)

(from [2, 4])

**What is alignment?** Finally, there is a term we have used several times in the above discussion that is important to understand: _alignment_. A pretrained language model is usually not useful. If we generate output with this model, the results will probably be repetitive and not very helpful. To create a more useful language model, we have to _align_ this model to the desires of the human user. In other words, instead of generating the most likely textual sequence, our language model learns to generate the textual sequence that is desired by a user.

> _“For our collection of preference annotations, we focus on helpfulness and safety. Helpfulness refers to how well Llama 2-Chat responses fulfill users’ requests and provide requested information; safety refers to whether Llama 2-Chat’s responses are unsafe.”_ - from [5]

Such alignment, which is accomplished via the three-step framework with SFT and RLHF outlined above, can be used to encourage a variety of behaviors and properties within an LLM. Typically, those training the model select a set of one or a few criteria that are emphasized throughout the alignment process. Common alignment criteria include: improving instruction following capabilities, discouraging harmful output, making the LLM more helpful, and many more. For example, [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [5] is aligned to be _i)_ helpful and _ii)_ harmless/safe; see above.

## What is SFT?

Supervised fine-tuning (SFT) is the first training step within the alignment process for LLMs, and it is actually quite simple. First, we need to curate a dataset of high-quality LLM outputs—_these are basically just examples of the LLM behaving correctly_; see below. Then, we directly fine-tune the model over these examples. Here, the “supervised” aspect of fine-tuning comes from the fact that we are collecting a dataset of examples that the model should emulate. Then, the model learns to replicate the style[2](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-2-136815345) of these examples during fine-tuning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbd7695-b32e-49a5-9d8b-dac180c767a1_1274x676.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbd7695-b32e-49a5-9d8b-dac180c767a1_1274x676.png)

(from [5])

**Relation to next token prediction.** Interestingly, SFT is not much different from [language model pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining)—_both pretraining and SFT use [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) as their underlying training objective_! The main difference arises in the data that is used. During pretraining, we use a massive corpus of raw textual data to train the model. In contrast, SFT uses a supervised dataset of high-quality LLM outputs. During each training iteration, we sample several examples, then fine-tune the model on this data using a next token prediction objective. Typically, the next token prediction objective is only applied to the portion of each example that corresponds to the LLM’s output (e.g., the response in the figure above).

#### Where did this come from?

The three-step alignment process—_including both SFT and RLHF_—was originally proposed by [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) [2] (though it was previously explored for summarization models in [21]), the precursor and sister model to [ChatGPT](https://openai.com/blog/chatgpt). Due to the success of both InstructGPT and ChatGPT, this three-step framework has become standardized and quite popular, leading to its use in a variety of subsequent language models (e.g., [Sparrow](https://cameronrwolfe.substack.com/i/93578656/improving-alignment-of-dialogue-agents-via-targeted-human-judgements) [4] and [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [6]). Alignment via SFT and RLHF is now used heavily in both research and practical applications.

**Fine-tuning before SFT.** Despite recent popularity of SFT, language model fine-tuning has long been a popular approach. For example, [GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt) [7] is fine-tuned directly on each task on which it is evaluated (see below), and encoder-only language models (e.g., [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [8])—_due to the fact that they are not commonly-used for generative tasks_—almost exclusively use a fine-tuning approach for solving downstream tasks. Furthermore, several LLMs have adopted fine-tuning approaches that are slightly different than SFT; e.g., [LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications) [9] fine-tunes on a variety of auxiliary tasks and [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code) [10] performs domain-specific fine-tuning (i.e., basically more pretraining on different data) on a code corpus.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60d46502-4340-48d7-8db6-057993f82060_1622x816.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60d46502-4340-48d7-8db6-057993f82060_1622x816.png)

(from [7])

Notably, SFT is slightly different than [generic fine-tuning](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.6-leveraging-pretrained-models-with-transfer-learning/). Typically, fine-tuning a deep learning model is done to teach the model how to solve a specific task, but it makes the model more specialized and less generic—_the model becomes a “[narrow expert](https://cameronrwolfe.substack.com/i/85568430/creating-foundation-models)”_. The model will likely solve the task on which it is fine-tuned more accurately compared to a generic model (e.g., see [GOAT](https://twitter.com/rasbt/status/1661754946625105920?s=20) [11]), but it may lose its ability to solve other tasks. In contrast, SFT is a core component of aligning language models, including generic [foundation models](https://crfm.stanford.edu/). Because we are fine-tuning the model to emulate a correct style or behavior, rather than to solve a particular task, it does not lose its generic problem solving abilities.

#### Pros and Cons of SFT

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bb96f2-21f3-4862-9870-0864c706e0ff_1980x1260.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bb96f2-21f3-4862-9870-0864c706e0ff_1980x1260.png)

(from [2])

SFT is simple to use—_the training process and objective are very similar to pretraining_. Plus, the approach is highly effective at performing alignment and—relative to pretraining—is computationally cheap (i.e, `100X` less expensive, if not more). As shown in the figure above, using SFT alone (i.e., without any RLHF) yields a clear benefit in terms of the model’s instruction following capabilities, correctness, coherence, and overall performance[3](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-3-136815345). In other words, SFT is a highly effective technique for improving the quality of a language model. However, we should keep in mind that it is not perfect! Here are a few downsides we should consider.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)

(from [12])

**Creating a dataset.** The results of SFT are heavily dependent upon the dataset that we curate. If this dataset contains a diverse set of examples that accurately capture all relevant alignment criteria and characterize the language model’s expected output, then SFT is a great approach. However, _how can we guarantee that the dataset used for SFT comprehensively captures all of the behaviors that we want to encourage during the alignment process?_ This can only be guaranteed through careful manual inspection of data, which is _i)_ not scalable and _ii)_ usually expensive. As an alternative, recent research has explored automated frameworks of generating datasets for SFT (e.g., [self instruct](https://cameronrwolfe.substack.com/i/125726849/the-self-instruct-framework) [12]; see above), but there is no guarantee on the quality of data. As such, SFT, despite its simplicity, requires the curation of a high-quality dataset, which can be difficult.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)

(from [5])

**Adding RLHF is beneficial.** Even after curating a high-quality dataset for SFT, recent research indicates that further benefit can be gained by performing RLHF. In other words, _fine-tuning a language model via SFT alone is not enough_. This finding was especially evident in the recent [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [5] publication, which performs alignment via both SFT and RLHF; see above. For SFT, LLaMA-2 uses a large (27,540 examples in total) dataset of dialogue sessions that were manually curated to ensure quality and diversity. Despite using a large and high-quality source of data for SFT, performing further RLHF yields massive benefits in terms of helpfulness and safety (i.e., the alignment criteria for LLaMA-2); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97eb6108-8b95-47a1-965e-85e8390d70b4_1892x672.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97eb6108-8b95-47a1-965e-85e8390d70b4_1892x672.png)

(from [5])

Furthermore, the authors note that, after SFT has been performed, the language model is capable of generating dialogue sessions of similar quality to those written by humans. As such, creating more data for SFT yields less of a benefit, as we can just automatically generate more data for SFT using the model itself.

> _“We found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF.”_ - from [5]

Put simply, the current consensus within the research community seems to be that the optimal approach to alignment is to _i)_ perform SFT over a moderately-sized dataset of examples with very high quality and _ii)_ invest remaining efforts into curating human preference data for fine-tuning via RLHF.

## Using SFT in Practice

Now that we understand the concept of SFT, let’s explore how this concept can be and has been used in both practical and research applications. First, we will look at an example of how we can perform SFT in Python. Then, we will overview several recent papers that have been published on the topic of SFT.

#### Implementation of SFT

As mentioned previously, the implementation of SFT is quite similar to language model pretraining. Under the hood, any implementation of SFT will use a [next token prediction](https://cameronrwolfe.substack.com/i/85568430/language-modeling) (also known as standard language modeling) objective, which we have already [learned about extensively](https://cameronrwolfe.substack.com/p/language-model-training-and-inference). In practice, one of the best tools that we can use for training an LLM with SFT is the [transformer reinforcement learning (TRL)](https://huggingface.co/docs/trl/index) Python library, which contains an implementation of SFT that can be used to fine-tune an existing language model with only a few lines of code.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73f5f528-93c4-4b1c-a212-b7b567e4bb32_1396x968.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73f5f528-93c4-4b1c-a212-b7b567e4bb32_1396x968.png)

**Performing SFT.** Built on top of the [HuggingFace transformers](https://huggingface.co/docs/transformers/index) library, TRL can train a language model (in this case, Meta’s [OPT model](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)) via SFT using the code shown above. This simple example demonstrates how easy training a model via SFT can be! Due to the simplicity, fine-tuning models via SFT has been incredibly popular within the open-source LLM research community. A quick visit to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) will show us a swath of interesting examples. Fine-tuning a pretrained LLM using SFT is currently one of the easiest and most effective ways to get your hands dirty with training open-source LLMs.

Beyond this basic definition of SFT, there are a few useful (and more advanced) techniques that we might want to use, such as applying supervision only on model responses (as opposed to the full dialogue or example), augmenting all response examples with shared prompt template, or even adopting a [parameter efficient fine-tuning (PEFT)](https://huggingface.co/blog/peft) approach (e.g., [LoRA](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html) [13]). Interestingly, the SFTTrainer class defined by TRL is adaptable and extensible enough to handle each of these cases. See the link below for more details on the implementation.

[Using SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)

#### SFT use cases in AI Research

Given that SFT is a standard component of the alignment process, it has been explored heavily within AI literature. We will overview several publications that have provided valuable insights on SFT. As always, the publications outlined below are not exhaustive. There are a massive number of papers on the topic of SFT (and AI in general). However, I’ve done my best to highlight some of the most valuable insights from the research community. If anything is missing, please feel free to share it in the comments for myself and others!

**InstructGPT.** The three part alignment process—_including SFT and RLHF_—used by most language models was first used by [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) [2], though it was previously explored for text summarization models in [21]. This publication laid the foundation for a lot of recent LLM advancements and contains many valuable insights into the alignment process. Unlike recent models proposed by OpenAI, the details of InstructGPT’s training process and architecture are fully-disclosed within the publication. As such, this model offers massive insight into the creation of powerful language models, and reading the blog posts for [ChatGPT](https://openai.com/blog/chatgpt) and [GPT-4](https://openai.com/research/gpt-4)[4](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-4-136815345) with this added context is much more informative.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd559d6-d5ce-4f5c-8b81-2e97e8f0b80a_2596x1418.png)

(from [17, 18, 19, 20])

**Imitation models.** During the recent [explosion of open-source language models](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) that followed the release of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone), SFT was utilized heavily within the imitation learning context. Namely, we could:

1. Start with an open-source base model.
    
2. Collect a dataset of dialogue sessions from a proprietary language model (e.g., ChatGPT or GPT-4).
    
3. Train the model (using SFT) over the resulting dataset.
    

These models (e.g., [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model), [Koala](https://cameronrwolfe.substack.com/i/114077195/koala-a-dialogue-model-for-academic-research), and [Vicuna](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality)) were cheap to train and performed quite well, highlighting that impressive results can be obtained with SFT using relatively minimal compute. Although early imitation models were later revealed to [perform poorly](https://cameronrwolfe.substack.com/p/imitation-models-and-the-open-source) compared to proprietary models, recent variants that are trained over larger imitation datasets (e.g., [Orca](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) [15]) perform well. Combining SFT with imitation learning is an cheap and easy way to make a decent LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff751858f-6f68-4f30-a7ba-d4c1ac13a4b4_1618x654.png)

(from [16])

**LIMA.** Research in imitation learning revealed that using proprietary language models to generate large datasets for SFT is a useful approach. In contrast, parallel research explored whether alignment could be achieved via smaller, carefully curated datasets. In [LIMA](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language) [16], authors curate a dataset of only 1K examples for SFT, and the resulting model is quite competitive with top open-source and proprietary LLMs; see above. In this case, the key to success is manual inspection of data to ensure both quality and diversity, which are found to be more important than the raw size of dataset used for SFT. These results are corroborated by LLaMA-2, where authors find that a moderately-sized dataset with high quality and diversity standards yields the best results for SFT.

**Open-source alignment.** Until the recent proposal of LLaMA-2 (and even afterwards), open-source LLMs were aligned using primarily SFT with minimal RLHF (if any). For example, several variants of the [MPT models](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), as well as the Instruct versions of [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) and [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) are created using SFT over a variety of different datasets (many of which are publicly available[5](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-5-136815345) on HuggingFace!). Plus, if we take a quick look at the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), we will see that a variety of the top models are versions of popular base models (e.g., LLaMA-2 or Falcon) that have been fine-tuned via SFT on a mix of different data. Notable examples of this include [Platypus](https://huggingface.co/papers/2308.07317), [WizardLM](https://arxiv.org/abs/2304.12244), [Airoboros](https://github.com/jondurbin/airoboros), [Guanaco](https://guanaco-model.github.io/), and more.

## Concluding Remarks

Within this overview, we have learned about SFT, how it can be used in practice, and what has been learned about it within current research. SFT is a powerful tool for AI practitioners, as it can be used to align a language model to certain human-defined objectives in a data-efficient manner. Although further benefit can be achieved via RLHF, SFT is simple to use (i.e., very similar to pretraining), computationally inexpensive, and highly effective. Such properties have led SFT to be adopted heavily within the open-source LLM research community, where a variety of new models are trained (using SFT) and released nearly every day. Given access to a high-quality base model (e.g., LLaMA-2), we can efficiently and easily fine-tune these models via SFT to handle a variety of different use cases.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[2] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[3] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[4] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[5] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[6] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[7] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[8] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[9] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." _arXiv preprint arXiv:2201.08239_ (2022).

[10] Chen, Mark, et al. "Evaluating large language models trained on code." _arXiv preprint arXiv:2107.03374_ (2021).

[11] Liu, Tiedong, and Bryan Kian Hsiang Low. "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks." _arXiv preprint arXiv:2305.14201_ (2023).

[12] Wang, Yizhong, et al. "Self-instruct: Aligning language model with self generated instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[13] Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." _arXiv preprint arXiv:2106.09685_ (2021).

[14] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[15] Mukherjee, Subhabrata, et al. "Orca: Progressive Learning from Complex Explanation Traces of GPT-4." _arXiv preprint arXiv:2306.02707_ (2023).

[16] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[17] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023).

[18] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023).

[19] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023).

[20] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023.

[21] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[1](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-anchor-1-136815345)

Interestingly, [the feedback](https://arxiv.org/abs/2309.00267) doesn’t need to be from humans for this step. Recent research is exploring reinforcement learning from AI feedback (RLAIF)!

[2](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-anchor-2-136815345)

Recent research on [LIMA](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) [6] has revealed that most of a language model’s knowledge is learned during pretraining, while the alignment process teaches the language model the correct style, behavior, or method of surfacing knowledge that it already has.

[3](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-anchor-3-136815345)

Obviously, this depends a lot on the quality of data that is used, as well as the alignment criteria that are defined for collecting this data.

[4](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-anchor-4-136815345)

GPT-4 also has a [technical report](https://arxiv.org/abs/2303.08774) with more details than the blog post, but it still does not fully disclose the details of the model architecture or training process. GPT-4 is, however, disclosed in detail within a recent SemiAnalysis publication.

[5](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised#footnote-anchor-5-136815345)

Notable examples of public SFT datasets include [Dolly15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [Baize](https://huggingface.co/project-baize/baize-lora-30B), [Ultrachat](https://huggingface.co/datasets/stingning/ultrachat), and more. Imitation-based datasets (e.g., for [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) and [Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna)) are also available. You can find these datasets within the model card of popular open-source LLMs

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Sowmya's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ec7f2fd-ff19-4b1a-8437-d9db190320c6_144x144.png)



](https://substack.com/profile/5440986-sowmya)

[

![Xinghao Wu's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41aa0a49-e699-4854-9ad8-931dac881540_144x144.png)



](https://substack.com/profile/21921005-xinghao-wu)

[

![Kaif Shaikh's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F492b52f5-119a-4ad8-a022-9ce2ccf1229e_144x144.png)



](https://substack.com/profile/70023998-kaif-shaikh)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Mehmet Ali's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd582a0c-3de6-41e7-8c03-9c49fe50f38b_144x144.png)



](https://substack.com/profile/76802542-mehmet-ali)

55 Likes∙

[4 Restacks](https://substack.com/note/p-136815345/restacks?utm_source=substack&utm_content=facepile-restacks)

55

- 

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sebastian Raschka, PhD's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)



](https://substack.com/profile/27393275-sebastian-raschka-phd?utm_source=comment)

[Sebastian Raschka, PhD](https://substack.com/profile/27393275-sebastian-raschka-phd?utm_source=substack-feed-item)

[Ahead of AI](https://magazine.sebastianraschka.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年9月12日](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comment/39957381 "2023年9月12日 20:49")

Liked by Cameron R. Wolfe, Ph.D.

Awesome article as always, Cameron!

I hope you don't mind my minor nit & correction: in your article, you mentioned in some places that InstructGPT originally proposed the three step process Pretraining -> SFT -> RLHF. As far as I know, that's not correct and the procedure was proposed 2 years earlier via the "Learning to summarize from human feedback" paper ([https://arxiv.org/abs/2009.01325](https://arxiv.org/abs/2009.01325)).

(PS: I have a list of a few additional PPO resources here: [https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives))

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comment/39957381)

[

![can kara's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0dd2641-79bd-4d47-b8d0-2d4f254438a2_144x144.png)



](https://substack.com/profile/105567451-can-kara?utm_source=comment)

[can kara](https://substack.com/profile/105567451-can-kara?utm_source=substack-feed-item)

[5月5日](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comment/55545947 "2024年5月5日 03:08")

I need an answer but can't really find it in here. Can a LLM learn new things buy fine-tuning? For example i need add some historical personas with their some informations that i know that the model hasn't seen these data during pre-training?

Like

Reply

Share

[1 reply](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comment/55545947)

[3 more comments...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# RLAIF: Reinforcement Learning from AI Feedback

### Making alignment via RLHF more scalable by automating human feedback...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Sep 18, 2023

23

- 

[

1

](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from/comments)

3

Share

[

![Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png "Deci AI Logo Vector Download - (.SVG + .PNG) - Logovectordl.Com")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6044b0b1-3002-4ce7-91d8-c752d896d340_900x500.png)

This newsletter is presented by [Deci AI](https://deci.ai/). Deci does a ton of interesting AI research. Most recently, they released [DeciLM-6B](https://huggingface.co/Deci/DeciLM-6b), an open-source foundation language model with an extremely efficient architecture. Read more about it [here](https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/).

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf036f92-2268-4998-9c70-c2c5a8da602f_2172x1330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf036f92-2268-4998-9c70-c2c5a8da602f_2172x1330.png)

(from [1, 2, 8])

Beyond using larger models and datasets for pretraining, the drastic increase in large language model (LLM) quality has been due to advancements in the alignment process, which is largely been fueled by finetuning techniques like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). RLHF in particular is an interesting technique, as it allows us to directly finetune a language model based on human-provided preferences. Put simply, we can just teach the model to produce outputs that humans prefer, which is a flexible and powerful framework. However, it requires that a large amount of human preference labels be collected, which can be expensive and time consuming. Within this overview, we will explore recent research that aims to automate the collection of human preferences for RLHF using AI, forming a new technique known as reinforcement learning from AI feedback (RLAIF).

## Background Information

This overview will study the alignment of language models via SFT, RLHF, and RLAIF. To understand these concepts, we need a working understanding of some relevant background concepts related to both LLMs and deep learning in general. We will briefly overview these concepts below and provide links for further reading so that those who are less familiar can go more in depth.

**The fundamentals.** In my opinion, the best resource for learning about deep learning fundamentals is the _[Practical Deep Learning for Coders](https://course.fast.ai/)_ course from [fast.ai](https://www.fast.ai/). This course is extremely practical and oriented in a top-down manner, meaning that you learn how to implement ideas in code and use all the relevant tools first, then dig deeper into the details afterwards to understand how everything works. If you are new to AI and don’t know what to learn first, start with these videos.

**Language models.** Beyond a basic understanding of deep learning, this overview will require some familiarity with language models. First, we need to learn about the transformer architecture—_more specifically the decoder-only transformer architecture_—that is used by nearly all generative language models; see below.

- _Transformer Architecture_ [[link](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom)]: Nearly all modern language models—_and many other deep learning models_—are based upon this architecture.
    
- _Decoder-only Transformers_ [[link](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)]: This is the specific variant of the transformer architecture that is used by most generative LLMs.
    

To learn more about the history of LLMs and how research in this area has progressed, we can also take a look at a few prior overviews listed below.

- _GPT and GPT-2_ [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]: these were some of the first language models proposed that match the style of models that we use today.
    
- _GPT-3 and Scaling Laws_ [[link](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]: these papers discovered that using large language models (LLMs) yields drastic performance improvements.
    
- _Modern LLMs_ [[link](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]: a variety of models were explored post GPT-3, but the best models were both _i)_ large and _ii)_ trained over tons of data.
    
- _Specialized LLMs_ [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)]: advancements in LLM research led to these powerful models being applied to a variety of domains and use cases.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba27b81a-b8b7-44c8-b408-937ab71841ec_1644x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba27b81a-b8b7-44c8-b408-937ab71841ec_1644x636.png)

(from [11])

**Training a language model.** The language model training process progresses in several phases; see above. First, we [pretrain](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) the model over a large corpus of unlabeled textual data, which is the most expensive part of training. After pretraining, we perform a three-part[1](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-1-136751520) [alignment](https://cameronrwolfe.substack.com/i/135824233/fine-tuning-process-and-llama-chat) process, including both supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF); see below. Alignment via SFT/RLHF was used in [15] for summarizing text with LLMs and explored for improving instruction following capabilities in generic LLMs by InstructGPT [11], the sister model to ChatGPT. This approach has since become standardized and is used by a variety of powerful models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ffa81-7b96-474f-832b-4be758e8d2e6_1176x638.png)

(from [11])

**More on RLHF.** Within this overview, we will primarily focus upon the RLHF phase of alignment, which finetunes the LLM directly on human feedback. Put simply, _humans identify outputs that they prefer, and the LLM learns to produce more outputs like this_. More specifically, we _i)_ obtain a set of prompts to use for RLHF, _ii)_ generate two or more responses to each prompt with our language model, and _iii)_ allow human annotators to rank responses based on their preferences. Using this dataset of human preferences, we can train a reward model, which is usually just a finetuned (potentially smaller) version of the LLM with an added regression head for predicting preference scores; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbc5c25-6828-484d-a41e-463010d51a34_1224x1266.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dbc5c25-6828-484d-a41e-463010d51a34_1224x1266.png)

Architecture of a reward model

We train this model over pairs of model responses where one response is preferred over the other. Using a [ranking loss](https://gombru.github.io/2019/04/03/ranking_loss/), we can train the model to predict accurate preference scores by just teaching it to rate preferred responses more highly than non-preferred responses. Once this model has been trained, RLHF finetunes the underlying LLM to generate outputs with higher preference scores using a reinforcement learning algorithm like [PPO](https://openai.com/research/openai-baselines-ppo), where the reward model is used to automate preference judgements in this process.

[Recent research](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation) has shown us that both SFT and RLHF are necessary for performing high-quality alignment. However, the exact implementation of these components varies a lot between publications. For a more detailed overview of RLHF and the many variants that exist, check out the article below!

[

![](https://substackcdn.com/image/fetch/w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AI

LLM Training: RLHF and Its Alternatives

I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials. RLHF is an integral part of the modern LLM training pipeline due to its ability to incorporate human preferences into the optimization landscape, which can improve the model's helpfulness and safety…

Read more

2 years ago · 74 likes · 2 comments · Sebastian Raschka, PhD

](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives?utm_source=substack&utm_campaign=post_embed&utm_medium=web)

## Automating RLHF with AI Feedback

> _“As AI systems become more capable, we would like to enlist their help to supervise other AIs.”_ - from [1]

Despite its effectiveness, RLHF requires a lot of human preference annotations to work well. For example, [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [12] is trained using >1M human preference annotations! We will now explore a method that mitigates this issue by using AI to automate preference annotations. More specifically, recent research has found that LLMs can generate accurate preference labels if prompted correctly. A variety of papers have explored this topic. First, we will explore a background paper that studies RLHF, followed by a few papers that propose alternative approaches for reinforcement learning from AI feedback (RLAIF).

#### [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862) [8]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8a082ca-15cf-4b3a-88d9-b55a6dea7678_1472x950.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8a082ca-15cf-4b3a-88d9-b55a6dea7678_1472x950.png)

(from [8])

In [8], authors train a language model to be helpful and harmless using RLHF. By following an iterative feedback approach that performs RLHF on a weekly basis with fresh data, authors in [8] find that they can train an LLM to be both helpful and harmless without comprising performance on any benchmarks and even improving performance on specialized tasks like coding or summarization; see above. Although human feedback is used to fine-tune the LLM in [8], this paper is the precursor to an AI-based feedback approach explored in [1] and, as such, provides necessary context for this overview. Interestingly, the human feedback dataset that is curated in [8] is freely available online and is extensively used in [1].

[HH-RLHF Dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf)

**Collecting data.** Feedback data is collected based on a model’s helpfulness and harmlessness, as judged by human annotators, on a prompt given to the model. Interestingly, we see in [8] that authors allow human annotators to interpret these terms loosely—_there are no detailed, itemized requirements written to further explain the meaning of helpful or harmless, allowing a large and diverse preference dataset to be collected_. Annotations for helpfulness and harmlessness are collected separately and with a different prompting approach:

- Helpful data is collected by asking humans to solicit help from the model with a text-based task (e.g., question answering, writing, discussion, etc.).
    
- Harmless data is collected by asking humans to adversarially probe[2](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-2-136751520) a model to get help with a harmful goal or use toxic language.
    

In all cases, two model responses are generated for each prompt, and the human annotator identifies the preferable response (i.e., a binary preference label) and a strength of preference based upon which response is more helpful or more harmless; see below. Authors spot check annotations to ensure that annotators that produce low-quality preference labels are removed from the dataset.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)

(from [8])

**Training setup.** The LLMs used in [8] have between 13M and 52B parameters and reflect those described in [9]. Experiments are performed with criteria-specific preference models (i.e., separate models for helpfulness and harmlessness)[3](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-3-136751520), as well as preference models that are trained over a mixture of helpful and harmless data. Given that helpfulness and harmfulness are often a tradeoff, we see in [8] that preference models trained on one criteria often perform poorly on the other.

> _“That is, helpfulness tends to increase harmfulness, since models are willing to obey pernicious requests, and conversely models trained to be harmless tend to be more evasive and generally less helpful.”_ - from [1]

Authors in [8] indicate that training preference models over a mixture of data that captures both alignment criteria can also achieve competitive performance. However, these models must be scaled to a certain size to achieve sufficient accuracy on their preference scores; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cccbcdd-6445-44b3-a0e0-01e7875aef04_1456x650.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cccbcdd-6445-44b3-a0e0-01e7875aef04_1456x650.png)

(from [8])

The tension between helpfulness and harmfulness for training preference models is more pronounced with smaller models—_larger preference models are better able to simultaneously capture both preference_s. Despite collecting the strength of each preference score, however, preference models in [8] are trained to just assign a better score to the preferable output via a [ranking loss](https://gombru.github.io/2019/04/03/ranking_loss/). After the preference model is trained, the LLM is optimized using [PPO](https://openai.com/research/openai-baselines-ppo)—_the most commonly-used RL algorithm for RLHF_ [10, 11]. Fresh data is collected iteratively, and the underlying LLM is finetuned over new data each week, as shown within the diagram below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53595ee9-c683-42f4-a715-db2e3e2c7953_1994x1022.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53595ee9-c683-42f4-a715-db2e3e2c7953_1994x1022.png)

(from [8])

**Does alignment degrade model quality?** Recently, there has been a lot of discussion about whether the alignment procedure degrades the overall accuracy of the underlying LLM. This question is deeply related to the tension between helpfulness and harmlessness—_avoiding harmful output may cause the model to be less helpful on certain problems_ (e.g., by avoiding the answer).

> _“A question that’s often raised about alignment training is whether it will compromise AI capabilities. We find that when RLHF is applied to large language models, the answer seems to be an almost-categorical no.”_ - from [8]

In [8], however, we see that finetuning with RLHF does not necessarily deteriorate performance across more generic natural language benchmarks; see below. While smaller models may see a slight deterioration, aligned models still perform quite well across other benchmarks. As such, _we learn that alignment does not always come at the cost of deteriorated performance on a broader set of tasks_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6ad259d-5c20-44e7-a0e4-1faac0d51451_1480x686.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6ad259d-5c20-44e7-a0e4-1faac0d51451_1480x686.png)

(from [8])

**Takeaways and analysis.** Although RLHF is not the focus of this overview, it is worthwhile to understand some of the major takeaways from [8], as they provide insight into the properties of RLHF and how it can be automated with AI (i.e., RLAIF). In particular, we learn a few useful lessons from this analysis:

- Smaller LLMs have an “_alignment tax_”—_their performance deteriorates on other benchmarks after alignment with RLHF_. However, larger models (13B and 52B parameters) have an opposite effect (i.e., an _“alignment bonus”_)!
    
- Alignment with RLHF is compatible with specialized language models. We can apply RLHF to models finetuned on code and it actually _improves_ their coding abilities!
    
- Larger preference models are better for making alignment more robust; see below.
    
- The iterative application of RLHF is effective[4](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-4-136751520). We can _i)_ collect new data, _ii)_ finetune the LLM with RLHF and _iii)_ redeploy this model to human annotators to collect more preference data on a weekly cadence.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62b4991a-f677-4ad4-86df-dde6cd322489_1472x928.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62b4991a-f677-4ad4-86df-dde6cd322489_1472x928.png)

(from [8])

#### [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) [1]

Research presented in [8] is quite interesting. RLHF is a powerful tool for aligning language models based on human feedback, but it is difficult to scale up! Aligning a language model with RLHF requires a lot of human preferences labels, usually `10X` more labels compared to a technique like SFT. For example, [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [3] uses 100,000 data points for SFT, but over 1,000,000 annotated examples are curated for RLHF; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728734d9-28f9-4372-b83c-2b0881466233_1974x1196.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728734d9-28f9-4372-b83c-2b0881466233_1974x1196.png)

With this in mind, we might wonder—_is there anyway to automate creation of human preference labels for RLHF?_ Arguably, training a reward model is a form of automation! This reward model is trained over human preference data, then used to generate preference labels during the reinforcement learning phase of RLHF. However, this process still requires the creation of human preference labels, which is expensive and time consuming.

> _“Results suggest that large language models may already be approaching the performance of crowdworkers in identifying and assessing harmful behavior, and so motivate using AI feedback.”_ - from [1]

In [1], authors set out with a goal of training a model that is helpful and harmless—_similarly to the model in [8]_—but their approach, called Constitutional AI, leverages AI-provided feedback for collecting harmful preference data[5](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-5-136751520) instead of humans. In other words, we completely remove human feedback for identifying harmful outputs in an attempt to make obtaining preferences or feedback for alignment with RLHF both more _scalable_ and _explainable_.

**Writing the LLM constitution.** In [1], human input for harmfulness is reduced to an extreme. In particular, 16 text-based principles are written (the constitution), which are then leveraged—_along with a some manually-curated examples for [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)_—to automate the collection of preference data for harmfulness. An example of a single principle from the constitution in [1] is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa249bf3e-8841-4588-bc21-d8a4a50425ad_1432x98.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa249bf3e-8841-4588-bc21-d8a4a50425ad_1432x98.png)

(from [1])

Not only does this approach allow high-quality preference data to be collected, but the principles used to guide the creation of those preferences are easy to read, explain, and understand. In this way, constitutional AI actually makes the alignment process more explainable, intuitive, and simple.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff2750a3-5ea6-40da-99ff-4b11fc305870_1184x656.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff2750a3-5ea6-40da-99ff-4b11fc305870_1184x656.png)

(from [1])

**The approach.** Constitutional AI uses both [SFT](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and reinforcement learning for language model alignment. Starting with an LLM that is purely helpful (i.e., it has no ability to avoid harmful output), we generate responses, which may be harmful, to a set of prompts, then repeat the following steps:

1. Randomly sample a single principle from the constitution.
    
2. Ask the model to critique its response based on this principle.
    
3. Ask the model to revise its response in light of this critique.
    

After this constitution-based refinement process (shown below) has been repeated multiple times for each prompt and response, we can finetune the LLM (using [SFT](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)) over the set of resulting responses to make its output much less harmful. The purpose of this initial supervised learning phase is to get the model “on distribution”, meaning that the model already performs relatively well and requires less exploration or training during the second phase of alignment.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1410489a-960a-40c0-b5b2-e2d8fb496023_1854x1168.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1410489a-960a-40c0-b5b2-e2d8fb496023_1854x1168.png)

(from [1])

After SFT has been performed, the LLM is further finetuned with reinforcement learning. This process is identical to RLHF, but we replace human preferences for harmlessness—_but not helpfulness, human annotations are still used for this criteria_—with feedback provided by a generic LLM. For each prompt in a dataset of harmful prompts, the underlying LLM, which has already undergone SFT, is used to generate two responses. Then, we generate a preference score using a generic language model (i.e., not the language model that has undergone SFT!) and the prompt template below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1b32a18-fdb7-4e0f-b7a3-d213b537d06b_1264x246.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1b32a18-fdb7-4e0f-b7a3-d213b537d06b_1264x246.png)

(from [1])

Again, we randomly sample a single principle from the constitution for each preference label that is created. All harmlessness preference labels in [1] are derived using a generic LLM via this multiple choice format. By taking and normalizing the [log probabilities](https://stackoverflow.com/questions/63334122/why-do-we-use-log-probability-in-deep-learning) of each potential response, we can create a dataset of soft preference labels. In particular, ~182K harmlessness preference examples are collected. Then, these are mixed with a dataset of ~135K helpfulness preference examples. From here, we can perform a procedure that is nearly identical to RLHF in [8], but human-provided harmlessness data is replaced with preference labels from an LLM—_reinforcement learning from AI feedback (RLAIF)_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb52e0df-5917-4ee9-8ed8-ce8af7b27e64_1172x690.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb52e0df-5917-4ee9-8ed8-ce8af7b27e64_1172x690.png)

(from [1])

**Better prompt engineering.** To improve the automated feedback provided via the approach described above, authors in [1] test some more advanced prompting techniques. First, utilizing [few-shot examples](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) within the critique and revision prompts used to generate examples for supervised learning is found to improve the quality of revised examples. Additionally, [chain of thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms#%C2%A7variants-of-cot-prompting) [13] is found to improve the the quality of revised responses. Finally, following a [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [14] approach that generates five responses via chain of thought prompting and averages the resulting preference labels yields a final, small performance boost; see above. Interestingly, however, chain of thought prompting seems to be less helpful during the reinforcement learning phase. For examples of prompts used in [1], check out the paper’s repository below.

[CAI Prompts](https://github.com/anthropics/ConstitutionalHarmlessnessPaper)

**Experimental setup.** In [1], we begin the alignment process with LLMs that are purely helpful—_these models are trained using RLHF over a dataset of human feedback that solely measures helpfulness._ Given these models as a starting point, our goal is to train a model that is both helpful and harmless. From [8], we know that such a model can be obtained via RLHF, but we are now trying to determine if a portion of the feedback can be automated via AI.

A dataset of ~182K red teaming prompts are collected, including 42K human written prompts and 140K prompts written by an LLM. All red team prompts are critiqued and revised four times prior to supervised learning, which uses both (AI-generated) harmless and (human written) helpful prompt and response pairs. Similarly, the reinforcement learning phase utilizes both AI-generated and human-generated preference feedback.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91af6a5-16d3-4f08-b003-ad590a045d67_1178x780.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91af6a5-16d3-4f08-b003-ad590a045d67_1178x780.png)

(from [1])

**Results and analysis.** We see in [1] that RLHF can be partially automated via AI-provided feedback with minimal performance degradation. Using AI-generated labels for harmlessness can still yield improvements in the underlying LLM’s harmlessness! As shown by the [Pareto frontier](https://www.youtube.com/watch?v=9sXEBzI1R5Q) in the above figure, constitutional AI can be used to finetune LLMs that achieve quite impressive levels of helpfulness and harmlessness. Plus, models trained in [1] are found to be less evasive, despite their harmlessness. Some other interesting takeaways include:

- Soft preference labels yield better results for reinforcement learning than hard (binary) preference labels.
    
- Performing two-part revisions (i.e., critique, then revise) for the supervised learning phase is less necessary for larger models, but drastically improves the quality of prompt and response pairs generated with smaller models.
    
- Increasing the number of principles in the constitution does not improve preference accuracy, but it does improve the diversity of responses (see below).
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc783b45-e817-46f2-a402-04c403018033_1186x458.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc783b45-e817-46f2-a402-04c403018033_1186x458.png)

(from [1])

#### **[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267) [2]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff631bf2-7bdd-4be8-9083-5f5982f53493_1180x658.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff631bf2-7bdd-4be8-9083-5f5982f53493_1180x658.png)

In [1], we see that the collection of human preference labels can at least be partially automated by using hybrid feedback from both humans and AI. However, full automation of feedback for RLHF was not explored until [2], where authors propose reinforcement learning from AI feedback (RLAIF). This technique is identical to RLHF, but it automates the creation of human preference labels with an off-the-shelf LLM[6](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-6-136751520). Although RLAIF in [2] is explored specifically for text summarization tasks, we see that the technique yields similar results compared to RLHF, indicating that the alignment process can be automated via feedback provided by a generic language model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0e55b03-4f72-4b3d-b958-c0f36a9e2ed0_1242x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0e55b03-4f72-4b3d-b958-c0f36a9e2ed0_1242x782.png)

(from [2])

**Automating preference labels.** To generate preference labels with an off-the-shelf LLM, authors just use the prompt template shown above. This template contains several components, as enumerated below:

- _Preamble_: instructions that describe the task.
    
- _Few-shot examples_: (optional) examples of text with correct preference scores.
    
- _Sample to annotate_: input text and pair of summaries to be labeled with preference scores.
    
- _Ending_: ending string that prompts the model to generate preference scores (e.g., “Preferred Summary =”).
    

Although a few different styles of preambles are tested in [2], the standard approach for creating AI-generated preference labels follows the structure described above. Instead of creating binary preference labels, however, authors in [2] collect the [log probability of tokens](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20) corresponding to each of the potential preference outputs (i.e., whether summary one or summary two is preferred), apply a softmax, and use this as a “soft” preference distribution.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8126a5c1-b7c4-4ae0-9d06-ec69f9221151_900x370.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8126a5c1-b7c4-4ae0-9d06-ec69f9221151_900x370.png)

(from [2])

To generate all preference labels, authors use the [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)-2 [4, 5] model from Google. Although multiple sizes of PaLM-2 exist, we learn in [2] that the quality of preference annotations improves as larger models are used; see above. As such, using a larger, generic PaLM-2 model for generating preference labels is best. The process of creating AI-generated preference labels is depicted below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9be159-bb8f-4ceb-a295-b8120f6e8677_1546x872.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e9be159-bb8f-4ceb-a295-b8120f6e8677_1546x872.png)

(from [2])

**Advanced prompting techniques.** Beyond the prompting framework outlined above, experiments are performed using more [advanced prompting techniques](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering), including [few-shot prompting](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning), [chain of thought (CoT) prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [6], and [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting) [7]. Interestingly, CoT prompting is found to be beneficial in producing accurate preference labels when a [two-stage approach](https://cameronrwolfe.substack.com/i/136366740/multi-modal-cot-reasoning)—_where we first generate a rationale, then concatenate this rationale with our input to generate a final preference label_—is adopted. However, few-shot exemplars and self-consistency are not found to benefit the quality of preference labels. The results of analysis with different prompting techniques is provided in the tables above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2da3363-b623-4754-903d-3ac1e3f05792_1578x808.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2da3363-b623-4754-903d-3ac1e3f05792_1578x808.png)

(from [2])

**Experimental setup.** The models trained in [2] follow a standard, three-stage approach that includes SFT[7](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-7-136751520) and RLHF. However, the RLHF component is replaced with RLAIF. In other words, we just generate preference feedback using the generic PaLM-2 model rather than a human, and everything else is exactly the same. Again, analysis in [2] solely considers a text summarization task, which is based on the [TL;DR dataset](https://huggingface.co/datasets/webis/tldr-17) (i.e., a dataset of Reddit posts from different communities that are paired with summaries written by the same author), as well as a corresponding [human preference dataset](https://huggingface.co/datasets/openai/summarize_from_feedback)—_curated by OpenAI_—that is created using data from TL;DR. To evaluate different summarization models, we use the following metrics:

- _AI labeler alignment_: measures how well AI-provided preferences labels align with human-provided preference labels.
    
- _Pairwise accuracy_: measures how accurate a trained reward model is with respect to a held-out set of human preferences.
    
- _Win rate_: measures how often one model is preferred by humans over another.
    

Beginning with a pretrained base model, we first train this model via SFT on a high-quality dataset of summarization examples from TL;DR. From here, two separate models are trained—_using either RLHF or RLAIF_—based on the human preference dataset created on top of TL;DR. The analysis in [2] compares the quality of the three resulting models (SFT, SFT+RLHF, and SFT+RLAIF), as well as the quality of preference labels produced by humans versus LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c256d07-67a2-4ce2-ae94-fb19c9301c0c_778x802.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c256d07-67a2-4ce2-ae94-fb19c9301c0c_778x802.png)

(from [2])

**Does RLAIF perform well?** When we compare the quality of models trained via SFT, SFT+RLHF, and SFT+RLAIF, we immediately learn that both RLHF and RLAIF-based models consistently outperform models trained solely via SFT; see above. In other words, further fine-tuning via either RLHF or RLAIF yields a clear—_and seemingly equal_—benefit in terms of performance. When we directly compare models fine-tuned via RLHF and RLAIF, we see that the win rate is (roughly) 50%, meaning that both models are equal in performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca0f6c6-1c5b-4827-8e9d-4532fd43c9f4_1536x798.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca0f6c6-1c5b-4827-8e9d-4532fd43c9f4_1536x798.png)

(from [2])

Going further, RLHF and RLAIF-based summaries are preferred over human written references summaries in 80% of cases. A qualitative comparison of outputs generated by each of the three different models is provided above. Overall, we see in [2] that—_on the task of text summarization_—finetuning language models with AI-generated feedback is highly effective. In fact, RLAIF seems to yield models that roughly match the performance of models trained via RLHF, indicating that RLAIF is an alternative to RLHF with appealing scaling properties in terms of required human annotation costs.

> _“We show that RLAIF can produce comparable improvements to RLHF without depending on human annotators.”_ - from [2]

## Takeaways

In many ways, alignment is the bedrock of modern advancements in language models. Creating a truly remarkable language model requires aligning this model’s output to the desires of a human user—_accurately predicting the most probable next token is simply not enough_. Going further, RLHF is a standardized, incredibly important component of the alignment process. Notably, RLHF has a major limitation—_it requires a ton of human preference data to be collected for it to work well_. This weakness is (arguably) the main reason that RLHF has not been as heavily explored within areas like open-source LLM research. RLHF requires human and monetary resources that are not always readily available.

Within this overview, we have learned about active directions of research that are aiming to mitigate this weakness by automating preference labeling with generic language models. We have seen that leveraging foundation models for such applications is quite effective and allows us to create aligned language models that have comparable quality relative to RLHF and require minimal (or no) human supervision. Applying RLAIF successfully is largely a prompt engineering problem—_we need to write prompts that can generate accurate preference labels_. However, we see in works like [1] and [2] that automating data cleaning and annotation tasks with LLMs is incredibly effective. Seemingly, AI-provided feedback is an extremely promising direction for improving the accessibility and effectiveness of alignment research for language models.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[2] Lee, Harrison, et al. “RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.” _arXiv preprint arXiv:2309.00267_ (2023).

[3] “Introducing Llama2: The next generation of our open source large language model”, _Meta_, https://ai.meta.com/llama/.

[4] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[5] Anil, Rohan, et al. "Palm 2 technical report." _arXiv preprint arXiv:2305.10403_ (2023).

[6] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[7] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

[8] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).

[9] Askell, Amanda, et al. "A general language assistant as a laboratory for alignment." _arXiv preprint arXiv:2112.00861_ (2021).

[10] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[11] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[12] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[13] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." _Advances in Neural Information Processing Systems_ 35 (2022): 24824-24837.

[14] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

[15] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[1](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-1-136751520)

It may be a bit confusing that the three-part alignment only contains SFT and RLHF. However, RLHF has two internal phases (i.e., training the reward model and optimizing the LLM via RL), which makes three phases in total.

[2](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-2-136751520)

This is also commonly referred to as “red teaming” in AI literature.

[3](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-3-136751520)

This approach is similar to [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [12], which uses separate reward models for each of their alignment criteria to avoid issues with conflicting objectives.

[4](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-4-136751520)

Several notable LLMs have since adopted a similar approach; e.g., [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [12] is finetuned with several rounds of RLHF with fresh data.

[5](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-5-136751520)

Notably, human labeling is still used to collect helpful-based preference data. The same [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset from [8] is used for this.

[6](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-6-136751520)

An off-the-shelf LLM is defined in [2] as a model that is _“pre-trained or instruction-tuned for general usage but not fine-tuned for a specific downstream task”_.

[7](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from#footnote-anchor-7-136751520)

Unlike [1], examples for SFT are not generated using an AI-based critique and refine framework in [2].

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Shyam Peri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa52e6507-3b1d-4353-be93-f409111bc4e2_96x96.jpeg)



](https://substack.com/profile/4926723-shyam-peri)

[

![KatyK's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd52b3c8e-d304-4ed4-bbe6-4bdc3693dc41_144x144.png)



](https://substack.com/profile/151101676-katyk)

[

![Sukjoon Kim's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f0d8cf3-4c50-45d5-bff6-42f7a200022e_500x500.jpeg)



](https://substack.com/profile/10353543-sukjoon-kim)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

23 Likes∙

[3 Restacks](https://substack.com/note/p-136751520/restacks?utm_source=substack&utm_content=facepile-restacks)

23

- 

[

1

](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sam's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d2c2a42-ebd9-437b-a596-36f04e639689_144x144.png)



](https://substack.com/profile/12042942-sam?utm_source=comment)

[Sam](https://substack.com/profile/12042942-sam?utm_source=substack-feed-item)

[2024年4月8日](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from/comment/53436030 "2024年4月8日 13:34")

Small typo

"harmless without comprising performance" to "harmless without compromising performance"

Great article, thanks!

Like

Reply

Share

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----

[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Basics of Reinforcement Learning for LLMs

### Understanding the problem formulation and basic algorithms for RL..

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Sep 25, 2023

171

- 

[

5

](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comments)

14

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2ae3102-9da4-4fd5-ab6f-a923aa89b7b4_2310x1330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2ae3102-9da4-4fd5-ab6f-a923aa89b7b4_2310x1330.png)

(from [2])

Recent AI research has revealed that reinforcement learning—_more specifically, [reinforcement learning from human feedback (RLHF)](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)_—is a key component of training a state-of-the-art large language model (LLM). Despite this fact, most open-source research on language models heavily emphasizes supervised learning strategies, such as [supervised fine-tuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised). This lack of emphasis upon reinforcement learning can be attributed to several factors, including the necessity to curate human preference data or the amount of data needed to perform high-quality RLHF. However, one undeniable factor that likely underlies skepticism towards reinforcement learning is the simple fact that it is not as commonly-used compared to supervised learning. As a result, AI practitioners (including myself!) avoid reinforcement learning due to a simple lack of understanding—_we tend to stick with using the approaches that we know best_.

> _“Many among us expressed a preference for supervised annotation, attracted by its denser signal… However, reinforcement learning proved highly effective, particularly given its cost and time effectiveness.”_ - from [8]

**This series.** In the next few overviews, we will aim to eliminate this problem by building a working understanding of reinforcement learning from the ground up. We will start with basic definitions and approaches—_covered in this overview_—and work our way towards modern algorithms (e.g., [PPO](https://openai.com/research/openai-baselines-ppo)) that are used to finetune language models with RLHF. Throughout this process, we will explore example implementations of these ideas, aiming to demystify and normalize the use of reinforcement learning in the language modeling domain. As we will see, these ideas are easy to use in practice if we take the time to understand how they work!

### What is Reinforcement Learning?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2df33399-d0ac-4582-8b96-fb9ebff374ea_2404x816.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2df33399-d0ac-4582-8b96-fb9ebff374ea_2404x816.png)

Comparison of supervised and reinforcement learning

At the highest level, reinforcement learning (RL) is just another way of training a machine learning model. In prior overviews, we have seen a variety of techniques for training neural networks, but the two most commonly-used techniques for language models are supervised and [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) learning.

**(Self-)Supervised Learning.** In supervised learning, we have a dataset of inputs (i.e., a sequence of text) with corresponding labels (e.g., a classification or completion of the input text), and we want to train our model to accurately predict those labels from the input. For example, maybe we want to finetune a language model (e.g., [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)) to classify sentences that contain explicit language. In this case, we can obtain a dataset of sentences with binary labels indicating whether the sentence contains explicit language or not. Then, we can train our language model to classify this data correctly by iteratively:

1. Sampling a mini-batch of data from the dataset.
    
2. Predicting the labels with the model.
    
3. Computing the loss (e.g., [CrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).
    
4. [Backpropagating](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd) the gradient through the model.
    
5. Performing a weight update.
    

Self-supervised learning is similar to the setup explained above, but there are no explicit labels within our dataset. Rather, the “labels” that we use are already present within the input data. For example, language models are pretrained with a self-supervised [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling) that trains the model to predict the next token given prior tokens as input. Here, the next token is already present within the data (assuming that we have access to the full textual sequence[1](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-1-137266538)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)

(from [2])

**When is RL useful?** Although RL is just another way of training a neural network, the training setup is different compared to supervised learning. Similarly to how humans learn, RL trains neural networks through trial and error. More specifically, the neural network will produce an output, receive some feedback about this output, then learn from the feedback. For example, when finetuning a language model with [reinforcement learning from human feedback (RLHF)](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives), the language model produces some text then receives a score/reward from a human annotator[2](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-2-137266538) that captures the quality of that text; see above. Then, we use RL to finetune the language model to generate outputs with high scores.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59f028ed-2581-4023-a14b-e599e1c47a3f_2158x856.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59f028ed-2581-4023-a14b-e599e1c47a3f_2158x856.png)

The environment is not differentiable within RL

In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. _Why?_ Well, the score that we get from the human is a bit of a black box. There’s no easy way for us to explain this score or connect it mathematically to the output of the neural network. In other words, _we cannot backpropagate a loss applied to this score through the rest of the neural network_. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text; see above.

**Big picture.** The above discussion starts to provide us with insight as to why RL is such a beautiful and promising learning algorithm for neural networks. RL allows us to learn from signals that are non-differentiable and, therefore, not compatible with supervised learning. Put simply, _this means that we can learn from arbitrary feedback on a neural network’s output_! In the case of RLHF, we can score the outputs generated by a language model according to any principle that we have in mind. Then, we can use RL to learn from these scores, no matter how we choose to define them! In this way, we can teach a language model to be helpful, harmless, honest, more capable (e.g., by using tools), and much more.

## A Formal Framework for RL

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd23b56bc-f8df-4938-acf8-ad840587a142_1904x978.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd23b56bc-f8df-4938-acf8-ad840587a142_1904x978.png)

The agent acts and receives rewards (and new states) from the environment

Problems that are solved via RL tend to be structured in a similar format. Namely, we have an _agent_ that is interacting with an _environment_; see above. The agent has a _state_ in the environment and produces actions, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. The agent’s goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent! Rather, _rewards may have a long horizon_, meaning that it takes several correct, consecutive actions to generate any positive reward.

#### Markov Decision Process (MDP)

To make things more formal and mathematically sound, we can formulate the system described above as a [Markov Decision Process (MDP)](https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da). Within an MDP, we have states, actions, rewards, transitions, and a policy; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3afd6c9-32a5-414d-9c89-40d1c3ed502b_2266x412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3afd6c9-32a5-414d-9c89-40d1c3ed502b_2266x412.png)

Components of an MDP

States and actions have discrete values, while rewards are real numbers. In an MDP, we define two types of functions: transition and policy functions. The policy takes a state as input[3](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-3-137266538), then outputs a probability distribution over possible actions. Given this output, we can make a decision for the action to be taken from a current state, and the transition is then a function that outputs the next state based upon the prior state and chosen action. Using these components, the agent can interact with the environment in an iterative fashion; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fedf33673-4c64-49a2-bd7a-d722dbf5e566_1700x934.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fedf33673-4c64-49a2-bd7a-d722dbf5e566_1700x934.png)

Structure of an MDP

One thing we might be wondering here is: _What is the difference between the agent and the policy?_ The distinction is a bit nuanced. However, we can think of the agent as _implementing_ the policy within its environment. The policy describes how the agent chooses its next action given the current state. The agent follows this strategy as it interacts with the environment, and our goal is to learn a policy that maximizes the reward that the agent receives from the environment.

As the agent interacts with the environment, we form a “trajectory” of states and actions that are chosen throughout this process. Then, given the reward associated with each of these states, we get a total return given by the equation below, where γ is the [discount factor](https://intuitivetutorial.com/2020/11/15/discount-factor/) (more explanation coming soon). This return is the summed reward across the agent’s full trajectory, but rewards achieved at later time steps are exponentially discounted by the factor γ; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4af64-1440-40ad-b478-c469592ce536_1758x466.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4af64-1440-40ad-b478-c469592ce536_1758x466.png)

Trajectory and the return

The goal of RL is to train an agent that maximizes this return. As shown by the equation below, we can characterize this as finding a policy that maximizes the return over trajectories that are sampled from the final policy[4](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-4-137266538).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e7cd87-0bda-4786-8d5e-f4e03ee40791_1176x528.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e7cd87-0bda-4786-8d5e-f4e03ee40791_1176x528.png)

Objective being solved by RL

**Example application.** As a simplified example of the setup described above, let’s consider training a neural network to navigate a `2 X 3` grid from some initial state to some final state; see below. Here, we see in the grid that the agent will receive a reward of +10 for reaching the desired final state and a reward of -10 whenever it visits the red square.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e569ecd-6761-4c22-8181-f32785c69418_1108x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e569ecd-6761-4c22-8181-f32785c69418_1108x726.png)

A simplistic RL environment

Our environment is the `2 X 3` grid and the state is given by the current position within this grid—_we can represent this as a [one-hot vector](https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/)_. We can implement our policy with a [feed-forward neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)[5](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-5-137266538) that takes the current one-hot position as input and predicts a probability distribution over potential actions (i.e., move up, move down, move left, move right). For each chosen action, the transition function simply moves the agent to the corresponding next position on the grid and avoids allowing the agent to move out of bounds. The optimal agent learns to reach the final state without passing through the red square; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cf1458d-2060-4c82-bf2f-87619f0d66a4_1094x712.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cf1458d-2060-4c82-bf2f-87619f0d66a4_1094x712.png)

The optimal (largest return) solution path

Like many problems that are solved with RL, this setup has an environment that is _not differentiable_ (i.e., we can’t compute a gradient and train the model in a supervised fashion) and contains long-term dependencies, meaning that we might have to learn how to perform several sequential actions to get any reward.

**Great… but how does this apply to language models?** The application described above is a traditional use case for RL, including an agent/policy that learns to interact with an external (potentially simulated) environment. There are numerous examples of such successful applications of RL; e.g., [Atari](https://www.youtube.com/watch?v=V1eYniJ0Rnk) [3], [Go](https://www.deepmind.com/research/highlighted-research/alphago), autonomous driving [4] and more. However, RL has recently been leveraged for finetuning language models. Although this is a drastically different use case, the components discussed above can be easily translated to language modeling!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)

Next token prediction with a language model

As has been discussed extensively in prior overviews, language models specialize in performing [next token prediction](https://cameronrwolfe.substack.com/p/language-model-training-and-inference); see above. In other words, our language model takes several tokens as input (i.e., a prefix) and predicts the next token based on this context. When generating text at inference time, this is done autoregressively, meaning that the language model continually:

1. Predicts the next token.
    
2. Adds the next token to the current input sequence.
    
3. Repeats.
    

To view this setup from the lens of RL, we can consider our language model to be the policy. Our state is just the current textual sequence. Given this state as input, the language model can produce an action—_the next token_—that modifies the current state to produce the next state—_the textual sequence with an added token_. Once a full textual sequence has been produced, we can obtain a reward by rating the quality of the language model’s output, either with a human or a [reward model](https://cameronrwolfe.substack.com/i/136751520/background-information) that has been trained over human preferences.

Although this setup is quite different from learning to navigate a simple grid (i.e., the model, data modality, environment, and reward function are all completely different!), we begin to see that the problem formulation used for RL is quite generic. _There are many different problems that we can solve using this approach_!

#### Important Terms and Definitions

Now that we understand the basic setup that is used for RL, we should overview some of the common terms we might see when studying this area of research. We have outlined a few notable terms and definitions below.

**Trajectory:** A trajectory is simply a sequence of states and actions that describe the path taken by an agent through an environment.

**Episode:** Sometimes, the environment we are exploring has a well-defined end state; e.g., reaching the final destination in our `2 X 3` grid. In these cases, we refer to the trajectory of actions and states from start to end state as an episode.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a9833ab-561f-42f2-892a-4b3e1d054f81_700x402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a9833ab-561f-42f2-892a-4b3e1d054f81_700x402.png)

Discounting rewards when computing the return

**Return:** Return is just the reward summed over an entire trajectory. As shown above, however, this sum typically includes a _discount factor_. Intuitively, this means that current rewards are more valuable than later rewards, due to both uncertainty and the simple fact that waiting to receive a reward is less desirable.

**Discount factor:** The concept of discounting goes beyond RL (e.g., discounting is a [core concept in finance](https://www.investopedia.com/terms/d/discounting.asp)) and refers to the basic idea of determining the current value of a future reward. As shown by the equation above, we handle discounting in RL via an exponential[6](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-6-137266538) discount factor. Although the intuitive explanation of the discount factor is easy to understand, the exact formulation we see above is rooted in mathematics and is actually a [complex topic of discussion](https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning); see below.

[More on Discount Factors](https://towardsdatascience.com/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942)

**On vs. Off-Policy:** In RL, we have a target policy that describes the policy our agent is aiming to learn. Additionally, we have a behavior policy that is being used by the agent to select actions as it interacts with the environment. The distinction between on and off-policy learning is subtle, but the difference between these two approaches lies in whether the behavior policy used to select actions as the agent navigates the environment during RL is the same (on-policy) as the target policy that we are trying to evaluate and improve or not (off-policy).

**ε-Greedy Policy:** RL trains a neural network via interaction with an environment. The policy that this neural network implements takes a current state as input and produces a probability distribution over potential actions as output. But, _how do we choose which action to actually execute?_ One of the most common approaches is an ε-greedy policy, which selects the action with the highest expected return most of the time (i.e., with probability 1 - ε) and a random action otherwise. Such an approach balances [exploration and exploitation](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/) by allowing the agent to explore new actions in addition to those that it knows to work well.

## Q-Learning: A Simple Introduction to RL

Now that we understand the framework that is typically used to solve problems with RL, let’s take a look at our first RL algorithm. This algorithm, called Q-Learning, is simple to understand and a great intro into the topic. Once we understand Q-Learning, we will also study our first deep RL algorithm (i.e., a system that trains a deep neural network with RL), called Deep Q-Learning.

#### Q-Learning: Modeling Q Values with a Lookup Table

Q-Learning is a [model-free](https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study) RL algorithm, meaning that we don’t have to learn a model for the environment with which the agent interacts. Concretely, this means that we don’t have to train a model to estimate the transition or reward functions—_these are just given to us as the agent interacts with the environment_. The goal of Q-Learning is to learn the value of any action at a particular state. We do this through learning a [Q function](https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7), which defines the value of a state-action pair as the expected return of taking that action at the current state under a certain policy and continuing afterwards according to the same policy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfbd7b83-a621-4567-95da-2007efb9266c_2316x1186.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfbd7b83-a621-4567-95da-2007efb9266c_2316x1186.png)

Storing Q values for state-action pairs in a lookup table

To learn this Q function, we create a lookup table for state-action pairs. Each row in this table represents a unique state, and each column represents a unique action; see above. The values within each of entry of the lookup table represent the Q value (i.e., the output of the Q function) for a particular state-action pair. These Q values are initialized as zero and updated—_using the [Bellman equation](https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7)_—as the agent interacts with the environment until they become optimal.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8534159-8348-4186-9699-5d1e87c5ee77_1816x1006.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8534159-8348-4186-9699-5d1e87c5ee77_1816x1006.png)

High-level depiction of the Q-learning algorithm

**The algorithm.** The first step of Q-learning is to initialize our Q values as zero and pick an initial state with which to start the learning process. Then, we iterate over the following steps (depicted above):

1. Pick an action to execute from the current state (using an ε-Greedy Policy).
    
2. Get a reward and next state from the (model-free) environment.
    
3. Update the Q value based on the Bellman equation.
    

As shown in the figure above, our update to the Q value considers the reward of the current action, the Q value of the current state, and the Q value of the next state. However, given that our agent might execute several actions within the next state, _it is unclear which Q value we should use for the next state when performing our update_. In Q-learning, we choose to use the maximum Q value, as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa02b4684-d03c-4c7e-ba68-3b0b46e60e2c_2346x438.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa02b4684-d03c-4c7e-ba68-3b0b46e60e2c_2346x438.png)

Q-learning update rule (based on Bellman equation)

Interestingly, Q-learning utilizes an ε-greedy policy when selecting actions, allowing new states and actions to be explored with a certain probability. When computing Q value updates, however, we always consider the next action with the maximum Q value, which may or may not be executed from the next state. In other words, _Q-learning estimates the return for state-action pairs by assuming a greedy policy that just selects the highest-return action at the next state_, even though we don’t follow such an approach when actually selecting an action. For this reason, Q-learning is an off-policy learning algorithm; see [here](https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning) for more details.

**Brief mathematical note.** The update rule used for Q-learning is mathematically guaranteed to find an optimal policy for any ([finite](https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-3-finite-markov-decision-processes-51e1f8d3ddb7)) MDP—_meaning that we will get a policy that maximizes our objective given sufficient iterations of the above process_. An approachable and (almost) self-contained proof of this result is provided in [5].

#### Deep Q-Learning

The foundation of Deep Q-learning (DQL) lies in the vanilla Q-learning algorithm described above. DQL is just an extension of Q-learning for deep reinforcement learning, meaning that we use an approach similar to Q-learning to train a deep neural network. Given that we are now using a more powerful model (rather than a lookup table), Deep Q-Learning can actually be leveraged in interesting (but still relatively simple) practical applications. Let’s take a look at this algorithm and a few related applications that might be of interest.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49bc7981-80ac-4af2-95ac-d73907528cc2_1824x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49bc7981-80ac-4af2-95ac-d73907528cc2_1824x1192.png)

Q-learning models a Q function with a lookup table, while Deep Q-learning models a Q function with a deep neural network.

**The problem with Q-Learning.** The size of the lookup table that we define for Q-learning is dependent upon the total number of states and actions that exist within an environment. In complex environments (e.g., high-resolution video games or real life), maintaining such a lookup table is intractable—_we need a more scalable approach_. DQL solves this problem by modeling the Q function with a neural network instead of a lookup table; see above. This neural network takes the current state as input and predicts the Q values of all possible actions from that state as output. DQL eliminates the need to store a massive lookup table! We just store the parameters of our neural network and use it to predict Q values.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31545263-091d-4c9a-ab38-85c6af28c1b2_2240x1246.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31545263-091d-4c9a-ab38-85c6af28c1b2_2240x1246.png)

Schematic depiction of DQL

**The algorithm.** In DQL, we have two neural networks: the _Q network_ and the _target network_. These networks are identical, but the exact architecture they use depend upon the problem being solved[7](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-7-137266538). To train these networks, we first gather data by interacting with the environment. This data is gathered using the current Q network with an ε-greedy policy. This process of gathering interaction data for training the Q network is referred to as _experience replay_; see above.

From here, we use data that has been collected to train the Q network. During each training iteration, we sample a batch of data and pass it through both the Q network and the target network. The Q network takes the current state as input and predicts the Q value of the action that is taken (i.e., predicted Q value), while the target network takes the next state as input and predicts the Q value of the best action that can be taken from that state[8](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-8-137266538) (i.e., target Q value).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92c54009-3926-43fb-967a-897b5029b76b_1682x444.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92c54009-3926-43fb-967a-897b5029b76b_1682x444.png)

Loss function for DQL

From here, we use the predicted Q value, the target Q value, and the observed reward to train the Q network with an [MSE loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html); see above. The target network is held fixed. Every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. Then, we just repeat this process until the Q network converges. Notably, the dataset we obtain from experience replay is cumulative, meaning that we maintain all of the data we have observed from the environment throughout all iterations.

**Why do we need the target network?** The vanilla Q-learning framework leverages two Q values in its update rule: a (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state. In DQL, we similarly have to generate both of these Q values. In theory, we could do this with a single neural network by making multiple passes through the Q network—_one for the predicted Q value and one for the target Q value_. However, the Q network’s weights are being updated at every training iteration, which would cause the target Q value to constantly fluctuate as the model is updated. To avoid this issue, we keep the target network separate and fixed, only updating its weights every several iterations to avoid creating a “moving target”.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cf946ad-6566-4c81-a001-f80b5fbe7b7b_1088x736.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cf946ad-6566-4c81-a001-f80b5fbe7b7b_1088x736.png)

This idea of using a separate network to produce a training target for another network—referred to as [knowledge distillation](https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764) [6]—is heavily utilized within deep learning. Furthermore, the idea of avoiding too much fluctuation in the weights of the teacher/target model has been addressed in this domain. For example, the mean teacher approach [7] updates the weights of the teacher model as an [exponential moving average](https://leimao.github.io/blog/Exponential-Moving-Average/) of the student network’s weights; see above. In this way, we can ensure a stable target is provided by the teacher during training.

**Practical applications.** DQL is a deep RL framework that has been used for several interesting practical applications. One early and notable demonstration of DQL was for playing [Atari breakout](https://www.youtube.com/watch?v=V1eYniJ0Rnk). In [3], authors from Google DeepMind show that Deep Q-Learning is a useful approach for training a agents that can successfully beat simple video games. For a more hands on tutorial, check out the article below that explores a similar approach for Space Invaders.

[DQL for Space Invaders](https://huggingface.co/blog/deep-rl-dqn)

## Final Remarks

We now have a basic understanding of RL, the associated problem formulation, and how such problems can be solved by algorithms like (deep) Q-learning. Although reinforcement learning is a complex topic, the algorithms and formulations we have studied so far are quite simple. Over the course of coming overviews, we will slowly build upon these concepts, eventually arriving at the algorithms that are used today for finetuning language models. As we will see, RL is an incredibly powerful learning approach that can be used to improve a variety of practical applications from LLMs to recommendation systems. Gaining a deep understanding of this concept, its capabilities, and how it can be implemented unlocks an entire realm of new possibilities beyond supervised learning.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[2] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).

[3] Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." _arXiv preprint arXiv:1312.5602_ (2013).

[4] Kiran, B. Ravi, et al. "Deep reinforcement learning for autonomous driving: A survey." _IEEE Transactions on Intelligent Transportation Systems_ 23.6 (2021): 4909-4926.

[5] Regehr, Matthew T., and Alex Ayoub. "An Elementary Proof that Q-learning Converges Almost Surely." _arXiv preprint arXiv:2108.02827_ (2021).

[6] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." _arXiv preprint arXiv:1503.02531_ (2015).

[7] Tarvainen, Antti, and Harri Valpola. "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results." _Advances in neural information processing systems_ 30 (2017).

[8] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[1](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-1-137266538)

In the case of language models, pretraining is performed over a massive corpus of textual data obtained from the internet.

[2](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-2-137266538)

It is not quite this simple. In reality, we ask human annotators to rank model responses (i.e., which one of these two responses is more preferable). Then, we use this dataset to train a neural network that outputs a score for each language model output, instead of asking human annotators to score the model’s generations directly.

[3](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-3-137266538)

Notably, the action that is chosen only depends on the current state and not any state history that precedes it. This is a key property of an MDP, which make the assumption that the next action only depends upon the current state.

[4](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-4-137266538)

Remember, our policy is a probability distribution over actions at each time step given the current state, so the exact trajectory produced is not deterministic. We can obtain many different trajectories depending upon how we sample actions from our policy.

[5](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-5-137266538)

The type of model architecture used to implement our policy will change depending on the data used to encode our state. For example, we will use a [CNN](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939) if our state is an image, a transformer/[RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) if our state is textual, and so forth.

[6](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-6-137266538)

The discount factor is “exponential” in this case because we multiply the reward by the discount factor raised to the power of `t` (i.e., the time step at which the reward is actually granted), which is an [exponential function](https://en.wikipedia.org/wiki/Exponential_function).

[7](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-7-137266538)

For example, if our state is an image, we will use a CNN. If our state is text, then we will use an RNN or transformer. If our state is just a vector, we can use a feedforward neural network.

[8](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning#footnote-anchor-8-137266538)

This is similar to the vanilla Q-learning update, which uses the best Q value from the next state when updating the Q value of the current state-action pair.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Marco Aurelio Sterpa's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2f3e36c-0236-4fd8-aca4-80e7e8bdd2f8_96x96.png)



](https://substack.com/profile/159709240-marco-aurelio-sterpa)

[

![Anaha's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03481655-e3f0-448e-aa7b-470a3fa1a289_144x144.png)



](https://substack.com/profile/139822954-anaha)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Seth Levine's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28f2b438-05dd-48d9-b1f6-eab8deed1dd4_1024x1024.jpeg)



](https://substack.com/profile/25128034-seth-levine)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

171 Likes∙

[14 Restacks](https://substack.com/note/p-137266538/restacks?utm_source=substack&utm_content=facepile-restacks)

171

- 

[

5

](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comments)

14

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Keagen Hadley's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5742747-c991-41b9-bb08-db02199842ec_349x349.png)



](https://substack.com/profile/59150337-keagen-hadley?utm_source=comment)

[Keagen Hadley](https://substack.com/profile/59150337-keagen-hadley?utm_source=substack-feed-item)

[The Clinician's Guide to Regula…](https://cliniciansguide.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[11月19日](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comment/77759112 "2024年11月19日 02:24")

Liked by Cameron R. Wolfe, Ph.D.

Fantastic article, Cameron.

Super informative!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comment/77759112)

[

![Sebastian Raschka, PhD's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg)



](https://substack.com/profile/27393275-sebastian-raschka-phd?utm_source=comment)

[Sebastian Raschka, PhD](https://substack.com/profile/27393275-sebastian-raschka-phd?utm_source=substack-feed-item)

[Ahead of AI](https://magazine.sebastianraschka.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年9月25日](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comment/40644822 "2023年9月25日 21:49")

Liked by Cameron R. Wolfe, Ph.D.

This is another really nice article, Cameron!

Sorry, but there is one thing I either misunderstood or is not entirely correct.

> In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text; see above.

I don't think it's necessary to be able to backpropagate through the system (here: human) that generates the score. You can think of the score as a label similar to what you have in supervised learning: a class label if it's discrete ("good" / "bad" for example) or a continuous number as in regression losses (e.g., a reward score reflecting how good the answer is).

There's actually also a recent paper called Direct Policy Optimization where they skip the RL part and update the LLM directly on the reward signal in supervised fashion: [https://arxiv.org/abs/2305.18290](https://arxiv.org/abs/2305.18290)

That's a small detail and I love the article though, and I do think that RL(HF) is a worthwhile approach for improving the helpfulness & safety of LLMs!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comment/40644822)

[3 more comments...](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----

[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Policy Gradients: The Foundation of RLHF

### Understanding policy optimization and how it is used in reinforcement learning...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Oct 02, 2023

27

- 

[

1

](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of/comments)

3

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8aad4b8-e54d-44ab-9cf4-1c775d8ebf8c_2400x1324.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8aad4b8-e54d-44ab-9cf4-1c775d8ebf8c_2400x1324.png)

Although useful for a variety of applications, reinforcement learning (RL) is a key component of the alignment process for large language models (LLMs) due to its use in [reinforcement learning from human feedback (RLHF)](https://aman.ai/primers/ai/RLHF/). Unfortunately, RL is less widely understood within the AI community. Namely, many practitioners (including myself) are more familiar with [supervised learning techniques](https://cameronrwolfe.substack.com/i/137266538/what-is-reinforcement-learning), which creates an implicit bias against using RL despite its massive utility. Within this series of overviews, our goal is to mitigate this bias via a comprehensive survey of RL that starts with basic ideas and moves towards modern algorithms like [proximal policy optimization (PPO)](https://openai.com/research/openai-baselines-ppo) [7] that are heavily used for RLHF.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee7fea0d-e11d-499a-a4e7-a841f2f3930b_1390x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee7fea0d-e11d-499a-a4e7-a841f2f3930b_1390x726.png)

Taxonomy of modern RL algorithms (from [5])

**This overview.** As shown above, there are two types of model-free RL algorithms: Q-Learning and Policy Optimization. Previously, we learned about Q-Learning, the basics of RL, and how these ideas can be generalized to language model finetuning. Within this overview, we will overview policy optimization and policy gradients, two ideas that are heavily utilized by modern RL algorithms. Here, we will focus on the core ideas behind policy optimization and deriving a policy gradient, as well as cover a few common variants of these ideas. Notably, PPO [7]—_the most commonly-used RL algorithm for finetuning LLMs_—is a policy optimization technique, making policy optimization a fundamentally important concept for finetuning LLMs with RL.

## Reinforcement Learning Basics

> _“In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future.”_ - from [5]

In a prior overview (linked below), we learned about the problem structure that is typically used for reinforcement learning (RL) and how this structure can be generalized to the setting of fine-tuning a language model.

[RL Basics for LLMs](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)

Understanding these fundamental ideas is important, as it lays a foundation for more complex RL algorithms. Here, we will briefly overview these key ideas and introduce some extra concepts that are related to policy optimization.

#### MDPs and Fundamental Components in RL

The RL framework can be formalized as a [Markov Decision Process (MDP)](https://cameronrwolfe.substack.com/i/137266538/markov-decision-process-mdp), which has states, actions, rewards, transitions, and a policy; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3afd6c9-32a5-414d-9c89-40d1c3ed502b_2266x412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3afd6c9-32a5-414d-9c89-40d1c3ed502b_2266x412.png)

Components of an MDP

For the purposes of this post, we will assume that our policy is a machine learning model (e.g., a deep neural network) with parameters θ. This policy takes a state as input and predicts some distribution over the action space. We use this output to decide what action should be taken next within the MDP; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46fe3655-614a-4842-b300-0120d63bf0be_1050x460.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46fe3655-614a-4842-b300-0120d63bf0be_1050x460.png)

Parameterized version of the policy

By using our policy to predict each next action, we can traverse an environment, receive rewards, and form a sequential trajectory of states and actions. Typically, we refer to the entity traversing the environment as an _agent_, and our agent implements the policy shown above when choosing each action. The process of exploring an environment according to some policy is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fedf33673-4c64-49a2-bd7a-d722dbf5e566_1700x934.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fedf33673-4c64-49a2-bd7a-d722dbf5e566_1700x934.png)

Structure of an MDP

**Reward and return.** As our agent traverses the environment, it receives positive or negative reward signals for the actions it chooses and the states that it visits. Our goal is to learn a policy from these reward signals that maximizes total reward across an entire trajectory sampled from the policy. This idea is captured by the return, which sums the total rewards over an agent’s trajectory; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4af64-1440-40ad-b478-c469592ce536_1758x466.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4af64-1440-40ad-b478-c469592ce536_1758x466.png)

Here, the return is formulated with a [discount factor](https://towardsdatascience.com/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942). However, this discount factor is not always present or necessary. The two major types of returns considered within RL are the infinite-horizon discounted reward and the finite-horizon undiscounted reward; see below. For the infinite-horizon variant, the discount factor is necessary mathematically to ensure that the infinite sum converges; see [here](https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning) for more discussion on the role of the discount factor.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c67dde8-0cbd-46ea-bb58-44a963cb0e0b_1720x450.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c67dde8-0cbd-46ea-bb58-44a963cb0e0b_1720x450.png)

Different variants of the return

#### Value and Advantage Functions

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51565c4-9c58-4783-b159-67d3018ae216_2628x454.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51565c4-9c58-4783-b159-67d3018ae216_2628x454.png)

Different value functions

One final concept that will be especially relevant to this post is that of a value function. In RL, there are four basic value functions (shown above), all of which assume the infinite-horizon discounted return:

- _On-Policy Value Function_: expected return if you start in state `s` and act according to policy π afterwards.
    
- _On-Policy Action-Value Function_: expected return if you start in state `s`, take some action `a` (may not come from the current policy), and act according to policy π afterwards.
    
- _Optimal Value Function_: expected return if you start in state `s` and always act according to the optimal policy afterwards.
    
- _Optimal Action-Value Function_: expected return if you start in state `s`, take some action `a` (may not come from the current policy), and act according to the optimal policy afterwards.
    

There is an important connection between the optimal policy in an environment and the optimal action-value function. Namely, the optimal policy selects the action in state `s` that maximizes the value of the optimal action-value function.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9930622e-f6ce-43de-8bc0-44c7b84f965e_2064x436.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9930622e-f6ce-43de-8bc0-44c7b84f965e_2064x436.png)

**Advantage functions.** Using the value functions described above, we can define a special type of function called an advantage function, which is heavily used in RL algorithms based on policy gradients. Put simply, _the advantage function characterizes how much better it is to take a certain action_ `a` _relative to a randomly-selected action in state_ `s` _given a policy π_ ; see above. Here, we should notice that the advantage function can be derived using the on-policy value and action-value functions defined before, as these functions assume that the agent acts according to a randomly-selected action from the policy π .

> _“The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.”_ - from [5]

**Connection to Bellman equations.** Finally, we should note that each of the value functions have their own Bellman equation that quantifies the value of a particular state or state-action pair in RL. Bellman equations are the foundation of RL algorithms such as [Q-Learning](https://cameronrwolfe.substack.com/i/137266538/q-learning-modeling-q-values-with-a-lookup-table) and [Deep Q-Learning](https://cameronrwolfe.substack.com/i/137266538/deep-q-learning). See [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#bellman-equations) for more details on how the Bellman Equation for each value function can be derived.

## Policy Optimization

We will now explore the basic idea behind policy optimization, how this idea can be used to derive a policy gradient, and several variants of the policy gradient that commonly appear within RL literature. During the learning process, we aim to find parameters θ for our policy that maximize the objective function below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74d02da2-0ae7-4f9b-8d3c-f28189b9d52b_1228x580.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74d02da2-0ae7-4f9b-8d3c-f28189b9d52b_1228x580.png)

Policy optimization objective

In words, this objective function measures the expected return of trajectories sampled from our policy within the specified environment[1](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-1-137421286). If we want to find parameters θ that maximize this objective function, one of the most fundamental techniques that we can use is gradient ascent, which iterates over parameters θ using the update rule shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44bd2d63-8e90-43f2-9924-60d4daa6c915_1304x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44bd2d63-8e90-43f2-9924-60d4daa6c915_1304x576.png)

Gradient ascent update rule for policy optimization

Gradient ascent/descent is a fundamental optimization algorithm that—_along with its many variants_—is heavily used for training machine learning models across a variety of different applications. See [here](https://www.ruder.io/optimizing-gradient-descent/) for a more comprehensive survey of gradient descent/ascent in ML and the popular variants that exist. At each step of gradient ascent/descent, the above update rule executes the following steps:

1. Compute the gradient of the objective with respect to the current parameters.
    
2. Multiply this gradient by the learning rate.
    
3. Tweak the parameters by the addition/subtraction[2](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-2-137421286) of this scaled gradient.
    

If we select the correct learning rate and perform sufficient iterations of this gradient ascent update, our policy should begin to increase the desired objective—_this is the fundamental idea behind policy optimization_. However, the number of required updates to reach convergence and whether we global or locally maximize the objective (if at all) depends upon the [properties of the objective function](https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf). Though we will not cover the details here, there is an entire field of research, called [optimization](https://akyrillidis.github.io/comp414-514/), that focuses upon mathematically analyzing the convergence of such (gradient-based) algorithms[3](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-3-137421286) for optimizing objective functions.

#### Deriving (and using) a Basic Policy Gradient

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ee92a-e6d4-4cff-954e-d7693e4d0a07_1180x262.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ee92a-e6d4-4cff-954e-d7693e4d0a07_1180x262.png)

We can optimize our policy by computing the gradient of the objective with respect to the current policy and performing gradient ascent.

If we want to implement the basic idea of policy optimization outlined above, the first question we might ask is: _How do we compute the gradient of our objective?_ To answer this question, we need a bit of math. We will outline the basic ideas for how to do this here, but we will not go too in depth, choosing instead to focus on practical takeaways and resulting algorithms. If you’re not interested in math, skip to the bottom of this section and pay attention to the final expression that we derive for the policy gradient (highlighted clearly).

To start, let’s use the definition of the [expected value](https://www.statisticshowto.com/probability-and-statistics/expected-value/) in statistics to “unroll” the gradient of our objective.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbfb9850-cd7e-43c4-8264-7c432c466833_942x466.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbfb9850-cd7e-43c4-8264-7c432c466833_942x466.png)

Begin deriving an expression for the policy gradient

What we see here is that the gradient of our objective relies upon the expected value of the return for our current policy. We can compute this expected return similarly to computing any other average value. Put simply, we consider all possible trajectories from our policy, compute their probability, multiply the return of each trajectory by its probability, and sum all of this together; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f278019-5fc8-49b7-8999-6f5897995b1a_1554x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f278019-5fc8-49b7-8999-6f5897995b1a_1554x860.png)

Different methods of computing expected return over trajectories

Because our policy can have a potentially infinite number of trajectories, we have to express this operation as an [integral](https://www.youtube.com/watch?v=rfG8ce4nNh0) instead of a discrete sum over trajectories. However, we see here that integrals are not super difficult to understand—_they are just a different way of computing an average over a potentially infinite number of values_!

From here, we should notice that this expression depends upon two quantities: the return of a trajectory and the probability of a trajectory under our current policy. Computing the return is easy—_we just get this from our environment_. We can compute the probability of a trajectory under the current policy as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F398b7661-b26c-488e-8e53-7f52ca479d25_1724x664.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F398b7661-b26c-488e-8e53-7f52ca479d25_1724x664.png)

Probability of a trajectory

Here, we use the [chain rule of probability](https://en.wikipedia.org/wiki/Chain_rule_\(probability\)) to derive the probability of the overall trajectory under the current policy by just multiplying the probability of each state and action in the trajectory. Then, by combining the expressions we have derived so far and applying the [log-derivative trick](https://andrewcharlesjones.github.io/journal/log-derivative.html), we arrive at the expression shown below; see [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) for a full derivation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5749f4d0-2776-4629-a794-62977eecdf16_1558x512.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5749f4d0-2776-4629-a794-62977eecdf16_1558x512.png)

Final expression for a basic policy gradient

Now, _we have an actual expression for the gradient of our objective function that we can use in gradient ascent_! Plus, this expression only depends on the return of a trajectory and the gradient of the log probability of an action given our current state. As long as we instantiate our policy such that the gradient of action probabilities is computable (e.g., this is pretty easy to do if our policy is implemented as a neural network), _we can easily derive both of these quantities_!

**Computing the policy gradient in practice.** Computing the expectation used in the expression above analytically would require an integral. In practice, however, we can estimate the value of this expectation by sampling a fixed number of trajectories. In other words, we can just:

- Sample several trajectories by letting the agent interact with the environment according to the current policy.
    
- Estimate the policy gradient using an average of relevant quantities over the fixed number of sample trajectories.
    

See below for a formal expression of this idea, where we sample several trajectories and use their [sample mean](https://en.wikipedia.org/wiki/Sample_mean_and_covariance) as an estimate of the policy gradient.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39417e93-be30-4df9-a969-b7eb234eed69_1398x330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39417e93-be30-4df9-a969-b7eb234eed69_1398x330.png)

Estimating the policy gradient in practice

We compute the policy gradient shown above[4](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-4-137421286) in every training iteration—_or gradient ascent step_. This constitutes one epoch of the training process. Intuitively, this update rule works well because performing gradient ascent with the above estimate for the policy gradient increases the log probability of actions within a trajectory that achieves a large return. Notably, this simple formulation of the policy gradient is used by the REINFORCE algorithm [1], which is a well-known and widely-used baseline within RL literature.

**An implementation.** Now that we have a basic understanding of policy gradients, we can look at an example implementation. A great example of policy gradients is provided within OpenAI’s [spinning up tutorial series](https://spinningup.openai.com/en/latest/index.html) for RL; see the link below. In this example, we can use one of several possible environments available within OpenAI’s [gym package](https://github.com/openai/gym), the policy is implemented as a [feed-forward neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks), and we can see a concrete example of a policy gradient being computed (in PyTorch) from a batch of experience data collected with our agent.

[Simple Policy Gradient Example](https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py)

#### Variants of the Basic Policy Gradient

There are several variants of the policy gradient that can be derived. Each of these variants address issues associated with the simple policy gradient that we learned about in the previous section. We will now overview some of these variants to gain a better grasp of the different policy gradient algorithms that exist.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6998a9a-6200-4dee-9a8d-d3136fddfd67_1478x218.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6998a9a-6200-4dee-9a8d-d3136fddfd67_1478x218.png)

**Reward-to-go trick.** Our initial policy gradient expression (copied above) increases the probability of a given action based upon the total return of a trajectory, which is a sum of all (potentially [discounted](https://cameronrwolfe.substack.com/i/137266538/important-terms-and-definitions)) rewards obtained along the entire trajectory. However, we might wonder: _Why should we consider rewards that are obtained before this action is even taken? Shouldn’t we only encourage actions based on rewards obtained after they are taken?_ The short answer is yes—this simple change leads to a new variant of the policy gradient expression that is commonly referred to as the “reward-to-go” policy gradient[5](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-5-137421286); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F512cbba4-e353-4d8b-93c7-224a0eef309e_1100x656.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F512cbba4-e353-4d8b-93c7-224a0eef309e_1100x656.png)

Expression for the reward-to-go policy gradient

We can derive the reward-to-go policy gradient using the [expected grad-log-prob (EGLP) lemma](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#expected-grad-log-prob-lemma). One of the main problems with our original expression for the policy gradient is that _estimating this gradient accurately requires a large number of sample trajectories_. Using the EGLP lemma, we can show that the reward-to-go policy gradient—_despite not changing the expected value of the policy gradient_—reduces the variance of our estimate and, therefore, reduces the total number of trajectories required to derive an accurate estimate of the policy gradient.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd202d1cb-9ee8-4540-9360-be8dea93a14b_2046x538.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd202d1cb-9ee8-4540-9360-be8dea93a14b_2046x538.png)

Adding a baseline in the reward-to-go policy gradient

**Adding a baseline.** Going further with the EGLP lemma, we can use it to show that the modified expression above also maintains the desired expectation of the policy gradient, while (again) reducing the variance. Here, we add a baseline function to our expression that only depends on the current state. Interestingly, there are several useful functions that we can consider as a baseline. For example, we could use the on-policy value function, which characterizes the expected return for an agent starting in a given state and acting according to the current policy. In this case, the above expression would only positively reinforce trajectories that achieve an above average return (i.e., greater than the baseline).

#### Vanilla Policy Gradient

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3050a6d2-3f39-463e-8043-cf47df65bdd4_1632x938.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3050a6d2-3f39-463e-8043-cf47df65bdd4_1632x938.png)

Different variants of the policy gradient

So far, we have seen three variants of the policy gradient; see above. The vanilla policy gradient has a similar structure to the formulations above, but it uses the advantage function as shown below. Again, this formulation of the policy gradient maintains the same expectation while reducing variance, meaning that we can accurately estimate the policy gradient with fewer sample trajectories.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dbd6ad6-4d9e-4085-b4a7-849b29789350_1662x470.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dbd6ad6-4d9e-4085-b4a7-849b29789350_1662x470.png)

The vanilla policy gradient

Similarly to other policy gradient algorithms, we can estimate the above expression using a sample mean (i.e., by sampling several trajectories in practice and taking a discrete average of the quantities within the above expression) and optimize a policy with this estimate of the vanilla policy gradient via gradient ascent. Given that vanilla policy gradient is an on-policy algorithm, we do this by allowing the current policy to interact with the environment and collect a sufficient amount of data. Then, we _i)_ collect these experiences into a mini-batch of data, _ii)_ compute a sample estimate of the above policy gradient, and _iii)_ update the parameters of our policy via gradient ascent. See below for pseudocode.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9aa9da-3045-4cfa-9c45-81ce77432fda_1292x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9aa9da-3045-4cfa-9c45-81ce77432fda_1292x536.png)

(from [4])

**Estimating the advantage function.** Although we have already discussed computing estimates of policy gradient via sampling, the expression for the vanilla policy gradient is a bit different from what we have seen so far due to its use of the advantage function. However, policy gradients are commonly formulated with the advantage function in RL, and various techniques have been proposed for deriving estimates of the advantage function. One of the most widely-used techniques is [Generalized Advantage Estimation (GAE)](https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737) [3]. The details of this technique are beyond the scope of this post, but the interested reader should see Section 3 of [3] for the derivation of this technique.

**Connection to language models.** Formulating the policy gradient with an advantage function is extremely common. In fact, RL algorithms that are commonly used for finetuning language models—_such as trust region policy optimization (TRPO) [6] and proximal policy optimization (PPO) [7]_—are also based upon a formulation of the policy gradient with an advantage function. Plus, both of these techniques use the GAE technique to estimate the advantage function as well! We will dive more into these more recent RL algorithms in future posts.

## Takeaways

We should now have a basic grasp of policy gradients, how they are derived, and the common variants of policy gradients that are used by popular RL algorithms. Some high-level takeaways are summarized in the points below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F077789df-fcaa-4095-b468-066845769ce7_750x74.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F077789df-fcaa-4095-b468-066845769ce7_750x74.png)

**Policy optimization** aims to learn a policy that maximizes the expected return over sampled trajectories. To learn this policy, we can use common gradient-based optimization algorithms, such as gradient ascent. However, doing this requires that we (approximately) compute the gradient of the expected return with respect to the current policy—_the policy gradient_; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7d4386-6214-48b9-9702-b64eeac87353_1478x218.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7d4386-6214-48b9-9702-b64eeac87353_1478x218.png)

**Basic policy gradient.** The most simple formulation of the policy gradient is shown above. We can compute this expression in practice by taking a sample mean over trajectories that are gathered from the environment by acting according to our current policy. A simple policy gradient implementation will _i)_ allow the current policy to interact and gather data from the environment, _ii)_ use this data to estimate the policy gradient shown above, and _iii)_ perform a gradient ascent update using this estimate.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6af3d43-8b89-4edc-94b0-b702c374a523_1410x218.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6af3d43-8b89-4edc-94b0-b702c374a523_1410x218.png)

**Policy gradient variants.** Unfortunately, the simplest variant of the policy gradient requires many trajectories to be sampled to generate an accurate estimate of the policy gradient. To mitigate this problem, several variants of the policy gradient can be derived, including the reward-to-go and baseline policy gradients. One especially notable formulation of the policy gradient, however, is the vanilla policy gradient (shown above). To estimate this quantity in practice, we can follow a similar approach as before. However, we need to adopt a specialized technique—_such as GAE [4]_—to estimate the advantage function.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Simple statistical gradient-following algorithms for connectionist reinforcement learning, Williams, Machine learning 1992

[2] Sutton, Richard S., et al. "Policy gradient methods for reinforcement learning with function approximation." _Advances in neural information processing systems_ 12 (1999).

[3] Schulman, John, et al. "High-dimensional continuous control using generalized advantage estimation." _arXiv preprint arXiv:1506.02438_ (2015).

[4] Schulman, John. _Optimizing expectations: From deep reinforcement learning to stochastic computation graphs_. Diss. UC Berkeley, 2016.

[5] Achiam, Josh. _Spinning Up in Deep RL._ OpenAI, 2018: [https://spinningup.openai.com/en/latest/index.html](https://spinningup.openai.com/en/latest/index.html)

[6] Schulman, John, et al. "Trust region policy optimization." _International conference on machine learning_. PMLR, 2015.

[7] Schulman, John, et al. "Proximal policy optimization algorithms." _arXiv preprint arXiv:1707.06347_ (2017).

[1](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-anchor-1-137421286)

Remember, the environment specifies the transition function and reward in a [model-free](https://cameronrwolfe.substack.com/i/137266538/q-learning-modeling-q-values-with-a-lookup-table) RL setup.

[2](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-anchor-2-137421286)

Gradient ascent uses addition, while gradient descent uses subtraction.

[3](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-anchor-3-137421286)

This is actually the area of research that I studied during my PhD! It is a really interesting field, as it lays the theoretical foundations for all of the practical algorithms that we use for training large neural networks today.

[4](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-anchor-4-137421286)

Here, the overline that we place over the gradient of the objective is used to indicate that we making an estimation. In other words, this is not an exact/analytical expression for the policy gradient.

[5](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of#footnote-anchor-5-137421286)

For simplicity, we assume that the return is finite horizon, meaning that there are a total number of steps `T` in the trajectory, and that the return is undiscounted, meaning that there is no discount factor applied to the rewards when computing the return.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Ido Ben-Zvi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F698242e6-b738-409c-8db5-ddae93ae2da7_96x96.jpeg)



](https://substack.com/profile/58446475-ido-ben-zvi)

[

![Mathias's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F865c6f06-35f5-4803-ac46-74ef587ad2d9_248x249.jpeg)



](https://substack.com/profile/155994838-mathias)

[

![Teja's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b66d939-a089-4b5c-ad35-2236805a1ff6_144x144.png)



](https://substack.com/profile/28702929-teja)

[

![Peter Morgan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca2835a8-fac2-4de5-8d8b-5fbb3709f0db_144x144.png)



](https://substack.com/profile/1501375-peter-morgan)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

27 Likes∙

[3 Restacks](https://substack.com/note/p-137421286/restacks?utm_source=substack&utm_content=facepile-restacks)

27

- 

[

1

](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Nerner's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe330235d-c7e5-45f2-85bb-0995f607eae6_144x144.png)



](https://substack.com/profile/152011495-nerner?utm_source=comment)

[Nerner](https://substack.com/profile/152011495-nerner?utm_source=substack-feed-item)

[2月7日](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of/comment/91719580 "2025年2月7日 18:36")

Hello!

Thank you very much for the post, I am a data scientist but RL has always scared me. But your posts encouraged me to take a shot at understanding them :)

I have two/three questions and remarks:

1. The way you use the subscript t in the policy in probability of a trajectory made me for a while think that the policy would get updated in every timestep of the trajectory because you use t to express time steps on the right side of the multiplication notation. (And I do not think that is the case with the algorithms here.)

2. You use the phrase "maintaining/keeping desired expectation of policy gradients" for variants of Basic Policy Gradients, what do you mean by that?

3. Why do we re-fit the baseline in the vanilla policy gradient?

Best!

Like

Reply

Share

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---

[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# StreamingLLM, QA-LoRA, GPT-4V, LLaVA, Reversal Curse and More

### Notable advances in LLM research prior to the week of October 10th, 2023...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Oct 10, 2023

18

- 

[

5

](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava/comments)

2

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

**New format.** You might notice this post is a bit different than my normal deep dive. I’m trying out a new format in which I cover a variety of recent research publications on the topic of language models. The goal here is to alternate between deep dives into particular topics and overviews of recent papers being published. This way, we can both _i)_ understand the broad scope of research topics being explored and _ii)_ develop an in-depth understanding of the papers that are most important. If you like this style of post, let me know!

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0453e4f-c603-4d87-8740-c7fea30f734e_1934x1072.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0453e4f-c603-4d87-8740-c7fea30f734e_1934x1072.png)

(from [1, 2, 4, 5])

Within this overview, we will take a look at a variety of recent publications on the topic of large language models (LLMs), including the following papers:

- [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) [1]
    
- [GPT-4V(ision) system card](https://openai.com/research/gpt-4v-system-card) [2]
    
- [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) [4]
    
- Physics of Language Models: [Part 3.1](https://arxiv.org/abs/2309.14316) (Knowledge Storage and Extraction) [9] and [Part 3.2](https://arxiv.org/abs/2309.14402) (Knowledge Manipulation) [10]
    

The papers above address important/open problems within AI research, such as _i)_ making LLMs less computationally burdensome, _ii)_ enabling longer sequences of text to be generated, _iii)_ incorporating visual (image) inputs, and _iv)_ better understanding how (transformer-based) LLMs process and store knowledge. These problems are incredibly important from both an application and research perspective, so let’s get into the details and implications for AI research.

---

## **[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) [1]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe5ebefb-9b38-476f-93d4-da1c1b4661b4_1072x712.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe5ebefb-9b38-476f-93d4-da1c1b4661b4_1072x712.png)

(from [1])

This paper stirred up a lot of discussion on Twitter/X last week after its publication, as it attempts to solve existing limitations that LLMs have with decoding very long sequences of text. When generating text with an LLM, we typically employ a [KV cache](https://x.com/cwolferesearch/status/1689388468911132672?s=20) to speed up the inference process. This is one of the most basic methods of improving inference speed, but it also consumes a lot of memory, especially when the sequence of generated text is long. Furthermore, LLMs fail to generalize to input sequence lengths longer than those seen during training—_they are typically limited to a fixed context length_. StreamingLLM [1] aims to solve these problems associated with generating long sequences of text.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0ae672f-4ca1-4cac-b050-476e3f15cda3_1884x664.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0ae672f-4ca1-4cac-b050-476e3f15cda3_1884x664.png)

(from [1])

**An intuitive baseline.** One previously-proposed approach for dealing with the problems outlined above is to just only consider a window of prior tokens within the attention computation. This way, we can evict tokens that go beyond a certain window length `L` and simply continue to generate text based on a sliding window of tokens within the past. However, this “window attention” approach has one major problem—_it drastically deteriorates performance once we start to generate enough text_. See the experiments above for examples of this behavior.

> _“It is evident that perplexity spikes when the text length surpasses the cache size, led by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance from the tokens being predicted, are crucial for maintaining the stability of LLMs.”_ - from [1]

**Attention sinks.** By examining input lengths at which [perplexity](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584) begins to spike, authors in [1] realize that LLM performance with window attention deteriorates when initial input tokens are evicted from the cache window. This means that, no matter how long the sequence of generated text, including the initial tokens within the attention window is important. This finding was confirmed after the attention maps of different layers in the LLM were examined, where authors observe that all layers (aside from initial few layers) focus their attention heavily on the tokens at the beginning of the input sequence.

> _“Due to the sequential nature of autoregressive language modeling, initial tokens are visible to all subsequent tokens, while later tokens are only visible to a limited set of subsequent tokens. As a result, initial tokens are more easily trained to serve as attention sinks, capturing unnecessary attention.”_ - from [1]

To explain this behavior, authors in [1] introduce the concept of an “attention sink”, where an LLM assigns unnecessarily-high attention values to a certain set of tokens. _Why does this happen?_ Put simply, the softmax function prevents all tokens within the attention mechanism from being assigned zero probability, so the model is forced to aggregate information from all token values in each of its attention heads, which causes unnecessary attention values to be “dumped” into certain tokens. In [1], authors find that multiple initial tokens within the LLM’s input sequence tend to serve as an attention sink; see below. Initial tokens seem to make great attention sinks, as they are visible to all subsequence tokens.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3d0ebf-619c-493b-8ea5-a4b76d0fa173_1866x602.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3d0ebf-619c-493b-8ea5-a4b76d0fa173_1866x602.png)

(from [1])

**StreamingLLM.** The approach proposed in [1] improves the ability of LLMs to generate long sequences of text by combining a rolling KV cache with the idea of an attention sink. In other words, the attention window used for generating text in [1] includes a rolling window of previous tokens, as well as the four initial tokens within the model’s input sequence; see below. In other words, initial tokens are always considered by the model when generating text of any length.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3711f52-df70-4505-87c4-c1dc1b66bf74_778x384.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3711f52-df70-4505-87c4-c1dc1b66bf74_778x384.png)

(from [1])

This approach requires no fine-tuning and can be incorporated into any existing (GPT-style) language model that uses a relative positional encoding scheme (e.g., [RoPE](https://arxiv.org/abs/2104.09864)). Experimental results with this approach are shown below, where we see that StreamingLLM maintains impressive language modeling performance and drastically decreases decoding latency with a stable/fixed memory footprint.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9a76beb-6d37-4e17-bfe3-c9893f28e59b_2142x800.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9a76beb-6d37-4e17-bfe3-c9893f28e59b_2142x800.png)

(from [1])

**Dedicated sink tokens.** Authors in [1] note that the observed pattern of having several tokens acting as attention sinks at the beginning of the input sequence is due to the lack of a unified “start” token. Most LLMs are trained over textual sequences directly sampled from the pretraining corpus, leading the initial token in each training sequence to be random. By always prepending a unified starting token—_called the sink token_—at the beginning of each sequence during pretraining, the model learns to use this particular token as an attention sink. As such, authors in [1] recommend that LLMs in the future be pretrained with a dedicated sink token to optimize streaming deployment; see above.

**Impact.** A lot of people on Twitter/X seemed to think that StreamingLLM increased the context window of language models significantly, as the paper claims (and achieves) generation of outputs as long as 4M tokens. However, it is important to notice that StreamingLLM does not compute dense attention over all of these tokens. Rather, it enables generation of very long sequences of text by performing attention over only initial (sink) tokens and a window of recent tokens. As such, _it does not fundamentally expand the context window_. Nonetheless, it is a great practical tool, as streaming long outputs from an LLM can be applied to a variety of different applications. Check out the code for StreamingLLM below.

[StreamingLLM Code](https://github.com/mit-han-lab/streaming-llm)

---

## [GPT-4V(ision) system card](https://openai.com/research/gpt-4v-system-card) [2]

> _“Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users.”_ - from [2]

If you haven’t heard of GPT-4V yet, you might be living under a rock. At a high level, GPT-4V is the (long-anticipated) multi-modal extension of GPT-4 that enables the model to process both textual and visual (i.e., images) input from the user. Currently, GPT-4V is available within ChatGPT Plus for select users[1](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-1-137603020). The system card for GPT-4V, which was released alongside the model, does not reveal many technical details. Rather, it focuses on evaluations and mitigation efforts taken to ensure that ingesting visual input within GPT-4 is safe.

**GPT-4V training.** GPT-4V was trained in 2022—in fact, the [original release of GPT-4](https://openai.com/research/gpt-4) mentioned multi-modal capabilities that were not available yet—and early access to the model started in March of this year. Notably, the training process of GPT-4 and GPT-4V are the same, as GPT-4V is based upon GPT-4. Both models undergo the standard LLM training pipeline of [pretraining](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) and alignment (or finetuning) via [reinforcement learning from human feedback (RLHF)](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70b0184b-f22f-4b48-942e-82657939127c_1546x616.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70b0184b-f22f-4b48-942e-82657939127c_1546x616.png)

(from [6])

**Beta usage.** The multi-modality of GPT-4V provides a ton of extra functionality, but it is also a risk surface—_adversarial users can leverage this new input modality to get the model to demonstrate harmful behavior_. OpenAI performed an extensive beta period to identify and mitigate risks associated with the model. In particular, two beta periods were organized for GPT-4V that served distinct purposes.

First, OpenAI collaborated with [Be My Eyes](https://www.bemyeyes.com/about) to develop a GPT-4V-based tool for describing smart phone images to blind and low-vision users. In this application, GPT-4V was found to achieve reduced levels of hallucinations and drastically improve the quality of [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition). Users of GPT-4V requested development of features related to analyzing and describing the faces/expression of others, which would help with better understanding social interactions. Despite the utility of GPT-4V, users were still recommended against relying upon GPT-4V for critical use cases (e.g., those related to safety or health).

GPT-4V was also released for early access to a small number of developers. This beta period was used to gather examples of queries that might come from users of the model (e.g., 20% of total queries requested explanations of images). As a result, OpenAI was able to better understand model usage and develop benchmarks for testing and mitigating issues that may occur with the general release of GPT-4V.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f7f1251-64d4-46e9-84fc-8a1c3b43a163_966x1190.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f7f1251-64d4-46e9-84fc-8a1c3b43a163_966x1190.png)

(from [2])

**Launching GPT-4V.** Developing the version of GPT-4V that was ultimately launched involved an extensive mitigation process of _i)_ identifying failure cases in which the model produces an undesirable output (i.e., red teaming) and _ii)_ further finetuning the model (via RLHF) to eliminate this harmful behavior. This iterative process drastically improves model quality; see above. Additionally, these improvements to GPT-4V can be augmented with an external refusal system (i.e., a module that detects when the model should refuse to answer a prompt), which further eliminates[2](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-2-137603020) undesirable responses to harmful prompts; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F367edb6c-eac8-4e45-990e-8c416c75e7d4_1914x968.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F367edb6c-eac8-4e45-990e-8c416c75e7d4_1914x968.png)

(from [2])

However, GPT-4V still has limitations that should be recognized. Notably, the model may lack (important) social context that can lead to harmful generations, still has simple biases (e.g., order of images in the prompt) that can impact its output, may perform unreliably in complex domains (e.g., science or medicine), and has the potential to generate harmful or incorrect instructions; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef7a0dfd-2f48-441f-8cbf-308be431f6f4_1770x1072.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef7a0dfd-2f48-441f-8cbf-308be431f6f4_1770x1072.png)

(from [2])

Put simply, GPT-4V has limitations, but extensive effort was (successfully) invested into mitigating the worst of these limitations prior to its release.

**Open-source version.** Notably, GPT-4V is a closed source model that can only be used via OpenAI offerings. Luckily, however, the open-source AI research community moves quickly! Open-source variants of GPT-4V have already been proposed that can execute dialogue with both textual and visual inputs. One notable example is [LLaVA](https://llava-vl.github.io/), which combines the [Vicuna LLM](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality) with a vision encoder to create an open-source, multi-modal language model.

**Cool uses of GPT-4V.** Upon the model’s release, we saw a variety of different interesting use cases for GPT-4V. A few of my favorites are listed below:

- Generating code from Figma [[link](https://x.com/mckaywrigley/status/1707796170905661761?s=20)]
    
- The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) [[link](https://arxiv.org/abs/2309.17421)]
    
    - This is an entire paper that explores use cases for GPT-4V, which makes it a great resources for learning about the model’s capabilities! 'Notably, this entire paper was published ~4 days after the GPT-4V model release, which is a great demonstration of the insane pace of AI research.
        
- Understanding schematics of an Arduino design [[link](https://x.com/Mascobot/status/1707607444187398383?s=20)]
    
- Picture of groceries to a grocery list [[link](https://x.com/mckaywrigley/status/1708557028149673990?s=20)]
    
- Converting screenshots and sketches to code [[link](https://x.com/DrJimFan/status/1707440731558969714?s=20)]
    

We have already seen tons of interesting use cases with GPT-4V, which demonstrates the massive utility of multi-modal LLMs. In the future, we’ll probably see more emphasis on multi-modal capabilities in this space, and the capabilities of these models to process visual information will only improve.

---

## **[QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) [4]**

To understand QA-LoRA, we need to briefly overview the idea of LoRA, why it is important, and how it has been used in recent research. LoRA is one of the most widely used forms of parameter-efficient fine-tuning for LLMs, so research on this topic tends to have a lot of utility for AI practitioners.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb55947c-1cb1-4d64-bb1a-b888a783008f_486x550.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb55947c-1cb1-4d64-bb1a-b888a783008f_486x550.png)

(from [7])

**Some background.** Low rank adaptation (LoRA) [7] for LLMs is a [parameter-efficient finetuning (PEFT)](https://huggingface.co/docs/peft/index) technique that allows us to finetune a pretrained LLM with minimal compute resources. To do this, we _i)_ start with a pretrained LLM, _ii)_ freeze (i.e., do not train) all of its weights, _iii)_ inject trainable rank decomposition matrices[3](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-3-137603020) (see above) into each layer of the transformer, and _iv)_ only train the extra, injected layers. With this approach, the forward pass of each layer in the model will be formulated as shown below, where `W0` is the original weight matrix for the layer and `AB` is the added rank decomposition matrix.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76ed15c0-6f64-4d8d-9777-18b12a44ee9d_594x72.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76ed15c0-6f64-4d8d-9777-18b12a44ee9d_594x72.png)

(from [7])

Compared to normal [adapter layers](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters), LoRA modules provide similar reductions in memory and computational complexity, as well as several extra benefits:

1. We can easily control/modify the number of trainable parameters—_even nearing the point of full pretraining_—by changing the sizes of `B` and `A`.
    
2. This does not change inference time at all, as we can just compute `W0 + BA` directly and use this weight matrix when performing inference.
    
3. We can recover the original weight matrix `W0` if we know `AB`, which in turn allows us to easily switch between several different LoRA models.
    

> _“Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times.”_ - from [7]

Additionally, recent research has introduced variants of LoRA that further improve memory efficiency. For example, QLoRA [8] explores a [quantized](https://www.tensorops.ai/post/what-are-quantized-llms) version of LoRA that further reduces memory overhead, allowing LLMs as large as 65B parameters to be finetuned on a single (48 Gb) GPU. For a practical (and more in-depth) overview of LoRA and related techniques, check out the post linked below.

[LoRA Overview](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73a313e2-836a-426a-be29-d9debf19e999_1358x294.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73a313e2-836a-426a-be29-d9debf19e999_1358x294.png)

(from [4])

**What is QA-LoRA?** The two major areas of research focused upon alleviating the computational burden (i.e., compute and memory overhead) of LLMs are:

1. _PEFT_: finetune pretrained LLMs with a small number of trainable parameters.
    
2. _Quantization_: convert trained weights of an LLM into low-bit representations.
    

The idea behind QA-LoRA [4] is to integrate these two ideas together in a simple and performant manner. Although prior research has attempted to perform post-training quantization (i.e., this means we quantize the weights after training and before inference) with PEFT, such an approach produces unsatisfying accuracy, especially at extreme quantization levels (e.g., 4-bits); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb45d09b-0fff-4c0d-9c80-1168b6b138b5_1350x648.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb45d09b-0fff-4c0d-9c80-1168b6b138b5_1350x648.png)

(from [4])

Unlike LoRA and QLoRA, QA-LoRA improves both training and inference efficiency by introducing a group-wise quantization scheme that separately quantizes different groups of weights in the model. The resulting technique performs efficient finetuning with quantized weights according to this scheme and has no need for post-training quantization—_which typically deteriorates accurac_y—prior to performing inference; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7849851a-6644-4c2e-bde9-0ebd3514c5e2_1346x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7849851a-6644-4c2e-bde9-0ebd3514c5e2_1346x732.png)

(from [4])

Despite its benefits, QA-LoRA is actually pretty easy to implement. An example implementation (in PyTorch-style pseudocode) is given in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50f2429-8fb6-41f3-a537-f68a1d247575_1620x798.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd50f2429-8fb6-41f3-a537-f68a1d247575_1620x798.png)

(from [4])

**Experimental results.** Overall, QA-LoRA’s quantization-aware training scheme achieves performance that is comparable to vanilla LoRA while matching the efficiency benefits of quantized techniques like QLoRA. As such, it is a practical approach that combines the efficiency benefits of both quantization and PEFT.

> _“QA-LoRA consistently outperforms QLoRA with PTQ on top of LLMs of different scales (the advantage becomes more significant when the quantization bit width is lower) and is on par with QLoRA without PTQ. Note that during inference, QA-LoRA has exactly the same complexity as QLoRA with PTQ and is much more efficient than QLoRA without PTQ.”_ - from [4]

---

## Physics of Language Models: [Part 3.1](https://arxiv.org/abs/2309.14316) (Knowledge Storage and Extraction) [9] and [Part 3.2](https://arxiv.org/abs/2309.14402) (Knowledge Manipulation) [10]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabc5f1e2-fac4-4ff1-8171-a9005446596a_1636x564.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabc5f1e2-fac4-4ff1-8171-a9005446596a_1636x564.png)

(from [5])

The [reversal curse](https://x.com/OwainEvans_UK/status/1705285631520407821?s=20) [5] for LLMs can characterized by the following question: _Does a language model trained on examples of the form “A <—> B” generalize to capturing the relationship “B <—> A”?_ For example, if a language model is trained on the sentence “George Washington was the first United States President”, can it easily answer the question “Who was the first United States Predident”? The answer—_as explored in [5]_—is no. Current LLMs struggle to capture relationships of this form. In fact, finetuning LLMs (e.g., [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt#%C2%A7other-useful-details) or [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)) to test this ability yields a generalization accuracy of 0%—_language models are terrible at solving this task_!

**Storing and extracting knowledge.** Providing further analysis in a similar vein to the reversal curse, a recent line of papers studies the knowledge storage, extraction, and manipulation properties of language models. Currently, we know that language models contain extensive knowledge about the world that can be extracted via simple prompting or question answering. However, it is unclear whether these question answering abilities come from direct exposure to similar (or the exact same) data during pretraining or if the language model is actually extracting information from the data on which it is trained. To answer this question, authors of [9] perform an in-depth analysis of how language models memorize knowledge during training and extract it during inference.

Notably, memorizing knowledge does not necessarily mean that the language model will be capable of extracting or manipulating this knowledge during inference. In other words, _there is a distinct difference between memory and knowledge_. Using synthetically generated biographical data (real data that is formatted properly and cleaned with LLaMA-2), authors in [9] set out to answer the question posed below.

> _“After training a language model on the biography dataset, can the model be finetuned to extract the knowledge to answer questions like “Where is the birth city of [name ]” or “What did [name ] study?”, and if so, how does the model achieve so?”_ - from [9]

Interestingly, we see in [9] that language models cannot be finetuned to extract relevant knowledge. Put simply, _this reveals that finetuning cannot recover from knowledge being stored improperly during pretraining_. However, this problem can be solved by adding further data augmentation to the pretraining process. These findings are depicted in the figure below (from the [author’s tweet](https://x.com/ZeyuanAllenZhu/status/1706479991721164990?s=20)).

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69049d21-fc21-496a-a5cf-90dc2e2a0239_1280x720.png "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69049d21-fc21-496a-a5cf-90dc2e2a0239_1280x720.png)

This analysis has links to a variety of other LLM papers. Most notably, we have seen in [prior work](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) that LLMs learn all of their knowledge in pretraining, while alignment/finetuning serves to modify the manner in which this information is surfaced. In [9], we see further support for this claim, as finetuning can only be used to uncover information that is properly stored during pretraining.

> _“A language model cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored and fully extractable in the models, and despite adequate instruct fine-tuning.”_ - from [10]"

**Manipulating knowledge.** Going further, analysis in [10] (performed by the same authors and released right after [9]!) expands upon work in [9] by studying the capabilities of language models to manipulate knowledge. Put simply, we learn from [9] about the ability of LLMs to store knowledge, _but this doesn’t tell us anything about how this knowledge can be leveraged to perform reasoning_. In particular, authors study four different forms of knowledge manipulation with LLMs:

1. _Retrieval_: “What is person A’s attribute X?”
    
2. _Classification_: “Is A’s attribute X even or odd?”
    
3. _Comparison_: “Is A greater than B in attribute X?”
    
4. _Inverse Search_: “Which person’s attribute X equals T?”
    

We learn in [10] that pretrained LLMs are good at retrieval but struggle with classification and comparison, unless [chain of thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) is used. These models also struggle with inverse search in all cases. Again, these problems are studied using a synthetic dataset that enables fine-grained control and analysis of an LLM’s ability to store and manipulate knowledge. However, the high-level takeaway from [10] is that _LLMs struggle with knowledge manipulation (beyond basic tasks like retrieval), even with properly-stored information and relevant finetuning_. These results are summarized in the figure below (from the [author’s tweet](https://x.com/ZeyuanAllenZhu/status/1706829354888798296?s=20)).

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb525ed35-f973-40b2-8f27-04c6715cb482_3840x2160.png "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb525ed35-f973-40b2-8f27-04c6715cb482_3840x2160.png)

(from

The findings from [9] and [10] give us more insight into how knowledge is stored and used in an LLM. Put simply, LLMs seems to learn all of their knowledge during pretraining. However, LLMs should not be used as a knowledge database, and they struggle with complex knowledge manipulation. Going further, tasks that require complex knowledge manipulation can be distilled into simpler (retrieval-style) tasks via chain of thought prompting, which breaks a larger problem into a multi-step process of smaller/simpler components.

---

## Honorable Mentions

Although we only summarized a few papers with the above overview, there are tons of awesome AI research papers being published every week. Some notable papers that I came across recently are listed below:

- _[Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting](https://arxiv.org/abs/2306.17563)_ [3]: studies pairwise ranking of textual documents with LLMs for search and information retrieval applications, finding that open-source LLMs can achieve state-of-the-art ranking performance.
    
- _[GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length](https://arxiv.org/abs/2310.00576)_ [11]: studies progressively increasing the length of textual sequences used during training, finding that models with longer sequence lengths can be trained more efficiently with this approach ([MPT](https://www.mosaicml.com/blog/mpt-30b) does this!).
    
- _[Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/)_: this is another open-source (Apache 2.0 license) LLM that was released by Mistral (a [high-profile AI startup](https://techcrunch.com/2023/06/13/frances-mistral-ai-blows-in-with-a-113m-seed-round-at-a-260m-valuation-to-take-on-openai/)). The model caused a lot of Twitter/X discussion, and it seems like Mistral will be releasing more/better models in the (near) future.
    

---

## Takeaways

We have seen a variety of different papers within this overview, but there are common patterns among each of these research topics. We can distill the major findings from this research into a few important takeaways:

- Current LLM applications require large context lengths and the ability to generate long sequences. StreamingLLM [1] is a practical trick for streaming long output sequences efficiently with minimal performance degradation.
    
- Although most LLMs until now have primarily leveraged a text-only interface, we are seeing a push towards multi-modal LLMs (e.g., GPT-4V [2]).
    
- LLMs have a large computational overhead, but LoRA-based techniques allow practitioners to finetune and use LLMs with minimal compute requirements. QA-LoRA [4] is another step towards democratizing the ability to train and manipulate LLMs with modest resources.
    
- LLMs memorize and extract information relatively well (assuming that such information is properly stored during pretraining), but they struggle with tasks that require extracting and manipulating relevant information [9, 10].
    

At a high level, recent research is pushing to understand how these models operate more deeply (i.e., what makes them so effective), as well as to make it easier and more efficient to train and use them in practical applications.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M., “Efficient Streaming Language Models with Attention Sinks”, arXiv e-prints, 2023.

[2] OpenAI et al., “GPT-4V(ision) System Card”, 2023.

[3] Qin, Zhen, et al. "Large language models are effective text rankers with pairwise ranking prompting." _arXiv preprint arXiv:2306.17563_ (2023).

[4] Xu, Yuhui, et al. "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models." _arXiv preprint arXiv:2309.14717_ (2023).

[5] Berglund, Lukas, et al. "The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"." _arXiv preprint arXiv:2309.12288_ (2023).

[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[7] Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." _arXiv preprint arXiv:2106.09685_ (2021).

[8] Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." _arXiv preprint arXiv:2305.14314_ (2023).

[9] Allen-Zhu, Zeyuan et al. ”Physics of Language Models: Part 3.1, Knowledge Storage and Extraction

”, _arXiv preprint arXiv:2309.14316_ (2023).

[10] Allen-Zhu, Zeyuan et al. ”Physics of Language Models: Part 3.2, Knowledge Manipulation”, _arXiv preprint arXiv:2309.14402_ (2023).

[11] Jin, Hongye et al. “GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length”, _arXiv preprint arXiv:2310.00576_ (2023).

[1](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-anchor-1-137603020)

ChatGPT Plus requires a paid ($20/month) subscription.

[2](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-anchor-2-137603020)

Notably, the OpenAI team actually achieves a refusal rate of 100% for the adversarial prompts present within their internal benchmark.

[3](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava#footnote-anchor-3-137603020)

The term “rank decomposition matrix” might sound super complicated, but the idea is simple—_we just parameterize our new weight matrix as a sum of the original weight matrix and a low-rank matrix_. In particular, we parameterize this as `W0 + BA`, where `W0` is the original weight matrix (size `d x n`) and `BA` is a product of lower rank weight matrices (sizes `d x k` and `k x n`, respectively).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Jungyoub Cha's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb746c7-f913-4634-ac3b-5611b69587c5_144x144.png)



](https://substack.com/profile/159946502-jungyoub-cha)

[

![Igor Sorochan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44c24966-3c36-4e38-b21d-e1f156ce8589_144x144.png)



](https://substack.com/profile/170206957-igor-sorochan)

[

![shanwei's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a0174b0-70cb-410d-9c16-08f0a2617c08_144x144.png)



](https://substack.com/profile/174030927-shanwei)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

18 Likes∙

[2 Restacks](https://substack.com/note/p-137603020/restacks?utm_source=substack&utm_content=facepile-restacks)

18

- 

[

5

](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![darin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fblack.png)



](https://substack.com/profile/174030554-darin?utm_source=comment)

[darin](https://substack.com/profile/174030554-darin?utm_source=substack-feed-item)

[darin](https://dronathon.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年10月11日](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava/comment/41616160 "2023年10月11日 00:43")

Liked by Cameron R. Wolfe, Ph.D.

trivial, but OCR has a typo—“optimal”

excellent write ups. i very much enjoy this style of post, as the field is expanding rapidly so breadth is necessary for good ideation.

how much do you find you learn when you write one of these up?

Like (1)

Reply

Share

[4 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava/comment/41616160)

[4 more comments...](https://cameronrwolfe.substack.com/p/streamingllm-qa-lora-gpt-4v-llava/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Proximal Policy Optimization (PPO): The Key to LLM Alignment

### Modern policy gradient algorithms and their application to language models...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Oct 23, 2023

26

- 

[

2

](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo/comments)

3

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful and informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57c7147a-7a85-49e7-ad26-c2dcaf6d5ffe_2492x1388.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57c7147a-7a85-49e7-ad26-c2dcaf6d5ffe_2492x1388.png)

(from [1, 2])

Recent AI research has revealed that reinforcement learning (RL)—_reinforcement learning from human feedback (RLHF) in particular_—is a key component of training large language models (LLMs). However, many AI practitioners (admittedly) avoid the use of RL due to several factors, including a lack of familiarity with RL or preference for supervised learning techniques. There are valid arguments against the use of RL; e.g., the curation of human preference data is expensive and RL can be data inefficient. However, _we should not avoid using RL simply due to a lack of understanding or familiarity_! These techniques are not difficult to grasp and, as shown by a variety of recent papers, can massively benefit LLM performance.

This overview is part three in a series that aims to demystify RL and how it is used to train LLMs. Although we have mostly covered fundamental ideas related to RL up until this point, we will now dive into the algorithm that lays the foundation for language model alignment—_Proximal Policy Optimization (PPO)_ [2]. As we will see, PPO works well and is incredibly easy to understand and use, making it a desirable algorithm from a practical perspective. For these reasons, PPO was originally selected in the implementation of RLHF used by OpenAI to align InstructGPT [6]. Shortly after, the popularization of InstructGPT’s sister model—ChatGPT—led both RLHF and PPO to become highly popular.

## Background Information

In this series, we are currently learning about reinforcement learning (RL) fundamentals with the goal of understanding the mechanics of language model alignment. More specifically, we want to learn exactly how [reinforcement learning from human feedback (RLHF)](https://huggingface.co/blog/rlhf) works. Given that many AI practitioners tend to avoid RL due to being more familiar with supervised learning, deeply understanding RLHF will add a new tool to any practitioner’s belt. Plus, research has demonstrated that RLHF is a pivotal aspect of the alignment process [8]—_just using [supervised fine-tuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) is not enough[1](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-1-138008873)_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ea5b18-4ef5-4d75-895c-335ee0b28ca2_2222x1038.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92ea5b18-4ef5-4d75-895c-335ee0b28ca2_2222x1038.png)

(from [8])

In this section, we will briefly cover the RL algorithms we’ve learned about in this series so far, focusing upon their limitations and the primary reasons why better algorithms are needed. Then, we will (once again) overview the problem setup of RL for language model alignment, which we should use as relevant context when learning about new algorithms. Finally, we’ll learn about the KL divergence, which is a useful concept for both RL and machine learning in general.

#### **What we have learned so far?**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee7fea0d-e11d-499a-a4e7-a841f2f3930b_1390x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee7fea0d-e11d-499a-a4e7-a841f2f3930b_1390x726.png)

Taxonomy of RL algorithms (from [9])

Until this point, our overviews within this series have mostly focused on fundamental concepts within RL, including:

- _Basics of RL for LLMs_ [[link](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)]: problem setup (with extensions to LLMs) and basic algorithms like (Deep) Q-Learning.
    
- _Policy Optimization_ [[link](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)]: understanding policy gradients—the class of optimization techniques used by RLHF—and basic algorithms in this space.
    

Within this post, we will build on these basic concepts by diving into two RL algorithms that are more directly related to RLHF: Trust Region Policy Optimization (TRPO) [1] and Proximal Policy Optimization (PPO) [2]. Similarly to the [vanilla policy gradient algorithm](https://cameronrwolfe.substack.com/i/137421286/vanilla-policy-gradient) that we saw in a prior overview, both of these algorithms are based upon policy gradients. However, PPO, which is an extension of TRPO, is the most commonly used RL algorithm for RLHF [6]!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac01f205-d1e5-4e77-8170-b083d9cfd670_2330x658.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac01f205-d1e5-4e77-8170-b083d9cfd670_2330x658.png)

(from [10])

**Why do we need a new algorithm?** So far, we have seen two main RL algorithms that can be used to train neural networks (see above):

- Deep Q-Learning (DQL) [[link](https://cameronrwolfe.substack.com/i/137266538/deep-q-learning)]
    

- Vanilla Policy Gradient Algorithm (VPG) [[link](https://cameronrwolfe.substack.com/i/137421286/vanilla-policy-gradient)]
    

However, these algorithms have notable limitations when used to solve complex problems. DQL can only be applied in relatively simple environments (e.g., game environments like [Atari](https://arxiv.org/abs/1312.5602)). Despite being effective for problems with discrete action spaces, DQL struggles to generalize to more realistic (continuous action space[2](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-2-138008873)) environments, where it is known to fail at solving even simple problems. Going further, VPG has poor data efficiency and robustness, meaning that we must collect tons of data from our environment to eliminate noise within the policy gradient estimate and, in turn, effectively train the underlying policy.

> _“There is room for improvement in developing a method that is scalable (to large models and parallel implementations), data efficient, and robust (i.e., successful on a variety of problems without hyperparameter tuning).”_ - from [2]

With this in mind, the motivation behind TRPO and PPO is to improve upon these issues. Namely, we want to derive an RL algorithm that is:

- Generally applicable (i.e., to both discrete and continuous problems)
    
- Data efficient
    
- Robust (i.e., works without too much tuning)
    
- Simple (i.e., not too difficult to understand/implement)
    

TRPO satisfies the first two points outlined above, while PPO satisfies all four. Due to its simplicity and effectiveness, PPO is widely used across domains and has become the go-to choice for aligning language models via RLHF.

#### Aligning Language Models with RL

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3139888e-9a0e-43b4-a29d-88d50a611b31_2408x468.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3139888e-9a0e-43b4-a29d-88d50a611b31_2408x468.png)

(from [6, 11])

Most modern language models are trained in several phases; see above. First, we perform [pretraining](https://cameronrwolfe.substack.com/p/language-model-training-and-inference), which is the most computationally expensive component of the training process. After pretraining, the LLM can accurately perform [next token prediction](https://cameronrwolfe.substack.com/i/85568430/language-modeling), but its output may be repetitive, uninteresting, or not useful. To solve this, we can finetune the model to improve its _alignment_, or ability to generate text that aligns with the desires of a human user.

> _“Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users.”_ - from [6]

Typically, we perform alignment by first selecting several alignment criteria (e.g., follow instructions, avoid harmful output, avoid hallucination, produce interesting/creative output, etc.), then finetuning the model—via SFT and RLHF (shown below)—to satisfy these criteria. Once the alignment process is complete, the final model can further finetuned and used to solve a downstream application via [prompting](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part) (or in-context learning).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcde8a44-12b1-4ac0-b1df-e267d21685e8_1736x958.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcde8a44-12b1-4ac0-b1df-e267d21685e8_1736x958.png)

(from [6])

**Applying RLHF.** As its name indicates, RLHF (depicted above) relies upon RL to train a language model from human feedback. We start with a set of prompts and generate several outputs for each prompt with the language model. From here, we ask a group of human annotators to rank/score the responses to each prompt according to our alignment criteria. Using these ranked responses, we can train a reward model that predicts a human preference score from a language model’s response. Then, we can use PPO[3](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-3-138008873) to finetune our language model to maximize the human preferences scores (predicted by the reward model) of its outputs.

**Alignment with RL.** Obviously, the language model alignment domain is slightly different from the typical RL setup that we have [learned about](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning). However, the components of RL actually generalize quite well to language models! To generate text, language models follow an autoregressive process of iteratively predicting the next token and adding this predicted token to the input sequence; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60484ea1-b003-441d-b071-f217487dc6d5_1456x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60484ea1-b003-441d-b071-f217487dc6d5_1456x782.png)

Generating text with a language model

From the lens of RL, our language model is the policy in this case. Given the current state (the textual input sequence), the language model produces an action[4](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-4-138008873)—_the next token_—that modifies the current state by adding a token to the current sequence. Once a full textual sequence has been produced, we can obtain a reward by rating the quality of the language model’s output with the reward model. To finetune the model with RL, we simply alternate between collecting data from the environment—_done by generating text with the language model then scoring it with the reward model_—and updating the policy according to the update rule defined by our RL algorithm of choice (e.g., VPG, TRPO, or PPO).

#### Kullback–Leibler (KL) Divergence

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d68b1ce-0ee4-40fb-8681-1867a9b25cb9_1336x326.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d68b1ce-0ee4-40fb-8681-1867a9b25cb9_1336x326.png)

Continuous and discrete formulations of entropy

At the highest level, the Kullback-Leibler (KL) Divergence is just a method of comparing two probability distributions. The idea of KL divergence has its roots in [information theory](https://towardsdatascience.com/information-theory-a-short-introduction-a37f09959a1e) and is highly related to the concept of [entropy](https://towardsdatascience.com/information-entropy-c037a90de58f). In the equation above, we can see common formulations of entropy for a probability distribution `p`. Intuitively, the entropy value captures how much information is stored within a probability distribution—_a lower entropy means that you would need fewer bits to encode the information stored within_ `p`.

**KL divergence formulation.** Instead of a single probability distribution `p`, the KL divergence considers two probability distributions: `p` and `q`. Then, mirroring the above entropy formulation, we compute KL divergence by finding the expected difference in log probabilities between these two distributions; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feab0a20d-9901-4ba7-a3ad-de82b1c9d923_1890x326.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feab0a20d-9901-4ba7-a3ad-de82b1c9d923_1890x326.png)

The KL divergence is commonly explained in the context of approximations. Namely, if we approximate `p` with `q`, the KL divergence is the number of bits we would expect to lose by making this approximation. For more details on the information theory perspective of KL divergence, see the article below.

[KL Divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

**Applications to AI/ML.** KL divergence is heavily used across different domains of AI/ML research. For example, it is commonly used in [loss functions](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) for training neural networks, either as the core loss or as an added [regularization](https://www.geeksforgeeks.org/regularization-in-machine-learning/) term. For example, [Variational Autoencoders (VAEs)](https://www.jeremyjordan.me/variational-autoencoders/) use the KL divergence to encourage similarity between the predicted latent distribution and a prior distribution.

> _The final reward function we use during optimization contains a [KL divergence] penalty term … we find this constraint is useful for training stability, and to reduce reward hacking.”_ - from [8]

Furthermore, KL divergence is used heavily within RL research. Here, we will see this concept used in the definition of TRPO, as well as to explain the intuition behind PPO’s update rule. Additionally, most implementations of RLHF [add an extra KL divergence term](https://arxiv.org/abs/2307.09288) to their loss function, which helps to prevent [reward hacking](https://en.wikipedia.org/wiki/AI_alignment#Specification_gaming_and_side_effects) and ensures updates to the language model’s policy are not too large.

## Better Algorithms for Reinforcement Learning

Within this section, we will learn about two new RL algorithms, called Trust Region Policy Optimization [1] and Proximal Policy Optimization (PPO) [2] that improve upon the algorithms we have learned about so far. Notably, TRPO and PPO both have drastically improved data efficiency, allowing us to train an effective policy faster and with less data. Going further, PPO is quite simple and robust compared to TRPO, leading to its use in a variety of popular domains.

#### [Trust Region Policy Optimization (TRPO)](https://arxiv.org/abs/1502.05477) [1]

We should recall that VPG is limited by the fact that it can only perform a single policy update for each estimate of the policy gradient that is derived. Given that VPG is notoriously data inefficient, meaning that we have to sample a lot of data when deriving a policy update, performing multiple (or larger) updates may seem enticing. However, such an approach is not justified theoretically and, in practice, leads to policy updates that are too large, thus damaging performance.

> _“While it is appealing to perform multiple steps of optimization on this loss using the same trajectory, doing so is not well-justified, and empirically it often leads to destructively large policy updates”_ - from [2]

Trust Region Policy Optimization (TRPO) [1] aims to solve the problem described above using an approach that is similar to VPG. At each step of the optimization process, however, we find the largest possible policy update that still improves performance. Put simply, TRPO allows us to learn faster by finding a reliable way to make larger policy updates that do not damage performance.

**TRPO formulation.** More specifically, we update the policy under a constraint—_based on the KL divergence_—that captures the distance between policies before and after the current update. Considering this constraint allows us to find a balance between update size and the amount of change to the underlying policy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01657179-c565-4847-9107-1a7f43dec589_1632x680.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01657179-c565-4847-9107-1a7f43dec589_1632x680.png)

TRPO update rule

To make this discussion a bit more concrete, the theoretical update rule used by TRPO is shown within the equation above. Here, the advantage function is computed using some advantage estimation technique, such as [Generalized Advantage Estimation (GAE)](https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737) [3].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa51fb80c-462c-42cc-a8cd-92551e6675b0_1786x132.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa51fb80c-462c-42cc-a8cd-92551e6675b0_1786x132.png)

Vanilla policy gradient update rule

Intuitively, this formulation looks quite similar to the update rule for the [VPG](https://cameronrwolfe.substack.com/i/137421286/vanilla-policy-gradient) algorithm (copied above for reference) with a few important differences:

- The terms in the expectation are modified slightly to express the probability of a given action `a` as a ratio between old and updated policies.
    
- The update has an added constraint based on the KL divergence between old and updated policies.
    
- Instead of performing [gradient ascent](https://www.ruder.io/optimizing-gradient-descent/), we are solving a constrained maximization problem to generate each new policy[5](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-5-138008873).
    

_How do we compute TRPO’s update in practice?_ Well, working with the analytical update rule shown above is difficult. But—_like many techniques in AI/ML_—we can find an approximation to this equation that works quite well and can be computed efficiently! The details of computing this approximation are beyond the scope of this post. However, the high-level steps we take in this process are:

1. Approximate the objective (and constraint) with a [Taylor expansion](https://www.youtube.com/watch?v=3d6DsjIBzJ4).
    
2. Solve this approximate objective function (using ideas from optimization research like [Lagrangians](https://encyclopediaofmath.org/wiki/Lagrangian) and [duality](https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf)).
    
3. Use the [conjugate gradient algorithm](https://en.wikipedia.org/wiki/Conjugate_gradient_method) to avoid inverting large matrices when solving the problem above (this is computationally intractable otherwise!).
    
4. Perform post-processing to ensure that the updated policy both satisfies the KL divergence constraint and results in an improvement to the objective.
    

For more details on his this update rule is derived, check out the article below.

[TRPO Details](https://spinningup.openai.com/en/latest/algorithms/trpo.html)

**Using TRPO in practice.** The implementation of TRPO is similar to that of VPG. We allow our current policy to interact with the environment and collect data[6](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-6-138008873). From this observed data, we can compute the approximate update for TRPO as described above. Then, we can continue the process of collecting data and performing an update until we arrive at a policy that performs quite well. Because we are using the actual policy being trained to collect the data used to train it, TRPO is an _on-policy_ reinforcement learning algorithm.

**KL Divergence Constraint.** Before moving on, let’s briefly consider the KL divergence constraint used by TRPO to better understand its role in the learning process. As mentioned previously, the VPG algorithm is based upon gradient ascent, which—by nature—ensures that updates to the policy’s parameters θ are not too large. In particular, we use a learning rate to perform updates with VPG, which can control the size of the update in the parameter space; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44bd2d63-8e90-43f2-9924-60d4daa6c915_1304x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44bd2d63-8e90-43f2-9924-60d4daa6c915_1304x576.png)

Policy optimization with gradient ascent

Here, only the size of the update to θ is controlled—_the old and updated policies are close in the parameter space_. Despite this fact, small tweaks to the parameters θ may completely change the underlying policy! Even a relatively small update can drastically change the policy’s performance (or even lead to collapse). Because small changes to θ can drastically alter the policy, ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy. As a result, we are constrained to relatively small updates within the VPG algorithm—_larger or multiple updates could be harmful_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff41e2a9e-30b3-463d-8600-7fb0161ca97d_1276x1254.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff41e2a9e-30b3-463d-8600-7fb0161ca97d_1276x1254.png)

(from [1])

TRPO sidesteps this issue by considering the size of our policy update from an alternative viewpoint. Namely, we compare updated and old policies using the KL divergence, which measures the difference in probability distributions over the action space produced by the two policies. Such an approach compares policies based upon the actions they take rather than their underlying parameters θ. In this way, we can perform large policy updates while ensuring that the new policy does not produce actions that are significantly different from the old policy. Such an approach allows us to drastically speed up the learning process as shown in the figure above (single path and vine are both based upon TRPO).

#### [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) [2]

> _“We [introduce] proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update. These methods have the stability and reliability of trust-region methods but are much simpler to implement … applicable in more general settings, and have better overall performance.”_- from [2]

TRPO has improved data efficiency, stability, and reliability compared to the VPG algorithm, but there are still limitations that need to be addressed. Namely, the algorithm is complicated, can only perform a single update each time new data is sampled from the environment, and is only applicable to certain problem setups[7](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-7-138008873). Aiming to develop a better approach, authors in [2] propose Proximal Policy Optimization (PPO), another policy gradient algorithm that alternates between collecting data from the environment and performing several epochs of training over this sampled data. PPO shares the reliability of TRPO and is _i)_ much simpler, _ii)_ more data efficient, and _iii)_ more generally applicable.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd572da35-32f5-403c-a32b-8aed5f9e365c_1056x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd572da35-32f5-403c-a32b-8aed5f9e365c_1056x282.png)

Reformulation of TRPO surrogate objective

**Reformulating the TRPO update rule.** Recall that, during each policy update, TRPO maximizes a surrogate objective to get the new policy. The surrogate objective being solved by TRPO can be reformulated as shown above. Here, we simplify our original expectation over actions/states sampled from a policy with the subscript `t`, which represents time steps along different trajectories sampled from the environment. This objective has two terms: the probability ratio and the [advantage function](https://cameronrwolfe.substack.com/i/137421286/value-and-advantage-functions). The expression for the probability ratio is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F974192ff-6652-4ae3-9988-b2059bd85c69_1512x290.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F974192ff-6652-4ae3-9988-b2059bd85c69_1512x290.png)

The probability ratio in TRPO

If we were to maximize this objective without any constraints, it would lead to a policy update that is too large (and potentially destructive)—_this is why we leverage the KL divergence constraint_. Put simply, adding a constraint to the policy update penalizes policy updates that move the probability ratio too far away from 1.

**PPO surrogate objective.** Similar to TRPO, we perform policy updates in PPO according to a surrogate objective. However, this surrogate objective has a “clipped” probability ratio, as shown in the equation below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59260a10-61e1-444c-a17e-2bf4a39a5d4e_1728x442.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59260a10-61e1-444c-a17e-2bf4a39a5d4e_1728x442.png)

PPO surrogate objective

The surrogate objective for PPO is expressed as a minimum of two values. The first value is the same surrogate objective from TRPO, while the second value is a “clipped” version of this objective that lies within a certain range. In practice, this expression is formulated such that there is no reward for moving the probability ratio beyond the interval `[1 - ϵ, 1 + ϵ]`; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c1affb2-9e51-45e5-a6c0-f604ef4642b4_2162x898.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c1affb2-9e51-45e5-a6c0-f604ef4642b4_2162x898.png)

(from [2])

In other words, _PPO has no incentive for excessively large policy updates_. Plus, by taking the minimum of the clipped and unclipped version of the surrogate objective, we only ignore excessive changes to the probability ratio if they improve the underlying objective. In the figure above, we see a basic depiction of this trend for both positive and negative values of the advantage function.

To understand PPO’s surrogate objective more intuitively, we should look at the figure below, which plots several objective functions as we interpolate between an old and updated policy obtained via PPO. In this figure, we see the KL divergence, the TRPO surrogate objective (labeled as `CPI`), the clipped surrogate objective, and the full PPO surrogate objective. From these plots, _we can see that the PPO surrogate objective is a pessimistic/lower bound for the TRPO surrogate objective, where a penalty is incurred for having too large of a policy update_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42020336-3f8a-4eac-b7a5-7f824d8325bc_2160x908.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42020336-3f8a-4eac-b7a5-7f824d8325bc_2160x908.png)

(from [2])

While TRPO sets a hard constraint to avoid policy updates that are too large, PPO simply formulates the surrogate objective such that a penalty is incurred if the KL divergence is too large. Such an approach is much simpler, as we no longer have to solve a difficult, constrained optimization problem. Rather, we can compute PPO’s surrogate loss with only minor tweaks to the VPG algorithm.

**The PPO algorithm and its benefits.** So, _how do we use this surrogate objective to train a neural network?_ PPO operates similarly to VPG and TRPO by alternating between collecting data from the environment and updating the underlying policy (i.e., it is an on-policy RL algorithm). If we are using a package like PyTorch that supports automatic differentiation, we can perform a policy update by simply constructing a loss function corresponding to the surrogate objective outlined above[8](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-8-138008873) and performing several iterations/epochs of [stochastic gradient ascent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) using this loss and samples from the observed data; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad92d2ea-c877-4aa0-a2aa-551d4166eabb_1764x626.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad92d2ea-c877-4aa0-a2aa-551d4166eabb_1764x626.png)

(from [2])

PPO has several benefits compared to TRPO. First, the implementation of PPO is much simpler compared to TRPO, as we can use automatic differentiation and gradient-based optimization techniques[9](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-9-138008873) instead of deriving an (approximate) solution for a complex, constrained objective function. Additionally, while TRPO makes only a single policy update each time new data is collected, PPO performs multiple epochs of optimization via stochastic gradient ascent over the surrogate objective, which improves data efficiency. See the link below for an accessible and well-documented implementation of PPO.

[PPO Implementation](https://github.com/ericyangyu/PPO-for-Beginners)

Finally, computing estimates of the advantage function (e.g., via GAE) typically requires that we learn a corresponding [value function](https://cameronrwolfe.substack.com/i/137421286/value-and-advantage-functions). In TRPO, we must learn this state-value function with a separate neural network. However, PPO—_due to its compatibility with a wider scope of architectures (including those with parameter sharing)_—can train a joint network for policy and value functions by just adding an extra term to the loss function that computes the [mean-squared error (MSE)](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) between estimated and actual value function values; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6364d1d2-9672-46c6-b465-a847193c2fdf_1620x448.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6364d1d2-9672-46c6-b465-a847193c2fdf_1620x448.png)

Combined surrogate and value function loss for PPO

**Does it perform well?** When PPO is compared to a variety of state-of-the-art RL algorithms on problems with continuous action spaces, we see that it tends to learn faster and outperforms prior techniques on nearly all tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04bb5e92-fdd5-4e30-bf97-9ac0ae01c5a6_2472x1208.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04bb5e92-fdd5-4e30-bf97-9ac0ae01c5a6_2472x1208.png)

(from [2])

In the Atari domain, PPO is found to perform comparably to algorithms like A2C [4] and ACER [5], despite these algorithms being tuned extensively for this domain. The table below outlines the number of games won by each algorithm.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7388faa1-e80d-4846-aedb-b8be10fabb3b_2458x414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7388faa1-e80d-4846-aedb-b8be10fabb3b_2458x414.png)

(from [2])

#### The Role of PPO in RLHF

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3861263-00a1-4f6b-94a2-2f0087ac8e79_1658x654.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3861263-00a1-4f6b-94a2-2f0087ac8e79_1658x654.png)

Steps of training a language model (from [6])

Although PPO was a useful advancement in mainstream RL research, this algorithm also had a massive impact on the space of language modeling. More specifically, [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) [6]—the predecessor to ChatGPT—was aligned (i.e., trained to produce output that aligns with human expectations) via a three-part framework (shown above) that includes both [supervised fine-tuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives). Although such an approach was explored previously for text summarization tasks [7], InstructGPT popularized this framework for training language foundation models, leading to its use in the creation of a variety of popular language models; e.g., ChatGPT, GPT-4, [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up), and [Sparrow](https://cameronrwolfe.substack.com/i/93578656/improving-alignment-of-dialogue-agents-via-targeted-human-judgements).

**How does this relate to PPO?** Due to its ease of use, PPO was the RL algorithm that was originally selected for use in RLHF by InstructGPT. The alignment strategy used by InstructGPT later became standardized and, though alternatives have been explored, PPO remains a popular choice for RLHF even today. As such, PPO is a pivotal aspect of language model alignment, and any AI practitioner with an interest in understanding or implementing the alignment process would benefit from a working understanding of PPO.

## Takeaways

Within this overview, we expanded our understanding of basic policy gradient algorithms, such as VPG, to include more recent RL algorithms like TRPO and PPO. Compared to prior algorithms, TRPO and PPO have improved data efficiency and better reliability/stability. Notably, PPO is the primary RL algorithm used by RLHF, making it a key component of the language model alignment process. Although many factors led to the use of PPO in this domain, _PPO’s ease-of-use is undoubtedly a key aspect of its success_. In particular, PPO inherits the data efficiency and reliability of TRPO with several added benefits:

- Improved robustness (i.e., not much tuning required)
    
- Better data efficiency
    
- Simplicity (i.e., only requires small tweaks to VPG)
    
- More general (i.e., applies to a wider class of model architectures)
    

Due to these (many) factors, PPO is a popular choice of RL algorithm among practitioners. As the learning algorithm used for RLHF, PPO is the foundation of language model alignment. With a deeper understanding of this algorithm, we can gain a new level of insight into how language models learn and behave.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

#### Bibliography

[1] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. “Trust region policy optimization”. In: CoRR, abs/1502.05477 (2015).

[2] Schulman, John, et al. "Proximal policy optimization algorithms." _arXiv preprint arXiv:1707.06347_ (2017).

[3] Schulman, John, et al. "High-dimensional continuous control using generalized advantage estimation." _arXiv preprint arXiv:1506.02438_ (2015).

[4] V. Mnih, et al. “Asynchronous methods for deep reinforcement learning”. In: arXiv preprint arXiv:1602.01783 (2016).

[5] Z. Wang et al. “Sample Efficient Actor-Critic with Experience Replay”. In: arXiv preprint arXiv:1611.01224 (2016).

[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[7] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[8] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[9] Achiam, Josh. _Spinning Up in Deep RL._ OpenAI, 2018: [https://spinningup.openai.com/en/latest/index.html](https://spinningup.openai.com/en/latest/index.html)

[10] Schulman, John. _Optimizing expectations: From deep reinforcement learning to stochastic computation graphs_. Diss. UC Berkeley, 2016.

[11] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-1-138008873)

Despite this fact, many open-source LLMs are aligned using solely SFT instead of a combination of SFT and RLHF. [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) breaks this trends by heavily investing into curating a massive human preference dataset for alignment via RLHF.

[2](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-2-138008873)

By continuous domain, we mean that actions outputted by the agent are continuous, rather than discrete. For example, driving a car is a continuous domain, as we can adjust the steering wheel and pedals according to a continuous output (i.e., angle or pressure). In contrast, chess would be a discrete domain, as there are a fixed number of actions capable of being taken at each state.

[3](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-3-138008873)

We can also use other algorithms, but PPO is the original (and most common) choice of RL algorithm for RLHF. [Recent research](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) has begun to explore alternative choices.

[4](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-4-138008873)

Language models have a discrete action space comprised of all tokens that can be outputted by the model. However, this action space is still quite large. Most language models have a vocabulary size of tens to hundreds of thousands of tokens!

[5](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-5-138008873)

In discussions of TRPO, we will see this referred to as solving the “surrogate” objective. This surrogate objective simply refers to the maximization problem that we solve during each update step of TRPO. More generally, surrogate functions refer to functions that approximate other functions. In TRPO, the surrogate objective is a problem that we solve in pursuit of achieving an optimal policy!

[6](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-6-138008873)

This process is often referred to as _experience replay_ and the data that we collect from this process is stored within a _replay buffer_.

[7](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-7-138008873)

Notably, TRPO cannot be used for neural network architectures that include noise (e.g., [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)) or perform any form of parameter sharing (e.g., between the policy and the [value function](https://cameronrwolfe.substack.com/i/137421286/value-and-advantage-functions)).

[8](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-8-138008873)

The objective that we use in practice is different from what we see in the equations. This is because we cannot compute an expectation exactly, as this would require taking an average over all possible trajectories from our current policy. Instead, we collect data from the environment and take a [sample mean](https://en.wikipedia.org/wiki/Sample_mean_and_covariance) to approximate this objective.

[9](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo#footnote-anchor-9-138008873)

These are the same exact tools that we use for training neural networks according to common objectives such as supervised of self-supervised learning!

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Juan Terven's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfcf763f-0d28-407c-ab06-934e8bcc37be_1024x1024.png)



](https://substack.com/profile/25323606-juan-terven)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![The Millennial Hub's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F50807bc9-aed5-41ea-ac8e-7a70b98931ae_500x500.png)



](https://substack.com/profile/42657902-the-millennial-hub)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael)

26 Likes∙

[3 Restacks](https://substack.com/note/p-138008873/restacks?utm_source=substack&utm_content=facepile-restacks)

26

- 

[

2

](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael?utm_source=comment)

[Michael](https://substack.com/profile/59416223-michael?utm_source=substack-feed-item)

[Lux Umbra Dei](https://runtothehorizn.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年10月23日](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo/comment/42332004 "2023年10月23日 18:58")Edited

Liked by Cameron R. Wolfe, Ph.D.

Cameron, could you recommend a SOTA textbook aimed at a graduate level audience on this topic? I know it is a dynamic area and a moving target, but a foundational text would be great.

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo/comment/42332004)

[1 more comment...](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---

[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Self-Critique, Self-RAG, NEFTune, Safe RLHF and More

### Notable advances in LLM research prior to the week of October 30th, 2023...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Oct 30, 2023

23

- 

[](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe/comments)

2

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c9de193-029a-4b0f-81d8-f5be91fe48b7_1770x992.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c9de193-029a-4b0f-81d8-f5be91fe48b7_1770x992.png)

(from [1, 2, 7, 9, 12])

Recent AI research has focused heavily on analyzing the capabilities of large language models (LLMs), improving the alignment (and finetuning) process, and making practical tools for LLMs (e.g., retrieval augmented generation) more effective. In this overview, we will take a look at several papers that propose advancements along these lines, including:

- [GPT-4 Doesn’t Know It’s Wrong](https://arxiv.org/abs/2310.12397) [1]
    
- [Can LLMs Really Improve by Self-critiquing Their Own Plans?](https://arxiv.org/abs/2310.08118) [2]
    
- [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) [7]
    
- [Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2310.12773) [9]
    
- [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511) [12]
    

These papers contain a variety of practical takeaways and interesting learnings that help us to better understand LLMs. Most notably, we learn that:

1. Simple practical tricks (e.g., adding noise to word embeddings) are still being discovered that improve LLM training.
    
2. LLMs are not very good at evaluating their outputs in certain applications.
    
3. Explicitly considering (and addressing) the tension between alignment criteria during RLHF can improve the alignment process.
    
4. RAG—_a simple technique that is used constantly in LLM applications_—can be improved via a more thoughtful approach to retrieval.
    

---

#### Can language models really self-critique? [[1](https://arxiv.org/abs/2310.12397), [2](https://arxiv.org/abs/2310.08118)]

> _“There is still the wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation.”_ - from [1]

A variety of recent techniques in AI research rely upon large, foundation language models to critique and refine their own output. Examples of such an approach being used practically include:

- [Self-Instruct](https://arxiv.org/abs/2212.10560) [3]: uses an LLM to iteratively generate and filter instructions to automatically generate an instruction tuning dataset.
    
- [Constitutional AI](https://cameronrwolfe.substack.com/i/136751520/constitutional-ai-harmlessness-from-ai-feedback) [4]: asks the LLM to critique and revise its own responses to prompts based on a set of alignment criteria.
    
- [Graph of Thoughts Prompting](https://cameronrwolfe.substack.com/i/136366740/graph-of-thoughts-solving-elaborate-problems-with-large-language-models) [5]: uses the LLM to score intermediate solutions to a problem while searching for a final solution[1](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-1-138301607).
    
- [Reasoning with Self-Verification](https://arxiv.org/abs/2212.09561) [6]: uses an LLM to verify/score its own solutions to determine the best possible answer to a reasoning problem.
    

More generally, powerful LLMs like GPT-4 are now commonly used to evaluate other language models, as they provide an automatic (and relatively reliable) approach for evaluating the quality of arbitrary responses to prompts. Traditional metrics like [ROUGE](https://en.wikipedia.org/wiki/ROUGE_\(metric\)) or [BLEU](https://en.wikipedia.org/wiki/BLEU) score struggle to holistically capture overall response quality in a manner comparable to GPT-4.

Given that using LLMs to critique and score their own output (or the output of other models) is becoming so common, we might begin to wonder: _Are LLMs truly capable of critiquing their own responses? Do these critiques have biases or limitations that we should consider?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c137e9c-1aa7-4cb0-8ab9-42b6030bdae6_1154x972.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c137e9c-1aa7-4cb0-8ab9-42b6030bdae6_1154x972.png)

(from [1])

These questions were recently explored by a pair of papers [1, 2] that call into question the ability of LLMs to self-critique. In the first paper, authors study the ability of GPT-4 to solve (or verify solutions to) [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problems via direct or iterative prompting[2](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-2-138301607). Immediately, the authors discover that LLMs struggle with both solving and verifying solutions to graph coloring problems via a direct prompting approach. Such a finding has implications for self-critiquing—_how can an LLM iteratively critique and solve a problem if it struggles to verify solutions?_

From here, authors attempt to solve the same graph coloring problems using GPT-4 with an approach that iteratively _i)_ generates a solution, _ii)_ critiques the solution, and _iii)_ outputs a revised solution. Several different approaches are tested within these experiments. First, GPT-4 is directly used to critique and revise its own solutions. However, we see no improvement in performance over baseline techniques in these experiments. In fact, performance oftentimes deteriorates. _Why is this the case?_ Well, we have already learned from the experiments described above that GPT-4 cannot verify solutions to graph coloring problems. As such, the model cannot recognize correct solutions and may even reject a correct answer to arrive at an incorrect final solution!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc157fcfa-c9bb-4e08-94d8-9e0aafdd174b_1336x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc157fcfa-c9bb-4e08-94d8-9e0aafdd174b_1336x930.png)

(from [1])

Given that LLMs cannot critique their own solutions, we might wonder whether better performance can be achieved by using a more reliable, external verifier. Authors in [1] craft three different techniques for studying this approach that use an external module to _i)_ verify the LLM’s solution and _ii)_ pass different amounts of information about failed solutions back to the LLM to help with iteratively revising its output. The different feedback approaches include:

- A simple pass/fail result
    
- A fail along with the first error in the solution
    
- A fail along with all errors in the solution
    

We see in these experiments that using an external verifier does modestly improve performance. However, the style and amount of feedback provided to the LLM by the verifier has no impact on performance—_providing a simple indication of pass/fail is just as good as pointing out all of the errors in the solution_!

> _“Our investigation thus raises significant grounds to be skeptical about the effectiveness of iterative prompting techniques in general, and those relying on the self-critiquing capabilities of LLMs in particular.”_ - from [1]

Such a finding leads authors in [1] to conclude that LLMs lack the ability to meaningfully self-critique and revise their responses. Rather, the ability of such models to iteratively solve problems is reliant upon the correct answer being present in the top-K responses from the language model. To verify this claim, authors test the approach of _i)_ randomly generating several solutions with the LLM and _ii)_ using an external verifier to determine if one of the solutions is correct. This approach is found to yield competitive performance; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdff6bb2f-db3a-453d-8b12-3f0705c4b74d_2164x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdff6bb2f-db3a-453d-8b12-3f0705c4b74d_2164x1008.png)

(from [1])

In the next work [2], authors[3](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-3-138301607) similarly study the ability of GPT-4 to self-critique. However, this study is conducted in the context of classical planning problems. For more details on classical planning problems and how they are solved, see [here](https://www.sfu.ca/~tjd/310summer2019/chp10_planning.html) or Section 3 of [2]. The LLM is used for both plan generation and verification. Additionally, the LLM receives feedback about its generated solution after verification—authors refer to this technique as “back-prompting”. To solve a planning problem, the LLM continuously generates a candidate solution and generates feedback based on its response[4](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-4-138301607) until either _i)_ the solution is approved as correct or _ii)_ a predefined number of iterations are met; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dca7b00-a7da-473c-8963-5d076abc7b70_1668x916.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dca7b00-a7da-473c-8963-5d076abc7b70_1668x916.png)

(from [2])

We see in [2] that self-critiquing appears to diminish the performance of LLMs in the planning domain. Compared to replacing LLM-based verification with an external verifier, the LLM-based approach suffers from false positives during verification, which leads resulting solutions to be less reliable; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f490e9f-4a1f-439e-b592-b3a63aa0c5af_1656x924.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f490e9f-4a1f-439e-b592-b3a63aa0c5af_1656x924.png)

(from [2])

Additionally, we see in [2]—similarly to results in [1]—that feedback provided by the LLM has minimal impact on generated plans. After testing multiple different levels of granularity in feedback provided to the LLM when revising its solution, there seems to be no correlation between the amount of information provided to the LLM and the accuracy of the final result; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06caad0-3d16-4107-834f-e9f5bd573851_1792x530.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06caad0-3d16-4107-834f-e9f5bd573851_1792x530.png)

(from [2])

**What do we learn?** In both [1] and [2], the poor performance of LLMs on iterative reasoning applications is due to GPT-4’s inability to verify correct solutions to the problem. Put simply, _the LLM (understandably) cannot accurately revise its solution to a problem if it is incapable of verifying whether the solution is correct in the first place_. Such a finding calls the ability of LLMs to self-critique their own solutions—an approach used quite frequently for complex reasoning problems—into question. However, we should keep in mind that these results were solely demonstrated on graph coloring and classical reasoning problems. As such, they may not be representative of the capabilities of LLMs in general.

> _“Our findings suggest that self-critiquing degrades the plan generation performance compared to when an external, sound verifier is utilized. This decline in performance can be directly attributed to the verifier LLM’s subpar results.”_ - from [2]

---

#### **[NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) [7]**

> _“Each step of NEFTune begins by sampling an instruction from the dataset, and converting its tokens to embedding vectors. NEFTune then departs from standard training by adding a random noise vector to the embeddings.”_ - from [7]

NEFTune is a simple trick for finetuning language models that can significantly boost the resulting model’s performance. In [7], this trick is studied in the context of [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) or general [instruction tuning](https://blog.research.google/2021/10/introducing-flan-more-generalizable.html). To use NEFTune, we add uniform random noise to the language model’s input word embeddings—_that’s it_! Such an approach is found to consistently improve performance across a variety of models and finetuning datasets; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F503e39f2-dc17-4867-ab08-da770f4cc39e_1278x1334.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F503e39f2-dc17-4867-ab08-da770f4cc39e_1278x1334.png)

(from [7])

To generate the random noise that is added to word embeddings, we can independently sample values in the range `[-1, 1]`, then scale these values according to the sequence length `L`, embedding dimension `d`, and two tunable parameters—α and `ɛ`. Such a scaling approach (as shown in the algorithm below) was inspired by work in adversarial ML [8].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecb43ef3-3001-4d19-a807-2a81cb8ce416_1086x516.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecb43ef3-3001-4d19-a807-2a81cb8ce416_1086x516.png)

(from [7])

When uniform generation of noise is compared to other strategies (e.g., Gaussian noise), we see that the proposed approach performs the best; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9c629ef-352a-4828-a01b-d5a961176a93_1086x386.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9c629ef-352a-4828-a01b-d5a961176a93_1086x386.png)

(from [7])

NEFTune is a simple trick that is easy to use and adds a slight (but consistent!) boost to LLM performance. For those who frequently finetune open-source LLMs and want to adopt NEFTune as a practical tool, the technique has already been added to the [TRL package](https://huggingface.co/docs/trl/index) by HuggingFace and can be quickly integrated into existing finetuning pipelines with minimal effort (see below)!

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1b5932-8788-420a-8c58-b7f15682d952_1200x975.jpeg "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1b5932-8788-420a-8c58-b7f15682d952_1200x975.jpeg)

(from [@lvwerra](https://x.com/lvwerra/status/1714287456680579210?s=20) on X)

---

#### **[Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2310.12773) [9]**

Due to the (potentially) massive societal impact of LLMs, AI researchers have invested significant effort into minimizing the number of harmful responses—_such as those that facilitate discrimination, bias, or misinformation_—generated by these models. Currently, reinforcement learning from human feedback (RLHF) is the go-to approach for aligning LLMs with human-defined criteria such as harmlessness and helpfulness. In most cases, the LLM is first pretrained, then aligned via both SFT and RLHF; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F559ca6b4-804d-4923-a7a1-ddf295188f36_1754x950.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F559ca6b4-804d-4923-a7a1-ddf295188f36_1754x950.png)

(from [11])

For example, top proprietary models (e.g., GPT-4 or Claude) are known to be aligned to minimize harmfulness. More specifically, these models are subjected to a red teaming process in which human annotators adversarially prompt the model to produce harmful responses. Then, finetuning is conducted via RLHF to reduce the prevalence of such harmful responses in the model’s future output.

> _“That is, helpfulness tends to increase harmfulness, since models are willing to obey pernicious requests, and conversely models trained to be harmless tend to be more evasive and generally less helpful.”_ - from [4]

**Underlying tension.** Although we want to encourage harmlessness within LLMs, making the model more harmless may make it less helpful (i.e., by avoiding responses to certain questions or becoming less helpful in general). For this reason, the two most commonly-used alignment criteria within LLM research are helpfulness and harmlessness (HH). These objectives have an underlying tension, as making a model more helpful may make it less harmless and vice versa. Navigating this tension and finding the correct balance between helpfulness and harmlessness during alignment is an important (and open) research problem.

In prior work [4, 10], we have seen that this HH tension is measurable—_reward models that perform well on harmlessness perform poorly on helpfulness and vice versa_. Usually, we mitigate this tension by training separate reward models for each alignment criterion. Then, we can optimize the LLM during RLHF based on two reward models that capture helpfulness and harmlessness separately. Going further, helpfulness and harmlessness are separated during annotation by having humans provide preference labels for a single alignment principle at a time—_each preference label focuses on helpfulness or harmlessness, but not both_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e397cf7-eb4d-498f-9d36-d5dd6622fb0e_1354x808.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e397cf7-eb4d-498f-9d36-d5dd6622fb0e_1354x808.png)

(from [9])

**Safe RLHF.** In [9], authors propose a more rigorous approach for balancing helpfulness and harmlessness during alignment, called Safe RLHF. Similar to prior work, a two part (SFT and RLHF) framework is leveraged for alignment and human preference data is collected separately for helpfulness and harmlessness.

During RLHF, two kinds of models are trained—a reward model and a cost model. The reward model is a standard reward model for RLHF that produces a helpfulness score for any given response, while the cost model produces a similar score that captures the response’s harmlessness. Instead of using these models as separate reward models, however, we see in [9] the cost can alternatively be used as a constraint during the optimization process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ffc1667-5aef-4d5c-a49c-aac8e5bf061b_916x262.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ffc1667-5aef-4d5c-a49c-aac8e5bf061b_916x262.png)

(from [9])

More specifically, the alignment process is reformulated to maximize the reward—_measured by the reward model (i.e., helpfulness)_—while satisfying a constraint on the cost—_measured by the cost model (i.e., harmlessness)_. The resulting objective is shown above. Using this approach, we can both ensure that the model is helpful and minimize the number of harmful responses that it produces. To solve this objective, we leverage the [Lagrangian method](https://machinelearningmastery.com/a-gentle-introduction-to-method-of-lagrange-multipliers/)[5](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-5-138301607)—an optimization technique for solving problems with constraints. This approach has a lot of similarities to [trust region policy optimization (TRPO)](https://cameronrwolfe.substack.com/i/138008873/trust-region-policy-optimization-trpo), a reinforcement learning (RL) algorithm that preceded PPO (i.e., the most commonly-used RL algorithm for RLHF).

> _“We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.”_ - from [9]

**Why should we do this?** Although the HH tension can be addressed via two separate reward models, we see in [9] that Safe RLHF may be superior due to its ability to adaptively balance the tradeoff between alignment criteria. Plus, LLMs aligned via Safe RLHF seem to perform better in experiments; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6499b0bf-8886-4b81-9cdb-4079ca1e80bd_896x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6499b0bf-8886-4b81-9cdb-4079ca1e80bd_896x1328.png)

(from [9])

Using three successive rounds of Safe RLHF, [Alpaca-7B](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model) is finetuned in [9] to produce the Beaver-7B model. Compared to the original Alpaca-7B and an SFT model, Beaver is both more helpful and less harmful, where the improvement in alignment becomes more evident after each round of Safe RLHF; see above.

**Takeaways.** Safe RLHF is a novel alignment approach that explicitly decouples conflicting alignment criteria within RLHF by modeling them as either rewards or costs. We can incorporate both rewards and costs into alignment by adding constraints based on the cost to the reward function during optimization via reinforcement learning. This approach, which has many similarities to trust region policy optimization, is shown to effectively balance conflicting alignment criteria such as helpfulness and harmlessness, which is a known issue in LLM research. However, the implementation of Safe RLHF is more complex than PPO-based RLHF due to the introduction of a constrained objective.

For those interested in learning more about RL and the inner-workings of RLHF, check out the recent series (from this newsletter) that outlines the use of RL for LLMs within AI research:

- Basics of RL for LLMs [[link](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)]
    
- Policy Gradients [[link](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)]
    
- TRPO and PPO [[link](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)]
    
- Reinforcement Learning from AI Feedback [[link](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from)]
    

---

#### **[Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511) [12]**

> _“Large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues.”_ - from [12]

[Retrieval Augmented Generation (RAG)](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1) is one of the most commonly-used practical tools for informing an LLM with up-to-date or domain-specialized information. RAG uses retrieval techniques to find relevant information that can be included as context within the LLM’s prompt. In [12], authors attempt to improve the quality of RAG by augmenting it with a self-reflection process. Using this approach (called Self-RAG), the LLM can decide:

- Whether retrieval is necessary or not
    
- If retrieved passages are actually relevant to the prompt
    

Instead of the typical RAG approach that always retrieves a fixed number of passages to include in the model’s prompt, Self-RAG can dynamically ensure that information within the prompt is both necessary and relevant. Then, by allowing the model to reflect upon its output, Self-RAG makes LLM responses both more factual and higher quality. The generation and self-reflection components within Self-RAG are handled by a single LLM that is trained end-to-end.

**How do we do this?** To teach an LLM how to self-reflect, authors in [12] introduce special tokens, called reflection tokens, to the model’s vocabulary. These reflection tokens, including retrieval and critique tokens, are predicted similarly to any other token (i.e., using [next-token prediction](https://cameronrwolfe.substack.com/p/language-model-training-and-inference)). Retrieval tokens are used to determine whether retrieval is necessary for responding to a prompt, while critique tokens are used to critique the quality of the model’s generations. This might sound vague, but let’s take a look at how Self-RAG is implemented.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6e5bad8-57ab-42e9-8fda-5afd39381bd7_1616x986.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6e5bad8-57ab-42e9-8fda-5afd39381bd7_1616x986.png)

(from [12])

Given some prompt as input, the implementation of Self-RAG (shown above) follows the steps below to generate an output:

- To determine if retrieval is necessary, the LLM first outputs a retrieval token.
    
- If retrieval is not necessary, the LLM generates a response and critiques this response (using a critique token) to determine if it is useful.
    
- If retrieval is deemed necessary, a retrieval module is called to retrieve the top-K passages based on the prompt.
    
- The LLM then uses critique tokens to predict whether each passage that is retrieved is relevant to the prompt or not.
    
- For each passage, the LLM generates a response and uses critique tokens to determine if it is _i)_ factually supported by the passage and _ii)_ useful.
    
- All responses are then ranked according to relevance, quality, and whether their information is supported by the retrieved passage.
    

The inference process for Self-RAG is formulated in more detail within the algorithm below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa71482a4-afce-4ba7-9371-87c69f28f714_1620x1130.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa71482a4-afce-4ba7-9371-87c69f28f714_1620x1130.png)

(from [12])

Determining whether each LLM response is factually supported by its corresponding passage makes fact verification easy within Self-RAG. For each generation, we can provide citations to relevant info that was used along with a corresponding self-assessment from the LLM.

**Training process.** To teach the LLM how to generate reflection tokens, authors in [12] collect a synthetic training dataset by prompting GPT-4. This dataset is initially used to train a critic model that can predict when reflection tokens should be used within a textual sequence. From here, the critic model is used to update a large corpus of training data in an offline fashion to contain reflection tokens. This dataset of textual sequences augmented with reflection tokens is used to train an LLM to use Self-RAG in an end-to-end fashion; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a68b5e2-9a42-45f3-80e0-5fe6d75462e9_1614x510.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a68b5e2-9a42-45f3-80e0-5fe6d75462e9_1614x510.png)

(from [12])

We see in [12] that self-reflection can be learned with a language modeling objective—_the model just learns how/when to predict necessary reflection tokens_. Following an end-to-end approach with pure supervised learning, Self-RAG is capable of learning how to perform the entire inference process.

> _“Our end-to-end training lets an LLM generate text informed by retrieved passages and criticize the output by learning to generate special tokens.”_ - from [12]

Authors in [12] train 7B and 13B parameter LLMs—based upon [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)—with the Self-RAG approach. From these experiments, we see that Self-RAG outperforms a variety of retrieval-augmented LLMs; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1e77ae-2405-4798-aa26-059512b322eb_1612x1426.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac1e77ae-2405-4798-aa26-059512b322eb_1612x1426.png)

(from [12])

**A note on practicality.** Self-RAG is an interesting approach that can make the RAG process more dynamic/adaptive. However, the technique proposed in [12] introduces a significant amount of added complexity to the inference process! One of the primary benefits of RAG is its simplicity and efficiency—_it is a basic, practical approach for improving LLM output quality_. Self-RAG is based upon the fundamental idea of making retrieval within RAG more dynamic. Although this idea is both useful and important, the added complexity of Self-RAG makes the approach not practical. We need a more realistic approach to use this in practice.

---

#### Honorable Mentions

- _[Prometheus](https://arxiv.org/abs/2310.08491)_ [14]: Proprietary LLMs like GPT-4 are commonly used to evaluate other LLMs, but this approach can be costly and unreliable. Authors in [14] construct a large dataset for evaluating LLM outputs and providing relevant feedback, then train a specialized LLM—called Prometheus—to perform accurate evaluations based on this dataset.
    
- _[Large Multimodal Models (LMMs)](https://huyenchip.com/2023/10/10/multimodal.html)_: this blog, written by [Chip Huyen](https://huyenchip.com/), does a great job of comprehensively outlining relevant knowledge and techniques in the hottest area of AI research—multimodal models[6](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-6-138301607).
    
- _[Survey on Factuality in LLMs](https://arxiv.org/abs/2310.07521)_ [14]: This survey studies and analyzes important considerations related to hallucinations within LLMs. Authors define the concept of factuality for LLMs, consider societal implications of incorrect information produced by these models, and analyze different causes of hallucinations within LLMs.
    
- _[GrowLength](https://arxiv.org/abs/2310.00576)_ [15]: This paper proposes a simple approach for accelerating the LLM pretraining process—meaning that the model converges faster—by progressively increasing the length of textual sequences used during training.
    
- _[FlashDecoding](https://princeton-nlp.github.io/flash-decoding/)_: This technique is an extension of FlashAttention that accelerates inference with LLMs by up to 8X on longer sequences.
    
- _[Pairwise PPO](https://arxiv.org/abs/2310.00212)_ [16]: This paper proposes an improvement to RLHF with PPO that works much better with relative/comparative feedback. Such an approach is useful because RLHF with PPO leverages human feedback in the form of relative preferences, but the reward model just produces a preference score, which requires post-processing and normalization to ensure the absolute value of rewards is within a reasonable scale. Pairwise PPO simply adapts this process to be more compatible with relative feedback.
    

---

#### Takeaways

Given that we have studied several papers across a variety of topics within this overview, we need to quickly recap the primary takeaways from each of these works. The primary learnings from each paper are outlined below.

**Is self-critique useful?** LLMs are commonly used to critique their own outputs (and those of other LLMs). In [1, 2], however, we see that LLMs struggle with critiquing their solutions on certain types of problems. Although it is unclear whether these results are representative of LLM capabilities in general, we learn that LLMs are not always capable of critiquing and revising their own responses.

**Simple training trick.** NEFTune shows us that adding random noise to input token embeddings during training and finetuning makes LLMs perform better. This simple practical trick is easy to implement and already available in TRL!

**Multiple alignment criteria.** Tension between alignment criteria—_such as helpfulness and harmlessness_—is an important practical consideration for LLMs. Prior work solves this tension via separate reward models, but new approaches (e.g., Safe RLHF) can better balance the tradeoff between alignment criteria.

**Better RAG.** Although RAG is extremely effective, it is a fixed approach that does not dynamically adapt the retrieval process based on the problem being solved. Self-RAG aims to make the retrieval process more dynamic, but introduces too much complexity to be used in practice. Nonetheless, exploring “smarter” retrieval techniques for RAG could be a fruitful area of future research.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Stechly, Kaya, Matthew Marquez, and Subbarao Kambhampati. "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems." _arXiv preprint arXiv:2310.12397_ (2023).

[2] Valmeekam, Karthik, Matthew Marquez, and Subbarao Kambhampati. "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?." _arXiv preprint arXiv:2310.08118_ (2023).

[3] Valmeekam, Karthik, Matthew Marquez, and Subbarao Kambhampati. "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?." _arXiv preprint arXiv:2310.08118_ (2023).

[4]  Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[5] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." _arXiv preprint arXiv:2308.09687_ (2023).

[6] Weng, Yixuan, et al. "Large language models are better reasoners with self-verification." _CoRR, abs/2212.09561_ (2023).

[7] Jain, Neel, et al. "NEFTune: Noisy Embeddings Improve Instruction Finetuning." _arXiv preprint arXiv:2310.05914_ (2023).

[8] Kong, Kezhi, et al. "Robust optimization as data augmentation for large-scale graphs." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022.

[9] Dai, Josef, et al. "Safe RLHF: Safe Reinforcement Learning from Human Feedback." _arXiv preprint arXiv:2310.12773_ (2023).

[10] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).

[11] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[12] Asai, Akari, et al. "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." _arXiv preprint arXiv:2310.11511_ (2023).

[13] Kim, Seungone, et al. "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models." _arXiv preprint arXiv:2310.08491_ (2023).

[14] Wang, Cunxiang, et al. "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity." _arXiv preprint arXiv:2310.07521_ (2023).

[15] Jin, Hongye, et al. "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length." _arXiv preprint arXiv:2310.00576_ (2023).

[16] Wu, Tianhao, et al. "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment." _arXiv preprint arXiv:2310.00212_ (2023).

[1](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-1-138301607)

[Tree of Thoughts (ToT) prompting](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) uses a similar self-verification approach as an intermediate step in solving complex reasoning problems.

[2](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-2-138301607)

Direct prompting refers to prompting the LLM once and generating a full solution to a problem in a single go. Iterative prompting, on the other hand, allows the LLM to iterate and critique its output via multiple prompts before outputting a final solution.

[3](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-3-138301607)

The authors of [1] and [2] are (mostly) different, but they come from the same research lab!

[4](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-4-138301607)

The generation and verification steps of iteratively solving planning problems provide different prompts to the same underlying LLM.

[5](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-5-138301607)

The idea of Lagrangians is to simply take your constraints and work them into your objective function as an additional (additive) term. This way, we can solve the problem normally, as if there are no constraints.

[6](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe#footnote-anchor-6-138301607)

Multimodal means that the model has input/output data with multiple different modalities (e.g., text, image, audio, etc.).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Ruoyu Huang's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F35d5afc4-0c30-4362-a457-a316073ed115_144x144.png)



](https://substack.com/profile/15155652-ruoyu-huang)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Domingo Gallardo's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73b13b8b-f9fe-482a-8a13-3872fab1b6d7_1929x1929.jpeg)



](https://substack.com/profile/808577-domingo-gallardo)

[

![darlin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29e363b8-0457-4b09-b998-d7799cc67d0d_2265x2265.png)



](https://substack.com/profile/127381235-darlin)

[

![Matt Gruner's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd9b6c1c-cdea-4b58-b1ad-dddb91fad763_1080x1080.jpeg)



](https://substack.com/profile/17923793-matt-gruner)

23 Likes∙

[2 Restacks](https://substack.com/note/p-138301607/restacks?utm_source=substack&utm_content=facepile-restacks)

23

- 

[](https://cameronrwolfe.substack.com/p/self-critique-self-rag-neftune-safe/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



---



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# The Story of RLHF: Origins, Motivations, Techniques, and Modern Applications

### How learning from human feedback revolutionized generative language models...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Nov 13, 2023

50

- 

[

5

](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comments)

6

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F761ed80b-9908-4d21-9d23-19ce9b424bdb_2388x1318.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F761ed80b-9908-4d21-9d23-19ce9b424bdb_2388x1318.png)

(from [1, 17, 18, 19])

For a long time, the AI community has leveraged different styles of language models (e.g., [n-gram models](https://en.wikipedia.org/wiki/Word_n-gram_language_model), [RNNs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), [transformers](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures), etc.) to automate generative and discriminative[1](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-1-138218863) natural language tasks. This area of research experienced a surge of interest in 2018 with the proposal of [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [10], which demonstrated that the transformer architecture, self-supervised pretraining, and supervised transfer learning form a powerful combination. In fact, BERT set new state-of-the-art performance on every benchmark on which it was applied at the time. Although BERT could not be used for generative tasks, we saw with the proposal of [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [11] that supervised transfer learning was effective in this domain as well. Despite these accomplishments, however, such models pale in comparison to the generative capabilities of LLMs like GPT-4 that we have today. To create a model like this, we need training techniques that go far beyond supervised learning.

> _“Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole.”_ - OpenAI [Founding Statement](https://openai.com/blog/introducing-openai) (Dec. 2015)

Modern generative language models are the combined result of numerous notable advancements in AI research, including the decoder-only transformer, next token prediction, prompting, neural scaling laws, and more. However, one of the biggest factors in creating the recent generative AI boom was our ability to align these models to the desires of human users. Primarily, alignment was made possible by directly training LLMs based on human feedback via reinforcement learning from human feedback (RLHF). Using this approach, we can teach LLMs to surpass human writing capabilities, follow complex instructions, avoid harmful outputs, cite their sources, and much more. Fundamentally, _RLHF enables the creation of AI systems that are more safe, capable, and useful_. Within this overview, we will develop a deep understanding of RLHF, its origins/motivations, the role that it plays in creating powerful LLMs, the key factors that make it so impactful, and how recent research aims to make LLM alignment even more effective.

## Where did RLHF come from?

Prior to learning about RLHF and the role that is plays in creating powerful language models, we need to understand some basic ideas that preceded and motivated the development of RLHF, such as:

- Supervised learning (and how RLHF is different)
    
- The LLM alignment process
    
- Evaluation metrics for LLMs (and the ROUGE score in particular)
    

By building a deeper understanding of these ideas, we will gain important context that makes the motivation for developing a training technique like RLHF more clear. In particular, we will see that RLHF solves major problems with supervised learning techniques that prevent language models from performing their best. By learning directly from human feedback, we can easily optimize LLMs to produce high-quality outputs that align with the motives of human users.

**General background.** Beyond these concepts, this overview will require a basic understanding of reinforcement learning (RL) and how it is used in the language modeling domain. We have covered these ideas extensively in prior posts.

- Basics of RL for LLMs [[link](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)]
    
- Policy Gradients [[link](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)]
    
- Proximal Policy Optimization PPO [[link](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)]
    

Additionally, a base-level understanding of (generative) LLMs will be helpful and necessary. For more details, check out the link below.

[Mechanics of an LLM](https://cameronrwolfe.substack.com/i/135273362/the-mechanics-of-a-language-model)

#### Supervised Learning for LLMs

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6556916-19cc-4072-9cc6-9f1563c8595e_2396x476.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6556916-19cc-4072-9cc6-9f1563c8595e_2396x476.png)

(from [2, 9])

Most generative LLMs are trained via a pipeline that includes pretraining, supervised finetuning (SFT), reinforcement learning from human feedback (RLHF), and (maybe) some additional finetuning depending on our application; see above. Within this overview, we will focus heavily upon the RLHF component of the LLM training pipeline. However, it is important to have a working understanding of pretraining and SFT as well. Read more on these topics below.

- Language Model Pretraining and Next Token Prediction [[link](https://cameronrwolfe.substack.com/p/language-model-training-and-inference)]
    
- Understanding Supervised Finetuning [[link](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)]
    

**Before RLHF.** When we contextualize modern generative LLMs with the language models that preceded them, RLHF is actually a recent addition to the training process. Previously, language models were trained to accomplish natural language tasks (e.g., summarization or classification) via a simplified transfer learning[2](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-2-138218863) procedure that includes just pretraining and finetuning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eb9bada-2964-4007-ba44-79b333baeea0_2310x1162.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eb9bada-2964-4007-ba44-79b333baeea0_2310x1162.png)

Depiction of the transfer learning process with examples (from [10, 11])

Notable examples of models that follow this approach include [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [10] and [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [11]. These models are first pretrained (using a [self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) objective) over a large corpus of unlabeled textual data, then finetuned on a downstream dataset. This finetuning process is supervised, meaning that we learn directly from human-annotated examples of correct solutions to a task. We should note, however, that the term “SFT” refers to a particular type of supervised finetuning for generative LLMs, where we finetune the model with a [next token prediction objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) over a dataset of reference generations that we want to imitate. The idea of finetuning via supervised learning (i.e., transfer learning), as is used by BERT and T5, is more generic and encompasses more than just generative-style tasks.

**Is supervised learning enough? (**Supervised) finetuning works well for closed-ended/discriminative tasks like classification, regression, question answering, retrieval, and more. Generative tasks, however, are slightly less compatible with supervised learning. Training language models to generate text in a supervised manner requires us to manually write examples of desirable outputs[3](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-3-138218863). Then, we can train the underlying model to maximize the probability of generating these provided examples. Even before the popularization of generative LLMs, such an approach was heavily utilized for tasks like summarization, where we could teach a pretrained language model to write high-quality summaries by simply training on example summaries written by humans.

> _“While this strategy has led to markedly improved performance, there is still a misalignment between this fine-tuning objective—maximizing the likelihood of human-written text—and what we care about—generating high-quality outputs as determined by humans.”_ - from [1]

This approach works well in certain domains. Even today, SFT is widely and successfully used within generative LLM research (especially in the [open-source community](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)). However, supervised training techniques have a fundamental limitation when it comes to learning how to generate text—_there is a misalignment between the supervised training objective and what we actually want_! Namely, we train the model to maximize the probability of human written generations, but what we want is a model that produces high-quality outputs. These two objectives are not always aligned, _which may lead us to wonder if a better finetuning approach exists_.

#### Language Model Alignment

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9295576d-46db-45db-b156-ccaae2ebaa99_2428x588.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9295576d-46db-45db-b156-ccaae2ebaa99_2428x588.png)

(from [2, 9])

If we examine the outputs of a generative LLM immediately after pretraining, we will see that, despite possessing a vast amount of knowledge, the model generates repetitive and uninteresting results. Even if this model can accurately predict the next token, this does not imply the ability to generate coherent and interesting text. Again, we have a misalignment between the objective used for pretraining—_next token prediction_—and what we actually want—_a model that generates high-quality outputs_. For this, we need something more. We need to _align_ the LLM.

**What is alignment?** Alignment refers to the idea of teaching an LLM to produce output that _aligns_ with human desires. When performing alignment, we typically start by defining a set of “criteria” that we want to instill within the LLM. For example, common alignment criteria might include:

- _Helpfulness_: the model fulfills user requests, follows detailed instructions, and provides information requested by the user.
    
- _Safety_: the model avoids responses that are “unsafe”.
    
- _Factuality_: the model does not “hallucinate” or generate factually incorrect information within its output.
    

To align an LLM, we can finetune the model using SFT and RLHF in sequence; see above. As discussed previously, however, SFT is a supervised learning technique that directly finetunes the LLM on human-written responses. As such, this approach is subject to the aforementioned limitations of supervised learning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0273ce3f-2b25-43f8-9efc-834242a23872_2052x760.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0273ce3f-2b25-43f8-9efc-834242a23872_2052x760.png)

RLHF directly finetunes an LLM based on human feedback

**The role of RLHF.** Unlike pretraining and SFT, RLHF is not a supervised learning technique. Rather, it leverages reinforcement learning (RL) to directly finetune an LLM based on feedback that is provided from humans; see above. Put simply, humans can just identify which outputs from the LLM that they prefer, and RLHF will finetune the model based on this feedback. Such an approach is fundamentally different from supervised learning techniques due to the fact that we can directly train the LLM to produce high-quality outputs. We just identify outputs that we like, and the LLM will learn to produce more outputs like this!

#### Evaluating Language Models (and the ROUGE Score)

Before learning more about RLHF, we need to understand how LLMs are evaluated, including the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [12] score in particular. To start this discussion, we should note that there are many ways that we could evaluate the output of a generative language model. For example, we could prompt a powerful language model like GPT-4 to evaluate the quality of a model’s output, or even leverage the reward model from RLHF (more info to come soon) to predict a preference/quality score. A full exploration of evaluation strategies for LLMs is out of scope for this overview, but those who are interested can see the article below for an in-depth discussion.

[Evaluating LLMs](https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance)

**Traditional metrics.** Prior to the LLM revolution, several popular evaluation metrics existed for language models that operated by comparing the model’s generated output to a reference output. Usually, these reference outputs are manually written by humans. [ROUGE score](https://huggingface.co/spaces/evaluate-metric/rouge), which is most commonly used to evaluate summarization tasks, is one of these classical metrics, and it works by simply counting the number of words—_or the number of [n-grams](https://kavita-ganesan.com/what-are-n-grams/#.ZUzYuezMKw0) for ROUGE-N_—in the reference output that also occur in the model’s generated output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc563515-7e46-430c-aff2-91ea88f73c9d_2170x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc563515-7e46-430c-aff2-91ea88f73c9d_2170x860.png)

Computing the ROUGE-N score

Going further, ROUGE is not the only such metric that operates by comparing a model’s output to a known reference output. We also have:

- _[Bilingual Evaluation Understudy (BLEU) score](https://huggingface.co/spaces/evaluate-metric/bleu) [13]_: commonly used to evaluate translation tasks by counting the number of matching n-grams between the generated output and the reference, then dividing this number by the total number of n-grams within the generated output.
    
- _[BERTScore](https://github.com/Tiiiger/bert_score)_ [14]: generates an embedding[4](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-4-138218863) for each n-gram in the generated output and reference output, then uses cosine similarity to compare n-grams from the two textual sequences.
    
- _[MoverScore](https://github.com/AIPHES/emnlp19-moverscore)_ [15]: generalizes BERTScore from requiring a one-to-one matching between n-grams to allow many-to-one matches, thus making the evaluation framework more flexible.
    

**Do we need something else?** Although these fixed metrics work reasonably well for tasks like summarization or translation, they don’t work well for tasks that are more open-ended, including generative tasks like [information-seeking dialogue](https://cameronrwolfe.substack.com/i/93578656/improving-alignment-of-dialogue-agents-via-targeted-human-judgements). _Why is this the case?_ There tends to be a poor correlation between these metrics and human judgements of a model’s output [16]. ROUGE and BLEU quantify the extent to which a model’s output matches some reference output. For many problems, however, there are numerous outputs a model could produce that are equally viable—_fixed metrics like ROUGE cannot account for this_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8879d5a5-7874-44c9-bfe4-38a67721f6b4_2338x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8879d5a5-7874-44c9-bfe4-38a67721f6b4_2338x858.png)

Example of ROUGE score limitations (from [2])

Again, we have a misalignment between what is being measured—_the overlap between two textual sequences_—and what we actually want to measure—_output quality_. Although such fixed metrics were heavily leveraged in the early days of NLP, model quality improved drastically as more effective training techniques like RLHF were introduced. As a result, the research community was forced to find more flexible metrics and address the poor correlation between traditional metrics like ROUGE and BLEU with the true quality of a model’s output.

## Learning from Human Feedback

> _“While [next-token prediction] has led to markedly improved performance, there is still a misalignment between this fine-tuning objective—maximizing the likelihood of human-written text—and what we care about—generating high-quality outputs as determined by humans.”_ - from [1]

[Next token prediction](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) is a training objective that is used almost universally for LLMs (i.e., during pretraining and [supervised funetuning](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)); see below. By learning to maximize the log-probabilities of (human-written) sequences of text within a dataset, this technique provides us with the ability to train language models over large corpora of unsupervised text. Despite the massive impact and general utility of next token prediction, however, the resulting LLM’s performance is highly dependent upon data quality and the model only learns to produce output that is comparable to its training set. In contrast, RLHF provides us with the ability to directly optimize an LLM based on human feedback, thus avoiding misalignment between the LLM’s training objective and the true goal of training—_producing high quality output as evaluated by human users_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fc7f778-b1d8-4c57-a753-407ec8de0b19_1968x580.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fc7f778-b1d8-4c57-a753-407ec8de0b19_1968x580.png)

(from [2, 3])

Within this section, we dive deeper into the ideas behind RLHF so that we can better understand:

- The role of RLHF in the LLM training process.
    
- The benefits of RLHF compared to supervised learning.
    

First, we will outline the details of RLHF as proposed by early works [1, 2] in LLM alignment. Then, we will cover notable research papers that have leveraged RLHF for LLM finetuning, including both the original/foundational papers in this space and new extensions of these techniques that have been explored more recently.

#### How does RLHF work?

> _“Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment”_ - from [5]

As credited by most AI researchers, RLHF was first applied to LLMs in [1] and [2], which study the problems of [abstractive summarization](https://www.prodigaltech.com/blog/extractive-vs-abstractive-summarization-how-does-it-work) and language model alignment, respectively. However, the methodologies proposed in these works draw heavily from previously-explored techniques:

- _[Better Rewards Yield Better Summaries](https://arxiv.org/abs/1909.01214)_ [4]: extends RL-based systems that directly use ROUGE score as the reward for training summarization models (i.e., this can lead to low human judgement scores) to instead obtain the reward from a reward model that is trained over human ratings of summaries.
    
- _[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)_ [5]: learns a reward model from human comparisons of model outputs and uses this reward model to finetune language models on a variety of different natural language tasks.
    

However, work in [1, 2] does not copy these techniques directly. As we will see, the modern approach to RLHF has been adapted to improve the efficiency of both human annotation and LLM finetuning. Going further, recent research explores the application of RLHF to much larger language models (i.e., [GPT-style](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) LLMs).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09dac13c-5b72-49b1-ba28-1db96ee1d988_1850x734.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09dac13c-5b72-49b1-ba28-1db96ee1d988_1850x734.png)

Implementations of RLHF explored for summarization and alignment applications in [1] and [2]

**Understanding RLHF.** We will now look at the implementation of RLHF that is used in [1] and [2]; see above. Typically, RLHF is applied in tandem with SFT—_the model obtained after pretraining and SFT serves as a starting point for RLHF_[5](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-5-138218863). Although there are slight differences between the methodologies of [1] and [2], the RLHF framework is comprised of three standard steps:

1. _Collect human comparisons_: human feedback is collected offline in large batches prior to each round of RLHF. A dataset of human feedback is comprised of prompts, several (LLM-generated) responses to each prompt, and a ranking of these responses based on human preference.
    
2. _Train a reward model_: a reward model is trained over the dataset of human comparisons to accurately predict a human preference score when given an output generated by an LLM as input.
    
3. _Optimize a policy according to the reward model_: the policy (i.e., the LLM) is finetuned using reinforcement learning—[PPO](https://cameronrwolfe.substack.com/i/138008873/proximal-policy-optimization-ppo) in particular—to maximize reward based upon human preferences scores generated by the reward model.
    

**Data collection.** Notably, the approach outlined above is heavily dependent upon the quality of feedback provided by human annotators. As such, a key aspect of successfully applying RLHF—_as is explicitly mentioned in both [1] and [2]_—is to maintain a close correspondence between annotators and AI researchers or practitioners (i.e., those training the model) via extensive onboarding, a shared communication channel, and close monitoring of agreement rates between annotators and researchers. Put simply, the rate of agreement between researchers and human annotators should be maximized to ensure that human preference data accurately reflect the desired alignment criteria.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cbd7d56-8877-4685-a4f6-3666b814017b_2082x1172.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cbd7d56-8877-4685-a4f6-3666b814017b_2082x1172.png)

Reward model architecture for RLHF

**Training the reward model.** The reward model shares the same underlying architecture as the LLM itself. However, the classification head[6](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-6-138218863) that is used for next-token prediction is removed and replaced with a regression head that predicts a preference score; see above. Interestingly, the reward model is typically initialized with the same weights as the LLM (either the pretrained model or the model trained via SFT), thus ensuring that the reward model shares the same knowledge base as the underlying LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa78c6d8f-f36c-4bc4-ae1c-e22b516da4ca_608x584.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa78c6d8f-f36c-4bc4-ae1c-e22b516da4ca_608x584.png)

(from [1])

To train the reward model, we take pairs of ranked responses as input, predict the preference score of each response, and apply a ranking loss; see above. The purpose of this ranking loss is to train the reward model to output a higher preference score for the preferred output and vice versa. In this way, the reward model’s training procedure aims to teach the model to accurately predict a human preference score given a prompt and response pair as input[7](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-7-138218863).

**Finetuning via RL.** To finetune the language model, we can formulate generating text via the LLM as an [RL problem](https://cameronrwolfe.substack.com/i/137266538/markov-decision-process-mdp). In this domain, our policy is the LLM and its corresponding parameters. Each token generated by the LLM corresponds to a single time step within the environment, and an entire episode is completed when the LLM outputs the `[EOS]` token (i.e., finishes generating a sequence). The state is given by the sequence being outputted by the LLM, and there is no explicit transition function, as we simply add each outputted token to the generated sequence. At the end of each episode, we receive a single reward—_generated by the reward model_—based upon the overall quality of the full sequence; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe475a454-a094-495e-85d9-c1345fd051e2_2306x1318.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe475a454-a094-495e-85d9-c1345fd051e2_2306x1318.png)

Finetuning setup for LLMs with RL

Using the setup described above, we can use (almost) any RL algorithm to finetune an LLM based upon feedback provided by the reward model_._ Early works in this space adopted [PPO](https://cameronrwolfe.substack.com/i/138008873/proximal-policy-optimization-ppo) as the go-to algorithm for RLHF due to its simplicity, robustness, and efficiency. As we will see, however, a variety of different algorithms have been explored and adopted in recent months.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcec88fb8-16c8-4e83-8430-83a8f7d66623_2218x1058.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcec88fb8-16c8-4e83-8430-83a8f7d66623_2218x1058.png)

(from [6])

**Iterative RLHF.** Before moving on, we should note that RLHF is typically not only applied once. Rather, most works tend to collect data in large batches and finetune the model via RLHF in an offline fashion. This process is repeated several times as more data is collected over time, allowing several “rounds” of the RLHF process to be performed. For example, [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [6] performs 5 successive rounds of RLHF (see above), while the [HH LLM from Anthropic](https://cameronrwolfe.substack.com/i/136751520/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback) [7] is finetuned via RLHF on a weekly cadence as new batches of preference data are collected. Such an approach is in contrast to more [traditional RL setups](https://cameronrwolfe.substack.com/i/137266538/q-learning-modeling-q-values-with-a-lookup-table), where data is collected and used to train the policy in an online fashion.

#### [Learning to Summarize from Human Feedback](https://arxiv.org/abs/2009.01325) [1]

The problem of abstractive summarization—_or using a language model to understand important aspects of a text and produce a human-readable summary_—has been studied for a long time. Prior to the popularization of RLHF, however, most approaches to this problem trained the language model in a supervised manner based on human-written reference summaries and performed evaluation via the ROUGE score. Although such an approach works relatively well, both supervised learning and the ROUGE score are a proxy for what is actually desired—_a model that writes high-quality summaries_. In [1], authors explore replacing supervised learning with RLHF, allowing a pretrained language model to be finetuned to produce high-quality summaries directly based on human feedback.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F377524f4-cff7-44f9-b717-ed1e842b50bb_1612x970.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F377524f4-cff7-44f9-b717-ed1e842b50bb_1612x970.png)

The approach proposed in [1], which is one of the foundational works exploring RLHF, allows us to optimize an LLM based on the quality of its responses, as assessed by human annotators. Beginning with a pretrained LLM, we can iteratively _i)_ collect human comparison data, _ii)_ train a reward model to predict human-preferred summaries and _iii)_ use the model as a reward function for finetuning via RL. With this approach, we can train an LLM to produce summaries that surpass the quality of human summaries and are even better than those produced by larger LLMs trained via supervised learning; see above.

**The methodology.** In [1], the LLM is first trained using supervised finetuning over human reference summaries, producing a supervised baseline that is later finetuned via RLHF. The methodology for RLHF proposed in [1] closely follows the general framework that we outlined in the previous section. However, authors in [1] study the problem of summarization in particular. As such, the methodology used for RLHF in [1] is specialized to this problem domain; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)

The RLHF process is comprised of three steps. First, a dataset of human feedback is collected by:

- Grabbing a textual input to summarize from the training dataset.
    
- Using several policies[8](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-8-138218863) to sample a variety of summaries for the input.
    
- Grabbing two summaries from the set of sampled responses.
    
- Asking a human annotator to identify the better of the two summaries.
    

Human comparison data is collected in large batches and used to finetune the LLM via RLHF in an offline fashion. Once the data has been collected, we use this comparison data to train a reward model that accurately predicts a human preference score given a summary produced by the LLM. From here, we use RL to finetune the model—_authors in [1] use the [PPO](https://cameronrwolfe.substack.com/i/138008873/proximal-policy-optimization-ppo) algorithm_—based on preference scores outputted by the reward model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82cc0057-8c4c-4182-8973-8845981befd6_1970x660.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82cc0057-8c4c-4182-8973-8845981befd6_1970x660.png)

Adding KL divergence to the RLHF objective (from [1])

Going beyond the basic RLHF framework we have seen so far, we see in [1] that the authors add a [KL divergence](https://cameronrwolfe.substack.com/i/138008873/kullbackleibler-kl-divergence) term to the objective optimized by RL, which penalizes the policy from becoming too different from the supervised baseline policy during RLHF; see above. Such an approach, which is frequently used and adopted from prior work, encourages exploration without mode collapse and prevents summaries written by the LLM from becoming too different from those that are seen during training. Furthermore, authors in [1] note that—_despite the ability of PPO to train an LLM that jointly models the [policy and value function](https://cameronrwolfe.substack.com/i/137421286/value-and-advantage-functions)_—they use separate models for the value function and policy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc088796c-52eb-45e5-afbc-195116ec5d1f_1612x764.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc088796c-52eb-45e5-afbc-195116ec5d1f_1612x764.png)

**Experimental results.** In [1], large pretrained models—matching the style of GPT-3 [8]—with 1.3 billion to 6.7 billion parameters are finetuned over the [TL;DR dataset](https://huggingface.co/datasets/openai/summarize_from_feedback) to summarize text. This dataset, which contains over three million posts from Reddit along with author-written summaries, is filtered to contain only 120K high-quality examples; see above. Models are first trained using SFT (i.e., the results of SFT are treated as a baseline across experiments), then afterwards with RLHF. Given that the lengths of summaries can have a drastic impact on the resulting quality score, authors in [1] constrain generated summaries to 48 tokens and finetune the model to produce summaries of this length.

Finetuning language models with human feedback outperforms a variety of strong English summarization baselines. Notably, the 1.3B summarization model outperforms a 10X larger model trained with SFT, and the 6.7B summarization model performs even better than the 1.3B model, revealing that summarization quality benefits from model scale. Furthermore, we see that summarization models trained via RLHF generalize better to new domains. In particular, the models in [1] are applied to summarizing news articles (i.e., these are not in the training data) and found to perform well without further finetuning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)

(from [1])

From here, summarization models are evaluated in terms of:

- _Coverage_: the summary covers all information from the original post.
    
- _Accuracy_: statements in the summary are accurate.
    
- _Coherence_: the summary is easy to read on its own.
    
- _Quality_: the overall quality of the summary is good.
    

When evaluated in this manner, we see that summarization models trained via RLHF benefit the most in terms of coverage, while coherence and accuracy are only slightly improved compared to supervised baseline models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1f3213a-8fd2-4703-8987-b2cfcbc5880a_662x672.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1f3213a-8fd2-4703-8987-b2cfcbc5880a_662x672.png)

(from [1])

**Looking forward.** Although RLHF was explored only in the context of summarization in [1], the authors of this paper had an incredible amount of foresight about what was to come. The approach proposed in [1] later became a standard methodology for aligning LLMs, as we will soon see with InstructGPT. Additionally, authors in [1] explicitly state their intent to leverage the proposed methodology to better align AI to human desires in the long term. This statement was made over two years prior to the proposal of ChatGPT, making the work in [1] a building block to major advancements in AI that were yet to come.

> _“The methods we present in this paper are motivated in part by longer-term concerns about the misalignment of AI systems with what humans want them to do. When misaligned summarization models make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems become more powerful and are given increasingly important tasks, the mistakes they make will likely become more subtle and safety-critical, making this an important area for further research.”_ - from [1]

#### **[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) [2]**

Going beyond the summarization domain, authors in [2] explore the use of RLHF for language model alignment by directly learning from human feedback. The resulting model, called InstructGPT [2], is the sister model and predecessor to ChatGPT. Given that this model is outlined and explained in detail within [2], this work grants us significant insight into how LLMs from OpenAI are trained.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45180b88-a11e-42e8-8910-ceca2c3b447a_1618x980.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45180b88-a11e-42e8-8910-ceca2c3b447a_1618x980.png)

(from [2])

Following an approach similar to [1], we start with a set of prompts that are either written by human annotators or collected from OpenAI’s API. We can then have annotators write responses to these prompts and finetune a pretrained LLM—_GPT-3 in particular [8]_—over these examples using SFT. Using this model, we can then collect comparison data by asking humans to select preferred outputs from the LLM and apply the same RLHF process that is outlined in [1] for finetuning. The resulting model is heavily preferred by humans (see above) and much better at following detailed instructions provided within each prompt.

> _“Making language models bigger does not inherently make them better at following a user’s intent.”_ - from [2]

**The alignment process.** Pretrained LLMs have a number of undesirable properties that we would want to fix during the alignment process, such as hallucinations, biased/toxic generations, or an inability to follow detailed instructions. To fix this, we need an alignment process that will train the LLM to satisfy human-desired criteria. In [2], the following alignment criteria are defined:

- _Helpful_: follows the user’s instructions and infers intention from [few-shot prompts](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) or other patterns.
    
- _Honest_: makes correct factual statements about the world.
    
- _Harmless_: avoids harmful outputs, such as those that denigrate a protected class or contain sexual/violent content.
    

Using RLHF, we can teach an LLM to reflect each of these qualities within its output. Then, we can evaluate whether each of these qualities are satisfied via human evaluations or by using closed-domain evaluation tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ee233ce-ea11-4928-bcbc-131c5fdc2f2f_1732x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ee233ce-ea11-4928-bcbc-131c5fdc2f2f_1732x930.png)

(from [2])

**The methodology.** Authors in [2] curate a team of 40 human annotators, who are screened with a test to judge their annotation quality, to collect finetuning data for the LLM. The approach for RLHF used in [2] matches the approach used in [1] almost completely. Using a pretrained LLM and a set of prompts for finetuning, the alignment process proceeds according to the following steps (shown above):

1. Collect human demonstrations of responses for each prompt.
    
2. Train the model in a supervised fashion over human demonstrations.
    
3. Collect comparison data.
    
4. Train a reward model.
    
5. Optimize the underlying LLM/policy with PPO.
    
6. Repeat steps 3-5
    

The distribution of prompts used for finetuning in [2] is outlined in the table below. For SFT, a dataset of over 13K prompt and response pairs is constructed. The reward model[9](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-9-138218863) is trained over 33K prompts, while a dataset of size 31K is used for finetuning with PPO. Unlike [1], human annotators are shown 4-9 responses to a prompt (i.e., instead of two) when collecting comparison data, allowing them to quickly rank these responses and generate a larger amount of comparison data more efficiently. Notably, the dataset used in [2] is 96% English.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b979ad-bd64-47c4-bfe7-64890b661ba9_1660x724.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b979ad-bd64-47c4-bfe7-64890b661ba9_1660x724.png)

(from [2])

Beyond the basic RLHF methodology used in [2], we see a few interesting tricks that are used to improve the finetuning process. First, a KL divergence term is added to the training objective used for RL (i.e., this is also done in [1]), which keeps the resulting model from diverging too much from the SFT model. Additionally, more pretraining updates are “mixed in” to the optimization process during RLHF, which mitigates the alignment tax and maintains the model’s performance across a wide set of natural language benchmarks.

> _“We were able to mitigate most of the performance degradations introduced by our fine-tuning. If this was not the case, these performance degradations would constitute an alignment tax—an additional cost for aligning the model.”_ - from [2]

The final finetuning objective used for RLHF in [2], including both added pretraining updates and the KL divergence term, is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f9c536d-024b-4bef-b0d0-275bd894d159_2330x566.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f9c536d-024b-4bef-b0d0-275bd894d159_2330x566.png)

(from [2])

**Experimental findings.** In [2], authors train three models with 1.3 billion parameters, 6 billion parameters, and 175 billion parameters (i.e., this model matches the architecture of [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt) [8]). From these experiments, we learn that human annotators prefer InstructGPT outputs over those of GPT-3, even for models with 10X fewer parameters; see below. Such a result has similarities to observations in [1], where we also see that finetuning via RLHF enables much smaller models to outperform larger models trained in a supervised manner.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08415ad7-db55-4f46-8415-2fb3da1c9ab6_1350x1348.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08415ad7-db55-4f46-8415-2fb3da1c9ab6_1350x1348.png)

(from [2])

Notably, outputs from InstructGPT-1.3B are preferred to those of GPT-3, which has 100X more parameters. Additionally, we see that InstructGPT-175B produces outputs that are preferred to GPT-3 85% of the time[10](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-10-138218863). Going further, InstructGPT models are found to more reliably follow explicit constraints and instructions provided by a human user; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc9280f9-a159-4e81-ab17-86faf28f47ba_1876x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc9280f9-a159-4e81-ab17-86faf28f47ba_1876x882.png)

(from [2])

Compared to pretrained and supervised models, InstructGPT is also found to be _i)_ more truthful, _ii)_ slightly less toxic, and _iii)_ capable of generalizing the ability to follow instructions across different domains. For example, InstructGPT is able to follow instructions for summarizing or answering questions about code and handle prompts written in different languages, despite the finetuning dataset—_which is 96% English!_—lacking sufficient data within this distribution. Although the model did not get recognized as widely as ChatGPT, InstructGPT was a major step forward in AI and alignment research that proposed a lot of the foundational concepts used today for creating high-quality foundation language models.

#### Modern Variants of RLHF

RLHF was shown to be highly effective in [1] and [2], but these were early works in the space of LLM alignment. Over the last year, numerous modifications to the RLHF methodology have been proposed, including completely new algorithms for aligning LLMs. Here, we will quickly review some recent advancements in RLHF to gain a better understanding of how these techniques are used today.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e5666f4-b294-40ad-8fa6-72f5c68e2f53_2210x1328.png)

(from [6])

**LLaMA-2 [6]** was one of the first open-source LLMs to invest extensively into alignment via RLHF (i.e., other open-source LLMs relied mostly upon SFT; see [here](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)). The RLHF process in [6] is comparable to [1, 2] with a few modifications:

- Human annotators only compare two responses to a prompt at once, instead of 4-9 in the case of InstructGPT.
    
- Instead of collecting binary preference data, human annotators are instructed to identify responses that are significantly or moderately better than others (i.e., more granular degrees of comparison).
    
- Comparisons are collected with respect to a single alignment criteria at a time (e.g., a human annotator may receive a prompt and response pair focused upon minimizing harmfulness in particular).
    

The approach outlined above slightly modifies the RLHF annotation process to _i)_ make it easier for human annotators, _ii)_ yield more accurate comparisons, and _iii)_ collect more granular feedback data on a model’s outputs. To incorporate non-binary human feedback into RLHF, we can simply add a margin into the training objective for a reward model, which encourages responses with a large difference in quality to be pushed further apart in their preference scores; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F661c833b-0102-4440-a3ae-995a6a10f688_1528x1150.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F661c833b-0102-4440-a3ae-995a6a10f688_1528x1150.png)

(from [6])

Going further, authors in [6] explore two different reinforcement algorithms for finetuning via RLHF: PPO and Rejection Sampling. After performing multiple rounds of RLHF, we see that the combination of these two learning algorithms in tandem can drastically improve learning efficiency. See [here](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e397cf7-eb4d-498f-9d36-d5dd6622fb0e_1354x808.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e397cf7-eb4d-498f-9d36-d5dd6622fb0e_1354x808.png)

(from [17])

**Safe RLHF [17]** is a recently-proposed modification to the basic RLHF algorithm. As previously discussed, the alignment process for LLMs requires the definition of several alignment criteria. However, the alignment criteria that we define might conflict with each other sometimes. For example, harmlessness and helpfulness are two commonly-used alignment criteria that tend to conflict with each other. Typically, these cases are addressed by training a separate reward model to capture each alignment criteria, thus avoiding any conflict in modeling human preferences. However, Safe RLHF proposes a new learning algorithm that better balances conflicting alignment criteria via the definition of rewards and costs within the alignment process; see above and read more [here](https://cameronrwolfe.substack.com/i/138301607/safe-rlhf-safe-reinforcement-learning-from-human-feedback).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c98505-9090-454f-9e42-3deae6d72837_2150x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c98505-9090-454f-9e42-3deae6d72837_2150x1004.png)

(from [18])

**Pairwise PPO [18].** One interesting aspect of RLHF is the manner in which the reward model is trained and used. Namely, the reward model is trained based on relative scores—_we want the preferred response to be scored higher than the other response_. During optimization with PPO, however, we directly use the scalar output of the reward model as a training signal—_we are not using the reward model’s output in a comparative manner_. To address this, authors in [18] propose a variant of PPO (shown above) that is modified and optimized to work better with the comparative human preference data that is collected for RLHF. Learn more [here](https://bair.berkeley.edu/blog/2023/10/16/p3o/).

**Reinforcement Learning from AI Feedback (RLAIF).** Although RLHF is useful, one major downside of this technique is that it requires a lot of human preference data to be collected. For example, LLaMA-2 uses over 1M human preference examples for alignment via RLHF. To mitigate this requirement upon human annotation, a recent line of work has explored automating human preference annotations with the use of a generic LLM. In other words, we perform RLHF with feedback provided by AI instead of humans. See [here](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5b9aa6c-06fe-4dba-ab47-b6d14f2c1eba_1606x560.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5b9aa6c-06fe-4dba-ab47-b6d14f2c1eba_1606x560.png)

(from [19])

**Direct Preference Optimization (DPO) [19].** Another downside of RLHF is that it is an (arguably) complicated training procedure that is oftentimes unstable, relies upon reinforcement learning, and trains an entirely separate reward model to automate human preference scores. As a solution, authors in [19] propose DPO. This approach, depicted in the figure above, is a simpler, but equally performant, alternative to RLHF that is more stable and eliminates the need for a reward model by finetuning the LLM using human preferences directly. Learn more [here](https://ai.plainenglish.io/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models-bae1c6d7ec29).

**Did I miss something?** A ton of research on LLM alignment and RLHF has been explored recently. If you are aware of any other important works that should be covered here, feel free to let me know in the comments! You can also learn more about RLHF variants by reading the overview below.

[

![](https://substackcdn.com/image/fetch/w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AI

LLM Training: RLHF and Its Alternatives

I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials. RLHF is an integral part of the modern LLM training pipeline due to its ability to incorporate human preferences into the optimization landscape, which can improve the model's helpfulness and safety…

Read more

2 years ago · 109 likes · 4 comments · Sebastian Raschka, PhD

](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives?utm_source=substack&utm_campaign=post_embed&utm_medium=web)

#### What makes RLHF so impactful?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51e06e23-946a-4eec-a804-d36fe6cfbecf_1946x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51e06e23-946a-4eec-a804-d36fe6cfbecf_1946x536.png)

(from [6])

We have now seen RLHF successfully used in several domains, providing clear empirical evidence that RLHF is an effective finetuning technique. However, we might wonder: _Why does RLHF work so much better than supervised learning?_ To answer this question, we will quickly overview the primary benefits of RLHF and highlight the key aspects of this methodology that make it so impactful.

**Human annotation process.** The first notable difference between supervised learning and RLHF is the manner in which data is annotated. For supervised learning, we train the LLM using human-provided demonstrations of high-quality output. This means that, for every training example, a human must manually write a full, high-quality response to the given prompt. In contrast, RLHF generates responses to prompts automatically via an LLM and simply asks the human annotator to rank several responses to the same prompt; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5217cf28-86ac-451a-ab9c-de566d752ba7_1470x1196.png)

(from [7])

Because ranking outputs is a much easier task compared to writing outputs from scratch, the annotation strategy for RLHF lessens the cognitive load of human annotators, which leads to several notable benefits:

1. Annotations are higher quality (i.e., more accurate).
    
2. Collecting annotations is faster and more efficient.
    
3. Individual annotations can be focused upon a particular alignment principle.
    

Given that reinforcement learning generally requires more data (compared to supervised learning) to perform well, the annotation style of RLHF is a crucial aspect of its effectiveness. By defining alignment criteria and focusing human comparisons on specific aspects of the alignment process, we can quickly collect a large volume of accurate comparison data for finetuning with RLHF.

**Beyond human quality.** The annotation strategy used for RLHF also has benefits beyond annotation efficiency. Namely, we should notice that all responses used for collecting comparison data within RLHF are generated automatically by an LLM. _This means that RLHF can train an LLM over responses that go beyond the writing capabilities of human annotators and, therefore, has the potential to surpass human performance._ In comparison, supervised learning is constrained to the quality of responses that are manually written by human annotators.

> _“During annotation, the model has the potential to venture into writing trajectories that even the best annotators may not chart. Nonetheless, humans can still provide valuable feedback when comparing two answers, beyond their own writing competencies.”_ - from [6]

Writing high-quality responses from scratch is incredibly difficult. As such, the quality of human annotations used for supervised finetuning can vary drastically, which introduces noise into the learning process. Remember, _the model learns from all responses shown to it during training, including both high-quality and lower quality data._ Plus, a human annotator (obviously) is not capable of writing a response that goes beyond their personal writing ability. However, this same annotator will usually be capable of recognizing when one response is better than another, even when they cannot personally write a response of comparable quality from scratch.

**Accurately capturing response quality.** Finally, we see in [1] and [2] that the reward model that is created for RLHF is surprisingly accurate at capturing the quality of model responses[11](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-11-138218863). Compared to automatic metrics like ROUGE, reward models provide a more consistent and accurate evaluation of model output quality, as judged by the agreement rate with human annotators. As such, optimizing the LLM based on the preference scores from this reward model tends to produce a model that performs quite well. We directly train the model to produce outputs that the reward model deems as preferable!

Given that reward models can so accurately evaluate response quality, the reward model from RLHF actually has two useful purposes: _i)_ finetuning the underlying policy and _ii)_ evaluating the quality of LLM outputs. Although this second purpose is somewhat of a side effect, it is nonetheless beneficial.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e48cecd-3537-488e-9fad-fc2aa3df966f_524x558.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e48cecd-3537-488e-9fad-fc2aa3df966f_524x558.png)

(from [1])

Going further, we see in [1] that reward models tend to obey [scaling laws](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt) that are somewhat similar to those of LLMs. In particular, the quality of the reward model improves as we increase the size of the model and the amount of comparison data used for training; see above. In [2], authors note that using a much larger reward model—175 billion parameters, as opposed to 6 billion—can lead to instability. However, reward model scaling trends are also observed in more recent works like LLaMA-2 [6], where we see that reward model quality continues to improve as larger models and more data are used; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd344ac-42e9-4654-bd80-57884f74c6a7_1272x532.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd344ac-42e9-4654-bd80-57884f74c6a7_1272x532.png)

(from [6])

## Closing Thoughts

We should now have a deep understanding of RLHF and its impactful role in the training of generative language models. RLHF is a key innovation—_along with several others (e.g., transformers, self-supervised learning, etc.)_—that catalyzed the recent generative AI boom. Despite being widely used across many applications even today, supervised learning techniques fall short when it comes to training useful and aligned language models. By enabling us to train generative LLMs directly from human feedback, RLHF empowered AI researchers to create generative models that were shockingly informative/useful and could even exceed the writing capabilities of humans. Some key takeaways are outlined below.

**Limitations of supervised learning.** To train a generative language model with supervised learning, we collect a dataset of human-written responses to prompts and finetune the model to mimic these responses. Such an approach, despite being widely used, has a few notable problems. First, collecting training data is hard, as it requires humans write high-quality reference responses from scratch. Furthermore, there is a notable misalignment between the training objective used for supervised learning and what we actually want. Namely, we train the model to produce outputs similar to its training data, but what we actually want is a model that generates useful outputs—_these two objectives are not always the same!_

**RLHF provides a solution.** To solve this misalignment, _why not just directly train the language model to produce outputs that we (the user) like?_ This is exactly the idea that inspired RLHF! To apply RLHF, we collect human preference data by having humans identify preferable responses, train a reward model to learn patterns in human preferences, then finetune the LLM with reinforcement learning to produce outputs that are more preferable to humans. RLHF is a flexible and effective alignment approach that allows us to create models that accomplish a variety of complex tasks in the exact manner desired by its human users.

**Continued evolution.** Despite the massive impact that RLHF has had on LLM alignment, it has notable limitations! For example, it requires a lot of human preference data to be curated (which can be expensive!), can struggle to handle conflicts between several alignment criteria, suffers from instability, and is more complicated compared to supervised learning. For this reason, AI researchers are actively iterating upon and improving RLHF, leading to the development of many RLHF variants such RLAIF, Safe RLHF, Pairwise DPO, and more.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[2] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[3] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[4] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." _arXiv preprint arXiv:1909.01214_ (2019).

[5] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." _arXiv preprint arXiv:1909.08593_ (2019).

[6] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[7] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." _arXiv preprint arXiv:2204.05862_ (2022).

[8] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[9] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[10] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[11] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[12] Chin-Yew, Lin. "Rouge: A package for automatic evaluation of summaries." _Proceedings of the Workshop on Text Summarization Branches Out, 2004_. 2004.

[13] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_. 2002.

[14] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." _arXiv preprint arXiv:1904.09675_ (2019).

[15] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." _arXiv preprint arXiv:1909.02622_ (2019).

[16] Sai, Ananya B., Akash Kumar Mohankumar, and Mitesh M. Khapra. "A survey of evaluation metrics used for NLG systems." _ACM Computing Surveys (CSUR)_ 55.2 (2022): 1-39.

[17] Dai, Josef, et al. "Safe RLHF: Safe Reinforcement Learning from Human Feedback." _arXiv preprint arXiv:2310.12773_ (2023).

[18] Wu, Tianhao, et al. "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment." _arXiv preprint arXiv:2310.00212_ (2023).

[19] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." _arXiv preprint arXiv:2305.18290_ (2023).

[1](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-1-138218863)

Examples of generative tasks include dialogue/chat and summarization, while discriminative tasks are those that predict something other than free-form text (e.g., classification or multiple choice question answering).

[2](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-2-138218863)

Transfer learning is a commonly-used term in AI research that refers taking a pretrained model and finetuning it to accomplish a particular task.

[3](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-3-138218863)

Alternatively, we could automatically generate training examples using a framework like [self-instruct](https://arxiv.org/abs/2212.10560).

[4](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-4-138218863)

Embedding-based metrics like BERTScore are nice because they can account for synonyms being present in the generated output and reference summary, while metrics like ROUGE and BLEU require and exact match between n-grams.

[5](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-5-138218863)

This is not always the case. However, initializing RLHF with a model trained via SFT tends to provide a much better starting point for the optimization process that enables faster learning, whereas starting with a model that has only underwent pretraining may require more exploration to find a high-quality policy.

[6](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-6-138218863)

The term “head” is commonly used in the AI community to refer to a single (usually feed-forward) layer at the end of the neural network that takes the network’s output as input and performs classification or regression.

[7](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-7-138218863)

Given that the reward model is trained based on relative preferences score via the ranking loss, we typically have to normalize preference values outputted by the reward model after training. For example, authors in [1] note that they normalize reward model outputs such that the rewards predicted over the training dataset have a mean score of 0. Later in the overview, we will see recent variants of RLHF that avoid this issue.

[8](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-8-138218863)

Responses to prompts are sampled from several different sources, including the current policy, the initial policy, human-written reference summaries, summaries from baseline models, summaries from models trained via supervised learning, and more.

[9](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-9-138218863)

Authors in [2] always use a 6B parameter LLM as the reward model. The largest model (i.e., 175B parameters) is found to be unstable during training in certain cases. Later work has shown that reward models follow strict scaling laws, meaning that larger models can achieve better reward accuracy. However, using smaller LLM architectures for the reward model is relatively common nonetheless.

[10](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-10-138218863)

When few-shot prompting is used, InstructGPT is only preferred 71% of the time. But, this is still an impressive win rate!

[11](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations#footnote-anchor-11-138218863)

Given that the RLHF optimization process is guided by the output of our reward model, it is incredibly important for this model’s output to be accurate! _Post-RLHF model quality is heavily dependent upon the accuracy of the reward model_.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Rajan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F040ca456-802b-491d-8bb5-eeb4636105ae_144x144.png)



](https://substack.com/profile/37169147-rajan)

[

![datoslios's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c3a858-15cc-4dd2-969f-7f8d12c04614_144x144.png)



](https://substack.com/profile/178012844-datoslios)

[

![Hardian Lawi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5d216b2-3b63-4b81-a08c-af7eac04702b_144x144.png)



](https://substack.com/profile/7532692-hardian-lawi)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

50 Likes∙

[6 Restacks](https://substack.com/note/p-138218863/restacks?utm_source=substack&utm_content=facepile-restacks)

50

- 

[

5

](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comments)

6

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sairam Sundaresan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55c2183f-7755-4692-8063-59df2751f834_800x800.png)



](https://substack.com/profile/85853406-sairam-sundaresan?utm_source=comment)

[Sairam Sundaresan](https://substack.com/profile/85853406-sairam-sundaresan?utm_source=substack-feed-item)

[Gradient Ascent](https://newsletter.artofsaience.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年11月14日](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comment/43584363 "2023年11月14日 02:46")

Liked by Cameron R. Wolfe, Ph.D.

This is phenomenally detailed. Going to come back to this in a while to absorb all the information. Great work, Cameron. This must have taken a lot of effort to put together

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comment/43584363)

[

![Finn's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe2bbe13-f241-4b90-bbf2-712748c77992_993x993.png)



](https://substack.com/profile/139930293-finn?utm_source=comment)

[Finn](https://substack.com/profile/139930293-finn?utm_source=substack-feed-item)

[2023年11月13日](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comment/43560629 "2023年11月13日 20:05")

Liked by Cameron R. Wolfe, Ph.D.

Thanks Cameron!

Like (1)

Reply

Share

[3 more comments...](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More

### Training a specialized LLM over your own data is easier than you think...

Nov 27, 2023

111

- 

[

14

](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comments)

11

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8867fda-7270-4990-9710-0c7cf2e13878_2354x1304.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8867fda-7270-4990-9710-0c7cf2e13878_2354x1304.png)

(from [1, 3, 6, 15, 19])

Due to the surge of interest in large language models (LLMs), AI practitioners are commonly asked questions such as: _How can we train a specialized LLM over our own data?_ However, answering this question is far from simple. Recent advances in generative AI are powered by massive models with many parameters, and training such an LLM requires expensive hardware (i.e., many expensive GPUs with a lot of memory) and fancy training techniques (e.g., [fully-sharded data parallel training](https://engineering.fb.com/2021/07/15/open-source/fsdp/)). Luckily, these models are usually trained in two phases—_pretraining_ and _finetuning_—where the former phase is (much) more expensive. Given that high-quality pretrained LLMs are readily available online, most AI practitioners can simply download a pretrained model and focus upon adapting this model (via finetuning) to their desired task.

> _“Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks.”_ - from [1]

Nonetheless, the size of the model does not change during finetuning! As a result, finetuning an LLM—_though cheaper than pretraining_—is not easy. We still need training techniques and hardware than can handle such a model. Plus, and every finetuning run creates an entirely separate “copy” of the LLM that we must store, maintain, and deploy—_this can quickly become both complicated and expensive_!

**How do we fix this?** Within this overview, we will learn about a popular solution to the issues outlined above—_parameter-efficient finetuning_. Instead of training the full model end-to-end, parameter-efficient finetuning leaves pretrained model weights fixed and only adapts a small number of task-specific parameters during finetuning. Such an approach drastically reduces memory overhead, simplifies the storage/deployment process, and allows us to finetune LLMs with more accessible hardware. Although the overview will include a many techniques (e.g., prefix tuning and adapter layers), our focus will be upon Low-Rank Adaptation (LoRA) [1], a simple and widely-used approach for efficiently finetuning LLMs.

## Background Information

Before diving into LoRA and its (many) variants, we need to go over some background information that’s necessary for understanding the rest of the overview. Given that this writeup is already quite long, we’ll keep this section brief and provide links to further reading for those who are less familiar.

#### The Structure of a Language Model

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bfba492-5e13-4883-bb7a-fa85c3ec810e_778x1168.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bfba492-5e13-4883-bb7a-fa85c3ec810e_778x1168.png)

(from [25])

The first step in understanding language models is developing a solid grasp of the architecture upon which these models are based—_the transformer architecture_ [25]; see above. The transformer architecture was originally proposed for Seq2Seq tasks (e.g., summarization, translation, conditional generation, etc.) and contains both an encoder and a decoder component.

- _Encoder_: each block has bidirectional self-attention and a feed-forward layer.
    
- _Decoder_: each block has causal self-attention, cross attention, and a feed-forward layer.
    

**Transformer variants.** The originally-proposed transformer architecture is usually referred to as an encoder-decoder transformer. However, two other popular variants of the transformer architecture exist: encoder-only and decoder-only transformers. These architectures are exactly as their name indicates—_they use only the encoder or decoder portion of the transformer architecture_. Notably, the decoder-only architecture removes not only the encoder, but also all of the cross attention layers in the decoder, as there is no longer any encoder to which the cross attention layers can attend. A summary of these different architectural variants, along with popular examples of each, is provided in the table below.

[

![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f98b75-1750-4f68-ba34-f3ef78da54f0_2412x1354.jpeg "Image")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f98b75-1750-4f68-ba34-f3ef78da54f0_2412x1354.jpeg)

Outline of the three major transformer architecture variants and their primary use cases (from [9, 10, 25])

Although [GPT-style](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) generative LLMs [14] (i.e., large decoder-only transformers) are very popular today, many types of useful language models exist. Encoder-only models (e.g., BERT [10]) are used for discriminative language understanding tasks (e.g., classification or retrieval)[1](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-1-138861994), while encoder-decoder models (e.g., T5 [10]) are great for performing conditional generation (e.g., summarization, translation, text-to-SQL, etc.). Learn more about these models and their popular use cases below.

- Language Understanding with BERT [[link](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)]
    
- T5: Text-to-Text Transformers [[Part One](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)] [[Part Two](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part-354)]
    

**Transformer layer components.** Within each transformer layer, two primary transformations are used: self-attention and a feed-forward transformation. Additionally, different styles of self-attention—_either bidirectional or masked_—are used within the decoder and encoder portions of the architecture. To learn more about these operations, check out the links below.

- _Bidirectional Self-Attention_ [[link](https://x.com/cwolferesearch/status/1641932082283700226?s=20)]: used in the encoder
    
- _Masked (Decoder) Self-Attention_ [[link](https://x.com/cwolferesearch/status/1644773244786941952?s=20)]: used in the decoder
    
- _Feed-Forward Layers_ [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]: used in both the encoder and the decoder
    

Additionally, encoder-decoder models have an extra [cross attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention) module within each block of the decoder following the masked self-attention.

**Putting it together.** Despite learning the concepts above, understanding how these pieces combined together within an LLM can be difficult. Check out the link below for more details on the structure and mechanics of language models.

[Mechanics of an LLM](https://cameronrwolfe.substack.com/i/135273362/the-mechanics-of-a-language-model)

#### How are LLMs trained?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f1ee28d-9941-4a6f-b10a-b5d0cd2ada46_2440x480.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f1ee28d-9941-4a6f-b10a-b5d0cd2ada46_2440x480.png)

The LLM training pipeline (from [12, 13])

Modern language models are trained in several steps. For example, encoder-only and encoder-decoder models are trained via a transfer learning approach that includes self-supervised pretraining and finetuning on a downstream task, while generative (GPT-style) LLMs follow the multi-step training procedure shown in the figure above. Within this discussion, we will mostly focus upon the training procedure of generative LLMs, which are the primary topic of this overview.

**Pretraining.** All language models are pretrained using some form of a self-supervised learning objective[2](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-2-138861994). For example, generative language models are usually pretrained using a next token prediction objective, while encoder-only and encoder-decoder models commonly use a Cloze task. Read more below.

- Pretraining with Next Token Prediction [[link](https://cameronrwolfe.substack.com/p/language-model-training-and-inference)]
    
- Pretraining with Cloze (Masked Language Modeling) [[link](https://cameronrwolfe.substack.com/i/76273144/training-bert)]
    

Self-supervised learning techniques do not rely on manual human annotation—_the “labels” used for supervision are already present in the data itself_. For example, next token prediction predicts the next word/token in a sequence of tokens sampled from a textual corpus (e.g., a book), while Cloze tasks mask and predict tokens in a sequence. We can collect massive datasets of unlabeled text (e.g., by scraping the internet) to use for self-supervised pretraining. Due to the scale of data available, the pretraining process is quite computationally expensive. So, we perform pretraining once and repeatedly use this same [foundation model](https://crfm.stanford.edu/) as a starting point for training a specialized model on many different tasks and applications.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0ba3b1a-c748-415f-b615-ddcdb1de9cae_1774x964.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0ba3b1a-c748-415f-b615-ddcdb1de9cae_1774x964.png)

(from [12])

**Alignment.** The alignment process, which is only applicable to generative LLMs, refers to the process of finetuning a language model such that its output aligns with the expectations of human users. To perform alignment, we first define several criteria (e.g., helpfulness, harmlessness, the ability to follow instructions, etc.) that we want to instill within the model. Then, we can align the model based on these criteria using two finetuning techniques in sequence (shown above):

- Supervised Finetuning (SFT) [[link](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)]
    
- Reinforcement Learning from Human Feedback (RLHF) [[link](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)]
    

SFT trains the language model over a set of high-quality reference outputs using a next token prediction objective, and the LLM learns to mimic the style and format of responses within this dataset. In contrast, RLHF collects feedback (from humans) on the LLM’s output and uses this feedback as a training signal.

**Finetuning.** After pretraining and alignment (i.e., an optional step for generative LLMs), we have a generic foundation model that can be used as a starting point for solving many tasks. To solve a task, however, we must adapt the language model to this task, usually via finetuning (i.e., further training on data from a task). The combination of pretraining followed by finetuning is commonly referred to as _transfer learning_, and this approach has numerous benefits:

- Pretraining, although expensive, is only performed once and can be shared as a starting point for finetuning on several different tasks.
    
- In most cases, we can download a pretrained model online and only focus on performing finetuning, which is cheap compared to pretraining.
    
- Finetuning a pretrained model usually performs better than training a model from scratch, despite requiring less finetuning data and training time.
    

Although finetuning is computationally cheap relative to pretraining or training from scratch, it can still be quite expensive, especially for the massive generative LLMs that have recently become popular. Within this overview, we will focus upon this finetuning process and how we can make it more efficient from a compute and memory perspective, _allowing practitioners to train specialized LLMs with fewer/smaller GPUs and less investment of time and money_.

#### Model Quantization

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b1499dd-d21f-4dd1-8b93-390c1c3141d2_1304x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b1499dd-d21f-4dd1-8b93-390c1c3141d2_1304x838.png)

Quantization in the forward and backward pass of neural network

The idea of quantization in deep learning refers to quantizing (i.e., converting to a format that uses fewer bits) a model’s activations and weights—_during the forward and/or backward pass_—such that we can use low precision arithmetic during training and/or inference. Assuming we have access to an accelerator that supports arithmetic at lower precisions, we can actually save costs by performing the model’s forward and backward pass using lower precision. Plus, we usually do not sacrifice performance when performing such quantization—_we get these efficiency benefits basically for free_! As we will see, quantization techniques are commonly combined with LoRA to save costs during both training and inference.

The literature on quantization for deep learning is vast. However, proposed techniques can be (roughly) categorized into three primary areas:

- _Quantized Training_ [[link](https://cameronrwolfe.substack.com/p/quantized-training-with-deep-networks-82ea7f516dc6)]: perform quantization during training to make the training process more efficient.
    
- _Post-Training Quantization_ [[link](https://lightning.ai/docs/pytorch/stable/advanced/post_training_quantization.html)]: quantize a model’s weights after training to make inference more efficient.
    
- _Quantization-Aware Training_ [[link](https://towardsdatascience.com/inside-quantization-aware-training-4f91c8837ead)]: train in a manner that makes the model more amenable to quantization, thus avoiding performance degradations due to post-training quantization[3](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-3-138861994).
    

**Side note.** One of my favorite applications of quantization is automatic mixed-precision (AMP) training. With AMP, we can train arbitrary neural networks using lower precision in certain portions of the network architecture. This approach can cut training time in half while achieving comparable performance without any code changes. To try this out, check out the [AMP package](https://developer.nvidia.com/automatic-mixed-precision) from NVIDIA, which has easy-to-use integrations with common packages like PyTorch, or the [mixed-precision training options](https://huggingface.co/docs/transformers/v4.18.0/en/performance) within HuggingFace.

## Adaptation of Foundation Models

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d82d066-b7f4-46bd-b6c6-f2dc4a1f9d37_1456x616.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d82d066-b7f4-46bd-b6c6-f2dc4a1f9d37_1456x616.png)

Schematic depiction of training from scratch, pretraining, and transfer learning (i.e., pretraining + finetuning)

Self-supervised pretraining has been heavily leveraged by language models even before the advent of the GPT-style LLMs that are so popular today. Put simply, self-supervised learning allows us to meaningfully pretrain language models over large amounts of unlabeled text. The resulting model can then be finetuned—_or trained further_—to accomplish some downstream task; see above. Given that pretraining endows the model with a certain amount of linguistic understanding, finetuning is more efficient than training from scratch—_the model converges more quickly, performs better, and requires less data to perform well_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bd26cfe-b962-470a-a7de-1ebecd47ff56_2338x496.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bd26cfe-b962-470a-a7de-1ebecd47ff56_2338x496.png)

(from [9, 10])

For example, BERT and T5 [9, 10] are pretrained using a Cloze objective[4](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-4-138861994) and finetuned to solve a variety of downstream tasks; see above. Generative LLMs follow a similar approach, but pretraining is performed with a [next token prediction objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction), which is more conducive to generating text. Then, the model can be finetuned using a variety of different techniques (e.g., [SFT](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised), [RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations), [task-specific finetuning](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt), etc.) to fulfill its role in a desired application.

**Adapting foundation models.** Despite the large variety of language models that exist, self-supervised pretraining is a common characteristic between most of them. _Why?_ Pretraining can be quite expensive due to the amount of unlabeled data on which we want to train[5](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-5-138861994). However, the pretraining process only needs to be performed once and can be shared (either publicly or within an organization) afterwards. We can finetune this single pretrained checkpoint any number of times to accomplish a variety of different downstream tasks.

> _“Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications.”_ - from [1]

For generative LLMs, the pretraining process is [especially expensive](https://www.mosaicml.com/blog/gpt-3-quality-for-500k), but it plays a massive role in the model’s downstream performance. In order for generative LLMs to perform well, we need to pretrain them over a large, high-quality corpus of data. Luckily, however, we usually don’t need to pay for the (massive) cost of this pretraining process—_a variety of pretrained (base) LLMs are openly available online_; e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone), [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up), [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source), and [Mistral](https://mistral.ai/news/announcing-mistral-7b/). Using finetuning or in-context learning, these models can be repurposed to solve a variety of different tasks. We will now take a look at several such approaches and consider how these models can be most efficiently adapted to solve a task.

#### In-Context Learning

Pretrained LLMs have rudimentary abilities to solve problems via prompting, but the alignment process improves their instruction following capabilities, making the model more capable of solving tasks via in-context learning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82c24150-4a87-4d37-808b-dd711c301cab_1964x978.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82c24150-4a87-4d37-808b-dd711c301cab_1964x978.png)

In-context learning refers to solving a variety of problems with a single model by writing task-specific prompts (i.e., textual inputs to the language model), rather than finetuning the model on each task. For more information on the prompting process and different techniques that exist, check out the articles below:

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    

Because no finetuning is required and one model can be used for all tasks, in-context learning is (by far) the easiest way to adapt an LLM to solve a downstream task. However, this approach lags behind the performance of finetuning, making finetuning a common approach for creating specialized LLMs in practice.

#### Full Finetuning

If in-context learning does not perform well enough for our needs, our next option is to finetune the model on our dataset. The most naive approach is full finetuning, where we train the model end-to-end and update all of its parameters over new data. This approach works well, but it has several downsides:

- The finetuned model contains as many parameters as the original model, which becomes more burdensome for large models (e.g., LLMs).
    
- We must store all of the model’s parameters each time we either retrain the model or train it on a new/different task.
    
- Training the full model is both compute and memory intensive.
    
- End-to-end training might require more hyperparameter tuning or data to avoid overfitting and achieve the best possible results.
    

Full finetuning becomes burdensome if we _i)_ want to frequently retrain the model or _ii)_ are finetuning the same model on many different tasks. In these cases, we end up with several “copies” of an already large model. Storing and deploying many independent instances of a large model can be challenging; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60d46502-4340-48d7-8db6-057993f82060_1622x816.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60d46502-4340-48d7-8db6-057993f82060_1622x816.png)

(from [14])

Beyond the burden of storing and deploying multiple finetuned models, training large models end-to-end is difficult in itself—_the memory overhead and amount of computation required_[6](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-6-138861994) _is significant_. To learn more, take a look at the link below, which details the finetuning process for the [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) 70B model! As we will see, finetuning large models end-to-end is not cheap and/or easy by any means.

[Finetuning LLaMA-2](https://huggingface.co/blog/ram-efficient-pytorch-fsdp)

#### Adapter Layers and Prefix Tuning

> _“Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient”_ - from [1]

Given these limitations, we might wonder whether we could finetune an LLM in a manner that is more compute/data efficient but maintains the performance of end-to-end finetuning. A variety of research has been done in this area that boils down to one, core idea—_only adapting a small portion of the model’s parameters (or added parameters) during finetuning_. Each finetuned model only has a small number of task-specific parameters that should be stored/loaded, thus lessoning the compute/memory overhead of finetuning and simplifying the deployment process. Prior to the proposal of LoRA [1], two main parameter-efficient finetuning techniques were used: _adapter layers_ and _prefix tuning_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10bdd394-5600-4098-bbdc-d82f0eb341b1_1742x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10bdd394-5600-4098-bbdc-d82f0eb341b1_1742x862.png)

Adapter layers add two bottleneck-style feedforward modules into each transformer block (from [2])

**Adapter layers** were originally proposed in [2], where authors inserted two extra adapter blocks into each transformer block. Each adapter block is a bottleneck-style feedforward module that _i)_ decreases the dimensionality of the input via a (trainable) linear layer, _ii)_ applies a non-linearity, and _iii)_ restores the original dimensionality of the input via a final (trainable) linear layer. Put simply, the adapter blocks are extra trainable modules inserted into the existing transformer block—_in [2], adapter blocks are inserted after both attention and feedforward layers_—that have a small number of parameters[7](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-7-138861994) and can be finetuned while keeping the weights of the pretrained model fixed. As such, each finetuned version of the model only has a small number of task-specific parameters associated with it.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4567103-c992-47d9-b824-a3eb19d65ca5_1580x1044.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4567103-c992-47d9-b824-a3eb19d65ca5_1580x1044.png)

(from [3])

Several variants of adapter layers have been proposed that are more efficient and even go beyond language models [4, 5]. For example, authors in [3] simplify the structure of adapter layers such that only a single task-specific adapter is added to each transformer block, as well as an extra [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) module; see above.

> _“Prefix tuning keeps language model parameters frozen, but optimizes a small continuous task vector. Prefix tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were virtual tokens.”_ - from [6]

**Prefix tuning.** Another parameter-efficient finetuning alternative is prefix tuning, which keeps the language model’s parameters frozen and only finetunes a few (trainable) token vectors that are added to the beginning of the model’s input sequence. These token vectors can be interchanged for different tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5381a048-df74-4871-9842-2d99caf8786b_1204x1480.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5381a048-df74-4871-9842-2d99caf8786b_1204x1480.png)

(from [6])

In other words, prefix tuning adds a few extra token vectors to the model’s input. However, these added vectors do not correspond to a specific word or token—_we train the entries of these vectors just like normal model parameters_. This idea was first proposed in [6], where we see that authors freeze all model parameters and train only a small set of prefix token vectors added to the model’s input layer for each task. These trainable prefix tokens can be shared across a task. Beyond prefix tuning as it was originally proposed, several works have extended this idea. For example, authors in [7] explore the creation of trainable “soft prompts” that condition an LLM’s output on certain tasks, while [8] proposes the concatenation of trainable (continuous) prompts with discrete (normal) prompts; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F359126b3-0b45-4695-98b4-6842cafee418_2324x674.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F359126b3-0b45-4695-98b4-6842cafee418_2324x674.png)

(from [7, 8])

**What’s the problem?** Both prefix tuning and adapter layers reduce the compute requirements, memory overhead, and number of trainable parameters associated with finetuning an LLM. Despite giving us a parameter-efficient and performant alternative to full finetuning, however, these approaches do not come without limitations. Adapter layers add layers to the underlying model that—_despite having very few parameters_—must be processed sequentially, resulting in extra inference latency and slower training. Furthermore, prefix tuning is oftentimes difficult to optimize (i.e., the finetuning process is less stable), reduces the available context window, and does not monotonically improve performance with respect to the number of trainable parameters[8](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-8-138861994). _Low-Rank Adaptation (LoRA) aims to eliminate such issues while maintaining performance that is comparable to full finetuning_.

> _“LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.”_ - from [1]

## Finetuning LLMs More Efficiently

Given our discussion of finetuning techniques so far, it should be clear that we need a finetuning approach that is not just parameter efficient, but also:

1. _Compute efficient_: the training process should be fast and cheap.
    
2. _Memory efficient_: we should not need massive GPUs to finetune an LLM.
    
3. _Easy to deploy_: we should not have to deploy several copies of an LLM for each task that we want to solve.
    

As we will see, Low-Rank Adaptation (LoRA) [1] checks all of these boxes! With LoRA, we lower the barrier to entry for finetuning specialized LLMs, achieve performance that is comparable to end-to-end finetuning, can easily switch between specific versions of a model, and have no increase in inference latency. Due to its practical utility, LoRA has also been explored heavily within the research community, leading to a plethora of variants and extensions.

#### **[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) [1]**

When we finetune a language model, we modify the underlying parameters of the model. To make this idea more concrete, we can formulate the parameter update derived from finetuning as shown in the equation below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e09c48-332d-4c44-baba-beabe29018a6_1034x400.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33e09c48-332d-4c44-baba-beabe29018a6_1034x400.png)

Finetuning updates the pretrained model’s weights

The core idea behind LoRA is to model this update to the model’s parameters with a low-rank decomposition[9](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-9-138861994), implemented in practice as a pair of [linear projections](https://www.cs.bu.edu/fac/snyder/cs132-book/L08MatrixofLinearTranformation.html). LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdccfb29-1e68-4032-a5b7-a37927e6df10_414x462.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdccfb29-1e68-4032-a5b7-a37927e6df10_414x462.png)

(from [1])

**Rank decomposition matrix.** Put simply, the rank decomposition matrix is just two linear projections that reduce and restore the dimensionality of the input. The output of these two linear projections is added to the output derived from the model’s pretrained weights. The updated layer formed by the addition of these two parallel transformations is formulated as shown below. As we can see, adding LoRA to a layer directly learns the update to the underlying layer’s weights.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc30cf275-2411-4cca-a872-c18fdb22fef1_1514x876.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc30cf275-2411-4cca-a872-c18fdb22fef1_1514x876.png)

LoRA trains a low-rank approximation to the weight matrix update derived during finetuning

The matrix product `AB` has the same dimension as a full finetuning update. Decomposing the update as a product of two smaller matrices ensures that the update is [low rank](https://www.mathsisfun.com/algebra/matrix-rank.html) and significantly reduces the number of parameters that we have to train. Instead of directly finetuning the parameters in the pretrained LLM’s layers, LoRA only optimizes the rank decomposition matrix, yielding a result that approximates the update derived from full finetuning. We initialize `A` with random, small values, while `B` is initialized as zero, ensuring that we begin the finetuning process with the model’s original, pretrained weights.

> _“We roughly recover the expressiveness of full fine-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices.”_ - from [1]

Increasing `r` improves LoRA’s approximation of the full finetuning update, but incredibly small values of `r` suffice in practice, allowing us to significantly reduce compute and memory costs with minimal impact on performance. For example, we can use LoRA to finetune GPT-3 using only 0.01% of total parameters and still achieve performance comparable to that of full finetuning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dd33a67-0398-4944-8467-d7d7f2e7eb86_1064x504.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dd33a67-0398-4944-8467-d7d7f2e7eb86_1064x504.png)

Scaling factor in LoRA

**Scaling factor.** Once the low-rank update to the weight matrix is derived, we scale it by a factor α prior to adding it to the model’s pretrained weights. Such an approach yields the adaptation rule shown above. The default value of the scaling factor is one, meaning that the pretrained weights and the low-rank weight update are weighted equally when computing the model’s forward pass. However, the value of α can be changed to balance the importance of the pretrained model and new task-specific adaptation. [Recent empirical analysis](https://lightning.ai/pages/community/lora-insights/) indicates that larger values of α are necessary for LoRA with a higher rank (i.e., larger `r` —> la α).

**Comparison to adapter layers.** At first glance, the approach used by LoRA might seem similar to adapter layers. However, there are two notable differences:

1. There is no non-linearity between the two linear projections.
    
2. The rank decomposition matrix is injected into an existing layer of the model, instead of being sequentially added as an extra layer.
    

The biggest impact of these changes is the fact that LoRA has no added inference latency compared to the original pretrained model; see below. When deploying a finetuned LoRA model into production, we can directly compute and store the updated weight matrix derived from LoRA. As such, the structure of the model is identical to the pretrained model—_the weights are just different_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F844119ae-e0c5-4cc8-9b01-a5ed73fa71b0_1080x426.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F844119ae-e0c5-4cc8-9b01-a5ed73fa71b0_1080x426.png)

By storing both the model’s pretrained weights and LoRA modules derived from finetuning on several different tasks, we can “switch out” LoRA modules by:

1. Subtracting the LoRA update for one task from the model’s weights.
    
2. Adding the LoRA update for another task to the model’s weights.
    

In comparison, switching between models that are finetuned end-to-end on different tasks requires loading all model parameters in and out of memory, creating a significant I/O bottleneck. _LoRA’s efficient parameterization of the weight update derived from finetuning makes switching between tasks efficient and easy_.

**Why does this work?** LoRA structures the weight update derived from finetuning with a low rank decomposition that contains very few trainable parameters. With this in mind, we might wonder: _Why does the model perform well despite dedicating so few parameters to finetuning? Wouldn’t we benefit from more trainable parameters?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1582d49-12cd-41a9-963a-6e3f8ee9ed57_1622x656.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1582d49-12cd-41a9-963a-6e3f8ee9ed57_1622x656.png)

(from [15])

To answer this question, we can look at prior research [15], which shows that large models (e.g., LLMs) tend to have a low intrinsic dimension. Although this idea sounds complicated, it just means that the weight matrices of very large models tend to be low rank. In other words, _not all of these parameters are necessary_! We can achieve comparable performance by decomposing these weight matrices into a representation that has way fewer trainable parameters; see above.

> _“Aghajanyan et al. show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low intrinsic rank.”_ - from [1]

Given that the parameters of large models have low intrinsic dimension, we can reasonably infer that the same is true of models that are finetuned—_the weight update derived from finetuning should also have a low intrinsic dimension_. As a result, techniques like LoRA that approximate the finetuning update with a low rank decomposition should be able to learn efficiently and effectively, producing a model with impressive performance despite having few trainable parameters.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74a1dfcf-3bb5-4898-889a-aebfee0338c5_1078x350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74a1dfcf-3bb5-4898-889a-aebfee0338c5_1078x350.png)

(from [1])

**LoRA for LLMs.** The general idea proposed by LoRA can be applied to any type of dense layer for a neural network (i.e., more than just transformers!). When applying LoRA to LLMs, however, authors in [1] only use LoRA to adapt attention layer weights. Feed-forward modules and pretrained weights are kept fixed. We only update the rank decomposition matrix inserted into each attention layer. In particular, LoRA is used in [1] to update the query and value matrices of the attention layer, which is found in experiments to yield the best results; see above.

> _“Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.”_ - from [1]

However, [subsequent empirical analysis](https://magazine.sebastianraschka.com/i/138081202/enable-lora-for-more-layers) revealed that better results can be achieved by applying LoRA to all weight matrices in the transformer. Although these results may depend upon the application, we see that adapting all weight matrices with LoRA tends to yield competitive performance.

**The Benefits of LoRA** are plentiful as we can probably tell. However, some of the most notable benefits of this approach include the following:

- A single pretrained model can be shared by several (much smaller) LoRA modules that adapt it to solve different tasks, which simplifies the deployment and hosting process.
    
- LoRA modules can be “baked in” to the weights of a pretrained model to avoid extra inference latency, and we can quickly switch between different LoRA modules to solve different tasks.
    
- When finetuning an LLM with LoRA, we only have to maintain the optimizer state for a very small number of parameters[10](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-10-138861994), which significantly reduces memory overhead and allows finetuning to be performed with more modest hardware (i.e., smaller/fewer GPUs with less memory).
    
- Finetuning with LoRA is significantly faster than end-to-end finetuning (i.e., roughly 25% faster in the case of GPT-3).
    

LoRA significantly reduces the barrier to entry for finetuning LLMs. Training is fast, we don’t need tons of fancy GPUs, each task has only a small number of task-specific parameters associated with it, and—_as we will see soon_—there are a variety of resources and repos available online to get started with using LoRA.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae43026-6b42-43e8-b0c2-b24a7b20ac37_1946x1276.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae43026-6b42-43e8-b0c2-b24a7b20ac37_1946x1276.png)

(from [2])

**Experimental results.** In [1], LoRA is tested with different types of LLMs, including encoder-only ([RoBERTa](https://arxiv.org/abs/1907.11692) [16] and [DeBERTa](https://arxiv.org/abs/2006.03654) [17]) and decoder-only ([GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) [18] and [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [11]) language models. In experiments with encoder-only architectures, we see that LoRA—_for both RoBERTa and DeBERTa_—is capable of producing results on par with or better than end-to-end finetuning; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6db182e-8c5f-4321-ac73-169d36e97320_1060x1114.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6db182e-8c5f-4321-ac73-169d36e97320_1060x1114.png)

(from [2])

When experiments are performed with generative LLMs, we see that LoRA handles these workloads well and is effective even with much larger models; see above. Notably, we see that LoRA matches or exceeds the performance of end-to-end finetuning on every dataset that is tested. Furthermore, LoRA’s performance is incredibly stable with respect to the number of trainable parameters that are used, especially when compared to techniques like prefix tuning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec2329b8-b295-4f6a-bda1-51dfe56a40a9_1630x680.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec2329b8-b295-4f6a-bda1-51dfe56a40a9_1630x680.png)

(from [1])

Put simply, LoRA can achieve impressive performance—_comparable to or beyond that of full finetuning_—with very few trainable parameters, which minimizes I/O bottlenecks, reduces memory usage, and speeds up the finetuning process.

#### Using LoRA in Practice

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cbee7ee-241f-4725-a193-24cd1367c400_2248x1142.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cbee7ee-241f-4725-a193-24cd1367c400_2248x1142.png)

Best libraries for finetuning with LoRA

The reason that LoRA has become so popular is because it is such a useful tool for AI practitioners. With LoRA, finetuning LLMs for our desired application is much easier than before! We don’t need tons of massive GPUs and the finetuning process is efficient, which makes it possible for (almost) anyone to train a specialized LLM on their own data. Plus, numerous efficient implementations of LoRA are already available online with tons of useful features; e.g., [gradient accumulation](https://lightning.ai/blog/gradient-accumulation/) to reduce memory usage, [mixed-precision training](https://lightning.ai/docs/pytorch/1.5.9/advanced/mixed_precision.html) to speedup finetuning, and easy integration with accelerators (GPUs/TPUs). Notable libraries that can be used to finetune LLMs with LoRA (shown above) include:

- PEFT by HuggingFace [[link](https://huggingface.co/docs/peft/index)]
    
- Lit-GPT by [Lightning AI](https://lightning.ai/) [[link](https://github.com/Lightning-AI/lit-gpt)]
    

In this section, we will briefly overview how we can use Lit-GPT to finetune an LLM with LoRA and provide some helpful tips for using LoRA in practice.

**Finetuning with LoRA.** The Lit-GPT library contains a variety of useful scripts that can be used to finetune [open-source LLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better) using LoRA; see [here](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_lora.md) for a full guide. After cloning the Lit-GPT repository and installing dependencies, the first step is to download a pretrained model to finetune with LoRA. To download [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) (this requires being [granted access to LLaMA-2](https://huggingface.co/blog/llama2) via HuggingFace first), we just _i)_ download the model from HuggingFace and _ii)_ convert this into the format needed for Lit-GPT. We can do this via the scripts shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d4576aa-e5cc-4c4f-9ad0-1a33a492b9c0_1986x558.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d4576aa-e5cc-4c4f-9ad0-1a33a492b9c0_1986x558.png)

Download the pretrained LLaMA-2 model (HuggingFace version)

After we’ve downloaded the pretrained model, we need a dataset to use for finetuning. Examples of popular [instruction tuning](https://blog.research.google/2021/10/introducing-flan-more-generalizable.html) datasets that are commonly used for LLM finetuning include:

- Alpaca [[link](https://github.com/tatsu-lab/stanford_alpaca)]
    
- Dolly [[link](https://huggingface.co/datasets/databricks/databricks-dolly-15k)]
    
- LongForm [[link](https://huggingface.co/datasets/akoksal/LongForm)]
    
- LIMA [[link](https://huggingface.co/datasets/GAIR/lima)]
    
- RedPajama [[link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)]
    

To download (and properly format) any of these datasets, we can simply use the [helper scripts](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/prepare_dataset.md) within Lit-GPT, which also support [creating our own](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_lora.md#tune-on-your-dataset) (custom) finetuning dataset. From here, all that’s left is to run the finetuning script, merge the model’s weights[11](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-11-138861994), and evaluate the resulting model, either on a set of specified tasks or by just chatting with the model to assess quality; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbca36552-6615-4a0b-89f1-162a0c6d2909_924x1176.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbca36552-6615-4a0b-89f1-162a0c6d2909_924x1176.png)

Finetuning and evaluating a model with LoRA using Lit-GPT

The finetuning script within Lit-GPT has several default configurations that are used for LoRA; see below. We can edit these options in the `finetune/lora.py` file prior to performing a finetuning run. For example, we might want to change the value of `r` that is used, or apply LoRA to all layers within the transformer.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc68bb437-504c-48fe-b8fd-18b5ec32169e_790x1028.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc68bb437-504c-48fe-b8fd-18b5ec32169e_790x1028.png)

Settings for LoRA finetuning

**Is LoRA just for LLMs?** Finally, it’s important to realize that LoRA is a general technique that can be used for any type of dense neural network layer—_we can finetune more than just LLMs with LoRA_. For example, the link below shows an example of using LoRA to finetune an image classification model.

[LoRA for Image Classification](https://huggingface.co/docs/peft/task_guides/image_classification_lora)

Furthermore, we should notice that LoRA is orthogonal to most existing (parameter-efficient) finetuning techniques, meaning that we can use both at the same time! _Why is this the case?_ LoRA does not directly modify the pretrained model’s weight matrices, but rather learns a low-rank update to these matrices that can (optionally) be fused with the pretrained weights to avoid inference latency. _This is an inline adaptation technique that adds no additional layers to the model_. As a result, we can perform end-to-end finetuning in addition to LoRA, as well as apply techniques like prefix tuning and adapter layers on top of LoRA.

> _“The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case.”_ - from [1]

**Further reading.** Within this section, we have only scratched the surface of how to use LoRA effectively in practice. Although this serves as a good starting point, there are so many details/findings that one could gather from running experiments with LoRA and learning the best practices for this technique.

[

![](https://substackcdn.com/image/fetch/w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AI

Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)

Low-rank adaptation (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it's an essential technique worth familiarizing oneself with. Last month, I shared an article with several LoRA experiments…

Read more

a year ago · 119 likes · 16 comments · Sebastian Raschka, PhD

](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?utm_source=substack&utm_campaign=post_embed&utm_medium=web)

To learn more about these details, I highly recommend the series of overviews written on LoRA by [Sebastian Raschka](https://twitter.com/rasbt) and the Lightning AI team:

- Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) [[link](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)]
    
- Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments [[link](https://lightning.ai/pages/community/lora-insights/)]
    
- Finetuning Falcon LLMs More Efficiently With LoRA and Adapters [[link](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)]
    
- Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA) [[link](https://lightning.ai/pages/community/tutorial/lora-llm/)]
    

Within these articles (especially the “_Practical Tips_” and “_Insights from Hundreds of Experiments_” articles), we will find a variety of practical takeaways that help us to better leverage LoRA for language model adaptation. Some of the best takeaways gathered from this extensive empirical analysis of LoRA include:

- The choice of optimizer (i.e., [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) or [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)) for LoRA does not make a huge difference in performance or memory overhead (assuming `r` is small).
    
- Performing multiple epochs of training over the finetuning dataset is oftentimes not beneficial (i.e., degrades performance).
    
- Applying LoRA across all weight matrices in the transformer is better than just applying LoRA to the query and value matrices, as proposed in [1].
    
- Setting α to 2X the value of `r` yields competitive results. Larger values of `r` call for larger values of α, and `r` is a hyperparameter that must be tuned.
    

Going beyond LoRA, the same group of researchers has written a variety of practical overviews of other parameter-efficient finetuning techniques (e.g., prefix tuning and adapter layers) as well:

- Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning [[link](https://magazine.sebastianraschka.com/p/understanding-parameter-efficient)]
    
- Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters [[link](https://lightning.ai/pages/community/article/understanding-llama-adapters/)]
    
- Finetuning Large Language Models [[link](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)]
    

#### LoRA Variants (there are a ton…)

Due to its practical utility, the proposal of LoRA catalyzed the development of an entire field of research devoted to parameter-efficient finetuning—_and LoRA in particular_—that is quite active. Within this section, we will (attempt to) overview most of the notable LoRA variants that have been recently proposed.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed362d46-f789-4685-b4b7-87755f6fdcd2_1622x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed362d46-f789-4685-b4b7-87755f6fdcd2_1622x784.png)

(from [19])

**QLoRA [19]** (shown above) is arguably the most popular LoRA variant. At a high level, QLoRA uses [model quantization](https://cameronrwolfe.substack.com/p/quantized-training-with-deep-networks-82ea7f516dc6) to reduce memory usage during finetuning with LoRA, while maintaining a (roughly) equal level of performance. More specifically, QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. Additionally, authors in [19] propose several novel quantization techniques for further reducing memory usage:

- _4-Bit NormalFloat (NF4) Format_: a new (quantized) data type that works better for weights that follow a normal distribution.
    
- _Double Quantization_: reduces memory footprint by quantizing both model weights and their corresponding quantization constants.
    
- _Paged Optimizers_: prevents memory spikes due to gradient checkpointing that cause out-of-memory errors when processing long sequences or training a large model[12](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-12-138861994).
    

In practice, _QLoRA saves memory at the cost of slightly-reduced training speed_. For example, we see [here](https://lightning.ai/pages/community/lora-insights/) that replacing LoRA with QLoRA to finetune LLaMA-2-7B reduces memory usage by 33% but increases wall-clock training time by 39%.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e6b33c-9058-47bd-a4ce-6b53368805ac_1888x1034.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e6b33c-9058-47bd-a4ce-6b53368805ac_1888x1034.png)

(from [20])

**QA-LoRA [20]** (shown above) is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. Two major techniques are used to reduce the compute/memory overhead of finetuning LLMs:

1. _Parameter-Efficient Finetuning (PEFT)_: finetune pretrained LLMs with a small number of trainable parameters (e.g., LoRA is one form of PEFT).
    
2. _Quantization_: convert trained weights of an LLM into low-bit representations.
    

QA-LoRA integrates these two ideas in a simple and performant manner. To do this, we could perform post-training quantization on a model finetuned with LoRA, but this approach has been shown to work poorly. Instead, QA-LoRA improves both training and inference efficiency by proposing a group-wise quantization scheme that separately quantizes different groups of weights in the model. Because such quantization is applied during training, there is no need for post-training quantization—_QA-LoRA finetunes in a quantization-aware manner_! Beyond QA-LoRA, LoftQ [23] studies a similar idea of applying quantization and LoRA finetuning on a pretrained model simultaneously.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75a95221-3226-4215-985e-9b280c0c9afd_2162x984.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75a95221-3226-4215-985e-9b280c0c9afd_2162x984.png)

(from [21])

**LongLoRA [21]** attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme; see above. Training LLMs with long context lengths is expensive because the cost of self-attention is quadratic with respect to the length of the input sequence. However, we can avoid some of this cost by _i)_ starting with a pretrained LLM and _ii)_ expanding its context length via finetuning. LongLoRA does exactly this, making the extension of a pretrained LLM’s context length via finetuning cheaper by:

1. Using sparse local attention instead of dense global attention (optional at inference time).
    
2. Using LoRA (authors find that this works well for context extension).
    

Put simply, LongLoRA is just an efficient finetuning technique that we can use to adapt a pretrained LLM to support longer context lengths.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe989d13-c559-4542-b5d3-3bb3d8aba6d0_1732x1084.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe989d13-c559-4542-b5d3-3bb3d8aba6d0_1732x1084.png)

(from [22])

**S-LoRA [22]** (shown above) aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA is a scalable deployment strategy that:

1. Stores all LoRA modules in main memory.
    
2. Puts modules being used to run the current query into GPU memory.
    
3. Uses unified paging to allocate GPU memory and avoid fragmentation.
    
4. Proposes a new tensor parallelism strategy to batch LoRA computations.
    

By combining these techniques, S-LoRA can serve thousands of LoRA modules on a single GPU (or across multiple GPUs) and increases the throughput of prior systems (e.g., PEFT by HuggingFace) by up to 4X!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a67e87f-ba26-4017-8e8a-26c409d4a332_1616x764.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a67e87f-ba26-4017-8e8a-26c409d4a332_1616x764.png)

(from [24])

**LLaMA-Adapter [24]** (shown above) is not based upon LoRA, but it is nonetheless a recent (and popular) variant of parameter-efficient finetuning for LLMs. At a high level, LLaMA-Adapter finetunes pretrained LLMs to improve their instruction following capabilities using a very small number of added trainable parameters. When used to fine-tune LLaMA-7B, this approach only adds 1.2M parameters to the underlying model and requires less than an hour of finetuning time[13](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-13-138861994). LLaMA-Adapter follows an approach similar to prefix tuning that adds a set of learnable task-adaptation prompts to the beginning of the transformers sequence at each layer. Such an approach preserves the pretrained model’s knowledge and allows adaptation to new tasks and instruction following (even including multi-modal instruction following!) to be learned with minimal data.

**Many, many more…** Although several notable LoRA variants have been covered above, there are a nearly limitless number of extensions to this technique that have been proposed. Other recent LoRA-inspired works include:

- _[LQ-LoRA](https://arxiv.org/abs/2311.12023)_ [26]: uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.
    
- _[MultiLoRA](https://arxiv.org/abs/2311.11501)_ [27]: extension of LoRA that better handles complex multi-task learning scenarios.
    
- _[LoRA-FA](https://arxiv.org/abs/2308.03303)_ [28]: freezes half of the low-rank decomposition matrix (i.e., the `A` matrix within the product `AB`) to further reduce memory overhead.
    
- _[Tied-LoRA](https://arxiv.org/abs/2311.09578)_ [29]: leverages weight tying to further improve the parameter efficiency of LoRA.
    
- _[GLoRA](https://arxiv.org/abs/2306.07967)_ [30]: extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.
    

Given that so many LoRA-inspired techniques exist, there are probably a few notable extensions that are missing from the list above. If you are aware of any other techniques that are worth including, let me know in the comments!

## Takeaways

We should now have a working understanding of LoRA, the several variants of this technique that have been proposed, and how these ideas can be applied in practice. LoRA is arguably the most widely-used practical tool for creating specialized LLMs, as it democratizes the finetuning process by significantly reducing hardware requirements. Some important takeaways are outlined below.

**Training an LLM.** The training process for language models (i.e., both encoder-only and decoder-only models) includes pretraining and finetuning. During pretraining, we train the model via a self-supervised objective over a large amount of unlabeled text. Although pretraining is expensive, we can reuse the resulting model numerous times as a starting point for finetuning on various tasks. Due to the public availability of many high-quality pretrained LLMs, most practitioners can simply download a pretrained model and focus upon the finetuning process without ever having to pretrain a model from scratch.

**Affordable finetuning.** The finetuning process is cheap relative to the cost of pretraining. However, modern LLMs (especially GPT-style models) have many parameters. As such, we need expensive hardware (i.e., GPUs with a lot of memory) to make the finetuning tractable, thus increasing the barrier to entry for finetuning an LLM. Several parameter-efficient finetuning approaches have been proposed as a solution to this issue, but one of the most widely-adopted strategies is LoRA, which injects a learnable low-rank weight update into each layer of the underlying model. LoRA minimizes the memory overhead of finetuning—_thus reducing hardware requirements_—and performs comparably to full finetuning.

> _“QLORA reduces the average memory requirements of finetuning a 65B parameter model from >780GB of GPU memory to <48GB without degrading the runtime or predictive performance compared to a 16- bit fully finetuned baseline.”_ - from [19]

**An ecosystem.** LoRA is a practically useful tool that gives (almost) anyone the power to train a specialized LLM over their data. As a result, LoRA has been widely studied within the AI research community, leading to a variety of extensions, alternatives, and practical tools to go along with it. One of the most notable extensions is QLoRa, which combines LoRA with model quantization to further reduce the memory overhead of LLM finetuning. However, this reduction in memory overhead comes at the cost of a slight decrease in training speed.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." _arXiv preprint arXiv:2106.09685_ (2021).

[2] Houlsby, Neil, et al. "Parameter-efficient transfer learning for NLP." _International Conference on Machine Learning_. PMLR, 2019.

[3] Lin, Zhaojiang, Andrea Madotto, and Pascale Fung. "Exploring versatile generative language model via parameter-efficient transfer learning." _arXiv preprint arXiv:2004.03829_ (2020).

[4] Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. "Learning multiple visual domains with residual adapters." _Advances in neural information processing systems_ 30 (2017).

[5] Rücklé, Andreas, et al. "Adapterdrop: On the efficiency of adapters in transformers." _arXiv preprint arXiv:2010.11918_ (2020).

[6] Li, Xiang Lisa, and Percy Liang. "Prefix-tuning: Optimizing continuous prompts for generation." _arXiv preprint arXiv:2101.00190_ (2021).

[7] Lester, Brian, Rami Al-Rfou, and Noah Constant. "The power of scale for parameter-efficient prompt tuning." _arXiv preprint arXiv:2104.08691_ (2021).

[8] Liu, Xiao, et al. "GPT understands, too." _AI Open_ (2023).

[9] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[10] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[11] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[12] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[13] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[14] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018).

[15] Li, Chunyuan, et al. "Measuring the intrinsic dimension of objective landscapes." _arXiv preprint arXiv:1804.08838_ (2018).

[16] Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." _arXiv preprint arXiv:1907.11692_ (2019).

[17] He, Pengcheng, et al. "Deberta: Decoding-enhanced bert with disentangled attention." _arXiv preprint arXiv:2006.03654_ (2020).

[18] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[19] Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." _arXiv preprint arXiv:2305.14314_ (2023).

[20] Xu, Yuhui, et al. "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models." _arXiv preprint arXiv:2309.14717_ (2023).

[21] Chen, Yukang, et al. "Longlora: Efficient fine-tuning of long-context large language models." _arXiv preprint arXiv:2309.12307_ (2023).

[22] Sheng, Ying, et al. "S-LoRA: Serving Thousands of Concurrent LoRA Adapters." _arXiv preprint arXiv:2311.03285_ (2023).

[23] Li, Yixiao, et al. "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models." _arXiv preprint arXiv:2310.08659_ (2023).

[24] Zhang, Renrui, et al. "Llama-adapter: Efficient fine-tuning of language models with zero-init attention." _arXiv preprint arXiv:2303.16199_ (2023).

[25] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[26] Guo, Han, et al. "LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning." _arXiv preprint arXiv:2311.12023_ (2023).

[27] Wang, Yiming, et al. "MultiLoRA: Democratizing LoRA for Better Multi-Task Learning." _arXiv preprint arXiv:2311.11501_ (2023).

[28] Zhang, Longteng, et al. "LoRA-FA: Memory-efficient low-rank adaptation for large language models fine-tuning." _arXiv preprint arXiv:2308.03303_ (2023).

[29] Renduchintala, Adithya, Tugrul Konuk, and Oleksii Kuchaiev. "Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying." _arXiv preprint arXiv:2311.09578_ (2023).

[30] Chavan, Arnav, et al. "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning." _arXiv preprint arXiv:2306.07967_ (2023).

[1](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-1-138861994)

Notably, encoder-only only models are not appropriate for generative tasks.

[2](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-2-138861994)

For those who are unfamiliar with self-supervised learning, check out the link [here](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning). It is one of the main advancements that has made deep learning so successful in the natural language domain!

[3](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-3-138861994)

Unlike quantized training, quantization-aware training may not save any cost during the training process. Rather, this technique focuses upon producing a network that meshes will with post-training quantization.

[4](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-4-138861994)

The Cloze objective, also commonly referred to as [masked language modeling (MLM)](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning), is a self-supervised objective that is commonly used for pretraining non-generative language models like BERT.

[5](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-5-138861994)

For generative LLMs, pretraining datasets include (potentially) [trillion of tokens](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) worth of raw textual data!

[6](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-6-138861994)

The compute cost is significantly less than pretraining. As we will see, however, the cost of finetuning can still be reduced below the cost of full finetuning.

[7](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-7-138861994)

This is due to using the bottleneck structure! If our input is of dimension `d`, a single linear layer that transformer this input and maintains the dimension would have `O(d^2)` parameters. In comparison, the bottleneck layer would contain `O(dr)` parameters, which is much smaller when `r « d`.

[8](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-8-138861994)

This means that performance might get worse when we add more trainable token vectors to the prefix, which makes prefix tuning unstable and difficult to tune.

[9](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-9-138861994)

In linear algebra, a “low-rank” matrix is one that has repetitive rows or columns. Any (non-null) matrix can be written as a product of two matrices; e.g., `W = AB`, were `W` is of size `m x n`, `A` is of size `m x r`, and `B` is of size `r x n`. This is called a rank factorization; read more [here](https://en.wikipedia.org/wiki/Rank_factorization). In LoRA, we call this matrix product a low-rank decomposition of `W`, as the rank of `AB` is at most `r`.

[10](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-10-138861994)

Remember, we are only training the parameters in the rank decomposition matrix.

[11](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-11-138861994)

By “merge”, we mean combine the result of LoRA with the pretrained model’s weights, such that added inference latency is avoided.

[12](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-12-138861994)

The authors in [19] implement this using NVIDIA’s [unified memory feature](https://developer.nvidia.com/blog/unified-memory-cuda-beginners), which allows us to page memory between the CPU and GPU to avoid memory errors. Put simply, we avoid out of memory errors by paging memory to the CPU when the GPU runs out of space and loading the data back into GPU memory once it is needed again.

[13](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#footnote-anchor-13-138861994)

However, authors in [24] use 8X A100 GPUs, so the hour of finetuning time occurs on a (relatively) beefy setup.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Ali Holmovaia's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa00ba17a-7339-43c4-81d2-859f0e351d76_96x96.jpeg)



](https://substack.com/profile/166111354-ali-holmovaia)

[

![Gleb Alshanskii's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95dc57d5-e149-47f7-b5ed-0caf62c9c0c9_96x96.jpeg)



](https://substack.com/profile/10354194-gleb-alshanskii)

[

![Abhijit Gairola's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc786373-41e8-43d6-9f62-b9d060137b58_96x96.jpeg)



](https://substack.com/profile/122920-abhijit-gairola)

[

![Aadesh Ingle's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9a873808-d894-4026-bc6c-9b1296a4e511_640x640.jpeg)



](https://substack.com/profile/9321415-aadesh-ingle)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

111 Likes∙

[11 Restacks](https://substack.com/note/p-138861994/restacks?utm_source=substack&utm_content=facepile-restacks)

111

- 

[

14

](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comments)

11

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Abdeli brimelli's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b281cfd-0490-428c-a81c-583715435fd2_144x144.png)



](https://substack.com/profile/151220457-abdeli-brimelli?utm_source=comment)

[Abdeli brimelli](https://substack.com/profile/151220457-abdeli-brimelli?utm_source=substack-feed-item)

[5月1日](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comment/55209914 "2024年5月1日 04:53")

Liked by Cameron R. Wolfe, Ph.D.

Hi Dr. Cameron R. Wolfe,

Thank you for the nice article. Can I cite this article knowledge share with my pears at work ?

Best Regards,

-Abdeli B.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comment/55209914)

[

![sandra's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Forange.png)



](https://substack.com/profile/213269362-sandra?utm_source=comment)

[sandra](https://substack.com/profile/213269362-sandra?utm_source=substack-feed-item)

[2024年3月7日](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comment/51124517 "2024年3月7日 22:53")

Liked by Cameron R. Wolfe, Ph.D.

can I cite this article for educational purpose ?

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comment/51124517)

[12 more comments...](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Explaining ChatGPT to Anyone in <20 Minutes

### Distilling the core components of generative LLMs into an accessible framework...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Dec 11, 2023

53

- 

[

9

](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comments)

4

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ef09429-8738-4c85-9269-6c8b0c18ee1f_2304x1350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ef09429-8738-4c85-9269-6c8b0c18ee1f_2304x1350.png)

Over the past few years, we have witnessed a rapid evolution of generative large language models (LLMs), culminating in the creation of unprecedented tools like ChatGPT. Generative AI has now become a popular topic among both researchers and the general public. Now more than ever before, it is important that researchers and engineers (i.e., those _building_ the technology) develop an ability to communicate the nuances of their creations to others. A failure to communicate the technical aspects of AI in an understandable and accessible manner could lead to widespread public skepticism (e.g., research on nuclear energy went down a [comparable path](https://en.wikipedia.org/wiki/Nuclear_power_debate)) or the enactment of overly-restrictive legislation that hinders forward progress in our field. Within this overview, we will take a small step towards solving these issues by proposing and outlining a simple, three-part framework for understanding and explaining generative LLMs.

**Presentation resources.** This post was inspired by a presentation that I recently gave for O’Reilly on the basics of LLMs. The goal of this presentation was to provide a “primer” that brought everyone up to speed with how generative LLMs work. The presentation lasted ~20 minutes (hence, the title of this article). For those interested in using the resources from this presentation, the slides are [here](https://docs.google.com/presentation/d/19DHOvJQQYSbPCWiWi3S5xbVONnmI6RJ6K3fvfn8AG70/edit?usp=sharing).

## The Core Components of Generative LLMs

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4622eab6-80ad-4ae6-8d66-92505e011cf8_2312x1286.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4622eab6-80ad-4ae6-8d66-92505e011cf8_2312x1286.png)

The quality of (large) language models has drastically improved

The purpose of this overview is simple. The quality of generative language models has drastically improved in the last year (see above), and we want to understand what changes and new techniques catalyzed this boost in quality. Here, we will stick to transformer-based language models, though the concept of a language model predates the transformer architecture—_dating back to recurrent neural network-based architectures (e.g., [ULMFit](https://medium.com/@ashleyha/lets-learn-about-universal-language-model-fine-tuning-ulmfit-through-practical-applications-fea0aed2cf96) [4]) or even [n-gram language models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)_.

**Top-level view.** To explain generative LLMs in a clear and simple manner, we must first identify the key ideas and technologies that underlie them. In the case of this overview, we will focus upon the following three components:

1. _Transformer architecture_: the neural network architecture used by LLMs.
    
2. _Language model pretraining_: the (initial) training process used by all LLMs.
    
3. _The alignment process_: how we teach LLMs to behave to our liking.
    

Together, these ideas describe the technology that powers generative LLMs like ChatGPT. Within this section, we will develop a working understanding of these key ideas and how they combine together to create a powerful LLM.

#### Transformer Architecture

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafc425a1-12b4-4070-bb47-a9da226d97bb_1456x1330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafc425a1-12b4-4070-bb47-a9da226d97bb_1456x1330.png)

(from [5])

Nearly all modern language models are based upon the transformer architecture (shown above) [5]—_a deep neural network architecture that was originally proposed for solving Seq2Seq tasks (e.g., summarization or language translation)[1](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-1-139646437)_. The transformer takes a sequence of text as input and is trained to perform some generative (e.g., summarization) or discriminative (e.g., classification) task. We will now overview the transformer architecture and how it is leveraged by LLMs to produce coherent and interesting sequences of text. For a more in-depth explanation of the transformer architecture in general, check out the link below.

[Transformer Deep Dive](https://jalammar.github.io/illustrated-transformer/)

**Encoder-decoder architecture.** The transformer has two primary components: the _encoder_ and the _decoder_. The encoder looks at the full sequence of text provided as input and builds a representation[2](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-2-139646437) of this text. Then, the decoder takes this representation as input and uses it to produce an output sequence. For example, if we train a transformer to translate a sentence from English to Chinese, the model will perform the following processing steps:

- The encoder ingests the English sentence.
    
- The encoder outputs a representation for the English sentence.
    
- The decoder ingests the encoder’s representation of the English sentence.
    
- The decoder generates the Chinese translation.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f86baf-9836-4217-b70a-642eb01e0f66_1634x642.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f86baf-9836-4217-b70a-642eb01e0f66_1634x642.png)

Decoder-only transformer architecture

**Decoder-only architecture.** Although the originally-proposed transformer has both an encoder and a decoder module, generative LLMs primarily use a decoder-only transformer architecture; see above. This architecture eliminates the encoder from the transformer architecture, leaving only the decoder. Given that the decoder’s role in the transformer is to generate textual output, we can intuitively understand why generative LLMs only use the decoder component—_the entire purpose of these models is to generate sequences of text_!

**Constructing the input.** To better understand how decoder-only transformers operate, let’s take a deeper look at the input to this model and how it is created. Generative LLMs take a sequence of text as input. However, we can’t just pass raw text or characters into the transformer. First, we tokenize this text, or break it into a sequence of tokens (i.e., words or sub-words); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d6f17ac-7ff0-42f9-a593-970409c7dd5b_1132x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d6f17ac-7ff0-42f9-a593-970409c7dd5b_1132x282.png)

Tokenizing a sequence of text

After tokenizing the input sequence, we convert each token into an associated, unique vector representation, forming a list of vectors that correspond to each token; see below. These token vectors are lists of numbers that quantitatively describe each token. After adding positional embeddings—_or extra vectors that capture the position of each token within the input sequence_—to these token vectors, we are left with the final input that is passed into the decoder-only transformer!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a69eb4-7eea-4d94-b15f-38ab3a58bc64_1876x722.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a69eb4-7eea-4d94-b15f-38ab3a58bc64_1876x722.png)

Generating token embeddings

Token vectors are stored in an embedding layer—_a big matrix, where each row stores the vector for a single token_—that is part of the transformer architecture. At first, these token vectors are randomly initialized, but we learn them during the training process just like any other neural network parameter!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e950f0-7c8b-472e-90d8-9fd62447abab_1436x508.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e950f0-7c8b-472e-90d8-9fd62447abab_1436x508.png)

Depiction of masked self-attention

**Processing the input.** Now that we understand how the model’s input is constructed, the next step is to better understand how each of the model’s layers transform this input. Each “block” of the decoder-only transformer takes a list of token vectors as input and produces a list of transformed[3](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-3-139646437) token vectors (with the same size) as output. This transformation is comprised of two operations:

1. _Masked Self-Attention_: transforms each token vector by considering tokens that precede it in the sequence; see above and [here](https://x.com/cwolferesearch/status/1644773244786941952?s=20) for more details.
    
2. _Feed-forward transformation_: transforms each token vector individually via a sequence of [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) and non-linearities (e.g., [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)); see below.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd76032dc-9ca1-460e-b865-de0159387c19_1438x660.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd76032dc-9ca1-460e-b865-de0159387c19_1438x660.png)

Applying a feed-forward transformation to each token vector

Together, masked self-attention and feed-forward transformations allow us to craft a rich representation of any textual sequence. These components each play a distinct and crucial role:

- The attention component grabs useful information from the broader sequence (i.e., tokens that precede the current token).
    
- The feed-forward component learns a pattern or transformation that is individually applied to each token vector.
    

By stacking several decoder-only transformer blocks on top of each other, we arrive at the architecture that is used by (nearly) all generative LLMs!

#### Language Model Pretraining

> _“Self-supervised learning obtains supervisory signals from the data itself, often leveraging the underlying structure in the data. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input.”_ - from [7]

Now that we understand the transformer, let’s dive in to the next key idea that underlies generative LLMs—_self-supervised pretraining_. [Self-supervised learning](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) refers to the idea of using signals that are already present in raw data to train a machine learning model. In the case of generative language models, the most commonly-used objective for self-supervised learning is next token prediction, also known as the [standard language modeling objective](https://cameronrwolfe.substack.com/i/135273362/the-language-modeling-objective). Interestingly, this objective—_despite being quite simple to understand_—is the core of all generative language models. We’ll now explain this objective, learn how it works, and develop an understanding of why it is so useful for generative LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F466c1fbd-2b7d-4115-900c-6562f49c3aa1_2048x1155.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F466c1fbd-2b7d-4115-900c-6562f49c3aa1_2048x1155.png)

Language model pretraining with next token prediction

**LLM pretraining.** To pretrain a generative language model, we first curate a large corpus of raw text (e.g., from books, the web, scientific publications, and much more) to use as a dataset. Starting from a randomly initialized model, we then pretrain the LLM by iteratively performing the following steps:

1. Sample a sequence of raw text from the dataset.
    
2. Pass this textual sequence through the decoder-only transformer.
    
3. Train the model to accurately predict the next token at each position within the sequence.
    

Here, the underlying training objective is self-supervised, as the label that we train the model to predict—_the next token within the sequence_—is always present within the underlying data. As a result, the model can learn from massive amounts of data without the need for human annotation. This ability to learn directly from a large textual corpus via next token prediction allows the model to develop an impressive understanding of language and knowledge base. For a more in-depth discussion of the pretraining process, check out the link below.

[More on Pretraining](https://cameronrwolfe.substack.com/p/language-model-training-and-inference)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb4774fc-54bb-4dbb-8104-85dd0e716d21_2324x546.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb4774fc-54bb-4dbb-8104-85dd0e716d21_2324x546.png)

Generating output with a language model

**Inference process.** Generative LLMs use a next token prediction strategy for both pretraining and inference! To produce an output sequence, the model follows an autoregressive[4](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-4-139646437) process (shown above) comprised of the following steps:

1. Take an initial textual sequence (i.e., a prompt) as input.
    
2. Predict the next token.
    
3. Add this token to the input sequence.
    
4. Repeat steps 2-3 until a terminal/stop token (i.e., `<EOS>`) is predicted.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afac10b-ec99-436c-bc89-cd848a7c562f_2196x686.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0afac10b-ec99-436c-bc89-cd848a7c562f_2196x686.png)

The best base models have many parameters and are trained over a lot of data (from [3, 9])

**Keys to success.** A decoder-only transformer that has been pretrained (using next token prediction) over a large textual corpus is typically referred to as a base (or pretrained) model. Notable examples of such models include [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [3], [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [9], and [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [10]. As we progressed from early base models (e.g., GPT [1]) to the models that we have today, two primary lessons were learned.

1. _Larger models are better_: increasing the size (i.e., number of trainable parameters) of the underlying model yields a smooth increase in performance, which led to the popularization of _large_ language models.
    
2. _More data_: despite yielding a smooth increase in performance, increasing the size of the underlying model alone is suboptimal. We must also increase the size of the underlying pretraining dataset to get the best results.
    

Put simply, _the best base models combine large model architectures with massive amounts of high-quality pretraining data_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5bbb0a0f-7bac-45cd-a982-d1992ec69efd_1622x540.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5bbb0a0f-7bac-45cd-a982-d1992ec69efd_1622x540.png)

(from [8])

**Computational cost.** Before moving on, we should note that the pretraining process for generative LLMs is incredibly expensive—_we are training a large model over a massive dataset_. Pretraining costs range from several hundreds of thousands (see above) to even [millions of dollars](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/?sh=427f4d144af7), if not more. Despite this cost, pretraining is an incredibly important step—_models like ChatGPT would not be possible without creation of or access to a high-quality base model_. To avoid the cost of language model pretraining, most practitioners simply download [open-source base models](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better) that have been made available online via platforms like HuggingFace[5](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-5-139646437).

#### The Alignment Process

> _“Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users.”_ - from [6]

After pretraining, the LLM can accurately perform next token prediction, but its output is oftentimes repetitive and uninteresting. For an example of this, we can look back to the beginning of this overview! Namely, GPT-2 (i.e., a pretrained LLM) struggles to produce helpful and interesting content. With this in mind, we might ask: _What do these models lack? How did we get from this to ChatGPT?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e451d39-6d4d-4c7b-976a-78e337e79100_1374x486.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e451d39-6d4d-4c7b-976a-78e337e79100_1374x486.png)

The alignment process for LLMs

**The alignment process**, which teaches a language model how to generate text that aligns with the desires of a human user, is the answer to the question above. For example, we can teach the model to:

- Follow detailed instructions
    
- Obey constraints in the prompt
    
- Avoid harmful outputs
    
- Avoid hallucinations (i.e., generating false information)
    
- Pretty much anything we can demonstrate to the LLM!
    

The objectives of the alignment process are typically referred to as _alignment criteria_, and we (i.e., the researchers training the model) must define these criteria at the outset of the alignment process. For example, two of the most commonly-used alignment criteria within AI research (e.g., see LLaMA-2 [10] and [Constitutional AI](https://cameronrwolfe.substack.com/i/136751520/constitutional-ai-harmlessness-from-ai-feedback) [11]) are helpfulness and harmlessness. To instill each of these alignment criteria within the model, we perform finetuning via supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbd7695-b32e-49a5-9d8b-dac180c767a1_1274x676.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbd7695-b32e-49a5-9d8b-dac180c767a1_1274x676.png)

(from [10])

**SFT** is simple to understand, as it is based upon the same objective as pretraining—_next token prediction_. However, instead of training the model over a bunch of raw text downloaded from the web, we use a more specific training dataset. Namely, we train the model on alignment-focused data, or demonstrations—_either written by humans or generated synthetically by another LLM_—of prompt and response pairs that satisfy the set of desired alignment criteria; see above. For a more in-depth discussion of SFT, check out the link below.

[More on SFT](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Although SFT is effective and widely-used, manually curating a dataset of high-quality model responses can be difficult. **RLHF** solves this issue by directly finetuning an LLM based on human feedback; see below for a depiction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)

(from [12])

This process proceeds in two phases. During the first phase, we start with a set of prompts and use the LLM—_or a group of several LLMs_—to generate two or more responses to each prompt. Then, a group of human annotators ranks responses to each prompt based on the defined alignment criteria, and we use this data to train a reward model that accurately predicts a human preference score given a prompt and a model’s response as input. Once the reward model is available, we use a reinforcement learning algorithm (e.g., [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)) during the second phase of RLHF to finetune the LLM to maximize human preference scores, as predicted by the reward model. For a more detailed discussion of RLHF, check out the link below.

[More on RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)

**Finetuning pipeline.** Although SFT and RLHF are standalone techniques, most state-of-the-art LLMs use the three-step alignment process, including both SFT and RLHF. This approach, depicted below, became a standard within LLM research after the proposal of InstructGPT [6][6](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-6-139646437)—_the predecessor to ChatGPT_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c82297-0e6c-4df9-86c9-c95f4e971ccd_1894x1022.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04c82297-0e6c-4df9-86c9-c95f4e971ccd_1894x1022.png)

The three-step alignment technique (from [6])

#### Putting Everything Together

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e90020d-25ce-4af0-b3fd-0c1faecf4c48_2328x1226.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e90020d-25ce-4af0-b3fd-0c1faecf4c48_2328x1226.png)

Key components of generative LLMs (from [5, 6])

As long as we understand the concepts that have been discussed so far, we now have a working understanding of ChatGPT. In particular, modern generative LLMs _i)_ use a transformer architecture, _ii)_ are pretrained over a large corpus of text downloaded from the web, and _iii)_ are aligned via SFT and RLHF; see above.

**The progression of LLM research.** To understand how these three language modeling components have influenced the development of LLMs and AI, we can study the progression of prominent papers within the language modeling domain; see below. Before the advent of the transformer architecture, language models were still around, but they were based upon simpler architectures (e.g., recurrent neural networks). Although the transformer was originally proposed to solve Seq2Seq tasks, we quickly saw the decoder-only variant of the transformer architecture applied to the language modeling domain with GPT and GPT-2 [1, 2][7](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-7-139646437).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f66d5ad-9e3f-4344-9477-83e21bbb0bf8_1956x748.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f66d5ad-9e3f-4344-9477-83e21bbb0bf8_1956x748.png)

Progression of language model technology and popularity

These early, transformer-based language models performed well, but we didn’t see truly impressive performance until much larger language models (i.e., true LLMs), such as GPT-3 [5], were explored. These models were found to have impressive few-shot learning capabilities across a variety of tasks as we scale them up in size. However, model size alone is not enough! We learn from Chinchilla [9] that the best base models are created by combining large models with large pretraining datasets—_both parameter and data scale are important_.

Pretrained LLMs can accurately solve many tasks, which led to recognition within the AI community. However, these models were oftentimes repetitive or uninteresting, struggled to follow instructions, and generally lacked in helpfulness. To solve this problem, the alignment process directly finetunes the LLM based on a set of criteria that describe desired model behavior. Such an approach drastically improves the quality of the underlying model, leading to the creation of powerful models like ChatGPT and a subsequent explosion of interest in generative LLMs; see below. In other words, _alignment is (arguably) the key advancement that made the powerful LLMs we see today possible_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F339538cb-55bc-46d4-aa24-c66a5103d94d_1966x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F339538cb-55bc-46d4-aa24-c66a5103d94d_1966x1004.png)

Alignment is the key to training powerful foundation language models

**What’s next?** At this point, we understand everything that goes into creating an LLM like ChatGPT. All that is left is to learn how to apply such models to solving practical problems. To solve downstream tasks with an LLM, there are two basic approaches we can take (shown below):

1. _In-context learning_: write a prompt to solve the desired task.
    
2. _Finetuning_: perform further training on data for the desired task.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80129e1b-d747-489b-bdf0-949b2b6b80c0_2610x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80129e1b-d747-489b-bdf0-949b2b6b80c0_2610x500.png)

Adapting an LLM to solve a downstream task (from [5, 6, 13])

For more information on in-context learning and prompting, as well as various methods of finetuning, check out the references below:

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    
- Understanding and Using Supervised Finetuning [[link](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)]
    
- Easily Train a Specialized LLM [[link](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft)]
    

## **Key Takeaways**

Within this overview, we distilled the key technologies underlying generative LLMs into a three-part framework that includes _i)_ the transformer architecture, _ii)_ language model pretraining, and _iii)_ the alignment process. The key points to remember with respect to each of these components are as follows:

- Generative LLMs are based upon the decoder-only transformer architecture.
    
- First, we pretrain these models over massive amounts of textual data using the next token prediction objective.
    
- After pretraining, we use SFT and RLHF (i.e., the three-step technique) to better align these models with the desires of human users.
    

By developing a deep understanding of these components and learning how to explain them to others (in simple terms), we can democratize understanding of this important technology. Today, AI-powered systems are impacting more people than ever before. AI is no longer just a research topic, but rather a topic of popular culture. As AI continues to evolve, it is paramount that builders, engineers, and researchers (i.e., people like us!) learn to effectively communicate about the technology, its pitfalls, and what is to come.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[3] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.

[4] Howard, Jeremy, and Sebastian Ruder. "Universal language model fine-tuning for text classification." _arXiv preprint arXiv:1801.06146_ (2018).

[5] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[7] LeCun, Yann et al. “Self-supervised learning: The dark matter of intelligence”, https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/ (2021).

[8] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” _MosaicML_, 22 June 2023, www.mosaicml.com/blog/mpt-30b.

[9] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[10] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." _arXiv preprint arXiv:2307.09288_ (2023). 

[11] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[12] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[13] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-1-139646437)

However, the transformer is now used in a wide variety of tasks and domains, ranging from computer vision to processing audio signals. In many ways, the transformer is becoming a general-purpose deep learning architecture for solving any task.

[2](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-2-139646437)

This “representation” is just a vector, or list of vectors, that quantitatively describe the input and can be used by some downstream model/module to solve a relevant task.

[3](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-3-139646437)

Hence, the “_transformer_” architecture!

[4](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-4-139646437)

This word seems fancy, but it just means that each output we produce is added to the input sequence iteratively before producing the next output.

[5](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-5-139646437)

[Here](https://huggingface.co/meta-llama/Llama-2-7b) is an example of a model you can download.

[6](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-6-139646437)

Technically, this three step technique was proposed prior to InstructGPT in [12]. However, this paper only studies training language models for summarization, rather than training generic [foundation models](https://crfm.stanford.edu/).

[7](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20#footnote-anchor-7-139646437)

Both GPT and GPT-2 are pretrained base models that undergo no alignment process. Alignment was not explore extensively until InstructGPT [6].

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Andrei Sedelkov's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309eced-85ef-46d7-9b6d-8f0400fdeafb_144x144.png)



](https://substack.com/profile/142442-andrei-sedelkov)

[

![Bernhard Conzelmann's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F911e0a35-9631-41ed-be53-3a88b6d0da2e_144x144.png)



](https://substack.com/profile/109143253-bernhard-conzelmann)

[

![Sinta S's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4a3a50f9-1ea0-4803-8e64-e1f98b12a71d_144x144.png)



](https://substack.com/profile/14558814-sinta-s)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

53 Likes∙

[4 Restacks](https://substack.com/note/p-139646437/restacks?utm_source=substack&utm_content=facepile-restacks)

53

- 

[

9

](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Monty's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F141e0f44-08b1-44e5-902f-d5c8436e9ed3_144x144.png)



](https://substack.com/profile/170871318-monty?utm_source=comment)

[Monty](https://substack.com/profile/170871318-monty?utm_source=substack-feed-item)

[Monty’s Substack](https://mountsattire0z.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年12月12日](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comment/45250354 "2023年12月12日 23:07")

Liked by Cameron R. Wolfe, Ph.D.

PDF book on Ai, ML, etc.? I am reading a lot atm on these subjects to learn more. This means my my understanding is cobbled together from many sources. it would be nice to read a single source to help me validate what I think I know so dar

Like (1)

Reply

Share

[4 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comment/45250354)

[

![Sairam Sundaresan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F55c2183f-7755-4692-8063-59df2751f834_800x800.png)



](https://substack.com/profile/85853406-sairam-sundaresan?utm_source=comment)

[Sairam Sundaresan](https://substack.com/profile/85853406-sairam-sundaresan?utm_source=substack-feed-item)

[Gradient Ascent](https://newsletter.artofsaience.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2023年12月12日](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comment/45191726 "2023年12月12日 00:27")

Liked by Cameron R. Wolfe, Ph.D.

This is both timely (the one year anniversary of ChatGPT) and extremely necessary. Congratulations on your talk with O'Reilly, Cameron. Is there a book in the near future? :)

Like (1)

Reply

Share

[3 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comment/45191726)

[7 more comments...](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Google Gemini: Fact or Fiction?

### Breaking down the capabilities of Google's highly anticipated OpenAI competitor...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Dec 21, 2023

89

- 

[

2

](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction/comments)

4

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company.

Join subscribers from Microsoft, Tesla, Google, Meta, and more that use Deep (Learning) Focus to better understand AI research!

Subscribe

If you like the newsletter, feel free to [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a62bde-009a-45b4-857f-d5f4461b9ea1_2412x1354.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a62bde-009a-45b4-857f-d5f4461b9ea1_2412x1354.png)

(from [1, 13, 22])

For several months (an eternity within the AI space), large language model (LLM) leaderboards have been dominated by OpenAI models like ChatGPT and GPT-4. Despite shockingly fast progress in the area of [open-source LLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation) and [massive investment of funds](https://techcrunch.com/2023/06/13/frances-mistral-ai-blows-in-with-a-113m-seed-round-at-a-260m-valuation-to-take-on-openai/) into AI research initiatives from companies both private and public, the dominance of OpenAI models has not faltered in the slightest—_being outperformed by GPT-4 on nearly all benchmarks was universally accepted as unavoidable, even in cutting-edge research_. With this in mind, the release of an LLM that can actually compete with models from OpenAI is a prospect that is both exciting (for AI practitioners) and frightening (for the company that has to do it). W_ho would willingly compete head-to-head with OpenAI?_ The answer is Google.

> _“As a neuroscientist as well as a computer scientist, I've wanted for years to try and create a kind of new generation of AI models that are inspired by the way we interact and understand the world, through all our senses.”_ - Demis Hassabis

After the recent release of Gemini—_a suite of several multimodal LLMs of varying sizes_—Google has marketed their new models as the largest competitor of OpenAI models to date. Within this overview, we will study the Gemini model suite, explain how these models operate, and evaluate whether this claim is legitimate. In short, Gemini models have many desirable properties (e.g., edge device compatibility and native multimodality) and perform—_for the most part_—comparably to top models from OpenAI. However, developing an accurate perspective of these models and their capabilities requires a deep dive that goes beyond the [marketing tactics](https://www.interconnects.ai/p/evals-are-marketing) of LLM evaluations. Here, we will attempt to develop such a perspective through an in-depth analysis of relevant models.

**What came before Gemini?** Google released a variety of different language models before Gemini. In fact, many foundational works on LLMs (e.g., [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) [27], [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher) [28], [LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications) [29], etc.) were published by Google, including a variety of multimodal LLMs (e.g., Flamingo [13], CoCa [14], and PaLI [15]). Prior to Gemini, Google provided access to two text-only LLMs via their APIs—[PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) and [PaLM-2](https://ai.google/discover/palm2/) [5, 6]. However, these models—_despite competing with the API-based LLM offering of OpenAI_—were not truly competitive.

## Gemini Model Architecture

To understand Gemini, we first need to understand its model architecture. Although these details are not fully disclosed, there are many pointers and hints provided in [1] that give us an idea of how the Gemini models are structured. We’ll dive into these topics now, aiming to develop an understanding of both the Gemini models in particular and how multimodal LLMs are typically structured.

#### A Suite of Models for any Application

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f4272cb-30aa-480a-94bc-04834bddb6dc_1950x1262.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f4272cb-30aa-480a-94bc-04834bddb6dc_1950x1262.png)

The Gemini model suite (from [3])

Gemini is not a single model, but rather a suite of several LLMs. Each of these LLMs is sized differently, granting each of them with a varying tradeoff between efficiency and problem solving capacity. _Why release such a suite of models?_ The goal of Gemini is to release a set of capable models that are suitable for any application, whether it be on-device or solving complex reasoning tasks with access to massive amounts of compute resources (e.g., a data center).

> _“Gemini is our most flexible model yet — able to efficiently run on everything from data centers to mobile devices.”_ - from [2]

**Different model sizes.** The initial release of Gemini includes three[1](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-1-139569597) different model variants:

1. _Gemini Ultra_: the largest and most capable model that achieves state-of-the-art performance on numerous, highly-complex tasks.
    
2. _Gemini Pro_: A performance-optimized (in terms of quality and latency) model that performs well across many tasks and can be deployed at scale.
    
3. _Gemini Nano_: a group of models that are designed to run efficiently on edge devices despite achieving impressive performance.
    

Gemini Nano contains two models with 1.8B parameters (Nano-1) and 3.25B parameters (Nano-2), which facilitates deployment onto devices with both high and low memory capacity. The exact sizes of Gemini Ultra and Pro are not mentioned, though the authors[2](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-2-139569597) do mention that _i)_ Gemini Pro requires a few weeks of training time and _ii)_ only a fraction of the compute resources of Gemini Ultra are required for Gemini Pro. As such, it seems that Gemini Ultra is a much larger model, while Gemini Pro is optimized for deployment at scale; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb8af6bf-300d-4bd0-be86-92d307d73b92_1918x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb8af6bf-300d-4bd0-be86-92d307d73b92_1918x732.png)

Gemini model family (from [1])

#### Decoder-only Architecture

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f86baf-9836-4217-b70a-642eb01e0f66_1634x642.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f86baf-9836-4217-b70a-642eb01e0f66_1634x642.png)

The decoder-only transformer architecture

Like many generative LLMs, Gemini is based upon a [decoder-only transformer architecture](https://x.com/cwolferesearch/status/1640446111348555776?s=20); see above for a depiction. The details of the model’s architecture are not explicitly outlined in the technical report, but we are given enough information to (roughly) infer some relevant details about the model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4134cb5d-82b4-492b-858a-9d4d2bd62267_822x1168.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4134cb5d-82b4-492b-858a-9d4d2bd62267_822x1168.png)

Multi-query attention (from [4])

**Architectural modifications.** Beyond the standard decoder-only architecture, Gemini makes a few modifications to improve efficiency and training stability. Although the exact changes are not explicitly stated, authors do mention that they use multi-query attention, an approach that makes multi-head attention more efficient by sharing key and value vectors between attention heads; see above. This technique is also used by [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [5], PaLM-2 [6] and [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [7].

Going further, Gemini’s architecture is optimized to improve efficiency—_both during training and inference_—and training stability. With this in mind, we might infer that Gemini leverages some of the optimization and architectural tricks that are heavily used in prior work to serve this purpose:

- _Lion optimizer_ [8]: an adaptive optimizer that has been shown (by LLMs like [MPT-7B](https://cameronrwolfe.substack.com/i/131642185/mpt-b-a-commercially-usable-llama-b)) to improve the stability of LLM training.
    
- _Low Precision Layer Normalization_: performs layer normalization in low precision to improve efficiency (used by MPT-7/30B); see [here](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html).
    
- _Flash Attention_ [9]: a hardware-aware efficient attention implementation that can drastically speed up training (used by numerous LLMs like MPT-7/30B and [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source)).
    
- _Flash Decoding_ [10]: an extension of flash attention that improves attention efficiency in the inference/decoding stage (in addition to training).
    

**Hardware-aware architecture design.** Beyond the techniques outlined above, we see in [1] that Gemini was optimized to maximize the efficiency of training and inference on [Google TPUs](https://cloud.google.com/tpu/docs/intro-to-tpu). Although the details are not stated explicitly, Gemini models were likely optimized and sized based on hardware compatibility and considerations. For example, MPT-30B was sized specifically to simplify deployment onto A100 GPUs—_using quantization, a model of exactly this size can be easily deployed on a single GPU_. Similarly, Gemini Ultra’s architecture is crafted to simplify deployment on TPUs, while Gemini Nano uses a smaller architecture for easier deployment onto edge devices (e.g., a Pixel 8 smartphone).

> _“Gemini Ultra: Our most capable model … it is efficiently serveable at scale on TPU accelerators due to the Gemini architecture.”_ - from [1]

#### Multimodal Language Models

Gemini models are [multimodal](https://www.v7labs.com/blog/multimodal-deep-learning-guide), meaning that they can accept input from and produce output with multiple modalities of data. Prior attempts at creating multimodal models heavily emphasize two modalities—_text and images_—and tend to use a post-hoc approach of _i)_ training separate models on each modality and _ii)_ “stitching” these models together with further finetuning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcb44312-17dd-402d-98b6-e603d71778c4_1192x484.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcb44312-17dd-402d-98b6-e603d71778c4_1192x484.png)

(from [11])

For example, LLaVA and LLaVA-1.5 [11, 12]—_two multimodal, open-source LLMs that are popular_—begin with a pretrained LLM (i.e., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)), combine it with a [pretrained vision transformer](https://arxiv.org/abs/2103.00020), and further train the models over a multimodal dataset (i.e., image and text as input, text as output); see above. Such an approach can mimic shallow multimodal functionality but fundamentally lacks the ability to solve complex, multimodal tasks (e.g., conceptual and complex reasoning).

> _“Gemini is built from the ground up for multimodality—reasoning seamlessly across text, images, video, audio, and code.”_ - from [2]

In contrast to many existing models, Gemini models are “natively” multimodal, _meaning that they are trained from the beginning using a multimodal approach_. The models can accept various types of input data (i.e., text, audio, images, video, and code) and can produce interleaved sequences of text and images as output; see below. Using multimodal data across the entire training process (i.e., both pretraining and finetuning) gives Gemini models the ability to reason thoroughly across data modalities and produce the best possible response.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d564bc2-abd1-4a13-bbcd-9325f790a2c1_2232x1138.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d564bc2-abd1-4a13-bbcd-9325f790a2c1_2232x1138.png)

(from [1])

**Multimodal architecture.** Although few details are provided on the exact model architecture used by Gemini, authors indicate in [1] that the Gemini model architecture mimics their prior multimodal transformer architectures, such as Flamingo [13], CoCa [14], and PaLI [15]. With this in mind, we will now take a deeper look at the Flamingo model architecture, which will give us a better idea of how Gemini models might be structured.

> _“These new layers offer an expressive way for the LM to incorporate visual information for the next-token prediction task”_ - from [13]

Flamingo is a family of visualinguistic foundation models that can ingest text, images, and videos as input and produce text as output. Similarly to Gemini, Flamingo models are trained using multimodal data from the web and can solve image and video understanding benchmarks via [few-shot prompting](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26d09f28-f106-420a-aff1-c855ba95232f_1290x910.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26d09f28-f106-420a-aff1-c855ba95232f_1290x910.png)

(from [13])

The goal of Flamingo, the architecture of which is depicted above, is to _i)_ leverage existing pretrained models to avoid unnecessary compute costs[3](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-3-139569597) and _ii)_ easily merge these models into a single, multimodal architecture. More specifically, authors in [15] use a pretrained vision encoder—_a [vision transformer](https://cameronrwolfe.substack.com/p/vision-transformers) that is similar to the image encoder used by [CLIP](https://cameronrwolfe.substack.com/p/using-clip-to-classify-images-without-any-labels-b255bb7205de)_—and the [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) LLM [27] as a starting point for Flamingo. Then, two modules are added that link these architectures together:

1. _Perceiver Resampler_ [16]: takes features from the vision encoder as input (either from images or video) and produces a fixed-size set of visual tokens.
    
2. _[Cross Attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html#cross-attention)_: incorporates visual information into existing LLM layers by inserting new cross attention modules that condition on visual input between existing, pretrained layers of the LLM.
    

The perceiver module allows us to distill high-dimensional visual input (i.e., images and video) into a fixed number of visual tokens that can be considered by the LLM similarly to any other (textual) token; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ade79a6-13e9-443b-9a77-413673824425_1296x926.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ade79a6-13e9-443b-9a77-413673824425_1296x926.png)

(from [13])

The visual tokens produced by the perceiver are used to condition the output of newly-initialized cross attention layers within the LLM. In particular, gated cross attention layers are added between each layer of the LLM; see below. Inserting these added attention layers enables the LLM to attend to visual features when processing textual tokens. More specifically, Flamingo is trained over interleaved sequences of text, images, and video, and each textual token attends to the last image or video present before the occurrence of that text within the sequence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe325f056-2532-4985-aa3a-cc09dbdd5b2f_1294x670.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe325f056-2532-4985-aa3a-cc09dbdd5b2f_1294x670.png)

(from [13])

As such, the LLM gains the ability to consider visual information when performing [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction). All pretrained model weights are held fixed, and we only train the added perceiver and cross attention modules. The result of this architectural synthesis is that Flamingo can easily process interleaved sequences of text and images (or videos) and output relevant text[4](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-4-139569597), enabling the model to solve complex multimodal tasks via a few-shot prompting approach.

> _“The visual encoding of Gemini models is inspired by our own foundational work on Flamingo, CoCa, and PaLI, with the important distinction that the models are multimodal from the beginning and can natively output images using discrete image tokens."_ - from [1]

**One big distinction.** Work on Flamingo in [13] gives us a decent idea of how the architecture of Gemini might be structured. However, there is an important distinction between Gemini and models like Flamingo or CoCa—_Gemini is multimodal from the start_. Rather than stitching pretrained models together, we perform the entire pretraining process (during which the model is trained end-to-end) using multimodal data. Additionally, we see in [1] that Gemini can _i)_ ingest several other modalities of data beyond just images/video and _ii)_ generate images by outputting discrete images tokens [18]; see below for a depiction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019901b3-a654-479d-badd-f645c58cdc25_1610x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019901b3-a654-479d-badd-f645c58cdc25_1610x862.png)

(from [18])

**How do we handle so many modalities?** The Flamingo model only considers text and image modalities, while Gemini can ingest video and audio as well. The techniques used for encoding these modalities is outlined below.

- _Video_: each video is broken down into a sequence of frames, and these frames are interleaved with textual input similarly to any other image.
    
- _Audio_: we encode audio with the [Universal Speech Model (USM)](https://sites.research.google/usm/) [19].
    

Notably, Gemini does not ingest audio data by mapping it to textual tokens. Rather, the model directly ingests features generated by USM at 16 kHz, which enables more nuances within the audio data to be captured by the LLM.

**Multimodality is trending!** The main distinctions between Gemini and prior attempts at building multimodal LLMs are that _i)_ Gemini considers many different modalities and _ii)_ the model is trained in a multimodal fashion from the beginning, rather than by cheaply “stitching together” models that are trained on a single modality. Authors in [1] claim that this “_natively multimodal_” training approach is the best way to teach an LLM to deeply reason over multimodal data.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98a3a826-e875-4555-baa3-4044004b80db_1698x232.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98a3a826-e875-4555-baa3-4044004b80db_1698x232.png)

(from OpenAI API docs)

More broadly, researchers in the AI community have heavily emphasized the development of multimodal LLMs in recent months; see [here](https://huyenchip.com/2023/10/10/multimodal.html) for details. For example, GPT-4V [17]—_the multimodal extension of GPT-4 that accepts both text and image inputs_—was released recently, and OpenAI is exploring integrations of DALLE-3 [18] into the ChatGPT app to enable ingestion and generation of images within a user’s chat session[5](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-5-139569597). As such, _the proposal of Gemini is simply furthering the trend of LLM research toward multimodality_.

#### Model Training and Dataset

> _“Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data.”_ - from [1]

The exact mixture of pretraining data used for Gemini was excluded from the technical report. However, the mixture of data used for training LLMs has been shown by prior work (e.g., [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source), [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), [BioMedLM](https://www.mosaicml.com/blog/introducing-pubmed-gpt), etc.) to be a massive factor in influencing the model’s quality. In [1], authors mention several considerations that provide useful insight into the training process and dataset used for Gemini.

**Diverse sources.** Whenever possible, we should pull data from many sources (e.g., web, books, code, etc.) for use during pretraining. Going beyond pure textual data, we should incorporate data from different modalities (e.g., image, audio, video), languages, and domains (e.g., coding) into the pretraining process. As shown by Chinchilla [27], the amount (and quality!) of pretraining data is incredibly important and has a direct/noticeable impact on model quality.

**Pay attention to your tokenizer.** Most practitioners just download a pretrained tokenizer and use this for their application, assuming that it works well. But, _this is not a good idea_! Tokenization problems cause lots of downstream issues that are hard to detect and can significantly deteriorate performance. For the best results, we should train our own tokenizer over data from our pretraining set. This way, the tokenizer is specialized to the type of data that the model will encounter. In [1], we see that Gemini follows this approach exactly. For details on how to train a [SentencePiece tokenizer](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15) on your own data, check out [this discussion](https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477).

**Cleanliness is key.** Data pipelines for LLM pretraining are complex. They include heuristics, model-based schemes, safety/toxicity filters, and much more. In prior work (e.g., [RefinedWeb](https://cameronrwolfe.substack.com/i/131393593/refinedweb-scalable-curation-of-text-from-the-web)), we see that authors emphasize the use of only simple heuristics for filtering pretraining data. However, _Gemini seems to throw the kitchen sink at the pretraining data pipeline_. They use all tools that are available to craft the cleanest pretraining dataset possible. Put simply, the best pipeline to use for processing pretraining data is not standardized. However, ensuring that the pretraining data is high-quality and clean is incredibly important.

**Data weighting.** Beyond data sources and quality, the frequency with which we sample data from each pretraining data source (i.e., the data weight) is important! To tune this data weight, we should run tuning experiments with smaller models and datasets to determine optimal settings. Interestingly, authors of Gemini also mention that varying the data weights throughout training (e.g., increasing the weight of domain-specific info towards the end of training) can be helpful.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8dfdef7-d64e-460a-8c11-e2c3cd383335_1960x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8dfdef7-d64e-460a-8c11-e2c3cd383335_1960x840.png)

(from [1])

**Distilling smaller models.** Finally, we see in [1] that authors leverage a knowledge distillation approach for training the smaller Gemini Nano models; see above. Although this might sound complex, it just means that Gemini Nano is trained using the outputs of larger Gemini models as a target. Such an approach—_referred to as [knowledge distillation](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7knowledge-distillation) [17] within AI research_—is commonly used and highly-effective. By distilling the knowledge of a larger network into a smaller network, we can get a smaller LLM to perform much better compared to just training it from scratch (i.e., without using the larger model’s output)[6](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-6-139569597).

**Other training (and infrastructural) details.** Gemini models are trained using [TPU accelerators](https://cloud.google.com/tpu/docs/intro-to-tpu)—[TPUv4](https://arxiv.org/abs/2304.01433) and [TPUv5e](https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga) in particular. These TPUs are custom-designed for AI workloads (i.e., training and serving big neural nets). Compared to PaLM-2, Gemini Ultra is trained using a much larger infrastructure, comprised of TPUv4 accelerators distributed across multiple data centers. TPUv4 devices are grouped into “super pods” that contain 4096 chips each, and Gemini Ultra is trained across several super pods in different data centers. Communication between TPUs in the same super pod is fast, but communication between super pods is (comparatively) slow. For this reason, Gemini Ultra is trained using a combination of model parallelism (within each superpod) and data parallelism (across superpods). Such an approach mimics the training strategy of [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [5].

> _“We trained Gemini 1.0 at scale on our AI-optimized infrastructure using Google’s in-house designed TPUs v4 and v5e. And we designed it to be our most reliable and scalable model to train, and our most efficient to serve.”_ - from [3]

Interestingly, Gemini is trained using Jax and Google’s [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) framework. This approach allows the entire training run to be orchestrated with a single Python process, thus simplifying development workflows. Additionally, authors avoid periodically writing model checkpoints to disk throughout training, choosing instead to keep in-memory replicas of model state that can be used to recover from any hardware failures[7](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-7-139569597). Such an approach speeds up recovery time and improves the overall throughput of the training process.

## How does it perform?

Given that Gemini is trained over several different modalities of data, we might wonder whether this model matches (or surpasses) the performance of both:

1. Other LLMs.
    
2. Models trained specifically on each domain or modality.
    

In [1], authors perform an extensive empirical validation of Gemini models across several text-based benchmarks and a comprehensive group of multimodal datasets. Additionally, Gemini is used to build a successor to [AlphaCode](https://deepmind.google/discover/blog/competitive-programming-with-alphacode/)—_an agent used for solving competitive programming problems_—called AlphaCode-2, which demonstrates the impressive coding capabilities of Gemini models.

#### The TL;DR…

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b4c042-c2c0-4294-ae94-242bc04aef2b_2464x1222.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b4c042-c2c0-4294-ae94-242bc04aef2b_2464x1222.png)

(from [2])

Gemini Ultra achieves state-of-the-art performance on 30/32 tasks considered, including both text-based and multimodal tasks. The only tasks on which Gemini Ultra is outperformed are purely text-based tasks, where GPT-4 achieves better performance in a few cases. Going further, Gemini Ultra is the first model to achieve 90% accuracy—_surpassing human accuracy of 89.8%_—on the MMLU dataset, though this score is achieved with a modified chain of thought prompting approach that is not used by baselines; see above. Seemingly, the results of Gemini Ultra are slightly emphasized, as certain specialized prompting approaches are used on select tasks. Nonetheless, the model’s performance on text-based tasks is quite impressive and rivals that of GPT-4 in most cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F748eb876-93ae-4bfb-9f7a-72b242451177_1594x484.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F748eb876-93ae-4bfb-9f7a-72b242451177_1594x484.png)

(from [1])

Gemini models shine the most on multimodal tasks. Gemini Ultra achieves new state-of-the-art performance on the MMMU benchmark, where it outperforms prior models by over 5%. However, authors in [1] again use a different prompting strategy to achieve these results. Using standardized prompting techniques, Gemini Ultra achieves only a 2.6% improvement over the prior best model; see above. Going further, Gemini models outperform all baseline techniques on a variety of video and audio understanding tasks, as well as demonstrate impressive cross-modal reasoning capabilities in qualitative experiments. Notably, these results are achieved without the use of OCR or audio transcription modules—_text, image, video, and audio data is directly ingested by the model_.

#### Text-Based Benchmarks

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4beab2e1-a2ef-4758-9381-d8c5da63b98f_1410x1344.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4beab2e1-a2ef-4758-9381-d8c5da63b98f_1410x1344.png)

(from [1])

The performance of Gemini models across all textual benchmarks is presented within the table above. On text-based benchmarks, Gemini Pro outperforms inference-optimized models like GPT-3.5, while Gemini Ultra outperforms nearly all current models. We will now break down some of these results by category to provide a more nuanced perspective on Gemini’s performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3e213d-912a-440c-aaa7-a7865b4973d8_1900x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3e213d-912a-440c-aaa7-a7865b4973d8_1900x862.png)

(from [1])

**Text-based, multitask performance.** On [MMLU](https://huggingface.co/datasets/lukaemon/mmlu)—_a popular benchmark that measures knowledge across 57 subjects_—Gemini Ultra is the first LLM to surpass human-level performance (89.8%) with a score of 90.04%; see above. Prior state-of-the-art performance on MMLU was 86.4%. However, Gemini Ultra uses a specialized variant of [Chain of Thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)[8](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-8-139569597) to achieve this score, which makes a direct comparison of these results somewhat misleading. Without the specialized prompting approach, Gemini is actually outperformed by GPT-4, and GPT-4 is not evaluated using the exact prompting strategy used for Gemini Ultra.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F604877e4-7e7b-4066-838d-6df80d59eb7d_2160x408.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F604877e4-7e7b-4066-838d-6df80d59eb7d_2160x408.png)

(from [1])

**Can Gemini do math?** On math problems, we see that Gemini Ultra achieves competitive performance; see above. More specifically, Gemini Ultra uses Chain of Thought prompting and [self-consistency](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)—_the same approach used by prior work_—to achieve new state-of-the-art performance on [GSM8K](https://huggingface.co/datasets/gsm8k). On more advanced problems, we see a similar boost in performance from using Gemini Ultra, even when using a simpler prompting approach (i.e., few-shot learning). Interestingly, smaller models (i.e., both Gemini Pro and GPT-3.5) perform quite poorly—_even nearing random performance in certain cases_—on challenging math benchmarks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe9b73b3-9efb-49d0-a1cd-d4dd5b4a64fc_1618x654.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe9b73b3-9efb-49d0-a1cd-d4dd5b4a64fc_1618x654.png)

(from [1])

**Coding benchmarks.** The coding abilities of Gemini models are evaluated:

1. On standard coding benchmarks.
    
2. As part of a new reasoning system called AlphaCode 2.
    

First, Gemini is evaluated on coding benchmarks like [HumanEval](https://huggingface.co/datasets/openai_humaneval) and Natural2Code—_an internal evaluation benchmark (from Google) for python code generation that has no leakage_[9](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-9-139569597) _from the web_. As shown above, Gemini Ultra achieves new state-of-the-art performance on coding benchmarks, while Gemini Pro outperforms GPT-4 on HumanEval but slightly underperforms GPT-4 on the new Natural2Code benchmark. Put simply, _Gemini Ultra has impressive coding abilities_, while Gemini Pro is (roughly) comparable to GPT-4 in this domain.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe934a31c-493e-4388-a247-3d96b9ba1306_1548x1366.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe934a31c-493e-4388-a247-3d96b9ba1306_1548x1366.png)

(from [22])

Beyond evaluation on standard coding benchmarks, Gemini Pro—_more specifically, a specialized version of Gemini Pro that is finetuned over competitive programming data_—is used to create a successor to the [AlphaCode](https://deepmind.google/discover/blog/competitive-programming-with-alphacode/) [21], a transformer-based coding agent that combines code generation with smart filtering algorithms to accurately solve competitive programming tasks. Gemini Pro is used to create [AlphaCode 2](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf) [22], which solves 43% of competition problems on [Codeforces](https://codeforces.com/) (AlphaCode solved 25%) and outperforms 85% of competition entrants (AlphaCode outperforms 50%). The Gemini-based AlphaCode 2 is a drastic improvement over prior work in terms of problem solving capabilities.

> _“The composition of powerful pretrained models with search and reasoning mechanisms is an exciting direction towards more general agents”_ - from [1]

As shown in the figure above, Gemini Pro is finetuned to both generate code and rank solutions—_basically acting as a [reward model](https://cameronrwolfe.substack.com/i/138218863/how-does-rlhf-work) that recognizes promising solutions_—for AlphaCode 2. To solve a problem, the language model first samples the search space by generating a bunch of candidate solutions. Then, we can perform filtering and clustering on these solutions to identify a smaller set of high-quality candidates. Finally, the LLM is used to score each of these solutions as part of a reranking step before a submission is made. Here, we see that powerful agents can be created by combining Gemini with additional tools like search and ranking.

**Context length test.** Gemini models are trained using a 32K context length, which is larger than prior models like PaLM-2[10](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-10-139569597). However, recent [empirical analysis](https://x.com/GregKamradt/status/1727018183608193393?s=20) has shown us that context window size can be misleading—_what matters is whether the model can actually utilize the larger context window_. For example, LLMs with large context windows oftentimes have a position or recency bias, where they only pay attention to information at the beginning/end of the context. The ability of LLMs to leverage their context window is typically tested with a [“needle in the haystack” approach](https://github.com/gkamradt/LLMTest_NeedleInAHaystack), where we insert facts at different locations in a context window of varying sizes and ask the model to restate the fact.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7ee36fc-de72-4608-8d1b-424f2d907f27_1412x468.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7ee36fc-de72-4608-8d1b-424f2d907f27_1412x468.png)

(from [1])

Gemini models are first evaluated by retrieving information from the beginning of a context window with varying length, instead of a needle in the haystack approach. Unsurprisingly, the model is able to consistently retrieve information in this setup. From here, authors in [1] evaluate Gemini over a held-out set of long documents by plotting the [negative log likelihood](https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81) of tokens at different positions within the sequence. As shown above, Gemini models seem to be relatively capable of recalling information across the full context window.

#### Problem Solving over Multiple Modalities

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96133008-057f-42ec-85f2-f22023b5c44f_1604x1240.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96133008-057f-42ec-85f2-f22023b5c44f_1604x1240.png)

(from [1])

Although Gemini performs (relatively) well on language-only benchmarks, one of the most enticing aspects of the model is its ability to extend the problem-solving capabilities of an LLM across multiple data modalities. Namely, Gemini models can pick up on details within their input (e.g., particular details of an image), aggregate relevant info over space/time in videos, and even handle dense temporal sequences such as audio. Going further, Gemini models can even augment their output by generating corresponding images; see above. Within this section, we will explore the multimodal capabilities and evaluations of Gemini models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a71815b-4079-4f12-bd4a-56bf2d5ac52d_1406x1354.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a71815b-4079-4f12-bd4a-56bf2d5ac52d_1406x1354.png)

(from [1])

**Image understanding.** The ability of Gemini models to process and reason over images is tested using four different kinds of tasks:

- Object recognition (e.g., [VQAv2](https://visualqa.org/))
    
- Transcription, or recognizing text in an image (e.g., [TextVQA](https://textvqa.org/))
    
- Chart understanding (e.g., [ChartQA](https://github.com/vis-nlp/ChartQA))
    
- Multi-modal reasoning (e.g., [MMMU](https://arxiv.org/abs/2311.16502))
    

In the table above, we see that Gemini Ultra achieves new state-of-the-art performance across several image understanding benchmarks, outperforming GPT-4V and several finetuned models for each benchmark. Here, models are primarily evaluated using a zero-shot approach that just instructs the model to generate a short answer corresponding to the task at hand (e.g., transcribe the text in an image). Gemini Pro achieves less impressive performance relative to Gemini Ultra, but it performs comparably to GPT-4V on a few benchmarks. Furthermore, we see in [1] that Gemini has the ability to reason over images in multiple languages, which adds another interesting dimension to these models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F748eb876-93ae-4bfb-9f7a-72b242451177_1594x484.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F748eb876-93ae-4bfb-9f7a-72b242451177_1594x484.png)

(from [1])

Most notably, Gemini Ultra achieves the best score of any model so far on MMMU, beating prior state-of-the-art by 5% overall; see above. MMMU is a recently-released benchmark consisting of questions about images that span 6 different disciplines and require college-level knowledge to solve. The impressive performance of Gemini on MMMU was heavily marketed within the models’ release and emphasizes the ability of these models to perform multimodal problem solving. We should note, however, that the 5% absolute improvement is (again) achieved with a different prompting strategy. When evaluated using a the same prompting strategy, Gemini Ultra outperforms GPT-4V by 2.6%.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba2b4f53-3450-4fbc-a69a-9ec88c185aa6_1840x1072.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba2b4f53-3450-4fbc-a69a-9ec88c185aa6_1840x1072.png)

(from [1])

**Video understanding.** To ingest video, Gemini models extract 16 equally-spaced frames (i.e., these are just images!) from each video clip to ingest as input. Gemini models can use this approach to achieve impressive performance on video captioning and question answering datasets; see above. Plus, the video processing capabilities of Gemini extend beyond the English language—_we see above that Gemini Ultra (and Pro) can even perform video captioning tasks in Chinese_.

> _“Video understanding is accomplished by encoding the video as a sequence of frames in the large context window. Video frames or images can be interleaved naturally with text or audio as part of the model input.”_ - from [1]

The video processing capabilities of Gemini are rudimentary—we are just processing video clips as individual frames/images. Yet, _few mainstream LLMs have these capabilities_! For example, GPT-4V only has the ability to process images. As a result, Gemini models pioneer the integration of video with LLMs. Given that videos are a huge, untapped source of data for AI, such an approach holds massive potential. Imagine, for example, if we could extend LLM pretraining to encompass not only text, but also all videos that are available on the web. Such an approach could drastically expand the volume of pretraining data that is available!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a403736-3e58-4e08-829a-59a1d1cabb69_1274x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a403736-3e58-4e08-829a-59a1d1cabb69_1274x596.png)

(from [1])

**Audio understanding.** Finally, Gemini can directly ingest audio data—_a capability that has not been heavily explored within LLM research_. When evaluated on automatic speech recognition (i.e., transcribing audio) and automatic speech translation (i.e., transcribing audio and translating it into a different language), Gemini Pro outperforms popular models like USM [19] and [Whisper](https://openai.com/research/whisper) [23] on all tasks; see above. Notably, these results were achieved with the less powerful Gemini Pro model, and even Gemini Nano outperforms prior models on a majority of tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59572388-0e3d-41d9-9145-d96bb7625ecc_1298x1070.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59572388-0e3d-41d9-9145-d96bb7625ecc_1298x1070.png)

(from [1])

**Putting it all together.** So far, we have seen that Gemini models have the ability to process and solve problems over individual modalities of data. But, the magic happens when we combine all of these modalities within a single model! As shown in the figure above, Gemini models have the ability to combine and synthesize data from each of the different modalities that it understands. Such an ability to ingest data from several, diverse sources is currently unavailable within any other model—_most existing LLMs focus upon two modalities of data at most (e.g., images and text)._ For a more in-depth overview of multimodal prompting and problem solving with Gemini, check out the tutorial below.

[Multimodal Prompting Guide](https://developers.googleblog.com/2023/12/how-its-made-gemini-multimodal-prompting.html)

## Gemini in the Wild!

Despite being proposed so recently, Gemini models are already being heavily used in practice. Bard is using a [finetuned version of Gemini Pro](https://blog.google/products/bard/google-bard-try-gemini-ai/) for help with solving questions that require advanced reasoning or planning, Gemini Ultra will be used in Bard early next year, Gemini Nano is being [deployed on the Pixel 8 smartphone](https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/) to power features like summarization and smart replies, and Gemini is being used to power Google’s [search generative experience (SGE)](https://blog.google/products/search/generative-ai-search/) with a 40% reduction in latency and better quality. Currently, Gemini Pro is accessible via the [Gemini API](https://ai.google.dev/docs), Google is releasing an [AICore framework](https://developer.android.com/ml/aicore) (in beta) for using Gemini Nano in on-device tasks, and Gemini Ultra is only available to select developers for experimentation and feedback as it undergoes further safety testing. Given the many practical use cases of Gemini models, let’s quickly take a look at some interesting analysis, applications, and tools that might be relevant.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d9255a-1862-44dc-ad79-0bf3948d2e5e_1080x618.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d9255a-1862-44dc-ad79-0bf3948d2e5e_1080x618.png)

(from [24])

**Evaluating Gemini’s language capabilities [24].** After the release of Gemini, [authors](https://x.com/gneubig/status/1737108966931673191?s=20) from Carnegie Mellon University released a 3rd party (objective) evaluation of the models’ language capabilities in comparison to GPT models from OpenAI. All of the code from these evaluations is [available online](https://github.com/neulab/gemini-benchmark) and fully reproducible. This analysis provides several valuable insights:

- Gemini Pro is (slightly) worse than GPT-3.5-Turbo on all tasks considered.
    
- Gemini excels at generating non-English output.
    
- Gemini is better at handling longer and more complex reasoning chains.
    

Gemini Ultra is not yet publicly accessible, so the results provided in [24] only apply to Gemini Pro. Nonetheless, we learn from this analysis that _i)_ Gemini Pro lags behind comparable models from OpenAI and _ii)_ the gap in performance between these models is less significant compared to top open-source models (e.g., [Mixtral](https://mistral.ai/news/mixtral-of-experts/)). Notably, however, several researchers in the AI community [have claimed](https://x.com/bindureddy/status/1737144502543077777?s=20) that the evaluations in [24] misrepresent the quality of Mixtral.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988b461e-70db-4d38-a9c8-6c10dcc0c875_2368x1106.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988b461e-70db-4d38-a9c8-6c10dcc0c875_2368x1106.png)

(from [25])

**Evaluating Gemini’s image understanding.** Authors in [25] provide a similar 3rd party analysis of Gemini Pro’s ability to process visual information in comparison to GPT-4V. The visual capabilities of these models are measured in the following domains: fundamental perception, advanced cognition, challenging vision tasks, and various expert capacities. The paper is incredibly long (128 pages), but the primary results can be boiled down into a few key points:

1. Gemini Pro and GPT-4V have comparable visual reasoning capabilities (i.e., _Gemini Pro is a valid competitor to GPT-4V_).
    
2. The models have different output styles—Gemini writes concise answers, while GPT-4V provides detailed steps and explanations; see above.
    
3. Open-source variants (e.g., Sphinx [26]) lag far behind GPT-4V and Gemini.
    
4. Multimodal LLMs still have a long way to go, as even Gemini Pro and GPT-4V have noticeable lapses in visual understanding/reasoning and lack robustness to slight variations in their prompts.
    

In other words, Gemini Pro’s visual understanding is impressive, and the model can reason over several additional modalities beyond just images!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e252d-6a0f-4fbb-a79c-90a2ba9deef3_1074x448.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff36e252d-6a0f-4fbb-a79c-90a2ba9deef3_1074x448.png)

**Gemini + LangChain.** Shortly after the release of Gemini, the popular LLM abstraction toolkit LangChain released an [integration with Gemini models](https://python.langchain.com/docs/integrations/chat/google_generative_ai). Notably, this is not the first integration of LangChain with Google models, as LangChain already provided support for PaLM-2; see [here](https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm.html). However, the integration with Gemini Pro provides both language and vision support, which unlocks a wide range of capabilities and applications for AI practitioners.

**Are the results legit?** Gemini was heavily marketed as the first legitimate competitor to OpenAI models like GPT-3.5-Turbo and GPT-4. Opinions of Gemini have been mixed to say the least… some are saying the [results are completely fake](https://x.com/joecarlsonshow/status/1733262373518258654?s=20), others seem to think [they are legit](https://x.com/abacaj/status/1732420265886544195?s=20), while many people are [somewhere in the middle](https://x.com/NaveenGRao/status/1732502264500322416?s=20). As we’ve seen so far, the results achieved by Gemini are definitely impressive, though (possibly) exaggerated in certain areas. However, we’ve yet to run one final test—_trying the model out for ourselves_.

To get a better feel for Gemini, let’s test a few interesting examples ourselves and compare the results that we get to those presented within the Gemini technical report. Given that Gemini Ultra has not yet been publicly released, we will obtain our results using Gemini Pro within Vertex AI. This tool only supports text and image inputs, but we can still use it to do some interesting analysis. First up, we can try to identify a plant and get some basic caretaking instructions; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94fbbcfa-5965-43a3-953b-45da5ee07dfd_2040x1018.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94fbbcfa-5965-43a3-953b-45da5ee07dfd_2040x1018.png)

(from [1] and Vertex AI)

The response provided by Gemini Pro is more concise, yet it still _i)_ correctly identifies the plant and _ii)_ provides the (correct) primary caretaking step for the plant. As such, the response by Gemini Pro in this case seems both reasonable and useful. Next up, we test out the spatial reasoning capabilities of Gemini Pro via a basic visual reasoning problem in which the model must detect several shapes in an image and determine a pattern in the sequence of shapes; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc5440-665c-4da7-b6bc-f47159c11769_2360x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc5440-665c-4da7-b6bc-f47159c11769_2360x860.png)

(from [1] and Vertex AI)

In this case, Gemini Pro produces the correct answer and (arguably) even provides us with a better chain of thought compared to Gemini Ultra. Although this problem is relatively simple, it demonstrates impressive visual recognition and reasoning capabilities. In other words, _the multimodal capabilities of Gemini are definitely legit_! Now, we can push these visual recognition and reasoning skills further by asking the model to do a little bit of (basic) math; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b65baf4-a7fd-41a8-8f60-a16843e6ccee_1436x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b65baf4-a7fd-41a8-8f60-a16843e6ccee_1436x782.png)

(from [1])

This time, Gemini Pro gets many of the reasoning steps for solving the problem correct; see below. However, the model fails to recognize the relation between the height and width of the parallelogram (i.e., they are both expressed in terms of `x`). As such, the model fails to arrive at a complete final answer, despite getting many of the reasoning steps correct while working towards a final solution.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F430e2140-80d8-48e3-ae96-cd9dc441c3e7_1586x1306.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F430e2140-80d8-48e3-ae96-cd9dc441c3e7_1586x1306.png)

(from Vertex AI)

Next, we can test Gemini’s ability to leverage both its knowledge base and visual recognition capabilities in tandem to solve an interesting problem—_determining the relationship between a picture of the moon and of a golf ball_. Interestingly, the moon is (apparently) the only celestial body in which humans have played golf. When we test the ability of Gemini Pro to draw this connection, we see that the model easily solves this problem and even provides more information than Gemini Ultra! After further researching the topic, I confirmed that the club used by Alan Shepard to perform the shot [was indeed a six-iron](https://www.space.com/apollo-14-moon-landing-golf-shot-analysis), but I cannot find any information on the length of the shot. Either way, the results are quite impressive!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F196c3ced-d487-4d0e-8356-57c06b09a759_1318x1340.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F196c3ced-d487-4d0e-8356-57c06b09a759_1318x1340.png)

(from [1] and Vertex AI)

Finally, there are three categories of basic testing that I find to be particularly useful when gauging the capabilities of any LLM:

1. Checking instruction following capabilities.
    
2. Determining if the model can write and explain code.
    
3. Seeing if the model can generate valid API calls.
    

These use cases are incredibly common in practice, and I personally leverage these particular skills frequently to build LLM applications. As such, these three categories of skills serve as a simple, yet important sanity check for determining the usefulness of any LLM. After performing several experiments, the instruction following capabilities of Gemini Pro seem quite strong, and the model adheres to prompts with varying degrees of instruction complexity; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10bc12e8-26f0-431f-a0b0-3cfa485f941e_2226x1062.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10bc12e8-26f0-431f-a0b0-3cfa485f941e_2226x1062.png)

Gemini Pro is good at following instructions (from Vertex AI)

When we ask Gemini to explain a snippet of code (even when provided as an image!), the results are useful and correct. Plus, we can even ask Gemini Pro to generate new or modified code and the results are promising. Put simply, _Gemini Pro passes the basic coding test and can reasonably explain and write code_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf2ec25a-6184-4655-8039-759bef6c4312_1210x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf2ec25a-6184-4655-8039-759bef6c4312_1210x1328.png)

Gemini Pro can explain, modify, and write code (from Vertex AI)

Finally, we can test the ability of Gemini Pro to generate valid function calls. Such a skill is important for building applications with LLMs (e.g., agent systems or integrating LLMs with [tools](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools)). In a variety of experiments, Gemini Pro seems more than capable of _i)_ understanding different APIs or functions, _ii)_ inferring when these APIs should be used, and _iii)_ generating valid JSON documents with correct fields that can be used to call an API; see below for a few examples.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a13c8f6-d895-4dac-b99a-49d29172af48_2308x620.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a13c8f6-d895-4dac-b99a-49d29172af48_2308x620.png)

Gemini Pro can make accurate function calls (from Vertex AI)

After spending several hours testing Gemini Pro, I’ve concluded that the Gemini models are very impressive. In other words, _the hype is real_ (not just a marketing tactic). Aside from GPT-4, Gemini Pro is without a doubt the easiest and most performant model with which I have worked. In fact, Gemini Pro seems easier to use (at least in my opinion) compared to GPT-3.5-Turbo in many cases. Plus, the model supports image inputs and is not even the most powerful in the Gemini suite. Now, all that’s left is to wait for the release of Gemini Ultra!

## Final Remarks

At this point, we have learned quite a lot about the Gemini model suite. These models were marketed as the first legitimate competitors to ChatGPT and GPT-4, and _these claims seem to be legit_! Sure, some results were emphasized on certain benchmarks with cherry-picked prompting techniques. However, Gemini models still perform comparably to state-of-the-art in nearly all cases, even when evaluated by a (non-biased) 3rd party! Plus, these models have several notable benefits, such as the support for numerous modalities and availability of efficient model variants for on-device deployment. Key takeaways are itemized below.

**Multimodality is here to stay!** Multimodal LLMs (or MLLMs) have become an incredibly popular topic in recent AI research. However, many MLLMs consider only two modalities—images and text. Gemini pushes the frontier of MLLMs to consider other modalities like audio and video. By adopting a native training approach over each of these modalities, we see that Gemini models develop impressive problem solving capabilities on both cross and uni-modal tasks!

**Native multimodality.** Saying that Gemini models are “natively” multimodal has a few different implications. First, multimodal data is used throughout the entire training process, including both pretraining and finetuning. Second, Gemini models directly ingest data from each modality. For example, Gemini directly ingests audio features generated from USM, rather than first transcribing audio into textual tokens and ingesting the textual input. By directly ingesting data from each modality, Gemini models are able to extend the already impressive reasoning capabilities of an LLM across numerous modalities of data.

**Comparison to OpenAI models.** Although a ton of empirical results were provided in [1], we can boil them down into two primary takeaways:

- Gemini Ultra is (roughly) comparable to GPT-4.
    
- Gemini Pro is (roughly) comparable to GPT-3.5-Turbo.
    

In both cases, these models seem to perform slightly worse than their OpenAI counterparts on text-based tasks. However, these models are also capable of solving multimodal tasks and seem to truly excel in this domain.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Google Gemini Team et al. “Gemini: A Family of Highly Capable Multimodal Models”, [https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) (2023).

[2] Sundar Pinchai and Demis Hassabis, “Introducing Gemini: our largest and most capable AI model”, [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/) (2023).

[3] Google Gemini Team, “Welcome to the Gemini Era”, [https://deepmind.google/technologies/gemini/#introduction](https://deepmind.google/technologies/gemini/#introduction) (2023).

[4] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[5] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[6] Anil, Rohan, et al. "Palm 2 technical report." _arXiv preprint arXiv:2305.10403_ (2023).

[7] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[8] Chen, Xiangning, et al. "Symbolic discovery of optimization algorithms." _arXiv preprint arXiv:2302.06675_ (2023).

[9] Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." _Advances in Neural Information Processing Systems_ 35 (2022): 16344-16359.

[10] Dao, Tri et al. “Flash-Decoding for long-context inference”, [https://www.together.ai/blog/flash-decoding-for-long-context-inference](https://www.together.ai/blog/flash-decoding-for-long-context-inference) (2023).

[11] Liu, Haotian, et al. "Visual instruction tuning." _arXiv preprint arXiv:2304.08485_ (2023).

[12] Liu, Haotian, et al. "Improved baselines with visual instruction tuning." _arXiv preprint arXiv:2310.03744_ (2023).

[13] Alayrac, Jean-Baptiste, et al. "Flamingo: a visual language model for few-shot learning." _Advances in Neural Information Processing Systems_ 35 (2022): 23716-23736.

[14] Yu, Jiahui, et al. "Coca: Contrastive captioners are image-text foundation models." _arXiv preprint arXiv:2205.01917_ (2022).

[15] Chen, Xi, et al. "Pali: A jointly-scaled multilingual language-image model." _arXiv preprint arXiv:2209.06794_ (2022).

[16] Jaegle, Andrew, et al. "Perceiver: General perception with iterative attention." _International conference on machine learning_. PMLR, 2021.

[17] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." _arXiv preprint arXiv:1503.02531_ (2015).

[18] Yu, Jiahui, et al. "Scaling autoregressive models for content-rich text-to-image generation." _arXiv preprint arXiv:2206.10789_ 2.3 (2022).

[19] Zhang, Yu, et al. "Google usm: Scaling automatic speech recognition beyond 100 languages." _arXiv preprint arXiv:2303.01037_ (2023).

[20] Michalak, Sarah E., et al. "Assessment of the impact of cosmic-ray-induced neutrons on hardware in the roadrunner supercomputer." _IEEE Transactions on Device and Materials Reliability_ 12.2 (2012): 445-454.

[21] Li, Yujia et al. “Competition-level code generation with alphacode.” Science, 378(6624):1092–1097, 2022.

[22] Google AlphaCode Team et al. “AlphaCode 2 Technical Report”, [https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) (2023).

[23] Radford, Alec, et al. "Robust speech recognition via large-scale weak supervision." _International Conference on Machine Learning_. PMLR, 2023.

[24] Akter, Syeda Nahida, et al. "An In-depth Look at Gemini's Language Abilities." _arXiv preprint arXiv:2312.11444_ (2023).

[25] Fu, Chaoyou, et al. “A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise.” _arXiv preprint arXiv:2312.12436_ (2023). 

[26] Lin, Ziyi, et al. "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models." _arXiv preprint arXiv:2311.07575_ (2023).

[27] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[28] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).

[29] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." _arXiv preprint arXiv:2201.08239_ (2022).

[1](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-1-139569597)

Arguably four Gemini models are released, as two different sizes of Gemini Nano are made available (1.8B parameters and 3.25B parameters).

[2](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-2-139569597)

Fun fact, the Gemini technical report has ~940 authors. That’s a lot of people, but still not the largest numbers of authors we’ve seen on [one paper](https://x.com/AlexGDimakis/status/1737598802415018157?s=20)!

[3](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-3-139569597)

Here, the reason we avoid compute costs is because the pretrained models begin the training process for Flamingo with a large amount of existing information already encoded within their weights. As such, we don’t have to perform extra training to re-learn all of this information!

[4](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-4-139569597)

Notably, Flamingo only takes images/videos and text as input, and only produces text as output. However, one can reasonably see how such an architecture can be generalized to handle more modalities of input (i.e., by adding more encoders beyond those for vision) and even generate outputs of a different modality.

[5](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-5-139569597)

The approach for this is actually (relatively) simple! We just train the LLM (i.e., ChatGPT) to identify when images should be generated and instruct the model to write a prompt that can be sent to DALLE-3 for image generation; see [here](https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise).

[6](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-6-139569597)

Put differently, if we train a smaller LLM from scratch and compare its performance to that of an LLM that is trained identically but also with knowledge distillation from a larger LLM, the model that uses knowledge distillation will (generally) perform better.

[7](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-7-139569597)

Training a model over such a large number of accelerators is incredibly difficult due to the high probability of device failure within the system. If you are training a neural net using one TPU, it’s pretty unlikely that this device will experience a failure. But, _what if you’re training over 10,000 of these devices?_ The probability and frequency of device failure within such a large system is relatively high. These failures come from a variety of sources, including anything from nodes being [preempted](https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms) to cosmic rays [20].

[8](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-8-139569597)

This prompting approach goes beyond basic chain of thought prompting by accounting for model uncertainty. The model generates `k` samples and checks for a consensus among these samples. If there is no consensus, the answer is generated using a greedy sample from the model without chain of thought prompting.

[9](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-9-139569597)

Typically, the term data “leakage” (or contamination) refers data that is used for evaluating LLMs being present in the model’s training set. Given that LLMs are pretrained using so much data, leakage is a common issue that needs to be addressed.

[10](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction#footnote-anchor-10-139569597)

PaLM-2 has a context window of 8K tokens. However, several recent models have quite large context windows (e.g., 128K for [GPT-4](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) and 200K for [Claude-2.1](https://www.anthropic.com/index/claude-2-1-prompting)) that go well beyond the 32K context of Gemini.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Faqir Ahmed's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb94b289d-5d0c-49dc-8537-149c1bd4c775_144x144.png)



](https://substack.com/profile/145307431-faqir-ahmed)

[

![Shaun's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16efbddd-50dd-4c4a-9c47-0c0a6d90d02a_1920x1080.jpeg)



](https://substack.com/profile/165546589-shaun)

[

![Uzair Ali's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eea032d-0b4c-4208-b571-2c92f1a12488_144x144.png)



](https://substack.com/profile/92236542-uzair-ali)

[

![taesiri's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F615de068-b7df-4577-bda0-6f864518d2aa_640x640.jpeg)



](https://substack.com/profile/5549752-taesiri)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

89 Likes∙

[4 Restacks](https://substack.com/note/p-139569597/restacks?utm_source=substack&utm_content=facepile-restacks)

89

- 

[

2

](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Joris C.'s avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72004f8-540c-494a-bb99-11c7926c8c77_144x144.png)



](https://substack.com/profile/138365757-joris-c?utm_source=comment)

[Joris C.](https://substack.com/profile/138365757-joris-c?utm_source=substack-feed-item)

[2023年12月25日](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction/comment/45999237 "2023年12月25日 05:24")

Liked by Cameron R. Wolfe, Ph.D.

It does seem that the CMU results for Mixtral are off.

LMsys' leaderboard has both Mixtral and Gemini Pro comparable to GPT 3.5 Turbo: [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) (last edit 20th of December)

For Mixtral, this complies with OpenCompass' recent results (24th): [https://github.com/open-compass/MixtralKit](https://github.com/open-compass/MixtralKit)

Also according to OpenCompass, Vision-Language of Gemini Pro and GPT 4V are comparable:

[https://opencompass.org.cn/leaderboard-multimodal](https://opencompass.org.cn/leaderboard-multimodal)

(though it's unclear what "detail: low" means for GPT 4)

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction/comment/45999237)

[1 more comment...](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# The Basics of AI-Powered (Vector) Search

### How the modern AI boom has completely revolutionized search applications...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jan 08, 2024

66

- 

[

1

](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search/comments)

4

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/), the commerce AI company. If you like the newsletter, feel free to subscribe below, [get in touch with me](https://cameronrwolfe.me/) or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/). I try my best to produce useful/informative content.

Subscribe

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09351a8-1a62-47db-a200-9383dc5143a7_2398x1344.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09351a8-1a62-47db-a200-9383dc5143a7_2398x1344.png)

(from [2])

The recent generative AI boom and advent of large language models (LLMs) has led many to wonder about the evolution of search engines. _Will dialogue-based LLMs replace traditional search engines, or will the tendency of these models to hallucinate make them an untrustworthy source of information?_ Currently, the answer to these questions is unclear, but the quick adoption of AI-centric search systems such as [you.com](https://you.com/) and [perplexity.ai](https://www.perplexity.ai/) indicates a widespread interest in augmenting search engines with modern advancements in language models. Ironically, however, _we have been heavily utilizing language models within search engines for years_! The proposal of BERT [1] led to a step-function improvement in our ability to assess semantic textual similarity, causing these language models to be adopted by a variety of popular search engines ([including Google](https://blog.google/products/search/search-language-understanding-bert/)!). Within this overview, we will analyze the components of such AI-powered search systems.

## Basic Components of a Search Engine

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2eb0172-0882-477a-9ff1-f410cbe8dd21_1938x960.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2eb0172-0882-477a-9ff1-f410cbe8dd21_1938x960.png)

Retrieval and ranking within a search engine

Search engines are one of the longest-standing and most widely-used applications of machine learning and AI. Most search engines are comprised of two basic components at their core (depicted above):

- _Retrieval_: from the set of all possible documents, identify a much smaller set of candidate documents that might be relevant to the user’s query.
    
- _Ranking_: use more fine-grained analysis to order the set of candidate documents such that the most relevant documents are shown first.
    

Depending upon our use case, the total number of documents over which we are searching could be very large (e.g., all products on Amazon or all web pages on Google). As such, the retrieval component of search must be efficient—_it quickly identifies a small subset of documents that are relevant to the user’s query_. Once we have identified a smaller set of candidate documents, we can use more complex techniques—_such as larger neural networks or more data_—to optimally order the candidate set in a manner that is personalized and relevant to the user’s query.

**More details.** The intuitive idea behind search is straightforward, but many different approaches exist for retrieval and ranking; see above. Some of these approaches are more traditional and only rely on basic machine learning, while others heavily leverage modern developments in large language models. In other words, the amount of “AI” present within “AI-powered search” is variable. We will now quickly overview these techniques to build a better understanding.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F407d89cc-a925-4b68-9d9b-5c0a2f563fec_1654x516.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F407d89cc-a925-4b68-9d9b-5c0a2f563fec_1654x516.png)

Structure of the search problem

As a quick disclaimer, we will assume that the basic structure of search includes a query from a user, as well as several documents to which we are trying to match this query; see above. We assume that all queries and documents contain purely text-based data. Throughout the overview, we will also use the terms “document” and “sentence” interchangeable; e.g., we might use “sentence similarity” to refer to finding similar sequences of text, such as a document and a query.

#### Lexical Search

The traditional approach to building a search engine is based upon matching words in a user’s query to words in a document. This approach, called lexical (or sparse) retrieval, relies upon a data structure called an [inverted index](https://www.geeksforgeeks.org/inverted-index/) to perform efficient keyword matching. The inverted index just contains a list of words and maps each word to a list of locations at which it occurs in various documents. Using this data structure, we can efficiently match terms to documents in which they appear and even count the frequency of terms in each document.

**Sparse retrieval.** To understand why this process is called sparse retrieval, we first need to understand how we represent our data. Lexical search algorithms are based upon word frequencies. If a word appears frequently both in the user’s query and a particular document, then this document might be a good match! To represent the space of possible words that can exist in a query or document, we define a fixed-size vocabulary of relevant words; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54b389a2-eab3-4d24-a76d-eb24e566f97a_1182x1140.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54b389a2-eab3-4d24-a76d-eb24e566f97a_1182x1140.png)

Creating a shared vocabulary of words across documents

From here, we can represent a sequence of text (i.e., a query or document) with a vector that contains an entry for each word in our vocabulary. Then, each of these entries can be filled with a number (possibly zero) corresponding to the frequency of that word within the text. Because our vocabulary is large and only a (relatively) small number of words occur in any given document, this vector representation is relatively _sparse_. This method of producing a vector representation, called the [bag of words model](https://en.wikipedia.org/wiki/Bag-of-words_model), forms the basis of sparse retrieval techniques.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecf1f309-7a21-4421-9581-c569ec93a9ef_2174x626.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecf1f309-7a21-4421-9581-c569ec93a9ef_2174x626.png)

Formula for computing BM25 scores between a query and a document

Given a bag of words representation of a query and a set of documents, the primary algorithm used for performing sparse retrieval is the [BM25](https://medium.com/@evertongomede/understanding-the-bm25-ranking-algorithm-19f6d45c6ce) ranking algorithm; see above. BM25 scores are entirely count-based. We score documents by _i)_ counting words that co-occur between the query and the document and _ii)_ normalizing these counts by metrics like inverse document frequency (i.e., assigns higher scores to words that occur less frequently) and relative document length.

**Does this perform well?** Although other sparse retrieval algorithms exist (e.g., [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)), BM25 achieves impressive performance. In fact, BM25 is a hard baseline to beat even for [more complex](https://arxiv.org/abs/1910.10687) sparse retrieval techniques and [modern approaches](https://arxiv.org/abs/2104.06967) that use deep learning; see below. Despite its simplicity, this algorithm performs incredibly well and remains the core of many search engines even today.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfad3e70-0808-40fa-b76c-1797bbc8743c_1996x1278.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfad3e70-0808-40fa-b76c-1797bbc8743c_1996x1278.png)

(from [17])

**Practical implementation.** Beyond performing well, BM25 is supported by major search tools like [Elastic](https://www.elastic.co/) and [RediSearch](https://redis.io/docs/interact/search-and-query/). Using these tools, the implementation of lexical search is both efficient and simple. We just:

- Store our documents within a database.
    
- Define a schema for building the inverted index.
    

From here, the inverted index will be built asynchronously over the documents in our database, and we can perform searches with BM25 via an abstracted (and easy to use) query language. For more details, check out the code tutorial below.

[Lexical Search with RediSearch](https://github.com/RediSearch/redisearch-getting-started)

#### Adding “AI” into a Search Engine

BM25 is a machine learning algorithm, and we can improve its performance by leveraging common data science techniques like [stemming](https://www.geeksforgeeks.org/introduction-to-stemming/), [lemmatization](https://en.wikipedia.org/wiki/Lemmatization), [stop word removal](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/), and more. However, calling BM25 an “AI-powered” search algorithm might be a bit of a stretch. So, we might wonder: _How can we create a smarter search algorithm?_ The short answer is that we can use deep learning to improve both the retrieval and ranking process. There are two types of models in particular that we use for this purpose—_bi-encoders and cross-encoders_—both of which are typically implemented using encoder-only (BERT) [1] models.

**Bi-encoders** form the basis of dense retrieval[1](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-1-140061921) algorithms. At the simplest level, bi-encoders take a sequence of text as input and produce a dense vector as output. However, the vectors produced by bi-encoders are semantically meaningful—_similar sequences of text produce vectors that are nearby in the vector space when processed by the bi-encoder_. As a result, we can match queries to documents by embedding both with a bi-encoder and performing a vector search to find documents with the highest [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)[2](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-2-140061921) relative to the query; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8576c95a-91c4-4beb-b1bf-af36a1557cd7_778x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8576c95a-91c4-4beb-b1bf-af36a1557cd7_778x858.png)

The structure of a bi-encoder

Using algorithms like [hierarchical navigable small word (HNSW)](https://arxiv.org/abs/1603.09320) [6], we can perform approximate nearest neighbor vector searches efficiently. Similar to performing lexical search, we can store document vectors within a database like Elastic or RediSearch and build an HNSW search index. Then, vector search can be performed by _i)_ using the bi-encoder to produce a vector for the user’s query and _ii)_ performing vector search to find the most similar documents; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd19f706e-65a8-4236-a652-d1bd5958e61c_2124x358.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd19f706e-65a8-4236-a652-d1bd5958e61c_2124x358.png)

Vector search pipeline with a bi-encoder

**Cross-encoders** are similar to bi-encoders in that they allow us to score the similarity between two sequences of text. Instead of separately creating a vector for each textual sequence, however, cross-encoders ingest both textual sequences using the same model; see below. The model is trained to predict an accurate similarity score for these textual sequences. Cross-encoders can more accurately predict textual similarity relative to bi-encoders. However, searching for similar documents with a cross-encoder is much more computationally expensive!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5655fd2b-53e9-4347-97e9-7685872e1e48_706x410.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5655fd2b-53e9-4347-97e9-7685872e1e48_706x410.png)

The structure of a cross-encoder

Namely, bi-encoders can be combined with vector search to efficiently discover similar documents, but cross-encoders require each textual pair to be passed through the model to receive a score. Searching for the most similar document to a user’s query with a cross-encoder requires that we iteratively compute the similarity between the query and every document. Given that this would be incredibly expensive, cross-encoders are usually only applied at the ranking stage of search. After we have retrieved a set of relevant candidate documents, we can pass these documents through a cross-encoder for more accurate re-ranking.

#### A Simple Framework for AI-Powered Search

As discussed, search systems have two basic components: retrieval and ranking. To create a basic AI-powered search system, the retrieval process would use both:

- Lexical retrieval with BM25.
    
- Dense retrieval with a bi-encoder.
    

We combine the results of these two retrieval algorithms by taking a weighted sum of each document’s score from BM25 and vector search. The combination of lexical and vector retrieval is typically referred to as _hybrid search_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72ab1d3-a9ee-42ea-ba81-8e03fa5f841e_1720x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72ab1d3-a9ee-42ea-ba81-8e03fa5f841e_1720x446.png)

AI-powered search framework

From retrieval, we obtain a smaller set of candidate documents that can be ranked using a cross-encoder that more accurately sorts the search results based on textual relevance; see above. Such a system leverages deep learning during both retrieval and ranking, forming a true AI-powered search system. This overview is dedicated to understanding how deep learning can be used to build better search algorithms. Primarily, we will focus upon the retrieval component of search—_training bi-encoders for vector search_. However, further reading material for ranking with cross-encoders will also be provided.

## Using BERT for Search

As previously mentioned, most of the commonly-used bi-encoders and cross-encoders are based upon BERT [1]. As such, understanding the encoder-only architecture and self-supervised training strategy of BERT is important. We will now provide an overview of BERT and how it can (or cannot) be used in search.

#### Encoder-Only Architecture

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a511c54-312e-4c7f-8623-0580175d8a38_1644x622.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a511c54-312e-4c7f-8623-0580175d8a38_1644x622.png)

Encoder-only transformer architecture

Although the original transformer architecture contains both an encoder and a decoder, [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) [1] leverages an encoder-only architecture; see above. The encoder-only architecture just contains several repeated layers of bidirectional self-attention and a feed-forward transformation, both followed by a residual connection and [layer normalization](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). Let’s dig into each of these components.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)

Tokenizing a textual sequence

**Crafting the input.** The first step in understanding the encoder-only architecture is understanding how its input is constructed. At the simplest level, the input to an encoder-only transformer is just a sequence of text. However, we use a tokenizer—_usually a [BPE](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) or [SentencePiece](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15) tokenizer_[3](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-3-140061921)—to break this text into a sequence of tokens (i.e., words and sub-words); see above. Importantly, there are also a few “special” tokens that are added to BERT’s input, including:

- `[CLS]`: a token that is always placed at the beginning of the sequence and serves as a representation of the entire sequence.
    
- `[SEP]`: a token that is placed in between multiple sentences that are fed as input to BERT and serves as a separator between the sentences.
    
- `[EOS]`: an “end of sequence” token that is placed at the end of BERT’s input sequence to indicate that the textual sequence is over.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd94c34a-c236-434f-9094-f7eefa06f4b7_1874x724.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd94c34a-c236-434f-9094-f7eefa06f4b7_1874x724.png)

Creating a sequence of token embeddings

Then, we can embed each of these tokens by performing a lookup within a large [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), forming a sequence of token vectors that are fed into the model as input; see above. Prior to being ingested by the model, however, each of these token embeddings must have a [positional embedding](https://vaclavkosar.com/ml/transformer-positional-embeddings-and-encodings) added to it, which allows the model to understand the position of each token in the underlying sequence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21f46fbf-9cca-4b05-9ee4-7f6f91febc24_1500x843.gif)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21f46fbf-9cca-4b05-9ee4-7f6f91febc24_1500x843.gif)

**Bidirectional self-attention.** Understanding the inner workings of the self-attention mechanism is beyond the scope of this post; see [here](https://x.com/cwolferesearch/status/1641932082283700226?s=20) for a more comprehensive breakdown. At a high level, however, the purpose of self-attention is to transform each token’s representation by looking at other tokens within the sequence. In the case of bidirectional[4](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-4-140061921) self-attention, each token’s representation is transformed by considering all other tokens within the sequence, including those both before and after the current token; see above. In this way, each token’s representation can be informed by the entire input sequence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcb9298-46a2-4d2d-af53-f82fa65c51d9_1334x828.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcb9298-46a2-4d2d-af53-f82fa65c51d9_1334x828.png)

Feed-forward transformation from a BERT model

**Feed-forward transformation.** In contrast, the feed-forward transformation within the encoder-only architecture plays somewhat of a different role. Namely, this operation is point-wise, meaning that the same transformation is applied to every token vector within the sequence. The transformation only considers a single token vector and applies a sequence of linear transformations—_usually two linear layers separated by a ReLU activation function_—to the vector, followed by normalization. An example of this transformation is provided above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba59330f-ab59-4c01-a343-c0b3b5d0c943_1988x634.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba59330f-ab59-4c01-a343-c0b3b5d0c943_1988x634.png)

Two major components of an encoder-only transformer block

**Putting everything together.** Bidirectional self-attention and feed-forward transformations each play a distinct and important role within the encoder-only transformer. The self-attention component learns useful patterns by considering the context of other tokens within the sequence, while the feed-forward transformation only considers individual tokens. Together, these operations allow the model accurately model complex patterns across the entire input sequence.

**Creating vector embeddings.** Before moving on, we need to discuss how BERT can be used to create a vector embedding for a sequence of text. Such an approach is leveraged by bi-encoders to craft vectors that can be used for vector search. Each layer of a BERT model takes a sequence of token vectors as input and produces an equal-size sequence of token vectors as output. As such, the output of BERT is a sequence of token vectors, and every intermediate layer within the model produces a sequence of vectors with the same size; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ea6648-d36e-4f42-b745-c7c4dbc06adb_1362x674.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ea6648-d36e-4f42-b745-c7c4dbc06adb_1362x674.png)

To convert these lists of token vectors into a single embedding that represents the full input sequence, we must perform some kind of pooling operation. With BERT, there are three styles of pooling that are commonly used:

- _Approach #1_: Use the final outputted `[CLS]` token representation.
    
- _Approach #2_: Take an average over output token vectors.
    
- _Approach #3_: Take an average (or max) of token vectors across layers.
    

Each of these approaches are displayed within the figure below. In general, the style of pooling that is adopted does not make a massive performance difference. However, taking an average over output token vectors (i.e., approach #2) is by far the most common approach for creating text embeddings with BERT.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24494e01-a211-46d4-a3fb-70cc2e0e4dc3_1486x822.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24494e01-a211-46d4-a3fb-70cc2e0e4dc3_1486x822.png)

Pooling approaches for creating sentence embeddings with BERT

#### **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) [1]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff658aa4f-4d01-41ce-95bd-4d444c93bb6c_909x388.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff658aa4f-4d01-41ce-95bd-4d444c93bb6c_909x388.png)

BERT pretraining and finetuning (from [1])

Now that we understand the encoder-only transformer architecture, BERT is pretty easy to grasp. BERT is just an encoder-only transformer that we pretrain using a few [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) objectives and finetune—_or train the model more (usually with a supervised objective)_—to solve some downstream task; see above. The self-supervised pretraining objectives used by BERT (shown below) include:

- _Cloze_: randomly mask tokens within the input sequence and train the model to predict these masked tokens.
    
- _Next sentence prediction_: given a pair of sentences as input, predict whether these sentence naturally follow each other in a textual corpus or not.
    

Neither of these tasks require human annotation. Rather, the “labels” that we use to pretrain the model are implicitly present within raw textual data, allowing the model to be trained over a massive textual corpus downloaded from the web. In the case of BERT, this corpus is all of [English Wikipedia](https://dumps.wikimedia.org/enwiki/) and [BookCorpus](https://en.wikipedia.org/wiki/BookCorpus)!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70b0456c-9b15-4487-86bd-c466efb2ae7c_1112x1350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70b0456c-9b15-4487-86bd-c466efb2ae7c_1112x1350.png)

Self-supervised pretraining objectives for BERT

**Why is BERT so great?** BERT was one of the first transformer-based language models[5](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-5-140061921) to leverage self-supervised pretraining over large amounts of raw textual data. Such an approach proved to be highly effective, as BERT set a new state-of-the-art performance on all tasks that were considered in [1]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F58bcb60d-4e58-4350-a4df-62b319a230a3_1607x1088.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F58bcb60d-4e58-4350-a4df-62b319a230a3_1607x1088.png)

BERT performance when finetuned on various downstream tasks (from [1])

Put simply, we can finetune a pretrained BERT model to solve a variety of different sentence and token-level classification tasks with incredibly high accuracy. For this reason, BERT revolutionized research in natural language processing, _replacing many domain-specific techniques with a single model that can solve nearly all tasks_! In [1], two different sizes of BERT were proposed:

- **BERT Base:** 12 layers, 768-dimensional hidden representations, 12 attention heads in each self-attention module, and 110M parameters.
    
- **BERT Large:** 24 layers, 1024-dimensional hidden representations, 16 attention heads in each self-attention module, and 340M parameters.
    

Notably, BERT Base is the same size as the original [GPT model](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt), which was one of the primary baselines used for comparison at the time of BERT’s proposal.

**BERT variants.** Due to the massive success of BERT and widespread popularity that it gained within AI research, many variants of this model have been created. For example, RoBERTa [3] is a popular BERT variant that carefully studies BERT’s pretraining process, discovering that a better model can be obtained by pretraining over more data and carefully tuning training hyperparameters[6](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-6-140061921). The main modifications made by RoBERTa to the pretraining procedure are:

- Training the model longer, with more data, using larger batches.
    
- Using longer textual sequences during pretraining.
    
- Removing the next sentence prediction objective.
    
- Dynamically changing the token masking pattern used by Cloze.
    

Going further, ALBERT [19] is a variant of BERT that proposes a parameter reduction technique to make BERT pretraining faster and less memory intensive. The resulting model outperforms BERT on a variety of benchmarks despite having fewer parameters. Additionally, [mBERT](https://cameronrwolfe.substack.com/i/78982044/multilingual-bert-mbert)—_a multilingual version of the original BERT model_—was released (by the same authors) shortly after the proposal of BERT, is jointly pretrained over many languages, and uses a shared [vocabulary and embedding space](https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a) across languages. Put simply, _mBERT is a single BERT model that learns unified representations across a large number of languages_.

> _“We show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models”_ - from [21]

Finally, [XLM-R](https://cameronrwolfe.substack.com/i/78982044/unsupervised-cross-lingual-representation-learning-at-scale) [21] improves upon mBERT by creating a multilingual version of the RoBERTa model. To achieve better performance, this model is pretrained over multilingual data from [Common Crawl](https://commoncrawl.org/), whereas mBERT is trained over a much smaller (i.e., ~100 times smaller to be exact) Wikipedia dataset. Plus, XLM-R improves the tokenization process for multilingual data and pretrains much longer compared to mBERT. However, the BERT variants don’t end here! To find more BERT-based models, check out the related pages on [HuggingFace.](https://huggingface.co/models?sort=downloads&search=BERT)

#### Vector Search with BERT

As explained above, BERT is incredibly useful for solving sentence and token-level classification problems, as well as [semantic textual similarity (STS)](https://www.sbert.net/examples/training/sts/README.html) tasks. More specifically, given two texts as input, we can accurately finetune a BERT model to predict the level of similarity between these two texts; see below. In fact, BERT set new state-of-the-art performance on such problems. However, as we have previously discussed, using BERT as a cross-encoder in this manner is inefficient and can, therefore, only be applied at the ranking stage of search.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa29b10eb-f0be-40e7-b33a-0f3bc7907cef_1620x724.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa29b10eb-f0be-40e7-b33a-0f3bc7907cef_1620x724.png)

Finetuning BERT on an STS task (from [1])

To more efficiently retrieve relevant documents with BERT, we must use BERT as a bi-encoder by producing textual embeddings for all documents and indexing them in a vector database. But, we learn in [2] that textual embeddings produced by BERT are not semantically meaningful—_BERT struggles to function as a bi-encoder and even performs poorly when used for clustering_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7a96ba3-8f8c-4707-ac22-5f0c7aaabf32_1860x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7a96ba3-8f8c-4707-ac22-5f0c7aaabf32_1860x804.png)

(from [2])

At this point, the savvy reader might be wondering: _How could BERT embeddings possibly not work well for semantic search?_ BERT models are widely used to power a variety of high-profile search use cases—_including [Google Search](https://blog.google/products/search/search-language-understanding-bert/)_! In this overview, we will aim to provide an answer to this question by introducing sBERT, a BERT variant that is optimized to produce more semantically meaningful text embeddings that can be use to build performant AI-powered search applications.

> _“BERT has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead.”_ - from [2]

## Better Bi-Encoders for Vector Search

Going beyond the basic BERT model, we will now explore several BERT variants that are optimized for use as bi-encoders. At a high level, these are just BERT models that are finetuned to produce more semantically meaningful text embeddings. In turn, these models are more compatible with vector search, making them a useful practical tool for any AI-powered search algorithm.

#### **[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)**[Extensions of sBERT](https://arxiv.org/abs/1908.10084) [2]

Given that vanilla BERT models work poorly for semantic similarity and clustering tasks, we might wonder: _How can we adapt these models to produce more useful embeddings for semantic search?_ We see in [2] with the proposal of sentence BERT (sBERT) that adapting BERT models in this way is not very difficult. We just need to finetune these models using a siamese or triplet network structure to derive semantically meaningful text or document representations!

**What are siamese and triplet networks?** The term siamese or triplet network might sound complex, but the concept is actually pretty simple! With sBERT, we use the same BERT [1] (or RoBERTa [3]) encoder-only transformer architecture with which we are already familiar. To make this a siamese network, however, we just pass two different inputs through the same model in parallel. Then, we can apply a loss to the two outputs that are generated; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd6cea76-089e-45c7-99cb-2f037d9264e1_838x430.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd6cea76-089e-45c7-99cb-2f037d9264e1_838x430.png)

Siamese network structure

For example, using this approach, we can train a BERT model to classify whether two sentences are similar or not. To do this, we would obtain a dataset of similar (and not similar) sentence pairs and train the siamese network to take a pair of sentences as input and classify whether they are similar or not.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25e0d82-66dd-439c-86ea-5d8fa4acdd3e_1030x426.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd25e0d82-66dd-439c-86ea-5d8fa4acdd3e_1030x426.png)

Triplet network structure

The idea behind triplet networks is nearly identical to siamese networks, but we pass three different inputs (instead of two) through the same network in parallel. This way, we have the ability to train the model using a triplet loss that considers three different model outputs; see above. For example, if we have access to a dataset with triplets of sentences, including an anchor sentence, a sentence that is similar to the anchor sentence, and a sentence that is different from the anchor sentence, we could train the model to simultaneously make the output of the similar sentences similar and the output of the dissimilar sentences different.

**Model architecture for sBERT.** In [2], sBERT models share the architecture of a normal BERT [1] or RoBERTa [3] model. To make text embeddings produced by these models more semantically meaningful, sBERT finetunes pretrained[7](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-7-140061921) BERT and RoBERTa models using three types of siamese or triplet network structures. Standard approaches are used to obtain sentence embeddings from BERT or RoBERTa (i.e., we take the mean of all output vectors by default).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef893302-d606-4443-b7fb-1f9c9034860c_752x778.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef893302-d606-4443-b7fb-1f9c9034860c_752x778.png)

(from [2])

The first finetuning setup for sBERT uses a **classification objective**; see above. Two sentences are taken as input and passed through a BERT model to yield embeddings `u` and `v`. We then concatenate these embeddings together—_along with their element-wise difference_—and pass this concatenated representation through a linear transformation, producing a vector of size `k`. Finally, we apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) to the output of the linear layer and train the model to perform `k`-way classification. For example, if we want to finetune sBERT using pairs of sentences that are either similar or dissimilar, we would have `k=2`. Because we only apply a simple, linear classification layer on top of the BERT embeddings, we force the model to craft semantically meaningful sentence embeddings to solve this task.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9aa4779-2356-40c8-9442-3e4a9c203593_758x710.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9aa4779-2356-40c8-9442-3e4a9c203593_758x710.png)

(from [2])

Next, authors in [2] explore a **regression objective** for finetuning sBERT; see above. This setup looks almost identical to the classification objective. Instead of concatenating vectors `u` and `v` together and applying a linear transformation, however, we just _i)_ compute the cosine similarity of `u` and `v` (i.e., via a [dot product](https://en.wikipedia.org/wiki/Dot_product) if `u` and `v` are [unit vectors](https://en.wikipedia.org/wiki/Unit_vector)) and _ii)_ perform regression to match this cosine similarity to a target similarity value. In this way, we can finetune sBERT over pairs of sentences with similarity scores between -1 (not similar) and 1 (similar).

The final finetuning setup explored in [2] uses a **triplet objective**. Each training example contains three sentences: an anchor sentence, a similar sentence, and a dissimilar sentence. We pass each of these sentences through a BERT model to obtain an embedding. Then, we apply a triplet loss (shown below) over these embeddings to move the embeddings of the anchor and similar sentences closer together and vice versa. Similarity is measured using euclidean distance, and we add a margin (Ɛ=1) to the loss to ensure that similar and dissimilar sentences have a sufficiently large difference in similarity to the anchor sentence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04064d83-2c88-4d7d-b299-ecff4278d0fa_878x366.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04064d83-2c88-4d7d-b299-ecff4278d0fa_878x366.png)

Triplet loss formulation for sBERT (from [2])

**Finetuning sBERT.** The three different networks structures overviewed above give us a brief glimpse at the different strategies that can be used to produce more semantically meaningful embeddings with BERT. But, _there are many possible finetuning setups for sBERT beyond these three_! As a practitioner, the exact setup derived will use similar principles—_usually a siamese/triplet BERT network with an added classification/regression head_—but will be adapted to the exact style of sentence or document pairs that are available. For example, we can have humans annotate a 1-5 similarity score for sentence pairs, collect descriptions of products that are commonly purchased together on an e-commerce store, and much more!

> _“We train SBERT on the combination of the SNLI and the Multi-Genre NLI dataset… with a 3-way softmax classifier objective function for one epoch.”_ - from [2]

The sBERT model that is proposed and analyzed in [2] is a specific instantiation of these ideas. In particular, the model is trained over two different datasets using a classification objective with three target classes (i.e., `k=3`):

- _[SNLI](https://nlp.stanford.edu/projects/snli/)_ [4]: 570K sentence pairs annotated with a label of contradiction, entailment, or neural[8](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-8-140061921).
    
- _[MultiNLI](https://cims.nyu.edu/~sbowman/multinli/)_ [5]: 430K sentences pairs (uses the same entailment labels as SNLI) that cover a range of genres of spoken and written text.
    

This finetuning procedure is relatively quick and inexpensive because we begin with a pretrained BERT or RoBERTa model. As we will see, this simple finetuning procedure drastically improves the quality of BERT embeddings in the context of vector search, clustering, and semantic similarity tasks.

**How does it perform?** In [2], sBERT is evaluated on several different semantic textual similarity (STS) tasks in comparison to sentence embedding methods like [InferSent](https://github.com/facebookresearch/InferSent) [7] and [Universal Sentence Encoder](https://huggingface.co/Dimitre/universal-sentence-encoder) [8]. Here, all techniques are compared using an approach that measures cosine similarity[9](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-9-140061921) between generated embeddings, rather than the (expensive) pairwise approach used by most cross-encoders. In other words, _sBERT is evaluated as a bi-encoder_. First, sBERT is evaluated on a variety of STS tasks in an unsupervised fashion (i.e., using no specific training data for any of the tasks); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3973c233-bc1d-404d-9ca1-a20d66fef3e3_2180x478.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3973c233-bc1d-404d-9ca1-a20d66fef3e3_2180x478.png)

(from [2])

Here, we see that directly using embeddings outputted from a pretrained BERT model yields poor performance. In fact, _the results achieved with pretrained BERT are even worse than those achieved via average GloVE [9] embeddings_! In contrast, we can outperform all baseline techniques—using either BERT or RoBERTa—via the siamese network structure and finetuning approach proposed by sBERT.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f62b6c7-a3a5-46af-9f3c-9b20edfcd421_2200x1104.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f62b6c7-a3a5-46af-9f3c-9b20edfcd421_2200x1104.png)

(from [2])

When sBERT models are further finetuned on the downstream task used for evaluation, the results observed follow a similar trend; see above. Here, we evaluate sBERT on the [STS benchmark](https://huggingface.co/datasets/stsb_multi_mt) [10]—a popular dataset for evaluating semantic similarity systems. The best results are achieved by performing both:

1. The normal sBERT finetuning procedure on NLI.
    
2. Further finetuning on the downstream task.
    

On these tasks, sBERT achieves comparable performance to a cross-encoder-style BERT model (i.e., the state-of-the-art approach for solving STS tasks), while still operating purely as a bi-encoder.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7690972-1079-4b77-904b-09748f46d226_2408x1046.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7690972-1079-4b77-904b-09748f46d226_2408x1046.png)

(from [2])

Going further, sBERT is evaluated on the [Argument Facet Similarity](https://nlds.soe.ucsc.edu/node/44) [11] dataset that contains pairs of sentences across different topics with varying levels of similarity, as well as on Wikipedia by measuring the similarity of sentences from the same or different sections of various articles; see above. Again, sBERT achieves results that are comparable to a cross-encoder-style BERT model on the argument facet similarity dataset, while baseline techniques perform quite poorly. Interestingly, however, sBERT struggles in the cross-topic domain, where models are trained on a few topics and evaluated on a held-out topic.

> _“BERT and RoBERTa have set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead.”_ - from [2]

**Main takeaway.** BERT models are great for a variety of tasks, but BERT’s textual embeddings are not semantically meaningful by default. The finetuning procedure proposed in [2] improves the semantic meaningfulness of BERT embeddings. As such, sBERT is appropriate for a broader range of tasks, including semantic similarity tasks like clustering and semantic (vector) search. Using optimized index structures, we can find similar documents with sBERT in milliseconds, whereas performing a similarity search with vanilla BERT (using a cross-encoder setup) could take tens or hundreds of hours[10](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-10-140061921)!

#### Useful Extensions of sBERT

From work in [2], we learn how to make BERT models more usable for dense retrieval. Given that producing semantically meaningful embeddings is so useful and important[11](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-11-140061921), this technique has been extended by several follow-up publications. Although many such extensions to sBERT exist, we will do our best to cover the most important of them within this section.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe734d8fc-0a63-40c1-9a6f-c2ef027e4723_1840x676.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe734d8fc-0a63-40c1-9a6f-c2ef027e4723_1840x676.png)

(from [12])

**Making sBERT multilingual.** One major limitation of work in [2] is the simple fact that the proposed model is English-only. With this in mind, we might wonder: _Could we (cheaply) expand a monolingual embedding model to understand multiple languages?_ Authors in [12] study this problem exactly, finding that such models can be created by leveraging the idea that translated sentences should have the same embedding as the original sentence. In particular, the original (monolingual) model is used to generate embeddings of sentences in the source language. Then, we translate these sentences and finetune the model over these translated sentences to mimic the original model’s embeddings. This approach is coined as multilingual [knowledge distillation](https://cameronrwolfe.substack.com/i/114077195/knowledge-distillation); see above.

> _“The student model learns a multilingual embedding space with two important properties: vector spaces are aligned across languages and vector space properties in the original language are adopted and transferred to other languages.”_ - from [12]

In [12], sBERT is used as the teacher, while the student model is based upon [XLM-R](https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/) [13]—_a BERT-style model that is pretrained on data from over 100 languages_. In addition to being both simple and low cost, the knowledge distillation approach proposed in [12] has several benefits:

- _Data efficiency_: embedding models can be expanded to understand more languages with (relatively) few data samples.
    
- _Hardware requirements_: the hardware required to finetune an embedding model via multilingual knowledge distillation is more reasonable compared to training a multilingual embedding model from scratch.
    
- _Rich embedding space_: the properties of the monolingual model’s embedding space are transferred to and aligned with the multilingual embedding model.
    

**Augmenting sBERT’s data.** Cross-encoders and bi-encoders have pros and cons. Cross-encoders tend to perform well, but they are too costly for practical applications (unless we only apply them in the ranking stage of search). In contrast, bi-encoders are more practical, but they require much more training data and finetuning to perform competitively; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2c83c35-051e-4190-80bb-c7bf41bb0c1a_1804x954.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2c83c35-051e-4190-80bb-c7bf41bb0c1a_1804x954.png)

(from [14])

In [14], authors propose a solution to this problem that generates a much larger training dataset for bi-encoders—_such as sBERT_—by using a BERT cross-encoder to label pairs of sentences that can then be used as extra data for the bi-encoder; see below. Such an approach can significantly improve the performance of sBERT, but the exact approach used to sample data for labeling with the cross-encoder is crucial. Namely, we cannot just randomly sample sentence pairs. Rather, we should use a sampling approach (e.g., BM25 sampling and/or semantic search or simply removing negative examples) to ensure the ratio of similar and dissimilar sentences matches the original training dataset.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1cf752e-45fa-4990-8877-972a57cb59c0_968x1176.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1cf752e-45fa-4990-8877-972a57cb59c0_968x1176.png)

(from [14])

**Pseudo-labeling for domain adaptation.** Continuing the trend of using cross-encoders to label data, authors in [15] use a similar approach to improve the ability of bi-encoders to handle domain shifts (i.e., usage on data that is different from the training dataset). The proposed approach, called Generative Pseudo Labeling (GPL), combines a query generator with a cross-encoder that is used to generate labels. See below for an illustration of this technique.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d562014-82b9-4feb-bf29-244139e3337f_1232x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d562014-82b9-4feb-bf29-244139e3337f_1232x446.png)

(from [15])

For a given target domain, authors use [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [18] to generate queries for this domain given a passage of text (i.e., the positive example or desired match) as an input. From here, an existing retrieval system—_either using lexical search with BM25 or vector search with a pretrained model_—is used to retrieve a fixed number of negative examples for each query. Then, we form triplets that include:

1. The query.
    
2. The correct passage.
    
3. A negative/incorrect passage.
    

For each triplet, the cross-encoder is used to predict a margin that can be used as a training signal. Using this approach, we can adapt a bi-encoder to a new domain without the need for labeled data. All we need is a valid set of passages within this domain, as well as a pretrained T5 and cross-encoder model.

**Large-scale semantic search.** In [16], authors study the performance of vector search with bi-encoders like sBERT with respect to the size of the underlying search index (i.e., the number of vectors over which we have to search). Interestingly, as shown in the table below, the quality of dense retrieval (i.e., vector search with bi-encoders) deteriorates as the size of the index increases. Additionally, higher-dimensional embeddings tend to perform slightly better.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe96120c3-c36c-426c-9709-aeff2e40d417_1274x836.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe96120c3-c36c-426c-9709-aeff2e40d417_1274x836.png)

(from [14])

Although the mathematical proof of this trend and the intricacies of vector search are beyond the scope of this post, we learn from analysis in [16] that i) vector search works best with a smaller, clean search index, but the overall quality of vector search—even within larger search indices—can be improved by introduced added “hard” negative examples to a bi-encoder’s training process.

**Benchmarking search systems.** Search systems are, without a doubt, one of the most popular and widely-studied applications of AI. As such, a massive number of search techniques and methodologies have been prosed over the last few decades. In fact, there is so much search-related information and research available that it can be difficult to determine which techniques perform best and how they compare to each other. Authors in [18] solve this problem by proposing a new, comprehensive information retrieval benchmark—_called Benchmarking-IR (BEIR)_—comprised of the tasks shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F214714e9-daef-488e-9090-130045cdf87e_2128x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F214714e9-daef-488e-9090-130045cdf87e_2128x782.png)

(from [18])

Additionally, authors provide extensive analysis of a variety of different search techniques, arriving at some practical takeaways:

- BM25 is a robust baseline that is quite difficult to beat.
    
- Re-ranking models (i.e., cross-encoders) yield the best performance, though their computational cost is high.
    
- Bi-encoders are more efficient, but they can perform poorly in certain domains (sometimes even worse than BM25!).
    
- Search systems that perform well in one domain might generalize quite poorly to other domains.
    

Put simply, we learn in [18] that there is a tradeoff between the efficiency and performance of search systems. Typically, the best approach will be achieved via a combination of lexical search with BM25 and vector search with a bi-encoder, as well as re-ranking the final few search results with a cross-encoder. This paper is a great practical resource for anyone that wishes to understand the different components of search systems and their impact on top-level performance.

#### SentenceTransformers: Semantic Search in Practice

[

![Logo](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38370192-17c6-4924-9bf4-10d0693dffc3_1024x512.png "Logo")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38370192-17c6-4924-9bf4-10d0693dffc3_1024x512.png)

(from Sentence Transformer Library’s documentation page)

Now that we have learned about sBERT and its extensions, we need to learn how to use these ideas in practice! All papers we have seen so far are implemented in a Python library, called [Sentence Transformers](https://www.sbert.net/), that is built on top of the PyTorch and HuggingFace. This package openly provides tons of state-of-the-art models—_including both [bi-encoders](https://www.sbert.net/docs/pretrained_models.html) and [cross-encoders](https://www.sbert.net/docs/pretrained_cross-encoders.html)_—and makes it easy to use these models and efficiently finetune them on your own data. Because this package is based upon sBERT, SentenceTransformer embeddings are semantically meaningful—_similar sentences yield embeddings that are close in vector space_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5e20488-0445-4366-8130-a6a1c190a366_1010x780.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5e20488-0445-4366-8130-a6a1c190a366_1010x780.png)

Generating sentence embeddings with a bi-encoder from Sentence Transformers

**Show me the code.** An example of embedding sentences with a bi-encoder using the SentenceTransformers library is shown above. Although BERT models natively produce a sequence of output embeddings for each sentence, Sentence Transformer models automatically perform mean pooling over these embeddings to produce a single output embedding. To perform semantic search with these embeddings, we simply need to compute their respective cosine distances.

> _“BERT outputs for each token in our input text an embedding. In order to create a fixed-sized sentence embedding out of this, the output embeddings for all tokens are averaged to yield a fixed-sized vector.” -_ from Sentence Transformer [docs](https://www.sbert.net/docs/quickstart.html)

Going further, the Sentence Transformers library also implements a variety of cross-encoder models. As described previously, cross-encoders take a pair of textual sequences (e.g., a search query and a passage or two sentences) as input and output a similarity score in the range `[0, 1]`; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e0a5184-86ce-4c4d-9f3c-4d486b9f6df5_892x742.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e0a5184-86ce-4c4d-9f3c-4d486b9f6df5_892x742.png)

Scoring pairs of text with a Sentence Transformers Cross-encoder

**More details.** Beyond what we’ve learned so far, Sentence Transformers also _i)_ supports over 100 different languages via the multilingual knowledge distillation approach we learned above before and _ii)_ has an added module for performing vector search with [both text and images](https://www.sbert.net/examples/applications/image-search/README.html); see below.

[

![ImageSearch](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617e7c24-af37-4ebe-99fa-3de844a87806_612x355.png "ImageSearch")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617e7c24-af37-4ebe-99fa-3de844a87806_612x355.png)

Joint embedding space for text and images (from Sentence Transformers docs)

The documentation for Sentence Transformers has a variety of useful guides and tutorials for supporting different styles of semantic search applications. See below for a quick list:

- Retrieval and Re-Ranking[12](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-12-140061921) [[link](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)]
    
- Semantic Search [[link](https://www.sbert.net/examples/applications/semantic-search/README.html)]
    
- Creating a Multilingual Embedding Model [[link](https://www.sbert.net/examples/training/multilingual/README.html)]
    
- Augmented sBERT [[link](https://www.sbert.net/examples/training/data_augmentation/README.html)]
    

## Final Remarks

Within this overview, we learned about the basic components of AI-powered search systems, including lexical search, bi-encoders, and cross-encoders. Then, we dove deeper into bi-encoders and how we can finetune BERT-style language models to provide semantically meaningful text embeddings that work especially well for dense retrieval. The major takeaways from this overview are as follows:

- Search algorithms proceed in two primary phases—_retrieval and ranking_.
    
- Lexical retrieval with BM25 uses keyword matching to produce high-quality search results and can be efficiently implemented with an inverted index.
    
- Going beyond BM25, we can improve retrieval and ranking quality with bi-encoders and cross-encoders, respectively.
    
- Both bi-encoder and cross-encoder models use a BERT-style architecture.
    
- BERT naturally performs well when finetuned as a cross-encoder, but we should only use such a model in the final ranking stage of search.
    
- To use BERT as a bi-encoder, we must finetune the model—_as is done by sBERT_—to yield more semantically meaningful embeddings.
    

#### Further Reading

We covered a lot of ground within this overview, but this information barely scratches the surface of techniques that have been proposed for search and information retrieval. Gaining a deep understanding of search algorithms requires extensive reading and practical experience—_there is a limitless amount of information online about building and improving search_. For example, cross-encoders are not the only approach that we can use for ranking. In fact, there is an entire sub-domain of research focused on the problem of [learning to rank](https://en.wikipedia.org/wiki/Learning_to_rank) for search. Notable examples include:

- RankNet [[link](https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf)]
    
- LambdaRank [[link](https://papers.nips.cc/paper_files/paper/2006/hash/af44c4c56f385c43f2529f9b1b018f6a-Abstract.html)]
    
- LambdaMART [[link](https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf)]
    
- SoftRank [[link](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankWsdm08Submitted.pdf)]
    

Additionally, this post focused upon bi-encoders instead of diving deeper into cross-encoders, though a lot of useful research on the topic of cross-encoders has been published! For example, researchers have studied the impact of using [multiple stages](https://arxiv.org/abs/2311.07994) of ranking models, using [generative large language models for ranking](https://arxiv.org/abs/2305.02156), and even [considering more data](https://arxiv.org/abs/2306.10979) when ranking via a cross-encoder.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59eb3b3b-ab4e-4e66-aebb-858a8eff2d7a_1328x640.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59eb3b3b-ab4e-4e66-aebb-858a8eff2d7a_1328x640.png)

(from [22])

Going further, cross-encoders can be expensive to use in practice, even when they are just used at the ranking stage. To mitigate this problem, researchers have proposed popular techniques like ColBERT [22] that augment a normal BERT bi-encoder with an added, late interaction stage that can compute more fine-grained textual similarities; see above. This late interaction step is more computationally efficient than a cross-encoder, allowing ColBERT to find a middle ground between bi-encoder and cross-encoders. Namely, we can use ColBERT as a bi-encoder for vector search, then perform ranking via a simplified module!

> _“While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score.”_ - from [22]

This overview is not meant to be an exhaustive overview of search techniques, but it should provide a good base of information for those interested in learning more. Now that we understand the basic components of AI-powered search, we are better positioned to stay up-to-date with this ever-evolving research problem.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[2] Reimers, Nils, and Iryna Gurevych. "Sentence-bert: Sentence embeddings using siamese bert-networks." _arXiv preprint arXiv:1908.10084_ (2019).

[3] Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." _arXiv preprint arXiv:1907.11692_ (2019).

[4] Bowman, Samuel R., et al. "A large annotated corpus for learning natural language inference." _arXiv preprint arXiv:1508.05326_ (2015).

[5] Williams, Adina, Nikita Nangia, and Samuel R. Bowman. "A broad-coverage challenge corpus for sentence understanding through inference." _arXiv preprint arXiv:1704.05426_ (2017).

[6] Malkov, Yu A., and Dmitry A. Yashunin. "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs." _IEEE transactions on pattern analysis and machine intelligence_ 42.4 (2018): 824-836.

[7] Conneau, Alexis, et al. "Supervised learning of universal sentence representations from natural language inference data." _arXiv preprint arXiv:1705.02364_ (2017).

[8] Cer, Daniel, et al. "Universal sentence encoder." _arXiv preprint arXiv:1803.11175_ (2018).

[9] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_. 2014.

[10] Cer, Daniel, et al. "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation." _arXiv preprint arXiv:1708.00055_ (2017).

[11] Misra , Amita et al. “[Measuring the Similarity of Sentential Arguments in Dialogue](http://www.aclweb.org/anthology/W/W16/W16-36.pdf#page=294)". In _The 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), Los Angeles, California, USA, 2016._

[12] Reimers, Nils, and Iryna Gurevych. "Making monolingual sentence embeddings multilingual using knowledge distillation." _arXiv preprint arXiv:2004.09813_ (2020).

[13] Conneau, Alexis, et al. "Unsupervised cross-lingual representation learning at scale." _arXiv preprint arXiv:1911.02116_ (2019).

[14] Thakur, Nandan, et al. "Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks." _arXiv preprint arXiv:2010.08240_ (2020).

[15] Wang, Kexin, et al. "Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval." _arXiv preprint arXiv:2112.07577_ (2021).

[16] Reimers, Nils, and Iryna Gurevych. "The curse of dense low-dimensional information retrieval for large index sizes." _arXiv preprint arXiv:2012.14210_ (2020).

[17] Thakur, Nandan, et al. "Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models." _arXiv preprint arXiv:2104.08663_ (2021).

[18] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[19] Lan, Zhenzhong, et al. "Albert: A lite bert for self-supervised learning of language representations." _arXiv preprint arXiv:1909.11942_ (2019).

[20] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[21] Conneau, Alexis, et al. "Unsupervised cross-lingual representation learning at scale." _arXiv preprint arXiv:1911.02116_ (2019).

[22] Khattab, Omar, and Matei Zaharia. "Colbert: Efficient and effective passage search via contextualized late interaction over bert." _Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval_. 2020.

[1](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-1-140061921)

Dense retrieval is also commonly referred to as semantic search, as vector embeddings tend to capture the semantic properties of the text from which they are created.

[2](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-2-140061921)

Cosine similarity is not the only metric that can be used for vector search. We can also use something like [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) or [Manhattan](https://simple.wikipedia.org/wiki/Manhattan_distance) distance.

[3](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-3-140061921)

See [here](https://huggingface.co/docs/transformers/tokenizer_summary) for a pretty good overview of some different tokenization techniques that are commonly used for language models.

[4](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-4-140061921)

The alternative is masked (or unidirectional/causal) self-attention, which looks only at preceding tokens instead of all tokens within the sequence. Masked self-attention is used by generative large language models (e.g., ChatGPT).

[5](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-5-140061921)

Notably, [ULMFit](https://arxiv.org/abs/1801.06146) pioneered a pretraining approach that was quite similar but used RNN-based language models instead of transformers.

[6](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-6-140061921)

The term “hyperparameters” refers to settings that are set/tuned by the practitioner that is training the model, such as the learning rate or the ratio of tokens that are masked out within the Cloze objective.

[7](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-7-140061921)

Here, we choose to start with the pretrained versions of these models and perform finetuning rather than training sBERT models from scratch because it saves us a significant amount of compute. _sBERT can be finetuned in <20 minutes_, but training such a network from scratch would require much more time and data.

[8](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-8-140061921)

This task of classifying sentence pairs as contradictory, neutral, or entailment is known as the [textual entailment](https://en.wikipedia.org/wiki/Textual_entailment) task—_a commonly-used semantic text similarity (STS) task that is widely-studied in research papers_.

[9](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-9-140061921)

Other distance metrics, such as [Euclidean and Manhattan distance](https://medium.com/analytics-vidhya/euclidean-and-manhattan-distance-metrics-in-machine-learning-a5942a8c9f2f), were also tested and found to yield similar results.

[10](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-10-140061921)

The exact time is heavily dependent upon hardware, dataset size, and the exact model being used. But, the basic takeaway is that performing a vector search over an [HNSW](https://arxiv.org/abs/1603.09320) index is way faster that using a cross-encoder to exhaustively compute document similarities across an entire dataset.

[11](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-11-140061921)

There are so many different tasks for which this could be used, including search engines, product recommendations, and retrieval augmented generation!

[12](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search#footnote-anchor-12-140061921)

These are the basic components of most semantic search pipelines! All we are missing is a hybrid retrieval algorithm that uses both dense retrieval (i.e., vector search) and sparse/lexical retrieval. This tutorial uses purely dense retrieval.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Juan Terven's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfcf763f-0d28-407c-ab06-934e8bcc37be_1024x1024.png)



](https://substack.com/profile/25323606-juan-terven)

[

![Baptiste's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5ddd68e-5ba6-409b-866d-d137b3922d68_144x144.png)



](https://substack.com/profile/134844029-baptiste)

[

![Tan YK's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8198d6bc-ed7d-40d2-8d38-ea2b67ccccbd_144x144.png)



](https://substack.com/profile/25324784-tan-yk)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

[

![Engineering World Company's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0da98f7e-4f29-4fd1-9f83-ab601e44a909_500x500.jpeg)



](https://substack.com/profile/96129407-engineering-world-company)

66 Likes∙

[4 Restacks](https://substack.com/note/p-140061921/restacks?utm_source=substack&utm_content=facepile-restacks)

66

- 

[

1

](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![ABINASH KUMAR MISHRA's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dafc480-e080-4dc2-bce8-bc6999e4fc1d_1200x1200.jpeg)



](https://substack.com/profile/85914079-abinash-kumar-mishra?utm_source=comment)

[ABINASH KUMAR MISHRA](https://substack.com/profile/85914079-abinash-kumar-mishra?utm_source=substack-feed-item)

[ABINASH KUMAR MISHRA](https://hustlercoder.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[3月11日](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search/comment/99519725 "2025年3月11日 07:19")

🌟 Let’s collaborate and crush goals together! 🌟 Subscribe to align your vision with actionable insights—spark success, grow faster, and thrive mutually. Join now! 🚀

Like

Reply

Share

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Sleeper Agents, LLM Safety, Finetuning vs. RAG, Synthetic Data, and More

### Notable advancements and topics in LLM research from January of 2024...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jan 22, 2024

41

- 

[

9

](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comments)

3

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/). If you like the newsletter, feel free to subscribe below, [get in touch](https://cameronrwolfe.me/), or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/).

Subscribe

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8f7aa94-7d2d-442b-ae7f-45ba578196d0_2402x1350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8f7aa94-7d2d-442b-ae7f-45ba578196d0_2402x1350.png)

(from [1, 2, 7, 8, 9])

Recent research on the topic of large language models (LLMs) has focused heavily upon important practical topics like LLM safety, synthetic training data, multi-modal architectures, and injecting knowledge into pretrained LLMs. Within this overview, we will look at a variety of recent publications spanning several LLM-related topics. By studying this set of recent papers, we will form a (somewhat) comprehensive view of topics being considered in current AI research. Furthermore, several practical takeaways can be gleaned from this work:

- Synthetic training data for LLMs is surprisingly effective and becoming more widely-used by the day.
    
- Teaching a pretrained LLM new knowledge during finetuning is hard, but retrieval augmented generation (RAG) does this very well.
    
- Aligning LLMs to ensure safe deployments is incredibly difficult.
    
- LLMs are inherently capable of supporting longer context lengths.
    

The themes outlined above will become more clear as we take a look at each paper within this overview. Although the volume of research being conducted on LLMs is staggering, we will see here that the high-level takeaways from these papers often overlap. As such, this massive amount of literature can be largely distilled into a smaller set of understandable and practical lessons.

---

#### **[DocLLM: A layout-aware generative language model for multimodal document understanding](https://arxiv.org/abs/2401.00908) [1]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dee7772-9473-4990-857a-a56648bb6d1e_1282x466.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dee7772-9473-4990-857a-a56648bb6d1e_1282x466.png)

(from [1])

A majority of proprietary information at most companies is stored within internal documents. These documents—_forms, invoices, receipts, reports, and more_—are both plentiful and full of information, making them a (potentially) massive source of training data for LLMs. However, this type of data must be processed differently than raw text, as it contains both textual and spatial information. In other words, the layout and font size of textual data play a role in comprehending a document.

> _“These visually rich documents feature complex layouts, bespoke type-setting, and often exhibit variations in templates, formats and quality”_ - from [1]

The best approach for representing supplemental spatial information of text in a document is unclear, but authors in [1] aim to find a workable technique. Their proposal, called DocLLM (shown above), is a lightweight extension of existing decoder-only LLM architectures that enables visual reasoning over documents.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79eb345a-ba94-4a69-b810-a0296df8c3e7_1280x720.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79eb345a-ba94-4a69-b810-a0296df8c3e7_1280x720.png)

(from [1])

**DocLLM architecture.** There is a recent trend in research on generative LLMs towards [multi-modal model architectures](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction), meaning that the model can accept inputs from several different modalities (e.g., images and text). The DocLLM architecture (depicted above) accepts inputs from two different modalities:

1. Text
    
2. Spatial Coordinates
    

In particular, the model is trained such that it can consider not only the text in a document, but also the location and size of this text. Prior work processes such data by adding a [vision encoder](https://cameronrwolfe.substack.com/p/vision-transformers) to the LLM that can _i)_ ingest an image as input and _ii)_ produce a fixed-size set of visual tokens to be considered by the LLM. Alternatively, DocLLM forgoes this image encoder, choosing instead to use a standard, [decoder-only transformer architecture](https://x.com/cwolferesearch/status/1640446111348555776?s=20) that directly ingests bounding box coordinates of text within a document. Such an approach works well, reduces overall model parameters, and improves processing time.

**How does this work?** The first step in applying DocLLM is to pass a document through an [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition) system. From this, we obtain the textual tokens present within the document, as well as bounding box coordinates that capture the position and size of each token.

> _“The spatial layout information is incorporated through bounding box coordinates of the text tokens obtained using optical character recognition (OCR), and does not rely on any vision encoder component.”_ - from [1]

Instead of adding or concatenating spatial information to textual token embeddings, spatial and textual information are processed as standalone modalities within DocLLM. The model’s input is a sequence of textual tokens, each having associated bounding box information. All textual tokens and bounding boxes are converted into corresponding vector representations, forming separate sequences of textual and spatial vectors. To consider both of these sequences within the normal attention mechanism, authors in [1] propose a modified form of attention that performs cross-alignment between the spatial and textual data by computing four different types of attention scores:

- Text-to-text
    
- Text-to-spatial
    
- Spatial-to-text
    
- Spatial-to-spatial
    

Each of these cross-modal relationships are modeled with a separate set of weight matrices, and their results are aggregated via a weighted sum that yields the final attention scores. Compared to using a vision encoder, this approach performs better, uses fewer added parameters, and is more compute efficient.

**Pretraining the model.** As opposed to the standard [next token prediction objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) that is used for pretraining most generative LLMs, DocLLM uses an infilling objective that predicts text based on both preceding and succeeding tokens. To ensure cohesiveness of the pretraining data, documents are divided into distinct blocks of text that are considered by DocLLM.

> _“We follow an autoregressive block infilling objective, where text blocks are randomly masked, and the masked blocks are shuffled and reconstructed in a sequential left-to-right fashion.”_ - from [1]

Given that data within a document can be laid out in a disjoint or heterogenous manner, authors in [1] claim that a simple next token prediction objective is too restrictive. In contrast, infilling objectives can generalize to irregular document layouts by simply dividing the document into coherent blocks of text, masking certain blocks, shuffling the blocks, and predicting masked blocks in a left-to-right manner. After pretraining, DocLLM is also [instruction tuned](https://blog.research.google/2021/10/introducing-flan-more-generalizable.html) over four different document-focused tasks, as shown in the table below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac0b48de-6927-4a82-af12-19bd7083c677_1278x374.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac0b48de-6927-4a82-af12-19bd7083c677_1278x374.png)

**How does it perform?** Several DocLLM models are trained using different backbones (i.e., [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) and [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)) and sizes (i.e., 1 billion and 7 billion parameters). Put simply, DocLLM models are found to excel on tasks that require document-based problem solving; see below. Compared to both standard (text-only) and multi-modal LLMs, we see a significant boost in performance from using DocLLM models. Although we can achieve impressive performance on simpler tasks (e.g., visual question answering) by combining powerful LLMs like GPT-4 with a simple OCR module, DocLLM significantly outperforms this approach on tasks that require reasoning over more complex documents.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb83bf45-d26a-45b1-8b89-fd3f641e5ebf_1294x682.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb83bf45-d26a-45b1-8b89-fd3f641e5ebf_1294x682.png)

(from [1])

---

#### **[LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325) [2]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F373c252e-19d8-40a0-840f-9c5cefe7aaf8_1982x864.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F373c252e-19d8-40a0-840f-9c5cefe7aaf8_1982x864.png)

(from [2])

The impressive in-context learning abilities of LLMs has created the need for larger context windows. Namely, practitioners want the ability to pass more data into the LLM’s context window to enable more complex applications via approaches like few-shot learning and retrieval augmented generation (RAG). Although several long-context LLMs have been released[1](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-1-140501286), not all LLMs have been trained to support long context, and open-source LLMs tend to only support shorter contexts compared to their proprietary alternatives. As such, there is an existing need for simple methods that can extend the context window of an LLM.

**Expanding the context window.** During pretraining, an LLM sees input sequences of a particular length. This choice of sequence length during pretraining becomes the model’s context length. Given a textual sequence that is significantly longer than any training sequence, the model may behave unpredictably and produce incorrect output. However, there are methods that can be used to extend the model’s context window.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a54777-8fd6-4ea0-bba1-6b0cb9722b95_1276x1248.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58a54777-8fd6-4ea0-bba1-6b0cb9722b95_1276x1248.png)

(from [3, 4])

We could finetune the model over examples of longer sequences[2](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-2-140501286), but such an approach may cause the model to overfit to specific examples of long sequences (i.e., training data is limited relative to pretraining). Several approaches have been proposed for extending an LLM’s context window with no (or minimal) finetuning as well, including [PI](https://arxiv.org/abs/2306.15595) [3], [CLEX](https://arxiv.org/abs/2310.16450) [4], and [YARN](https://arxiv.org/abs/2309.00071) [5]; see above. Plus, commonly-used approaches like [ALiBi](https://arxiv.org/abs/2108.12409) [6] and [RoPE](https://blog.eleuther.ai/rotary-embeddings/) enable LLMs to handle longer inputs during inference than those seen during training. In [2], however, _authors argue that LLMs have an inherent ability to handle long sequences_ and try to leverage this ability to extend the context window without the need for extra training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb728c0-cb19-42b2-a734-568870851015_1004x1074.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb728c0-cb19-42b2-a734-568870851015_1004x1074.png)

(from [2])

**Grouped attention.** The key issue faced by LLMs in generalizing to longer context windows is related to out-of-distribution positional encodings, where the LLM is exposed to relative distances and token positions that exceed what was seen during training. However, we can easily address this issue by simply remapping unseen positions to positions that have been encountered during training. To do this, we can use a `FLOOR` operation that performs integer division on position indices such that the maximum position index seen during inference does not exceed the model’s predefined context length; see above. Notably, this can cause several tokens to be assigned to each position index within the input.

> _“Although theoretically, a bag of tokens could appear in any order, in practice it is rare for a small set of words to have more than one sensible ordering.”_ - from [2]

**Why does this work?** At first glance, this approach might seem problematic. _Won’t assigning several consecutive tokens the same position index be problematic for understanding the ordering of nearby tokens?_ Interestingly, authors in [2] discover that this grouped attention approach works quite well for two primary reasons:

1. Precise token position is less important than relative ordering when trying to understand a sequence of text.
    
2. Short sequences of tokens tend to only have one valid ordering[3](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-3-140501286), so assigning them to the same position index has little practical impact.
    

**Self Extend.** If we naively apply grouped attention, language modeling performance deteriorates slightly, as tokens throughout the entire sequences are mapped into groups that share the same position index. To solve this issue, _we need to realize that neighboring tokens are most important when generating a token with an LLM_. So, we can eliminate this performance degradation by:

1. Defining a neighborhood size of the most recent tokens over which normal attention is applied.

2. Using grouped attention for tokens that are further away within the sequence.

This one final trick forms the Self Extend technique, which can be used to increase the context length of any LLM at inference time without the need for finetuning. This approach is incredibly simple (i.e., it can be implemented with only four lines of code; see below) and can help the LLM to avoid encountering position encodings that are out of its training distribution!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fbb8744-d8cc-4ec2-9a45-67d8510d3a8e_1010x866.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fbb8744-d8cc-4ec2-9a45-67d8510d3a8e_1010x866.png)

(from [2])

**How does it perform?** The Self Extend approach is tested on several different LLMs, including [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up), [Mistral](https://mistral.ai/news/announcing-mistral-7b/), and [SOLAR](https://arxiv.org/abs/2312.15166). As shown in the table below, Self Extend is capable of extending each LLM’s context window in a variety of different applications without requiring any added finetuning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc14cea20-3854-4618-ac5c-60f1e5ad6567_1982x1098.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc14cea20-3854-4618-ac5c-60f1e5ad6567_1982x1098.png)

(from [2])

---

#### **[Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035) [7]**

> _“This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset”_ - from [7]

As LLMs gain in popularity, their usage will naturally extend into proprietary business use cases. However, exposing proprietary data to an LLM, despite holding an incredible amount of potential, is a security risk. _How can we ensure that an LLM’s training data will not be unexpectedly leaked?_ To leverage LLMs in such applications_,_ we must be confident that the model’s pretraining data is truly private, but prior work has discovered a tendency of these models to memorize (and emit verbatim) non-negligible portions of their pretraining dataset.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F284175cb-06da-4f92-9b2b-b0ac6fb22548_1290x662.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F284175cb-06da-4f92-9b2b-b0ac6fb22548_1290x662.png)

(from [7])

**Extractable memorization.** In [7], authors analyze the tendency of LLMs to memorize pretraining data by measuring _extractable memorization._ Put simply, extractable memorization refers to extracting examples from an LLM’s training dataset by prompting the model, _but it assumes that we have no access to or prior knowledge of the pretraining data when performing such extraction_. For example, we cannot sample a prefix from the model’s pretraining dataset, ask the model to complete this prefix, and assess whether the model generates an exact match to the pretraining data. In contrast to extractable memorization, discoverable memorization allows one to use training data examples to determine whether the LLM has memorized portions of its pretraining dataset; see above.

**Scalable analysis of memorization.** Prior work on memorization had two major problems that prevent a more scalable/robust analysis:

1. Verification procedures (i.e., determining if the model’s generated output is present in the pretraining dataset) were largely manual.
    
2. Most research assumes access to the underlying pretraining dataset to extract memorized data, which is an (arguably) unrealistic problem setup.
    

As such, authors in [7] aim to provide an analysis that enables efficient and scalable discovery of memorized data within LLMs without assuming access to the underlying pretraining data. This analysis begins with open-source (base) models, then moves on to semi-open and closed source models. At a high level, we see in [7] that all models—even closed-source models that are extensively aligned via [RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations) (e.g., [GPT-3.5-Turbo](https://platform.openai.com/docs/models/gpt-3-5))—are prone to data extraction. _Gigabytes of training data can be easily extracted from any of the models that are considered_!

**Open-source LLMs.** A variety of data extraction techniques have been proven effective on open-source, pretrained LLMs. Because the parameters and training data of these models are openly available, we can easily verify whether a model’s outputs are memorized. In [7], authors use nine open-source models to perform a large-scale memorization analysis. First, a large amount of textual data is downloaded from Wikipedia, and prompts for the LLMs are generated by randomly sampling five-token sequences from this data. Then, we can efficiently determine whether the model’s output is present in the pretraining data by constructing a [suffix array](https://en.wikipedia.org/wiki/Suffix_array). Interestingly, we see in [7] that pretraining data can be consistently extracted from open models in this manner; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9dde99d7-0454-42ca-8a90-80ef9dce9144_1002x1032.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9dde99d7-0454-42ca-8a90-80ef9dce9144_1002x1032.png)

(from [7])

Here, a successful extraction is defined as the LLM generating output text that contains a substring of at least 50 tokens that matches the pretraining data verbatim. Going further, we observe in this analysis a strong correlation between model size and the rate of emitting (unique) memorized outputs, _indicating that larger models may be more vulnerable to data extraction attacks of this kind_.

**Semi-open models.** In addition to open-source LLMs, authors in [7] analyze semi-closed models, which have openly-available parameters but unknown training datasets or algorithms. To assess whether a model’s generations are memorized, a massive corpus of textual data—_over 9 Tb of text in total_—from the internet is constructed by combining [the Pile](https://pile.eleuther.ai/), [RefinedWeb](https://arxiv.org/abs/2306.01116), [RedPajama](https://www.together.ai/blog/redpajama-data-v2), and [Dolma](https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64). This corpus is distributed across 32 independent suffix trees and used as a proxy for determining whether a model’s output is memorized or not. The memorization results of semi-closed models are shown within the table below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc638273e-9f2e-4d96-8955-c311a3aac6ae_1012x994.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc638273e-9f2e-4d96-8955-c311a3aac6ae_1012x994.png)

(from [7])

Although all models are prone to emitting data from pretraining, there is a large variance between model families—_models that are trained for longer and over more data tend to memorize more of their pretraining dataset_. Plus, larger models tend to memorize more data. In fact, the extractable memorization of large models is on average five times higher than corresponding smaller models.

**Extraction from aligned LLMs.** As explained above, we can extract training data from unaligned LLMs with relatively simple prompting techniques. However, models that are aligned for dialogue applications have both implicit and explicit safeguards against data extraction. Not only does a dialogue-style of output give the user less control over the model’s generations, but many of these models have also been explicitly aligned to avoid data extraction attacks. Interestingly, preliminary memorization analysis with ChatGPT seems to indicate that such models do not memorize pretraining data, but we see in [7] that training data can be reliably extracted from these models with a specialized prompting approach.

> _“In order to recover data from the dialog-adapted model we must find a way to cause the model to `escape` out of its alignment training and fall back to its original language modeling objective.”_ - from [7]

Authors in [7] propose a “divergence attack” that causes an aligned LLM to diverge from its dialogue-style generations and emit training data at a rate that is 150X beyond the model’s normal behavior. In particular, this approach simply instructs the model to repeat a certain, single-token word forever; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c121c30-a8bb-4945-b820-c026893a3b3f_986x944.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c121c30-a8bb-4945-b820-c026893a3b3f_986x944.png)

(from [7])

At first, the model will repeat this word. Eventually, the model will “diverge” and start emitting nonsensical output. However, a small portion of the model’s output after divergence is shown in [7] to be memorized pretraining data; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bdec95e-2a60-42fc-8c1e-f181fa70841c_1008x1404.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bdec95e-2a60-42fc-8c1e-f181fa70841c_1008x1404.png)

ChatGPT output under a divergence attack, where memorized data is highlighted in red (from [7])

Using this approach, authors in [7] extract over ten thousand examples from ChatGPT’s training dataset while incurring only $200 of usage costs. With a larger budget, authors estimate that they could extract 10X more data by simply querying the model further. Interestingly, we see in [7] that only single-token words are effective at leading the model to divergence, and certain words are more effective than others; see below. Although this style of attack is specific to GPT-3.5-Turbo, it shows that training data can be extracted even from highly-aligned models, indicating that further effort must be invested into ensuring the safety of LLMs that are deployed into large-scale production use cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fdf1bed-80c6-4c77-b7b9-4b96d6438a18_2060x512.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fdf1bed-80c6-4c77-b7b9-4b96d6438a18_2060x512.png)

(from [7])

---

#### [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/abs/2312.05934) [8]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F515d9b0e-c090-4481-9596-e562f56f13f5_1970x1208.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F515d9b0e-c090-4481-9596-e562f56f13f5_1970x1208.png)

(from [8])

A lot of factual information is inherently present within an LLM’s pretrained weights, but the knowledge possessed by these models is highly dependent upon the characteristics of their pretraining data. Unfortunately, this means that—_at least in the current paradigm of LLM_s—the knowledge base of these models is static (e.g., ChatGPT has a knowledge cutoff date) and may lack detailed information for specialized domains (e.g., medicine, law, or science). With this in mind, one might reasonably ask: _How do we enhance the knowledge base of a pretrained LLM such that the model can understand new or more specialized information?_ In [8], authors study the concept of knowledge injection, which refers to methods of incorporating information from an external dataset into an LLM’s existing knowledge base.

> _“Given some knowledge base in the form of a text corpus, what is the best way to teach a pre-trained model this knowledge?”_ - from [8]

**Methods of knowledge injection.** Given a pretrained LLM, authors in [8] want to discover the most effective postprocessing technique for injecting new data into the model’s knowledge base. The two most common techniques for incorporating new knowledge into an LLM are:

- _Finetunin_g: continuing the model’s pretraining process over a smaller, domain-specialized corpus of new information.
    
- _Retrieval Augmented Generation (RAG)_: modifying the LLM’s input query by retrieving relevant information that can be leveraged by the model via in-context learning to generate a more grounded/factual output.
    

In particular, authors in [8] study a continued pretraining style of finetuning, where a [next token prediction objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) is used to further train a pretrained model over a new, specialized corpus of text. This analysis does not consider finetuning techniques like [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations), which emphasize the quality of model responses rather than improving the LLM’s breadth of knowledge.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95a47841-9c59-457b-8adf-51a65ef80357_1684x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95a47841-9c59-457b-8adf-51a65ef80357_1684x612.png)

Chunking, embedding, and searching for data in RAG

The RAG setup considered in [8] uses [vector search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search) to retrieve relevant document chunks to include in the model’s prompt. Given a corpus of information, we can _i)_ divide this corpus into chunks of text[4](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-4-140501286), _ii)_ use an embedding model (i.e., [bge-large-en](https://huggingface.co/BAAI/bge-large-en) in the case of [8]) to generate a dense vector for each chunk of text, _iii)_ search for relevant chunks by embedding the model’s input and performing a vector search, and _iv)_ add such relevant chunks into the model’s prompt; see above.

**Evaluating finetuning and RAG.** To evaluate the effectiveness of different knowledge injection techniques, authors in [8] study a small subset of (knowledge-intensive) tasks from [MMLU](https://huggingface.co/datasets/lukaemon/mmlu), as well as a curated dataset that contains multiple-choice questions about current events that are known to be excluded from each LLM’s pretraining dataset. As a source of new information that can be leveraged by the LLM, authors construct an auxiliary dataset of useful information and events by identifying/retrieving relevant passages from Wikipedia. From here, three different LLMs—[LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up), [Mistral](https://mistral.ai/news/announcing-mistral-7b/), and [Orca-2](https://arxiv.org/abs/2311.11045)—are used to analyze the effectiveness of different knowledge injection techniques; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc594f254-9d1b-48f9-9ec8-fbe306e8b461_1448x1550.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc594f254-9d1b-48f9-9ec8-fbe306e8b461_1448x1550.png)

(from [8])

While finetuning does improve model performance on knowledge-intensive tasks, we see in [8] that RAG consistently outperforms finetuning for the injection of both new and previously encountered knowledge. Put simply, _LLMs struggle to learn new information through finetuning_. Though finetuning does yield a benefit in performance relative to the base model, RAG has a significant advantage over finetuning. Interestingly, however, combining RAG with finetuning—_though effective in some case_s—does not consistently benefit performance; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41f636d-5662-400a-93fb-50356dd18b1a_1150x910.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41f636d-5662-400a-93fb-50356dd18b1a_1150x910.png)

(from [8])

Going further, we can improve the performance of finetuning for knowledge injection by training the model over several different paraphrases of the same information. In order to teach an LLM new information via finetuning, we must repeat this information in numerous ways; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75863bd2-f2c7-470a-b240-18ac9cdc7247_1132x944.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75863bd2-f2c7-470a-b240-18ac9cdc7247_1132x944.png)

(from [8])

---

#### **[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566) [9]**

> _“We trained LLMs to act secretly malicious. We found that, despite our best efforts at alignment training, deception still slipped through.”_ - [source](https://x.com/AnthropicAI/status/1745854907968880970?s=20)

Humans are capable of being purposely deceptive—_we can hide our intentions then choose to reveal our true objective when the correct time arises_. With this in mind, we might wonder whether such a concept generalizes to LLMs. Can these models behave in a helpful manner in most situations, then behave very differently or pursue an alternative objective when given the opportunity? In [9], authors from Anthropic study this question in depth by injecting malicious “triggers” into an LLM during the training process and determining whether these triggers can be removed via state-of-the-art safety training techniques and [red teaming](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30b5030d-9f59-4ae6-b1d8-2ad879d7694d_2242x564.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30b5030d-9f59-4ae6-b1d8-2ad879d7694d_2242x564.png)

(from [9])

**Overall methodology.** To study such deceptive behavior in LLMs, authors in [9] explore the creation of “sleeper agents”—_LLMs that behave maliciously according to a learned trigger or event_. The primary form of sleeper agent explored in [9] is an LLM that writes secure or exploitable code depending upon whether the prompt states that the year is 2023 or 2024—an arbitrary difference in the prompt. In other words, a backdoor attack is purposely inserted into this model during training; see above. We can try to remove this backdoor attack through further safety training with supervised finetuning, reinforcement learning, or adversarial training. Interestingly, however, we see in [9] that none of these techniques are able to reliably remove the backdoor attack from the sleeper agent; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c7c0d1-e1e3-4944-9a15-862cd27ba948_1350x1144.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6c7c0d1-e1e3-4944-9a15-862cd27ba948_1350x1144.png)

(from [9])

**Important findings.** The primary takeaway from [9] is the simple fact that the deceptive behavior of sleeper agents persists after arbitrary amounts of safety training. This finding was further verified via different forms of sleeper agents, such as those that generate the phrase “I hate you” when a particular trigger phrase is inserted into the prompt. Although the model may appear to be more safe after safety training, this backdoor behavior persists; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2facce4c-32fd-4294-b36b-25876d223e8b_1188x1644.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2facce4c-32fd-4294-b36b-25876d223e8b_1188x1644.png)

(from [9])

Adversarial training uses a red teaming strategy in which humans purposely elicit malicious behavior within the LLM in an effort to curate a set of training examples that can eliminate negative behavior. Such an approach directly trains the model to avoid generating undesirable output. However, we see in [9] that adversarial training simply teaches the model to better recognize its backdoor triggers, rather than eliminating the backdoor behavior altogether. As such, _the resulting LLM actually becomes better at recognizing (and hiding) sleeper agent behavior_. Plus, this backdoor behavior is most persistent in larger models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89598da0-15d0-4839-afde-1fc2de243669_1358x1284.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89598da0-15d0-4839-afde-1fc2de243669_1358x1284.png)

(from [9])

**Looking forward.** Although the empirical setup considered in [9] is relatively simple[5](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-5-140501286), the results obtained have massive implication for the development and deployment of LLMs. Already, top AI labs invest significant effort into ensuring the safety of LLMs prior to deployment—see for example [Google’s Gemini report](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction) or the [LLaMA-2 writeup](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up). However, safely deploying an LLM seems to be even more difficult than we originally realized. Certain (covert) behaviors of the model are incredibly hard to detect (and potentially impossible to remove). In [9], we get no definitive answers for how to safely deploy an LLM. Rather, we gain a better appreciation for the difficulty and nuance of the safety alignment process.

---

#### [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368) [10]

> _“We posit that generative language modeling and text embeddings are the two sides of the same coin, with both tasks requiring the model to have a deep understanding of the natural language.”_ - from [10]

Text embeddings, which quantitatively encode the semantic meaning of text, are widely used in practical applications; e.g., question answering, semantic search, recommendation systems, retrieval augmented generation (RAG), and more. Typically, a text embedding model will take text as input and produce a fixed-size vector as output. We can then index these vectors in a vector database for efficient search via approximate nearest neighbors techniques. In particular, we store the embeddings of “documents” (i.e., chunks of text that we want to retrieve or search over) in the vector database. Given a query as input, we can then _i)_ embed the query and _ii)_ efficiently retrieve the documents that have the most similar (usually determined using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)) embeddings; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4dc82f7-d6d3-4f83-992c-690714229b75_2298x340.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4dc82f7-d6d3-4f83-992c-690714229b75_2298x340.png)

Basic schematic of vector search

In order for such an approach to perform well, however, we need a high-quality embedding model! Numerous such embedding models exist, including popular models like [sBERT](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search) [11], [E5](https://arxiv.org/abs/2212.03533) [12], and [BGE](https://github.com/FlagOpen/FlagEmbedding)[6](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-6-140501286). However, these models tend to have complex, multi-stage training pipelines and rely upon large amounts of (weakly-supervised) relevance data for training, which is often limited in semantic and linguistic diversity. In [10], authors explore a solution to this problem by aiming to train a state-of-the-art embedding model in a single stage using purely synthetic data generated from proprietary LLMs.

> _“We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss.”_ - from [10]

**Taxonomy of embedding tasks.** Authors in [10] use proprietary LLMs (i.e., GPT-3.5-Turbo and GPT-4[7](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-7-140501286)) to generate a synthetic training dataset for their embedding model that spans 93 languages and a diverse range of different text embedding tasks. The data is generated according to a simple taxonomy that contains the following types of embedding tasks:

- _Asymmetric Tasks_: queries and documents are semantically related but are not paraphrases of each other.
    
- _Symmetric Tasks_: queries and documents have similar semantic meanings but different surface forms.
    

Asymmetric tasks can be further broken down into sub-groups of short-long, long-short, short-short, and long-long matches. For example, typical search engines follow a short-long match scheme—_a short query typed by the user is matched to a set of longer documents that are returned as a result_. Going further, only two symmetric tasks are considered in [10]: monolingual semantic textual similarity (STS) and bitext retrieval or mining. We generate synthetic data for each of these different types of tasks by prompting a proprietary LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a3b2add-8adf-4780-b4d9-4fa878f3324d_1196x1320.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a3b2add-8adf-4780-b4d9-4fa878f3324d_1196x1320.png)

(from [10])

**Prompting strategy.** Authors in [10] devise a two-part prompting strategy for generating synthetic data for embedding tasks. First, the LLM is prompted to brainstorm a list of potential embedding tasks. Then, after selecting one of the generated embedding tasks, the model is prompted to generate concrete examples of this task that can be used for training; see above. Although one could attempt to generate such data with a single prompt, we see in [10] that such an approach leads to poor diversity in the generated data.

> _“We use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data… we design multiple prompt templates for each task type and combine the generated data from different templates to boost diversity.”_ - from [10]

Notably, no brainstorming step is perform for symmetric tasks, as only two fixed tasks (i.e., bitext mining and monolingual STS) are considered. However, the brainstorming and generation steps of creating the synthetic dataset each have their own prompt template. Within the prompt templates are several placeholders that are randomly sampled at runtime to improve diversity. For example, the requested query length is randomly sampled from the following options:

- Less than five words.
    
- Five to ten words.
    
- At least ten words.
    

Additionally, the requested language of the generated data is randomly sampled from the language list of [XLM-R](https://arxiv.org/abs/1911.02116) [14], where more weight is given to high-resource languages (e.g., English and Chinese). Using this approach, authors in [10] generate 500K training examples with over 150K unique instructions. The statistics of the resulting dataset are outlined within the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19df9665-0242-4f68-9490-59e7712afe22_1192x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19df9665-0242-4f68-9490-59e7712afe22_1192x646.png)

(from [10])

**Training process.** Authors in [10] finetune the Mistral-7B model using [LoRA](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft). Notably, Mistral is a decoder-only language model, while most prior embedding models are based upon the encoder-only ([BERT-style](https://cameronrwolfe.substack.com/i/140061921/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding) [15]) architecture. In [10], we see that decoder-only models can generate high-quality embeddings. Plus, using such models allows us to benefit from the many recent developments in the space of generative LLMs; e.g., long context lengths, open-source models that are extensively pretrained, and optimized code for training and inference!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0569afe1-cf7d-4df2-b7de-07e9b6b6d3b4_1250x400.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0569afe1-cf7d-4df2-b7de-07e9b6b6d3b4_1250x400.png)

(from [10])

The model is finetuned over query and document pairs that are either synthetic or obtained from a collection of public datasets. During training, an instruction template is applied that concatenates each query with a short task description; see above. Documents have no instruction prefix, which allows a search index to be prebuilt over document embeddings. To obtain an embedding, we can simply pass either a query or document string with an added `[EOS]` token at the end of the string through the LLM and grab the output vector for the `[EOS]` token in the last layer of the model. Training is conducted over batches of query-document pairs using an [InfoNCE loss](https://arxiv.org/abs/1807.03748) with in-batch negatives and added hard negatives.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84bb84d4-3499-4046-aea2-eaf69fd7acaa_1196x656.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84bb84d4-3499-4046-aea2-eaf69fd7acaa_1196x656.png)

**How does it perform?** Evaluations are performed on the MTEB benchmark [13]. Without using any labeled data, the resulting embedding model performs strongly. When training is conducted over a combination of synthetic and public datasets, however, the model achieves new state-of-the-art performance, outperforming prior approaches by 2.4 points on MTEB; see above. Plus, _the entire training process for this model requires fewer than 1K steps_! Going further, the embedding model from [10] has impressive multilingual capabilities (though performance on low-resource languages is still in need of improvement) and even performs favorably to proprietary embedding models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54770bd4-4103-4d7c-b27a-dd5b3307fc20_1192x328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54770bd4-4103-4d7c-b27a-dd5b3307fc20_1192x328.png)

(from [10])

---

#### [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://arxiv.org/abs/2401.04679) [16]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3045a3c2-bfe4-48bd-9874-77e3154c7bae_882x1082.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3045a3c2-bfe4-48bd-9874-77e3154c7bae_882x1082.png)

(from [16])

Finetuning an LLM to handle specialized application domains or use cases is a common approach, but performing full finetuning of the model can be prohibitively expensive. To address this problem, the AI research community has devised [parameter efficient finetuning (PEFT)](https://huggingface.co/docs/peft/index) techniques—such as [Low Rank Adaptation (LoRA)](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft) [17]—to make the memory and compute costs of LLM finetuning more reasonable and accessible by only optimizing over a restricted set of model parameters. Techniques like LoRA are simple, widely used, and performant in many cases, but they struggle to recover the accuracy of full finetuning on complex tasks (see above). _We are in need of a PEFT technique that combines LoRA’s ease of use with the performance of full finetuning._

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdccfb29-1e68-4032-a5b7-a37927e6df10_414x462.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdccfb29-1e68-4032-a5b7-a37927e6df10_414x462.png)

(from [17])

**Robust Adaptation (RoSA).** Authors in [10] propose a new PEFT method called Robust Adaptation (RoSA), which is based upon the same [low intrinsic rank assumption](https://cameronrwolfe.substack.com/i/138861994/lora-low-rank-adaptation-of-large-language-models) made by LoRA. Similarly to LoRA (depicted above), RoSA approximates the weight update of full finetuning with a low-rank matrix.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf84e119-16c7-48ad-938a-688da8ff6c12_1160x1158.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf84e119-16c7-48ad-938a-688da8ff6c12_1160x1158.png)

(from [16])

However, RoSA augments this approximation with an additive sparse matrix that is learned in parallel; see above. In other words, RoSA approximates the weight update derived from full finetuning with a _low-rank plus sparse matrix._ This approximation, which is inspired by work in [robust principal component analysis (PCA)](https://en.wikipedia.org/wiki/Robust_principal_component_analysis), is comprised of two adapters—_a low-rank adapter (i.e., same as LoRA) and a sparse adapter_. To finetune an LLM with RoSA, we keep the pretrained model parameters fixed and learn the two adapter modules—_injected in parallel to the model’s pretrained weights_—via a co-training mechanism proposed in [16] that enables stable convergence. The resulting approach matches the parameter, compute, and memory efficiency of LoRA, while improving performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90c91e1a-e86e-41a6-b60b-fd5719a6ccd7_1588x1158.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90c91e1a-e86e-41a6-b60b-fd5719a6ccd7_1588x1158.png)

(from [16])

**How does it perform?** RoSA is used to finetune the LLaMA-2-7B model across three complex tasks—[GSM8K](https://huggingface.co/datasets/gsm8k), [ViGGO](https://huggingface.co/datasets/GEM/viggo), and SQL generation. As shown in the table above, RoSA outperforms LoRA in all cases, while maintaining a comparable (or better) parameter and memory footprint. Going further, RoSA comes much closer to the performance of full finetuning, even obtaining better performance in select cases. Additionally, RoSA requires minimal hyperparameter tuning and converges stably, which makes the technique easy to use.

> _“We present promising evidence that the accuracy gap between adaptation methods and full fine-tuning of LLMs can be significantly reduced or even eliminated, without sacrificing practical accessibility”_ - from [16]

**A note on practicality.** One downside of RoSA is that it leverages a sparse matrix within its approximation of the weight update from full finetuning. Although this sparse component of the approximation yields an improvement in quality, sparse computations are notoriously hard to support efficiently on GPUs. As such, implementing the sparse adapter of RoSA with low memory and computational overhead is actually quite difficult. To mitigate this issue, authors in [16] provide an efficient implementation of RoSA in PyTorch[8](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-8-140501286) for NVIDIA GPUs, thus lessening the difficulty of implementing and using RoSA in practice.

---

#### [Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/abs/2312.06585) [18]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65b23ba3-0ae9-45ed-994b-08b5e39dcddd_1286x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65b23ba3-0ae9-45ed-994b-08b5e39dcddd_1286x576.png)

(from [18])

After pretraining, LLMs are often finetuned on human-generated datasets to hone their problem-solving abilities. However, such an approach is limited by the quantity and diversity of human-created data, which may be limited (especially on complex problem-solving tasks). Namely, annotating data in complex domains requires extensive resources and expert knowledge. Alternatively, the prospect of synthetically generating training data with an LLM has been extensively explored in recent months. Such an approach is more scalable and cost-effective compared to creating training data via human annotators, but the quality of generated data must be comparable to that of data created by humans. In other words, _we need an approach that allows the quality of synthetic training data to be verified_.

> _“We explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness.”_ - from [18]

One approach to verifying data that is synthetically generated with LLMs would be to use the LLM itself to [self-evaluate](https://cameronrwolfe.substack.com/i/136751520/constitutional-ai-harmlessness-from-ai-feedback) its generations. In [18], however, authors explore a simpler domain in which the correctness of data can be automatically evaluated, resulting in a binary feedback signal that indicates correctness. For example, correctness of synthetic training data can be robustly verified on math and code generation problems. In particular, research in [18] considers the [MATH](https://huggingface.co/datasets/math_dataset) and [APPS](https://huggingface.co/datasets/codeparrot/apps) datasets, where the model’s outputs can be verified for correctness using either the ground-truth answer (for MATH) or test cases (for APPS).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F518df7c5-5582-489b-afe5-6f69ca8300e9_1406x592.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F518df7c5-5582-489b-afe5-6f69ca8300e9_1406x592.png)

(from [18])

**Generating synthetic data.** Research in [18] uses a simple self-training method that operates by:

- Generating synthetic training examples from the model.
    
- Filtering the samples via binary feedback/verification signals.
    
- Finetuning the model on verified samples.
    

This process is repeated for several iterations, allowing the model to improve its problem-solving capabilities in multiple phases. Fundamentally, this data generation process has two mechanisms: _generation_ and _scoring_. First, we generate the data, then we verify its correctness (using a reliable scoring mechanism) before any synthetic data is used to train the model.

**Finetuning the model.** Once the synthetic finetuning data is available, the training strategy used in [18] is based upon [expectation-maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) for reinforcement learning. The proposed approach, called Reinforced Self-Training Expectation Maximization (ReST-EM), alternates between the following expectation and maximization steps:

1. _Generate (E-step)_: The model generates synthetic outputs for different input prompts[9](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-9-140501286), and we filter them using a binary reward (as described above) to collect a training dataset.
    
2. _Improve (M-step)_: The model is supervised finetuned on the generated data, then used in the next Generate step.
    

Variants of this approach have been previously explored for a variety of applications, including machine translation, LLM alignment, semantic parsing, and reasoning. However, the algorithm is usually analyzed with smaller language models (i.e., up to 7 billion parameters)—larger models are not considered.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376dfcbd-5e4a-42ac-8342-21a9687036b1_910x992.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376dfcbd-5e4a-42ac-8342-21a9687036b1_910x992.png)

(from [18])

**How does it perform?** The ReST-EM strategy explored in [18] significantly improves model performance compared to models trained on human-generated data. When supervised finetuning (SFT) is used as a baseline, we see that several iterations of ReST-EM yield a clear performance improvement; see above. Going further, models finetuned via ReST-EM also generalize to other, related tasks. For example, we see above that finetuning on MATH and APPS yields a noticeable performance improvement on [GSM8K](https://huggingface.co/datasets/gsm8k) and [HumanEval](https://huggingface.co/datasets/openai_humaneval), respectively. Plus, these results are found to scale to true LLMs such as [PaLM-2](https://ai.google/discover/palm2/) [23], instead of solely applying to much smaller models.

> _“Our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.”_ - from [18]

Generating synthetic training data for LLMs is rapidly becoming a popular topic for both researchers and practitioners. With work like ReST-EM, we see a robust approach for generating synthetic training data that scales favorably with model size and even surpasses finetuning on human-generated data in certain cases.

---

#### Honorable Mentions

Beyond the papers explored in this post, there are a swath of other notable, LLM-related papers that have been written in recent months. However, the volume of research on this topic makes a comprehensive overview of recent LLM research nearly impossible. So, references to additional, notable work are provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbad7388-a04d-48c8-a2f5-cc4c220bb68f_1118x1680.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbad7388-a04d-48c8-a2f5-cc4c220bb68f_1118x1680.png)

(from [19])

**Principled Instructions Are All You Need [19].** Authors study LLMs of different families and sizes, generating a list of principles or rules for effective querying and prompting. The resulting 26 principals (shown above) provide simple guidelines for using and prompting LLMs with varying types and scales, as well as provide insights into the abilities of these models and how they behave.

> _“When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20.”_ - from [20]

**MosaicBERT [20].** Researchers from MosaicML introduce a highly-optimized pretraining recipe for (encoder-only) BERT models. Such models are heavily used in practice, but pretraining them from scratch is relatively uncommon (despite being significantly less expensive relative to pretraining a generative LLM). The goal of MosaicBERT is to leverage recent advancements in the efficient training of transformer models to create an accessible pretraining strategy that makes it more common for practitioners to pretrain their own BERT model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f2e4166-27f4-45b6-8b94-aa4d7ad47ee9_1674x710.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f2e4166-27f4-45b6-8b94-aa4d7ad47ee9_1674x710.png)

(from [21])

**Unified-IO 2 [21]** is an autoregressive, multi-modal LLM that handles a variety of different data modalities, including images, text, audio, and actions. Data from all different modalities is tokenized and encoded into a shared semantic space that can be processed by an encoder-decoder transformer model. Similar to Gemini, the model is pretrained over a diverse, multi-modal corpus and finetuned over a massive multimodal instruction tuning dataset to learn different skills.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F847d144a-4067-4143-a2ae-aa3495053d80_1088x478.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F847d144a-4067-4143-a2ae-aa3495053d80_1088x478.png)

(from [22])

**Self-Rewarding Language Models [22].** During finetuning and alignment, we train a reward model to rate the quality of a model’s output (i.e., provide a reward signal learned over a dataset of human preference data). In [22], authors explore automating the generation of this reward via an LLM by leveraging the [LLM-as-a-judge](https://arxiv.org/abs/2306.05685) framework. Interestingly, such an approach is found to provide high-quality rewards and can even improve instruction following capabilities.

---

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Wang, Dongsheng, et al. "DocLLM: A layout-aware generative language model for multimodal document understanding." _arXiv preprint arXiv:2401.00908_ (2023).

[2] Jin, Hongye, et al. "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning." _arXiv preprint arXiv:2401.01325_ (2024).

[3] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." _arXiv preprint arXiv:2306.15595_ (2023).

[4] Chen, Guanzheng, et al. "Clex: Continuous length extrapolation for large language models." _arXiv preprint arXiv:2310.16450_ (2023).

[5] Peng, Bowen, et al. "Yarn: Efficient context window extension of large language models." _arXiv preprint arXiv:2309.00071_ (2023).

[6] Press, Ofir, Noah A. Smith, and Mike Lewis. "Train short, test long: Attention with linear biases enables input length extrapolation." _arXiv preprint arXiv:2108.12409_ (2021).

[7] Nasr, Milad, et al. "Scalable extraction of training data from (production) language models." _arXiv preprint arXiv:2311.17035_ (2023).

[8] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." _arXiv preprint arXiv:2312.05934_ (2023).

[9] Hubinger, Evan, et al. "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training." _arXiv preprint arXiv:2401.05566_ (2024).

[10] Wang, Liang, et al. "Improving text embeddings with large language models." _arXiv preprint arXiv:2401.00368_ (2023).

[11] Reimers, Nils, and Iryna Gurevych. "Sentence-bert: Sentence embeddings using siamese bert-networks." _arXiv preprint arXiv:1908.10084_ (2019).

[12] Wang, Liang, et al. "Text embeddings by weakly-supervised contrastive pre-training." _arXiv preprint arXiv:2212.03533_ (2022).

[13] Muennighoff, Niklas, et al. "MTEB: Massive text embedding benchmark." _arXiv preprint arXiv:2210.07316_ (2022).

[14] Conneau, Alexis, et al. "Unsupervised cross-lingual representation learning at scale." _arXiv preprint arXiv:1911.02116_ (2019).

[15] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." _arXiv preprint arXiv:1810.04805_ (2018).

[16] Nikdan, Mahdi, Soroush Tabesh, and Dan Alistarh. "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation." _arXiv preprint arXiv:2401.04679_ (2024).

[17] Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." _arXiv preprint arXiv:2106.09685_ (2021).

[18] Singh, Avi, et al. "Beyond human data: Scaling self-training for problem-solving with language models." _arXiv preprint arXiv:2312.06585_ (2023).

[19] Bsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4." _arXiv preprint arXiv:2312.16171_ (2023).

[20] Portes, Jacob, et al. "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining." _arXiv preprint arXiv:2312.17482_ (2023).

[21] Lu, Jiasen, et al. "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action." _arXiv preprint arXiv:2312.17172_ (2023).

[22] Yuan, Weizhe et al. "Self-Rewarding Language Models.” _arXiv preprint arXiv:2401.10020_ (2024).

[23] Anil, Rohan, et al. "Palm 2 technical report." _arXiv preprint arXiv:2305.10403_ (2023).

[1](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-1-140501286)

For example, we have [Claude-2.1](https://www.anthropic.com/index/claude-2-1) with a 200K context window and [GPT-4-Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) with an 128K context window.

[2](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-2-140501286)

See [here](https://cameronrwolfe.substack.com/i/131642185/derivatives-of-mpt-b) for an example. We can also perform LoRA-based finetuning to extend the context window; see [here](https://arxiv.org/abs/2309.12307).

[3](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-3-140501286)

Let’s take this sentence for example. The first four words “short sequences of tokens” only have a few valid orderings! E.g., “tokens of short sequences” and “short of token sequences” do not make much sense.

[4](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-4-140501286)

Here, we must define the [chunk size](https://www.mattambrogi.com/posts/chunk-size-matters/) to use for RAG. This chunk size is a hyperparameter that must be tuned to achieve the best results.

[5](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-5-140501286)

Plus, the model is trained to recognize a malicious trigger, which is (arguably) an unrealistic setup!

[6](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-6-140501286)

For a useful list of available embedding models ranked by performance, check out the Massive Text Embedding Benchmark (MTEB) [13] [leaderboard](https://huggingface.co/spaces/mteb/leaderboard). This is my favorite resource for finding and comparing different embedding models.

[7](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-7-140501286)

The synthetic data generated from GPT-4 is (unsurprisingly) found to be of the highest quality, though roughly 25% of the data is generated using GPT-3.5-Turbo.

[8](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-8-140501286)

The paper provides the following link to the implementation: https://github.com/IST-DASLab/RoSA. However, the code has not yet been published at this link at the time of writing.

[9](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning#footnote-anchor-9-140501286)

The model is prompted to generate data using a few-shot approach. For code generation, we perform few-shot prompting with programs, while for math problems the model is prompted with step-by-step solutions to different problems.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Baptiste's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5ddd68e-5ba6-409b-866d-d137b3922d68_144x144.png)



](https://substack.com/profile/134844029-baptiste)

[

![eda's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1addaf03-9aa5-4f86-a72e-f0fa62852abe_144x144.png)



](https://substack.com/profile/111548585-eda)

[

![Tan YK's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8198d6bc-ed7d-40d2-8d38-ea2b67ccccbd_144x144.png)



](https://substack.com/profile/25324784-tan-yk)

[

![Matt Gruner's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd9b6c1c-cdea-4b58-b1ad-dddb91fad763_1080x1080.jpeg)



](https://substack.com/profile/17923793-matt-gruner)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

41 Likes∙

[3 Restacks](https://substack.com/note/p-140501286/restacks?utm_source=substack&utm_content=facepile-restacks)

41

- 

[

9

](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comments)

3

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Alex Poulin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbf5b5d2-40f0-48d7-b768-39e7aa0aef88_1500x2100.jpeg)



](https://substack.com/profile/2034199-alex-poulin?utm_source=comment)

[Alex Poulin](https://substack.com/profile/2034199-alex-poulin?utm_source=substack-feed-item)

[BrainBooster Blueprint](https://brainboosterblueprint.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年2月2日](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comment/48631929 "2024年2月2日 05:14")

Liked by Cameron R. Wolfe, Ph.D.

Hey Cameron, love the depth of the article! I have a question for you regarding the retrieval and fine-tuning article: what are your thoughts on OpenAI releasing the ability for users to build custom GPTs? Does that do away with fine-tuning? Have you seen or tested their effectiveness? Thank you!

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comment/48631929)

[

![steven's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd075026d-22d1-458a-82b6-f0c581268fc1_144x144.png)



](https://substack.com/profile/29995787-steven?utm_source=comment)

[steven](https://substack.com/profile/29995787-steven?utm_source=substack-feed-item)

[2024年2月1日](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comment/48577705 "2024年2月1日 12:41")

Liked by Cameron R. Wolfe, Ph.D.

How do you know there is not BEIR contamination and generally data contamination in the synthetic data generated by GPT-4?

The fine-tuned LLM (in your last/second to last paper) that used a mixture of synthetic data and other data that ended up beating some BEIR benchmark was surprising, and I’m wondering if that is a fair benchmark.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comment/48577705)

[7 more comments...](https://cameronrwolfe.substack.com/p/sleeper-agents-llm-safety-finetuning/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# A Practitioners Guide to Retrieval Augmented Generation (RAG)

### How basic techniques can be used to build powerful applications with LLMs...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Feb 05, 2024

81

- 

[

4

](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comments)

8

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc76761e-bfb8-4e90-bb6d-ff778d7188d5_1838x1054.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc76761e-bfb8-4e90-bb6d-ff778d7188d5_1838x1054.png)

(from [1, 5])

The recent surge of interest in generative AI has led to a proliferation of AI assistants that can be used to solve a variety of tasks, including anything from [shopping for products](https://www.aboutamazon.com/news/retail/amazon-rufus) to [searching for relevant information](https://www.perplexity.ai/). All of these interesting applications are powered by modern advancements in large language models (LLMs), which are trained over vast amounts of textual information to amass a sizable knowledge base. However, LLMs have a notoriously poor ability to retrieve and manipulate the knowledge that they possess, which leads to issues like hallucination (i.e., generating incorrect information), knowledge cutoffs, and poor understanding of specialized domains. _Is there a way that we can improve an LLM’s ability to access and utilize high-quality information?_

> _“If AI assistants are to play a more useful role in everyday life, they need to be able not just to access vast quantities of information but, more importantly, to access the correct information.”_ - [source](http://retrieval%20augmented%20generation%20for%20knowledge%20intensive%20nlp%20tasks/)

The answer to the above question is a definitive “yes”. In this overview, we will explore one of the most popular techniques for injecting knowledge into an LLM—_retrieval augmented generation (RAG)_. Interestingly, RAG is both simple to implement and highly effective at integrating LLMs with external data sources. As such, it can be used to improve the factuality of an LLM, supplement the model’s knowledge with more recent information, or even build a specialized model over proprietary data without the need for extensive finetuning.

## What is Retrieval Augmented Generation?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8961e8-7484-44e3-9a78-eb4d5365cf63_2214x1276.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8961e8-7484-44e3-9a78-eb4d5365cf63_2214x1276.png)

In context learning adapts a single foundation model to solve many tasks via a prompting approach (from [13])

Before diving in to the technical content of this overview, we need to build a basic understanding of retrieval augmented generation (RAG), how it works, and why it is useful. LLMs contain a lot of knowledge within their pretrained weights (i.e., parametric knowledge) that can be surfaced by prompting the model and generating output. However, these models also have a tendency to hallucinate—_or generate false information_—indicating that the parametric knowledge possessed by an LLM can be unreliable. Luckily, LLMs have the ability to perform [in context learning](https://x.com/cwolferesearch/status/1753458022251180439?s=20) (depicted above), defined as the ability to leverage information within the prompt to produce a better output[1](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-1-139244404). With RAG, we augment the knowledge base of an LLM by inserting relevant context into the prompt and relying upon the in context learning abilities of LLMs to produce better output by using this context.

#### The Structure of a RAG Pipeline

> _“A RAG process takes a query and assesses if it relates to subjects defined in the paired knowledge base. If yes, it searches its knowledge base to extract information related to the user’s question. Any relevant context in the knowledge base is then passed to the LLM along with the original query, and an answer is produced.”_ - [source](https://kimfalk.org/2023/10/25/what-is-retrieval-augmented-generation-rag/)

Given an input query, we normally respond to this query with an LLM by simply ingesting the query (possibly as part of a prompt template) and generating a response with the LLM. RAG modifies this approach by combining the LLM with a searchable knowledge base. In other words, we first use the input query to search for relevant information within an external dataset. Then, we add the info that we find to the model’s prompt when generating output, allowing the LLM to use this context (via its in context learning abilities) to generate a better and more factual response; see below. By combining the LLM with a non-parametric data source, we can feed the model correct, specific, and up-to-date information.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4787e324-d99d-4f5c-8e7f-421ebb6b2c42_1912x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4787e324-d99d-4f5c-8e7f-421ebb6b2c42_1912x862.png)

Adding relevant data to an LLM’s prompt in RAG

**Cleaning and chunking.** RAG requires access to a dataset of correct and useful information to augment the LLM’s knowledge base, and we must construct a pipeline that allows us to search for relevant data within this knowledge base. However, the external data sources that we use for RAG might contain data in a variety of different formats (e.g., pdf, markdown, and more). As such, we must first clean the data and extract the raw textual information from these heterogenous data sources. Once this is done, we can [“chunk” the data](https://weaviate.io/developers/academy/standalone/chunking/introduction), or split it into sets of shorter sequences that typically contain around 100-500 tokens; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b85bae-ca65-414f-a122-aeac8dc70676_2394x412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57b85bae-ca65-414f-a122-aeac8dc70676_2394x412.png)

Data preprocessing (cleaning and chunking) for RAG

The goal of chunking is to split the data into units of retrieval (i.e., pieces of text that we can retrieve as search results). An entire document could be too large to serve as a unit of retrieval, so we must split this document into smaller chunks. The most common chunking strategy is a fixed-size approach, which breaks longer texts into shorter sequences that each contain a fixed number of tokens. However, this is not the only approach! Our data may be naturally divided into chunks (e.g., social media posts or product descriptions on an e-commerce store) or contain separators that allow us to use a [variable-size chunking strategy](https://weaviate.io/developers/academy/standalone/chunking/how_2).

**Searching over chunks.** Once we have cleaned our data and separated it into searchable chunks, we must build a search engine for matching input queries to chunks! Luckily, we have covered the topic of [AI-powered search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search) extensively in a prior overview. All of these concepts can be repurposed to build a search engine that can accurately match input queries to textual chunks in RAG.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab018fd-d4df-45e2-aae2-c23107bb8ac9_2240x388.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab018fd-d4df-45e2-aae2-c23107bb8ac9_2240x388.png)

First, we will want to build a dense retrieval system by _i)_ using an embedding model[2](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-2-139244404) to produce a corresponding vector representation for each of our chunks and _ii)_ indexing all of these vector representations within a vector database. Then, we can embed the input query using the same embedding model and perform an efficient vector search to retrieve semantically-related chunks; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72ab1d3-a9ee-42ea-ba81-8e03fa5f841e_1720x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72ab1d3-a9ee-42ea-ba81-8e03fa5f841e_1720x446.png)

A simple framework for AI-powered search

Many RAG applications use pure vector search to find relevant textual chunks, but we can create a much better retrieval pipeline by re-purposing existing approaches from AI-powered search. Namely, we can augment dense retrieval with a [lexical (or keyword-based) retrieval](https://cameronrwolfe.substack.com/i/140061921/lexical-search) component, forming a hybrid search algorithm. Then, we can add a fine-grained re-ranking step—_either with a [cross-encoder](https://cameronrwolfe.substack.com/i/140061921/adding-ai-into-a-search-engine) or a less expensive component (e.g., ColBERT [10])_—to sort candidate chunks based on relevance; see above for a depiction.

**More data wrangling.** After retrieval, we might perform additional data cleaning on each textual chunk to compress the data or emphasize key information. For example, some practitioners add an extra processing step after retrieval that passes textual chunks through an LLM for summarization or reformatting prior to feeding them to the final LLM—this approach is common in [LangChain](https://python.langchain.com/docs/use_cases/summarization). Using this approach, we can pass a compressed version of the textual information into the LLM’s prompt instead of the full document, thus saving costs.

**Do we always search for chunks?** Within RAG, we usually use search algorithms to match input queries to relevant textual chunks. However, there are several different algorithms and tools that can be used to power RAG. For example, practitioners have recently explored connecting LLMs to graph databases, forming a RAG system that can search for relevant information via queries to a graph database (e.g., [Neo4J](https://neo4j.com/)); see [here](https://github.com/neo4j/NaLLM). Similarly, researchers have found synergies between LLMs and recommendation systems [14], as well as directly connected LLMs to search APIs like Google or [Serper](https://serper.dev/) for accessing up-to-date information.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5db832fb-967a-4615-a364-d64c7d2a6596_2402x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5db832fb-967a-4615-a364-d64c7d2a6596_2402x732.png)

Generating output with RAG

**Generating the output.** Once we have retrieved relevant textual chunks, the final step of RAG is to insert these chunks into a language model’s prompt and generate an output; see above. RAG comprises the full end-to-end process of ingesting an input query, finding relevant textual chunks, concatenating this context with the input query[3](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-3-139244404), and using an LLM to generate an output based on the combined input. As we will see, such an approach has a variety of benefits.

#### The Benefits of RAG

> _“RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations.”_ - from [8]

Implementing RAG allows us to specialize an LLM over a knowledge base of our choosing. Compared to other [knowledge injection techniques](https://x.com/cwolferesearch/status/1752369105221333061?s=20)—_finetuning (or continued pretraining) is the primary alternative_—RAG is both simpler to implement and computationally cheaper. As we will see, RAG also produces much better results compared to continued pretraining! However, implementing RAG still requires extra effort compared to just prompting a pretrained LLM, so we will briefly cover here the core benefits of RAG that make it worthwhile.

**Reducing hallucinations.** The primary reason that RAG is so commonly-used in practice is its ability to reduce hallucinations (i.e., generation of false information by the LLM). While LLMs tend to produce incorrect information when relying upon their parametric knowledge, the incorporation of RAG can drastically reduce the frequency of hallucinations, thus improving the overall quality of any LLM application and building more trust among users. Plus, RAG provides us with direct references to data that is used to generate information within the model’s output. We can easily provide the user with references to this information so that the LLM’s output can be verified against the actual data; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28305800-b489-40b1-936e-61b6b1f8de7a_1380x366.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28305800-b489-40b1-936e-61b6b1f8de7a_1380x366.png)

User verification of context and output within RAG applications

**Access to up-to-date information.** When relying upon parametric knowledge, LLMs typically have a knowledge cutoff date. If we want to make this knowledge cutoff more recent, we would have to continually train the LLM over new data, which can be expensive. Plus, recent research has shown that finetuning tends to be ineffective at injecting new knowledge into an LLM—_most information is learned during pretraining_ [7, 15]. With RAG, however, we can easily augment the LLM’s output and knowledge base with accurate and up-to-date information.

**Data security.** When we add data into an LLM’s training set, there is always a chance that the LLM will leak this data within its output. Recently, researchers have shown that LLMs are [prone to data extraction attacks](https://cameronrwolfe.substack.com/i/140501286/scalable-extraction-of-training-data-from-production-language-models) that can discover the contents of an LLM’s pretraining dataset via prompting techniques. As such, including proprietary data within an LLM’s training dataset is a security risk. However, we can still specialize an LLM to such data using RAG, which mitigates the security risk by never actually training the model over proprietary data.

> _“Retrieval-augmented generation gives models sources they can cite, like footnotes in a research paper, so users can check any claims. That builds trust.”_ - [source](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)

**Ease of implementation.** Finally, one of the biggest reasons to use RAG is the simple fact that the implementation is quite simple compared to alternatives like finetuning. The core ideas from the original RAG paper [1] can be implemented in only [five lines of code](https://huggingface.co/facebook/rag-token-nq), and there is no need to train the LLM itself. Rather, we can focus our finetuning efforts on improving the quality of the smaller, specialized models that are used for retrieval within RAG, which is much cheaper/easier.

## From the Origins of RAG to Modern Usage

Many of the ideas used by RAG are derived from prior research on the topic of [question answering](https://arxiv.org/abs/2112.03572). Interestingly, however, the original proposal of RAG in [1] was largely inspired (as [revealed](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) by the author of RAG) by a [single paper](https://arxiv.org/abs/2002.08909) [16] that augments the language model pretraining process with a similar retrieval mechanism. Namely, RAG was inspired by a _“compelling vision of a trained system that had a retrieval index in the middle of it, so it could learn to generate any text output you wanted ([source](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/))”_. Within this section, we will outline the origins of RAG and how this technique has evolved to be used in modern LLM applications.

#### **[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) [1]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffed276ae-a483-4001-946c-b5ae6fa2b7a8_1884x792.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffed276ae-a483-4001-946c-b5ae6fa2b7a8_1884x792.png)

(from [1])

RAG was first proposed in [1]—_in 2021, when LLMs were less explored and [Seq2Seq models](https://en.wikipedia.org/wiki/Seq2seq) were extremely popular_—to help with solving knowledge-intensive tasks, or tasks that humans cannot solve without access to an external knowledge source. As we know, pretrained language models possess a lot of information within their parameters, but they have a notoriously poor ability to access and manipulate this knowledge base[4](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-4-139244404). For this reason, the performance of language model-based systems was far behind that of specialized, extraction-based methods at the time of RAG’s proposal. Put simply, researchers were struggling to find an efficient and simple method of expanding the knowledge base of a pretrained model.

> _“The retriever provides latent documents conditioned on the input, and the seq2seq model then conditions on these latent documents together with the input to generate the output.”_ - from [1]

**How can RAG help?** The idea behind RAG is to improve a pretrained language model’s ability to access and use knowledge by connecting it with a non-parametric memory store—_typically a set of documents or textual data over which we can perform retrieval_; see below. Using this approach, we can dynamically retrieve relevant information from our datastore when generating output with the model. Not only does this approach provide extra (factual) context to the model, but it also allows us (i.e., the people using/training the model) to examine the results of retrieval and gain more insight into the LLM’s problem-solving process. In comparison, _the generations of a pretrained language model are largely a black box_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca7a1503-fd4f-4807-9dde-b7e275277b82_2194x736.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca7a1503-fd4f-4807-9dde-b7e275277b82_2194x736.png)

RAG integrates LLMs with a searchable knowledge base

The pretrained model in [1] is actually finetuned using this RAG setup. As such, the RAG strategy proposed in [1] is not simply an inference-time technique for improving factuality. Rather, _it is a general-purpose finetuning recipe that allows us to connect pretrained language models with external information sources_.

**Details on the setup.** Formally, RAG considers an input sequence `x` (i.e., the prompt) and uses this input to retrieve documents `z` (i.e., the text chunks), which are used as context when generating a target sequence `y`. For retrieval, authors in [1] use the [dense passage retrieval (DPR) model](https://arxiv.org/abs/2004.04906) [2][5](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-5-139244404), a pretrained [bi-encoder](https://cameronrwolfe.substack.com/i/140061921/adding-ai-into-a-search-engine) that uses separate BERT models to encode queries (i.e., query encoder) and documents (i.e., document encoder); see below. For generation, a pretrained [BART model](https://arxiv.org/abs/1910.13461) [3] is used. BART is an encoder-decoder (Seq2Seq) language model that is pretrained using a denoising objective[6](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-6-139244404). Both the retriever and the generator in [1] are based upon pretrained models, which makes finetuning optional—_the RAG setup already possesses the ability to retrieve and leverage knowledge via its pretrained components_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5f301d-8416-49c7-9092-43fbbc6e7995_1664x488.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5f301d-8416-49c7-9092-43fbbc6e7995_1664x488.png)

DPR bi-encoder setup (from [1])

The data used for RAG in [1] is a Wikipedia dump that is chunked into sequences of 100 tokens. The chunk size used for RAG is a hyperparameter that must be tuned depending upon the application. Each chunk is converted to a vector embedding using DPR’s pretrained document encoder. Using these embeddings, we can build an index for efficient vector search and retrieve relevant chunks when given a sequence of text (e.g., a prompt or message) as input.

**Training with RAG.** The dataset used to train the RAG model in [1] contains pairs of input queries and desired responses. When training the model in [1], we first embed the input query using the query encoder of DPR and perform a nearest neighbor search within the document index to return the `K` most similar textual chunks. From here, we can concatenate a textual chunk with the input query and pass this concatenated input to BART to generate an output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcbe81ec-459f-4bfc-9303-d4c03ea71e36_2044x908.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcbe81ec-459f-4bfc-9303-d4c03ea71e36_2044x908.png)

(from [1, 3])

The model in [1] only takes a single document as input when generating output with BART. As such, we must _marginalize_ over the top `K` documents when generating text, meaning that we predict a distribution over generated text using each individual document. In other words, we run a forward pass of BART with each of the different documents used as input. Then, we take a weighted sum over the model’s outputs (i.e., each output is a probability distribution over generated text) based upon the probability of the document used as input. This document probability is derived from the retrieval score (e.g., cosine similarity) of the document. In [1], two methods of marginalizing over documents are proposed:

- _RAG-Sequence_: the same document is used to predict each target token.
    
- _RAG-Token_: each target token is predicted with a different document.
    

At inference time, we can generate an output sequence using either of these approaches using a modified form of [beam search](https://en.wikipedia.org/wiki/Beam_search). To train the model, we simply use a [standard language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling) that maximizes the log probability of the target output sequence. Notably, the RAG approach proposed in [1] only trains the DPR query encoder and the BART generator, leaving the document encoder fixed. This way, we can avoid having to constantly rebuild the vector search index used for retrieval, which would be expensive.

**How does it perform?** The RAG formulation proposed in [1] is evaluated across a wide variety of knowledge-intensive NLP tasks. On these datasets, the RAG formulation is compared to:

- _Extractive methods_: operate by predicting an answer in the form of a span of text from a retrieved document.
    
- _Closed-book methods_: operate by generating an answer to a question without any associated retrieval mechanism.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0d1f9e7-c188-4e6b-8dbd-4d9cd5c152bb_2020x710.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0d1f9e7-c188-4e6b-8dbd-4d9cd5c152bb_2020x710.png)

(from [1])

As shown in the tables above, RAG sets new state-of-the-art performance on open domain question answering tasks (left table), outperforming both extractive and Seq2Seq models. Interestingly, RAG even outperforms baselines that use a cross-encoder-style retriever for documents. Compared to extractive approaches, RAG is more flexible, as questions can still be answered even when they are not directly present within any of the retrieved documents.

> _“RAG combines the generation flexibility of the closed-book (parametric only) approaches and the performance of open-book retrieval-based approaches.”_ - from [1]

On abstractive question answering tests, RAG achieves near state-of-the-art performance. Unlike RAG, baseline techniques are given access to a gold passage that contains the answer to each question, and many questions are quite difficult to answer without access to this information (i.e., necessary information might not be present in Wikipedia). Despite this deficit, RAG tends to generate responses that are more specific, diverse, and factually grounded.

#### Using RAG in the Age of LLMs

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)

The modern RAG pipeline

Although RAG was originally proposed in [1], this strategy—_with some minor differences_—is still heavily used today to improve the factuality of modern LLMs. The structure of RAG used for LLMs is shown within the figure above. The main differences between this approach and that of [1] are the following:

- Finetuning is optional and oftentimes not used. Instead, we rely upon the in context learning abilities of the LLM to leverage the retrieved data.
    
- Due to the large context windows present in most LLMs, we can pass several documents into the model’s input at once when generating a response[7](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-7-139244404).
    

Going further, the RAG approach in [1] uses purely vector search (with a bi-encoder) to retrieve document chunks. However, there is no reason that we have to use pure vector search! Put simply, _the document retrieval mechanism used for RAG is just a search engine_. So, we can apply everything we know about [AI-powered search](https://cameronrwolfe.substack.com/i/140061921/a-simple-framework-for-ai-powered-search) to craft the best RAG pipeline possible!

> _“Giving your LLM access to a database it can write to and search across is very useful, but it’s ultimately best conceptualized as giving an agent access to a search engine, versus actually having more memory.”_ - [source](https://blog.elicit.com/search-vs-vector-db/)

Within this section, we will go over more recent research that builds upon work in [1] and applies this RAG framework to modern, generative ([decoder-only](https://x.com/cwolferesearch/status/1640446111348555776?s=20)) LLMs. As we will see, RAG is highly impactful in this domain due to the emergent ability of LLMs to perform in context learning. Namely, _we can inject knowledge into an LLM by just including relevant information in the prompt_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7f70b0-6f38-42ba-ada0-15603b1560d1_1198x624.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7f70b0-6f38-42ba-ada0-15603b1560d1_1198x624.png)

(from [4])

**How Context Affects Language Models' Factual Predictions [4].** Pretrained LLMs have factual information encoded within their parameters, but there are limitations with leveraging this knowledge base—_pretrained LLMs tend to struggle with storing and extracting (or manipulating) knowledge in a reliable fashion_. Using RAG, we can mitigate these issues by injecting reliable and relevant knowledge directly into the model’s input. However, existing approaches—_including work in [1]_—use a supervised approach for RAG, where the model is directly trained to leverage this context. In [4], authors explore an unsupervised approach for RAG that leverages a pretrained retrieval mechanism and generator, finding that the benefit of RAG is still large when no finetuning is performed; see above.

> _“Supporting a web scale collection of potentially millions of changing APIs requires rethinking our approach to how we integrate tools.”_ - from [5]

**Gorilla: Large Language Models Connected with Massive APIs [5].** Combining language models with [external tools](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools) is a popular topic in AI research. However, these techniques usually teach the underlying LLM to leverage a small, fixed set of potential tools (e.g., a calculator or search engine) to solve problems. In contrast, authors in [5] develop a retrieval-based finetuning strategy to train an LLM, called Gorilla, to use over 1,600 different deep learning model APIs (e.g., from HuggingFace or TensorFlow Hub) for problem solving; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60243e28-c792-4262-8656-f123196af0dc_1616x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60243e28-c792-4262-8656-f123196af0dc_1616x1192.png)

(from [5])

First, the documentation for all of these different deep learning model APIs is downloaded. Then, a [self-instruct](https://arxiv.org/abs/2212.10560) [6] approach is used to generate a finetuning dataset that pairs questions with an associated response that leverages a call to one of the relevant APIs. From here, the model is finetuned over this dataset in a retrieval-aware manner, in which a pretrained information retrieval system is used to retrieve the documentation of the most relevant APIs for solving each question. This documentation is then passed into the model’s prompt when generating output, thus teaching the model to leverage the documentation of retrieved APIs when solving a problem and generating API calls; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)

(from [5])

Unlike most RAG applications, Gorilla is actually finetuned to better leverage its retrieval mechanism. Interestingly, such an approach allows the model to adapt to real-time changes in an API’s documentation at inference time and even enables the model to generate fewer hallucinations by leveraging relevant documentation.

**Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs [7].** In [7], authors study the concept of knowledge injection, which refers to methods of incorporating information from an external dataset into an LLM’s knowledge base. Given a pretrained LLM, the two basic ways that we can inject knowledge into this model are _i)_ finetuning (i.e., continued pretraining) and _ii)_ RAG.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F515d9b0e-c090-4481-9596-e562f56f13f5_1970x1208.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F515d9b0e-c090-4481-9596-e562f56f13f5_1970x1208.png)

(from [4])

We see in [4] that RAG far outperforms finetuning with respect to injecting new sources of information into an LLM’s responses; see below. Interestingly, combining finetuning with RAG does not consistently outperform RAG alone, thus revealing the impact of RAG on the LLM’s factuality and response quality.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41f636d-5662-400a-93fb-50356dd18b1a_1150x910.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc41f636d-5662-400a-93fb-50356dd18b1a_1150x910.png)

(from [4])

**RAGAS: Automated Evaluation of Retrieval Augmented Generation [8].** RAG is an effective tool for LLM applications. However, the approach is difficult to evaluate, as there are many dimensions of “performance” that characterize an effective RAG pipeline:

- The ability to identify relevant documents.
    
- Properly exploiting data in the documents via in context learning.
    
- Generating a high-quality, grounded output.
    

RAG is not just a retrieval system, but rather a multi-step process of finding useful information and leveraging this information to generate better output with LLMs. In [8], authors propose an approach, called Retrieval Augmented Generation Assessment (RAGAS), for evaluating these complex RAG pipelines without any human-annotated datasets or reference answers. In particular, three classes of metrics are used for evaluation:

1. _Faithfulness_: the answer is grounded in the given context.
    
2. _Answer relevance_: the answer addresses the provided question.
    
3. _Context relevance_: the retrieved context is focused and contains as little irrelevant information as possible.
    

Together, these metrics—_as claimed by authors in [8]_—holistically characterize the performance of any RAG pipeline. Additionally, we can evaluate each of these metrics in an automated fashion by prompting powerful foundation models like ChatGPT or GPT-4. For example, faithfulness is evaluated in [8] by prompting an LLM to extract a set of factual statements from the generated answer, then prompting an LLM again to determine if each of these statements can be inferred from the provided context; see below. Answer and context relevance are evaluated similarly (potentially with some added tricks based on embedding similarity[8](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-8-139244404)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc233aa7-9a64-4b08-8b3c-1955d3fad59b_1632x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc233aa7-9a64-4b08-8b3c-1955d3fad59b_1632x882.png)

Evaluating RAG faithfulness (from [8])

Notably, the RAGAS toolset is not just a paper. These tools, which are now quite popular among LLM practitioners, have been implemented and openly [released online](https://github.com/explodinggradients/ragas). The documentation of RAGAS tools is provided at the link below.

[RAGAS Docs](https://docs.ragas.io/en/stable/)

## Practical Tips for RAG Applications

Although a variety of papers have been published on the topic of RAG, this technique is most popular among practitioners. As a result, many of the best takeaways for how to successfully use RAG are hidden within blog posts, discussion forums, and other non-academic publications. Within this section, we will capture some of this domain knowledge by outlining the most important practical lessons of which one should be aware when building a RAG application.

#### RAG is a Search Engine!

When applying RAG in practical applications, we should realize that the retrieval pipeline used for RAG is [just a search engine](https://blog.elicit.com/search-vs-vector-db/)! Namely, the same retrieval and ranking techniques that have been used by search engines for years can be applied by RAG to find more relevant textual chunks. From this realization, there are several practical tips that can be derived for improving RAG.

**Don’t just use vector search.** Many RAG systems purely leverage dense retrieval for finding relevant textual chunks. Such an approach is quite simple, as we can just _i)_ generate an embedding for the input prompt and _ii)_ search for related chunks in our vector database. However, semantic search has a tendency to yield false positives and may have noisy results. To solve this, we should perform hybrid retrieval using a combination of vector and lexical search—_just like a normal (AI-powered) search engine_! The approach to vector search does not change, but we can perform a parallel lexical search by:

1. Extracting keywords from the input prompt[9](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-9-139244404).
    
2. Performing a lexical search with these keywords.
    
3. Taking a weighted combination of results from lexical/vector search.
    

By performing hybrid search, we make our RAG pipeline more robust and reduce the frequency of irrelevant chunks in the model’s context. Plus, adopting keyword-based search allows us to perform clever tricks like promoting documents with important keywords, excluding documents with negative keywords, or even augmenting documents with [synthetically-generated data](https://blog.vespa.ai/search-vespa-ai/) for better matching!

**Optimizing the RAG pipeline.** To improve our retrieval system, we need to collect metrics that allow us to evaluate its results similarly to any normal search engine. One way this can be done is by displaying the textual chunks used for certain generations to the end user similarly to a citation, such that the user can use the information retrieved by RAG to verify the factual correctness of the model’s output. As part of this system, we could then prompt the user to provide binary feedback (i.e., thumbs up or thumbs down) as to whether the information was actually relevant; see below. Using this feedback, we can evaluate the results of our retrieval system using traditional search metrics (e.g., [DGC or nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)), test changes to the system via AB tests, and iteratively improve our results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F936dbefa-1694-411e-82f9-10779ea7a7bf_1212x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F936dbefa-1694-411e-82f9-10779ea7a7bf_1212x636.png)

(from [17])

Evaluations for RAG must go beyond simply verifying the results of retrieval. Even if we retrieve the perfect set of context to include within the model’s prompt, the generated output may still be incorrect. To evaluate the generation component of RAG, the AI community relies heavily upon automated metrics such as RAGAS [8] or [LLM as a Judge](https://arxiv.org/abs/2306.05685) [9][10](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-10-139244404), which perform evaluations by prompting LLMs like GPT-4; see [here](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part) for more details. These techniques seem to provide reliable feedback on the quality of generated output. To successfully apply RAG in practice, however, it is important that we evaluate all parts of the end-to-end RAG system—_including both retrieval and generation_—so that we can reliably benchmark improvements that are made to each component.

**Improving over time.** Once we have built a proper retrieval pipeline and can evaluate the end-to-end RAG system, the last step of applying RAG is to perform iterative improvements using a combination of better models and data. There are a variety of improvements that can be investigated, including (but not limited to):

- Adding ranking to the retrieval pipeline, either using a cross-encoder or a hybrid model that performs both retrieval and ranking (e.g., [ColBERT](https://arxiv.org/abs/2004.12832) [10]).
    
- Finetuning the embedding model for dense retrieval over human-collected relevance data (i.e., pairs of input prompts with relevant/irrelevant passages).
    
- Finetuning the LLM generator over examples of high-quality outputs so that it learns to better follow instructions and leverage useful context.
    
- Using LLMs to augment either the input prompt or the textual chunks with extra synthetic data to improve retrieval.
    

For each of these changes, we can measure their impact over historical data in an offline manner. To truly understand whether they positively impact the RAG system, however, we should rely upon online AB tests that compare metrics from the new and improved system to the prior system in real-time tests with humans.

#### Optimizing the Context Window

Successfully applying RAG is not just a matter of retrieving the correct context—_prompt engineering plays a massive role_. Once we have the relevant data, we must craft a prompt that _i)_ includes this context and _ii)_ formats it in a way that elicits a grounded output from the LLM. Within this section, we will investigate a few strategies for crafting effective prompts with RAG to gain a better understanding of how to properly include context within a model’s prompt.

**RAG needs a larger context window.** During pretraining, an LLM sees input sequences of a particular length. This choice of sequence length during pretraining becomes the model’s [context length](https://cameronrwolfe.substack.com/i/117151147/the-context-window). Recently, we have seen a trend in AI research towards the creation of LLMs with longer context lengths[11](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-11-139244404). See, for example, [MPT-StoryWriter-65K](https://cameronrwolfe.substack.com/i/131642185/derivatives-of-mpt-b), [Claude-2.1](https://www.anthropic.com/news/claude-2-1), or [GPT-4-Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday), which have context lengths of 65K, 200K, and 128K, respectively. For reference, the Great Gatsby (i.e., an entire book!) [only contains ~70K tokens](https://medium.com/gopenai/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c). Although not all LLMs have a large context window, RAG requires a model with a large context window so that we can include a sufficient number of textual chunks in the model’s prompt.

**Maximizing diversity.** Once we’ve been sure to select an LLM with a sufficiently large context length, the next step in applying RAG is to determine how to select the best context to include in the prompt. Although the textual chunks to be included are selected by our retrieval pipeline, we can optimize our prompting strategy by adding a specialized [selection component](https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5)[12](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-12-139244404) that sub-selects the results of retrieval. _Selection does not change the retrieval process of RAG_. Rather, selection is added to the end of the retrieval pipeline—_after relevant chunks of text have already been identified and ranked_—to determine how documents can best be sub-selected and ordered within the resulting prompt.

One popular selection approach is a diversity ranker, which can be used to maximize the diversity of textual chunks included in the model’s prompt by performing the following steps:

1. Use the retrieval pipeline to generate a large set of documents that could be included in the model’s prompt.
    
2. Select the document that is most similar to the input (or query), as determined by embedding cosine similarity.
    
3. For each remaining document, select the document that is least similar to the documents that are already selected[13](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-13-139244404).
    

Notably, this strategy solely optimizes for the diversity of selected context, so it is important that we apply this selection strategy after a set of relevant documents has been identified by the retrieval pipeline. Otherwise, the diversity ranker would select diverse, but irrelevant, textual chunks to include in the context.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d47a3d-fa4a-48b4-a7dc-05f5c926ec46_1632x662.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d47a3d-fa4a-48b4-a7dc-05f5c926ec46_1632x662.png)

Lost in the middle selection for RAG

**Optimizing context layout.** Despite increases in context lengths, recent research indicates that LLMs struggle to capture information in the middle of a large context window [11]. Information at the beginning and end of the context window is captured most accurately, causing certain data to be “lost in the middle”. To solve this issue, we can adopt a selection strategy that is more mindful of where context is placed in the prompt. In particular, we can take the relevant textual chunks from our retrieval pipeline and iteratively place the most relevant chunks at the beginning and end of the context window; see below. Such an approach avoids inserting textual chunks in order of relevance, choosing instead to place the most relevant chunks at the beginning and end of the prompt.

#### Data Cleaning and Formatting

In most RAG applications, our model will be retrieving textual information from many different sources. For example, an assistant that is built to discuss the details of a codebase with a programmer may pull information from the code itself, documentation pages, blog posts, user discussion threads, and more. In this case, the data being used for RAG has a variety of different formats that could lead to artifacts (e.g., logos, icons, special symbols, and code blocks) within the text that have the potential to confuse the LLM when generating output. In order for the application to function properly, we must extract, clean, and format the text from each of these heterogenous sources. Put simply, _there’s a lot more to preprocessing data for RAG than just splitting textual data into chunks_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb70d46d6-188f-4191-97e2-3f02aa9b10e3_2294x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb70d46d6-188f-4191-97e2-3f02aa9b10e3_2294x576.png)

(from [12])

**Performance impact.** If text is not extracted properly from each knowledge source, the performance of our RAG application will noticeably deteriorate! On the flip side, cleaning and formatting data in a standardized manner will noticeably improve performance. As shown in [this blog post](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part), investing into proper data preprocessing for RAG has several benefits (see above):

- 20% boost in the correctness of LLM-generated answers.
    
- 64% reduction in the number of tokens passed into the model[14](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-14-139244404).
    
- Noticeable improvement in overall LLM behavior.
    

> _“We wrote a quick workflow that leveraged LLM-as-judge and iteratively figured out the cleanup code to remove extraneous formatting tokens from Markdown files and webpages.”_ - from [12]

**Data cleaning pipeline.** The details of any data cleaning pipeline for RAG will depend heavily upon our application and data. To craft a functioning data pipeline, we should _i)_ observe large amounts of data within our knowledge base, _ii)_ visually inspect whether unwanted artifacts are present, and _iii)_ amend issues that we find by adding changes to the data cleaning pipeline. Although this approach isn’t flashy or cool, any AI/ML practitioner knows that 90% of time building an application will be spent observing and working with data.

If we aren’t interested in manually inspecting data and want a sexier approach, we can automate the process of creating a functional data preprocessing pipeline by using LLM-as-a-Judge [9] to iteratively construct the code for cleaning up and properly formatting data. Such an approach was recently shown to retain useful information, remove formatting errors, and drastically reduce the average size of documents [12]. See [here](https://gist.github.com/suanmiao/7b1d82dfb94d457a8d521f3cc35613f5) for the resulting data preprocessing script and below for an example of a reformatted document after cleanup.

[

![Data Cleaning](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5da3ccdb-02cc-47a7-b948-40ade66f5c06_1200x601.jpeg "Data Cleaning")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5da3ccdb-02cc-47a7-b948-40ade66f5c06_1200x601.jpeg)

Textual chunk before and after data cleaning (from [12])

#### Further Practical Resources for RAG

As previously mentioned, some of the best resources for learning about RAG are not published within academic journals or conferences. There are a variety of blog posts and practical write ups that have helped me to gain insight for how to better leverage RAG. Some of the most notable resources are outlined below.

- What is Retrieval Augmented Generation? [[link](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)]
    
- Building RAG-based LLM Applications for Production [[link](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)]
    
- Best Practices for LLM Evaluation of RAG Applications [[link](https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part)]
    
- Building Conversational Search with RAG at Vespa [[link](https://blog.vespa.ai/search-vespa-ai/)]
    
- RAG Finetuning with Ray and HuggingFace [[link](https://huggingface.co/blog/ray-rag)]
    

## Closing Thoughts

At this point, we should have a comprehensive grasp of RAG, its inner workings, and how we can best approach building a high-performing LLM application using RAG. Both the concept and implementation of RAG are simple, which—_when combined with its impressive performance_—is what makes the technique so popular among practitioners. However, successfully applying RAG in practice involves more than putting together a minimal functioning pipeline with pretrained components. Namely, we must refine our RAG approach by:

1. Creating a high-performing hybrid retrieval algorithm (potentially with a re-ranking component) that can accurately identify relevant textual chunks.
    
2. Constructing a functional data preprocessing pipeline that properly formats data and removes harmful artifacts before the data is used for RAG.
    
3. Finding the correct prompting strategy that allows the LLM to reliably incorporate useful context when generating output.
    
4. Putting detailed evaluations in place for both the retrieval pipeline (i.e., using traditional search metrics) and the generation component (using RAGAS or LLM-as-a-judge [8, 9]).
    
5. Collecting data over time that can be used to improve the RAG pipeline’s ability to discover relevant context and generate useful output.
    

Going further, creating a robust evaluation suite allows us to improve each of the components listed above by quantitatively testing (via offline metrics or an AB test) iterative improvements to our RAG pipeline, such as a modified retrieval algorithm or a finetuned component of the system. As such, our approach to RAG should mature (and improve!) over time as we test and discover new ideas.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." _Advances in Neural Information Processing Systems_ 33 (2020): 9459-9474.

[2] Karpukhin, Vladimir, et al. "Dense passage retrieval for open-domain question answering." _arXiv preprint arXiv:2004.04906_ (2020).

[3] Lewis, Mike, et al. "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension." _arXiv preprint arXiv:1910.13461_ (2019).

[4] Petroni, Fabio, et al. "How context affects language models' factual predictions." _arXiv preprint arXiv:2005.04611_ (2020).

[5] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." _arXiv preprint arXiv:2305.15334_ (2023).

[6] Wang, Yizhong, et al. "Self-instruct: Aligning language model with self generated instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[7] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." _arXiv preprint arXiv:2312.05934_ (2023).

[8] Es, Shahul, et al. "Ragas: Automated evaluation of retrieval augmented generation." _arXiv preprint arXiv:2309.15217_ (2023).

[9] Zheng, Lianmin, et al. "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena." _arXiv preprint arXiv:2306.05685_ (2023).

[10] Khattab, Omar, and Matei Zaharia. "Colbert: Efficient and effective passage search via contextualized late interaction over bert." _Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval_. 2020.

[11] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." _arXiv preprint arXiv:2307.03172_ (2023).

[12] Leng, Quinn, el al. “Announcing MLflow 2.8 LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications, Part 2.” https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part (2023).

[13] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.

[14] Wang, Yan, et al. "Enhancing recommender systems with large language model reasoning graphs." _arXiv preprint arXiv:2308.10835_ (2023).

[15] Zhou, Chunting, et al. "Lima: Less is more for alignment." _arXiv preprint arXiv:2305.11206_ (2023).

[16] Guu, Kelvin, et al. "Retrieval augmented language model pre-training." _International conference on machine learning_. PMLR, 2020.

[17] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[1](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-1-139244404)

Interestingly, in context learning is an emergent capability of LLMs, meaning that it is most noticeable in larger models. In context learning ability was first demonstrated by the impressive few-shot learning capabilities of [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [13].

[2](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-2-139244404)

In nearly all cases, we will use an encoder-only embedding model (e.g., [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert), [sBERT](https://cameronrwolfe.substack.com/i/140061921/sentence-bert-sentence-embeddings-using-siamese-bert-networksextensions-of-sbert), [ColBERT](https://arxiv.org/abs/2004.12832), etc.) for vector search. However, recent research has indicated that decoder-only models (i.e., the architecture used for most modern, generative LLMs) can produce high-quality embeddings as well!

[3](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-3-139244404)

We can also explore other ways of adding context to the query, such as by creating a more generic prompt template.

[4](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-4-139244404)

For more information, check out recent research on the [reversal curse and knowledge manipulation](https://cameronrwolfe.substack.com/i/137603020/physics-of-language-models-part-knowledge-storage-and-extraction-and-part-knowledge-manipulation) within LLMs. These models oftentimes struggle to perform even simple manipulations (e.g., reversal) of factual relationships within their knowledge base.

[5](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-5-139244404)

The original RAG paper purely uses vector search (with a bi-encoder) to retrieve relevant documents.

[6](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-6-139244404)

The denoising objective used by BART considers several perturbations to the original sequence of text, such as token masking/deletion, masking entire sequences of tokens, permuting sentences in a document, or even rotating a sequence about a chosen token. Given the permuted input, the BART model is trained to reconstruct the original sequence of text during pretraining.

[7](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-7-139244404)

The number of textual chunks that we actually pass into the model’s prompt is dependent upon several factors, such as _i)_ the model’s context window, _ii)_ the chunk size, and _iii)_ the application we are solving.

[8](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-8-139244404)

Context relevance follows a simple approach of prompting an LLM to determine whether sentences from the retrieved context are actually relevant or not. For answer relevance, however, we prompt an LLM to generate potential questions associated with the generated answer, then we take the average cosine similarity between the embeddings of these questions and the actual question as the final score.

[9](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-9-139244404)

This can be done via traditional query understanding techniques, or we can simply prompt an LLM to generate a list of keyword associated with the input.

[10](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-10-139244404)

LLMs can effectively evaluate unstructured outputs (semi-)reliably and at a low cost. However, human feedback remains the gold standard for evaluating an LLM’s output.

[11](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-11-139244404)

Plus, there has been a ton of research on extending the context length of existing, pretrained LLMs or making them more capable of handling longer inputs; e.g., [ALiBi](https://arxiv.org/abs/2108.12409), [RoPE](https://arxiv.org/abs/2104.09864), [Self Extend](https://cameronrwolfe.substack.com/i/140501286/llm-maybe-longlm-self-extend-llm-context-window-without-tuning), [LongLoRA](https://arxiv.org/abs/2309.12307), and more.

[12](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-12-139244404)

Here, I call this step “selection” rather than ranking as to avoid confusion with re-ranking within search, which sorts documents based on textual relevance. Selection refers to the process of deciding the order of documents as they are inserted into the model’s prompt, and textual relevance is assumed to already be known at this step.

[13](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-13-139244404)

This is a greedy approach for selecting the most diverse subset of documents. The resulting set is not optimal in terms of diversity, but this efficient approximation does a good job of constructing a diverse set of documents in practice.

[14](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval#footnote-anchor-14-139244404)

The cost reduction is due to a reduction in the average size of textual chunks after artifacts and unnecessary components are removed from the text.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Shaun Gittens's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb10e6094-7b07-4110-88fc-5823168675df_144x144.png)



](https://substack.com/profile/1280316-shaun-gittens)

[

![Daniil Belazovschi's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b25a6a5-4f56-43fd-b7e4-c09892b4f3ff_96x96.jpeg)



](https://substack.com/profile/42391753-daniil-belazovschi)

[

![darlin's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29e363b8-0457-4b09-b998-d7799cc67d0d_2265x2265.png)



](https://substack.com/profile/127381235-darlin)

[

![CarolG's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0ce15ba5-d6b5-481f-bc45-6850d98197e0_144x144.png)



](https://substack.com/profile/4041140-carolg)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

81 Likes∙

[8 Restacks](https://substack.com/note/p-139244404/restacks?utm_source=substack&utm_content=facepile-restacks)

81

- 

[

4

](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comments)

8

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sahar Mor's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa06b2072-0444-44f7-8106-7892097e4128_1690x1762.png)



](https://substack.com/profile/3770805-sahar-mor?utm_source=comment)

[Sahar Mor](https://substack.com/profile/3770805-sahar-mor?utm_source=substack-feed-item)

[AI Tidbits](https://www.aitidbits.ai/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年2月6日](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comment/48885808 "2024年2月6日 01:10")

Liked by Cameron R. Wolfe, Ph.D.

Such a great write-up. The more accessible RAG is, the more widespread the adoption of LLMs becomes. For example, chunking and preprocessing, which remain manual steps that many aren't experienced with, can be semi-automated based on the task at hand. A superior chunking method can lead to a double-digit increase in performance.

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comment/48885808)

[

![Phil's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fpurple.png)



](https://substack.com/profile/240816060-phil?utm_source=comment)

[Phil](https://substack.com/profile/240816060-phil?utm_source=substack-feed-item)

[5月31日](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comment/57762292 "2024年5月31日 14:20")

Liked by Cameron R. Wolfe, Ph.D.

Thank you so much for this deep dive, very informative! I'm a UX researcher that's new to conversational AI design (no ML practitioner, so please bare with me!) and for our LLM-powered voice assistant and chatbot, RAG has obviously become immensely important. I was wondering, when it comes to preprocessing data and curating a knowledge base for RAG, do the data for the external knowledge base need to be formatted/"cleaned" in a specific way? For example, is it sufficient to simply use links to a website and embedded PDFs/documents are then automatically included in the knowledge base, or would one need to extract and separately add the contents of the PDFs/documents to the database?

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comment/57762292)

[2 more comments...](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Dolma, OLMo, and the Future of Open-Source LLMs

### How making open-source LLMs truly open will change AI research for the better...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Feb 20, 2024

29

- 

[

2

](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open/comments)

2

Share

This newsletter is presented by [Rebuy](https://www.rebuyengine.com/). If you like the newsletter, feel free to subscribe below, [get in touch](https://cameronrwolfe.me/), or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/).

Subscribe

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beb6967-91b7-4dce-9b76-368bbe56f18f_2334x1350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beb6967-91b7-4dce-9b76-368bbe56f18f_2334x1350.png)

(from [1, 24, 27])

Although (large) language models (LLMs) have been a topic of interest within the AI research community for years, the proposal of ChatGPT drastically increased the commercial viability of (and amount of public interest in) this technology. As a result, most research on the topic of LLMs quickly became proprietary, _despite the massive emphasis AI researchers had placed on transparency and open-source for years_. Due to the massive role that open collaboration plays in technological progress, certain researchers responded to this newfound proprietary nature of AI research by beginning to replicate LLM research advancements in an open-source fashion, as documented in the overviews below:

- The History of Open-Source LLMs: Early Days [[link](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-early)]
    
- The History of Open-Source LLMs: Better Base Models [[link](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better)]
    
- The History of Open-Source LLMs: Imitation and Alignment [[link](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)]
    

For quite some time, the quality of open-source LLMs lagged behind their proprietary counterparts. Recently, however, the gap between open-source and proprietary LLMs has been significantly reduced. As a result, open-source LLM research has become more mainstream and is being emphasized heavily.

> _“As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.”_ - from [1]

The increasing popularity of open-source LLMs is widely considered to be beneficial. However, there is one problem: _the definition of open-source varies drastically between models_. In many cases, open-source LLMs simply release their model parameters and inference code, while obfuscating other important details like the contents of the pretraining dataset, training hyperparameters, evaluation strategy and more. Plus, many open-source LLMs come with restrictive licenses that may forbid commercial use, the construction of synthetic datasets, and more.

**This overview.** Here, we will overview the recently-released Dolma dataset [1] and OLMo [10] suite of LLMs. Unlike prior work in open-source LLMs, Dolma and OLMo are completely transparent and open-source. The process of constructing the Dolma pretraining corpus, which is released under the [AI2 impact license](https://allenai.org/impact-license), is fully-documented in [1]. Plus, authors release code and tooling for re-building the dataset from scratch. Similarly, OLMo is released under an Apache-2.0 license along with all training data, training/evaluation/adaptation code, checkpoints, training logs, and more. Dolma and OLMo take a massive step towards demystifying the LLM preparing process by _i)_ releasing all relevant resources and _ii)_ analyzing any and all choices made in the process of training OLMo.

## [Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research](https://arxiv.org/abs/2402.00159) [1]

> _“The most powerful language models are built by a few organizations who withhold most model development details_[1](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-1-141461162)_… the composition of language model pretraining data is often vaguely stated, even in cases where the model itself is released for public use.”_ - from [1]

Although LLMs are now commonly used for a variety of natural language processing (NLP) tasks, most of these models’ specifications are kept hidden from the practitioners. For example, we rarely have any information regarding the composition of an LLM’s pretraining dataset—_even “open” models (e.g., [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [2]) do not release information about their data or how the model can be reproduced_! The details of constructing a pretraining dataset for an LLM are largely a black box, which makes performing research on pretraining data and its impact on the downstream LLM quite difficult for those outside of a few top LLM labs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5d08aa5-15ae-446e-b855-616fb577e15b_1608x872.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5d08aa5-15ae-446e-b855-616fb577e15b_1608x872.png)

(from [1])

In [1], authors from [AI2](https://allenai.org/) solve this problem by releasing Data for Open Language Models’ Appetite (Dolma), a fully-open pretraining dataset containing over three trillion tokens of English text[2](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-2-141461162). This dataset is collected from a variety of sources, including web content, scientific papers, code, public-domain books, social media posts, and encyclopedic materials; see above. The process of constructing this dataset—_including design principles and the empirical reasoning behind each decision_—is documented in [1]. Additionally, the authors release a full toolkit for efficiently reproducing and modifying the resulting dataset. As a result, Dolma enables researchers to better study the impact of pretraining data composition on the capabilities and limitations of the downstream language models.

- Dolma Toolkit [[link](https://github.com/allenai/dolma)]
    
- Dolma Dataset [[link](https://huggingface.co/datasets/allenai/dolma)]
    

**Dolma design principles.** The end goal of creating the Dolma dataset is to _i)_ create the largest (to date) open—_meaning that both the data and the process for curating the data are shared_—pretraining corpus and _ii)_ use this pretraining corpus to train an open language model, called Open Language Model (OLMo). To create this corpus, authors in [1] aim to pull from data sources that are commonly used in prior work, as well as process the data in a way that aligns with existing best practices. In areas where implementations differ or best practices are unknown, an empirical approach is adopted, whereby smaller variants of OLMo are trained and evaluated to determine the downstream impact of changes to the pretraining dataset. To minimize risk of harm to individuals, one can easily request—_using [this form](https://docs.google.com/forms/d/e/1FAIpQLSfL6KzFR7xNJj6MPyV1uikIpj-VmrftC9mjty2nXzSClU2rnw/viewform)_—for their data to be removed from the Dolma corpus.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F574c3aa5-ac58-4487-a591-8a32399979f8_1740x884.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F574c3aa5-ac58-4487-a591-8a32399979f8_1740x884.png)

Observing that many LLMs can be improved by pretraining over more data, authors in [1] aim to make Dolma sufficiently large. The impact of data size on model performance was analyzed by the [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms) language model [3], where authors showed that larger models must be pretrained over sufficiently large pretraining datasets to truly optimize their performance. However, _even recent LLMs could usually be improved by pretraining over more data_! As shown above, for example, LLaMA-2 models have not yet fully converged after pretraining over 2T tokens. As such, authors in [1] aim to make Dolma sufficiently large (i.e., 3T tokens) to facilitate further studies on pretraining dataset size.

#### Prior Open Pretraining Datasets

> _“Lack of access to pretraining corpora alongside corresponding language models has been a major obstacle for the broader research community.”_ - from [1]

Despite the massive popularity of LLMs, information about their pretraining corpora is rarely discussed. Even “open” LLMs usually avoid releasing the datasets on which they are trained, or even a recipe for reproducing this dataset. As such, Dolma takes a large leap towards pretraining transparency by openly releasing a full-size pretraining corpus, along with the tools needed to reproduce it. As we will see in this section, however, Dolma is not the only effort that has tried to shed light on the process of constructing pretraining data for LLMs.

**Colossal Clean Crawled Corpus (C4) [6].** This pretraining dataset was originally constructed for the T5 model; see [here](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) for more details. The data is sourced from [Common Crawl](https://commoncrawl.org/), and the full dataset contains about 750Gb of text in total. We see in [6] that authors choose to create the C4 dataset from scratch due to the lack of high-quality, publicly-available pretraining datasets for language models. C4 is limited in scale compared to the pretraining datasets that are used by modern LLMs. Nonetheless, many models (e.g., [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher), [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms), [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact), and more) use C4 as a subset of their pretraining data, as the quality of data is quite high.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F001280d5-6e00-40e8-bcef-80c450309a5f_1754x1044.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F001280d5-6e00-40e8-bcef-80c450309a5f_1754x1044.png)

(from [18])

**Responsible Open-science Open-collaboration Text Sources (ROOTS) [17].** The ROOTS corpus was developed as part of the [BigScience](https://bigscience.huggingface.co/) research initiative for training [BLOOM](https://cameronrwolfe.substack.com/i/135273362/bloom-an-open-multilingual-language-model) [18]—_one of the early proposals in the space of open-source language models_. ROOTS is a multilingual dataset that spans over 46 natural languages and 13 programming languages; see above. The dataset is comprised of over 1.6Tb of text in total (i.e., twice the size of C4). However, the English portion of this dataset is, again, limited in scale—_too small for training English-only LLMs_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d20b6ed-0afe-4790-b206-2244a35c2e6c_1566x1032.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d20b6ed-0afe-4790-b206-2244a35c2e6c_1566x1032.png)

(from [19])

**The Pile [19]** is a 825 Gb pretraining corpus for LLMs that was created by combining 22 existing datasets for natural language processing; see above. At the time of publishing, code used to construct the Pile was also [released publicly](https://pile.eleuther.ai/). Similar to C4, the Pile is a high-quality dataset that is limited in scale, and it is used as a subset of the pretraining dataset for many LLMs (e.g., [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15) and [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ec6621-b166-46b9-b3cd-31c3c5b5099b_1644x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F09ec6621-b166-46b9-b3cd-31c3c5b5099b_1644x784.png)

(from [20])

**RefinedWeb [20]** is a pretraining corpus that was curated to train the [Falcon LLMs](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source). The corpus is comprised of web data that undergoes a purely heuristic-based filtering pipeline (i.e., no model-based filtering). RefinedWeb is composed primarily of web data, demonstrating that high-quality pretraining datasets can be constructed without using curated sources of data. Interestingly, we see that LLMs trained over RefinedWeb perform quite favorably despite relying heavily upon web data; see above. The full corpus is very large—_5 trillion tokens in total_! However, only a small portion of the data (600B tokens) is released to the public, so the openly-available portion of RefinedWeb is relatively small.

**RedPajama [21, 22]** is an initiative (led by [Together AI](https://www.together.ai/)) to reproduce leading open-source LLMs and release them to the public with a more permissive license. As a first step in this direction, authors recreated the pretraining dataset for LLaMA (see [here](https://www.together.ai/blog/redpajama) for more details), resulting in the RedPajama v1 and v2 datasets. Despite being similar to Dolma in nature, RedPajama has a more limited scope. Namely, the creators of RedPajama are trying to reproduce LLaMA, while Dolma attempts to provide a transparent resource that empowers others to study all notable aspects of LLM pretraining. As such, Dolma pulls from a variety of additional data sources (e.g., scientific papers, code, conversational forums, and more).

#### Creating the Dolma Corpus

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f55035a-9496-40b7-bc25-12c80860e60d_1966x1250.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f55035a-9496-40b7-bc25-12c80860e60d_1966x1250.png)

(from [1])

The process of constructing a pretraining dataset for an LLM consists of three primary components:

1. Acquisition: obtaining content from a variety of sources.
    
2. _Cleanup_: using heuristics and model-based techniques to filter the data.
    
3. _Mixing_: deduplication and up/down-sampling of data sources.
    

To acquire data, authors in [1] use commonly used data sources for pretraining LLMs. For example, web data is derived from [Common Crawl](https://commoncrawl.org/), code is derived from [The Stack](https://huggingface.co/datasets/bigcode/the-stack), and conversational forums are derived from the [Pushshift Reddit Dataset](https://arxiv.org/abs/2001.08435). Other sources of pretraining data include [C4](https://huggingface.co/datasets/c4) (web data), [PeS2o](https://github.com/allenai/peS2o) (academic literature), [Project Gutenberg](https://www.gutenberg.org/) (books), and Wikipedia (encyclopedic content).

> _“We create a high-performance toolkit to facilitate efficient processing on hundreds of terabytes of text content. The toolkit is designed for high portability: it can run any platform from consumer hardware to a distributed cluster environment”_ - from [1]

To process data for pretraining, we rely upon four primary transformations:

1. Language filtering.
    
2. Quality filtering.
    
3. Content filtering.
    
4. Deduplication.
    

Each of these steps are implemented within the Dolma toolkit. We will now go over each of these transformations and how they are implemented for different data sources. In [1], several variations of these transformations are typically implemented and tested by comparing the downstream performance of a 1B parameter OLMo model pretrained on a subset (i.e., 150B tokens) of Dolma. However, we can also evaluate different choices for the data preprocessing pipeline by manually inspecting the resulting data[3](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-3-141461162) with tools like [WIMBD](https://wimbd.apps.allenai.org/)!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadebdc14-ad87-4307-bda4-5421d688a42f_942x622.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadebdc14-ad87-4307-bda4-5421d688a42f_942x622.png)

(from [34])

**What is fastText?** Within this section, we will see several mentions of fastText classifiers being used to filter pretraining data for LLMs. [fastText](https://fasttext.cc/) is a free and open-source library that can be used to learn text representations or train lightweight text classifiers. As documented in [34], fastText classifiers have a simple architecture that takes an average of word representations within a sentence, then passes the result into a linear classifier; see above. These classifiers perform well—_even similarly to deep learning-based classifiers in certain cases_. We can further improve the quality of fastText models by incorporating [n-gram](https://en.wikipedia.org/wiki/N-gram) representations along with default unigram word representations.

> _“We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.”_ - from [34]

Most importantly, fastText classifiers are fast to train and evaluate. Due to the simple model architecture, inference is lightweight and training can be executed efficiently via (asynchronous) distribution across CPUs. As such, fastText classifiers are ideal for large-scale data processing applications, such as filtering a pretraining dataset for an LLM. In fact, the CCNet data pipeline [35]—_a popular reference architecture for creating high-quality, deduplicated, and monolingual text datasets from web data_—uses a fastText classifier for language identification.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab16e03-6cde-41e4-8acd-f27207bf2a3e_788x512.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ab16e03-6cde-41e4-8acd-f27207bf2a3e_788x512.png)

Language filtering in [1] for web text (left), code (middle), and conversational data (right)

**Language filtering.** The Dolma dataset is English-only[4](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-4-141461162), which means that we must implement a tool for filtering non-English content from acquired data. To do this, we typically use a lightweight classifier—_authors in [1] use the CCNet pipeline_[5](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-5-141461162) _with a fastText language identification model_—to predict the primary language of each document[6](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-6-141461162). Then, we can set a threshold that keeps only documents with a sufficiently-high English score. Typically, this language filtering component, which is applied to both web and conversation data in Dolma, eliminates a large amount of source data (e.g., 61.7% of web data is removed). However, language filtering is model-based and, therefore, imperfect—_a certain amount of non-English data will always be present_.

> _“Language filtering is never perfect, and multilingual data is never completely removed from pretraining corpora.”_ - from [1]

For the code portion of Dolma, authors in [1] implement a “language filtering” component that removes `json` and `csv` files from the corpus. Such a strategy eliminates large, data-heavy files from the corpus that are not useful for training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea9b0c20-95c1-46bf-b4e0-76fbca040fd7_896x490.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea9b0c20-95c1-46bf-b4e0-76fbca040fd7_896x490.png)

Quality filtering in [1] for web text (left), code (middle), and conversational data (right)

**Quality filtering** refers to the process of removing “low quality” text from a pretraining corpus. However, there is a lot of contention within the research community about the definition of low quality text and how it can best be filtered from an LLM’s pretraining corpus. In particular, there is an active debate about whether quality filters should rely solely upon heuristics (e.g., [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher) [3] and [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [4] adopt this approach) or if machine learning models should be used for quality filtering (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) [5] filters low quality content using [n-gram language models](https://en.wikipedia.org/wiki/Word_n-gram_language_model) and simplistic classification models). In [1], authors choose to avoid model-based filtering techniques and rely solely upon heuristic filtering methods. For web text, authors find that combining filtering techniques from Gopher [3] and C4 [6] yields the best results; see the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86b9f648-f88e-4e4e-943e-8a37b98551c1_1526x1204.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86b9f648-f88e-4e4e-943e-8a37b98551c1_1526x1204.png)

(from [1])

Even after heuristic quality filtering is performed, we see in [1] that web text still contains a variety of repeated n-gram sequences; see the table above. These sequences, which often serve as webpage layout elements, are relatively uncommon in the pretraining corpus. However, authors in [1] mention that these sequences must nonetheless be removed to avoid [loss spikes](https://arxiv.org/abs/2312.16903) during training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfe16c85-f8de-46f3-b948-738373d25502_2368x754.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfe16c85-f8de-46f3-b948-738373d25502_2368x754.png)

Quality filtering heuristics used in [1]

A simpler set of heuristics is adopted for filtering conversational data, whereby authors filter conversations based on _i)_ length, _ii)_ community votes, and _iii)_ flags from moderators and the community for each conversation. For code data, Dolma uses a combination of quality filters from [RedPajama v1](https://www.together.ai/blog/redpajama) and StarCoder [7] that removes data and templated code from the corpus, as well as code that is derived from unpopular repositories, has a bad ratio of comments to code, and more. The full set of quality filtering heuristics used by Dolma are listed above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1de11e09-c6ec-4242-ae1b-e8f96166bfa2_886x510.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1de11e09-c6ec-4242-ae1b-e8f96166bfa2_886x510.png)

Content filtering in [1] for web text (left), code (middle), and conversational data (right)

**Content filtering** focuses upon removing harmful text—_primarily toxic content and personally identifiable information (PII)_—from the pretraining dataset. To identify toxic content, authors in [1] train a pair of fastText classifiers, which are then used to tag (and remove) spans of toxic text, to classify hateful and not-safe-for-work (NSFW) content based on the [Jigsaw Toxic Comments dataset](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04bbd4ac-ebf2-466c-af88-34b8c0e74545_1872x856.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04bbd4ac-ebf2-466c-af88-34b8c0e74545_1872x856.png)

(from [1])

In [1], we see that authors adopt a conservative threshold for removing toxic text—_a text span must be classified as toxic with a relatively high probability for it to be removed_. This approach yields slightly degraded downstream performance (shown above), but it is adopted to avoid removing too much data from the corpus.

To detect PII, a series of regular expression are adopted to find spans of text corresponding to email addresses, IP addresses, and phone numbers. From here, these spans of text are either masked, or the entire document is removed from the corpus (if more than 5 pieces of PII are detected in a single document). For code data, authors use [extra tools](https://pypi.org/project/detect-secrets/) to detect and remove code secrets from the data.

> _“Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy”_ - from [9]

**Deduplication.** Recent research has shown that deduplicating an LLM’s pretraining dataset makes the model’s training more data efficient[7](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-7-141461162) [9]. As such, crafting a robust (and efficient) deduplication pipeline is an incredibly important aspect of building pretraining corpora for LLMs. In [1], authors perform three stages of deduplication for web text:

1. _URL-based_: eliminates web pages that are scraped multiple times.
    
2. _Document-based_: eliminates pages that contain exactly the same text.
    
3. _Paragraph-based_: eliminates individual paragraphs with the same text.
    

All stages are implemented efficiently by using a [Bloom filter](https://systemdesign.one/bloom-filters-explained/). For other sources of data beyond web data (e.g., code or conversational data), a similar strategy is used for deduplication, though paragraph-level deduplication may be unnecessary for sources with shorter documents. URL and document-based deduplication are typically performed first due to their efficiency, while paragraph-based deduplication is saved for the later stages of the dataset construction process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7ecd23d-ee22-4375-afb6-0b999012bc72_1614x688.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7ecd23d-ee22-4375-afb6-0b999012bc72_1614x688.png)

(from [1])

**Putting it all together.** When constructing Dolma, authors in [1] perform preprocessing steps in a specific order to ensure efficiency. Namely, URL and document-level deduplication are performed first, followed by quality and content filtering, while paragraph-level deduplication is performed last. As shown in the figure above, this combination of preprocessing steps yields an LLM that achieves the best performance on downstream tasks. Compared to other pretraining corpora that are available, Dolma offers a larger pool of tokens at comparable quality and equal diversity in data composition.

## **[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) [10]**

> _“We believe that a large, diverse population of open models is the best hope for scientific progress on understanding language models and engineering progress on improving their utility.”_ - from [10]

OLMo is a set of truly open LLMs that are pretrained on Dolma. OLMo models match the performance of state-of-the-art open LLMs and can be used to more deeply study the science of language modeling. The OLMo suite of models are completely open (i.e., [Apache 2.0 license](https://www.planetcrust.com/what-does-apache-2-0-license-mean)) and come with a variety of artifacts:

- Model weights [[link](https://huggingface.co/allenai/OLMo-7B)]
    
- Training code [[link](https://github.com/allenai/OLMo)]
    
- Evaluation code [[link](https://github.com/allenai/OLMo)]
    
- Adaptation code [[link](https://github.com/allenai/open-instruct)]
    
- Training logs [[link](https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5)]
    

OLMo is comprised of five models in total—_four 7B models and one 1B model_. Plus, authors in [10] openly publish checkpoints for these models that are recorded every 1K training iterations. The combination of OLMo and Dolma allows us to deeply analyze the relationship between pretraining data and LLM performance—_a poorly understood and rarely analyzed topic (at least openly) within the AI community_.

**Prior “open” LLMs.** OLMo models are truly open, as they release their data, training/evaluation code, model weights, inference code, and more. In contrast, most “open” LLMs only release model weights and inference code. Plus, these resources might be associated with a relatively restrictive license (e.g., LLaMA-2 uses a [custom license](https://ai.meta.com/llama/license/) instead of Apache 2.0). As such, the openness of open LLMs tends to lie somewhere on a spectrum (see [here](https://github.com/eugeneyan/open-llms) for an itemized list):

- [Mistral](https://mistral.ai/news/announcing-mistral-7b/) [36] provides model weights and a brief report.
    
- [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [2] provides detailed alignment instructions but minimal information about the pretraining dataset.
    
- [MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact) [31] provides detailed instructions about constructing the model’s pretraining dataset, but does not actually release the data.
    
- [Falcon](https://cameronrwolfe.substack.com/p/falcon-the-pinnacle-of-open-source) [4] releases a partial subset of the model’s pretraining data, along with a report on the model and data.
    
- [BLOOM](https://cameronrwolfe.substack.com/i/135273362/bloom-an-open-multilingual-language-model) [18] releases training code, model checkpoints, and training data, but the license is restrictive.
    

In [10], authors take inspiration from the design choices of LLMs outlined above. Going further, they aim to create a truly open LLM—_OLMo is released with an Apache 2.0 license and Dolma is released with [AI2’s impact license](https://allenai.org/impact-license)_—that adopts the best practices of these models and provides more insight into the intricate details of their creation. Currently, the model that holds the most similarity to OLMo is [LLM360](https://www.llm360.ai/), which targets a similar goal of releasing a truly open LLM.

#### The Evaluation Process

OLMo models are evaluated using two different techniques: _perplexity_ and _downstream task_ evaluation. In this section, we will overview these evaluation techniques, their purpose, and how they are implemented for OLMo.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd162da5e-a14f-42ba-bf51-9425b199fd35_1242x1188.png)

Predicting a probability distribution over potential next tokens

**What is perplexity?** Prior to understanding perplexity evaluation, we have to understand the concept of perplexity in general. Put simply, perplexity is a metric that can be used to evaluate a language model by measuring how well it predicts known samples of text. When autoregressively generating output via [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction), the LLM predicts a probability distribution over potential next tokens. From these token-level probabilities, we can easily compute the probability for a sentence generated by the LLM via the product rule of probabilities; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0fdf04-638b-41dc-a589-672212ef8c39_2394x482.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb0fdf04-638b-41dc-a589-672212ef8c39_2394x482.png)

Computing the probability of a textual sequence with an LLM

The metric above can give us a good idea of how well a language model “fits” the data—_the model should assign high probabilities to valid, high-quality sequences of text_. The problem with this probability, however, is that it is highly sensitive to the length of the sentence (i.e., longer sentences will have more probabilities multiplied together for each token)! To solve this, we can take a [geometric mean](https://en.wikipedia.org/wiki/Geometric_mean)[8](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-8-141461162) to normalize the probability computed above by the length of the sentence; see below. The resulting metric captures the probability of a textual sequence in a manner that is not dependent upon or influenced by the length of the sequence.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e020631-38f7-4ed4-901f-84d5c1e9eb9f_1766x338.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e020631-38f7-4ed4-901f-84d5c1e9eb9f_1766x338.png)

Normalizing textual sequence probability using a geometric mean

_How does this relate to perplexity?_ Well, perplexity is just the reciprocal of this number! Low (high) perplexity values indicate that a language model assigns high (low) probability to textual sequences used for evaluation and, therefore, fits the evaluation data well (poorly). The perplexity metric is commonly used to measure the quality of a language model’s “fit” to a corpus of high-quality text data.

**Perplexity evaluation.** To perform perplexity-based evaluation, authors construct an evaluation dataset called Perplexity Analysis for Language Model Assessment ([Paloma](https://huggingface.co/datasets/allenai/paloma)) [23] by aggregating textual sequences from a diverse set of 585 domains collected across 18 different sources of textual data and evaluate the LLM by measuring perplexity on textual sequences from this dataset. Compared to prior work, Paloma significantly improves the diversity of perplexity-based evaluation benchmarks, allowing us to determine whether an LLM can accurately model text across a wide variety of domains. Notably, authors in [10] remove all data present in Paloma from OLMo’s pretraining dataset, ensuring that data contamination does not have an impact on perplexity evaluations[9](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-9-141461162).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F337bb1d2-3264-40ee-9276-0bc4a3b4b775_1866x754.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F337bb1d2-3264-40ee-9276-0bc4a3b4b775_1866x754.png)

(from [24])

**Downstream task evaluation.** Perplexity-based evaluations are useful for understanding whether an LLM understands a domain of text well. For example, we can measure perplexity of a few LLMs over a corpus of scientific publications to determine which LLM best captures this data. However, perplexity-based evaluations fail to directly measure how well an LLM performs on downstream tasks. For this, authors in [10] evaluate the model using the Catwalk framework [24], which provides a standardized abstraction for evaluating various LLMs across a wide variety of tasks and datasets; see above. For OLMo in particular, authors select nine reasoning tasks—_chosen based on similarity to the task set used to evaluate LLaMA and LLaMA-2 [2, 5]_—for downstream evaluations. Models are evaluated solely using a [zero-shot prompting strategy](https://cameronrwolfe.substack.com/i/117151147/zero-shot-learning)[10](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-10-141461162) [25].

> _“We perform downstream evaluations to make decisions around model architecture, initialization, optimizers, learning rate schedule, and data mixtures. We call this our online evaluation as it runs in-loop every 1000 training steps and provides an early and continuous signal on the quality of the model being trained.”_ - from [10]

**In-loop evaluations.** Beyond the offline evaluation of OLMo described above, we see in [10] that OLMo undergoes similar evaluations in an online fashion. Namely, researchers test a variety of different model hyperparameters (e.g., architecture choices, initialization strategies, optimizers, learning rate schedules, etc.) and rely upon online evaluations performed every 1000 training steps to continuously evaluate hyperparameter settings. Online and offline evaluation rely upon the same perplexity and downstream task metrics, but online evaluation provides a continuous performance signal of choices made throughout model training.

#### Model Architecture

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf63ac4d-75b9-4a45-8d6c-af7672d5b2ed_1620x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf63ac4d-75b9-4a45-8d6c-af7672d5b2ed_1620x636.png)

The decoder-only transformer architecture

The OLMo LLM—_like many other modern LLMs_—is based upon the [decoder-only transformer architecture](https://x.com/cwolferesearch/status/1640446111348555776?s=20). For more information on the different variants of the transformer model architecture, check out [this writeup](https://cameronrwolfe.substack.com/i/135273362/the-transformer-architecture-and-its-variants). Put simply, the standard transformer architecture has two components: an _encoder_ and a _decoder_. The decoder-only architecture—_as is implied by its name_—only uses the decoder component of the transformer. As depicted in the image above, each layer of this model uses masked, multi-headed self-attention and a feed-forward network to craft a rich representation of a textual sequence and the relationships between words in this sequence. Using this representation, the model can autoregressively generate coherent sequences of text when given a textual prompt as input.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b59911-3bd5-46e8-a9ac-f2254392e988_1618x388.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b59911-3bd5-46e8-a9ac-f2254392e988_1618x388.png)

(from [10])

**The OLMo suite.** Three model sizes of OLMo are included in the release; see above. The 1 billion and 7 billion parameter models are released along with the writeup [10]. At the time of writing, the 65 billion parameter model is still training and will be released in the near future. Although OLMo uses a decoder-only architecture, we have seen in [prior overviews](https://cameronrwolfe.substack.com/i/135439692/optimizing-for-faster-inference) that many recent LLMs make modifications to this architecture to improve inference and training efficiency, as well as to eliminate issues like loss spikes and slow divergence during training.

The architecture choices of OLMo, which we will cover in this section, are enumerated and compared to several popular LLMs in the table below. Many hyperparameters of the OLMO model architecture are quite similar to other LLMs. For example, OLMo shares the same hidden dimension, number of heads/layers, and MLP ratio as LLaMA-2 [2]. However, OLMo does have a slightly shorter context window—_only 2K tokens_—compared to LLaMA-2.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcca6b4bf-3dbf-4282-8086-264159700503_1702x1014.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcca6b4bf-3dbf-4282-8086-264159700503_1702x1014.png)

(from [10])

**SwiGLU activation.** Each transformer block passes intermediate activation values (i.e., the output of feed-forward and attention layers) through an activation function. Although the [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation function is standard in most deep neural network architectures, LLMs tend to adopt a different suite of (more complex) activation functions; see [11] for a detailed writeup on this topic. In particular, OLMo adopts the SwiGLU activation function—_shown in the equation below_—which is quite popular among recent LLMs like [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [2] and [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive) [16].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2c6f35a-440a-4a93-b312-dbf73f41958d_1194x402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2c6f35a-440a-4a93-b312-dbf73f41958d_1194x402.png)

The SwiGLU activation function

The Swish activation function is a smoother function compared to ReLU and has been shown to achieve better performance in several applications [12]. SwiGLU is a combination of this Swish activation with a Glu activation [13]. Interestingly, SwiGlu requires three matrix multiplications and is, therefore, more compute heavy compared to activation functions like ReLU. However, _SwiGLU improves LLM performance in experiments that use a fixed amount of computation_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff125254a-28af-43c4-80e0-d2273b1702c9_1888x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff125254a-28af-43c4-80e0-d2273b1702c9_1888x612.png)

Basic formulation of layer normalization

**Non-parametric layer normalization.** Each block of the transformer architecture contains intermediate layer normalization operations, as formulated in the figure above. Here, we normalize a value using the mean and variance of all values within each input sequence. A small additive constant is included in the denominator alongside the variance to avoid any issues with taking a square root of zero or dividing by zero. After layer normalization is applied, we typically have two learnable parameters that apply an element-wise [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) to the module’s output. However, authors in [10] choose to eliminate this affine transformation (i.e., we can easily [disable this setting](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) in PyTorch), which is found to improve both training speed and inference efficiency.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa21fa14-e4d3-4cbd-a3a1-bd6874eb512f_1676x1052.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa21fa14-e4d3-4cbd-a3a1-bd6874eb512f_1676x1052.png)

(from [14])

**Better positional embeddings.** Instead of the absolute positional embeddings used by the original transformer architecture, most modern LLMs (including OLMo) adopt a [rotary positional embedding (RoPE) strategy](https://blog.eleuther.ai/rotary-embeddings/) [14]. Put simply, RoPE (shown above) combines the benefits of [absolute](https://cameronrwolfe.substack.com/i/76273144/berts-architecture) and [relative positional embeddings](https://jaketae.github.io/study/relative-positional-encoding/) by _i)_ encoding the absolute position of each token with a rotation matrix and _ii)_ directly injecting relative position information into the self-attention operation. Using this approach, both the absolute and relative position of each token can be captured, allowing the LLM to generalize to longer input sequence lengths. RoPE is implemented in common libraries like HuggingFace and used by most modern LLMs due to its positive impact on performance.

**The tokenizer.** The creators of OLMo use the GPT-NeoX tokenizer [37], which is found to be suited well to Dolma due to the fact that it is trained over a corpus of web data (C4 dataset). Plus, the GPT-NeoX tokenizer has a permissive license, which is not true of all tokenizers. For example, using LLaMA-2’s tokenizer would cause the license from LLaMA-2 to apply to OLMo. The only change made to the GPT-NeoX tokenizer is the addition of extra tokens that are used for masking personally identifiable information (PII).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00e40c54-581e-4ab6-bb72-9d19e5d038ae_2402x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00e40c54-581e-4ab6-bb72-9d19e5d038ae_2402x804.png)

Sequential and parallel transformer blocks

**Other design choices.** Interestingly, all bias terms are excluded from the OLMo model architecture, which is shown in [10] to improve training stability. Going further, authors adopt a sequential (as opposed to parallel) transformer block formulation; see above. No weight tying is used by OLMo[11](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-11-141461162). Additionally, the model uses a vanilla (full) variant of [multi-headed self-attention](https://x.com/cwolferesearch/status/1644773244786941952?s=20). In contrast, several recent models have adopted grouped or multi-query attention variants that improve the inference/decoding efficiency of attention (potentially at the cost of reduced performance) by sharing key and value vectors between attention heads; see below. For example, LLaMA-2 [2] uses grouped-query attention, while PaLM [16] uses multi-query attention. We see in [10] that authors forego these efficient attention variants due to their impact upon performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)

(from [15])

#### The Training Process

OLMo models are trained over a 2T token subset of Dolma. However, the model may be trained for more than one epoch over this data. For example, OLMo-1B is trained on 2T tokens (i.e., one epoch), while OLMo-7B is trained over ~2.5T tokens (i.e., 1.25 epochs). Interestingly, authors in [10] claim that repeating data during training does not negatively impact model performance. All models are trained using the ZeRO [26] optimizer strategy via PyTorch’s [Fully Sharded Data Parallel (FSDP)](https://engineering.fb.com/2021/07/15/open-source/fsdp/) framework [27]—_a distributed, multi-GPU/node training strategy that reduces memory consumption by sharding (i.e., splitting into multiple pieces) model weights and their corresponding states within the optimizer across GPUs_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c791eac-6017-45be-af12-a69654025268_1182x728.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c791eac-6017-45be-af12-a69654025268_1182x728.png)

(from [27])

All OLMo models are trained using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html). Authors use mixed-precision training, which is built into PyTorch FSDP via the [automatic mixed precision (amp) module](https://pytorch.org/docs/stable/amp.html), to improve training throughput. Additionally, training is replicated on clusters of both NVIDIA and AMD GPUs to ensure portability of OLMo across different training infrastructures; see below. Interestingly, the resulting models trained on each cluster perform nearly identically, although slight changes in hyperparameters were necessary to optimize throughput.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F162b93f0-0a2f-4278-a1c8-638fdad0c0e4_1404x486.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F162b93f0-0a2f-4278-a1c8-638fdad0c0e4_1404x486.png)

(from [10])

## Empirical Analysis of Dolma and OLMo

> _“Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.”_ - from [10]

Within this section, we will briefly overview the empirical analysis of Dolma and OLMo from [1] and [10]. Interestingly, the open nature of the Dolma pretraining dataset allows us to easily study the downstream performance impact of changes to the model’s pretraining dataset—_a topic that is poorly elaborated within current research_. Additionally, we can study questions such as:

- Does decontamination of pretraining data impact model performance?
    
- Does learning from code make LLMs better reasoners?
    
- How does pretraining data mixture impact the LLMs knowledge base?
    

As we will see, OLMo models perform competitively with the most popular open LLMs, but they do not definitely set a new state-of-the-art. Nonetheless, OLMo is a robust starting point for valuable research into the performance and behavior of LLMs, providing both _i)_ a high-quality model that can be used by anyone and _ii)_ sufficient details and resources to replicate and expand upon this model.

#### The Impact of Pretraining Data on Performance

To validate choices made when constructing Dolma, authors in [1] train several OLMo-1B models. These models are used to compare strategies for data decontamination and mixing in particular. The resulting models are evaluated using the benchmarks discussed previously. Notably, such a transparent evaluation of hyperparameter settings and design choices for LLM pretraining datasets is currently absent from AI literature. This analysis emphasizes the beauty of OLMo and Dolma’s openness—_allowing the process of gathering pretraining data for LLM’s to be rigorously analyzed and completely demystified._

**Decontamination.** Given that the pretraining datasets of most modern LLMs are massive, the probability of testing and evaluation data being “leaked” within the model’s pretraining dataset is quite high. In fact, prior work has shown that large-scale language corpora often contain copies of benchmarks used for evaluating LLMs [28]. As such, one might argue that the impressive performance of LLMs on downstream tasks could be attributed to test set leakage—_maybe the model just memorized the answers to these tasks during pretraining_. However, the impact of data contamination on LLM performance is a hotly debated topic. For example, work in [28] shows that decontamination of pretraining data increases LLM perplexity (i.e., degrades performance) on validation data, while other analysis struggles to prove that decontamination has a consistent positive or negative impact [16, 25].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac67a2b3-7a7b-4dd4-9b0f-0dab4fd764a8_1080x368.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac67a2b3-7a7b-4dd4-9b0f-0dab4fd764a8_1080x368.png)

(from [1])

To evaluate the impact of data contamination, authors in [1] train OLMo-1B models over a 221B token subset of RedPajama. One of the models is trained over a decontaminated version of this dataset[12](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-12-141461162), while the other is trained using the full data. As shown in the table above, this decontamination approach does not have a clear negative (or positive) impact on the model’s performance. As such, a similar decontamination strategy—_with an added rule for ignoring repeated spaces, punctuation, and emojis_—is used when constructing Dolma.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F913b012a-4031-4372-91de-daa725024359_1598x886.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F913b012a-4031-4372-91de-daa725024359_1598x886.png)

(from [1])

**Data Mixology.** After pretraining data is curated across several sources, we must decide how to actually “mix” this data—_or up/down sample each source to create the final dataset_. The mixing strategy is an important hyperparameter that has a massive impact on the LLM’s performance. However, data mixology for LLMs is largely a black box due to a lack of rigorous analysis within AI literature. Authors aim to solve this issue in [1] by comparing several different mixing strategies; see above. The high-level takeaway from this analysis is the simple fact that the chosen data mixture has a noticeable impact on the model’s ability to capture certain subdomains. _LLMs understand data that they have seen during pretraining and struggle with specialized domains unless they are exposed to data from this domain (e.g., scientific publications or code) during pretraining_. As such, one should avoid domain mismatches between the pretraining and application of LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72746e4a-0976-48f8-b088-e1c0c5c7a264_1610x380.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72746e4a-0976-48f8-b088-e1c0c5c7a264_1610x380.png)

(from [1])

Recently, researchers have argued that adding code to an LLM’s pretraining dataset can improve the resulting model’s overall reasoning capabilities [30]. To test this claim, authors in [1] compare the performance of several OLMo-1B models, trained with varying mixtures of code in their pretraining data. From these experiments, we see that mixing code into the pretraining data does improve performance on (text-based) commonsense reasoning tasks; see above. Although difficult reasoning benchmarks like GSM8K cannot be solved by a vanilla LLM, we see in [1] that one can train an LLM to solve these tasks by:

- Pretraining the model on a sufficient amount of code.
    
- Finetuning the model over program-aided outputs.
    
- Asking the LLM to generate an executable Python snippet.
    

_LLMs that are pretrained on code are better reasoners and can even leverage code generation as a mechanism to solve more difficult reasoning tasks_!

#### Evaluating the OLMo Suite

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c27f528-12ab-4ad0-8e36-1aa5c405a411_1880x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c27f528-12ab-4ad0-8e36-1aa5c405a411_1880x726.png)

(from [10])

The final OLMo-7B model is trained over 2.46T tokens of text from Dolma prior to being evaluated and compared to a variety of other (semi-)open LLMs, such as LLaMA [5], LLaMA-2 [2], MPT [31], Pythia [32], Falcon [4], and RPJ-Incite [33]. The results of downstream task evaluations are shown in the table above, where we see that OLMo-7B performs comparably to other open LLMs. Although it does not set new state-of-the-art performance across all tasks, OLMo does perform best on two tasks and is a top-3 model for nearly all tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe912030-00f8-4caf-b844-c5b65c207217_1344x1170.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe912030-00f8-4caf-b844-c5b65c207217_1344x1170.png)

(from [10])

The results of perplexity-based evaluations are shown in the figure above, where performance is reported in terms of [bits per byte](https://vaclavkosar.com/ml/bits-per-byte-and-bits-per-character) (rather than perplexity) to account for the different vocabularies used by each LLM. Unlike other models being compared, OLMo’s pretraining data was explicitly decontaminated against Paloma. Nonetheless, OLMo is found to achieve a competitive fit across the different domains of Paloma, where the fit is best for domains that have the highest similarity to data that is present in OLMo’s pretraining dataset. Overall, OLMo-7B’s performance is not groundbreaking, but the model is competitive.

## Conclusion

> _“This is the first step in a long series of planned releases, continuing with larger models, instruction tuned models, and more modalities and variants down the line.”_ - from [10]

In summary, Dolma and OLMo take a massive step toward improving the transparency of LLM pretraining. While previous models vary in their level of openness, OLMo is completely open, choosing to release all information and artifacts (with a permissive license!) relevant to recreating the model from scratch. Plus, the associated tools (e.g., training/evaluation code and data toolkit) allow researchers to test new ideas and variations of the approach used in [1, 10]. Notably, OLMo does not set a new state-of-the-art performance across tasks considered in [1]. Though the model is competitive, _state-of-the-art performance is not its purpose_. Rather, OLMo and Dolma aim to provide a fully-documented starting point for LLM pretraining research, thus allowing others in the open-source community to truly understand and build upon this work.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), deep learning Ph.D. and Director of AI at [Rebuy](https://www.rebuyengine.com/). This is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Soldaini, Luca, et al. "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research." _arXiv preprint arXiv:2402.00159_ (2024).

[2] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." _arXiv preprint arXiv:2307.09288_ (2023). 

[3] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).

[4] Almazrouei, Ebtesam, et al. "The falcon series of open language models." _arXiv preprint arXiv:2311.16867_ (2023).

[5] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[6] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[8] Li, Raymond, et al. "StarCoder: may the source be with you!." _arXiv preprint arXiv:2305.06161_ (2023).

[9] Lee, Katherine, et al. "Deduplicating training data makes language models better." _arXiv preprint arXiv:2107.06499_ (2021).

[10] Groeneveld, Dirk, et al. "OLMo: Accelerating the Science of Language Models." _arXiv preprint arXiv:2402.00838_ (2024).

[11] Shazeer, Noam. "Glu variants improve transformer." _arXiv preprint arXiv:2002.05202_ (2020).

[12] Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. "Searching for activation functions." _arXiv preprint arXiv:1710.05941_ (2017).

[13] Dauphin, Yann N., et al. "Language modeling with gated convolutional networks." _International conference on machine learning_. PMLR, 2017.

[14] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[15] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." _arXiv preprint arXiv:2305.13245_ (2023).

[16] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[17] Laurençon, Hugo, et al. "The bigscience roots corpus: A 1.6 tb composite multilingual dataset." _Advances in Neural Information Processing Systems_ 35 (2022): 31809-31826.

[18] Workshop, BigScience, et al. "Bloom: A 176b-parameter open-access multilingual language model." _arXiv preprint arXiv:2211.05100_ (2022).

[19] Gao, Leo, et al. "The pile: An 800gb dataset of diverse text for language modeling." _arXiv preprint arXiv:2101.00027_ (2020).

[20] Penedo, Guilherme, et al. "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only." _arXiv preprint arXiv:2306.01116_ (2023). 

[21] Together Computer. Redpajama-data-1t, https://huggingface.co/datasets/ togethercomputer/RedPajama-Data-1T.

[22] Together Computer. Redpajama-data-v2, https://huggingface.co/datasets/ togethercomputer/RedPajama-Data-V2.

[23] Magnusson, Ian, et al. "Paloma: A Benchmark for Evaluating Language Model Fit." _arXiv preprint arXiv:2312.10523_ (2023).

[24] Groeneveld, Dirk, et al. "Catwalk: A unified language model evaluation framework for many datasets." _arXiv preprint arXiv:2312.10253_ (2023).

[25] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.

[26] Rajbhandari, Samyam, et al. "Zero: Memory optimizations toward training trillion parameter models." _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_. IEEE, 2020.

[27] Zhao, Yanli, et al. "Pytorch FSDP: experiences on scaling fully sharded data parallel." _arXiv preprint arXiv:2304.11277_ (2023).

[28] Yang, Shuo, et al. "Rethinking benchmark and contamination for language models with rephrased samples." _arXiv preprint arXiv:2311.04850_ (2023).

[29] Lee, Katherine, et al. "Deduplicating training data makes language models better." _arXiv preprint arXiv:2107.06499_ (2021).

[30] Madaan, Aman, et al. "Language models of code are few-shot commonsense learners." _arXiv preprint arXiv:2210.07128_ (2022).

[31] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _Databricks_, 5 May 2023, https://www.databricks.com/blog/mpt-7b.

[32] Biderman, Stella, et al. "Pythia: A suite for analyzing large language models across training and scaling." _International Conference on Machine Learning_. PMLR, 2023.

[33] Together Computer. [RedPajama-INCITE-7B-Base](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base) URL [https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base).

[34] Joulin, Armand, et al. "Bag of tricks for efficient text classification." _arXiv preprint arXiv:1607.01759_ (2016).

[35] Wenzek, Guillaume, et al. "CCNet: Extracting high quality monolingual datasets from web crawl data." _arXiv preprint arXiv:1911.00359_ (2019).

[36] Jiang, Albert Q., et al. "Mistral 7B." _arXiv preprint arXiv:2310.06825_ (2023).

[37] Black, Sid, et al. "Gpt-neox-20b: An open-source autoregressive language model." _arXiv preprint arXiv:2204.06745_ (2022).

[1](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-1-141461162)

Examples of such models include [Claude](https://www.anthropic.com/news/claude-2-1), GPT-4, [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive), and [Gemini](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction).

[2](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-2-141461162)

This is quite large! For reference, MassiveText contains 2.3T tokens, while RefinedWeb contains 5T tokens (although only 600B tokens are released publicly).

[3](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-3-141461162)

This may seem tedious or annoying, but manually inspecting data is one of the highest-ROI activities that you can perform! Typically, inspecting data allows you to learn more about a model than studying properties/predictions of the model itself.

[4](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-4-141461162)

Currently, training LLMs over English-only data is a standard practice that is used across most recent models, though many researchers have expressed discontent with this choice reinforcing English as the “default” language.

[5](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-5-141461162)

In addition to language filtering, CCNet performs some preliminary deduplication by identifying and removing common paragraphs in each Common Crawl snapshot.

[6](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-6-141461162)

Documents that are too large to process with the fastText language identification model can be split into individual paragraphs for processing. Then, we can average the score of individual paragraphs to obtain a document score.

[7](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-7-141461162)

Intuitively, this makes sense! Accurate deduplication ensures that the model is not training over several copies of the same data. Instead, all of the data is unique, and the LLM does not waste any training iterations on repeated data.

[8](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-8-141461162)

When we are taking a product of several probabilities (or just numbers in general), the geometric mean is the standard way to take an average over all of these values within the product.

[9](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-9-141461162)

Interestingly, OLMo is the only LLM of this scale that implements explicit data de-contamination prior to performing perplexity evaluations.

[10](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-10-141461162)

For each task, the model is used to assign a probability to candidate completions (e.g., multiple choice answers), then the most likely response is selected as the answer. Similarly to computing perplexity, the likelihood of each response is normalized by length (e.g., by the number of characters or words in the response).

[11](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-11-141461162)

[Weight tying](https://paperswithcode.com/method/weight-tying) refers to the practice of using shared weights between the LLM’s token embedding matrix and the softmax layer used for next token prediction; see [here](https://cameronrwolfe.substack.com/i/136638774/the-decoder-only-transformer) for more details. This approach can drastically reduce the total number of model parameters while improving performance in certain cases.

[12](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open#footnote-anchor-12-141461162)

The dataset is decontaminated using the paragraph-level deduplication strategy used for Dolma. This approach removes 2.17% of tokens and 0.66% of documents.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Eric Lebigot's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4f1c090-77b3-4563-841b-2da548f0ce6b_144x144.png)



](https://substack.com/profile/4030497-eric-lebigot)

[

![Dr. Holger Lindberg Joergensen's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf810c6f-a979-4fc0-ba4a-e4361992d16e_144x144.png)



](https://substack.com/profile/46511-dr-holger-lindberg-joergensen)

[

![Saurabh Chandra's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea6d1c7-eeef-4b44-8993-3605026c7be7_640x480.jpeg)



](https://substack.com/profile/166492749-saurabh-chandra)

[

![andy's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0413e5f6-7a86-44c2-96e0-625575d0c3ca_144x144.png)



](https://substack.com/profile/31341503-andy)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

29 Likes∙

[2 Restacks](https://substack.com/note/p-141461162/restacks?utm_source=substack&utm_content=facepile-restacks)

29

- 

[

2

](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open/comments)

2

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Yang Zhang's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9629f58-d1cb-49e5-8fb9-7aa8104909ad_144x144.png)



](https://substack.com/profile/12761547-yang-zhang?utm_source=comment)

[Yang Zhang](https://substack.com/profile/12761547-yang-zhang?utm_source=substack-feed-item)

[Yang’s Newsletter](https://yaaang.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年2月27日](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open/comment/50429680 "2024年2月27日 08:51")

Liked by Cameron R. Wolfe, Ph.D.

Great write up! Minor note, isn't Pythia Apache license?

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open/comment/50429680)

[1 more comment...](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Decoder-Only Transformers: The Workhorse of Generative LLMs

### Building the world's most influential neural network architecture from scratch...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Mar 04, 2024

117

- 

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

7

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff05f4d4-c156-44cb-8eb1-9effcfe59871_2392x1314.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff05f4d4-c156-44cb-8eb1-9effcfe59871_2392x1314.png)

(from [1, 8])

The current pace of AI research is staggering. Keeping up with the most recent publications is a difficult feat, leaving even experts in the field feeling as if they are failing to grasp the finer details of this evolving frontier. In the domain of large language models (LLMs) especially, impactful research is being released constantly, including anything from new foundation models (e.g., Gemma [15] and OLMo [12]) to better alignment techniques (e.g., DPO [32] versus PPO [33] versus REINFORCE [34]) to exotic topics like [model merging](https://www.interconnects.ai/p/model-merging). Despite these rapid advancements, however, one component of LLMs has remained constant—_the decoder-only transformer architecture_. Shockingly, the architecture used by most modern LLMs is nearly identical to that of the original GPT model. We just make the model much larger, modify it slightly, and use a more extensive training (and alignment) process. For this reason, the decoder-only transformer architecture is one of the most fundamental and important ideas in AI research. Within this overview, we will comprehensively explain this architecture, implement all of its components from scratch, and explore how it has evolved in recent research.

## The Self-Attention Operation

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F966a44ef-fe4b-4d19-900c-543af44dacb2_1614x1042.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F966a44ef-fe4b-4d19-900c-543af44dacb2_1614x1042.png)

(from [1])

Given that the transformer architecture was proposed in a paper titled _“Attention Is All You Need”_ [1], it probably comes as no surprise that self-attention is at the core of all modern language models. Put simply, self-attention transforms the representation of each token in a sequence based upon its relationship to other tokens in the sequence; see above. But, _how exactly does this work?_ In this section, we will explain the concepts behind self-attention step-by-step, as well as build an implementation (in PyTorch) of the self-attention variant used by LLMs.

#### Understanding Scaled Dot Product Attention

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85c1f60b-8521-411b-9b84-54973929c251_1500x843.gif)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85c1f60b-8521-411b-9b84-54973929c251_1500x843.gif)

> _“An attention function [maps] a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.”_ - from [1]

**Projecting the input.** The input to a self-attention layer is simply a batch of token sequences, where each token in the sequence is represented with a vector. Assuming we use a batch size `B` and each sequence is of length `T`, then our self-attention layer receives an [tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) of shape [`B, T, d]` as input, where `d` is the dimensionality of the token vectors. For simplicity, we will first outline the self-attention operation using only one sequence of tokens as input; see below. However, the same concepts can be easily applied to a batch of sequences.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e52f31-6644-45e3-b8a9-3c5a6fc7021a_1298x696.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e52f31-6644-45e3-b8a9-3c5a6fc7021a_1298x696.png)

Sequence of token vectors in list and matrix form

The first step of self-attention is to perform three separate (linear) projections of the token vectors in our input sequence, forming key, query, and value vector sequences. To do this, we have three weight matrices—_corresponding to the key, query, and value projections_—that are used to project each of the input token vectors, forming new sequences of transformed token vectors. Because we do this three times, we end up with three separate sequences of token vectors; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c1496a8-30e5-4df3-9cbf-c729bdbc452e_1318x1066.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c1496a8-30e5-4df3-9cbf-c729bdbc452e_1318x1066.png)

Creating the query, key, and token vectors

**Computing attention scores.** After projecting the input, attention scores are generated using the key and query vectors. We compute an attention score `a[i, j]` for every pair of tokens `[i, j]` within the sequence. Attention scores lie in the range `[0, 1]` and quantitatively characterize how much token `j` should be considered when computing the new representation for token `i`. Practically, we compute `a[i, j]` by taking the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the query vector for token `i` with the key vector for token `j`; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32583e97-78bc-4e11-81f5-40ffa42cf94a_2042x1228.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32583e97-78bc-4e11-81f5-40ffa42cf94a_2042x1228.png)

Computing attention scores from query and key vectors

We can efficiently compute all pairwise attention scores in a sequence by stacking the query and key vectors into two matrices and multiplying the query matrix with the transposed key matrix. The result of this operation is a matrix of size `[T, T]`—_we will call this the attention matrix_—that contains all pairwise attention scores in the sequence. From here, we divide each value in the attention matrix by the square root of `d`—_an approach that has been found to improve training stability [1]_—and apply a [softmax operation](https://en.wikipedia.org/wiki/Softmax_function) to each row of the attention matrix; see below. After softmax has been applied, the attention scores for each token lie within the range `[0, 1]` and form a valid probability distribution.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1da389c-8274-4ebf-82e7-8117bc0d763f_1402x680.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1da389c-8274-4ebf-82e7-8117bc0d763f_1402x680.png)

Computing the attention matrix

**Value vectors.** Once we have the attention scores, deriving the output of self-attention is easy. The output for each token is simply a weighted combination of value vectors, where the weights are given by the attention scores. To compute this output in batch, we can simply stack all value vectors into a matrix and take the product of the attention matrix with the value matrix. Notably, self-attention preserves the size of its input—_a transformed,_ `d`_-dimensional output vector is produced for each token vector within the input_. If we write out this matrix multiplication by hand, we will see that each token’s output representation is just a weighted average of value vectors with weights given by attention scores.

#### Causal Self-Attention for LLMs

The self-attention operation described above forms the basis of the transformer architecture. However, the transformer’s decoder uses a slightly more complex version of self-attention called masked, multi-headed self-attention. First, we will learn the differences between masked and bidirectional self-attention. Then, we will discuss how attention can be computed across multiple “heads” in parallel.

**Masked self-attention.** Decoder-only transformers use a variant of self-attention called masked (or causal) self-attention. While vanilla (or bidirectional) self-attention—_as described in the previous section_—allows all tokens within the sequence to be considered when computing attention scores, masked self-attention modifies the underlying attention pattern by “masking out” tokens that follow a given token within the sequence. For example, let’s consider our token sequence `[“LLM”, “#s”, “are”, “cool”, “.”]` and assume we are trying to compute attention scores for the token `“are”`. So far, we have learned that self-attention will compute an attention score between `“are”` and every other token within the sequence. With masked self-attention, however, we only compute attention scores for `“LLM”`, `“#s”`, and `“are”`. _Masked self-attention prohibits us from looking forward in the sequence during self-attention_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57676b4-4a28-4627-96f0-bdb3928b7ccf_2360x622.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe57676b4-4a28-4627-96f0-bdb3928b7ccf_2360x622.png)

Masking attention scores in causal self-attention

Masked self-attention is implemented similarly to bidirectional self-attention in practice. Once the query and key matrices have been multiplied, we have an attention matrix of size `[T, T]` with each token’s attention scores across the full sequence. Prior to performing the softmax operation across each row of this matrix, however, we can set all values above the diagonal of the attention matrix to negative infinity; see above. By doing this, we ensure that, for each token, all tokens that follow this token in the sequence are given an attention score of zero after the softmax operation has been applied. In other words, _we mask each token’s attention scores to exclude any future tokens within the sequence_.

**Attention heads.** The attention operation we have described so far uses softmax to normalize attention scores that are computed across the sequence. Although this approach forms a valid probability distribution, it also limits the ability of self-attention to focus on multiple positions within the sequence—_the probability distribution can easily be dominated by one (or a few) words_. To solve this issue, we typically compute attention across multiple “heads” in parallel; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png)

(from [1])

Within each head, the masked attention operation is identical. However, we _i)_ use separate key, query, and value projections for each head and _ii)_ reduce the dimension of the key, query, and value vectors to keep computational costs reasonable. Typically, we will change the dimensionality of these vectors from `d` to `d // H[1](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-1-142044446)`, where `H` is the number of attention heads. Using this approach, each attention head can learn a unique representational subspace and focus on different parts of the underlying sequence. Plus, we avoid added computational costs by reducing the dimension of vectors used by each attention head.

Finally, there’s one more detail that we have to consider with multi-headed self-attention: _How do we combine the output of each head?_ Well, there are a variety of different options (e.g., concatenation, averaging, projecting, etc.). However, the vanilla implementation of multi-headed self-attention typically:

- Concatenates the output of each head.
    
- Linearly projects the concatenated output.
    

Because each attention head outputs token vectors of dimension `d // H`, the concatenated output of all attention heads has dimension `d` (i.e., same as the attention layer’s input dimension).

#### Implementing Causal Self-Attention in PyTorch

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||"""|
||Source: https://github.com/karpathy/nanoGPT/blob/master/model.py|
||"""|
|||
||import math|
||import torch|
||from torch import nn|
||import torch.nn.functional as F|
|||
||class CausalSelfAttention(nn.Module):|
|||
||def __init__(|
||self,|
||d,|
||H,|
||T,|
||bias=False,|
||dropout=0.2,|
||):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||H: number of attention heads|
||T: maximum length of input sequences (in tokens)|
||bias: whether or not to use bias in linear layers|
||dropout: probability of dropout|
||"""|
||super().__init__()|
||assert d % H == 0|
|||
||# key, query, value projections for all heads, but in a batch|
||# output is 3X the dimension because it includes key, query and value|
||self.c_attn = nn.Linear(d, 3*d, bias=bias)|
|||
||# projection of concatenated attention head outputs|
||self.c_proj = nn.Linear(d, d, bias=bias)|
|||
||# dropout modules|
||self.attn_dropout = nn.Dropout(dropout)|
||self.resid_dropout = nn.Dropout(dropout)|
||self.H = H|
||self.d = d|
|||
||# causal mask to ensure that attention is only applied to|
||# the left in the input sequence|
||self.register_buffer("mask", torch.tril(torch.ones(T, T))|
||.view(1, 1, T, T))|
|||
||def forward(self, x):|
||B, T, _ = x.size() # batch size, sequence length, embedding dimensionality|
|||
||# compute query, key, and value vectors for all heads in batch|
||# split the output into separate query, key, and value tensors|
||q, k, v = self.c_attn(x).split(self.d, dim=2) # [B, T, d]|
|||
||# reshape tensor into sequences of smaller token vectors for each head|
||k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2) # [B, H, T, d // H]|
||q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)|
||v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)|
|||
||# compute the attention matrix, perform masking, and apply dropout|
||att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # [B, H, T, T]|
||att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))|
||att = F.softmax(att, dim=-1)|
||att = self.attn_dropout(att)|
|||
||# compute output vectors for each token|
||y = att @ v # [B, H, T, d // H]|
|||
||# concatenate outputs from each attention head and linearly project|
||y = y.transpose(1, 2).contiguous().view(B, T, self.d)|
||y = self.resid_dropout(self.c_proj(y))|
||return y|

[view raw](https://gist.github.com/wolfecameron/26863dbbc322b15d2e224a2569868256/raw/21a836285584d6437e477f035a26c39efdc5f442/causal_self_attention.py)[causal_self_attention.py](https://gist.github.com/wolfecameron/26863dbbc322b15d2e224a2569868256#file-causal_self_attention-py) hosted with ❤ by [GitHub](https://github.com/)

The implementation of masked, multi-headed self-attention (full code is shown above) should be pretty easy to follow if we have understood the discussion up to this point! First, we perform the key, query, and value projections using a simple linear layer in PyTorch. We can perform the key, query, and value projections for all self-attention heads using a single linear layer! This layer takes a sequence of token embeddings of dimension `d` as input and produces token embeddings of size `3 * d` as output. From here, we can split the output into sequences of `d`-dimensional key, query, and value vectors. Then, each `d`-dimensional vector can be reshaped into `H` smaller vectors—_one for each attention head_—and we can [transpose](https://en.wikipedia.org/wiki/Transpose) the tensor to yield an output of shape `[B, H, T, d // H]`, where `B` is the number of sequences in the batch being processed; see below.

```
q, k, v  = self.c_attn(x).split(self.d, dim=2)
k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2)
q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)
v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)
```

From here, we can compute attention scores across all tokens within each head and across the entire batch using basic matrix/tensor multiplication. First, we multiply the query tensor by the transpose of the key matrix, thus computing the unnormalized attention matrix of size `[B, H, T, T]`. We then divide this result by `sqrt(d)` and apply softmax over the last dimension, thus transforming each token’s attention scores across the sequence into a probability distribution. Prior to the softmax, however, we fill all entries of the attention matrix above the diagonal with a value of negative infinity; see below.

```
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att, dim=-1)
att = self.attn_dropout(att)
```

Optionally, we can also perform dropout on attention scores [2][2](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-2-142044446), which has been shown to regularize the training process and improve generalization. Once the attention matrix has been computed, we can derive the final output of self-attention by multiplying the attention matrix with the value matrix, which takes a weighted average of value vectors for each token based on attention scores. The result of this computation is a tensor of size `[B, H, T, d // H]`, but we can concatenate the output of each attention head by simply transposing and reshaping the tensor to be of size`[B, T, d]`; see below.

```
y = att @ v
y = y.transpose(1, 2).contiguous().view(B, T, self.d)
y = self.resid_dropout(self.c_proj(y))
```

Finally, we perform one last linear projection of this concatenated output (optionally with dropout) to get our final result, as shown in the code above.

## The Decoder-Only Transformer Block

The decoder-only transformer architecture is comprised of several “blocks” with identical structure that are stacked in sequence. Within each of these blocks, there are two primary components:

1. Masked, multi-headed self-attention.
    
2. A feed-forward transformation.
    

Additionally, we usually surround these components with a residual connection and a normalization layer. Within this section, we will discuss this block structure in more detail and provide a concrete implementation in PyTorch.

#### Layer Normalization

Although [high-performance GPUs](https://www.nvidia.com/en-us/data-center/h200/) and advancements in model architectures may make us think otherwise, _training large, deep neural networks has not always been a breeze_! Early attempts at training neural networks with many layers were largely unsuccessful due to issues with [vanishing, exploding, and unstable gradients](http://neuralnetworksanddeeplearning.com/chap5.html). Several advancements have been proposed to address these issues:

- Better methods of initializing weights (e.g., [Xavier](https://www.geeksforgeeks.org/xavier-initialization/) or [He](https://www.geeksforgeeks.org/kaiming-initialization-in-deep-learning/) initialization).
    
- Replacing [sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation functions with [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) [5] (i.e., this keeps gradients in the activation function from becoming very small).
    
- Normalizing intermediate neural network activations [6].
    

Within this section, we will focus on the final advancement mentioned above—_normalization_. The motivation behind normalization is quite simple. The intermediate activation values of a deep neural network can become unstable (i.e., very large or very small) because we repeatedly multiply them by a matrix of model parameters. For example, if we run the PyTorch code snippet below, we will see that repeating the same (random) matrix multiplication many times in a row causes the values of our output to become incredibly large!

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||import torch|
|||
||# experiment settings|
||d = 5|
||nlayers = 100|
||normalize = False # set True to use normalization|
|||
||# create vector with random entries between [-1, 1]|
||input_vector = (torch.rand(d) - 0.5) * 2.0|
|||
||# create matrix with random entries between [-1, 1]|
||# by which we can repeatedly multiply the input vector|
||weight_matrix = (torch.rand(d, d) - 0.5) * 2.0|
|||
||output = input_vector|
||for i in range(nlayers):|
||# optionally perform normalization|
||if normalize:|
||output = (output - torch.mean(output)) / torch.std(output)|
|||
||# repeatedly multiply the vector by the matrix|
||output = weight_matrix @ output|
|||
||# observe output values|
||print(output)|

[view raw](https://gist.github.com/wolfecameron/f9cb6645dc87a165ce3a7fae980610a4/raw/dbd72de3ef5e3714d68afb467c29b1446cf24f82/exploding_activations.py)[exploding_activations.py](https://gist.github.com/wolfecameron/f9cb6645dc87a165ce3a7fae980610a4#file-exploding_activations-py) hosted with ❤ by [GitHub](https://github.com/)

To solve this, we can normalize[3](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-3-142044446) the activation values between each matrix multiplication, allowing activation values to remain stable over time. This is exactly the idea that is used by normalization layers within a neural network. Let’s take a look at a few popular variants of normalization that exist.

**Normalization variants.** Depending up the domain and architecture being used, there are several normalization techniques that we can adopt. The two most common forms of normalization are[4](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-4-142044446):

- Batch Normalization [6]
    
- Layer Normalization [7]
    

These techniques are quite similar. For both of them, we just transform activation values using the equation shown below. The difference between them lies in how we choose to compute the mean and standard deviation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ceb7ece-20b0-4db6-a834-9127bbb8685e_800x816.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ceb7ece-20b0-4db6-a834-9127bbb8685e_800x816.png)

Standard equation for normalization

Batch normalization—_as the name indicates_—computes a per-dimension mean and standard deviation over the entire mini-batch; see below. Although this approach works well, it is limited by the fact that we must process a sufficiently large mini-batch of inputs to get a reliable estimate of the mean and variance. This becomes an issue during inference, where processing only a small number of input examples at once is common. For this reason, we must compute a running estimate[5](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-5-142044446) of the mean and standard deviation during training that can be used for inference. Nonetheless, batch normalization is widely used and is the standard choice of normalization technique within computer vision applications.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe8777c3-8464-48ee-906c-504ff9a824ea_2670x676.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe8777c3-8464-48ee-906c-504ff9a824ea_2670x676.png)

(from [8])

Layer normalization eliminates batch normalization’s dependence upon the batch dimension by computing the mean and standard deviation over the final dimension of the input. In the case of decoder-only transformers, this means that we compute normalization statistics over the embedding dimension; see above.

Currently, batch normalization is commonly used for computer vision tasks, while layer normalization is standard for natural language processing tasks. The original transformer architecture adopted layer normalization within its implementation [10], and this choice has been a standard for the transformer ever since. However, layer normalization was also used by earlier language models—_those based on recurrent neural networks_[6](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-6-142044446)—prior to the proposal of the transformer.

**Affine transformation.** Normalization layers in deep networks are also typically combined with an affine transformation. This might sound complicated, but it just means that we modify layer normalization as shown in the equation below. After normalizing the activation value, we multiply it by a constant γ, as well as add a constant β. Both of these constants are learnable and treated the same as a normal model parameter. Additionally, we see below that layer normalization uses a slightly modified form of standard deviation in the denominator that incorporates a small, fixed constant ε to avoid issues with dividing by zero.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff125254a-28af-43c4-80e0-d2273b1702c9_1888x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff125254a-28af-43c4-80e0-d2273b1702c9_1888x612.png)

Layer normalization with an affine transformation

Layer normalization is implemented in PyTorch and can be easily accessed either via the [associated module](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) or its [functional form](https://pytorch.org/docs/stable/generated/torch.nn.functional.layer_norm.html).

#### Feed-Forward Transformation

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21b82064-554d-43d2-b6f1-589090e830ce_1294x410.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21b82064-554d-43d2-b6f1-589090e830ce_1294x410.png)

Schematic depiction of a pointwise feed-forward transformation

Each decoder-only transformer block contains a pointwise[7](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-7-142044446) feed-forward transformation; see above. This transformation passes every token vector within its input through a small, feed-forward neural network. This neural network consists of two linear layers—_with optional bias_[8](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-8-142044446)—that are separated by a non-linear activation function. The neural network’s hidden dimension is usually larger—_4X larger in the case of GPT [3], GPT-2 [4], and many other LLMs_—than the dimension of the token vector it takes as input.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2c6f35a-440a-4a93-b312-dbf73f41958d_1194x402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2c6f35a-440a-4a93-b312-dbf73f41958d_1194x402.png)

The SwiGLU activation function

**Activation function.** _Which activation function should we use in an LLM’s feed-forward layers?_ In [13], authors compare the performance of numerous activation functions, finding that the SwiGLU activation (shown above) yields the best performance given a fixed amount of compute. For this reason, SwiGLU is commonly used by popular LLMs like [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) [11] and [OLMo](https://cameronrwolfe.substack.com/p/dolma-olmo-and-the-future-of-open) [12]. However, not all LLMs use SwiGLU; e.g., both Falcon [14] and Gemma [15] use [GeLU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html).

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||"""|
||Source: https://github.com/karpathy/nanoGPT/blob/master/model.py|
||"""|
|||
||from torch import nn|
|||
||class FFNN(nn.Module):|
|||
||def __init__(|
||self,|
||d,|
||bias=False,|
||dropout=0.2,|
||):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||bias: whether or not to use bias in linear layers|
||dropout: probability of dropout|
||"""|
||super().__init__()|
||self.c_fc = nn.Linear(d, 4 * d, bias=bias)|
||self.gelu = nn.GELU()|
||self.c_proj = nn.Linear(4 * d, d, bias=bias)|
||self.dropout = nn.Dropout(dropout)|
|||
||def forward(self, x):|
||x = self.c_fc(x) # [B, T, 4*d]|
||x = self.gelu(x)|
||x = self.c_proj(x) # [B, T, d]|
||x = self.dropout(x)|
||return x|

[view raw](https://gist.github.com/wolfecameron/3ed9274a0297aab403b5e2d2254ee0ac/raw/70535fc537da3bddb776b05665815646cdd07d6b/transformer_ffnn.py)[transformer_ffnn.py](https://gist.github.com/wolfecameron/3ed9274a0297aab403b5e2d2254ee0ac#file-transformer_ffnn-py) hosted with ❤ by [GitHub](https://github.com/)

**Implementation in PyTorch.** Implementing the feed-forward component of a transformer block is simple; see above. We just need a few [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with an activation function in between. In the above implementation, an input of size `[B, T, d]` is provided to the first linear layer, which has an input dimension of `d` and an output dimension of `h = 4 * d`. The first linear layer performs a [batched matrix multiplication](https://pytorch.org/docs/stable/generated/torch.bmm.html) of all `d`-dimensional vectors in this input by a matrix of size `d x h`, forming an output of size `[B, T, h]`. From here, we apply the non-linear activation function to this output and pass it through the next linear layer, which has an input dimension of `h` and an output dimension of `d`. Finally, we can (optionally) apply dropout to the output of the second linear layer, which has size `[B, T, d]`, to regularize the model during training.

#### Residual Connections

We typically add residual connections between each of the self-attention and feed-forward sub-layers of the transformer block. The concept of a residual connection was originally proposed by the [ResNet architecture](https://arxiv.org/abs/1512.03385) [16], which is a widely used (and famous) convolutional neural network architecture for computer vision tasks like image classification and object detection. Residual connections are simple to understand conceptually. Instead of just passing neural network activations through a layer in the network, we _i)_ store the input to the layer, _ii)_ compute the layer’s output, and _iii)_ add the layer’s input to the layer’s output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ff943fa-b4a9-4b89-864b-cdaaae24475e_752x534.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ff943fa-b4a9-4b89-864b-cdaaae24475e_752x534.png)

A generic neural network layer with a residual connection

Residual connections are a generic idea that can be applied to any neural network layer that does not change the dimension of the input[9](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-9-142044446). By adding residual connections, we can mitigate problems with vanishing and exploding gradients, as well as improve the overall ease and stability of the training process. Residual connections provide a “shortcut” that allows gradients to flow freely through the network during backpropagation[10](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-10-142044446). The benefits of residual connections have been extensively explored and analyzed within deep learning literature, leading to a variety of interesting intuitions regarding their utility [17, 18, 19].

#### Putting It All Together!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png)

A decoder-only transformer block

To construct a full decoder-only transformer block, we have to use all of the components that we have talked about so far:

- Masked, multi-headed self-attention
    
- Layer normalization
    
- Pointwise feed-forward transformation
    
- Residual Connections
    

The layout of a decoder-only transformer block is shown in the figure above. As we will soon learn, the exact layout of the block may change depending upon the implementation. However, the schematic above matches the vanilla structure of decoder-only transformer blocks used by most GPT-style LLMs. This same structure of decoder-only transformer block is implemented in PyTorch below.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||"""|
||Source: https://github.com/karpathy/nanoGPT/blob/master/model.py|
||"""|
|||
||from torch import nn|
|||
||class Block(nn.Module):|
||def __init__(|
||self,|
||d,|
||H,|
||T,|
||bias=False,|
||dropout=0.2,|
||):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||H: number of attention heads|
||T: maximum length of input sequences (in tokens)|
||bias: whether or not to use bias in linear layers|
||dropout: probability of dropout|
||"""|
||super().__init__()|
||self.ln_1 = nn.LayerNorm(d)|
||self.attn = CausalSelfAttention(d, H, T, bias, dropout)|
||self.ln_2 = nn.LayerNorm(d)|
||self.ffnn = FFNN(d, bias, dropout)|
|||
||def forward(self, x):|
||x = x + self.attn(self.ln_1(x))|
||x = x + self.ffnn(self.ln_2(x))|
||return x|

[view raw](https://gist.github.com/wolfecameron/0ad044748283c90b4d3002bdc5dbc674/raw/3b264c837ac526a21a1088e46f52237e04404674/decoder_only_block.py)[decoder_only_block.py](https://gist.github.com/wolfecameron/0ad044748283c90b4d3002bdc5dbc674#file-decoder_only_block-py) hosted with ❤ by [GitHub](https://github.com/)

## The Decoder-Only Transformer

We will now take a look at the full decoder-only transformer architecture, which is primarily composed the building blocks we have seen so far. However, there are a few extra details that we have to cover, such as constructing the model’s input and using the model’s output to predict/generate text. Compared to self-attention, these details are relatively simple to understand, but covering them is necessary to get the full picture of how a decoder-only transformer architecture operates.

#### Constructing the Model’s Input

As outlined previously, the input to a transformer block is expected to be a (batched) sequence of token vectors, usually in the form of a tensor with shape `[B, T, d]`. However, the LLM usually receives input in the form of a textual prompt. _How do we convert this textual prompt into a sequence of token vectors?_

**Tokenization.** The transformer receives raw text as input. The first step in processing this text is to tokenize it, or convert it into a series of discrete words or sub-words. These words and sub-words are commonly called tokens; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56dd3364-44d1-4587-a0b8-3909f1f02f31_1132x282.png)

Converting raw text into a sequence of tokens

The tokenization process is handled by the model’s tokenizer, which uses an algorithm like Byte-Pair Encoding (BPE) [20], SentencePiece [21], or WordPiece [22] to break text into sequences of tokens; see [here](https://huggingface.co/docs/transformers/en/tokenizer_summary) for more details. The tokenizer has a fixed-size vocabulary—_usually containing around 50K to 300K unique tokens_—that defines the set of known tokens that can be formed from a raw sequence of text. The tokenizer has its own training pipeline that derives its underlying vocabulary and typically implements two major functions:

- _Encode_: convert a string into a sequence of tokens
    
- _Decode_: convert a sequence of tokens into a string
    

Tokenization is an oftentimes overlooked aspect of LLM training and usage. However, _failing to investigate and understand the tokenization process for an LLM is a huge mistake_! Tokenization is the first step in creating the model’s input and, therefore, has a massive impact on the downstream model’s performance. Issues with an LLM can often be traced back to nuanced bugs in the tokenization process that are difficult to detect[11](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-11-142044446). As such, I would highly encourage the interested reader to dive deeper into the tokenization process. For an in-depth and practical overview of BPE tokenizers—_the most commonly-used tokenizers for LLMs_—check out the recently published video below from [Andrej Karpathy](https://karpathy.ai/).

[Building GPT's Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

**Token embeddings.** Once we have tokenized our text and formed a sequence of tokens, we must convert each of these tokens into a corresponding embedding vector. To do this, we create an [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), which is a part of the decoder-only transformer model. This embedding layer is just a matrix with `d` columns and `V` rows, where `V` is the size of the tokenizer’s vocabulary. Each token within the vocabulary is associated with an integer index that corresponds to a row in this embedding matrix. We can convert tokens into a `d`-dimensional embedding by simply looking up the token’s entry in this embedding layer; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png)

Looking up token embeddings in an embedding layer

This embedding layer is trained during the LLM’s training process similarly to any other model parameters! Token embeddings are not fixed, but rather learned from data.

**Position embeddings.** Now, we have converted our raw text into a sequence of token vectors. If we do this for an entire batch of textual sequences, we will have the input of size `[B, T, d]` expected by our transformer blocks. However, there is one final step that we need to perform—_positional embedding_.

> _“Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.”_ - from [1]

In studying the self-attention mechanism, we might notice that the position of each token in the sequence is not considered when computing the output[12](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-12-142044446)! However, the order of words within a sequence of text is obviously important (e.g., _“I have to read this book.”_ vs. _“I have this book to read.”_). Therefore, we need some way of injecting positional information into the self-attention process. In [1], this was done by adding positional embeddings of dimension `d` to each token within the model’s input. Because each position within the sequence has a unique position embedding, the position of each token can be distinguished.

Similarly to token embeddings, we can store position embeddings in an embedding layer and learn them from data during the LLM’s training process—_this approach is simple to implement_. Alternatively, we can generate fixed token embeddings via some rule or equation. In [1], positional embeddings are generated using sine and cosine functions, as shown within the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f37014-74da-4a51-991f-d7212e527df3_1314x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f37014-74da-4a51-991f-d7212e527df3_1314x784.png)

Generating positional embeddings with sine and cosine functions (from [1])

These approaches are referred to as “absolute” positional embedding strategies, as the embedding being used is determined by the token’s absolute position in the sequence. As we will see later in this post, absolute positional embedding strategies fail to generalize to sequences that are longer than those seen during training, which has led to the proposal of more generalizable strategies.

#### The Full Decoder-Only Transformer Model

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png)

Structure of a decoder-only transformer model

Once we have constructed the model’s input, we simply pass this input through a sequence of decoder-only transformer blocks; see above. The total number of transformer blocks depends on the size of the model; e.g., OLMo-7B [12] has 32 layers and OLMo-65B has 80 layers; see below. Transformer blocks preserve the size of their input, so the output of the model’s body—_including all transformer blocks_—is a sequence of token vectors that is the same size as the input.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44c53b4f-df32-4e15-94c3-3c1d2beb20ce_1016x220.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44c53b4f-df32-4e15-94c3-3c1d2beb20ce_1016x220.png)

Specifications of OLMo LLMs (from [12])

Increasing the number of transformer blocks/layers within the underlying LLM is one of the primary ways of increasing the size of the model. Alternatively, we can increase the value of `d` (i.e., the dimension of token embeddings), which increases the size of weight matrices for all attention and feed-forward layers in the model. As shown above, we typically scale up the size of a decoder-only transformer by simultaneously increasing both _i)_ the number of layers and _ii)_ the hidden dimension. Oftentimes, we also increase the number of heads within each attention layer, but this does not impact the number of parameters in the model if we assume that each attention head has a dimension of `d // H`.

**Classification head.** Finally, there is one final detail of the decoder-only transformer architecture that we have to consider. Once we have passed our input sequence through the model’s body, we receive as output a same-size sequence of token vectors. To generate text or perform next token prediction (see [here](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) for more details on this process), we convert each token vector into a probability distribution over potential next tokens. To do this, we can add one extra linear layer with input dimension `d` and output dimension `V` (i.e., size of the vocabulary), which serves as a classification head, to the end of the model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cad60e3-cec7-4bfa-9ad1-356b6d181f7c_1640x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cad60e3-cec7-4bfa-9ad1-356b6d181f7c_1640x862.png)

Adding a classification head to the decoder-only transformer

Using this linear layer, we can convert each token vector in our output into a probability distribution over the token vocabulary. From the probability distribution over tokens, we can perform:

- _Next token prediction_: the LLM pretraining objective that trains the model to predict the next token for every token within the input sequence using a [cross entropy loss function](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).
    
- _Inference_: autoregressively sample the best next token[13](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-13-142044446) to generate based upon the token distribution generated by the model.
    

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||"""|
||Source: https://github.com/karpathy/nanoGPT/blob/master/model.py|
||"""|
|||
||import torch|
||from torch import nn|
||import torch.nn.functional as F|
|||
||class GPT(nn.Module):|
|||
||def __init__(self,|
||d,|
||H,|
||T,|
||V,|
||layers,|
||bias=False,|
||dropout=0.2,|
||):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||H: number of attention heads|
||T: maximum length of input sequences (in tokens)|
||V: size of the token vocabulary|
||layers: number of decoder-only blocks|
||bias: whether or not to use bias in linear layers|
||dropout: probability of dropout|
||"""|
||super().__init__()|
||self.transformer = nn.ModuleDict(dict(|
||wte=nn.Embedding(V, d), # token embeddings|
||wpe=nn.Embedding(T, d), # position embeddings|
||drop=nn.Dropout(dropout),|
||blocks=nn.ModuleList([Block(d, H, T, bias, dropout) for _ in range(layers)]),|
||ln_f=nn.LayerNorm(d),|
||head=nn.Linear(d, V, bias=bias),|
||))|
|||
||def forward(self, idx, targets=None):|
||# idx is a [B, T] matrix of token indices|
||# targets is a [B, T] matrix of target (next) token indices|
||device = idx.device|
||_, T = idx.size() # [B, T]|
||pos = torch.arange(0, T, dtype=torch.long, device=device)|
|||
||# generate token and position embeddings|
||tok_emb = self.transformer.wte(idx) # [B, T, d]|
||pos_emb = self.transformer.wpe(pos) # [T, d]|
||x = self.transformer.drop(tok_emb + pos_emb)|
|||
||# pass through all decoder-only blocks|
||for block in self.transformer.blocks:|
||x = block(x)|
||x = self.transformer.ln_f(x) # final layer norm|
|||
||if targets is not None:|
||# compute the loss if we are given targets|
||logits = self.transformer.head(x)|
||loss = F.cross_entropy(|
||logits.view(-1, logits.size(-1)),|
||targets.view(-1),|
||ignore_index=-1,|
||)|
||else:|
||# only look at last token if performing inference|
||logits = self.transformer.head(x[:, [-1], :])|
||loss = None|
|||
||return logits, loss|

[view raw](https://gist.github.com/wolfecameron/f574c5c9a61f3b3a045b2cbd9593cfd7/raw/077f4b4703699cf1d738a0f3d8dcc4fdd98d207d/gpt.py)[gpt.py](https://gist.github.com/wolfecameron/f574c5c9a61f3b3a045b2cbd9593cfd7#file-gpt-py) hosted with ❤ by [GitHub](https://github.com/)

**Full architecture (in PyTorch).** The full implementation of the decoder-only transformer architecture is depicted above. Given that we have already discussed each component of this architecture, the code above should be relatively straightforward. The only modifications that are made are:

1. Applying dropout to token and position embeddings prior to passing them as input to the first decoder-only transformer block.
    
2. The addition of a final layer normalization module that normalizes the output of the decoder-only transformer blocks prior to the classification head.
    

Once we have passed our input through all decoder-only transformer blocks, we can either pass all output token embeddings through the linear classification layer, allowing us to apply a next token prediction loss across the entire sequence (i.e., done during pretraining). Or, we can only pass the final output token embedding through the linear classification layer, which allows us to sample the next token to include in the model’s generated output (i.e., done during inference).

## Modern Variants of the Architecture

Now that we understand the decoder-only transformer architecture, we can look at some of the variants of this architecture being used by modern LLMs. In most cases, the core details of the decoder-only transformer are maintained. However, the recent spike of interest in generative LLMs has produced a variety of useful modification to the decoder-only transformer that improve performance, boost speed (both during training and inference), make the training process more stable, allow the model to handle longer input sequences, and much more.

#### Transformer Block Layouts

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0dd7479-8196-439a-8300-5ed21fc10731_964x1410.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0dd7479-8196-439a-8300-5ed21fc10731_964x1410.png)

(from [1])

The layout of the decoder-only transformer block that we have seen so far is the standard transformer block configuration. However, the order of normalization operations within this block may change depending upon the implementation. For example, we can see in the figure above that layer normalization operations are depicted as coming after the attention and feed-forward layers in the original transformer architecture [1]. Additionally, _some architectures perform normalization at both locations_; e.g., Gemma [15] normalizes both the input and the output of each transformer sub-layer, as explained below.

> _“We normalize both input and output of each transformer sub-layer, a deviation from the standard practice of solely normalizing one or the other.”_ - from [15]

**Parallel blocks.** Alternative block structures have been explored within the literature as well. For example, Falcon [14] and PaLM [24] use a parallel transformer block structure that passes input through the attention and feed-forward layers in parallel instead of in sequence; see below. Such an approach lessens the communication costs of distributed training[14](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-14-142044446) and is found by both models to yield no noticeable degradation in performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F746ac795-9569-4435-a0dc-9fa085c3a4a6_1078x668.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F746ac795-9569-4435-a0dc-9fa085c3a4a6_1078x668.png)

(from [14])

#### Normalization Strategies

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa497df28-f15d-476f-95db-19ba5171117c_1052x524.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa497df28-f15d-476f-95db-19ba5171117c_1052x524.png)

RMSNorm formulation

In addition to changing the exact location of normalization layers within the transformer block, the normalization strategy used varies between different models. While most models use layer normalization, **Root Mean Square Layer Normalization [29]** ([RMSNorm](https://github.com/bzhangGo/rmsnorm) for short!) is also popular. RMSNorm, which is formulated as shown above, is just a simplified version of layer normalization that has been shown to improve training stability and generalization. Plus, RMSNorm is 10-50% more efficient than layer normalization despite performing similarly, which has led models like LLaMA [30] and LLaMA-2 [11] to adopt this approach.

**Better layer normalization.** Going further, certain LLMs have adopted modified forms of layer normalization. For example, MPT [26] models use [low precision layer normalization](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html) to improve hardware utilization during training, though this approach may cause loss spikes to arise in rare cases. Similarly, many LLMs (e.g., OLMo [12], LLaMA-2 [11], and PaLM [24]) exclude the bias terms within layer normalization; see below. In fact, _many of these models also exclude bias from all layers of the transformer altogether_! Excluding bias terms within the transformer maintains or improves the LLM’s performance and yields a (modest) speedup.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f7afdf8-1c55-4edc-8a15-66674a1f631e_1136x684.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f7afdf8-1c55-4edc-8a15-66674a1f631e_1136x684.png)

(from [12])

#### Efficient (Masked) Self-Attention

Although self-attention is the foundation of the transformer architecture, this operation is somewhat inefficient—_it is an_ `O(N^2)` _operation_! For this reason, a plethora of efficient attention variants have been proposed; [Reformer](https://arxiv.org/abs/2001.04451), [SMYRF](https://arxiv.org/abs/2010.05315), and [Performer](https://arxiv.org/abs/2009.14794) to name a few. Many of these techniques theoretically reduce the complexity of self-attention to `O(N)`, but they _fail to achieve measurable speedups in practice_. To solve this issue, **FlashAttention [25]** reformulates the self-attention operation in an efficient and IO-aware manner; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55a6c128-6a6a-4a17-b587-06d5bb90f83a_1896x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55a6c128-6a6a-4a17-b587-06d5bb90f83a_1896x1008.png)

(from [25])

The inner workings of FlashAttention are mostly hardware-related; see [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/) for more details. However, the result is a drop-in replacement for the self-attention operation that has a variety of awesome benefits:

- Speeds up BERT-large training time by 15%.
    
- Improves training speed by `3X` for GPT-2.
    
- Enables longer context lengths for LLMs (due to better memory efficiency).
    

After the [PyTorch 2.0 release](https://pytorch.org/blog/accelerated-pytorch-2/), scaled dot product attention—_this is the variant of self-attention we learned about in this post_—can be replaced with FlashAttention to improve efficiency[15](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-15-142044446)! For this reason, many recent LLMs (e.g., Falcon [14] and MPT [26]) use FlashAttention. Plus, there is still active research being published in this area, which has resulted in some interesting developments:

1. [FlashAttention-2](https://arxiv.org/abs/2307.08691): modifies FlashAttention to yield further gains in efficiency.
    
2. [FlashDecoding](https://pytorch.org/blog/flash-decoding/): an extension of FlashAttention that focuses upon improving inference efficiency in addition to training efficiency.
    
    [
    
    ![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c6fdc1-8f5f-4ce2-89b4-3ee33123f207_2004x1162.png)
    
    
    
    ](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71c6fdc1-8f5f-4ce2-89b4-3ee33123f207_2004x1162.png)
    
    Multi-query attention shares key and value projections between attention heads (from [1])
    

**Multi and Grouped Query Attention.** Beyond FlashAttention, several recent LLMs (e.g., [Gemini](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction) [27], Falcon [14], and PaLM [24]) use multi-query attention, an efficient self-attention implementation that shares key and value projections between all attention heads in a layer; see above. Instead of performing a separate projection for each head, all heads share the same projection matrix for keys and the same projection matrix for values. This change does not make training any faster, but it significantly improves the inference speed of the resulting LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)

(from [28])

Unfortunately, multi-query attention can cause slight deteriorations in performance, which led some LLMS (e.g., LLaMA-2) to search for alternatives. Instead of sharing all key and value projections across attention heads, grouped-query attention (GQA) [28] divides the `H` total self-attention heads into groups and shares key/value projections within the same group; see above. Such an approach is an interpolation between vanilla multi-headed self-attention and multi-query attention, which uses a shared key and value projection across all `H` heads. Interestingly, _GQA maintains the performance of vanilla multi-headed causal self-attention and achieves comparable efficiency compared to multi-query attention_.

#### Better Positional Embeddings

> _“We find that transformer language models (LMs) that use sinusoidal position embeddings have very weak extrapolation abilities.”_ - from [31]

The position embedding technique we have learned about so far uses additive positional embeddings determined by the absolute position of each token in a sequence. Although this approach is simple, it limits the model’s ability to generalize to sequences longer than those seen during training. As a result, we must pretrain the LLM over longer sequences (i.e., this can be quite expensive) if we need to accept longer inputs at inference time. For this reason, a variety of alternative position encoding schemes were proposed, including [relative position](https://jaketae.github.io/study/relative-positional-encoding/) embeddings that only consider the distance between tokens rather than their absolute position. Here, we will study two of the most commonly used strategies for injecting position information into an LLM—_RoPE [23] and ALiBi [31]_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F252af5ba-5791-46aa-b8b4-f06fa201c9ae_1338x842.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F252af5ba-5791-46aa-b8b4-f06fa201c9ae_1338x842.png)

(from [23])

**Rotary Positional Embeddings (RoPE) [23]** are a hybrid of absolute and relative positional embeddings that incorporate position into self-attention by:

1. Encoding absolute position with a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix).
    
2. Adding relative position information directly into the self-attention operation.
    

Notably, RoPE injects position information at every layer of the transformer, rather than just the model’s input sequence. Such an approach is found to yield a balance between absolute and relative position information, provides flexibility to expand to longer sequence lengths, and has decaying inter-token dependency as relative distances increase (i.e., tokens that are far apart pay less attention to each other). RoPE has gained in popularity recently, leading in its use in popular LLMs like PaLM[16](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-16-142044446) [24], Falcon [14], OLMo [12], LLaMA/LLaMA-2 [11, 30], and more!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a7901b5-cf22-4de7-8f68-89df89c1ba1f_1878x766.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a7901b5-cf22-4de7-8f68-89df89c1ba1f_1878x766.png)

(from [31])

**Attention with Linear Biases [31]** is a follow-up technique that was proposed to improve the extrapolation abilities of position embedding strategies. Instead of using position embeddings, ALiBi incorporates position information directly into self-attention at each layer of the transformer by adding a static, non-learned bias to the attention matrix; see above. We compute the attention matrix normally (i.e., as a product of the query and key matrices) but add a constant bias to the values of the attention matrix that penalizes scores between more distant queries and keys. We can implement this approach very easily by adding these extra biases to the attention mask that is used for computing causal self-attention.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96815637-a806-458c-a222-c70c680ccc28_1882x1042.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96815637-a806-458c-a222-c70c680ccc28_1882x1042.png)

(from [31])

Despite its simplicity, this approach outperforms both vanilla position embedding techniques and RoPE in terms of extrapolating to sequences longer than those seen in training; see above. Plus, compute and memory costs are not increased significantly. ALiBi was adopted by the MPT models [26], which were finetuned to support input lengths up to (and exceeding) [65K tokens](https://huggingface.co/mosaicml/mpt-7b-storywriter)!

## Takeaways

Despite the staggering pace of innovation, decoder-only transformers remain the cornerstone of research on generative LLMs. In fact, model architectures used by most modern LLMs, though much larger and modified in nuanced ways, largely match the architecture of the original GPT model [3]. As such, building a working understanding of the decoder-only transformer architecture is an absolute necessity for anyone interested in better understanding the inner workings of a language model. From the information in this overview, we can decompose our understanding of decoder-only transformer models into the following core ideas.

**Constructing the input.** Decoder-only transformers receive a textual prompt as input. First, we use a tokenizer—_based upon an algorithm like Byte-Pair Encoding_—to break this text into discrete tokens. Then, we map each of these tokens to a corresponding token vector stored within an embedding layer. This process forms a sequence of token vectors that are passed to the model as input. Optionally, we can augment these token vectors with additive positional embeddings.

**Causal self-attention** is the core of the decoder-only transformer and allows the model to learn from relationships between tokens in the input. The vanilla self-attention operation transforms each token’s representation by taking a weighted combination of other token representations, where weights are given by pairwise attention (or importance) scores between tokens. Causal self-attention follows a similar strategy but only computes attention scores for preceding tokens in the sequence. Attention is performed in parallel across several heads, each of which can focus upon different parts of the input sequence.

**Feed-forward transformations** are performed within each block of the decoder-only transformer, allowing us to individually transform each token’s representation. This feed-forward component is a small neural network that is applied in a pointwise manner to each token vector. Given a token vector as input, we pass this vector through a linear projection that increases its size by ~4X, apply a non-linear activation function (e.g., SwiGLU or GeLU), then perform another linear projection that restores the original size of the token vector.

**Transformer blocks** are stacked in sequence to form the body of the decoder-only transformer architecture. The exact layout of the decoder-only transformer block may change depending upon the implementation, but two primary sub-layers are always present:

1. Causal self-attention
    
2. Feed-forward transformation
    

Additionally, these sub-layers are surrounded by a layer normalization module—_either before or after the sub-layer (or both!)_—and a residual connection.

**Classification head.**  The decoder-only transformer has one final classification head that takes token vectors from the transformer’s final output layer as input and outputs a vector with the same size as the vocabulary of the model’s tokenizer. This vector can be used to either train the LLM via next token prediction or generate text at inference time via sampling strategies like nucleus sampling and beam search.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), and this is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[2] Zehui, Lin, et al. "Dropattention: A regularization method for fully-connected self-attention networks." _arXiv preprint arXiv:1907.11065_ (2019).

[3] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018). 

[4] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[5] Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of training deep feedforward neural networks." _Proceedings of the thirteenth international conference on artificial intelligence and statistics_. JMLR Workshop and Conference Proceedings, 2010.

[6] Ioffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." _International conference on machine learning_. pmlr, 2015.

[7] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer normalization." _arXiv preprint arXiv:1607.06450_ (2016).

[8] Wu, Yuxin, and Kaiming He. "Group normalization." _Proceedings of the European conference on computer vision (ECCV)_. 2018.

[9] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. "Instance normalization: The missing ingredient for fast stylization." _arXiv preprint arXiv:1607.08022_ (2016).

[10] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[11] Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." _arXiv preprint arXiv:2307.09288_ (2023).

[12] Groeneveld, Dirk, et al. "Olmo: Accelerating the science of language models." _arXiv preprint arXiv:2402.00838_ (2024).

[13] Shazeer, Noam. "Glu variants improve transformer." _arXiv preprint arXiv:2002.05202_ (2020).

[14] Almazrouei, Ebtesam, et al. "The falcon series of open language models." _arXiv preprint arXiv:2311.16867_ (2023).

[15] Google DeepMind (Gemma Team). “Gemma: Open Models Based on Gemini Research and Technology” (2024).

[16] He, Kaiming, et al. "Deep residual learning for image recognition." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

[17] Jastrzębski, Stanisław, et al. "Residual connections encourage iterative inference." _arXiv preprint arXiv:1710.04773_ (2017).

[18] Veit, Andreas, Michael J. Wilber, and Serge Belongie. "Residual networks behave like ensembles of relatively shallow networks." _Advances in neural information processing systems_ 29 (2016).

[19] Li, Hao, et al. "Visualizing the loss landscape of neural nets." _Advances in neural information processing systems_ 31 (2018).

[20] Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural machine translation of rare words with subword units." _arXiv preprint arXiv:1508.07909_ (2015).

[21] Kudo, Taku, and John Richardson. "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing." _arXiv preprint arXiv:1808.06226_ (2018).

[22] Wu, Yonghui, et al. "Google's neural machine translation system: Bridging the gap between human and machine translation." _arXiv preprint arXiv:1609.08144_ (2016).

[23] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." _arXiv preprint arXiv:2104.09864_ (2021).

[24] Chowdhery, Aakanksha, et al. "Palm: Scaling language modeling with pathways." _arXiv preprint arXiv:2204.02311_ (2022).

[25] Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." _Advances in Neural Information Processing Systems_ 35 (2022): 16344-16359.

[26] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” _Databricks_, 5 May 2023, https://www.databricks.com/blog/mpt-7b.

[27] Google Gemini Team et al. “Gemini: A Family of Highly Capable Multimodal Models”, [https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) (2023).

[28] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." _arXiv preprint arXiv:2305.13245_ (2023).

[29] Zhang, Biao, and Rico Sennrich. "Root mean square layer normalization." _Advances in Neural Information Processing Systems_ 32 (2019).

[30] Touvron, Hugo, et al. "Llama: Open and efficient foundation language models." _arXiv preprint arXiv:2302.13971_ (2023).

[31] Press, Ofir, Noah A. Smith, and Mike Lewis. "Train short, test long: Attention with linear biases enables input length extrapolation." _arXiv preprint arXiv:2108.12409_ (2021).

[32] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." _Advances in Neural Information Processing Systems_ 36 (2024).

[33] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in Neural Information Processing Systems_ 35 (2022): 27730-27744.

[34] Ahmadian, Arash, et al. "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs." _arXiv preprint arXiv:2402.14740_ (2024).

[1](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-1-142044446)

Here, the `//` symbol indicates that we are performing integer division.

[2](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-2-142044446)

To perform dropout on attention scores, we just pass the attention matrix through a [Dropout module](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) in PyTorch.

[3](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-3-142044446)

Many different types of normalization exist; see [here](https://en.wikipedia.org/wiki/Normalization_\(statistics\)) for more details. In deep learning, we typically use the [standard score](https://en.wikipedia.org/wiki/Standard_score) form of normalization, which transforms each value by subtracting the mean and dividing by the standard deviation

[4](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-4-142044446)

Notably, group [8] and instance [9] normalization are also widely-used normalization techniques. However, these normalization techniques are more commonly used for computer vision, so we avoid including them in the discussion here. See [this article](https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf) for a more comprehensive discussion.

[5](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-5-142044446)

This is typically done by taking an [exponentially moving average](https://towardsdatascience.com/intuitive-explanation-of-exponential-moving-average-2eb9693ea4dc) of mean and standard deviation values that are observed for each mini-batch during training.

[6](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-6-142044446)

In fact, the original motivation for layer normalization was the simple fact that batch normalization did not work for recurrent neural networks! As is explained in [7], batch normalization struggles to handle small batch sizes and inputs that vary across time steps, such as for recurrent neural networks. For these cases, normalizing each input independently (via layer normalization) works much better.

[7](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-7-142044446)

The word “pointwise” indicates that the same operation is applied to every token vector in the sequence. In this case, we individually pass every token vector in the sequence through the same feed-forward neural network with the same weights.

[8](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-8-142044446)

As explained in the NanoGPT repository and a few related publications [11, 12], excluding bias (for both linear/feed-forward layers and layer normalization) actually makes LLMs slightly better, faster, and more stable!

[9](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-9-142044446)

We can also apply residual connections to layers that have differently-sized inputs and outputs. To do this, we just have to add an extra linear projection that transforms the shape of the input to match that of the output.

[10](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-10-142044446)

For those who are not familiar with computing gradients in a neural network via backpropagation, check out [this awesome book chapter](http://neuralnetworksanddeeplearning.com/chap2.html).

[11](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-11-142044446)

In fact, the first project I worked on involving ([BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)-style) language models was nearly a failure! After a few weeks of trying to debug the model’s performance, I fixed most of the issues I was facing by training a custom tokenizer over my own data.

[12](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-12-142044446)

For masked self-attention, this is not 100% accurate, as the position of the token in the sequence may change which tokens are masked or not masked.

[13](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-13-142044446)

There are numerous different ways that we can perform this sampling at inference time; e.g., greedy selection, modifying the temperature, nucleus sampling, top-K sampling, and more. See [here](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) for a great overview of decoding techniques.

[14](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-14-142044446)

In particular, the parallel block reduces the communication costs of tensor parallel training by reducing the required number of `all_reduce` operations from two to one within each transformer block.

[15](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-15-142044446)

See the `CausalSelfAttention` implementation in [this file](https://github.com/karpathy/nanoGPT/blob/master/model.py) for an example invocation of FlashAttention in PyTorch.

[16](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse#footnote-anchor-16-142044446)

See [here](https://github.com/lucidrains/PaLM-rlhf-pytorch/blob/main/palm_rlhf_pytorch/palm.py) for an implementation of RoPE embeddings within PaLM.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Dr. Holger Lindberg Joergensen's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf810c6f-a979-4fc0-ba4a-e4361992d16e_144x144.png)



](https://substack.com/profile/46511-dr-holger-lindberg-joergensen)

[

![Andreas Pester's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7380ca77-df7b-40d4-9100-96ba6259061e_144x144.png)



](https://substack.com/profile/135907579-andreas-pester)

[

![R Deeksha's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031416cb-3e1a-4d9a-8405-4a48dd63bfff_96x96.jpeg)



](https://substack.com/profile/209943426-r-deeksha)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

117 Likes∙

[7 Restacks](https://substack.com/note/p-142044446/restacks?utm_source=substack&utm_content=facepile-restacks)

117

- 

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

7

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Sam's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d2c2a42-ebd9-437b-a596-36f04e639689_144x144.png)



](https://substack.com/profile/12042942-sam?utm_source=comment)

[Sam](https://substack.com/profile/12042942-sam?utm_source=substack-feed-item)

[2024年3月5日](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comment/50941496 "2024年3月5日 09:22")

Liked by Cameron R. Wolfe, Ph.D.

A question that I have at the end -- if ALiBi indeed "outperforms" both RoPE and vanilla positional embedding techniques (with respect to extrapolating to longer sequences), why was it "only" (*) used for MPT, rather than for recent models like LLaMA-2, which used RoPE?

I'd like to better understand (maybe a different post, maybe even one you've done) the dimensions across which I'd compare different attention variants.

Like (2)

Reply

Share

[3 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comment/50941496)

[

![Srikar's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fyellow.png)



](https://substack.com/profile/218087326-srikar?utm_source=comment)

[Srikar](https://substack.com/profile/218087326-srikar?utm_source=substack-feed-item)

[2024年3月22日](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comment/52214525 "2024年3月22日 22:12")

Liked by Cameron R. Wolfe, Ph.D.

Is it a typo?

which has an input dimension of d and an output dimension of h = 3 * d,

in the code it is 4*d

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comment/52214525)

[12 more comments...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

TopLatestDiscussions

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

[Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

[Understanding how SFT works from the idea to a working implementation...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Sep 11, 2023 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

55

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68686a01-2b31-4694-8c04-a562ffd725ad_2210x1244.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture




----



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation

### What two decades of research taught us about sparse language models...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Mar 18, 2024

98

- 

[

16

](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comments)

15

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facddd5f6-9938-4de0-80ca-b673b4b34bd8_2330x1340.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facddd5f6-9938-4de0-80ca-b673b4b34bd8_2330x1340.png)

Modern advancements in large language models (LLMs) are mostly a product of scaling laws [6]. As we increase the size of the underlying model, we see a smooth increase in performance, assuming that the model is trained over a sufficiently large dataset [7]. Such scaling laws eventually led us to the creation of GPT-3, as well as other (more powerful) LLMs that followed it. When scaling up these dense LLMs, we will eventually reach a hardware-imposed limit due to heightened memory costs and the dependence of the model’s compute footprint on the total number of parameters. As a result, compute and data have become the primary bottleneck for training better LLMs—_creating a better model is relatively simple given access to more compute and data_. In this overview, we will study how this limitation can be avoided by training sparsely activated language models.

> _“As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models have become one of the most promising model architectures due to their significant training cost reduction compared to quality equivalent dense models.”_ - from [12]

Mixture-of-Experts (MoE) layers are simple and allow us to increase the size or capacity of a language model without a corresponding increase in compute. We just replace certain layers of the model with multiple copies of the layer—_called “experts”_—that have their own parameters. Then, we can use a gating mechanism to (sparsely) select the experts used to process each input. This idea has its roots in research on conditional computation in the early 1990s [15, 30] and allows us to train massive models in a tractable manner, which is helpful in domains—_such as language modeling_—that benefit from models with extra capacity. Here, we will study the MoE, its origins, and how it has evolved over the last two decades.

#### TL;DR: What is a Mixture of Experts (MoE)?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9540283e-b4bc-4c3f-943c-5df959b1733b_1656x818.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9540283e-b4bc-4c3f-943c-5df959b1733b_1656x818.png)

Decoder-only transformer architecture

The standard decoder-only transformer architecture used by most generative LLMs is shown in the figure above; see [here](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) for an in-depth overview of this architecture. In the context of LLMs, MoEs make a simple modification to this architecture—_we replace the feed-forward sub-layer with an MoE layer_! This MoE layer is comprised of several experts (i.e., anywhere from a few experts [13] to thousands [5]), where each expert is its own feed-forward sub-layer with an independent set of parameters; see below for a depiction.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8111341-9453-4b46-820b-34283ea32673_1502x868.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8111341-9453-4b46-820b-34283ea32673_1502x868.png)

MoE layer in a transformer block (from [5])

We can similarly convert encoder-decoder transformers, such as [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [8], into MoE models by replacing feed-forward sub-layers in both the encoder and the decoder with MoE layers. However, we usually only convert a portion of these sub-layers (e.g., every other layer) to MoE layers. In this overview, we will primarily overview work with MoE models based upon the encoder-decoder transformer, while work on MoE-based autoregressive LLMS will be covered in a future post.

> _“The ST-MoE models have 32 experts with an expert layer frequency of 1/4 (every fourth FFN layer is replaced by an MoE layer).”_ - from [24]

**Sparse experts.** This approach might seem problematic because it adds a ton of parameters to the model. The MoE model has multiple independent neural networks (i.e., instead of a single feed-forward neural network) within each feed-forward sub-layer of the transformer. However, _we only use a small portion of each MoE layer’s experts in the forward pass_! Given a list of tokens as input, we use a routing mechanism to sparsely select a set of experts to which each token will be sent. For this reason, the computational cost of an MoE model’s forward pass is much less than that of a dense model with the same number of parameters.

**Components of an MoE layer.** When applied to transformer models, MoE layers have two primary components:

- _Sparse MoE Layer_: replaces dense feed-forward layers in the transformer with a sparse layer of several, similarly-structured “experts”.
    
- _Router_: determines which tokens in the MoE layer are sent to which experts.
    

Each expert in the sparse MoE layer is just a feed-forward neural network with its own independent set of parameters[1](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-1-142423094). The architecture of each expert mimics the feed-forward sub-layer used in the standard transformer architecture. The router takes each token as input and produces a probability distribution over experts that determines to which expert each token is sent; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f6008b-3ce6-4d9e-bf3f-eb039ba5fb31_1436x722.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f6008b-3ce6-4d9e-bf3f-eb039ba5fb31_1436x722.png)

Structure of an MoE layer (from [5])

The router has its own set of parameters and is trained jointly with the rest of the network. Each token can be sent to many experts, but we impose sparsity by only sending a token to its top-`K` experts. For example, many models set `k=1` or `k=2`, meaning that each token is processed by either one or two experts, respectively.

**Greater capacity, fixed computation.** Increasing the size and capacity of a language model is one of the primary avenues of improving performance, assuming that we have access to a sufficiently large training dataset. However, the cost of training increases with the size of the underlying model, which makes larger models burdensome in practice. MoE models avoid this expense by only using a subset of the model’s parameters during inference.

> _“Assuming just two experts are being used per token, the inference speed is like using a 12B model (as opposed to a 14B model), because it computes 2x7B matrix multiplications, but with some layers shared”_ - from [HuggingFace MoE blog](https://huggingface.co/blog/moe)

For example, assume we have a 7B parameter LLM and replace each of its feed-forward sub-layers with an MoE layer comprised of eight experts, where two experts are activated for each token. This is the exact architecture that was used for [Mixtral-8x7B](https://mistral.ai/news/mixtral-of-experts/) [13], an MoE variant of [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) [14]. The full model has about 47B parameters[2](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-2-142423094), all of which must be loaded into memory. But, the model’s inference cost is comparable to a 14B parameter model. Only two experts are used to process each token, which yields ~2 x 7B matrix multiplications[3](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-3-142423094). In this way, we achieve the capacity of a ~50B parameter model without incurring the cost!

**Pros and cons.** MoE models are widely used due to their ability to train larger models with a fixed compute budget, but using an MoE-style LLM has both pros and cons! MoE models pretrain faster compared to dense models, as well as have much faster inference compared to a dense model with the same number of parameters. _MoEs allow us to increase model capacity while keeping the amount of compute that we use (relatively) low_. However, MoE-style LLMs also consume more VRAM, as all experts must be loaded into memory. Additionally, MoE models are prone to overfitting and notoriously difficult to finetune, which makes them more complicated to use in practical applications compared to dense models.

## Origins of the Mixture-of-Experts

Although MoEs are very popular in recent AI research, the idea has been around for a long time. In fact, the concept of conditional computation—_or dynamically turning parts of a neural network on/off_—has its roots in work from the early 1990s! In this section, we will explore this early work on conditional computation and study how it has evolved to form the idea of an MoE. Eventually, these ideas were applied (with some success) to training early, [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)-based [4] language models.

#### Early Work on Conditional Computation

> _“Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation”_ - from [1]

The idea of an MoE has its origins in work done by [Geoffrey Hinton](https://www.cs.toronto.edu/~hinton/) in the early 1990’s [15], which proposes a supervised learning framework for an ensemble of several networks. Each of these networks is expected to process a different subset of the training data, and the choice of which network to use is handled by a gating mechanism; see below. In other words, each network in this system is an expert that specializes in processing some domain (or region) of the input data.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33dfb1a-7b1b-435f-8fd0-4aaf17b1989e_1064x1290.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33dfb1a-7b1b-435f-8fd0-4aaf17b1989e_1064x1290.png)

(from [15])

Since the proposal of [15], several works have explored and extended this idea of conditional computation. For example, authors in [30] (also written in the early 1990s!) propose a hierarchical (tree-structured) MoE that can be trained in a supervised fashion using an [expectation-maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm); see below. There are many such works that study this topic in-depth.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27ef6b06-0b78-4701-8402-c32fd048fac0_1114x1110.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27ef6b06-0b78-4701-8402-c32fd048fac0_1114x1110.png)

(from [30])

**Estimating or propagating gradients through stochastic neurons for conditional computation [16].** This work explores four possible techniques for estimating the gradient of stochastic neurons and hard-threshold activation functions; e.g., by using [REINFORCE](https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code), a straight-through estimator, or additive/multiplicative noise. Although these terms might seem unfamiliar, conditional computation is one example of a hard-threshold activation function—_the activations of certain neurons are completely eliminated (or set to zero) from the computation_. The estimators that are proposed in [16] are necessary for training networks that use conditional computation. The neural network structure explored in [16] is comprised of many computational units connected by a distributed network of gates that can be used to eliminate chunks of computation. By turning off portions of this network, we can greatly reduce the computational cost of a large neural network.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c50ae4f-e799-402f-9480-96c9e8dc73b5_1612x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c50ae4f-e799-402f-9480-96c9e8dc73b5_1612x612.png)

(from [17])

**Low-rank approximations for conditional feedforward computation in deep neural networks [17].** This work explores a neural network setup in which the nodes of the network are supplemented with gating units that determine whether a node should be calculated or not; see above. Because [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) sets all negative activation values to zero within a neural network, any node with a negative pre-activation value can be removed from the computation altogether. Expanding upon this idea, authors show in [17] that the sign of hidden activations prior to ReLU can be estimated by computing a low-rank approximation of the weight matrix, resulting in significant efficiency gains when the network is sparse.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6996f044-db6d-486b-8e5e-08bbfe8346ba_1358x1108.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6996f044-db6d-486b-8e5e-08bbfe8346ba_1358x1108.png)

(from [18])

**Learning factored representations in a deep mixture of experts [18].** This work considers MoE layers comprised of several expert networks that specialize in processing different regions of the input space. Inputs are mapped to these experts using a learned gating mechanism, allowing larger networks to be computed sparsely. Going further, authors in [18] consider deeper networks with multiple, consecutive MoE layers (i.e., a “stacked” MoE), as shown in the figure above. When testing these networks on the MNIST dataset, we see that the Deep MoE learns unique features at each layer; e.g., the first layer learns location-dependent features, while the second layer learns class-specific features.

> _“Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters on-demand, on a per-example basis.”_ - from [19]

**Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning [19].** Larger models are widely known to perform better[4](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-4-142423094). If the amount of computation was not an issue, we could use larger datasets and models to yield improved generalization, but the amount of computation used by a deep network increases with the parameter count. To avoid this issue and better leverage conditional computation, authors in [19] propose a novel weight matrix parameterization that turns off groups of parameters when specific patterns of hidden activations are observed[5](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-5-142423094). Such an approach can exponentially increase the ratio of parameters to computation.

**Conditional computation in neural networks for faster models [20].** This work explores reinforcement learning-based training strategies (i.e., REINFORCE) for neural networks that use conditional computation. We see that this approach, which uses [policy gradient algorithms](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of), can be used to train networks in a way that maintains accuracy while maximizing computation speed. Plus, we can use regularization to diversify the patterns of conditional computation observed in the networks. In experiments, this learning strategy is shown to yield networks that achieve a favorable balance between compute cost and performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d875176-8eb0-42a1-9f24-636de24f79f4_1286x934.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d875176-8eb0-42a1-9f24-636de24f79f4_1286x934.png)

(from [21])

**Dynamic Capacity Networks (DCNs) [21]** adaptively assign capacity to different portions of the input data by defining low-capacity and high-capacity sub-networks. For most of the data, low-capacity sub-networks are applied. However, a gradient-based attention mechanism can be used to select portions of the input—_based upon the network’s sensitivity to this region of the data_—to which high-capacity sub-networks will be applied; see above. We see in experiments that DCNs can reduce computation while achieving performance that is comparable to (or better than) that of traditional convolutional neural networks (CNNs).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5864b3a-9982-4130-8054-581821cd5604_1608x1148.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5864b3a-9982-4130-8054-581821cd5604_1608x1148.png)

(from [22])

**Deep sequential neural networks [22]** discards the traditional notion of a “layer” used within neural network architectures, choosing instead to construct an entire set of candidate transformations that can be selectively applied at each layer. When processing an input, one element from the candidate set is chosen at each layer, forming a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)-like architecture based upon a sequence of transformations. Depending upon properties of the input data, the selected sequence of transformations may vary, which increases the expression power of the model compared to classical multi-layer networks. Similar to prior work, this sequential architecture is trained using a policy gradient approach.

#### **[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) [1]**

> _“We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state-of-the-art results on public language modeling and translation data sets.”_ - from [1]

The idea of conditional computation holds an incredible amount of promise. Especially in the domain of language modeling where training datasets tend to be massive[6](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-6-142423094), the ability to increase the model’s underlying capacity without causing a corresponding increase in computation is enticing to say the least. Despite being studied for over two decades, MoEs fell short of their promise due to various technical challenges. In [1], authors attempt to overcome some of these challenges with the proposal of the Sparsely-Gated Mixture-Of-Experts layer, _showing that MoEs can be applied to language modeling and translation domains_.

**Prior issues with MoEs.** Although work in [1] was one of the first to apply MoEs to language modeling, the idea of an MoE dates back to the early 1990s. With this in mind, we might wonder: _What prevented MoEs from being more widely adopted?_ Several technical roadblocks contributed to this lack of adoption:

- GPUs are great at performing arithmetic efficiently, but they are not as good at [branching](https://enccs.github.io/gpu-programming/3-gpu-problems/#what-are-gpus-not-good-for) (i.e., a major component of conditional computation).
    
- Large batch sizes are needed to train neural networks efficiently, but MoEs reduce batch sizes (i.e., each expert only receives a portion of the input batch).
    
- Increased model capacity is most impactful when studying domains with larger training datasets, and MoEs were largely studied in computer vision domains on problems with insufficiently-large training datasets.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9da5280e-f349-43fb-8218-4235728ef69d_1600x870.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9da5280e-f349-43fb-8218-4235728ef69d_1600x870.png)

(from [1])

**What is a Sparse MoE?** The style of MoE layers used by most modern LLMs is similar to the sparsely-gated mixture-of-experts layer proposed in [1] (depicted above), which is a generic neural network module with two components:

- _Experts_: each layer has several “experts” that are standalone neural network modules or layers with independent sets of parameters.
    
- _Router_: a parametric (and learnable) gating mechanism that selects a (sparse) group of experts used to process each input.
    

In [1], each expert within an MoE is a feed-forward neural network with an identical architecture. However, more complex architectures could be used; e.g., we can even create “hierarchical” MoE modules by implementing each expert as another MoE! Both the experts and the gating mechanism are jointly trained along with other network parameters via gradient descent.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f66db0-1360-408d-966d-197f69ad6115_1070x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f66db0-1360-408d-966d-197f69ad6115_1070x606.png)

Computing the output of an MoE module

To compute the output of an MoE module, we take a weighted combination of expert outputs, where weights are provided by the router; see above. The router outputs a `N`-dimensional vector of weights (i.e., `N` is the number of experts). Although this approach might not initially seem useful, the magic happens when the router’s output is sparse—_experts that receive a weight of zero are no longer considered when computing the output of an MoE_. For this reason, MoEs allow us to create, train and use very large neural networks without significant compute requirements, as only a portion of model parameters are used at any given time.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5f416c2-8e3a-4ba1-b291-6107f08c1bb1_792x76.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5f416c2-8e3a-4ba1-b291-6107f08c1bb1_792x76.png)

A simple softmax gating mechanism (from [1])

**Gating mechanism.** Many different strategies have been proposed for routing within an MoE. The simplest approach would be to multiply our input by a weight matrix and apply [softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html); see above. However, this approach does not guarantee that the selection of experts will be sparse[7](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-7-142423094). To solve this issue, authors in [1] propose a modified gating mechanism that adds sparsity and noise to this simplistic softmax gating mechanism; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1d300e3-87a7-4ca0-9b30-e21b32ea5720_1928x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1d300e3-87a7-4ca0-9b30-e21b32ea5720_1928x1182.png)

Routing mechanism implementation for sparse MoE (from [1])

The gating mechanism above performs routing similarly to the softmax gating mechanism, but we include two additional steps:

1. An adjustable amount of [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise) is added to the output of the router prior to applying softmax.
    
2. All but the output of the top-`K` experts are masked (i.e., set to -∞) to ensure that the selection of experts is sparse.
    

**Balancing the experts.** One issue with MoEs is that the network has a tendency to repeatedly utilize the same few experts during training. Instead of learning to use all experts uniformly, the gating mechanism will converge to a state that always selects the same set of experts for every input. This is a self-fulfilling loop—_if one expert is selected most frequently, it will be trained more rapidly and, therefore, continue to be selected over the other experts_! Prior work has proposed several approaches for solving this issue [2, 3], but we see in [1] that experts can be balanced by adding a simple “soft” constraint to the training loss; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fe223b2-73e9-4f32-b42f-88ddf26cf7a4_1930x758.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fe223b2-73e9-4f32-b42f-88ddf26cf7a4_1930x758.png)

Auxiliary loss for balancing expert utilization (from [1])

We first define an “importance” score for each expert over a batch of input data, which can be computed by taking a sum over the gate values for each expert across the batch. Put simply, experts that are selected frequently in the batch will have a high importance score. Then, we can compute an auxiliary loss function by taking the squared [coefficient of variation (CV)](https://en.wikipedia.org/wiki/Coefficient_of_variation) of expert importance scores; see above. If experts all have very similar importance scores, the CV will be small and vice versa. This loss term can be added to the model’s training objective to encourage experts to receive equal importance within each batch.

**Empirical evaluation.** To test the performance of their sparse MoE layer, authors in [1] train a language model that inserts an MoE module between stacked layers of an [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)[8](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-8-142423094). The MoE module is applied [convolutionally](https://www.youtube.com/watch?v=KuXjwB4LzSA), meaning that several LSTM time steps are processed at once (in batch). Additionally, a new distributed training strategy is proposed that combines [model and data parallel training](https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/) to eliminate issues with shrinking batch sizes in the MoE. Such a strategy, along with some other system optimizations, allows language models with as many as 137B parameters to be trained with reasonable compute requirements!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16acf073-c546-4642-93d3-e6fde256fed4_1342x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16acf073-c546-4642-93d3-e6fde256fed4_1342x646.png)

(from [1])

These models are evaluated on language modeling datasets of varying sizes (i.e., 1, 10, and 100 billion words). Both flat and hierarchical MoEs are tested with varying numbers of experts, and all models are trained such that their total computational cost is roughly equal (i.e., the number of active experts `k` is the same)[9](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-9-142423094). Even when studying the smallest dataset, we see that the largest of the MoE models yields a noticeable improvement in [perplexity](https://cameronrwolfe.substack.com/i/141461162/the-evaluation-process) (lower is better); see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cf574f0-306a-4f62-951f-07b4419cdfe0_2132x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cf574f0-306a-4f62-951f-07b4419cdfe0_2132x860.png)

(from [1])

If the size of the training dataset is sufficiently small, then adding more capacity via more experts has diminishing returns, as shown above. However, we see on larger datasets that performance continues to improve up to a size of 68B parameters (i.e., 65,536 experts)! Such a finding emphasizes the synergy between MoE layers and the language modeling domain—_added model capacity is helpful given a sufficiently large training corpus_. Work in [1] showed for the first time that these benefits could be achieved in practice without noticeably increasing compute costs, setting the stage for MoEs to become a popular tool for LLMs.

> _“The widening gap between the two lines demonstrates (unsurprisingly) that increased model capacity helps more on larger training sets.”_ - from [1]

## Applying Mixture-of-Experts to Transformers

Now that we have studied early work on conditional computation, we can take a look at some applications of MoEs to the transformer architecture. Today, MoE-based LLM architectures (e.g., [Mixtral](https://arxiv.org/pdf/2401.04088.pdf) [13] or [Grok](https://x.ai/blog/grok-os)) have become popular, but MoEs were explored for language models in several stages. Here, we will take a look at applying MoEs to the [encoder-decoder transformer architecture](https://cameronrwolfe.substack.com/i/135273362/the-transformer-architecture-and-its-variants). This work is relevant to modern LLM applications and derives countless lessons for using MoEs effectively in practice. Such research sets the stage for later work that explores MoE-based generative LLMs, which will be covered in a future overview.

#### **[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) [5]**

After the proposal of the sparsely-gated MoE in [1], work on transformers and language models had yet to begin using these ideas—_adoption was hindered by the general complexity of MoEs, as well as issues like high communication costs and training instability_. Aiming to solve these issues, authors in [5] propose an MoE-based encoder-decoder transformer architecture, called the Switch Transformer, that uses a simplified gating mechanism to make training more stable, thus making MoEs a more realistic and practical choice for language modeling applications.

> _“Simple architectures— backed by a generous computational budget, data set size and parameter count—surpass more complicated algorithms”_ - from [5]

When the Switch Transformer was proposed, researchers were just beginning to study [neural scaling laws](https://cameronrwolfe.substack.com/i/88082618/scaling-laws-for-neural-language-models) [6], which show that a language model’s performance smoothly improves as we increase its size[10](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-10-142423094). With this in mind, work in [5] is motivated by the fact that MoE models allow us to add another dimension to the analysis of neural scaling laws—_we can increase model size while keeping the computational complexity of the model’s forward pass constant_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0da5bff-db95-4891-9369-647d7f8f0ef6_1790x1254.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0da5bff-db95-4891-9369-647d7f8f0ef6_1790x1254.png)

(from [5])

**Applying MoE to transformers.** To create an MoE variant of an encoder-decoder transformer, we can simply convert the feed-forward sub-layers of the model into MoE layers; see above. The feed-forward transformation is applied in a pointwise fashion, meaning that each token is passed individually through the feed-forward network. For this reason, each token within the sequence is individually routed to its set of corresponding experts. For example, each token in the sequence `[“I”, “love”, “LLM”, “#s”]` is passed through the routing function, forming a probability distribution over experts. Then, we select the top-`K` experts for each individual token—_tokens in the same sequence are not always sent to the same experts_.

**Better routing.** In prior work, the minimum number of active experts in any MoE layer was two. Using at least two experts was thought to be necessary to have non-trivial gradients in the routing function[11](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-11-142423094). In [5], authors propose routing each token to only a single expert—_this is called a switch layer_. By routing to a single expert, we simplify the routing function, reduce computational overhead, and lessen communication costs while improving the model’s performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e3619d7-93ba-4264-a2bf-d726cacf5f59_944x814.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e3619d7-93ba-4264-a2bf-d726cacf5f59_944x814.png)

Softmax routing in the switch transformer (from [5])

The routing function used by the Switch Transformer (shown above) is just a softmax gating mechanism. We pass each token vector through a linear layer that produces an output of size `N` (i.e., the number of experts), then apply a softmax transformation to convert this output into a probability distribution over experts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd77d1b36-0dbd-4988-a2c0-18ab5f4269e1_910x470.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd77d1b36-0dbd-4988-a2c0-18ab5f4269e1_910x470.png)

Computing MoE output for Switch Transformer (from [5])

From here, we compute the output of the switch layer by _i)_ selecting a single expert and _ii)_ scaling the output of this expert by the probability assigned to that expert by the routing function; see above. Notably, this approach differs from [1] because it computes a distribution over all experts, rather than only considering the distribution over top-`K` experts. In other words, we still compute a probability distribution over all `N` experts in the routing function, then compute the layer’s output by scaling the output of the selected expert by its probability; see above. This approach allows us to train the MoE model even when `K=1`.

**Simple load balancing.** In [1], authors employ multiple auxiliary loss functions to balance importance scores and perform load balancing between experts (i.e., meaning that each expert is sent a roughly equal number of tokens from the batch). We see in [5] that both of these objectives can be achieved with a single auxiliary loss function that is applied at each switch layer in the model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F323e83a3-9b68-447c-bff9-c06931264fac_1368x714.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F323e83a3-9b68-447c-bff9-c06931264fac_1368x714.png)

Simplified auxiliary loss for load balancing (from [5])

This loss, which is differentiable with respect to `P` and can be easily incorporated into training, encourages both the fraction of tokens allocated to each expert and the fraction of router probability allocated to each expert to be `1/N`, _meaning that experts are equally important and receive a balanced number of tokens_.

**Capacity factor.** Within the Switch Transformer, we set a global “expert capacity” variable that determines the maximum number of tokens that can be routed to each expert in any MoE layer. The equation for expert capacity is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88159ea6-3a7a-4f13-9335-b2584fbd0608_774x102.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88159ea6-3a7a-4f13-9335-b2584fbd0608_774x102.png)

(from [5])

In a switch layer, each token is routed to the expert that is assigned the highest probability by the routing mechanism. If too many tokens (i.e., exceeding the expert capacity) are sent to a single expert, computation for these tokens will be skipped. These “dropped” tokens are passed directly to the next layer via the residual connection. Setting the capacity factor greater than one allows the MoE to handle cases where tokens are not perfectly balanced across experts; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52297bf3-b503-43a4-9618-4c02ac60c8f7_1758x1178.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52297bf3-b503-43a4-9618-4c02ac60c8f7_1758x1178.png)

(from [5])

The expert capacity must be tuned to avoid having too many dropped tokens—_the load balancing loss can also help with this by evenly routing tokens to experts_—while not wasting computation or memory. Beyond the Switch Transformer, the capacity factor is an important hyperparameter for any MoE model that we can tune to improve hardware utilization by balancing computation across experts.

**Practical evaluation.** Authors in [5] evaluate a Switch Transformer model based upon [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part) [8]. The model is pretrained on [C4](https://huggingface.co/datasets/c4) using a [masked language modeling (MLM) objective](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning) and a [mixed precision strategy](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) that uses `bfloat16` format for certain operations. By only using low precision on select operations (e.g., the routing function is chosen to be performed in full precision), we can improve efficiency without causing training instabilities. Interestingly, authors note that they increase the dropout rate on MoE layers to avoid overfitting.

> _“The guiding principle for Switch Transformers is to maximize the parameter count of a Transformer model in a simple and computationally efficient way.”_ - from [5]

Using this approach, Switch Transformer models with up to 1.6T parameters are trained. To create these models, we start with a T5 architecture, convert each feed-forward sub-layer into an MoE, and increase the number of experts within each MoE layer! Despite the size of these models, the amount of compute spent on each token does not change much. In fact, _training the_ _Switch Transformer is up to 7X faster than training the base T5 model in certain cases_. The MoE model has better statistical learning efficiency[12](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-12-142423094) compared to T5 and performs better given a fixed amount of computation or time for training; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb73bcd-8051-4ff9-82e5-26ae067f2d8d_1820x1436.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb73bcd-8051-4ff9-82e5-26ae067f2d8d_1820x1436.png)

(from [5])

Compared to standard MoE models [1], the Switch Transformer has a smaller compute footprint—_due to having fewer active experts_—and achieves better performance on a speed-quality basis. In other words, the Switch Transformer outperforms a comparable MoE model that is trained using the same amount of compute or for the same amount of time. We also see above that the Switch Transformer performs better at lower capacity factors compared to MoE, which allows us to improve training efficiency via better load balancing between experts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49c99d51-cbb5-4301-b87c-ae0613ebdfe3_1520x1080.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49c99d51-cbb5-4301-b87c-ae0613ebdfe3_1520x1080.png)

(from [5])

**Scaling properties.** The results in [5] show us that MoE models—and the Switch Transformer variant in particular_—_benefit language modeling applications. _Why have MoEs become so popular for language modeling?_ The simple reason is that MoE models allow us to scale up the size of the model at a stable computational cost. As shown in the figure above, LLM performance consistently improves as the number of experts is increased, producing scaling trends comparable to those observed for dense models. MoE models allow us to get the performance of a larger model while maintaining reasonable training and inference costs.

#### [ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906) [24]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2dc85c3-451a-455c-94de-39a93256a213_1920x892.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2dc85c3-451a-455c-94de-39a93256a213_1920x892.png)

(from [24])

Although it made MoE models more practical/usable, the Switch Transformer still had several issues. The model suffered from training instability, and authors in [5] show that the Switch Transformer—_despite achieving pretraining speedups over dense models_—lags behind the performance of prior state-of-the-art models when finetuned on popular benchmarks like [SuperGLUE](https://super.gluebenchmark.com/). Put simply, these MoE-based language models only perform well when the dataset is very large, otherwise their performance deteriorates due to overfitting. In [24], authors study the instability and finetuning difficulties of MoE models in-depth, providing a design guide (summarized above) for practical and reliable training of sparse models. Using this guide, authors train a sparse 269B parameter encoder-decoder model that is the first MoE-based model to achieve state-of-the-art transfer learning[13](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-13-142423094) results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55542f71-0614-48ff-a80e-14db8555e3b0_1878x878.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55542f71-0614-48ff-a80e-14db8555e3b0_1878x878.png)

(from [24])

**Addressing training instability.** Despite the benefits of sparse MoE models, they are much more prone to instabilities during training; see above. In [24], authors consider several techniques for mitigating this training instability and compare them based upon _i)_ the performance of the model and _ii)_ the ratio of training runs that are stable. Several techniques are found to improve stability at the cost of deteriorated model performance, such as:

- Injecting noise into the model via dropout or input jitter (i.e., multiplying the router’s logits by a random value in the range `[0.99, 1.01]`).
    
- Clipping gradients or weight updates based on a fixed threshold to remedy [exploding gradients](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) during backpropagation.
    
- Removing components with multiplicative interactions (e.g., GeGLU activation [27] or the scaling factor in RMSNorm [28]) from the architecture.
    

We learn in [24] that constraining the values of the router’s logits (i.e., the output values of the router prior to softmax being applied) has the biggest impact on training stability. In prior work [5], we have seen that the routing mechanism cannot be computed in low precision because MoE models are sensitive to roundoff errors. Due to the exponential in the routing function (shown below), such roundoff errors can occur in the router and lead to training instabilities.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527a5df7-dbf9-42a1-941f-a76c274712bf_976x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F527a5df7-dbf9-42a1-941f-a76c274712bf_976x646.png)

Softmax transformation in an MoE router

For this reason, router logits that are too large could negatively impact training stability. However, simply clipping (i.e., applying a hard threshold to remove large values) router logits can damage model performance. Instead, the router z-loss (formulated below) is proposed in [25] as an auxiliary loss that encourages smaller values (and fewer roundoff errors) within the routing mechanism.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8409efb-5193-49d7-ad23-67f587b83c3e_1030x640.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8409efb-5193-49d7-ad23-67f587b83c3e_1030x640.png)

Router z-loss (from [24])

This loss just penalizes large logits within the gating mechanism of the MoE. To incorporate the router z-loss into the training process of the MoE model, we can simply combine it with the normal training loss and the auxiliary load balancing loss (see below), where we weight each of these losses according to their relative importance. The router z-loss significantly improves training stability without causing any noticeable deterioration in model quality.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e490ded-c495-4efc-8e74-dd9819fe760f_998x476.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e490ded-c495-4efc-8e74-dd9819fe760f_998x476.png)

Combining losses for an MoE model (from [24])

**Debugging finetuning issues.** Although sparse models perform well with large datasets, we’ve seen that finetuning these models can be difficult. In [24], authors confirm that these finetuning ailments are due to overfitting by studying the performance of pretrained sparse and dense models on downstream tasks with both small ([CommitmentBank](https://github.com/mcdm/CommitmentBank)) and large ([ReCoRD](https://sheng-z.github.io/ReCoRD-explorer/)) training datasets[14](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-14-142423094). In these experiments, we see that the sparse model converges more quickly than the dense model in both cases. However, the validation quality of the sparse model is much worse than that of the dense model on the downstream task with less data. In other words, _the sparse model overfits due to a lack of sufficient data_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa72299-a767-4cea-8942-85ca3be9f81a_1616x922.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fa72299-a767-4cea-8942-85ca3be9f81a_1616x922.png)

(from [24])

To mitigate overfitting, authors first attempt to only finetune certain portions of the MoE model’s parameters. Interestingly, finetuning just the MoE layers yields a drastic deterioration in performance, while selectively finetuning other groups of parameters performs comparably to finetuning all parameters; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e7532af-e9ef-48bf-af83-27eb92eae8d0_1606x866.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e7532af-e9ef-48bf-af83-27eb92eae8d0_1606x866.png)

(from [24])

Going further, we see that sparse and dense models tend to require different hyperparameter settings for effective finetuning. In particular, MoE models benefit from high learning rates and smaller batch sizes, both of which improve generalization by introducing more noise to the training process. Additionally, we see that the capacity factor does not drastically impact finetuning results. In fact, we can remove the auxiliary load balancing loss during finetuning without any issues—_sparse models are robust to dropping up to 10-15% of tokens_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07384108-ca61-4595-bde0-54abef2bd445_1646x818.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07384108-ca61-4595-bde0-54abef2bd445_1646x818.png)

(from [24])

However, there is no definitive solution provided in [24] that solves the difficulty of finetuning MoE models. Authors confirm that these issues arise due to overfitting and analyze different choices that help to combat overfitting. However, even the optimally-designed models analyzed in the experimental section of [24] continue to overfit on smaller downstream tasks! Improved forms of regularization for finetuning MoEs are still an active area of research.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63b188d9-9318-491c-a5f6-50d57c713e4a_1626x406.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63b188d9-9318-491c-a5f6-50d57c713e4a_1626x406.png)

(from [24])

**Optimal MoE design.** Based on their extensive analysis, authors in [24] outline high-level takeaways (shown above) for effectively designing and using MoE models[15](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-15-142423094). This analysis focuses primarily upon choosing the correct capacity factor and number of experts within the MoE. Put simply, _we see in [24] that the optimal setup for an MoE model is hardware-dependent_. For example, authors recommend using at most one expert per TPU/GPU core and note that optimal values for the capacity factor and number of experts are hardware-dependent. Changes to the capacity factor—_due to scaling up the maximum number of tokens sent to each expert_—have several downstream effects:

- Higher memory requirements for activations.
    
- Heightened communication costs.
    
- Increased computation in each expert’s forward pass.
    

On the flip side, higher capacity factors can improve model quality! For a system with fast interconnects and powerful GPUs, the spike in computation and communication costs might be worth it. However, the choice of MoE parameters is a tradeoff that depends heavily upon the available hardware.

**Practical takeaway.** Authors in [24] conclude their work by using the lessons outlined above to train a 269B parameter encoder-decoder transformer model—_with the same compute complexity as a 32B parameter dense model—_based upon T5-Large [8]. Compared to prior MoE models, the ST-MoE-32B model has fewer parameters and a higher computational complexity. There are fewer, but larger, experts in each MoE layer. Previously, such high-[FLOP](https://en.wikipedia.org/wiki/FLOPS) MoEs were found to be unstable during training, but the addition of the router z-loss enables the ST-MoE-32B model to be trained successfully. Because it has fewer parameters compared to prior MoE models, ST-MoE-32B is easier to finetune and deploy.

> _“We sought a balance between FLOPs and parameters. High-FLOP sparse models were previously unstable in our setting (i.e. encoder-decoder models, Adafactor optimizer), but the router z-loss enabled us to proceed.”_ - from [24]

The model is pretrained on 1.5 trillion tokens of data from a combination of C4 and the dataset used to train GLaM [25]. Afterwards, finetuning is performed over a mixture of all downstream tasks—_where each task is sampled proportionally according to its training set size_[16](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-16-142423094)—prior to evaluation. The results of this model are displayed below, where we see that it sets new state-of-the-art performance across a wide range of NLP tasks. Notably, ST-MoE-32B is the first sparse model to achieve state-of-the-art results in the transfer learning domain, as prior models struggled heavily with overfitting during the finetuning process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85df0812-d4d9-460a-a979-eaeac39c9dde_1346x1114.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85df0812-d4d9-460a-a979-eaeac39c9dde_1346x1114.png)

(from [24])

#### Other Notable MoE Papers

Prior to the popularization of MoE-style LLMs, many papers were written on this topic beyond the research we explored in [5] and [24]. Some notable publications in this area are outlined below to provide a more comprehensive picture of research that has been conducted on MoEs in the language modeling domain.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7222754a-0e0b-4845-8c73-5491aaa785d7_1350x1072.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7222754a-0e0b-4845-8c73-5491aaa785d7_1350x1072.png)

(from [9])

**GShard [9]** proposes a framework and set of tools for easily implementing parallel computation patterns in neural networks, such as a Sparsely-Gated MoE layer. This framework is then used to scale a transformer-based neural machine translation model to a size of over 600B parameters. Using GShard, this model can be efficiently trained on over 2048 TPUs in 4 days and surpasses the quality of prior models on translating over 100 different languages to English. Some useful takeaways for effectively training MoE models in GShard include:

- Every other feed-forward sub-layer in the encoder-decoder transformer is replaced with an MoE layer.
    
- Top-2 routing is used in both the encoder and the decoder.
    
- The routing mechanism always picks the top expert, but the second expert is randomly chosen with probability proportional to its weight.
    

> _“Where data-parallelism can be viewed as splitting tensors and operations along the batch dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors.”_ - from [31]

**Mesh-Tensorflow [31]** greatly simplified the exploration of efficient, model-parallel distributed training strategies, which split the model itself across multiple devices (i.e., GPUs or TPUS)[17](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-17-142423094), for neural networks. Previously, data parallel strategies, which split the input batch of data across multiple devices, were the dominant approach for distributed training of neural networks, but data parallel training has several downsides; e.g., training large models is harder because each device must keep a full copy of the model in memory and efficiency deteriorates with small batch sizes. Put simply, Mesh-Tensorflow is just a general language that can be used to specify distributed computations. After the release of this tool, MoE layers were re-introduced into the transformer architecture, as programming data-parallel and model-parallel training strategies became very simple. In [31], authors use Mesh-Tensorflow to train a 5B parameter transformer model that surpasses state-of-the-art results on machine translation tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8151b840-9cd8-4e87-8a37-020030d4c4b7_1374x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8151b840-9cd8-4e87-8a37-020030d4c4b7_1374x646.png)

(from [10])

**M6 [10]** proposes a new (Chinese-only) dataset, which contains 1.9TB worth of images and 292GB of text, for pretraining multi-modal language models. From this dataset, several language models are pretrained using a novel, cross-modal pretraining strategy; see above for a depiction. One of the models that is trained in [10] is a 100B parameter MoE model that replaces all feed-forward sub-layers in the transformer depicted above with MoE layers comprised of 1024 experts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d04ed69-8839-466e-a6b4-4f0b121463ae_1604x1010.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d04ed69-8839-466e-a6b4-4f0b121463ae_1604x1010.png)

(from [11])

**Scaling Vision with Sparse Mixture of Experts [11].** Although MoE models are clearly beneficial for language modeling, it is unclear whether similar benefits will hold for computer vision. In [11], authors explored a Vision MoE (V-MoE) model that combines the [vision transformer architecture](https://cameronrwolfe.substack.com/p/vision-transformers) with sparse MoE layers; see above. Put simply, we just replace a subset of the feed-forward sub-layers in the vision transformer—_such as every other layer or the last_ `n` _layers_—with MoE layers. Such an approach yields a desirable tradeoff between performance and compute. Going further, authors in [11] propose a new routing algorithm that can adaptively leverage more compute for certain inputs at inference time.

> _“V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time.”_ - from [11]

**DeepSpeed-MoE [12]** observes that performing efficient inference with MoE models is difficult. Scaling up dense LLMs can eventually run into hardware limitations, but MoE models can be scaled (almost) arbitrarily. For this reason, MoE models are a promising architectural advancement, but their adoption is often limited by their larger size and unique architecture. Aiming to make conditional computation easier to use in practice, authors in [12] create the DeepSpeed-MoE framework for end-to-end training and inference of MoE models. This framework defines several MoE architectures, proposes model compression techniques that reduce MoE size up to 3.7X, and implements an optimized inference engine that can improve latency by 7.3X.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298c755e-beb5-41a7-824d-e88d1dbe25f9_1466x1080.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298c755e-beb5-41a7-824d-e88d1dbe25f9_1466x1080.png)

(from [23])

**Beyond English-Centric Multilingual Machine Translation [23].** This work creates a Many-to-Many machine translation model that can translate directly between any pair of 100 languages. While prior models are highly English-centric, authors in [23] aim to create a model that better reflects translation needs worldwide. First, an open-source training dataset is created that contains supervised language translation examples across thousands of language pairs. To learn from this data, we need a model with sufficient capacity. Authors in [23] explore high-capacity models that combine dense scaling—_or just increasing the size of the model_—with the use of sparse, language-specific groups of parameters.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044fa414-4c0e-4e8f-989b-179cc7315b19_1124x1562.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044fa414-4c0e-4e8f-989b-179cc7315b19_1124x1562.png)

(from [25])

**GLaM [25]** extends the work of Switch Transformers (i.e., applied to encoder-decoder transformer architectures [26]) to [decoder-only language models](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse). The goal of this work is to train a [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) quality model more efficiently—_using an order of magnitude less compute_. GLaM, short for Generalist Language Model, provides a suite of LLMs that use MoE layers to scale up model capacity while maintaining reasonable training costs. The largest model in this suite has 1.2 trillion parameters in total (i.e., 7X larger than GPT-3), performs better than GPT-3 across nearly 30 NLP tasks, is trained using only 1/3 of the energy used for GPT-3, and requires half of the computation of GPT-3 at inference time; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2cb11d5-72fe-4eaf-bb61-9361a53b079e_1120x722.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2cb11d5-72fe-4eaf-bb61-9361a53b079e_1120x722.png)

(from [25])

GLaM models use top-two routing (i.e., two active experts in each layer). Authors in [25] also explore larger capacity factors, finding that this metric is useful for controlling the amount of compute used during training or inference.

## Final Thoughts

> _“In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost.”_ - from [5]

After studying over two decades of research on MoEs, we should have a decent grasp of the idea, what makes it effective, and how we can apply it to train better language models. Research on this topic has gone through several phases, but the key takeaways from all of this work can be summarized as shown below.

**Core idea.** The fundamental idea behind an MoE is to decouple a model’s parameter count from the amount of compute that it uses. To do this, we can simply replace layers within the model with several experts that are sparsely activated. Then, a gating mechanism can determine which expert should be used for a given input. In this way, we increase the capacity of the model while only using a portion of the model’s total parameters in the forward pass.

**Components of an MoE.** There are two primary components of an MoE: _i)_ the experts and _ii)_ a routing mechanism. Each layer has several, similarly-structured experts that can be used to process the input data. Given a set of tokens as input, we use the routing mechanism to (sparsely) select the experts to which each token should be sent. Then, we compute the corresponding experts’ output for each token and combine their results to yield the output of the MoE layer. Many MoE models use some variant of the softmax gating mechanism as a router.

**Applying MoEs to transformers.** In the language modeling domain, we usually create MoE models based upon the transformer architecture. To do this, we consider every feed-forward sub-layer within the transformer—_or some smaller subset of these layers; e.g., every other layer_—and replace them with an MoE layer. Each expert in the MoE is a feed-forward neural network with an independent set of parameters that matches the architecture of the initial feed-forward sub-layer. In most cases, top one or top two routing is used for transformer MoE models.

**The good and the bad.** As we have seen throughout this post, MoE models have many benefits compared to dense models, but they also have many downsides! Given a sufficiently large pretraining dataset, MoE models tend to learn faster than a compute-matched dense model. We can also increase the capacity of an MoE model significantly, allowing the performance of a much larger model to be achieved with a lesser computational burden. This property of MoEs is highly beneficial in settings with tons of training data, hence the popularity of MoEs in the language modeling domain. However, MoE models also:

- Consume more memory (i.e., we must store all experts in memory).
    
- Struggle with training stability (though work in [24] helps to solve this).
    
- Tend to overfit during finetuning if there is not enough training data.
    

All things considered, the choice of whether to use an MoE or not is highly application dependent. If we have nearly unlimited amounts of training data, as is true of most modern language models, then MoEs are highly beneficial and allow us to scale up our model’s capacity significantly, even beyond hardware-imposed limits. In smaller scale applications, however, MoEs are less straightforward to apply compared to dense models and could even perform worse!

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), and this is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." _arXiv preprint arXiv:1701.06538_ (2017).

[2] Eigen, David, Marc'Aurelio Ranzato, and Ilya Sutskever. "Learning factored representations in a deep mixture of experts." _arXiv preprint arXiv:1312.4314_ (2013).

[3] Bengio, Emmanuel, et al. "Conditional computation in neural networks for faster models." _arXiv preprint arXiv:1511.06297_ (2015).

[4] Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." _Neural computation_ 9.8 (1997): 1735-1780.

[5] Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." _Journal of Machine Learning Research_ 23.120 (2022): 1-39.

[6] Kaplan, Jared, et al. "Scaling laws for neural language models." _arXiv preprint arXiv:2001.08361_ (2020).

[7] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[8] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _The Journal of Machine Learning Research_ 21.1 (2020): 5485-5551.

[9] Lepikhin, Dmitry, et al. "Gshard: Scaling giant models with conditional computation and automatic sharding." _arXiv preprint arXiv:2006.16668_ (2020).

[10] Lin, Junyang, et al. "M6: A chinese multimodal pretrainer." _arXiv preprint arXiv:2103.00823_ (2021).

[11] Riquelme, Carlos, et al. "Scaling vision with sparse mixture of experts." _Advances in Neural Information Processing Systems_ 34 (2021): 8583-8595.

[12] Rajbhandari, Samyam, et al. "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale." _International conference on machine learning_. PMLR, 2022.

[13] Jiang, Albert Q., et al. "Mixtral of experts." _arXiv preprint arXiv:2401.04088_ (2024).

[14] Jiang, Albert Q., et al. "Mistral 7B." _arXiv preprint arXiv:2310.06825_ (2023).

[15] Jacobs, Robert A., et al. "Adaptive mixtures of local experts." _Neural computation_ 3.1 (1991): 79-87.

[16] Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. "Estimating or propagating gradients through stochastic neurons for conditional computation." _arXiv preprint arXiv:1308.3432_ (2013).

[17] Davis, Andrew, and Itamar Arel. "Low-rank approximations for conditional feedforward computation in deep neural networks." _arXiv preprint arXiv:1312.4461_ (2013).

[18] Eigen, David, Marc'Aurelio Ranzato, and Ilya Sutskever. "Learning factored representations in a deep mixture of experts." _arXiv preprint arXiv:1312.4314_ (2013).

[19] Cho, Kyunghyun, and Yoshua Bengio. "Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning." _arXiv preprint arXiv:1406.7362_ (2014).

[20] Bengio, Emmanuel, et al. "Conditional computation in neural networks for faster models." _arXiv preprint arXiv:1511.06297_ (2015).

[21] Almahairi, Amjad, et al. "Dynamic capacity networks." _International Conference on Machine Learning_. PMLR, 2016.

[22] Denoyer, Ludovic, and Patrick Gallinari. "Deep sequential neural network." _arXiv preprint arXiv:1410.0510_ (2014).

[23] Fan, Angela, et al. "Beyond english-centric multilingual machine translation." _Journal of Machine Learning Research_ 22.107 (2021): 1-48.

[24] Zoph, Barret, et al. "St-moe: Designing stable and transferable sparse expert models." _arXiv preprint arXiv:2202.08906_ (2022).

[25] Du, Nan, et al. "Glam: Efficient scaling of language models with mixture-of-experts." _International Conference on Machine Learning_. PMLR, 2022.

[26] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[27] Shazeer, Noam. "Glu variants improve transformer." _arXiv preprint arXiv:2002.05202_ (2020).

[28] Zhang, Biao, and Rico Sennrich. "Root mean square layer normalization." _Advances in Neural Information Processing Systems_ 32 (2019).

[29] Clark, Aidan, et al. "Unified scaling laws for routed language models." _International conference on machine learning_. PMLR, 2022.

[30] Jordan, Michael I., and Robert A. Jacobs. "Hierarchical mixtures of experts and the EM algorithm." _Neural computation_ 6.2 (1994): 181-214.

[31] Shazeer, Noam, et al. "Mesh-tensorflow: Deep learning for supercomputers." _Advances in neural information processing systems_ 31 (2018).

[1](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-1-142423094)

We can implement MoEs with different expert structures; e.g., more complex architectures or hierarchical MoEs, where each expert is an MoE itself. In practical applications with transformers, experts are usually just feed-forward neural networks.

[2](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-2-142423094)

The total number of parameters is slightly less than 8 x 7B = 56B because only feed-forward sub-layers are converted to MoEs, while the rest of the parameters in the network are shared.

[3](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-3-142423094)

The inference cost of this model will actually be closer to that of a 12B parameter model (i.e., slightly less than that of a 14B parameter model) if we account for shared (non-MoE) layers within the transformer.

[4](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-4-142423094)

This statement is not true for all machine learning models. However, it tends to be true in deep learning and is especially true for language models. For more details, one can see relevant research on [neural scaling laws](https://cameronrwolfe.substack.com/i/88082618/scaling-laws-for-neural-language-models).

[5](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-5-142423094)

To avoid overfitting due to these extra parameters, authors in [19] propose a tree-structured parameterization, in which gating units (i.e., prefixes for patterns within the hidden activations) are stored within each node of the tree.

[6](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-6-142423094)

Extra model capacity is typically most useful when we have a large training dataset, as the added capacity enables the model to “absorb” the knowledge in this vast dataset.

[7](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-7-142423094)

In fact, each expert is guaranteed to receive a non-zero score when using this approach!

[8](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-8-142423094)

Long short term memory (LSTM) networks [4]—_the most popular language modeling architecture prior to the popularization of GPT-style LLMs_—are a type of recurrent neural network (RNN) that have been modified to better handle long input sequences.

[9](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-9-142423094)

Interestingly, authors in [1] note that only 37-46% of total floating point operations by the LSTM are consumed by experts from the MoE.

[10](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-10-142423094)

Early works in this space study LLM performance as a function of model size. However, later work (e.g., Chinchilla [7]) discovers that performance improves as a function of both model and pretraining dataset size.

[11](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-11-142423094)

The intuition here is that we need to compare at least two experts in each forward pass to be able to effectively learn which experts should be selected.

[12](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-12-142423094)

This sounds complicated, but it just means that the model’s performance improves more quickly with respect to the number of examples observed during training. For example, a model that achieves 95% accuracy after observing 10 training examples is more statistically efficient than a model that achieves 90% accuracy after observing the same number of training examples.

[13](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-13-142423094)

Transfer learning simply refers to the process of finetuning a pretrained model on some data to solve a downstream task.

[14](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-14-142423094)

Both of the tasks that are considered are sub-tasks of the [SuperGLUE benchmark](https://super.gluebenchmark.com/).

[15](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-15-142423094)

Concurrent work in [29] performs a similar analysis, where authors propose using higher layer frequency (i.e., converting more feed-forward sub-layers in the transformer to MoE layers) and using top-one routing, similar to the Switch Transformer [5].

[16](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-16-142423094)

However, the size of the training set used to determine the data mixture is thresholded by a maximum size to avoid a few tasks dominating the finetuning process.

[17](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth#footnote-anchor-17-142423094)

See [here](https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/) for a great, in-depth discussion of model and data parallel training.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Dr. Holger Lindberg Joergensen's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf810c6f-a979-4fc0-ba4a-e4361992d16e_144x144.png)



](https://substack.com/profile/46511-dr-holger-lindberg-joergensen)

[

![Felix's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f6b0298-05f0-4fb8-9551-9f1947298628_144x144.png)



](https://substack.com/profile/85824821-felix)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

[

![米斯特's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F867e9bd3-3d48-433e-b41d-3ceeabb17f5c_48x48.png)



](https://substack.com/profile/119205644-7c7365af7279)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

98 Likes∙

[15 Restacks](https://substack.com/note/p-142423094/restacks?utm_source=substack&utm_content=facepile-restacks)

98

- 

[

16

](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comments)

15

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![CodeCompass's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc68eb12c-3638-4aa1-ac3b-0317e5f2f2df_677x670.png)



](https://substack.com/profile/45941603-codecompass?utm_source=comment)

[CodeCompass](https://substack.com/profile/45941603-codecompass?utm_source=substack-feed-item)

[The Code Compass](https://codecompass00.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年4月8日](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comment/53410902 "2024年4月8日 03:35")

Liked by Cameron R. Wolfe, Ph.D.

Thanks for this fantastically detailed write-up!

Since I am from a computer vision background I have seen MoEs used for a different modality than text. I have seen them being used "conditionally fuse" information based on the "quality and content" of various inputs.

Imagine a CNN that does semantic segmentation of a scene with multi-modal inputs such as RGB images, infrared image, etc. The model then learns to "weigh" the output of each modality branch. The weighting is conditioned on the inputs. So if the RGB image is washed out due to high exposure because your RGB camera is facing the sun, the model can give the RGB branch a lower weight and prefer information from other branches to produce the segmentation mask output.

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comment/53410902)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael?utm_source=comment)

[Michael](https://substack.com/profile/59416223-michael?utm_source=substack-feed-item)

[Lux Umbra Dei](https://runtothehorizn.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年3月29日](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comment/52700632 "2024年3月29日 07:28")

Liked by Cameron R. Wolfe, Ph.D.

Cameron, thiscwas a really excellent overview- shows your impressive command of the materials. Would love to see a book by you on the topic.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comment/52700632)

[14 more comments...](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# DBRX, Continual Pretraining, RewardBench, Faster Inference, and More

### A deep dive into recent and impactful advancements in LLM research...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Apr 01, 2024

37

- 

[

6

](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comments)

4

Share

If you like the newsletter, feel free to subscribe below, [get in touch](https://cameronrwolfe.me/), or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and/or [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/).

Subscribe

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaf42a4f-7049-48a2-9a21-024ee745636b_2394x1320.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaf42a4f-7049-48a2-9a21-024ee745636b_2394x1320.png)

(from [1, 5, 8. 21])

Research on large language models (LLMs) is evolving rapidly. Recently, even models and techniques that have achieved state-of-the-art performance for long durations of time are being quickly dethroned. For example, GPT-4 was (arguably) surpassed by [Claude-3](https://www.anthropic.com/news/claude-3-family), while [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) replaced [Mixtral](https://mistral.ai/news/mixtral-of-experts/) as the highest-performing open LLM. Within this overview, we will take an in-depth look at numerous recent advancements in LLM research, including:

- New models like DBRX and [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1) (and new architectures!).
    
- Dynamic training algorithms that allow us to efficiently train LLMs over expanding and evolving datasets to eliminate knowledge cutoffs.
    
- Benchmarks that give us a new understanding of how LLMs function.
    
- Faster decoding algorithms that increase the inference speed of LLMs by more than 10×.
    

Although these advancements address a wide set of topics, they work towards a unified goal—_making LLMs more practical to train and use_. By releasing new models, evaluating those models, developing better training algorithms, and devising more cost-efficient hosting methods, we are exploring new avenues of making LLMs more performant in terms of accuracy or cost. Given the staggering pace of LLM research, the practical effectiveness of these models is beginning to improve exponentially as research continues to progress towards this unified goal.

---

#### **[🧱DBRX🧱: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) [21]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb350e0ed-a8b6-44f7-806a-d581b785e74d_1934x1230.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb350e0ed-a8b6-44f7-806a-d581b785e74d_1934x1230.png)

(from [21])

DBRX is the next model in the [series of Open LLMs](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact) released by [Mosaic](https://www.databricks.com/research/mosaic). Two versions of the model are released—_a base model ([DBRX base](https://huggingface.co/databricks/dbrx-base)) and a finetuned model ([DBRX Instruct](https://huggingface.co/databricks/dbrx-instruct))_—under an open license (i.e., the [Databricks open model license](https://www.databricks.com/legal/open-model-license)). Mosaic entered the space of open LLMs with their proposal of [MPT-7B](https://www.databricks.com/blog/mpt-7b) in May of 2023—_less than a year ago_! Since then, Mosaic has been [acquired for $1.3B](https://www.databricks.com/company/newsroom/press-releases/databricks-completes-acquisition-mosaicml), trained thousands of specialized LLMs for customers, and still found time to release several open LLMs—_DBRX being the latest_—and improve the end-to-end efficiency of LLM pretraining by over 4×. The MoE-based DBRX model achieves new state-of-the-art performance for open language models in terms of both quality and efficiency across a massive suite of evaluations.

**Model architecture.** DBRX is a [Mixture-of-Experts (MoE)](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth) model that is pretrained on 12T tokens[1](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-1-142727381) of curated text. Compared to popular models like [ST-MoE](https://cameronrwolfe.substack.com/i/142423094/st-moe-designing-stable-and-transferable-sparse-expert-models) [23] that use a smaller number of large experts, DBRX takes an opposite strategy, using a fine-grained MoE architecture with a larger number of small experts. The model has 132B parameters in total with 36B parameters active at any given time (i.e., each MoE layer has 16 experts in total and four experts are chosen in each forward pass). Relative to models like [Mixtral](https://mistral.ai/news/mixtral-of-experts/) and [Grok-1](https://x.ai/blog/grok-os) that have only eight experts (and two active experts), DBRX has 65× more potential combinations of experts that can be used, which is found to improve model quality. Notably, MoE models are hard to train (and finetune) due to instabilities that arise during training and communciation bottlenecks. However, Mosaic has created a [robust and repeatable recipe](https://www.databricks.com/blog/turbocharged-training-optimizing-databricks-mosaic-ai-stack-fp8) for efficiently training such models.

> _“We estimate that our new pretraining data is at least 2x better token-for-token than the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach the same model quality.”_ - from [21]

**More DBRX details.** Although the pretraining dataset for DBRX is much larger than prior models, authors in [21] also claim that the data used to train DBRX is higher-quality[2](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-2-142727381). As a result of this quality increase, the statistical training efficiency of DBRX is higher than normal—_training is faster because we achieve higher accuracy with fewer tokens_. Additionally, a curriculum learning strategy— _meaning that the data mixture is dynamically changed throughout pretraining_—is adopted for training DBRX and found to improve model quality.

DBRX has a longer context length of 32K and uses the GPT-4 tokenizer[3](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-3-142727381) (available via [tiktoken](https://github.com/openai/tiktoken)) instead of the GPT-NeoX tokenizer used by prior MPT models. According to researchers at Mosaic, the GPT-4 tokenizer was selected partly due to performance and partly due to the practical consideration that it makes pricing comparisons to competing models (i.e., GPT-4) more direct/simple.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f7d600-d692-4c95-93db-4078c2c4ca7e_1970x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f7d600-d692-4c95-93db-4078c2c4ca7e_1970x638.png)

(from [21])

**Better efficiency.** The proposal of DBRX comes with large improvements in terms of both training and inference efficiency. As mentioned above, DBRX is trained over an optimized dataset that improves training efficiency by roughly 2×. However, a variety of other factors yield further boosts in training efficiency:

- The MoE architecture, which is found in smaller-scale experiments (i.e., on DBRX MoE-B, a 23.5B parameter model with 6.6B active parameters) to require 1.7× fewer FLOPS during training compared to a performance-matched LLaMA-2-13B model.
    
- Other modifications to the [decoder-only architecture](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) (i.e., [RoPE](https://cameronrwolfe.substack.com/i/142044446/better-positional-embeddings), [GLU activation](https://pytorch.org/docs/stable/generated/torch.nn.GLU.html), and [GQA](https://cameronrwolfe.substack.com/i/142044446/efficient-masked-self-attention)).
    
- “Better optimization strategies” (hopefully more details [coming soon](https://x.com/jefrankle/status/1772961615454867719?s=20)).
    

When considering all data, architecture, and optimization changes, the end-to-end training process of DBRX requires 4× less compute than prior models released by Mosaic. To determine this number, authors in [21] compare a smaller variants of DBRX, DBRX MoE-A (i.e., a 7.7B parameter LLM with 2.2B active parameters) to MPT-7B, finding that DBRX MoE-A achieves similar performance on the [Databricks Gauntlet](https://github.com/mosaicml/llm-foundry) while using 3.7× fewer FLOPS during training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb50b0e9-7f1a-457c-acf9-b9c9f495620b_1960x1228.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb50b0e9-7f1a-457c-acf9-b9c9f495620b_1960x1228.png)

(from [21])

DBRX also comes with improvements to inference efficiency—_up to 2× faster than LLaMA-2-70B at 150 tokens per second per user in load tests_. Mosaic has an [optimized serving infrastructure](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) that uses [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and 16 bit precision, which is [very fast](https://x.com/natolambert/status/1772999462538887493?s=20) (try it out [here](https://huggingface.co/spaces/databricks/dbrx-instruct)). The MoE architecture of DBRX also aids inference efficiency due to the relatively low number of active parameters. For example, DBRX is 40% of the size of Grok-1 in both total and active parameters.

> _“Using an MoE architecture makes it possible to attain better tradeoffs between model quality and inference efficiency than dense models typically achieve.”_ - from [21]

**Empirical evaluation.** In comparison to other open LLMs, we see that DBRX Instruct achieves the best performance on composite benchmarks by a large margin compared to Mixtral; see below. Notably, DBRX has impressive programming skills, outperforming models like Grok-1 that are more than twice its size and specialized coding models like [CodeLLaMA-70B](https://ai.meta.com/blog/code-llama-large-language-model-coding/) (despite being a general-purpose LLM!). DBRX also performs well on reasoning/math tasks[4](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-4-142727381).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77fe464f-f391-4fc1-aa96-8649f07ca17e_1968x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77fe464f-f391-4fc1-aa96-8649f07ca17e_1968x1182.png)

(from [21])

Compared to closed models, DBRX surpasses the performance of GPT-3.5 and is competitive with [Gemini Pro](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction). Gemini Pro only outperforms DBRX on [GSM8K](https://huggingface.co/datasets/gsm8k), while [Mixtral-Medium](https://docs.mistral.ai/guides/model-selection/#mistral-medium-intermediate-tasks-that-require-language-transformation) performs better on a select few tasks that are considered; see below. At a high level, DBRX seems to be good at general knowledge and commonsense reasoning tasks, as well as programming and math. DBRX is also shown in [21] to perform competitively with models like Mixtral and GPT-3.5/4-Turbo on tasks involving RAG and long context.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6abdb5-8d58-475e-84f4-4e567bceae00_1886x968.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6abdb5-8d58-475e-84f4-4e567bceae00_1886x968.png)

(from [21])

**The research process.** Although my take is definitely biased, Mosaic has one of the best (if not the best) LLM research teams in the world. DBRX was created in less than three months using a cluster of 3K H100 GPUs with a budget of around $10M. Mosaic is an applied research team, meaning that a lot of their work is customer-focused—_they have trained thousands of specialized LLMs for various Databricks customers_. However, this focus upon applications is exactly what makes the team so interesting—_it forces researchers to sift through the noise of AI research and [find techniques that work](https://x.com/jefrankle/status/1773384010531831956?s=20)_. Plus, the team still finds enough free time to conduct legitimate research and has had a massive impact on the trajectory of open-source LLMs through the MPT/DBRX models. For more details on Mosaic’s research team and the building of DBRX, check out the awesome article below.

[Building DBRX (Wired)](https://www.wired.com/story/dbrx-inside-the-creation-of-the-worlds-most-powerful-open-source-ai-model/)

**Competing LLMs.** Almost immediately after the release of DBRX, several companies responded by releasing new models of their own:

- xAI released Grok-1.5 [[link](https://x.ai/blog/grok-1.5)]
    
- AI21 release Jamba [[link](https://www.ai21.com/jamba)]
    
- Qwen released Qwen1.5-MoE [[link](https://qwenlm.github.io/blog/qwen-moe/)]
    
- SambaNova released Samba-CoE v0.2 [[link](https://x.com/SambaNovaAI/status/1773420223175213174?s=20)]
    

The Samba-CoE model has received a lot of pushback for claiming to outperform DBRX because _i)_ the model is not released openly and _ii)_ it is a composition of experts (CoE) model, or an ensemble of multiple LLMs with output coordinated via a (proprietary) router. In other words, this is a complete “apples-to-oranges” comparison, but Samba-CoE v0.2 does seem to perform well (see below) and is fast (i.e., 330 tokens per second) despite being a CoE.

[

![Samba-Coe v0.3 outperforming open-source models](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc5ed3fe-4ddc-4c58-bd62-3546f7dbd7b8_870x560.png "Samba-Coe v0.3 outperforming open-source models")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc5ed3fe-4ddc-4c58-bd62-3546f7dbd7b8_870x560.png)

([source](https://sambanova.ai/blog/accurate-models-at-blazing-speed))

Qwen1.5-MoE [25] is a smaller MoE model that has 14.3B total parameters and 2.7B active parameters. This model is competitive with similarly-sized state-of-the-art dense models like [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) or [Gemma-7B](https://arxiv.org/abs/2403.08295). Compared to the prior [Qwen1.5-7B](https://huggingface.co/Qwen/Qwen1.5-7B) dense model, Qwen’s MoE model achieves a 75% decrease in training costs and has 1.74× faster inference. Similarly to DBRX, Qwen1.5-MoE uses fine-grained experts, but each MoE layer has a larger number of experts—_64 for each layer instead of 16_. Interestingly, four of these experts are set to always be active during any forward pass. Additionally, a custom weight initialization strategy is used that begins by re-purposing the weights of Qwen-1.8B for the MoE model (with some added randomness), which authors find to improve convergence speed. Despite its smaller size, Qwen1.5-MoE performs quite well; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F419bf47a-5f5a-42cc-8d3d-80f7983a3ecd_1344x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F419bf47a-5f5a-42cc-8d3d-80f7983a3ecd_1344x646.png)

(from [25])

Grok-1.5—which is a followup to the open release of [Grok-1](https://x.ai/blog/grok-os), a 304B parameter (~76B active) MoE with [open weights](https://github.com/xai-org/grok-1)—is (unfortunately) not an open model, but it will be available on X in the near future. The model comes with an expanded context length (128K tokens), as well as improved capabilities on reasoning, math, and coding tasks; see below. Despite the longer context, Grok-1.5 achieves nearly perfect results on the [needle in the haystack test](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)—_a benchmark commonly used to determine how well an LLM pays attention to a large context window_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13f844e4-815c-4dd8-b09a-f5991340a45a_1474x1266.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13f844e4-815c-4dd8-b09a-f5991340a45a_1474x1266.png)

([source](https://x.ai/blog/grok-1.5))

> _“Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.”_ - from [24]

The most interesting of the DBRX launch competitors is [Jamba](https://www.ai21.com/blog/announcing-jamba) [24], an open (Apache 2.0 license) model (download it [here](https://huggingface.co/ai21labs/Jamba-v0.1)) based upon a [state space model (SSM)](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train), called [Mamba](https://github.com/state-spaces/mamba), that has recently shown promise in the language modeling domain. Jamba is an SSM-transformer model that is the first production-scale variant of Mamba, derived by combining SSMs with key components of the transformer architecture. In particular, Jamba is composed of MoE layers, as well as selected parts from Mamba and the transformer; see below. MoE layers are used to add extra capacity to the model, and authors in [24] mention that number of experts is tuned to maximize quality/throughput on a single 80GB GPU.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe144c5ef-7b83-48b9-9ba3-8241dc79b12f_1140x682.jpeg)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe144c5ef-7b83-48b9-9ba3-8241dc79b12f_1140x682.jpeg)

(from [24])

The model has a 256K context window and can even fit 140K tokens of context on a single GPU! Compared to LLMs like Mixtral, Jamba has 3× faster inference due to the ability of SSMs (like Mamba) to address key limitations of the vanilla transformer architecture, such as:

- Larger memory footprint with long contexts.
    
- Slow inference with long contexts (self-attention cost grows quadratically).
    

Typically, the downside of Mamba is that it does not match the quality of top LLMs due to not performing attention over the entire context. However, the performance of Jamba is actually quite impressive, as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb503cf24-8034-4bba-ae8c-ad4fe4f16f43_2280x1132.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb503cf24-8034-4bba-ae8c-ad4fe4f16f43_2280x1132.png)

(from [24])

---

#### **[RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) [1]**

Most use cases with LLMs require specializing the model to understand new information that was not present during pretraining or finetuning. For example, we might have a set of company documents about which we want the model to answer questions. _What is the best way to bake this knowledge into our LLM?_ This question has been studied a lot recently, spurring the creation of a research topic referred to as knowledge injection. Within this line of research, we have seen that [retrieval augmented generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval) is an effective approach [2], but authors in [1] extend this technique by proposing retrieval augmented finetuning (RAFT)—_a finetuning recipe for making LLMs better at RAG._

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)

A basic RAG pipeline

**Different QA setups.** When performing RAG, we assume access to information that can be used by the LLM to better answer a question; see above. However, there are a few different problem setups that are commonly considered for performing question answering with an LLM:

- _Closed book_: use the LLM as a chatbot and rely upon its internal (parametric) knowledge to answer the question properly.
    
- _Open book_: allow the LLM to refer to external sources of information[5](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-5-142727381) when answering the question.
    

In [1], we study a modification of the open book setup called domain-specific open book question answering. This setup assumes that we have both _i)_ an open book setup and _ii)_ prior knowledge of the domain in which the LLM will be tested. By having access to relevant, domain-specific information ahead of time, we can finetune the model on this data to aid the knowledge injection process.

For example, if we want to build an LLM to answer questions based on a fixed set of internal company documents, we can finetune our model on these documents prior to performing any question answering. As we will see, RAFT is just a particular finetuning strategy that is proposed for domain-specific open book question answering tasks. This finetuning strategy focuses upon improving the LLM’s ability to perform RAG, thus boosting the quality of knowledge injection.

[

![RAFT analogy to open-book](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f178bde-7c41-4bf9-9c3c-3e53afc287b9_9396x1697.png "RAFT analogy to open-book")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f178bde-7c41-4bf9-9c3c-3e53afc287b9_9396x1697.png)

Illustration of the difference between closed book, open book, and RAFT experimental settings (from [1])

**What is RAFT?** The first step in performing RAFT is to construct a training dataset. Each data example in this training dataset has three components:

1. A question that the LLM is expected to answer.
    
2. A set of documents that the LLM can use to help answer the question.
    
3. A [chain-of-thought (CoT)](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)-style [3] answer generated from one of the documents.
    

Using these components, we can use a prompt template to construct training examples with the structure shown below. Here, we should notice that portions of the response that are directly sampled from a document are surrounded by the special `##begin_quote##` and `##end_quote##` tokens, which allow the model to learn to rely upon document context when producing an answer.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70cfb7b4-2ae1-49e2-bb4d-4a621f6c8070_1824x748.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70cfb7b4-2ae1-49e2-bb4d-4a621f6c8070_1824x748.png)

Structure of a RAFT training example (from [1])

We can then finetune the LLM—_using [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)_—on such data to produce answers from the provided documents. When creating training examples for RAFT, we use CoT-style responses (i.e., these are usually generated with the help of GPT-4) in particular to hone the model’s reasoning capabilities. The model learns to output CoT-style answers based upon the question and provided context. This finetuning process is the core of the RAFT approach.

**Document types.** Although each training example in RAFT contains a set of documents, we can further classify these documents as one of two types:

- _Oracle documents_: the answer to the question can be reasonably deduced from these documents.
    
- _Distractor documents_: these documents do not contain useful (answer-related) information.
    

During training with RAFT, we set a hyperparameter `P`, such that `P`% of questions in the dataset have both oracle and discriminator documents included within the training example. For `1-P`% of the documents, we only use distractor documents and remove the oracle document. By doing this, RAFT trains the model to disregard documents that have no useful information for crafting a response.

> _“By removing the oracle documents in some instances of the training data, we are compelling the model to memorize domain-knowledge.”_ - from [1]

Additionally, excluding relevant documents also forces the LLM to learn new, domain-specific information during finetuning to a certain extent. See below for a depiction of the RAFT pipeline and its comparison to standard RAG.

[

![Data](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f0b37d0-abe2-4d62-bfc8-645887686379_5330x1957.png "Data")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f0b37d0-abe2-4d62-bfc8-645887686379_5330x1957.png)

RAFT training pipeline (from [1])

The RAFT approach is very similar to finetuning strategies proposed for teaching LLMs to make valid API calls; e.g., [Gorilla](https://cameronrwolfe.substack.com/i/125726849/gorilla-large-language-models-connected-with-massive-apis) [4] or [RAT](https://gorilla.cs.berkeley.edu/blogs/3_retriever_aware_training.html). However, RAFT generalizes these approaches beyond just API calls, focusing on making the model generally better at using any form of context for RAG applications.

**Empirical evaluation.** In [1], authors finetune a LLaMA-2-7B model using the RAFT approach. When evaluating this model, we see that it becomes much better at performing RAG on the documents upon which it is trained; see below.

[

![Gorilla Input and Output](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcedac9b9-7329-4949-82f6-8182d501d72c_2406x648.png "Gorilla Input and Output")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcedac9b9-7329-4949-82f6-8182d501d72c_2406x648.png)

Evaluating RAFT with LLaMA-2-7B (from [1])

Although these results are exactly what we would expect, they truly demonstrate the benefit of RAFT for domain-specific open book question answering. Namely, if we have access to the documents about which we want an LLM to answer questions ahead of time, then we can finetune the LLM over these documents (using RAFT) to significantly improve the quality of RAG!

---

#### [Language models scale reliably with over-training and on downstream tasks](https://arxiv.org/abs/2403.08540) [5]

> _“We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters.”_ - from [5]

Prior work has discovered scaling laws [6, 7] for language models, which show that model performance improves as we _i)_ make the model larger and _ii)_ increase the amount of data over which the model is pretrained. However, there are two main limitations of this work that make scaling laws less practical:

1. These works always study the compute-optimal regime [6], but language models are rarely trained in a compute-optimal manner in practice. Usually, we train smaller models over a larger number of tokens (i.e., overtraining) relative to the compute-optimal approach[6](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-6-142727381) because smaller models are cheaper to use at inference time.
    
2. Performance is measured in terms of next-token prediction loss when measuring scaling laws, but performance on downstream tasks is what actually matters in most applications.
    

With this in mind, we might wonder: _Do scaling laws still hold for more practical settings?_ In [5], authors aim to answer this question by training language models with sizes ranging from 0.011B to 6.9B parameters over a variable number of tokens samples from [RebPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [C4](https://huggingface.co/datasets/c4), or [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb/blob/main/README.md). From these models, we can measure scaling laws for overtrained models, as well as study the correlation between model loss and aggregate performance on downstream tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76498290-0303-4c2d-8dd6-ad15e661f4d2_346x68.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76498290-0303-4c2d-8dd6-ad15e661f4d2_346x68.png)

Basic power law formulation

**What is a power law?** To understand scaling laws, we first need to understand the concept of a power law. In its most basic form, a power law is expressed as shown in the equation above[7](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-7-142727381). A power law simply tells us that two variables (`x` and `y`) have a relationship—_changing_ `x` _produces a [scale-invariant](http://felix.physics.sunysb.edu/~allen/540-05/scaling.html) change in_ `y`. The shape of this relationship is controlled by the values of constants `λ` and `α`. If we plot this power law (with settings `λ = 1.0`, `α = 0.5`, and `0 < x, y < 1`) regularly and in log-log format, we get the graphs shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fffdbd0-f675-43e8-82e1-d8528f9ac6da_1000x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fffdbd0-f675-43e8-82e1-d8528f9ac6da_1000x500.png)

Depiction of an inverse power law between quantities `x` and `y`

The linear log-log plot shown above, which has a slope of `-α` and a y-intercept of `log(λ)`, is the signature of a power law. For LLMs, the quantity `y` is typically the next-token prediction loss on a validation set, while `x` can be several different quantities of interest. For example, we can study the validation loss with respect to compute used during training, number of model parameters, or amount of pretraining data. By plotting the power laws with respect to each of these quantities, we get a familiar picture; see below. The model’s loss decreases smoothly—_according to a power law_—as each of these quantities are increased.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faea6353e-f8d7-4f87-b528-be278427ed43_678x432.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faea6353e-f8d7-4f87-b528-be278427ed43_678x432.png)

Power law between the test loss and size of an LLM (from [7])

**Why is this useful?** As we know, training a language model is an expensive process. The benefit of power laws is that they can help us to mitigate this expense. Namely, we can

1. Train several smaller models.
    
2. Fit a power law to their performance.
    
3. Use this power law to extrapolate and predict the performance of larger (and more expensive) training runs.
    

Using this approach, we can predict the performance of our LLM in advance and determine the optimal training settings without wasting compute on tuning!

> _“To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate … to larger scales.”_ - from [5]

If we can extrapolate LLM performance reliably, we can iterate quickly at smaller scales and select the best settings for large-scale training runs. In the literature, this strategy has been used to predict compute-optimal settings by solving the formulation shown below, which tells us the optimal model and dataset size to use for achieving the best possible test loss given a fixed compute budget.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a3cd49-409d-401f-a05c-cc458a2025e8_2196x724.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a3cd49-409d-401f-a05c-cc458a2025e8_2196x724.png)

The compute-optimal training regime

**Overtrained LLMs.** Authors in [5] begin by generalizing the compute-optimal training regime to the overtrained regime, where we train smaller models on more tokens. Although this approach is not compute-optimal, it is often adopted in practice due to the simple fact that the compute-optimal training formulation shown above completely ignores the inference cost of the resulting LLM. In most cases, we will happily pay a larger training cost to reduce the size of the model—_assuming the model still achieves competitive performance_—that we have to deploy.

> _“While loss should be higher than in the compute-optimal allocation for a given training budget, the resulting models have fewer parameters and thus are cheaper at inference.”_ - from [5]

To study overtraining, we can first introduce a ratio called the token multiplier that can be computed as `M = D / N`. Compute-optimal settings typically have a token multiplier of `M ≈ 20`. Exploring the overtraining regime, we see in [5] that scaling laws hold for different values of `M` ranging from 20 to 640; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3e883bc-1ca2-418b-9b9f-1889c801f96e_1590x972.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3e883bc-1ca2-418b-9b9f-1889c801f96e_1590x972.png)

(from [5])

From this analysis, we learn that overtrained models obey similar power laws as those trained in a compute-optimal manner. Going further, we see in [5] that the value of `M` does not change the slope of the power law! Rather, only the y-intercept is changed, creating the group of parallel lines shown in the figure above. From these scaling laws, we can easily predict the performance of larger, overtrained models. For example, authors in [5] predict the validation loss of 1.4B and 6.9B parameter LLMs trained on 900B and 138B tokens, respectively, from a group of smaller-scale experiments that use 300× less compute.

**Downstream performance.** Taking the same group of LLMs used to discover scaling laws based upon the validation loss, we can plot their average top-1 error over a set of downstream evaluation tasks[8](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-8-142727381). When the authors in [5] create these plots, they see that top-1 error in downstream tasks exponentially decays with respect to the model’s validation loss on the C4 dataset; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b9421e9-74fe-4041-aee4-662a088405e9_1340x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b9421e9-74fe-4041-aee4-662a088405e9_1340x746.png)

(from [5])

This analysis reveals a clear trend in downstream task performance that authors in [5] use to propose a scaling law for top-1 error. Although the relationship between validation loss and top-1 error is dataset-dependent, we can fit a function to predict top-1 error as a function of compute and the amount of over-training. In [5], this strategy is used to predict the average top-1 error over downstream tasks for 1.4B and 6.9B parameter LLMs using 20× less compute.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa4ca530-6689-4625-8af8-3fd734786db4_1402x968.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa4ca530-6689-4625-8af8-3fd734786db4_1402x968.png)

(from [5])

**Empirical evaluation.** The first step of experiments in [5] is to train smaller LLMs with different values of `M`. We can then use scaling laws to extrapolate model performance to larger values of `N` and `M`. The main results of these experiments are shown in the figure above, where we see that the performance of larger-scale training runs—_in terms of both validation loss and top-1 error on downstream tasks_—can be reliably predicted using the scaling laws outlined in [5]. In particular, we see 0.7% relative error in predicting validation loss and 3.6% relative error in predicting top-1 error. As such, we clearly see in [5] that scaling laws are useful for predicting the performance of large-scale training runs in practice.

---

#### **[RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787) [8]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6539e2fa-ef3b-404e-bb3a-934b3d51de05_2232x502.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6539e2fa-ef3b-404e-bb3a-934b3d51de05_2232x502.png)

(from [8])

[Reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations) is almost universally used to power the LLM alignment process. Through alignment, practitioners can instill a variety of capabilities within an LLM; e.g., instruction following, steerability, safety, etc. One key component of RLHF is the reward model (RM), a copy of the original LLM that has been finetuned to predict whether a human user will prefer one piece of text over another. Despite the key role of RMs in RLHF, however, RMs—_and the entire RLHF process in general!_—are rarely documented and poorly analyzed within the AI research community. Many types of RMs and training strategies exist, which makes best practices for training effective RMs that yield good downstream results with RLHF unclear.

> _“Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models.”_ - from [8]

To mitigate these issues, authors in [8] propose RewardBench—_a common framework for comprehensively evaluating many different types of RMs_. The goal of RewardBench is to create an open benchmark (i.e., meaning that all [data](https://huggingface.co/datasets/allenai/reward-bench) and [evaluation code](https://github.com/allenai/reward-bench) is released) that can be used to granularly chart the landscape of 50+ RMs that are publicly available—_authors in [8] even create a [public leaderboard](https://huggingface.co/spaces/allenai/reward-bench) to fulfill this purpose_. This benchmark is an easily-extendable piece of infrastructure that can be expanded to new RMs and datasets that are proposed over time.

RewardBench, which includes new evaluation datasets across several categories of RM performance, is the most comprehensive evaluation suite yet created for RMs[9](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-9-142727381), and it can be easily extended to include new evaluation suites or custom sources of data. By studying the performance of RMs on RewardBench, we can better understand why RMs work. RewardBench aims to create a common framework that allows us to evaluate RMs in a structured manner across many capabilities that might be created during alignment.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb00fc152-1ef3-420e-9610-4ab01865d62f_1642x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb00fc152-1ef3-420e-9610-4ab01865d62f_1642x1004.png)

(from [9])

**What is RLHF?** Oftentimes, the goal of LLM alignment is difficult to concretely define. Instead, we usually construct a set of alignment criteria (e.g., helpfulness, safety, steerability, instruction following, etc.) that can be used to characterize a high-quality response from an LLM. Following this strategy, the first step of the RLHF process (depicted above) is constructing a preference dataset. Each example in this dataset contains a prompt and at least two responses, where the responses are ranked by a human annotator according to their quality. In most cases, we assume that each prompt has two responses, and the human annotator simply identifies the “better” response according to the alignment criteria.

> _“The prevalence of RLHF stems from its efficacy at circumventing one of the greatest difficulties in integrating human values and preferences into language models: specifying an explicit reward”_ - from [8]

From the preference data, we can train an RM to identify whether a human user will prefer one piece of text over another. Then, we can use a reinforcement learning (RL) algorithm like [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) to finetune the LLM according to the signal from the RM. This process finetunes the LLM to produce outputs that are more preferable to humans, as measured by the RM. In this way, we can instill the alignment criteria into the LLM without explicitly specifying a reward function, _which is the core characteristic that underlies the popularity of RLHF_. For more details on RL and how it can be used to finetune an LLM, check out the articles below:

- Basics of Reinforcement Learning for LLMs [[link](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning)]
    
- Policy Gradients: The Foundation of RLHF [[link](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)]
    
- Proximal Policy Optimization (PPO) [[link](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)]
    

**RLHF variants.** In the typical setup, an RM outputs a single, scalar value that captures the human preference score for an LLM’s output. The architecture of the RM is identical to the original LLM, but we append a linear layer to the end of this model that predicts a single value (or logit). As formulated in the figure below, the RM in most cases is trained using a classification objective to predict human preference probabilities according to the [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) [10].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64d6082a-c0fe-4286-a09d-9906b061ea16_1458x558.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64d6082a-c0fe-4286-a09d-9906b061ea16_1458x558.png)

(from [8])

However, there are also popular alternatives to RLHF, such as [Direct Preference Optimization (DPO)](https://www.interconnects.ai/p/the-dpo-debate) [11] that use different RM structures. In the case of DPO, we completely eliminate the need for a separate reward model during alignment. Instead, an implicit reward is constructed using probabilities of the model being trained, as well as probabilities of the base model. Such an approach has gained in popularity recently due to its simplicity and lower compute footprint. However, the DPO models and the difference between DPO and classifier-based RMs has not been extensively analyzed—_an issue that authors in [8] aim to solve_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de7117d-4544-4294-b907-8ce711fe597b_1610x704.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de7117d-4544-4294-b907-8ce711fe597b_1610x704.png)

Scoring technique used by RewardBench (from [8])

**RewardBench design.** The RewardBench dataset is comprised of prompts paired with two responses—_one good and one bad_. To evaluate an RM, we can simply test whether it is capable of identifying the preferred response. This ability to correctly identify the preferred response can be easily captured via an accuracy metric, as shown within the figure above. Then, we can compare different RMs by seeing where they agree or disagree across this dataset. To study RMs on a more granular level, authors also create difficult preference examples that have subtle, but verifiable, reasons why one response should be preferred to another. Ideally, the RM should capture these subtle differences and assign credit to the preferable response in a stable manner. To ensure that length bias [12] does not skew results, authors ensure that all response pairs within RewardBench are of similar length.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ee8aee-08da-4942-bb7f-4d0d34f83d9d_1652x1328.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1ee8aee-08da-4942-bb7f-4d0d34f83d9d_1652x1328.png)

(from [8])

**Evaluation subsets.** As previously mentioned, RewardBench aims to capture RM performance across all relevant categories. For this reason, five different subsets are created for evaluating different capabilities:

1. _Chat_: tests the RM’s ability to distinguish correct chat responses; data is selected from [AlpacaEval](https://huggingface.co/datasets/tatsu-lab/alpaca_eval) and [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench).
    
2. _Chat Hard_: tests the RM’s ability to identify trick questions and subtle differences between responses; data selected from response pairs with similar ratings on MT Bench and adversarial data taken from [LLMBar](https://github.com/princeton-nlp/LLMBar).
    
3. _Safety_: tests refusals of dangerous prompts and ability to avoid incorrect refusals (even with similar trigger words); data selected from [XSTest](https://arxiv.org/abs/2308.01263), [Do-Not-Answer](https://arxiv.org/abs/2308.13387), and an internal dataset from AI2.
    
4. _Reasoning_: tests coding and reasoning abilities; data selected from [HumanEvalPack](https://huggingface.co/datasets/bigcode/humanevalpack)[10](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-10-142727381) and [PRM800K](https://github.com/openai/prm800k).
    
5. _Prior datasets_: existing preference datasets (e.g., [Anthropic’s HH dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf), [Stanford Human Preferences dataset](https://huggingface.co/datasets/stanfordnlp/SHP), and [OpenAI’s learning to summarize dataset](https://huggingface.co/datasets/openai/summarize_from_feedback)) are also aggregated and added into their own category of RewardBench to encourage consistency with prior work.
    

Within each category of RewardBench, models are evaluated in terms of their accuracy. To generate an aggregate score per category, we take a weighted average of examples within that category. For example, the chat subset takes a weighted average of accuracy on AlpacaEval and MT Bench based on the size of each dataset, while the reasoning category weights coding and reasoning subsets equally. To get a top-level score on RewardBench, we just take a uniform average of accuracy across all of the different data subsets.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a2c6e1-49ff-470c-b567-815aa23fac19_1622x1272.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a2c6e1-49ff-470c-b567-815aa23fac19_1622x1272.png)

(from [8])

**Analysis of RMs.** The empirical results from [8] are outlined above. The empirical analysis of [8] is extensive, and I would recommend reading the paper for all of the details. However, the high-level takeaways of this analysis can be summarized as follows:

- The performance of 50+ different RMs with sizes ranging from 300M to 70B parameters is measured, and results are grouped into categories based on the size of the RM (i.e., small, medium or large).
    
- Only large RMs perform well consistently on the more difficult Chat Hard and Reasoning subsets of RewardBench. Most RMs struggle with the Chat Hard and Reasoning subsets, revealing an area of improvement for RMs.
    
- We see a clear, monotonic performance improvement as DPO models become larger, but this trend is less clear for classifier-based RMs—_there are striations in performance that resemble standard LLM evaluations_.
    
- Results on prior evaluation datasets are not entirely consistent with RewardBench. For example, DPO models consistently perform well on RewardBench but struggle on legacy benchmarks.
    
- RMs have various training strategies that result in different distributions and magnitudes of rewards assigned by each RM (see below). The research community has yet to select a standard output distribution for RMs that results in the best downstream performance with RLHF.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb96e85a1-0781-474b-93b5-0360c564bbeb_1600x1078.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb96e85a1-0781-474b-93b5-0360c564bbeb_1600x1078.png)

(from [8])

Work in [8] is just the start of an entire movement—_largely pioneered by [Nathan Lambert](https://www.natolambert.com/) at AI2_—for democratizing understanding of LLM alignment and RLHF. In fact, Nathan has already began partnering with proprietary LLM providers like Cohere—_see [here](https://x.com/natolambert/status/1773376174636613964?s=20) for a discussion_—to benchmark their RMs on RewardBench and provide more transparency into the alignment process.

---

#### **[Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) [13]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7d00ef7-4e2e-462f-9dfa-c4fe0452cb7b_1990x388.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7d00ef7-4e2e-462f-9dfa-c4fe0452cb7b_1990x388.png)

The standard LLM training pipeline (from [15, 16])

As shown in the figure above, the first step of the LLM training pipeline is to pretrain the randomly-initialized model over a massive corpus of raw text. However, there are two key limitations of this approach:

1. Pretraining is very expensive.
    
2. Training data is constantly evolving/expanding in the real world.
    

Ideally, we would want the ability to easily train or adapt the LLM to understand fresh data as it becomes available, but frequently re-running the pretraining process is cost prohibitive. Such a dilemma has caused the topic of knowledge injection, which refers to the process of teaching an LLM new or domain-specific knowledge, to become popular in recent research.

> _“Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute.”_ - from [13]

In [prior work](https://cameronrwolfe.substack.com/i/140501286/fine-tuning-or-retrieval-comparing-knowledge-injection-in-llms) [14], we have seen that retrieval-based approaches for knowledge injection—_[retrieval-augmented generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval) in particular_—are effective in practice and outperform continued pretraining (or finetuning) handily in most cases. In [13], however, authors focus on the continued pretraining regime and show that these findings are less applicable in settings where we are adapting the LLM to larger amounts (i.e., several hundred billion tokens) of incoming data. In these cases, continued pretraining is surprisingly effective when tuned properly.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4992257e-f992-4887-be5a-99496bfe3b39_1986x74.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4992257e-f992-4887-be5a-99496bfe3b39_1986x74.png)

The continual pretraining setup

**Continual pretraining.** Authors in [13] propose a particular training setup, called continual pretraining (see above), in which we have a sequence of `N` datasets, where N >= 2 and each dataset is no smaller than 100B tokens. There may be large distribution shifts (e.g., the introduction of a new language or skill) between each of these datasets, and our goal is to learn from these datasets in sequence. One popular training strategy that follows this structure is continued pretraining of a publicly-available base model (e.g., [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)) on domain-specific data.

**The naive approach.** Given the continual pretraining setup described above, we might wonder how we can incorporate a new, large corpus of data into an LLM. Immediately, there are two naive approaches that we might consider:

1. Combine this data with the original pretraining set and re-run the pretraining process (from scratch) over the full dataset.
    
2. Finetune the LLM over this data using a continued pretraining approach.
    

Unfortunately, these approaches have limitations: _i)_ pretraining is expensive and cannot be performed frequently, _ii)_ the model might not adapt well to the new dataset, and _iii)_ finetuning (or continued pretraining) on the new data in isolation may lead to [catastrophic forgetting](https://neurosciencenews.com/ai-continuous-learning-23671/)[11](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-11-142727381). To get around these issues, we need a simple and scalable continual learning strategy that does not deteriorate the LLM’s understanding of prior data—_this is exactly the core contribution of [13]_.

**Training strategies.** To study continual pretraining, authors in [13] derive two different training setups[12](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-12-142727381):

1. _Continual pretraining_: given an LLM that has been pretrained on `D1`, we further pretrain the model on `D2`.
    
2. _Combined pretraining_: we combine datasets `D1` and `D2` together, then pretrain a randomly-initialized model on the full data.
    

Of the two approaches outlined above, we know that combined pretraining will work well. However, this approach is expensive, as we must pretrain the LLM from scratch each time we want to incorporate new data. Simply continuing the pretraining process over the new data would be much easier and more dynamic, but this approach could also lead to catastrophic forgetting if handled incorrectly.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F193fa7d9-9115-47b6-adb5-a7714832e24f_2238x1402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F193fa7d9-9115-47b6-adb5-a7714832e24f_2238x1402.png)

(from [13])

**A practical strategy for continual pretraining.** In [13], authors discover that—_given a few additional tricks_—we can achieve impressive performance with continual pretraining, even matching or exceeding the performance of combined pretraining! The key details for performing effective continual pretraining, which are outlined in the figure above, are twofold:

1. We must perform a linear warmup and cosine decay of the learning rate during continued pretraining, just as is done during pretraining.
    
2. We must add a small amount of the previous data into the continual pretraining process to avoid catastrophic forgetting (i.e., similar to using a [replay buffer in continual learning](https://cameronrwolfe.substack.com/p/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4)).
    

> _“We demonstrate, across two model sizes and distribution shifts, that a simple and scalable combination of LR re-warming, LR re-decaying, and compute-equivalent replay allows continually pre-trained models to attain similar performance to models re-trained on the union of all data while using significantly less compute.”_ - from [13]

When we adopt these simple tricks, LLMs perform surprisingly well in the continual pretraining setting; see below. Plus, authors in [13] even show that this approach is robust to relatively large domain shifts; e.g., learning a new language.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd57e51bb-9274-4ec1-bf95-694b82770ced_1908x1126.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd57e51bb-9274-4ec1-bf95-694b82770ced_1908x1126.png)

(from [13])

**Brief personal note.** I am especially fond of this paper because I studied similar topics during my PhD. My findings, documented in [this paper](https://arxiv.org/abs/2211.04624), were that we can develop practical (and surprisingly effective) methods for continual training of neural networks by just storing previous data in a replay buffer and sampling data from this buffer throughout the training process. This approach, although very simple, yields impressive results—_even exceeding the performance of models trained using standard training strategies_—if we are sure to tune the hyperparameters properly. As we see in [13], such a strategy generalizes almost perfectly to LLMs!

---

#### **[Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](https://arxiv.org/abs/2402.12374) [17]**

Despite the rapid adoption of LLMs, efficiently serving these models is difficult. LLMs generate tokens using an [autoregressive next token prediction process](https://cameronrwolfe.substack.com/i/136638774/autoregressive-inference-process); see below. This process generates a single token at a time, but generating this token requires the use of all model parameters, which creates an IO bottleneck and causes hardware to be poorly utilized during inference/decoding. Authors in [17] propose a hardware-aware [speculative decoding](https://www.youtube.com/watch?v=S-8yr_RibJ4) strategy, called Sequoia, that mitigates these issues. Sequoia can serve LLMs efficiently on consumer GPUs, increases decoding speed up to 10X (see above[13](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-13-142727381)), and maintains the original output distribution of the LLM (i.e., no approximations are made).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08729f45-ace9-419d-80dd-4520c878cfac_2300x1164.png)

Generating text with a language model

**What is speculative decoding?** [Speculative decoding](https://arxiv.org/abs/2302.01318) is a widely-used technique for speeding up the inference process of an LLM without changing its output distribution. To do this, we create one or more smaller “draft” models that can predict (or “speculate”) output tokens that the LLM will produce. The predictions of draft model(s) can then be organized into a tree structure, where nodes within the tree represent sequences of speculated tokens; see below. The correctness of the speculations are then verified in parallel through a single forward pass of the actual LLM. By storing speculated tokens in a tree (i.e., as opposed to a sequence) we can increase the number of tokens accepted by the LLM during verification by providing several options for each token position instead of a single option.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67bdd6ab-4add-475b-a2ba-24061c4a3912_2038x932.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67bdd6ab-4add-475b-a2ba-24061c4a3912_2038x932.png)

(from [18])

In the figure above, we see a comparison of incremental decoding (i.e., the vanilla, autoregressive token generation process for LLMs) to both sequence and tree-based speculative decoding. Although tree-based speculative decoding yields improved inference throughput in many cases, there are still issues with existing techniques! For example, algorithms like [SpecInfer](https://arxiv.org/abs/2305.09781) [18] and [Spectr](https://arxiv.org/abs/2310.15141) [19] are sub-optimal for larger token trees, are sensitive to inference hyperparameter (e.g., [temperature](https://x.com/cwolferesearch/status/1671628210180698112?s=20)), and cannot optimize the size and shape of the speculated tree based upon the available hardware configuration. With this in mind, authors in [17] aim to answer the following question: _“How can we design an optimal tree-based speculative decoding method to maximize speedups on modern hardware?”_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b12eaec-b9a6-47ef-b544-6fcc068ee764_1892x962.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b12eaec-b9a6-47ef-b544-6fcc068ee764_1892x962.png)

(from [17])

**Sequoia [17]**, as depicted within the figure above, can obtain up to 10× speedups compared to incremental decoding strategies. To obtain these speedups, authors in [17] make three key contributions:

1. _Optimize the tree structure_: In Sequoia, tree construction is formulated as a constrained optimization problem[14](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-14-142727381), and a dynamic programming (DP) approach is used to discover the optimal speculative token tree.
    
2. _Avoiding the same mistakes_: Sequoia samples tokens without replacement from the draft model, which _i)_ prevents the draft model from making the same mistake twice and _ii)_ still maintains the LLM’s output distribution.
    
3. _Considering the hardware_: Sequoia uses a hardware-aware tree optimizer, where verification time is a hardware dependent function of the number of tokens being verified, to solve for the optimal tree shape and depth.
    

At a high level, Sequoia [17] goes beyond prior work by considering the impact of hardware configuration on verification time and investing heavily into finding an optimal tree structure. The result is a hardware-aware speculative decoding framework that can drastically improve the efficiency of serving large LLMs, even on consumer GPUs (e.g., Nvidia RTX-4090 or 2080Ti). When implemented on top of HuggingFace / [Accelerate](https://huggingface.co/docs/accelerate/en/index), the results of Sequoia are impressive; see below.

> _“We can serve a Llama2-70B on a single RTX-4090 with an average time between tokens (TBT) as low as 0.57s, which is 8X faster than a highly optimized offloading serving system, 9X faster than DeepSpeed-Zero Offloading. On a single 2080Ti GPU (only 11GB memory), Vicuna-33B can be served with a TBT of 0.87s.”_ - from [17]

---

#### **[DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging](https://arxiv.org/abs/2402.02622) [20]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ab02ac-19f6-4320-a68f-2ef5d072302f_2302x846.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ab02ac-19f6-4320-a68f-2ef5d072302f_2302x846.png)

(from [20])

The transformer architecture is used almost universally across domains, but researchers are still making fundamental improvements to this architecture. In [20], a simple modification is proposed that provides—_at each layer of the transformer_—direct access to the output of prior layers. In particular, we take a weighted average of all prior layers’ output representations after each transformer block, and the weights used in this average are learned during training; see above. This operation, called a Depth Weighted Average (DWA), is used to create the DenseFormer architecture in [20], which requires fewer layers to perform well and has better data efficiency, a lower memory footprint, and faster inference times compared to the vanilla transformer model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F920ca701-2949-49c4-a572-a46c73684804_1924x1458.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F920ca701-2949-49c4-a572-a46c73684804_1924x1458.png)

(from [2])

**Making it efficient.** Added DWA operations in the DenseFormer have minimal memory[15](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-15-142727381) and compute overhead. However, the DWA operation is IO intensive, as we must consider all prior layers’ output representations when computing the current layer’s output. To solve this, authors in [20] add dilation and periodicity to DWA. At a high level, this means that we modify DWA to sparsely consider prior layers’ outputs (i.e., instead of always considering all prior outputs). When depicting the weights of DWA as a matrix (shown above), dilation and periodicity just sparsify the row and column entries of this matrix, respectively, ensuring that each layer of the DenseFormer only considers a subset of prior layer outputs in DWA. By adding sparsity to DWA, the DenseFormer achieves the same perplexity gains while matching the speed of the vanilla transformer model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45e2ce13-a275-4588-b5ff-44b7ddc0f199_2294x898.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45e2ce13-a275-4588-b5ff-44b7ddc0f199_2294x898.png)

(from [20])

**Why does this work well?** To understand the inner workings of the DenseFormer, authors investigate patterns within the learned DWA weights. As shown in the figure above, there are three visible patterns that emerge in the weights of DWA:

1. Diagonal entries always have high weights, indicating that we consider the current layer’s output heavily relative to prior layers.
    
2. The first column of the matrix is weighted heavily, indicating that the DenseFormer assigns high weights to the model’s initial embedding vectors.
    
3. A block of high weights exists near the final layer, which seems to perform an aggregation over outputs of the last several layers.
    

Overall, adding DWA operations into the transformer allows the model to more easily re-use features from earlier layers without propagating them through many layers—_there is a direct path from early layers to later layers_. This property seems to improve data efficiency and heighten the model’s accuracy in general.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac371ece-8304-4896-816e-5b972125fd88_1970x1024.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac371ece-8304-4896-816e-5b972125fd88_1970x1024.png)

(from [20])

**Performance and implementation.** With direct access to all previous blocks’ outputs (i.e., no sparsity in DWA), the 48-block [DenseFormer](https://twitter.com/hashtag/DenseFormer?src=hashtag_click) far outperforms the vanilla transformer model, but this performance comes at the cost of degraded inference and training speeds. As shown above, however, we can modify the dilation and periodicity to mitigate this issue, eventually achieving comparable improvements in performance without degrading the model’s speed. The DenseFormer can be implemented in only a few lines of code by using the [Python package](https://github.com/epfml/DenseFormer) created by the authors of [20]. Despite its benefits, one notable criticism of the DenseFormer architecture is its lack of compatibility with [pipeline parallelism](https://pytorch.org/docs/stable/pipeline.html)—_an important component of scaling training for LLMs_.

---

#### Honorable Mentions

We covered a variety of papers and topics within this post, but research on LLMs is moving incredibly fast. New techniques and models are being proposed every day. Here are some other interesting, recent contributions that I’ve had my eye on:

- _“MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training”_: An extensive empirical writeup on training multimodal large language models (MLLMs) from Apple [[link](https://arxiv.org/abs/2403.09611)]
    
- _“Recurrent Drafter for Fast Speculative Decoding in Large Language Models”:_ Another speculative decoding algorithm that claims to improve inference efficiency for LLMs (also from Apple[16](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-16-142727381)) [[link](https://arxiv.org/abs/2403.09919)]
    
- _“Stealing Part of a Production Language Model”:_ This paper shows that you can exploit the logprobs returned from an LLM API to extract information about the model behind the API, such as the hidden dimension or even the entire token embedding projection layer [[link](https://arxiv.org/abs/2403.06634)]
    
- _“Training great LLMs entirely from ground up in the wilderness as a startup”_: Very interesting blog post that details the process (and difficulties) of setting up necessary infrastructure from scratch for training LLMs at a startup [[link](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)]
    
- _“tinyBenchmarks: evaluating LLMs with fewer examples”:_ The evaluation process for LLMs is typically very expensive, but this paper explores cheaper methods for evaluating LLMs with similar reliability [[link](https://arxiv.org/abs/2402.14992)]
    
- _“Set the Clock: Temporal Alignment of Pretrained Language Models”:_ LLMs are trained on data from varying points in time, but this paper explores the alignment of an LLM’s internal knowledge to a certain point in time (i.e., “temporal alignment”) [[link](https://arxiv.org/abs/2402.16797)]
    
- _“A Survey on Data Selection for Language Models”_: An extremely thorough survey of existing research on how to optimally construct pretraining datasets for LLMs [[link](https://arxiv.org/abs/2402.16827)]
    

---

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), and this is the Deep (Learning) Focus newsletter, where I help readers understand AI research via overviews of relevant topics from the ground up. If you like the newsletter, please subscribe, share it, or follow me on [Medium](https://medium.com/@wolfecameron), [X](https://twitter.com/cwolferesearch), and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

#### Bibliography

[1] Zhang, Tianjun, et al. "RAFT: Adapting Language Model to Domain Specific RAG.” _arXiv preprint arXiv:2403.10131_ (2024).

[2] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." _arXiv preprint arXiv:2312.05934_ (2023).

[3] Wei, Jason, et al. "Chain of thought prompting elicits reasoning in large language models." _arXiv preprint arXiv:2201.11903_ (2022).

[4] Patil, Shishir G., et al. "Gorilla: Large Language Model Connected with Massive APIs." _arXiv preprint arXiv:2305.15334_ (2023).

[5] Gadre, Samir Yitzhak, et al. "Language models scale reliably with over-training and on downstream tasks." _arXiv preprint arXiv:2403.08540_ (2024).

[6] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[7] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).

[8] Lambert, Nathan, et al. "RewardBench: Evaluating Reward Models for Language Modeling." _arXiv preprint arXiv:2403.13787_ (2024).

[9] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[10] Bradley, Ralph Allan, and Milton E. Terry. "Rank analysis of incomplete block designs: I. The method of paired comparisons." _Biometrika_ 39.3/4 (1952): 324-345.

[11] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." _Advances in Neural Information Processing Systems_ 36 (2024).

[12] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.

[13] Ibrahim, Adam, et al. "Simple and Scalable Strategies to Continually Pre-train Large Language Models." _arXiv preprint arXiv:2403.08763_ (2024).

[14] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." _arXiv preprint arXiv:2312.05934_ (2023).

[15] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[16] Glaese, Amelia, et al. "Improving alignment of dialogue agents via targeted human judgements." _arXiv preprint arXiv:2209.14375_ (2022).

[17] Chen, Zhuoming, et al. "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding." _arXiv preprint arXiv:2402.12374_ (2024).

[18] Miao, Xupeng, et al. "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification." _arXiv preprint arXiv:2305.09781_ (2023).

[19] Sun, Ziteng, et al. "Spectr: Fast speculative decoding via optimal transport." _Advances in Neural Information Processing Systems_ 36 (2024).

[20] Pagliardini, Matteo, et al. "DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging." _arXiv preprint arXiv:2402.02622_ (2024).

[21] “Introducing DBRX: A New State-of-the-Art Open LLM.” _Databricks_ _Mosaic AI Research_, 27 March 2024, https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm.

[22] Madaan, Aman, et al. "Language models of code are few-shot commonsense learners." _arXiv preprint arXiv:2210.07128_ (2022).

[23] Zoph, Barret, et al. "St-moe: Designing stable and transferable sparse expert models." _arXiv preprint arXiv:2202.08906_ (2022).

[24] “Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model” _AI21 Labs_, 28 March 2024, https://www.ai21.com/blog/announcing-jamba.

[25] “Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters”, _Qwen_, 28 March 2024, https://qwenlm.github.io/blog/qwen-moe/

[1](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-1-142727381)

This is a massive dataset! For reference, MPT-7B and MPT-30B were trained on 1T tokens of text.

[2](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-2-142727381)

The quality of datasets used to train DBRX are a result of the research by [Cody Blakeney](https://twitter.com/code_star) and the rest of the Data Research team at Mosaic. In fact, some members of this team have even branched off to form an [entirely new startup](https://www.datologyai.com/) that is 100% focused upon improving data quality.

[3](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-3-142727381)

According to researchers at Mosaic, the GPT-4 tokenizer was selected partly due to performance and partly due to the practical consideration that it makes pricing comparisons to competing models (i.e., GPT-4) more direct/simple.

[4](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-4-142727381)

This comes as no surprise because the model has strong coding abilities—_likely because the pretraining data mixture contains a lot of code_—and training on more code is known to yield improved reasoning capabilities in the LLM [22].

[5](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-5-142727381)

Usually, this information is surfaced via a retrieval mechanism. For example, RAG typically retrieves the `K` most relevant documents to include in the model’s prompt when answering a question.

[6](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-6-142727381)

For example, if the compute-optimal approach to achieve a given loss L is a 7B parameter model trained over 800B tokens, maybe we will instead train a 3.5B parameter model over 2T tokens to achieve the same loss L.

[7](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-7-142727381)

Here, we consider an inverse power law (i.e., `p` is negative) because scaling laws are always inverse for language models. The loss _decreases_ with more compute.

[8](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-8-142727381)

In [5], authors use the [evaluation gauntlet](https://github.com/mosaicml/llm-foundry/tree/main/scripts/eval) within Mosaic AI’s [LLM foundry](https://github.com/mosaicml/llm-foundry) to compute downstream performance metrics.

[9](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-9-142727381)

Some analysis of RMs has been conducted, but it is usually done on datasets with known issues. For example, both Anthropic’s [helpful and harmless dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf), as well as OpenAI’s [learning to summarize dataset](https://huggingface.co/datasets/openai/summarize_from_feedback) are commonly used to evaluate RMs despite being known to have high levels of inter-annotator disagreement.

[10](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-10-142727381)

Examples are created by pairing correct code from the existing dataset with code that is known to contain bugs.

[11](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-11-142727381)

“Catastrophic forgetting” is a common term in active/online learning research. Put simply, it refers to the phenomenon of a neural network completely forgetting about previously learned skills or data when being trained on new data.

[12](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-12-142727381)

In describing these setups, we assume that `N = 2`. However, we can adapt similar training strategies for `N >= 3`.

[13](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-13-142727381)

This video depicts the decoding/inference process (at 4X speed) of LLaMA-2-70B on a single RTX-4090 GPU both with and without Sequoia.

[14](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-14-142727381)

“In this optimization problem, we aim to maximize the expected number of tokens F(T ) generated by verifying a token tree T , under a constraint on the size of T .” - from [17]

[15](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-15-142727381)

The number of total added weights is `d(d + 3) / 2`, where `d` is the depth of the transformer. This number is negligible in comparison to the parameter count of modern transformer models (e.g., LLMs).

[16](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench#footnote-anchor-16-142727381)

Apple seems to be publishing a lot of AI-related technical reports recently, indicating that they may be trying to heighten their reach and involvement within the AI research community.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Fabbri Paolo's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54d12824-9b91-4536-8696-59f5c1628689_766x1434.jpeg)



](https://substack.com/profile/1483954-fabbri-paolo)

[

![Nathan Lambert's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fedcdfb-e137-4f6a-9089-a46add6c6242_500x500.jpeg)



](https://substack.com/profile/10472909-nathan-lambert)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

37 Likes∙

[4 Restacks](https://substack.com/note/p-142727381/restacks?utm_source=substack&utm_content=facepile-restacks)

37

- 

[

6

](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comments)

4

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Riccardo Vocca's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90ca84a9-e17f-4a0b-ac49-4885c5a700fd_800x800.png)



](https://substack.com/profile/28560656-riccardo-vocca?utm_source=comment)

[Riccardo Vocca](https://substack.com/profile/28560656-riccardo-vocca?utm_source=substack-feed-item)

[The Intelligent Friend](https://theintelligentfriend.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年4月9日](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comment/53509155 "2024年4月9日 14:53")

Liked by Cameron R. Wolfe, Ph.D.

Hi Cameron, congratulations on these numbers, the academic depth and clarity with which they are written is impressive! I also write a paper-based newsletter, so that's even more inspirational!

I was curious: do you usually choose a topic first and then do research on the topic or is it usually a paper that inspires you and then you go accordingly?

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comment/53509155)

[

![Mr. Teera's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28715c9f-e7a9-4e2c-8ce4-b4f1b01f1199_300x300.png)



](https://substack.com/profile/187844953-mr-teera?utm_source=comment)

[Mr. Teera](https://substack.com/profile/187844953-mr-teera?utm_source=substack-feed-item)

[Mr. Teera](https://teeraurekesawaphd385341.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[2024年4月2日](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comment/52985689 "2024年4月2日 11:37")

Liked by Cameron R. Wolfe, Ph.D.

how can you find and updata latest paper about all this is interesting, you have source or technique to do this, please suggest me, I'm new to this.

Like (1)

Reply

Share

[2 replies by Cameron R. Wolfe, Ph.D. and others](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comment/52985689)

[4 more comments...](https://cameronrwolfe.substack.com/p/dbrx-continual-pretraining-rewardbench/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



-----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Modern Advances in Prompt Engineering

### Distilling and understanding the most rapidly-evolving research topic in AI...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Apr 29, 2024

111

- 

[

7

](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comments)

17

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F824499cf-f366-4612-a437-8785af41ee50_2134x1184.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F824499cf-f366-4612-a437-8785af41ee50_2134x1184.png)

(from [17, 29, 35, 39])

Due to their ease of use, large language models (LLMs) have seen an explosive rise to popularity. By just crafting a textual prompt, even those who are completely unfamiliar with deep learning can leverage massive neural networks to quickly solve a wide variety of complex problems. Over time, these models have become even easier to use via improved instruction following capabilities and alignment. However, effectively prompting LLMs is both an art and a science—_significant_ _performance improvements can be achieved by slightly tweaking our prompting implementation or strategy_. In this overview, we will develop a comprehensive understanding of prompt engineering, beginning with basic concepts and going all the way to cutting-edge techniques that have been proposed in recent months.

## What is prompt engineering?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a25852-e696-4033-9887-1560baf4f37c_2456x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06a25852-e696-4033-9887-1560baf4f37c_2456x840.png)

One major reason that LLMs are so popular is because their text-to-text interface makes them incredibly simple to use. In a prior generation, solving a task with deep learning would require that we (at a minimum) finetune a model over some data to teach the model how to solve that task. Plus, most of these models were narrow experts, meaning that they would specialize in solving a single task. Due to the emergent [in-context learning](https://x.com/cwolferesearch/status/1753458022251180439) abilities of LLMs, however, we can solve a variety of problems via a textual prompt; see above. Previously complex problem solving processes have been [abstracted into natural language](https://karpathy.medium.com/software-2-0-a64152b37c35)!

> _“Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use LMs for a wide variety of applications and research topics.”_ - from [1]

**What is prompt engineering?** The simplicity of LLMs has democratized their use. You don’t need to be a data scientist or an MLE to use an LLM—_as long as you understand English (or your language of choice) you can solve relatively complex problems with an LLM_! When solving a problem with an LLM, however, the results that we achieve depend heavily upon the textual prompt provided to the model. For this reason, prompt engineering—_the empirical science of testing different prompts to optimize an LLM’s performance_—has become extremely popular and impactful, resulting in the discovery of many techniques and best practices.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7df4b8f5-6894-4e63-8057-36db0d19f095_1574x928.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7df4b8f5-6894-4e63-8057-36db0d19f095_1574x928.png)

Including indicators within a prompt

**Prompt components.** There are many ways to prompt an LLM. However, most prompting strategies share a few common components:

- _Input Data_: the actual data that the LLM is expected to process (e.g., the sentence being translated or classified, the document being summarized, etc.).
    
- _Exemplars_: concrete examples of correct input-output pairs that are included within the prompt.
    
- _Instruction_: a textual description of the output that is expected of the model.
    
- _Indicators_: tags or formatting elements that are used to create structure within the prompt; see above.
    
- _Context_: any extra information provided to the LLM in the prompt.
    

In the figure below, we see an example that combines all of the above-mentioned prompt components within a single prompt for sentence classification.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba4dbc86-70b9-476e-8c63-78bd6d4c0c75_1266x916.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba4dbc86-70b9-476e-8c63-78bd6d4c0c75_1266x916.png)

Prompt with all components in the style of LLaMA-2 (see [here](https://huggingface.co/blog/llama2#how-to-prompt-llama-2))

**The context window.** During pretraining, an LLM sees input sequences of a particular length. This choice of sequence length during pretraining becomes the model’s _“context length”_, or the maximum length of sequence that the model is capable of processing. Given a textual sequence that is significantly longer than this predetermined context length, the model may behave unpredictably and produce incorrect output. However, there are methods—_such as [Self-Extend](https://cameronrwolfe.substack.com/i/140501286/llm-maybe-longlm-self-extend-llm-context-window-without-tuning) or [positional interpolation](https://arxiv.org/abs/2306.15595)_—that can be used to extend the model’s context window.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe32bfe12-7ae3-4670-a1a8-34a72c6cda89_1834x944.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe32bfe12-7ae3-4670-a1a8-34a72c6cda89_1834x944.png)

Illustration of positional interpolation with RoPE (from [34])

Recent research on LLMs has emphasized the creation of [long context windows](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/), which allow the model to process more information within each prompt (e.g., more exemplars or a larger amount of context). As we will see, however, _not all LLMs pay perfect attention to their context_! The ability of an LLM to leverage information within a long context window is typically assessed via a [needle in the haystack test](https://github.com/gkamradt/LLMTest_NeedleInAHaystack), which _i)_ embeds a random fact within the context, _ii)_ asks the model to retrieve the fact, and _iii)_ repeats this test over various context lengths and positions of the fact in the context. Such a test yields a picture like the one shown below, where we can easily spot deficiencies in the context window.

[

![GPT-4-128 Context Testing](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9546823-fd44-4b5a-ad85-2d8da8ae69d7_1712x958.png "GPT-4-128 Context Testing")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9546823-fd44-4b5a-ad85-2d8da8ae69d7_1712x958.png)

([source](https://github.com/gkamradt/LLMTest_NeedleInAHaystack))

**My prompt engineering strategy.** The details of prompt engineering differ a lot based upon the model being used. However, there are a few general principals that are often useful for guiding the prompt engineering process:

1. _Be empirical_: the first step of prompt engineering is to setup a reliable way of evaluating your prompt (e.g., via test cases, human evaluators, or [LLM-as-a-judge](https://arxiv.org/abs/2306.05685)) so that you can easily evaluate changes to a prompt.
    
2. _Start simple_: the first prompt you try should not be a chain-of-thought prompt (or some other specialized prompting technique). Start with the simplest prompt possible and slowly add complexity while measuring the change in performance (see above) to determine if extra complexity is necessary[1](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-1-143156742).
    
3. _Be specific and direct_: eliminate ambiguity in the prompt and try to be concise, direct, and specific when describing the desired output of the LLM.
    
4. _Use exemplars_: if describing the desired output is difficult, try adding some exemplars to the prompt. Exemplars eliminate ambiguity by providing concrete examples of what is expected of the LLM.
    
5. _Avoid complexity (if possible)_: complex prompting strategies are sometimes necessary (e.g., to solve multi-step reasoning problems), but we should think twice before using such approaches. Be empirical and use the established evaluation strategy to truly determine whether the complexity is necessary.
    

To summarize everything above, my personal prompt engineering strategy is to _i)_ invest into a really good evaluation framework, _ii)_ start with a simple prompt, and _iii)_ slowly add complexity as necessary to achieve the desired level of performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb7772f-305e-4d12-9ca3-68ec34dcf714_2096x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddb7772f-305e-4d12-9ca3-68ec34dcf714_2096x636.png)

Writing prompts is an iterative process!

## Prompting Techniques

We have previously learned about a variety of prompting techniques through a series of related overviews:

- Practical Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)]
    
- Advanced Prompt Engineering [[link](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)]
    
- Chain of Thought Prompting [[link](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)]
    
- Prompt Ensembles [[link](https://cameronrwolfe.substack.com/p/prompt-ensembles-make-llms-more-reliable)]
    

We will now overview relevant prompting techniques once again, providing a foundation for the more complex approaches that will be introduced later in the post. As we learn about each of these techniques, however, we should keep in mind the importance of simplicity in prompt engineering. _Just because a prompting technique is more intricate or complex does not make it better than simpler strategies_!

#### Basic Prompting Strategies

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe749d0ff-794c-4fd5-a8bf-d75873bf3e38_924x322.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe749d0ff-794c-4fd5-a8bf-d75873bf3e38_924x322.png)

(from [3])

**Zero-shot prompting** (shown above)—_as popularized by [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt)_ [2]—is one of the most basic prompting strategies that we can employ. To solve a task via zero-shot prompting, we just _i)_ describe the task in the prompt and _ii)_ prompt the model to solve the problem. In the case of the problem above, the task is translating a word from English to French, and we prompt the model to make this translation via the string “cheese =>”, which prompts the model to emit the French translation of the word cheese. Several examples of zero-shot prompts are provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc2ea13-cf4b-4b6c-955f-835dbbf27be4_2412x752.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc2ea13-cf4b-4b6c-955f-835dbbf27be4_2412x752.png)

Zero-shot learning (outputs produced with GPT-3.5-Turbo)

Although zero-shot learning performs well in some cases, it is limited by the ambiguity of task descriptions. Performance is dependent upon the creation of a clear/comprehensive description, and we rely upon the model’s ability to produce the correct output based on this description alone. Oftentimes, we can achieve better performance by inserting more concrete information into the prompt.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50ee0351-5743-4f1a-bdee-2f0e13074579_1590x1044.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50ee0351-5743-4f1a-bdee-2f0e13074579_1590x1044.png)

(from [3])

**Few-shot prompting** does exactly this by inserting several examples of correct problem solutions into the prompt. This strategy was popularized by [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) [3], which showed that LLMs develop impressive few-shot learning abilities at scale; see above. Intuitively, few-shot learning eliminates the ambiguity of zero-shot learning by providing several examples of the expected output. As such, the model can understand the correct behavior directly from these exemplars, rather than inferring the desired behavior from the task description; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17c1bf7a-0962-479a-8510-e20fd94882db_944x542.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17c1bf7a-0962-479a-8510-e20fd94882db_944x542.png)

(from [3])

The LLM can learn from these examples provided within the prompt, a strategy commonly referred to as _“in-context learning”_; see below. However, this style of learning is not like normal training of a neural network—_the parameters of the model are not modified at all_. Rather, we put relevant information in the prompt, and the model can use this information as context for generating a better output.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc2d031-d1b8-487c-9db6-9cb55baab3ad_1688x832.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bc2d031-d1b8-487c-9db6-9cb55baab3ad_1688x832.png)

(from [3])

When using few-shot learning in practice, there are two key settings that we must tune properly:

1. The number of exemplars to use.
    
2. The strategy for selecting exemplars.
    

To determine the correct number of exemplars to use, we can perform some basic [hyperparameter tuning](https://www.jeremyjordan.me/hyperparameter-tuning/) using an evaluation set. Many papers have explored strategies for exemplar selection (e.g., based on [random selection](https://arxiv.org/abs/2005.14165), [diversity](https://arxiv.org/abs/2209.01975), [semantic similarity](https://arxiv.org/abs/2101.06804), [active learning](https://arxiv.org/abs/2302.12246), or more [complex metrics](https://arxiv.org/abs/2211.04486))[2](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-2-143156742). However, random selection of exemplars is oftentimes an effective strategy in practice. Beyond these strategies, there are a variety of practical rules and findings relevant to few-shot learning that we should always keep in mind [4, 5]:

- The distribution of labels—_even if they are incorrect_—for exemplars can impact the model’s answer, as the model is biased towards common labels.
    
- The answer is biased towards recently-observed exemplars in the prompt[3](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-3-143156742).
    
- The formatting of exemplars in the prompt is important.
    
- Selecting exemplars randomly can help to remove bias (e.g., position or majority label bias) within the model’s generated answer.
    

Despite its simplicity, few-shot learning is one of the most effective prompting strategies and is widely-used within practical applications.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F709a92cd-ab31-4343-bf83-62fc865d9c13_1154x928.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F709a92cd-ab31-4343-bf83-62fc865d9c13_1154x928.png)

Several examples of instruction prompts (from [6])

**Instruction prompting** is a more direct method of expressing the LLM’s desired output. With few-shot learning, we explain our intent to the model via concrete exemplars of a task being solved, _but these exemplars consume a lot of tokens_! Simply explaining our intent to the model in words would be much more efficient. For this to work well, the LLM being used must be [aligned](https://cameronrwolfe.substack.com/i/138218863/language-model-alignment) to consistently follow instructions. Such models are said to be _“steerable”_ because they understand detailed instructions provided and can adjust their output accordingly.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53f305e4-aebd-42cd-afe3-54f7be1961a0_1614x752.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53f305e4-aebd-42cd-afe3-54f7be1961a0_1614x752.png)

(from [6])

Research on LLMs has heavily focused upon improving instruction following capabilities. Pre-trained LLMs are not good at following instructions out-of-the-box. As shown by InstructGPT [6], however, we can align models to be much better at following instructions via a combination of [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). We see in the figure above that this strategy can improve instruction following, as well as other key properties of the LLM (e.g., factuality and constraint following).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f48aa5f-60f5-46ef-9de3-57bd700bf350_1264x1220.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f48aa5f-60f5-46ef-9de3-57bd700bf350_1264x1220.png)

Role prompting with LaMDA (from [8])

Given recent advancements in LLM alignment, instruction prompting—_which can even be combined with few-shot prompting [7]_—is a highly effective approach that is commonly used in practical applications. In fact, several popular prompting strategies (e.g., [role prompting](https://learnprompting.org/docs/basics/roles), [specifying an audience](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#instruction-prompting), or [tool usage](https://arxiv.org/abs/2303.17580) to name a few) are just more specific versions of instruction prompting! When writing instructions, we should be clear and precise to ensure the best possible results.

#### Advanced Prompting Strategies

Although the prompting techniques outlined above are highly effective, sometimes more complex prompts can be useful for solving difficult problems (e.g., math/coding or multi-step reasoning problems). Because LLMs naturally struggle with these problems[4](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-4-143156742) (i.e., reasoning capabilities do not improve monotonically with model scale [9]), a majority of existing research on prompt engineering is focused upon improving reasoning and complex problem solving capabilities—_simple prompts will work for solving most other problems_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [10])

**Chain of Thought (CoT) prompting [10]** elicits reasoning capabilities in LLMs by inserting a chain of thought (i.e., a series of intermediate reasoning steps) into exemplars within a model’s prompt; see above. By augmenting each exemplar with a chain of thought, the model learns (via in-context learning) to generate a similar chain of thought prior to outputting the final answer for the problem in question. Interestingly, we see in [10] that sufficiently large models (i.e., >100B parameters) benefit heavily from this approach on arithmetic, commonsense and symbolic reasoning tasks—_explicitly explaining the underlying reasoning process for solving a problem actually makes the model more effective at reasoning_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d770953-ca52-4025-9945-683b5195c67d_1604x1394.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d770953-ca52-4025-9945-683b5195c67d_1604x1394.png)

(from [10])

The implementation of CoT prompting is simple. Instead of each few-shot exemplar having only an input and output, exemplars are triplets of the form `(input, chain of thought, output)`; see above. The major downside of this approach is that we must manually (or synthetically) curate exemplars that include a full rationale for the solution to a problem, which can be expensive and/or time consuming. As such, many papers focus upon eliminating the dependence of CoT prompting upon human-written rationales!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e413891-7356-4f90-b36c-0a046eab0ccb_1932x550.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e413891-7356-4f90-b36c-0a046eab0ccb_1932x550.png)

(from [11, 12])

**CoT variants.** Due to the effectiveness and popularity of CoT prompting, numerous extensions of this approach have been proposed. For example, zero-shot CoT [11] prompting eliminates few-shot exemplars and instead encourages the model to generate a problem-solving rationale by appending the words _“Let’s think step by step.”_ to the end of the prompt. We can also improve the robustness of the reasoning process by _i)_ independently generating multiple chains of thought when solving a problem and _ii)_ taking a majority vote of the final answers produced with each chain of thought[5](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-5-143156742). Despite increasing the cost of solving a problem, this approach, called self-consistency [12], improves the reliability of LLMs when solving more complex classes of reasoning problems.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd80814e2-0e61-4119-978f-87f1790d04bd_866x684.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd80814e2-0e61-4119-978f-87f1790d04bd_866x684.png)

(from [13])

Least-to-most prompting [13] goes beyond CoT prompting by explicitly breaking down a complex problem into multiple parts; see above. Each sub-problem is solved individually, and the solution to each sub-problem is passed as context for solving the next sub-problem. Once we have reached the final sub-problem, we can use the context of prior solutions to output a final answer to the question.

> _“It is perhaps surprising that underlying all this progress [for LLMs] is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion.”_ - from [14]

**Tree of thoughts (ToT) prompting [14].** Techniques like CoT prompting follow a left-to-right generation approach that uses [next-token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) to output a solution in a single attempt. Such an approach, although effective in certain scenarios, may fail to solve complex problems that can benefit from extensive planning, strategic lookahead, backtracking, and exploration of numerous viable solutions in parallel. _This is where ToT prompting comes in_! ToT prompting—_somewhat similarly to least-to-most prompting [13]_—breaks a complex problem into a series of simpler problems (or “thoughts”) that can be solved individually.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff04dd32f-0938-47dc-a8c8-c6a4377a5b12_1604x858.png)

(from [14])

Unlike CoT prompting, ToT prompting does not require that we follow a single path of thoughts when solving a problem. Additionally, ToT prompting does not simply take a majority vote of multiple reasoning paths like self-consistency; see above. During exploration, the LLM generates many thoughts and continually evaluates its progress toward a final solution via natural language (i.e., we just prompt the model!). By leveraging the model’s self-evaluation of its own progress towards a final solution, we can power the exploration process with widely-used search algorithms (e.g., [breadth-first search or depth-first search](https://www.geeksforgeeks.org/difference-between-bfs-and-dfs/)), allowing lookahead and backtracking to be performed within the problem-solving process. Check out [this overview](https://cameronrwolfe.substack.com/p/tree-of-thoughts-prompting) for a more detailed explanation of ToT prompting.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc113cb01-ac85-4e0e-a418-a21c5dd9a4e0_1728x718.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc113cb01-ac85-4e0e-a418-a21c5dd9a4e0_1728x718.png)

(from [35])

**Graph of Thoughts (GoT) prompting [35, 36].** Later work generalized research on ToT prompting to graph-based strategies for reasoning. Overall, these techniques are similar to ToT prompting, but they make no assumption that the path of thoughts used to generate a final solution is linear. Rather, we can re-use thoughts or even recurse through a sequence of several thoughts when deriving a solution; see above. Multiple graph-based prompting strategies have been proposed (see [here](https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning) for more details) [35, 36]. However, these prompting techniques—_as well as ToT prompting_—have been criticized for their lack of practicality. Namely, solving a reasoning problem with GoT prompting could potentially require a massive number of inference steps from the LLM!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F836d1309-73a6-44c6-9f22-056742ac3cee_2318x746.png)

A basic RAG pipeline

**Retrieval Augmented Generation (RAG) [37]** (shown above), though not purely a prompting technique, is a widely used strategy that improves the quality of an LLM’s output by retrieving relevant context to include in the prompt. To retrieve useful context, we can use just use existing search techniques; e.g., pure vector search or a hybrid search engine. Despite its simplicity, research has shown that RAG is incredibly effective at injecting knowledge into an LLM and reducing the number of hallucinations generated by the model [38]. Plus, we can easily provide citations to users of the LLM by simply exposing the relevant documents being retrieved by RAG. However, the manner in which we process and retrieve data, as well as how we structure the context that is inserted into the prompt, can have a significant impact upon performance; see [here](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F525f50a2-5174-4851-99b3-0bf251d59477_908x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F525f50a2-5174-4851-99b3-0bf251d59477_908x930.png)

(from [39])

**Generated knowledge prompting [39]** is an interesting alternative to RAG that uses an LLM to generate relevant context to include in the prompt instead of retrieving this context from an external database; see above. Despite being very simple and having positive performance indications, this approach (obviously) lacks in reliability due to the tendency of LLMs to hallucinate information.

## Recent Directions of Research

Although we have covered a variety of prompting techniques so far, many papers have been published recently that both expand upon these methods and explore completely new styles of prompts for solving complex problems. Here, we have separated this work into several categories based upon the topic or focus:

- Reasoning
    
- Tool Usage
    
- Program-Aided Language Models
    
- Context Windows
    
- Writing
    
- Miscellaneous (other notable papers)
    

For each category, a variety of different works are covered. Given the massive amount of research that has been published on the topic of prompt engineering, however, there’s a good chance that several papers were missed. If you know of a good paper that should be included, please share it in the comments!

#### Improving Reasoning Capabilities

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec269102-c433-4bb8-8276-33124df49574_1392x810.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec269102-c433-4bb8-8276-33124df49574_1392x810.png)

(from [15])

**Auto-CoT [15].** CoT prompting uses intermediate reasoning steps to solve complex problems, and there are two ways we can elicit these reasoning steps within an LLM’s output (depicted above):

1. _Zero-shot_: prompt the LLM to “think step-by-step”.
    
2. _Manual_: provide several few-shot examples of questions, rationales, and answers prior to answering the desired question.
    

Although LLMs are decent zero-shot reasoners, providing concrete exemplars consistently yields better performance with CoT prompting. However, this strategy also requires human annotators—_or the prompt engineer_—to craft manual demonstrations of rationales used to answer each question. Crafting these manual demonstrations is time consuming, but it can be avoided!

> _“We show that such manual efforts may be eliminated by leveraging LLMs with the Let’s think step by step prompt to generate reasoning chains for demonstrations one by one.”_ - from [15]

In [15], authors propose an automatic CoT (Auto-CoT) prompting approach that uses zero-shot CoT prompting to automatically generate examples for manual CoT prompting, thus eliminating the need to manually craft problem-solving rationales. However, because these automatically-generated rationales are incorrect in some cases, a few tricks are required for Auto-CoT to work well.

Given a question as input, a naive approach would be to _i)_ retrieve a set of similar questions (e.g., using an embedding model like sBERT and [vector search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search)), _ii)_ generate rationales/answers for each of these questions with zero-shot CoT prompting, and _iii)_ perform manual CoT prompting with the automatically-generated demonstrations. However, this approach works quite poorly, which authors in [1] claim is due to mistakes in the LLM-generated rationales. To solve this, we simply need to ensure the generated rationales are sufficiently diverse.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599d84f3-d343-4085-8050-2642c216c500_1398x842.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599d84f3-d343-4085-8050-2642c216c500_1398x842.png)

(from [15])

Given a dataset of questions that should be answered by the LLM, authors in [15] devise a two-part strategy (shown above) for selecting/generating demonstrations used within the prompt for Auto-CoT:

- Divides the questions into `k` clusters using question embeddings from [sBERT](https://cameronrwolfe.substack.com/i/140061921/sentence-bert-sentence-embeddings-using-siamese-bert-networksextensions-of-sbert) and [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering).
    
- Selects a representative question from each cluster and generates an associated rationale for each question using zero-shot CoT.
    

Such an approach ensures that the diversity of demonstrations used for Auto-CoT is high, which reduces the correlation between mistakes made by the model across synthetic rationales. In experiments with GPT-3, Auto-CoT consistently matches or exceeds the performance of few-shot CoT prompting, which requires the manual creation of demonstrations, on over ten different benchmarks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18f2b729-03de-4100-bcce-55dd3d6f5542_1344x830.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18f2b729-03de-4100-bcce-55dd3d6f5542_1344x830.png)

(from [16])

**Complexity-Based Prompting [16].** Given that CoT prompting relies upon selecting demonstrations of problem-solving rationales to include in the prompt, we might wonder: _How do we best select these demonstrations?_ In [16], authors show that selecting demonstrations based on their complexity is a good heuristic. We can measure the complexity of a demonstration by simply counting the number of steps present within the chain of thought, where individual steps are separated by newline characters (`\n`). The complexity-based prompting approach proposed in [16] advocates sampling demonstrations with the highest complexity.

> _“The reasoning performance of GPT-3 175B clearly improves with the increased input prompt complexity.”_ - from [16]

Interestingly, authors in [16] discover that including demonstrations with more reasoning steps in the CoT prompt substantially improves performance on multi-step reasoning tasks. Going further, one can extend this strategy to the output space by using a self-consistency approach that takes a majority vote over `k` generated outputs with the highest complexity. Compared to alternative selection schemes like manual tuning and retrieval-based selection, complexity-based prompting performs favorably, achieving state-of-the-art performance on several datasets (i.e., [GSM8K](https://huggingface.co/datasets/gsm8k), [MultiArith](https://huggingface.co/datasets/ChilleD/MultiArith), and [MathQA](https://huggingface.co/datasets/math_qa)) with GPT-3 and Codex.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bff3de2-7f9b-427f-a158-acbb1c1420b6_1610x1332.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1bff3de2-7f9b-427f-a158-acbb1c1420b6_1610x1332.png)

(from [17])

**Progressive-Hint Prompting (PHP) [17].** One downside of CoT prompting is that it solves a problem in a single shot. Given a question as input, we generate a rationale and answer, but the LLM does not get a chance to consider or revise this answer. One can achieve better performance by repeating this process multiple times and taking a majority vote—_this is just self-consistency_—but none of these generations consider the LLM’s prior outputs to better inform the answer.

> _“PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question.”_ - from [17]

To solve this issue, authors in [17] propose PHP to leverage prior outputs of the LLM to iteratively refine the generated rationale. Intuitively, the LLM can use rationales previously generated by the model as hints towards discovering the correct answer. Concretely, PHP proceeds in three steps:

1. Given a question, prompt the LLM to provide a base answer.
    
2. Concatenate the question and base answer, then prompt the LLM to generate a revised answer based on this input.
    
3. Repeat step two until the LLM’s answer is stable for at least two iterations.
    

Such an approach allows the LLM to iteratively refine its answer over several passes, using its prior output as context during the process. Additionally, PHP is fully compatible with CoT prompting and self-consistency—_we can combine these techniques to further improve performance_. In experiments, PHP improves the performance of GPT-3.5 in comparison to a complexity-based prompting strategy, and using PHP with GPT-4 yields state-of-the-art performance on several notable datasets (e.g., [SVAMP](https://huggingface.co/datasets/ChilleD/SVAMP), GSM8K, [AQuA](https://huggingface.co/datasets/aqua_rat), and [MATH](https://huggingface.co/datasets/math_dataset)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e704c2f-9eb1-4a27-b43a-1c239f31eca7_1336x526.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e704c2f-9eb1-4a27-b43a-1c239f31eca7_1336x526.png)

(from [18])

**Decomposed Prompting (DecomP) [18]** tries to address the difficulty of solving multi-step reasoning problems with complex steps via prompting. As tasks become more complex, few-shot prompting (i.e., showing a few examples of a correct solution) will fall short. However, we can do better by decomposing complex tasks into sub-tasks that can be solved independently via prompting. In particular, authors in [18] propose a prompting framework with two components:

1. _Decomposer_: prompts an LLM to decompose a problem into a series of simpler sub-tasks.
    
2. _Sub-task handlers_: uses a separate prompt to solve a simpler sub-task (as dictated by the decomposer) with an LLM.
    

The decomposer and sub-task handlers are just LLMs prompted in a few-shot manner. The DecomP strategy proposed above uses one prompt to identify solvable sub-tasks that are then delegated to another system (e.g., a new prompt, different LLM, or tool) to be solved. Such a modular approach has many benefits:

- Tasks with long context can be decomposed into multiple components.
    
- Each sub-task can be shown a broader set of examples.
    
- Complex sub-tasks can be further decomposed into sub-tasks if needed.
    
- Instead of solving all sub-tasks with an LLM, we can also use other symbolic systems (e.g., a task-specific model, retrieval mechanism, etc.).
    

Let’s consider a simple task as an example. Given a set of words as input, we want to extract the third character of each word, concatenate these characters, and provide their concatenation as output. To do this, we can create a sequence of three sub-tasks: _i)_ collect the list of words, _ii)_ extract the third letter of each word, and _iii)_ concatenate the extracted letters. We can implement each of these sub-tasks with a separate few-shot prompt, as shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb0fdef1-cbc8-4ef8-8cc1-f23ce6d8471c_1600x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb0fdef1-cbc8-4ef8-8cc1-f23ce6d8471c_1600x606.png)

(from [18])

Within Decomp, sub-tasks are iteratively generated by the decomposer, solved, and returned (with relevant output) to the decomposer to generate the next sub-task. The decomposer will continue to generate sub-tasks, acting as a controller for the reasoning process, until the end-of-question `[EOQ]` marker is generated, signifying that the final answer has been produced; see below. Overall, DecomP can be thought of as a more general/flexible version of least-to-most prompting.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ce5cef6-7ba2-40d1-b540-d86dbb2835b3_1608x830.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ce5cef6-7ba2-40d1-b540-d86dbb2835b3_1608x830.png)

(from [18])

**Hypotheses-to-Theories [29].** Reasoning abilities can be elicited within LLMs by prompting the model with example rationales that decompose a complex task into simple steps. However, the model may hallucinate when producing output and performance is poor on tasks that go beyond conventional or common knowledge. Put simply, _problems occur when there is a mismatch between the LLM’s knowledge base and the knowledge required to solve a task_. To solve this problem, we need a prompting approach that empowers the LLM to discover and apply necessary knowledge when solving complex reasoning problems.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d994fc6-c57e-4028-b9ac-e17d379ce363_1602x1126.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d994fc6-c57e-4028-b9ac-e17d379ce363_1602x1126.png)

(from [29])

Inspired by the scientific discovery process of humans, authors in [29] propose a prompting technique, called Hypotheses-to-Theories (HtT) prompting, that follows a strategy of freely proposing (potentially incorrect) hypotheses, only keeping those that can be verified empirically, and using these verified hypothesis to solve a problem. At a high level, the goal of this strategy is to learn a rule library for the LLM that can be used for problem solving. More concretely, HtT prompting (depicted above) is comprised of two steps:

1. _Induction_: The LLM is asked to generate and verify rules over a set of training examples. Rules that appear often and frequently produce a correct answer are collected to form a rule library.
    
2. _Deduction_: the LLM is prompted to use the rule set generated via induction to perform reasoning and answer a question.
    

By using a rule set during reasoning, HtT prompting reduces the probability of hallucinations. Such a finding is verified across both numerical and relational reasoning tasks, where HtT prompting is shown to provide a 11-27% absolute improvement in accuracy compared to prior prompting techniques (e.g., CoT prompting). Interestingly, the rules generated by HtT prompting are also interpretable and even transferable to different (but similar) problems.

#### Tool Usage

Although LLMs are powerful, they have notable limitations! For example, LLMs make arithmetic mistakes, lack access to current information, and even struggle to comprehend the progression of time. Many advancements in the human race have been catalyzed by access to new and innovative tools (e.g., the [printing press](https://www.history.com/news/printing-press-renaissance) or [computer](https://www.youtube.com/watch?v=L40B08nWoMk)), and the same may be true of LLMs. Namely, we can solve many limitations of these models by giving them access to a set of external, specialized tools (e.g., a calculator or search engine) and teaching the model when, where, and how to properly invoke these tools to more reliably solve problems. For more info, check out the prior overviews on this topic below:

- Teaching Language Models to Use Tools [[link](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools)]
    
- Language Models and Friends [[link](https://cameronrwolfe.substack.com/p/language-models-and-friends-gorilla)]
    
- Can language models make their own tools? [[link](https://cameronrwolfe.substack.com/p/can-language-models-make-their-own)]
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9396abd-34ec-4a58-bdca-4e2548476f03_2144x700.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9396abd-34ec-4a58-bdca-4e2548476f03_2144x700.png)

(from [32])

**Toolformer [32]** was one of the first works to explore the integration of LLMs with external tools. These tools are made available to the model via a simple, fixed set of text-to-text APIs; see above. To use the tools, the LLM must learn to _i)_ identify scenarios that require a tool, _ii)_ specify which tool to use, _iii)_ provide relevant textual input to the tool’s API, and _iv)_ use text returned from the API to craft a response. The LLM is taught these skills by constructing a synthetic training dataset that starts with an initial seed dataset and uses a more powerful LLM (e.g., GPT-4) to add examples of valid API calls into the data; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe24f1112-591c-4cc5-845e-2b01391194b1_752x1076.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe24f1112-591c-4cc5-845e-2b01391194b1_752x1076.png)

(from [32])

From here, we can simply finetune an LLM over this data. The model will learn to generate and process calls to necessary APIs directly within the textual sequence that it generates. In this case, handling API calls in an inline manner is simple because we only consider APIs with textual input and output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb430281-e4d5-4b7e-b6ea-5b76ecba8992_758x1054.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb430281-e4d5-4b7e-b6ea-5b76ecba8992_758x1054.png)

(from [32])

> _“LLMs face inherent limitations, such as an inability to access up-to-date information or perform precise mathematical reasoning … enhancing current LLMs with the capability to automatically compose external tools for real-world task solving is critical to address these drawbacks.”_ - from [19]

**Chameleon [19]** aims to mitigate the limitations of LLMs referenced above. Interestingly, some of these limitations are not addressed by existing work on integrating LLMs with external tools, as the set of tools used is typically fixed (or domain-specific) and cannot always generalize to new domains. To create a more generic framework, Chameleon uses a “plug-and-play” strategy that uses a central LLM-based controller to generate a program—_written in natural language_—that composes several tools to solve a complex reasoning task; see below. Unlike prior work, the tools available to Chameleon are quite comprehensive; e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and more.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86507a92-7e5e-49c7-ba25-2637c6223503_1342x688.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86507a92-7e5e-49c7-ba25-2637c6223503_1342x688.png)

(from [19])

The Chameleon framework has two primary components:

1. _Planner_: decomposes the input query into sub-tasks that can be solved via available tools.
    
2. _Module inventory_: a set of task-specific tools (along with descriptions and usage examples) that are available for Chameleon to use.
    

The planner, which is implemented with an LLM, uses natural language to generate calls to external tools (e.g., `image_captioner` or `query_generator`). We can identify these tools via simple string matching, and the sequence of tools outputted by the planner forms a natural language program that can be executed by calling each of the corresponding task-specific modules. Examples of prompts used for the planner and a task-specific module are shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cbe7583-47ed-45fa-9ca6-3f53ade26520_1656x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cbe7583-47ed-45fa-9ca6-3f53ade26520_1656x1008.png)

(from [19])

To teach the controller when to use certain tools, we include tool descriptions and usage examples within a few-shot prompt, which can easily be extended to new tools and modules. Because we leverage the planner’s in-context learning abilities to generate a solution, no training or curated rules are required to solve real-world queries. Rather, we simply provide examples of available tools to the LLM, which can then use this information to infer a sequence of tools that can be executed to yield the correct final response to a query. Going further, this tool sequence is human readable and can be easily debugged by a human user.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08781afc-0d91-4d2b-9891-a810623c2179_1618x644.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08781afc-0d91-4d2b-9891-a810623c2179_1618x644.png)

(from [19])

In experiments, Chameleon is applied to two complex, multi-modal (i.e., meaning that both text and images are involved) reasoning tasks with GPT-4: [ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA) and [TabMWP](https://github.com/lupantech/PromptPG). Chameleon achieves new state-of-the-art performance of 86.54% on ScienceQA, outperforming CoT prompting with GPT-4 and GPT-3 by 2.55% and 11.37%, respectively. On TabMWP, we see a similar improvement with Chameleon, which achieves an accuracy of 98.78%. However, it should be noted that Chameleon’s effectiveness is powered by GPT-4’s ability to infer constraints and construct rational/consistent plans for solving complex reasoning problems.

> _“We propose a simple yet effective method, called GPT4Tools, designed to empower open-source LLMs with the ability to use tools via self-instruct from advanced LLMs” - from [20]_

**GPT4Tools [20].** Although a variety of papers have demonstrated the ability of LLMs to leverage tools in a few-shot manner, most of these papers are dependent upon proprietary language models and purely leverage prompt engineering to facilitate tool usage, leading us to wonder if similar results could be replicated with open LLMs. In [20], authors propose an approach that uses self-instruct [21] to generate a finetuning dataset that can be used to enable open-source LLMs (e.g., [LLaMA](https://cameronrwolfe.substack.com/i/135439692/llama-a-leap-in-open-source-quality) and [OPT](https://cameronrwolfe.substack.com/i/135273362/open-pre-trained-transformers-opt-language-models)) to use a set of multimodal tools.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80abfb29-4238-4251-bda4-e06e5bededaf_1342x1160.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80abfb29-4238-4251-bda4-e06e5bededaf_1342x1160.png)

(from [21])

First, authors use a self-instruct approach to generate a tool usage dataset by prompting a powerful teacher model (i.e., ChatGPT) to create examples of relevant tools being used. Within the prompts, both visual content—_captions and bounding boxes extracted from an image_—and tool descriptions are included. The teacher leverages this information to generate tool-related instructions that can be used to process multimodal information and solve problems; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde2cc6c7-ac8f-4485-98f3-ff226bdb4cbf_1880x506.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde2cc6c7-ac8f-4485-98f3-ff226bdb4cbf_1880x506.png)

(from [21])

Once the dataset is generated, [Low-Rank Adaptation (LoRA)](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft) can be used to easily finetune an open-source LLM to solve a range of visual problems with the help of multimodal tools. In [20], this approach is shown to improve accuracy of calls that are made by the LLM to known tools (i.e., those included in the finetuning dataset), as well as improve the model’s ability to generalize to new tools in a zero-shot manner. A direct comparison of GPT4Tools to prior work on integrating LLMs with external tools is provided within the table above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F551d868f-9e9b-40d0-8d4f-fdc85fdfbe8b_2138x1120.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F551d868f-9e9b-40d0-8d4f-fdc85fdfbe8b_2138x1120.png)

(from [30])

**Gorilla [30].** Although many works have studied integrating LLMs with a fixed set of tools, authors in [30] tackle the broader goal of teaching an LLM to use any model API that is available online. To do this, a retrieval technique is adopted that _i)_ searches for model APIs that are relevant to solving a problem and _ii)_ adds the documentation for these APIs into the model’s context. Such an approach allows the LLM to access a massive number of changing tools, but hallucination (e.g., incorrect arguments or calls to a non-existent API) may still occur; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3c7983b-29ab-4301-84f2-50cd9d6aa28c_1608x536.png)

(from [30])

To solve this issue, authors in [30] construct a dataset with examples of using over 1,600 different model APIs using self-instruct [21]. Within each example, both the prompt and relevant documentation are used as context to generate an output. In other words, this is a retrieval-aware finetuning process (similar to [RAFT](https://cameronrwolfe.substack.com/i/142727381/raft-adapting-language-model-to-domain-specific-rag)); see above. The resulting model, called Gorilla (a finetuned version of [LLaMA-7B](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)), is an interface for leveraging a variety of different deep learning model APIs to solve problems. The resulting LLM can use a massive number of APIs and _can even adapt to changes in the documentation for any of these APIs!_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F469e1fa3-1ff3-48f3-a459-00210aa6fba6_1186x1414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F469e1fa3-1ff3-48f3-a459-00210aa6fba6_1186x1414.png)

(from [31])

**HuggingGPT [31]** is pretty similar to Gorilla in that it explores the integration of LLMs with specialized deep learning models (e.g., for image recognition, video detection, text classification, and much more) via a tool usage approach. The LLM serves as the “brain” of a problem solving system, which plans how to solve a problem and coordinates efforts between different deep learning models that solve necessary subtasks for this problem. Unlike Gorilla, however, HuggingGPT does not perform any finetuning. Problem solving is decomposed into four steps:

1. _Task planning:_ use the LLM to decompose a user’s request into solvable tasks.
    
2. _Model selection_: select models from HuggingFace to use for solving tasks.
    
3. _Task execution_: run each selected model and return results to the LLM.
    
4. _Response generation_: use the LLM to generate a final response for the user.
    

For each of these steps, we leverage prompting with curated instructions and exemplars to yield the desired behavior; see below for example prompts. Given a sufficiently powerful foundation model, such an approach is quite effective.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f6466c8-645a-4bcb-a2c0-374ce1d7f900_1182x1554.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f6466c8-645a-4bcb-a2c0-374ce1d7f900_1182x1554.png)

(from [31])

#### Program-Aided Language Models

> _“The computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding.”_ - from [41]

Integrating LLMs with external tools is an interesting avenue of research, and one of the most useful tools to which these models can be given access is the ability to write and execute programs. Most prompting techniques solve complex problems in two steps:

1. Generate a problem-solving rationale.
    
2. Use this rationale to actually solve the problem.
    

In CoT prompting, we rely upon the LLM to solve both of these steps, _but these models only excel at solving the first step_! In fact, generating an incorrect answer despite outputting a correct rationale is a common failure case for LLMs. To solve this problem, we can teach the model to output a rationale in the form of interleaved language and code (e.g., a Python program with useful comments). Then, we can generate a final answer by simply executing the provided code!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccd5a835-8604-44b8-a0da-27921ea8af2c_1646x1428.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccd5a835-8604-44b8-a0da-27921ea8af2c_1646x1428.png)

(from [40])

**Program-Aided Language Model (PAL) [40]** is similar to CoT prompting in that the LLM is tasked with breaking a problem down into a series of intermediate steps to find a solution. However, this rationale contains both natural language and programatic components. We can execute the code from the rationale (using a sandboxed Python environment) to generate a reliable final solution—_the process of actually generating the solution is delegated to a code interpreter_. In [40], we see that an LLM that has been sufficiently trained on code (e.g., [Codex](https://arxiv.org/abs/2107.03374)) can be taught to solve problems in this way using a few-shot learning approach.

> _“This bridges an important gap in chain-of-thought-like methods, where reasoning chains can be correct but produce an incorrect answer.”_ - from [40]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f486545-4d69-49e6-abb2-7f90049d7c77_2180x1530.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f486545-4d69-49e6-abb2-7f90049d7c77_2180x1530.png)

(from [41])

**Program of Thoughts (PoT) prompting [41]** is quite similar to PAL in that it _i)_ uses a code-augmented prompting technique and _ii)_ delegates the process of deriving a solution to a code interpreter. This process relies upon a few-shot prompting strategy; see above. Unlike PaL, however, code written by PoT relies upon a symbolic math library called [SymPy](https://github.com/sympy/sympy). This package allows the user to define mathematical “symbols” that can be combined to form complex expressions that are evaluated via SymPy’s `solve` function; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39fd5b16-44ad-4625-b434-e18951b7847f_2340x698.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39fd5b16-44ad-4625-b434-e18951b7847f_2340x698.png)

(from [41])

At a high level, PoT directly addresses the inability of LLMs to solve complex equations by providing access to a symbolic math library that enables such equations to be easily composed/evaluated, whereas PAL is focused upon more generally solving problems via a combination of natural language and code. Check out [this related overview](https://cameronrwolfe.substack.com/p/program-aided-language-models) for more information on program-aided models.

#### Understanding and Using the Context Window

Given the recent popularity of RAG and emphasis upon [long context windows](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c) within state-of-the-art LLMs, understanding how these models process the context provided in their prompts is important. Luckily, recent research has studied the topics of context windows and in-context learning in depth, resulting in several interesting takeaways that are relevant to prompt engineering.

**Large Language Models Can Be Easily Distracted by Irrelevant Context [22].** When prompting a language model, we usually include only relevant context and information within the prompt. However, in real-world applications, the model’s prompt usually contains contextually similar information that may or may not be relevant to the particular problem being solved. With this in mind, we might wonder: _Does adding irrelevant context to the prompt have negative side effects?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813f7d4c-4ae4-4a1e-a073-f582cd3fbed0_1128x750.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813f7d4c-4ae4-4a1e-a073-f582cd3fbed0_1128x750.png)

(from [22])

In [22], authors study the distractibility of modern LLMs, finding that the performance of these models can drastically deteriorate when irrelevant context is included in the prompt. To measure LLM distractibility, authors introduce a new Grade-School Math with Irrelevant Context (GSM-IC) dataset, which contains arithmetic reasoning problems with irrelevant information in the problem description; see above. Then, we can measure whether an LLM is distracted by irrelevant context by simply testing if the addition of an irrelevant sentence to the model’s prompt changes the resulting solution to a problem.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd435d7aa-339a-453b-89cc-8b9807e116c4_1646x1300.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd435d7aa-339a-453b-89cc-8b9807e116c4_1646x1300.png)

(from [22])

This strategy is used to test both Codex and GPT-3.5 with several different prompting techniques (see above for a depiction):

- CoT prompting (and zero-shot CoT prompting)
    
- Least-to-most prompting
    
- Prompting with programs
    

Interestingly, the performance of these models drastically deteriorates when irrelevant information is included in the context. However, the impact of irrelevant context can be mitigated by _i)_ using self-consistency, _ii)_ adding an instruction for the model to ignore irrelevant information, and _iii)_ including few-shot examples that demonstrate solving problems with irrelevant information. LLMs are capable of learning to ignore information via instructions or context.

> _“We prepend an instruction sentence ‘feel free to ignore irrelevant information in the problem description’ to the exemplars.”_ - from [22]

**Lost in the Middle [23].** Generative LLMs have a text-to-text format, meaning that they take a sequence of text as input (i.e., the prompt) and produce a corresponding sequence of text as output. The input passed to the LLM has a variable length—_it can be a short (zero-shot) problem description, or it can be a complex instruction containing large amounts of external context (e.g., for RAG)._ For this reason, LLMs must be capable of operating over long contexts and using the entirety of this context to effectively solve downstream tasks.

Along these lines, authors in [23] study the ability of several LLMs—_both open ([MPT](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact)) and closed models (GPT-3.5-Turbo and Claude-1.3)_—to concretely leverage information provided to them within long contexts. In particular, two types of tasks are studied in [23]:

- _Multi-document QA_: similar to the standard RAG setup, this problem requires the model to reason over several documents to answer a question.
    
- _Key-value retrieval_: this is a synthetic task that tests the model’s ability to retrieve matching tokens by returning the value associated with a key from a collection of JSON key-value pairs provided as context.
    

When solving these tasks, authors control both _i_) the length of the input context (by using more documents or key-value pairs) and _ii)_ the position—_beginning, middle, or end_—of the relevant context within the input. Then, we can study the impact of changes in context length and position on model performance. In experiments, we see a clear “U-shaped” performance curve (shown below) based on the position of relevant information within the model’s context.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb4a2567-cffd-41b0-ab5c-421bc0ceb498_900x1312.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb4a2567-cffd-41b0-ab5c-421bc0ceb498_900x1312.png)

(from [23])

This visualization shows us that LLMs pay the most attention to information at the beginning and end of their context. When relevant information is in the middle of the context, model performance degrades significantly—_the information is “lost in the middle”_. In fact, GPT-3.5-Turbo performs better without any relevant context on multi-document QA tasks than when relevant documents are placed in the middle of the context. Performance varies a lot as we tweak the position of relevant information, and models with extended contexts do not show signs of improved robustness to these positional biases; see below. However, these issues have been improved in more recent models (e.g., [Gemini-1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) and [Claude-3](https://www.anthropic.com/news/claude-3-family)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd560977c-ed29-4593-877e-71b819a39e15_906x1120.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd560977c-ed29-4593-877e-71b819a39e15_906x1120.png)

(from [23])

**Large Language Models Are Latent Variable Models [24].** Although we know that LLMs have in-context learning abilities, it is unclear how these abilities emerge from standard language model pretraining. Additionally, in-context learning is generally sensitive to the choice and format of examples used for few-shot learning. Certain demonstrations are effective examples for the model, while others are not. Currently, there are no standard criteria for choosing the best examples for few-shot learning. In [24], authors study this topic, aiming to find a practical strategy for identifying the best possible few-shot exemplars.

> _“In-context learning has been demonstrated to be an effective technique for a wide range of NLP tasks. However, it is sensitive to the choice, format, and even the order of the demonstrations used.”_ - from [24]

Many papers have studied the mechanics of in-context learning from a theoretical perspective, but few of them provide practical or actionable insights. In [24], authors view LLMs in the lens of simple [topic](https://en.wikipedia.org/wiki/Topic_model) / [latent variable](https://en.wikipedia.org/wiki/Latent_variable_model) models that relate the generation of new tokens to prior tokens observed by the language model. The details can be found in the paper, but at a high level this formulation allows us to theoretically describe the language model’s output with respect to the format and task information used within the model’s input prompt.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5846d77-5cac-488c-9dfd-4cabf50bd509_1888x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5846d77-5cac-488c-9dfd-4cabf50bd509_1888x732.png)

(from [24])

From this formulation, authors develop a practical technique for selecting the best possible few-shot exemplars that uses a smaller language model to measure the [posterior probability](https://en.wikipedia.org/wiki/Posterior_probability) of the model’s input—_this tells us the likelihood of different input exemplars based upon the model’s input and parameters_. We can use exemplars selected with the smaller LLM for in-context learning with a larger model (see above), which is found to yield a practical benefit. Put simply, this paper proposes an interesting (and relatively simple) theoretical view of in-context learning that can be used in practice to select better few-shot exemplars.

#### Improving Writing Capabilities

> _“SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.”_ - from [25]

**Skeleton-of-Thought (SoT) [25]** is a prompting technique that aims to reduces the latency of generating output with an LLM. As we know, generating output with an LLM can be expensive for a few reasons:

- The models are large, so compute/memory/IO costs are high.
    
- The attention operation is IO bound and has memory/compute complexity that increases quadratically with sequence length.
    
- The output is generated sequentially one token at at time (i.e., [using next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction)).
    

In [25], authors try to address the last problem mentioned above—_the latency of sequential decoding_. Put simply, sequential decoding is a problem because we generate one token at a time and, therefore, cannot parallelize the generation of tokens in the output sequence. For this reason, the cost of generating an output is directly related to the length of the output. It takes much longer to generate an output sequence with many tokens. But, _can we avoid fully sequential decoding?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1467d923-adf1-41f6-a6be-c2136ffaa5c0_1606x1020.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1467d923-adf1-41f6-a6be-c2136ffaa5c0_1606x1020.png)

(from [25])

We see in [25] that a more efficient decoding strategy can be devised by mimicking the thinking and writing process of humans without requiring any changes to the model, system, or hardware. In particular, humans tend to plan an outline for what they want to write, then fill in details for each element of their outline. _This is not a purely sequential process_[6](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-6-143156742)! Inspired by this idea, authors in [25] propose Skeleton-of-Thought (SoT) prompting (see above), which has two steps:

1. Prompt the LLM to generate a skeleton/outline of its answer.
    
2. Conduct parallel API calls to fill in the contents of each outline element.
    

Although this might seem a bit vague, we can see how this works by checking out the SoT prompt shown below. The process is pretty simple—_we just generate the skeleton and use a generic prompt template to fill in all of the remaining details._

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2e445f-f9b5-48f0-af95-e252d3d554c8_1626x986.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2e445f-f9b5-48f0-af95-e252d3d554c8_1626x986.png)

(from [25])

By generating each element of the skeleton in parallel, we can save a lot on inference latency. For example, the question shown at the beginning of this summary can be answered in 12 seconds (instead of 22 seconds) without making any changes to the underlying model or system—_we just use SoT prompting_. Similar speedups are observed across 12 different LLMs in [25]. Interestingly, authors also note that creating an outline can oftentimes improve writing quality[7](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-7-143156742).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9475efbf-e7b4-44f5-9e39-f46445e8619d_1882x1324.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9475efbf-e7b4-44f5-9e39-f46445e8619d_1882x1324.png)

(from [27])

**Directional Stimulus Prompting [27].** Given the computational expense of finetuning, prompting is usually the easiest way to solve a task with an LLM. However, prompting has limitations—_guiding the LLM towards generating output with our desired content or style can be difficult_. To solve this issue, authors in [27] propose directional stimulus prompting (DSP), which introduces a “directional stimulus” into the LLM’s prompt as shown in the figure above.

This stimulus is just a textual hint or clue that gives the LLM more information about the expected output. The directional stimulus, which is instance-specific and solely based upon the input query, is generated using a smaller model (e.g., [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)) that is much easier to train or finetune relative to the LLM. By doing this, we can circumvent the difficulty of directly training the LLM, choosing instead to finetune the model used to generate the directional stimulus[8](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-8-143156742). DSP is evaluated on summarization, dialogue, and reasoning tasks, where it is found to improve model performance while requiring minimal labeled data.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9afab7f-6cfd-4a75-9a91-7fead1d7beae_1528x984.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9afab7f-6cfd-4a75-9a91-7fead1d7beae_1528x984.png)

(from [28])

**Chain of Density Prompting [28].** Recent developments in LLMs have revolutionized the problem of automatic summarization, as we can simply prompt an LLM to generate a high-quality summary instead of performing finetuning over labeled data. When automatically generating a summary, one important aspect of the resulting summary’s quality is information density. We want the summary to present all relevant information in a concise manner, but we want to avoid writing summaries that are overly dense or illegible.

To study this tradeoff in information density, authors in [28] propose chain of density (CoD) prompting, which starts by generating a summary via a vanilla prompt to GPT-4. From here, CoD prompting is used to iteratively add additional entities into the summary while keeping the length of the summary fixed, thus increasing the summary’s information density. Interestingly, we see in [28] that humans prefer summaries that are almost as dense as human-written summaries, but more dense than those generated by a vanilla prompt to GPT-4. By using CoD prompting, we can explore this tradeoff and generate higher-quality summaries.

> _“Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt.”_ - from [28]

#### Other Notable Papers

- **Active Prompting [26]** tackles the difficulty of selecting (and annotating) exemplars for CoT prompting by providing a technique—_based upon work in uncertainty-based active learning_—for identifying the most helpful exemplars to select (and annotate) for solving a particular reasoning problem.
    
- **TaskMatrix [33]** is a position paper—_meaning that it presents a position or outlook on a notable issue_—that considers the integration of [foundation models](https://crfm.stanford.edu/) with millions of different APIs.
    
- **Set of Marks Prompting [42]** is a visual prompting method that employs pretrained segmentation models to partition an image into regions and overlay these regions with a set of marks (i.e., alphanumerics, masks, boxes, etc.) to improve the visual grounding of models like [GPT-4V](https://openai.com/research/gpt-4v-system-card).
    
- **Multimodal CoT Prompting [43]** extends CoT prompting to inputs that include both images and text by treating rationale and answer generation as two distinct steps in the problem solving process.
    
- The topic of [automatic prompting](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design) (i.e., generating better prompts via an optimization process) is not explored in this post.
    
- _Anything else?_ Please let me know in the comments!
    

## Conclusion

Within this overview, we have learned about everything from the basics of prompt engineering to cutting-edge techniques that have been proposed in the last two months! This post contains a massive amount of information, but many of the techniques we have seen are slight variations that leverage the same core prompt components: _instructions, exemplars, context, and problem-solving rationales._ Plus, we should recall the prompt engineering strategy proposed at the outset:

1. Start by creating a comprehensive evaluation strategy that allows you to easily and quantitatively measure the quality of a prompt.
    
2. The first prompt you write should be simple (e.g., an instruction prompt).
    
3. As you make the prompt more complex, make sure that the added complexity results in a corresponding performance improvement.
    
4. Continue iterating on the prompt until the desired performance is reached.
    

Many problems can be solved via simple instruction and few-shot prompts. For complex reasoning problems, it may be necessary to use a more advanced strategy like CoT prompting with self-consistency. Additionally, we have seen a variety of prompting strategies that are useful for specific problem domains (e.g., PoT prompting for math problems or CoD prompting for summarization). Although being aware of these techniques is useful, their use cases are relatively rare, and we should only use them if we see a clear and measurable performance impact.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), and this is the Deep (Learning) Focus newsletter, where I help readers understand AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Saravia, Elvis, et al. “Prompt Engineering Guide”, [https://github.com/dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) (2022).

[2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."

[3] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[4] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?."

[5] Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." _International conference on machine learning_. PMLR, 2021.

[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[7] Ye, Seonghyeon, et al. "Investigating the effectiveness of task-agnostic prefix prompt for instruction following." _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 38. No. 17. 2024.

[8] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." _arXiv preprint arXiv:2201.08239_ (2022).

[9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." _arXiv preprint arXiv:2112.11446_ (2021).

[10] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." _Advances in neural information processing systems_ 35 (2022): 24824-24837.

[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." _arXiv preprint arXiv:2205.11916_ (2022).

[12] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." _arXiv preprint arXiv:2203.11171_ (2022).

[13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." _arXiv preprint arXiv:2205.10625_ (2022).

[14] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." _arXiv preprint arXiv:2305.10601_ (2023).

[15] Zhang, Zhuosheng, et al. "Automatic chain of thought prompting in large language models." _arXiv preprint arXiv:2210.03493_ (2022).

[16] Fu, Yao, et al. "Complexity-based prompting for multi-step reasoning." _The Eleventh International Conference on Learning Representations_. 2022.

[17] Zheng, Chuanyang, et al. "Progressive-hint prompting improves reasoning in large language models." _arXiv preprint arXiv:2304.09797_ (2023).

[18] Khot, Tushar, et al. "Decomposed prompting: A modular approach for solving complex tasks." _arXiv preprint arXiv:2210.02406_ (2022).

[19] Lu, Pan, et al. "Chameleon: Plug-and-play compositional reasoning with large language models." _Advances in Neural Information Processing Systems_ 36 (2024).

[20] Yang, Rui, et al. "Gpt4tools: Teaching large language model to use tools via self-instruction." _Advances in Neural Information Processing Systems_ 36 (2024).

[21] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[22] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." _International Conference on Machine Learning_. PMLR, 2023.

[23] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." _Transactions of the Association for Computational Linguistics_ 12 (2024): 157-173.

[24] Wang, Xinyi, et al. "Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning." _Advances in Neural Information Processing Systems_ 36 (2024).

[25] Ning, Xuefei, et al. "Skeleton-of-thought: Large language models can do parallel decoding." _arXiv preprint arXiv:2307.15337_ (2023).

[26] Diao, Shizhe, et al. "Active prompting with chain-of-thought for large language models." _arXiv preprint arXiv:2302.12246_ (2023).

[27] Li, Zekun, et al. "Guiding large language models via directional stimulus prompting." _Advances in Neural Information Processing Systems_ 36 (2024).

[28] Adams, Griffin, et al. "From sparse to dense: GPT-4 summarization with chain of density prompting." _arXiv preprint arXiv:2309.04269_ (2023).

[29] Zhu, Zhaocheng, et al. "Large language models can learn rules." _arXiv preprint arXiv:2310.07064_ (2023).

[30] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." _arXiv preprint arXiv:2305.15334_ (2023).

[31] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." _arXiv preprint arXiv:2303.17580_ (2023).

[32] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." _arXiv preprint arXiv:2302.04761_ (2023).

[33] Liang, Yaobo, et al. "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis." _arXiv preprint arXiv:2303.16434_ (2023).

[34] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." _arXiv preprint arXiv:2306.15595_ (2023).

[35] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." _arXiv preprint arXiv:2308.09687_ (2023).

[36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." _arXiv preprint arXiv:2305.16582_ (2023).

[37] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." _Advances in Neural Information Processing Systems_ 33 (2020): 9459-9474.

[38] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." _arXiv preprint arXiv:2312.05934_ (2023).

[39] Liu, Jiacheng, et al. "Generated knowledge prompting for commonsense reasoning." _arXiv preprint arXiv:2110.08387_ (2021).

[40] Gao, Luyu, et al. "PAL: Program-aided Language Models." _arXiv preprint arXiv:2211.10435_ (2022).

[41] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." _arXiv preprint arXiv:2211.12588_ (2022).

[42] Yang, Jianwei, et al. "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v." _arXiv preprint arXiv:2310.11441_ (2023).

[43] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." _arXiv preprint arXiv:2302.00923_ (2023).

[1](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-1-143156742)

Remember, longer prompts are always more expensive! When using an API, you pay for these tokens directly. For open-source models, you pay for the extra tokens in extra compute and latency costs.

[2](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-2-143156742)

For a great overview of these techniques (as well as more prompting techniques and a variety of other awesome topics), check out [Lillian Weng’s blog](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)!

[3](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-3-143156742)

To eliminate this issue, we can randomly order or even permute the exemplars within the prompt and generate output for multiple permutations of exemplars.

[4](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-4-143156742)

In fact, poor reasoning capabilities are one of the [go-to criticisms](https://bdtechtalks.com/2022/06/27/large-language-models-logical-reasoning/) of modern LLMs. Many researchers cite the inability of LLMs to reason as evidence of a shallow understanding of important concepts.

[5](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-5-143156742)

Many researchers have argued that this majority vote strategy is insufficient for solving complex problems. This has led to a lot of research on prompt ensembles and other self-consistency variants that was in-depth [here](https://cameronrwolfe.substack.com/p/prompt-ensembles-make-llms-more-reliable).

[6](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-6-143156742)

In fact, writing this newsletter is far from sequential. My writing strategy is usually to _i)_ overview a bunch of similar papers, _ii)_ add some sections with shared ideas and background, then _iii)_ write the intro and conclusion. This is the exact opposite of a sequential process (the intro is the last thing that I write!).

[7](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-7-143156742)

When we ask the LLM to generate a rationale our outline for its answer, we oftentimes see a benefit from this process, similarly to benefits observed with CoT prompting. However, it is worth noting that SoT prompting performs multiple disjoint generations (i.e., one to generate the skeleton, then one for each skeleton component), while a CoT prompt is typically used to generate output in a single pass.

[8](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering#footnote-anchor-8-143156742)

This model can be finetuned using either [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) or an RL-based strategy similar to [RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Ahmed Ayman's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03f3e781-bc0d-4361-99ac-4c0f25b68d73_1940x1293.jpeg)



](https://substack.com/profile/195329500-ahmed-ayman)

[

![t@j-mahal's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16c00643-aebf-427a-a080-897f62932197_586x586.jpeg)



](https://substack.com/profile/876115-tj-mahal)

[

![Anushree Chatterjee's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dfd92a2-a54a-4981-8a1d-2c965eaf6af8_1122x1123.jpeg)



](https://substack.com/profile/175605986-anushree-chatterjee)

[

![Imagine's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4b4d45-de1a-4731-a48b-c910f5382a2f_144x144.png)



](https://substack.com/profile/22329267-imagine)

[

![Linh Kid's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ff052df-201b-45c0-927c-6547097a825f_3840x2880.jpeg)



](https://substack.com/profile/46245532-linh-kid)

111 Likes∙

[17 Restacks](https://substack.com/note/p-143156742/restacks?utm_source=substack&utm_content=facepile-restacks)

111

- 

[

7

](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comments)

17

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![A.J. Sutter's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3fc2c28-5d9e-49f1-a5cd-4c9b7aa92a17_144x144.png)



](https://substack.com/profile/71642608-aj-sutter?utm_source=comment)

[A.J. Sutter](https://substack.com/profile/71642608-aj-sutter?utm_source=substack-feed-item)

[A.J. Sutter](https://ajsutter.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[5月8日](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comment/55829055 "2024年5月8日 10:39")

Liked by Cameron R. Wolfe, Ph.D.

What is the timeframe for something to be "modern," in this context? ChatGPT wasn't even released 18 months ago! Isn't it ALL "modern"?

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comment/55829055)

[

![Faraz Ansari's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d901c5d-3302-41b0-ae2b-355cc4847a12_144x144.png)



](https://substack.com/profile/165373393-faraz-ansari?utm_source=comment)

[Faraz Ansari](https://substack.com/profile/165373393-faraz-ansari?utm_source=substack-feed-item)

[Faraz’s Substack](https://farazansari.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[5月4日](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comment/55457947 "2024年5月4日 00:42")

Liked by Cameron R. Wolfe, Ph.D.

This is really nice and detailed reference material for LLMs and prompt engineering. I really liked the images and illustrations. Which tool do you use to create these elegant workflow images?

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comment/55457947)

[5 more comments...](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


-----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Summarization and the Evolution of LLMs

### How research on abstractive summarization changed language models forever...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jun 03, 2024

55

- 

[

3

](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comments)

6

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd334bdf6-552c-402a-876c-19f7388cc00d_2132x1180.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd334bdf6-552c-402a-876c-19f7388cc00d_2132x1180.png)

(from [2, 4, 6, 10, 29])

The capabilities of large language models (LLMs) have progressed at a staggering pace in recent years. However, many of the core ideas surrounding LLMs—_self-supervised pretraining, the transformer, learning from human feedback, and more_—have roots in natural language processing research from years before. These concepts are not new, but rather an accumulation of ideas from over a decade of relevant research. As a result, fundamental research on core problems in natural language processing (e.g., machine translation, summarization, question answering and more) is incredibly important! In this overview, we’ll demonstrate this point by focusing upon the problem of (abstractive) text summarization, which has had a heavy influence on the evolution of LLM research over time.

> _“Text summarization aims to compress long document(s) into a short, fluent, and human-readable form that preserves the most salient information from the source document.”_ - from [11]

At a high level, summarization refers to the task of compressing a sequence of text into a much shorter sequence that still captures key information. Although summarization is typically applied to standard problems like summarizing news articles, the problem setup is actually quite generic and can encompass a variety of interesting applications:

- Compressing large, unstructured sequences of text as a pre-processing step for [retrieval augmented generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval).
    
- Writing [understandable textual summaries](https://arxiv.org/abs/2308.10053) for the output of a recommender system.
    
- Generating a recap of a meeting from a transcribed recording.
    

Because the task is so generic and powerful, text summarization is surrounded by a massive body of practically useful research. As we will see, many key ideas from summarization research have been adopted by modern LLMs. These two lines of research are highly coupled, and gaining a deep understanding of summarization research gives us a new and improved view of why and how LLMs work so well!

## Useful Background on Summarization

Prior to diving into recent summarization research, we need to learn the basics. In this section, we’ll address the task of summarization at a high level, going over the different types of summarization that exist, research on summarization before the popularization of LLMs, evaluation metrics for summarization, and more.

#### Types of Summarization

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2588478f-cbe8-42d1-b091-e2f0220a4597_1910x950.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2588478f-cbe8-42d1-b091-e2f0220a4597_1910x950.png)

Abstractive vs. extractive summarization

The goal of summarization is to produce a shorter piece of text that captures the key ideas of a longer source document, _thus compressing the information to only its core components_. In the literature, there are two major classes of summarization techniques that are studied (see above for a depiction):

1. _Extractive_: the summary is constructed by selectively copying entire sentences or spans of text from the source document.
    
2. _Abstractive_: rephrases the information from the source document, forming a shorter explanation of the relevant information.
    

Both of these summarization strategies are widely studied, but summaries generated with an LLM are considered to be abstractive. _Why?_ Well, an LLM is usually not conditioned to directly copy sentences from the source document. _The model can freely generate text based upon provided context_, producing abstractive summaries that arbitrarily rephrase information from the source document.

As we will see, however, summaries generated with an LLM tend to be (relatively) extractive in practice. Plus, recent research has began to explore the related topic of _i)_ using extractive summarization techniques to identify key context to include as input to an abstractive summarization model [7, 12] or even _ii)_ teaching an LLM to cite relevant sources (or spans of text) when generating a response [10]; see below for a depiction. For this reason, significant overlap exists between extractive and abstractive summarization, even if techniques might differ.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9c549a-b263-473d-9c01-bc3f913c1402_2024x832.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9c549a-b263-473d-9c01-bc3f913c1402_2024x832.png)

(from [10])

**Further reading.** Because the focus of this post is summarization with LLMs, we will not go into much depth on abstractive and extractive summarization prior to the popularization of LLMs. However, such research is still highly relevant, as it lays the foundation for work that we will study in this post. A list of curated papers is provided below for those who are interested:

- _[SummEval: Re-evaluating Summarization Evaluation](https://arxiv.org/abs/2007.12626)_ [11]: This work provides a deep dive into automatic evaluation metrics for summarization, explores the limitations of existing protocols, and re-evaluates a suite of 24 abstractive and extractive summarization models using their proposed improvements.
    
- _[On the Abstractiveness of Neural Document Summarization](https://aclanthology.org/D18-1089/)_ [13]: This work studies abstractive summarization techniques, finding that many abstractive summaries are actually highly extractive in nature!
    
- _[On Faithfulness and Factuality in Abstractive Summarization](https://arxiv.org/abs/2005.00661)_ [15]: This work provides an extensive analysis of faithfulness and factuality for abstractive summarization techniques. Similar analysis is provided in [14, 16, 17].
    
- Notable extractive summarization techniques include [NEUSUM](https://arxiv.org/abs/1807.02305), [BanditSum](https://arxiv.org/abs/1809.09672), [LATENT](https://arxiv.org/abs/1808.07187), [REFRESH](https://arxiv.org/abs/1802.08636), [RNES](https://arxiv.org/abs/1804.07036), [JECS](https://arxiv.org/abs/1902.00863), and [STRASS](https://arxiv.org/abs/1907.07323). See [here](https://ieeexplore.ieee.org/document/7944061) for a more comprehensive survey of extractive summarization techniques.
    
- Notable (pre-LLM) abstractive summarization techniques include [Pointer Generator](https://arxiv.org/abs/1704.04368), [Fast-abs-rl](https://arxiv.org/abs/1805.11080), [Bottom-Up](https://arxiv.org/abs/1808.10792), [ROUGESal](https://arxiv.org/abs/1804.06451), [Soft-MT](https://arxiv.org/abs/1805.11004), [SENECA](https://arxiv.org/abs/1909.02059), [T5](https://arxiv.org/abs/1910.10683), [BertSum](https://arxiv.org/abs/1908.08345), [Pegasus](https://arxiv.org/abs/1912.08777), [BART](https://arxiv.org/abs/1910.13461), and [UniLM](https://arxiv.org/abs/1905.03197). See [here](https://arxiv.org/abs/2204.09519) for a more comprehensive survey of abstractive summarization techniques.
    

Additionally, much of the early research on LLMs (e.g., [GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) and [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)) uses abstractive summarization as a key task for evaluating model quality.

#### Writing Summaries with an LLM

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23952336-59d7-41ce-a2b0-6ec5f82f7e92_904x594.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23952336-59d7-41ce-a2b0-6ec5f82f7e92_904x594.png)

(from [6])

As we will see throughout the remainder of this post, LLMs are great tools for generating high-quality, abstractive summaries. If we want to use an LLM for summarization, there are several approaches we can take; see above. At a high level, these approaches either:

1. Use purely in-context learning or
    
2. Finetune a custom model on summarization-specific data.
    

If out-of-the-box models work well for our application, in-context learning will be the simplest approach to take. We can just call an API—_with some added context in the prompt_—to generate a summary. If prompting is not sufficient, however, we can explore finetuning a custom model, either in a supervised fashion or using preference tuning. The basics for each of these techniques are outlined below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e886844-c750-4acf-afdd-73f633a01b22_1920x976.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e886844-c750-4acf-afdd-73f633a01b22_1920x976.png)

(from [18])

**In-context learning** refers to the ability of a single foundation LLM to solve a variety of different downstream tasks accurately by leveraging information provided in the prompt; see above. In the case of summarization, for example, we can place several articles along with their associated summaries within the prompt, allowing the model to use these summaries as context when generating its final output summary. Unlike finetuning, we do not update any of the model’s parameters during in-context learning. Rather, _we rely upon the model’s ability to properly leverage context provided in the prompt to better solve a problem_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01f2db93-54ca-489f-9893-14e6350e5cc8_990x634.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01f2db93-54ca-489f-9893-14e6350e5cc8_990x634.png)

In-context learning is an emergent capability of LLMs[1](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-1-144374854) that was first observed with the proposal of GPT-3 [18]. As shown in the figure above, LLMs become more capable [few-shot learners](https://cameronrwolfe.substack.com/i/143156742/basic-prompting-strategies) at a certain scale, indicating that larger models can more effectively leverage context in their prompt to solve problems. To use in-context learning, we can call a proprietary API (e.g., from OpenAI or Anthropic) with a simple prompt—_such as the one shown above_—to generate a summary.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71e4308c-1b3b-46db-b3cd-1bc66a3c747c_1214x932.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71e4308c-1b3b-46db-b3cd-1bc66a3c747c_1214x932.png)

(from [6])

**Supervised finetuning.** If prompting does not yield summaries of sufficient quality, the next step we can take is to finetune a pretrained LLM over a dataset of high-quality summaries. To do this, we simply collect—_either manually via human annotators or synthetically with a powerful LLM_—a set of well written document-summary pairs that are relevant to the task we are trying to solve.

> _“Large-scale language model pretraining has become increasingly prevalent for achieving high performance … [These models] are fine-tuned using supervised learning to maximize the log probability of a set of human demonstrations.”_ - from [2]

For example, if we want to summarize restaurant reviews, we should collect a dataset of reviews for a set of restaurants along with curated summaries of the reviews for each of these restaurants. Beginning with a pretrained base model, we can then finetune this model using a [standard language modeling objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) over our collected data. Such an approach is referred to as [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised). This combination of large-scale pretraining with supervised finetuning over summarization-specific data is highly effective and was the go-to approach for training summarization models prior to the popularization of foundation LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc5afbb-87d1-4608-a0ee-e9cd37f12b44_788x550.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc5afbb-87d1-4608-a0ee-e9cd37f12b44_788x550.png)

(from [19])

**Instruction tuning.** Beyond supervised training on summarization data, we can perform general-purpose [instruction tuning](https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/), in which we use task templates (see above) to simultaneously finetune a single model on data from several tasks. Notably, summarization can be one of the tasks that is included in the instruction tuning process_,_ and the resulting model can be leveraged to accurately solve a variety of different tasks. In fact, instruction tuned models are even found to generalize well to tasks that are unseen during training; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37570c8b-b824-4503-98e2-87462a597233_1550x418.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37570c8b-b824-4503-98e2-87462a597233_1550x418.png)

(from [19])

**Human feedback (preference tuning).** Finally, we can directly use human feedback to finetune a summarization model. Such an approach, commonly referred to as preference tuning, laid the foundation for later work on LLM alignment via [reinforcerment learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). In fact, InstructGPT [5]—_the predecessor to ChatGPT_—cites research on learning to summarize via human feedback [2] as inspiration for their alignment strategy[2](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-2-144374854)! As such, we will heavily focus on preference tuning strategies in this overview.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972b9563-affc-458c-88ec-244ce6009764_540x990.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972b9563-affc-458c-88ec-244ce6009764_540x990.png)

(from [2])

To train an LLM based on human feedback, we first collect preference data. Each preference data example is a pair of summaries for the same source document, where one summary is identified (usually by a human) as “better” than the other; see above. This preference data can be used to train a reward model.

> _“Given a post and a candidate summary, we train a reward model to predict the log odds that this summary is the better one, as judged by our labelers.”_ - from [2]

The reward model takes a summary—_or a sequence of text in general_—as input and produces a scalar score as output, which represents the human preference score for a given summary. To train the reward model, we use a ranking loss that simply maximizes the log probability that the preferred summary in a pair will receive a better score than the rejected summary; see below. Prior work has shown that such a pairwise ranking loss outperforms learning preference scores directly via regression [1]. After training, the reward model can take a source document and summary as input and produce a scalar score as output, where higher scores indicate that the summary will be preferred by humans and vice versa.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca9ba87-df3a-4316-88c5-0de945a138ff_714x980.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca9ba87-df3a-4316-88c5-0de945a138ff_714x980.png)

(from [2])

We can then use this reward model as a signal for training our summarization model via a [reinforcement learning (RL)](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning) algorithm (e.g., [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)). In this way, we use human feedback—_in the form of preference pairs_—to train a summarization model! For more details on RLHF, check out the deep dive below.

[The Story of RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)

#### Popular Datasets for Summarization

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5360a4bb-78c0-4e78-b2b1-d37ef6cf0930_2096x438.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5360a4bb-78c0-4e78-b2b1-d37ef6cf0930_2096x438.png)

(from [20])

Although summarization is a generic problem, there are a few common datasets that are used almost universally within the literature. Summarization research tends to focus heavily upon news summarization. For example, the [CNN / DailyMail Corpus](https://huggingface.co/datasets/abisee/cnn_dailymail) [20] is one of the most widely-used datasets; see above for a sample from this dataset. In total, the dataset contains over 300K articles from CNN and DailyMail with associated summaries for each of the articles. [XSum](https://huggingface.co/datasets/EdinburghNLP/xsum) [21], another single-document abstractive summarization dataset, is also commonly used within summarization research. Having a similar size to CNN / DailyMail (i.e., ~230K summaries), XSum is comprised of simple, single-sentence summaries that capture the main idea of each article in the dataset.

> _“We chose the TL;DR dataset over the more commonly used CNN/DM dataset primarily because very strong performance can be attained on CNN/DM with simple extractive baselines.”_ - from [2]

Despite being widely used, the CNN / DailyMail and XSum datasets are relatively easy to solve—_sometimes we need a more complex dataset_. Recent work has explored the [TL;DR dataset](https://huggingface.co/datasets/webis/tldr-17) [22], which contains 3 million posts from Reddit along with summaries written by the author of each post; see below. Due to its size, this dataset is usually filtered to improve quality and ensure that only posts from relevant topics are included; see Section 3.2 in [2] for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F378ddf11-daf2-4d9c-90df-4d344f3ab2f3_1084x412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F378ddf11-daf2-4d9c-90df-4d344f3ab2f3_1084x412.png)

(from [22])

**Quality issues.** Most summarization datasets include both source documents and reference summaries for each document. However, _just because we call them “references” does not mean the summaries included in these datasets are high quality_! In fact, many papers [8, 11] observe that reference summaries in datasets like CNN / DailyMail are much worse than human-written summaries. This finding is problematic for a few reasons:

1. We often use these reference summaries for supervised training.
    
2. These references may be used to compute evaluation metrics.
    

A few examples of low-quality references summaries from CNN / DailyMail are shown within the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f770890-715a-4bb9-a1f9-217a5aa94c46_1228x378.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f770890-715a-4bb9-a1f9-217a5aa94c46_1228x378.png)

(from [11])

#### How can we evaluate a summary?

> _“Evaluating abstractive summaries is challenging—it’s not as straightforward as evaluating constrained translations or code generations where we can test for functionality.”_ - [Eugene Yan](https://eugeneyan.com/writing/abstractive/)

Given that abstractive summarization is open-ended, evaluating summary quality can be difficult. There are many viable ways that a source document can be summarized, and determining whether a given summary is “better” than another is subjective! For this reason, properly evaluating abstractive summaries begins with us—_the humans and researchers_—formulating a set of criteria that describe a “good” summary. For example, the following criteria are proposed in [23]:

- _Fluency_: the sentences in the summary are easy to read and free of errors.
    
- _Coherence_: the summary as a whole is easy to read, cohesive, and structured in a reasonable manner.
    
- _Relevance_: the summary includes the most “important” information from the source document.
    
- _Consistency_: information in the summary is correct and in alignment with the source document (i.e., no hallucinations or incorrect information).
    

However, these are not the only set of criteria we can define! Arguably, fluency is largely solved by modern LLMs[3](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-3-144374854), and the criteria that we care about may change depending upon the use case we are trying to solve. For example, authors in [7] devise an alternative set of criteria—_including faithfulness, factuality, and genericity_—that is more suited to summarizing opinions or reviews (e.g., on Yelp) from users.

Defining the desired properties of a good summary is the first step in the evaluation process. Once this is clear, we can begin to think about crafting better abstractive summarization models by measuring performance and iterating.

**Human evaluation.** There are many automatic strategies for assessing abstractive summary quality, but human evaluation is the most reliable evaluation approach, serving as the “ground truth” for quality evaluation across summarization research. To truly know the quality of a model, _we must subject it to human evaluation_. Despite this, human evaluation is not a silver bullet! Getting accurate, reliable, and consistent quality labels from humans is extremely difficult[4](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-4-144374854), especially on subjective tasks like abstractive summarization. Humans constantly disagree with each other (on more things than just summary quality!), which can make the evaluation process quite noisy. To mitigate these issues, we can use metrics like [Fleiss’ kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa) and [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha) to monitor the level of agreement between human annotators—_or between annotators and researchers_.

**Traditional (automatic) metrics.** Although human evaluation serves as ground truth for summary quality, we cannot rely purely upon human evaluation because collecting human quality assessments is expensive and time consuming. We need automatic metrics that allow us to more quickly iterate upon our model between human evaluation trials. First, we’ll take a look at more traditional automatic evaluation metrics for summarization tasks, which fall into two categories:

1. Reference-based
    
2. Reference-free (or context-based)
    

Reference-based metrics assume that we have a target or reference summary—_usually written by a human_—available that can be used to measure summary quality, while reference-free metrics assess summary quality purely based upon the generated summary and source document.

The most commonly-used evaluation metric for summarization is the [Recall-Oriented Understudy for Gisting Evaluation (ROUGE)](https://huggingface.co/spaces/evaluate-metric/rouge) score, which works by simply counting the number of words—_or the number of [n-grams](https://kavita-ganesan.com/what-are-n-grams/#.ZUzYuezMKw0) for ROUGE-N_—in the reference summary that also occur in the model’s generated output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc563515-7e46-430c-aff2-91ea88f73c9d_2170x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc563515-7e46-430c-aff2-91ea88f73c9d_2170x860.png)

ROUGE is a reference-based metric that measures overlap between reference and output summaries. Beyond ROUGE, there are many other reference-based metrics that use a similar strategy to compute summary quality:

- _[Bilingual Evaluation Understudy (BLEU) score](https://huggingface.co/spaces/evaluate-metric/bleu) [25]_: commonly used to evaluate translation tasks by counting the number of matching n-grams between the generated output and the reference, then dividing this number by the total number of n-grams within the generated output.
    
- _[BERTScore](https://github.com/Tiiiger/bert_score)_ [26]: generates an embedding (using [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)) for each n-gram in the generated output and reference output, then uses cosine similarity to compare n-grams from the two textual sequences, enabling semantic matches between n-grams instead of just exact matches.
    
- _[MoverScore](https://github.com/AIPHES/emnlp19-moverscore)_ [27]: generalizes BERTScore from requiring a one-to-one matching between n-grams to allow many-to-one matches, thus making the evaluation framework more flexible.
    

In certain cases, reference-based metrics may be undesirable; e.g., our reference summaries could be of low quality, or maybe we don’t have access to reference summaries at all! To handle these cases, we can derive a context-based version of ROUGE—_called ROUGE-C [24]_—by comparing the output summary to the source document instead of a reference summary; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e0de880-766e-4059-bbf3-a235667a1014_840x226.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e0de880-766e-4059-bbf3-a235667a1014_840x226.png)

(from [24])

Using the same strategy, we can generate reference-free variants of BERTScore and MoverScore too! For more details on the variety of reference-free and reference-based summarization metrics that exist, check out [this paper](https://arxiv.org/abs/2007.12626).

> _“Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references.”_ - from [29]

**LLM-as-a-Judge.** One popular strategy for evaluating LLM outputs (including for abstractive summarization tasks) is LLM-as-a-Judge [28], which uses a powerful LLM (e.g., GPT-4) for evaluation. To rate or score a generated output, _we simply prompt an LLM_! This can be done in a few different ways:

- Asking the LLM to identify the preferred output within a pair of generated outputs (shown below).
    
- Asking the LLM generate a scalar score (within a specified range) for a single generated output based upon criteria that are outlined in the prompt.
    
- Asking the LLM to rate a generated output based upon several [few-shot examples](https://cameronrwolfe.substack.com/i/143156742/basic-prompting-strategies) that demonstrate accurate scoring.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf74dbf7-8338-4fa8-9a8d-985659cd0e50_1076x990.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf74dbf7-8338-4fa8-9a8d-985659cd0e50_1076x990.png)

(from [28])

LLM-as-a-judge is a new, powerful, and reference-free strategy for evaluating an LLM’s output. However, this evaluation approach introduces several forms of bias:

- _Position bias_: the position of a generated output within the model’s prompt can influence the resulting score. To solve this, we can generate multiple scores with randomly-sampled positions and take their average.
    
- _Verbosity bias_: Models like GPT-4 tend to prefer more verbose outputs. We can solve this by normalizing the length of generated outputs.
    
- _Self-enhancement bias_: GPT-4 and other models tend to score their own output higher than that of other models, so we should be careful when using any LLM to score its own generations!
    
- _Limited capabilities_: LLMs are not perfect! So, we might run into limitations when using LLM-as-a-Judge to score a model’s output on a problem that the judge itself struggles to solve (e.g., complex math or reasoning problems).
    

Despite these biases, LLM-as-a-judge-style evaluations tend to be surprisingly robust and correlate well with human ratings across a variety of applications, leading to their widespread adoption (for summarization and beyond) in recent research. In [29], authors augment LLM-based evaluations with [chain of thought prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) and form-filling, creating a new evaluation strategy called G-Eval that improves evaluation quality for summarization tasks in particular; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f79460c-8cf9-4123-bf56-43558fc418a5_1234x906.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f79460c-8cf9-4123-bf56-43558fc418a5_1234x906.png)

(from [29])

**Reward models.** As discussed above, preference tuning—_one of the most effective strategies for training summarization models_—involves training a reward model over a human preference dataset. This model’s output is used as a reward signal for finetuning with RL, but the same reward signal can be re-purposed for quality evaluation purposes! The reward model takes a generated summary as input and predicts a human preference score for this summary. As such, the output of a reward model is a proxy for human preferences that can be directly used as a reference-free quality assessment. For more information, check out the awesome writeup (from [Nathan Lambert](https://www.interconnects.ai/)) below on evaluating reward models.

[RewardBench](https://arxiv.org/abs/2403.13787)

## Improving Summaries with Human Feedback

> _“Compared to the supervised learning paradigm, which pushes the summariser to reproduce the reference summaries at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries.”_ - from [1]

For a long time, the state-of-the-art approach for training summarization models was to perform supervised finetuning of a pretrained base model over a dataset of high-quality reference summaries. This approach is effective, but we can get better results via preference tuning, as we will see in this section. _Human feedback allows us to train much better summarization models._ However, such research goes beyond the topic of summarization. Similar techniques were re-purposed by recent LLM alignment research, forming the basis of the [LLM training pipeline](https://cameronrwolfe.substack.com/p/explaining-chatgpt-to-anyone-in-20).

#### [Better Rewards Yield Better Summaries](https://arxiv.org/abs/1909.01214) [1]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F794dcc04-5067-484c-b110-52d6481ae963_1514x1166.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F794dcc04-5067-484c-b110-52d6481ae963_1514x1166.png)

(from [1]

Supervised learning was originally the most commonly-used paradigm for training summarization models—_we just train models to mimic human-written reference summaries_. More recently, researchers began exploring the use of RL for training summarization models. Initial attempts directly used ROUGE scores as rewards[5](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-5-144374854), but ROUGE scores correlate poorly with human quality evaluations; see above. As such, authors in [1] attempt to find a better reward function that guides models towards summaries that are more preferable to humans via RL.

> _“To find a better reward function to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries.”_ - from [1]

**Learning the reward.** Authors in [1] propose learning a reward function from a dataset of human preferences. This dataset, which is taken from [prior work](https://arxiv.org/abs/1807.02202), contains 2,500 summaries (with human ratings) of 500 news articles from the [CNN / DailyMail corpus](https://arxiv.org/abs/1602.06023). Using this data, we can train a reward model to predict a human rating given a document and system summary as input. Unlike techniques that use ROUGE as the reward, _reference summaries are not required to compute the reward_! To train the reward model, we can use either a regression objective or a preference learning objective, which captures whether the reward function accurately identifies the human-preferred summary; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe02c30ea-365b-4039-8f13-38afb62d27c5_1462x1190.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe02c30ea-365b-4039-8f13-38afb62d27c5_1462x1190.png)

(from [1])

Although several architectures are considered for the reward function in [1], the approach that yields the best results is a [feed-forward network](https://cameronrwolfe.substack.com/i/142044446/feed-forward-transformation) that takes as input concatenated embeddings—_generated using BERT_—of the summary and input document. We see in [1] that the preference learning objective yields better results compared to the regression objective, thus explaining why preference learning objectives are now almost [universally used](https://cameronrwolfe.substack.com/i/138218863/how-does-rlhf-work) for training reward models. In the table below, we see that the best reward function uses:

- BERT for embeddings.
    
- Feed-forward network (or MLP) for predicting the reward.
    
- The preference-based learning objective.
    

This reward function is found to yield predictions that correlate well with human judgements and identify “good” summaries with high recall and precision.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b4933f-814a-439d-9487-105d0a9f8eb3_1834x602.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6b4933f-814a-439d-9487-105d0a9f8eb3_1834x602.png)

(from [1])

**Better summarization models.** Beyond accurately predicting human preferences, the reward models trained in [1] can be used to create better extractive and abstractive summarization models. Compared to both supervised baselines and models trained using RL with ROUGE as a reward, models trained with rewards learned from human feedback are found to produce summaries with much higher human ratings (even higher than prior state-of-the-art systems!); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff64d0bd1-dcf2-423e-bb6c-5dc57595ee9a_1928x528.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff64d0bd1-dcf2-423e-bb6c-5dc57595ee9a_1928x528.png)

(from [1])

Put simply, we see in [1] that learning a reward function from human feedback can provide a superior learning signal for training RL-based summarization models. Later work adopts this lesson and extends it to the domain of LLMs to learn a variety of tasks (including summarization) from human feedback.

#### [Learning to Summarize from Human Feedback](https://arxiv.org/abs/2009.01325) [2]

Supervised finetuning uses a next token prediction objective to maximize the log probability of a set of human demonstrations. _We are teaching the model to assign high probability to human-written text_[6](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-6-144374854). Evaluating these models via ROUGE allows us to quantify how closely the model’s output matches a reference. This approach works (relatively) well, but authors in [2] note that supervised learning and ROUGE are just proxies for our actual objective—_writing high-quality summaries_.

> _“We show that it is possible to significantly improve summary quality by training a model to optimize for human preferences.”_ - from [1]

Humans do not always write perfect summaries, and numerous equally valid summaries can be written for any one source document. As such, training a summarization model to exactly match a human-written summary is a flawed approach! All references summaries—_even those that are low quality_—are equally emphasized during the training process, and we have no way to account for the diversity of valid summaries. With this in mind, we may begin to wonder: _Can we find an objective that directly optimizes the model based on summary quality?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)

(from [2])

**Learning from human feedback.** In [2], authors do exactly this by proposing a three-part framework that enables LLMs to be finetuned based on preference data from humans; see above. The LLM is first trained using supervised finetuning over human reference summaries, producing a supervised baseline that is then further finetuned with RL. The RLHF process in [2] begins by collecting a dataset of human feedback by:

- Taking a textual input (source document) from the training dataset.
    
- Using several policies (e.g., pretrained model, supervised baseline, current model, or the human reference summary) to sample summaries of the input.
    
- Picking two summaries from the set of samples responses.
    
- Asking a human annotator to identify the better of the two summaries.
    

Human comparison data is collected in large batches and used to finetune the model—_a [decoder-only LLM](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)_—via RLHF in an offline fashion. Once the data has been collected, we use this comparison data to train a reward model that accurately predicts a human preference score given a summary produced by the LLM. From here, we use RL to finetune the model—_authors in [2] use the [PPO](https://cameronrwolfe.substack.com/i/138008873/proximal-policy-optimization-ppo) algorithm_—based on preference scores outputted by the reward model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82cc0057-8c4c-4182-8973-8845981befd6_1970x660.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82cc0057-8c4c-4182-8973-8845981befd6_1970x660.png)

Adding KL divergence to the optimization objective for PPO (from [2])

**Avoiding drift.** The authors in [2] add a [KL divergence](https://cameronrwolfe.substack.com/i/138008873/kullbackleibler-kl-divergence) term to the objective being optimized by PPO, which penalizes the policy from becoming too different from the supervised baseline policy during RLHF; see above. Such an approach, which is now commonly used (e.g., see Eq. 4 in the [LLaMA-2 report](https://arxiv.org/abs/2307.09288)), encourages exploration without mode collapse[7](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-7-144374854) and prevents summaries written by the LLM from becoming too different from those that are seen during training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa953b9b1-e15a-4bb5-aa3a-ef473ca43d3d_1610x974.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa953b9b1-e15a-4bb5-aa3a-ef473ca43d3d_1610x974.png)

(from [2])

**Is learning from feedback effective?** Authors in [2] finetune several [GPT-style](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) language models with 1.3B to 6.7B parameters on the [TL;DR dataset](https://huggingface.co/datasets/openai/summarize_from_feedback) using the strategy described above. Models trained via human feedback are found to generate summaries that are consistently preferred by humans compared to those written by models trained solely via supervised learning; see above. The 1.3B human feedback model outperforms a 10× larger model trained with supervised learning alone, and the 6.7B human feedback model performs even better than the 1.3B model—_summarization quality benefits from model scale_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)

(from [2])

Summarization models trained with human feedback seem to generalize better to new domains. For example, models finetuned on TL;DR in [2] are found to perform well on news-centric datasets without further finetuning; see above.

> _“We confirm that our reward model outperforms other metrics such as ROUGE at predicting human preferences, and that optimizing our reward model directly results in better summaries than optimizing ROUGE according to humans”_ - from [1]

**Reward model > ROUGE.** Although reference-based metrics like ROUGE are standard for evaluating summaries, authors in [2] observe that ROUGE scores tend to correlate poorly with human preferences. For this reason, summarization models finetuned with ROUGE as the reward signal for PPO are outperformed by those that train a reward model to predict human preferences. Reward models are found in [2] to predict human preferences very accurately, _revealing that preference-based metrics are a useful approach for evaluating summary quality_.

#### **[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) [3]**

Prior to work in [2], the same authors (from OpenAI) explored the use of human feedback for finetuning pretrained LLMs on four different tasks [3]—_stylistic continuation of text with either positive sentiment or physically descriptive language, as well as summarization on the TL;DR and CNN/Daily Mail datasets_. The finetuning strategy used in [3] mostly matches that of [2], but there is no supervised finetuning component—_we solely finetune based on human feedbadck_; see below. In [3], preference tuning is presented as an alternative to supervised learning, rather than a technique that can be applied in tandem.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f643e19-e16c-429c-a63e-56f0047e8196_1124x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f643e19-e16c-429c-a63e-56f0047e8196_1124x782.png)

(from [3])

For stylistic continuation, authors collect a dataset of 5,000 preference pairs, while over 60,000 preference pairs are collected for summarization. The human feedback models for stylistic continuation are found to be preferred to supervised baselines 77% of the time. Similarly, human annotators prefer the summarization models trained with human feedback over supervised baselines, but the human feedback model is also surprisingly outperformed by a simple baseline that just copies the first three sentences of the document being summarized; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33a45731-942a-41b5-952f-d0254deebf17_1450x692.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33a45731-942a-41b5-952f-d0254deebf17_1450x692.png)

(from [3])

Interestingly, analysis in [3] reveals that human feedback-based summarization models learn a very simple policy for producing abstractive summaries, which authors in [3] refer to as “smart copying”. The model tends to copy large spans of text or sentences from the source text, skipping sentences that are irrelevant and not worth including in the summary. This mechanism emerges naturally during the training process—_there is no explicit architectural component added to the summarization model that encourages copying_. Despite its simplicity, this learned copying mechanism results in favorable ratings from human annotators.

> _“We would like to apply reinforcement learning to complex tasks defined only by human judgment, where we can only tell whether a result is good or bad by asking humans.”_ - from [3]

**Useful takeaways.** Compared to work in [2], the summarization models trained using human feedback in [3] are much less powerful and tend to be outperformed by simple baselines. This negative result is likely due to the difference in models being used and the exclusion of supervised finetuning from the training process. Nonetheless, research in [3] lays a solid foundation for later advancements, and there are several interesting takeaways that can be gleaned from this analysis:

- Learning from human feedback works best for tasks that lack _i)_ sufficient supervised data and _ii)_ good automatic proxies to be used for a reward signal (e.g., we see in [2] that ROUGE is not a useful reward function).
    
- Adding a KL divergence term to the objective used for finetuning with PPO can help to avoid excessive drift[8](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-8-144374854).
    
- Humans tend to rely on simple (and imperfect) heuristics to judge the quality of an LLM’s output.
    
- Online data collection—_or continuing to collect additional data to retrain the reward model as the LLM is iteratively finetuned and improved_—yields improved performance.
    

The above findings on the benefits of online data collection have had a lasting impact on LLM research! Recent models like [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) undergo several “rounds” of RLHF with freshly-collected data during their alignment process. Plus, we continue to see in [recent research](https://arxiv.org/abs/2305.15717) that the limitations of human evaluation are a very important consideration when measuring LLM output quality!

#### **[Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) [4]**

> _“We use models trained on smaller parts of the task to assist humans in giving feedback on the broader task.”_ - from [4]

Curating reference summaries or preference labels for summarization, although (potentially) a bit time consuming, is not super difficult. The human annotator just has to:

1. Read the article being summarized.
    
2. Either write a summary or assess the quality of presented summaries.
    

However, this process becomes more difficult when the text being summarized is very long; e.g., an entire fiction novel. In this case, evaluating a summary of the novel is time consuming, as the human annotator must have either read the book already or needs to spend time reading the book to accurately assess a summary. In [4], authors propose a scalable approach—_by combining human feedback with recursive task decomposition_—for humans to provide a training signal on such tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7f02cd0-0b83-446f-8b00-f55748048812_1352x1336.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7f02cd0-0b83-446f-8b00-f55748048812_1352x1336.png)

(from [4])

**Recursive task decomposition.** The core idea proposed in [4] is to break a long text into smaller chunks that can be recursively summarized, forming a tree of summarization tasks (shown above) where leafs are just standard summarization tasks over a reasonably-sized chunk of text. First, the model is used to summarize small chunks of a book. Then, the same model ingests these leaf summaries to summarize larger portions of the book—_summaries of shorter passages are used as input for summarizing larger portions of the book._ By taking this approach, humans can efficiently supervise this task[9](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-9-144374854), despite lacking an in-depth knowledge of the source text as a whole, as the model only summarizes small chunks of text, either from the book itself or from previously-generated summaries; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c2ef598-4fe7-483b-9607-db7ef1c4dd0e_1508x946.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c2ef598-4fe7-483b-9607-db7ef1c4dd0e_1508x946.png)

(from [4])

At inference time, the model is first used to summarize small sections of the book. Then, these summaries are used recursively to produce higher-level summaries until a summary of the entire book is generated. Due to this recursive strategy, _the approach proposed in [4] can summarize texts of arbitrary length_!

**Creating a dataset.** If we recursively decompose the summarization of a long text to a sufficient depth, we will eventually end up with a set of reasonable summarization sub-tasks that can be easily supervised by humans and used to train an LLM. Recursive summarization has three main operations:

1. _Decompose_: identify that a text is too long to summarize directly and spawn several summarization sub-tasks over shorter portions of the text.
    
2. _Respond_: solve a sub-task by producing a summary.
    
3. _Compose_: same as “respond”, but the model is shown solutions to several sub-tasks (i.e., previously-generated summaries) when producing a summary.
    

For books, the decompose operation can be performed algorithmically—_just chunk long sequences of text into shorter sequences_—instead of asking the LLM to generate sub-tasks. With this strategy, obtaining training data is simple! We just ask humans to either manually summarize a certain sub-task or compare the quality of two summaries that are produced for a certain sub-task. If a node is not a leaf, meaning that humans are summarizing a longer text, then the LLM is used to recursively produce summaries of all child tasks to use as context.

> _“We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively.”_ - from [4]

Given this dataset of supervised examples and preference labels, the training strategy used in [4] is nearly identical to that of [2]. Starting with a pretrained [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) model, we first finetune the model in a supervised manner—_this is called behavioral cloning in [4]_—over human-written reference summaries. Then, we perform preference tuning over collected preference labels for several iterations. See below for a full description of the training algorithm used in [4].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b88871-eba8-4926-9597-1d4f4ca5b03c_1270x610.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b88871-eba8-4926-9597-1d4f4ca5b03c_1270x610.png)

(from [4])

**Empirical results.** The training data for [4] is collected using the subset of books data (i.e., [Books1 and Books2 datasets](https://gregoreite.com/drilling-down-details-on-the-ai-training-datasets/#Books1_Books2_15)) from the pretraining dataset of GPT-3. This data is used to finetune GPT-3 (and a smaller model variant with 6B parameters instead of 175B) to recursively summarize books. Then, the model is evaluated over a set of 40 popular books published in 2020—_indicating that they are not included in GPT-3’s pretraining dataset_—that span several genres; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27044671-33d9-4359-88b2-2e95e0cf20be_1198x1366.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27044671-33d9-4359-88b2-2e95e0cf20be_1198x1366.png)

(from [4])

Two labelers are asked to read each of these books, write a summary, and rate summaries from various models. As show below, models trained with human preferences far outperform those trained solely using supervised learning. However, _all models are still far behind human performance_, revealing that abstractive book summarization is an incredibly hard task to solve. In fact, only 5-15% of model-generated summaries are found to match human quality.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40c02249-2a2a-42b0-a5cb-ba0d61788e28_1194x662.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40c02249-2a2a-42b0-a5cb-ba0d61788e28_1194x662.png)

(from [4])

We see in [4] that training on the first sub-task (i.e., summarizing short chunks from the book) is most important—_models trained on only this task can generalize relatively well to higher-level summarization tasks_. However, performance when generating full book summaries is still disappointing. Human preference scores for individual sub-tasks are much higher than scores assigned to summaries that are produced via a full decomposition of a book; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc149d15-9bc0-4cc0-a8c5-e4581196ddfe_1184x1094.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc149d15-9bc0-4cc0-a8c5-e4581196ddfe_1184x1094.png)

(from [4])

#### **[Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155) [5]**

> _“Making language models bigger does not inherently make them better at following a user’s intent… these models are not aligned with their users… we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.”_ - from [5]

In [5], we see—_from authors at OpenAI_—that similar training strategies proposed in [2] can be used to make generic, foundational language models better at following instructions in their prompt. The result of this work is a language model called InstructGPT—_the predecessor to ChatGPT that laid the foundation for nearly all research on modern LLMs_.

The core idea behind InstructGPT is to finetune a pretrained LLM using SFT and RLHF (shown below) to encourage alignment, _defined as the ability to generate text that aligns with the desires of a human user_. Typically, we capture these desires concretely by defining a fixed set of alignment criteria (e.g., follow instructions, avoid harmful output, avoid lying, produce interesting or creative output, etc.).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fe7cb88-a13f-4534-ac29-aceb535996d8_1614x1172.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fe7cb88-a13f-4534-ac29-aceb535996d8_1614x1172.png)

(from [5])

After pretraining, the LLM is likely to generate repetitive (or unhelpful) text, hallucinate frequently, and struggle with following complex instructions within the prompt. Through the alignment process, however, we can teach the LLM necessary skills to avoid these shortcomings and satisfy desired alignment criteria, producing a model that is drastically more useful and interesting.

**Creating InstructGPT.** The alignment process for InstructGPT begins with a pretrained language model—_experiments are run with 1.3B, 6B, and 175B models_—and a set of prompts[10](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-10-144374854) that the LLM should be able to answer. First, human annotators manually provide answers for each of these prompts, creating a dataset over which the pretrained model is finetuned via supervised learning. This initial training process exposes the model to instruction-like prompts, thus creating a better starting point for further finetuning with human feedback.

> _“The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API).”_ - from [5]

From here, a dataset of human preference labels is collected, where each preference example ranks multiple responses to the same prompt based on quality. This preference dataset is used to train a reward model (RM)—_which shares the same architecture as the LLM_—that predicts a scalar reward given a prompt and a generated response as input; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7eaa7758-d7a4-433d-b5b9-99bc2ebb3f4b_698x778.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7eaa7758-d7a4-433d-b5b9-99bc2ebb3f4b_698x778.png)

Reward model architecture

We can use this RM to finetune the LLM via [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) over set of relevant prompts. This training pipeline, which almost exactly matches the proposal of [2], forms the standard three-part alignment procedure used by nearly all LLMs. Here, the approach is just applied to alignment rather than summarization!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F934e599b-0348-44ee-b79f-979779863618_1610x962.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F934e599b-0348-44ee-b79f-979779863618_1610x962.png)

(from [5])

**Empirical findings.** We see in [5] that learning from human feedback is an incredibly effective strategy for producing aligned (and useful) LLMs. In particular, authors in [5] observe that:

- Human annotators prefer InstructGPT outputs over GPT-3 (see above).
    
- InstructGPT tends to be more truthful than GPT-3.
    
- GPT-3 produces more toxic output than InstructGPT.
    
- InstructGPT is better at following instructions, even those that go beyond the distribution of prompts used for finetuning.
    

Compared to GPT-3, InstructGPT has very favorable alignment properties. As shown in the table below, InstructGPT tends to be much more steerable than GPT-3, meaning that the user can better control the model’s behavior by providing constraints, instructions, or details within the prompt.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809306f-5848-4e89-b4ab-6d3e11f3c270_1612x750.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809306f-5848-4e89-b4ab-6d3e11f3c270_1612x750.png)

(from [5])

Despite all of these benefits, we see that the alignment procedure also comes with an _“alignment tax”_ in the form of performance regressions on public benchmarks compared to the pretrained model. To avoid this tax, we can simply perform intermittent updates over the pretraining dataset during preference tuning.

## Summarization in the Age of LLMs

Given that InstructGPT was not focused upon summarization, we might be wondering why this paper was included within this overview. However, InstructGPT represents a paradigm shift in NLP research away from narrow experts—_models that specialize in solving a single kind of task_—and towards [foundation models](https://crfm.stanford.edu/)—_models that can accurately solve a wide variety of tasks_. Research on abstractive summarization provided a starting point for later research on foundation models. Over time, however, researchers began to focus less on the problem of abstractive summarization in particular, choosing instead to focus on creating better foundation models. For these models, abstractive summarization is just one of the many tasks that they are capable of solving accurately.

#### **[News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/abs/2209.12356) [6]**

> _“We compare these approaches using A/B testing on a new corpus of recent news articles, and find that our study participants overwhelmingly prefer GPT-3 summaries.”_ - from [6]

As mentioned above, the proposal of powerful LLMs led to a paradigm shift in AI research where we could accurately solve a variety of tasks by simply prompting a foundation model. In [6], authors study the impact of this paradigm shift on summarization research, focusing on news summarization.

**How good is GPT-3?** Despite not being explicitly trained on summarization-specific data, we see in [1] that GPT-3 is very good at writing summaries that are preferable to humans, even when we only prompt the model with a task description (i.e., [zero-shot regime](https://cameronrwolfe.substack.com/i/143156742/basic-prompting-strategies))! Going further, GPT-3 does not suffer from dataset-specific issues (e.g., learning from poorly-written or inaccurate reference summaries) due to the fact that it is a generic model trained on a massive textual corpus that can solve a variety of tasks without explicit finetuning. For this reason, _GPT-3 naturally generalizes to new summarization domains without requiring a sizeable dataset_. We just need an instruction or some few-shot examples.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbcade40a-77c7-4ebd-884c-55f0ddc28c55_1232x648.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbcade40a-77c7-4ebd-884c-55f0ddc28c55_1232x648.png)

(from [6])

Compared to SFT and instruction-tuned models, GPT-3 generates summaries that achieve 20% higher scores from humans across all datasets, indicating a clear preference for GPT-3; see above. However, the choice of the best summarization model is only unanimous in <30% of cases, revealing that high-quality summaries can be difficult to compare—_the choice of the “best” model is not straightforward_.

**Analyzing automatic metrics.** Authors in [6] also extensively analyze the effectiveness of automatic metrics in evaluating summary quality. Prior research seems to indicate that automatic metrics like ROUGE are useful for discerning large quality differences between summarization models but struggle to capture small performance differences (i.e., automatic metrics are less useful for comparing models with nuanced performance differences). We see in [6] that this rule of thumb is less straightforward in the age of LLMs. GPT-3 summaries score significantly worse than baselines—_a seven point difference in ROUGE!_—despite being preferred almost unanimously in human trials; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7322c74a-89a8-4042-a19d-b24df6368fae_1350x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7322c74a-89a8-4042-a19d-b24df6368fae_1350x838.png)

(from [6])

These results provide further evidence that reference-based metrics correlate poorly with human preferences when considering powerful summarization models. Going further, we see in [6] that similar results—_both in terms of the quality of GPT-3 summaries and limitations of automatic metrics_—are found to hold for more specialized summarization tasks like keyword and aspect-based summarization, which perform guided summarization based on specific keywords or topics.

#### **[Prompted Opinion Summarization with GPT-3.5](https://arxiv.org/abs/2211.15914) [7]**

Although most papers study text summarization (e.g., summarizing news articles), authors in [7] consider the more complex task of opinion summarization[11](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-11-144374854) to determine if this task can be solved with [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5-turbo). _Why is opinion summarization more complex than text summarization?_ There are a few reasons:

- Different opinions might be contradictory, thus requiring a more nuanced summary that accurately reflects different viewpoints[12](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-12-144374854).
    
- The length of all opinions being summarized oftentimes exceeds the LLM’s [context length](https://cameronrwolfe.substack.com/i/143156742/what-is-prompt-engineering), so we need a pipeline to process and summarize the data.
    

The opinion summarization tasks is less widely explored than general text summarization. Similarly to [4], a recursive strategy can be used to summarize long opinion sequences, but alternative techniques are also explored in [7]; e.g., clustering opinions into thematic groups or automatically identifying the most salient (or important) opinions to be included in the summary.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23cc271-8ecc-4179-808a-31bcc0707538_1600x780.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23cc271-8ecc-4179-808a-31bcc0707538_1600x780.png)

(from [7])

**Summarization pipelines.** Authors in [7] explore a variety of different opinion summarization pipelines, but three primary approaches are emphasized:

1. A hierarchical approach that recursively chunks and summarizes text to produce the final output (i.e., chunking with repeated summarization).
    
2. A pre-extraction strategy that uses an extractive summarization model, called [QFSumm](https://arxiv.org/abs/2110.08296), to identify the most salient reviews prior to summarization.
    
3. A clustering approach that separates reviews into clusters based upon their topics or ratings, summarizes each cluster recursively, then generates a final summary; see above.
    

We see in [7] that GPT-3.5 produces useful summaries via basic, recursive summarization for short sequences of reviews. When processing longer sequences, however, _we see that repeated chunking and summarization can deteriorate summary quality_. We can mitigate this issue via the pre-extraction and clustering strategies described above, as these techniques tend to reduce the number of recursive summarization steps required to generate a final output.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c40ead0-7e87-4185-83f3-48e4729e5cf2_1056x810.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c40ead0-7e87-4185-83f3-48e4729e5cf2_1056x810.png)

(from [7])

Examples of opinion summaries generated with these strategies (on the [SPACE](https://paperswithcode.com/dataset/space-opinion-summarization) hotel opinion summarization dataset) are shown within the figure above. As we can see, topic-wise clustering seems to yield more abstractive summaries that provide a high-level view of various opinions, while the extractive strategy focuses more on specific points that are made within each of the input reviews.

**Human evaluation.** Beyond the summarization pipelines outlined above, authors in [7] explore a wide variety of different pipelines by modularizing each pipeline component and using a plug-and-play approach. These pipelines are used to generate summaries on the SPACE and [FewSum](https://arxiv.org/abs/2004.14884) (contains Amazon and Yelp reviews) opinion summarization datasets and are evaluated based on factuality, faithfulness, relevance, and representativeness of their generated summaries.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d47f82-bb2b-4bc7-afc1-1b508aedf72a_1490x744.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d47f82-bb2b-4bc7-afc1-1b508aedf72a_1490x744.png)

(from [7])

As shown above, the best summarization pipeline differs based upon the dataset and property being evaluated. However, summaries generated with GPT-3.5 far outperform baseline techniques in terms of human evaluation, despite performing poorly on automatic metrics. As such, the summarization pipelines proposed in [7] prove to be an advancement over prior state-of-the-art.

#### **[Benchmarking Large Language Models for News Summarization](https://arxiv.org/abs/2301.13848) [8]**

> _“We find instruction tuning, and not model size, is the key to the LLM’s zero-shot summarization capability.”_ - from [8]

At this point, we know that LLMs can summarize text really well, outperforming prior state-of-the-art techniques in most cases. But, _why is this the case?_ The design decisions that underlie the success of LLMs on summarization tasks are poorly understood. In [8], authors conduct extensive human evaluations of ten different LLMs—_encompassing several pretraining methods, prompts, and model scales_—on summarization tasks. From these experiments, we clearly see that instruction tuning the key component that makes LLMs effective summarizers.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe25e558-6357-408a-aa6b-8af15cdbea68_896x646.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe25e558-6357-408a-aa6b-8af15cdbea68_896x646.png)

(from [8])

**Poor reference summaries.** Most evaluations performed in [8] are conducted on the CNN / DailyMail and XSum datasets. Interestingly, authors show that most reference summaries from these datasets are judged by humans to be of low quality, indicating that existing summarization studies are limited by the low-quality reference summaries present in common datasets. In fact, authors in [8] show that the poor correlation between reference-based evaluation metrics (e.g., ROUGE) and human preferences is worsened by low-quality reference summaries, thus calling a variety of prior summarization research into question.

> _“To address the quality issues of reference summaries and better understand how LLMs compare to human summary writers, we recruit freelance writers … to re-annotate 100 articles from the test set of CNN/DM and XSUM.”_ - from [8]

To solve this issue with poor reference summaries, a fresh set of 100 examples from the CNN / DailyMail and XSum datasets are re-annotated by humans. These high-quality reference summaries can be used for more reliable evaluations that do not artificially degrade LLM performance across summarization tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0a8d7af-1e03-44d9-8210-2efc77cee47c_1332x664.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0a8d7af-1e03-44d9-8210-2efc77cee47c_1332x664.png)

(from [8])

**Experimental setup.** Ten different LLMs are considered for measuring summarization quality; see above. Models are evaluated using either a zero or five-shot prompt with the basic template shown below.

`“Article: [article]. Summarize the article in three sentences. Summary:”`

Again, authors emphasize the low quality of reference summaries present within existing summarization datasets. In fact, using reference summaries as in-context learning examples was found to degrade LLM performance! Several qualitative examples of LLM-generated output summaries are shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72c9cacf-f833-4230-8c41-18a045fabb0d_1530x1010.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72c9cacf-f833-4230-8c41-18a045fabb0d_1530x1010.png)

(from [8])

**The main takeaways** from the experimental analysis in [8] are twofold:

1. Instruction tuning clearly benefits summarization performance.
    
2. LLM-generated summaries are extractive in nature, whereas human-written summaries tend to contain more abstraction or paraphrasing; see below.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c3c02dc-3efc-4cfd-8c87-143b383431cc_758x920.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c3c02dc-3efc-4cfd-8c87-143b383431cc_758x920.png)

(from [8])

LLMs that have undergone instruction tuning clearly outperform those that have not, indicating that [self-supervised pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining) is not enough to yield competitive summarization results. Additionally, LLM-generated summaries tend to directly copy information from the original article, though the copied information is synthesized in a coherent fashion. In contrast, human written summaries tend to paraphrase information rather than copying from the source material. Despite their extractive nature, LLM-generated summaries are preferred equally to those written by humans when compared in human evaluation trials; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21292ccd-3d73-489b-8e87-ed2e1a297213_2166x802.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21292ccd-3d73-489b-8e87-ed2e1a297213_2166x802.png)

(from [8])

#### **[ChatGPT vs. Human-Authored Text](https://arxiv.org/abs/2306.07799) [9]**

> _“The stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types”_ - from [9]

One interesting form of summarization is _controllable summarization_, in which we instruct the model to write a summary that targets a particular audience. In [9], authors study the performance of GPT-3.5-Turbo on summarizing scientific information for both experts and non-experts, aiming to identify the behavioral differences (compared to human-written summaries), limitations, and challenges faced when performing controllable summarization with an LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F151d2923-ca0a-425a-b593-a4de76c6615a_1918x584.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F151d2923-ca0a-425a-b593-a4de76c6615a_1918x584.png)

Prompts for controllable summarization

**Specifying an audience.** To perform controllable summarization with an LLM, we simply have to modify our prompt. In the case of [9], we can specify the audience for which we want to write the summary within an instruction; see above. These prompts are used to summarize scientific literature from the [eLife dataset](https://paperswithcode.com/dataset/elife), where 500 random samples are taken from the dataset for evaluation. The LLM is prompted in a zero-shot manner to generate all summaries.

**Evaluation metrics.** To determine whether GPT-3.5-Turbo is able to sufficiently adapt its generated summary towards the specified target audience, several automatic metrics are used for evaluation:

- _[Flesch Reading Ease](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests)_: measures readability via the average number of syllables per word and the average number of average words per sentence. Texts that score higher are easier-to-understand.
    
- _[Coleman-Liau Index](https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index)_: captures textual difficulty by measuring the average number of characters per sentence and the average number of sentences per 100 words. Texts that score higher are more challenging to understand.
    
- _[Dale-Chall Readability Score](https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula)_: compares the number of complex words in a text with a list of common words. Texts that score higher are more challenging to understand.
    

Beyond these readability metrics, ROUGE scores and n-gram novelty[13](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-13-144374854) are measured, as well as metrics like [SummaC](https://arxiv.org/abs/2111.09525) and [named entity hallucination](https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2021-ma-timfischer.pdf) that detect factual inconsistencies within the model-generated summaries.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac71d33-4045-46c2-a68c-74076257d9d6_606x670.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faac71d33-4045-46c2-a68c-74076257d9d6_606x670.png)

(from [9])

**Key takeaways.** The analysis in [9] gives us two key pieces of information:

1. GPT-3.5-Turbo is worse than humans at tailoring its output towards a particular audience.
    
2. Model-generated summaries tend to be more extractive and commonly contain hallucinations.
    

As shown in the table above, GPT-3.5-Turbo generates summaries that achieve Flesch Reading Ease scores within the ranges of `[31, 38]` for layman summaries and `[28, 37]` for expert summaries. In contrast, layman and expert summaries written by humans achieve average Flesch Reading Ease scores of 53.1 and 22.5, respectively—_the magnitude of difference in readability scores is drastically different between human and model-written summaries_. Providing few-shot examples can mitigate this issue, but GPT-3.5-Turbo is nonetheless found to be less effective on controllable summarization tasks compared to humans.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf82e3-05f5-4a08-81c6-c01c017ab517_610x480.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8acf82e3-05f5-4a08-81c6-c01c017ab517_610x480.png)

(from [9])

Summaries generated with GPT-3.5-Turbo also have a lower n-gram novelty than those written by humans, indicating the model-written summaries are more extractive in nature; see above. We also see that summaries generated with GPT-3.5-Turbo tend to have frequent hallucinations, as measured by the overlap of entities and topics between the summary and source material; see below. All of these results seem to indicate that GPT-3.5-Turbo has room for improvement in the domain of controllable summarization, though it should be noted that more recent models (e.g., [GPT-4o](https://openai.com/index/hello-gpt-4o/)) would likely perform better.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F887d0e4d-af3c-4dab-8591-3b96ce2ca55d_1542x640.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F887d0e4d-af3c-4dab-8591-3b96ce2ca55d_1542x640.png)

(from [9])

## Concluding Remarks

We have now taken a look at several generations of summarization research, ranging from supervised finetuning, to preference tuning, to modern foundation models. From this work, there are several common themes that arise:

- The difficulty (and importance) of evaluation.
    
- The value of learning from human feedback.
    
- The importance of data quality.
    
- The impressive capabilities of modern foundation models.
    

Although this overview focused specifically upon summarization research, the findings from this work are highly generalizable. Summarization is a fundamental task in natural language processing due to its roots in conditional generation (i.e., teaching a model to generate an output given some input). Many other important tasks—_machine translation, text classification, keyword extraction, and question answering to name a few_—follow a very similar pattern! A deep understanding of summarization research is helpful for solving a much broader class of problems.

> _“We use reinforcement learning from human feedback (RLHF; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions.”_ - from [5]

The fundamental role of summarization within natural language processing research has led these techniques to be adopted heavily in the age of LLMs. A variety of impactful approaches in language modeling research are heavily influenced by summarization papers! For example, InstructGPT adopts a training algorithm that was previously proposed for training better summarization models from human feedback [2], while modern alignment procedures use an iterative finetuning strategy that was advocated by earlier summarization research [3]. The research presented in this overview is practically useful. More importantly, however, it lays a foundation for the advancements we see in LLMs today.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." _arXiv preprint arXiv:1909.01214_ (2019).

[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." _Advances in Neural Information Processing Systems_ 33 (2020): 3008-3021.

[3] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." _arXiv preprint arXiv:1909.08593_ (2019).

[4] Wu, Jeff, et al. "Recursively summarizing books with human feedback." _arXiv preprint arXiv:2109.10862_ (2021).

[5] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[6] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. "News summarization and evaluation in the era of gpt-3." _arXiv preprint arXiv:2209.12356_ (2022).

[7] Bhaskar, Adithya, Alexander R. Fabbri, and Greg Durrett. "Prompted opinion summarization with GPT-3.5." _arXiv preprint arXiv:2211.15914_ (2022).

[8] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." _Transactions of the Association for Computational Linguistics_ 12 (2024): 39-57.

[9] Pu, Dongqi, and Vera Demberg. "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer." _arXiv preprint arXiv:2306.07799_ (2023).

[10] Menick, Jacob, et al. "Teaching language models to support answers with verified quotes." _arXiv preprint arXiv:2203.11147_ (2022).

[11] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." _Transactions of the Association for Computational Linguistics_ 9 (2021): 391-409.

[12] Subramanian, Sandeep, et al. "On extractive and abstractive neural document summarization with transformer language models." _arXiv preprint arXiv:1909.03186_ (2019).

[13] Zhang, Fang-Fang, Jin-ge Yao, and Rui Yan. "On the abstractiveness of neural document summarization." _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. 2018.

[14] Kryściński, Wojciech, et al. "Evaluating the factual consistency of abstractive text summarization." _arXiv preprint arXiv:1910.12840_ (2019).

[15] Maynez, Joshua, et al. "On faithfulness and factuality in abstractive summarization." _arXiv preprint arXiv:2005.00661_ (2020).

[16] Durmus, Esin, He He, and Mona Diab. "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization." _arXiv preprint arXiv:2005.03754_ (2020).

[17] Wang, Alex, Kyunghyun Cho, and Mike Lewis. "Asking and answering questions to evaluate the factual consistency of summaries." _arXiv preprint arXiv:2004.04228_ (2020).

[18] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[19] Wei, Jason, et al. "Finetuned language models are zero-shot learners." _arXiv preprint arXiv:2109.01652_ (2021).

[20] Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." _arXiv preprint arXiv:1602.06023_ (2016).

[21] Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization." _arXiv preprint arXiv:1808.08745_ (2018).

[22] Völske, Michael, et al. "Tl; dr: Mining reddit to learn automatic summarization." _Proceedings of the Workshop on New Frontiers in Summarization_. 2017.

[23] Kryściński, Wojciech, et al. "Neural text summarization: A critical evaluation." _arXiv preprint arXiv:1908.08960_ (2019).

[24] He, Tingting, et al. "ROUGE-C: A fully automated evaluation method for multi-document summarization." _2008 IEEE International Conference on Granular Computing_. IEEE, 2008.

[25] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.

[26] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).

[27] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." arXiv preprint arXiv:1909.02622 (2019).

[28] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." _Advances in Neural Information Processing Systems_ 36 (2024).

[29] Liu, Yang, et al. "Gpteval: Nlg evaluation using gpt-4 with better human alignment." _arXiv preprint arXiv:2303.16634_ (2023).

[1](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-1-144374854)

By an “emergent” capability, we mean a capability of the model that appears only after a certain scale (in terms of data or compute) is explored.

[2](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-2-144374854)

Alignment, which is commonly tackled via a combination of supervised finetuning and preference tuning, refers to the process of finetuning an LLM to generate outputs that better align with the desires of a human user; see [here](https://cameronrwolfe.substack.com/i/139646437/the-alignment-process) for details.

[3](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-3-144374854)

More specifically, the output of modern LLMs tends to be fluent and free of simple grammatical/syntactical errors automatically, so evaluating fluency is arguably needless for such models.

[4](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-4-144374854)

After working at a data annotation company for over two years, I am well-versed in the difficulty of getting large groups of humans to accurately and reliably annotate/score large amounts of data!

[5](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-5-144374854)

Here, we assume that each example used during training has a reference summary. Then, we can compute the summarization output with our model, find the ROUGE score using the reference summary and the model’s output, then use the ROUGE score as a reward signal for training via RL.

[6](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-6-144374854)

Remember, LLMs operate by [assigning probabilities](https://x.com/cwolferesearch/status/1671628210180698112) to each token they output. We want the probabilities assigned to human-written summaries to be high, as these summaries represent reasonable outputs that can be generated by the LLM.

[7](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-7-144374854)

Mode collapse refers to a phenomenon in which the LLM loses its output diversity and begins only producing a narrow set of outputs with a particular style (or set of styles).

[8](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-8-144374854)

Drift simply refers to the finetuned model becoming too different from some reference model (e.g., the model prior to the finetuning process).

[9](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-9-144374854)

Authors in [3] note that by decomposing a book summarization task with a depth of three, we can summarize books with thousands of words while reducing annotation costs of the summarization task by 50×.

[10](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-10-144374854)

Most of these prompts are taken from the OpenAI API, thus ensuring that there is sufficient overlap between the prompts used for training and real world use cases.

[11](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-11-144374854)

Opinion (or review) summarization refers to the task of writing a summary of opinions or reviews left by many users for a particular product or service (e.g., reviews of a product or opinions posted about a restaurant).

[12](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-12-144374854)

For this reason, extractive-style summaries that copy text directly from the source tend to perform poorly for opinion summarization, despite being an effective approach for text summarization.

[13](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of#footnote-anchor-13-144374854)

N-gram novelty refers to the ratio of n-grams generated within the summary that are not present within the source text.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![James Le's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd92c7fe-0109-44bd-a10f-19c8bd4840c0_3596x2514.jpeg)



](https://substack.com/profile/6009523-james-le)

[

![SUVROJYOTI BISWAS's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d1ec132-0fbc-4708-bb73-f628e913b2d2_953x953.jpeg)



](https://substack.com/profile/219888043-suvrojyoti-biswas)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c0a0ee-ed05-46f0-bf99-c12827265e28_144x144.png)



](https://substack.com/profile/49759370-madan-kumar-y)

[

![Shankar Krishnan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e36f35c-06c6-4f8a-b8bf-099a2e1b2504_144x144.png)



](https://substack.com/profile/22573005-shankar-krishnan)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

55 Likes∙

[6 Restacks](https://substack.com/note/p-144374854/restacks?utm_source=substack&utm_content=facepile-restacks)

55

- 

[

3

](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comments)

6

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Meng Li's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4206cf36-9fcc-4b06-95e1-d751f9f4c3b7_388x388.jpeg)



](https://substack.com/profile/138649186-meng-li?utm_source=comment)

[Meng Li](https://substack.com/profile/138649186-meng-li?utm_source=substack-feed-item)

[AI Disruption](https://aidisruption.ai/?utm_content=comment_metadata&utm_source=substack-feed-item)

[6月7日](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comment/58410965 "2024年6月7日 12:30")

Liked by Cameron R. Wolfe, Ph.D.

Large language models (LLMs) have rapidly advanced, building on years of NLP research. Summarization, both abstractive and extractive, exemplifies their progress. This comprehensive overview highlights the evolution and importance of fundamental research in shaping LLM capabilities.

Like (3)

Reply

Share

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry?utm_source=comment)

[Obrian Henry](https://substack.com/profile/45646766-obrian-henry?utm_source=substack-feed-item)

[Obrian’s Newsletter](https://obrian.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[6月6日](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comment/58283415 "2024年6月6日 03:50")

Liked by Cameron R. Wolfe, Ph.D.

Great analysis, well worth the wait!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comment/58283415)

[1 more comment...](https://cameronrwolfe.substack.com/p/summarization-and-the-evolution-of/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Using LLMs for Evaluation

### LLM-as-a-Judge and other scalable additions to human quality ratings...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jul 22, 2024

104

- 

[

14

](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comments)

13

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3dbdd3c-7052-4dff-b204-b615608fc248_1848x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3dbdd3c-7052-4dff-b204-b615608fc248_1848x1004.png)

(from [13, 16, 17])

As large language models (LLMs) have become more and more capable, one of the most difficult aspects of working with these models is determining how to properly evaluate them. Many powerful models exist, and they each solve a wide variety of complex, open-ended tasks. As a result, discerning differences in performance between these models can be difficult. The most reliable method of evaluating LLMs is with human feedback, but collecting data from humans is noisy, time consuming, and expensive. Despite being a valuable and necessary source of truth for measuring model capabilities, human evaluation—_when used in isolation_—impedes our ability to iterate quickly during model development. To solve this problem, we need an evaluation metric that is quick, cost effective, and simple but maintains a high correlation with the results of human evaluation.

> _“While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans.” - from [17]_

Ironically, the ever-increasing capabilities of LLMs have produced a potential solution to this evaluation problem. We can use the LLM itself for evaluation, an approach commonly referred to as LLM-as-a-Judge [17]. This technique was originally explored after the release of GPT-4—_the first LLM that was capable of evaluating the quality of other models’ output_. Since then, a variety of publications have analyzed LLM-as-a-Judge, uncovering best practices for its implementation and outlining important sources of bias of which we should be aware. Throughout the course of this overview, we will take a look at many of these publications and build a deep, practical understanding of LLM evaluations.

## What is [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) [17]?

Many traditional metrics exist for evaluating the quality of textual sequences. These metrics may be reference-based or reference-free, indicating whether a “ground truth” sequence is needed as a reference for measuring quality. These metrics work well on narrower tasks like machine translation or summarization; see [here](https://cameronrwolfe.substack.com/i/144374854/how-can-we-evaluate-a-summary) for details. However, modern LLMs solve diverse, open-ended tasks and have been extensively aligned based on human preferences, which is difficult to detect using legacy NLP benchmarks. For this use case, traditional metrics tend to break down and have been shown to correlate poorly with human preferences.

> _“LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.”_ - from [17]

LLM-as-a-judge is a reference-free metric that directly prompts a powerful LLM to evaluate the quality of another model’s output. Despite its limitations, this technique is found to consistently agree with human preferences in addition to being capable of evaluating a wide variety of open-ended tasks in a scalable manner and with minimal implementation changes. To evaluate a new task, _we just need to tweak our prompt_! This metric was proposed after the release of GPT-4 and has since grown in popularity, culminating in the publication of an in-depth analysis of LLM-as-a-judge metrics in [17]. Today, LLM-as-a-judge is, along with human evaluation, one of the most widely-used evaluation techniques for LLMs—_it excels in the task of evaluating a model’s alignment with human preferences._

#### MT-Bench and Chatbot Arena

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b7af95b-f980-4912-913d-a8f0f2aab26b_1920x704.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b7af95b-f980-4912-913d-a8f0f2aab26b_1920x704.png)

(from [17])

To expand available benchmarks that can be used to measure LLM performance in open-ended dialogue applications, authors in [17] develop two datasets for assessing human preferences. The **MT-bench** dataset is a fixed set of [80 high-quality questions](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts). These questions, which span eight genres[1](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-1-141159804), are heavily focused upon multi-turn conversation and instruction-following capabilities, which are (arguably) the two most important skills for foundation LLMs. Examples of questions from MT-bench are shown above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8b28000-bc1c-45bb-bcb4-5e6f7213bb50_2132x1670.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8b28000-bc1c-45bb-bcb4-5e6f7213bb50_2132x1670.png)

A screenshot of the Chatbot Arena interface (from [17])

The second dataset, called **Chatbot arena**, is more of a platform than a dataset. The arena is a crowdsourced battle platform that allows users to simultaneously interact with two unknown LLMs and select the model that is better; see above. No predefined questions are used. Instead, users pose their own questions and a response is provided by both LLMs, allowing data to be collected across a wide variety of different use cases. To avoid bias, the identity of the model is preserved until after the human provides their preference. Authors in [17] collect a large amount of human feedback from MT-bench and Chatbot Arena that is used to evaluate the correlation of LLM-as-a-Judge with human preferences.

**Chatbot Arena Leaderboard.** Using human preferences collected from the arena, we can compute [Elo scores](https://en.wikipedia.org/wiki/Elo_rating_system) and rank models based on human preferences. Today, over 1.5M pairwise preferences for >100 models have been shared on Chatbot Arena, which has become one of the most widely-referenced LLM leaderboards; see [here](https://chat.lmsys.org/). For more details, check out [Nathan Lambert](https://www.natolambert.com/)’s post on this topic below.

[Chatbot Arena (Interconnects)](https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation)

#### Different Setups for LLM-as-a-Judge

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74d136a6-2eb6-4158-8f85-55fa26fa3c8f_1974x1234.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74d136a6-2eb6-4158-8f85-55fa26fa3c8f_1974x1234.png)

Various LLM-as-a-Judge prompts (from [17])

Compared to human evaluation, LLM-as-a-Judge is a simple and scalable alternative that _i)_ reduces the need for human involvement in the evaluation process and _ii)_ enables faster model iterations. To perform evaluations with LLM-as-a-Judge, _all we need to do is write a prompt_! But, there are a few different structures of prompts that are commonly used (shown above):

1. _Pairwise comparison_: the judge is presented with a question and two model responses and asked to identify the better response.
    
2. _Pointwise scoring_[2](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-2-141159804): the judge is given a single response to a question and asked to assign a score; e.g., using a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) from one to five.
    
3. _Reference-guided scoring_: the judge is given a reference solution in addition to the question and response(s) to help with the scoring process.
    

Any of these techniques can be combined with [chain of thought (CoT) prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) to improve scoring quality. To do this, we can simply use a [zero-shot CoT prompt](https://arxiv.org/abs/2205.11916) by appending something like _“Please write a step-by-step explanation of your score”_ to the judge’s prompt. However, we should be sure to ask the LLM to output the rationale prior to its score (as opposed to afterwards), as recommended in [16][3](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-3-141159804).

> _“The conclusions generated by the model are not supported by the explanation generated afterward.”_ - from [16]

Having the LLM output a human-readable rationale along with its score is an easy and useful explainability trick. We can use these explanations to gain a deeper understanding of a model’s performance and shortcomings; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24296f20-910e-4693-b68b-9904d70b4ea2_1608x1478.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24296f20-910e-4693-b68b-9904d70b4ea2_1608x1478.png)

(from [17])

**Which setup should we use?** Each of these scoring strategies has pros and cons—_no one approach is best_. For example, pairwise comparison is not scalable, as it requires every combination of model outputs be compared to each other. However, pointwise scoring tends to be [less stable](https://x.com/aparnadhinak/status/1748368364395721128), as it expects the judge to possess a relatively consistent internal scoring mechanism—_absolute scores are much more likely to fluctuate compared to pairwise comparisons_. Typically, the style of LLM-as-a-Judge evaluation that should be used depends upon the details of our application. We don’t always have two models to compare, so pointwise scoring might make the most sense in those cases and vice versa.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71dab9f1-ac95-4b0d-b497-ede7b157a709_1614x500.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71dab9f1-ac95-4b0d-b497-ede7b157a709_1614x500.png)

(from [17])

**Does it work well?** In [17], we clearly see that strong LLM judges like GPT-4 can very accurately measure human preferences. In fact, GPT-4 is found to achieve an 80% agreement rate with human preference scores, _which matches the agreement rate that human annotators have with themselves_; see above. This ability of LLM judges to accurately predict human preferences should come as no surprise—most modern LLMs are extensively finetuned on human preference data.

#### Biases (and how we can avoid them…)

> _“We identify biases and limitations of LLM judges. However, we… show the agreement between LLM judges and humans is high despite these limitations.”_ - from [17]

Although LLM-as-a-Judge can accurately predict human preferences, this evaluation strategy is not perfect—_it introduces several new sources of bias into the evaluation process_. Already, we know that LLMs have a swath of limitations, such as questionable reasoning capabilities, sensitivity to minor changes in the prompt, and a tendency to generate verbose outputs. Many of these weaknesses lead to corresponding biases within LLM-as-a-Judge evaluations:

1. _Position bias_: the judge may favor outputs based upon their position within the prompt (e.g., the first response in a pairwise prompt).
    
2. _Verbosity bias_: the judge may assign better scores to outputs based upon their length (i.e., longer responses receive higher scores).
    
3. _Self-enhancement bias_: the judge tends to favor responses that are generated by itself (e.g., GPT-4 assigns high scores to its own outputs).
    

Beyond the sources of bias outlined above, LLM judges tend to have difficulties with grading responses to questions that they struggle to answer themselves; e.g., complex reasoning and math questions. Additionally, _judges are easily mislead by incorrect information in their context_ [18]. If one of the responses being graded is incorrect, the judge may be misled by this context and output an inaccurate score.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91d0ad84-ae69-47c6-b609-06a128f9c754_1616x572.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91d0ad84-ae69-47c6-b609-06a128f9c754_1616x572.png)

(from [17])

**Digging deeper into bias.** Beyond the sources of bias outlined in [17], there are many other works—_some of which we will explore later in this overview_ —that have deeply analyzed bias in LLM-as-a-judge evaluations. A list of these works has been provided below for easy reference and further reading:

- Humans or LLMs as the Judge? A Study on Judgement Biases [[link](https://arxiv.org/abs/2402.10669)]
    
- Evaluation Biases for Large Language Models [[link](https://arxiv.org/abs/2307.03025)]
    
- Cognitive Biases in Large Language Models as Evaluators [[link](https://arxiv.org/abs/2309.17012)]
    
- Large Language Models are Inconsistent and Biased Evaluators [[link](https://arxiv.org/abs/2405.01724)]
    
- Large Language Models are Not Yet Human-Level Evaluators [[link](https://arxiv.org/abs/2305.13091)]
    

**How can we reduce bias?** To lessen the impact of biases on the results of LLM-as-a-Judge evaluations, there are several techniques that we can use:

- Randomizing the position of model outputs within the prompt, generating several scores, and taking an average of scores with different positions—we will refer to this as the _position switching trick_.
    
- Providing [few-shot examples](https://cameronrwolfe.substack.com/i/143156742/basic-prompting-strategies) to demonstrate the natural distribution of scores and help with calibrating the judge’s internal scoring mechanism.
    
- Providing correct answers to difficult math and reasoning questions within the prompt as a reference for the judge during the evaluation process.
    
- Using several different models as a judge (e.g., Claude, Gemini and GPT-4) to lessen the impact of self-enhancement bias.
    

Although these techniques are useful, LLM-as-a-judge is a flawed metric—[as are all metrics](https://en.wikipedia.org/wiki/All_models_are_wrong)—that will never be perfect. As such, we should always be aware of these biases and consider how they impact our analysis. Think about the model(s) being evaluated, what we are trying to measure, how the evaluation is set up, and in what ways the underlying judge could skew the results of this evaluation.

## Early Work on LLM Evaluations

Prior to the proposal and analysis of LLM-as-a-Judge in [17], a variety of earlier works studied similar techniques. These studies began with the proposal of GPT-4, which was the first LLM that was powerful enough to evaluate text quality. As we will see, this approach quickly caught fire and began spreading through the community due to its ease of use, generality, and effectiveness.

#### **[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) [1]**

To make LLM-powered evaluations possible, we first need access to a sufficiently powerful LLM that can reliably evaluate the output of other models. Although prior models were indeed impressive, we did not have access to such a model until the proposal of GPT-4. Immediately after the proposal of this model, however, researchers began to explore the feasibility of LLM evaluators!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff454ab5a-e881-4427-bf25-a7a6298aeb91_1794x1078.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff454ab5a-e881-4427-bf25-a7a6298aeb91_1794x1078.png)

(from [1])

**How good is GPT-4?** The first work [1] to explore the use of GPT-4 as an evaluator—_published less than ten days after the model’s release_[4](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-4-141159804)—was not focused upon evaluation. Rather, this work had a more general goal of exploring the capabilities of GPT-4, resulting in a 155-page analysis that spans a (shockingly) wide number of topics:

- Proficiency in solving coding and math problems.
    
- Using tools and interacting with humans; e.g., [theory of mind problems](https://arxiv.org/abs/2302.02083), or the model’s ability to explain its outputs to humans.
    
- Drawing basic figures/pictures using [TikZ](https://www.overleaf.com/learn/latex/TikZ_package), generating useful plots, and performing more general data analysis.
    
- Proving mathematical theorems, or even doing this while rhyming every line of the proof; see above.
    

The conclusion of this analysis is that GPT-4 excels at virtually all tasks that are considered and is a substantial improvement over ChatGPT. In fact, authors observe that GPT-4’s outputs are indistinguishable (or better than) those of humans in many cases and even go as far as saying that GPT-4 is a significant step towards artificial general intelligence (AGI); see below.

> _“The combination of the generality of GPT-4’s capabilities… and its performance on a wide spectrum of tasks at or beyond human-level makes us comfortable with saying that GPT-4 is a significant step towards AGI.”_ - from [1]

Although authors’ claim that GPT-4 demonstrates signs of (general) intelligence was—_and continues to be_—highly controversial, disagreements around LLM capabilities and progress toward AGI oftentimes boil down to a lack of (or difference in) rigorous definitions of these concepts. _How can we measure intelligence without first defining it_[5](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-5-141159804)_?_ Luckily, we don’t have to worry about defining AGI for the purposes of this post. All we need to know is that GPT-4 is a very capable model and that these capabilities open up a lot of doors when it comes to using LLMs to evaluate the outputs of other LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f43b9cb-7e1a-49d8-8596-47b910df1d74_1288x1152.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f43b9cb-7e1a-49d8-8596-47b910df1d74_1288x1152.png)

(from [1])

**GPT-4 as an evaluator.** Authors in [1] are the first to explore the use of GPT-4 as a judge, but their analysis is actually quite brief (i.e., less than one page in total)! After addressing the limitations of existing evaluation metrics in judging the similarity of statements, authors evaluate the ability of GPT-4—_using the prompt shown above_—to judge the similarity of a model’s response to a reference answer. Specifically, GPT-4 is tasked with determining whether a model’s response is more similar to a reference answer or an answer generated with GPT-3. Two responses to a statement are provided to the judge, which then identifies the option that is a better reflection of the original statement.

> _“[GPT-4] can determine which answer in a pair is closer to the gold answer, and this determination reasonably aligns with a human performing the same task.” -_ from [1]

From this analysis, we can see that GPT-4 can judge semantic similarities between statements much better than simple metrics like ROUGE or BLEU. To improve the quality of these evaluations, authors ask GPT-4 to list the pros and cons of each response before identifying the preferred output. When we compare the judgements made by GPT-4 to that of humans, we see some significant differences. For example, GPT-4 prefers GPT-4-generated responses in 87.76% of cases compared to 47.61% of cases for human evaluators; see the table below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa13a2281-feb3-4259-bf39-01f479db693d_1916x436.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa13a2281-feb3-4259-bf39-01f479db693d_1916x436.png)

(from [1])

GPT-4’s level of agreement with human evaluators is low—_slightly above 50%_—and we already see some clear indications of bias in [1]; e.g., longer responses are scored better by GPT-4. This lack of alignment may be due to differences in the evaluation setup—GPT-4 is forced to choose a winner between the two responses, while humans can select a tie. However, the level of agreement between GPT-4 and humans is still surprisingly low, leading authors in [1] to conclude that more research is necessary to calibrate GPT-4’s evaluation capabilities.

#### **[Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/) [2]**

> _“Evaluating chatbots is never a simple task. With recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework.”_ - from [2]

Vicuna is an open-source chatbot created by finetuning [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)-13B over a set of user conversations with ChatGPT—_collected from [ShareGPT](https://sharegpt.com/)_. We have covered the details of Vicuna in a [prior overview](https://cameronrwolfe.substack.com/i/114077195/vicuna-an-open-source-chatbot-with-chatgpt-quality). However, Vicuna is relevant to this overview due to the fact that authors chose to primarily evaluate the quality of this model’s output automatically with GPT-4; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3478758a-f597-4bee-a07c-0dd4e3cda9b2_1024x458.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3478758a-f597-4bee-a07c-0dd4e3cda9b2_1024x458.png)

(from [2])

In fact, Vicuna was one of the first works to perform such LLM-powered evaluations, which received backlash[6](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-6-141159804) at the time. However, the use of GPT-4 for evaluation purposes in [2] led to a wave of analysis of this technique because:

1. The evaluation results seemed to be relatively consistent and promising!
    
2. Using GPT-4 as an evaluator is a reference-free, automatic evaluation strategy that can be applied to any task (i.e., very generic and simple).
    
3. GPT-4 can output a rationale along with its evaluation, which improves the human interpretability of evaluation results.
    

**Evaluation setup.** The questions used to test Vicuna span eight categories; see [here](https://github.com/lm-sys/vicuna-blog-eval/blob/main/eval/table/question.jsonl). There are 80 questions in total, which are written using the help of GPT-4. Authors observe that GPT-4 can generate challenging questions for state-of-the-art chatbots via careful prompt engineering. To evaluate the answers of different chatbots on these questions, the output from each model is collected and GPT-4 is asked to rate generated outputs in terms of helpfulness, relevance, accuracy, and detail. The prompts used for evaluation, which are surprisingly simple, can be seen [here](https://github.com/lm-sys/vicuna-blog-eval/blob/main/eval/table/prompt.jsonl). Separate, more specific prompts are used for evaluating coding and math tasks due to the difficulty of such evaluations; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6744535-5ffb-42a0-89bf-af2e30e3bbfb_2294x920.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6744535-5ffb-42a0-89bf-af2e30e3bbfb_2294x920.png)

Prompts for GPT-4 evaluations

GPT-4 is asked to rate the quality of two responses from different models on a scale from one to ten and provide an explanation for these scores. Producing such an explanation via CoT prompting[7](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-7-141159804) tends to improve the accuracy of GPT-4’s ratings, as well as provides a human-readable rationale to explain the rating. There are several other useful observations we can make about these prompts:

- The general prompt asks GPT-4 to avoid bias—_including positional bias_—when generating scores, revealing that authors in [2] experienced issues with positional bias. Later work [8] confirmed that Vicuna’s evaluation strategy has strong positional bias, but this issue can be solved via position switching.
    
- All prompts explicitly specify output format so that ratings can be easily and automatically parsed from the response. Such an approach is only viable with a powerful instruction following model like GPT-4.
    
- Prompts for coding and math evaluation are more detailed compared to the prompt used for general questions and introduce more in-depth, problem-specific details to improve the quality of ratings on such questions.
    
- Coding and math prompts use a few different tricks to solicit better ratings, such as asking GPT-4 to critique provided solutions or solve the problem itself before providing a rating.
    

**Vicuna’s unique approach.** Rating two responses in each prompt is a standard practice that is commonly used today, as it allows for better relative comparisons between models—_we can easily ask GPT-4 to explain which of the two models provides a better response for a given question_. However, it’s important to note that Vicuna’s setup is slightly different than standard pairwise comparison. Rather than asking the model for the preferred response, the model is prompted to assign a score to each example, and the preferred example is determined based on these scores. This is another valid LLM-as-a-Judge setup that has been used by several papers.

**Does this work well?** Although GPT-4 struggles with grading coding and math questions, the results of these evaluations are relatively consistent and come with detailed explanations! Using these evaluations, authors observe that Vicuna is preferred to other open-source models for 90% of questions, and Vicuna is rated as better than or equal to ChatGPT in 45% of cases; see below. However, later work [8] reveals that these evaluations are biased towards longer outputs and (somewhat) poorly correlated with human preferences. So, we should be aware of these biases and shortcomings when interpreting these results.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf5b689-1008-4659-9ace-8b1e7b248a53_3950x2272.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcf5b689-1008-4659-9ace-8b1e7b248a53_3950x2272.png)

Response comparison from GPT-4 (from [2])

More specifically, we see in [later work](https://arxiv.org/abs/2305.15717) that imitation models like Vicuna match the style of powerful models like ChatGPT but lack their factuality and knowledge base. Nonetheless, the analysis in [2] is useful because _i)_ we see that response style and instruction-following capabilities of open-source models can be improved by finetuning on outputs from a more powerful model and _ii)_ the viability of LLM-powered evaluations is more clearly demonstrated.

> _“While this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach… Developing a comprehensive evaluation system for chatbots remains an open question.”_ - from [2]

**Looking forward.** Despite the nascent nature of GPT-4 evaluations in [2], this work lays a powerful foundation for subsequent analysis of LLM-powered evaluations. Later work provides a variety of useful analyses of the strengths and weaknesses of LLM-as-a-Judge techniques, but the prompts that are used to generate reliable scores tend to be (relatively) similar to what we see in [2]! In fact, the LLM-as-a-Judge publication itself [17] is written by the same same authors as Vicuna and heavily utilizes many of the techniques that are proposed in [2]. Plus, the set of 80 questions used to evaluate Vicuna is widely used in other papers.

#### [AlpacaEval: An Automatic Evaluator of Instruction-Following Models](https://github.com/tatsu-lab/alpaca_eval) [8]

AlpacaEval [8], originally proposed in mid-2023, is one of the most popular LLM-based, automated evaluation metrics (and [leaderboards](https://tatsu-lab.github.io/alpaca_eval/)) for instruction-following language models. The evaluation strategy—_based upon [AlpacaFarm](https://crfm.stanford.edu/2023/05/22/alpaca-farm.html) [9], a simulator that uses LLM evaluators to automate the creation of [RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)-style, pairwise preference labels_[8](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-8-141159804)—uses a fixed set of 805 instructions that span a comprehensive set of simple, assistant-style tasks; see [here](https://huggingface.co/datasets/tatsu-lab/alpaca_eval). For each instruction, we generate output with two LLMs—_a baseline model and the model being evaluated_. Then, an LLM evaluator is used to rate the quality of each model’s output (i.e., pairwise setup), allowing a win-rate between the two models’ outputs to be calculated.

[

![LC AlpacaEval is the most highly correlated benchmark with Chat Arena.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b6ee6b1-d964-4d6b-b987-59ce34be550d_3824x1356.png "LC AlpacaEval is the most highly correlated benchmark with Chat Arena.")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b6ee6b1-d964-4d6b-b987-59ce34be550d_3824x1356.png)

(from [10])

**Why is this useful?** The goal of AlpacaEval is to create a fast and cheap automatic evaluation pipeline that is highly correlated with human preferences. The current iteration of AlpacaEval runs in less than three minutes, costs less than $10[9](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-9-141159804), and has a 0.98 Spearman correlation with human evaluation (taken from the Chatbot Arena); see above. In comparison, performing human evaluation is subject to noise and disagreements, is significantly more expensive, and may require several weeks of annotation time. Because AlpacaEval is so efficient, this metric is perfect for model development—_it provides a reliable proxy for human evaluation of simple instruction-following tasks that is quick and cheap to compute_.

**How does the evaluator work?** Prompts used for the evaluator in AlpacaEval are shown in the figure below[10](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-10-141159804). These prompts use a [chat template structure](https://huggingface.co/docs/transformers/main/en/chat_templating), which matches the style of inputs used by most [proprietary chat completion APIs](https://platform.openai.com/docs/api-reference/chat) and is used to distinguish roles and messages within a multi-turn conversation. For every instruction, a pair of responses is passed to the evaluator as shown below, and we receive the preferred output—_either a binary response or the [logprobs](https://cookbook.openai.com/examples/using_logprobs) of each option from the LLM_—as a response. This response represents the probability that the response of the model being evaluated is better than that of the baseline model for a given instruction within the dataset. By taking an average of these probabilities over the full dataset, we can compute a win-rate, which measures the ratio of time a model’s output is preferred over that of a baseline model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c543af-2299-4e8a-81a4-1210ca8222d3_2082x1384.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c543af-2299-4e8a-81a4-1210ca8222d3_2082x1384.png)

Evaluator prompt in AlpacaEval ([source](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/alpaca_eval_gpt4/alpaca_eval.txt))

The quality of evaluators is verified by measuring agreement with a set of 2.5K human evaluations, which are available [here](https://huggingface.co/datasets/tatsu-lab/alpaca_eval/blob/main/alpaca_farm_human_crossannotations.json). However, several other factors (e.g., cost and latency) are also relevant when selecting the best evaluator for any given use case. Originally, AlpacaEval generated a single preference response with a temperature of zero for each instance within the dataset. However, the quality of automated preference annotations was improved in later iterations by:

- Randomizing the position of model outputs within the prompt (or sampling multiple preference scores for each possible position of model outputs).
    
- Measuring the logprobs[11](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-11-141159804) of each response instead of generating a binary preference response.
    
- Using a better model (GPT-4-Turbo) as the evaluator.
    

Authors also revised and simplified the core evaluator’s prompt by shortening the instructions and only outputting a single token in the response; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a4105e1-cd2c-4d11-924f-4fc615e83f9e_2454x1330.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a4105e1-cd2c-4d11-924f-4fc615e83f9e_2454x1330.png)

Evaluator prompt in AlpacaEval-2.0 ([source](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/alpaca_eval_clf_gpt4_turbo/alpaca_eval_clf.txt))

**Mitigating length bias.** As we have seen, using LLMs as evaluators can cause several subtle sources of bias to be introduced to the evaluation process. We must be cognizant of these biases and do our best to remove or account for them.

> _“What would the AlpacaEval metric be, if the outputs of all models had the same length as those of the baseline?”_ - from [10]

One known and prevalent bias of LLM evaluators is towards longer outputs (i.e., verbosity bias)—_certain proprietary LLMs (e.g., GPT-4 or GPT-4-Turbo) tend to prefer longer outputs to shorter ones_. As a result, AlpacaEval may score longer outputs better than shorter ones given fixed, comparable, or even worse content quality.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf70e575-31a9-4d29-9921-15d657db5400_1862x744.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf70e575-31a9-4d29-9921-15d657db5400_1862x744.png)

(from [10])

To combat this bias, researchers extended the AlpacaEval metric [10] with a simple, regression-based debiasing process. In particular, a linear regression model is trained that takes three attributes (shown above) as input: _i)_ the model[12](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-12-141159804), _ii)_ instruction difficulty, and _iii)_ normalized output length. Once this model has been trained, we can “zero out” the contribution of terms that are believed to have spurious correlations with output quality, leaving only the true quality score. In the case of [10], we just remove the length term from the regression and compute a win rate per usual, yielding a length-controlled AlpacaEval score.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc6cb48-7941-44c3-b6d3-039a231acdeb_1614x700.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fc6cb48-7941-44c3-b6d3-039a231acdeb_1614x700.png)

(from [10])

Length-controlled AlpacaEval—_now used in the public leaderboard_—is found in [10] to be less game-able than the standard metric. In other words, _we cannot significantly change the results of length-controlled AlpacaEval by just asking a model to be more or less verbose._ As shown in the table above, the win rate of AlpacaEval can drastically change depending on the model’s verbosity. The length-controlled version of AlpacaEval is found to better correlate with human ratings, improving the Spearman correlation of AlpacaEval with Chatbot arena from 0.94 to 0.98.

#### Other Early Usage of LLM-Powered Evaluations

Shortly after the proposal of Vicuna, using GPT-4 as an evaluator became increasingly common. At this time, little analysis had been done to prove the reliability of LLM-powered evaluations. However, there are several notable factors that made this style of evaluations so popular:

- The implementation is easy—_just a prompt and an API call_!
    
- The open-ended nature of LLM outputs makes evaluation with traditional/automatic metrics very difficult (e.g., ROUGE or BLEU)[13](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-13-141159804).
    
- Human evaluation—_our source of ground truth for evaluating LLMs_—is noisy, expensive, and time consuming.
    

The research community needed a more accessible evaluation strategy that _i)_ could reliably measure performance across a wide number of tasks and _ii)_ allowed us to experiment and iterate more quickly. As we will see in the next few papers, LLM-based evaluations quickly began to fill this gap by providing researchers with an automatic, reference-free metric that enabled fast model iterations.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff004f2d3-f0de-4a6e-a3da-b88f9da8c1d8_1610x644.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff004f2d3-f0de-4a6e-a3da-b88f9da8c1d8_1610x644.png)

(from [3])

**LIMA: Less is More for Alignment.** LIMA [3] studies our ability to align pretrained language models (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)) using [supervised finetuning](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) with a limited amount of data. Interestingly, authors observe that only 1,000 curated finetuning examples are sufficient for achieving remarkably strong performance. These results suggest that LLMs learn most of their knowledge during pretraining, while finetuning optimizes the format of the model’s output. This phenomenon is referred to as the Superficial Alignment Hypothesis; see below.

> _“We define the Superficial Alignment Hypothesis: A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used.”_ - from [3]

Both human and LLM-powered evaluation are used in [3]. A single response for each model being tested is generated for every prompt. Humans are then asked to compare the outputs of LIMA (anonymously) to all baseline models by picking a preferred response; see below. This evaluation process can be automated by:

1. Providing the same exact prompt to GPT-4.
    
2. Asking the model to pick a preferred response.
    

In both human and model-based evaluations, LIMA is found to outperform prior open models like [Alpaca](https://cameronrwolfe.substack.com/i/114077195/alpaca-an-instruction-following-llama-model), despite being finetuned on significantly less data. LIMA also outperforms GPT-3.5 and matches or exceeds GPT-4’s performance on a decent number of test prompts—_34% to 43% of prompts in particular_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f458f22-021a-45f7-8f99-8f654b2a3ca0_1614x704.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f458f22-021a-45f7-8f99-8f654b2a3ca0_1614x704.png)

(from [3])

Although pairwise comparison is used in most experiments, authors also explore using GPT-4 (or GPT-3.5 in some ablation experiments) to score the helpfulness of model responses in a pointwise fashion on a six point Likert scale; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96de3206-8d48-412f-b31a-a6f81b9df2df_1616x1082.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96de3206-8d48-412f-b31a-a6f81b9df2df_1616x1082.png)

(from [3])

**Guanaco.** Authors in [4] propose quantized low-rank adaptation (Q-LoRA), a parameter-efficient training strategy that makes finetuning LLMs much easier on commodity hardware (i.e., consumer GPUs with less memory). See [this overview](https://cameronrwolfe.substack.com/i/138861994/lora-variants-there-are-a-ton) for more details on Q-LoRA. The main benefit of training LLMs with Q-LoRA is the reduced memory consumption. We even see in [4] that an LLM with 65 billion parameters can be finetuned using Q-LoRA with a single 48Gb GPU!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F206e6120-4a88-4aee-8dc5-627e6afc83e9_1610x780.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F206e6120-4a88-4aee-8dc5-627e6afc83e9_1610x780.png)

(from [4])

Authors in [4] use Q-LoRA to train the Guanaco suite of chatbot-style, open LLMs. These models are evaluated using both humans and GPT-4. Interestingly, GPT-4 is found to provide meaningful and reliable performance metrics, whereas legacy benchmarks do not provide accurate measures of chatbot performance.

> _“GPT-4 evaluations are a cheap and reasonable alternative to human evaluation… we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots.”_ - from [4]

Two styles of LLM-based evaluations are used in [4]. In the first setup, GPT-4 is prompted with a response from both ChatGPT and another model and asked to:

- Assign a score in the range `[1, 10]` to both responses.
    
- Provide an explanation for these scores.
    

Notably, this setup exactly matches the automatic evaluation strategy used by Vicuna [2]. From here, the performance of a model is reported relative to that of ChatGPT. More specifically, we measure the ratio between the total sum of scores achieved by each model and the total score of ChatGPT; see below. Guanaco models are found to achieve impressive performance relative to ChatGPT.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99c23183-84a8-463a-a546-38970c529fa3_1608x1080.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99c23183-84a8-463a-a546-38970c529fa3_1608x1080.png)

In the second setup, GPT-4 performs direct comparisons between model outputs. These comparisons are presented to GPT-4 as a three-class labeling problem; see below. GPT-4 is prompted to either pick the better response or declare a tie between the two responses, as well as provide a detailed explanation for its choice. Using this approach, authors in [4] conduct head-to-head comparisons between Guanaco, ChatGPT, and other relevant baseline models. Interestingly, we see in [4] that GPT-4 demonstrates a clear position bias towards the first response in the prompt, which is eliminated using the position switching trick.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53167fa0-b9b3-44ea-b566-728abd97d726_2106x1104.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53167fa0-b9b3-44ea-b566-728abd97d726_2106x1104.png)

Example prompt for pairwise comparison (with ties)

**The False Promise of Imitating Proprietary LLMs.** In the wake of [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) and the [many finetuned LLMs](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms) that were created as a result of this model, these was a lot of momentum surrounding open LLMs. At the time, many finetuned versions of LLaMA were trained using an imitation strategy—_we generate responses to a diverse and large set of prompts with a more powerful model (e.g., ChatGPT) and directly finetune the open model over this data_[14](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-14-141159804). These models seemed to perform very well, indicating that the gap in performance between open and closed LLMs might [quickly disappear](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). In [5], however, more targeted evaluations paint a clearer, sobering picture of the performance of these open imitation models.

> _“Initially, we were surprised by the output quality of our imitation models… When conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT.”_ - from [5]

Put simply, imitation models are great at mimicking the style of ChatGPT, which can easily trick human annotators into perceiving the model’s output as high quality. However, these models lack the factuality of more powerful LLMs like ChatGPT, which is revealed via more extensive evaluation; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35a43271-5cf8-4283-9612-2cd7719f2516_1072x874.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35a43271-5cf8-4283-9612-2cd7719f2516_1072x874.png)

(from [5])

Given the emphasis of [5] upon rigorous evaluation, we might find it interesting that authors also leverage GPT-4 as a judge for part of their analysis! First, they finetune a variety of LLaMA models using:

1. Varying amounts of imitation data.
    
2. Different sizes of base models.
    

Evaluation of these models is then performed by asking both humans and GPT-4 for feedback. For LLM-as-a-Judge evaluations, authors adopt the same strategy proposed by Vicuna [2], where the quality of model outputs is judged by via a pairwise prompt to GPT-4. GPT-4 is given a response from both ChatGPT and an imitation model, then asked to output a preference ranking between these outputs. The prompts used for evaluation with GPT-4 exactly match the prompts given to humans for evaluation. As shown below, similar trends in performance are observed for both human and LLM evaluation, where we see that increasing the size of the base model is more beneficial than collecting more imitation data.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdab7faf5-547e-42e6-9964-722ad129c611_1232x1166.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdab7faf5-547e-42e6-9964-722ad129c611_1232x1166.png)

(from [5])

Such a result provides further evidence that pairwise evaluation with GPT-4 is effective. GPT-4 can reliably predict win-rates between outputs generated by different LLMs (e.g., ChatGPT and Vicuna) that correlate well with ratings obtained via human annotation in an aggregate sense.

**Tülu.** Authors in [6] perform a large scale analysis of open instruction-following datasets. A range of LLaMA-based LLMs with sizes from 7 to 65B parameters are finetuned over each of these datasets. To address documented issues with insufficient or misleading evaluation of open LLMs, however, these finetuning experiments are accompanied with an extensive evaluation suite, thus providing a clear picture of whether open models are making legitimate progress towards the performance of proprietary models. The best model, called Tülu, is trained over a mixture of several different instruction-following datasets.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54a9965d-4d22-437e-8165-09981d310b45_1290x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54a9965d-4d22-437e-8165-09981d310b45_1290x1004.png)

(from [6])

To evaluate open-ended instruction following capabilities, the AlpacaEval [8] benchmark is adopted. In particular, GPT-4 is used to compute the win-rate—_using a pairwise prompting strategy_—of each model being tested against GPT-3.5-Turbo ([davinci-003](https://platform.openai.com/docs/deprecations)). To avoid position bias, authors adopt the position switching trick. As shown in the table above, Tülu models perform comparably to the baseline model but lag behind the performance of top proprietary models.

## The Popularization of LLM-as-a-Judge

After LLM-as-a-Judge evaluations were heavily adopted by early work on open LLMs, this strategy became increasingly common across the LLM research community as a whole. In this section, we will overview other major publications—_beyond the core paper that proposes and analyzes the LLM-as-a-Judge technique [17]_—that provide useful analysis and insights on LLM-as-a-Judge evaluations.

#### **[Can Large Language Models Be an Alternative to Human Evaluations?](https://arxiv.org/abs/2305.01937) [11]**

Human evaluation is the standard for reliably evaluating text quality, but that doesn’t mean that humans can perfectly evaluate text! As any practitioner who has tried to collect human-annotated data would know, human evaluation—_despite being incredibly valuable_—is a noisy, time intensive, and expensive process.

> _“This paper is **the first** to propose using LLMs as an alternative to human evaluation and show their effectiveness.” - from [11]_

With this in mind, we might wonder whether LLMs can be used as an alternative to human evaluation. To investigate this question, authors in [11] run a parallel study that uses both humans and LLMs to assess writing quality[15](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-15-141159804). Although such techniques were briefly explored (and used) in the papers we have seen so far, work in [11] was the first to rigorously analyze these techniques in comparison to human evaluation. Interestingly, _we see in [11] that LLMs can evaluate text quality consistently with humans when given the same instructions and examples_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1345c0e0-f582-4f25-a90a-a31f968f914c_1838x1204.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1345c0e0-f582-4f25-a90a-a31f968f914c_1838x1204.png)

(from [11])

**Human vs. LLM evaluation.** The task considered in [11] is open-ended story generation, which pairs short prompts with a story based on the prompt. Both human and LLM-written stories are evaluated. As shown in the figure above, given a particular story, we can ask both a human annotator and an LLM—_using the same instructions and sample given to the human_—to provide a rating for each story in our dataset. Then, we can compare the results of these evaluations to determine the level of correlation that exists between human and LLM evaluation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F278a8c8c-4f14-488a-8ba5-6464b399833b_2204x856.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F278a8c8c-4f14-488a-8ba5-6464b399833b_2204x856.png)

Human evaluation setup (from [11])

When evaluating a sample, the LLM is given the exact same inputs as a human evaluator. The model is asked to rate each story on a Likert scale from one to five (i.e., pointwise scoring), and the score is produced by freely generating text with the model—_we can just parse the score from the model’s response_. However, several different quality scores are considered when rating a story:

1. _Grammar_: How grammatically correct is the story’s text?
    
2. _Cohesiveness_: Do the sentences of the story fit well together?
    
3. _Likability_: Is the story enjoyable?
    
4. _Relevance_: Does the story match the prompt?
    

The setup for evaluating these characteristics with human evaluators is shown above. The corresponding prompts used for executing the same evaluations with an LLM are shown below, where each characteristic receives its own prompt.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78a97afd-e788-4dfe-a0ce-2af322830dc7_2254x928.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78a97afd-e788-4dfe-a0ce-2af322830dc7_2254x928.png)

LLM evaluation prompts (from [11])

**Does LLM evaluation work?** Given that story generation is a specialized task, authors in [11] hire expert human evaluators (i.e., English teachers) to evaluate story quality. The task of these annotators is to evaluate the quality of stories written for 200 prompts, where for each prompt we _i)_ sample a response from [GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) (i.e., a weaker LLM) and _ii)_ have a human write a response to the prompt. Our goal is to determine whether both human and LLM evaluators can detect a quality difference between these model and human-generated stories.

A few different LLMs are used for evaluation, including [T0](https://arxiv.org/abs/2110.08207) [12] and several GPT variants from OpenAI. When we compared the results of human and LLM evaluation, several interesting observations can be made:

- Weaker LLM evaluators (e.g., T0 and early GPT variants) struggle to detect a quality difference between GPT-2 and human-written stories.
    
- Both expert human evaluators and more powerful LLMs show a clear preference towards human-written stories.
    
- More recent models (e.g., ChatGPT) can both rate story quality accurately and provide insightful explanations for their scores.
    
- The most difficult characteristic to rate is likability, indicating that subjective characteristics are harder to evaluate for both humans and LLMs.
    

The results above show that sufficiently powerful LLMs can evaluate basic writing quality characteristics, but these results are evaluated in aggregate—_we test for differences in average scores of human and model-generated stories across the entire dataset_. When we evaluate whether humans and LLMs evaluate individual stories similarly, the results are not quite as clear. However, we do see a weak positive correlation between human and LLM-assigned quality scores; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2ef4b3-9030-4158-bb27-073283e89d8c_906x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae2ef4b3-9030-4158-bb27-073283e89d8c_906x446.png)

(from [11])

**The role of humans.** Given the results in [11], one might begin to wonder whether LLMs can fully automate human evaluation. There are many benefits of LLM evaluation, such as reproducibility, ease of use, cost, and efficiency. Plus, certain aspects of human data annotation are [ethically questionable](https://www.washingtonpost.com/world/2023/08/28/scale-ai-remotasks-philippines-artificial-intelligence/), and LLM-powered evaluations could (potentially) help us to avoid some of these downsides. But, humans are still highly necessary to continuously monitor the quality of LLM evaluations and detect inaccuracies or drift. _We cannot ever be fully confident in LLM evaluations due to the various sources of bias that this approach introduces._

> _“We recommend using LLM evaluation as a cheap and fast quality judgment, while human evaluation is best used to collect feedback from humans prior to deploying the system in real-world applications.”_ - from [11]

Both human and LLM evaluations have limitations. Therefore, _using them in tandem is the best option_. Together, these tools can be used to more reliably scale the evaluation process by allowing human experts to be more effective and accurate in their role. We use LLM evaluations to iterate quickly and test new ideas during model development. Human experts oversee the evaluation process, interpret the results, suggest improvements, and increase reliability over time.

#### **[G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634) [13]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec0dce9-cdda-4fd0-903c-0c09d93d5bce_2298x890.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec0dce9-cdda-4fd0-903c-0c09d93d5bce_2298x890.png)

(from [14])

Authors in [13] propose one of the first LLM-powered evaluation techniques that is shown to be highly correlated with human evaluation results. This work builds upon prior research in [13] and [14] that uses an LLM to score text based on the probability assigned to that text by the LLM; see above. The idea behind this work is intriguing, but it assumes that the LLM evaluator assigns high probability to high-quality text, which is not always true. As such, these techniques tended to be unreliable and produced scores that correlate poorly with human evaluation, warranting a more systematic investigation of similar techniques in [13].

> _“We present G-EVAL, a framework of using LLMs with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs.”_ - from [13]

Compared to prior work, the technique proposed in [13]—_called G-Eval_—makes two major changes:

1. Combining LLM evaluation with a new style of chain-of-thought (CoT) prompting, called Auto-CoT.
    
2. Using a form filling paradigm (i.e., prompting the LLM to output a quality score) instead of measuring the probability of a textual sequence.
    

These two changes are found to be highly impactful, allowing G-Eval to outperform a variety of baseline evaluation techniques by a large margin and reach an acceptable level of correlation with human evaluation scores.

**G-Eval** uses a two-step generation process to evaluate a sequence of text; see below. First, the LLM is given a task description and a set of criteria by which to evaluate the task. Using a CoT prompt, the LLM is then asked to generate a sequence of steps to be used for evaluation. This set of evaluation steps is used as an additional input for the model when the LLM is tasked with outputting a score for a given task example (e.g., an article-summary pair).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29508043-615a-4de4-9808-c313f396f174_1240x914.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29508043-615a-4de4-9808-c313f396f174_1240x914.png)

(from [13])

The figure above provides an example of the prompts used to apply G-Eval for evaluating summary coherence. This process of generating a description of steps to be used by the LLM for evaluation is referred to as Auto-CoT. This approach differs from standard CoT prompting by asking the model to output a generic sequence of evaluation steps for a task rather than just providing a more detailed explanation for each individual score that is produced.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e574250-fe89-4417-8362-903681e84526_1242x672.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e574250-fe89-4417-8362-903681e84526_1242x672.png)

Weighted scoring strategy (from [13])

**Weighted scoring.** Instead of having the LLM directly output a quality score, we see in [13] that more reliable scores can be generated via a weighted average; see above. Here, we just measure the probability (e.g., using logprobs in the OpenAI API) associated with each score, then use these probabilities to compute our final score as a weighted average. If we are evaluating a sequence of text on a Likert scale from one to five, we simply _i)_ find the probability of each score, _ii)_ multiply each score by its probability, and _iii)_ take a sum over all weighted scores.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a36bf5c-cb54-4640-a863-46bfdf1d9144_1538x902.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a36bf5c-cb54-4640-a863-46bfdf1d9144_1538x902.png)

(from [13])

**Empirical analysis.** G-Eval is tested on both text summarization and dialogue generation tasks in [13], where it is found to outperform a variety of [reference-based and reference-free](https://cameronrwolfe.substack.com/i/144374854/how-can-we-evaluate-a-summary) baseline metrics; see above. The Auto-CoT strategy used by G-Eval is found to improve the quality of LLM evaluation by providing more context and guidance to the underlying model during the scoring process. But, G-Eval does have a few notable limitations:

1. Sensitivity to the exact prompt and instructions being used.
    
2. Measurable bias towards LLM-generated texts (i.e., self-enhancement bias).
    
3. The LLM usually outputs integer scores and tends to be biased towards a single number within the grading scale (e.g., a score of three might be the most common output within a Likert scale from one to four).
    

Nonetheless, G-Eval is one of the earliest papers to show that LLMs can evaluate text in a reliable and useful manner. When using GPT-4 as an evaluator, _G-Eval achieves a [Spearman correlation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) of 0.514 with humans ratings on summarization tasks_, which is a significant leap compared to prior work on this topic!

#### [Large Language Models are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) [16]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed96784b-9c86-47e7-af90-fd532352979f_1646x832.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed96784b-9c86-47e7-af90-fd532352979f_1646x832.png)

Positioning of examples within an LLM evaluation prompt

Assume we are using an LLM evaluator to compare the quality of two model outputs generated in response to some prompt. To do this, we would usually pass both outputs to the LLM along with the source instruction and ask the model to identify the better response (i.e., pairwise scoring). However, we must select a position for each of the outputs within the prompt; see above. Although this may seem like an arbitrary choice, we see in [16] that this positioning can drastically impact evaluation results—_most LLM evaluators have a strong position bias_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d3751b9-2d87-49d2-bcf4-99e876c91b7c_902x718.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d3751b9-2d87-49d2-bcf4-99e876c91b7c_902x718.png)

(from [16])

**How bad is position bias?** To quantify the impact of position bias on LLM evaluations, authors in [16] conduct an in-depth study with both ChatGPT and GPT-4. Interestingly, both models demonstrate a clear position bias. However, GPT-4 tends to prefer the first response in the prompt, while ChatGPT favors the second response. These models are used to evaluate the quality of outputs from ChatGPT, Vicuna-13B, and Alpaca-13B in a pairwise fashion; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae48accb-a82e-4353-bd16-282c849f49d1_2442x718.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae48accb-a82e-4353-bd16-282c849f49d1_2442x718.png)

(from [16])

In the table above, we see that evaluation results change significantly depending on the position of outputs within the prompt. In fact, the win-rate of Vicuna-13B over ChatGPT goes from 2.5% to 82.5% when its outputs are switched from the first to second position. In other words, _the results of LLM evaluation are entirely dependent upon the position of a model’s output within the prompt_! Notably, GPT-4’s positional bias is less pronounced compared to ChatGPT and tends to be less severe when there is a clear difference in quality between model outputs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F369212a0-cfb0-4f60-84e6-e3ffc666a648_1348x900.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F369212a0-cfb0-4f60-84e6-e3ffc666a648_1348x900.png)

(from [16])

**What can we do about it?** We learn about two useful calibration tactics for eliminating position bias in [16]:

1. _Multiple-Evidence Calibration_: asking the model to generate evidence (i.e., an explanation or rationale similar to CoT prompting) prior to outputting a final score can improve evaluation quality.
    
2. _Balanced Position Calibration_: generating evidence and a score several different times for the same example[16](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-16-141159804), switching (or randomly selecting) the position of model responses each time.
    

The balanced position calibration approach (shown above) is very similar to the position switching trick proposed in [17]. Using these strategies in tandem significantly decreases the positional bias of LLM evaluators; see below. However, selecting an appropriate temperature is extremely important for techniques like Multiple Evidence Calibration that rely upon the generation of several outputs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95db92e-3c20-49d6-ad81-080fff7707cb_1744x1116.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95db92e-3c20-49d6-ad81-080fff7707cb_1744x1116.png)

(from [16])

#### Specialized Judges and Synthetic Data

Although we have seen a lot of papers so far, LLM-as-a-judge is an incredibly popular technique that has catalyzed a wide scope of different research topics. Two of the most interesting areas of related research are:

- Finetuning custom LLMs for evaluation.
    
- Using LLM-as-a-Judge to generate synthetic data.
    

We provide a brief overview of these topics and relevant papers below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa3394f-410c-4719-8c17-31d18d12470f_1616x1608.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa3394f-410c-4719-8c17-31d18d12470f_1616x1608.png)

(from [19])

**Training specialized LLM judges.** Most of the papers we have seen in this overview use proprietary models as the judge. However, we can also finetune our own LLM judge! The most notable example of this approach is Prometheus [19, 20] (shown above), but numerous papers have been published on this topic [21, 22, 23]. The main impediment to training custom evaluators was the quality of the base model. However, the release of [LLaMA-3](https://llama.meta.com/llama3/) seems to have largely removed this impediment, making it more possible to use open LLMs as evaluators.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff631bf2-7bdd-4be8-9083-5f5982f53493_1180x658.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff631bf2-7bdd-4be8-9083-5f5982f53493_1180x658.png)

(from [24])

**Reinforcement Learning from AI Feedback (RLAIF).** As we have seen, LLM judges can accurately predict human preferences. [Reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)—_the most commonly-used algorithm for training LLMs on human preference data_—relies upon large datasets of human preference pairs. As a result, it might not be a surprise that using LLM-as-a-Judge-style prompts to collect synthetic preference data can be beneficial; see above. This approach has been explored by several papers [24, 25] and is [rumored](https://www.interconnects.ai/p/llm-synthetic-data) to be heavily used within top industry labs; see below for a more extensive writeup.

[More on RLAIF](https://cameronrwolfe.substack.com/p/rlaif-reinforcement-learning-from)

## Practical Takeaways

If we learn nothing else from this overview, we should remember the following about LLM-as-a-Judge evaluations:

- The approach is general, reference-free, and applicable to (nearly) any task.
    
- Implementing LLM-as-a-Judge is simple—_it just requires a prompt_.
    
- LLM-as-a-Judge evaluations are cheap and quick, making them perfect for increasing iteration speed during model development.
    
- Correlation with human preferences is generally good.
    
- Several sources of bias exist that make this metric imperfect, so we should be sure to use LLM-as-a-Judge in tandem with human evaluation.
    

Beyond these basic takeaways, we have seen a massive number of papers in this overview that propose a swath of practical tips and tricks for properly leveraging LLM-as-a-Judge. The key points from these papers are outlined below.

**LLM-as-a-Judge setups.** There are several different ways that we can structure the prompt and evaluation process for LLM-as-a-Judge. The two core strategies are pairwise and pointwise evaluation, which ask the judge to score a pair of model outputs and a single model output, respectively. However, variants of these setups also exist. For example, Vicuna [2] uses a pairwise scoring approach that asks the judge to assign an individual score to each model output, while authors in [17] propose augmenting pointwise scoring prompts with a solution that can be used by the judge as a reference (i.e., reference-guided evaluation). Concrete examples of LLM-as-a-Judge implementations for several different setups are provided by [Vicuna](https://github.com/lm-sys/vicuna-blog-eval), [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval), and the [LLM-as-a-Judge publication](https://arxiv.org/abs/2306.05685) itself[17](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-17-141159804).

**More on pointwise scoring.** One downside of pointwise scoring is that the judge may lack a stable internal scoring mechanism. Due to existing in a continuous space, pointwise scores tend to fluctuate a lot, making them less reliable than pairwise comparison. However, the implementation of LLM-as-a-Judge is typically dictated by our application—_we cannot always use a pairwise setup_. To improve the reliability of pointwise scoring, we can _i)_ add a [grading rubric](https://x.com/seungonekim/status/1749289437165769177) (i.e., an explanation for each score in the scale being used) to the judge’s prompt, _ii)_ provide few-shot examples to calibrate the judge’s scoring mechanism, or _iii)_ measure the logprobs of each possible score to compute a weighted output.

**Better explainability.** Combining LLM-as-a-Judge with CoT prompting—_and zero-shot CoT prompting in particular due to its ease of implementation_—is incredibly powerful. CoT prompting has been shown to improve the reasoning capabilities of LLMs, and it also improves the accuracy of LLM-as-a-Judge evaluations. We should ask the model to output a rationale prior to generating a score to ensure that the judge’s final score is supported by the explanation. These rationales are also beneficial in terms of explainability, as we can manually read them to gain a deeper understanding of our model’s performance.

**Choosing the correct temperature.** To ensure that the results of LLM-as-a-Judge are (relatively) deterministic, we should use a low [temperature](https://x.com/cwolferesearch/status/1671628210180698112) setting (e.g., 0.1). However, we should be cognizant of the temperature setting’s impact on scoring—_we see in [12] that lower temperatures skew the judge’s output towards lower scores_! As such, we should always make sure that any LLM-as-a-Judge results being directly compared are obtained using the same temperature. Additionally, we should use a slightly higher temperature when sampling multiple scores per example, such as for [self-consistency](https://arxiv.org/abs/2203.11171) or Multiple Evidence Calibration [16].

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Bubeck, Sébastien, et al. "Sparks of artificial general intelligence: Early experiments with gpt-4." _arXiv preprint arXiv:2303.12712_ (2023).

[2] Vicuna Team, et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” https://lmsys.org/blog/2023-03-30-vicuna/ (2023).

[3] Zhou, Chunting, et al. "Lima: Less is more for alignment." _Advances in Neural Information Processing Systems_ 36 (2024).

[4] Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." _Advances in Neural Information Processing Systems_ 36 (2024).

[5] Gudibande, Arnav, et al. "The false promise of imitating proprietary llms." _arXiv preprint arXiv:2305.15717_ (2023).

[6] Wang, Yizhong, et al. "How far can camels go? exploring the state of instruction tuning on open resources." _Advances in Neural Information Processing Systems_ 36 (2023): 74764-74786.

[7] Gemma Team, et al. "Gemma 2: Improving Open Language Models at a Practical Size." _https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf_ (2024).

[8] Li, Xuechen, et al. “Alpacaeval: An automatic evaluator of instruction-following models.” https://github.com/tatsu-lab/alpaca_eval, 2023.

[9] Dubois, Yann, et al. "Alpacafarm: A simulation framework for methods that learn from human feedback." _Advances in Neural Information Processing Systems_ 36 (2024).

[10] Dubois, Yann, et al. "Length-controlled alpacaeval: A simple way to debias automatic evaluators." _arXiv preprint arXiv:2404.04475_ (2024).

[11] Chiang, Cheng-Han, and Hung-yi Lee. "Can large language models be an alternative to human evaluations?." _arXiv preprint arXiv:2305.01937_ (2023).

[12] Sanh, Victor, et al. "Multitask prompted training enables zero-shot task generalization." _arXiv preprint arXiv:2110.08207_ (2021).

[13] Liu, Yang, et al. "G-eval: Nlg evaluation using gpt-4 with better human alignment." _arXiv preprint arXiv:2303.16634_ (2023).

[14] Wang, Jiaan, et al. "Is chatgpt a good nlg evaluator? a preliminary study." _arXiv preprint arXiv:2303.04048_ (2023).

[15] Fu, Jinlan, et al. "Gptscore: Evaluate as you desire." _arXiv preprint arXiv:2302.04166_ (2023).

[16] Wang, Peiyi, et al. "Large language models are not fair evaluators." _arXiv preprint arXiv:2305.17926_ (2023).

[17] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." _Advances in Neural Information Processing Systems_ 36 (2024).

[18] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." _International Conference on Machine Learning_. PMLR, 2023.

[19] Kim, Seungone, et al. "Prometheus: Inducing fine-grained evaluation capability in language models." _The Twelfth International Conference on Learning Representations_. 2023.

[20] Kim, Seungone, et al. "Prometheus 2: An open source language model specialized in evaluating other language models." _arXiv preprint arXiv:2405.01535_ (2024).

[21] Zhu, Lianghui, Xinggang Wang, and Xinlong Wang. "Judgelm: Fine-tuned large language models are scalable judges." _arXiv preprint arXiv:2310.17631_ (2023).

[22] Wang, Yidong, et al. "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization." _arXiv preprint arXiv:2306.05087_ (2023).

[23] Li, Junlong, et al. "Generative judge for evaluating alignment." _arXiv preprint arXiv:2310.05470_ (2023).

[24] Lee, Harrison, et al. "Rlaif: Scaling reinforcement learning from human feedback with ai feedback." _arXiv preprint arXiv:2309.00267_ (2023).

[25] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[1](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-1-141159804)

The genres include writing, roleplay, extraction, reasoning, math, coding, and several knowledge genres (e.g., STEM and humanities / social sciences). Each category has ten questions associated with it in MT-bench.

[2](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-2-141159804)

In [17], this setup is called “single-answer grading”, but I personally use the term “pointwise” to distinguish this setup from the pairwise grading scheme.

[3](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-3-141159804)

Interestingly, prior work [22] has tried having the judge output a rationale after the score, which is found to yield less benefit in terms of scoring accuracy.

[4](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-4-141159804)

The quick release of this paper was made possible by the fact that OpenAI provided the Microsoft researchers that wrote this paper with intermittent access to GPT-4 to run various experiments throughout the model’s development process.

[5](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-5-141159804)

In [1], authors attempt to define AGI as _“systems that demonstrate broad capabilities of intelligence, including reasoning, planning, and the ability to learn from experience, and with these capabilities at or above human-level.”_ However, definitions continue to differ within the AI community, making progress towards AGI difficult to rigorously evaluate.

[6](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-6-141159804)

Given that little analysis had been performed on LLM-as-a-judge-style evaluations at the time, many researchers claimed that these evaluations were not rigorous and that there is no guarantee that GPT-4 will produce high-quality evaluation results.

[7](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-7-141159804)

These prompts use [zero-shot chain of thought prompting](https://arxiv.org/abs/2205.11916) in particular, as no chain of thought exemplars are provided within the prompt.

[8](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-8-141159804)

AlpacaEval improves upon AlpacaFarm by merging instructions and inputs within the evaluator’s prompt, handling longer outputs (i.e., 2K tokens instead of 300), and using output randomization to eliminate position bias.

[9](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-9-141159804)

The costs depend on the exact LLM used for evaluation and the length of outputs being rated. When using GPT-4-Turbo, executing the AlpacaEval benchmarks costs less than $10 in API credits.

[10](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-10-141159804)

The implementation of this evaluator is inspired by both AlpacaFarm and [Aviary](https://github.com/ray-project/ray-llm/tree/master), which are previous attempts at using LLMs to automate pairwise feedback.

[11](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-11-141159804)

As seen in the [settings of AlpacaEval-2.0](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/configs.yaml), authors also set the decoding temperature of the LLM to 1.0 when using logprobs to obtain a preference score.

[12](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-12-141159804)

Within AlpacaEval, there are only two possible models, which are labeled `b` and `m` within the equation.

[13](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-13-141159804)

In general, setting up proper evaluations is universally agreed upon within the research community to be one of the most difficult aspects of working with LLMs.

[14](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-14-141159804)

This is still a very common finetuning strategy; e.g., Gemma-2 [7] was finetuned using an imitation approach.

[15](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-15-141159804)

Another task is also considered that uses the human or LLM to assess the quality of adversarial attacks created for fooling text classification models. However, this task was omitted because it is very specific.

[16](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-16-141159804)

This approach is just [self-consistency](https://arxiv.org/abs/2203.11171), but applied in the LLM evaluation setting.

[17](https://cameronrwolfe.substack.com/p/llm-as-a-judge#footnote-anchor-17-141159804)

Example prompt templates are provided in Appendix A for several different LLM-as-a-Judge setups.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Luke's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfdd24ae-7b1a-4d40-bb66-313988e7a590_144x144.png)



](https://substack.com/profile/1258562-luke)

[

![SUVROJYOTI BISWAS's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d1ec132-0fbc-4708-bb73-f628e913b2d2_953x953.jpeg)



](https://substack.com/profile/219888043-suvrojyoti-biswas)

[

![Adam Gospodarczyk's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29a3a05e-834d-4c48-9e18-859b7920980a_400x400.jpeg)



](https://substack.com/profile/8247555-adam-gospodarczyk)

[

![Silvester's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0ba714e-0520-4b86-83b1-a4a1d72d41fc_96x96.png)



](https://substack.com/profile/119493434-silvester)

[

![ASHISH AGRAWAL's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3da03f61-5539-4074-88f6-ed5db5e2d199_144x144.png)



](https://substack.com/profile/56663813-ashish-agrawal)

104 Likes∙

[13 Restacks](https://substack.com/note/p-141159804/restacks?utm_source=substack&utm_content=facepile-restacks)

104

- 

[

14

](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comments)

13

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Logan Thorneloe's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba547486-5348-4c08-934e-7e11412b5ee7_500x500.png)



](https://substack.com/profile/43759292-logan-thorneloe?utm_source=comment)

[Logan Thorneloe](https://substack.com/profile/43759292-logan-thorneloe?utm_source=substack-feed-item)

[Society's Backend](https://societysbackend.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[7月24日](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comment/63043022 "2024年7月24日 00:20")

Liked by Cameron R. Wolfe, Ph.D.

Excellent article Cameron! I’m curious if anyone has found smaller models to be good judges especially since they’ve taken off recently. I know achieving GPT-4 performance was vital for LLM-as-a-judge to work, but can we make it less expensive by using smaller models? Or are they not quite up to par yet?

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comment/63043022)

[

![Adi Pradhan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6947b805-6bd0-4115-9763-ce7e5879e86e_1977x1977.jpeg)



](https://substack.com/profile/301044-adi-pradhan?utm_source=comment)

[Adi Pradhan](https://substack.com/profile/301044-adi-pradhan?utm_source=substack-feed-item)

[socratify](https://socratify.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[7月22日](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comment/62878234 "2024年7月22日 19:08")

Liked by Cameron R. Wolfe, Ph.D.

Really great summary! I've been reading LLM as a judge papers this past week and this feels like a very comprehensive overview. Thanks for posting

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comment/62878234)

[12 more comments...](https://cameronrwolfe.substack.com/p/llm-as-a-judge/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Model Merging: A Survey

### From modern LLM applications to the early days of machine learning research...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Sep 16, 2024

71

- 

[

8

](https://cameronrwolfe.substack.com/p/model-merging/comments)

8

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F452256f9-a822-46b7-96b4-3bec0913af2e_2556x1430.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F452256f9-a822-46b7-96b4-3bec0913af2e_2556x1430.png)

(from [1, 2, 8, 9, 10, 12, 15])

To improve the performance of a machine learning model, we can train several models independently and average their predictions at inference time to form an ensemble. Ensembling has been used for decades in machine learning, but this approach comes with the downside of increased inference costs—_we must compute the output of several models!_[1](https://cameronrwolfe.substack.com/p/model-merging#footnote-1-147448898) To avoid this issue, researchers have explored alternative techniques for combining models. These explorations eventually led to the popularization of weight-space ensembles, which average the weights of the models in an ensemble—_forming a single, merged model_—instead of averaging their predictions. This technique was found to perform quite well, matching or exceeding the performance of vanilla, output-space ensembles in many cases.

> _“We unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs.”_ - from [3]

Today, model merging is a popular research topic, but this idea is not new whatsoever—_we can trace it all the way back to the 1990s_ [7]! In the deep learning era[2](https://cameronrwolfe.substack.com/p/model-merging#footnote-2-147448898), techniques related to model merging have repeatedly appeared in research topics like mode connectivity, generalization, continual learning and more. In the last few years especially, the level of interest in model merging has exploded due to its effectiveness in applications with large language models (LLMs). We have seen model merging used to combine the capabilities of several foundation models, inject new skills into a model, or even improve the alignment process. In this overview, we will take a deep look at all of this research, starting from the beginning and working our way up to modern applications with LLMs.

## Foundations and Background Information

Before diving into recent research on model merging, we will take a look at the early work in this space. Additionally, we will explore a few different, but related, research topics that from the basis of model merging. By better understanding these techniques and their origins, we will gain a more nuanced perspective on model merging techniques, allowing us to more deeply understand the core ideas in this space, where they come from, and why they work so well.

#### The Origins of Model Merging

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd600b414-3b2a-480b-a1e9-625af92f7d62_2016x1168.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd600b414-3b2a-480b-a1e9-625af92f7d62_2016x1168.png)

(from [7])

Model merging is a popular research topic as of late, but the history of this technique is quite extensive, dating all the way back to the mid 1990s! Authors in [7] observe that practitioners oftentimes train several machine learning models for a given problem, where each model differs in its architecture, training data composition, and/or hyperparameter settings. These models are then used to form an ensemble by combining the outputs of the various models, either by taking an average of outputs or learning weights to be associated with each model’s output.

> _“We … propose that under certain conditions one can average model parameters directly instead of retaining all networks and combine their outputs.”_ - from [7]

In the case of (simple) neural networks, we see in [7] that one can average model parameters directly instead of averaging model outputs. This approach yields similar performance to taking an average of each model’s output, while saving on both storage and compute costs. Most of the work we will see in this overview came long after [7], but this work served as a catalyst for model merging research, which—_as we will see_—became a fruitful and important topic of investigation.

#### (Linear) Mode Connectivity

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b901147-d5d5-4f1a-86f6-f7ddabea020f_1500x843.gif)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b901147-d5d5-4f1a-86f6-f7ddabea020f_1500x843.gif)

When we first learn about training a machine learning model via gradient-based optimization techniques (e.g., [stochastic gradient descent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)), we are typically shown a very simple, 1D view of a loss function being minimized; see above. In reality, the loss landscape of a neural network is non-convex and chaotic, as shown in the image below. However, there are some predictable properties and behaviors of these loss landscapes that we have observed empirically, which make them a little bit less intimidating. One of these interesting properties is _mode connectivity_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4f8ed7-b496-46fb-9903-0d3256a6a597_1074x568.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce4f8ed7-b496-46fb-9903-0d3256a6a597_1074x568.png)

([source](https://arxiv.org/abs/1712.09913))

**Mode connectivity** is an idea that was originally observed and coined in [11]. The training dynamics of neural networks are complex, as we see in the visualization above. For this reason, we might think that two independently-trained neural networks[3](https://cameronrwolfe.substack.com/p/model-merging#footnote-3-147448898) would end up in completely different regions of the optimization landscape. In [11], we learn that this is not always true—_the training trajectory of a neural network becomes relatively predictable after a certain number of iterations_.

> _“We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant.”_ - from [11]

In particular, authors in [11] observe that the weights of neural networks that are trained independently can be connected together in the loss landscape via a path of constant training and test accuracy[4](https://cameronrwolfe.substack.com/p/model-merging#footnote-4-147448898), which can be discovered via their novel training procedure. Interestingly, these “modes” (i.e., the location of the trained network’s weights in the loss landscape) are usually connected by simple curves, as shown within the figure below. This idea was coined _mode connectivity_ due to our ability to connect the modes of these networks via simple paths of constant performance. This property was shown in [11] to hold for numerous computer vision architectures (primarily [ResNets](https://arxiv.org/abs/1512.03385)) trained on several popular datasets.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecc97147-77d2-467f-a033-2af740a64fad_1838x1028.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecc97147-77d2-467f-a033-2af740a64fad_1838x1028.png)

(from [11])

**Linear mode connectivity** is a more specific case of mode connectivity that is observed and analyzed in [12]. Authors in this paper study and compare the properties of vision models trained with different random noise. In particular, the data ordering and augmentation adopted during training, which is referred to as _SGD noise_ in [12], is varied. Then, the mode connectivity between the resulting models obtained via training with varying levels of SGD noise is studied.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0a59565-69ca-4067-9364-e67157e49d43_2174x750.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0a59565-69ca-4067-9364-e67157e49d43_2174x750.png)

(from [12])

To check if two networks are linearly mode connected, we just (linearly) [interpolate](https://en.wikipedia.org/wiki/Interpolation) between their weights and check that the training and test loss of models obtained along this path of linear interpolation is constant. Verifying linear mode connectivity is much simpler compared to mode connectivity in general, as we just have to check a linear path between the models’ weights, instead of using a more complex training algorithm to search for an arbitrary mode connected path. Going further, authors in [12] extend this analysis by only varying SGD noise after `k` training iterations have been performed; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6760e336-58ba-49a8-ad04-54b5ae4de5f0_2484x1262.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6760e336-58ba-49a8-ad04-54b5ae4de5f0_2484x1262.png)

(from [12])

We learn in [12] that neural networks become stable to SGD noise—_meaning that models trained with varying levels of SGD noise are still linearly mode connected after training_—very early in training. After a certain (reasonable) amount of training, _all networks obtained from the same base model are linearly mode connected_. As we will see, this finding is highly related to model merging, as we typically merge models by averaging or interpolating their weights. As such, linear mode connectivity provides empirical intuition for why such interpolated weights perform well!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7279519a-c71e-433e-90ba-ffcc1647b117_1604x766.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7279519a-c71e-433e-90ba-ffcc1647b117_1604x766.png)

(from [13])

To make this idea a bit more specific, later work in [13] shows models that are finetuned from the same pretrained weights end up in the same basin (or region) of the loss landscape. In other words, independently finetuned models that start from the same base model end up close to each other in the parameter space—_this observation has deep connections to research on [critical learning periods](https://cameronrwolfe.substack.com/p/critical-learning-periods-in-deep-networks-35b2f17c4bbe)_. This region of the loss landscape contains several viable model parameters settings that can be discovered, which provides further explanation for the effectiveness of merging and interpolation techniques that will be explored throughout this overview.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71f5c6e8-217d-47dc-b48e-77f9d6374d64_2152x1002.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71f5c6e8-217d-47dc-b48e-77f9d6374d64_2152x1002.png)

(from [14])

**Mode connectivity and LLMs.** In [14], authors perform a similar analysis of linear mode connectivity for LLMs. From these experiments, we learn that finetuned LLMs tend to be linearly mode connected, but these models must be finetuned starting from the same pretrained weights for mode connectivity to hold! As shown above, numerous interpolation strategies are explored in [14] with GPT-style LLMs—_[GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2) in particular._ This work successfully demonstrates the applicability of linear mode connectivity to modern language models.

#### Pruning and Sparsity for Language Models

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe69db49f-4948-43ab-8d53-445cabdc4e1a_2052x666.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe69db49f-4948-43ab-8d53-445cabdc4e1a_2052x666.png)

(from [15])

Throughout this overview, we will study several works that explain the mechanisms that make model merging possible. At a high level, one important reason that model merging tends to be so effective is that large neural networks—_and LLMs in particular_—usually exhibit high levels of sparsity. The weights and activations within these models have a few important values, while other values are either redundant or not impactful; see above. As a result, we can eliminate a large ratio of model parameters via techniques like pruning without meaningfully impacting performance, as well as merge parameters without a high likelihood of two important weights or activations conflicting with each other.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8393f834-f04e-42a4-a25f-c84938fd658a_1400x384.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8393f834-f04e-42a4-a25f-c84938fd658a_1400x384.png)

Key components of neural network pruning

**Neural network pruning** starts with a large neural network and aims to derive from this larger network a smaller network with comparable performance. To do this, we usually follow a multi-step process (shown above) of:

1. Training the model to convergence.
    
2. Pruning the model’s weights (e.g., via a heuristic like removing the lowest-magnitude weights in each layer).
    
3. (Optionally) rewinding the model’s remaining weights to their original values.
    
4. Training the subnetwork (i.e., the pruned model) to convergence.
    

We can repeatedly apply steps two through four to derive an _iterative_ pruning strategy, which allows higher performance to be maintained by slowly removing weights from the model instead of pruning many weights at the same time. By following this strategy, we can derive incredibly small networks that achieve performance that is comparable to—_or even better than_—the larger models from which they are derived. For this reason, pruning became a popular research topic in the late 2010’s and continues to be an active direction of research even today.

> _“We articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation—reach test accuracy comparable to the original network in a similar number of iterations.”_ - from [16]

In a prior overview[5](https://cameronrwolfe.substack.com/p/model-merging#footnote-5-147448898), we learned about the topic of neural network pruning in great depth; see [here](https://cameronrwolfe.substack.com/p/saga-of-the-lottery-ticket-hypothesis-af30091f5cb). For those who want to learn more, I have also listed my favorite pruning papers below:

- _[Learning Structured Sparsity in Deep Neural Networks](https://arxiv.org/abs/1608.03665)_: this paper is the first to explore pruning via the [L1-norm](https://mathworld.wolfram.com/L1-Norm.html) heuristic (i.e., removing low magnitude weights), which is the most commonly-used pruning strategy.
    
- _[Pruning Filters for Efficient ConvNets](https://arxiv.org/abs/1608.08710)_: this paper shows significant reductions in the compute cost of a neural network can be achieved without a reduction in performance by removing filters with low L1-Norm from a model’s layers.
    
- _[Rethinking the Value of Network Pruning](http://rethinking%20the%20value%20of%20network%20pruning/)_: this work performs an extensive empirical analysis of different pruning techniques and settings to determine best practices for obtaining high-performing subnetworks.
    
- _[The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)_: this paper discovers and analyzes the lottery ticket hypothesis (LTH), which shows that high-performing subnetworks exist within the randomly-initialized weights of a neural network.
    

Despite becoming a very popular research topic in recent years, the idea of pruning has deep roots in early research on neural networks. Some of the first works to explore this idea were written in the early 1990s [17, 18]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49af9e0d-819e-4af5-8583-5726cebdd786_1868x736.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49af9e0d-819e-4af5-8583-5726cebdd786_1868x736.png)

(from [17, 18])

**Pruning in the age of LLMs.** More recently, research on neural network pruning has been modernized in the age of LLMs. In [19], authors propose a pruning algorithm, called SparseGPT, that can be used to prune GPT-style language models—_using an unstructured approach_—to over 50% sparsity in one shot, meaning that no retraining is required after pruning. Eliminating retraining from the pruning procedure reduces compute costs significantly. The SparseGPT algorithm, which is shown below, operates by reducing the pruning process into a series of sparse regression problems that can be approximated efficiently.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e6bfb65-0737-4381-b0ce-2cc798956761_1966x700.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e6bfb65-0737-4381-b0ce-2cc798956761_1966x700.png)

(from [19])

Shortly after, authors in [19] performed analysis showing that magnitude-based pruning—_a popular and widely-used technique for pruning_—works poorly for LLMs. Although SparseGPT improves upon this technique, authors mention that this pruning algorithm takes around 4-5 hours to execute for a ~100B parameter LLM when running on a single GPU. In other words, _the SparseGPT algorithm is still computationally expensive even if it does not require retraining to be performed_.

> _“Considering the past success of magnitude pruning on smaller networks, this result suggests that LLMs, despite having 100 to 1000 times more parameters, are substantially more difficult to prune directly.”_ - from [20]

As a solution, Wanda (pruning by **W**eights **AND** **A**ctivations)—_a very simple pruning approach for LLMs_—is proposed and analyzed in [20]. This approach determines which weights to prune by multiplying each of the model’s weights by their corresponding input activations on a per-output basis; see below for the exact formulation. Similar to SparseGPT, this technique requires no retraining. Additionally, Wanda is more efficient as a whole, and we can achieve higher levels of sparsity without damaging the performance of the pruned model. From this work, we learn that effectively pruning LLMs is difficult, but possible.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4434c0-9d56-41df-b927-665521ff3f4e_2158x844.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4434c0-9d56-41df-b927-665521ff3f4e_2158x844.png)

(from [20])

Although the technique used for pruning within Wanda might seem random, the algorithm in [20] is actually inspired by the recent observation that LLMs tend to have a small number of very high magnitude activations within each of their hidden layers. Interestingly, these high magnitude features (or outliers) seem to be an emergent property of larger models (i.e., ~7B parameters and beyond).

> _“At around 6.7B parameters, a phase shift occurs, and all transformer layers and 75% of all sequence dimensions are affected by extreme magnitude features.”_ - from [21]

This property was first observed in the quantization context [21], where authors develop a more performant 8-bit quantization technique for LLM inference by adopting tricks to property deal with these outliers features; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbff40d34-f893-4f19-899d-2ad115b1bef1_954x1252.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbff40d34-f893-4f19-899d-2ad115b1bef1_954x1252.png)

(from [21])

However, another work [15] also explores this property in depth, finding that these massive activations exist across a wide variety of different models and training settings. Put simply, work in [15] and [21] shows us that LLMs tend to have incredibly sparse activations that can be exploited for better pruning.

**Why does this matter?** Although pruning and model merging are separate topics, they are highly related due to their joint dependence upon the same fundamental idea—_sparsity_. Understanding pruning algorithms and related research is highly beneficial when learning about model merging, especially given that pruning is such a comprehensively-studied topic with an abundance of great ideas.

## Early Work on Model Merging

Now that we understand the fundamental concepts that underlie the concept of model merging, we will look at a collection of notable papers that initially explored the concept of model merging for deep neural networks. Most of these papers were published prior to the popularization of LLMs, but many of the techniques are repurposed in the research we see on model merging today.

#### **[Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482) [5]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb3f22f8-1801-4ad3-a128-5e319a2adf6a_1638x600.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb3f22f8-1801-4ad3-a128-5e319a2adf6a_1638x600.png)

(from [5])

When we finetune a pretrained model on some downstream task, the best training hyperparameters to use are oftentimes unknown. As a result, we usually _i)_ run several training trials with different hyperparameters, _ii)_ select the model that performs best on some held-out validation set, and _iii)_ discard the remaining models. Instead of discarding remaining models, we could form an ensemble of these models, which usually improves performance [6]. But, this approach drastically increases inference costs. In [5], authors explore an alternative strategy—_merging the weights of these models_—that can have the best of both worlds!

> _“Model soups can approach the performance of ensembling, with no additional computational cost or memory relative to a single model during inference.”_ - from [5]

**Model soups.** The idea proposed in [5] is very simple—_we finetune several identical models with different hyperparameter settings and merge their weights_. Although we usually discard all but the best model when performing hyperparameter tuning, this approach is inspired by work on model ensembling that shows the benefit of averaging the predictions of these models. There are a few different ways we can merge the models’ weights! The following techniques are considered in [5]:

- _Average_: just take a uniform average of models’ weights.
    
- _Greedy_: select only models with performance above some threshold (e.g., average model performance) and take the average of selected models’ weights.
    

The resulting merged model is referred to as a “model soup”. Authors in [5] also propose a more sophisticated technique for _learning_ optimal merging coefficients between model weights. However, this approach requires all models to be loaded into memory at the same time, which is impractical.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e3ed84c-a7f0-4c99-ad8f-ac51abf4a4a5_1806x556.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e3ed84c-a7f0-4c99-ad8f-ac51abf4a4a5_1806x556.png)

When comparing average and greedy merging, we see in [5] that better performance is generally achieved with greedy merging. The issue with average merging is that certain hyperparameter settings may lead a subset of resulting models to perform poorly compared to others. Greedy merging removes these models from consideration via a simple performance filter; see above. More specifically, greedy soups are constructed by sequentially adding each model to the soup only if performance on a held-out validation set is improved. The list of considered model is sorted in decreasing order of validation set performance beforehand, _ensuring that the greedy soups is no worse than the best individual model_.

**How well does this work?** Experiments in [5] largely consider the task of image classification on ImageNet, but a variety of models are considered, including [CLIP](https://arxiv.org/abs/2103.00020), [ALIGN](https://arxiv.org/abs/2102.05918), [BASIC](https://arxiv.org/abs/2111.10050), and several [vision transformer (ViT)](https://cameronrwolfe.substack.com/p/vision-transformers) variants. These models are all pretrained on a separate large-scale dataset (e.g., [WIT](https://arxiv.org/abs/2103.01913) or [JFT-3B](https://paperswithcode.com/dataset/jft-3b)), finetuned on ImageNet, and evaluated on ImageNet[6](https://cameronrwolfe.substack.com/p/model-merging#footnote-6-147448898).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdd46868-e88c-4fc8-b0c2-0d98442c7f35_1564x544.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdd46868-e88c-4fc8-b0c2-0d98442c7f35_1564x544.png)

(from [5])

The high level results of these experiments are outlined in the table above. For any number of models, the greedy soup consistently outperforms the best single model of any hyperparameter sweep, as well as nearly matches the performance of model ensembles in most cases. Unlike an ensemble, however, the model soup incurs no additional inference or memory cost relative to a single model!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a245a2-2eca-41ff-8b46-49febda41465_1886x352.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a245a2-2eca-41ff-8b46-49febda41465_1886x352.png)

(from [5])

Authors in [5] even reach new state-of-the-art performance on ImageNet, achieving a test accuracy of 90.94%[7](https://cameronrwolfe.substack.com/p/model-merging#footnote-7-147448898) with a soup of [ViT-G](https://arxiv.org/abs/2106.04560) models. Model soups also perform better in the zero-shot regime and on new tasks that go beyond the distribution of the ImageNet test set. Additionally, model soups are found to yield useful results on text classification tasks with transformers; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7fe5747-27d1-44a7-b000-b8c453f88fe2_1970x1038.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7fe5747-27d1-44a7-b000-b8c453f88fe2_1970x1038.png)

(from [5])

**Why does this work?** Authors in [5] provide extensive analysis to motivate and explain the effectiveness of model soups. Much of this analysis draws parallels to prior research on linear mode connectivity. As shown in the figure above, finetuned models tend to lie within a similar basin of the loss landscape, meaning that the loss is relatively stable—_and might even decrease_—when we interpolate between the weights of these finetuned models! The best performing model is not any of the individual finetuned models. Rather, it lies somewhere in the loss landscape between all of these models. So, interpolating between these models via a model soup allows us to discover higher-performing models!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63867b16-5afc-4b58-9bea-03f6ae7e17d1_2294x1196.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63867b16-5afc-4b58-9bea-03f6ae7e17d1_2294x1196.png)

(from [36])

**Model Ratatouille [36]** is an interesting extension to model soups that aims to repurpose the large variety of finetuned foundation models that are available online. Our goal is to finetune a model on a downstream task. Instead of directly finetuning a single model, however, we do the following (shown above):

1. Obtain several openly-available models that have been finetuned on different auxiliary tasks.
    
2. Finetune each of these models separately on our downstream task.
    
3. Merge the weights (via an average) of all finetuned models.
    

We see in [36] that such a technique outperforms prior merging techniques, demonstrating that the extra information learned by these models when finetuned on diverse auxiliary tasks is useful for forming model soups.

#### [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903) [8]

> _“Can zero-shot [foundation] models be fine-tuned without reducing accuracy under distribution shift?”_ - from [8]

When we train a foundation model, we would like for this model to work well across a broad distribution of data. The advent of [self-supervised pretraining](https://cameronrwolfe.substack.com/i/139646437/language-model-pretraining) has largely solved this problem—_pretrained LLMs can solve a wide variety of problems in a zero-shot manner due to their sizable knowledge base that is derived from the massive text corpus on which the model is pretrained_. Still, we can improve the performance of a pretrained model on a particular target domain by finetuning the model on a targeted set of data from that domain[8](https://cameronrwolfe.substack.com/p/model-merging#footnote-8-147448898); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F354c2e1c-c4f9-499f-9d14-76c18cc0f738_2464x846.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F354c2e1c-c4f9-499f-9d14-76c18cc0f738_2464x846.png)

Depiction of pretraining and finetuning

Despite the utility of finetuning, there are downsides of which we should be aware! Finetuning a pretrained model on a domain-specific dataset:

- Improves the model’s performance on data from that particular domain.
    
- Degrades the model’s performance on data beyond that particular domain.
    

Put simply, _finetuning makes the model less generic_, which can degrade its performance on data that is different from the finetuning dataset. In [8], authors try to mitigate this issue by adopting a simple model merging approach.

**Finetuning and robustness.** The negative impact of finetuning on the robustness of a pretrained model makes sense intuitively. Finetuning a pretrained model specializes this model to the properties of data from the target domain, which comes at the cost of model performance in a broader sense. Although the impact of finetuning on model robustness makes sense intuitively, authors in [8] analyze this relationship in more detail. In particular, several interesting findings regarding the impact of finetuning on model performance are outlined:

1. The model’s performance improves on the target distribution.
    
2. The model’s performance degrades under various distribution shifts (i.e., data that goes beyond the target distribution).
    
3. Hyperparameter settings have a very large impact on robustness.
    
4. More “aggressive” finetuning (e.g., using a larger learning rate) exacerbates these findings—_target domain performance improves more and performance is even worse under distribution shifts_.
    

To measure accuracy under distribution shifts, we can simply adopt existing datasets from [out-of-distribution (OOD) generalization research](https://arxiv.org/abs/2108.13624). For example, the ImageNet dataset has numerous alternative test sets that can be used to study several kinds of distribution shifts; see below for a few examples.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f19b5bb-c9ef-4b87-ae00-101c6a44fbd8_1682x642.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f19b5bb-c9ef-4b87-ae00-101c6a44fbd8_1682x642.png)

(from [8])

**Weight-Space Ensembles for Fine-Tuning (WiSE-FT).** The technique proposed in [8] is simple. We _i)_ start with a pretrained model, _ii)_ finetune the model on a target dataset, and _iii)_ interpolate between the weights of the pretrained and finetuned models; see below. Although we can arbitrarily interpolate between the weights of the pretrained and finetuned models, we see in [8] that simply taking an average of these weights works well in most cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F786d1c7c-5778-4096-be2c-a02b03eea745_1742x632.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F786d1c7c-5778-4096-be2c-a02b03eea745_1742x632.png)

Merging model weights in WiSE-FT (from [8])

Extending upon research on linear mode connectivity, authors in [8] observe that finetuned models are typically mode connected to their associated base model. More generally, we learn that models sharing a large part of their training trajectory—_such as a pretrained model and any finetuned models derived from this pretrained model_—tend to be mode connected, which allows us to merge these models without causing a catastrophic impact to performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a3190fa-4f8c-4d48-9921-49243d4b4890_1512x1292.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a3190fa-4f8c-4d48-9921-49243d4b4890_1512x1292.png)

(from [8])

**Does this work well?** The impact of WiSE-FT is mostly studied using pretrained [CLIP](https://arxiv.org/abs/2103.00020) models on the ImageNet dataset, where we see that merging pretrained and finetuned models yields a happy medium between the performance of both models. The results of the analysis in [8] are summarized in the figure above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff692788-7829-447b-b127-bcbd3e08f793_1862x966.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff692788-7829-447b-b127-bcbd3e08f793_1862x966.png)

(from [8])

In short, WiSE-FT yields the following benefits compared to a model obtained via standard finetuning (see table above for more details):

- Improved accuracy under distribution shifts.
    
- Comparable (or improved) accuracy in the target domain.
    

We also see that WiSE-FT can mitigate the sensitivity of model robustness to hyperparameter settings—_we can recover the performance of the best hyperparameter setting in nearly all cases by just changing the interpolation coefficient_[9](https://cameronrwolfe.substack.com/p/model-merging#footnote-9-147448898)_._ WiSE-FT also has no added computational costs during finetuning or inference.

**Why does this work well?** Beyond observing the performance benefits of WiSE-FT, authors dig a bit deeper into the mechanics of this technique by studying the relationship between the predictions generated by the pretrained, finetuned, and merged models. Interestingly, we see from this analysis that the finetuned model frequently overrides the predictions of the pretrained model when evaluating on in-domain data that is similar to the finetuning dataset. In contrast, predictions on out-of-distribution model are usually handled by the pretrained model! Put simply, the merged model naturally relies on the more appropriate model based upon the data (and task) being considered by a given input example.

> _“Overall, WiSE-FT is simple, universally applicable in the problems we studied, and can be implemented in a few lines of code. Hence we encourage its adoption for fine-tuning zero-shot models.”_ - from [8]

**Implementation details.** Given that WiSE-FT just takes a weighted average of model parameters, this technique is actually very easy to implement! An example using PyTorch syntax is provided in the algorithm below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc10a4bb9-eb0a-4a23-93dc-6421a7e28e91_792x746.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc10a4bb9-eb0a-4a23-93dc-6421a7e28e91_792x746.png)

(from [8])

**Further research.** WiSE-FT is further analyzed in [42], where we see that this method also improves generalization via the “FalseFalseTrue” phenomenon. More specifically, WiSE-FT is observed to correct numerous cases where each model makes an incorrect prediction. The merged model is correct, but both of the source models are wrong! After analyzing this property theoretically, authors conclude that this property is largely due to impact of diverse feature sets on OOD generalization, thus providing (for the first time!) theoretical intuition for the ability of weight-space ensembles to outperform output-space ensembles.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F220060ac-0d9b-4a4c-98a8-760079c94ec4_2140x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F220060ac-0d9b-4a4c-98a8-760079c94ec4_2140x782.png)

(from [49])

An extension of WiSE-FT for LLMs, called an LM-Cocktail, is proposed in [49]. This approach merges finetuned language models with their pretrained base models, which mimics the strategy used in WiSE-FT. However, the technique is slightly more general, as additional models—_such as those finetuned on data from other domains (i.e., peer models)_—can also be included in the merge; see above.

#### **[Model Stock: All we need is just a few fine-tuned models](https://arxiv.org/abs/2403.19522) [9]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a7ce1a5-74c0-4ab6-9556-f661848e0e18_1404x984.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a7ce1a5-74c0-4ab6-9556-f661848e0e18_1404x984.png)

(from [9])

Authors in [8] present a more recent extension of the model soups [5] and WiSE-FT [8] papers that aims to find a better tradeoff between:

- The performance of the merged model.
    
- The number of finetuned models we need to train (and merge).
    

Beginning with a pretrained model, the model soups technique proposed in [5] mandates that we perform several, independent finetuning runs, then take an average of the resulting models’ weights. Unfortunately, this technique typically requires that we finetune dozens of models, which is computationally expensive! Instead, WiSE-FT proposes finetuning a single model and interpolating between the weights of this model and the pretrained model, but performance is lacking.

> _“This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model.”_ - from [9]

To solve these issues, authors in [9] deeply analyze the geometric properties of finetuned weights relative to a pretrained model. This analysis allows them to devise an efficient merging strategy, called a model “stock”, that can achieve performance comparable to a model soup with only two finetuning runs, thus saving a massive amount of computation in terms of total training costs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F670a222e-8535-47fe-9011-12b4abac4d05_1404x728.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F670a222e-8535-47fe-9011-12b4abac4d05_1404x728.png)

(from [9])

**Geometric analysis.** To arrive at this efficient merging strategy, we need to understand the properties of finetuned weights. _What are model soups and WiSE-FT actually doing, and why do they work well?_ To answer these questions, authors in [9] finetune over 50 pretrained CLIP models and study how the finetuned weights relate to each other[10](https://cameronrwolfe.substack.com/p/model-merging#footnote-10-147448898), arriving at an interesting observation (depicted above):

1. All finetuned weights lie on a thin shell (or sphere) that is centered around a central point (i.e., the average of the finetuned weights), meaning the distance between finetuned weights and this center is (roughly) constant.
    
2. The center point lies at a different location in space relative to the pretrained weights, but these positions still satisfy predictable geometric properties.
    

This observation is empirically validated in [9] using several different finetuning setups, models, and datasets. Furthermore, we see from the analysis in [9] that these “central” weights—_those lying at the center of all finetuned weights_—consistently achieve optimal performance, exceeding the performance of all finetuned models and the pretrained model. This finding explains the utility of model soups—_the model soup is an average of finetuned models and, therefore, an approximation of these central weights that are found to perform well_.

> _“We uncover a strong link between the performance and proximity to the center of the weight space [and] introduce a method that approximates a center-close weight using only two finetuned models.”_ - from [9]

With this analysis in mind, WiSE-FT can be viewed as simply interpolating between the weights of a finetuned and pretrained model, which can be used to discover a point in space that is closer to the high-performing center; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d0c6a3-6749-4077-abc5-3a9ff7e6bd16_1412x528.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d0c6a3-6749-4077-abc5-3a9ff7e6bd16_1412x528.png)

(from [9])

**How can we use this?** The analysis performed in [9] is incredibly thorough and interesting, but the details are intricate and beyond the scope of this post. I’d highly encourage the interested reader to check out sections two and three of the paper for the full details. The main question we want to answer here is: _How can we practically leverage this information to create a better model merging technique?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecdf66c8-909f-4056-92b4-f6973abc76f4_1416x812.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecdf66c8-909f-4056-92b4-f6973abc76f4_1416x812.png)

(from [9])

The high-level answer is simple—_we can use the geometric relationship between pretrained and finetuned model weights to efficiently approximate the center point_. This approximation, which uses the properties discovered in [9] to directly solve for the center point, only requires two finetuned models to be generated; see above. The pretrained model serves as an “anchor point”, and we can approximate the center point by projecting it onto the plane formed by the weights of all three models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf8b444b-92cf-44af-9282-910521cc1702_1302x792.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf8b444b-92cf-44af-9282-910521cc1702_1302x792.png)

Creating a model stock (from [9])

There is a lot of fancy math here that can be a bit difficult to understand, _but the practical result of this extensive analysis is just a different equation (shown above) for finding the best interpolation coefficient between finetuned weights_! So, this approach is not actually that much different than WiSE-FT at the end of the day, we just _i)_ train two finetuned models instead of one and _ii)_ use a more intricate technique—_based upon the geoemtric analysis in [9]_—to optimally merge the models. However, WiSE-FT is more widely adopted in the literature due to its simplicity.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08957412-ea76-4455-b4b6-1e35f023b40a_1408x812.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08957412-ea76-4455-b4b6-1e35f023b40a_1408x812.png)

(from [9])

**Empirical results.** We see in [9] that closer proximity to the center point formed by the average of finetuned model weights—_assuming the average is taken over a sufficiently large number of finetuned models_—yields better performance, both on the target domain and under a distribution shift. Most experiments in [9] are performed using [pretrained CLIP](https://arxiv.org/abs/2103.00020) models that are finetuned on ImageNet, thus matching the experimental setup of WiSE-FT [8]. Model stocks with only two finetuned models are found to reach state-of-the-art performance on ImageNet and match the performance of model soups [5]—_obtained using dozens of finetuned models_—in this regime, thus yielding a significant reduction in training costs; see above. Interestingly, adding more models (i.e., three or four finetuned models) to a model stock does not yield a significant performance benefit; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b816d39-d48c-4312-aa9c-73055a305f8b_782x608.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b816d39-d48c-4312-aa9c-73055a305f8b_782x608.png)

#### Weight Averaging Techniques

> _“We show that SGD generally converges to a point near the boundary of the wide flat region of optimal points. SWA is able to find a point centered in this region, often with slightly worse train loss but substantially better test error.”_ - from [22]

One common variant of model merging is simply taking an average of model weights at several points throughout the model’s training trajectory. More specifically, we _i)_ record several checkpoints of the model’s weights throughout training, _ii)_ take an average of the model’s weights at several checkpoints, and _iii)_ use these averaged weights as our final model. This technique was originally proposed in [22][11](https://cameronrwolfe.substack.com/p/model-merging#footnote-11-147448898) and is referred to as _stochastic weight averaging (SWA)_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83fe23f-591e-4d19-bc2a-3385b3498a6e_2276x984.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb83fe23f-591e-4d19-bc2a-3385b3498a6e_2276x984.png)

(from [22])

Compared to standard training (with stochastic gradient descent), SWA is shown in [22] to help the resulting model to generalize better. Plus, this technique is easy to implement and has no computational overhead, whereas alternative techniques (e.g., creating an ensemble of models) have significant increases in computation at inference time. We start with a pretrained network and apply SWA during the finetuning process, which ensures that the checkpoints are mode connected [13]. Numerous other weight averaging techniques have been explored as well:

- [DiWA](https://arxiv.org/abs/2205.09739) [35] extends upon prior weight averaging techniques by improving the diversity of the models being averaged.
    
- [SWAD](https://arxiv.org/abs/2102.08604) [41] extends SWA via a stochastic weight sampling strategy that finds flatter minima—_with better generalization properties_—in the loss landscape.
    
- [Fuse to Forget](https://arxiv.org/abs/2311.07682) [43] studies the properties of knowledge within averaged models, finding that the resulting models tend to _i)_ lose unshared knowledge and _ii)_ have their shared knowledge enhanced.
    
- Authors in [44] show that weight averaging can be used as a mechanism to mitigate catastrophic forgetting of old tasks when automatic speech recognition (ASR) models are adapted to new tasks.
    
- We see in [45] that weight averaging is useful for merging learned policies—_implemented as a “decision” transformer_—for locomotion within reinforcement learning (RL), while [46] shows us that weight averaging is generally useful for improving training stability for RL with deep neural networks.
    

**EMA of weights.** Instead of taking an average over a finite and discrete number of model checkpoints, we can take an [exponentially moving average (EMA)](https://leimao.github.io/blog/Exponential-Moving-Average/) of model weights throughout the training process. This technique, which is an extension of SWA, was heavily adopted by vision models in the late 2010s (e.g., [InceptionNet](https://arxiv.org/abs/1512.00567), [EfficientNet](https://arxiv.org/abs/1905.11946), [MnasNet](https://arxiv.org/abs/1807.11626), and more) but is not commonly covered in papers—_it’s more of a practical implementation detail that can be found in the code repositories for these models_. However, the concept of using EMA during training is mentioned in the original Adam optimizer paper [24]; see Section 7.2 [here](https://arxiv.org/abs/1412.6980).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87500599-7a5f-4809-b629-92245ca1b464_1432x960.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87500599-7a5f-4809-b629-92245ca1b464_1432x960.png)

(from [26])

The idea of using models—_or predictions_—obtained via EMA as a target for self or semi-supervised learning has also been explored a lot [25, 26]; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46b2f4fa-240d-4548-b6cf-81a2067ae97c_834x888.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46b2f4fa-240d-4548-b6cf-81a2067ae97c_834x888.png)

**Extension to LLMs.** In [27], authors explore an extension of SWA for LLM pretraining, leading to faster convergence and improved generalization. Two key observations inspire the proposed technique in [27]:

1. Models trained with a larger learning rate see more of a benefit from averaging model checkpoints along the training trajectory.
    
2. Averaging model trajectories that are further apart in the training process leads to larger gains.
    

Based on these findings, authors propose LAWA (**LA**test **W**eight **A**veraging), which performs checkpoint averaging using a sliding window along the training trajectory of an LLM; see above for an illustration. LAWA simply maintains a first-in-first-out buffer of checkpoints sampled throughout the training process and computes an average over the `k` most recent checkpoints in this buffer. Checkpoints are inserted into the buffer with many training steps in between, ensuring that more distant checkpoints (i.e., those from the earlier phases of training) are included within the merging process; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2631968c-c1e9-4feb-adfc-e09b66f00e36_2148x374.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2631968c-c1e9-4feb-adfc-e09b66f00e36_2148x374.png)

(from [27])

In [27], we see that LAWA outperforms conventional checkpoint averaging techniques like EMA and SWA in the language modeling domain, where larger learning rates are usually preferred. LLMs trained with LAWA converge faster and generalize better. The observed benefits of LAWA tend to improve as we add more training steps between the checkpoints that are sampled for averaging.

## Model Merging for Large Language Models

Now that we have learned about early work on model merging, we will take a look at more recent techniques, such as task vectors, TIES, DARE, and more. Those who have tracked recent developments in LLM research are likely to have seen these techniques. Many of these algorithms are supported within popular open-source software for LLMs, such as the [mergekit](https://huggingface.co/blog/mlabonne/merge-models) [54]. In this section, we will take a look at the most common algorithms used for merging LLMs, as well as learn about how model merging has transformed the LLM alignment process.

#### **[Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089) [1]**

> _“With task arithmetic, practitioners can reuse or transfer knowledge from models they create, or from the multitude of publicly available models all without requiring access to data or additional training.”_ - from [1]

As practitioners, we usually solve tasks by _i)_ starting with a pretrained model (e.g., LLaMA-3 or Mistral) and _ii)_ adapting (or [steering](https://x.com/cwolferesearch/status/1645535869556727815)) this model to our desired use case. For example, we might further finetune the model on a downstream task, try to reduce the model’s bias, or perform [alignment](https://cameronrwolfe.substack.com/i/138218863/language-model-alignment). To do this, we have two basic options at our disposal:

- Specify desired behavior as an instruction within the model’s prompt (i.e., use [prompt engineering](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering)).
    
- Finetune the model on extra data.
    

Usually, we will try to solve a problem via prompting first due to simplicity, then perform finetuning if performance is short of what we need or expect. However, the process of finetuning an LLM requires task-specific data and can be expensive, both in terms of time and monetary costs. In [1], authors propose a much simpler and easier way of editing pretrained models, called task arithmetic.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc04793ca-23ad-468a-991f-7ff4848c45e4_714x818.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc04793ca-23ad-468a-991f-7ff4848c45e4_714x818.png)

Depiction of a task vector (from [1])

**What is a task vector?** The first concept introduced in [1] is a task vector, which simply refers to a vector that is obtained by subtracting a finetuned model’s weights from a pretrained model; see above. Intuitively, a task vector encodes all of the information needed to solve a task that is learned via finetuning. These task vectors live within the parameter space of a neural network, meaning that they are the same size and shape as the weights of the model we are trying to edit.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68818efb-b617-4479-a69b-32805edb6095_624x684.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68818efb-b617-4479-a69b-32805edb6095_624x684.png)

In [1], we see that these task vectors can be used to change the behavior of a pretrained model—_task vectors are a simple and cheap alternative to prompting or finetuning_. In particular, we can edit a model by performing arithmetic between the model’s parameters and a task vector, as shown above. The scaling term is typically tuned via a hold-out validation set. Intuitively, these task vectors move the parameters of our model towards those of a model with the desired behavior. This approach requires that all models share the exact same architecture.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bda1c4b-4f57-48a8-b03d-c1752c6eaa94_1348x728.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bda1c4b-4f57-48a8-b03d-c1752c6eaa94_1348x728.png)

**Types of arithmetic.** Beyond basic addition, we see in [1] that there are several forms of meaningful arithmetic that can be performed with task vectors; see above. We can add a task vector to learn a new skill or negate a task vector to eliminate a skill. _We can even add or negate several task vectors at once!_

> _“Negating a vector can be used to remove undesirable behaviors or unlearn tasks, while adding task vectors leads to better multi-task models, or even improves performance on a single task.”_ - from [1]

Going further, authors find in [1] that analogies between task vectors hold as well. Assume we have four tasks A, B, C, and D with a analogous relationship given by _“A is to B as C is to D”_. Then, we can improve performance on task D using the task vector shown below. Here, we construct a task vector by:

- Finding the difference in task vectors between tasks A and B.
    
- Adding this difference to the task vector for task C.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7dcb89d-5ce9-4a8c-8a6a-0565b4b6d442_434x302.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7dcb89d-5ce9-4a8c-8a6a-0565b4b6d442_434x302.png)

Analogy-based task vector (from [1])

**Experimental results.** Task vectors are applied to numerous types of models in [1], including both LLMs and specialized models (e.g., image and text classifiers). We see that adding and negating task vectors is clearly effective. For example, we can obtain a toxicity task vector by finetuning an LLM on toxic text. Then, we can make an LLM less toxic by negating this task vector; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a327f0a-a3b6-4f1f-b1ac-876b39158744_1236x512.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a327f0a-a3b6-4f1f-b1ac-876b39158744_1236x512.png)

(from [1])

We can also forget certain image classification tasks via negation, as well as learn new tasks—_even multiple tasks at the same time_—via addition. For example, the image below show how pairs of task vectors can improve an image classifier’s performance on downstream tasks, while the table shows that tasks vectors obtained from an LLM that has been finetuned on a downstream task can be used to improve a model’s performance on that task without extra finetuning!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5faea64-7570-4783-8622-efd58669f0e1_864x752.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5faea64-7570-4783-8622-efd58669f0e1_864x752.png)

(from [1])

Additionally, these improvements in performance seem to avoid associated degradations in performance on control tasks—_the model does not get worse in other areas as a result of the task vector_. Similar experimental results are observed for analogous task vectors, where we see that most task vectors are [orthogonal](https://en.wikipedia.org/wiki/Orthogonality) and can be used to generalize to new domains. In many ways, this analysis is reminiscent of early analysis of [word vectors](https://arxiv.org/abs/1301.3781), where we observed similar analogous relationships between the vectors of associated words; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e8007ac-2c20-4dd1-abb7-f1bcd5c506a5_604x436.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e8007ac-2c20-4dd1-abb7-f1bcd5c506a5_604x436.png)

([source](https://arxiv.org/abs/1810.04882))

**Key benefits.** Compared to prompting or finetuning, editing models via task arithmetic is cheap and easy. We do not need any external data or GPUs for training. Rather, we just need access to a finetuned model—_many of which are already available online_! Task arithmetic only requires element-wise operations on the model’s weights to quickly experiment with different task vectors. So, we can easily re-use or transfer knowledge from models that are openly available.

#### [TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708) [2]

> _“When merging a parameter that is influential for one model but redundant (i.e. not influential) for other models, the influential value may be obscured by the redundant values, lowering the overall model performance.”_ - from [2]

Given the proliferation of task-specific, finetuned models online, model merging is a promising technique that could help to consolidate these models. However, the performance of basic model merging techniques (e.g., averaging or weighted averaging of parameters) tends to degrade as we merge larger numbers of models. In [2], authors study the task vector-based model merging regime [1] and identify that performance degradations due to model merging are largely caused by “interference” that occurs between model parameters during the merging process. In particular, two key sources of interference are identified:

1. _Redundant parameters_: many parameters in a task vector are redundant, and removing these parameters does not impact performance.
    
2. _Sign disagreements_: certain parameters may have a positive value for some models and negative value for others, which causes a conflict.
    

A schematic depiction of these different interference patterns is provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd85c544f-3192-4d90-80a2-ed4d7843caf5_980x896.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd85c544f-3192-4d90-80a2-ed4d7843caf5_980x896.png)

(from [2])

To prove that this interference is occurring, we can just study basic properties of model parameters. In the (left) figure below, we first see that model performance is largely determined by a small group of (high magnitude) parameters, while most other parameters are redundant—_meaning that they do not impact the model’s performance when removed_[12](https://cameronrwolfe.substack.com/p/model-merging#footnote-12-147448898). Similarly, we see that sign conflicts are very common and become more common as the number of models considered is increased.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73cf7b3b-082e-441b-be53-e0f640824acb_1350x542.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73cf7b3b-082e-441b-be53-e0f640824acb_1350x542.png)

(from [2])

**Dealing with interference.** The method devised in [2] for mitigating interference, called **T**r**I**m, **E**lect **S**ign, and Merge (TIES-Merging), adds three additional steps to the task vector-based model merging process:

- _Trim_: retain only influential weights (i.e., the top `K`% of highest-magnitude values) in each task vector and set others to zero.
    
- _Elect Sign_: choose the sign of total highest magnitude for each element across task vectors—_resulting in a sign vector_[13](https://cameronrwolfe.substack.com/p/model-merging#footnote-13-147448898)_—_by taking a element-wise sum across task vectors and seeing whether each resulting element is positive or negative.
    
- _Disjoint Merge_: take the average of task vector values that agree with the majority sign, thus ignoring parameters that are either trimmed or have a sign conflict.
    

Once we have completed these three additional steps, we can perform model merging normally with the resulting task vector. The three steps within TIES-Merging are depicted below. Interestingly, we see in [2] that maintaining only the top 20% of task vector components yields stable performance—_indicating that a large majority of task vector components are redundant!_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd03017b-007a-4a13-9133-ef929fdfc04a_1344x766.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd03017b-007a-4a13-9133-ef929fdfc04a_1344x766.png)

(from [2])

**Less interference is beneficial.** When TIES-merging is analyzed empirically, there are a variety of interesting findings from which we can learn. Overall, we see that TIES merging yields a clear benefit across a variety of experimental seeings; see below. In particular, TIES works well for multiple modalities (text and vision) and is even compatible with [parameter-efficient finetuning](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8175006e-b17f-4927-bd6a-bf0d905aa359_1186x488.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8175006e-b17f-4927-bd6a-bf0d905aa359_1186x488.png)

(from [2])

Compared to baseline techniques, TIES-merging is also found to generalize better to new tasks and have improved scaling properties as the number of models being merged is (reasonably) increased; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F159c85e4-68ad-430a-92d3-c55ca544e2de_1292x1050.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F159c85e4-68ad-430a-92d3-c55ca544e2de_1292x1050.png)

(from [2])

Both components of TIES-Merging—_removing redundant parameter and electing a majority sign_—are important, but properly estimating the majority sign seems to be especially important; e.g., flipping the sign of high magnitude parameters drastically deteriorates performance. Additionally, authors include interesting experiments in [2] that craft an [oracle](https://en.wikipedia.org/wiki/Oracle_machine) for estimating the majority sign by taking the sign of a model trained in a multi-task fashion. Using this sign oracle during the election process of TIES-Merging is actually found to improve performance!

**Further finetuning.** To go beyond the traditional model merging setup, we can perform extra finetuning with a model that is obtained via merging. In this domain, TIES-merging is shown in [2] to provide us with a better starting point. In particular, models obtained via TIES-Merging outperform those obtained via baseline merging techniques after further finetuning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8a7edf7-58ca-4bfc-96b4-042df2151ac7_594x406.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8a7edf7-58ca-4bfc-96b4-042df2151ac7_594x406.png)

(from [2])

#### **[Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099) [3]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7e5fa3a-95b3-4e07-b7f9-d51700a754db_1648x1250.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7e5fa3a-95b3-4e07-b7f9-d51700a754db_1648x1250.png)

(from [3])

> _“In XMen’s Apocalypse, the character can absorb the powers of other mutants to strengthen himself… the protagonist in Super Mario can gain superpowers by absorbing in-game items… we astonishingly find that language models (LMs) can enhance their capabilities by absorbing other models without retraining or even GPUs.”_ - from [3]

Authors in [3] propose an addition to existing model merging methods that is especially effective for language models that have underwent [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised). When studying the different in parameters values between the base model and the model obtained after SFT—_referred to as “delta parameters”_—we see (again) in [3] that these parameter values have a lot of redundancy. As a result, many of these delta parameters can be eliminated via a technique proposed in [3], called **D**rop **A**nd **RE**scale (DARE). The DARE process makes language models finetuned with SFT much more amenable to model merging.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18082791-5848-441a-8694-d3ad66f5c039_704x914.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18082791-5848-441a-8694-d3ad66f5c039_704x914.png)

(from [3])

**What is DARE?** The concept proposed in [3] is actually very simple. We just:

1. Randomly drop delta parameters (with probability `p`).
    
2. Rescale remaining parameters by a factor of `1 / (1 - p)`.
    
3. Add the remaining pruned and scaled parameters to the weights of the pretrained base model.
    

These steps are outlined in the figure above. Notably, _DARE is not a model merging technique_. Rather, it is a technique for sparsifying delta parameters within an SFT model that is found empirically to have minimal impact on the resulting model’s performance. In fact, we can even use DARE to eliminate up to 99% of delta parameters for sufficiently large language models; see below. Such a finding demonstrates that delta parameters for SFT models are highly redundant.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbed1bde3-5c94-44a9-8e63-79b4e6d9a851_962x520.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbed1bde3-5c94-44a9-8e63-79b4e6d9a851_962x520.png)

(from [3])

**Application to model merging.** Although DARE itself is not a model merging technique, it is a useful plug-in for existing methods (e.g., TIES-Merging [2]). The delta parameters considered by DARE are identical to the task vectors considered by prior model merging techniques [1]. DARE sparsifies these task vectors without damaging the underlying model’s performance, _which mitigates interference when the parameters of these models are actually merged_! Assuming models being merged share the same backbone, the chance of interference is much lower when merging multiple models to which DARE has been applied—_many of the delta parameters within these models will have been set to zero_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b0041a-8a33-4bb2-b2c2-f32d4f271727_1656x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b0041a-8a33-4bb2-b2c2-f32d4f271727_1656x930.png)

(from [3])

In [3], authors merge various flavors of language models—_including both [encoder-only](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders) and [decoder-only](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) models_—with and without DARE. In the table above, we see that using DARE on top of existing model merging techniques for decoder-only LLMs tends to (slightly) improve the performance of the merged model. This difference in performance is more noticeable when merging several specialized models (e.g., instruction-following, math, and code models).

To demonstrate its utility, DARE is used to create two 7B parameter LLMs by merging the [NeuralBeagle](https://huggingface.co/mlabonne/NeuralBeagle14-7B) / [Turdus](https://huggingface.co/udkai/Turdus) (`supermario-v1`) and [WildMorcoroni](https://huggingface.co/BarryFutureman/WildMarcoroni-Variant1-7B) / [WestSeverus](https://huggingface.co/PetroGPT/WestSeverus-7B-DPO-v2) (`supermario-v2`) models. These models achieve top performance (at the time) among 7B models on the [Open LLM leaderboard](https://huggingface.co/open-llm-leaderboard); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F773b9392-bec8-4a88-b4a0-3c41e3636298_1122x426.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F773b9392-bec8-4a88-b4a0-3c41e3636298_1122x426.png)

(from [2])

The performance impact of DARE is even more noticeable for encoder-only models; see below (left). The performance of several specialized models can be maintained after merging even while keeping only a small number of delta parameters from each SFT model. However, the rescaling step of DARE is essential to performance under higher levels of sparsity; see below (right).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f6cd8a4-8ea2-4626-ae2e-00fb59012ecd_1436x460.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f6cd8a4-8ea2-4626-ae2e-00fb59012ecd_1436x460.png)

(from [3])

**Can we always use DARE?** DARE is a sparsification technique that provides useful insight about language models (i.e., delta parameters are very sparse) and can be used to yield a slight boost in model merging performance. However, _DARE is not applicable to models in every setting_. We see in [3] that language models finetuned via SFT have uniquely small delta parameters—_minimal modifications are made to the pretrained LLM_. When similar models are finetuned using a [continued pretraining setup](https://arxiv.org/abs/2407.07263), we observe delta parameters with much larger magnitudes. As a result, applying DARE in this setting is more damaging to performance, especially when dropping larger ratios of delta parameters.

> _“This finding further confirms that SFT primarily unlocks the abilities of pre-trained LMs, rather than introducing new capabilities.”_ - from [3]

This finding has connections to prior analysis of the [Superficial Alignment Hypothesis](https://cameronrwolfe.substack.com/i/134561977/lima-less-is-more-for-alignment) [4], which posits that:

- All of a language model’s knowledge is learned during pretraining.
    
- Alignment serves the purposes of teaching the model how to properly surface this knowledge (e.g., style, format, tone, etc.).
    
- Alignment can be highly data-efficient because of this.
    

With this in mind, we should not be too surprised that language models finetuned with SFT—_an alignment technique_—show a relatively small delta compared to the pretrained model. Similarly, continued pretraining should have a larger delta, as it usually serves the purpose of injecting new knowledge into the model.

#### **[WARP: On the Benefits of Weight Averaged Rewarded Policies](https://arxiv.org/abs/2406.16768) [10]**

> _“While weight averaging was initially mostly used for discriminative tasks… it is now becoming popular for generative tasks; its use in KL-constrained RLHF has already shown preliminary successes.”_ - from [10]

As we have seen, model merging has a long history of interesting applications and techniques within deep learning. Recently, however, we have begun to see model merging explored in the context of [LLM alignment](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation). The success of model merging in this domain has had a noticeable impact on pipelines used for training frontier models—_merging is becoming a commonly-used component_.

**Refresher on alignment.** Most LLMs are trained using a three-stage process that includes [pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining), [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations); see below. During pretraining, we train the language model—_using [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction)_—over large amounts of unlabeled text to build a large base of knowledge within the model. From here, we perform SFT and/or RLHF. These algorithms power the LLM finetuning (or alignment) process and are less computationally expensive relative to pretraining.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a07a22d-e7d4-4e78-895f-7cb957adc0d5_1978x902.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a07a22d-e7d4-4e78-895f-7cb957adc0d5_1978x902.png)

Steps of training and aligning a language model

Typically, we first perform SFT over a (relatively) small[14](https://cameronrwolfe.substack.com/p/model-merging#footnote-14-147448898), high-quality set of examples, providing us with a better “starting point” for RLHF. Then, we apply RLHF in an iterative fashion by continually collecting new batches of preference data and further finetuning the model. The purpose of alignment is not to instill new knowledge within the LLM, but rather to teach the model how to surface its existing knowledge to human users in a preferable manner.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa333055d-b628-4bfd-8afb-0a9b3b2f80ae_1806x482.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa333055d-b628-4bfd-8afb-0a9b3b2f80ae_1806x482.png)

RLHF objective with KL divergence (from [10])

**Why do we need model merging?** When using RLHF, we usually add a [Kullback-Leibler (KL) divergence](https://dibyaghosh.com/blog/probability/kldivergence.html) term (shown above) to the objective being used, which measures the distance between the current model and the SFT model—_or some other anchor model_. At a high level, this divergence term captures how much the model has changed during training with RLHF. By making sure this term does not become too large, we can avoid issues like:

- Forgetting of knowledge from pretraining (i.e., the alignment tax [28]).
    
- Reward hacking (e.g., verbose, unsafe, or flawed outputs).
    
- Decline in the diversity of model outputs [29].
    

However, adding this divergence term also hinders the reward optimization, _forming a tradeoff between the model’s final reward and the KL divergence_. As outlined in [10], model merging is an effective technique for finding better tradeoffs between the KL divergence and reward during finetuning with RLHF!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11dfa2ba-897c-47ad-a3bd-85b977c1d5e2_2074x650.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11dfa2ba-897c-47ad-a3bd-85b977c1d5e2_2074x650.png)

(from [10])

**Weight Averaged Rewarded Policies (WARP)** [10], depicted in the figure above, incorporates three types of model merging at three different phases of the LLM alignment process:

1. We use an **Exponential Moving Average (EMA)** of model weights as an anchor for the KL divergence during finetuning with RLHF.
    
2. We independently finetune `M` models via RLHF, then merge the policies using task vectors and **spherical linear interpolation (SLERP)**; see [here](https://en.wikipedia.org/wiki/Slerp) and above.
    
3. We **linearly interpolate** the result towards the (SFT) initialization.
    

SLERP [37] can also be plugged in to any of the other model merging techniques we have seen so far as an alternative to linear interpolation.

> _“Merging task vectors, either with SLERP or LERP, combines their abilities. The difference is that SLERP preserves their norms, reaching higher rewards than the base models.”_ - from [10]

This multi-stage merging approach—_see below for a full outline of each stage_—uses several of the model merging techniques we’ve seen so far, including EMA [24, 27], weight averaging [22], task vectors [1], and WiSE-FT [8].

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb06a7ae-0126-4162-8130-a4af6359e347_1912x996.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb06a7ae-0126-4162-8130-a4af6359e347_1912x996.png)

Three different merging steps in WARP (from [10])

WARP has additional training costs—_due to the need to independently train several models via RLHF_—but has no additional memory or compute requirements at inference time. Model merging yields a unique benefit at each stage of the WARP algorithm. Using the EMA of model weights as an anchor for KL divergence allows the divergence constraint to be relaxed over time—_weights are tied to the SFT model early on and gradient updates become more aggressive at the end of training_.

Similarly, using SLERP to merge the weights of several RLHF models yields an improvement in reward, while interpolating back towards the SFT initialization finds a better tradeoff between reward and KL divergence. WARP is applied in an iterative fashion by performing several “rounds” of finetuning, where the final model at each iteration is used as an initialization for the next. This iterative approach matches the usual strategy used for RLHF within the literature.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d61245d-ba11-4a4e-b758-fdc1b941d68c_1268x1212.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d61245d-ba11-4a4e-b758-fdc1b941d68c_1268x1212.png)

(fr10])

**Using WARP in practice.** The result of this combination of merging techniques is a model that achieves the best possible tradeoff between KL and reward. The benefits of WARP are empirically validated in [10] using Gemma [30]. Models are evaluated in terms of _i)_ their final reward and _ii)_ KL divergence with respect to the SFT initialization. From experiments, we see that WARP achieves a better reward-KL tradeoff compared to other RL-based alignment techniques; see above. Due to this finding, WARP was officially adopted within the alignment process used for the more recent Gemma-2 model [31]; see [here](https://huggingface.co/blog/gemma2) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972c814e-d2a4-429d-9907-dba6cd97de4f_2150x730.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972c814e-d2a4-429d-9907-dba6cd97de4f_2150x730.png)

(from [48])

> _“We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model.”_ - from [47]

**What came before this?** Authors in [10] were not the first to try to address this balance between reward and KL. Elastic reset [47] eliminates KL divergence from the RLHF objective and instead periodically resets the model weights to the EMA of itself throughout the finetuning process, which enables higher rewards to be achieved with less drift. Alternatively, authors in [48] propose a similar strategy for updating the anchor model within RLHF by periodically setting the anchor model equal to the EMA of model weights; see above. Notably, this update strategy is quite similar to the first stage WARP.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde281fd6-a4cd-4cda-be2d-1b55a7d79451_1846x580.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde281fd6-a4cd-4cda-be2d-1b55a7d79451_1846x580.png)

(from [51])

In [50], we see that merging pre and post-RLHF models—_similarly to WiSE-FT or stage three of WARP_—is an effective method of mitigating the alignment tax, allowing us to achieve high reward without significantly degrading benchmark performance relative to the pre-RLHF model. Authors also perform some analysis to show that the benefits of model averaging are related to an increase in feature diversity, which aligns with findings in [42]. Going further, we see in [51] that LLMs finetuned via SFT also tend to suffer from an alignment tax, which can be solved via a model merging strategy that:

- Trains many SFT “sub-models” on different subsets of instruction-following data, thus mitigating sources of bias in the dataset used for SFT.
    
- Merges all SFT sub-models into a single model via a weighted average.
    

As we can see from this work, the ideas explored in WARP have their foundations in prior research. However, WARP combines these ideas into a novel, unified framework that is highly effective in the LLM alignment domain.

#### Advanced Topics in Model Merging Research

Beyond the core papers explored above, a variety of research has been published on the topic of model merging in the last few years, especially due to the technique’s popularity in the LLM domain. In this section, we outline several recent areas of research related to model merging that are especially interesting.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a9721c-206a-49a8-9dd0-704318945131_1090x1430.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a9721c-206a-49a8-9dd0-704318945131_1090x1430.png)

(from [52, 53])

**Merging reward models.** So far, we have seen several examples of techniques for integrating model merging into the preference tuning process for LLMs [10, 47, 48, 50]. A few recent papers have focused on the more specific application of merging for creating better reward models—_a single step within the RLHF process_. The two primary techniques in this space, which are quite similar to each other, are called **W**eight **A**veraged **R**eward **M**odels (WARM) [52] and Reward Soups [53]. This research, which aims to lessen the chances of reward hacking, independently trains several reward models with different hyperparameters and merges all of these models together; see above. Reward models obtained via such a merging strategy are found to be more robust and reliable compared to individual reward models, leading to improved downstream results with RLHF.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fe13695-1db4-427a-bcae-70eebcf6da74_2228x860.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fe13695-1db4-427a-bcae-70eebcf6da74_2228x860.png)

(from [38, 39])

**Merging independent models.** Usually, we merge models that are derived from the same pretrained model. These models share the same architecture and are guaranteed to be linearly mode connected. However, recent work has explored merging models that are trained independently. The foundation of this work is the idea of _permutation invariance_ [38, 39], which claims that all models trained to convergence via SGD—_even those that are independently trained_—lie in the same region of the loss landscape if we permute (or shuffle) their weights properly; see above. If we find the proper permutation such that this is the case (i.e., see [38] for an algorithm to do this), these models are linearly mode connected—_as shown in [39]_—and we can successfully merge their weights.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0c8237-c3b5-4ff7-9a37-e74376fbdede_1874x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f0c8237-c3b5-4ff7-9a37-e74376fbdede_1874x782.png)

(from [40])

More recent work [40] has also explored distillation-based strategies for merging several LLMs with different architectures by _i)_ computing the outputs of several source LLMs for each input text and _ii)_ training a target LLM to align with the output distributions produced by each of the source models; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F607153bc-76c7-47fe-b27c-b959d7fdc7c8_1608x1112.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F607153bc-76c7-47fe-b27c-b959d7fdc7c8_1608x1112.png)

(from [32])

**Evolutionary model merging.** Evolutionary algorithms are a class of optimization techniques that serve as an alternative to gradient-based optimization. Instead of using gradient descent to train a model, we simply evolve a population of models by iteratively selecting (and modifying) a dynamic, high-performing—_as measured by a loss or objective function_—subset of models within the population; see [here](https://blog.evolv.ai/ai-101-intro-to-evolutionary-algorithms) for an overview of this topic. In [32], authors explore the intersection of evolutionary algorithms and model merging, finding that we can use these techniques to automatically discover effective combinations of open-source models; see above. In particular, we can evolve the weights used to merge a group of models at each layer, finding a more optimal model combination during the merging process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c476308-7fce-49c5-bf25-7287a61d1cde_1612x692.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c476308-7fce-49c5-bf25-7287a61d1cde_1612x692.png)

(from [34])

**Adaptive (or automatic) merging.** Instead of evolving the coefficients used for model merging, authors in [34] show that we can use unsupervised methods to discover optimal merging parameters without the need for any training data. This technique, called AdaMerge, is based on the idea of [entropy minimization](https://proceedings.neurips.cc/paper_files/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf). We take a bunch of unlabeled data, compute the model’s predictions on this data, and train the merging coefficients to minimize the entropy of these predictions; see above. Given that this technique is mostly applied to classification tasks with [ViT](https://huggingface.co/docs/transformers/en/model_doc/vit) models in [34], this means that we train the model to place most of its probability mass on a single class, as opposed to being “unsure” (i.e., assigning relatively high probability to several classes). This technique is shown to yield competitive results in the multi-task learning domain for classification models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2879ff64-a27f-4c6a-be74-a29cd10269b6_2142x772.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2879ff64-a27f-4c6a-be74-a29cd10269b6_2142x772.png)

(from [33])

**Passthrough merging.** LLM practitioners have recently explored the idea of combining several LLMs into one by simply concatenating their layers. This approach yields models with weird parameter counts (e.g., [Goliath-120B](https://huggingface.co/alpindale/goliath-120b) or [Solar-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)), referred to as Frankenstein models (or “FrankenMerges”) by the AI community; see [here](https://huggingface.co/blog/mlabonne/merge-models#4-passthrough) for details. Passthrough merging has not been explored extensively in the literature, but authors in [33] do discuss this technique—_referred to as depth up-scaling in their report_—for Solar-10.7B; see above. Nonetheless, passthrough merging has been used extensively by practitioners, resulting in the creation of several impressive models when paired with additional training.

## Concluding Thoughts

We have seen a lot of information within this overview, but the fundamental idea we have covered is incredibly simple—_taking a (weighted) average of weights for a group of models is an effective way of combining them_. This simple idea has spawned an entire field of research, impacting a variety of important topics like neural network training and optimization, transfer learning, sparsity and pruning, LLM alignment, and more. To summarize some of this amazing research, a few basic takeaways from model merging research over the years have been outlined below.

**A new technique?** The idea of model merging is not new whatsoever. In fact, this idea is nearly as old as the concept of an ensemble [7]. The intuition behind model merging is quite simple. We know that forming an output-space ensemble of several machine learning models can benefit performance, but this approach increases inference costs. Instead, we can take a weight-space ensemble via a (weighted) average of the models’ parameters, forming a single merged model with performance that rivals that of an ensemble. Many papers have extended this concept over time, but the fundamental idea and intuition remains the same!

**Why does this work?** To understand the effectiveness of model merging, we need to understand two fundamental concepts: _mode connectivity_ and _sparsity_. Linear mode connectivity tells us that different finetuned models lie in a similar basin of the loss landscape and that all models obtained via interpolation of these models have constant performance. For this reason, merging—_or interpolating_—models just yields another high-performing model! Plus, the fact that deep networks (including LLMs) exhibit high levels of sparsity implies that the likelihood of a conflict occurring between merged model parameters is relatively low.

**Where should we start?** The most basic implementation of model merging that we can derive is a uniform average of model weights, which works well in many cases. Beyond this simple approach, the fundamental idea behind model merging is best characterized by the concept of a task vector [1]. After creating task vectors for different finetuned models, we can perform arbitrary arithmetic and create a variety of interesting model combinations or merges, as well as perform (weighted) model averaging. To determine the optimal weights to use for model merging, we can simply measure performance on a hold-out validation set.

**Advanced strategies.** If simple model merging techniques based upon averaging or task vectors do not yield our desired result, we can explore more advanced model merging techniques, such as TIES, DARE, SLERP, and more. These strategies improve upon baseline performance by increasing sparsity, accounting for interference between merged parameters, maintaining useful geometric properties of merged model weights, and more. Going further, new merging techniques (e.g., Passthrough merging) are being proposed all the time. We should always try simple techniques first, but more advanced merging strategies have been applied successfully in practice and may be beneficial to some use cases.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Ilharco, Gabriel, et al. "Editing models with task arithmetic." _arXiv preprint arXiv:2212.04089_ (2022).

[2] Yadav, Prateek, et al. "Ties-merging: Resolving interference when merging models." _Advances in Neural Information Processing Systems_ 36 (2024).

[3] Yu, Le, et al. "Language models are super mario: Absorbing abilities from homologous models as a free lunch." _Forty-first International Conference on Machine Learning_. 2024.

[4] Zhou, Chunting, et al. "Lima: Less is more for alignment." _Advances in Neural Information Processing Systems_ 36 (2024).

[5] Wortsman, Mitchell, et al. "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time." _International conference on machine learning_. PMLR, 2022.

[6] Dietterich, Thomas G. "Ensemble methods in machine learning." _International workshop on multiple classifier systems_. Berlin, Heidelberg: Springer Berlin Heidelberg, 2000.

[7] Utans, Joachim. "Weight averaging for neural networks and local resampling schemes." _Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press_. Citeseer, 1996.

[8] Wortsman, Mitchell, et al. "Robust fine-tuning of zero-shot models." _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 2022.

[9] Jang, Dong-Hwan, Sangdoo Yun, and Dongyoon Han. "Model Stock: All we need is just a few fine-tuned models." _arXiv preprint arXiv:2403.19522_ (2024).

[10] Ramé, Alexandre, et al. "WARP: On the Benefits of Weight Averaged Rewarded Policies." _arXiv preprint arXiv:2406.16768_ (2024).

[11] Garipov, Timur, et al. "Loss surfaces, mode connectivity, and fast ensembling of dnns." _Advances in neural information processing systems_ 31 (2018).

[12] Frankle, Jonathan, et al. "Linear mode connectivity and the lottery ticket hypothesis." _International Conference on Machine Learning_. PMLR, 2020.

[13] Neyshabur, Behnam, Hanie Sedghi, and Chiyuan Zhang. "What is being transferred in transfer learning?." _Advances in neural information processing systems_ 33 (2020): 512-523.

[14] Rofin, Mark, Nikita Balagansky, and Daniil Gavrilov. "Linear interpolation in parameter space is good enough for fine-tuned language models." _arXiv preprint arXiv:2211.12092_ (2022).

[15] Sun, Mingjie, et al. "Massive activations in large language models." _arXiv preprint arXiv:2402.17762_ (2024).

[16] Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." _arXiv preprint arXiv:1803.03635_ (2018).

[17] LeCun, Yann, John Denker, and Sara Solla. "Optimal brain damage." _Advances in neural information processing systems_ 2 (1989).

[18] Hassibi, Babak, David G. Stork, and Gregory J. Wolff. "Optimal brain surgeon and general network pruning." _IEEE international conference on neural networks_. IEEE, 1993.

[19] Frantar, Elias, and Dan Alistarh. "Sparsegpt: Massive language models can be accurately pruned in one-shot." _International Conference on Machine Learning_. PMLR, 2023.

[20] Sun, Mingjie, et al. "A simple and effective pruning approach for large language models." _arXiv preprint arXiv:2306.11695_ (2023).

[21] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. In NeurIPS, 2022.

[22] Izmailov, Pavel, et al. "Averaging weights leads to wider optima and better generalization." _arXiv preprint arXiv:1803.05407_ (2018).

[23] David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.

[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[25] Laine, Samuli and Aila, Timo. Temporal Ensembling for Semi-Supervised Learning. arXiv:1610.02242 [cs], October 2016. arXiv: 1610.02242.

[26] Tarvainen, Antti, and Harri Valpola. "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results." _Advances in neural information processing systems_ 30 (2017).

[27] Sanyal, Sunny, et al. "Early weight averaging meets high learning rates for llm pre-training." _arXiv preprint arXiv:2306.03241_ (2023).

[28] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[29] Kirk, Robert, et al. "Understanding the effects of rlhf on llm generalisation and diversity." _arXiv preprint arXiv:2310.06452_ (2023).

[30] Team, Gemma, et al. "Gemma: Open models based on gemini research and technology." _arXiv preprint arXiv:2403.08295_ (2024).

[31] Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size." _arXiv preprint arXiv:2408.00118_ (2024).

[32] Akiba, Takuya, et al. "Evolutionary optimization of model merging recipes." _arXiv preprint arXiv:2403.13187_ (2024).

[33] Kim, Dahyun, et al. "Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling." _arXiv preprint arXiv:2312.15166_ (2023).

[34] Yang, Enneng, et al. "Adamerging: Adaptive model merging for multi-task learning." _arXiv preprint arXiv:2310.02575_ (2023).

[35] Rame, Alexandre, et al. "Diverse weight averaging for out-of-distribution generalization." _Advances in Neural Information Processing Systems_ 35 (2022): 10821-10836.

[36] Ramé, Alexandre, et al. "Model ratatouille: Recycling diverse models for out-of-distribution generalization." _International Conference on Machine Learning_. PMLR, 2023.

[37] Ken Shoemake. Animating rotation with quaternion curves. In SIGGRAPH, 1985.

[38] Ainsworth, Samuel K., Jonathan Hayase, and Siddhartha Srinivasa. "Git re-basin: Merging models modulo permutation symmetries." _arXiv preprint arXiv:2209.04836_ (2022).

[39] Entezari, Rahim, et al. "The role of permutation invariance in linear mode connectivity of neural networks." _arXiv preprint arXiv:2110.06296_ (2021).

[40] Wan, Fanqi, et al. "Knowledge fusion of large language models." _arXiv preprint arXiv:2401.10491_ (2024).

[41] Cha, Junbum, et al. "Swad: Domain generalization by seeking flat minima." _Advances in Neural Information Processing Systems_ 34 (2021): 22405-22418.

[42] Lin, Yong, et al. "Spurious feature diversification improves out-of-distribution generalization." _arXiv preprint arXiv:2309.17230_ (2023).

[43] Zaman, Kerem, Leshem Choshen, and Shashank Srivastava. "Fuse to forget: Bias reduction and selective memorization through model fusion." _arXiv preprint arXiv:2311.07682_ (2023).

[44] Eeckt, Steven Vander. "Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition." _arXiv preprint arXiv:2210.15282_ (2022).

[45] Lawson, Daniel, and Ahmed H. Qureshi. "Merging decision transformers: Weight averaging for forming multi-task policies." _2024 IEEE International Conference on Robotics and Automation (ICRA)_. IEEE, 2024.

[46] Nikishin, Evgenii, et al. "Improving stability in deep reinforcement learning with weight averaging." _Uncertainty in artificial intelligence workshop on uncertainty in Deep learning_. 2018.

[47] Noukhovitch, Michael, et al. "Language model alignment with elastic reset." _Advances in Neural Information Processing Systems_ 36 (2024).

[48] Gorbatovski, Alexey, et al. "Learn your reference model for real good alignment." _arXiv preprint arXiv:2404.09656_ (2024).

[49] Xiao, Shitao, et al. "Lm-cocktail: Resilient tuning of language models via model merging." _arXiv preprint arXiv:2311.13534_ (2023).

[50] Lin, Yong, et al. “Mitigating the alignment tax of rlhf.” arXiv preprint _arXiv:2309.06256_ (2024).

[51] Fu, Tingchen, et al. "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction." _arXiv preprint arXiv:2405.13432_ (2024).

[52] Ramé, Alexandre, et al. "Warm: On the benefits of weight averaged reward models." _arXiv preprint arXiv:2401.12187_ (2024).

[53] Rame, Alexandre, et al. "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards." _Advances in Neural Information Processing Systems_ 36 (2024).

[54] Goddard, Charles, et al. "Arcee's MergeKit: A Toolkit for Merging Large Language Models." _arXiv preprint arXiv:2403.13257_ (2024).

[1](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-1-147448898)

[Multi-task learning](https://arxiv.org/abs/2404.18961) is an alternative technique that avoids this issue with increased inference cost. However, multi-task learning is not a clean alternative to a model ensemble, as the situations in which these two techniques would be applied can be different (e.g., we can create an ensemble of models trained on only one task).

[2](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-2-147448898)

The “deep learning era” in this context is considered to be anything after [AlexNet](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) was proposed in 2012.

[3](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-3-147448898)

Here, we assume that the networks have identical size / architecture and are just trained independently with different random seeds (potentially with some differences in other hyperparameters or settings).

[4](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-4-147448898)

By this, we mean that any weights along this connected path between the weights of the original networks yield a model with training and test accuracy that match or improve upon that of the original models.

[5](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-5-147448898)

This overview was the first Deep (Learning) Focus newsletter that was ever released!

[6](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-6-147448898)

Performance is measured both on the normal ImageNet test set, as well as several other ImageNet-related test sets with various distribution shifts (e.g., [ImageNet-V2](https://imagenetv2.org/), [ImageNet-R](https://github.com/hendrycks/imagenet-r), [ImageNet-Sketch](https://github.com/HaohanWang/ImageNet-Sketch), and more).

[7](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-7-147448898)

Prior state-of-the-art was 90.88%, achieved by [CoAtNet](https://arxiv.org/abs/2106.04803). Today, state-of-the-art performance on ImageNet is [slightly higher](https://paperswithcode.com/sota/image-classification-on-imagenet) (around 92%).

[8](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-8-147448898)

See [this paper](https://arxiv.org/abs/2305.14201) as a great example of this phenomenon. We can outperform GPT-4 on particular target domains via finetuning!

[9](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-9-147448898)

Also, changing the coefficient used during model merging is easy! No extra training is required, whereas tweaking hyperparameters requires the model to be retrained.

[10](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-10-147448898)

Notably, all comparisons are performed in a layer-wise fashion, meaning that we only compare the weights of different finetuned models within the same layer.

[11](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-11-147448898)

This idea is explored in [22] for deep learning, but this technique in general is not new and can be seen in much earlier work; e.g., see [23] for an example.

[12](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-12-147448898)

Again, this finding has a strong connection to research on model pruning, where we see [very similar observations](https://arxiv.org/abs/1810.05270)!

[13](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-13-147448898)

This sign vector is just a vector with the same size as a task vector, where each element is either -1 or 1. The sign vector indicates the majority sign of each parameter across task vectors, as dictated by the elect sign step.

[14](https://cameronrwolfe.substack.com/p/model-merging#footnote-anchor-14-147448898)

In most cases, the size of SFT datasets are within the range of a few thousand to a few tens of thousands of examples, but we have seen SFT applied successfully with [as few as 1K examples](https://arxiv.org/abs/2305.11206) (i.e., the algorithm is very data efficient).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Anish's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8937bb07-d892-4f59-a6bd-32da0c1f321c_1080x1080.jpeg)



](https://substack.com/profile/90205286-anish)

[

![Luke's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfdd24ae-7b1a-4d40-bb66-313988e7a590_144x144.png)



](https://substack.com/profile/1258562-luke)

[

![Chan Kim's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8616d08e-9779-4035-b4d2-2f75f90a59ea_1024x1024.jpeg)



](https://substack.com/profile/139919115-chan-kim)

[

![SUVROJYOTI BISWAS's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d1ec132-0fbc-4708-bb73-f628e913b2d2_953x953.jpeg)



](https://substack.com/profile/219888043-suvrojyoti-biswas)

[

![Madan Kumar Y's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd17ef447-c4da-439e-ab8d-a2407e2458b2_144x144.png)



](https://substack.com/profile/51267156-madan-kumar-y)

71 Likes∙

[8 Restacks](https://substack.com/note/p-147448898/restacks?utm_source=substack&utm_content=facepile-restacks)

71

- 

[

8

](https://cameronrwolfe.substack.com/p/model-merging/comments)

8

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Devansh's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff955b89-d08e-4cb7-8add-709e6dc14d8e_1080x1080.jpeg)



](https://substack.com/profile/8101724-devansh?utm_source=comment)

[Devansh](https://substack.com/profile/8101724-devansh?utm_source=substack-feed-item)

[Artificial Intelligence Made Si…](https://artificialintelligencemadesimple.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[9月18日](https://cameronrwolfe.substack.com/p/model-merging/comment/69399537 "2024年9月18日 09:59")

Liked by Cameron R. Wolfe, Ph.D.

How you continue to keep up this quality every single time is beyond me. I'm convinced that you're three different people pretending to be one

Like (3)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/model-merging/comment/69399537)

[

![Obrian Henry's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27fddcfd-ebf9-48af-82d9-1331d5b8a902_4167x4167.png)



](https://substack.com/profile/45646766-obrian-henry?utm_source=comment)

[Obrian Henry](https://substack.com/profile/45646766-obrian-henry?utm_source=substack-feed-item)

[Obrian’s Newsletter](https://obrian.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[9月17日](https://cameronrwolfe.substack.com/p/model-merging/comment/69302804 "2024年9月17日 18:59")

Liked by Cameron R. Wolfe, Ph.D.

Another incredible and detailed article. This substack is probably the most detailed I have seen as it relates to LLMs and deep learning concepts.

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/model-merging/comment/69302804)

[6 more comments...](https://cameronrwolfe.substack.com/p/model-merging/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


-----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Automatic Prompt Optimization

### Practical techniques for improving prompt quality without manual effort...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Nov 04, 2024

87

- 

[

9

](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comments)

7

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe277e337-6419-431b-8da2-e1741f8a875c_2166x1216.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe277e337-6419-431b-8da2-e1741f8a875c_2166x1216.png)

(from [2, 3, 12, 14, 15, 16])

Prompting—_both of large language models (LLMs) in particular and other types of frontier models_—is arguably one of the most impactful advancements in modern AI. By writing an instruction in natural language, nearly anyone can leverage the power of massive neural networks to solve a wide variety of practical tasks. Prior generations of deep learning models have achieved impressive feats in terms of performance, but LLMs do more than just perform well—_they are intuitive to use_. Non-experts can interact with these models and see their potential first hand, which has directed an unprecedented amount of attention to AI research.

> _“By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans.”_ - from [1]

Although prompts provide an intuitive interface to LLMs, this interface is far from perfect. Many models are overly sensitive to small changes in the prompt. Tweaking the prompt’s wording or structure can lead to incorrect and unexpected results. For this reason, writing effective prompts requires non-trivial domain expertise and has become a sought-after skill. Most prompts are created via an iterative trial and error process with a human in the loop. We just tweak and test the prompt repeatedly, using our knowledge of prompting—_and of the LLM being prompted_—to guide our search towards a prompt that performs well.

**Improving prompts automatically.** Prompting requires a lot of human effort and is imperfect by nature. To mitigate these issues, recent research has explored the idea of automatic prompt optimization, which uses data to improve the quality of a prompt algorithmically. There are a few key benefits to this approach:

1. Less manual effort is required to find a good prompt.
    
2. Prompts are searched for and discovered systematically, enabling prompts that exceed the performance of those written by humans to be found.
    

Prompt optimization techniques allow us to improve the quality of our prompts automatically, instead of relying upon heuristics and domain knowledge. In this overview, we will learn about the extensive literature that has been published on this topic, focusing on the most practical techniques for creating better prompts.

## Preliminaries: Prompting and Optimization

This overview will primarily study the overlap of optimization and prompt engineering, but we must not forget that these two ideas—_optimization and prompt engineering_—are also distinct fields of research. By learning about these ideas in isolation, we will understand how the key concepts behind them can be combined to create algorithms for automatically improving the quality of a prompt.

#### What is optimization?

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16435ce3-e4df-4640-a15b-1e4b41df5820_1172x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16435ce3-e4df-4640-a15b-1e4b41df5820_1172x606.png)

Optimizing a function

Optimization refers to the idea of finding an optimal solution or point within a certain function, referred to as the “objective function”. Usually, this means finding the point that minimizes (or maximizes) the value of the function; see above. By finding this optimal point, we are “optimizing” the function, or performing optimization_._ Having existed [for centuries](http://www.mitrikitti.fi/opthist.html), the field of optimization is rich, broad, and incredibly valuable. The idea of optimizing an objective function can be used to solve a massive number of important tasks, including anything from routing traffic to training a neural network. A comprehensive explanation of optimization is beyond the scope of this post, but [many resources](https://web.stanford.edu/~boyd/cvxbook/) are available.

**Types of optimization algorithms.** Countless optimization algorithms have been proposed. Even for the specific application of training a neural network, there is a [vast array of optimization algorithms](https://www.ruder.io/optimizing-gradient-descent/) to choose from. Despite this variety, optimization algorithms can be loosely grouped into two categories:

1. Gradient-based
    
2. Gradient-free
    

**Gradient-based optimization** repeatedly _i)_ computes the gradient of our function and _ii_) uses this gradient to update the current solution. The gradient points in the direction of (locally) increasing function value, so we can use the gradient to find regions of higher or lower function values. We show a depiction of this process below, where we continually compute the gradient and move in the opposite direction of the gradient to minimize a function (i.e., [gradient descent](https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe1c7ee5-60e3-44d0-b3d6-e8b9766ec167_1500x843.gif)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe1c7ee5-60e3-44d0-b3d6-e8b9766ec167_1500x843.gif)

A schematic depiction of gradient descent

Machine learning practitioners are oftentimes most familiar with gradient-based optimization algorithms. These algorithms underlie the idea of [backpropagation](https://www.nature.com/articles/323533a0), which is used almost universally for training neural networks and other machine learning models. Gradient-based algorithms are popular because they are both effective and efficient! Even when optimizing a large number of variables (e.g., the parameters of an LLM), finding a solution is computationally feasible by just following the direction of the gradient, which is (relatively) cheap to compute.

**Gradient-free algorithms** are a class of optimization algorithms that do not use any gradient information; e.g., [brute force search](https://www.geeksforgeeks.org/brute-force-approach-and-its-pros-and-cons/) and [hill climbing](https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/) are two basic examples. Many gradient-free optimization algorithms exist, but one of the most popular class of algorithms—_and the class of algorithms we will encounter in this overview_—is evolutionary algorithms (EA); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8daaba2-727e-4d6f-971b-40373d9c7bce_942x797.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8daaba2-727e-4d6f-971b-40373d9c7bce_942x797.png)

(from [18])

Inspired by the idea of biological evolution, EAs maintain a “population” of candidate solutions and repeatedly:

- Modify the members of this population—_the candidate solutions_—via evolutionary operators like mutation and crossover to produce new population members, simulating the act of reproduction.
    
- Select the best members—_based upon some objective function_—from the population to continue evolving (i.e., survival of the fittest).
    

Many variants of EAs exist [19], but the most common instantiations are [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) and [differential evolution](https://en.wikipedia.org/wiki/Differential_evolution). In general, EAs are considered to be less efficient compared to gradient-based optimization algorithms. For example, training a large neural network (e.g., an LLM) is difficult without gradient information. The search space is very large, so tweaking the model’s parameters, measuring the loss, and hoping to find a better LLM will not get us very far.

> _“EAs typically start with an initial population of_ `N` _solutions, then iteratively generate new solutions using evolutionary operators (e.g., mutation and crossover) on the current population and update it based on a fitness function.”_ - from [16]

However, there are also many benefits to EAs; e.g., these algorithms balance the tradeoff between exploration and exploitation very well. Gradient-based algorithms produce a single solution, while EAs maintain an entire population! This property is very useful for certain classes of problems, which has led to the practical adoption of EAs in many interesting domains such as [evolving neural network architectures](https://www.uber.com/blog/deep-neuroevolution/) or optimizing computer network topologies [19].

**Using LLMs as optimizers.** Recently, researchers have explored the idea of using LLMs as gradient-free optimizers. To do this, we provide information to the LLM about the current solution to a problem—_including the solution’s performance_—and the optimization trajectory (i.e., the history of previous solutions). Then, we can prompt an LLM to produce a new—_and hopefully better_—solution to the problem. By measuring the performance of the new solution and repeating this process, we create a new type of optimization algorithm. As we will see, such LLM-based optimization techniques are used regularly for optimizing prompts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb00ddef3-0c02-42ab-b1bb-9c368236b1ec_1298x1396.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb00ddef3-0c02-42ab-b1bb-9c368236b1ec_1298x1396.png)

(from [28])

Many papers cover the topic of using LLMs as optimizers. For example, authors in [3] use LLMs to optimize simple regression problems and solve the [traveling salesman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem). Researchers have also explored:

- Combining LLMs with EAs [27, 28].
    
- Using LLMs for hyperparameter optimization [29].
    
- Automating [neural architecture search](https://en.wikipedia.org/wiki/Neural_architecture_search) with an LLM [30].
    

> _“Instead of formally defining the optimization problem and deriving the update step with a programmed solver, we describe the optimization problem in natural language, then instruct the LLM to iteratively generate new solutions based on the problem description and the previously found solutions.”_ - from [3]

#### The Basics of Prompt Engineering

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e098a6a-3681-44e7-a3d8-b04555d8492f_2080x784.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e098a6a-3681-44e7-a3d8-b04555d8492f_2080x784.png)

The generic, text-to-text structure of an LLM is both intuitive and powerful. Instead of having a scientist train a specialized / narrow model over data for each task that needs to be solved, we can have (almost) anyone explain the problem in writing—_via a prompt_—and generate a reasonable solution with no (or minimal[1](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-1-149351213)) training data by passing this prompt to an LLM; see above. The approachability of prompts unlocks significant potential, as non-ML domain experts can easily prototype and demonstrate impactful applications of LLMs [22].

> _“LLMs are shown to be sensitive to the prompt format; in particular, semantically similar prompts may have drastically different performance, and the optimal prompt formats can be model-specific and task-specific.”_ - from [3]

Unfortunately, LLMs are sensitive to minor changes in prompts—_the exact details of how we phrase and structure our prompt makes a big difference_. This issue can make prompting LLMs difficult for non-experts [21]. As a result, prompt engineering (i.e., the art of creating better prompts) has become a legitimate, popular, and useful skill. This overview will primarily focus on methods for automatically improving prompts, but we will go over some prompting basics here.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba4dbc86-70b9-476e-8c63-78bd6d4c0c75_1266x916.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba4dbc86-70b9-476e-8c63-78bd6d4c0c75_1266x916.png)

Example of a prompt that combines all components

**Anatomy of a prompt.** Although we can take many different approaches for writing prompts, there are a few standard components that are usually present within any kind of prompt:

- _Instruction_: a textual description of the output that is expected of the model.
    
- _Examples / Exemplars_: concrete examples of correct input-output pairs (i.e., demonstrations) that are included within the prompt.
    
- _Context_: any extra information provided to the LLM in the prompt.
    
- _Input Data_: the actual data that the LLM is expected to process (e.g., the sentence being translated or classified, the document being summarized, etc.).
    
- _Structure / Indicators_: different tags, headers, or organizational strings that we can include to separate or structure the parts of our prompt.
    

An example of a prompt that uses all of these components is shown above, but a prompt need not contain all of these components. We can selectively leverage them based on the details or needs of the problem we are trying to solve.

**Prompting techniques.** In prior overviews, we have studied different classes of prompting techniques extensively:

- [Practical Prompt Engineering](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part): basic concepts, zero / few shot prompting, instructions, and more.
    
- [Advanced Prompt Engineering](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering): chain of thought (CoT) prompting, variants of CoT prompting, context retrieval, and more.
    
- [Modern Advances in Prompt Engineering](https://cameronrwolfe.substack.com/p/modern-advances-in-prompt-engineering): recent prompting research (e.g., tool usage, reasoning, program-aided prompts, long-form writing, and more).
    

Beyond the overviews listed above, there are countless online resources for learning more about prompt engineering; e.g., [learn prompting](https://learnprompting.org/docs/introduction), [prompting guide (from The Gradient)](https://thegradient.pub/prompting/), prompt engineering surveys [23, 24, 25], and more.

**Framework for writing better prompts.** Prompt engineering is an empirical science that is largely based upon trial-and-error. To write better prompts, we should continually _i)_ tweak the original prompt, _ii)_ measure the performance of the new prompt, and _iii)_ select the better prompt. We also want to ensure that our prompt is not needlessly complex. There are a few reasons for this:

- If a complex prompt is performing poorly, how will we know what part of the prompt is broken and what needs to be fixed?
    
- Complex prompts are typically longer and will consume more tokens, which increases monetary costs.
    

For these reasons, we should begin the prompt engineering process with a simple prompt (e.g., a few-shot or instruction-based prompt). Then, we (slowly) increase the prompt’s complexity—_by adding few-shot examples, using advanced techniques like CoT, or just tweaking the instruction_—while monitoring performance over time. By following this process, we improve our prompt and justify increased complexity with improved performance; see below. The process ends once we have a prompt that reaches an acceptable level of performance for our use case.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F984f6f8a-e5e8-4e06-a681-48f52b6c1d3f_1456x442.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F984f6f8a-e5e8-4e06-a681-48f52b6c1d3f_1456x442.png)

**Prompt engineering is just optimization!** If we examine the steps of the prompt engineering process outlined above, _we will realize that this is an optimization problem_! We repeatedly tweak the solution—_our prompt_—and analyze whether the new solution is better or not. In this setup, the “optimizer” is a human prompt engineer who uses their judgement and knowledge of prompting to determine the next best prompt to try out. Given that prompt engineering requires so much trial and error, however, we might wonder whether we can replace[2](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-2-149351213) the human in this optimization process with an automated approach or algorithm.

> _“The large and discrete prompt space makes [prompt engineering] challenging for optimization, especially when only API access to the LLM is available.”_ - from [3]

**Why is it difficult to optimize prompts?** As we just learned, there are two classes of optimization algorithms that we can consider for automating the prompt engineering process. Unfortunately, using gradient-based algorithms for prompt optimization is difficult for a few reasons:

1. For many LLMs, we only have access to an API, which prevents us from gathering / using any gradient information.
    
2. Prompts are composed of discrete tokens and using gradient-based algorithms to optimize a discrete solution is complicated.
    

As we will see in this overview, there are ways that we can avoid these issues and use gradient-based optimization to find better prompts. However, we will mostly rely upon gradient-free algorithms for prompt optimization due to the reasons listed above. In fact, EAs are actually one of the most successful and widely-used classes of algorithms for discrete optimization problems [26]!

## Early Work on Prompt Optimization

The idea of optimizing a prompt is not new. Researchers began studying this problem as soon as the idea of a prompt was introduced. In this section, we will study some of the early work on prompt optimization, which was applied to both [encoder-only](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) variants of language models as well as [GPT-style](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) LLMs. This work inspired later techniques for prompt optimization that are commonly used today.

#### Generating Synthetic Prompts

As mentioned above, directly optimizing a prompt is difficult due to the discrete nature of tokens within the prompt. One way that we can work around this issue is by training an LLM to generate the prompt as output. By doing this, we can train the weights of the LLM that generates the prompt, instead of trying to optimize the prompt directly. This is a practical and commonly-used technique. As we will see, however, most work in this space uses pretrained LLMs to write prompts and does not explicitly optimize the LLM to write better prompts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cb470ac-dd7f-453e-9cff-1965bbcec5b5_1072x606.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cb470ac-dd7f-453e-9cff-1965bbcec5b5_1072x606.png)

(from [7])

**Instruction induction [7]** is one of the first works to explore generating a prompt with a pretrained LLM. This technique aims to predict an underlying task given a few concrete ([in-context](https://thegradient.pub/in-context-learning-in-context/)) examples of that task as input. We craft a prompt that contains several input-output examples and ends by asking the LLM to infer the task being solved by completing the sequence “the instruction was…”; see above.

> _“We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions.”_ - from [7]

LLMs do not solve this task out-of-the-box; e.g., a pretrained [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners) model achieves only 10% accuracy. However, larger models begin to perform well on this task when they have been aligned to follow instructions. The [InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback) model—_an early, GPT-3-based LLM variant that is [aligned to follow human instructions via RLHF](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)_—achieves nearly 70% accuracy on the instruction induction task.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa20e2f81-368f-444a-b7a6-b8ffe7dac581_1836x1034.png)

(from [8])

**Self-Instruct [8]** was one of the first frameworks proposed for using LLMs to generate synthetic [instruction tuning](https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/) datasets. Beginning with a small set of seed tasks, this framework prompts an LLM—_using the seed tasks as input_—to generate new tasks that can be solved. Afterwards, we can use the same LLM to create concrete demonstrations for each task, which are filtered based on quality. The result of this pipeline is an LLM-generated instruction tuning dataset, containing a large number of input-output examples for a wide variety of tasks.

> _“Self-Instruct … [improves] the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates … samples from a language model, then filters invalid or similar ones before using them to finetune the original model.”_ - from [8]

Many papers use similar frameworks to generate synthetic instructions. For example, authors in [10] use a pretrained LLM to diversify a seed set of prompts and discover better prompts for analyzing an LLM’s knowledge base. Variants of Self-Instruct have also been proposed, such as a [second version](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#data-generation-process) of the technique that reduces generation costs while improving data quality.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc19ffb-79cf-4e30-b260-9f325c32a351_1110x688.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc19ffb-79cf-4e30-b260-9f325c32a351_1110x688.png)

(from [9])

**WizardLM [9]** designs a Self-Instruct style framework that is tailored towards creating highly complex instruction tuning datasets. The crux of this method is a technique called EvolInstruct that uses an LLM to iteratively rewrite—_or evolve_—instructions to increase their complexity. There are two basic ways that we can evolve any given instruction:

1. _In-Depth_: make the current instruction more complex by adding constraints, requiring more reasoning steps, complicating the input, and more (i.e., keep the same instruction and make it more complex).
    
2. _In-Breadth_: enhancing the topic coverage, skill coverage, and overall diversity of the instruction tuning dataset (i.e., generate an instruction for a topic that is not covered in the data yet).
    

Each of these types of evolution has numerous variants. Example prompts for both in-depth and in-breadth evolution in [9] are provided below, though many additional prompts are leveraged in the implementation of EvolInstruct.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d4e30f6-6230-460e-8a14-e194688b4e05_2568x1124.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d4e30f6-6230-460e-8a14-e194688b4e05_2568x1124.png)

Examples of prompts for evolution (from [9])

Given these strategies for evolving a prompt, we can apply a three step approach of _i)_ evolving instructions, _ii)_ generating instruction responses, and _iii)_ removing or eliminating prompts that are not viable or fail to become more complex. By following these steps, we can bootstrap the Self-Instruct framework to generate a synthetic dataset with highly complex instructions; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447dac7-a51e-4c8d-b12b-866ec00a158f_1062x1030.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447dac7-a51e-4c8d-b12b-866ec00a158f_1062x1030.png)

(from [9])

#### Soft Prompts: Prefix and Prompt Tuning

If we want to improve a prompt, we can begin by simply tweaking the wording or instructions present within the prompt. This approach is called “hard” prompt tuning[3](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-3-149351213), as we are explicitly changing the words (or tokens) present within the prompt in a discrete manner; see above. However, hard prompt tuning is not the only tool available to use—_we can explore soft, continuous updates to the prompt_.

> _“Unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens.”_ - from [4]

**Prefix tuning [4]** was one of the first papers to explore continuous updates to the prompt of an LLM. While traditional model finetuning trains all of a model’s parameters over a downstream dataset, prefix tuning leaves nearly all of the model’s parameters fixed. Instead, we append a “prefix”—_or a sequence of additional token vectors at the beginning of the model’s input_—to the model’s prompt and train the contents of this prefix directly. Instead of just updating the input prompt, however, prefix tuning adds a learnable prefix to the input of every transformer block within the underlying model; see below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff89a3ad2-941c-4bb7-84bb-e857db672440_2120x1268.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff89a3ad2-941c-4bb7-84bb-e857db672440_2120x1268.png)

Transformer block with a learnable prefix

In prefix tuning, we train only the added prefixes in the model. Instead of directly training the contents of the prefix, authors in [4] find that passing the learnable prefix through a feed-forward transformation prior to concatenation with each layer’s input makes training much more stable. This technique, which adds more learnable parameters to the model, is referred to as “reparameterization”.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ed7dbb-4325-4057-afa3-1fa417ca3ee2_714x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ed7dbb-4325-4057-afa3-1fa417ca3ee2_714x612.png)

(from [5])

All parameters other than the prefix remain frozen during training, which drastically reduces—_by 100X or more as shown in the figure above_—the total number of parameters being learned during finetuning. Nonetheless, we can drastically improve downstream task performance via prefix tuning; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47049ab3-4cd1-4c51-ab57-d9349ff72621_1628x742.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47049ab3-4cd1-4c51-ab57-d9349ff72621_1628x742.png)

(from [4])

Although prefix tuning performs well in the above experiments, the only base model considered for finetuning experiments in [4] is [GPT-2](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2), meaning that these results remain to be proven with more modern models and datasets.

**Prompt tuning [5]** is a simplification of prefix tuning [4] that was proposed concurrently. Again, we start with a pretrained language model and freeze the model’s parameters. Instead of training the model, we create a “soft” prompt, or a sequence of tokens that are concatenated to the beginning of the LLM’s input. Then, we learn the soft prompt by training it—_similarly to any other model parameter_—over our dataset via gradient descent; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b032ac4-9b72-4020-bd7c-960a1c430093_1056x1192.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b032ac4-9b72-4020-bd7c-960a1c430093_1056x1192.png)

(from [5])

Compared to prefix tuning, prompt tuning only prepends prefix tokens to the input layer—_instead of every layer_—and does not require reparameterization[4](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-4-149351213) for training to be stable. As a result, the total number of parameters learned during prompt tuning is much lower compared to to prefix tuning.

> _“We … show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be competitive with model tuning.”_ - from [5]

Despite the small number of learnable parameters, prompt tuning is surprisingly effective and even catches up with the performance of end-to-end model training as the underlying LLM becomes larger; see below. Additionally, the performance of prompt tuning usually exceeds that of manually-engineered prompts—_though we should note that human performance is dependent on the human writing the prompt_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6f21bfe-88cd-408f-9ed0-e59403bf3d9d_1060x1312.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6f21bfe-88cd-408f-9ed0-e59403bf3d9d_1060x1312.png)

(from [5])

**Are soft prompts actually prompts?** Soft prompts are effective in terms of their performance, but one could argue that these technique actually do nothing to improve the prompt engineering process. The goal of prompt engineering is to discover a text-based prompt that works well given the application domain and LLM being considered. Instead of solving this problem, prompt and prefix tuning eliminate the constraint of our prompt being discrete / text-based. The “prompt” discovered by these techniques is just a continuous vector learned via gradient-based optimization. Similarly to any other model parameter, this vector is not human-interpretable and cannot be transferred for use with another LLM.

> _“This becomes less practical with scale, as computing gradients becomes expensive and access to models shifts to APIs that may not provide gradient access.”_ - from [1]

For this reason, prefix and prompt tuning are closer to parameter efficient finetuning (PEFT) techniques than they are to automatic prompt optimization strategies. Rather than searching for a better prompt, we add a small number of additional parameters to the model and directly train these parameters—_while keeping the pretrained model fixed_—over a smaller, domain specific dataset. These additional parameters just happen to be concatenated to our prompt instead of injected into the internal weight matrices of the model. For more information on PEFT techniques, check out my overview of this topic at the below link.

[

## Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More

](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft)

2023年11月27日

[![Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More](https://substackcdn.com/image/fetch/w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff70830a4-d0ce-42d8-a814-bd02d73042e5_2410x1354.png)](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft)

Instead of training the full model end-to-end, parameter-efficient finetuning leaves pretrained model weights fixed and only adapts a small number of task-specific parameters during finetuning. Such an approach drastically reduces memory overhead, simplifies the storage/deployment process, and allows us to finetune LLMs with more accessible hardware.

[

Read full story

](https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft)

**API-based access.** The other issue with prompt and prefix tuning is that these techniques cannot be used when we can only access an LLM via an API. We need full access to the model’s weights for any form of gradient-based training, but LLM APIs only provide access to the model’s output—_these LLMs are truly a black box_! For this reason, prompt and prefix tuning are only compatible with [open-source LLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation), but researchers have tried to create workarounds for this issue.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8493ec-308a-45c5-958f-4058f51c9f3e_1588x812.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8493ec-308a-45c5-958f-4058f51c9f3e_1588x812.png)

(from [6])

InstructZero [6] inserts another LLM between the soft prompt and the LLM API. This (open-source) LLM can be trained to produce a text-based instruction given the soft prompt and additional task information as input; see above. Then, the generated instruction can be passed to an LLM API normally. Put simply, we use the extra LLM to “decode” the soft prompt into a text-based prompt before it is passed to an API. To improve quality of generated instructions, we can use [Bayesian optimization](https://arxiv.org/abs/1807.02811), which produces instructions that match or exceed the performance of prompts discovered via other automatic prompting methods.

**More work on soft prompts.** Prefix and prompt tuning are the most widely known soft prompting techniques, but there are numerous other papers that have considered this idea. Here are examples of other notable papers in this space:

- [Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599) are formed by finetuning the entire prompt provided to a language model—_either from an existing prompt or random initialization_—to form a soft prompt, which can then be mixed or combined with other soft prompts to achieve better task performance.
    
- [WARP](https://arxiv.org/abs/2101.00121) addresses the problem of adapting a language model to several downstream tasks by learning task-specific word embeddings that can be concatenated to the model’s input to solve different tasks.
    
- [P-Tuning](https://arxiv.org/abs/2103.10385) addresses the instability of prompt engineering by concatenating a series of trainable embeddings with the model’s prompt, where the model can either be finetuned along with the added embeddings or kept frozen.
    
- [PADA](https://arxiv.org/abs/2102.12206) improves the ability of LLMs to adapt to new domains by training a language model to first predict a domain-specific prompt for solving a problem, then use this generated prompt to actually solve the problem.
    

Many articles have been written about soft prompts as well. [Sebastian Rashka](https://sebastianraschka.com/) wrote a detailed explanation of [prompt and prefix tuning](https://magazine.sebastianraschka.com/p/understanding-parameter-efficient), while [Lilian Weng](https://lilianweng.github.io/) explores this topic in both her [text generation](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design) and [prompt engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design) posts.

#### Discrete Prompt Optimization

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3686d27-31d6-4fde-9308-35fd7cfeb9e1_1234x612.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3686d27-31d6-4fde-9308-35fd7cfeb9e1_1234x612.png)

(from [11])

As we have seen, soft prompts have limitations in terms of interpretability, reusability across models, and applicability to API-based LLMs. Unfortunately, these limitations destroy several key benefits of prompting. With this in mind, we might wonder whether we can automatically optimize a prompt while maintaining some of these properties. During the early days of prompting, papers were published on the idea of finding discrete “trigger" tokens that can be included in a model’s prompt to help with solving certain problems.

> _“Writing prompts is not only time consuming, but it is not clear that the same phrasing will be effective for every model, nor is it clear what criteria determine whether a particular phrasing the best to elicit the desired information.”_ - from [11]

**AutoPrompt** [11][5](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-5-149351213) performs a gradient-guided search over a discrete set of tokens to discover an optimal set of additional tokens to include in an LLM’s prompt. This approach is applied to the task of probing knowledge within LLMs, where we see that these additional tokens lead to more reliable and stable performance on this task. By using trigger tokens, we can optimize the LLM’s performance via a rigorous (gradient-based) search procedure, rather than heuristically searching for the best possible prompt through manual trial and error.

Due to the use of gradient information, we see in [11] that the training procedure used by AutoPrompt to edit prompt tokens can be unstable. As an alternative, later work explored optimizing prompts via [reinforcement learning (RL)](https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning). The idea behind using RL to optimize prompts is pretty simple. First, we define a policy network, which is just an LLM that generates a candidate prompt as output. Then, we can define the reward for this policy network as the performance of the prompt that it generates! See below for a depiction of this framework.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c9b20a6-8f19-4607-a9e0-50891f91e1ad_2454x1436.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c9b20a6-8f19-4607-a9e0-50891f91e1ad_2454x1436.png)

Using RL to optimize an LLM

This setup is very similar to training an LLM via [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). In both cases, our policy is an LLM, and that policy performs actions by generating tokens. Once a full sequence is generated, we compute a reward for this sequence. In RLHF, this reward is the output of a reward model, which is a proxy for human preferences. For prompt optimization, this reward is determined by the performance of the prompt; see above.

**RLPrompt** [12] is one of the most well known papers that uses RL to optimize discrete prompts. The policy network that generates prompts is implemented as a pretrained LLM. The parameters of this model are kept frozen, but we add an additional feed-forward layer to the model that is trained via RL; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab98aa49-c107-4cf1-b053-2bdee998a324_1288x546.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab98aa49-c107-4cf1-b053-2bdee998a324_1288x546.png)

(from [12])

This policy network is used to generate candidate prompts that are ingested by another LLM to solve a task—_the policy network and the LLM that ingests the prompts need not be the same_. The reward is derived by measuring the performance (e.g., classification accuracy or adherence of the output to a given style) achieved with a given prompt. Here, the reward function is dependent upon output that is generated by an LLM. For this reason, reward signals within the RLPrompt framework are unpredictable, complex, and difficult to optimize. Despite this issue, authors in [12] show that various reward stabilization techniques can be used to make the optimization process more reliable.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbf3297b-f8a8-4245-9af9-a74fc4bfcf76_1356x530.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbf3297b-f8a8-4245-9af9-a74fc4bfcf76_1356x530.png)

(from [12])

When applied to classification and style transfer tasks, RLPrompt is found to generate prompts that outperform both finetuning and prompt tuning strategies; see above. However, our desire of automatically optimizing prompts that can be easily interpreted by humans falls short. The prompts discovered by RLPrompt, despite transferring well when used with other LLMs, tend to be ungrammatical or even “gibberish”. Depending on our outlook, this finding could lead us to wonder whether prompting in some ways is a language of its own.

> _“The resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.”_ - from [12]

**TEMPERA** [13]—_inspired by work on [instance-dependent prompt optimization](https://arxiv.org/abs/2206.01958) (i.e., optimizing a prompt dynamically for each input)_—follows a different approach of using RL to make adaptations to a prompt at test / inference time. Given an initial prompt, we can edit the properties of this prompt by _i)_ making changes to the instructions, _ii)_ adding or removing few-shot exemplars, and _iii)_ modifying verbalizers[6](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-6-149351213). We can train an agent to perform these edits via RL by defining the reward as the difference in performance before and after a given edit; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc3c290f-7c25-401b-8eba-057a0c687244_1078x508.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc3c290f-7c25-401b-8eba-057a0c687244_1078x508.png)

(from [13])

TEMPERA learns a function that takes the original prompt, the current input query to the LLM, a set of few-shot exemplars, and a pool of verbalizers as input, then generates a final prompt via a sequence of edits; e.g., swapping examples or editing verbalizers. From an RL perspective, our state is given by the current prompt, and we can perform actions by editing the prompt. To represent the state, we can just use the text of the current prompt. However, authors in [13] choose to embed this text by passing it through a pretrained LLM. By doing this, we can implement a simple policy network that:

1. Takes the state / prompt embedding as input.
    
2. Produces the action / edit to perform as output.
    

TEMPERA considers a fixed set of edits that can be performed to a prompt, so this action space is discrete! We can directly predict the prompt edit to be performed with the policy network. By defining rewards as the performance improvement generated by a given edit, we can optimize this entire system with off-the-shelf RL algorithms like PPO. This approach is found to be data efficient, useful for classification, and even impactful for solving harder tasks with LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa65cee60-be42-4cb3-9423-67bfd0ce8147_1668x1070.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa65cee60-be42-4cb3-9423-67bfd0ce8147_1668x1070.png)

(from [13])

**Why does this work?** By using RL, we can finally implement a training procedure for optimizing discrete prompts. Compared to the other approaches we have seen so far, techniques like RLPrompt make two key changes to make this possible:

1. We generate the prompt with an LLM.
    
2. We optimize this LLM based on the quality of the prompts that it generates.
    

By generating the prompt with an LLM, we can focus on optimizing the (continuous) weights of the LLM instead of the discrete prompt. This same approach is used by techniques like Instruction Induction [7] or Self-Instruct [8], but we go beyond these techniques by using RL to teach the model to create better prompts. We need RL to perform this optimization, as the system used to generate the reward is based on an LLM and, therefore, not differentiable! In other words, we can’t easily compute a gradient to use for optimization purposes.

## Automatically Optimizing Prompts for LLMs

Now that we’ve looked at early work on optimizing prompts, we will learn about the more recent and popular papers on automatic prompt optimization. Nearly all of these papers rely upon the ability of LLMs to optimize prompts. Using an LLM as a gradient-free optimizer is (arguably) less rigorous compared to traditional and established optimization algorithms. However, such approaches are simple conceptually, easy to implement / apply in practice, and highly effective, which has led LLM-based prompt optimization algorithms to become quite popular.

#### [Large Language Models are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910) [1]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f464120-a418-4eec-86dd-87d21bc5652c_906x642.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f464120-a418-4eec-86dd-87d21bc5652c_906x642.png)

(from [1])

Writing an effective prompt requires a lot of trial and error. Prompt engineering is a black box optimization problem—_we don’t know a priori how compatible a prompt will be with any given model_. In [1], we see one of the first attempts to make the human prompt engineering process more efficient with an LLM.

> _“We call this problem natural language program synthesis and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions.”_ - from [1]

The proposed approach, called Automatic Prompt Engineer (APE), searches over a pool of prompts proposed by an LLM to find the prompt that performs best. This setup uses separate LLMs to propose and evaluate prompts. For evaluation, we generate output via [zero-shot inference](https://cameronrwolfe.substack.com/i/117151147/zero-shot-learning) and evaluate the output according to a chosen scoring function. Despite its simplicity, APE is shown in [1] to find prompts that match or surpass the performance of prompts written by humans on the same task, _revealing that LLMs actually excel at the task of writing prompts_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F457a3d02-34e6-466a-8169-e4ccf6f104ac_2470x1262.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F457a3d02-34e6-466a-8169-e4ccf6f104ac_2470x1262.png)

Instruction generation templates for APE (from [1])

**How does this work?** APE has two main operations: _proposal_ and _scoring_. For proposal, we ask the LLM to generate new candidate prompts for a task; see above for a set of examples prompts. As shown above, the generation of new instructions can be done in a forward or reverse fashion. In the forward mode, we simply generate new instruction via [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction), which is identical to generating any other kind of text with an LLM. Reverse generation, on the other hand, is based upon [infilling](https://ai.stanford.edu/blog/infilling-by-language-modeling/) (i.e., inserting missing text / tokens into the middle of a sequence), which is not possible with a [decoder-only LLM](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)[7](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-7-149351213). Depending on the task, we might tweak the wording of the instruction generation template.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F569eb694-e78d-4614-b27e-a59f83ba04e7_2328x538.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F569eb694-e78d-4614-b27e-a59f83ba04e7_2328x538.png)

Steps of APE (from [1])

We see in [1] that LLMs can infer better instructions from prior instructions. To determine the quality of generated instructions, we can simply evaluate them in a zero-shot fashion with another LLM, either using output accuracy or a softer metric (e.g., the [log probability](https://cookbook.openai.com/examples/using_logprobs) of the correct output). The evaluation of each generated prompt occurs on a fixed training set that is used throughout the prompt optimization process. Compared to finetuning, prompt optimization via APE requires much fewer training examples. After the optimization is complete, we typically evaluate the final prompt over a hold-out test set.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F973c960a-9fe3-490f-bfa0-11e1d83ce71c_578x470.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F973c960a-9fe3-490f-bfa0-11e1d83ce71c_578x470.png)

(from [1])

To achieve the best possible performance, we see in [1] that we should ask the LLM to generate ~64 prompts for selection. If we generate more prompts, we begin to see diminishing returns in terms of performance—_the best prompt that is discovered does not improve significantly in terms of its accuracy_.

> _“Although this approach improves the overall quality of the proposal set, the highest scoring instruction tends to remain the same with more stages.”_ - from [1]

**Iterative generation.** We can also optimize prompts with APE in an iterative fashion. In this setup, we still ask an LLM to propose new instructions, calculate a score for each instruction, and select instructions with the top scores. Going a step further, however, we can repeat this process by taking these generated instructions as input, using them as context to propose more instruction variants in a similar fashion, and selecting the prompts that perform best; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13faa491-b94e-4156-9314-1d7dfc5299ff_1822x534.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13faa491-b94e-4156-9314-1d7dfc5299ff_1822x534.png)

Iterative APE (from [1])

Using this strategy, we can explore different variants of prompts proposed by the LLM. However, we see in [1] that iterative APE only improves the overall quality of the suite of prompts being explored. The performance of the best prompt discovered by APE remains constant after any number of iterations; see below. For this reason, _the added cost of iterative APE is potentially unnecessary_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9f5aaef-0882-4b46-babc-759abb9d5340_1884x652.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9f5aaef-0882-4b46-babc-759abb9d5340_1884x652.png)

(from [1])

**Does APE discover better prompts?** APE is evaluated based on its ability to write effective prompts in several settings, including zero-shot prompts, few-shot prompts, CoT prompts, and prompts for steering the behavior of an LLM (e.g., being more truthful). Across experiments, we see that APE is able to find prompts that outperform those written by humans. In particular, APE produces prompts that outperform human-written prompts on 24 out of 24 Instruction Induction tasks and 17 out of 21 BIG-Bench tasks; see below for details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6efeaf5c-6178-4ec7-9f0b-8fb1f029bc5c_1870x950.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6efeaf5c-6178-4ec7-9f0b-8fb1f029bc5c_1870x950.png)

(from [1])

By examining the prompts proposed by APE, we can even deduce useful tricks for writing effective prompts. In many cases, these tricks generalize well across tasks and teach us how to properly prompt certain models. We also see in [1] that the performance of instruction candidates improves with LLM size, _indicating that using a large LLM to propose new instructions within APE is helpful._

#### **[Automatic Prompt Optimization](https://arxiv.org/abs/2305.03495) [2]**

Although APE works well, the process that it uses for optimizing prompts is random and directionless. We simply use an LLM to propose a bunch of new prompt variants and select the generated prompt that performs the best. There is no iterative optimization procedure within this algorithm. Rather, we purely rely upon the ability of the optimizer LLM to propose a wide variety of promising prompt variants that can be evaluated in a single pass to find a better prompt.

> _“The algorithm uses batches of data to form natural language gradients that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then propagated into the prompt by editing the prompt in the opposite semantic direction of the gradient.”_ - from [2]

When viewed from the correct lens, optimizing prompts with an LLM is similar to traditional, gradient-based optimization. In [2], authors propose a technique for prompt optimization, called Automatic Prompt Optimization (APO)[8](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-8-149351213), that is inspired by this analogy. APO is a generic technique that requires only a (small) training dataset, an initial prompt, and access to an LLM API to work. This algorithm uses batches of training data to derive “gradients”—_just text-based critiques that outline limitations of the current prompt_—that guide edits and improvements to the prompt, mimicking a gradient update; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8a1bac-f271-49de-9f25-42499b53c804_760x916.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8a1bac-f271-49de-9f25-42499b53c804_760x916.png)

(from [2])

**Optimization framework.** APO aims to apply discrete improvements to a prompt guided by natural language gradients. These gradients are derived by:

- Executing the current prompt over a training dataset with an LLM.
    
- Measuring the prompt’s performance according to some objective function.
    
- Using an LLM to critique the key limitations in performance of this prompt on the training dataset.
    

The gradient that is derived simply captures a textual summary of various issues that exist within the current prompt. Using this summary, we can then prompt an LLM—_using the gradient and the current prompt as input_—to edit the existing prompt in a way that reduces these issues. APO applies these steps iteratively to find an optimal prompt. See below for a depiction of this framework.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3771ee92-476a-4fd3-ba30-4187f201f45c_904x1226.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3771ee92-476a-4fd3-ba30-4187f201f45c_904x1226.png)

(from [2])

**Gradients and edits.** APO creates a recursive feedback loop that optimizes a prompt by _i)_ collecting errors made by the current prompt on the training data, _ii)_ summarizing these errors via a natural language gradient, _iii)_ using the gradient to generate several modified versions of the prompt, _iv)_ selecting the best of the edited prompts, and _v)_ repeating this process several times.

> _“We mirror the steps of gradient descent with text-based dialogue, substituting differentiation with LLM feedback and backpropagation with LLM editing.”_ - from [2]

To generate “gradients”, we show an LLM examples of errors made by the current prompt on the training dataset and ask the model to propose reasons for these errors. We then use these reasons as context for editing the prompt; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F526535da-522e-4764-aa01-0781604bc20c_1462x1134.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F526535da-522e-4764-aa01-0781604bc20c_1462x1134.png)

Using natural language gradients to edit a prompt (from [2])

At each iteration, we generate several edits to the current prompt—_the number of edits is a hyperparameter of the optimization process_. Additionally, we can expand our search space by explicitly generating multiple wordings of each prompt, a technique that authors in [2] refer to as [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) sampling; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3230c41b-1e50-4d81-9e97-f2137688f49f_958x446.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3230c41b-1e50-4d81-9e97-f2137688f49f_958x446.png)

Monte Carlo sampling in APO (from [2])

**Search and selection.** By generating several variants of each prompt, we can search for an optimal prompt via [beam search](https://en.wikipedia.org/wiki/Beam_search). In other words, we maintain several candidates for the best prompts at each iteration, propose `N` edits to each prompt, then select the best `B` edits to the prompt—_by measuring performance over the training set_—to keep in the next iteration; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbef3e5-65c3-40e2-a125-00c5ae4904a1_1600x900.gif)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbef3e5-65c3-40e2-a125-00c5ae4904a1_1600x900.gif)

Using beam search to iteratively select the best prompts

As we might expect, beam search can get expensive if we continually evaluate all prompt candidates over the entire training dataset. However, we see in [2] that we can use [bandit techniques](https://en.wikipedia.org/wiki/Multi-armed_bandit)—_those related to the [best arm identification problem](http://sbubeck.com/COLT10_ABM.pdf) in particular_—to make the search process more efficient. Although the details of this approach are beyond to scope of this post, the high-level idea is that we can use statistics to select which prompts are worth fully evaluating instead of just naively evaluating every prompt over the entire training dataset.

**Does this work well?** APO is evaluated for prompting GPT-3.5 on four different text-based classification tasks, including jailbreak detection, hate speech detection, fake news detection, and sarcasm detection. _Why are only classification problems considered?_ The explicit choice of classification tasks for evaluating APO is likely due to the simplicity of defining an objective function for these problems—_we can evaluate each prompt based on classification accuracy_. For complex or open-ended tasks, defining an objective function is not simple, which makes the application of techniques like APO less straightforward.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8c23bd5-a1d8-47ec-80a6-33f8626dcc4a_1414x422.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8c23bd5-a1d8-47ec-80a6-33f8626dcc4a_1414x422.png)

(from [2])

As shown above, APO outperforms other state-of-the-art prompt optimization algorithms across all datasets. Notably, we see that the performance of APE—_the Monte Carlo (MC) technique in the figure above_—is worse than APO, indicating that we benefit from adding a more concrete direction—_via natural language gradients_—to the prompt optimization process. We see in [2] that APO can improve the performance of the initial prompt by up to 31%, exceeding improvements seen with other techniques despite the fact that APO requires LLM API calls.

#### [GRIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models](https://arxiv.org/abs/2203.07281) [14]

> _“We propose Gradient-free Instructional Prompt Search (GRIPS), an automated procedure for improving instructional prompts via an iterative, local, edit-based, and gradient-free search.”_ - from [14]

Instead of using an LLM to edit our prompt as part of our prompt optimization process, we can use heuristic strategies to generate new prompt variants to consider. In [14], authors propose GrIPS, which is a gradient-free method that searches for better prompts via iterative application of a fixed set of heuristic edit operations. Like the approaches we have seen so far, GrIPS takes a human-written prompt as input and returns an improved, edited prompt as output.

**Heuristic prompt search.** GrIPS can be applied to both instruction-based prompts and prompts that include in-context examples (shown below) but focuses purely upon improving the instruction—_we do not try to optimize the selection of examples in GrIPS_. To search over the space of possible prompts, we begin with an initial prompt and iteratively perform a series of edits to this prompt. Then, we can test the performance of the edited prompts over a pool of training examples[9](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-9-149351213), allowing us to determine which of the edited prompts are best.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeb2dc60-cdd6-4f47-8c32-4d89dcddb788_1536x410.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeb2dc60-cdd6-4f47-8c32-4d89dcddb788_1536x410.png)

(from [14])

This procedure is somewhat similar to the techniques we have seen so far, but we apply a set of heuristic edits to the prompt during the search process instead of relying upon an LLM to generate edited versions of a prompt. This technique can work with arbitrary types of LLMs, including those exposed via an API.

**Details of search.** The optimization framework for GrIPS is shown below. We begin the search process with a base instruction, which is written by a human. At each iteration, we select a certain number of edit operations to apply to this prompt, producing several new prompt candidates. These candidates are then scored so that the best of them can be identified. If the score of the best candidate exceeds that of the current base instruction, then we use the best candidate as the base instruction moving forward. Otherwise, we maintain the current base instruction and repeat. This is a greedy search process that terminates when the score of the base instruction does not improve for several iterations.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c5b8c8-de64-43bf-9226-1e7cafa20852_1226x488.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4c5b8c8-de64-43bf-9226-1e7cafa20852_1226x488.png)

(from [14])

Similarly to [2], we can easily adapt this greedy search procedure to perform beam search by maintaining a set of several instructions at each iteration. However, such a modification increases the cost of the search process by introducing a larger number of candidate prompts that need to be evaluated at each step.

**Types of edits.** In GrIPS, we always apply edits at a phrase level, allowing us to meaningfully modify a prompt while maintaining its structure. To deconstruct a prompt into phrases, we can use a [constituency parser](https://nlpprogress.com/english/constituency_parsing.html). Then, four different types of edit operations are considered over the phrases in a prompt:

1. _Delete_: remove all occurrences of a certain phrase from the instruction and store the deleted phrase for use in the add operation.
    
2. _Swap_: given two phrases, replace all occurrences of the first phrase in the instruction with the second phrase and vice versa.
    
3. _Paraphrase_: replace all occurrences of a certain phrase from the instruction with a paraphrased version[10](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-10-149351213) of that phrase.
    
4. _Addition_: sample a phrase deleted in a previous iteration and add it back to the instruction at a random phrase boundary.
    

**Practical usage.** GrIPS is primarily evaluated on binary classification tasks from the [Natural Instructions dataset](https://instructions.apps.allenai.org/). Similarly to observations in [12], we see that prompts discovered by GrIPS tend to improve accuracy but are not always coherent. The optimized prompts contain semantically awkward and confusing phrasing. Nonetheless, we see in [14] that using GrIPS to optimize the base instruction consistently improves the accuracy of various LLMs by as much as 2-10%. Additionally, manual rewriting of prompts—_and even gradient-based prompt tuning in some cases_—tends to be outperformed by GrIPS, which is found to work best for optimizing task-specific (as opposed to generic) instructions; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdcedf31-b3df-4e29-bddc-7151e3d54d38_1790x588.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdcedf31-b3df-4e29-bddc-7151e3d54d38_1790x588.png)

(from [14])

#### **[Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409) [3]**

> _“Instead of formally defining the optimization problem and deriving the update step with a programmed solver, we describe the optimization problem in natural language, then instruct the LLM to iteratively generate new solutions based on the problem description and the previously found solutions.”_ - from [3]

One of the most popular and widely used prompt optimization techniques today is Optimization by Prompting (OPRO). However, OPRO can do more than just optimize prompts. It is a generic optimization algorithm that operates by:

1. Describing an optimization task in natural language.
    
2. Showing an optimizer LLM examples of prior solutions to the optimization task along with their objective values.
    
3. Asking the optimizer LLM to infer new / better solutions to the problem.
    
4. Testing the inferred solutions via an evaluator LLM.
    

By repeating the steps above, we create an extremely flexible, gradient-free optimization algorithm. Instead of formally specifying the problem we are trying to solve with math, we can just explain the problem in natural language. Such a description is sufficient for performing optimization with OPRO, which makes the algorithm widely applicable and easy to extend to new tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F757f9287-2cdb-42ed-adfd-20ebad5c3a80_2146x960.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F757f9287-2cdb-42ed-adfd-20ebad5c3a80_2146x960.png)

(from [3])

Given the correct sampling strategy, the trajectory of this optimization algorithm is relatively stable, indicating once again that the LLMs are capable of learning from past solutions to propose new and better solutions; see above. Although OPRO can be applied to many problems, we see in [3] that this algorithm is especially useful for automatic prompt optimization. First, we will learn about how OPRO works, then we will explore how it can be used to optimize prompts.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7de3ae7-b2aa-4827-8f82-6593e7cf097d_2140x1018.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7de3ae7-b2aa-4827-8f82-6593e7cf097d_2140x1018.png)

(from [3])

**OPRO Framework.** The high-level steps followed by OPRO are outlined above. At each step of the optimization process, OPRO generates new solutions based on a meta-prompt, which contains a textual description of the optimization problem and prior solutions that have been proposed. Given the meta-prompt, we then generate several new solutions at once[11](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-11-149351213). By generating several candidate solutions at each steps, we can ensure a (relatively) stable optimization trajectory, as the LLM is given several chances to generate a viable solution.

From here, we evaluate each new solution based on an objective function (e.g., accuracy) and add the best solutions to the meta-prompt for the next optimization step. In this way, we choose the best solutions to be passed to the next iteration of the optimization process. Our goal is to find a solution that optimizes the objective function, and the optimization process terminates once the LLM is unable to propose new solutions that yield improvements in the objective.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7ac47d5-9859-496d-8d63-7eae651c1f99_1704x820.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7ac47d5-9859-496d-8d63-7eae651c1f99_1704x820.png)

Key components of OPRO (from [3])

**Key components or ORPO.** As we can see from this framework, OPRO has two main components within its optimization process (depicted above):

- _Optimizer_: takes the meta-prompt as input and proposes new solutions.
    
- _Evaluator:_ takes a solution as input and computes an objective value.
    

The optimizer is always implemented as an LLM. To successfully apply OPRO, we must select a sufficiently powerful LLM as the optimizer, as deducing the best solutions from context provided in the meta-prompt requires complex reasoning capabilities. Plus, we see in [1] that larger LLMs tend to be better at this task.

The evaluator may also be implemented as an LLM depending on the problem being solved, _but the optimizer and evaluator need not be the same LLM_. For example, linear regression problems have a simple evaluator (i.e., we can directly compute an objective value from the solution), but prompt optimization problems require that we use an evaluator LLM to measure output quality for each prompt.

> _“While traditional optimization often requires a decently large training set, our experiment shows that a small number or fraction of training samples is sufficient.”_ - from [3]

Optimization with OPRO requires both training and testing data. We use the training dataset to compute objective values during the optimization process, while the test set is used to assess the final performance of generated solutions after the optimization process has concluded. Compared to most optimization algorithms, OPRO requires very little training data.

**More on the meta-prompt.** The meta-prompt provides all necessary context that the optimizer LLM needs to propose a better solution to the problem being solved. The meta-prompt has two primary components:

1. A textual description of the optimization problem.
    
2. Prior solutions and their objective values (i.e., the optimization trajectory).
    

The optimization trajectory is sorted such that the best solutions appear at the end of the meta-prompt. Beyond this core information, we also include randomly selected examples from the training dataset to demonstrate the expected output format, as well as general instructions to follow when creating new solutions (e.g., “be concise” or “generate a new instruction that maximizes accuracy”). An example of a meta-prompt for a prompt optimization task is shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F135892b2-166f-4927-9f16-e0b5779d7ad9_1372x1362.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F135892b2-166f-4927-9f16-e0b5779d7ad9_1372x1362.png)

(from [3])

**OPRO’s performance.** In [3], OPRO is evaluated on both [GSM8K](https://huggingface.co/datasets/openai/gsm8k) and [Big-Bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard). On these datasets, OPRO discovers prompts that outperform human-written prompts by 8% and 50%, respectively. Compared to prior approaches like APE [1], OPRO is shown to be capable of finding more complex instructions that achieve better performance; see below. Interestingly, the style of instructions that are discovered tend to vary a lot depending upon the optimizer LLM being used.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F281f1405-bf61-418a-adc5-361b5e407c72_1190x1244.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F281f1405-bf61-418a-adc5-361b5e407c72_1190x1244.png)

(from [3])

**Advanced reasoning systems (o1).** As mentioned before, inferring new solutions from the optimization trajectory is a complex reasoning problem. We have already seen that larger LLMs are better at solving this task [1], indicating that we benefit from a more powerful optimizer LLM with better reasoning capabilities. Additionally, _prompt optimization is a one-time cost_. We optimize our prompt once, but the optimized prompt is typically used in a downstream application for a much longer duration. As a result, the cost of using a much more expensive optimizer LLM is amortized by the lifetime of the prompt that it produces.

[

![An image of the new ChatGPT dropdown that displays the new "o1-preview" model option over a bright yellow and blue abstract background](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F480dcf60-9e51-40f7-a8dc-9538795dbfbc_3840x2160.webp "An image of the new ChatGPT dropdown that displays the new "o1-preview" model option over a bright yellow and blue abstract background")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F480dcf60-9e51-40f7-a8dc-9538795dbfbc_3840x2160.webp)

(from [o1-preview announcement](https://openai.com/index/introducing-openai-o1-preview/))

The recent proposal of LLM-based reasoning systems like OpenAI’s o1 unlocks new possibilities for prompt optimization. These systems are trained to follow an extensive reasoning process by which the model can dynamically decide whether more compute is needed to solve complex problems; see [here](https://www.interconnects.ai/p/reverse-engineering-openai-o1) for details. Such an advanced reasoning strategy can be expensive in some cases; e.g., o1 may “think” for [over a minute](https://x.com/axioma_ai/status/1834355853576556769) before responding to a prompt. However, using extra compute at inference time may be worthwhile for complex reasoning problems. In the case of automatic prompt optimization, the latency and cost of the optimizer LLM’s response in not nearly as important as the quality of the generated prompt, making this a prime application of these advanced reasoning systems.

> _“We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.”_ - [source](https://openai.com/index/learning-to-reason-with-llms/)

#### Optimizing Prompts via Evolution

The prompt optimization strategies we have seen so far begin with a prompt (or set of prompts) written by a human and iteratively:

1. Generate variants of the prompt(s).
    
2. Select the best performing prompt(s) from these variants.
    

In some ways, this optimization process can be viewed as an EA. A population of prompts is maintained, mutated, and selected over time. We use a population size of one (unless beam search is being used), and the population is mutated by prompting an LLM (or using a heuristic, edit-based strategy) to modify the current prompt. Inspired by this work, researchers have recently explored more explicit strategies for applying EAs to prompt optimization. As we will see, these algorithms are not so different from those that we have seen so far!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40890af3-bc91-4436-a773-1b924586a2d3_1232x896.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40890af3-bc91-4436-a773-1b924586a2d3_1232x896.png)

(from [15])

**Genetic Prompt Search (GPS)** [15] (shown above) employs a genetic algorithm to automatically discover high performing prompts. First, the algorithm is instantiated with a set—_or population_—of hand-written prompts. Then, we can mutate these prompts via the following operations:

- _Back translation_: translate the prompt to one of 11 other languages, then translate back to English.
    
- _Cloze_[12](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-12-149351213): mask out several tokens in the prompt and use a pretrained language model (e.g., [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)) to fill them in, thus generating a new prompt.
    
- _Sentence continuation_: prompt an LLM to “write two sentences that mean the same thing” and provide the current prompt as the first sentence.
    

Each of these strategies can mutate the current population of prompts to generate new prompt variants. From here, we can score the generated prompts using a hold-out validation set to determine those that should be retained in the next iteration—_or generation_—of evolution. This algorithm outperforms manual prompt engineering and can even outperform prompt tuning in some cases.

> _“Sequences of phrases in prompts can be regarded as gene sequences in typical EAs, making them compatible with the natural evolutionary process.”_ - from [16]

**EvoPrompt** [16] again leverages concepts from EAs to create a prompt optimizer the performs well and converges quickly. Starting with a population of prompts that are written manually, EvoPrompt uses the two primary evolutionary operators—_mutation and crossover_—to generate new prompts. At each generation, the best prompts are selected and maintained for further evolution by measuring their performance over a hold-out validation set.

By definition, evolutionary operators take a sequence as input and output a modified sequence. Traditionally, operators are applied independently to each element of the sequence—_we update each element with no knowledge of whether the elements around it have been changed._ For prompt optimization, this approach is problematic because all tokens within the prompt are interrelated.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49a8cda-930d-4b58-add0-0604f79910cc_1182x754.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49a8cda-930d-4b58-add0-0604f79910cc_1182x754.png)

(from [16])

To solve this problem, EvoPrompt implements evolutionary operators via prompting and relies upon the expertise of an LLM to evolve prompts in a logical manner. Examples of crossover and mutation prompts used for prompt optimization via a genetic algorithm are shown above. However, EvoPrompt supports a variety of EAs beyond just genetic algorithms! For example, we see below a set of prompts that can be used to implement a differential evolution strategy for prompt optimization. The EvoPrompt framework is sufficiently flexible to substitute different EAs in a plug-and-play manner.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa56556e7-c27d-4d71-a32a-3e3e9f3c44c4_1182x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa56556e7-c27d-4d71-a32a-3e3e9f3c44c4_1182x1182.png)

(from [16])

EvoPrompt is tested with both closed and open-source LLMs (e.g., Alpaca and GPT-3.5) on several tasks, where it is found to outperform algorithms like APE [1] and APO [2]. The differential evolution variant of EvoPrompt outperforms the genetic algorithm variant in most cases; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5283b55e-79b6-4376-af94-5efb9a3627b1_1214x648.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5283b55e-79b6-4376-af94-5efb9a3627b1_1214x648.png)

(from [16])

**Prompbreeder** [17] was proposed shortly after EvoPrompt. The ideas behind these two techniques are similar:

- They both begin with a human written population of prompts.
    
- They both implement evolutionary operators like mutation and crossover via a prompt to an LLM.
    
- They both perform selection by measuring performance on a validation set.
    

The main difference between these techniques is that Promptbreeder optimizes more than just the task prompt—_the prompt used to implement evolutionary operators is optimized as well_! Such an approach allows us to introduce a self-improvement mechanism within the prompt optimization algorithm itself.

> _“That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutation-prompts that improve these task-prompts.”_ - from [17]

Promptbreeder is found to outperform a variety of powerful prompting strategies (e.g., [chain of thought](https://arxiv.org/abs/2201.11903) and [plan and solve](https://arxiv.org/abs/2305.04091) prompting) on reasoning benchmarks. While other prompt optimization algorithms commonly see diminishing returns and a saturation in performance when extending the optimization process to larger numbers of iterations [1], Promptbreeder dynamically adapts the optimization process over time, allowing intricate prompts to be discovered that are more capable of solving difficult tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7ed9a7e-1268-4467-a3fb-d6f996538b35_1084x974.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7ed9a7e-1268-4467-a3fb-d6f996538b35_1084x974.png)

(from [17])

## Practical Tips and Takeaways

We have seen a wide variety of prompt optimization techniques in this overview, including methods that learn soft prompts directly from data, leverage LLMs as gradient-free optimizers, use RL to train models that are better at generating prompts, and more. We outline the key takeaways from all of this research below.

**Why is prompt optimization hard?** As we have seen, prompt optimization is not a normal optimization problem for a few reasons:

1. The prompt we are trying to optimize is made up of discrete tokens.
    
2. We lack access to gradient information in most cases.
    

If we are using an LLM API, we have very limited information available to improve our prompt. Additionally, the fact that prompts are discrete makes the application of gradient-based optimization algorithms difficult. Successful prompt optimization algorithms have avoided these issues by _i)_ adopting gradient-free optimization algorithms that resemble EAs and _ii)_ relying upon the ability of LLMs to infer better prompts from those that have been tried previously.

**LLMs are good prompt engineers.** One of the primary takeaways from recent prompt optimization papers is the fact that LLMs are good at writing prompts. Assuming we provide the right information as context, we can create surprisingly powerful prompt optimization algorithms by just iteratively prompting an LLM to critique and improve a prompt. Larger (and more capable) LLMs tend to be better at this task. So, applying advanced models like [OpenAI’s o1](https://openai.com/index/introducing-openai-o1-preview/) to prompt optimization is an interesting opportunity that has not yet been explored.

**What should we use?** Since we have seen many prompt optimization algorithms, we might not know which of these techniques to actually use. Much of the research we have seen is useful. However, LLM-based prompt optimizers (e.g., OPRO [3]) are the most straightforward and practical prompt optimization techniques by far. We can use these algorithms to automatically improve any prompt that we write, producing a better prompt that is still interpretable by a human. OPRO is beneficial and easy to apply, which has made it popular among LLM practitioners. An open implementation of OPRO is provided [here](https://github.com/google-deepmind/opro).

**Are humans still necessary?** Although prompt optimization techniques can drastically reduce manual prompt engineering effort, eliminating human prompt engineers altogether is unlikely. All of the algorithms we have seen so far require a human prompt engineer to provide an initial prompt as a starting point for the optimization process. Additionally, techniques like OPRO will require human intervention / guidance in practice to find the best possible prompts. Put simply, _automatic prompt optimization techniques are assistive in nature_. These algorithms automate some of the basic, manual effort of prompt engineering, but they do not eliminate the need for human prompt engineers. However, it should be noted that LLMs are naturally becoming less sensitive to subtle changes in their prompts over time, which makes prompt engineering less necessary in general.

**Limitations of prompt optimization.** The prompt optimization algorithms we have seen so far are useful for improving the wording and basic structure of a prompt. However, many more choices are included in the construction of a performant LLM system. For example, we may need to use [retrieval augmented generation (RAG)](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval), add extra data sources into our prompt, find the best few-shot examples to use, and more. Automating these higher-level choices goes beyond the capabilities of the prompt optimization algorithms we have seen so far, but researchers are currently developing tools like DSPy [31] that can automate both the high-level design and low-level implementation of LLM systems.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Zhou, Yongchao, et al. "Large language models are human-level prompt engineers." _arXiv preprint arXiv:2211.01910_ (2022).

[2] Pryzant, Reid, et al. "Automatic prompt optimization with" gradient descent" and beam search." _arXiv preprint arXiv:2305.03495_ (2023).

[3] Yang, Chengrun et al. “Large Language Models as Optimizers.” _arXiv abs/2309.03409_ (2023).

[4] Li, Xiang Lisa, and Percy Liang. "Prefix-tuning: Optimizing continuous prompts for generation." _arXiv preprint arXiv:2101.00190_ (2021).

[5] Lester, Brian, Rami Al-Rfou, and Noah Constant. "The power of scale for parameter-efficient prompt tuning." _arXiv preprint arXiv:2104.08691_ (2021).

[6] Chen, Lichang, et al. "Instructzero: Efficient instruction optimization for black-box large language models." _arXiv preprint arXiv:2306.03082_ (2023).

[7] Honovich, Or, et al. "Instruction induction: From few examples to natural language task descriptions." _arXiv preprint arXiv:2205.10782_ (2022).

[8] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." _arXiv preprint arXiv:2212.10560_ (2022).

[9] Xu, Can, et al. "Wizardlm: Empowering large language models to follow complex instructions." _arXiv preprint arXiv:2304.12244_ (2023).

[10] Jiang, Zhengbao, et al. "How can we know what language models know?." _Transactions of the Association for Computational Linguistics_ 8 (2020): 423-438.

[11] Shin, Taylor, et al. "Autoprompt: Eliciting knowledge from language models with automatically generated prompts." _arXiv preprint arXiv:2010.15980_ (2020).

[12] Deng, Mingkai, et al. "Rlprompt: Optimizing discrete text prompts with reinforcement learning." _arXiv preprint arXiv:2205.12548_ (2022).

[13] Zhang, Tianjun, et al. "Tempera: Test-time prompting via reinforcement learning." _arXiv preprint arXiv:2211.11890_ (2022).

[14] Prasad, Archiki, et al. "Grips: Gradient-free, edit-based instruction search for prompting large language models." _arXiv preprint arXiv:2203.07281_ (2022).

[15] Xu, Hanwei, et al. "GPS: Genetic prompt search for efficient few-shot learning." _arXiv preprint arXiv:2210.17041_ (2022).

[16] Guo, Qingyan, et al. "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers." _arXiv preprint arXiv:2309.08532_ (2023).

[17] Fernando, Chrisantha, et al. "Promptbreeder: Self-referential self-improvement via prompt evolution." _arXiv preprint arXiv:2309.16797_ (2023).

[18] Janga Reddy, M., and D. Nagesh Kumar. "Evolutionary algorithms, swarm intelligence methods, and their applications in water resources engineering: a state-of-the-art review." _h2oj_ 3.1 (2020): 135-188.

[19] Corne, David W., and Michael A. Lones. "Evolutionary algorithms." _arXiv preprint arXiv:1805.11014_ (2018).

[20] Ai, Hua, et al. "Topology optimization of computer communication network based on improved genetic algorithm." _Journal of Intelligent Systems_ 31.1 (2022): 651-659.

[21] Zamfirescu-Pereira, J. D., et al. "Why Johnny can’t prompt: how non-AI experts try (and fail) to design LLM prompts." _Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems_. 2023.

[22] Jiang, Ellen, et al. "Prompt-based prototyping with large language models." _Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems_. 2022.

[23] Vatsal, Shubham, and Harsh Dubey. "A survey of prompt engineering methods in large language models for different nlp tasks." _arXiv preprint arXiv:2407.12994_ (2024).

[24] Sahoo, Pranab, et al. "A systematic survey of prompt engineering in large language models: Techniques and applications." _arXiv preprint arXiv:2402.07927_ (2024).

[25] Reynolds, Laria, and Kyle McDonell. "Prompt programming for large language models: Beyond the few-shot paradigm." _Extended abstracts of the 2021 CHI conference on human factors in computing systems_. 2021.

[26] Doerr, Benjamin, and Frank Neumann. "A survey on recent progress in the theory of evolutionary algorithms for discrete optimization." _ACM Transactions on Evolutionary Learning and Optimization_ 1.4 (2021): 1-43.

[27] Lehman, Joel, et al. "Evolution through large models." _Handbook of Evolutionary Machine Learning_. Singapore: Springer Nature Singapore, 2023. 331-366.

[28] Meyerson, Elliot, et al. "Language model crossover: Variation through few-shot prompting." _arXiv preprint arXiv:2302.12170_ (2023).

[29] Chen, Yutian, et al. "Towards learning universal hyperparameter optimizers with transformers." _Advances in Neural Information Processing Systems_ 35 (2022): 32053-32068.

[30] Chen, Angelica, David Dohan, and David So. "EvoPrompting: language models for code-level neural architecture search." _Advances in Neural Information Processing Systems_ 36 (2024).

[31] Khattab, Omar, et al. "Dspy: Compiling declarative language model calls into self-improving pipelines." _arXiv preprint arXiv:2310.03714_ (2023).

[1](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-1-149351213)

We may include [few-shot examples](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) in our prompts, which—_although not used to actually update the model’s parameters_—can be considered a form of “training” data.

[2](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-2-149351213)

We don’t have to 100% replace the prompt engineer! Automatic prompt optimization algorithms can also be assistive in nature, making them more efficient.

[3](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-3-149351213)

Hard prompt tuning is just another name for prompt engineering! We are just changing the words of a prompt to create different, human-readable prompts.

[4](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-4-149351213)

The reparameterization approach used by prefix tuning adds additional learnable parameters to the model. Each prefix has an additional feed-forward neural network, which has parameters of its own, associated with it!

[5](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-5-149351213)

This work is an early exploration of optimizing prompts in a discrete manner using gradient-based techniques. However, there are other works that came before it; e.g., [this paper](https://arxiv.org/abs/1908.07125) explores similar techniques for discovering “trigger tokens” in LLMs.

[6](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-6-149351213)

A verbalizer is a component of a prompt template that can be used to map labels words to actual words. For example, a verbalizer of “positive” canned by mapped to the word “great”, while a verbalizer of “negative” can be mapped to the word “horrible”.

[7](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-7-149351213)

Instead, we need to use an LLM that is capable of infilling. This requires an [encoder-only](https://cameronrwolfe.substack.com/p/language-understanding-with-bert) or e[ncoder-decoder](https://cameronrwolfe.substack.com/i/108182616/different-transformer-architectures) transformer architecture; e.g., [T5](https://metaflowui.prod.netflix.net/?_group_limit=30&_limit=30&_order=-ts_epoch&flow_id=GenAICopyFlow_Test&status=completed%2Cfailed%2Crunning) is a popular model that is trained via an infilling objective.

[8](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-8-149351213)

This technique was called Prompt Optimization with Textual Gradients (ProTeGi) when the paper was first published, but authors later changed the name of the technique to Automatic Prompt Optimization (APO).

[9](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-9-149351213)

Again, we should follow machine learning best practices here by using a separate pool of examples for training and testing of prompts.

[10](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-10-149351213)

Here, the paraphrased version of the phrase is generated using a [PEGASUS model](https://huggingface.co/tuner007/pegasus_paraphrase).

[11](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-11-149351213)

We can balance the explore-exploit tradeoff within OPRO by simply tweaking the [temperature](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) parameter used by the LLM when generating new solutions!

[12](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization#footnote-anchor-12-149351213)

[Cloze](https://en.wikipedia.org/wiki/Cloze_test) is a fundamental machine learning concept. It represents the task of masking out words (or other types of tokens) within a sequence and trying to predict them. This task is also known as masked language modeling and is used to pretrain models like [BERT](https://arxiv.org/abs/1810.04805).

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Marco Aurelio Sterpa's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb79e8633-a78a-42aa-b1e4-12949e2cbbb8_144x144.png)



](https://substack.com/profile/28516222-marco-aurelio-sterpa)

[

![Luke's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfdd24ae-7b1a-4d40-bb66-313988e7a590_144x144.png)



](https://substack.com/profile/1258562-luke)

[

![Ihor's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea605a9-e5c3-461b-9f84-e8d7776720aa_144x144.png)



](https://substack.com/profile/14042337-ihor)

[

![baconnier loic's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa400ca63-5c50-40cb-b0a5-fe4c12671dae_1284x1282.jpeg)



](https://substack.com/profile/106530911-baconnier-loic)

[

![Martin Chesbrough's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffac4ac83-8d8d-4962-9a27-725bae278737_749x562.jpeg)



](https://substack.com/profile/6973698-martin-chesbrough)

87 Likes∙

[7 Restacks](https://substack.com/note/p-149351213/restacks?utm_source=substack&utm_content=facepile-restacks)

87

- 

[

9

](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comments)

7

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael?utm_source=comment)

[Michael](https://substack.com/profile/59416223-michael?utm_source=substack-feed-item)

[Lux Umbra Dei](https://runtothehorizn.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[11月5日](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comment/75473707 "2024年11月5日 01:42")

Liked by Cameron R. Wolfe, Ph.D.

Simply the finest treatment of the topic I've yet seen. Thanks so much for the effort you expended on this! You really should consider writing a book. I'd buy it in a flash.

Like (3)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comment/75473707)

[

![baconnier loic's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa400ca63-5c50-40cb-b0a5-fe4c12671dae_1284x1282.jpeg)



](https://substack.com/profile/106530911-baconnier-loic?utm_source=comment)

[baconnier loic](https://substack.com/profile/106530911-baconnier-loic?utm_source=substack-feed-item)

[baconnier loic](https://baconnierloic.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[11月4日](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comment/75418170 "2024年11月4日 19:27")

Liked by Cameron R. Wolfe, Ph.D.

Awasome article

I have developed an Hugging face space named PROMPT++ to refine prompts automatically

[https://huggingface.co/spaces/baconnier/prompt-plus-plus](https://huggingface.co/spaces/baconnier/prompt-plus-plus)

Still in development and feedback appreciated

Like (3)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comment/75418170)

[7 more comments...](https://cameronrwolfe.substack.com/p/automatic-prompt-optimization/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


---


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Finetuning LLM Judges for Evaluation

### The Prometheus suite, JudgeLM, PandaLM, AutoJ, and more...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Dec 02, 2024

77

- 

[

4

](https://cameronrwolfe.substack.com/p/finetuned-judge/comments)

9

Share

Hi everyone! I’ll be at [NeurIPS](https://neurips.cc/) this year. If you want to chat, you can find me at the Netflix booth intermittently during the conference or [send me a message](https://cameronrwolfe.me/)!

-Cameron

---

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F455588e8-0d8e-464d-8ec5-093945c2cff0_2262x1332.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F455588e8-0d8e-464d-8ec5-093945c2cff0_2262x1332.png)

(from [1, 2, 3, 9, 23])

Evaluating large language models (LLMs) has become more difficult as their capabilities have expanded. Modern foundation models have a wide scope, and their output is usually open-ended—_meaning that any input has many viable outputs_. For these reasons, programmatically evaluating LLMs is a complex and active research problem. Evaluating any single capability of an LLM is already difficult, and LLMs have numerous capabilities and behaviors worthy of evaluation.

> _“Evaluating the quality of outputs produced by LLMs is progressively becoming difficult, as the outputs cover an extremely diverse distribution of text and complex tasks. To address this issue, LLM-based evaluation has emerged as a scalable and cheap paradigm for assessing LLM-generated text”_ - from [2]

The most reliable way to judge the performance of an LLM is by having humans evaluate the model’s outputs, but human evaluation is noisy, expensive, and time consuming. Although a certain amount of human evaluation is always necessary, relying purely upon human evaluation is not scalable. _We must be able to efficiently test the capabilities of a new LLM_. This need motivated the proposal of [LLM-as-a-Judge](https://cameronrwolfe.substack.com/p/llm-as-a-judge) [8], which prompts a proprietary LLM to evaluate another LLM’s output.

LLM-as-a-Judge is heavily used in current research, but it is less effective for domain-specific applications that require the evaluation of granular criteria with which a proprietary LLM may be less familiar. For these cases, we may need to train our own specialized LLM judge, and the goal of this overview is to gain a comprehensive understanding of how this can be achieved. Let’s start by taking a broad look at the different ways we can approach evaluating an LLM.

## How to Evaluate an LLM

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa63dd24d-bbf5-4eca-9f49-6b19161ccfef_1450x686.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa63dd24d-bbf5-4eca-9f49-6b19161ccfef_1450x686.png)

Automatic metrics are an (imperfect) proxy for human evaluation that allow us to perform more / faster model iterations between human evaluation trials

To evaluate an LLM, we use a combination of human evaluation and automatic metrics; see above. Human evaluation serves as our definitive source of truth in terms of the model’s performance, but human evaluation is also incredibly laborious. To increase our pace of model development, we need to rely on automatic metrics that can be measured more efficiently, allowing us to train and evaluate models at a faster pace. Automatic metrics are an imperfect proxy for human opinions, so we must continue to monitor the model’s performance via human evaluation. However, we can use automatic metrics to test a much larger number of models between each human evaluation trial.

> _“Existing benchmarks and traditional metrics do not adequately estimate the capabilities of LLMs in open-ended scenarios. A new benchmark method that could evaluate LLMs comprehensively in open-ended tasks is needed.”_ - from [9]

**Types of automatic evaluation.** There are two primary automatic evaluation strategies used for most modern LLMs (shown below): _perplexity_ and _model-based_ evaluation. Perplexity-based evaluation encompasses most traditional NLP benchmarks, such as [MMLU](https://arxiv.org/abs/2009.03300) or [BIG-bench](https://arxiv.org/abs/2206.04615). We can think of these benchmarks as multiple-choice-style questions that test the general knowledge of an LLM. However, these benchmarks fall short when evaluating LLMs in more open-ended settings, where the LLM is expected to generate longer, stylized answers.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1997570b-e533-456f-8482-2f4750589910_932x414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1997570b-e533-456f-8482-2f4750589910_932x414.png)

(from [23])

To evaluate longer, open-ended outputs, we have two options:

1. _Human evaluation_: rely upon humans to rate the model’s output.
    
2. _Model-based evaluation_: use a powerful LLM to rate the model’s output.
    

Because of the limitations of human evaluation, model-based evaluation strategies—_such as [LLM-as-a-Judge](https://cameronrwolfe.substack.com/p/llm-as-a-judge) [8]_—have become the go-to approach for evaluating LLMs. Such model-based evaluation techniques do not require references, are easy to implement, and can handle a wide variety of open-ended tasks.

Now we understand the basic categories of evaluation, including both human and automatic evaluations. We also understand that there are several ways to perform automatic evaluation (i.e., benchmarks or model-based evaluation). So, let’s take a deeper look at each of these evaluation strategies, starting with human evaluation and moving on to various automatic evaluation techniques.

#### Human Evaluation

> _“Human evaluation has consistently been the predominant method, for its inherent reliability and capacity to assess nuanced and subjective dimensions in texts. In many situations, humans can naturally discern the most important factors of assessment, such as brevity, creativity, tone, and cultural sensitivities.”_ - from [1]

Human evaluation is our definitive source of truth when evaluating an LLM. The humans that perform this evaluation—_referred to as human evaluators or annotators_—can be sourced in a variety of ways; e.g., we can [crowdsource](https://arxiv.org/abs/2403.04132), hire[1](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-1-151216391), or even [use ourselves](https://crfm.stanford.edu/2023/03/13/alpaca.html)—_the model developers_—to evaluate the output of an LLM. Although human evaluation is our source of truth in terms of model quality, this does NOT mean that human evaluation is perfect. In fact, the exact opposite is usually true in practice—_human evaluation is noisy, difficult, and can be prone to bias._

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc88b560a-3ca5-46d5-ab1b-e156b3c07b8f_1484x876.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc88b560a-3ca5-46d5-ab1b-e156b3c07b8f_1484x876.png)

The human evaluation (or annotation) process

**Agreement and calibration.** Even on seemingly subjective tasks, human agreement, which is measured by simply counting matching evaluations or using metrics like [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa), [Fleiss' Kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa), and [Krippendorff's Alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha), can be low. So, we must invest effort into “calibrating” human annotators (i.e., teaching them how to evaluate our particular task consistently and accurately); see above. Calibration usually involves having a meeting among human annotators[2](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-2-151216391) to discuss and resolve disagreements. Then, we can take the results of these discussions and incorporate them into the evaluation guidelines for our task.

**Crafting guidelines.** To document how humans should evaluate or annotate a certain task, we must create a set of written guidelines that explain:

1. What exactly we are aiming to evaluate (i.e., the evaluation criteria).
    
2. How to properly evaluate these criteria.
    

These guidelines, comprised of both written instructions and concrete examples of correct or incorrect annotations[3](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-3-151216391), are a detailed resource that humans can continually reference throughout the annotation process. Written guidelines make the annotation process more consistent, as well as simplify the task of onboarding new annotators. _All necessary information that is needed to correctly perform our annotation task should be clearly outlined within these guidelines._

> _“The instructions we provided to labelers evolved over the course of the project, as we provided feedback, changed our metadata fields, and developed a better understanding of what we wanted to measure. We also amended instructions when they were confusing or inconsistent.”_ - from [22]

These guidelines are not static. Rather, we can continually update them over time as we work with human annotators and try to achieve higher levels of agreement. In most projects, our understanding of what we are trying to evaluate becomes more clear and detailed over time. Although we might think that a certain evaluation task is simple or straightforward, most annotation tasks naturally contain a surprising amount of subjectivity. This subjectivity only becomes clear we we try to get a group of human of consistently agree on a given task.

**Continuous monitoring.** Usually, we invest a ton of up front effort into crafting guidelines and calibrating human evaluators for our evaluation task. Once we are happy with the evaluation criteria we have put in place and reach a reasonable level of agreement when evaluating these criteria, we can be more confident in the results of human evaluation and start to use these results as a measure of our LLM’s performance. However, we should continue to measure agreement among human evaluators over time to ensure there is no deterioration or drift. There are several reasons that agreement may decline—_new humans may enter the group of evaluators, evaluators may forget about the guidelines over time, or we might even change our guidelines_! Ensuring that human evaluations are consistent and accurate is a never-ending (but extremely necessary) battle.

#### Traditional (Automatic) Metrics

In prior generations of research, we could evaluate the performance of a language model using traditional (automatic) metrics like [ROUGE](https://en.wikipedia.org/wiki/ROUGE_\(metric\)) (for summarization) or [BLEU](https://en.wikipedia.org/wiki/BLEU) (for translation)[4](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-4-151216391). These metrics, which are some of the simplest automatic evaluation techniques that exist, are reference-based, meaning that they operate by comparing the LLM’s output to some “golden” reference answer—_usually by counting the number of matching n-grams_[5](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-5-151216391). If the model’s output is similar to the reference answer, then the score is good and vice versa; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0db8b0bd-b31e-4f5d-a335-53f4b782bed9_2184x1426.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0db8b0bd-b31e-4f5d-a335-53f4b782bed9_2184x1426.png)

Definition of BLEU and ROUGE

As the capabilities of language models have rapidly progressed over the last few years, these traditional metrics have become less and less effective. Recent research has shown that these metrics now correlate poorly with human preferences when evaluating the output of an LLM [21]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F794dcc04-5067-484c-b110-52d6481ae963_1514x1166.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F794dcc04-5067-484c-b110-52d6481ae963_1514x1166.png)

As such, traditional metrics are less effective when used to evaluate LLMs, but these metrics are still relatively common because they are so cheap and simple to compute. Assuming that we have access to reference answers, we can use these metrics as a quick and simple sanity check for a variety of use cases.

> _“We find that all [traditional] metrics we consider have low correlation with the human judgement.”_ - from [21]

**Why do traditional metrics fall short?** There are a few reasons that traditional metrics tend to correlate poorly with human preferences, but the most pressing issue is that a majority of these metrics are reference-based. They operate by comparing the model’s output to a ground truth answer that we want to match—_basically a more sophisticated version of fuzzy matching_. However, modern LLMs are incredibly open-ended! Given a single prompt, there are many equally-valid responses that could be generated by the LLM. So, if we evaluate according to a single reference, _we fail to capture the spectrum of valid responses that exist_.

#### LLM-as-a-Judge

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8174cbd0-a4d0-4117-9c5f-bc09994eca7e_1404x1290.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8174cbd0-a4d0-4117-9c5f-bc09994eca7e_1404x1290.png)

(from [8])

The shortcomings of traditional evaluation metrics have led to the development of more flexible and general evaluation techniques. One of the most popular of these techniques is a model-based evaluation strategy, called LLM-as-a-Judge [8], that performs evaluation my prompting a language model. More specifically, we rely upon a powerful LLM as a “judge” to evaluate the outputs of another LLM. As shown in [8], LLM-as-a-Judge has high correlation with human preferences, is easy to implement (i.e., just write a prompt!), can handle open-ended outputs, and is flexible enough to capture a wide variety of different evaluation criteria.

> _“LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.”_ - from [8]

Within this post, we will use the term “LLM-as-a-Judge” to refer specifically to prompting off-the-shelf, proprietary LLMs (e.g., [GPT-4o](https://openai.com/index/hello-gpt-4o/) or [Gemini-1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)) for evaluation purposes (i.e., as opposed to finetuning our own specialized LLM judge). We will go over some basic concepts for LLM-as-a-Judge here, but there are also a variety of helpful posts and papers that have been written on this topic (including a previous post from this newsletter):

- Using LLMs for Evaluation [[link](https://cameronrwolfe.substack.com/p/llm-as-a-judge)]
    
- Creating a LLM-as-a-Judge That Drives Business Results [[link](https://hamel.dev/blog/posts/llm-judge/)]
    
- Evaluating the Effectiveness of LLM-Evaluators [[link](https://eugeneyan.com/writing/llm-evaluators/)]
    
- A Survey on LLM-as-a-Judge [[link](https://arxiv.org/abs/2411.15594)]
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93a896-224f-4bdf-b5aa-6512f498b1d3_2580x870.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93a896-224f-4bdf-b5aa-6512f498b1d3_2580x870.png)

(from [8])

**Scoring setups.** There are two primary ways that we can evaluate a model output with LLM-as-a-Judge (prompts are shown above):

- _Pairwise:_ the judge is presented with a prompt and two responses to the prompt, then asked to select the better response.
    
- _Pointwise:_ the judge is presented a single prompt and response, then asked to grade the response; e.g., using a [Likert scale](https://en.wikipedia.org/wiki/Likert_scale) from 1-5.
    

There are several other terms that can be used to refer to the pointwise scoring setup as well, such as direct assessment or single-response grading. Beyond these two scoring setups, we may also see reference-based scoring setups that include a reference in the LLM judge’s prompt when asking for a score. Reference-guided grading can be applied to either of the scoring setups; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172675eb-d21f-4b42-a497-7459c76a8a69_922x738.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172675eb-d21f-4b42-a497-7459c76a8a69_922x738.png)

(from [8])

For a public implementation of LLM-as-a-Judge, check out [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval). This widely-used leaderboard uses LLM-as-a-Judge to predict human preference scores for LLM outputs. All of the prompts used for evaluation—_including both current and previous iterations of the leaderboard_—are openly shared.

**Which setup should we use?** In general, no one scoring setup is best for LLM-as-a-Judge. The optimal choice of scoring setup is usually application dependent. Pairwise scoring tends to be more stable compared to pointwise scoring but is also susceptible to [position bias](https://cameronrwolfe.substack.com/i/141159804/biases-and-how-we-can-avoid-them) and less scalable. With pairwise scoring, we can only grade pairs of model outputs in a relative fashion, whereas pointwise scoring is more versatile and allows us to simply assign a single score to a model output. However, pointwise scoring is a more challenging task, as it relies upon the LLM judge being able to assign a score solely based upon its internal knowledge instead of comparing one output to another. To make this issue less pronounced, we can optionally perform pointwise scoring with references.

> _“Some evaluation tasks, such as assessing faithfulness or instruction-following, don’t fit the pairwise comparison paradigm. For example, a response is either faithful to the provided context or it is not—evaluating a response as more faithful than the alternative address the eval criteria.”_ - [Eugene Yan](https://eugeneyan.com/writing/llm-evaluators/#key-considerations-before-adopting-an-llm-evaluator)

Objective criteria (e.g., factuality) are oftentimes difficult to evaluate in a pairwise fashion, as model outputs tend to either satisfy these criteria or not—_objective crtieria are usually binary in nature_. We can evaluate whether a given model output is factual or not, but evaluating whether one model output is more factual than another is somewhat ill-defined. In contrast, subjective evaluation criteria are better handled via a pairwise scoring setup due to the fact that relative scoring is more well-defined, stable, and reliable.

**Specialized judges.** Although LLM-as-a-Judge is an incredibly effective and widely-used approach, there are several limitations to this technique:

- The LLM is not transparent and comes with security concerns.
    
- We do not control the versioning of the judge—_someone could update the model (and break our evals as a result)_.
    
- Every call to the LLM judge costs money, so cost can become a concern if we are evaluating model outputs at scale.
    
- Proprietary LLM judges are best at tasks that are highly aligned with their training data; e.g., predicting human preference scores.
    
- Proprietary LLMs are generic (i.e., not specialized to perform evaluation) and tend to avoid providing strong scores or opinions.
    

Most of these limitation are caused by using a proprietary LLM over which we have little control. So, we might wonder: _Could we just train our own LLM judge?_ As we might deduce from the title of this overview, the answer is yes! Finetuning our own LLM judge is a great way to create a domain-specific evaluation model that can provide more granular, accurate and critical feedback.

#### Meta-Evaluation: Evaluating our Evaluation Model

Before we learn how to train our own LLM judge, _we need to know how we will determine whether the judge is performing well or not_. To evaluate our LLM judge, we must first collect a set of human evaluation data. We should be very confident that this data is accurate and reliable, as we will use it to measure the quality of our evaluation models. Once we have a set of high-quality data annotated by humans, we can perform meta-evaluation (i.e., _evaluate our evaluator_) by simply comparing the evaluator’s output to the results of human evaluation.

If our evaluator produces binary output (e.g., pairwise scoring or single-response grading with a binary scale), we can simply use [classification metrics](https://eugeneyan.com/writing/evals/#classificationextraction-roc-pr-class-distributions). Given that classification metrics are so easy to interpret, many practitioners have made the argument that sticking to binary scoring is best for LLM-as-a-Judge.

> _“We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin.”_ - from [23]

For scoring settings that are not binary (e.g., single-response grading with a 1-5 Likert scale), we must measure the correlation between human and automatic evaluation scores[6](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-6-151216391). An example of correlation metrics being used in a recent LLM-as-a-Judge paper is provided above. [Spearman correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) is probably the most commonly-used correlation metric, but many others exist (e.g., [Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa), [Kendall’s tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), or [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)). Unfortunately, these metrics are more difficult than classification metrics to interpret, so we should always be familiar with the nuances of the particular correlation metric we are using.

## Early Research on Finetuned Judges

At first, most practitioners relied heavily upon proprietary LLMs for LLM-as-a-Judge-style evaluations. _Why was this the case?_ In order to evaluate a model’s output for a particular task, the judge—_unless a reference answer is provided as input_—must be able to solve that task itself. The capabilities of open-source LLMs have lagged behind those of proprietary models for quite some time, so closed models were generally a better choice for evaluation purposes. As we will see in this section, however, researchers began to finetune specialized LLM judges as more capable, open-source LLMs were released (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone) and [LLaMA-2](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)).

#### [Applying Finetuning to LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) [8]

> _“A fine-tuned Vicuna-13B model shows strong potential to be used as a cheap open-sourced replacement for expensive closed-sourced LLMs.”_ - from [8]

One of the first papers to explore the creation of a finetuned LLM judge was actually the original LLM-as-a-Judge paper! The majority of this paper is focused upon leveraging proprietary LLMs to evaluate human preferences via a set of generic prompts; see [here](https://cameronrwolfe.substack.com/i/141159804/what-is-llm-as-a-judge) for details. However, authors mention the potential of finetuning a custom LLM judge and perform early experiments in this direction.

**Why finetune?** Although LLM-as-a-Judge is a useful technique, we see in [8] that there are several clear motivations for finetuning our own LLM judge instead of using an off-the-shelf model:

- API-based evaluations can be expensive.
    
- Using a proprietary model provides no control or transparency.
    
- Open source models are becoming more capable over time.
    

For these reasons, specialized evaluators are a promising direction of research if finetuned judges are capable of matching the performance of proprietary models.

**Does this work?** The finetuned evaluator created in [8] is based upon [Vicuna-13B](https://lmsys.org/blog/2023-03-30-vicuna/), a derivative of LLaMA. Without any finetuning, this model is found to be a poor judge. The base model has a high error rate, struggles to follow templates or instructions provided for evaluation, and suffers from severe positional bias.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e74dd72-bf33-41c3-ae70-aef630ef323a_1080x350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e74dd72-bf33-41c3-ae70-aef630ef323a_1080x350.png)

(from [8])

However, the model’s evaluation capabilities are drastically improved by finetuning on human votes from [Chatbot Arena](https://arxiv.org/abs/2403.04132). Authors train the model on 20K single-turn votes that compare the outputs of a wide variety of different LLMs. To simplify the evaluation process, we just formulate evaluation as a three-way classification problem (i.e., win, lose or tie) for the LLM. After finetuning, the following properties of the finetuned judge are observed:

- The positional bias of Vicuna is drastically reduced; shown above.
    
- The scoring accuracy of the model is much better.
    
- The finetuned LLM judge still falls short of GPT-4’s performance.
    

The exploration of finetuned models in [8] is minimal (i.e, just one page in the appendix). However, the initial results were sufficiently promising to inspire further work in this direction given the benefits of open evaluation models.

#### **[PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](https://arxiv.org/abs/2306.05087) [6]**

One of the most common use cases for LLM judges is for identifying the best model among a set of models (i.e., automatic evaluation). Discerning changes in performance between LLMs is difficult, especially if the models being compared are both of high quality. We can lean on humans to provide feedback on our models, but this process is slow—_having a judge that can quickly and accurately identify improvements in model performance is very useful_.

> _“The goal is to ensure PandaLM not only prioritizes objective response correctness but also emphasizes critical subjective aspects such as relative conciseness, clarity, comprehensiveness, formality, and adherence to instructions.”_ - from [6]

In [6], authors propose PandaLM[7](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-7-151216391)—_a specialized evaluator LLM that can be used to identify the highest performing model within a group_. This model goes beyond evaluating basic properties of a response (e.g., correctness) by addressing subjective criteria like clarity, conciseness, and comprehensiveness.

The main motivation for PandaLM is to better automate the hyperparameter tuning process for LLMs. Using this model, we can easily compare several models trained with varying hyperparameters and identify the best model or setting. We see in [6] that all models perform better when PandaLM is used to identify the optimal training settings. Such a finding shows that there are several ways in which PandaLM can be used to improve the performance of an LLM.

**Training PandaLM.** To train this model, authors create a custom evaluation dataset comprised of over 300K training examples. Each example in this dataset consists of an `(instruction, input, response1, response2)` input tuple and an `(evaluation result, evaluation rationale)` output tuple. A reference answer is provided for all dataset examples, and the evaluation result simply selects one of the responses as better or declares a tie. The authors create PandaLM by finetuning LLaMA[8](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-8-151216391) models of several sizes on this dataset.

> _“During the fine-tuning phase of PandaLM, we use the standard cross-entropy loss targeting the next token prediction. The model operates in a sequence-to-sequence paradigm without the necessity for a separate classification head.”_ - from [6]

All instructions and inputs within this dataset are sampled from the [training dataset for Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html). Several models—_[LLaMA-7B](https://www.google.com/search?q=LLAMA%3A+LLMs+for+everyone&rlz=1C5GCCM_en&oq=LLAMA%3A+LLMs+for+everyone&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg6MgYIAhBFGEDSAQg1NDg2ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8), [BLOOM-7B](https://cameronrwolfe.substack.com/i/135273362/bloom-an-open-multilingual-language-model), [Cerebras-GPT-7B](https://arxiv.org/abs/2304.03208), [OPT-7B](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15), and [Pythia](https://arxiv.org/abs/2304.01373)_—are used to generate the pairs of responses. To generate the corresponding evaluation results and rationales for these pairs, a synthetic approach based upon [Self-Instruct](https://arxiv.org/abs/2212.10560) and GPT-4 is used. Several heuristics (e.g., hand crafted rules and filtering invalid or unstable evaluations) are used to ensure the quality of the training data. A smaller test set of “golden” test examples is collected from a group of human annotators for meta-evaluation purposes.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f9427ae-5d0d-4be5-bd76-88129034d086_1288x748.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f9427ae-5d0d-4be5-bd76-88129034d086_1288x748.png)

(from [6])

**PandaLM in practice.** When PandaLM is compared to other evaluators, we see that that these models—_GPT-3.5 and GPT-4 in particular_—have similar trends in preferences. More specifically, all of these models output a consistent ranking of other models during evaluation; see above. When PandaLM is directly compared to proprietary models, we see that _i)_ PandaLM-7B produces competitive results and _ii)_ PandaLM-70B surpasses the evaluation performance of GPT-4; see below. These results indicate that increasing model size benefits evaluation capabilities.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ec425cd-55fa-45a3-92ff-3183d8b0453c_1372x242.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ec425cd-55fa-45a3-92ff-3183d8b0453c_1372x242.png)

(from [6])

PandaLM models—_and especially the larger model_—are also highly effective in specialized domains, such as the legal or biological setting. Because PandaLM is finetuned on evaluation data, this model excels not just in general evaluation settings, but also in specialized or domain-specific evaluation settings that align well with its training data. For example, PandaLM is (successfully) used to score responses to LSAT questions and biomedical QA datasets in [6]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342f3cf2-dfe9-4fcb-80db-d44438feb882_1194x402.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342f3cf2-dfe9-4fcb-80db-d44438feb882_1194x402.png)

(from [6])

#### **[JudgeLM: Fine-tuned Large Language Models are Scalable Judges](https://arxiv.org/abs/2310.17631) [9]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F204430e9-698d-4cf1-810f-b04a6f871f10_1076x880.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F204430e9-698d-4cf1-810f-b04a6f871f10_1076x880.png)

(from [9])

Most of the early work on finetuned LLM judges does very little to analyze the factors that contribute most to the quality of the resulting judge model. In [9], authors aim to solve this problem by exploring various important performance factors when training their customized JudgeLM model:

- The amount, quality and diversity of training data.
    
- The quality and size of the underlying base model.
    
- The impact of bias on the LLM judge’s output.
    
- The ability to generalize to different scoring setups (e.g., multi-turn chat, single answer grading, pairwise ranking, [multi-modal models](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms), etc.)
    

There are many reasons a finetuned LLM judge might fall short of a proprietary model in terms of evaluation capabilities; e.g., the size and quality of the base model, data quality issues, bias, and more. However, we see in [9] that we can address these issues in practice to create effective finetuned judges.

**High-quality data** is one of the major keys to success in [9]. To create an effective finetuned judge, we must have a large-scale dataset that is comprised of high quality, granular[9](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-9-151216391), and diverse data. Moving in this direction, authors of this paper claim that _“the dataset [they] introduce stands as the most diverse and high-quality one”._ To construct this dataset, we start by selecting 105K seed instructions from a variety of public sources (e.g., [ShareGPT](https://sharegpt.com/) and the [Alpaca-GPT-4 dataset](https://huggingface.co/datasets/vicgalle/alpaca-gpt4)).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9341e3-a792-4278-a472-ecc79894107b_1072x1042.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9341e3-a792-4278-a472-ecc79894107b_1072x1042.png)

(from [9])

Answers to these seed tasks are generated synthetically from a variety of LLMs, such as [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), and [LLAMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone). By combining LLM generated answers with original answers from the source dataset, we obtain a set of multiple answers for each instruction in the dataset. Plus, we have access to reference answers to each instruction from the source dataset. From here, we can randomly sample pairs of responses for each instruction, as well as generate scores and rationales for each of these pairs using GPT-4. In [9], GPT-4 serves as a powerful “teacher” model for JudgeLM—_the finetuned judge is trained on the results of GPT-4’s evaluation_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a80e6b-2dd6-4a9a-a6d1-4894b388ff45_1764x732.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a80e6b-2dd6-4a9a-a6d1-4894b388ff45_1764x732.png)

The templates used for generating scores and rationales with GPT-4—_including templates that both do and do not use references_—are shown above. These prompts are relatively standard and largely match what we see in most public LLM-as-a-Judge setups. However, GPT-4 is provided with relatively detailed evaluation criteria when generating its scores and explanations, which encourages the model to evaluate responses on a more granular level. Such granular evaluation capabilities are transferred to JudgeLM by finetuning on this data.

**Creating the model.** Several sizes of JudgeLM models—_including a 7B, 13B, and 33B parameter model_—are trained over datasets with sizes ranging from 3.5K to 100K examples in [9]. JudgeLM is trained using an [instruction tuning](https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/) approach with [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised). The input and output structure of the model is depicted in the figure below. As input, we provide two different responses to a single instruction, as well as a reference answer to this instruction. The model then generates separate scores for each response and provides a rationale for the scores that are generated. We can translate the scores assigned to each answer into a pairwise result by simply comparing the scores to each other.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe82f556f-5775-47bf-b6a7-a8c12c5f7af1_1072x1042.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe82f556f-5775-47bf-b6a7-a8c12c5f7af1_1072x1042.png)

(from [9])

**Mitigating bias** is a primary focus of training JudgeLM. More specifically, there are three types of bias that authors try to combat in their evaluator:

1. _Position bias_: this is the same position bias we have seen before, where the judge prefers one answer over another due to its position in the prompt.
    
2. _Knowledge bias_: this form of bias occurs when the model lacks critical knowledge from pretraining that is needed to evaluate a response.
    
3. _Format bias_: this form of bias occurs when the model only performs well when provided a prompt with a specific format (e.g., the format with which the model is finetuned).
    

Several strategies exist for combatting bias. For example, format bias can be avoided by exposing the LLM judge to multiple prompt formats during finetuning, and knowledge bias can be addressed by simply providing reference answers as input. To address positional bias, we can use position switching—_repeating evaluation with answers at all possible positions_—during both training and inference. Usually, we consider responses that are ranked differently after position switching a tie. Swapping positions during finetuning forces JudgeLM to pay more attention to the content of the answer rather than the position, and using the same trick during inference lets us ensure that the impact of position bias on the model’s output is minimal. When used together, these tricks drastically reduce the bias of the JudgeLM model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff18762c8-f15e-4c17-a2e8-e11b4ae27d49_1114x664.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff18762c8-f15e-4c17-a2e8-e11b4ae27d49_1114x664.png)

(from [9])

**How does JudgeLM perform?** When tested on the JudgeLM evaluation dataset, JudgeLM outperforms PandaLM [6] and GPT-3.5 both with and without reference answers provided as input; see below. In this case, JudgeLM is finetuned over data from a very similar distribution, while the baseline models are not. However, JudgeLM is also shown to perform well on the PandaLM evaluation dataset [6]. JudgeLM’s performance is also reported in terms of agreement with GPT-4 (not humans)—_the same model used to generate the training data for JudgeLM_. Generally, the JudgeLM models perform relatively well, improve in performance with size, and are more robust to position switching compared to other models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F029d35ed-5004-4c26-8f0e-3555b5e31986_1658x502.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F029d35ed-5004-4c26-8f0e-3555b5e31986_1658x502.png)

(from [9])

#### [Generative Judge for Evaluating Alignment](https://arxiv.org/abs/2310.05470) [7]

> _“Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques.”_ - from [8]

In [7], authors propose another finetuned LLM for evaluation purposes, called Auto-J, that specializes in domain-specific grading. Some unique properties of the Auto-J model include:

- The ability to perform pairwise and direct assessment scoring.
    
- An emphasis on providing high-quality, structured explanations.
    
- The use of human-generated queries for training[10](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-10-151216391) to mimic diverse and realistic scenarios—_the training data is grounded in questions asked by humans_.
    

The resulting Auto-J model is flexible, interpretable, and practical. The model is optimized to perform scenario-specific evaluations in real-world settings.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c88081c-fb12-400f-9c6f-668701c33960_1070x394.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c88081c-fb12-400f-9c6f-668701c33960_1070x394.png)

(from [7])

**Training data.** Auto-J is trained on both pairwise and direct assessment scoring data. The steps for constructing the model’s training dataset are depicted above. We begin by defining the evaluation criteria—_or scenarios_—on which Auto-J is trained. In particular, 58 evaluation scenarios are selected—_each with their own definition and criteria_—that can be categorized into eight groups; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6b5ca6e-0fb0-49ac-988b-7d094e2c48d3_1074x622.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6b5ca6e-0fb0-49ac-988b-7d094e2c48d3_1074x622.png)

(from [7])

Then, we collect real-world queries for each scenario and synthetically generate responses to these queries using GPT-4. Up to 100 training examples are generated for each scenario, and the final size of the dataset is ~3,500 examples.

> _“We guide GPT-4 with carefully hand-written criteria for each scenario to collect desired evaluation judgments as our supervised training signals and apply heuristic filtering strategies and post-processing methods to unify output formats and mitigate noise.”_ - from [7]

Queries are largely collected from public sources that are based upon human interaction; e.g., [Chatbot Arena](https://arxiv.org/abs/2403.04132) and [WebGPT](https://arxiv.org/abs/2112.09332). These queries accurately reflect the kind of questions that a model will encounter in the wild, allowing Auto-J to specialize in realistic evaluation settings. Evaluation scores—_including both pairwise and direct assessment scores_—are generated using GPT-4. To ensure the quality of synthetic data, authors introduce several heuristics to filter the final dataset (e.g., discarding inconsistent or incorrectly formatted predictions).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40df0b07-5b9a-4ff3-8b06-f9587979233b_446x414.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40df0b07-5b9a-4ff3-8b06-f9587979233b_446x414.png)

Auto-J has less positional bias compared to other LLM judges (from [7])

**Training Auto-J.** In [7], [LLaMA-2-13B-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat) is used as a base model, and we finetune this base model over the dataset described above. Interestingly, authors choose to not provide the evaluation criteria as input to the model. _They claim that learning such criteria implicitly from the data is a better approach, as excluding criteria from the input makes the model more general_. Such a strategy directly contradicts later papers on this topic, which emphasize the importance of including reference materials (e.g., criteria and scoring rubrics) in the model’s input [1]. Auto-J is trained over data with multiple scoring formats from this dataset, and authors adopt position switching during training to avoid positional bias; see above.

> _“To lessen the positional bias in pairwise comparison, we apply a simple data augmentation trick. For each pairwise training sample, we swap the order of two responses in the input and alternate the `Response 1` and `Response 2` in the evaluation judgment.”_ - from [7]

**Evaluation results.** To evaluate AutoJ, a test dataset is created similarly to the model’s training dataset. However, all scoring results for the test dataset are generated with human annotators. When compared to a variety of other LLMs, Auto-J is found to outperform nearly every model except for GPT-4; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dbb3c79-e9dd-48b1-bdd4-ba4193b8cfea_1074x520.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dbb3c79-e9dd-48b1-bdd4-ba4193b8cfea_1074x520.png)

(from [7])

Such a result indicates that specialized LLM judges can perform relatively well—_matching or exceeding the performance of proprietary models in some cases_—when finetuned on domain-specific data for a particular evaluation setting. However, the agreement rate of Auto-J with human evaluators on this dataset is still low. In other words, _we still have a long way to go in terms of the LLM judge’s performance_.

#### Critiques, Verification and Synthetic Data

In addition to these early attempts at finetuning LLM judges, several papers were concurrently published on the (highly-related) topic of using LLMs to verify, critique, or provide feedback on the output of another model. Such models are very similar to LLM judges, as both models can grade and provide feedback for a response from an LLM. However, critic models and verifiers go beyond just grading a response. They actually edit the LLM’s response or trigger the creation of a new response—_assuming the original response was not good_.

For example, [SelFee](https://lklab.kaist.ac.kr/SelFee/) (depicted above) [10] is a LLaMA-based, specialized critic model that is trained over generations, feedback, and revised generations from ChatGPT. Given an original response as input, SelFee can provide feedback for this response, as well as revise the response based on the feedback. Additionally, SelFee can continue to revise its output until the quality meets a given threshold, making this model both a verifier (i.e., by checking the quality of its own output) and a critic (i.e., by providing feedback and revising its responses).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c00883b-6c05-4ceb-a22e-1c9a55fd8432_2330x714.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c00883b-6c05-4ceb-a22e-1c9a55fd8432_2330x714.png)

Different forms of synthetic data creation with LLMs (from [11])

Many techniques for critiquing LLM outputs have been explored; e.g., [Self-Correction](https://arxiv.org/abs/2211.00053), [PEER](https://arxiv.org/abs/2208.11663), and [Self-Critique](https://arxiv.org/abs/2206.05802). We can also use similar techniques to generate and verify synthetic training data for both preference tuning and SFT. Common examples of this approach include [RLAIF](https://arxiv.org/abs/2309.00267), [Constitutional AI](https://arxiv.org/abs/2212.08073) (shown above) [11], and more. All of these techniques—_LLM-as-a-Judge, finetuned judges, synthetic data creation, critique models, and verifiers_—function very similarly. They just output scores and explanations! But, each type of model is used for a slightly different purpose; e.g., grading a model output for evaluation purposes versus grading a model output to determine if it should be used for finetuning. Recent research argues that we can unify these techniques—_or at least some of them_—via a single LLM that is capable of performing all necessary scoring tasks [12, 17].

## Surpassing LLM-as-a-Judge with Finetuning

The idea of finetuning an LLM judge was not popularized until the proposal of Prometheus [1]—_a finetuned LLM judge that is capable of performing domain-specific, fine-grained evaluation_. Prometheus has significantly improved evaluation capabilities compared to prior finetuned judges. However, this model does not make any major methodological advancements—_it just finetunes starting from a much better base model (LLaMA-2)_. Due to its ability to match the evaluation quality of many proprietary LLMs, Prometheus quickly became popular, leading to the release of several model variants that we will overview in this section.

#### **[Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) [1]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a26aa61-47ef-45a7-93a2-e6c5b11ff588_1620x1604.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a26aa61-47ef-45a7-93a2-e6c5b11ff588_1620x1604.png)

(from [1])

Using proprietary LLMs for evaluation is simple—_we just describe our scoring criteria in a prompt and let the model do the rest_. However, such evaluators have uncontrolled versioning and can be expensive. Plus, proprietary LLMs may struggle to evaluate custom criteria, as they are more commonly used to evaluate generic criteria (e.g., [human preferences](https://github.com/tatsu-lab/alpaca_eval)). As a solution, authors in [1] create a finetuned evaluator LLM, called Prometheus, that can match or exceed the performance of proprietary evaluators when finetuned on a particular domain.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c3a9c0-06f5-476e-9b1f-57d4627eda3f_1602x1100.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c3a9c0-06f5-476e-9b1f-57d4627eda3f_1602x1100.png)

(from [1])

**Training dataset.** Prometheus is trained to ingest custom scoring rubrics as input, allowing the model to easily generalize to specialized domains and criteria. The model is trained over a new dataset called the [Feedback Collection](https://huggingface.co/datasets/prometheus-eval/Feedback-Collection) that is created synthetically with GPT-4 and covers a wide variety of evaluation tasks. Each dataset example has a few common components (shown above):

- _Instruction_: the instruction used to prompt an LLM.
    
- _Response_: the response (to the above instruction) that we are evaluating.
    
- _Rubric_: a customized rubric that specifies the criteria by which the response should be scored / evaluated.
    
- _Reference Answer_: an example of a response to the instruction that would receive the best-possible score.
    
- _Rationale_: an explanation[11](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-11-151216391) for why the response received a particular score.
    
- _Score:_ a 1-5 [Likert score](https://en.wikipedia.org/wiki/Likert_scale) that is assigned to the response.
    

The scoring rubric consists of _i)_ a general description of the scoring criteria and _ii)_ a description of each score that the model can assign. For Prometheus, the options for scoring are a 1-5 Likert scale, so the model expects each of these scores and their meaning to be described. By taking the rubric as input, the model learns to perform fine-grained evaluation and generalize to diverse scoring settings. Authors in [1] are some of the first to demonstrate the importance of providing high-quality reference materials as input to a finetuned LLM judge.

> _“We are first to explore the importance of including various reference materials—particularly the reference answers—to effectively induce fine-grained evaluation capability”_ - from [1]

**Generating the data.** The Feedback Collection is generated synthetically with GPT-4. To create this dataset, we start with a set of 50 human-written scoring rubrics. Then, the dataset is generated by:

1. Expanding to 1K scoring rubrics by prompting GPT-4 with examples of the manually-written scoring rubrics.
    
2. Generating instructions for each scoring rubric, resulting in 20K total instructions (i.e., 20 instructions for each rubric).
    
3. Creating training instances for each instruction by sequentially generating a response, feedback, and score for each instruction.
    

A visual summary of the above steps that are followed to synthetically generate the Feedback Collection with GPT-4 is provided in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b03195b-f841-4fc1-9dd3-4a25a266a311_1602x1658.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b03195b-f841-4fc1-9dd3-4a25a266a311_1602x1658.png)

(from [1])

To make the dataset balanced, we prompt GPT-4 to generate five different training examples for each instruction—_one for each 1-5 Likert score_. Additionally, we ensure that each training example has the same length to avoid [verbosity bias](https://cameronrwolfe.substack.com/i/141159804/biases-and-how-we-can-avoid-them) within the evaluator. A summary of the final dataset is provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac426b78-c32e-4423-be9e-307f104245c1_1610x322.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac426b78-c32e-4423-be9e-307f104245c1_1610x322.png)

(from [1])

**Training Prometheus.** [LLaMA-2-13B-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat) is used as a base model for Prometheus. The model is trained using the Feedback Collection dataset, as well as [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and [Vicuna Bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge). Given all reference material as input, the model is trained—_using a [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) strategy_—to sequentially provide feedback then score the response. Importantly, _we generate feedback prior to actually scoring the response_, allowing the model to use this feedback as context when assigning as score. Additionally, the authors note that it is very important to insert a special token—such as `[RESULT]`—between the feedback and the score to ensure that the model does not degenerate[12](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-12-151216391) during inference.

> _“Similar to Chain-of-Thought Finetuning, we fine-tune to sequentially generate the feedback and then the score.”_ - from [1]

**Is Prometheus an effective evaluator?** Prometheus is evaluated across several datasets in comparison to GPT-3.5, GPT-4, and several open-source base models in terms of its ability to match the evaluation scores provided by humans and produce useful feedback. In these experiments, we see that the scores generated by Prometheus have a Pearson correlation of 0.897 with human evaluators, which is comparable to the correlation of scores generated by GPT-4 and much better than the correlation of scores generated by GPT-3.5; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F765cdfe1-5fbd-478e-aa2f-89cf5774d055_1610x898.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F765cdfe1-5fbd-478e-aa2f-89cf5774d055_1610x898.png)

(from [1])

These results show us that we can achieve impressive levels of agreement with human evaluation—_matching or exceeding powerful models like GPT-4_—by directly finetuning an LLM on high-quality evaluation data within a certain domain. Additionally, humans select feedback from Prometheus as preferable to that of GPT-4 in 58.67% of cases, indicating that the model’s explanations are useful. Prometheus’ feedback is preferred to that of GPT-3.5 in nearly 80% of cases!

> _“We conclude that whereas GPT-4 tends to be more neutral and abstract, Prometheus shows a clear trend of expressing its opinion of whether the given response is good or not.”_ - from [1]

When humans are asked to explain why feedback from Prometheus is preferred, we learn that the finetuned model tends to be more critical and generates targeted feedback; see below. Unlike proprietary models, Prometheus is not a general-purpose LLM. Rather, it is a specialized model for providing critiques and feedback. As a result, this model is less neutral, abstract, and generic relative to proprietary foundation models when used in an evaluation setting.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe56d4d65-f876-4698-949a-aad8a7b3994c_1606x1076.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe56d4d65-f876-4698-949a-aad8a7b3994c_1606x1076.png)

(from [1])

#### **[Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) [2]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae04a0a-eaaa-4536-b27a-941dfe4fb189_600x536.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae04a0a-eaaa-4536-b27a-941dfe4fb189_600x536.png)

(from [2])

Continuing the direction of research from [1], Promtheus 2 [2] is another open LLM that is finetuned to perform fine-grained evaluation with custom criteria. The original Prometheus model has several limitations. For example, the model has a long way to go in terms of scoring quality—_there is still a noticeable amount of disagreement with human quality scores_. Most notably, however, Prometheus is only capable of scoring via direct assessment. Although we can derive a ranking by comparing pointwise scores from Prometheus, this model—_and most of the open evaluators proposed before it_—lacks the ability to perform pairwise scoring. In [2], authors unify these paradigms with a single, fine-grained evaluation model that closely matches the accuracy of humans and GPT-4 for both types of scoring.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5535029-55d3-42a5-a49b-b331c54a415a_1226x734.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5535029-55d3-42a5-a49b-b331c54a415a_1226x734.png)

(from [2])

**Towards pairwise scoring.** For Prometheus 2, the pairwise scoring setup has a few notable differences compared to direct assessment scoring:

- No reference answers are present within the reference material.
    
- The reference material simply describes the evaluation criterion.
    
- The feedback provided with the score has a different style.
    

Instead of providing just an explanation for a score, the feedback provided within the pairwise scoring setting must compare and contrast the two responses that are provided, using their differences to identify one as superior to the other; see above. Most prior models and datasets only perform pairwise scoring according to generic feedback and tend to omit pairwise feedback alongside the score[13](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-13-151216391).

**The Preference Collection.** Extending upon the Feedback Collection from [1], authors in [2] create an additional dataset for pairwise scoring, called the Preference Collection. The Feedback Collection contains five responses for each instruction—_one response for each 1-5 Likert score_. To create a pairwise version of this dataset, we can simply consider all combinations of these responses. For each instruction, ten possible pairs can be formed, and we can derive a score for each pair by just comparing Likert scores; see below for a summary.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F643c18f8-7e11-415d-b79e-0a6571125c35_606x516.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F643c18f8-7e11-415d-b79e-0a6571125c35_606x516.png)

(from [2])

Although many pairwise scoring datasets tend to provide only scores without explanations, leveraging chain of thought-style feedback for improved scoring and explainability is a key component of Prometheus. As such, authors in [2] augment the Preference Collection by prompting GPT-4 to explain each pairwise score, focusing on the similarities and differences of each response.

> _“To generate new verbal feedback for each pair of responses, we prompt GPT-4 to identify the commonalities and differences of the two responses.”_ - from [2]

**Model merging.** There are a few different ways we can build support for both direct assessment and pairwise scoring within an LLM judge:

- _Prompting_: we can adopt an LLM-as-a-Judge approach and create different prompts to send to an off-the-shelf model for each scoring setup.
    

- _Single-format training_: we can train separate LLMs on each scoring format and use them separately.
    
- _Join training_: we can train a single model on both direct assessment and pairwise scoring examples[14](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-14-151216391), allowing the model to function in both settings.
    

Of the approaches above, only joint training produces a single, specialized model that can support both scoring setups—_prompting relies upon generic / off-the-shelf models and single-format training produces a separate model for each scoring format_. Alternatively, we can combine the models produced by single format training via model merging. For those who are not familiar with the concept of model merging, check out the comprehensive overview of this topic below.

[

## Model Merging: A Survey

](https://cameronrwolfe.substack.com/p/model-merging)

[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

2024年9月16日

[![Model Merging: A Survey](https://substackcdn.com/image/fetch/w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feea90630-3376-4b9a-8a7c-c410713b195d_2564x1426.png)](https://cameronrwolfe.substack.com/p/model-merging)

Unlike an ensemble, which averages the predictions of several models, model merging averages the weights of these models, forming a single model that mixes their capabilities without any added inference cost.

[

Read full story

](https://cameronrwolfe.substack.com/p/model-merging)

This is exactly the approach taken by Prometheus 2. We first independently train two LLMs over the Feedback and Preference collections. Then, we merge the weights of these models using a simple, linear merging scheme (shown below) to produce a single model that can perform both pairwise and pointwise scoring.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8e5ea42-159a-4adb-9c01-43fd20c3bf1a_960x470.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8e5ea42-159a-4adb-9c01-43fd20c3bf1a_960x470.png)

Linear model merging (from [2])

Most notably, this merging approach is shown to outperform LLMs that are jointly trained or trained individually on each scoring format. Authors test several merging schemes in [2] (e.g., [task arithmetic](https://cameronrwolfe.substack.com/i/147448898/editing-models-with-task-arithmetic), [TIES](https://cameronrwolfe.substack.com/i/147448898/ties-merging-resolving-interference-when-merging-models), and [DARE](https://cameronrwolfe.substack.com/i/147448898/language-models-are-super-mario-absorbing-abilities-from-homologous-models-as-a-free-lunch)). In some cases, these more advanced merging strategies yield a benefit. For example, DARE yields the best performance when Mixtral 8x-7B is used as the base model. However, the linear merging scheme—_with a coefficient of 0.5 (i.e., taking an average of the models’ weights)_—performs well in most cases and is easy to implement.

> _“We show that merging the weights of evaluator LLMs trained on direct assessment and pairwise ranking feedback datasets results in a unified evaluator LM that excels in both schemes.”_ - from [2]

**A unified evaluator.** Using [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) or [Mixtral 8x-7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) as a base model, different versions of Prometheus 2 are created by _i)_ finetuning separate copies of these models on each scoring dataset and _ii)_ combining the weights of these single-format evaluators using various merging algorithms. The resulting model is found to outperform all open evaluator LLMs on several direct assessment and pairwise scoring benchmarks. In particular, Prometheus 2 improves the Pearson correlation with proprietary evaluators by 0.2 units across all datasets and reduces the performance gap with GPT-4 by ~50% on pairwise ranking datasets; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3049af03-dff7-451d-84d0-942262f16498_1238x1304.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3049af03-dff7-451d-84d0-942262f16498_1238x1304.png)

(from [2])

We might notice in the table above that the performance improvement of Prometheus 2 are not as significant in the pairwise scoring setting. However, this table is comparing Prometheus 2 to both open evaluators and reward models. These reward models—_which are used for predicting rewards within [reinforcement leraning from human feedback](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)_—specialize in pairwise scoring only, are finetuned on this task, and do not output any feedback with their scores. The reward models achieve impressive performance in terms of pairwise ranking accuracy. We also see in [2] that Prometheus 2 tends to be more consistent than other evaluators across scoring settings, _meaning that the selected response within a pairwise setting will usually also receive a higher score in the direct assessment setting_.

#### **[Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) [3]**

All of the work we have looked at so far focuses upon text-only LLMs, but Vision-Language Models (VLMs)—_referring to LLMs that can take both images and text as input_—have recently gained in popularity. Both proprietary (e.g., [GPT-4V](https://openai.com/index/gpt-4v-system-card/) and [Gemini](https://deepmind.google/technologies/gemini/)) and open (e.g., [LLaVA](https://llava-vl.github.io/) and LLaMA-3.4) VLMs exist. Aside from being able to handle multiple input modalities, these models are not much different from text-based LLMs in terms of their training or architecture. See below for an accessible overview of VLMs with practical examples from recent research.

[

![](https://substackcdn.com/image/fetch/w_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png)Ahead of AI

Understanding Multimodal LLMs

It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published…

Read more

6 months ago · 219 likes · 37 comments · Sebastian Raschka, PhD

](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms?utm_source=substack&utm_campaign=post_embed&utm_medium=web)

VLMs generate (textual) output given a combination of images and instructions as input. Relative to text-based LLMs, evaluating VLMs is slightly more difficult because we must both:

1. Check whether the VLM follows the instruction (same as before).
    
2. Determine if the VLM’s response is “grounded” in the image.
    

To do this, we could derive a text-based representation of the image (e.g., via a [captioning model](https://arxiv.org/abs/2103.00020)) and pass this information to a text-based LLM judge; e.g., GPT-4 or Prometheus [5]. However, such a multi-stage pipeline is prone to errors—_it makes much more sense to directly use a VLM as the judge_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07a1de85-b944-4b7d-a562-c9c8d3101643_1222x1228.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07a1de85-b944-4b7d-a562-c9c8d3101643_1222x1228.png)

(from [3])

**Prometheus-Vision** is an open, VLM-based evaluator proposed in [3]. Proprietary VLMs like [GPT-4V](https://openai.com/index/gpt-4v-system-card/) can and have been used to evaluate other VLMs [4]. Following the direction of prior Prometheus models, Prometheus-Vision is the first open VLM that can evaluate according to fine-grained, user-defined criteria. In addition to providing more transparency and control, Prometheus-Vision produces scores that correlate well with scores from humans and proprietary models. This model is surprisingly effective and can even capture subtle nuances in the data; e.g., the difference between artwork and parody (see above figure).

> _“In contrast to the language domain, to the best of our knowledge, there do not exist any available feedback, critique, or preference datasets applicable to train an evaluator VLM that could assess in a fine-grained manner.”_ - from [3]

**The Perception Collection**, which is inspired by the Feedback Collection, is an image-and-text-based evaluation dataset created to train Prometheus-Vision. At the time of publication, no such multi-modal feedback dataset yet existed for training VLM-based evaluation models. Each dataset instance in the Perception Collection has five input components:

- _Image_: an image provided by the user to the VLM as input.
    

- _Instruction_: a textual instruction provided by the user to the VLM.
    
- _Response_: a textual response generated by the VLM based upon the image and instruction provided as input.
    
- _Rubric_: a set of detailed scoring guidelines—_including a description of the criteria and an explanation of each possible score on a 1-5 Likert Scale_—that should be referred to when producing a score.
    
- _Reference Answer_: an example response to the image and instruction input that would receive a score of 5.
    

In [3], authors generate all reference answers synthetically using GPT-4V. Each dataset instance also has two output components: _i)_ a written rationale or feedback and _ii)_ a scoring decision according to a 1-5 Likert scale; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17ff62e6-d5b9-4f9d-92fe-116cb4060fc6_1226x1018.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17ff62e6-d5b9-4f9d-92fe-116cb4060fc6_1226x1018.png)

(from [3])

To create this dataset, we begin by sampling 5K images from [MS-COCO](https://cocodataset.org/#home) and [MMMU](https://mmmu-benchmark.github.io/). Then, we follow a procedure that is nearly identical to the generation strategy used for the Feedback Collection: _i)_ manually craft 50 seed rubrics, _ii)_ use GPT-4 to synthetically generate 15K rubrics, _iii)_ generate 30K instructions (and associated reference answers) for these rubrics, and _iv)_ augment each instruction with five responses—_one for each score_—and associated feedback[15](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-15-151216391). Compared to the Feedback Collection, the Perception Collection has a larger number of rubrics (i.e., 15K vs. 1K) but fewer instructions per rubric (two vs. 20); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e4fe921-174d-4d54-8434-406afab47195_918x590.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e4fe921-174d-4d54-8434-406afab47195_918x590.png)

(from [3])

**Training the model.** To train Prometheus-Vision, we cannot use a text-only base model—_we must begin with an open (pretrained) VLM_. In [3], authors use the 7B and 13B parameter variants of [LLaVA-1.5](https://llava-vl.github.io/) as base models. Similarly to prior Prometheus models, Prometheus-Vision is trained using a chain of thought finetuning strategy. We teach the model—_using an SFT approach_—to sequentially generate feedback then a score. All training examples explicitly separate the feedback and score with a fixed phrase: `‘So the overall score is’`. By providing reference materials (e.g., rubrics and reference answers) as input, we create a model that excels in performing customized, fine-grained evaluation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dd427ec-aa92-436f-91ce-f35e66294a7c_1034x652.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dd427ec-aa92-436f-91ce-f35e66294a7c_1034x652.png)

(from [3])

**Empirical results.** Prometheus-Vision outperforms both GPT-3.5-Turbo and the text-only Prometheus model in terms of correlation with human scores. However, both GPT-4 and GPT-4V achieve a higher level of agreement with human evaluation. The biggest dip in performance for Prometheus-Vision is observed on [VisIT Bench](https://visit-bench.github.io/), which contains a lot of text-heavy images; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faca316d3-b496-4eaa-b773-38c3842ff51a_1791x1008.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faca316d3-b496-4eaa-b773-38c3842ff51a_1791x1008.png)

Data examples from VisIT Bench ([source](https://visit-bench.github.io/))

On this data, Prometheus-Vision performs poorly, while GPT-4 performs well (better than GPT-4V!) because it ingests text extracted from the image as input. On other datasets, _Prometheus-Vision is comparable in scoring quality to GPT-4 and GPT-4V and even exceeds their correlation with human scores in several cases_.

> _“Previous works have highlighted a phenomenon known as length bias, which refers to a tendency of evaluator models to prefer longer responses… Self-enhancement bias is … where evaluators tend to prefer their own responses.”_ - from [3]

When we ask humans to rate provided feedback, we see that Prometheus-Vision largely matches the quality of feedback produced by GPT-4 and GPT-4V. Authors in [3] also test Prometheus-Vision for various [kinds of biases](https://cameronrwolfe.substack.com/i/141159804/biases-and-how-we-can-avoid-them), such as length bias or self-enhancement bias. The model was not found to exhibit obvious sources of biases, though measuring such biases in isolation is quite difficult.

#### Other Types of Finetuned Judges

Beyond the Prometheus models, there have been a wide variety of recent attempts at finetuning LLMs for evaluation purposes. We provide a brief reference of work in this area below, each accompanied with a description of their contribution.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb5371b7-d085-4582-8714-55bbc4e4235b_1336x584.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb5371b7-d085-4582-8714-55bbc4e4235b_1336x584.png)

(from [13])

**Self-rewarding LLMs [13]** attempt to improve and automate the alignment process with the help of an LLM evaluator. Instead of training reward models from human preferences, authors in this work train the LLM itself—_the same model that is being finetuned or aligned_—to provide its own rewards and feedback via LLM-as-a-Judge style prompts; see above. When used to finetune a LLaMA-2 model, this approach is shown to be capable of producing useful feedback and improving the performance of the underlying LLM. Similarly to Prometheus, Self-Rewarding LLMs are trained to act as an LLM judge, but the feedback from this judge is directly used as a reward signal for finetuning the LLM itself.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088b262d-9528-47e9-9e52-fcb154d307b8_1078x530.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F088b262d-9528-47e9-9e52-fcb154d307b8_1078x530.png)

(from [15])

**LLM-as-a-Meta-Judge.** Self-rewarding LLMs are an interesting approach, but one shortcoming of this technique is that it does not include any improvement mechanism for the LLM judge. In [15], authors expand upon this idea via an LLM-as-a-Meta-Judge technique that explicitly allows the LLM judge—_in addition to the LLM itself_—to self-improve throughout the training process by creating finetuning data for both the LLM and the LLM judge; see above.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6beea16d-be61-4a8b-91a1-1697ea922f79_1252x430.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6beea16d-be61-4a8b-91a1-1697ea922f79_1252x430.png)

(from [14])

**Self-taught evaluators [14]** train model-based evaluators without any human preference data. Such an approach is beneficial because human preference judgements are expensive to collect and can become stale over time. As an alternative, authors in [14] propose an iterative scheme that uses an LLM to generate synthetic evaluation data. Starting with a set of instructions, we just generate contrasting model outputs with an LLM and train an LLM judge to evaluate these outputs by producing explanations and scores. Without using any human-labeled data, this self-teaching approach is shown to produce evaluators that outperform both proprietary LLM judges (e.g., GPT-4) and reward models finetuned on human preferences in terms of their scoring accuracy.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20485756-43d5-4a10-97be-68b2279adbf4_1278x598.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20485756-43d5-4a10-97be-68b2279adbf4_1278x598.png)

(from [16])

**Foundational Large Autorater Models (FLAMe) [16]** are a family of foundational evaluation / judge models that are trained over a massive amount of human preference data—_over 5M human judgements spanning more than 100 different quality assessment tasks_. Compared to other LLM evaluators, FLAMe models generalize well to other evaluation tasks, are shown to be less biased, and can even be further finetuned. FLAMe models can be great base models for finetuning specialized LLM judges. These models consistently outperform proprietary LLM-as-a-Judge models based on GPT-4 and Claude variants, as well as LLM judges that have been finetuned on synthetically-generated data (e.g., from GPT-4).

> _“Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, and scalability.”_ - from [18]

**Finetuned versus proprietary LLM judges.** The tradeoff between finetuned and proprietary LLM judges is deeply analyzed in [18]. From this analysis, we see that finetuned judges work very well for in-domain evaluation—_the models are apt at evaluating data that is similar to the data on which they are trained_. Compared to proprietary LLM judges, however, these models do not generalize as well to new or different tasks. Put simply, _finetuned LLM judges tend to be very task-specific_, which is not necessarily a surprising result.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8503781e-ebb7-48f9-84c9-c645fe5d3731_1338x634.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8503781e-ebb7-48f9-84c9-c645fe5d3731_1338x634.png)

(from [19])

**Direct judgement preference optimization [19]** attempts to create LLMs with more advanced evaluation capabilities by using [preference optimization](https://arxiv.org/abs/2305.18290). To do this, authors collect preference pairs for three different evaluation use cases: _i)_ single rating, _ii)_ pairwise comparison, and _iii)_ classification. These first two uses cases match the common LLM-as-a-Judge scoring setups. The classification use case formulates evaluation as a binary classification problem; e.g., “Is this property satisfied in the response or not?”. Beyond these use cases, an auxiliary task is also introduced that trains the LLM to deduce the response being scored given the instruction and evaluation result as input; see above. For each of these use cases, we obtain preference pairs of positive and negative responses, then update the evaluator using direct preference optimization (DPO) [20].

> _“We show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.”_ - from [21]

**How do LLM judges assign their scores?** When using an LLM judge, we might wonder if the LLM generates scores based on the evaluation criteria or just assigning higher scores to text that is highly aligned with its training data. In [21], authors aim to answer this question via a rigorous analysis of LLM judges under several scoring settings; see below. We see from this analysis that LLM-as-a-Judge models benefit only slightly from the inclusion of detailed scoring instructions and criteria in their prompts. In fact, perplexity-based evaluation—_scoring a response based on the likelihood assigned to it by the model_—aligns better with human preferences in some cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6994220d-f90d-462a-805a-05233c0a3909_1322x712.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6994220d-f90d-462a-805a-05233c0a3909_1322x712.png)

(from [21])

Such a result indicates that the scoring mechanism of LLM judges is driven by the contents of their training data, which slightly contradicts findings from Prometheus [1] that reference materials are very important. However, the analysis in [21] is mostly focused on evaluating response quality from a human preference perspective. Given that most LLMs are trained on preference data, it makes sense that the model assigns high probability to responses that are preferable to humans. In contrast, Prometheus considers a wide range of evaluation criteria beyond human preferences making evaluation criteria are more relevant.

## Step-by-Step Guide: Finetuning an LLM Judge

Now that we have taken a deep look at nearly all of the research recently conducted on the topic of finetuning LLMs for the purpose of evaluation, we need a generic framework for how to create our own LLM judge. Luckily, Prometheus already provides such a framework [1]. Below, we present (and slightly modify) this framework as a summary of what we have learned so far in this overview.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fe09c6-3ac5-475b-b787-902524cb3fba_1820x1026.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5fe09c6-3ac5-475b-b787-902524cb3fba_1820x1026.png)

Example of evaluation criteria from Prometheus’ Feedback Collection

**(1) Solidify the evaluation criteria.** The first step of evaluation is deciding what exactly we want to evaluate. In particular, we should:

- Outline a specific set of criteria that we care about.
    
- Write a detailed description for each of these criteria.
    

An example of a [scoring rubric](https://huggingface.co/datasets/prometheus-eval/Feedback-Collection) used by Prometheus is provided above, which includes both scoring criteria / guidelines and multiple references answers. Over time, we must evolve, refine, and expand our criteria to make the human (and model-based) evaluation process more consistent and accurate.

**(2) Prepare a dataset.** Before we train or use any LLMs for evaluation, we need (high-quality) human evaluation data. Human-labeled data helps us to easily determine if our evaluator is accurate or not. This data should reflect the kind of data that the LLM judge will encounter in the wild. If we are just using this data for meta-evaluation purposes (e.g., to measure the performance of LLM-as-a-Judge), then we don’t need much data (e.g., 100-1K examples). Finetuning a Prometheus-style model will require more data (e.g., 1K-100K examples).

Human data collection is a difficult—_but essential_—part of the evaluation process. If humans cannot consistently agree on our evaluation task, then it will be impossible to measure whether an LLM is accurately evaluating the task. _Not investing_ _sufficiently into human evaluation is a common mistake_. But, developing or finetuning an LLM judge on low quality data is a waste of both time and energy.

**(2.5) Use synthetic data.** The Prometheus models [1, 2, 3] show us that we can use synthetic data to train high-quality LLM judges. In fact, nearly all of the data used to train these models is synthetically generated—_most of the instructions, the responses, the scores, and the feedback_! Using purely synthetic training data can introduce bias by exposing the model to only a narrow distribution of data during training, but combining human and synthetic data can be incredibly effective.

**(3) Focus on the rationales!** If we want to finetune an accurate LLM judge, we obviously want the scores and rankings over which the model is trained to be accurate. Going further, we should be sure to create high-quality rationales for each score. LLMs [struggle to learn new knowledge](https://arxiv.org/abs/2305.11206) during finetuning. However, finetuning CAN be used to teach our LLM judge more effective formats or styles of feedback. As such, tweaking the rationales over which the LLM judge is trained is a great way to make the resulting model more helpful.

**(4) Use reference answers.** Although this step is optional, we can also prepare reference answers to go along with each example in our dataset. As outlined previously, reference answers can improve the performance of both LLM-as-a-Judge and finetuned evaluators. For finetuned evaluators, references are useful due to their ability to address the knowledge bias created by gaps or issues in the pretrained knowledge of the judge’s base model. Reference answers are also useful for improving the accuracy and reliability of pointwise scoring.

**(5) Train the model.** Once all of our data (and optionally reference answers) have been collected, then we can train our LLM judge using a basic SFT approach. For a full implementation of this process, check out the [Prometheus GitHub repo](https://github.com/prometheus-eval/prometheus-eval). During training, we should create a hold-out test dataset[16](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-16-151216391) that can be used to meta-evaluate the model’s performance. Performance is evaluated using either classification (for binary output) or correlation (for everything else) metrics.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Kim, Seungone, et al. "Prometheus: Inducing Evaluation Capability in Language Models." _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_. 2023.

[2] Kim, Seungone, et al. "Prometheus 2: An open source language model specialized in evaluating other language models." _arXiv preprint arXiv:2405.01535_ (2024).

[3] Lee, Seongyun, et al. "Prometheusvision: Vision-language model as a judge for fine-grained evaluation." _arXiv preprint arXiv:2401.06591_ (2024).

[4] Chen, Dongping, et al. "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark." _arXiv preprint arXiv:2402.04788_ (2024).

[5] Bai, Shuai, et al. "Touchstone: Evaluating vision-language models by language models." _arXiv preprint arXiv:2308.16890_ (2023).

[6] Wang, Yidong, et al. "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.(2024)." _URL https://arxiv. org/abs/2306.05087_ 3.4 (2024).

[7] Li, Junlong, et al. "Generative judge for evaluating alignment." _arXiv preprint arXiv:2310.05470_ (2023).

[8] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." _Advances in Neural Information Processing Systems_ 36 (2024).

[9] Zhu, Lianghui, Xinggang Wang, and Xinlong Wang. "Judgelm: Fine-tuned large language models are scalable judges." _arXiv preprint arXiv:2310.17631_ (2023).

[10] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023. URL [https://kaistai.github.io/SelFee/](https://kaistai.github.io/SelFee/).

[11] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[12] Zhang, Lunjun, et al. "Generative verifiers: Reward modeling as next-token prediction." _arXiv preprint arXiv:2408.15240_ (2024).

[13] Yuan, Weizhe, et al. "Self-rewarding language models." _arXiv preprint arXiv:2401.10020_ (2024).

[14] Wang, Tianlu, et al. "Self-taught evaluators." _arXiv preprint arXiv:2408.02666_ (2024).

[15] Wu, Tianhao, et al. "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge." _arXiv preprint arXiv:2407.19594_ (2024).

[16] Vu, Tu, et al. "Foundational autoraters: Taming large language models for better automatic evaluation." _arXiv preprint arXiv:2407.10817_ (2024).

[17] Ankner, Zachary, et al. "Critique-out-loud reward models." _arXiv preprint arXiv:2408.11791_ (2024).

[18] Huang, Hui, et al. "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers." _arXiv preprint arXiv:2403.02839_ (2024).

[19] Wang, Peifeng, et al. "Direct judgement preference optimization." _arXiv preprint arXiv:2409.14664_ (2024).

[20] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." _Advances in Neural Information Processing Systems_ 36 (2024).

[21] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." arXiv preprint arXiv:1909.01214 (2019).

[22] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[23] Jha, Aditi, et al. "Limit: Less is more for instruction tuning across evaluation paradigms." _arXiv preprint arXiv:2311.13133_ (2023).

[1](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-1-151216391)

For example, OpenAI is known to have heavily relied upon human annotation workforces from ScaleAI to train and evaluate their models [22].

[2](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-2-151216391)

We can also created shared (asynchronous) communication channels between annotators and researchers or something else. We just need some medium to facilitate discussion between relevant parties; see Appendix B of [22] for details

[3](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-3-151216391)

See [here](https://eugeneyan.com/writing/labeling-guidelines/) for a great overview of how to write guidelines for data annotation.

[4](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-4-151216391)

Many more traditional, reference-based metrics exist beyond these two as well; e.g., [BERTScore](https://arxiv.org/abs/1904.09675), [MoverScore](https://arxiv.org/abs/1909.02622), [METEOR](https://aclanthology.org/W05-0909/), [COMET](https://arxiv.org/abs/2009.09025), and more.

[5](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-5-151216391)

N-grams are just sequential groups of words; e.g., unigrams (single words), bigrams (two-word sequences), trigrams (three-word sequences), and more.

[6](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-6-151216391)

In some papers, authors compute correlation metrics between their finetuned judge and a proprietary LLM (e.g., GPT-4) instead of a human.

[7](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-7-151216391)

The acronym for PandaLM comes from re**P**roducible **AND** **A**utomated **L**anguage **M**odel Assessment.

[8](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-8-151216391)

Notably, PandaLM is finetuned purely using supervised finetuning (SFT). There is no specialized classification head created within the model for pairwise evaluation.

[9](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-9-151216391)

By “granular”, we mean that the dataset spans a wide variety of different evaluation criteria that consider specific aspects of model performance.

[10](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-10-151216391)

Although the queries used for training are provided by human users, the responses to these queries are still generated with GPT-4. So, the training data for Auto-J is still synthetically created.

[11](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-11-151216391)

These explanations mimic the rationales used within [chain of thought prompting](https://arxiv.org/abs/2201.11903). Parallel research has shown that finetuning LLMs over such rationales is an effective approach; see [here](https://arxiv.org/abs/2305.14045).

[12](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-12-151216391)

In other words, we want to ensure that the model always generates a score after the feedback, instead of continuing to generate endless feedback or starting to artificially generate the next instruction to be scored.

[13](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-13-151216391)

Examples include the [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) (from Anthropic) and [UltraFeedback](https://arxiv.org/abs/2310.01377) datasets.

[14](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-14-151216391)

This can be done via some variant of [multi-task learning](https://www.ruder.io/multi-task/), [instruction tuning](https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/), or other similar methodologies.

[15](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-15-151216391)

These outputs are sampled from several different VLMs, including [Fuyu-8B](https://www.adept.ai/blog/fuyu-8b), LLaVA-1.5 (13B), and GPT-4V.

[16](https://cameronrwolfe.substack.com/p/finetuned-judge#footnote-anchor-16-151216391)

We should also create an additional validation dataset to use for tuning the training hyperparameters of our LLM judge.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Avgoustinos Filippoupolitis's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4daabf1d-fefa-46eb-86b6-d502321948ff_96x96.jpeg)



](https://substack.com/profile/141899702-avgoustinos-filippoupolitis)

[

![Zack's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7ac2fd3-302e-45b8-ac8a-412b7dd9cc6f_524x524.jpeg)



](https://substack.com/profile/88896015-zack)

[

![B.V. CHANDRAHAAS's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F631a4c27-a9b1-4734-bdc8-bff91101db8e_96x96.png)



](https://substack.com/profile/265340444-bv-chandrahaas)

[

![Fabio's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16082203-d2de-4d99-b41f-01fb4f3e6b9b_800x1200.jpeg)



](https://substack.com/profile/30571260-fabio)

[

![Ian's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61540ff7-dd69-4e6e-ab7c-53141fe22758_892x896.jpeg)



](https://substack.com/profile/153392378-ian)

77 Likes∙

[9 Restacks](https://substack.com/note/p-151216391/restacks?utm_source=substack&utm_content=facepile-restacks)

77

- 

[

4

](https://cameronrwolfe.substack.com/p/finetuned-judge/comments)

9

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![DATUMO's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4060e2dd-287c-4f88-8b41-69d8000bdfb4_144x144.png)



](https://substack.com/profile/288484020-datumo?utm_source=comment)

[DATUMO](https://substack.com/profile/288484020-datumo?utm_source=substack-feed-item)

[DATUMO’s Substack](https://datumo.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[12月6日](https://cameronrwolfe.substack.com/p/finetuned-judge/comment/80290700 "2024年12月6日 13:34")

Liked by Cameron R. Wolfe, Ph.D.

Really enjoyed this take on fine-tuned judges and how they can improve AI evaluation!

It’s so cool to see new ideas for going beyond standard metrics to really understand model performance.

At DATUMO, we’ve been exploring ways to make LLM evaluations more reliable too—it’s such an exciting space to be in. Looking forward to more posts like this!

<a href="[https://datumo.com](https://datumo.com/)" target="_blank">DATUMO</a>

Like (2)

Reply

Share

[

![Ryan Callihan's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28f8fd01-1bad-4d39-b8b8-ebc29a6c1b45_758x892.jpeg)



](https://substack.com/profile/107478147-ryan-callihan?utm_source=comment)

[Ryan Callihan](https://substack.com/profile/107478147-ryan-callihan?utm_source=substack-feed-item)

[12月5日](https://cameronrwolfe.substack.com/p/finetuned-judge/comment/80147814 "2024年12月5日 16:59")

Liked by Cameron R. Wolfe, Ph.D.

This is probably the best overview of LLMs as Judges I have seen.

It made me realise that I haven’t seen a lot of new evaluation metrics / models / strategies since Prometheus.

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/finetuned-judge/comment/80147814)

[2 more comments...](https://cameronrwolfe.substack.com/p/finetuned-judge/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Scaling Laws for LLMs: From GPT-3 to o3

### Understanding the current state of LLM scaling and the future of AI research...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jan 06, 2025

122

- 

[

8

](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comments)

14

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98592f1d-1d20-4c88-a681-9d4dac0289d4_2014x1130.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98592f1d-1d20-4c88-a681-9d4dac0289d4_2014x1130.png)

(from [1, 7, 10, 21])

A majority of recent advancements in AI research—_and large language models (LLMs) in particular_—have been driven by scale. If we train larger models over more data, we get better results. This relationship can be defined more rigorously via a scaling law, which is just an equation that describes how an LLM’s test loss will decrease as we increase some quantity of interest (e.g., training compute). Scaling laws help us to predict the results of larger and more expensive training runs, giving us the necessary confidence to continue investing in scale.

> _“If you have a large dataset and you train a very big neural network, then success is guaranteed!”_ - Ilya Sutskever

For years, scaling laws have been a predictable North Star for AI research. In fact, the success of early frontier labs like OpenAI has even been credited to their [religious level of belief](https://www.youtube.com/watch?v=xXCBz_8hM9w) in scaling laws. However, the continuation of scaling has recently been called into question by reports[1](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-1-152758713) claiming that top research labs are struggling to create the next generation of better LLMs. These claims might lead us to wonder: _Will scaling hit a wall and, if so, are there other paths forward?_

This overview will answer these questions from the ground up, beginning with an in-depth explanation of LLM scaling laws and the surrounding research. The idea of a scaling law is simple, but there are a variety of public misconceptions around scaling—_the science behind this research is actually very specific_. Using this detailed understanding of scaling, we will then discuss recent trends in LLM research and contributing factors to the “plateau” of scaling laws. Finally, we will use this information to more clearly illustrate the future of AI research, focusing on a few key ideas—_including scaling_—that could continue to drive progress.

## Fundamental Scaling Concepts for LLMs

To understand the state of scaling for LLMs, we first need to build a general understanding of scaling laws. We will build this understanding from the ground up, starting with the concept of a power law. Then, we will explore how power laws have been applied in LLM research to derive the scaling laws we use today.

#### What is a power law?

[Power laws](https://en.wikipedia.org/wiki/Power_law) are the fundamental concept that underlie LLM scaling. Put simply, power laws just describe a relationship between two quantities. For LLMs, the first of these quantities is the LLM’s test loss—_or some other related performance metric (e.g., downstream task accuracy [7])_—and the other is some setting that we are trying to scale, such as the number of model parameters. For example, we may see a statement like the following when studying the scaling properties of an LLM.

> _“With enough training data, scaling of validation loss should be approximately a smooth power law as a function of model size.”_ - from [4]

Such a statement tells us that a measurable relationship exists between the model’s test loss and the total number of model parameters. A change to one of these quantities will produce a relative, [scale-invariant](http://felix.physics.sunysb.edu/~allen/540-05/scaling.html#:~:text=scale%20invariance%20of%20power%20law%20functions&text=The%20function%20y%3Dxp,by%20a%20scale%20factor%20a.) change in the other. In other words, we know from this relationship that increasing the total number of model parameters—_assuming other conditions (e.g., having sufficient training data) are met_—will result in a decrease of the test loss by predictable factor.

**Power law formulation.** A basic power law is expressed via the equation below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e231b5-3002-4bec-a733-512e3f6730a5_240x68.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e231b5-3002-4bec-a733-512e3f6730a5_240x68.png)

The two quantities being studied here are `x` and `y`, while `a` and `p` are constants that describe the relationship between these quantities. If we plot this power law function[2](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-2-152758713), we get the figure shown below. We provide plots in both normal and log scale because most papers that study LLM scaling use a log scale.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)

Plot of a basic power law between `x` and `y`

However, the plots provided for LLM scaling do not look like the plot shown above—_they are usually flipped upside down_; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82fbae3c-e5d1-4936-8328-057eba9893a3_806x524.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82fbae3c-e5d1-4936-8328-057eba9893a3_806x524.png)

(from [1])

This is just an _inverse_ power law, which can be formulated as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19681045-c6e7-41dc-aee7-8fb87ec02b5f_376x178.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19681045-c6e7-41dc-aee7-8fb87ec02b5f_376x178.png)

The equation for an inverse power law is nearly identically to that of a standard power law, but we use a negative exponent for `p`. Making the exponent of the power law negative flips the plots upside down; see below for an example.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)

Plot of an inverse power law between `x` and `y`

This inverse power law, when plotted using a log scale, yields the signature linear relationship that is characteristic of most LLM scaling laws. Nearly every paper covered in this overview will produce such a plot to study how scaling up a variety of different factors (e.g., size, compute, data, etc.) impacts an LLM’s performance. Now, let’s take a more practical look at power laws by learning about one of the first papers to study them in the context of LLM scaling [1].

#### **[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) [1]**

In the early days of language models, we did not yet understand the impact of scale on performance. Language models were a promising area of research, but the current generation of models (e.g., the [original GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)) at that time had limited capabilities. We had yet to discover the power of larger models, and the path towards creating better language models was not immediately clear. _Is the shape of the model (i.e., the number and size of layers) important? Does making a model larger help it to perform better? How much data is needed to train these larger models?_

> _“The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude.”_ - from [1]

In [1], authors aim to answer these questions by analyzing the impact of several factors—_such as model size, model shape, dataset size, training compute, and batch size_—on model performance. From this analysis, we learn that LLM performance improves smoothly as we increase:

1. The number of model parameters.
    
2. The size of the dataset.
    
3. The amount of compute used for training.
    

More specifically, _a power law relationship is observed between each of these factors and the LLM’s test loss when performance is not bottlenecked by the other two factors._

**Experimental setup.** To fit their power laws, authors pretrain LLMs with sizes up to 1.5B parameters over subsets of the [WebText2 corpus](https://github.com/EleutherAI/openwebtext2), containing anywhere from 22M to 23B tokens. All models are trained using a fixed context length of 1,024 tokens and a standard [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) ([cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) loss. The same loss is measured over a hold-out test set and used as our primary performance metric. _This setup matches the standard pretraining setup for most LLMs_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F071a53ae-e1d5-4af6-bcd5-4402cc27e924_2144x1244.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F071a53ae-e1d5-4af6-bcd5-4402cc27e924_2144x1244.png)

(from [1])

**Power laws for LLM scaling.** The performance of LLMs trained in [1]—_in terms of their test loss on WebText2_—is shown to steadily improve with more parameters, data, and compute[3](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-3-152758713). These trends span eight orders of magnitude in compute, six orders of magnitude in model size, and two orders of magnitude in dataset size. The exact power law relationship and the equations that are fit to each of them are provided in the figure above. Each of the equations here are very similar to the inverse power law equation that we saw before. However, we set `a = 1` and add an additional multiplicative constant inside of the parenthesis[4](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-4-152758713).

Authors in [1] note one small detail that is necessary to properly fit these power laws. We do not include [positional or token embeddings](https://cameronrwolfe.substack.com/i/142044446/constructing-the-models-input) when counting the total number of model parameters, which yields cleaner scaling trends; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe90d52a4-413f-4140-977d-3e0c52497dae_1968x808.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe90d52a4-413f-4140-977d-3e0c52497dae_1968x808.png)

(from [1])

These power laws are only applicable if training is not bottlenecked by the other factors. So, all three of these components—_model size, data, and compute_—should be scaled up simultaneously for optimal performance. If we scale up any of these components in isolation, we will reach a point of diminishing returns.

**What do power laws tell us?** Although the power law plots provided in [1] look quite promising, we should notice that these plots are generated using a log scale. If we generate normal plots (i.e., without log scale), we get the figures below, where we see that the shape of the power law resembles an exponential decay.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabf07f96-f9d9-467f-a459-fb24788f4e33_2540x630.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabf07f96-f9d9-467f-a459-fb24788f4e33_2540x630.png)

Power law plots without log scale

Such a finding may seem counterintuitive given a lot of the online rhetoric around scaling and AGI. In many cases, it seems like the intuition we are being fed is that the quality LLMs improves exponentially with logarithmic increases in compute, but this is not the case whatsoever. In reality, _increasing the quality of an LLM becomes exponentially more difficult with scale_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9603be2-d7fd-4f59-bda2-1e5f76f17f8d_1254x502.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9603be2-d7fd-4f59-bda2-1e5f76f17f8d_1254x502.png)

(from [1])

**Other useful findings.** Beyond the power laws observed in [1], we also see that the other factors that are considered, such as model shape or architecture settings, have minimal influence on model performance; see above. Scale is by far the largest contributing factor for creating better LLMs—_more data, compute, and model parameters yields a smooth improvement in the LLM’s performance_.

> _“Larger models are significantly more sampleefficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.”_ - from [1]

Interestingly, the empirical analysis in [1] indicates that larger LLMs tend to be more sample efficient, meaning that they reach the same level of test loss with less data relative to smaller models. For this reason, _pretraining an LLM to convergence is (arguably) sub-optimal_. Instead, we could train a much larger model on less data, stopping the training process far before convergence. Such an approach is optimal in terms of the amount of training compute used but it does not account for inference costs. Practically speaking, we usually train smaller models over more data because smaller models are cheaper to host.

Authors also extensively analyze the relationship between model size and the amount of data used for pretraining, finding that the size of the dataset does not need to be increased as quickly as the model size. _An ~8X increase in model size requires a ~5X increase in the amount of training data to avoid overfitting_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb29aeb01-107f-4b6f-b462-7477f6bd863b_1256x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb29aeb01-107f-4b6f-b462-7477f6bd863b_1256x576.png)

(from [1])

The scaling laws discovered in [1] are also replicated on several other datasets, where we see that the same scaling laws hold after adding a fixed offset to the test loss (i.e., to account for the fact that the dataset is different); see above. These results make a compelling case for LLM scaling. We achieve very clear and measurable benefits from training larger models longer and on more data, which created an appetite for pretraining LLMs at a much larger scale.

> _“These results show that language modeling performance improves smoothly and predictably as we appropriately scale up model size, data, and compute. We expect that larger language models will perform better and be more sample efficient than current models.”_ - from [1]

#### Practical Usage of Scaling Laws

The fact that large-scale pretraining is so beneficial presents us with a slight dilemma. The best results are achieved by training massive models over copious amounts of data. However, these training runs are [incredibly expensive](https://techcrunch.com/2024/03/27/databricks-spent-10m-on-a-generative-ai-model-that-still-cant-beat-gpt-4/), _which means they also incur a lot of risk_. What if we spend $10 million training a model that fails to meet our expectations? Given the expense of pretraining, we cannot perform any model-specific tuning, and we must be sure that the model we train will perform well. We need to develop a strategy for tuning these models and forecasting their performance without spending too much money.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bae506a-dddf-440b-be73-87bafe6d7d0f_1884x1032.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bae506a-dddf-440b-be73-87bafe6d7d0f_1884x1032.png)

(from [11])

This is where scaling laws come in. So far, we have seen some empirical analysis that was conducted to prove that scaling laws exist, _but these scaling laws also have a very practical use case within AI research_. In particular, we can:

- Train a bunch of smaller models using various training settings.
    
- Fit scaling laws based on the performance of smaller models.
    
- Use the scaling law to extrapolate the performance of a much larger model.
    

Of course, this approach has limitations. Predicting the performance of larger models from smaller models is difficult and can be inaccurate. Models may behave differently depending on scale. However, [various approaches](https://arxiv.org/abs/2203.03466) have been proposed to make this more feasible, and scaling laws are now commonly used for this purpose. The ability to forecast the performance of larger models using scaling laws gives us more confidence (and peace of mind) as researchers. Plus, scaling laws provide a simple way to justify investment into AI research.

## Scaling and The Age of Pretraining

> _“This is what has been the driver of all progress we see today—extraordinarily large neural networks trained on huge datasets.”_ - Ilya Sutskever

The discovery of scaling laws catalyzed much of the recent progress in LLM research. To get better results, we just train increasingly large models on bigger (and [better](https://cameronrwolfe.substack.com/p/data-is-the-foundation-of-language)!) datasets. This strategy was followed to create several models within the GPT lineage, as well as most of the notable models from groups other than OpenAI. Here, we will take a deeper look at this progression of scaling research—_recently described by Ilya Sutskever as the “age of pretraining”_[5](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-5-152758713).

#### **The GPT Lineage: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) [2], [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [3], [GPT-3](https://arxiv.org/abs/2005.14165) [4], and [GPT-4](https://arxiv.org/abs/2303.08774) [5]**

The most widely known and visible application of LLM scaling laws was in the creation of OpenAI’s GPT lineage of models. We will primarily focus open earlier models in this lineage—_up to GPT-3_—because:

1. The details of these models were shared more openly.
    
2. Later models heavily benefitted from advancements in [post-training research](https://www.interconnects.ai/p/frontier-model-post-training) in addition to scaling up the pretraining process.
    

We will also cover some of the known scaling results from models like GPT-4.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216100ce-7a2b-4313-b047-25451e29620f_1872x936.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216100ce-7a2b-4313-b047-25451e29620f_1872x936.png)

(from [2])

**The original GPT model** [2] was actually quite small—_12 layers and 117M parameters in total_. This model is first pretrained over [BooksCorpus](https://paperswithcode.com/dataset/bookcorpus), a dataset that contains the raw text of ~7,000 books. Then, we finetune the model to solve a variety of different downstream tasks by using a supervised training objective and creating a separate classification head for each task; see above. This paper was one of the first to perform large-scale, [self-supervised pretraining](https://cameronrwolfe.substack.com/p/language-model-training-and-inference) of a [decoder-only transformer](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse), which led to a few interesting findings:

- Self-supervised pretraining on flat text is incredibly effective.
    
- Using long, contiguous spans of text for pretraining is important.
    
- When pretrained in this manner, a single model can be finetuned to solve a wide variety of different tasks with state-of-the-art accuracy[6](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-6-152758713).
    

Overall, GPT was not an especially noteworthy model, but it laid some important foundations (i.e., decoder-only transformer and self-supervised pretraining) for later work that explored similar models at a much larger scale.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb97d85f1-23c5-4db8-b087-c738f11cfcb1_1032x400.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb97d85f1-23c5-4db8-b087-c738f11cfcb1_1032x400.png)

(from [3])

**GPT-2** [3] was proposed shortly after GPT and includes a collection of several models with sizes up to 1.5B parameters; see above. These models share the same architecture as the GPT model and are pretrained using the same self-supervised language modeling objective. However, GPT-2 makes two big changes to the pretraining process compared to GPT:

- The models are pretrained on [WebText](https://paperswithcode.com/dataset/webtext), which is _i)_ much larger than BooksCorpus and _ii)_ created by scraping data from the internet.
    
- The models are not finetuned on downstream tasks. Instead, we solve tasks by performing [zero-shot inference](https://cameronrwolfe.substack.com/i/117151147/zero-shot-learning)[7](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-7-152758713) with the pretrained model.
    

GPT-2 models fall short of state-of-the-art performance on most benchmarks[8](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-8-152758713), but their performance consistently improves with the size of the model—_scaling up the number of model parameters yields a clear benefit_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a242e94-54ba-44e1-9f99-663a8330a67d_1966x800.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a242e94-54ba-44e1-9f99-663a8330a67d_1966x800.png)

(from [3])

Authors in [3] also reveal that GPT-2 models—_despite their impressive results_—still seem to underfit the WebText corpus. From this finding, we can infer that continued scaling of LLM pretraining—_in terms of both model and data size_—should be beneficial. Although the GPT-2 models were not particularly powerful, the analysis presented by these models provided the confidence we needed to continue scaling and eventually reach an inflection point in AI research.

> _“A language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement.”_ - from [3]

**GPT-3** [4] was a watershed moment in AI research that definitively confirmed the benefit of large-scale pretraining for LLMs. The model has over 175B parameters, making it over 100x larger than the largest GPT-2 model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d361dbb-7fba-48cf-8e69-d2f043d6732d_1898x520.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d361dbb-7fba-48cf-8e69-d2f043d6732d_1898x520.png)

(from [4])

Again, GPT-3 uses a decoder-only model architecture that is quite similar to that of prior models, but we pretrain the model over a much larger dataset based upon [CommonCrawl](https://commoncrawl.org/). This dataset is ~10x bigger than the prior WebText dataset, and authors in [4] combine the larger pretraining dataset with several other sources of pretraining data, creating a mixture of different corpora; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e2ebf77-22bd-41e5-a42f-cc815c28f130_2368x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e2ebf77-22bd-41e5-a42f-cc815c28f130_2368x636.png)

(from [4])

GPT-3 is primarily evaluated in [4] by using a [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) approach. The differences between few-shot prompting (used by GPT-3), zero-shot prompting (used by GPT-2), and finetuning (used by GPT) are illustrated below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf3c018f-b4fe-41fc-aefe-a2c323fa74f2_1524x1350.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf3c018f-b4fe-41fc-aefe-a2c323fa74f2_1524x1350.png)

(from [4])

Few-shot learning is a new paradigm in which the LLM learns how to perform a task based upon examples that are placed within its context window. Authors in [4] refer to this concept as “in-context learning”. In this case, the LLM does not actually “learn”—_the model’s weights are not updated at all_. Rather, the examples in the model’s input are used as context for generating a more accurate output. We see in [4] that GPT-3 is a highly capable few-shot learner, seeming to indicate that in-context learning is an emergent[9](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-9-152758713) ability of larger models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cc525df-2702-4ca8-98ff-81d7b70bf56e_1902x1104.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cc525df-2702-4ca8-98ff-81d7b70bf56e_1902x1104.png)

(from [4])

When GPT-3 is evaluated on various language understanding tasks, we see that using a larger model significantly benefits few-shot learning performance, as shown in the figures below. Larger models make better—_and more efficient_—use of the information in their context windows relative to smaller models. GPT-3 is able to surpass state-of-the-art performance on several tasks via few-shot learning, and the model’s performance improves smoothly with size.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73777717-0e01-44e8-9f16-c7e93be4896b_2818x818.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73777717-0e01-44e8-9f16-c7e93be4896b_2818x818.png)

(from [4])

The fact that a single model was able to perform this well across so many tasks was incredibly impressive at the time. Solving each of these tasks did not require any finetuning or changes to the underlying model—_we just had to tweak the model’s prompt_. GPT-3 was one of the first true [foundation models](https://blogs.nvidia.com/blog/what-are-foundation-models/) to be released. This model ushered the next era of AI research and introduced a completely new and intuitive paradigm for interacting with LLMs (i.e., prompting).

**Beyond GPT-3.** The impressive performance of GPT-3 created an explosion of interest in LLM research, primarily focused upon large-scale pretraining. The next few models released by OpenAI—_InstructGPT [8], [ChatGPT](https://openai.com/index/chatgpt/) and GPT-4 [5]_—used a combination of large-scale pretraining and new post-training techniques (i.e., [supervised finetuning](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)) to drastically improve LLM quality. These models were so impressive that they even caused the amount of public interest in AI research to skyrocket.

> _“GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior.”_ - from [5]

At this time, OpenAI began to publish fewer details on their research. Instead, new models were just released via [their API](https://platform.openai.com/docs/api-reference/introduction), which kept the public from learning how these models were created. Luckily, some useful information can be gleaned from the material that OpenAI did publish. For example, InstructGPT [8]—_the predecessor to ChatGPT_—has an associated paper that documents the model’s post training strategy in detail; see below. Given that this paper also states that GPT-3 is the base model for InstructGPT, we can reasonably infer that this model’s gains in performance are mostly unrelated to scaling up the pretraining process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff49c076a-ce2e-42b4-bffe-8b2a12912851_1860x998.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff49c076a-ce2e-42b4-bffe-8b2a12912851_1860x998.png)

(from [8])

Compared to ChatGPT, GPT-4 had a noticeable improvement in capabilities. However, researchers chose to share very few of GPT-4’s technical details. The technical report for GPT-4 [5] simply tells us that:

1. GPT-4 is transformer-based.
    
2. The model is pretrained using next token prediction.
    
3. Both public and licensed 3rd party data is used.
    
4. The model is finetuned with reinforcement learning from human feedback.
    

Nonetheless, the importance of scaling is very evident within this technical report. Authors note that a key challenge within this work was developing a scalable training architecture that behaves predictably across different scales, allowing the results of smaller runs to be extrapolated to provide confidence in the larger-scale (and significantly more expensive!) training exercises.

> _“The final loss of properly-trained large language models is … approximated by power laws in the amount of compute used to train the model.”_ - from [5]

Large-scale pretraining is incredibly expensive, so we usually only get one chance at getting it right—_there is no room for model-specific tuning_. Scaling laws play a key role in this process. We can train models using 1,000-10,000x less compute and use the results of these training runs to fit power laws. Then, these power laws can be used to predict the performance of much larger models. In particular, we see in [8] that the performance of GPT-4 is predicted using a power law that measures the relationship between compute and test loss; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F310c8a15-8acb-4bd6-871a-ebb657386a31_1394x710.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F310c8a15-8acb-4bd6-871a-ebb657386a31_1394x710.png)

Scaling law formulation for training GPT-4 (from [5])

This expression looks nearly identical to what we have seen before, but it has an added irreducible loss term to account for the fact that the LLM’s test loss might never reach zero. Once fitted, the scaling law was used to predict the final performance of GPT-4 with very high accuracy; see below for a depiction. Here, we should note that the plot is not generated using a log scale, and we see that the improvement in loss clearly begins to decay with increasing compute!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb862c34d-6730-41c5-8071-e79f26fbda5d_1096x772.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb862c34d-6730-41c5-8071-e79f26fbda5d_1096x772.png)

(from [5])

Authors in [5] also note that the test loss is not an easily interpretable metric and try to predict a variety of other performance metrics. For example, a scaling law is fit to predict the LLM’s pass rate on the [HumanEval](https://github.com/openai/human-eval) coding benchmark. First, the problems in HumanEval are split into buckets based on their difficulty. Then, a scaling law is fit to predict the LLM’s pass rate. We see in [5] that the pass rate of GPT-4 can be accurately predicted on HumanEval using this approach based on experiments that require 1,000x less compute; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95f0364e-8bdb-4144-8ff0-cb6c723af58e_1098x708.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95f0364e-8bdb-4144-8ff0-cb6c723af58e_1098x708.png)

(from [5])

As we can see, scaling up the pretraining process is valuable. However, large-scale pretraining is also very expensive. _Scaling laws make this process more predictable_, allowing us to avoid needless or excessive compute costs.

#### [Chinchilla:](https://arxiv.org/abs/2203.15556) **[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) [5]**

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79dd6964-bd10-4aa3-ae43-969c2f83a611_2440x1068.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79dd6964-bd10-4aa3-ae43-969c2f83a611_2440x1068.png)

(from [9])

In [1], authors recommend increasing model size faster than the size of the dataset when scaling up LLM pretraining. However, most of the pretraining research after GPT-3 indicated that we should do the opposite. We trained models that were significantly larger than GPT-3—_such as the 530B parameter MT-NLG [9] model_—but the size of the dataset used to train these models was similar to that of GPT-3; see above. These models did not improve upon the performance of GPT-3, whereas models that used a combination of more parameters with more data (e.g., Gopher [10]) performed much better; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d435bf-c109-44a1-8090-afe85175e184_1470x1200.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d435bf-c109-44a1-8090-afe85175e184_1470x1200.png)

(from [10])

**Compute-optimal scaling laws.** Inspired by these observations, authors in [6] completely reconsidered the best practices for scaling laws that were originally proposed in [1]. The analysis of scaling laws in [6] was conducted with much larger models, yielding results that are slightly different from before. More specifically, LLMs with sizes ranging from 70M to 17B parameters are trained over datasets with sizes exceeding one trillion tokens; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a31e405-b7d2-4724-bfe3-ef69912da43d_1904x850.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a31e405-b7d2-4724-bfe3-ef69912da43d_1904x850.png)

(from [10])

By training LLMs with many different combinations of model and data size, we can discover a power law that predicts an LLM’s test loss as a function of these factors. From these power laws, we can determine which training settings work best for a given compute budget. Authors in [6] argue that compute-optimal training[10](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-10-152758713) should scale model and data size proportionally. Such a finding reveals that most LLM are undertrained for their size—_we would benefit from training existing LLMs over significantly more data_. For example, the scaling laws fit in [6] predict that Gopher should have been trained on a 20x larger dataset!

> _“The amount of training data that is projected to be needed is far beyond what is currently used to train large models.”_ - from [6]

**Chinchilla.** The analysis provided in [6] emphasizes the importance of data scale. _Large models need to be trained over more data to reach their best performance_. To validate this finding, authors train a 70 billion parameter LLM, called Chinchilla. Compared to prior models, Chinchilla is smaller but has a larger pretraining dataset—_1.4T training tokens in total_. Chinchilla uses the same data and evaluation strategy as Gopher [10]. Despite being 4x smaller than Gopher, Chinchilla consistently outperforms the larger model; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F248efee3-31f7-4b53-9709-270254e13547_1138x1394.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F248efee3-31f7-4b53-9709-270254e13547_1138x1394.png)

(from [6])

Scaling laws proposed by Chinchilla [6] became a standard in AI research for years afterward. _“Chinchilla-optimal”_ is now a commonly-used term. Even today, after a wide variety of additional scaling research has been published, Chinchilla and its associated scaling laws are referenced constantly.

## The “Death” of Scaling Laws

Scaling laws have recently become a popular (and contentious) topic within AI research. As we have seen throughout this overview, scaling has fueled most improvements in AI throughout the age of pretraining. As the pace of model releases and improvements slowed in the latter half of 2024[11](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-11-152758713), however, we began to see widespread questioning of model scaling, seeming to indicate that AI research—_and scaling laws in particular_—could be hitting a wall.

- [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11) claims that OpenAI is shifting their product strategy due to hitting a plateau in scaling current methods.
    
- [The Information](https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows) claims that the rate of improvement in GPT models is starting to slow down.
    
- [Bloomberg](https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai) highlights the difficulties being faced by several frontier labs in trying to build AI that is more capable.
    
- [TechCrunch](https://techcrunch.com/2024/11/20/ai-scaling-laws-are-showing-diminishing-returns-forcing-ai-labs-to-change-course/) claims that scaling is starting to yield diminishing returns.
    
- [Time](https://time.com/7178328/is-ai-progress-slowing-down/) publishes a nuanced essay that highlights various contributing factors to the narrative that AI research is slowing down.
    
- Ilya Sutskever states that _“pretraining as we know it will end”_ during his [test of time award speech](https://www.youtube.com/watch?v=1yvBqasHLZs) at NeurIPS’24.
    

At the same time, many experts are arguing the opposite. For example, [Dario Amodei](https://www.youtube.com/watch?v=ugvHCXCOmm4) (Anthropic CEO) has said that scaling is _“probably… going to continue”_, while Sam Altman has continued pushing the narrative that _[“there is no wall”](https://x.com/sama/status/1856941766915641580)_. In this section, we will add more color to this discussion by providing a grounded explanation of the current state of scaling and various issues that may exist.

#### Slower Scaling: What does it mean? Why is it happening?

> _“Both narratives can be true: Scaling is still working at a technical level. The rate of improvement for users is slowing.”_ - [Nathan Lambert](https://www.interconnects.ai/p/scaling-realities)

So… _is scaling slowing down?_ The answer is complex and highly dependent upon our exact definition of “slowing down”. So far, the most reasonable response that I’ve seen to this question is that both answers are correct. For this reason, we will not try to answer this question. Instead, we will simply take a deeper look at what exactly the research tells us about this topic so that we can build a more nuanced understanding of the current (and future) state of scaling for LLMs.

**What do scaling laws tell us?** First, we need to recall the technical definition of a scaling law. Scaling laws define a relationship, based upon a power law, between training compute (or model / dataset size) and the test loss of an LLM. However, _the nature of this relationship is often misunderstood_. The idea of getting exponential performance improvements from logarithmic increases in compute is a myth. Scaling laws look more like an exponential decay, meaning that we will have to work harder over time to get further performance improvements; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95f0364e-8bdb-4144-8ff0-cb6c723af58e_1098x708.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95f0364e-8bdb-4144-8ff0-cb6c723af58e_1098x708.png)

(from [5])

In other words, _scaling laws plateau naturally over time_. In this way, the “slowdown” we are currently experiencing is arguably an expected part of LLM scaling laws.

> _“Practitioners often use downstream benchmark accuracy as a proxy for model quality and not loss on perplexity evaluation sets.”_ - from [7]

**Defining performance.** _How do we measure whether LLMs are improving or not?_ From the perspective of scaling laws, LLM performance is usually measured via the model’s test loss during pretraining, but the impact of lower test loss on an LLM’s capabilities is unclear. _Does lower loss lead to higher accuracy on downstream tasks? Does lower loss cause an LLM to acquire new capabilities?_ There is a disconnect here between what scaling laws tell us and what we actually care about:

- Scaling laws tell us that increasing the scale of pretraining will smoothly decrease the LLM’s test loss.
    
- We care about getting a “better” LLM.
    

Depending on who you are, your expectations of new AI systems—_and the approach that you use to evaluate these new systems_—will vary drastically. Average AI users tend to focus upon general chat applications, while practitioners often care about an LLM’s performance on downstream tasks. In contrast, researchers at top frontier labs seem to have high (and very particular) expectations of AI systems; e.g., [writing a Ph.D. thesis](https://time.com/7178328/is-ai-progress-slowing-down/) or solving [advanced mathematical reasoning problems](https://epoch.ai/frontiermath/the-benchmark). Given that LLMs have such broad capabilities, evaluation is difficult, and there are many lenses from which we can view an LLM’s performance; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a11f01c-02c4-4ede-bf3f-cbda3f5df9a5_1250x840.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a11f01c-02c4-4ede-bf3f-cbda3f5df9a5_1250x840.png)

(from [15])

Given this drastic variance in model expectations, providing definitive proof that scaling is “working” will always be a struggle. We need a more specific definition of success for scaling laws. If the science tells us that larger models will achieve lower loss, this does not mean that new models will meet everyone’s expectations. A failure to achieve AGI or surpass the capabilities of award-winning human mathematicians is not proof that scaling is not still working at a technical level! Put differently, one could argue that the “slowdown” of scaling is a perception and expectation problem rather than a technical problem with scaling laws.

**Data death.** To scale up LLM pretraining, we must increase both model and dataset size. Earlier research [1] seemed to indicate that the amount of data was not as important as the size of the model, but we see with Chinchilla [6] that dataset size is equally important. Plus, more recent work argues that most researchers prefer to “overtrain” their models—_or pretrain them on datasets that go beyond Chinchilla-optimality in size_—to save on inference costs [7].

> _“Scaling studies usually focus on the compute-optimal training regime... As larger models are more expensive at inference, it is now common practice to over-train smaller models.”_ - from [7]

All of this research brings us to one simple conclusion—_scaling up LLM pretraining will require that we create larger pretraining datasets_. This fact forms the basis for one of the key criticisms of LLM scaling laws. Many researchers believe that there may not be enough data available to continue scaling the pretraining process. For context, a large majority of pretraining data used for current LLMs is obtained via web scraping; see below. Given that we only have one internet, finding completely new sources of large-scale, high-quality pretraining data may be hard.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3fe055b-9ca4-410c-b36d-d6e8047b4b28_1554x924.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3fe055b-9ca4-410c-b36d-d6e8047b4b28_1554x924.png)

([source](https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/edit#slide=id.g31d042f750e_0_808))

Even Ilya Sutskever has [recently made this argument](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/), claiming that _i)_ compute is growing quickly but _ii)_ data is not growing due to reliance upon web scraping. Therefore, he believes that we cannot continue scaling up the pretraining process forever. Pretraining as we know it will end, and we must find new avenues of progress for AI research. In other words, _“we have achieved peak data”_.

#### The Next Generation of Scale for Pretraining

Scaling will eventually lead to diminishing returns, and the data-centric argument against the continuation of scaling is both sound and compelling. However, there are still several directions of research that could improve the pretraining process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81237158-d1dd-4f39-a77b-ea3f70f83a6d_1756x970.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81237158-d1dd-4f39-a77b-ea3f70f83a6d_1756x970.png)

**Synthetic data.** To scale up the pretraining process several orders of magnitude, we will likely need to rely upon synthetically-generated data. Despite worries that over reliance upon synthetic data will lead to diversity issues [14], we are seeing increased—_and [seemingly successful](https://www.interconnects.ai/p/llm-synthetic-data)_—usage of synthetic data for LLMs [12]. Additionally, curriculum learning [13] and [continued pretraining](https://www.databricks.com/blog/characterizing-datasets-and-building-better-models-continued-pre-training) strategies have led to a variety of meaningful improvements via adjustments to pretraining data; e.g., changing data mixtures or adding instruction data at the end of pretraining.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe851103a-aa40-47cf-b2cc-85f8cee44e06_1274x910.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe851103a-aa40-47cf-b2cc-85f8cee44e06_1274x910.png)

(from [7])

**Practical scaling laws.** Recent research has attempted to address the limitations of test loss-based scaling laws. For example, authors in [7] define scaling laws that can be used to predict the performance of an LLM on downstream benchmarks from the [LLM Foundry](https://github.com/mosaicml/llm-foundry); see above. Interpreting these kinds of metrics is much easier for humans. We might not know what a 5% decrease in test loss means, but jumping from 85% to 90% accuracy on our benchmark of interest is usually easy to grasp. Several other works have also explored the idea of using scaling laws to provide more practical and meaningful estimates of LLM performance; e.g., after post training and quantization [16] or during the pretraining process [17].

**DeepSeek-v3.** Despite recent arguments, we are still seeing semi-frequent advancements by scaling the LLM pretraining process. For example, DeepSeek-v3 [18]—_a 671B parameter_[12](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-12-152758713) _[mixture-of-experts](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth) (MoE) model_—was recently released. In addition to being open-source, this model is pretrained on 14.8T tokens of text and surpasses the performance of GPT-4o and Claude-3.5-Sonnet; see below for the model’s performance and [here](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL) for the license. For reference, LLaMA-3 models are trained on over 15T of raw text data; see [here](https://ai.meta.com/blog/meta-llama-3/) for more details.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9dbe15be-8b38-4584-bba9-c9c5e20c5806_1586x1018.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9dbe15be-8b38-4584-bba9-c9c5e20c5806_1586x1018.png)

(from [18])

The ability to outperform models like GPT-4o is a significant jump for open weights LLMs—_even the [largest LLaMA models](https://ai.meta.com/blog/meta-llama-3-1/) have fallen short of this goal_. DeepSeek-v3 adopts a variety of interesting tricks:

- An optimized MoE architecture from [DeepSeek-v2](https://arxiv.org/abs/2405.04434).
    
- A new auxiliary-loss-free strategy for [load balancing](https://cameronrwolfe.substack.com/i/142423094/switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsity) the MoE.
    
- A [multi-token prediction](https://arxiv.org/abs/2404.19737) training objective.
    
- Distillation of reasoning capabilities from a long-chain-of-thought model (i.e., similar to [OpenAI’s o1](https://openai.com/index/learning-to-reason-with-llms/)).
    

The model also undergoes post-training, including supervised finetuning and reinforcement learning from human feedback, to align it to human preferences.

> _“We train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back.”_ - from [8]

However, the biggest key to DeepSeek-v3’s impressive performance is pretraining scale—_this is a massive model trained over an equally-massive dataset_! Training such a large model is difficult for a variety of reasons (e.g., GPU failures and loss spikes). DeepSeek-v3 has a surprisingly stable pretraining process and is trained at a reasonable cost by LLM standards; see below. _These results indicate that larger-scale pretraining jobs are becoming more manageable and efficient over time_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f81e548-b371-4576-a694-a50748f5f148_1586x288.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f81e548-b371-4576-a694-a50748f5f148_1586x288.png)

(from [18])

**Increasing scale by an OOM.** To continue testing our scaling laws, we must train LLMs that are several orders of magnitude beyond current models. Setting aside our opinions on the utility of scaling, there are still a variety of limitations that stand in the way of training models of this scale. We will need:

- Larger compute clusters[13](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-13-152758713).
    
- More (and better) hardware.
    
- Massive amounts of power.
    
- New algorithms (e.g., for larger-scale distributed training, [potentially spanning multiple data centers](https://www.youtube.com/watch?v=pE3KKUKXcTM)).
    

Training the next generations of models is not just a matter of securing funding for more GPUs, it is a multi-disciplinary feat of engineering. Such complex efforts take time. For reference, GPT-4 was released in March of 2023, nearly three years—_33 months in particular_—after the release of GPT-3. It is reasonable to expect a similar timeline (if not longer) for unlocking another 10-100x increase in scale.

> _“At every order of magnitude scale up, different innovations have to be found.”_ - [Ege Erdil](https://scholar.google.com/citations?user=x4aaIwcAAAAJ&hl=en) ([Epoch AI](https://epoch.ai/))

## The Future(s) of AI Research

Now that we more deeply understand the state of scaling for pretraining, let’s assume (purely for the purpose of discussion) that pretraining research will hit a sudden wall. Even if model capabilities do not improve at all in the near future, there are a variety of ways in which AI research can continue to rapidly progress. We have already talked about some of these topics (e.g., synthetic data). In this section, we will focus on two topics in particular that are currently popular:

1. LLM systems / agents.
    
2. Reasoning models.
    

#### Building Useful LLM Systems

Most LLM-based applications today operate in a single-model paradigm. In other words, we solve tasks by passing that task to a single LLM and directly using the model’s output as the answer to that task; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c6aaa13-cbb4-45e9-baa7-f65e6f176538_1014x976.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c6aaa13-cbb4-45e9-baa7-f65e6f176538_1014x976.png)

If we want to improve such a system (i.e., solve harder tasks with higher accuracy), we could simply improve the capabilities of the underlying model, but such an approach is dependent upon the creation of more capable models. Instead, we could go beyond the single-model paradigm by building an LLM-based system that combines several LLMs—_or other components_—to solve a complex task.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775634bb-6519-4495-a3b1-83a437b590de_2240x782.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775634bb-6519-4495-a3b1-83a437b590de_2240x782.png)

**LLM system basics.** The goal of an LLM system is to break complex tasks into smaller parts that are easier to solve for an LLM or other module. There are two primary strategies we can use to accomplish this goal (depicted above):

1. _Task decomposition_: break the task itself into smaller sub-tasks that can be solved individually and aggregated[14](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-14-152758713) afterwards to form a final answer.
    
2. _Chaining_: solve a task or sub-task by making several sequential calls to an LLM instead of a single call.
    

These strategies can be used alone or in tandem. For example, assume that we want to build a system to summarize books. To do this, we could decompose the task by first summarizing each chapter of the book. From here, we can either:

- Further decompose the task into smaller chunks of text to summarize (i.e., similar to [recursive / hierarchical decomposition](https://openai.com/index/summarizing-books/)).
    
- Chain several LLM calls together; e.g., have one LLM extract all important facts or information from the chapter and another LLM produce the chapter summary based on these key facts.
    

Then, we can aggregate these results by asking an LLM to summarize the concatenated chapter summaries, forming a summary of the full novel. Most complex tasks can be decomposed into simple parts that are easy to solve, which makes such LLM systems very powerful. These systems can become incredibly sophisticated as we perform more extensive decomposition and chaining, making them an interesting (and impactful) area of applied AI research.

**Building LLM-based products.** Despite the success and popularity of LLMs, the number of practical (and widely-adopted) use cases for LLMs is still very small. The largest use cases for LLMs today are code generation and chat, both of which are relatively obvious applications for LLMs[15](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-15-152758713); see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb4408e7-7a56-4ffb-8561-e8261dda1022_2560x1258.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb4408e7-7a56-4ffb-8561-e8261dda1022_2560x1258.png)

([source](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/))

Given that there are so many ripe domains for the application of LLMs, _simply building more LLM-based products that are genuinely useful is an important area of applied AI research_. We already have access to very capable models, but using these models to build a product worth using is an entirely different problem. Solving this problem requires learning how to build reliable and capable LLM systems.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1a733fb-4558-4ec4-8aa8-f130199a646c_1788x1212.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1a733fb-4558-4ec4-8aa8-f130199a646c_1788x1212.png)

(from [19])

**Agents.** The line between LLM systems and agents is blurry because the term “agent” has been [overloaded](https://www.interconnects.ai/p/the-ai-agent-spectrum) by the AI community. However, the key concept that we should understand is that _LLM systems can be expanded in a variety or interesting and meaningful ways_. For example, we can augment LLMs by teaching them how to use tools (e.g., calculators, search engines, etc.) when solving a problem; see above. Additionally, we can allow LLMs to execute their own programs or even perform actions for us; e.g., booking a hotel or sending an email. The many modules and tools that can be integrated with an LLM present endless possibilities for building LLM systems that are more capable and useful.

**Robustness** is one of the biggest roadblocks to building more capable LLM / agent systems. Assume that we have an LLM system that makes ten different calls to an LLM. Additionally, let’s assume that each of these LLM calls have a 95% likelihood of success and that all of the calls need to succeed in order to generate the correct final outputs. Although the individual components of this system are reasonably accurate, _the overall system has a success rate of 60%_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e7871ad-0e65-4b86-a7bb-3a7b62566e4f_1274x294.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e7871ad-0e65-4b86-a7bb-3a7b62566e4f_1274x294.png)

(from [20])

As we add more components, this problem becomes exponentially worse, which limits the complexity of LLM / agent systems that we can build. Building more complex systems will require that we drastically improve the robustness of each individual system component. [Recent research](https://www.interconnects.ai/p/how-scaling-changes-model-behavior) indicates that robustness can be improved via scaling. However, we can also improve robustness via better meta-generation algorithms; see above. Instead of generating a single output from an LLM, these algorithms use ideas like parallel decoding, (step-level) verification, critiques, and more to obtain a more refined and accurate output from an LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb7f02e-15bf-4445-aec6-f98dd061e36a_1276x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafb7f02e-15bf-4445-aec6-f98dd061e36a_1276x726.png)

(from [20])

This area of research is rapidly progressing and is likely to become a key driver of progress in AI research; see [20] for an in-depth survey of the topic. As meta-generation algorithms improve, LLMs will become more robust, and we will become capable of building increasingly complex LLM / agent systems.

#### Reasoning Models and New Scaling Paradigms

A common criticism of early LLMs was that they simply memorized data and had very little reasoning capabilities. However, the inability of LLMs to reason has been largely debunked over the last few years. We have learned from recent research that these models likely had the inherent ability to reason all along, but we have to use the correct prompting or training approach to elicit this ability.

**Chain of thought (CoT) prompting** [22] was one of the first techniques to demonstrate the reasoning capabilities of LLMs. The approach is simple and prompt-based. We just ask the LLM to provide an explanation for its response prior to generating the actual response; see [here](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) for more details. When an LLM generates a rationale that outlines the [step-by-step](https://arxiv.org/abs/2205.11916) process used to arrive at a response, its reasoning capabilities improve significantly. Plus, this explanation is human-readable and can make the model’s output more interpretable!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [22])

The idea of a chain of thought is both generic and powerful. In fact, chains of thought have become the key concept behind improving LLM reasoning capabilities, and we have seen this technique repurposed in many ways:

- LLM-as-a-Judge-style evaluation models usually provide scoring rationales prior to generating a final evaluation result [23, 24].
    
- Supervised finetuning and instruction tuning strategies have been proposed for teaching smaller / open LLMs to write better chains of thought [25, 26].
    
- LLMs are commonly asked to reflect upon and critique or verify their own outputs, then revise their output based on this information [12, 27].
    

Complex reasoning is an active research topic that is quickly progressing. New training algorithms that teach LLMs to incorporate (step-level) verification [28, 29] into their reasoning process have shown promising results, and we will likely continue to see improvements as new and better training strategies emerge.

**OpenAI’s o1 reasoning model** [21] marks a significant jump in the reasoning capabilities of LLMs. The reasoning strategy used by o1 is heavily based upon chains of thought. Similar to how humans think before responding to a question, o1 will spend time “thinking” before providing a response. Practically speaking, the “thoughts” that o1 generates are just long chains of thought that the model uses to think through a problem, break the problem into simpler steps, try various approaches to solving the problem, and even correct its own mistakes[16](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-16-152758713).

> _“OpenAI o1 [is] a new large language model trained with RL to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.”_ - from [21]

The details of o1’s exact training strategy are not publicly shared. However, we know that o1 is taught to reason using a “large-scale reinforcement learning” algorithm that is “highly data-efficient” and focuses upon refining the model’s ability to generate useful chains of thought. Based upon [public comments](https://x.com/__nmca__/status/1870170098989674833) from OpenAI researchers and [recent rhetoric](https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop) regarding o1, the model seems to be trained using pure reinforcement learning, which contradicts earlier opinions that o1 may be using some form of tree search at inference time.

[

![Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F896f5300-4bfe-4595-80e7-1063b4225a5b_2400x1650.png "Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F896f5300-4bfe-4595-80e7-1063b4225a5b_2400x1650.png)

Comparison between GPT-4o and o1 on reasoning-heavy tasks (from [21])

As mentioned, the performance of o1 on complex reasoning tasks is impressive. o1 outperforms GPT-4o on nearly all reasoning-heavy tasks; see above. As an example of o1’s reasoning capabilities, the model:

- Places in the 89th percentile on competitive programming questions from [Codeforces](https://codeforces.com/).
    
- Reaches the top 500 students in the US in a qualifier for the USA Math Olympiad ([AIME](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid=AfmBOoplGn5Sl-tAVYpjGeZiZChoPCs-y6Ofqk90mzlvesrqhkA5kL6s)).
    
- Exceeds the accuracy of human PhD candidates on graduate-level physics, biology and chemistry problems ([GPQA](https://arxiv.org/abs/2311.12022)).
    

[

![The image shows two scatter plots comparing "o1 AIME accuracy" during training and at test time. Both charts have "pass@1 accuracy" on the y-axis and compute (log scale) on the x-axis. The dots indicate increasing accuracy with more compute time.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9522717b-cade-49a2-a2e4-f60eb1007127_1980x1113.webp "The image shows two scatter plots comparing "o1 AIME accuracy" during training and at test time. Both charts have "pass@1 accuracy" on the y-axis and compute (log scale) on the x-axis. The dots indicate increasing accuracy with more compute time.")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9522717b-cade-49a2-a2e4-f60eb1007127_1980x1113.webp)

(from [22])

**From o1 to o3.** One of the most interesting aspects of o1 is that the model’s reasoning capabilities can be improved by using more compute at inference time. To solve problems of increasing complexity, the model can simply generate progressively longer chains of thought; see [here](https://openai.com/index/learning-to-reason-with-llms/) for examples. Using more inference-time compute to generate these longer chains of thought yields a smooth increase in the model’s reasoning performance; see below.

> _“We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute).”_ - from [22]

Similarly, we see in the figure above that o1’s performance improves smoothly as we invest more compute into training via reinforcement learning. This is exactly the approach that was followed to create the o3 reasoning model. The evaluation results for this model were [previewed](https://www.youtube.com/watch?v=SKBG1sqdyIU&t=2s) by OpenAI in late 2024, and very few details about o3 have been publicly shared. However, given that this model was released so quickly after o1 (i.e., three months later), o3 is most likely a [“scaled up” version of o1](https://www.interconnects.ai/p/openais-o3-the-2024-finale-of-ai) that invests more compute into reinforcement learning.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2975bbda-3d86-4cf6-9108-9c20abc9ec59_1286x1300.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2975bbda-3d86-4cf6-9108-9c20abc9ec59_1286x1300.png)

([source](https://arcprize.org/blog/oai-o3-pub-breakthrough))

The o3 model has not yet been released at the time of writing, but the results achieved by scaling o1 are incredibly impressive (even shocking in some cases). The most notable achievements of o3 are listed below:

- A score of 87.5% on the [ARC-AGI benchmark](https://arcprize.org/blog/oai-o3-pub-breakthrough) on which GPT-4o achieves only 5% accuracy. o3 is the first model to exceed human-level performance of 85% on ARC-AGI. This benchmark has been described as a “North Star” towards AGI and has remained unbeaten[17](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-17-152758713) for over five years.
    
- An accuracy of 71.7% on [SWE-Bench Verified](https://openai.com/index/introducing-swe-bench-verified/) and an [Elo score](https://en.wikipedia.org/wiki/Elo_rating_system) of 2727 on Codeforces, which ranks o3 among the top 200 human competitive programmers on the planet.
    
- An accuracy of 25.2% on EpochAI’s [FrontierMath benchmark](https://epoch.ai/frontiermath), _improving upon the previous state-of-the-art accuracy of 2.0%_. This benchmark was described by [Terence Tao](https://en.wikipedia.org/wiki/Terence_Tao) as “incredibly difficult” and likely to be unsolved by AI systems for “several years at least”.
    

A distilled version of o3, called o3-mini, was also previewed, which performs very well and comes with significant improvements in compute efficiency.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e98bbb-e1d6-454b-ac9f-c43a38ba0fb5_2516x952.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e98bbb-e1d6-454b-ac9f-c43a38ba0fb5_2516x952.png)

(from [21] and [here](https://arcprize.org/blog/oai-o3-pub-breakthrough))

**New paradigms for scaling.** After reading this overview, many of the plots presented by o1 and o3 (see above) might look pretty familiar—_these are log-scale plots where we see a smooth, linear increase in performance with more compute_! In other words, we see a clear power law relationship between the performance of these reasoning models and two different quantities:

- Training-time (reinforcement learning) compute.
    
- Inference-time compute.
    

Scaling o1-style models differs from traditional scaling laws. Instead of scaling up the pretraining process, we are scaling the amount of compute invested into post training and inference. _This is a completely new scaling paradigm_, and the results achieved by scaling reasoning models are great so far. Such a finding shows us that other avenues of scaling—_beyond pretraining_—clearly exist. With the advent of reasoning models, we have discovered our next hill to climb. Although it may come in a different form, _scaling will continue to drive progress in AI research_.

## Closing Thoughts

We now have a clearer view of scaling laws, their impact on LLMs, and the future directions of progress for AI research. As we have learned, there are many contributing factors to the recent criticism of scaling laws:

- The natural decay in scaling laws.
    
- The high variance in expectations of LLM capabilities.
    
- The latency of large-scale, inter-disciplinary engineering efforts.
    

These issues are valid, _but_ _none of them indicate that scaling is not still working as expected_. Investments into large-scale pretraining will (and should) continue, but improvements will become exponentially harder over time. As a result, alternative directions of progress (e.g., agents and reasoning) will become more important. As we invest into these new areas of research, however, the fundamental idea of scaling will continue to play a massive role. Whether scaling will continue is not a question. _The true question is what we will scale next_.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Kaplan, Jared, et al. "Scaling laws for neural language models." _arXiv preprint arXiv:2001.08361_ (2020).

[2] Radford, Alec. "Improving language understanding by generative pre-training." (2018).

[3] Radford, Alec, et al. "Language models are unsupervised multitask learners." _OpenAI blog_ 1.8 (2019): 9.

[4] Brown, Tom, et al. "Language models are few-shot learners." _Advances in neural information processing systems_ 33 (2020): 1877-1901.

[5] Achiam, Josh, et al. "Gpt-4 technical report." _arXiv preprint arXiv:2303.08774_ (2023).

[6] Hoffmann, Jordan, et al. "Training compute-optimal large language models." _arXiv preprint arXiv:2203.15556_ (2022).

[7] Gadre, Samir Yitzhak, et al. "Language models scale reliably with over-training and on downstream tasks." _arXiv preprint arXiv:2403.08540_ (2024).

[8] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." _Advances in neural information processing systems_ 35 (2022): 27730-27744.

[9] Smith, Shaden, et al. "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model." _arXiv preprint arXiv:2201.11990_ (2022).

[10] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." _arXiv preprint arXiv:2112.11446_ (2021).

[11] Bhagia, Akshita, et al. "Establishing Task Scaling Laws via Compute-Efficient Model Ladders." _arXiv preprint arXiv:2412.04403_ (2024).

[12] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." _arXiv preprint arXiv:2212.08073_ (2022).

[13] Blakeney, Cody, et al. "Does your data spark joy? Performance gains from domain upsampling at the end of training." _arXiv preprint arXiv:2406.03476_ (2024).

[14] Chen, Hao, et al. "On the Diversity of Synthetic Data and its Impact on Training Large Language Models." _arXiv preprint arXiv:2410.15226_ (2024).

[15] Guo, Zishan, et al. "Evaluating large language models: A comprehensive survey." _arXiv preprint arXiv:2310.19736_ (2023).

[16] Xu, Zifei, et al. "Scaling laws for post-training quantized large language models." _arXiv preprint arXiv:2410.12119_ (2024).

[17] Xiong, Yizhe, et al. "Temporal scaling law for large language models." _arXiv preprint arXiv:2404.17785_ (2024).

[18] DeepSeek-AI et al. "DeepSeek-v3 Technical Report." _https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf_ (2024).

[19] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." arXiv preprint arXiv:2302.04761 (2023).

[20] Welleck, Sean, et al. "From decoding to meta-generation: Inference-time algorithms for large language models." _arXiv preprint arXiv:2406.16838_ (2024).

[21] OpenAI et al. “Learning to Reason with LLMs.” _https://openai.com/index/learning-to-reason-with-llms/_ (2024).

[22] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." _Advances in neural information processing systems_ 35 (2022): 24824-24837.

[23] Liu, Yang, et al. "G-eval: Nlg evaluation using gpt-4 with better human alignment." _arXiv preprint arXiv:2303.16634_ (2023).

[24] Kim, Seungone, et al. "Prometheus: Inducing fine-grained evaluation capability in language models." _The Twelfth International Conference on Learning Representations_. 2023.

[25] Ho, Namgyu, Laura Schmid, and Se-Young Yun. "Large language models are reasoning teachers." _arXiv preprint arXiv:2212.10071_ (2022).

[26] Kim, Seungone, et al. "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning." _arXiv preprint arXiv:2305.14045_ (2023).

[27] Weng, Yixuan, et al. "Large language models are better reasoners with self-verification." _arXiv preprint arXiv:2212.09561_ (2022).

[28] Lightman, Hunter, et al. "Let's verify step by step." _arXiv preprint arXiv:2305.20050_ (2023).

[29] Zhang, Lunjun, et al. "Generative verifiers: Reward modeling as next-token prediction." _arXiv preprint arXiv:2408.15240_ (2024).

[1](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-1-152758713)

The two primary reports are from [The Information](https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows) and [Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/).

[2](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-2-152758713)

We use the following settings to generate the plot: `a = 1`, `p = 0.5`, and `0 < x < 1`.

[3](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-3-152758713)

Compute is defined in [1] as `6NBS`, where `N` is the number of model parameters, `B` is the batch size used during training, and `S` is the total number of training steps.

[4](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-4-152758713)

This extra multiplicative constant does not change the behavior of the power law. To understand why this is the case, we must understand the definition of [scale invariance](http://felix.physics.sunysb.edu/~allen/540-05/scaling.html#:~:text=scale%20invariance%20of%20power%20law%20functions&text=The%20function%20y%3Dxp,by%20a%20scale%20factor%20a.). Because power laws are scale invariant, the fundamental characteristics of a power law are the same even when we scale up or down by a certain factor. The behavior observed will be the same at any scale!

[5](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-5-152758713)

This description was from Ilya’s test of time award for [this paper](https://arxiv.org/abs/1409.3215) at NeurIPS’24.

[6](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-6-152758713)

Although this might seem obvious now, we should remember that most NLP tasks at the time (e.g., summarization and QA) had entire areas of research devoted to them! Each of these tasks had associated task-specific architectures that were specialized in performing that task, and GPT was a single generic model that could outperform most of these architectures across several different tasks.

[7](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-7-152758713)

This means that we just describe each task in the LLM’s prompt and use the same model to solve different tasks—_only the prompt changes between tasks_.

[8](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-8-152758713)

This is to be expected because these models use zero-shot inference and are not finetuned at all on any of the downstream tasks.

[9](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-9-152758713)

By an “emergent” ability, we mean a skill possessed by the LLM that only emerges after a certain scale has been reached (e.g., a sufficiently large model).

[10](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-10-152758713)

Here, we define “compute-optimal” as the training setting that yields the best possible performance—_in terms of test loss_—at a fixed training compute cost.

best performance at a fixed compute cost

[11](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-11-152758713)

For example, Anthropic has continually pushed back their release of [Claude 3.5 Opus](https://www.anthropic.com/news/claude-3-5-sonnet), Google has only released the [flash variant of Gemini-2](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/), and OpenAI only released [GPT-4o](https://openai.com/index/hello-gpt-4o/) in 2024 (until [o1](https://openai.com/o1/) and o3 were released in December), which was arguably not significantly more capable than GPT-4.

[12](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-12-152758713)

Only 37 billion of these parameters are active during inference for a single token.

[13](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-13-152758713)

For example, xAI has recently built a [new datacenter](https://www.datacenterfrontier.com/machine-learning/article/55244139/the-colossus-ai-supercomputer-elon-musks-drive-toward-data-center-ai-technology-domination) with 100,000 NVIDIA GPUs in Memphis, while Anthropic leadership has [voiced a desire](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=1103s) to increase their compute spend by up to 100x over the next few years.

[14](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-14-152758713)

The aggregation step can be implemented in a variety of ways. For example, we can manually aggregate responses (e.g., via concatenation), use an LLM, or pretty much anything in between!

[15](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-15-152758713)

This is not because these tasks are simple. Both code generation and chat are hard to solve, but they are (arguably) fairly obvious applications for LLMs.

[16](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-16-152758713)

OpenAI has chosen to hide these long chains of thought from the users of o1. The argument behind this choice is that these rationales provide insight into the model’s though process that can be used to debug or monitor the model. However, the model should be allowed to express its pure thoughts without any of the safety filtering that is necessary for user-facing model outputs.

[17](https://cameronrwolfe.substack.com/p/llm-scaling-laws#footnote-anchor-17-152758713)

Currently, ARC-AGI is technically still unbeaten because o3 exceeds the compute requirement for the benchmark. However, the model still achieves an accuracy of 75.7% using a lower compute setting.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Bilal Shahid's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5880a6d8-998a-4ea8-b869-bc2275b0d347_96x96.png)



](https://substack.com/profile/242564486-bilal-shahid)

[

![Fabbri Paolo's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54d12824-9b91-4536-8696-59f5c1628689_766x1434.jpeg)



](https://substack.com/profile/1483954-fabbri-paolo)

[

![Jaewan Park's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0472a3b-cbaa-46a6-9316-0421a0325ba1_460x460.jpeg)



](https://substack.com/profile/296877548-jaewan-park)

[

![Peter Xing's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9f30792-c26a-4b49-9bd4-015e1047ecbc_533x533.jpeg)



](https://substack.com/profile/17521482-peter-xing)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael)

122 Likes∙

[14 Restacks](https://substack.com/note/p-152758713/restacks?utm_source=substack&utm_content=facepile-restacks)

122

- 

[

8

](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comments)

14

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Yousef Kotp's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71725482-2e87-4e02-8a55-07a55e62e8e0_800x800.jpeg)



](https://substack.com/profile/139437526-yousef-kotp?utm_source=comment)

[Yousef Kotp](https://substack.com/profile/139437526-yousef-kotp?utm_source=substack-feed-item)

[2月16日](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comment/93714490 "2025年2月16日 00:43")

Liked by Cameron R. Wolfe, Ph.D.

Perfect article, thank you so much for your writing and amazing insights!

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comment/93714490)

[

![Srivatsa Srinivas's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9d386d-425c-497e-a5a7-27ce13e91501_96x96.jpeg)



](https://substack.com/profile/245194252-srivatsa-srinivas?utm_source=comment)

[Srivatsa Srinivas](https://substack.com/profile/245194252-srivatsa-srinivas?utm_source=substack-feed-item)

[1月20日](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comment/87203636 "2025年1月20日 06:43")

Liked by Cameron R. Wolfe, Ph.D.

Hi Cameron,

Thanks for the awesome article

You might be interested in this [https://arxiv.org/abs/2501.04682](https://arxiv.org/abs/2501.04682) article for the o1/o3 section

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comment/87203636)

[6 more comments...](https://cameronrwolfe.substack.com/p/llm-scaling-laws/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----



[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Mixture-of-Experts (MoE) LLMs

### Understanding models like DeepSeek, Grok, and Mixtral from the ground up...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Jan 27, 2025

214

- 

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

24

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cbbb885-f965-4f56-80fe-2b7e28842237_2254x1258.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cbbb885-f965-4f56-80fe-2b7e28842237_2254x1258.png)

(from [2, 5, 14])

In an area of study that is rapidly changing, the decoder-only transformer architecture has remained one of the few enduring staples in large language model (LLM) research. This architecture has been used since the proposal of the [original GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) and has remained largely unchanged, aside from minor tweaks to improve efficiency. One of the most meaningful modifications to be explored for this architecture, however, is the Mixture-of-Experts (MoE) layer.

> _“Using an MoE architecture makes it possible to attain better tradeoffs between model quality and inference efficiency than dense models typically achieve.”_ - from [11]

MoE-based LLMs introduce sparsity to the model’s architecture, allowing us to significantly increase its size—_in terms of the number of total parameters_—without a corresponding increase in compute costs. This modification, which has been successfully adopted by recent models like Grok [9] and DeepSeek-v3 [15], makes the exploration of extremely large models more tractable and compute efficient. In this overview, we will learn about the fundamentals of MoEs and explore how this idea has been recently applied to create more powerful LLMs.

## Fundamentals of MoEs for LLMs

The MoE-based LLMs that we will study in this overview are based upon the decoder-only transformer architecture. We will not cover the details of this architecture here, but please see [this article](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) if you are unfamiliar. The decoder-only transformer is comprised of repeated blocks containing normalization (e.g., layer normalization or [RMS layer normalization](https://arxiv.org/abs/1910.07467)), masked multi-headed self-attention or a feed-forward transformation, and a residual connection; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F379c5d72-9aca-4b50-bd9c-d5ad9454f477_1622x798.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F379c5d72-9aca-4b50-bd9c-d5ad9454f477_1622x798.png)

The decoder-only transformer architecture

In this section, we will cover the fundamentals of MoEs. This explanation is based upon seminal papers that _i)_ proposed the standard MoE layer and _ii)_ extended this idea to be used in transformer architectures. The papers are:

1. [The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) [1]
    
2. [Switch Transformers](http://switch%20transformers/) [2]
    
3. [Stable and Transferable Mixture-of-Experts (ST-MoE)](https://arxiv.org/abs/2202.08906) [3]
    

For a more detailed break down of these papers and the origins of the MoE architecture, please see the more detailed overview of these ideas below.

[](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)

[

![Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation](https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa419dd3a-57da-4a9a-a446-31ce4b001a7d_2398x1346.png)

](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)

[

#### Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation

](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)

[](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)

[](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

2024年3月18日

[

Read full story

](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth)

**Quick preliminaries.** To understand MoEs and routing algorithms, we must first understand the structure of input for the decoder-transformer (and each of its layers). Of course, LLMs take text as input, but this text undergoes extensive processing before the LLM actually sees it. First, the text is tokenized (shown below)—_or converted into a list of discrete tokens_. These tokens are just words and sub-words. The LLM has a fixed set of tokens that it understands and is trained on, referred to as the model’s “vocabulary”. Vocabulary sizes change between models, but sizes of 64K to 256K total tokens are relatively common.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8aadf17-3bf6-4b79-9688-b6bfbc5840b1_1830x888.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8aadf17-3bf6-4b79-9688-b6bfbc5840b1_1830x888.png)

Tokenizing and vectoring text for an LLM

After the text has been converted to tokens, we can vectorize each token in the input. In addition to having a vocabulary, an LLM has a token embedding layer, which stores a (learned[1](https://cameronrwolfe.substack.com/p/moe-llms#footnote-1-154340424)) vector embedding for every token in its vocabulary. We can lookup the vector for each token in this layer, forming an input matrix. If each token embedding is `d`-dimensional and there are `C` total tokens in our input, then the total size of this input matrix is `C` by `d`; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f723f2-056a-4fc0-a3f7-7aa151fe297e_1194x1026.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f723f2-056a-4fc0-a3f7-7aa151fe297e_1194x1026.png)

Input matrix of token vectors

Each layer of the transformer—_and each sub-layer within every transformer block_—maintains the size of this input. As a result, the input (and output) for any feed-forward or attention module in the transformer is a matrix of this same size!

#### What are “experts”?

In the decoder-only transformer architecture, the main modification made by an MoE is within the feed-forward component of the transformer block. In the standard architecture, we have a single feed-forward neural network—_usually made up of two feed-forward layers with a non-linear activation in between_—through which every token is passed individually; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d3f6b5-316f-474b-a2cc-243cc22ac7ac_1870x548.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d3f6b5-316f-474b-a2cc-243cc22ac7ac_1870x548.png)

An MoE slightly modifies this block structure. Instead of having a single feed-forward network within the feed-forward component of the block, we create several feed-forward networks, _each with their own independent weights_. We refer to each of these networks as an “expert”. For example, an MoE-based LLM may have eight independent experts in each of its feed-forward sub-layers.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a99797b-4392-421b-82b0-62932d968217_684x84.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a99797b-4392-421b-82b0-62932d968217_684x84.png)

The experts within a transformer layer can be defined as shown above. We have `N` experts in a layer, and we can refer to the `i`-th expert using the notation `E_i`.

**Creating an MoE-based transformer.** To create an MoE-based decoder-only transformer architecture, we simply convert the transformer’s feed-forward layers to MoE—_or expert_—layers. Each expert within the MoE layer has an architecture that is identical to the original, feed-forward network from that layer—_we just have several independent copies of the original feed-forward network_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbb9a24-440d-4d26-8092-b6d72dafb55e_1482x858.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fbb9a24-440d-4d26-8092-b6d72dafb55e_1482x858.png)

Adding experts to a decoder-only transformer block (from [2])

However, we need not use experts for every feed-forward layer in the transformer. Most MoE-based LLMs use a stride of `P`, meaning that every `P`-th layer is converted into an expert layer and other layer are left untouched—_these are “interleaved” MoE layers_. This approach can be used to achieve a better balance between the resulting model’s performance and efficiency.

#### Routing Algorithms

One of the primary benefits of MoE-based architectures is their efficiency, but using experts alone does not improve efficiency! In fact, adding more experts to each layer of the model significantly increases the total number parameters—_and the amount of necessary compute_—for the model. To make the architecture more efficient, we must sparsely select the experts that should be used in each layer!

**Selecting experts.** Let’s consider a single token—_represented by a_ `d`_-dimensional token vector_. Our goal is to select a subset of experts (of size `k`) to process this token. In the MoE literature, _we usually say that the token will be “routed” to these experts_. We need an algorithm to compute and optimize this routing operation.

The simplest possible routing algorithm would apply a linear transformation to the token vector, forming a vector of size `N` (i.e., the number of experts). Then, we can apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to form a probability distribution over the set of experts for our token. We can use this distribution to choose experts to which our token should be routed by simply selecting the top-`K` experts in the distribution.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1189a50c-ad49-4e09-8fca-b800532e101a_1156x856.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1189a50c-ad49-4e09-8fca-b800532e101a_1156x856.png)

Computing output of routing mechanism

This routing strategy was used in [1], the paper that proposed the sparse MoE layer structure that we use today; see above. However, _such a routing mechanism does not explicitly encourage a balanced selection of experts_. For this reason, the model is likely to converge to a state of repeatedly selecting the same few experts for every token instead of fully and uniformly utilizing its expert layers, as explained below. This phenomenon is commonly referred to as “routing collapse”.

> _“The gating network tends to converge to a state where it always produces large weights for the same few experts. This imbalance is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more by the gating network.”_ - from [1]

**Active parameters.** Because we only select a subset of experts to process each token within an MoE layer, there is a concept of “active” parameters in the MoE literature. Put simply, only a small portion of the MoE model’s total parameters—_given by the experts selected at each MoE layer_—are active when processing a given token. As a result, the total computation performed by the MoE is proportional to the number of active parameters, rather than the total number of parameters.

#### Auxiliary Losses and Expert Load Balancing

To encourage a balanced selection of experts during training, we can simply add an additional constraint to the training loss that rewards the model for uniformly leveraging each of its experts. In [1], this is done by defining an “importance” score for each expert. The importance score is based upon the probability predicted for each expert by the routing mechanism.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fd12a9-7327-47e3-b7b7-400e801bf5c8_2640x1354.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fd12a9-7327-47e3-b7b7-400e801bf5c8_2640x1354.png)

Details of computing the importance loss (from [1])

Given a batch of data, we compute importance by taking a sum of the probabilities assigned to each expert across all tokens in the batch; see above. Then, to determine if these probabilities are balanced, we can take the squared [coefficient of variation (CV)](https://en.wikipedia.org/wiki/Coefficient_of_variation) of the expert importance scores. Put simply, _the CV will be a small value if all experts have similar importance scores and vice versa_.

From here, we can simply add the importance loss shown above to our standard language modeling loss to from our new (regularized) training objective. This additional importance loss term helps to ensure that the MoE assigns equal probability to experts throughout the training process.

**Load balancing.** Although the importance loss described above is useful, just because experts are assigned equal importance does not mean that tokens are routed uniformly. For example, experts would have equal importance with:

- A few tokens that assign them very high probability.
    
- A much larger number of tokens that assign lower probability.
    

As a result, the number of tokens dispatched to each expert can still be highly non-uniform even when using an importance loss, which can lead to excessive memory usage and generally degraded efficiency for the MoE[2](https://cameronrwolfe.substack.com/p/moe-llms#footnote-2-154340424).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F644cec45-8dff-491e-9d41-e53ee4b0c7df_1574x764.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F644cec45-8dff-491e-9d41-e53ee4b0c7df_1574x764.png)

The expert load balancing loss (from [2])

To solve this problem, we can create a single auxiliary loss term (shown above) that captures both expert importance and load balancing, defined as the equal routing of tokens between each of the experts. Such an approach is proposed in [2], where authors create a loss that considers two quantities:

1. The fraction of router probability allocated to each expert[3](https://cameronrwolfe.substack.com/p/moe-llms#footnote-3-154340424).
    
2. The fraction of tokens dispatched to each expert.
    

If we store both of these quantities in their own `N`-dimensional vectors, we can create a single loss term by taking the [dot product](https://en.wikipedia.org/wiki/Dot_product)[4](https://cameronrwolfe.substack.com/p/moe-llms#footnote-4-154340424) of these two vectors. The resulting loss is minimized when experts receive uniform probability and load balancing, thus capturing both of our goals within a single auxiliary loss term!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1790688e-5328-45f2-98c0-717ba6041470_2090x636.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1790688e-5328-45f2-98c0-717ba6041470_2090x636.png)

The router-z loss (from [3])

**Router z-loss.** The auxiliary load balancing loss described above is widely used throughout the MoE literature, but authors in [3] propose an extra auxiliary loss term, called the router z-loss, that can further improve training stability. The router z-loss constrains the size of the [logits](https://wandb.ai/amanarora/Written-Reports/reports/Understanding-Logits-Sigmoid-Softmax-and-Cross-Entropy-Loss-in-Deep-Learning--Vmlldzo0NDMzNTU3#logits)—_not the probabilities, so this is before softmax is applied_—predicted by the routing mechanism. Ideally, we do not want these logits to be too big. However, these logits can become very large—_leading to [round-off](https://en.wikipedia.org/wiki/Round-off_error) errors that can destabilize the training process even when using full (_`float32`_) precision_—when passed through the router’s (exponential) softmax function.

> _“The router computes the probability distribution over the experts in float32 precision. However, at the largest scales, we find this is insufficient to yield reliable training.”_ - from [3]

To encourage the router to predict smaller logits, we can use the loss term shown above. Given that this loss focuses solely upon regularizing the router’s logits and performs no load balancing, we typically use the router z-loss in tandem with the auxiliary load balancing loss proposed in [2]. Both of these losses are added on top of the LLM’s standard language modeling loss; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726a1e49-0aaa-45dd-a9d0-5386edc2ecc1_2522x288.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F726a1e49-0aaa-45dd-a9d0-5386edc2ecc1_2522x288.png)

#### Expert Capacity

The computation performed in an MoE layer is dynamic due to the routing decisions made during both training and inference. However, when we look at most practical implementations of sparse models, we will see that they usually have static batch sizes—_this is a useful trick for improving hardware utilization_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F417c5fc8-2524-48e1-a9ef-460b4476d323_1784x1184.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F417c5fc8-2524-48e1-a9ef-460b4476d323_1784x1184.png)

(from [2])

**Expert capacity.** To formalize the fixed batch size that we set for each expert, we can define the expert capacity. The expert capacity is defined as shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4039c04-bf0b-482c-abd8-c76f500f228f_2232x144.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4039c04-bf0b-482c-abd8-c76f500f228f_2232x144.png)

The expert capacity defines the maximum number of tokens in a batch that can be sent to each expert. If the number of tokens routed to an expert exceeds the expert capacity, then we just “drop” these extra tokens. More specifically, we perform no computation for these tokens and let their representation flow directly to the next layer via the transformer’s residual connection.

> _“To improve hardware utilization, most implementations of sparse models have static batch sizes for each expert. The expert capacity refers to the number of tokens that can be routed to each expert. If this capacity is exceeded then the overflowed tokens… are passed to the next layer through a residual connection.”_ - from [3]

Expert capacity is controlled via the capacity factor setting. A capacity factor of one means that tokens are routed in a perfectly balanced manner across experts. Alternatively, setting the capacity factor above one provides extra buffer to accommodate for an imbalance in tokens between experts. However, this comes at a cost (e.g., higher memory usage and lower efficiency).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3b8826b-25dd-454b-a686-11c45a20bd50_1104x496.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3b8826b-25dd-454b-a686-11c45a20bd50_1104x496.png)

(from [2])

**How do we set the capacity factor?** Interestingly, MoE models tend to perform well with relatively low capacity factors [2, 3]; see above. However, we need to ensure that the number of dropped tokens is not too large (i.e., this can be done empirically) to avoid any impact to the training run. We can also use different capacity factors for training and inference; e.g., ST-MoE [3] uses a capacity factor of 1.25 during training and a capacity factor of 2.0 during evaluation.

#### Computing the Output of an MoE Layer

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf751012-739f-4bac-92be-4e22cce61e9d_1368x648.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf751012-739f-4bac-92be-4e22cce61e9d_1368x648.png)

Computing the output of an MoE layer

Once we have the router’s output, we compute the final output as follows:

1. Send the tokens to their selected experts.
    
2. Compute the output of the experts for these tokens.
    
3. Take a weighted average of expert outputs, where the weights are simply the probabilities assigned to each expert by the router.
    

In the equation above, we have formalized the process for computing the output of the MoE layer for a single token. The output for this token is just a weighted average of the outputs from each of its `K` active experts.

**Shared experts** are an idea that has been more recently introduced in the MoE literature [14, 15]. The idea is simple:

- We have two groups of experts—_shared experts and routed experts_.
    
- All tokens are always passed through the shared experts.
    
- Tokens are passed through the routed experts according to a normal MoE routing mechanism.
    

This idea of shared experts is depicted below, where we see that routing is only applied to a subset of the experts within an MoE layer. Usually, the number of shared experts must be lower than the number of routed experts—_increasing the number of shared experts degrades the sparsity benefits of the MoE_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ec49e67-8f67-4eea-8759-c27231ffacf5_1212x628.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ec49e67-8f67-4eea-8759-c27231ffacf5_1212x628.png)

Shared vs. routed experts (from [14])

The motivation behind using shared experts is minimizing the amount of redundant information between experts. By having a set of shared experts, _we can allow the network to store shared information within these experts_, rather than having to replicate the same information across several different experts. To compute the output of an MoE layer with shared experts, we simply add the shared experts’ output to the normal, routed output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9aceff4e-ff4c-4729-a628-a6ac28bfa600_1198x406.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9aceff4e-ff4c-4729-a628-a6ac28bfa600_1198x406.png)

Computing output of an MoE layer with shared experts

#### Putting it all Together: Decoder-only LLMs with MoE Layers

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e41ca54-f2be-437c-9b89-c45916b245cf_1634x818.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e41ca54-f2be-437c-9b89-c45916b245cf_1634x818.png)

A full MoE block in a transformer (from [2])

A full depiction of an MoE layer is provided above. In an MoE, we modify the block structure of the standard decoder-only transformer by replacing the feed-forward network with an expert layer. Put simply, this expert layer contains several independent copies of the original feed-forward network. Notably, all of these components within the MoE layer—_the normal layer(s), the experts, and the routing mechanism_—are trained jointly via gradient descent.

For each token, we can choose which experts to be used via a routing mechanism, which is usually implemented via a simple linear transformation of the token vector. Putting this together, the modified block structure for an MoE contains:

- A self-attention layer.
    
- A residual connection and a normalization operation.
    
- A routing mechanism that determines the routing of tokens to experts.
    
- An expert layer with multiple independent feed-forward networks.
    
- A final add and normalize operation that is applied to the final output of the expert layer for each token.
    

Aside from the modified block structure, the transformer architecture remains the same. We also only convert every `P`-th block of the transformer to use an MoE layer—_other blocks remain unchanged_. Some MoEs use experts at every layer, but it is common to set `P` to two, four, or even six in practice. This trick can be useful for controlling the total number of parameters consumed by the MoE LLM.

## The Pros and Cons of Using MoEs

Now that we understand the basics of MoEs, we might wonder: _Why would we want to use an MoE instead of a dense model?_ The biggest selling point of MoEs is their efficiency, but these models also have notable drawbacks. Let’s quickly go over some of the most important pros and cons of MoEs to be aware of.

**Benefits of MoEs.** [LLMs benefit from scale](https://cameronrwolfe.substack.com/p/llm-scaling-laws)—_larger models and larger datasets lead to better performance_. However, scaling up LLMs comes at a cost! One of the key benefits of MoEs is their ability to circumvent issues with scaling up—_they allow us to increase the size of our model at a fixed computational cost per token_. This way, we can train larger models than would be possible if we restricted ourselves to only dense models. In the language modeling domain, the extra parameters and representational capacity of these sparse models make a big difference.

> _“As LLMs become increasingly prevalent, enhancing their performance without proportionally increasing computational resources is a critical challenge.”_ - from [12]

The computational benefit of MoEs is (arguably) most impactful during inference. MoE models are large in terms of their number of total parameters, so we need a sufficient number of GPUs available to store these parameters. However, we only use a fixed portion of these parameters when processing each token, which drastically improves compute efficiency. Inference is faster at low batch sizes, and throughput is higher at large batch sizes [5]. Interestingly, MoEs are also more efficient to train. For example, the Switch Transformer reported a 7× pretraining speedup from the use of an MoE architecture [2]; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44b313dc-540d-4ba7-8711-fe9fbd081260_1184x786.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44b313dc-540d-4ba7-8711-fe9fbd081260_1184x786.png)

(from [2])

**Drawbacks of using MoEs.** Despite these benefits, MoEs are also:

- Prone to instabilities during training.
    
- Difficult to finetune (i.e., due to issues with overfitting).
    
- Sensitive to low / mixed precision training techniques.
    
- Sensitive to hyperparameter settings (e.g., weight initialization).
    

Put simply, there are more bells and whistles required to get the most out of an MoE. For this reason, _MoEs may not be the best choice in every scenario_; e.g., a dense model may be an easier choice if we are looking to finetune an LLM on some task. However, if we can use them properly, MoEs have a variety of benefits.

## Mixture-of-Experts Language Models

Now that we understand the most important and fundamental concepts of MoEs, lets take a deeper look at how these concepts have been applied in the language modeling domain. Due to the fact that LLMs benefit from increased scale, MoEs have been widely adopted and seen great success within LLM research.

#### [Mixtral of Experts](https://arxiv.org/abs/2401.04088) [5]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdff049a9-0a22-4894-8f62-997c4d3fad78_1350x532.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdff049a9-0a22-4894-8f62-997c4d3fad78_1350x532.png)

(from [5])

Mixtral 8×7B (a.k.a. Mixtral of Experts) is an MoE-based extension of the open-source [Mistral-7B model](https://arxiv.org/abs/2310.06825) [6] that is fluent in English, French, Italian, German, and Spanish. Both of these models have open weights under an [Apache 2.0](https://fossa.com/blog/open-source-licenses-101-apache-license-2-0/) license, as well as corresponding [technical reports](https://arxiv.org/abs/2310.06825) that provide details about the models.

Mixtral converts every layer of Mistral to an expert layer with eight experts. Two of these experts are active for each token, yielding a model with 47 billion total parameters and 13 billion active parameters. The model also has a [context length](https://cameronrwolfe.substack.com/i/143156742/what-is-prompt-engineering) of 32K, which is 4× larger than its non-MoE counterpart. As shown in the figure above, Mixtral outperforms Mistral across the board and especially excels in code generation, math, and multilingual benchmarks, even exceeding the performance of the larger [LLaMA-2-70B](https://arxiv.org/abs/2307.09288) model in some cases.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a7dc1e2-e66c-4a30-a0a7-518ae7e3a566_1536x596.png)

(from [7])

**Mistral-7B architecture.** The base LLM architecture for Mixtral 8×7B is a decoder-only transformer that exactly matches the architecture settings of Mistral-7B [6]. Compared to the standard decoder-only LLM architecture, there are a few changes made by Mistral-7B:

1. _[Grouped-Query Attention (GQA)](https://cameronrwolfe.substack.com/i/142044446/efficient-masked-self-attention) [7]_: shares key and value projections between groups of self-attention heads to improve efficiency; see above.
    
2. _[Sliding Window Attention (SWA)](https://arxiv.org/abs/2004.05150) [8]_: computes (masked) self-attention over a fixed window of size `W` for each token to allow the LLM to handle sequences of arbitrary length with a reduced inference cost[5](https://cameronrwolfe.substack.com/p/moe-llms#footnote-5-154340424); see below.
    

Because we use SWA, the model can use tricks like [rolling buffer / circular caches](https://github.com/NVIDIA/TensorRT-LLM/blob/b171e879563ff0ba4eb35b94cf0e59a471e13d80/docs/source/advanced/gpt-attention.md#sliding-window-attention-cyclic-rolling-buffer-kv-cache) to make the [KV cache](https://training.continuumlabs.ai/inference/why-is-inference-important/key-value-cache) more memory efficient or [chunked prefill](https://developer.nvidia.com/blog/streamlining-ai-inference-performance-and-deployment-with-nvidia-tensorrt-llm-chunked-prefill/#balancing_prefill_and_decode_phases_with_chunked_prefill) to increase inference speed. Mixtral 8×7B adopts the same architecture conventions.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F394e1379-d10e-4db6-af55-0b6c9b66c247_1182x648.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F394e1379-d10e-4db6-af55-0b6c9b66c247_1182x648.png)

(from [6])

**More details.** As mentioned previously, Mixtral converts every layer of the LLM to an expert layer. Within each expert layer, a simple routing mechanism is adopted that takes a softmax over the Top-`K` logits of a linear layer for every token—_this matches the routing mechanism discussed at the start of this overview_; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae644a78-4c91-4ba4-9773-a8b0e146e41b_1608x576.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae644a78-4c91-4ba4-9773-a8b0e146e41b_1608x576.png)

(from [5])

Authors mention in [5] that Mixtral is also pretrained over a multilingual corpus, allowing the model to understand multiple languages. As shown below, Mixtral universally outperforms LLaMA models on multi-lingual benchmarks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F837543f9-aa0c-4fa3-b2c3-c1f7bed38896_1358x270.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F837543f9-aa0c-4fa3-b2c3-c1f7bed38896_1358x270.png)

(from [5])

**Routing analysis.** To conclude the paper, authors in [5] perform some detailed analysis of how experts are selected for tokens across several domains to see if any interpretable patterns can be deduced. When we plot the distribution of tokens assigned to different experts for various topic areas within [The Pile](https://pile.eleuther.ai/), there are no obvious patterns in token assignment that arise; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5420191d-b73d-4b4f-bcd7-4f762006605f_1342x1068.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5420191d-b73d-4b4f-bcd7-4f762006605f_1342x1068.png)

(from [5])

However, the MoE does exhibit some structured behavior. For example, the words “self” in Python code and “Question” in English—_even though they are comprised of multiple tokens_—often get routed through the same expert. Similarly, indentation tokens in code are usually sent to the same expert, and consecutive sequences—_just sequences of tokens that are close to each other_—generally are sent to the same expert; see below. These results indicate that _i)_ experts are not specialized by topic but _ii)_ the MoE’s routing mechanism does obey some structured behavior with respect to the syntax or contents of the model’s input.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f121afd-9afa-46cd-be0d-c1833a67a024_1340x864.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f121afd-9afa-46cd-be0d-c1833a67a024_1340x864.png)

(from [5])

**Scaling up.** After Mixtral, a larger version of the model was released, called [Mixtral-8×22B](https://mistral.ai/news/mixtral-8x22b/). This model has 141 billion total parameters and 39 billion active parameters, making it ~3× larger than the original Mixtral model. Mixtral-8×22B is especially capable on coding and mathematics tasks, has an expanded context length of 64K, and is natively capable of [function calling](https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools). The key benefits of Mixtral-8×22B compared to other open models are summarized below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe4dbfbd-b740-4458-84da-baa705537b5d_2084x1206.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe4dbfbd-b740-4458-84da-baa705537b5d_2084x1206.png)

([source](https://mistral.ai/news/mixtral-8x22b/))

#### [Grok](https://x.ai/blog/grok-os) (from [xAI](https://x.ai/)) [9]

Although there is no detailed technical report on the model, one of the most notable recent examples of MoE-based LLMs is xAI’s Grok. The initial Grok-1 model was released in early 2024. Researchers revealed that the model is a 314 billion parameter MoE with 25% of weights active for each token (i.e., ~70-80 billion active parameters). The architecture and [base model weights](https://huggingface.co/xai-org/grok-1) of Grok-1 were open-sourced under and Apache 2.0 license. However, this is a pretrained base model, and no details were provided on the model’s post training process.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9725fa0c-1631-4bc2-bf89-1b4417d60a10_1588x700.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9725fa0c-1631-4bc2-bf89-1b4417d60a10_1588x700.png)

(from [10])

**Grok-1.5 [10].** Shortly after the initial release of Grok-1[6](https://cameronrwolfe.substack.com/p/moe-llms#footnote-6-154340424), a follow-up version of the model was published with better reasoning and long context understanding capabilities. For example, Grok-1.5 performs much better on math and coding-related tasks; see above.

> _“The model can handle longer and more complex prompts, while still maintaining its instruction-following capability as its context window expands.”_ - from [10]

Grok-1.5 can process sequences up to 128K tokens with perfect retrieval on the [needle in a haystack test](https://github.com/gkamradt/LLMTest_NeedleInAHaystack); see below. Authors also note that the model maintains solid instruction-following capabilities when given a lot of context, which is a [much better sign](https://arxiv.org/abs/2406.10149) of long context capabilities compared to pure retrieval[7](https://cameronrwolfe.substack.com/p/moe-llms#footnote-7-154340424).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d250ac8-e79a-4b30-8bc9-31dac5624b38_1604x862.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d250ac8-e79a-4b30-8bc9-31dac5624b38_1604x862.png)

(from [10])

Given that Grok-1.5 and Grok-1 were released in such close succession, we can infer that the advancements made by Grok-1.5 are driven by post training—_it is extremely unlikely that a different pretrained base model was created during this time_.

**Grok-2.** More recently, [Grok-2](https://x.ai/blog/grok-2) was released, which has improved reasoning, coding, and chat—_as measured by [Chatbot Arena](https://arxiv.org/abs/2403.04132)—_capabilities. Grok-2 also has a variety of other small improvements (e.g., tool usage, retrieval, factuality, etc.) and a distilled version of Grok-2, called Grok-2-mini, was released with the main model. However, no public details were shared about the architecture of Grok-2—_the model was likely trained from scratch and may or may not be MoE-based_.

#### [DBRX](https://arxiv.org/abs/2403.04132) (from [Mosaic](https://www.databricks.com/research/mosaic)) [11]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb350e0ed-a8b6-44f7-806a-d581b785e74d_1934x1230.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb350e0ed-a8b6-44f7-806a-d581b785e74d_1934x1230.png)

(from [11])

DBRX is the latest model in the [series of Open LLMs](https://cameronrwolfe.substack.com/p/democratizing-ai-mosaicmls-impact) released by [Mosaic](https://www.databricks.com/research/mosaic). Two versions of the model were released—_a base model ([DBRX base](https://huggingface.co/databricks/dbrx-base)) and a finetuned model ([DBRX Instruct](https://huggingface.co/databricks/dbrx-instruct))_—under an open license (i.e., the [Databricks open model license](https://www.databricks.com/legal/open-model-license)). DBRX is an MoE-based LLM with the following specifications:

- 132 billion total parameters with 36 billion active parameters.
    
- 16 experts in each MoE layer with 4 experts active for each token.
    
- Pretrained on 12 trillion tokens of optimized text.
    
- 4× improvement in pretraining efficiency.
    

Most notably, DBRX is a “fine-grained” MoE model. In other words, the model uses a larger number of experts in each MoE layer, but each individual expert is smaller. For reference, both Mixtral and Grok-1 contain eight experts—_two of which are active for any given token_—within each of their MoE layers. By using fine-grained experts, each MoE layer has more expert combinations (65× more in particular) to choose from, which was found to improve quality in [11].

**Training data.** The pretraining dataset for DBRX is very large[8](https://cameronrwolfe.substack.com/p/moe-llms#footnote-8-154340424), but authors in [11] also invest significantly into improving the quality of the data. As a result, the statistical training efficiency of DBRX is higher than normal—_training is faster because we achieve higher accuracy with fewer tokens_. More specifically, authors in [11] estimate that the new data is 2× more efficient token-for-token, meaning that we can train over half as many tokens and achieve the same level of performance. This claim was verified by testing the impact of the new model’s pretraining data in isolation (i.e, using a fixed model with different pretraining data).

> _“In isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T tokens using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet compared to 30.9% for MPT-7B.”_ - from [11]

Additionally, curriculum learning is used to train DBRX—_the mixture of pretraining data is dynamically changed throughout the pretraining process_. The details of this curriculum learning strategy were later outlined in [this paper](https://arxiv.org/abs/2406.03476). The curriculum learning strategy used by DBRX just upsamples smaller, domain-specific datasets towards the end of training because this data is higher quality relative to data obtained via web-crawling. This simple curriculum learning strategy is found to provide a significant boost in performance on difficult benchmarks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56cc31c0-8721-416d-abcd-5cbf8e53c1d6_1596x704.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56cc31c0-8721-416d-abcd-5cbf8e53c1d6_1596x704.png)

([source](https://arxiv.org/abs/2406.03476))

**Tokenizer and context window.** DBRX has a context length of 32K and uses the GPT-4 tokenizer (available via [tiktoken](https://github.com/openai/tiktoken)). According to the authors, the GPT-4 tokenizer was selected mostly due to performance. This tokenizer has a large vocabulary and is very token efficient, which naturally improves decoding and training speed by representing the same amount of text with fewer tokens.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f7d600-d692-4c95-93db-4078c2c4ca7e_1970x638.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80f7d600-d692-4c95-93db-4078c2c4ca7e_1970x638.png)

(from [11])

**Efficiency wins.** The proposal of DBRX comes with large improvements in terms of pretraining efficiency. Beyond what we have learned about so far, there are several additional sources of efficiency gains mentioned in [11]:

- The MoE architecture, which is found in smaller-scale experiments to require 1.7× fewer FLOPS during training.
    
- Other modifications to the [decoder-only architecture](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) (i.e., [RoPE](https://cameronrwolfe.substack.com/i/142044446/better-positional-embeddings), [GLU activation](https://pytorch.org/docs/stable/generated/torch.nn.GLU.html), and [GQA](https://cameronrwolfe.substack.com/i/142044446/efficient-masked-self-attention)).
    
- _“Better optimization strategies”_.
    

When considering all data, architecture, and optimization changes, the end-to-end training process for DBRX requires 4× less compute when compared to the pretraining pipeline used for prior models. To determine this number, authors in [11] compare a smaller variants of DBRX to their prior [MPT-7B model](https://www.databricks.com/blog/mpt-7b), finding that the smaller DBRX models achieve similar performance on the [Databricks Gauntlet](https://github.com/mosaicml/llm-foundry) while using 3.7× fewer FLOPS during training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb50b0e9-7f1a-457c-acf9-b9c9f495620b_1960x1228.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb50b0e9-7f1a-457c-acf9-b9c9f495620b_1960x1228.png)

(from [21])

DBRX also comes with improvements to inference efficiency—_up to 2× faster than LLaMA-2-70B at 150 tokens per second per user in load tests_. These measurements are made using an [optimized serving infrastructure](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) with [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and 16 bit precision, which is [very fast](https://x.com/natolambert/status/1772999462538887493?s=20). The MoE architecture of DBRX also aids inference efficiency due to the relatively low number of active parameters. For example, DBRX is 40% of the size of Grok-1 in both total and active parameters.

> “Training mixture-of-experts models is hard. We had to overcome a variety of scientific and performance challenges to build a pipeline robust enough to repeatably train DBRX-class models in an efficient manner.” - from [11]

Training MoEs is generally difficult due to instabilities that arise during training, communication bottlenecks, and more. However, DBRX achieves impressive results in terms of stability, efficiency, and performance due to the optimized pretraining strategy outlined in [11]. Notably, there is not single change or advancement that enables these results. _The impressive pretraining pipeline used by DBRX is enabled by a large number of small, practical changes_.

**Empirical evaluation.** In comparison to other open LLMs, we see that DBRX-Instruct achieves better performance on composite benchmarks by a large margin when compared to Mixtral; see below. Despite being a general-purpose LLM, DBRX has impressive programming skills, outperforming Grok-1 (more than twice its size!) and even specialized coding models like [CodeLLaMA-70B](https://ai.meta.com/blog/code-llama-large-language-model-coding/). DBRX also performs well on reasoning and math-based tasks.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77fe464f-f391-4fc1-aa96-8649f07ca17e_1968x1182.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77fe464f-f391-4fc1-aa96-8649f07ca17e_1968x1182.png)

(from [11])

Compared to closed models, DBRX surpasses the performance of GPT-3.5 and is competitive with [Gemini-1.0 Pro](https://cameronrwolfe.substack.com/p/google-gemini-fact-or-fiction). Gemini-1.0 Pro only outperforms DBRX on [GSM8K](https://huggingface.co/datasets/gsm8k), while [Mixtral-Medium](https://docs.mistral.ai/guides/model-selection/#mistral-medium-intermediate-tasks-that-require-language-transformation) performs better on a select few tasks that are considered; see below. At a high level, DBRX seems to be good at programming, math, general knowledge, commonsense reasoning, and retrieval / [RAG](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6abdb5-8d58-475e-84f4-4e567bceae00_1886x968.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea6abdb5-8d58-475e-84f4-4e567bceae00_1886x968.png)

(from [21])

#### [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) [12]

Despite the success of MoEs in the language modeling domain, the number of truly open-source MoEs—_meaning that code, information, data, weights and more are all shared publicly_—is relatively low. To solve this issue, OpenMoE [12] conducts a large-scale effort to train a suite of decoder-only MoE LLMs ranging from 650 million to 34 billion parameters. These models adopt fine-grained experts with varying granularity (i.e., 16 or 32 experts). The findings from this effort are documented in [12] and all models are shared openly. The authors also provide a well-documented code repository that can be used to reproduce their results.

[OpenMoE Repo](https://github.com/XueFuzhao/OpenMoE)

> _“Using an MoE every layer introduces more computational overhead during routing and induces a worse cost-effectiveness trade-off than interleaved MoE usage.”_ - from [12]

**Design choices.** OpenMoE models adopt the settings of ST-MoE [3], including the same routing mechanism and number of active experts (i.e., `k = 2`). Authors choose to covert only every fourth or sixth transformer block to an MoE layer, finding that larger strides yield a better tradeoff in terms of cost and efficiency.

The pretraining dataset used for OpenMoE contains a heavy distribution of code. In fact, code comprised over 50% of the dataset during the early phases of pretraining, but this ratio was adjusted later in training due to being sub-optimal; see below. For alignment, OpenMoE undergoes SFT after pretraining—_using the data from [WildChat](https://arxiv.org/abs/2405.01470)_—to induce better instruction-following capabilities.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf1d6fd-5607-4b2a-8588-ae4005a35142_1356x744.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cf1d6fd-5607-4b2a-8588-ae4005a35142_1356x744.png)

(from [12])

**Routing dynamics.** One of the key contributions of OpenMoE is a detailed analysis of the routing decisions made within the models. First, we see that—_similarly to results shown in prior work [5]_—experts do not tend to specialize in any particular domain; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad376545-5625-4629-858c-f1c93eb9e618_1176x538.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad376545-5625-4629-858c-f1c93eb9e618_1176x538.png)

(from [12])

However, we do see some level of expert specialization across natural languages and specific tasks, as shown in the figure below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3a863d3-b6fb-460b-81a8-61225c286e11_952x526.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3a863d3-b6fb-460b-81a8-61225c286e11_952x526.png)

(from [12])

When we dig into this trend, however, we see that the dynamics of token routing are primarily dictated by the token ID. In other words, the same token will almost always be routed to the same expert, _no matter the context in which that token exists_. This pattern is referred to as “Context-Independent Specialization” in [12].

> _“This is a very interesting finding because the tokens with the same Token ID have very diverse contexts in different sentences. For instance, the token ‘an’ can also be part of ‘an apple’ or ‘another’. However, all these tokens have very strong specialization on only a few fixed experts.”_ - from [12]

Interestingly, experts have observable patterns in the tokens that they prefer; see below. For example, “have”, “has” and “had” are all routed to the same expert, while one expert receives the “=”, “and” and “\n” tokens—_very common tokens within coding languages_. We see in [12] that such routing patterns are solidified during the early stages of pretraining and rarely change later in training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c65074-8ede-4efa-a501-c897334cd9e2_974x372.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c65074-8ede-4efa-a501-c897334cd9e2_974x372.png)

(from [12])

**Routing issues.** Beyond the routing patterns observed in [12], we also see that OpenMoE models exhibit some routing behaviors that could damage their performance. For example, the models tend to drop tokens later in the sequence, which can damage performance on long sequence tasks (e.g., multi-turn chat).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97d05563-0260-40a7-9af0-9acd82d68fba_2106x908.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97d05563-0260-40a7-9af0-9acd82d68fba_2106x908.png)

OpenMoE models perform worse on multi-turn chat problems (from [12])

Because routing dynamics are fixed during the early phases of the pretraining process, these behaviors are hard to fix during post training. In fact, OpenMoE models are observed to generally struggle with the domain gap between data during pretraining and SFT[9](https://cameronrwolfe.substack.com/p/moe-llms#footnote-9-154340424)—_the token routing dynamics become sporadic due to the difference in data composition_. To solve these issues, authors in [12] recommend mixing instruction-following data into the pretraining dataset.

**Model evaluation.** Overall, OpenMoE models do not set new state-of-the-art performance among MoE LLMs—_authors in [12] openly state this fact and admit that the performance of OpenMoE models could be greatly improved through better design_. The larger contribution of OpenMoE models is their transparency. The details and artifacts publicly shared in [12] can accelerate open research efforts on MoEs by providing necessary resources to conduct further research on this topic.

#### [DeepSeek-v2](https://arxiv.org/abs/2405.04434) [14] and [DeepSeek-V3](https://arxiv.org/abs/2412.19437) [15]

The recently-proposed DeepSeek MoE models, including DeepSeek-v2 [14] and DeepSeek-v3 [15], have made waves within the LLM research community for a variety of reasons:

- Their weights are shared publicly.
    
- They come with technical reports that share many details.
    
- Their performance is impressive—_on par with many closed models_.
    
- Their training costs are pretty reasonable.
    

As we will see, the DeepSeek models make a variety of unique design choices that maximize both their training efficiency and downstream performance.

**DeepSeek-v2 [14]—**_a 236 billion parameter MoE with 21 billion active parameters_—proposes the MoE architecture used by the later DeepSeek-V3 model. The DeepSeek MoE models are a bit different than prior models, as they slightly modify the underlying transformer block to boost performance and efficiency. As shown below, the DeepSeek-v2 model—_in addition to performing well_—is quite impressive from a training and inference efficiency perspective, making it a strong starting point for the much larger DeepSeek-v3 model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76588c2-e5fe-48d7-9574-954a13db6c03_1542x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76588c2-e5fe-48d7-9574-954a13db6c03_1542x838.png)

(from [14])

**Multi-head latent attention (MLA).** Instead of standard, multi-headed attention, DeepSeek-v2 adopts MLA, which is an efficient attention variant. Similarly to [multi-query attention](https://arxiv.org/abs/1911.02150) or [grouped-query attention](https://arxiv.org/abs/2305.13245), MLA aims to minimize memory consumed by the model’s [KV cache](https://dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/). Unlike other efficient attention variants, however, MLA does not have a significant performance tradeoff.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39c9522f-3e36-42e9-98c8-3b580f913718_1170x488.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39c9522f-3e36-42e9-98c8-3b580f913718_1170x488.png)

(from [14])

In particular, this gain in memory efficiency is achieved via a low-rank, joint projection that allows us to represent all key and value vectors with a much smaller (latent) vector; see above. We can upsample this vector—_just linearly project it to form several, larger vectors_—to restore the full key and value vectors, but we only have to store the latent vector in our KV cache, thus drastically reducing memory consumption. _Adopting MLA decreases the size of DeepSeek-v2’s KV cache by over 93% compared to a 67 billion parameter dense model_.

**DeepSeek MoE architecture.** Beyond using MLA, DeepSeek models adopt a unique MoE layer structure. Similar to DBRX, these models use fine-grained experts. However, a subset of these experts are shared. The motivation for adopting such a structure is to encourage specialization among a larger number of experts while minimizing redundant information between experts. A full schematic of the block structure used by DeepSeek models is provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2bf05fc-8703-4bf5-9610-edce7c6a6a91_1534x1384.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2bf05fc-8703-4bf5-9610-edce7c6a6a91_1534x1384.png)

(from [14])

Authors in [14] also adopt an interesting load balancing strategy for handling the fine-grained experts used by DeepSeek-v2. In addition to using the auxiliary load balancing loss proposed in [2], DeepSeek-v2 has two auxiliary loss terms that aim to balance communication between devices during distributed training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bf7a65d-5541-4b40-ad45-1ab02529544d_1340x954.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bf7a65d-5541-4b40-ad45-1ab02529544d_1340x954.png)

Device-level load balancing auxiliary loss (from [14])

Using fine-grained experts means that we must dispatch each token to a larger number of experts. In a distributed training setting, experts may be on different devices and multiple experts reside on each device. To ensure communication and computation are balanced between devices, we need additional auxiliary losses that _i)_ group experts by the device on which they reside and _ii)_ encourage the MoE to perform balanced routing on a per-device basis. For example, the auxiliary loss shown above encourages balanced _computation_ among devices. There is an extra loss proposed in [14] to encourage balanced _communication_ among devices as well.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)

(from [15])

**DeepSeek-v3 [15]** is a much larger version of DeepSeek-v2[10](https://cameronrwolfe.substack.com/p/moe-llms#footnote-10-154340424), having 671 billion total parameters and 37B active parameters. This larger model is pretrained on a massive corpus comprised of 14.8 trillion tokens. After pretraining, a multi-phase post training pipeline is applied:

- The model first undergoes a two-stage context extension procedure in which it is finetuned (via SFT) to have a maximum context length of 32K, then further finetuned to have a context length of 128K.
    
- After context extension, the model undergoes further SFT and RLHF to align it to human preferences.
    
- Capabilities from the recently-proposed [R1 reasoning model](https://arxiv.org/abs/2501.12948) are also distilled into DeepSeek-v2 during post training.
    

The final DeepSeek-v3 model outperforms closed-source models and achieves similar performance to even the best closed LLMs; see above. DeepSeek-v3 also makes several modifications to the training and load balancing strategy for the MoE, leading the model’s training process to be both efficient and stable.

> _“Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable… we did not experience any irrecoverable loss spikes or perform any rollbacks.”_ - from [15]

The architecture of DeepSeek-v3 is inspired by its predecessor; e.g., MLA, fine-grained experts, and shared experts are all used by DeepSeek-v3. Unlike DeepSeek-v2, however, DeepSeek-v3 uses a **Multi-Token Prediction (MTP)** training objective. This objective is an extension of the supervised, cross entropy-based [next token prediction objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) that is used almost universally for training LLMs. Instead of predicting the next token for each token within a sequence, MTP predicts `D` future tokens. These predictions are made sequentially by a set of additional modules that are added to the model’s architecture; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ac297b7-3d37-46ec-9b30-f45d2560bb04_1846x964.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ac297b7-3d37-46ec-9b30-f45d2560bb04_1846x964.png)

(from [15])

Once several future tokens have been predicted, we can apply the cross-entropy loss normally. Applying this loss over several future tokens predicted via MTP provides a richer training signal to the model, which improves training efficiency and overall performance. Going further, these additional modules used for MTP can also be used to improve inference efficiency via [speculative decoding](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/). However, authors in [15] state that the MTP strategy is used purely to benefit model performance—_additional modules are discarded after training_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa69f7cc-41ac-4b4f-9a13-c7b791a31430_1836x480.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa69f7cc-41ac-4b4f-9a13-c7b791a31430_1836x480.png)

Auxiliary-loss-free load balancing strategy (from [15])

An **auxiliary-loss-free load balancing** strategy is also used by DeepSeek-v3 that simply adds a per-expert bias term to the selection of Top-`K` experts; see above. At each iteration, the bias term for each expert is either increased or decreased by a fixed factor `γ` based upon whether that expert was underloaded or overloaded, respectively. Importantly, these biases are only used when selecting the top-`K` experts—_they do not impact the computation of expert probability within the router_. This approach is found to effectively balance expert utilization within the MoE and eliminate performance deterioration due to the use of load balancing losses. However, authors in [15] do note that they still use an auxiliary load balancing loss (with a very low scaling factor) when training DeepSeek-v3.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff45514-375d-4e7d-ab22-f95285b865a4_1834x336.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff45514-375d-4e7d-ab22-f95285b865a4_1834x336.png)

(from [15])

**Training efficiency.** Due to the efficiency and performance benefits of the strategies outlined above, DeepSeek-v3 is incredibly economical. Plus, the model is trained using a novel FP8 mixed precision training framework, _marking the first validation of 8-bit training for large-scale LLMs_. In total, training the final model was estimated[11](https://cameronrwolfe.substack.com/p/moe-llms#footnote-11-154340424) to cost ~$5.6M; see above. In short, the DeepSeek-v3 is:

- Trained in a very economical fashion (and with several novel advancements like FP8 training and MTP!)
    
- Incredibly impressive for an open model—_highly competitive with even the best closed LLMs_.
    
- Based on an interesting MoE architecture with several novel modifications.
    

**Reasoning.** DeepSeek-v3 also serves as a base model for [DeepSeek-R1](https://arxiv.org/abs/2501.12948) [13], a recently-released open reasoning model. Put simply, R1 is an open replication of the [o1-style of models](https://openai.com/index/learning-to-reason-with-llms/) that have been recently explored by OpenAI. As explained in its detailed technical report, this model uses pure reinforcement learning to learn how to solve complex (verifiable) reasoning tasks by crafting extremely long chains of thought. As shown in the figure below, the performance of R1 is quite impressive, especially for an open model. However, the capabilities of R1 would not be possible without first having access to an incredibly-capable base model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97f49231-0329-4f2e-ab77-a274906e27ae_1948x1258.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97f49231-0329-4f2e-ab77-a274906e27ae_1948x1258.png)

(from [13])

## Final Thoughts

MoEs have many benefits that are especially suited to language modeling. They enable exploration of larger scales without drastic increases in compute, reduce training costs, and can be efficiently hosted. Although the idea of sparsity has existed for a long time within the machine learning literature, MoEs are an especially impactful instantiation of sparsity. They leverage sparsity in a manner that is compatible with modern hardware and can be practically implemented on a GPU. Interestingly, early MoE variants struggled with adoption due to their complexity, instability, and difficulty of use. However, the advancements we have seen in this overview have turned the MoE into something practical and impactful—_a simple and promising extension of the decoder-only transformer architecture_.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." arXiv preprint arXiv:1701.06538 (2017).

[2] Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.

[3] Zoph, Barret, et al. "St-moe: Designing stable and transferable sparse expert models." arXiv preprint arXiv:2202.08906 (2022).

[5] Jiang, Albert Q., et al. "Mixtral of experts." arXiv preprint arXiv:2401.04088 (2024).

[6] Jiang, Albert Q., et al. "Mistral 7B." arXiv preprint arXiv:2310.06825 (2023).

[7] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." arXiv preprint arXiv:2305.13245 (2023).

[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "Longformer: The long-document transformer." _arXiv preprint arXiv:2004.05150_ (2020).

[9] xAI. “Open Release of Grok-1” _[https://x.ai/blog/grok-os](https://x.ai/blog/grok-os)_ (2024).

[10] xAI. “Announcing Grok-1.5” _[https://x.ai/blog/grok-1.5](https://x.ai/blog/grok-1.5)_ (2024).

[11] Mosaic Research (Databricks). “Introducing DBRX: A New State-of-the-Art Open LLM” _[https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)_ (2024).

[12] Xue, Fuzhao, et al. "Openmoe: An early effort on open mixture-of-experts language models." _arXiv preprint arXiv:2402.01739_ (2024).

[13] Guo, Daya, et al. "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning." _arXiv preprint arXiv:2501.12948_ (2025).

[14] Liu, Aixin, et al. "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model." _arXiv preprint arXiv:2405.04434_ (2024).

[15] Liu, Aixin, et al. "Deepseek-v3 technical report." _arXiv preprint arXiv:2412.19437_ (2024).

[1](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-1-154340424)

In other words, the contents of this embedding layer are updated (via gradient descent) throughout the training process, similarly to any other model parameter.

[2](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-2-154340424)

To improve both hardware (i.e., GPU utilization, throughput, etc.) and statistical (i.e., how quickly the model learns from data) efficiency, we want to have a relatively uniform number of tokens dispatched to all experts in each batch of data.

[3](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-3-154340424)

This quantity is predicted by our routing algorithm and is, therefore, differentiable. So, the loss function as a whole is differentiable even though the fraction of tokens sent to each expert is not itself a differentiable quantity.

[4](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-4-154340424)

We also have to multiple this loss term by `N` to ensure that the loss remains constant as the number of experts is increased.

[5](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-5-154340424)

It may seem like SWA limits each token to only “look at” a few tokens within a sequence. However, if we stack `k` consecutive layers of SWA on top of each other, the effective context window for each token increases. The representation of the current token is actually influenced by the `k⋅W` tokens that come before it.

[6](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-6-154340424)

Both of these models were released in March of 2024 within ~10 days of each other!

[7](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-7-154340424)

The needle in a haystack test tests an LLM’s ability to retrieve information in its context. However, the model may still have bad long context abilities even if it scores perfectly on the needle in a haystack test; e.g., instruction-following or reasoning capabilities could get way worse when given a long context.

[8](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-8-154340424)

For reference, the prior models released by the same team—_[MPT-7B](https://www.databricks.com/blog/mpt-7b) and [MPT-30B](https://www.databricks.com/blog/mpt-30b)_—were pretrained on only 1 trillion tokens of text.

[9](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-9-154340424)

We encounter much different styles of data during SFT compared to pretraining; e.g., multi-turn chat data, instruction templates, and more.

[10](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-10-154340424)

Between these models, DeepSeek also released an intermediate model, called DeepSeek-v2.5; see [here](https://api-docs.deepseek.com/news/news1210) for details.

[11](https://cameronrwolfe.substack.com/p/moe-llms#footnote-anchor-11-154340424)

These estimates are made by assuming a $2 rental price per H800 GPU hour. Additionally, they reflect the pure compute cost of training the final model only, excluding any supplemental costs or experiments. The actual total cost of training DeepSeek-v3 is [undoubtedly much larger](https://www.interconnects.ai/p/deepseek-v3-and-the-actual-cost-of) than this reported number.

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![DTP's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2044f02e-eca7-485a-bb6b-0475ec226f22_144x144.png)



](https://substack.com/profile/147708802-dtp)

[

![Borja Esteve's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faee2ff67-56b7-437b-a139-aff4a737b79c_144x144.png)



](https://substack.com/profile/133165009-borja-esteve)

[

![Umar Abdullah's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10228b0e-fcf9-4992-a2e1-0f62749304ad_96x96.png)



](https://substack.com/profile/23235761-umar-abdullah)

[

![Roy's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae6e9ada-53cc-470a-b369-8097c2e996a6_1124x1125.jpeg)



](https://substack.com/profile/6200894-roy)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

214 Likes∙

[24 Restacks](https://substack.com/note/p-154340424/restacks?utm_source=substack&utm_content=facepile-restacks)

214

- 

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

24

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Dr. Ashish Bamania's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff41b7f65-55d7-4099-969a-931c2ddd2f5f_612x612.png)



](https://substack.com/profile/155457308-dr-ashish-bamania?utm_source=comment)

[Dr. Ashish Bamania](https://substack.com/profile/155457308-dr-ashish-bamania?utm_source=substack-feed-item)

[Into AI](https://intoai.pub/?utm_content=comment_metadata&utm_source=substack-feed-item)

[1月27日](https://cameronrwolfe.substack.com/p/moe-llms/comment/88959949 "2025年1月27日 18:44")

Liked by Cameron R. Wolfe, Ph.D.

Great deep dive and very helpful. Thanks for writing this!

Like (4)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/moe-llms/comment/88959949)

[

![DesignREM support's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dc0bea2-977e-44d1-8fb9-df75f3aa41e8_144x144.png)



](https://substack.com/profile/5640296-designrem-support?utm_source=comment)

[DesignREM support](https://substack.com/profile/5640296-designrem-support?utm_source=substack-feed-item)

[DesignREM’s Substack](https://nvg291.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[1月28日](https://cameronrwolfe.substack.com/p/moe-llms/comment/89187897 "2025年1月28日 15:39")

Liked by Cameron R. Wolfe, Ph.D.

Amazing article, a true gem. I would be interested to learn more about how DeepSeek used reinforcement learning to such great effect.

Like (2)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/moe-llms/comment/89187897)

[8 more comments...](https://cameronrwolfe.substack.com/p/moe-llms/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

[Understanding how SFT works from the idea to a working implementation...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Sep 11, 2023 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

55

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68686a01-2b31-4694-8c04-a562ffd725ad_2210x1244.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Demystifying Reasoning Models

### Understanding reasoning models and their relation to standard LLMs...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Feb 18, 2025

204

- 

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

23

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4fb1867-b78e-4db6-aea7-14251a3facce_2389x1336.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4fb1867-b78e-4db6-aea7-14251a3facce_2389x1336.png)

(from [4, 13, 22])

For the last several years, we have used a relatively fixed pipeline for training large language models (LLMs); see below. First, we pretrain these language models over raw textual data from the internet. Afterwards, we align them—_or train them to produce outputs that are preferable to humans_—using a combination of [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). Both pretraining and alignment play a key role in model quality, but a large majority of advancements in this paradigm have been driven by [LLM scaling laws](https://cameronrwolfe.substack.com/p/llm-scaling-laws)—_we get better results by pretraining larger models on more data_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac82c7c1-fcbd-4b32-b9cd-febfadd77c19_1720x562.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac82c7c1-fcbd-4b32-b9cd-febfadd77c19_1720x562.png)

Training pipeline for a standard LLM

Recently, a completely new paradigm in LLM research has emerged: _reasoning_. Reasoning models approach problem solving in a completely different manner compared to standard LLMs. In particular, they spend a variable amount of time “thinking” prior to providing their final answer to a question. Training models that are able to think effectively (e.g., decompose problems, detect errors in their thinking, explore alternative solutions and more) requires new strategies, usually involving large-scale reinforcement learning (RL). Additionally, such models give rise to new forms of scaling laws for training via RL and inference; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88a91669-f7f0-41aa-b0f0-78392da2115a_1254x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88a91669-f7f0-41aa-b0f0-78392da2115a_1254x804.png)

(from [4])

In this overview, we will learn more about recent advancements in reasoning models. To start, we will focus on several (closed) reasoning models that were proposed first by OpenAI. We will contextualize the explanation of these models with the fundamental ideas that underlie LLM reasoning capabilities. Afterwards, we will explore recently-proposed (open) reasoning models, outlining necessary details for creating such a model from scratch. Reasoning models are different from standard LLMs. But, don’t worry. A lot of the key concepts of LLMs still apply to reasoning models. _We will clarify important distinctions throughout._

## The Age of Reasoning

Just as AI progress was seemingly [starting to slow down](https://cameronrwolfe.substack.com/p/llm-scaling-laws), we witnessed a sudden and significant improvement in LLM capabilities with the popularization of [reasoning models](https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html). First to be released was OpenAI’s [o1-preview](https://openai.com/index/introducing-openai-o1-preview/) [4], followed by a series of distilled (i.e., smaller) models like o1-mini and later model variants like [o3](https://openai.com/index/openai-o3-mini/) [6]. In response, other companies released similar reasoning models, such as [Google’s Gemini 2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/). In this section, we will explore these initial, closed reasoning models and the basic ideas behind how they work.

#### Initial Reasoning Models: o1 and o1-mini

> _“We've developed a new series of AI models designed to spend more time thinking before they respond.”_ - from [4]

The release of **o1-preview** [4, 5] by OpenAI made two things very clear:

1. Reasoning models can solve verifiable tasks—_such as math and coding tasks_—very accurately.
    
2. The approach taken by reasoning models to solve these problems is very different from that of a traditional LLM.
    

**Long CoT.** The main difference between a reasoning model and a standard LLM is the ability to “think” before answering a question. The reasoning model’s thoughts are just long chains of thought—_or_ _long CoT for short, sometimes referred to as a reasoning trace or trajectory_—outputted by the LLM. This long CoT is generated no differently than any other sequence of text. However, these reasoning trajectories exhibit very interesting properties that are more akin to search algorithms than vanilla text generation. For example, the model will:

- Think through each part of a complex problem.
    
- Decompose complex problems into smaller, solvable parts.
    
- Critique its own (partial) solutions and find errors.
    
- Explore many alternative solutions.
    

For some concrete examples of these reasoning trajectories, see [this blog post](https://openai.com/index/learning-to-reason-with-llms/). Notably, the long CoT used by OpenAI’s reasoning models are “internal”, meaning that they are hidden from the user when interacting with the model. Instead, the user sees a model-written summary of the long CoT; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c08cfd9-85a6-4079-b510-59857ae05c3e_1970x1174.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c08cfd9-85a6-4079-b510-59857ae05c3e_1970x1174.png)

([source](https://openai.com/index/learning-to-reason-with-llms/))

The long CoT output of reasoning models gives us an easy way to control the inference-time compute of an LLM. If we want to spend more compute on solving a problem, we can simply generate a longer CoT. Similarly, less complex problems can be solved with a shorter CoT, thus saving compute at inference time.

**Reasoning capabilities.** Initial reasoning models were actually less capable than standard LLMs in many ways[1](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-1-153722335), but they improve the reasoning capabilities of an LLM by several orders of magnitude. For example, _o1-preview unanimously outperforms GPT-4o and even rivals the performance of human experts on most complex reasoning tasks_; see below. To achieve these results, o1-preview is evaluated using maximal inference-time compute[2](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-2-153722335) and either _i)_ a single output sample (solid bar) or _ii)_ a majority vote among 64 parallel output samples (shaded bar).

[

![Competition evals for Math (AIME 2024), Code (CodeForces), and PhD-Level Science Questions (GPQA Diamond)](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde143ac3-dbf4-476c-9524-282b23c1034c_2700x1050.png "Competition evals for Math (AIME 2024), Code (CodeForces), and PhD-Level Science Questions (GPQA Diamond)")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde143ac3-dbf4-476c-9524-282b23c1034c_2700x1050.png)

o1 models vs. GPT-4o on reasoning tasks (from [5])

Beyond o1-preview, **OpenAI’s o1**—_the full version of o1 that was released a few months after the preview_—places among the top 500 students in the US on the math olympiad qualification exam ([AIME 2024](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid=AfmBOopg_BQh_GIwm9fLXXJSK812QdJcW_e6uohok7JzFaFCbie0twRk)) and ranks within the 11th percentile of competitive human programmers on [Codeforces](https://arxiv.org/abs/2501.01257). For reference, GPT-4o only solved 12% of AIME problems, while o1 solves anywhere from 74% to 93% of the problems depending upon inference settings. See the figure below for a more detailed comparison between the performance of o1 and GPT-4o.

[

![Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd030dac8-57ff-4d51-a8a5-7bbbec5fc3ba_2400x1650.png "Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd030dac8-57ff-4d51-a8a5-7bbbec5fc3ba_2400x1650.png)

Improvement of o1 over GPT-4o (from [5])

Similarly, **o1-mini**—_a cheaper and faster version of o1_—has impressive reasoning capabilities despite its 80% cost reduction relative to the full o1 model. This model, despite having limited world knowledge compared to o1, is especially capable at coding tasks and performs very well given its efficiency.

#### State-of-the-Art Reasoning Models: o3 and o3-mini

[

![o Series Performance](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeccad4f-894f-4593-9573-ff3285420af7_1200x675.jpeg "o Series Performance")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeccad4f-894f-4593-9573-ff3285420af7_1200x675.jpeg)

Performance of OpenAI’s o3 on ARC-AGI ([source](https://arcprize.org/blog/oai-o3-pub-breakthrough))

Shortly after the announcement and release of o1 models, OpenAI announced **o3**—_the most recent model in the o1 lineage_. This model was initially just announced (not released). We were able to see the model’s performance on several notable benchmarks—_as measured by OpenAI_—but could not actually use the model. The metrics released by OpenAI were very impressive. In fact, _the performance of o3 was quite shocking to many people_. The most notable achievements of o3 are:

- A score of 87.5% on the [ARC-AGI benchmark](https://arcprize.org/blog/oai-o3-pub-breakthrough)—_the “North Star” towards AGI that was left unbeaten[3](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-3-153722335) for five years_—on which GPT-4o achieves 5% accuracy. o3 is the first model to exceed human-level performance of 85% on ARC-AGI.
    
- An accuracy of 71.7% on [SWE-Bench Verified](https://openai.com/index/introducing-swe-bench-verified/) and an [Elo score](https://en.wikipedia.org/wiki/Elo_rating_system) of 2727 on Codeforces, _ranking o3 among the top 200 competitive programmers on the planet_.
    
- An accuracy of 25.2% on EpochAI’s [FrontierMath benchmark](https://epoch.ai/frontiermath), _improving upon the previous state-of-the-art accuracy of 2.0%_[4](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-4-153722335).
    

However, the public did not have access to the o3 model to verify any of these results. The full o3 model still has yet to be released at the time of writing, but OpenAI did recently release a smaller version of the model—_**o3-mini**_ [6].

> _“Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.”_ - from [6]

Compared to other reasoning models from OpenAI, o3-mini is more cost effective and production-ready. For example, this model supports features like function calling, web search and structured outputs[5](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-5-153722335). o3-mini also has multiple settings—_including low, medium and high effort_—for the amount of reasoning that it performs when solving a problem. This setting can be directly specified in the API request, and the model performs very impressively—_on par with o1 in many cases_—depending on the level of reasoning effort; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F809e35bd-3da6-4382-8635-dcff356f25c0_2424x1332.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F809e35bd-3da6-4382-8635-dcff356f25c0_2424x1332.png)

o3-mini performance breakdown (from [6])

In most cases, o3-mini with low reasoning effort matches the performance of o1-mini, while o3-mini with high reasoning effort exceeds the performance of all other reasoning models released by OpenAI (including the full o1 model).

o3-mini also has better world knowledge (i.e., improved factuality), is noticeably more efficient, and scores higher in human preference studies compared to prior reasoning models; see below. In particular, authors in [6] mention that during internal A/B tests _“o3-mini delivered responses 24% faster than o1-mini, with an average response time of 7.7 seconds compared to 10.16 seconds.”_ o3-mini is the most efficient model released (so far) of OpenAI’s o1-style reasoning models.

[

![The chart compares win rates for STEM and non-STEM tasks across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in both categories, with a higher win rate for STEM tasks.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044cb648-2c4d-4aaa-88bb-bf4548876d24_1944x994.webp "The chart compares win rates for STEM and non-STEM tasks across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in both categories, with a higher win rate for STEM tasks.")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044cb648-2c4d-4aaa-88bb-bf4548876d24_1944x994.webp)

Win-rate of o3-mini vs. o1-mini on STEM / non-STEM prompts (from [6])

**Other model providers.** The release of o1-style models by OpenAI was quickly followed by other model providers. For example, Google recently released the experimental [Gemini-2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/), which maintains the signature long context of Gemini models—_1M token context window_—and achieves respectable metrics on key verifiable tasks (e.g., AIME and GPQA). However, _this model still lags behind the performance of o1 and o3-mini_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78afa03-d704-43f4-b001-3965969a3b84_1070x556.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78afa03-d704-43f4-b001-3965969a3b84_1070x556.png)

([source](https://deepmind.google/technologies/gemini/flash-thinking/))

Very recently, a reasoning beta was announced for Grok-3 that is very compelling. As shown below, the Grok-3 reasoning model exceeds the performance of o3-mini with high reasoning efforts and even comes close to matching the full o3 model in a few cases; e.g., 96% accuracy on AIME’24, compared to the 97% accuracy of o3. Grok-3, which was trained using a [massive new compute cluster](https://www.datacenterfrontier.com/machine-learning/article/55244139/the-colossus-ai-supercomputer-elon-musks-drive-toward-data-center-ai-technology-domination), is impressive (especially given the youth of xAI). At the time of writing, the reasoning beta of Grok-3 is the closest competitor to reasoning models from OpenAI.

[

![r/singularity - Grok 3 Reasoning Benchmarks](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bc6bd5-d713-4c5e-9740-9a5e3ec81923_640x318.png "r/singularity - Grok 3 Reasoning Benchmarks")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bc6bd5-d713-4c5e-9740-9a5e3ec81923_640x318.png)

(from Grok-3 announcement video on X)

#### Benchmarks for Reasoning Models

> _“Recent frontier models do so well on MATH and GSM8K that these benchmarks are no longer effective at differentiating models.”_ - from [5]

Before learning more about how reasoning models work, let’s take a deeper look at their performance. To truly understand the capabilities of these models, we need to do more than just look at metrics—_we need to inspect concrete examples of the problems that these models are solving_. For example, consider [GSM8K](https://arxiv.org/abs/2110.14168) (shown below), a grade-school level math benchmark. These questions might seem trivial, but LLMs struggled to accurately solve this benchmark for [several years](https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87c06563-9df0-4cd4-8e8b-62acf408ffce_2300x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87c06563-9df0-4cd4-8e8b-62acf408ffce_2300x838.png)

Example questions from GSM8K ([source](https://huggingface.co/datasets/openai/gsm8k))

With the advent of reasoning models, this benchmark has been completely saturated—_we can no longer use it to meaningfully evaluate the best reasoning models_. Instead, we are beginning to solve much harder problems with LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95dc2906-5bef-4d7a-a234-5e833d189ba1_1900x248.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95dc2906-5bef-4d7a-a234-5e833d189ba1_1900x248.png)

Example problem from AIME 2024 ([source](https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems))

For example, consider the [15th problem from AIME 2024](https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_15), as shown above. This problem is quite complex and goes beyond the arithmetic reasoning questions found in GSM8K. There are (at least) six different ways that this problem can be solved, all of which require knowledge of advanced mathematical techniques (e.g., derivatives, [number theory](https://en.wikipedia.org/wiki/Number_theory) or [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)).

Additionally, the complex benchmarks being solved by reasoning models go beyond math! For example, GPQA [7] contains hundreds of multiple-choice questions from several scientific domains; e.g., Biology, Physics, and Chemistry. All of these questions are written by domain experts and verified to be both very difficult and “Google-proof”, meaning that non-experts struggle to solve these problems even when given sufficient time and unrestricted internet access.

> _“We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy, while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web.”_ - from [7]

The ARC-AGI benchmark—_described as a “material stepping stone toward AGI”_—involves a variety of grid-based puzzles in which the LLM must learn patterns among input-output grids and perfectly replicate this learned pattern on a final output example; see below. Most LLMs struggle to solve these puzzles (e.g., GPT-4o achieves an accuracy of only 5%), but reasoning models perform quite well on this benchmark—_30-90% accuracy depending on the compute budget_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb2e0506-6107-4e23-8ef5-3e0f4bb1e6e8_1538x1062.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb2e0506-6107-4e23-8ef5-3e0f4bb1e6e8_1538x1062.png)

To say the least, _these are a different caliber of (non-trivial) problems that reasoning LLMs are beginning to solve_. Despite the difficulty of these benchmarks, modern reasoning models are found to be remarkably capable—_OpenAI’s o3 model is reported to achieve a score of nearly 97% on AIME 2024_. After manually inspecting some of these questions, we can truly understand the gravity of this result.

## Fundamentals of Reasoning Models

> “_We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute).”_ - from [1]

Although the reasoning models presented above are clearly impressive, there are all closed models. So, _we have no information about how they actually work_. The only information we are given is the above quote and the plot shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fe00c0c-da10-431b-8316-4ea3939e50fe_1264x645.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fe00c0c-da10-431b-8316-4ea3939e50fe_1264x645.png)

(from [5])

From this limited information, however, we can draw some useful conclusions. Mainly, there are two key components involved in scaling a reasoning model:

- More training via RL.
    
- More inference-time compute (i.e., inference-time scaling).
    

Although OpenAI does not reveal many of the details behind their approach to scaling these two components of a reasoning model, there is still [a lot of research](https://github.com/srush/awesome-o1) that has been published on this topic. To provide more context, let’s briefly take a look at some of this work—_along with details shared by OpenAI_—to outline some of the key concepts that underlie how reasoning models are trained and used.

#### Reinforcement Learning with Verifiable Rewards

One detail that we should immediately notice about o1-style models is that they are primarily used for and evaluated on problems that are verifiable in nature; e.g., math and coding. But, _what exactly does “verifiable” mean in this context?_ First, we assume that we have access to either _i)_ a ground truth answer for the problem or _ii)_ some rules-based technique that can be used to verify correctness.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb865992-1eee-4fdb-b98a-165f4d555e11_1774x608.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb865992-1eee-4fdb-b98a-165f4d555e11_1774x608.png)

Verifying a math problem via exact string match

For example, we can define a ground truth final answer for most math problem—this is done in [GSM8K](https://huggingface.co/datasets/openai/gsm8k) with the `#### <answer>` syntax. Then, we can extract the final answer from the LLM’s output and compare this answer to the ground truth using a basic string match; see above. Similarly, if we have test cases prepared for a coding question, we can simply execute the code produced by our LLM and check whether the provided solution satisfies all of the test cases.

> _“Reinforcement Learning with Verifiable Rewards (RLVR) can be seen as a simplified form of existing approaches for bootstrapping LM reasoning or a simpler form of RL with execution feedback, in which we simply use answer matching or constraint verification as a binary signal to train the model.”_ - from [13]

Saying that a domain is “verifiable” does NOT mean that we can automatically verify arbitrary solutions to problems in this domain. Rather, we will often need access to ground truth answers—_typically obtained from humans_—for verification.

However, there are some behaviors that can be verified using simple rules instead of ground truth. For example, we can determine whether a reasoning model has the correct output format, follows certain instructions, or produces outputs of a particular length (e.g., the low, medium or high reasoning effort used by o3-mini) by performing simple checks with a set of hard-coded rules.

**Verification complexities.** Verifying an LLM’s output can become quite complex depending on the problems we are solving. Even for math problems, verifying a match between the LLM’s answer and ground truth is difficult. For example, the solution may be presented in a different form or format, leading to false negative verifications. In these cases, simple string matching may not be enough! Instead, we can prompt an LLM to tell us whether the two solutions are a match or not, which has been found to drastically reduce incorrect verifications [14]. For code, implementing verification is tough as well—_it requires constructing a data pipeline that can very efficiently execute and verify test cases within our training setup_.

> _“We do not apply neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale RL process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.”_ - from [1]

**Neural verification.** Beyond the verifiable problems outlined above, we can also consider weaker forms of verification. For example, creative writing is a task that is difficult to verify. However, we can:

1. Train a [neural reward model](https://arxiv.org/abs/2403.13787) or verifier.
    
2. Score our LLM’s output with this model.
    
3. Use the predicted score as a reward or verification signal.
    

Such a setup is very similar to [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). In this case, we are training our reward model to perform binary verification based on the correctness or quality of the model’s response[6](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-6-153722335). However, using a neural verifier comes with the risk of [reward hacking](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/), especially when performing large-scale RL. The model is trained for longer and does much more exploring of the reward landscape, thus increasing the risk of reward hacking. As a result, many recent reasoning models have avoided this approach.

**Learning from verifiable rewards.** We now understand verification, but how can verification be used to train an LLM? The idea here is simple: _we just directly use the verification result as a reward signal for training with RL_; see below. There are many different ways of implementing this idea (e.g., [process rewards](https://arxiv.org/abs/2305.20050) or [pure RL](https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop)), but they share the common theme of using RL to learn from verifiable rewards. _This is the fundamental concept upon which all modern reasoning models are based_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7334cdb5-5398-47d2-98bb-01ca41a58879_1854x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7334cdb5-5398-47d2-98bb-01ca41a58879_1854x726.png)

(from [13])

For a complete exposition of methods that can be used to learn from verifiable rewards with RL, check out the incredible video by [Sasha Rush](https://rush-nlp.com/) below.

#### Inference-Time Strategies: Chain of Thought and Decoding

There are two basic ways[7](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-7-153722335) that we can increase the amount of compute that our language model is consuming at inference time:

- Generate more tokens (i.e., longer output sequence).
    
- Generate multiple outputs.
    

In this section, we will go into these techniques in more detail, exploring how they are practically implemented in LLMs via chains of thought and different decoding strategies; e.g., parallel versus sequential decoding.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [8])

**Chain of thought.** We already know that reasoning models use long CoT as their medium for reasoning. Proposed in [8], a chain of thought—_at the simplest level_—is just an explanation that an LLM provides for its own output. In most cases, these explanations are written prior to the LLM generating its final answer, allowing the model to use its explanation as context when generating its answer; see above.

The long CoT used by reasoning models is much different than a standard CoT. A standard CoT is concise and human-readable. A long CoT is several thousand tokens long[8](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-8-153722335). Although it can be used for interpretability purposes, the long CoT is not optimized for human readability. Rather, it is an extensive reasoning trace that approaches problem solving in a detailed manner and contains a variety of complex reasoning behaviors (e.g., backtracking and self-refinement).

> _“We have decided not to show the raw chains of thought to users… We strive to partially make up for [this decision] by teaching the model to reproduce useful ideas from the chain of thought in the answer. For the o1 model series we show a model-generated summary of the chain of thought.”_ - from [5]

Additionally, reasoning models logically separate their CoT from the final output of the model. For example, OpenAI avoids exposing the long CoT directly to users and instead provides an LLM-generated summary of the long CoT to supplement the reasoning model’s final answer. Such a logical separation is fundamentally necessary due to the length of CoT. Most users will only read the final answer—_reading the entire reasoning trace would be incredibly time consuming_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7b26d4a-0d1c-4e27-a63d-5fe7035e83b1_604x278.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7b26d4a-0d1c-4e27-a63d-5fe7035e83b1_604x278.png)

(from [15])

**Parallel decoding.** To improve the accuracy of an LLM’s final output, we may also use parallel decoding techniques; see above. The idea here is simple: _instead of generating a single output with our LLM, we generate multiple outputs and aggregate these outputs to form a single, final answer_. This aggregation can be done in many ways; e.g., using [majority vote](https://arxiv.org/abs/2203.11171) or consensus, using [weighted voting](https://arxiv.org/abs/2206.02336), identifying the best output(s) with a [neural reward model or verifier](https://arxiv.org/abs/2408.15240) (i.e., also known as [Best-of-N or rejection sampling](https://arxiv.org/abs/2110.14168)), or [other domain-specific algorithms](https://arxiv.org/abs/2210.02441).

The main benefit of these approaches is their simplicity and effectiveness. Scaling up parallel decoding is easy—_we just generate, verify and aggregate a larger number of outputs—_and yields meaningful boosts in performance [9, 10, 11]. Parallel decoding techniques are clearly used by o1-style models—_just look at the details of the plots provided in their blog posts (shown below)_! However, parallel decoding techniques cannot by themselves explain some of the more complex reasoning behaviors exhibited by recently released reasoning models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f574b5-9d41-4b11-b49a-2d6b4c9e95ee_1942x1120.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f574b5-9d41-4b11-b49a-2d6b4c9e95ee_1942x1120.png)

(from [5])

As a side note, we can also apply the idea of rejection sampling to training (i.e., training vs. test-time rejection sampling). To do this, we just:

- Sample several outputs or trajectories.
    
- Use our reward model (or other scoring mechanism) to pick the best outputs.
    
- Train on these outputs.
    

This approach is commonly used in practice; e.g., LLaMA models perform several rounds of training-time rejection sampling in their post training process prior to the application of RLHF. Rejection sampling is very effective in practice and is easier to implement and scale compared to [PPO-based RLHF](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo).

> _“We adopt a relatively simple post-training procedure based on supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO) as opposed to more complex reinforcement learning algorithms that tend to be less stable and harder to scale.”_ - from [12]

**Self-refinement.** Beyond parallel decoding, we can also consider critique or self-refinement strategies for decoding. First, the LLM generates an initial response. Then, feedback—_either from the LLM or some external sourc_e—is provided for the response, and the LLM can revise its response based on the feedback. This cycle can repeat for an arbitrary number of iterations; see below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8ce6da-c042-4dc3-adeb-89f0f0cc1263_898x378.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8ce6da-c042-4dc3-adeb-89f0f0cc1263_898x378.png)

(from [15])

Several different approaches for refinement exist, but they can be broadly categorized into two groups:

- _Extrinsic_: feedback comes from some external verifier or module.
    
- _Intrinsic_: the LLM provides feedback on its own generation.
    

The results and practical effectiveness of refinement are somewhat mixed. There are many successful examples of using extrinsic feedback—_such as from a verifier [16] or a code interpreter [17]_—to refine the output of an LLM. Whether intrinsic refinement is effective is highly dependent upon the quality of feedback provided by the LLM. Intrinsic refinement can work well for simple tasks [18]. However, this approach struggles to generalize to more complex tasks (e.g., math) [19].

> _“When LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way.”_ - from [18]

## Open Reasoning: DeepSeek-R1 and More

So far, we have learned about the basic concepts that allow us to instill reasoning capabilities within an LLM. However, all of the models we have learned about are closed—_we have no way of knowing how exactly these models were created_. Luckily, several open reasoning models have been recently released. The most notable of these models, which we will cover in this section, is called DeepSeek-R1 [1]. In addition to matching the performance of OpenAI’s o1, this model comes with a full technical report that provides sufficient details for replication and, therefore, completely demystifies the process needed to create a powerful reasoning model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728166d1-a874-48ab-a2a4-ea81e0636228_1224x730.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728166d1-a874-48ab-a2a4-ea81e0636228_1224x730.png)

(from [1])

The core idea behind DeepSeek-R1 aligns well with what we have learned for far. The model is trained with RL on verifiable tasks, where it learns to leverage long CoT to solve complex reasoning problems. Interestingly, the RL training process is the key contributor to the model’s strong reasoning capabilities. Multiple versions of this model—_DeepSeek-R1-Zero and DeepSeek-R1_—are released that have comparable reasoning capabilities. As we will see, the first of these models completely forgoes any supervised training, demonstrating that complex reasoning capabilities naturally emerge from large-scale training with RL.

> _“DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors.”_ - from [1]

**DeepSeek-v3.** The creation of both DeepSeek-R1-Zero and DeepSeek-R1 begins with a powerful base model, called DeepSeek-v3 [2]. In addition to having open weights and a detailed technical report [2], this model surpasses the performance of prior open LLMs and even matches the quality of closed models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)

(from [2])

DeepSeek-v3 is a 671 billion parameter Mixture-of-Experts (MoE) model. If you are unfamiliar with MoEs, please check out the post below, which explains the concept and provides several practical examples, including DeepSeek-v3.

[](https://cameronrwolfe.substack.com/p/moe-llms)

[

![Mixture-of-Experts (MoE) LLMs](https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

](https://cameronrwolfe.substack.com/p/moe-llms)

[

#### Mixture-of-Experts (MoE) LLMs

](https://cameronrwolfe.substack.com/p/moe-llms)

[](https://cameronrwolfe.substack.com/p/moe-llms)

[](https://cameronrwolfe.substack.com/p/moe-llms)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

1月27日

[

Read full story

](https://cameronrwolfe.substack.com/p/moe-llms)

To improve inference and training efficiency, DeepSeek-v3 makes the following design choices (see [here](https://cameronrwolfe.substack.com/i/154340424/deepseek-v-and-deepseek-v) for more details):

- Uses Multi-Headed Latent Attention (MLA).
    
- Adopts an optimized MoE structure (e.g., fine-grained and shared experts).
    
- Uses a multi-token prediction objective during pretraining.
    
- Forgoes load balancing losses typically used to train MoE models.
    
- Decreases precision to FP8 throughout training by adopting a novel quantized training strategy that is proposed in [2].
    

For these reasons, the training of DeepSeek-v3 is very economical compared to other models—_the model is impressive in terms of both performance and efficiency_. Several prior versions of this model were released that inspire some of the design decisions made by DeepSeek-v3; e.g., see [DeepSeek-v2](https://arxiv.org/abs/2405.04434) and [DeepSeek-v2.5](https://api-docs.deepseek.com/news/news1210)[9](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-9-153722335).

#### DeepSeek-R1-Zero

> _“We explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process.”_ - from [1]

The first reasoning model proposed by DeepSeek was DeepSeek-R1-Zero. This model adopts an interesting training strategy that teaches the model to reason purely via large-scale RL—_without any SFT_. The model naturally explores and learns to leverage long CoT to solve complex reasoning problems through RL. DeepSeek-R1-Zero is the first open research effort to show that reasoning capabilities can be developed without supervised training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c284b27-d0f4-4699-b4a0-24c37e8eef88_1840x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c284b27-d0f4-4699-b4a0-24c37e8eef88_1840x882.png)

(from [22])

**RL with GRPO.** The training of DeepSeek-R1-Zero begins with the DeepSeek-v3 [2] base model. We directly finetune this base model via RL. In particular, authors in [1] select [Group Relative Policy Optimization (GRPO)](https://huggingface.co/docs/trl/main/en/grpo_trainer) [3], which is depicted in the figure above, as their RL algorithm. The selection of RL algorithms for LLM training is an open and active research topic. Traditionally, researchers have used [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) for training LLMs, but there is a recent trend towards adopting simpler RL algorithms—_such as [REINFORCE](https://arxiv.org/abs/2402.14740) or [GRPO](https://arxiv.org/abs/2501.12599)_—for LLM training. The main reasons provided for the selection of GRPO in [1] are:

- A reduction in the cost of RL training.
    
- The elimination of the critic model, which is (usually) the same size as the policy model (i.e., the LLM itself).
    

**Defining rewards.** Unlike most traditional work on RL with LLMs, no neural reward models—_meaning LLM-based reward models that are trained over preference data_—are used to train DeepSeek-R1-Zero. Rather, the authors use a rules-based reward system, which _i)_ avoids reward hacking, _ii)_ saves on compute costs[10](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-10-153722335), and _iii)_ is simpler to implement. There are two types of rewards used in particular:

1. _Accuracy reward_: evaluates whether the model’s response is correct.
    
2. _Format reward_: enforces a desired format on the model’s output.
    

DeepSeek-R1-Zero is trained purely on automatically verifiable tasks, such as math and coding problems. For math problems with deterministic results, the model can provide its answer in a specified format, allowing us to verify via basic string matching. Similarly, coding problems can be verified by executing the code produced by the LLM in a sandbox over predefined test cases.

> _“The neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.”_ - from [1]

As mentioned above, the format reward provides a positive training signal when the model produces an output that uses the correct format or template. The format used in [1] simply places the model’s long CoT—_or the thinking / reasoning process_—between two special tokens: `<think>` and `</think>`. The model then produces its answer separately—_between the_ `<answer>` _and_ `</answer>` _tags_—after the completion of the reasoning process; see below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bdc9fc1-4032-41ba-9d7a-946f4826f826_1840x454.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bdc9fc1-4032-41ba-9d7a-946f4826f826_1840x454.png)

(from [1])

**Learning via RL.** Despite using no SFT, DeepSeek-R1-Zero shows clear progress in its reasoning capabilities throughout the RL training process. The model’s performance on AIME 2024 is plotted below as training progresses. Here, the model’s performance gradually improves, eventually reaching parity with o1-preview[11](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-11-153722335). After training completes, DeepSeek-R1-Zero has improved from an initial performance of 15.6% to 71.0%—_or 86.7% when using majority voting with 16 votes_—on AIME 2024! Such results mirror the trends in performance we see with closed reasoning models—_DeepSeek-R1-Zero achieves impressive performance after RL training and can further improve its performance via parallel decoding strategies_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe19787e1-df29-413b-8ab3-7ed137eca9d9_1844x1028.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe19787e1-df29-413b-8ab3-7ed137eca9d9_1844x1028.png)

(from [1])

A full performance comparison between DeepSeek-R1-Zero and o1 models is provided in the table below. DeepSeek-R1 matches or exceeds the performance of o1-mini in most cases and performs comparably to o1-preview on several tasks. However, reasoning models from OpenAI perform much better in the coding domain—_DeepSeek-R1-Zero is clearly a less powerful coding model_. As we will soon see, this problem is fixed in DeepSeek-R1 (the follow-up model).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba93d001-c99e-4b80-a371-b97d92ea1adc_2008x506.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba93d001-c99e-4b80-a371-b97d92ea1adc_2008x506.png)

(from [1])

**What is happening here?** Clearly, DeepSeek-R1-Zero gains impressive reasoning capabilities from the RL training process outlined in [1]. However, _the dynamics of the model’s learning process are also quite observable_! Because we perform no SFT-style training, we can closely monitor the progression of the model’s reasoning strategy throughout the RL training process. As shown below, DeepSeek-R1-Zero learns to leverage more “thinking time”—_or just generate progressively longer chains of thought_—to improve its reasoning process as training progresses. The model naturally learns to leverage more test-time compute to solve harder problems!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e006bb-5959-485b-bb4a-d45b235a8a9d_1800x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e006bb-5959-485b-bb4a-d45b235a8a9d_1800x1004.png)

(from [1])

Authors in [1] also observe several interesting tendencies that emerge naturally during training with RL. For example, the model develops an ability to reflect upon its own solutions by revisiting and evaluating prior components of its reasoning process. Similarly, the model begins to explicitly test out and explore alternative solutions or approaches during the problem solving process. This behavior is not explicitly programmed—_it arises naturally during training with RL_!

> _“The self-evolution of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously.”_ - from [1]

At the most basic level, the RL environment constructed in [1] allows the model to explore different strategies for arriving at a correct—_as determined by verification_—final solution. During exploration, we reward the model for:

1. Using the correct reasoning template or structure.
    
2. Producing a correct final solution.
    

From these rewards alone, the model learns how to solve complex reasoning problems. We do not explicitly need to teach the model how to decompose problems, search for a solution, perform backtracking, or evaluate its own line of thought. Instead, we just provide the correct incentives (or rewards) to the model during the training process. Then, the LLM can autonomously learn necessary behaviors for solving problems via an RL-based “self-evolution” process.

#### DeepSeek-R1

DeepSeek-R1-Zero shows us that LLMs can develop impressive reasoning capabilities from pure RL with no SFT, but this model has some minor bugs. For example, its readability is poor[12](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-12-153722335) and it incorrectly mixes languages together. Put simply, DeepSeek-R1-Zero is very good at reasoning, _but it lacks some of the desirable properties of a well-[aligned](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation) LLM_. As a solution, authors in [1] propose a new, multi-stage training process that integrates some “cold start” SFT data into training along with some other tricks. This training pipeline is used to create DeepSeek-R1, an LLM that is both aligned and capable of complex reasoning.

Similarly to DeepSeek-R1-Zero, we begin with DeepSeek-v3 as a base model. Then, DeepSeek-R1 undergoes four stages of training, including two SFT phases and two RL phases. The purpose of the SFT phases is to provide a better starting point for exploration during each of the RL phases. This training pipeline is one of the key contributions of [1]—_it provides an effective recipe for combining reasoning-style training with the standard post training recipe for LLMs._ Let’s take a deeper look at each stage of the training recipe used for DeepSeek-R1.

> _“To prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.”_ - from [1]

**Phase One: Cold Start (or Reasoning-Oriented SFT).** Prior to RL training, R1 is trained via SFT over a small dataset of long CoT examples, which is referred to in [1] as “cold start” data. There are a few different approaches that we can use to collect this cold start data:

1. Prompt a model (e.g., DeepSeek-v3) to produce long CoT data, either with few-shot examples or by instructing the model to generate detailed answers with accompanied reflection and verification.
    
2. Use the R1-Zero model to generate a large number of long CoT outputs, then ask humans to post-process and select the model’s best outputs.
    

Authors in [1] combine these approaches to collect “thousands of cold-start data” on which DeepSeek-v3 is finetuned directly via SFT. Because we are using long CoT data, _this is a reasoning-oriented finetuning process_. From this cold start data, the model learns a viable (initial) template for solving reasoning problems.

The data used for reasoning-oriented SFT introduces a human prior into DeepSeek-R1’s training process. We can explicitly select the style and pattern of data from which the model learns during this stage. For example, authors in [1] mention that they structure this data to include summaries of each long CoT, thus teaching the model to summarize its entire reasoning process prior to providing its final answer. This data serves as a seed for the RL training process—_the model begins its self-exploration by matching the style of the SFT training data._

**Stage Two: Reasoning-Oriented RL.** After SFT, we just repeat the large-scale RL training process proposed by R1-Zero to enhance the underlying model’s ability to handle reasoning-intensive tasks. The only change made for DeepSeek-R1 is the addition of a language consistency reward, calculated as the portion of the model’s output written in the desired target language. This language consistency reward is found in [1] to slightly deteriorate the model’s reasoning capabilities. However, language consistency improves the overall alignment of the resulting model with human preferences—_the model’s output is more fluent and readable_.

**Stage Three: Rejection sampling.** After the convergence of reasoning-oriented RL, we use the resulting model to collect a large and diverse SFT dataset. Unlike the initial cold start SFT phase, however, we collect more than just reasoning-oriented data. Namely, we augment the reasoning data with general purpose data so that the model can learn from a broader set of problems and domains.

To collect more reasoning data, authors in [1]:

1. Curate a diverse set of reasoning-based prompts.
    
2. Generate candidate trajectories[13](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-13-153722335) using the model from phase two.
    
3. Perform rejection sampling—_or filter and select the top trajectories based on the quality and correctness or each trajectory_.
    

This is the same training-time rejection sampling process that we learned about earlier in this post! Interestingly, we rely upon more than rules-based techniques for verification in this phase. We also incorporate additional data from non-verifiable domains by using DeepSeek-v3 as a [generative reward model](https://arxiv.org/abs/2408.15240) or weak verifier. After applying heuristic filtering (e.g., removing outputs with language mixing or long paragraphs), we arrive at a final set of 600K reasoning trajectories.

> _“We reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting.”_ - from [1]

The SFT dataset from this stage includes a substantial ratio of non-reasoning data (e.g., writing or translation examples). We source this data from the same post training dataset used for DeepSeek-v3. However, the data is augmented by asking DeepSeek-v3 to generate a long CoT to explain the outputs of complex queries—_simpler queries, however, are not given any CoT_. A total of 200K non-reasoning examples are collected, forming an SFT dataset of 800K examples.

**Stage Four: General-purpose RLHF.** The final training stage of DeepSeek-R1 aligns the model with human preferences while continuing to hone its reasoning abilities. Similarly to the prior stage, we train the model over a combination of reasoning-based and general purpose data. In particular, we train the model using RL with a combination of different rewards for each type of data:

- Rules-based rewards (same as R1-Zero) for reasoning-based problems.
    
- Neural reward models—_trained over human preference pairs, just as in standard RLHF_—for general purpose data.
    

DeepSeek-R1 is aligned to be more helpful and harmless on general purpose data. These are two [very common alignment criteria](https://arxiv.org/abs/2204.05862) used in LLM research. Each of these criteria are modeled with a separate neural reward model that is trained over a (supervised) dataset of human preferences. Helpfulness rewards are only measured over the final answer of the model (i.e., excluding the long CoT), while harmless rewards consider the model’s entire output trajectory[14](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-14-153722335). By combining rules and preference-based rewards, DeepSeek-R1 can be aligned to human preferences while maintaining strong reasoning performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d42ce87-35e7-4af2-8a45-cf348df75132_1918x1094.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d42ce87-35e7-4af2-8a45-cf348df75132_1918x1094.png)

(from [1])

**How does it perform?** As shown above, R1 matches or surpasses the performance of o1 on most reasoning tasks. Unlike R1-Zero, R1 also has reasonably strong coding abilities. On general purpose tasks, R1 continues to perform well as a result of its hybrid training pipeline. In general, R1 is a very capable model that seems to be on par with OpenAI’s o1 and can solve a wide variety of tasks—_including both traditional and reasoning-oriented tasks_—with high accuracy.

One interesting observation about this model (and other reasoning models) is that it performs poorly on instruction following benchmarks (e.g., [IF-Eval](https://arxiv.org/abs/2311.07911)) compared to standard LLMs. Currently, _reasoning models seem to be worse than standard LLMs at following instructions_. In the future, I personally believe this trend is likely to reverse. In theory, reasoning models should be capable of leveraging their thought process to better interpret and adhere to a prompt provided by a human user. For example, [deliberative alignment](https://arxiv.org/abs/2412.16339) follows a somewhat similar approach.

**Is SFT necessary?** R1-Zero emphasizes the ability to train strong reasoning models without SFT, while the full R1 model uses several SFT phases to obtain a stronger, final model. So, we might begin to wonder: _Should we use SFT of not?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6b1fbd1-3f9b-4983-8914-1a93d2d2fa87_2388x1154.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6b1fbd1-3f9b-4983-8914-1a93d2d2fa87_2388x1154.png)

Is SFT necessary for reasoning models?

For a standard LLM, SFT provides a high-quality starting point for RLHF. If we applied RLHF directly to the base model, the learning process would be much less efficient. Data for SFT is either synthetically generated or manually created by humans. Generally, collecting data for SFT is expensive (both in terms of time and money). _We have to manually write a good response from scratch for the LLM_!

Collecting such SFT data for reasoning models is more difficult due to their long CoT. Asking humans to manually create long CoT data would be time consuming and expensive! Our only option is to generate this data synthetically, but:

1. Generating this particular style of output with a model may still be hard.
    
2. Correctly verifying such long outputs is difficult.
    

Given the additional complexity of collecting SFT data for reasoning models, authors in [1] first try to avoid SFT altogether! From these experiments, we see that such reasoning abilities naturally emerge from pure RL—_this is an incredible discovery_! However, the resulting model has several shortcomings (e.g., language mixing). When we train over some SFT prior to RL (i.e., a “cold start”), we provide a better prior to RL, which _i)_ eliminates instability during the initial phases of RL training, _ii)_ speeds up up training and _iii)_ improves model quality. So, SFT is not completely necessary, _but it is still practically useful if we have the data_!

#### Distilled Models

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e1abb7a-4035-421b-bcbe-35ccfdb71e47_1248x534.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e1abb7a-4035-421b-bcbe-35ccfdb71e47_1248x534.png)

Illustration of the knowledge distillation process ([source](https://arxiv.org/abs/2006.05525))

Beyond DeepSeek-R1, authors in [1] release a series of dense models that are distilled from R1. The [distillation process](https://arxiv.org/abs/2402.13116) is found to significantly enhance the reasoning capabilities of smaller and more efficient models. The full DeepSeek-R1 model is large (i.e., a 671 billion parameter [Mixture-of-Experts model](https://cameronrwolfe.substack.com/i/154340424/deepseek-v-and-deepseek-v)), so these distilled models are practically useful—_they are_ _comparable to R1 but more cost sensitive and easier to use_. Additionally, the release of these distilled models matches recent trends in closed reasoning models (e.g., o1-mini and o3-mini).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa60aba-ec97-40c9-b10a-1b1a262ff251_1222x574.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa60aba-ec97-40c9-b10a-1b1a262ff251_1222x574.png)

(from [1])

**Distilling R1.** To create these models, we begin with several sizes of two base models[15](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-15-153722335)—_Qwen-2.5 [20] and LLaMA-3 [21]_. We then train the base models via SFT over the 800,000 supervised training examples curated in the third stage of the training pipeline for DeepSeek-R1—_that’s it_!

This is a simple knowledge distillation pipeline, _but the results are impressive_. As shown above, the distilled Qwen2.5-14B model outperforms [QwQ-32B-Preview](https://qwenlm.github.io/blog/qwq-32b-preview/), which was the best open reasoning model prior to the release of R1. Additionally, even the smallest distilled models outperform standard closed LLMs that are not optimized for reasoning (e.g., GPT-4o), while the 32 and 70 billion parameter distilled models exceed the performance of o1-mini on most benchmarks.

> _“Distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL require enormous computational power and may not even achieve the performance of distillation.”_ - from [1]

**Distillation versus RL.** Although we see that distillation is effective in the discussion above, we might wonder whether we could get better results by just directly applying the large-scale RL training process used by DeepSeek-R1 to these smaller models. Interestingly, authors in [1] observe that distilling the Qwen2.5-32B base model from R1—_using the distillation approach described above_—outperforms directly training this model via large-scale RL; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbc4ed3b-81bd-44a2-b8b7-5c0ec792f3cd_2464x406.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbc4ed3b-81bd-44a2-b8b7-5c0ec792f3cd_2464x406.png)

(from [1])

In other words, the reasoning patterns discovered by large models are crucial for improving the reasoning capabilities of these smaller, dense models. However, authors in [1] do make the following additional points:

- It is possible that the performance of distilled models could be further improved via added training with RL.
    
- “Advancing beyond the boundaries of intelligence”—_or creating new reasoning models that even exceed the performance of models like DeepSeek-R1_—will still require powerful base models and large-scale training with RL.
    

**Other distilled reasoning models.** Given the simplicity of training high-quality reasoning models via distillation, a wide variety of reasoning models were released by the research community following the proposal of R1. Some of the most notable releases are:

- [Sky-T1](https://novasky-ai.github.io/posts/sky-t1/) and [Sky-T1-Flash](https://novasky-ai.github.io/posts/reduce-overthinking/)
    
- [Bespoke Stratos](https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation)
    
- [LIMO](https://arxiv.org/abs/2502.03387)
    
- [S1](https://arxiv.org/abs/2501.19393)
    
- [RedStar](https://arxiv.org/abs/2501.11284)
    

There are many more models that have been released as well! The current pace of reasoning model releases is reminiscent of the post-LLaMA era of LLM research. After the release of a powerful open base model (i.e., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)), we saw a wide variety of model variants released that were based on this model (e.g., [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) and many more). Now, we have access to a strong open reasoning model, as we are seeing a very similar trend! The research in this area is very interesting and deserving of its own post—_stay tuned_!

## Key Emerging Trends

We have now learned about a variety of reasoning models, beginning with closed models like o1 or o3 and ending with a fully-outlined replication of these models in DeepSeek-R1. As we have learned about this research, there are a few common trends that begin to emerge. These trends, outlined below, make some important distinctions between research on reasoning models and standard LLMs.

**Long CoT (and inference-time scaling).** The key distinction between reasoning models and standard LLMs is their output structure. Instead of just directly generating a final answer (with an optional concise explanation), reasoning models generate a long CoT that describes their reasoning process in great detail. This long CoT can be of variable length, enabling controllable compute costs at inference time: _longer CoT = more tokens = more compute_. In this way, using more compute at inference time—_by generating a longer CoT_—has become a tool that can allow users to dynamically improve a model’s reasoning capabilities.

**Self-evolution through RL.** Obviously, the ability of LLMs to execute complex reasoning strategies within their long CoT is new and exciting. From recent research, we learn that the key contributor to the development of these special abilities is large-scale RL training. We see in [1] that such reasoning capabilities naturally emerge during RL if the model is correctly incentivized, usually via rules-based rewards that are deterministic and reliable. Additionally, we can further improve a model’s reasoning capabilities by using more compute for training via RL—_this is yet another scaling law that we can leverage_!

**Less supervision.** The dependence of reasoning models upon human supervision is less pronounced relative to standard LLMs. In particular, rewards during RL training are derived primarily from rules-based systems, instead of relying upon human preferences. Of course, reasoning models still have several areas of dependence upon human supervision; e.g., the base model is trained with human-curated data and verification relies upon human-provided ground truth labels. However, there is still a big push by reasoning models like R1 (and especially R1-Zero) to demonstrate that reasoning capabilities can develop autonomously.

**Distillation is effective.** Now that we have access to large and powerful reasoning models, we can distill the capabilities of these models into smaller, dense models using simple strategies! This finding has led to an explosion of research in this area, and we are likely to see many more efficient and distilled reasoning models released in the near future. One key question in this area is whether smaller models will generalize or [struggle to fully match](https://arxiv.org/abs/2305.15717) the breadth of their teachers.

> _“When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance.”_ - from [1]

**New problems to solve.** Above all else, the advent of reasoning models has raised a variety of new (and interesting!) questions that we need to solve:

- How do we handle safety training for long CoT?
    
- What is the best balance between general / reasoning capabilities?
    
- What is the optimal role of SFT in training reasoning models?
    
- How do we minimize “overthinking” in long CoT?
    
- How do we handle efficient hosting of reasoning models?
    

As mentioned at the beginning of this post, reasoning models are a truly new type of LLM that will force us to rethink existing frameworks. Solidified techniques that have been used for years (e.g., few-shot prompting) are becoming obsolete for these new models. _The field of LLM research is re-inventing itself once again_.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." _arXiv preprint arXiv:2501.12948_ (2025).

[2] Liu, Aixin, et al. "Deepseek-v3 technical report." _arXiv preprint arXiv:2412.19437_ (2024).

[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." _arXiv preprint arXiv:2402.03300_ (2024).

[4] OpenAI. “Introducing OpenAI o1-preview” _[https://openai.com/index/introducing-openai-o1-preview/](https://openai.com/index/introducing-openai-o1-preview/)_ (2024).

[5] OpenAI. “Learning to Reason with LLMs” _[https://openai.com/index/learning-to-reason-with-llms/](https://openai.com/index/learning-to-reason-with-llms/)_ (2024).

[6] OpenAI. “OpenAI o3-mini” _[https://openai.com/index/openai-o3-mini/](https://openai.com/index/openai-o3-mini/)_ (2025).

[7] Rein, David, et al. "Gpqa: A graduate-level google-proof q&a benchmark." arXiv preprint arXiv:2311.12022 (2023).

[8] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.

[9] Zelikman, Eric, et al. "Star: Bootstrapping reasoning with reasoning." Advances in Neural Information Processing Systems 35 (2022): 15476-15488.

[10] Gulcehre, Caglar, et al. "Reinforced self-training (rest) for language modeling." arXiv preprint arXiv:2308.08998 (2023).

[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).

[12] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).

[13] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).

[14] Bespoke Labs. “Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation” _[https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation](https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation)_ (2025).

[15] Welleck, Sean, et al. "From decoding to meta-generation: Inference-time algorithms for large language models." _arXiv preprint arXiv:2406.16838_ (2024).

[16] Aggarwal, Pranjal, Bryan Parno, and Sean Welleck. "AlphaVerus: Bootstrapping formally verified code generation through self-improving translation and treefinement." _arXiv preprint arXiv:2412.06176_ (2024).

[17] Chen, Xinyun, et al. "Teaching large language models to self-debug." _arXiv preprint arXiv:2304.05128_ (2023).

[18] Wang, Yifei, et al. "A Theoretical Understanding of Self-Correction through In-context Alignment." _arXiv preprint arXiv:2405.18634_ (2024).

[19] Huang, Jie, et al. "Large language models cannot self-correct reasoning yet." _arXiv preprint arXiv:2310.01798_ (2023).

[20] Yang, An, et al. "Qwen2. 5 technical report." _arXiv preprint arXiv:2412.15115_ (2024).

[21] Dubey, Abhimanyu, et al. "The llama 3 herd of models." _arXiv preprint arXiv:2407.21783_ (2024).

[22] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." _arXiv preprint arXiv:2402.03300_ (2024).

[1](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-1-153722335)

For example, o1-preview did not have the ability to upload files, could not understand other modalities of data (e.g., images), and had no web search capabilities.

[2](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-2-153722335)

Although the details of how OpenAI controls the amount of inference-time compute used by o1-style models are not clear, it seems from [their blog post](https://openai.com/index/learning-to-reason-with-llms/) that these models have multiple “settings” for the amount of compute that they can use at inference time. These settings are likely related to the length of the model’s long CoT, so high inference-time compute settings would simply generate very long chains of thought.

[3](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-3-153722335)

Technically, this benchmark is still unbeaten because o3 exceeded the maximum computational budget when achieving >85% accuracy.

[4](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-4-153722335)

This benchmark was described by [Terence Tao](https://en.wikipedia.org/wiki/Terence_Tao) as likely to be unsolved by AI systems for “several years at least”. There has been some recent questioning of OpenAI’s performance on this benchmark due to [conflict of interest](https://techcrunch.com/2025/01/19/ai-benchmarking-organization-criticized-for-waiting-to-disclose-funding-from-openai/) between OpenAI and the organization that created this benchmark ([EpochAI](https://epoch.ai/)).

[5](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-5-153722335)

Notably, o3-mini does NOT have vision support, unlike o1.

[6](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-6-153722335)

In contrast, RLHF trains the reward model over various kinds of human preferences, usually via a [ranking loss](https://gombru.github.io/2019/04/03/ranking_loss/).

[7](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-7-153722335)

In addition to these two techniques, we could also perform some sort of search (e.g., [monte carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search))—see [here](https://arxiv.org/abs/2405.00451) for an example. However, we can also categorize search-based methods as generating more tokens at inference time.

[8](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-8-153722335)

The length of a long CoT may vary depending on model settings (e.g., OpenAI provides several settings for “reasoning effort”) or problem difficulty.

[9](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-9-153722335)

There is also a [DeepSeek-v1 model](https://arxiv.org/abs/2401.02954), but this model is dense (i.e., not an MoE) and much different from the model family that is used for DeepSeek-R1.

[10](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-10-153722335)

The compute savings come from the fact that we do not have to train (or run inference on) any reward models.

[11](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-11-153722335)

See [here](https://platform.openai.com/docs/models#o1) for a full list of OpenAI’s o1 models. For clarity, the `o1-0912` model mentioned in [1] is the same as the `o1-preview` model.

[12](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-12-153722335)

For example, the model lacks markdown formatting and highlighting within its answers, which is a common feature for modern LLMs.

[13](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-13-153722335)

In [1], authors refer to the long CoT outputs generated by the DeepSeek-R1 model variants as “trajectories”.

[14](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-14-153722335)

Notably, this is in direct contrast to the (original) approach adopted by OpenAI. o1-style models have their long CoT hidden from the end user, and these reasoning traces do not undergo any safety training. The rationale for this strategy is to allow the model to be more transparent in its trajectory, which improves interpretability.

[15](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-15-153722335)

The exact models used are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. Notably, we do not always start with the base model—_many of these models have undergone post training_!

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Sarvagya's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48fc47cc-1fcb-43ca-8165-e4939312387e_144x144.png)



](https://substack.com/profile/227554059-sarvagya)

[

![Amir's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d602761-5fd8-4db1-a4bd-0b60c244a990_144x144.png)



](https://substack.com/profile/22365321-amir)

[

![Wietse Smits's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a01e6c3-c38f-46f5-899a-488e51408614_96x96.jpeg)



](https://substack.com/profile/279808259-wietse-smits)

[

![Hannuri Ha's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb81f8cf0-c938-4d30-905a-5a6a99dc8bbf_336x336.jpeg)



](https://substack.com/profile/93148505-hannuri-ha)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

204 Likes∙

[23 Restacks](https://substack.com/note/p-153722335/restacks?utm_source=substack&utm_content=facepile-restacks)

204

- 

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

23

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Gianfranco Filice's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c97316a-6e8a-48c8-81d0-fcdca1268119_400x400.jpeg)



](https://substack.com/profile/4259088-gianfranco-filice?utm_source=comment)

[Gianfranco Filice](https://substack.com/profile/4259088-gianfranco-filice?utm_source=substack-feed-item)

[Gianfranco’s Substack](https://gianfrancofilice.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[3月13日](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/99977105 "2025年3月13日 02:15")

Liked by Cameron R. Wolfe, Ph.D.

A post of exceptional quality, thanks Cameron.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/99977105)

[

![Hugo Delamain's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F81cde8c7-b5c9-42b4-a437-e1767aae452b_1536x2048.jpeg)



](https://substack.com/profile/46119466-hugo-delamain?utm_source=comment)

[Hugo Delamain](https://substack.com/profile/46119466-hugo-delamain?utm_source=substack-feed-item)

[The Business of Stuff](https://www.businessofstuff.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[3月23日](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/102674015 "2025年3月23日 16:24")

This is great - exactly what I was looking for!

Like

Reply

Share

[1 more comment...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

[Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

[Understanding how SFT works from the idea to a working implementation...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Sep 11, 2023 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

55

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68686a01-2b31-4694-8c04-a562ffd725ad_2210x1244.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Demystifying Reasoning Models

### Understanding reasoning models and their relation to standard LLMs...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Feb 18, 2025

204

- 

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

23

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4fb1867-b78e-4db6-aea7-14251a3facce_2389x1336.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4fb1867-b78e-4db6-aea7-14251a3facce_2389x1336.png)

(from [4, 13, 22])

For the last several years, we have used a relatively fixed pipeline for training large language models (LLMs); see below. First, we pretrain these language models over raw textual data from the internet. Afterwards, we align them—_or train them to produce outputs that are preferable to humans_—using a combination of [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). Both pretraining and alignment play a key role in model quality, but a large majority of advancements in this paradigm have been driven by [LLM scaling laws](https://cameronrwolfe.substack.com/p/llm-scaling-laws)—_we get better results by pretraining larger models on more data_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac82c7c1-fcbd-4b32-b9cd-febfadd77c19_1720x562.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac82c7c1-fcbd-4b32-b9cd-febfadd77c19_1720x562.png)

Training pipeline for a standard LLM

Recently, a completely new paradigm in LLM research has emerged: _reasoning_. Reasoning models approach problem solving in a completely different manner compared to standard LLMs. In particular, they spend a variable amount of time “thinking” prior to providing their final answer to a question. Training models that are able to think effectively (e.g., decompose problems, detect errors in their thinking, explore alternative solutions and more) requires new strategies, usually involving large-scale reinforcement learning (RL). Additionally, such models give rise to new forms of scaling laws for training via RL and inference; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88a91669-f7f0-41aa-b0f0-78392da2115a_1254x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88a91669-f7f0-41aa-b0f0-78392da2115a_1254x804.png)

(from [4])

In this overview, we will learn more about recent advancements in reasoning models. To start, we will focus on several (closed) reasoning models that were proposed first by OpenAI. We will contextualize the explanation of these models with the fundamental ideas that underlie LLM reasoning capabilities. Afterwards, we will explore recently-proposed (open) reasoning models, outlining necessary details for creating such a model from scratch. Reasoning models are different from standard LLMs. But, don’t worry. A lot of the key concepts of LLMs still apply to reasoning models. _We will clarify important distinctions throughout._

## The Age of Reasoning

Just as AI progress was seemingly [starting to slow down](https://cameronrwolfe.substack.com/p/llm-scaling-laws), we witnessed a sudden and significant improvement in LLM capabilities with the popularization of [reasoning models](https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html). First to be released was OpenAI’s [o1-preview](https://openai.com/index/introducing-openai-o1-preview/) [4], followed by a series of distilled (i.e., smaller) models like o1-mini and later model variants like [o3](https://openai.com/index/openai-o3-mini/) [6]. In response, other companies released similar reasoning models, such as [Google’s Gemini 2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/). In this section, we will explore these initial, closed reasoning models and the basic ideas behind how they work.

#### Initial Reasoning Models: o1 and o1-mini

> _“We've developed a new series of AI models designed to spend more time thinking before they respond.”_ - from [4]

The release of **o1-preview** [4, 5] by OpenAI made two things very clear:

1. Reasoning models can solve verifiable tasks—_such as math and coding tasks_—very accurately.
    
2. The approach taken by reasoning models to solve these problems is very different from that of a traditional LLM.
    

**Long CoT.** The main difference between a reasoning model and a standard LLM is the ability to “think” before answering a question. The reasoning model’s thoughts are just long chains of thought—_or_ _long CoT for short, sometimes referred to as a reasoning trace or trajectory_—outputted by the LLM. This long CoT is generated no differently than any other sequence of text. However, these reasoning trajectories exhibit very interesting properties that are more akin to search algorithms than vanilla text generation. For example, the model will:

- Think through each part of a complex problem.
    
- Decompose complex problems into smaller, solvable parts.
    
- Critique its own (partial) solutions and find errors.
    
- Explore many alternative solutions.
    

For some concrete examples of these reasoning trajectories, see [this blog post](https://openai.com/index/learning-to-reason-with-llms/). Notably, the long CoT used by OpenAI’s reasoning models are “internal”, meaning that they are hidden from the user when interacting with the model. Instead, the user sees a model-written summary of the long CoT; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c08cfd9-85a6-4079-b510-59857ae05c3e_1970x1174.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c08cfd9-85a6-4079-b510-59857ae05c3e_1970x1174.png)

([source](https://openai.com/index/learning-to-reason-with-llms/))

The long CoT output of reasoning models gives us an easy way to control the inference-time compute of an LLM. If we want to spend more compute on solving a problem, we can simply generate a longer CoT. Similarly, less complex problems can be solved with a shorter CoT, thus saving compute at inference time.

**Reasoning capabilities.** Initial reasoning models were actually less capable than standard LLMs in many ways[1](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-1-153722335), but they improve the reasoning capabilities of an LLM by several orders of magnitude. For example, _o1-preview unanimously outperforms GPT-4o and even rivals the performance of human experts on most complex reasoning tasks_; see below. To achieve these results, o1-preview is evaluated using maximal inference-time compute[2](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-2-153722335) and either _i)_ a single output sample (solid bar) or _ii)_ a majority vote among 64 parallel output samples (shaded bar).

[

![Competition evals for Math (AIME 2024), Code (CodeForces), and PhD-Level Science Questions (GPQA Diamond)](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde143ac3-dbf4-476c-9524-282b23c1034c_2700x1050.png "Competition evals for Math (AIME 2024), Code (CodeForces), and PhD-Level Science Questions (GPQA Diamond)")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde143ac3-dbf4-476c-9524-282b23c1034c_2700x1050.png)

o1 models vs. GPT-4o on reasoning tasks (from [5])

Beyond o1-preview, **OpenAI’s o1**—_the full version of o1 that was released a few months after the preview_—places among the top 500 students in the US on the math olympiad qualification exam ([AIME 2024](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?srsltid=AfmBOopg_BQh_GIwm9fLXXJSK812QdJcW_e6uohok7JzFaFCbie0twRk)) and ranks within the 11th percentile of competitive human programmers on [Codeforces](https://arxiv.org/abs/2501.01257). For reference, GPT-4o only solved 12% of AIME problems, while o1 solves anywhere from 74% to 93% of the problems depending upon inference settings. See the figure below for a more detailed comparison between the performance of o1 and GPT-4o.

[

![Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd030dac8-57ff-4d51-a8a5-7bbbec5fc3ba_2400x1650.png "Breakdown of the accuracy and raw score of gpt-4o vs. o1 on various competition evals")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd030dac8-57ff-4d51-a8a5-7bbbec5fc3ba_2400x1650.png)

Improvement of o1 over GPT-4o (from [5])

Similarly, **o1-mini**—_a cheaper and faster version of o1_—has impressive reasoning capabilities despite its 80% cost reduction relative to the full o1 model. This model, despite having limited world knowledge compared to o1, is especially capable at coding tasks and performs very well given its efficiency.

#### State-of-the-Art Reasoning Models: o3 and o3-mini

[

![o Series Performance](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeccad4f-894f-4593-9573-ff3285420af7_1200x675.jpeg "o Series Performance")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeccad4f-894f-4593-9573-ff3285420af7_1200x675.jpeg)

Performance of OpenAI’s o3 on ARC-AGI ([source](https://arcprize.org/blog/oai-o3-pub-breakthrough))

Shortly after the announcement and release of o1 models, OpenAI announced **o3**—_the most recent model in the o1 lineage_. This model was initially just announced (not released). We were able to see the model’s performance on several notable benchmarks—_as measured by OpenAI_—but could not actually use the model. The metrics released by OpenAI were very impressive. In fact, _the performance of o3 was quite shocking to many people_. The most notable achievements of o3 are:

- A score of 87.5% on the [ARC-AGI benchmark](https://arcprize.org/blog/oai-o3-pub-breakthrough)—_the “North Star” towards AGI that was left unbeaten[3](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-3-153722335) for five years_—on which GPT-4o achieves 5% accuracy. o3 is the first model to exceed human-level performance of 85% on ARC-AGI.
    
- An accuracy of 71.7% on [SWE-Bench Verified](https://openai.com/index/introducing-swe-bench-verified/) and an [Elo score](https://en.wikipedia.org/wiki/Elo_rating_system) of 2727 on Codeforces, _ranking o3 among the top 200 competitive programmers on the planet_.
    
- An accuracy of 25.2% on EpochAI’s [FrontierMath benchmark](https://epoch.ai/frontiermath), _improving upon the previous state-of-the-art accuracy of 2.0%_[4](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-4-153722335).
    

However, the public did not have access to the o3 model to verify any of these results. The full o3 model still has yet to be released at the time of writing, but OpenAI did recently release a smaller version of the model—_**o3-mini**_ [6].

> _“Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.”_ - from [6]

Compared to other reasoning models from OpenAI, o3-mini is more cost effective and production-ready. For example, this model supports features like function calling, web search and structured outputs[5](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-5-153722335). o3-mini also has multiple settings—_including low, medium and high effort_—for the amount of reasoning that it performs when solving a problem. This setting can be directly specified in the API request, and the model performs very impressively—_on par with o1 in many cases_—depending on the level of reasoning effort; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F809e35bd-3da6-4382-8635-dcff356f25c0_2424x1332.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F809e35bd-3da6-4382-8635-dcff356f25c0_2424x1332.png)

o3-mini performance breakdown (from [6])

In most cases, o3-mini with low reasoning effort matches the performance of o1-mini, while o3-mini with high reasoning effort exceeds the performance of all other reasoning models released by OpenAI (including the full o1 model).

o3-mini also has better world knowledge (i.e., improved factuality), is noticeably more efficient, and scores higher in human preference studies compared to prior reasoning models; see below. In particular, authors in [6] mention that during internal A/B tests _“o3-mini delivered responses 24% faster than o1-mini, with an average response time of 7.7 seconds compared to 10.16 seconds.”_ o3-mini is the most efficient model released (so far) of OpenAI’s o1-style reasoning models.

[

![The chart compares win rates for STEM and non-STEM tasks across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in both categories, with a higher win rate for STEM tasks.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044cb648-2c4d-4aaa-88bb-bf4548876d24_1944x994.webp "The chart compares win rates for STEM and non-STEM tasks across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in both categories, with a higher win rate for STEM tasks.")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F044cb648-2c4d-4aaa-88bb-bf4548876d24_1944x994.webp)

Win-rate of o3-mini vs. o1-mini on STEM / non-STEM prompts (from [6])

**Other model providers.** The release of o1-style models by OpenAI was quickly followed by other model providers. For example, Google recently released the experimental [Gemini-2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/), which maintains the signature long context of Gemini models—_1M token context window_—and achieves respectable metrics on key verifiable tasks (e.g., AIME and GPQA). However, _this model still lags behind the performance of o1 and o3-mini_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78afa03-d704-43f4-b001-3965969a3b84_1070x556.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78afa03-d704-43f4-b001-3965969a3b84_1070x556.png)

([source](https://deepmind.google/technologies/gemini/flash-thinking/))

Very recently, a reasoning beta was announced for Grok-3 that is very compelling. As shown below, the Grok-3 reasoning model exceeds the performance of o3-mini with high reasoning efforts and even comes close to matching the full o3 model in a few cases; e.g., 96% accuracy on AIME’24, compared to the 97% accuracy of o3. Grok-3, which was trained using a [massive new compute cluster](https://www.datacenterfrontier.com/machine-learning/article/55244139/the-colossus-ai-supercomputer-elon-musks-drive-toward-data-center-ai-technology-domination), is impressive (especially given the youth of xAI). At the time of writing, the reasoning beta of Grok-3 is the closest competitor to reasoning models from OpenAI.

[

![r/singularity - Grok 3 Reasoning Benchmarks](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bc6bd5-d713-4c5e-9740-9a5e3ec81923_640x318.png "r/singularity - Grok 3 Reasoning Benchmarks")



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bc6bd5-d713-4c5e-9740-9a5e3ec81923_640x318.png)

(from Grok-3 announcement video on X)

#### Benchmarks for Reasoning Models

> _“Recent frontier models do so well on MATH and GSM8K that these benchmarks are no longer effective at differentiating models.”_ - from [5]

Before learning more about how reasoning models work, let’s take a deeper look at their performance. To truly understand the capabilities of these models, we need to do more than just look at metrics—_we need to inspect concrete examples of the problems that these models are solving_. For example, consider [GSM8K](https://arxiv.org/abs/2110.14168) (shown below), a grade-school level math benchmark. These questions might seem trivial, but LLMs struggled to accurately solve this benchmark for [several years](https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87c06563-9df0-4cd4-8e8b-62acf408ffce_2300x838.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87c06563-9df0-4cd4-8e8b-62acf408ffce_2300x838.png)

Example questions from GSM8K ([source](https://huggingface.co/datasets/openai/gsm8k))

With the advent of reasoning models, this benchmark has been completely saturated—_we can no longer use it to meaningfully evaluate the best reasoning models_. Instead, we are beginning to solve much harder problems with LLMs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95dc2906-5bef-4d7a-a234-5e833d189ba1_1900x248.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95dc2906-5bef-4d7a-a234-5e833d189ba1_1900x248.png)

Example problem from AIME 2024 ([source](https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems))

For example, consider the [15th problem from AIME 2024](https://artofproblemsolving.com/wiki/index.php/2024_AIME_I_Problems/Problem_15), as shown above. This problem is quite complex and goes beyond the arithmetic reasoning questions found in GSM8K. There are (at least) six different ways that this problem can be solved, all of which require knowledge of advanced mathematical techniques (e.g., derivatives, [number theory](https://en.wikipedia.org/wiki/Number_theory) or [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)).

Additionally, the complex benchmarks being solved by reasoning models go beyond math! For example, GPQA [7] contains hundreds of multiple-choice questions from several scientific domains; e.g., Biology, Physics, and Chemistry. All of these questions are written by domain experts and verified to be both very difficult and “Google-proof”, meaning that non-experts struggle to solve these problems even when given sufficient time and unrestricted internet access.

> _“We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy, while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web.”_ - from [7]

The ARC-AGI benchmark—_described as a “material stepping stone toward AGI”_—involves a variety of grid-based puzzles in which the LLM must learn patterns among input-output grids and perfectly replicate this learned pattern on a final output example; see below. Most LLMs struggle to solve these puzzles (e.g., GPT-4o achieves an accuracy of only 5%), but reasoning models perform quite well on this benchmark—_30-90% accuracy depending on the compute budget_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb2e0506-6107-4e23-8ef5-3e0f4bb1e6e8_1538x1062.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb2e0506-6107-4e23-8ef5-3e0f4bb1e6e8_1538x1062.png)

To say the least, _these are a different caliber of (non-trivial) problems that reasoning LLMs are beginning to solve_. Despite the difficulty of these benchmarks, modern reasoning models are found to be remarkably capable—_OpenAI’s o3 model is reported to achieve a score of nearly 97% on AIME 2024_. After manually inspecting some of these questions, we can truly understand the gravity of this result.

## Fundamentals of Reasoning Models

> “_We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute).”_ - from [1]

Although the reasoning models presented above are clearly impressive, there are all closed models. So, _we have no information about how they actually work_. The only information we are given is the above quote and the plot shown below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fe00c0c-da10-431b-8316-4ea3939e50fe_1264x645.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fe00c0c-da10-431b-8316-4ea3939e50fe_1264x645.png)

(from [5])

From this limited information, however, we can draw some useful conclusions. Mainly, there are two key components involved in scaling a reasoning model:

- More training via RL.
    
- More inference-time compute (i.e., inference-time scaling).
    

Although OpenAI does not reveal many of the details behind their approach to scaling these two components of a reasoning model, there is still [a lot of research](https://github.com/srush/awesome-o1) that has been published on this topic. To provide more context, let’s briefly take a look at some of this work—_along with details shared by OpenAI_—to outline some of the key concepts that underlie how reasoning models are trained and used.

#### Reinforcement Learning with Verifiable Rewards

One detail that we should immediately notice about o1-style models is that they are primarily used for and evaluated on problems that are verifiable in nature; e.g., math and coding. But, _what exactly does “verifiable” mean in this context?_ First, we assume that we have access to either _i)_ a ground truth answer for the problem or _ii)_ some rules-based technique that can be used to verify correctness.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb865992-1eee-4fdb-b98a-165f4d555e11_1774x608.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb865992-1eee-4fdb-b98a-165f4d555e11_1774x608.png)

Verifying a math problem via exact string match

For example, we can define a ground truth final answer for most math problem—this is done in [GSM8K](https://huggingface.co/datasets/openai/gsm8k) with the `#### <answer>` syntax. Then, we can extract the final answer from the LLM’s output and compare this answer to the ground truth using a basic string match; see above. Similarly, if we have test cases prepared for a coding question, we can simply execute the code produced by our LLM and check whether the provided solution satisfies all of the test cases.

> _“Reinforcement Learning with Verifiable Rewards (RLVR) can be seen as a simplified form of existing approaches for bootstrapping LM reasoning or a simpler form of RL with execution feedback, in which we simply use answer matching or constraint verification as a binary signal to train the model.”_ - from [13]

Saying that a domain is “verifiable” does NOT mean that we can automatically verify arbitrary solutions to problems in this domain. Rather, we will often need access to ground truth answers—_typically obtained from humans_—for verification.

However, there are some behaviors that can be verified using simple rules instead of ground truth. For example, we can determine whether a reasoning model has the correct output format, follows certain instructions, or produces outputs of a particular length (e.g., the low, medium or high reasoning effort used by o3-mini) by performing simple checks with a set of hard-coded rules.

**Verification complexities.** Verifying an LLM’s output can become quite complex depending on the problems we are solving. Even for math problems, verifying a match between the LLM’s answer and ground truth is difficult. For example, the solution may be presented in a different form or format, leading to false negative verifications. In these cases, simple string matching may not be enough! Instead, we can prompt an LLM to tell us whether the two solutions are a match or not, which has been found to drastically reduce incorrect verifications [14]. For code, implementing verification is tough as well—_it requires constructing a data pipeline that can very efficiently execute and verify test cases within our training setup_.

> _“We do not apply neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale RL process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.”_ - from [1]

**Neural verification.** Beyond the verifiable problems outlined above, we can also consider weaker forms of verification. For example, creative writing is a task that is difficult to verify. However, we can:

1. Train a [neural reward model](https://arxiv.org/abs/2403.13787) or verifier.
    
2. Score our LLM’s output with this model.
    
3. Use the predicted score as a reward or verification signal.
    

Such a setup is very similar to [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations). In this case, we are training our reward model to perform binary verification based on the correctness or quality of the model’s response[6](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-6-153722335). However, using a neural verifier comes with the risk of [reward hacking](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/), especially when performing large-scale RL. The model is trained for longer and does much more exploring of the reward landscape, thus increasing the risk of reward hacking. As a result, many recent reasoning models have avoided this approach.

**Learning from verifiable rewards.** We now understand verification, but how can verification be used to train an LLM? The idea here is simple: _we just directly use the verification result as a reward signal for training with RL_; see below. There are many different ways of implementing this idea (e.g., [process rewards](https://arxiv.org/abs/2305.20050) or [pure RL](https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop)), but they share the common theme of using RL to learn from verifiable rewards. _This is the fundamental concept upon which all modern reasoning models are based_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7334cdb5-5398-47d2-98bb-01ca41a58879_1854x726.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7334cdb5-5398-47d2-98bb-01ca41a58879_1854x726.png)

(from [13])

For a complete exposition of methods that can be used to learn from verifiable rewards with RL, check out the incredible video by [Sasha Rush](https://rush-nlp.com/) below.

#### Inference-Time Strategies: Chain of Thought and Decoding

There are two basic ways[7](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-7-153722335) that we can increase the amount of compute that our language model is consuming at inference time:

- Generate more tokens (i.e., longer output sequence).
    
- Generate multiple outputs.
    

In this section, we will go into these techniques in more detail, exploring how they are practically implemented in LLMs via chains of thought and different decoding strategies; e.g., parallel versus sequential decoding.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

(from [8])

**Chain of thought.** We already know that reasoning models use long CoT as their medium for reasoning. Proposed in [8], a chain of thought—_at the simplest level_—is just an explanation that an LLM provides for its own output. In most cases, these explanations are written prior to the LLM generating its final answer, allowing the model to use its explanation as context when generating its answer; see above.

The long CoT used by reasoning models is much different than a standard CoT. A standard CoT is concise and human-readable. A long CoT is several thousand tokens long[8](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-8-153722335). Although it can be used for interpretability purposes, the long CoT is not optimized for human readability. Rather, it is an extensive reasoning trace that approaches problem solving in a detailed manner and contains a variety of complex reasoning behaviors (e.g., backtracking and self-refinement).

> _“We have decided not to show the raw chains of thought to users… We strive to partially make up for [this decision] by teaching the model to reproduce useful ideas from the chain of thought in the answer. For the o1 model series we show a model-generated summary of the chain of thought.”_ - from [5]

Additionally, reasoning models logically separate their CoT from the final output of the model. For example, OpenAI avoids exposing the long CoT directly to users and instead provides an LLM-generated summary of the long CoT to supplement the reasoning model’s final answer. Such a logical separation is fundamentally necessary due to the length of CoT. Most users will only read the final answer—_reading the entire reasoning trace would be incredibly time consuming_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7b26d4a-0d1c-4e27-a63d-5fe7035e83b1_604x278.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa7b26d4a-0d1c-4e27-a63d-5fe7035e83b1_604x278.png)

(from [15])

**Parallel decoding.** To improve the accuracy of an LLM’s final output, we may also use parallel decoding techniques; see above. The idea here is simple: _instead of generating a single output with our LLM, we generate multiple outputs and aggregate these outputs to form a single, final answer_. This aggregation can be done in many ways; e.g., using [majority vote](https://arxiv.org/abs/2203.11171) or consensus, using [weighted voting](https://arxiv.org/abs/2206.02336), identifying the best output(s) with a [neural reward model or verifier](https://arxiv.org/abs/2408.15240) (i.e., also known as [Best-of-N or rejection sampling](https://arxiv.org/abs/2110.14168)), or [other domain-specific algorithms](https://arxiv.org/abs/2210.02441).

The main benefit of these approaches is their simplicity and effectiveness. Scaling up parallel decoding is easy—_we just generate, verify and aggregate a larger number of outputs—_and yields meaningful boosts in performance [9, 10, 11]. Parallel decoding techniques are clearly used by o1-style models—_just look at the details of the plots provided in their blog posts (shown below)_! However, parallel decoding techniques cannot by themselves explain some of the more complex reasoning behaviors exhibited by recently released reasoning models.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f574b5-9d41-4b11-b49a-2d6b4c9e95ee_1942x1120.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f574b5-9d41-4b11-b49a-2d6b4c9e95ee_1942x1120.png)

(from [5])

As a side note, we can also apply the idea of rejection sampling to training (i.e., training vs. test-time rejection sampling). To do this, we just:

- Sample several outputs or trajectories.
    
- Use our reward model (or other scoring mechanism) to pick the best outputs.
    
- Train on these outputs.
    

This approach is commonly used in practice; e.g., LLaMA models perform several rounds of training-time rejection sampling in their post training process prior to the application of RLHF. Rejection sampling is very effective in practice and is easier to implement and scale compared to [PPO-based RLHF](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo).

> _“We adopt a relatively simple post-training procedure based on supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO) as opposed to more complex reinforcement learning algorithms that tend to be less stable and harder to scale.”_ - from [12]

**Self-refinement.** Beyond parallel decoding, we can also consider critique or self-refinement strategies for decoding. First, the LLM generates an initial response. Then, feedback—_either from the LLM or some external sourc_e—is provided for the response, and the LLM can revise its response based on the feedback. This cycle can repeat for an arbitrary number of iterations; see below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8ce6da-c042-4dc3-adeb-89f0f0cc1263_898x378.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8ce6da-c042-4dc3-adeb-89f0f0cc1263_898x378.png)

(from [15])

Several different approaches for refinement exist, but they can be broadly categorized into two groups:

- _Extrinsic_: feedback comes from some external verifier or module.
    
- _Intrinsic_: the LLM provides feedback on its own generation.
    

The results and practical effectiveness of refinement are somewhat mixed. There are many successful examples of using extrinsic feedback—_such as from a verifier [16] or a code interpreter [17]_—to refine the output of an LLM. Whether intrinsic refinement is effective is highly dependent upon the quality of feedback provided by the LLM. Intrinsic refinement can work well for simple tasks [18]. However, this approach struggles to generalize to more complex tasks (e.g., math) [19].

> _“When LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way.”_ - from [18]

## Open Reasoning: DeepSeek-R1 and More

So far, we have learned about the basic concepts that allow us to instill reasoning capabilities within an LLM. However, all of the models we have learned about are closed—_we have no way of knowing how exactly these models were created_. Luckily, several open reasoning models have been recently released. The most notable of these models, which we will cover in this section, is called DeepSeek-R1 [1]. In addition to matching the performance of OpenAI’s o1, this model comes with a full technical report that provides sufficient details for replication and, therefore, completely demystifies the process needed to create a powerful reasoning model.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728166d1-a874-48ab-a2a4-ea81e0636228_1224x730.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F728166d1-a874-48ab-a2a4-ea81e0636228_1224x730.png)

(from [1])

The core idea behind DeepSeek-R1 aligns well with what we have learned for far. The model is trained with RL on verifiable tasks, where it learns to leverage long CoT to solve complex reasoning problems. Interestingly, the RL training process is the key contributor to the model’s strong reasoning capabilities. Multiple versions of this model—_DeepSeek-R1-Zero and DeepSeek-R1_—are released that have comparable reasoning capabilities. As we will see, the first of these models completely forgoes any supervised training, demonstrating that complex reasoning capabilities naturally emerge from large-scale training with RL.

> _“DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors.”_ - from [1]

**DeepSeek-v3.** The creation of both DeepSeek-R1-Zero and DeepSeek-R1 begins with a powerful base model, called DeepSeek-v3 [2]. In addition to having open weights and a detailed technical report [2], this model surpasses the performance of prior open LLMs and even matches the quality of closed models; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26d7720-a597-49c3-82b7-5ee830132411_1846x1186.png)

(from [2])

DeepSeek-v3 is a 671 billion parameter Mixture-of-Experts (MoE) model. If you are unfamiliar with MoEs, please check out the post below, which explains the concept and provides several practical examples, including DeepSeek-v3.

[](https://cameronrwolfe.substack.com/p/moe-llms)

[

![Mixture-of-Experts (MoE) LLMs](https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

](https://cameronrwolfe.substack.com/p/moe-llms)

[

#### Mixture-of-Experts (MoE) LLMs

](https://cameronrwolfe.substack.com/p/moe-llms)

[](https://cameronrwolfe.substack.com/p/moe-llms)

[](https://cameronrwolfe.substack.com/p/moe-llms)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

1月27日

[

Read full story

](https://cameronrwolfe.substack.com/p/moe-llms)

To improve inference and training efficiency, DeepSeek-v3 makes the following design choices (see [here](https://cameronrwolfe.substack.com/i/154340424/deepseek-v-and-deepseek-v) for more details):

- Uses Multi-Headed Latent Attention (MLA).
    
- Adopts an optimized MoE structure (e.g., fine-grained and shared experts).
    
- Uses a multi-token prediction objective during pretraining.
    
- Forgoes load balancing losses typically used to train MoE models.
    
- Decreases precision to FP8 throughout training by adopting a novel quantized training strategy that is proposed in [2].
    

For these reasons, the training of DeepSeek-v3 is very economical compared to other models—_the model is impressive in terms of both performance and efficiency_. Several prior versions of this model were released that inspire some of the design decisions made by DeepSeek-v3; e.g., see [DeepSeek-v2](https://arxiv.org/abs/2405.04434) and [DeepSeek-v2.5](https://api-docs.deepseek.com/news/news1210)[9](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-9-153722335).

#### DeepSeek-R1-Zero

> _“We explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process.”_ - from [1]

The first reasoning model proposed by DeepSeek was DeepSeek-R1-Zero. This model adopts an interesting training strategy that teaches the model to reason purely via large-scale RL—_without any SFT_. The model naturally explores and learns to leverage long CoT to solve complex reasoning problems through RL. DeepSeek-R1-Zero is the first open research effort to show that reasoning capabilities can be developed without supervised training.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c284b27-d0f4-4699-b4a0-24c37e8eef88_1840x882.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c284b27-d0f4-4699-b4a0-24c37e8eef88_1840x882.png)

(from [22])

**RL with GRPO.** The training of DeepSeek-R1-Zero begins with the DeepSeek-v3 [2] base model. We directly finetune this base model via RL. In particular, authors in [1] select [Group Relative Policy Optimization (GRPO)](https://huggingface.co/docs/trl/main/en/grpo_trainer) [3], which is depicted in the figure above, as their RL algorithm. The selection of RL algorithms for LLM training is an open and active research topic. Traditionally, researchers have used [PPO](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) for training LLMs, but there is a recent trend towards adopting simpler RL algorithms—_such as [REINFORCE](https://arxiv.org/abs/2402.14740) or [GRPO](https://arxiv.org/abs/2501.12599)_—for LLM training. The main reasons provided for the selection of GRPO in [1] are:

- A reduction in the cost of RL training.
    
- The elimination of the critic model, which is (usually) the same size as the policy model (i.e., the LLM itself).
    

**Defining rewards.** Unlike most traditional work on RL with LLMs, no neural reward models—_meaning LLM-based reward models that are trained over preference data_—are used to train DeepSeek-R1-Zero. Rather, the authors use a rules-based reward system, which _i)_ avoids reward hacking, _ii)_ saves on compute costs[10](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-10-153722335), and _iii)_ is simpler to implement. There are two types of rewards used in particular:

1. _Accuracy reward_: evaluates whether the model’s response is correct.
    
2. _Format reward_: enforces a desired format on the model’s output.
    

DeepSeek-R1-Zero is trained purely on automatically verifiable tasks, such as math and coding problems. For math problems with deterministic results, the model can provide its answer in a specified format, allowing us to verify via basic string matching. Similarly, coding problems can be verified by executing the code produced by the LLM in a sandbox over predefined test cases.

> _“The neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.”_ - from [1]

As mentioned above, the format reward provides a positive training signal when the model produces an output that uses the correct format or template. The format used in [1] simply places the model’s long CoT—_or the thinking / reasoning process_—between two special tokens: `<think>` and `</think>`. The model then produces its answer separately—_between the_ `<answer>` _and_ `</answer>` _tags_—after the completion of the reasoning process; see below for an illustration.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bdc9fc1-4032-41ba-9d7a-946f4826f826_1840x454.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9bdc9fc1-4032-41ba-9d7a-946f4826f826_1840x454.png)

(from [1])

**Learning via RL.** Despite using no SFT, DeepSeek-R1-Zero shows clear progress in its reasoning capabilities throughout the RL training process. The model’s performance on AIME 2024 is plotted below as training progresses. Here, the model’s performance gradually improves, eventually reaching parity with o1-preview[11](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-11-153722335). After training completes, DeepSeek-R1-Zero has improved from an initial performance of 15.6% to 71.0%—_or 86.7% when using majority voting with 16 votes_—on AIME 2024! Such results mirror the trends in performance we see with closed reasoning models—_DeepSeek-R1-Zero achieves impressive performance after RL training and can further improve its performance via parallel decoding strategies_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe19787e1-df29-413b-8ab3-7ed137eca9d9_1844x1028.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe19787e1-df29-413b-8ab3-7ed137eca9d9_1844x1028.png)

(from [1])

A full performance comparison between DeepSeek-R1-Zero and o1 models is provided in the table below. DeepSeek-R1 matches or exceeds the performance of o1-mini in most cases and performs comparably to o1-preview on several tasks. However, reasoning models from OpenAI perform much better in the coding domain—_DeepSeek-R1-Zero is clearly a less powerful coding model_. As we will soon see, this problem is fixed in DeepSeek-R1 (the follow-up model).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba93d001-c99e-4b80-a371-b97d92ea1adc_2008x506.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba93d001-c99e-4b80-a371-b97d92ea1adc_2008x506.png)

(from [1])

**What is happening here?** Clearly, DeepSeek-R1-Zero gains impressive reasoning capabilities from the RL training process outlined in [1]. However, _the dynamics of the model’s learning process are also quite observable_! Because we perform no SFT-style training, we can closely monitor the progression of the model’s reasoning strategy throughout the RL training process. As shown below, DeepSeek-R1-Zero learns to leverage more “thinking time”—_or just generate progressively longer chains of thought_—to improve its reasoning process as training progresses. The model naturally learns to leverage more test-time compute to solve harder problems!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e006bb-5959-485b-bb4a-d45b235a8a9d_1800x1004.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e006bb-5959-485b-bb4a-d45b235a8a9d_1800x1004.png)

(from [1])

Authors in [1] also observe several interesting tendencies that emerge naturally during training with RL. For example, the model develops an ability to reflect upon its own solutions by revisiting and evaluating prior components of its reasoning process. Similarly, the model begins to explicitly test out and explore alternative solutions or approaches during the problem solving process. This behavior is not explicitly programmed—_it arises naturally during training with RL_!

> _“The self-evolution of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously.”_ - from [1]

At the most basic level, the RL environment constructed in [1] allows the model to explore different strategies for arriving at a correct—_as determined by verification_—final solution. During exploration, we reward the model for:

1. Using the correct reasoning template or structure.
    
2. Producing a correct final solution.
    

From these rewards alone, the model learns how to solve complex reasoning problems. We do not explicitly need to teach the model how to decompose problems, search for a solution, perform backtracking, or evaluate its own line of thought. Instead, we just provide the correct incentives (or rewards) to the model during the training process. Then, the LLM can autonomously learn necessary behaviors for solving problems via an RL-based “self-evolution” process.

#### DeepSeek-R1

DeepSeek-R1-Zero shows us that LLMs can develop impressive reasoning capabilities from pure RL with no SFT, but this model has some minor bugs. For example, its readability is poor[12](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-12-153722335) and it incorrectly mixes languages together. Put simply, DeepSeek-R1-Zero is very good at reasoning, _but it lacks some of the desirable properties of a well-[aligned](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation) LLM_. As a solution, authors in [1] propose a new, multi-stage training process that integrates some “cold start” SFT data into training along with some other tricks. This training pipeline is used to create DeepSeek-R1, an LLM that is both aligned and capable of complex reasoning.

Similarly to DeepSeek-R1-Zero, we begin with DeepSeek-v3 as a base model. Then, DeepSeek-R1 undergoes four stages of training, including two SFT phases and two RL phases. The purpose of the SFT phases is to provide a better starting point for exploration during each of the RL phases. This training pipeline is one of the key contributions of [1]—_it provides an effective recipe for combining reasoning-style training with the standard post training recipe for LLMs._ Let’s take a deeper look at each stage of the training recipe used for DeepSeek-R1.

> _“To prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.”_ - from [1]

**Phase One: Cold Start (or Reasoning-Oriented SFT).** Prior to RL training, R1 is trained via SFT over a small dataset of long CoT examples, which is referred to in [1] as “cold start” data. There are a few different approaches that we can use to collect this cold start data:

1. Prompt a model (e.g., DeepSeek-v3) to produce long CoT data, either with few-shot examples or by instructing the model to generate detailed answers with accompanied reflection and verification.
    
2. Use the R1-Zero model to generate a large number of long CoT outputs, then ask humans to post-process and select the model’s best outputs.
    

Authors in [1] combine these approaches to collect “thousands of cold-start data” on which DeepSeek-v3 is finetuned directly via SFT. Because we are using long CoT data, _this is a reasoning-oriented finetuning process_. From this cold start data, the model learns a viable (initial) template for solving reasoning problems.

The data used for reasoning-oriented SFT introduces a human prior into DeepSeek-R1’s training process. We can explicitly select the style and pattern of data from which the model learns during this stage. For example, authors in [1] mention that they structure this data to include summaries of each long CoT, thus teaching the model to summarize its entire reasoning process prior to providing its final answer. This data serves as a seed for the RL training process—_the model begins its self-exploration by matching the style of the SFT training data._

**Stage Two: Reasoning-Oriented RL.** After SFT, we just repeat the large-scale RL training process proposed by R1-Zero to enhance the underlying model’s ability to handle reasoning-intensive tasks. The only change made for DeepSeek-R1 is the addition of a language consistency reward, calculated as the portion of the model’s output written in the desired target language. This language consistency reward is found in [1] to slightly deteriorate the model’s reasoning capabilities. However, language consistency improves the overall alignment of the resulting model with human preferences—_the model’s output is more fluent and readable_.

**Stage Three: Rejection sampling.** After the convergence of reasoning-oriented RL, we use the resulting model to collect a large and diverse SFT dataset. Unlike the initial cold start SFT phase, however, we collect more than just reasoning-oriented data. Namely, we augment the reasoning data with general purpose data so that the model can learn from a broader set of problems and domains.

To collect more reasoning data, authors in [1]:

1. Curate a diverse set of reasoning-based prompts.
    
2. Generate candidate trajectories[13](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-13-153722335) using the model from phase two.
    
3. Perform rejection sampling—_or filter and select the top trajectories based on the quality and correctness or each trajectory_.
    

This is the same training-time rejection sampling process that we learned about earlier in this post! Interestingly, we rely upon more than rules-based techniques for verification in this phase. We also incorporate additional data from non-verifiable domains by using DeepSeek-v3 as a [generative reward model](https://arxiv.org/abs/2408.15240) or weak verifier. After applying heuristic filtering (e.g., removing outputs with language mixing or long paragraphs), we arrive at a final set of 600K reasoning trajectories.

> _“We reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting.”_ - from [1]

The SFT dataset from this stage includes a substantial ratio of non-reasoning data (e.g., writing or translation examples). We source this data from the same post training dataset used for DeepSeek-v3. However, the data is augmented by asking DeepSeek-v3 to generate a long CoT to explain the outputs of complex queries—_simpler queries, however, are not given any CoT_. A total of 200K non-reasoning examples are collected, forming an SFT dataset of 800K examples.

**Stage Four: General-purpose RLHF.** The final training stage of DeepSeek-R1 aligns the model with human preferences while continuing to hone its reasoning abilities. Similarly to the prior stage, we train the model over a combination of reasoning-based and general purpose data. In particular, we train the model using RL with a combination of different rewards for each type of data:

- Rules-based rewards (same as R1-Zero) for reasoning-based problems.
    
- Neural reward models—_trained over human preference pairs, just as in standard RLHF_—for general purpose data.
    

DeepSeek-R1 is aligned to be more helpful and harmless on general purpose data. These are two [very common alignment criteria](https://arxiv.org/abs/2204.05862) used in LLM research. Each of these criteria are modeled with a separate neural reward model that is trained over a (supervised) dataset of human preferences. Helpfulness rewards are only measured over the final answer of the model (i.e., excluding the long CoT), while harmless rewards consider the model’s entire output trajectory[14](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-14-153722335). By combining rules and preference-based rewards, DeepSeek-R1 can be aligned to human preferences while maintaining strong reasoning performance.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d42ce87-35e7-4af2-8a45-cf348df75132_1918x1094.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d42ce87-35e7-4af2-8a45-cf348df75132_1918x1094.png)

(from [1])

**How does it perform?** As shown above, R1 matches or surpasses the performance of o1 on most reasoning tasks. Unlike R1-Zero, R1 also has reasonably strong coding abilities. On general purpose tasks, R1 continues to perform well as a result of its hybrid training pipeline. In general, R1 is a very capable model that seems to be on par with OpenAI’s o1 and can solve a wide variety of tasks—_including both traditional and reasoning-oriented tasks_—with high accuracy.

One interesting observation about this model (and other reasoning models) is that it performs poorly on instruction following benchmarks (e.g., [IF-Eval](https://arxiv.org/abs/2311.07911)) compared to standard LLMs. Currently, _reasoning models seem to be worse than standard LLMs at following instructions_. In the future, I personally believe this trend is likely to reverse. In theory, reasoning models should be capable of leveraging their thought process to better interpret and adhere to a prompt provided by a human user. For example, [deliberative alignment](https://arxiv.org/abs/2412.16339) follows a somewhat similar approach.

**Is SFT necessary?** R1-Zero emphasizes the ability to train strong reasoning models without SFT, while the full R1 model uses several SFT phases to obtain a stronger, final model. So, we might begin to wonder: _Should we use SFT of not?_

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6b1fbd1-3f9b-4983-8914-1a93d2d2fa87_2388x1154.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6b1fbd1-3f9b-4983-8914-1a93d2d2fa87_2388x1154.png)

Is SFT necessary for reasoning models?

For a standard LLM, SFT provides a high-quality starting point for RLHF. If we applied RLHF directly to the base model, the learning process would be much less efficient. Data for SFT is either synthetically generated or manually created by humans. Generally, collecting data for SFT is expensive (both in terms of time and money). _We have to manually write a good response from scratch for the LLM_!

Collecting such SFT data for reasoning models is more difficult due to their long CoT. Asking humans to manually create long CoT data would be time consuming and expensive! Our only option is to generate this data synthetically, but:

1. Generating this particular style of output with a model may still be hard.
    
2. Correctly verifying such long outputs is difficult.
    

Given the additional complexity of collecting SFT data for reasoning models, authors in [1] first try to avoid SFT altogether! From these experiments, we see that such reasoning abilities naturally emerge from pure RL—_this is an incredible discovery_! However, the resulting model has several shortcomings (e.g., language mixing). When we train over some SFT prior to RL (i.e., a “cold start”), we provide a better prior to RL, which _i)_ eliminates instability during the initial phases of RL training, _ii)_ speeds up up training and _iii)_ improves model quality. So, SFT is not completely necessary, _but it is still practically useful if we have the data_!

#### Distilled Models

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e1abb7a-4035-421b-bcbe-35ccfdb71e47_1248x534.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e1abb7a-4035-421b-bcbe-35ccfdb71e47_1248x534.png)

Illustration of the knowledge distillation process ([source](https://arxiv.org/abs/2006.05525))

Beyond DeepSeek-R1, authors in [1] release a series of dense models that are distilled from R1. The [distillation process](https://arxiv.org/abs/2402.13116) is found to significantly enhance the reasoning capabilities of smaller and more efficient models. The full DeepSeek-R1 model is large (i.e., a 671 billion parameter [Mixture-of-Experts model](https://cameronrwolfe.substack.com/i/154340424/deepseek-v-and-deepseek-v)), so these distilled models are practically useful—_they are_ _comparable to R1 but more cost sensitive and easier to use_. Additionally, the release of these distilled models matches recent trends in closed reasoning models (e.g., o1-mini and o3-mini).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa60aba-ec97-40c9-b10a-1b1a262ff251_1222x574.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa60aba-ec97-40c9-b10a-1b1a262ff251_1222x574.png)

(from [1])

**Distilling R1.** To create these models, we begin with several sizes of two base models[15](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-15-153722335)—_Qwen-2.5 [20] and LLaMA-3 [21]_. We then train the base models via SFT over the 800,000 supervised training examples curated in the third stage of the training pipeline for DeepSeek-R1—_that’s it_!

This is a simple knowledge distillation pipeline, _but the results are impressive_. As shown above, the distilled Qwen2.5-14B model outperforms [QwQ-32B-Preview](https://qwenlm.github.io/blog/qwq-32b-preview/), which was the best open reasoning model prior to the release of R1. Additionally, even the smallest distilled models outperform standard closed LLMs that are not optimized for reasoning (e.g., GPT-4o), while the 32 and 70 billion parameter distilled models exceed the performance of o1-mini on most benchmarks.

> _“Distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL require enormous computational power and may not even achieve the performance of distillation.”_ - from [1]

**Distillation versus RL.** Although we see that distillation is effective in the discussion above, we might wonder whether we could get better results by just directly applying the large-scale RL training process used by DeepSeek-R1 to these smaller models. Interestingly, authors in [1] observe that distilling the Qwen2.5-32B base model from R1—_using the distillation approach described above_—outperforms directly training this model via large-scale RL; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbc4ed3b-81bd-44a2-b8b7-5c0ec792f3cd_2464x406.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbc4ed3b-81bd-44a2-b8b7-5c0ec792f3cd_2464x406.png)

(from [1])

In other words, the reasoning patterns discovered by large models are crucial for improving the reasoning capabilities of these smaller, dense models. However, authors in [1] do make the following additional points:

- It is possible that the performance of distilled models could be further improved via added training with RL.
    
- “Advancing beyond the boundaries of intelligence”—_or creating new reasoning models that even exceed the performance of models like DeepSeek-R1_—will still require powerful base models and large-scale training with RL.
    

**Other distilled reasoning models.** Given the simplicity of training high-quality reasoning models via distillation, a wide variety of reasoning models were released by the research community following the proposal of R1. Some of the most notable releases are:

- [Sky-T1](https://novasky-ai.github.io/posts/sky-t1/) and [Sky-T1-Flash](https://novasky-ai.github.io/posts/reduce-overthinking/)
    
- [Bespoke Stratos](https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation)
    
- [LIMO](https://arxiv.org/abs/2502.03387)
    
- [S1](https://arxiv.org/abs/2501.19393)
    
- [RedStar](https://arxiv.org/abs/2501.11284)
    

There are many more models that have been released as well! The current pace of reasoning model releases is reminiscent of the post-LLaMA era of LLM research. After the release of a powerful open base model (i.e., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)), we saw a wide variety of model variants released that were based on this model (e.g., [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/), [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) and many more). Now, we have access to a strong open reasoning model, as we are seeing a very similar trend! The research in this area is very interesting and deserving of its own post—_stay tuned_!

## Key Emerging Trends

We have now learned about a variety of reasoning models, beginning with closed models like o1 or o3 and ending with a fully-outlined replication of these models in DeepSeek-R1. As we have learned about this research, there are a few common trends that begin to emerge. These trends, outlined below, make some important distinctions between research on reasoning models and standard LLMs.

**Long CoT (and inference-time scaling).** The key distinction between reasoning models and standard LLMs is their output structure. Instead of just directly generating a final answer (with an optional concise explanation), reasoning models generate a long CoT that describes their reasoning process in great detail. This long CoT can be of variable length, enabling controllable compute costs at inference time: _longer CoT = more tokens = more compute_. In this way, using more compute at inference time—_by generating a longer CoT_—has become a tool that can allow users to dynamically improve a model’s reasoning capabilities.

**Self-evolution through RL.** Obviously, the ability of LLMs to execute complex reasoning strategies within their long CoT is new and exciting. From recent research, we learn that the key contributor to the development of these special abilities is large-scale RL training. We see in [1] that such reasoning capabilities naturally emerge during RL if the model is correctly incentivized, usually via rules-based rewards that are deterministic and reliable. Additionally, we can further improve a model’s reasoning capabilities by using more compute for training via RL—_this is yet another scaling law that we can leverage_!

**Less supervision.** The dependence of reasoning models upon human supervision is less pronounced relative to standard LLMs. In particular, rewards during RL training are derived primarily from rules-based systems, instead of relying upon human preferences. Of course, reasoning models still have several areas of dependence upon human supervision; e.g., the base model is trained with human-curated data and verification relies upon human-provided ground truth labels. However, there is still a big push by reasoning models like R1 (and especially R1-Zero) to demonstrate that reasoning capabilities can develop autonomously.

**Distillation is effective.** Now that we have access to large and powerful reasoning models, we can distill the capabilities of these models into smaller, dense models using simple strategies! This finding has led to an explosion of research in this area, and we are likely to see many more efficient and distilled reasoning models released in the near future. One key question in this area is whether smaller models will generalize or [struggle to fully match](https://arxiv.org/abs/2305.15717) the breadth of their teachers.

> _“When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance.”_ - from [1]

**New problems to solve.** Above all else, the advent of reasoning models has raised a variety of new (and interesting!) questions that we need to solve:

- How do we handle safety training for long CoT?
    
- What is the best balance between general / reasoning capabilities?
    
- What is the optimal role of SFT in training reasoning models?
    
- How do we minimize “overthinking” in long CoT?
    
- How do we handle efficient hosting of reasoning models?
    

As mentioned at the beginning of this post, reasoning models are a truly new type of LLM that will force us to rethink existing frameworks. Solidified techniques that have been used for years (e.g., few-shot prompting) are becoming obsolete for these new models. _The field of LLM research is re-inventing itself once again_.

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." _arXiv preprint arXiv:2501.12948_ (2025).

[2] Liu, Aixin, et al. "Deepseek-v3 technical report." _arXiv preprint arXiv:2412.19437_ (2024).

[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." _arXiv preprint arXiv:2402.03300_ (2024).

[4] OpenAI. “Introducing OpenAI o1-preview” _[https://openai.com/index/introducing-openai-o1-preview/](https://openai.com/index/introducing-openai-o1-preview/)_ (2024).

[5] OpenAI. “Learning to Reason with LLMs” _[https://openai.com/index/learning-to-reason-with-llms/](https://openai.com/index/learning-to-reason-with-llms/)_ (2024).

[6] OpenAI. “OpenAI o3-mini” _[https://openai.com/index/openai-o3-mini/](https://openai.com/index/openai-o3-mini/)_ (2025).

[7] Rein, David, et al. "Gpqa: A graduate-level google-proof q&a benchmark." arXiv preprint arXiv:2311.12022 (2023).

[8] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.

[9] Zelikman, Eric, et al. "Star: Bootstrapping reasoning with reasoning." Advances in Neural Information Processing Systems 35 (2022): 15476-15488.

[10] Gulcehre, Caglar, et al. "Reinforced self-training (rest) for language modeling." arXiv preprint arXiv:2308.08998 (2023).

[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).

[12] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).

[13] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).

[14] Bespoke Labs. “Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation” _[https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation](https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation)_ (2025).

[15] Welleck, Sean, et al. "From decoding to meta-generation: Inference-time algorithms for large language models." _arXiv preprint arXiv:2406.16838_ (2024).

[16] Aggarwal, Pranjal, Bryan Parno, and Sean Welleck. "AlphaVerus: Bootstrapping formally verified code generation through self-improving translation and treefinement." _arXiv preprint arXiv:2412.06176_ (2024).

[17] Chen, Xinyun, et al. "Teaching large language models to self-debug." _arXiv preprint arXiv:2304.05128_ (2023).

[18] Wang, Yifei, et al. "A Theoretical Understanding of Self-Correction through In-context Alignment." _arXiv preprint arXiv:2405.18634_ (2024).

[19] Huang, Jie, et al. "Large language models cannot self-correct reasoning yet." _arXiv preprint arXiv:2310.01798_ (2023).

[20] Yang, An, et al. "Qwen2. 5 technical report." _arXiv preprint arXiv:2412.15115_ (2024).

[21] Dubey, Abhimanyu, et al. "The llama 3 herd of models." _arXiv preprint arXiv:2407.21783_ (2024).

[22] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." _arXiv preprint arXiv:2402.03300_ (2024).

[1](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-1-153722335)

For example, o1-preview did not have the ability to upload files, could not understand other modalities of data (e.g., images), and had no web search capabilities.

[2](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-2-153722335)

Although the details of how OpenAI controls the amount of inference-time compute used by o1-style models are not clear, it seems from [their blog post](https://openai.com/index/learning-to-reason-with-llms/) that these models have multiple “settings” for the amount of compute that they can use at inference time. These settings are likely related to the length of the model’s long CoT, so high inference-time compute settings would simply generate very long chains of thought.

[3](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-3-153722335)

Technically, this benchmark is still unbeaten because o3 exceeded the maximum computational budget when achieving >85% accuracy.

[4](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-4-153722335)

This benchmark was described by [Terence Tao](https://en.wikipedia.org/wiki/Terence_Tao) as likely to be unsolved by AI systems for “several years at least”. There has been some recent questioning of OpenAI’s performance on this benchmark due to [conflict of interest](https://techcrunch.com/2025/01/19/ai-benchmarking-organization-criticized-for-waiting-to-disclose-funding-from-openai/) between OpenAI and the organization that created this benchmark ([EpochAI](https://epoch.ai/)).

[5](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-5-153722335)

Notably, o3-mini does NOT have vision support, unlike o1.

[6](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-6-153722335)

In contrast, RLHF trains the reward model over various kinds of human preferences, usually via a [ranking loss](https://gombru.github.io/2019/04/03/ranking_loss/).

[7](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-7-153722335)

In addition to these two techniques, we could also perform some sort of search (e.g., [monte carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search))—see [here](https://arxiv.org/abs/2405.00451) for an example. However, we can also categorize search-based methods as generating more tokens at inference time.

[8](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-8-153722335)

The length of a long CoT may vary depending on model settings (e.g., OpenAI provides several settings for “reasoning effort”) or problem difficulty.

[9](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-9-153722335)

There is also a [DeepSeek-v1 model](https://arxiv.org/abs/2401.02954), but this model is dense (i.e., not an MoE) and much different from the model family that is used for DeepSeek-R1.

[10](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-10-153722335)

The compute savings come from the fact that we do not have to train (or run inference on) any reward models.

[11](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-11-153722335)

See [here](https://platform.openai.com/docs/models#o1) for a full list of OpenAI’s o1 models. For clarity, the `o1-0912` model mentioned in [1] is the same as the `o1-preview` model.

[12](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-12-153722335)

For example, the model lacks markdown formatting and highlighting within its answers, which is a common feature for modern LLMs.

[13](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-13-153722335)

In [1], authors refer to the long CoT outputs generated by the DeepSeek-R1 model variants as “trajectories”.

[14](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-14-153722335)

Notably, this is in direct contrast to the (original) approach adopted by OpenAI. o1-style models have their long CoT hidden from the end user, and these reasoning traces do not undergo any safety training. The rationale for this strategy is to allow the model to be more transparent in its trajectory, which improves interpretability.

[15](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models#footnote-anchor-15-153722335)

The exact models used are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. Notably, we do not always start with the base model—_many of these models have undergone post training_!

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Sarvagya's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48fc47cc-1fcb-43ca-8165-e4939312387e_144x144.png)



](https://substack.com/profile/227554059-sarvagya)

[

![Amir's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d602761-5fd8-4db1-a4bd-0b60c244a990_144x144.png)



](https://substack.com/profile/22365321-amir)

[

![Wietse Smits's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a01e6c3-c38f-46f5-899a-488e51408614_96x96.jpeg)



](https://substack.com/profile/279808259-wietse-smits)

[

![Hannuri Ha's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb81f8cf0-c938-4d30-905a-5a6a99dc8bbf_336x336.jpeg)



](https://substack.com/profile/93148505-hannuri-ha)

[

![Michael Spencer's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75d1bf99-dcf3-4af6-be2a-416c08c954a1_450x450.jpeg)



](https://substack.com/profile/21731691-michael-spencer)

204 Likes∙

[23 Restacks](https://substack.com/note/p-153722335/restacks?utm_source=substack&utm_content=facepile-restacks)

204

- 

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

23

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![Gianfranco Filice's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c97316a-6e8a-48c8-81d0-fcdca1268119_400x400.jpeg)



](https://substack.com/profile/4259088-gianfranco-filice?utm_source=comment)

[Gianfranco Filice](https://substack.com/profile/4259088-gianfranco-filice?utm_source=substack-feed-item)

[Gianfranco’s Substack](https://gianfrancofilice.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[3月13日](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/99977105 "2025年3月13日 02:15")

Liked by Cameron R. Wolfe, Ph.D.

A post of exceptional quality, thanks Cameron.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/99977105)

[

![Hugo Delamain's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F81cde8c7-b5c9-42b4-a437-e1767aae452b_1536x2048.jpeg)



](https://substack.com/profile/46119466-hugo-delamain?utm_source=comment)

[Hugo Delamain](https://substack.com/profile/46119466-hugo-delamain?utm_source=substack-feed-item)

[The Business of Stuff](https://www.businessofstuff.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[3月23日](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comment/102674015 "2025年3月23日 16:24")

This is great - exactly what I was looking for!

Like

Reply

Share

[1 more comment...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

[Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

[Understanding how SFT works from the idea to a working implementation...](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

Sep 11, 2023 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

55

[

5

](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68686a01-2b31-4694-8c04-a562ffd725ad_2210x1244.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture


----


[

![Deep (Learning) Focus](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png)



](https://cameronrwolfe.substack.com/)

# [Deep (Learning) Focus](https://cameronrwolfe.substack.com/)

Subscribe

![dfsj's avatar](https://substackcdn.com/image/fetch/w_40,h_40,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

# Vision Large Language Models (vLLMs)

### Teaching LLMs to understand images and videos in addition to text...

[

![Cameron R. Wolfe, Ph.D.'s avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg)



](https://substack.com/@cwolferesearch)

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

Mar 31, 2025

112

- 

[

8

](https://cameronrwolfe.substack.com/p/vision-llms/comments)

12

Share

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18346de8-f6b0-447e-a759-3bf2488f55e0_2270x1218.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18346de8-f6b0-447e-a759-3bf2488f55e0_2270x1218.png)

After the popularization of text-based large language models (LLMs), one of the most important questions within the research community was how we could extend such powerful models to understand other modalities of data (e.g., images, video or speech). Research on multi-modal LLMs is promising for several reasons:

- Improving model capabilities.
    
- Uncovering new sources of training data.
    
- Expanding the scope of problems that LLMs can solve.
    

Recently, vision-based LLMs—_or vLLMs for short, these are LLMs that can ingest images and videos as input in addition to text_—have become more popular. For example, most recent OpenAI models support visual inputs, and Meta has released a vision-based variant of LLAMA-3, called LLaMA-3.2 Vision. In this overview, we will aim to understand how vLLMs work from first principles, starting with basic concepts and eventually studying how LLaMA-3.2 Vision is practically implemented. As we will learn, vLLMs—_despite their impressive capabilities_—are not actually much different than text-based LLMs.

## The Building Blocks of vLLMs

To fully understand vLLMs, we need to start from the beginning. In this section, we will cover some of the fundamental concepts used to build these models, including ideas like cross-attention and encoders for images and video. We will (mostly) assume knowledge of the basic concepts behind text-based LLMs, such as a high-level understanding of the transformer architecture. However, readers who are unfamiliar with these concepts can find more details at the link below.

[](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[

![Decoder-Only Transformers: The Workhorse of Generative LLMs](https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[

#### Decoder-Only Transformers: The Workhorse of Generative LLMs

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)[Cameron R. Wolfe, Ph.D.](https://substack.com/profile/29736521-cameron-r-wolfe-phd)

·

2024年3月4日

[

Read full story

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

#### Cross-Attention (and Transformers)

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0c20b8a-b589-4282-8be7-e2509f4e0803_912x1112.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0c20b8a-b589-4282-8be7-e2509f4e0803_912x1112.png)

(from [8])

The [transformer architecture](https://jalammar.github.io/illustrated-transformer/) [8] is used universally within language modeling research. In its original form, the transformer architecture has two components: _an encoder and a decoder_. As shown above, the encoder and decoder contain repeated blocks of:

1. _Self-attention_: transforms each token vector based on the other tokens that are present in the sequence.
    
2. _Feed-forward transformation_: transforms each token vector individually.
    

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a4634e6-3d72-4053-84bf-84bef43101d5_1630x804.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a4634e6-3d72-4053-84bf-84bef43101d5_1630x804.png)

Decoder-only transformer architecture

The **decoder-only transformer** is the variant of the transformer architecture that is most commonly used by [GPT-style](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (generative) LLMs. Most vLLMs also utilize a decoder-only architecture, but additional modules are added to the architecture to handle vision-based inputs. Put simply, this architecture is the same as the transformer, but it has no encoder component—_hence the name “decoder-only”_.

**Original decoder.** The decoder-only transformer only has masked self-attention and a feed-forward transformation in each of its blocks. However, the decoder from the original transformer architecture has an extra cross-attention module in each of its blocks. Self-attention computes attention over the tokens in a single sequence. In contrast, cross-attention considers two sequences of tokens—_the tokens from the encoder and the tokens from the decoder—_and computes attention between these two sequences[1](https://cameronrwolfe.substack.com/p/vision-llms#footnote-1-158954054). By doing this, we allow the decoder to consider the representations produced by the encoder when generating its output! Let’s first try to understand self-attention, then we will cover cross-attention.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8fec4d1-3b72-4e17-8a01-2e7c4f3b7a5c_1949x930.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8fec4d1-3b72-4e17-8a01-2e7c4f3b7a5c_1949x930.png)

**Self-attention at a glance.** The input to a self-attention mechanism is a sequence of token vectors. Self-attention forms an output representation for each token by considering all other tokens in the sequence. To do this, the self-attention operation creates three separate linear projections—_called the keys, queries, and values_—of the token vectors. As shown above, we can then use the keys and queries to compute an attention score between every pair of tokens in the sequence. This attention score captures how important each token is to every other token in the sequence—_or how much some token should “pay attention to” another token_. We can multiply these attention scores by the values to obtain our final output. A basic implementation of self-attention is provided below[2](https://cameronrwolfe.substack.com/p/vision-llms#footnote-2-158954054).

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||import math|
||import torch|
||from torch import nn|
||import torch.nn.functional as F|
|||
||class SelfAttention(nn.Module):|
|||
||def __init__(self, d):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||"""|
||super().__init__()|
||self.d = d|
|||
||# key, query, value projections for all heads, but in a batch|
||# output is 3X the dimension because it includes key, query and value|
||self.c_attn = nn.Linear(d, 3*d, bias=False)|
|||
||def forward(self, x):|
||# compute query, key, and value vectors in batch|
||# split the output into separate query, key, and value tensors|
||q, k, v = self.c_attn(x).split(self.d, dim=2)|
|||
||# compute the attention matrix and apply dropout|
||att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))|
||att = F.softmax(att, dim=-1)|
|||
||# compute output vectors for each token|
||y = att @ v|
||return y|

[view raw](https://gist.github.com/wolfecameron/a809ef1e9bd176344ab59303c3e00389/raw/efb77715beacf745fa2f72e3fb10a1ccc21c8757/bidir_self_attn.py)[bidir_self_attn.py](https://gist.github.com/wolfecameron/a809ef1e9bd176344ab59303c3e00389#file-bidir_self_attn-py) hosted with ❤ by [GitHub](https://github.com/)

**How does cross-attention work?** A schematic depiction of cross-attention is provided below. As we can see, this module is not much different than self-attention. The key difference here is in the initial linear projections used to compute the key, query and value matrices. Instead of computing all three of these matrices by linearly projecting a single sequence of token vectors, we linearly project two different sequences of vectors; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc806734f-037f-4f6a-8863-b7383964d8ec_2098x1058.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc806734f-037f-4f6a-8863-b7383964d8ec_2098x1058.png)

The query matrix is produced by linearly projecting the first sequence, while both key and value matrices are produced by linearly projecting the second sequence. As a result, our attention matrix contains all pairwise attention scores between tokens in the first and second sequence. The length of the sequences need not be equal, and the length of the output will match that of the first sequence.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. [Learn more about bidirectional Unicode characters](https://github.co/hiddenchars)

[Show hidden characters](https://cameronrwolfe.substack.com/p/%7B%7B%20revealButtonHref%20%7D%7D)

|   |   |
|---|---|
||import math|
||import torch|
||from torch import nn|
||import torch.nn.functional as F|
|||
||class CrossAttention(nn.Module):|
|||
||def __init__(self, d):|
||"""|
||Arguments:|
||d: size of embedding dimension|
||"""|
||super().__init__()|
||self.d = d|
|||
||# linear projection for producing query matrix|
||self.w_q = nn.Linear(d, d, bias=False)|
|||
||# linear projection for producing key / value matrices|
||self.w_kv = nn.Linear(d, 2*d, bias=False)|
|||
||def forward(self, x_1, x_2):|
||# compute query, key, and value matrices|
||q = self.w_q(x_1)|
||k, v = self.w_kv(x_2).split(self.d, dim=2)|
|||
||# compute the attention matrix and apply dropout|
||att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))|
||att = F.softmax(att, dim=-1)|
|||
||# compute output vectors for each token in x_1|
||y = att @ v|
||return y|

[view raw](https://gist.github.com/wolfecameron/5646b2092d41d6d31ec1abb28b3b930a/raw/761cf359329f08286e4f8ae24c31447e79c4259d/cross_attention.py)[cross_attention.py](https://gist.github.com/wolfecameron/5646b2092d41d6d31ec1abb28b3b930a#file-cross_attention-py) hosted with ❤ by [GitHub](https://github.com/)

An implementation of cross-attention is provided above. As outlined in this implementation, we are no longer computing attention scores between tokens within a single sequence. Rather, we are computing inter-sequence attention scores, _thus forming a fused representation of the two input sequences_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F528bb4b8-06a4-4e44-81a5-c49d7c285f42_2232x1316.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F528bb4b8-06a4-4e44-81a5-c49d7c285f42_2232x1316.png)

Integrating image encoder features into an LLM with cross-attention

**Application to vLLMs.** Our explanation of cross-attention might seem random at this point in the overview. As we will see, however, cross-attention is used constantly in multi-modal LLM research. We can use cross-attention to fuse image representations produced by a vision model into a text-based LLM; see above. In other words, we can incorporate visual information into an LLM as it generates its output, allowing the model to ingest and interpret images (or other modalities of data) as input in addition to just text!

#### [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) [3]

> _“We apply a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.”_ - from [3]

Although the transformer (and its many variants like [BERT](https://arxiv.org/abs/1810.04805) and [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)) were first proposed for natural language processing applications, this influential model architecture has since been expanded to applications in the computer vision domain. The Vision Transformer [3] (or ViT for short) is the most commonly used architecture today. As shown in the figure below, this architecture looks very similar to an [encoder-only (BERT-style) transformer architecture](https://cameronrwolfe.substack.com/p/language-understanding-with-bert). We simply take a sequence of vectors as input and apply a sequence of transformer blocks that contain both _i)_ bidirectional self-attention and _ii)_ a feed-forward transformation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e2d7b7-26a1-4e68-939d-caaab3f133d9_1758x1200.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e2d7b7-26a1-4e68-939d-caaab3f133d9_1758x1200.png)

Standard Vision Transformer (ViT) architecture

**Handling input images.** The input for a vision transformer is an image. In order to pass this image as input to our transformer, however, we need to convert the image into a list of vectors—_resembling a sequence of textual token vectors_. For ViTs, we do this by segmenting an image into a set of patches and flattening each patch into a vector. From here, these vectors may not be of the same size expected by the transformer, so we just linearly project them into the correct dimension.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b21b27b-e8a1-4542-b323-c3c17abbe379_1078x662.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b21b27b-e8a1-4542-b323-c3c17abbe379_1078x662.png)

(from [3])

Similarly to a normal transformer, we add positional embeddings to the vector for each patch. Here, the positional embedding captures the 2D position of each patch within an image. The output of this transformer architecture is a sequence of vectors for each patch that is of the same size as the input. To solve tasks like image classification, we can just add an additional classification module (e.g., a linear layer) to the end of this model, as shown in the figure above.

**Why the encoder?** We use an encoder-only transformer architecture for the ViT, instead of the decoder-only transformer architecture that is used by most LLMs. _The reason for this is that the ViT is not generative_. For LLMs, we train the model via [next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) to generate sequences of text. As a result, we need to use masked self-attention in each transformer layer so that the model cannot look forward in the sequence at future tokens. Otherwise, the model would be able to cheat when predicting the next token! In contrast, the ViT should be able to look at the entire sequence of image patches to form a good representation of the image—_we do not need to predict the next patch in this input sequence_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6013a7d8-f5e8-4b43-a15f-8690e0bbe93c_1516x412.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6013a7d8-f5e8-4b43-a15f-8690e0bbe93c_1516x412.png)

(from [3])

**Training ViT.** The original ViT model in [3] shares the same architecture as BERT. As shown above, multiple sizes of ViT are trained, the largest of which is ViT-H (or ViT-Huge)—_we will see this model again later in the overview_. All ViT models are trained using supervised image classification on datasets of varying sizes. When ViTs are trained over small or mid-sized datasets (e.g., ImageNet), they perform comparably to—_or slightly worse than_—ResNets[3](https://cameronrwolfe.substack.com/p/vision-llms#footnote-3-158954054) of comparable size. However, ViTs begin to shine when pretrained over much larger datasets (e.g., [JFT-300M](https://paperswithcode.com/dataset/jft-300m)) and finetuned afterwards on downstream tasks; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a07f5db-8f99-4901-a703-5f64d5dac7c1_2078x1142.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a07f5db-8f99-4901-a703-5f64d5dac7c1_2078x1142.png)

(from [3])

#### [Contrastive Language-Image Pre-Training (CLIP)](https://arxiv.org/abs/2103.00020) [4]

The standard ViT is trained over a large dataset of supervised image classification examples. These models perform best when pretrained over a massive volume of annotated (usually by humans) data, which is difficult and expensive to obtain. In [4], authors explore an alternative approach that uses image-caption pairs, which are more readily available online, to train a powerful image representation model. This approach is called Contrastive Language-Image Pre-Training (CLIP).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e019441-7531-4d38-8a45-7e523dabebac_3284x1548.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e019441-7531-4d38-8a45-7e523dabebac_3284x1548.png)

(from [4])

**CLIP architecture.** The CLIP model is made up of two independent components: an image encoder and a text encoder. Given an image-text pair as input, we pass these inputs separately to their corresponding encoder to get an associated vector representation. The image encoder is a standard ViT model [3], whereas the text encoder is a [decoder-only transformer](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse) (i.e., a typical GPT-style LLM). CLIP’s text encoder is not used to generate text (at least in [4]), but the authors use a decoder-only architecture to simplify the extension of CLIP to generative applications in the future. A depiction of CLIP’s architecture is provided above.

> _“The simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet.”_ - from [4]

**Contrastive learning.** There are many ways that we could approach training the CLIP model described above. For example, we could classify the images based on the words in the caption [5] or use the LLM component of the architecture to generate captions based on the image [6]. However, these objectives were found in prior work to either perform poorly or cause the model to learn slowly. The key contribution of [4] is the idea of using a simple and efficient training objective— _based upon ideas from [contrastive learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)_—to learn from image-text pairs.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d7c264c-a19a-43f4-b867-353f68581cbe_864x430.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d7c264c-a19a-43f4-b867-353f68581cbe_864x430.png)

A schematic depiction of CLIP training objective

More specifically, CLIP is trained using the simple task of classifying the correct caption for an image among a group of candidate captions (i.e., all other captions within a training batch). Practically, this objective is implemented by:

1. Passing a group of images and textual captions through their respective encoders (i.e., the ViT for images and the LLM for text).
    
2. Maximizing the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between image and text embeddings (obtained from the encoders) of the true image-caption pairs.
    
3. Minimizing the cosine similarity between all other image-caption pairs.
    

This objective is referred to as a [multi-class N-pair (or InfoNCE) loss](https://github.com/RElbers/info-nce-pytorch) [7] and is commonly used in the contrastive and metric learning literature.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e4e6f45-c169-4d06-887e-9a23b5bf1a55_2380x1048.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e4e6f45-c169-4d06-887e-9a23b5bf1a55_2380x1048.png)

Zero-shot classification (left [4]) and CLIP training efficiency (right [4])

**Using CLIP.** Although the CLIP model is trained with both an image and text encoder, _most of the work we will see in this overview only uses the image encoder from CLIP_. The key contribution of CLIP is not the model architecture, but rather the training objective. Using both an image and text encoder allows us to train the image encoder using the contrastive objective described above, which is very efficient (see above) and does not rely on large amounts of supervised data. The CLIP model architecture can be useful as a whole; e.g., we can use it to perform zero-shot image classification as shown above. However, _we can also train a CLIP model solely for the purpose of obtaining a high-quality image encoder_!

#### From Images to Videos

To process an image with an LLM, we can simply pass this image to an image encoder (e.g., CLIP) to produce a set of vectors—_or embeddings_—to represent this image. Then, the LLM can take these embeddings as an additional input (we will cover more details on this later in the overview). However, _what if we have access to a video instead of an image?_ Interestingly, processing video inputs with an LLM is not that much different than processing image inputs—_we just need some strategy for converting this video into a set of vectors, similarly to an image_!

**What is a video?** At the simplest level, a video is just an ordered list of images, commonly referred to as “frames”. Usually, images are stored in [RGB format](https://en.wikipedia.org/wiki/RGB_color_model). For example, the image in the figure below has three color channels—_red, blue and green_—as well as a height and width of five. The size of this image is `3 (color channels) × 5 (height) × 5 (width)`. We can also stack several images into a mini-batch of images, forming a tensor of size `batch × 3 × 5 × 5`.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5357ad54-32e7-44d4-9fcc-0236114a062c_1630x824.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5357ad54-32e7-44d4-9fcc-0236114a062c_1630x824.png)

Comparing the data structure of images and videos

The structure of video is not much different—_a video is just a collection of ordered frames_. When viewed in the correct temporal order, these frames reveal the movement of a scene through time, forming a video. Similar to images, each of these frames are usually represented in RGB-format, and all frames in a video have the same spatial resolution. For example, the video in the figure above has three frames, each with three color channels and a height and width of five, forming a tensor of size `3 (frames) × 3 (color channels) × 5 (height) × 5 (width)`. We can also create a mini-batch of videos, but we must make sure that each video has the same number of frames—_this is usually done by extracting fixed-length “clips” from the video (e.g., with 64 frames)_.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb253f0e6-f5e7-475f-adee-b5b3dd94430a_1884x1122.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb253f0e6-f5e7-475f-adee-b5b3dd94430a_1884x1122.png)

Sub-sampling frames in a video

**Frame rate.** Videos usually have a fixed number of [frames per second (FPS)](https://en.wikipedia.org/wiki/Frame_rate) at which they are recorded. For example, 24 FPS is a common frame rate, which means that each second of the video will contain 24 frames. For watching movies or playing video games, having a granular frame rate is important—_we do not want to have any visually perceptible gaps between the frames of the video_. However, neural networks do not need to process videos at this level of granularity. As shown above, we can save computational costs by sub-sampling the frames within a video; e.g., sampling every eighth frame of a 24 FPS video to simulate 3 FPS.

**Encoding a video.** Once we have sub-sampled video frames, we can simply treat a video as a set of images! Usually, we pass each video frame independently through an image encoder like CLIP, yielding a corresponding set of vectors to represent each video frame. Then, an LLM can ingest the vectors for these video frames as an additional input, similarly to an image. But, there is still a problem here: _the number of vectors produced for the video is large and sometimes unpredictable because the video can be of any length_. We need an additional module to aggregate the frame representations for a video into a single, fixed-sized set of vectors!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a19ba8-3763-4bc1-baf5-026d74507645_2172x986.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a19ba8-3763-4bc1-baf5-026d74507645_2172x986.png)

(from [9])

This is where the **Perceiver** [9] and **Perceiver Resampler** [10] come in handy. The perceiver (shown above) is an attention-based neural network architecture that can ingest high-dimensional input—_such as large set of vectors of variable size produced from the frames of a video_—and output a fixed-size representation based upon this input. Put simply, this means that we can pass all of our video vectors to the Perceiver and it will give us a fixed-size set of vectors in return. Then, we can easily integrate this additional input into an LLM, just like an image!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4277d6a0-7606-4ad8-a8f2-e70308ddd1de_1792x926.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4277d6a0-7606-4ad8-a8f2-e70308ddd1de_1792x926.png)

(from [10])

The Perceiver was originally applied to multi-modal LLMs by Flamingo [10], which proposed the Perceiver Resampler; see above. Flamingo samples video at one FPS (i.e., a single frame from every second of video). Each sub-sampled frame of a video is passed independently through an image encoder[4](https://cameronrwolfe.substack.com/p/vision-llms#footnote-4-158954054), producing a corresponding image embedding. Before passing these image embeddings to the text-based LLM, however, we pass them through a Perceiver architecture that produces a fixed (64) number of visual token vectors for the video. Then, _we integrate these vectors into the LLM using cross-attention as described before_.

## vLLM Architectures and Training Strategies

We now understand most background concepts relevant to vLLMs. Next, we will use these concepts to build an understanding of vLLMs from the ground up. In this section, we will focus on the architectures and training strategies that are commonly used to create vLLMs. We will keep this discussion conceptual for now, then apply these ideas to implementing a real vLLM in the next section.

#### vLLM Architecture Variants

The architecture of a vLLM always has two primary components: the LLM backbone and the vision encoder. The LLM backbone is just a standard decoder-only transformer, while the vision encoder is usually a CLIP / ViT model (with an optional Perceiver Resampler if we want to handle video-based inputs). There are two common vLLM architecture variants that fuse these components together: _the unified embedding and cross-modality attention architecture_. We use the naming scheme for these architectures proposed by [Sebastian Raschka](https://sebastianraschka.com/) in his [great overview of vLLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms). Now, let’s learn about how these architectures work.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42d39309-0469-4908-9b37-5204415f85c1_1648x842.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42d39309-0469-4908-9b37-5204415f85c1_1648x842.png)

Generating token vectors from raw text

**Token vectors.** The LLM backbone takes raw text as input, but this text is first tokenized into a set of discrete tokens and converted into token vectors by retrieving the corresponding embedding for each token from an embedding layer; see above. This set of token vectors can be directly passed as input to the decoder-only transformer architecture. Similarly for images (or videos), we produce a set of token vectors from the vision encoder by passing an image or video through the vision encoder, which returns a set of visual token vectors as output; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd022205-f5bc-4580-a4b3-3a03648d37d1_1288x1066.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd022205-f5bc-4580-a4b3-3a03648d37d1_1288x1066.png)

Generating image token vectors with a vision encoder

**Unified embedding.** Now, we have a set of text and image (or video) token vectors as input. The first common vLLM architecture simply:

1. Concatenates these two modalities of vectors together, forming a single sequence of token vectors.
    
2. Passes these concatenated vectors directly as input to a decoder-only transformer architecture.
    

This architecture, referred to as a unified embedding architecture, is depicted in the figure below. Notably, the size of the visual token vectors may not match that of the text token vectors. So, we usually linearly project the token vectors from the vision encoder into the correct dimension prior to concatenation.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f88da96-bb2a-49c7-a3db-171fa92bb2fa_1498x1158.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f88da96-bb2a-49c7-a3db-171fa92bb2fa_1498x1158.png)

The Unified Embedding Architecture

The unified embedding architecture is conceptually simple, but it increases the length of input passed to the LLM, which can cause a significant corresponding increase in computational cost during both training and inference. _These visual tokens are passed through every layer of our powerful LLM backbone_! Luckily, we can get around this issue by using a slightly different kind of vLLM architecture.

**Cross-modality attention.** Instead of concatenating text and vision token vectors, we can just pass the text token vectors as input to the LLM. To incorporate vision info, we can add extra cross-attention modules that perform cross-attention between the text and vision token vector into select layers of the LLM—_usually every second or fourth layer_. This architectural variant is usually referred to as a cross-modality attention architecture; see below for a depiction. Notably, this architecture looks very similar to the original transformer decoder—_we just perform cross attention with the image encoder instead of the transformer encoder_!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc46c381b-da70-4f32-9067-570ca1fbb56b_1660x1076.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc46c381b-da70-4f32-9067-570ca1fbb56b_1660x1076.png)

The Cross-Modality Attention Architecture

The benefit of this architecture is that we do not increase the length of the input passed to the LLM. Rather, we merge visual information into the LLM by using cross-attention, which is much more computationally efficient. Additionally, the cross-modality attention architecture adds new layers into the model architecture for fusing visual and textual information, rather than relying on the existing layers of the LLM to perform this fusion. For this reason, _we can actually leave the LLM backbone fixed during training and only train the added layers_, thus ensuring that the LLM’s performance on text-only tasks is not changed at all.

#### How do we train vLLMs?

In this overview, we will only consider LLMs that can ingest visual inputs—_these models still only generate text as output_. So, we can train these models similarly to any other LLM: using [next-token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction). Even for the unified embedding architecture, we primarily train the model by predicting textual tokens—_we do not typically try to predict visual tokens (i.e., perform next-image prediction)_.

> _“The visual encoding of Gemini models is inspired by our own foundational work on Flamingo, CoCa, and PaLI, with the important distinction that the models are multimodal from the beginning and can natively output images using discrete image tokens.”_ - from [11]

Going beyond the training objective, however, there are several strategies that we can follow for training a vLLM. For example, we could perform **native multi-modal training**, meaning that we initialize all components of the architecture from scratch and train the model using multi-modal data (i.e., text, images, videos and more) from the beginning; e.g., this approach is used to train Gemini [11].

In practice, however, native multi-modality is complex and difficult. There are many issues that we may encounter when using such approach:

- Getting access to a large volume of paired image-and-text data is hard.
    
- Efficient tokenization of visual data at pretraining scale is hard.
    
- Imbalances between modalities can arise; e.g., the model may learn to ignore images because text usually provides enough info for next token prediction.
    

For these reasons, vLLMs are more frequently trained using a **compositional approach**. Specifically, this means that we start by pretraining the LLM backbone and the vision encoder independently. Then, we have an additional training phase—_we will call this the fusion stage_—that combines the text and vision models together into a single vLLM. This approach has several benefits:

- The development of text and image models can be parallelized.
    
- Existing text-based LLMs—_which are very powerful and advanced_—can be used as a starting point for training vLLMs.
    
- A much larger volume of data is available because we can use text-only, vision-only, and paired text-and-vision data for training.
    

During the fusion phase, we may or may not train the full vLLM architecture. For example, when using a cross-modality attention architecture, we can freeze the LLM backbone during fusion and only train the cross-attention and vision encoder layers. Such an approach is common in the literature because it allows us to start with an existing, text-based LLM and create a corresponding vLLM without making any modifications to the underlying LLM backbone. As we will see, this was the exact approach used to train the LLaMA-3.2 Vision models.

## LLaMA-3.2 Vision: Powerful, Open vLLMs

Now that we understand the concepts underlying vLLMs, let’s take a look at a practical case study. The LLaMA-3 [1] LLMs were originally text-only but have since been extended to handle image (and video) inputs. These models are also (mostly) open source[5](https://cameronrwolfe.substack.com/p/vision-llms#footnote-5-158954054), so we can gain a deep understanding of them by _i)_ studying the details provided in their corresponding technical reports and _ii)_ looking at their code. In this section, we will study in detail how the LLaMA-3 suite of LLMs has been extended to create a corresponding suite of vLLMs.

#### [Extending LLaMA-3 to Images and Video](https://arxiv.org/abs/2407.21783) [1]

Proposed in [1], LLaMA-3 is one of the most popular and powerful suites of open-source LLMs. LLaMA-3 models are all dense—_meaning they do not use an [MoE architecture](https://cameronrwolfe.substack.com/p/moe-llms)_[6](https://cameronrwolfe.substack.com/p/vision-llms#footnote-6-158954054)—and come in three different sizes: 8B, 70B, and 405B. These models improve upon prior [LLaMA-2 models](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up) by an order of magnitude—_they have a 30× larger context window (128k vs. 4k), use a 30× (15.6T tokens vs. 1.8T tokens) larger dataset, and are trained using 50× the amount of compute._

> _“We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks… The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach.”_ - from [2]

The initial LLaMA-3 models only accept text as input. However, authors include experiments in [1] that incorporate both vision (i.e., image and video) and speech features. _We will learn how LLaMA-3 is trained on visual inputs in this section_.

**Compositional vLLMs.** LLaMA-3 follows a compositional approach to creating a multi-modal model. We begin by independently pretraining both a vision encoder and a text-only LLM. Here, the text-only LLM is the text-based LLaMA-3 model, while the vision encoder is a pretrained CLIP model. Adopting a cross-modality attention architecture, we then insert cross-attention layers between these two models and focus on training these extra layers. We will refer to these cross-attention layers as an “image adapter” for convenience. By doing this, the LLM is taught to incorporate additional visual features when generating output.

The **vision encoder** for LLaMA-3 is based upon the ViT [3] architecture—_the 630M parameter ViT-H_[7](https://cameronrwolfe.substack.com/p/vision-llms#footnote-7-158954054) _model in particular_—and is pretrained via a contrastive objective on 2.5B image-text pairs. In other words, _this model is nearly identical to the image encoder component of the CLIP [4] architecture!_ We create visual features with this model by passing an image through the model and extracting the corresponding embeddings; see below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54ca3ac2-9461-4c6b-b22c-ec7535f726cc_1178x1178.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54ca3ac2-9461-4c6b-b22c-ec7535f726cc_1178x1178.png)

Concatenating embeddings from multiple ViT layers

Notably, we know from [prior research](https://arxiv.org/abs/2312.00784) that image encoders trained with contrastive (CLIP-style) objectives capture semantic information but fail to capture the fine-grained perceptual details of an image. For this reason, any LLM relying upon such visual features may fail to answer questions that require exact localization within an image; see below from an example with [GPT-4V](https://cdn.openai.com/papers/GPTV_System_Card.pdf). As shown above, this issue is addressed in LLaMA-3 by extracting visual features from several different layers[8](https://cameronrwolfe.substack.com/p/vision-llms#footnote-8-158954054) of the vision encoder and concatenating them together.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F266ca8c9-2059-43b2-aa4e-ff22f37e7608_1072x1148.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F266ca8c9-2059-43b2-aa4e-ff22f37e7608_1072x1148.png)

([source](https://arxiv.org/abs/2312.00784))

LLaMA-3 also adds several additional self-attention layers after the image encoder and prior to fusion with the LLM—_the final image encoder has a total of 850M parameters_. This encoder produces 7680-dimensional embeddings for each patch in the input image, and each image has `16 × 16 = 256` patches in total.

> _“We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model.”_ - from [1]

**Image adapter.** To incorporate features from the image encoder into LLaMA-3, we use a cross-attention-based image adapter. More specifically, cross-attention layers, which compute attention between the textual tokens of the LLM and the image embeddings of the image encoder, are added to every fourth transformer block of the LLM. These cross-attention layers significantly increase the size of the model; e.g., LLaMA-3-405B has ~500B parameters with the image adapter. However, the image adapter allows the LLM to incorporate information from the image encoder into its token representations when generating text.

**Video adapter.** In addition to images, authors in [1] extend LLaMA-3 to support video inputs. Given that videos are just a sequence of images (or frames), we do not have to significantly modify the existing architecture. The model takes 64 frames as input, each of which is passed through the existing image encoder; see below. To capture the temporal relationship between frames, we use a Perceiver Resampler, which aggregates the representation of 32 consecutive frames into one. Finally, additional video cross-attention layers are added into the LLM.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe96d2d1d-97f2-4d42-a94a-08219ec4deee_1604x998.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe96d2d1d-97f2-4d42-a94a-08219ec4deee_1604x998.png)

(from [1])

The full architecture of the multi-modal LLaMA-3, including both video and image components, is shown above. Here, we can see that both image and video inputs are first processed via the image encoder, then incorporated into the LLM via cross-attention layers. For videos, we add an extra aggregation module—_the Perceiver Resampler_—to capture the sequential relationship between video frames.

**Pretraining dataset.** Both the image encoder and cross-attention layers are trained on a large dataset of image-text pairs. This dataset is filtered to _i)_ remove non-English captions, _ii)_ remove duplicates, iii) remove low-quality data, and _iv)_ maximize diversity (i.e., based on n-gram [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) scores). A very similar process is followed to collect video-text pairs for training the video adapter.

To improve the document understanding capabilities of LLaMA-3, authors in [1] also concatenate [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) output to the end of each textual caption and collect a large number of documents—_represented as images_—with associated text. Other notable sources of multi-modal training data for LLaMA-3 include:

- _Visual grounding_: noun phrases in the text are linked to bounding boxes / masks in the image that are either overlaid in the image or specified via (normalized) coordinates in the text.
    
- _Screenshot parsing_: screenshots from HTML code are rendered and the model is asked to predict the code that produced an element—_indicated by an overlaid bounding box_—in the screenshot.
    
- _Question-answer pairs_: a large volume of QA data from several sources.
    
- _Synthetic captions_: images with synthetic captions generated by an early version of LLaMA-3. Authors in [1] observe that synthetic captions tend to be more comprehensive than the original human-written captions.
    
- _Synthetic structured images_: charts, tables, flowcharts, math equations, and more accompanied by a structured representation (e.g., markdown or LaTeX).
    

**Image adapter training.** Prior to training the image adapter, the image encoder is pretrained for several epochs over the image-text pairs in the dataset described above. When training the adapter, the weights of the image encoder are not fixed—_they continue to be updated_. However, the LLM weights are frozen during this training process. As a result, the LLM backbone of the multi-modal LLaMA-3 model is identical to text-only LLaMA-3, _ensuring parity on text-only tasks_.

The image adapter is trained in two phases, both of which use a [standard language modeling objective](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction) applied on the textual caption. In the first phase, all images are resized to a lower resolution to make the training process as efficient as possible. This initial training phase is followed by a second, shorter phase in which we increase the resolution of images and use a smaller (sampled) version of the original dataset that emphasizes the highest quality data. After both training phases are complete, we train the video adapter—_beginning with the fully-trained image encoder and adapter_—over the video-text dataset using a similar process.

> _“After pre-training, we fine-tune the model on highly curated multi-modal conversational data to enable chat capabilities. We further implement direct preference optimization (DPO) to boost human evaluation performance and rejection sampling to improve multi-modal reasoning capabilities.” - from [1]_

**Post training.** Similar to the text-based LLaMA-3 model, multi-modal models undergo an entire post training procedure that aligns the model to human preferences, teaches it how to follow instructions, improves its ability to handle conversational inputs and more. Similarly to the text-only LLaMA-3 model, multi-modal models are post trained using a combination of [supervised finetuning (SFT)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised), [rejection sampling (RS)](https://arxiv.org/abs/2110.14168) and [direct preference optimization (DPO)](https://arxiv.org/abs/2305.18290) applied multiple times sequentially (i.e., in “rounds”). This process is depicted below, and a full overview of post training for LLaMA-3 can be found [here](https://www.interconnects.ai/p/frontier-model-post-training).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefcd982d-ef70-4c48-be86-79ae58b6496b_1276x614.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefcd982d-ef70-4c48-be86-79ae58b6496b_1276x614.png)

(from [1])

Unlike when we are training the image encoder and adapter, we do not use the weights of the base LLaMA-3 model for our LLM during post training. Instead, we replace the weights of this base model with those of the [LLaMA-3-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model, which has already undergone extensive post training. The dataset for post training is collected from a variety of sources:

- Academic datasets that have been converted into conversational format either via a template or by rewriting with an LLM.
    
- Human-annotated datasets that are collected by _i)_ providing a seed image or video and asking the human to write an associated conversation or _ii)_ asking humans to compare model outputs to form preference pairs.
    
- Synthetic datasets collected by giving the text representation (i.e., caption) of an image or video to an LLM and prompting the model to generate related question-answer pairs.
    
- Existing model outputs that have been subtly (but meaningfully) perturbed by an LLM to produce an error, thus forming a preference pair.
    

Several unique strategies are adopted to optimize the post trained model’s performance. For example, authors train several models—_with different hyperparameters_—at each stage of post training and obtain the final model by taking the average of these models’ weights. This [model merging approach](https://cameronrwolfe.substack.com/p/model-merging) outperforms the best model obtained via a [hyperparameter grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization).

#### [LLaMA-3.2: Medium-Sized Vision LLMs](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) [2]

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f18040b-5c65-4ffa-96b6-44647e3bea57_3840x2160.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f18040b-5c65-4ffa-96b6-44647e3bea57_3840x2160.png)

(from [2])

Preliminary experiments with multi-modal LLaMA-3 models were provided in [1], but these models were not officially released until LLaMA-3.2 [2]. The 11B and 90B parameter LLaMA-3.2 Vision models[9](https://cameronrwolfe.substack.com/p/vision-llms#footnote-9-158954054) were the first LLaMA models to support images as input and have strong capabilities on visual understanding tasks like image captioning and document understanding. Other modalities explored in [1]—_such as speech and video_—were not included in LLaMA-3.2, and no multi-modal version of the largest (405B parameter) LLaMA-3.1 model is released.

**LLaMA-3.2 Vision architecture.** The architecture described in [2] for the LLaMA-3.2 Vision models perfectly matches that of the preliminary models outlined in [1]. These models are comprised of:

- A pretrained LLM backbone.
    
- A pretrained vision encoder.
    
- Several cross-attention layers between the LLM and vision encoder.
    

The LLM backbone for LLaMA-3.2 is simply the text-only LLaMA-3.1-8B and LLaMA-3.1-70B models. The vision LLMs are trained in several stages on image-text pairs, but the LLM backbone is not updated during training—_we only update the image encoder and adapter layers_. As a result, the performance of LLaMA-3.2 Vision models on text-only tasks is left in tact relative to LLaMA-3.1.

**Stages of training.** As mentioned previously, the LLaMA-3.2 Vision models are trained in multiple stages. First, we must pretrain the LLM backbone and image encoder independently of each other. We then integrate these models together by adding cross attention layers between the two models and pretrain the combined vision model over a large (and noisy) dataset of image-text pairs. Lastly, we train the model further on a medium-sized dataset of higher-quality, enhanced data and perform post training. The post training strategy for vision models includes several rounds of SFT, rejection sampling and DPO (i.e., same as LLaMA-3.1).

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05814b16-71ad-4958-8622-d1e662c48939_1452x1116.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05814b16-71ad-4958-8622-d1e662c48939_1452x1116.png)

(from [2])

**Model evaluation.** For text-based tasks, the performance of LLaMA-3.2 models is identical to that of LLaMa-3.1—_the LLM backbone is left unchanged by the multi-modal pretraining process_. However, authors also evaluate the LLaMA-3.2 Vision models across a wide range of visual understanding tasks in [2]; see above. Most notably, these models have strong performance on tasks that involve documents, charts or diagrams. Such an ability is not surprising given that the model is trained over a large number of document-text pairs, as well as synthetic images of charts and tables. On other visual understanding tasks, LLaMA-3.2 continues to perform well and is competitive with several leading foundation models.

#### LLaMA-3.2 Vision Implementation

Now that we’ve learned about the LLaMA-3.2 Vision models, let’s take a deeper look at their implementation. To do this, we will study their code in [torchtune](https://github.com/pytorch/torchtune). For simplicity, we will omit some details from the implementation and instead present pseudocode that outlines the key modeling components. However, those who are interested can always read through the [full code](https://github.com/pytorch/torchtune/tree/main/torchtune/models/llama3_2_vision) in torchtune!

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e163a5c-aa41-48ce-a6b8-1fea597ef0a0_1476x1128.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e163a5c-aa41-48ce-a6b8-1fea597ef0a0_1476x1128.png)

**Top-level structure.** If we look at the primary function for instantiating a LLaMA-3.2 Vision architecture, we will see that the model is—_as we should expect_—made up of two primary components: an image encoder and an LLM backbone (called the vision decoder above). These two models are combined in a `FusionModel`. As shown above, we can toggle the trainable components of this `FusionModel`, which handles setting each model component as trainable or not and passes the output of the vision encoder to the vision decoder in a generic fashion.

```
# compute the output of the vision encoder
encoder_embed = None
if encoder_input is not None:
    encoder_embed = self.encoder(**encoder_input)

# pass the vision encoder output to the vision decoder
output = self.decoder(
   tokens=tokens,
   mask=mask,
   encoder_input=encoder_embed,
   encoder_mask=encoder_mask,
   input_pos=input_pos,
)
```

Notably, the input-output structure of the `FusionModel` is identical to that of a standard transformer decoder in PyTorch[10](https://cameronrwolfe.substack.com/p/vision-llms#footnote-10-158954054)—_these two types of models can be used interchangeably_. As shown in the code above, we can also supply an encoder mask that allows us to mask any image tokens from chosen textual tokens.

> _“DeepFusion is a type of fused model architecture where a pretrained encoder is combined with a pretrained decoder (LLM)… This module makes no assumptions on how the encoder and decoder are fused; it simply passes in the encoder embeddings to the decoder and lets the decoder handle any fusion.”_ - [source](https://github.com/pytorch/torchtune/blob/main/torchtune/modules/model_fusion/_deep_fusion.py)

The **vision encoder** used by LLaMA-3.2 Vision is a standard, [CLIP-based vision encoder](https://pytorch.org/torchtune/0.5/generated/torchtune.models.clip.clip_vision_encoder.html#torchtune.models.clip.clip_vision_encoder). This encoder passes an input image through CLIP to retrieve a set of image embeddings. From here, we do not directly pass the output of CLIP to the vision decoder—_there is an additional_ `VisionProjectionHead` _module that sits between CLIP and the vision decoder_. The implementation is provided below.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aed6e0-ffe0-490f-b768-e30454fb5ea6_1542x3028.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aed6e0-ffe0-490f-b768-e30454fb5ea6_1542x3028.png)

This module passes the CLIP embeddings through several extra self-attention layers prior to being ingested by the vision decoder. Additionally, the projection head pulls features from several hidden layers of the CLIP model—_instead of just taking the final layer’s output_—to ensure that perceptual information is not lost. All of these embeddings are concatenated together and linearly projected so that they match the size of textual token vectors used by the vision decoder.

[

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3d9f263-68b8-4df6-9fad-113744c8755b_1664x2766.png)



](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3d9f263-68b8-4df6-9fad-113744c8755b_1664x2766.png)

The **vision decoder** for LLaMA-3.2 Vision is nearly identical to a standard, text-based LLM; see above. We just modify this architecture to add cross-attention layers to a subset of the layers in the decoder. To do this, a `FusionLayer` is used, which keeps the parameters of the cross-attention layer and decoder block separate. This way, _we can toggle whether each of these components should be trained or not_. For example, LLAMA-3.2 trains the cross-attention layers and leaves the LLM backbone fixed throughout the multi-modal training process.

## Closing Remarks

The primary takeaway that we should glean from this overview is the fact that vLLMs are not much different than standard, text-based LLMs. We simply add an additional image encoder to this model, as well as some extra layers to fuse the two models together. The fusion between the image encoder and the text-based LLM can be accomplished either via a unified embedding architecture or with cross-modality attention. From here, we can just train this combined model (in multiple phases) over image-text pairs, forming a powerful vLLM. Many variants of vLLMs exist, _but the fundamental ideas behind them really are that simple_!

#### New to the newsletter?

Hi! I’m [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Machine Learning Scientist at [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. If you like the newsletter, please subscribe, share it, or follow me on [X](https://twitter.com/cwolferesearch) and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Grattafiori, Aaron, et al. "The llama 3 herd of models." _arXiv preprint arXiv:2407.21783_ (2024).

[2] Meta LLaMA Team. “Llama 3.2: Revolutionizing edge AI and vision with open, customizable models” https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ (2024).

[3] Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." _arXiv preprint arXiv:2010.11929_ (2020).

[4] Radford, Alec, et al. "Learning transferable visual models from natural language supervision." _International conference on machine learning_. PmLR, 2021.

[5] Joulin, Armand, et al. "Learning visual features from large weakly supervised data." _Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14_. Springer International Publishing, 2016.

[6] Desai, Karan, and Justin Johnson. "Virtex: Learning visual representations from textual annotations." _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 2021.

[7] Sohn, Kihyuk. “Improved deep metric learning with multi-class n-pair loss objective.” Advances in neural information processing systems 29 (2016).

[8] Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017).

[9] Jaegle, Andrew, et al. "Perceiver: General perception with iterative attention." _International conference on machine learning_. PMLR, 2021.

[10] Alayrac, Jean-Baptiste, et al. "Flamingo: a visual language model for few-shot learning." _Advances in neural information processing systems_ 35 (2022): 23716-23736.

[11] Team, Gemini, et al. "Gemini: A family of highly capable multimodal models, 2024." _arXiv preprint arXiv:2312.11805_ (2024).

[1](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-1-158954054)

Obviously, the decoder-only transformer has no encoder component, so the cross attention modules are simply removed from this architecture.

[2](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-2-158954054)

When we compute attention scores, we divide all attention scores by the square root of `d`, the size of the vectors being used for self-attention. This is called scaled dot product attention, and performing this division helps to improve training stability.

[3](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-3-158954054)

Prior to the proposal of ViTs, the most commonly-used architecture for computer vision tasks was convolutional neural networks (CNNs), or [ResNets](https://arxiv.org/abs/1512.03385) in particular.

[4](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-4-158954054)

Specifically, Flamingo uses a [ResNet](https://arxiv.org/abs/1512.03385) architecture to produce image embeddings, but we could also use CLIP (the more commonly-used vision encoder for LLMs).

[5](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-5-158954054)

See [this writeup](https://www.interconnects.ai/p/an-open-source-llm) for a deeper overview of the actual definition of open source and different kinds of “open” LLMs that exist.

[6](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-6-158954054)

In [1], authors mention that they avoid using an MoE architecture due to their design principle of maximizing simplicity. MoEs are more complex and difficult to train.

[7](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-7-158954054)

The “H” here just stands for “Huge”. This is the biggest ViT architecture in terms of total parameters explored in [3].

[8](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-8-158954054)

The motivation for this strategy is that different layers of the ViT will capture different kinds of information. For example, the early layers of the model are likely to capture low-level spatial details, while later layers capture semantic information; see [this paper](https://arxiv.org/abs/1311.2901).

[9](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-9-158954054)

Lightweight models with 1B and 3B parameters were also released as part of LLaMA-3.2, but these models only support textual input.

[10](https://cameronrwolfe.substack.com/p/vision-llms#footnote-anchor-10-158954054)

Remember, the transformer decoder has the option to provide a sequence of token vectors from an encoder as input by default. In the standard transformer, the encoder is the text encoder from the full encoder-decoder architecture. For LLaMA-3.2 Vision, the encoder is a vision encoder!

---

#### Subscribe to Deep (Learning) Focus

By Cameron R. Wolfe · Launched 3 years ago

I contextualize and explain important topics in AI research.

Subscribe

By subscribing, I agree to Substack's [Terms of Use](https://substack.com/tos), and acknowledge its [Information Collection Notice](https://substack.com/ccpa#personal-data-collected) and [Privacy Policy](https://substack.com/privacy).

[

![Razeen's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff29905d3-4992-48d8-b22d-c2edf4db5e53_144x144.png)



](https://substack.com/profile/102149308-razeen)

[

![Jeswanth Mukesh's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdb8b2c4-8238-4302-91a4-b7b442594ec8_144x144.png)



](https://substack.com/profile/262820619-jeswanth-mukesh)

[

![Ihor's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea605a9-e5c3-461b-9f84-e8d7776720aa_144x144.png)



](https://substack.com/profile/14042337-ihor)

[

![Hajar's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5e00d0c-19f9-4688-a5e9-cfee0ff4bdeb_144x144.png)



](https://substack.com/profile/256093529-hajar)

[

![Emmanuel Maminta's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4f1db49-fcdd-4643-ac7a-9cfa347cdb71_144x144.png)



](https://substack.com/profile/174228865-emmanuel-maminta)

112 Likes∙

[12 Restacks](https://substack.com/note/p-158954054/restacks?utm_source=substack&utm_content=facepile-restacks)

112

- 

[

8

](https://cameronrwolfe.substack.com/p/vision-llms/comments)

12

Share

#### Discussion about this post

CommentsRestacks

![dfsj's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c03b8d8-032e-4d23-8164-a30abec05eb2_144x144.png)

[

![ROHITH VENKATA REDDY's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fblack.png)



](https://substack.com/profile/40165044-rohith-venkata-reddy?utm_source=comment)

[ROHITH VENKATA REDDY](https://substack.com/profile/40165044-rohith-venkata-reddy?utm_source=substack-feed-item)

[4月13日](https://cameronrwolfe.substack.com/p/vision-llms/comment/108371304 "2025年4月13日 15:53")

Liked by Cameron R. Wolfe, Ph.D.

Well wrote and to the point.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/vision-llms/comment/108371304)

[

![Michael's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a2eb040-e118-40b5-a79c-f01ad5503f4c_534x800.jpeg)



](https://substack.com/profile/59416223-michael?utm_source=comment)

[Michael](https://substack.com/profile/59416223-michael?utm_source=substack-feed-item)

[Lux Umbra Dei](https://runtothehorizn.substack.com/?utm_content=comment_metadata&utm_source=substack-feed-item)

[4月1日](https://cameronrwolfe.substack.com/p/vision-llms/comment/104949935 "2025年4月1日 07:50")

Liked by Cameron R. Wolfe, Ph.D.

As always, excellent.

Like (1)

Reply

Share

[1 reply by Cameron R. Wolfe, Ph.D.](https://cameronrwolfe.substack.com/p/vision-llms/comment/104949935)

[6 more comments...](https://cameronrwolfe.substack.com/p/vision-llms/comments)

TopLatestDiscussions

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

[Building the world's most influential neural network architecture from scratch...](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

Mar 4, 2024 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

117

[

14

](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3c9db5-400a-49de-a235-e09bc3aa3689_2392x1342.png)

[Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

[Understanding reasoning models and their relation to standard LLMs...](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)

Feb 18 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

204

[

3

](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23d9c87e-b238-4fdd-996e-4ed4465b9931_2334x1282.png)

[Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms)

[Understanding models like DeepSeek, Grok, and Mixtral from the ground up...](https://cameronrwolfe.substack.com/p/moe-llms)

Jan 27 • 

[Cameron R. Wolfe, Ph.D.](https://substack.com/@cwolferesearch)

214

[

10

](https://cameronrwolfe.substack.com/p/moe-llms/comments)

![](https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fdf1382-38dc-45fc-a741-b62babfd99c5_2258x1268.png)

See all

Ready for more?

Subscribe

© 2025 Cameron R. Wolfe

[Privacy](https://substack.com/privacy) ∙ [Terms](https://substack.com/tos) ∙ [Collection notice](https://substack.com/ccpa#personal-data-collected)

[Start writing](https://substack.com/signup?utm_source=substack&utm_medium=web&utm_content=footer)[Get the app](https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&utm_content=web-footer-button)

[Substack](https://substack.com/) is the home for great culture



-----


