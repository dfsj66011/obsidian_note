
尽管大型语言模型（LLMs）被广泛应用于各种场景，但它们在解决基于推理的任务时通常面临困难。随着诸如“思维链”（Chain of Thought）和“由简到繁提示法”（Least-to-Most prompting）等提示技术的出现，这一问题得到了显著改善。从宏观层面来看，这些技术通过在模型提示中提供问题解决思路的示例，鼓励大型语言模型展现出推理行为。随后，模型能够学会输出此类思路，并针对底层问题生成逐步解决方案。值得注意的是，这是一种仅依赖提示的方法，无需进行微调，这表明只要提示具备足够的上下文信息，大型语言模型就具备推理能力。

尽管像思维链提示这样的技术被证明是有效的，但大型语言模型（LLM）被期望同时生成解决问题的思维链和最终答案。有趣的是，这种方法会导致一些特殊的失败案例，即LLM可能会为解决问题生成合理的推理过程，但最终给出的答案却是不正确的。通常，这类错误是由简单的失误（例如，糟糕的算术运算）造成的。为了解决这个问题，最近的研究探索了一种编程式的方法，鼓励LLM生成包含自然语言和代码组件的思维链。然后，LLM可以通过外部解释器运行这段代码，以获得所需的输出。

为了理解为何这种方法会有用，我们应当注意到，大语言模型（LLMs）难以处理的诸多问题（例如算术错误、无法评估复杂表达式等）都能在程序中轻松表述并解决。因此，在具备编程能力的大语言模型（例如Codex）上使用链式思维风格的提示词，能让我们将大语言模型的优势与任意Python程序的计算能力相结合！更具体地说，可以引导大语言模型生成包含自然语言和代码成分的问题解决思路，从而生成一个脚本，该脚本可由外部解释器运行以计算出问题的最终答案。我们将在这篇概述中探讨这种方法，它对大语言模型在解决基于推理的任务时的准确性和可靠性有着极大的帮助。

尽管现代大语言模型（LLM）具备令人难以置信的能力，但这些模型均基于一种简单的预训练流程，即对大量无标注文本数据进行下一词元预测。尽管我们可以调整这一流程的细节（例如所使用数据的类型或混合比例），但大多数大语言模型的基本预训练方法仍保持不变。我们只需简单地 i) 从预训练语料库中抽取一些文本，以及 ii) 教会模型准确预测语料库中的下一个词/词元。就是这样！这种简单而深刻的方法为所有现代语言建模奠定了基础。

但……通过多年研究积累的一些额外技巧和经验教训，能让我们打造出像ChatGPT或GPT - 4一样强大语言模型。大多数模型采用相同的仅解码器架构，但仅靠预训练无法打造出高性能的语言模型。我们需要：
 - 足够的规模（即大型模型和预训练数据集）。
 - 通过监督微调（SFT）和基于人类反馈的强化学习（RLHF）[11, 12]进行行为优化。
 - [可选]领域专业化（即针对特定类型的数据对模型进行微调，如代码或对话）。

如果我们正确地执行所有这些步骤，就能创建一个强大的基础模型，该模型能够通过文本提示解决各种任务。值得注意的是，语言模型的大部分知识和信息是通过预训练习得的（参见此处的“训练过程”部分），但在预训练之后进行的这些额外优化步骤使大语言模型更具可控性和趣味性；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc63736e9-2ede-4e12-aea6-602478d8e300_1600x954.png)


大语言模型不擅长什么？语言模型在各种不同的应用中都展现出了令人印象深刻的表现，但它们并非完美无缺。这些模型存在一些已知的局限性，例如：
- 难以进行大数相加
- 无法评估/解决复杂方程
- 在迭代过程的推理方面存在困难

例如，如果我们向一个大语言模型（LLM）提供斐波那契数列的描述，然后让它计算第100个数，它很有可能会失败！这是为什么呢？我们知道，大语言模型在进行算术运算时会遇到困难，而求解斐波那契数列（除非该模型采用暴力记忆的方法）需要在两个数之间进行多次迭代相加。如果该模型在每次迭代中进行加法运算的正确率为95%，那么计算出的第100个斐波那契数正确的概率将不到1%！

快速声明。GPT - 4 的近期发布使得对大型语言模型（LLM）局限性的论断变得更难了。例如，GPT - 4 完全能够算出第 100 个斐波那契数，甚至只需极少的提示就能求解一些（相对）复杂的方程；具体见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F452f51d2-3d7b-4de9-86df-67c5e55b5b4a_2122x1084.png)

考虑到这一点，关于大型语言模型（LLM）能力的任何表述都需要谨慎对待。这一领域正在迅速发展，模型每天都在变得越来越强大且令人印象深刻（确实如此）。

### 教大型语言模型如何编程

正如上文所述，创建高性能大语言模型（LLM）的一个（可选）环节是领域专业化。预训练之后的大语言模型具有很强的通用性，但只能执行一项任务——预测下一个词元。如果我们希望得到一款在特定领域有所专长的大语言模型，或者希望它擅长执行特定任务（例如信息探寻式对话或编写剧本），就需要用大量能体现该任务正确行为的数据对模型进行微调。这项技术最成功的应用场景之一，尤其与本概述相关的是创建能够编写代码的语言模型。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F661611ff-488c-4964-a30a-19c182b6bcbe_996x1086.png)

与通常从互联网下载大量文本数据用于预训练语言模型类似，我们可以从公共来源（例如 GitHub）下载大量代码来训练大语言模型（LLM），这使得编程成为专门化大语言模型一个特别完美的应用场景。这类模型的一个著名例子是 Codex [4]，它使用从互联网下载的无标签文本数据和代码的组合进行训练。给定一个 Python 函数注释，Codex 的任务是生成一个执行该注释中描述任务的可用 Python 函数；见上文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d859b5e-f9fe-4833-a263-cffa6c6d7bce_1564x868.png)

Codex在人工策划的编码任务中表现极其出色（见上文），甚至被用于驱动GitHub Copilot编程助手，这表明大型语言模型（LLMs）的应用不仅限于自然语言！我们还可以将其应用于许多遵循类似结构的其他问题。在这种情况下，我们通过在代码数据集上进行进一步的语言模型预训练，将预训练的大型语言模型适配到新的领域。值得注意的是，Codex能够生成代码和基于自然语言的输出，使其成为一个特别多才多艺且有用的LLM。此外，创建这种特定领域的模型相对简单——我们只需要大量的代码进行训练。

### 思维链（CoT）提示

除前文所述的局限性之外，大型语言模型（LLMs）最初还因无法解决推理任务而受到批评。然而，该领域的研究催生了诸如思维链（CoT）提示[3]这类突破性技术，使得大型语言模型能够相当准确地解决基于推理的任务。思维链提示背后的理念很简单。我们只需运用小样本学习来教大型语言模型如何针对任何推理任务输出详细解释其答案的问题解决思路；具体内容见下文。这种方法极具实用性，因为我们只需在提示中包含少量问题解决思路的示例，而以往的研究需要整理整个此类思路的数据集来进行微调。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

与教大语言模型如何编码不同，我们通过思维链（CoT）提示发现，这类模型无需任何微调就能解决推理任务！相反，我们只需采用一种更好的提示方法来“解锁”大语言模型解决复杂推理任务的能力。

> 大型预训练语言模型具备内在的推理能力，但需要特定的提示来释放其潜能。——引自[13]

鉴于在之前的综述中我们已经对思维链（CoT）提示及其众多变体有了很多了解，在此我不会对此展开详细探讨。然而，思维链提示有一个值得注意的方面——大型语言模型（LLM）需要同时做到以下两点：一是生成思维链；二是从该思维链中提取最终答案。尽管思维链提示很有效，但我们可能会开始思考：依赖大型语言模型准确完成这两个步骤真的好吗？

# 在大型语言模型中解耦推理与计算

我们知道，语言模型（在采用正确的提示方法时）能够提供准确的问题解决思路或其输出的详细解释。然而，生成正确的思路并不意味着大语言模型就能正确解决问题！如果大语言模型在得出最终答案时出现了一个小的算术错误会怎样？鉴于大语言模型的根本局限性，像思维链（CoT）提示这样的技术常常会遇到令人沮丧的失败案例，即模型生成了准确的思路，但却得出了错误的最终答案。这类错误通常被称为大语言模型的组合性差距。

> _“我们测量模型能够正确回答所有子问题但无法生成整体解决方案的频率，我们将这一比率称为组合性差距。——引自[16]

在本节中，我们将探讨近期旨在利用经过代码训练的大型语言模型（LLMs）（例如 Codex [4]）的独特能力来解决这一问题的研究，这些能力可用来编写连贯且功能完备的程序。我们可以依靠 LLMs 来生成问题解决的推理过程。但是，我们并非要求 LLM 直接给出实际答案，而是提示模型生成与该推理过程相关的程序，该程序在使用独立的代码解释器执行后，能够生成最终答案。因此，我们的推理过程成为了代码与语言的混合体——基本上是一个带有说明性注释的 Python 脚本！

### 程序辅助语言模型（PaL）

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccd5a835-8604-44b8-a0da-27921ea8af2c_1646x1428.png)


在[1]中，作者提出了一种受思维链（CoT）启发的技术，称为程序辅助语言模型（PaL），该技术利用大型语言模型（LLM）将基于推理的问题分解为逐步的解题思路。然而，这种思路既包含自然语言，也包含（基于Python的）程序化组件。在生成了这种混合思路后，我们可以通过Python解释器执行提示中的程序化部分来解决问题。这种方法的目标是消除这样一种情况：即大型语言模型生成了正确的推理链，但最终仍然得出了错误的答案。

> “这弥合了类似思维链方法中的一个重要差距，在这些方法中，推理链可能是正确的，但会产生错误的答案。” —— 引自[1]

借助PaL，我们可以利用大语言模型（LLM）生成解决问题的推理过程，但计算最终解的过程（即模型通常难以应对的部分！）则交由代码解释器来完成，从而消除了出现算术或逻辑错误的可能性。因此，LLM只需学习如何生成解决问题的推理过程——解决方案是通过编程方式推导出来的。我们可以通过少样本学习教会LLM生成这种混合推理。然而，要实现这一点，我们需要一个在自然语言和代码（例如Codex [4]）上都经过预训练的LLM。

理解PaL。从高层次来看，PaL采用的方法与CoT提示法非常相似。我们使用少样本提示法，提供了一些将问题分解为相关理由的示例。CoT与PaL之间的主要区别在于，PaL中使用的理由由自然语言和程序语句交错组成；详见下文

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ee8ec7c-fed6-4f68-8685-c647fb49e6d9_804x554.png)

在PaL推理过程中的每一步都增加了一个程序化语句。然后，当这些程序化语句被综合起来时，可以通过单独的Python解释器执行它们以生成最终答案（即通过单次事后执行）。PaL通过少样本学习教导大语言模型（LLM）生成一个逐步解决问题的程序。有趣的是，[1]中的作者通过利用Python注释语法（即#字符），鼓励LLM生成基于自然语言的中间步骤，这使得语言组件能够被插入到生成的程序中。换句话说，我们正在教导LLM通过带有信息性注释的逐步程序来解决推理任务！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b1ed6b5-4a22-4448-85cb-9b36bbf943dc_2388x970.png)

与思维链（CoT）提示不同，PaL所使用的少样本示例中并不包含最终解决方案。相反，示例只是一些穿插有自然语言语句的程序（仅此而已！）。最终解决方案的生成工作被委托给Python解释器，因此大语言模型无需学习如何执行这一步骤；详见上文。

进一步地，[1]中的作者观察到为程序中使用的变量赋予有意义的名称是有益的。这一发现表明，PaL 提出的推理过程是一种真正的混合方法，融合了基于语言和基于程序的组件。在编程和语言模态之间建立实体间的符号链接很重要；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87594484-0a5c-4dff-a8a1-b0654f2ce07c_1966x748.png)

这项工作效果如何？PaL在多种符号推理、数学推理和算法推理任务中进行了评估，结果表明它能缓解连续提示（CoT prompting）常见的许多问题。该方法与标准小样本学习（[1]中称为“直接”提示）以及CoT提示进行了对比。在数学推理任务中，结合Codex [4]的PaL轻松超越了以往各种不同模型的提示方法。值得注意的是，PaL甚至优于Minerva [5]——一个大语言模型，该模型是专门针对大量定量推理数据进行了微调的；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3986b50a-5cd1-447d-be97-37a853a6ca90_2292x790.png)

从上表中我们还应该注意到，使用Codex的PaL在GSM8K上实现了最先进的性能，比PaLM - 540B（即更大的模型！）结合思维链（CoT）的表现高出15%的绝对第一准确率。有趣的是，[1]中的作者指出，GSM8K主要集中在较小数字的数学文字问题上（即50%的数字在0 - 8之间），并提出了GSM - Hard——这个数据集的一个包含更大数字的版本。在更难的数据集上，与结合CoT提示的PaLM相比，PaL在绝对第一准确率上实现了近40%的提升，这表明程序辅助提示对于需要用大数进行复杂算术的问题更为有效。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad023890-4546-4581-acd6-61207ea2cf03_1972x534.png)

在符号推理和算法推理任务中，PaL 再次展现出显著优势；详见上文。事实上，PaL 几乎完全解决了这一类别中五个数据集中的四个，准确率超过 90%。此外，随着问题复杂度的增加，PaL 似乎能保持稳定的性能；详见下文。在这里，我们看到，以大数字或推理任务中更多对象的形式增加的复杂性，在编程上很容易处理，尽管直接用大型语言模型（LLM）处理这种复杂性可能会引发问题。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d01bee5-af12-4d49-8936-3f45b50d55b1_1002x668.png)



### 思想程序（PoT）提示

正如前文所述，采用思维链（CoT）提示的推理过程包含两个不同的步骤：
1. 生成基于语言（或程序）的解决方案原理
2. 根据这一原理计算最终答案

大型语言模型（LLMs）擅长执行上述第一步，但在计算最终答案时可能会遇到困难。通常，这个问题是由于算术错误或无法计算复杂表达式而发生的。简而言之，大型语言模型在处理复杂的数值任务时存在困难。在文献[2]中，作者旨在利用一种称为“思维程序（PoT）提示”的代码增强提示方法来缓解这一问题，并使大型语言模型能够准确解决复杂的数值任务。

> 在程序转换（PoT）中，计算可以委托给程序解释器，该解释器用于执行生成的程序，从而将复杂计算与推理和语言理解解耦。——引自[2]

正如我们所料，PoT提示方法与PaL非常相似。这两种技术都使用基于代码的提示增强技术来解决复杂的推理任务，并将推理过程中必要的部分委托给代码解释器。更具体地说，PoT提示利用基于代码的大语言模型（例如Codex [4]）进行少样本学习，以生成包含自然语言陈述和代码（Python）的混合推理依据。然后，输出中的代码部分被卸载到解释器中进行评估，从而将推理和计算解耦。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f486545-4d69-49e6-abb2-7f90049d7c77_2180x1530.png)


相比之下，思维链（CoT）提示直接让大语言模型（LLM）进行推理和计算。这存在一个问题，因为大语言模型在以下方面存在困难：
1. 进行基本算术运算（尤其是处理较大数字时）
2. 计算复杂数学表达式（如多项式或微分方程）
3. 解决需要迭代的问题

上述图表展示了此类问题，其中一个采用思维链（CoT）提示的大语言模型（LLM）未能对一个简单的三次方程进行评估，也未能对斐波那契数列的迭代计算进行推理。幸运的是，我们可以借助程序相当轻松地解决这些问题！例如，我们可以使用 for 循环来计算斐波那契数列，三次方程也能轻松地用 Python 语法来表达。然后，我们只需运行这个程序就能生成正确的输出结果，从而消除对大语言模型不必要的依赖。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff98677e1-d143-4445-affe-276790e17060_2340x1228.png)

PoT的细节。与PaL类似，PoT提示生成的问题解决理由同时包含语言和代码部分。通过一系列包含问题和相关“思维程序”（即多步程序，其中穿插自然语言语句解释计算过程）的少样本示例，教会大语言模型生成此类理由；详见上文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39fd5b16-44ad-4625-b434-e18951b7847f_2340x698.png)

与PaL不同，PoT编写的代码依赖于一个名为SymPy的符号数学库。该软件包允许用户定义数学“符号”，然后将这些符号组合在一起形成复杂的表达式。要计算这些表达式，只需将它们传递到SymPy的solve函数中；详见上文。如需了解更多详情，请查看下面的教程。

尽管 PoT 提示法使用了符号数学，但它与尝试直接用大型语言模型（LLM）生成数学方程式的方法有所不同，先前的研究表明直接生成数学方程式相当困难[3]。这是因为 PoT 提示法：
1. 通过多步骤、基于推理的过程生成符号方程式。  
2. 将符号变量与具有语义意义的名称关联起来。

与PaL类似，[2]中的作者指出，为程序中的变量赋予有意义的名称确实会对大语言模型的性能产生显著影响。

结果表明，PoT提示方法在使用Codex [4] 和GPT - 3 [7] 对多个数学文字问题和金融问答数据集（例如FinQA [8] 和ConvFinQA [9]）进行评估时表现出色。采用多种不同的LLM作为基线模型，分别运用少样本学习和CoT提示方法（包括一种可使用外部计算器的CoT提示变体）。如下表所示，在所有情况下，PoT提示方法的性能都显著优于基线模型，这凸显了解耦推理和计算的价值。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0799890c-7092-4ef6-97f6-b94b2f8b1464_1770x960.png)

有趣的是，文献[2]的作者也发现，零样本程序思维链（PoT）提示（即类似于零样本链式思维[CoT]提示[10]）效果相当不错。即便不为大语言模型精心策划多个融入程序的推理示例，通过PoT提示，我们也能让大语言模型在数值任务上取得不错的表现。此外，作者就使用PoT提示提出了一个有趣的实践性见解。为避免生成完全基于语言的推理（即带有全部注释的程序），他们不得不手动降低“#”符号出现的概率。虽然这只是一个小细节，但值得牢记——我们不希望生成的程序全是注释！此外，这也表明，让这类技术在实践中发挥作用往往很棘手、难度较大。

### 我们能做得更好吗？

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f901c92-54ea-45dd-a8b1-0256d87f6e3d_1886x1116.png)

在大多数实验中，“提示与应答学习”（PaL）和“基于提示的思维链”（PoT）都采用了贪婪解码策略，即大语言模型（LLM）通过迭代选择概率最高的下一个标记来生成输出序列。然而，我们可以使用多种更优的解码策略！一种值得关注（且超级简单）的策略是自洽性策略[14]。该技术使用相同的大语言模型和提示来解决一个问题，并生成多个不同的输出结果。然后，通过对所有生成的输出结果进行多数投票来确定最终答案；详见上文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa48d7320-542e-45d6-870b-d9025f836f33_1538x1078.png)

当将自洽性（Self - Consistency）应用于程序推理提示（PoT prompting）时，我们立即看到了显著的益处！如上所示，在[2]所涉及的几乎每个数据集中，采用自洽性的程序推理提示（PoT）都实现了新的最优性能。同样，程序辅助学习（PaL）[1]也从自洽性的使用中受益，并且甚至被用于探索更复杂的解码/提示策略，例如从易到难提示（least - to - most prompting）[15]（即一种按步骤依次解决推理任务的程序推理提示变体）。当与这种更复杂的提示风格相结合时，程序辅助学习（PaL）变得更加有效；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F339449d2-cbee-48b5-a844-ec4af8cf8f13_2276x988.png)

尽管PaL和PoT的表现相当不错，但通过对它们的提示技术进行一些易于实施的改进，我们可以让它们变得更好。这样的发现激发了进一步的实验。也许我们可以通过利用其他有用的技术（如提示集成）来获得额外的性能提升。

# 最后想法

尽管大型语言模型（LLMs）本身就很有用，但从本概述中我们可以看出，当它们能够使用有用的工具时，会变得更加出色。特别是，我们了解到将大型语言模型与外部代码解释器连接起来，对提升其在推理任务中的表现极为有益。然而，要实现这一点，我们需要能够编写代码的大型语言模型。以下是一些总结要点。

为什么这种方法有效？PaL 和 PoT 的有效性源于大型语言模型（LLMs）能够生成准确的问题解决思路，但在处理算术和迭代等简单任务时往往表现不佳。幸运的是，这些概念可以在程序中轻松建模，因此将 LLMs 与外部代码解释器相结合成为一种直观且强大的解决推理问题的技术。简而言之，我们通过让 LLMs 发挥其优势，并将剩余的问题解决部分委托给能更可靠地生成解决方案的代码解释器，从而获得了巨大的收益。

我们该如何解决大型语言模型（LLM）的弱点呢？正如本文简要提到的，随着更强大的模型（如GPT - 4）的发布，许多已知的LLM短板正在得到解决。然而，在这篇概述中我们可以看到，存在一些解决此类问题的替代方法，这些方法甚至可能更为可靠。特别是，依靠外部代码解释器可以解决因LLM在解决基于推理的任务时存在局限性而产生的问题。赋予模型执行代码的能力无疑扩大了其能力范围，这促使我们思考可能对LLM有用的其他工具。

将思维表达为程序。这项工作真正突出了程序可以被解释为一种用于表达个人思维的结构化语言这一事实。与自然语言相比，编程语言更具约束性，这使得它们能够轻松地表达迭代、模拟复杂方程等。然而，程序的形式化特性也限制了其表达能力——用自然语言写一首诗比用Python脚本容易得多（假设不调用GPT - 4 API）！在我看来，思考自然语言和代码之间的差异相当有趣。我们在这里看到，将它们结合起来可以发挥两者的优势。
