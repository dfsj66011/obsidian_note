

### GPT-2

GPT-2 的提案遵循了与其前身类似的模式。该模型使用语言建模目标进行预训练，但不执行微调，而是选择以零样本方式解决下游任务。简而言之，GPT-2 通过以下方式执行多任务学习：

1. 对原始文本数据进行通用 LM 预训练
2. 使用文本“提示”对各种任务执行零样本推理

虽然性能并不出色，但我们需要记住，_GPT-2 无需进行微调即可解决任何这些任务_。所有这些结果都是通过零样本推理实现的，这使得 GPT 在某些任务上的竞争性能相当令人印象深刻。

### GPT-3

这篇概述中将多次提到 *幂律* 的概念。即一个量的变化会导致另一个量发生相对的、比例不变的变化。

为了更具体地说明，幂律可以通过以下方程表达。$$y=ax^p$$
这里，我们研究的两个量是 $x$ 和 $y$，而 $a$ 和 $p$ 决定了这些量之间幂律的形状/行为。绘制这个幂律（设 $a = 1$，$p = 0.5$，且 $0 < x, y < 1$）会得到下图，其中将两个轴转换为对数刻度会产生幂律特有的线性趋势。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)
幂律简单地告诉我们，一个量随着另一个量的幂变化。在本概述中，我们将看到幂律的逆版本，如下所示。$$y=a\left(\frac{1}{x}\right)^p$$值得注意的是，这与之前的方程相同，只是 $p$ 的指数为负。这个负指数产生的图如下所示，其中一个量随着另一个量的增加而减少。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)

这一分析揭示了语言模型训练行为的一些基本特性。例如，如果参数总数固定，调整架构细节对模型性能的影响很小。然而，模型的测试损失随着模型大小、数据量和训练计算量的变化呈现幂律关系。

如果我们要扩大语言模型的训练规模，应该：

1. 将大部分额外计算资源投入到增加模型规模中（即，较大的模型在样本效率上更高）。
2. 增加数据集的规模（但不需要像模型规模那样多），以避免过拟合。
3. 略微增加批量大小（即，根据关键批量大小）。
4. 在模型显著收敛之前停止训练，以优化训练计算的使用。



### GPT-3 之后

随着 GPT-3 的提出，我们看到通过扩大 LLM 的规模可以获得很多好处。然而，在这篇综述中，我们探讨的问题是，模型规模是否是解决所有问题的答案。总体而言，我们了解到，仅仅扩大 LLM 的规模并不是提升任务无关性能的唯一必要因素。此外，这也带来了一些缺点。以下是主要结论的总结。

**更大的 LLM = 更多的工程努力。** 随着 LLM 规模的增大，处理它们变得越来越困难。我们已经从 GPT-3 中看到了这一点——训练一个拥有 175B 参数的模型需要结合多种分布式训练技术，是一项重大的工程壮举。对于更大的模型，如 MT-NLG，训练过程变得更加复杂。最近的努力已经降低了训练 LLM 的成本，但训练和部署这些模型所需的工程工作仍然很大。

**数据很重要。** 最初，扩大 LLM 的规模似乎是实现更好性能的首选方法。GPT-3 很出色，_为什么不让它更大呢_？然而，当我们看到随着模型变大，性能提升趋于平缓时，我们了解到在更多数据上进行训练也非常重要。LLM 性能的最大提升（例如 Gopher 和 Chinchilla）是通过模型和数据集规模的结合（大致成比例）实现的。

**深度还是宽度？** 这虽然不太显著，但当前研究似乎表明，常用的 LLM 架构可能比实际需要的要深。在某些情况下，可能更有意义的是让它们稍微浅一些，并将节省下来的参数投入到每一层的宽度中。

**监督性能仍然占主导地位。** 尽管我们观察到了 LLM 令人难以置信的任务无关性能优势，但我们必须将这些结果放在背景中进行考量。从这篇综述中的研究中我们看到，这些技术仍然不及监督训练性能。在很多情况下，通过任务特定的微调，我们仍然可以获得显著的性能提升。虽然任务无关的基础模型是一种美好的愿景，但在不进行任何任务特定适应的情况下利用这些模型进行实际应用可能还需要一段时间。_如果微调一点能大大提高我们的性能，为什么不这样做呢_？


### PaLM

在本概述中，我们将探讨 Pathways 语言模型（PaLM），这是一个使用谷歌的 Pathways 框架训练的 540B 参数的大型语言模型。通过消除流水线并行，该架构实现了惊人的训练吞吐量，使 PaLM 能够在更大规模的数据集上进行预训练。PaLM 清楚地表明，关于规模，LLM 的性能尚未达到瓶颈。只要有足够高效的训练基础设施，允许在更多数据上预训练更大规模的模型，我们就能持续看到性能的提升。

**SwiGLU 激活函数**   大多数 LLM 在每层中使用的前馈神经网络结构相似。即，这个网络执行两个前馈变换（不使用偏置，并单独应用于序列中的每个 token 向量），中间使用 ReLU 激活函数。然而，后续研究表明，选择其他激活函数可能会更好。

特别是，PaLM 使用 SwiGLU 激活函数，它结合了 Swish 和 GLU 激活。该激活函数由以下公式给出：$$\text{SwiGLU}(x) = \text{Swish}(xW) \cdot xV$$其中，Swish 激活函数定义为：$$\text{Swish}(x) = x \cdot \text{Sigmoid}(\beta x)$$换句话说，SwiGLU 是输入的两个线性变换的逐元素乘积，其中一个应用了 Swish 激活。虽然这个激活函数需要进行三次矩阵乘法，但最近的研究发现，在固定计算量下，它能带来性能优势。与 ReLU 等普通激活相比，SwiGLU 似乎提供了显著的性能提升。


#### 关键要点

尽管最初尝试训练超过 GPT-3 规模的 LLMs 并不太成功，但在 PaLM 上我们看到，只需要一个高效的训练框架来进行更广泛的预训练。通过使用 Pathways 框架，PaLM 能够在比同规模的先前模型（如 MT-NLG）更大的数据集上进行训练。最终的 LLM 具备了令人印象深刻的多语言理解和推理能力，我们发现增加模型规模通常能带来显著的好处。以下是 PaLM 的一些重要收获。

**幂律总是成立吗？** 关于 LLMs 的众多出版物显示，LLM 性能与各种量（如非嵌入模型参数、数据集大小、训练计算量等）之间存在幂律关系。虽然这种趋势在整体性能上成立，但在分别考察每项任务的性能时，情况会更复杂。某些任务从规模中获得的好处不成比例，而其他任务则没有太大收益。因此，规模对 LLMs 通常有帮助，但结果因所解决的下游任务而异。

**我们应该避免流水线并行吗？** PaLM 的主要卖点之一是其使用的高效 Pathways 训练框架。通常，在多个 TPU pods 或计算节点上进行训练需要使用流水线并行，因为内存带宽有限。然而，通过去除流水线并行，仅使用数据和模型并行进行 TPU pods 间的训练，PaLM 实现了突破性的训练效率和吞吐量。这些训练框架的提升使 PaLM 能够在更多数据上进行训练，从而实现了模型的出色表现。

**LLM 规模与推理能力。** 之前关于 LLMs 的研究常常指出其推理能力较差。事实上，LLMs 执行推理任务的能力似乎随着规模的增加而下降。然而，在 PaLM 上我们看到情况并非总是如此。如果将更大的 LLMs 与更多的预训练数据和正确的提示方法（如链式思维提示）结合，我们会看到 LLM 推理能力的显著提升！


### T5

T5 的贡献不在于新的架构或训练方法。相反，研究完全基于现有技术。T5 的目标是：*1）* 分析迁移学习设置，*2）* 确定最有效的方法。

**模型**    T5 使用的编码器-解码器架构与原始 Transformer 非常相似。不同之处在于：

1. 在每个注意力和前馈转换之前立即应用 LayerNorm（即，在残差路径之外）
2. 对 LayerNorm 不使用加性偏置（即，仅使用缩放并消除加性偏置）
3. 使用简单的位置嵌入方案，将标量添加到用于计算注意力权重的对数值上
4. 在整个网络中应用 Dropout（例如，注意力权重、前馈网络、跳跃连接等）

研究了多种策略，但主要结论是：(i) 去噪目标效果最佳，(ii) 去噪目标的不同变体表现相似，(iii) 最小化目标长度的策略在计算上最为高效。

### LLaMA


*什么是 LLaMA？* LLaMA 是一系列参数规模从 7B 到 65B 不等的 LLM。受 Chinchilla 的启发，这些 LLM 的规模比同类模型略小，但进行了大量预训练（即模型更小，处理的词元更多），并且开发目的是提供一组不同的模型，以在性能和推理效率之间实现不同的权衡。LLaMA 模型的表现令人惊讶；例如，13B 参数的模型大致可与 GPT-3 相媲美，而 65B 参数的模型其性能往往超过 PaLM。

除了令人印象深刻的性能外，LLaMA 仅使用公开可用的数据进行预训练。在大型语言模型（LLM）领域朝着开源迈出一步（后退），LLaMA 模型可以完全从在线资源中复制。最近的模型如 GPT-4 已知是使用公共数据和专有/私有数据的组合进行训练的。

#### RMSNorm

均方根层归一化（RMSNorm）定义如下方程所示。$$\overline{a}_i = \frac{a_i}{\texttt{RMS}},~~where~~\texttt{RMS} = \sqrt{ \frac{1}{n}\sum_{i=1}^n a_i^2}$$
RMSNorm 与 LayerNorm 有些相似，但在对神经网络的激活值进行归一化时，它去除了均值居中操作（并且使用了略有修改的分母）。与 LayerNorm 相比，RMSNorm 计算效率更高且更简单，使其能够以 10 - 50% 的效率提升实现相当的性能水平。

#### SwiGLU 激活函数

最初，这个非线性函数是修正线性单元（ReLU）激活函数。然而，最近的研究表明，这不是最佳选择。$$\texttt{SwiGLU} (x) = \texttt{Swish}(xW) \cdot xV$$特别是，LLaMA（以及其他像 PaLM 这样的大型语言模型）选择使用上文公式中给出的 SwiGLU 激活函数。在此，我们将 Swish 激活函数定义如下。$$\texttt{Swish}(x) = x \cdot \texttt{Sigmoid}(\beta x)$$SwiGLU 是输入 x 的两个线性变换的逐元素乘积，其中一个线性变换应用了 Swish 激活函数。这种激活函数需要三次矩阵乘法，但即使计算量保持不变，与其他激活函数相比，它也能提高性能。

#### 重新物化（或重新计算）

重新物化（Rematerialization），也称为重新计算（recomputation），是在 LLMs 的训练中使用的一种技术，它以额外的计算成本来减少内存消耗。通常，在计算神经网络的前向传播时，我们会存储/保留每层的激活值，以便在后向传播期间使用（这对于计算权重更新是必要的！）。但是，这需要大量的内存！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02f2c30b-7e1b-42b0-a18c-6dfea9ada815_1164x814.png)

重新计算的基本思想是在反向传播过程中重新计算某些中间激活值，而不是在前向传播过程中将它们存储在内存中。这有助于减少训练期间的峰值内存使用量，从而允许在可用内存限制内训练更大的模型或使用更大的批量大小。这对于大型语言模型尤为重要，因为它们规模庞大且消耗大量内存。

### 要点  

可与 GPT-4 相媲美 链接
- Koala：在互联网对话数据上微调的 LLaMA 链接
- ChatLLaMA：利用少量计算资源，在您自己的数据上创建个性化版本的 ChatGPT 链接
- ColossalChat：基于 LLaMA 并采用 RLHF 流程的类似 ChatGPT 的模型 链接

LLaMA 的影响可能会显著增加。就我个人而言，看到关于开放大型语言模型（LLMs）的研究不断取得进展，我感到无比兴奋。我希望让这些模型更容易获取，将促使更广泛的研究社区进行更彻底的调查和研究。以下给出了一些基本的要点。

*开源大型语言模型（LLMs）*   目前，大型语言模型生态系统正见证着一场有趣的冲突，在这场冲突中，有两种不同的方法被用于向公众推出这些强大的基础模型。一方面，像 ChatGPT 和 GPT-4 这样的模型仅通过付费应用程序编程接口（API）发布，使得研究界无法详细访问此类模型。像 LLaMA 这样的贡献则逆势而行，为研究界提供了对完整模型的访问权限。

*什么尺寸最好？* LLaMA 并没有发布单一模型，而是提供了一系列不同尺寸的大型语言模型（LLMs）。先前关于大型语言模型的研究倾向于提倡使用更大的模型，因为更大的大型语言模型在训练期间通常可以用更少的总体计算成本达到令人印象深刻的性能水平。然而，如果我们更广泛地预训练一个较小的模型，LLaMA 表明我们可以在实现显著降低推理成本的同时达到可比的性能水平。因此，（至少）考虑使用较小的大型语言模型是有意义的，特别是当我们必须部署它们时。值得注意的是，一些 LLaMA 模型可以在单个 GPU 上运行，这极大地提高了此类大型语言模型的可及性。

*令人印象深刻的性能？*   在 LLaMA 提出之前，许多研究小组试图发布流行的大型语言模型（LLM）的开源版本（例如，OPT 基本上是一个开源的GPT-3）。但是，这些模型的性能比通过API访问的付费模型差得多。尽管在某些情况下LLaMA的表现并非最佳，但它是一个巨大的进步，因为它通常优于流行的最先进的大型语言模型（取决于所使用的模型大小）。


## Beyond LLaMA



通过将其输出作为训练目标，我们可以将一些来自更大网络的信息提炼到正在训练的较小的“学生”网络中。有关知识蒸馏及其众多变体的更多信息，请查看下面的链接。

[Knowledge Distillation Survey](https://arxiv.org/abs/2006.05525)

### 其他东西……

在整个概述中，我们还将提及 OpenAI 目录中的一些特定模型的名称（例如 text-davinci-003）。有关 OpenAI API 中提供的模型（及相关描述）列表，请参阅此处。

### [Alpaca: An Instruction-following LLaMA model](https://crfm.stanford.edu/2023/03/13/alpaca.html) 

> _“在学术界对指令遵循模型进行研究一直很困难，因为没有容易获取的在能力上接近OpenAI的text-davinci-003等闭源模型的模型。” 

阿尔帕卡（Alpaca）[3] 是LLaMA - 7B [1] 大型语言模型（LLM）的微调版本，其性能与OpenAI的text - davinci - 003（即GPT - 3.5）相似。阿尔帕卡的微调过程基于Self - Instruct [2]，在此过程中，从性能更高的LLM（即text - davinci - 003）收集遵循指令的数据，并用于监督微调（SFT）。简单来说，阿尔帕卡表明，在遵循指令的情境下，小型开源LLM的质量可以通过在高质量数据上进行微调得到极大提升。此外，阿尔帕卡的整个微调过程仅花费600美元（包括数据收集和微调），这使得此类遵循指令的LLM在研究目的下易于且低成本复制。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd507e047-bfb6-4d05-a575-07d4ffb591f1_1434x618.png)

**方法**   要通过监督微调（SFT）创建一个遵循指令的大型语言模型（LLM），我们需要 i) 一个高质量的预训练语言模型以及 ii) 用于监督微调的遵循指令的数据。幸运的是，LLaMA 的近期发布提供了易于获取的预训练语言模型。获取遵循指令的数据则稍微复杂一些，但有一种方法是自我指令[2]。从高层次上讲，自我指令通过引导大型语言模型生成的输出来进行进一步训练。在 Alpaca 的案例中，我们使用 text-davinci-003 通过以下方式生成遵循指令的数据：

1. 从self-instruct的种子集中选取 175 个指令和输出对作为起始。
2. 通过向大型语言模型（LLM）提供种子集作为少样本学习的上下文示例，提示其生成更多指令。

[3]的作者还采用了一些技巧（例如，修改提示以及更高效的解码/生成过程），与原始的self-instruct [2]相比，使数据生成过程成本更低且更高效。总体而言，通过OpenAI API生成指令遵循数据的成本为52K条指令遵循示例不到500美元。

然后，使用基于 HuggingFace 的训练框架，在这些数据上对 LLaMA-7B 模型进行微调。通过使用全分片数据并行（FSDP）和混合精度训练技术，微调过程在 8 个 A100 GPU 上缩短至3小时，成本不到100美元。用于创建Alpaca的代码/数据可在线获取。然而，禁止商业使用Alpaca，原因如下：i) Alpaca所基于的LLaMA具有非商业许可证；ii) OpenAI禁止使用其模型来训练竞争性LLM。

[Alpaca Code](https://github.com/tatsu-lab/stanford_alpaca)

**结果**   阿尔帕卡（Alpaca）在用于自我指令（self-instruct）的评估集指令（即主要与电子邮件、社交媒体和生产力相关的任务）以及作者手写的开放域指令上进行了评估。在这些任务中，发现阿尔帕卡的表现与text-davinci-003相似（即在测试的大约180个案例中，有50%的情况下表现最佳）。尽管这种评估的范围显然有限，但考虑到阿尔帕卡比GPT - 3.5小得多且相对容易复制，其表现仍然相当令人印象深刻。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb76be6ec-f52f-4687-9b88-5db3455053c6_1420x740.png)

与text-davinci-003类似，Alpaca的输出通常比ChatGPT的输出短。换句话说，该模型的风格反映了用于生成微调所用的指令遵循数据的LLM的风格。

### [Vicuna: An Open-Source Chatbot with 90% ChatGPT Quality](https://vicuna.lmsys.org/) [4]

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F239ec346-e0c2-45c0-bc3c-c4e571a35348_1532x1386.png)

像ChatGPT这样的信息检索对话代理（或聊天机器人）非常出色，但这类模型的训练框架和架构却不为人知，这阻碍了开源研究的发展。为了解决这个问题，[4]的作者提出了Vicuna，这是一个通过微调LLaMA-13B[1]（即一个与GPT-3性能相当的小型语言模型）创建的开源聊天机器人。Vicuna的微调数据是与ChatGPT的用户对话示例，整个微调过程可以以不到300美元的成本复制，从而使聊天机器人更易于用于研究目的。与Alpaca相比，Vicuna与ChatGPT更为相似，并能生成更详细、更有结构的答案。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3478758a-f597-4bee-a07c-0dd4e3cda9b2_1024x458.png)

**方法**   用于与Vicuna进行监督微调（SFT）的数据是通过公共应用程序编程接口（API）从ShareGPT下载的，ShareGPT是一个允许用户分享其与ChatGPT对话的平台。在微调之前，作者会过滤掉不适当和低质量的数据，并将较长的对话分割成适合LLaMA - 13B最大上下文长度的较短片段。总共收集了7万个对话。与Alpaca类似，该模型在8个A100 GPU上使用全量梯度分散并行（FSDP）进行训练（进行了一些修改以降低成本并处理长序列），这大约需要一天时间；详见上文。作者公开提供了用于训练和托管Vicuna的代码。

[Vicuna Code](https://github.com/lm-sys/FastChat)

下表提供了Vicuna与开源大型语言模型LLaMA和Alpaca更全面的比较。接下来我们将讨论如何对Vicuna进行评估。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0742a316-aae9-4929-921c-0cff93187e7d_1524x1248.png)

**结果**   准确评估聊天机器人相当困难，而且随着聊天机器人质量的提高，评估难度会更大。例如，[4]中的作者声称，用于评估Alpaca的自指令评估集已被最近的聊天机器人有效解决，这使得模型之间的差异难以辨别。鉴于现有基准测试的局限性以及创建新的全面评估集的难度，[4]中的作者选择了不同的策略：使用大型语言模型进行评估。

> _“随着GPT - 4的最新进展，我们好奇其能力是否已达到类人水平，从而能够实现一个用于基准测试生成和性能评估的自动化评估框架。”_ 

在这一点上，我们可能会认为这实际上是不可能奏效的。聊天嵌套？然而，令人惊讶的是，基于最近提出的GPT - 4模型[6]构建一个评估框架效果很好。首先，[4]的作者设计了八类问题（例如，角色扮演场景和数学任务）。然后，提示GPT - 4在每个类别中生成一系列多样化的问题。有趣的是，发现GPT - 4能够生成让最近的聊天机器人难以回答的难题。

特别是，GPT - 4被用于在每个类别中生成十个问题，并对五个不同聊天机器人（即LLaMA - 13B、Alpaca - 13B、Vicuna - 13B、Bard和ChatGPT）的输出进行评估。更进一步说，通过让GPT - 4根据详细程度、有用性、相关性和准确性对答案的质量进行评级来判断每个模型输出的质量。尽管以这种方式进行评估似乎有些牵强，但GPT - 4相当一致地对模型进行了排名，甚至还解释了其推理过程。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61e91dff-1f9c-4eb0-9b0f-c02e08fa50ae_900x416.png)

根据GPT - 4的判断，Vicuna的输出质量相对于ChatGPT达到了92%；详见上文。这个比例是通过让GPT - 4为每个模型的输出打分得到的。然后，通过计算所有问题的总质量得分，可以评估模型之间的相对性能。尽管这种评估方法并不严谨，但它相当有趣，相对一致，并且促使我们思考大型语言模型（LLM）领域未来发展的有趣方式。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc78507d2-9ac5-4419-b566-de6f73bbe101_840x480.png)

与其他开源模型相比，我们发现GPT - 4倾向于选择Vicuna的输出。此外，在45%的问题上，Vicuna生成的结果质量超过或与ChatGPT相当。对于一个仅需300美元就能进行微调的模型来说，这种质量水平相当令人印象深刻！

### [Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) [5]

> _“如果使用精心收集的数据进行训练，体积小到可以在本地运行的模型也能在很大程度上展现其更大规模同类模型的性能。”_ 

在这一点上，我们可能会开始怀疑我们是否会有用尽可供以命名大型语言模型（LLMs）的动物名称的一天。不过，考拉（Koala）与小羊驼（Vicuna）和阿尔帕卡（Alpaca）类似，因为它依然致力于缩小专有大型语言模型和开源大型语言模型之间的质量差距。更具体地说，考拉（Koala）是LLaMA-13B的一个版本，它已经针对来自多种来源的对话数据进行了微调，这些来源包括公共数据集以及互联网上可获取的与其他高质量大型语言模型的对话。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a48c6e-7b89-43d1-8e09-145f47b3632a_1512x296.png)

当在实际提示上进行评估时，发现Koala-13B与ChatGPT相比表现具有竞争力，甚至超过了相关的Alpaca模型。因此，Koala的研究结果继续支持我们在所有基于LLaMA的工作中看到的趋势。即我们看到，给定用于微调的正确数据，较小的模型也能达到令人印象深刻的质量。这样的发现可能会让我们思考：我们是否过于关注模型规模，而对数据质量的关注不够？

**方法**  考拉（Koala）使用来自公共数据集和互联网的对话数据进行微调。然而，[5]中的作者们着重强调了为微调精心策划高质量数据集的重要性。用于微调考拉（Koala）的数据大致可分为基于蒸馏的数据（即来自其他大型语言模型的对话）或开源数据（即存在于公共数据集中的数据），其中包括来自ShareGPT、HC3、OIG、Anthropic HH以及OpenAI WebGPT/摘要的数据。此外，微调集甚至包括用于训练Alpaca[3]模型的数据。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F322813ba-a456-42e4-bb7a-31e983abf754_1970x686.png)

所有这些数据都是基于对话的。然而，值得注意的是，一些数据集针对每个问题包含多个被评为好或不好的对话或回复。有趣的是，我们可以利用之前的技术[8]将这些信息纳入到大型语言模型（LLM）的微调过程中。具体而言，这是通过条件训练来实现的，在这种训练中，我们可以简单地用人工偏好标记对用于训练LLM的数据进行条件设定（例如，只需追加关于对话是好还是不好的文本信息）；见上文。这种方法能够提升性能，并使我们甚至可以使用质量较低的对话来进行模型训练。

文献[5]的作者将用于训练和托管Koala的框架公开可用。该模型使用八个V100 GPU训练了两个轮次，大约需要6小时。总体而言，在可抢占/现货实例（假设我们可以使用可抢占/现货实例）的情况下，训练这个模型的计算成本不到100美元，这意味着Koala是我们迄今为止所见到的模型中复现成本最低的！

[Koala Code](https://github.com/young-geng/EasyLM)

**结果**  文献[5]中的作者训练了两种不同类型的Koala模型：

- Koala-distill：仅在蒸馏数据（即来自其他聊天机器人的对话示例）上进行微调
- Koala-all：使用上述所有数据进行微调

基于人体试验和反馈，将这些考拉模型的质量与阿尔帕卡（Alpaca）和ChatGPT的质量进行比较。为了进行评估，使用了阿尔帕卡[3]评估集中的问题以及来自互联网的一组真实用户查询。作者选择向评估集中添加更多问题，因为阿尔帕卡的评估集与其训练数据非常相似（即，二者均源自self-instruct[2]）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F540fe427-9302-45ca-8be6-16b63eac5f62_1702x864.png)

当人类从质量和正确性的角度评判不同大型语言模型（LLM）的输出时，发现“Koala-all”的表现常常超过“Alpaca”，并且在大量情况下其质量与“ChatGPT”相当甚至更优。此外，我们发现“Koala-distill”的表现实际上优于“Koala-all”。鉴于“Koala-distill”的微调数据集更小（即仅包含来自“ChatGPT”的示例对话），这一点有些违反直觉，但这告诉我们用于微调的数据类型和质量极其重要。也就是说，使用由更大、更好的大型语言模型生成的对话进行微调非常有效。

> _“构建强大的对话模型的关键可能更多地在于精心策划具有多样化用户查询的高质量对话数据。”_ 


### 更进一步……

尽管LLaMA提出的时间并不长，但Alpaca、Vicuna和Koala并非仅有的受LLaMA推动（或启发）的重要模型。下面我们可以看到近期发布的其他一些开源语言模型的列表。

- • Lit-LLaMA：在Apache-2.0许可证下开源的LLaMA复现版本（允许商业用途）。
- • ChatLLaMA：使用LLaMA、您自己的数据以及尽可能少的计算资源制作ChatGPT的个性化版本。
- • FreedomGPT：一个开源的聊天机器人（基于Alpaca），强调无审查。
- • ColossalChat：一个开源的ChatGPT复制品，附带一个完全实现的（并且公开的）基于LLaMA的RLHF管道（包括数据收集、监督微调、奖励模型训练和强化学习微调；见下文）。
- • StackLLaMA：为生产强大的聊天机器人提供基于RLHF的微调的开源实现和讨论（特别是以LLaMA为起点）。
- • GPT4All：基于LLaMA和GPT-J训练开源LLMs的演示、数据和代码（具有Apache-2.0许可证！）。
- • Baize：一个基于LLaMA的开源聊天机器人，使用LoRA（一种参数高效的微调方法）进行微调。
- • Galpaca：Galactica（科学领域的语言模型）的一个版本，已经在与Alpaca相同的数据集上进行了微调。
- • Dolly 2.0：该模型不基于LLaMA，而是一个经过指令微调达到ChatGPT类似质量并且开放商业使用的开源聊天机器人。
- • Open Assistant：一个开源聊天机器人（可与ChatGPT相媲美），能够理解任务、与第三方系统交互并检索信息。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F718f0b30-eaa8-4952-be9d-7b7bc428d449_1618x1172.png)

除了提出的各种模型之外，由于LLaMA的出现，大型语言模型（LLM）的研究和使用也变得更加容易。LLaMA - 13B已经可以仅使用单个GPU来运行，但现在我们甚至可以在本地（例如，在苹果笔记本电脑上）进行运行！

- • Alpaca.cpp：在本地运行Alpaca的开源复现版本。
- • GPTQ - 4 - LLaMA：LLaMA的4位量化版本。
- • LLaMA.cpp：对几种开源大型语言模型进行4位量化推理，这使得可以在本地托管（例如，在苹果笔记本电脑上）。  
    似乎大型语言模型很快将比以往任何时候都更广泛地供人们使用。

## 要点

我们可以从这项工作中推断出的主要观点是：i) LLaMA激发了大量开源大型语言模型（LLM）的研究；ii) 由于LLaMA的存在，围绕大型语言模型的研究/使用变得更容易获取。如果一个月前有人告诉我，我能在我的苹果笔记本电脑上运行一个性能接近ChatGPT的大型语言模型，我不会相信的。这是一个令人兴奋的时代，我很感激能成为这样一个了不起的社区的一小部分！下面列出了一些基本的收获。

大型语言模型（LLMs）适用于所有人。如果我们之前对此还有所质疑，那么现在我们知道研究界确实可以对大型语言模型展开有价值的研究。几周前，我们大多数人都认为由于对数据和计算能力的极高要求，大型语言模型很难被广泛使用。然而，现在我们只需花费几百美元就能训练出具有ChatGPT水平（或至少接近其水平）的模型，甚至还能在笔记本电脑上使用这些模型进行对话！

较小的模型就足够了吗？长期以来，模型规模（连同大规模预训练数据集一起）一直是高性能大型语言模型（LLMs）的一个重要组成部分。然而，像Koala和Vicuna这样的模型向我们展示了较小的LLMs实际上可以表现得非常出色（甚至在某些情况下可以与ChatGPT等强大的LLMs相媲美）。这样的发现凸显了数据质量的重要性。在我们在这里看到的工作中，最有效的技术往往使用较大LLMs的输出作为训练数据，这表明知识蒸馏可能是创建小而强大的LLMs的一个重要组成部分。

商业上可行？尽管这些技术中的许多都很酷，但在商业应用中使用它们却很困难。例如，OpenAI禁止使用ChatGPT（或任何其他API模型）来训练竞争模型，从而阻止了基于OpenAI API的知识蒸馏方法。此外，即使是LLaMA本身也禁止商业使用。因此，像Alpaca、Koala和Vicuna这样的模型仅从研究角度而言是有趣的，它们的方法不能用于任何商业使用的模型。然而，有了像Lit-LLaMA这样的提议，这些模型的商业可行版本似乎可能会慢慢出现。



## CoT


大型语言模型（LLMs）的成功源于我们能够（使用语言建模目标）在海量文本语料库上预训练仅解码器的Transformer模型。鉴于我们预训练了足够大的模型，LLMs是非常强大的小样本学习者。换句话说，这意味着我们只需制定一个文本提示（可能包含几个正确输出的示例），并让LLM生成正确答案，就可以解决各种不同的问题（例如，翻译、句子分类、摘要等）。

尽管大型语言模型（LLMs）功能强大，但这些模型始终难以解决一些问题。特别是推理问题（例如算术推理或常识推理）极其困难。最初为解决这一问题所做的尝试是，在各种推理问题的解决方案和解释的监督数据集上对大型语言模型进行微调，并采用特定任务的验证模块[3, 4]。然而，近期的研究发现，可以利用少样本学习来更轻松地解决这一问题 。

> “本文的目标是赋予语言模型生成思维链的能力——一系列连贯的中间推理步骤，从而得出问题的最终答案。” —— 引自[1]

特别是，思维链（CoT）提示[1]是一种最近提出的技术，它通过少样本学习来提高大型语言模型（LLM）在基于推理的任务上的性能。与标准提示技术类似，思维链提示将几个推理问题的示例解决方案插入到大型语言模型的提示中。然后，每个示例都与一个思维链配对，即解决问题的一系列中间推理步骤。然后，大型语言模型（以少样本的方式）学习在解决推理问题时生成类似的思维链。这种方法使用最少的数据（即仅几个用于提示的示例），不需要针对特定任务进行微调，并显著提高了大型语言模型在基于推理的基准测试中的性能，尤其是对于较大的模型。

### 核心概念

我们不会在本节中介绍大型语言模型（LLMs）的基础知识。相反，我们将着重加深对大型语言模型的提示和少样本学习的理解，并探讨如何利用这些技术来解决这些模型的一个核心局限性：它们无法解决推理任务。


### Prompting 和 Few-Shot Learning

在像GPT和GPT - 2这样的语言模型被提出之后，我们知道通过自监督的下一词预测（或语言建模）目标进行预训练是非常强大的。然而，如何适配这些通用的基础模型来解决特定的下游任务并不那么清晰。例如，GPT通过在下游任务上微调模型，而GPT - 2则以零样本的方式解决问题；具体见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46457a6f-577b-43ed-bd40-c5ad188a34f4_1818x1130.png)

在GPT - 3[2]被提出之后，我们发现规模足够大的大型语言模型（LLMs）能够很好地进行少样本学习。通过语言建模目标进行预训练后，GPT-3（一个具有1750亿参数的大型语言模型）被发现无需任何微调就能准确解决各种不同的语言任务。我们可以用提示方法来代替微调。  
更具体地说，提示利用语言模型的文本到文本结构，提供如下输入：

- “将这个句子翻译成英语：`<sentence> =>`”
- “总结以下文档： `<document> =>`”

这些解决任务的“提示”使得语言模型能够进行零样本（即，未见过正确输出的示例；见上文）或少样本（即，在提示中插入少量正确输出的示例；见下文）推理。语言模型给出的最恰当的输出应该能够解决任务（例如，总结文档或完成推理任务），这意味着我们可以通过准确的下一标记预测来解决各种问题！
![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5762db-6c25-45fb-8854-d64c760989f6_1342x1216.png)

我们可以通过提示来完成很多工作。事实上，最近专门创建了一个提示工程领域，以研究如何优化提示的措辞或结构以提高大型语言模型（LLM）的性能。但是，在这个不断发展的领域中，敏感性是一个需要重点考虑的因素。输入提示稍有扰动，大型语言模型的性能就可能发生巨大变化（例如，在SST - 2数据集上，对少量示例进行排列会使GPT - 3的准确率从93.4%降至54.3% [13]）。因此，在我们对提示方法的研究中，我们的目标是找到既i) 性能良好又ii) 不受敏感性影响的技术。

### 我们能否通过规模化来解决推理问题？

如上所述，大型语言模型（LLM）的少样本学习性能会随着规模扩大而提升，但大型模型并非我们所需要的一切。强大的大型语言模型需要将大型模型与大规模预训练数据集相结合[14]。考虑到这一点，我们可能会问自己：大型语言模型在基于推理的数据集上的性能如何？随着规模扩大，大型语言模型在推理方面是否会变得更好？

> “仅仅扩大模型规模对于在算术、常识和符号推理等具有挑战性的任务上实现高性能还不够” —— 引自[1]

有趣的是，我们发现使用更大的模型和预训练数据集并不能提高大型语言模型（LLM）的推理能力（例如，可参考对Gopher[15]的分析）。事实上，这些模型因无法解决基本推理任务而饱受诟病。因此，许多研究人员声称，大型语言模型只是在“反刍”训练数据，而非进行任何复杂的推理或分析。无论如何，本综述将重点关注旨在解决这一问题并使大型语言模型能更轻松地解决基本推理任务的提示技术。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F725341ae-60c3-4074-946f-eb16ece5ab87_1622x808.png)

**先前的方法**  在进一步了解我们如何帮助大型语言模型（LLMs）解决推理问题之前，了解这一领域先前的方法是有用的。针对算术、常识和符号推理任务的基础技术会进行特定任务的微调，也就是说，模型会在每个推理问题的监督示例上进行训练。更进一步说，最佳方法会训练一个补充性的“验证”模块，该模块能够判断大型语言模型在推理任务上的输出是否正确[4]。在测试时，这个验证器可以在为一个问题生成多个答案后推导出最有可能的输出；见上文。

尽管这些技术在某些情况下可能效果相对不错，但由于以下几个原因，它们存在一定局限性：

1. 需要进行特定任务的微调。
2. 必须针对每个任务对模型架构进行调整（即通过验证模型进行调整）。
3. 必须收集大量的有监督数据 。

考虑到这些限制因素，使用仅提示的方法（例如思维链提示）来解决推理任务会简单得多。我们可以避免微调，保持相同的模型架构，收集更少的数据，并使用单个预训练模型检查点解决许多任务。

### CoT Prompting

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F599a636e-b0b2-4de3-84c8-3edf906bfa82_1616x882.png)

虽然我们可能总体上理解提示（prompting）的概念，但什么是思维链（CoT）提示呢？思维链提示只是指一种特定的提示技术，它在大型语言模型（LLM）的提示中插入一个思维链（即一系列中间推理步骤）；见上文。对于足够大的模型（参数超过1000亿），这种方法显著提高了大型语言模型在算术、常识和符号推理任务上的复杂推理能力。

思维链提示从何而来？在提出思维链提示之前，我们已经知道少样本学习对大型语言模型（LLMs）非常有效；见下文。我们不是微调大型语言模型来执行任务，而是在生成最终答案之前，仅用少量正确输出的示例“提示”一个通用模型。这种方法对一系列任务都非常成功。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F516ed49c-c4a2-4087-a1d8-ffce624f4f9f_2518x842.png)

此外，我们从相关工作中了解到，生成解释如何得出最终答案的自然语言理由对于算术推理任务是有益的。我们可以训练或微调模型来生成这些理由[3, 4]，但这需要为不同的推理任务创建一个高质量的理 由数据集，这是昂贵且耗时的！


> “仅采用提示方法很重要，因为它不需要大量的训练数据集，并且单个模型可以在不失一般性的情况下执行许多任务。” - 引自[1]

思维链提示结合了少样本提示的优势和生成自然语言推理的好处。我们无需进行额外的训练或微调，只需在提示中插入一些推理示例（即思维链），就能让大型语言模型通过少样本学习生成类似的推理。

### 思维链提示是如何工作的？

当我们像人类一样解决推理任务时，通常会将问题分解成更小的任务。例如，在计算餐厅给多少小费时，我通常会执行以下步骤：

- 取账单的总金额：56.00美元
- 计算总金额的10%：5.60美元
- 将该值乘以2（得出20%的小费）：11.20美元

尽管这个例子很简单，但这个思路可以延伸到我们人类所解决的各种心智推理任务中。我们通过生成思维链（在[1]中定义为“一系列连贯的中间推理步骤，最终得出问题的最终答案”）来解决这类任务。简单来说，思维链提示使大型语言模型具备了生成类似思维链的能力。

> “我们探究语言模型针对推理任务执行少样本提示的能力，所给提示由三元组构成：[输入，思维链，输出]。” —— 引自[1]

下面展示了结合推理任务和针对几个不同问题的思维链的解决方案示例。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d770953-ca52-4025-9945-683b5195c67d_1604x1394.png)

学习思维链。要教会大型语言模型生成解决问题的推理过程，我们只需将此类推理过程的示例插入到它们的提示中。然后，大型语言模型可以利用其小样本学习能力，在解决任何推理问题时生成类似的思维链。如下所示，提示通常包含几个思维链示例。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff291e2c5-0a45-4dc5-86f1-34df1552c8d0_1670x692.png)

文献[1]中的作者发现，这种提示方法使大型语言模型（LLMs）在解决问题时生成相似的思维链，这有助于推理能力，并具有几个显著的好处：

- 可解释性：大型语言模型生成的思维链可用于更好地理解模型的最终答案。
- 适用性：思维链提示可用于任何可以通过语言由人类解决的任务。
- 提示：不需要对任何大型语言模型进行训练或微调。我们只需在提示中插入几个思维链示例！  
    此外，大型语言模型甚至可以通过生成步骤更多的思维链，为复杂的推理问题分配更多的计算资源。这模仿了我们人类通常会做的事情！

此外，大型语言模型（LLMs）甚至可以通过生成步骤更多的思维链，为复杂的推理问题分配更多的计算资源。这与我们人类通常的做法很相似！

### CoT Prompting is Massively Beneficial

为了评估其对大型语言模型（LLMs）解决推理问题能力的影响，在算术、常识和符号推理基准上对思维链（CoT）提示进行了测试。使用几种不同的预训练大型语言模型进行评估，包括GPT - 3[2]、LaMDA[5]、PaLM[6]、Codex[7]和UL2[8]。作为基线，在[1]中作者使用了GPT - 3提出的标准少样本提示。所有模型在评估期间都使用贪婪解码，但通过对多个样本进行多数投票可以获得更好的结果[9]。

算术推理。算术推理任务包括数学应用题。这类问题对人类来说很容易解决，但已知大型语言模型（LLMs）在处理此类问题时会遇到困难。下面提供了[1]中使用的算术推理数据集的概述。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6487d8fa-0fe8-46d0-9a75-e441c912c9cc_1110x852.png)

对于思维链（CoT）提示，手动编写了一组八个少样本示例（无需进行大量的提示工程），并应用于除AQuA之外的所有数据集（AQuA具有多项选择结构）。下面展示了使用思维链提示对多个大型语言模型（LLMs）在算术推理数据集上进行实验的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F303b4687-7860-4d71-864d-aec0b090c6f2_1846x1210.png)

通过这些实验，我们发现了思维链提示的几个显著特性。首先，思维链提示对较大的大型语言模型（即参数超过1000亿）效果要好得多。研究发现，较小的模型会产生不合逻辑的思维链，从而降低性能，不如标准提示。此外，更复杂的问题（例如GSM8K）从思维链提示中受益更多。与先前的最先进方法（进行特定任务的微调）相比，使用GPT-3和PaLM-540B的思维链提示在所有情况下都取得了相当或更好的性能。

当我们对思维链提示生成的正确答案和错误答案进行定性分析时，我们了解到以下情况：

- 大多数正确答案是逻辑思维链的结果，只有少数情况是碰巧从不正确的思维链预测出正确答案。
- 46% 的错误答案几乎是正确的，这意味着它们包含带有小错误的思维链。
- 56% 的错误答案是由于在理解或推理方面存在重大问题的思维链导致的。

[1] 中的作者还分析了思维链提示对不同结构（例如，打乱少样本示例）的鲁棒性，并对思维链提示的各个方面进行了消融实验，发现思维链对模型性能有一致且独特的提升。有趣的是，思维链提示对小的提示扰动并不十分敏感。

常识推理。常识推理问题假定具备一般背景知识，并要求对物理和人类互动进行推理。采用与算术推理实验类似的设置（除了一些需要手动策划少样本示例的数据集外），作者对各种预训练的大型语言模型（LLMs）在常识推理任务上进行了评估，得出如下图所示的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f685d54-6a15-49fe-b6c3-46440f0f6999_1076x470.png)

简而言之，思维链（CoT）提示在常识推理问题上也被发现能带来巨大的好处。同样，我们看到更大的模型从思维链提示中受益更多。然而，标准提示和思维链提示的性能都随着模型规模的增长而提高，其中思维链提示往往能取得稍好一些的性能。

符号推理。[1]中的作者还对思维链提示在符号推理任务上进行了评估，例如：

- 最后字母串联：要求模型对序列中每个单词的最后字母进行串联并输出。
- 硬币翻转：要求模型确定在一系列硬币翻转后硬币是否仍然正面朝上。

进一步地，考虑了领域内和领域外的符号推理测试，其中领域外的示例是指那些需要比训练期间或在少样本提示中看到的示例更多的推理步骤（例如，字母接龙中单词更多或者抛硬币序列更长）的示例。领域内和领域外评估的结果如下所示。

![|200](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64d4e762-6720-4b73-9f7b-0b20ad2a73d7_446x824.png)

尽管这些任务较为简单，但我们从上文可以看出，思维链（CoT）提示法 i) 提升了符号推理任务的表现；ii) 能够更好地泛化到需要更多推理步骤的跨领域问题。此外，我们再次观察到，较小的模型无论是否有思维链提示都无法解决符号推理任务。因此，思维链提示似乎是符号推理的一种非常有益的方法 。

### 思维链提示的变体

在[1]中提出思维链（CoT）提示方法后，人们又提出了几种变体，这些变体能够提升大型语言模型（LLMs）的推理能力。这些不同的变体为激发大型语言模型中的“推理”行为提供了多种有趣且实用的方法。下面列出了一些值得关注的思维链提示变体。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89900c8b-d33a-4cbf-bffc-bf0729db0353_1618x1128.png)

**零样本思维链（zero-shot CoT）。​**​ 零样本思维链提示[10] 是对思维链提示[1] 的一个简单后续改进。为了鼓励大型语言模型（LLM）生成思维链，零样本思维链只需在所提问题的末尾添加“让我们一步步思考。” 这样简单的表述。通过对大型语言模型的提示进行这一简单添加，我们在[10] 中看到，即使没有观察到此类行为的任何明确示例，大型语言模型也能够生成思维链，从而使它们在推理任务中得出更准确的答案。有关零样本思维链与其他提示方法的比较，请参见上文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8ae4c1d-cdca-4f7b-a792-714e74c65885_1612x950.png)

自我一致性。自我一致性是思维链提示的一种变体，它利用大型语言模型（LLM）生成多个思维链，然后从这些生成结果中进行多数投票，将得票最多的作为最终答案；详见上文。在思维链提示无效的情况下，使用自我一致性通常能改善结果。简单来说，自我一致性只是用一个通过大型语言模型生成多个答案并选取其中最常见答案的流程，取代了[1]中使用的贪婪解码程序。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4141d1fc-1665-408e-9c55-1ea64970026a_1082x918.png)

*最少到最多提示法*。最少到最多提示法通过首先将一个问题分解为较小的子问题，然后分别解决这些子问题，从而超越了思维链提示法。随着每个子问题的解决，其答案会被包含在用于解决下一个子问题的提示中。与思维链提示法相比，最少到最多提示法在一些任务（例如最后一个字母连接）上提高了准确性，并改善了对需要更多推理步骤的域外问题的泛化能力。  

*提示工程*。正如上面的示例（以及思维链提示法的总体思想）所示，为大型语言模型精心策划一个有用的提示是一门艺术。为了了解更多关于如何设计更有效的提示，我强烈推荐下面链接的Learn Prompting网站上提供的课程。

### 总结

在此概述中，我们看到标准提示不足以充分发挥大型语言模型（LLMs）的能力。相反，它似乎为大型语言模型的性能提供了一种“下限”，尤其是在更困难的基于推理的任务中。思维链（CoT）提示通过利用大型语言模型的少样本学习能力，在解决基于推理的问题时引导生成连贯的多步推理过程，从而超越了标准提示技术。这种方法对大型语言模型的性能极为有益，对于较大的模型更是如此。以下是一些要点。

思维链提示的效用。大型语言模型（LLMs）不擅长处理常识、算术和符号推理等任务。然而，思维链提示能极大提高在这些任务上的表现。此外，这种方法无需进行微调，并且只需要极少的额外数据（即，仅需一组用于少样本学习的示例）。因此，这是一种易于使用的技术，只要在提示工程和精心挑选少量示例方面下些功夫，就能帮助预训练的大型语言模型解决它们通常难以处理的任务 。

推理能力随规模而显现。并非所有模型都能从思维链（CoT）提示中受益。事实上，与较小的模型相比，较大的大型语言模型（LLMs）从思维链提示中获得了不成比例的更大益处。在文献[1]中，作者观察到，在参数超过1000亿的模型中，思维链提示的益处开始显现。这个确切的数字很可能在很大程度上取决于实验设置，但总体思路很明确：思维链提示的益处在较大的模型中最为明显。


> “思维链模拟了人类推理者的思维过程。这并没有回答神经网络实际上是否在进行推理。”——摘自[1]

大型语言模型（LLMs）真的懂得如何推理吗？思维链（CoT）提示有助于大型语言模型解决某些推理任务，但这并不一定意味着大型语言模型具备复杂的推理能力。[1]中的作者甚至明确指出，对思维链提示的分析并未解答大型语言模型是否真的在推理这一问题。相反，思维链提示是一种经验性技术，可用于更准确地解决算术、常识和符号推理等通常令大型语言模型感到棘手的任务。无论我们是否相信大型语言模型具备推理能力，我们都可以认同这种技术在实践中是有用的。
