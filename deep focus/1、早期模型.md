

### GPT-2

GPT-2 的提案遵循了与其前身类似的模式。该模型使用语言建模目标进行预训练，但不执行微调，而是选择以零样本方式解决下游任务。简而言之，GPT-2 通过以下方式执行多任务学习：

1. 对原始文本数据进行通用 LM 预训练
2. 使用文本“提示”对各种任务执行零样本推理

虽然性能并不出色，但我们需要记住，_GPT-2 无需进行微调即可解决任何这些任务_。所有这些结果都是通过零样本推理实现的，这使得 GPT 在某些任务上的竞争性能相当令人印象深刻。

### GPT-3

这篇概述中将多次提到 *幂律* 的概念。即一个量的变化会导致另一个量发生相对的、比例不变的变化。

为了更具体地说明，幂律可以通过以下方程表达。$$y=ax^p$$
这里，我们研究的两个量是 $x$ 和 $y$，而 $a$ 和 $p$ 决定了这些量之间幂律的形状/行为。绘制这个幂律（设 $a = 1$，$p = 0.5$，且 $0 < x, y < 1$）会得到下图，其中将两个轴转换为对数刻度会产生幂律特有的线性趋势。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)
幂律简单地告诉我们，一个量随着另一个量的幂变化。在本概述中，我们将看到幂律的逆版本，如下所示。$$y=a\left(\frac{1}{x}\right)^p$$值得注意的是，这与之前的方程相同，只是 $p$ 的指数为负。这个负指数产生的图如下所示，其中一个量随着另一个量的增加而减少。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)

这一分析揭示了语言模型训练行为的一些基本特性。例如，如果参数总数固定，调整架构细节对模型性能的影响很小。然而，模型的测试损失随着模型大小、数据量和训练计算量的变化呈现幂律关系。

如果我们要扩大语言模型的训练规模，应该：

1. 将大部分额外计算资源投入到增加模型规模中（即，较大的模型在样本效率上更高）。
2. 增加数据集的规模（但不需要像模型规模那样多），以避免过拟合。
3. 略微增加批量大小（即，根据关键批量大小）。
4. 在模型显著收敛之前停止训练，以优化训练计算的使用。



### GPT-3 之后

随着 GPT-3 的提出，我们看到通过扩大 LLM 的规模可以获得很多好处。然而，在这篇综述中，我们探讨的问题是，模型规模是否是解决所有问题的答案。总体而言，我们了解到，仅仅扩大 LLM 的规模并不是提升任务无关性能的唯一必要因素。此外，这也带来了一些缺点。以下是主要结论的总结。

**更大的 LLM = 更多的工程努力。** 随着 LLM 规模的增大，处理它们变得越来越困难。我们已经从 GPT-3 中看到了这一点——训练一个拥有 175B 参数的模型需要结合多种分布式训练技术，是一项重大的工程壮举。对于更大的模型，如 MT-NLG，训练过程变得更加复杂。最近的努力已经降低了训练 LLM 的成本，但训练和部署这些模型所需的工程工作仍然很大。

**数据很重要。** 最初，扩大 LLM 的规模似乎是实现更好性能的首选方法。GPT-3 很出色，_为什么不让它更大呢_？然而，当我们看到随着模型变大，性能提升趋于平缓时，我们了解到在更多数据上进行训练也非常重要。LLM 性能的最大提升（例如 Gopher 和 Chinchilla）是通过模型和数据集规模的结合（大致成比例）实现的。

**深度还是宽度？** 这虽然不太显著，但当前研究似乎表明，常用的 LLM 架构可能比实际需要的要深。在某些情况下，可能更有意义的是让它们稍微浅一些，并将节省下来的参数投入到每一层的宽度中。

**监督性能仍然占主导地位。** 尽管我们观察到了 LLM 令人难以置信的任务无关性能优势，但我们必须将这些结果放在背景中进行考量。从这篇综述中的研究中我们看到，这些技术仍然不及监督训练性能。在很多情况下，通过任务特定的微调，我们仍然可以获得显著的性能提升。虽然任务无关的基础模型是一种美好的愿景，但在不进行任何任务特定适应的情况下利用这些模型进行实际应用可能还需要一段时间。_如果微调一点能大大提高我们的性能，为什么不这样做呢_？


### PaLM

在本概述中，我们将探讨 Pathways 语言模型（PaLM），这是一个使用谷歌的 Pathways 框架训练的 540B 参数的大型语言模型。通过消除流水线并行，该架构实现了惊人的训练吞吐量，使 PaLM 能够在更大规模的数据集上进行预训练。PaLM 清楚地表明，关于规模，LLM 的性能尚未达到瓶颈。只要有足够高效的训练基础设施，允许在更多数据上预训练更大规模的模型，我们就能持续看到性能的提升。

**SwiGLU 激活函数**   大多数 LLM 在每层中使用的前馈神经网络结构相似。即，这个网络执行两个前馈变换（不使用偏置，并单独应用于序列中的每个 token 向量），中间使用 ReLU 激活函数。然而，后续研究表明，选择其他激活函数可能会更好。

特别是，PaLM 使用 SwiGLU 激活函数，它结合了 Swish 和 GLU 激活。该激活函数由以下公式给出：$$\text{SwiGLU}(x) = \text{Swish}(xW) \cdot xV$$其中，Swish 激活函数定义为：$$\text{Swish}(x) = x \cdot \text{Sigmoid}(\beta x)$$换句话说，SwiGLU 是输入的两个线性变换的逐元素乘积，其中一个应用了 Swish 激活。虽然这个激活函数需要进行三次矩阵乘法，但最近的研究发现，在固定计算量下，它能带来性能优势。与 ReLU 等普通激活相比，SwiGLU 似乎提供了显著的性能提升。


的研究利用“思维链提示”（即在最终输出前在模型内生成多个推理“步骤”）来提升大型语言模型的推理能力；见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76ddeb3c-90e0-44aa-8657-74c5ea6fcbea_1432x636.png)

在评估 PaLM 时，作者发现，将这种规模的模型与思维链提示结合，足以在算术和常识推理任务上达到最新的准确率。此前的方法依赖于领域特定的架构、微调，甚至是任务特定的验证模块来解决这些推理任务。相比之下，PaLM 仅通过少样本思维链提示（以及用于算术推理任务的外部计算器模块）来解决这些任务；见下文。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8316252d-fe5d-43f6-ae34-ec5a16a8ce93_1408x1376.png)

有趣的是，我们发现最大的 PaLM 模型在推理能力上明显优于较小的变体。鉴于之前的研究发现模型规模对推理性能的影响常常是混合的（有时是负面的），这一发现很有趣。PaLM 的结果表明，在正确的提示方法下，模型（和数据）规模似乎能提升推理性能。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f388369-e932-4c10-8d04-4d2eeae0ebba_1432x536.png)

#### 关键要点

尽管最初尝试训练超过 GPT-3 规模的 LLMs 并不太成功，但在 PaLM 上我们看到，只需要一个高效的训练框架来进行更广泛的预训练。通过使用 Pathways 框架，PaLM 能够在比同规模的先前模型（如 MT-NLG）更大的数据集上进行训练。最终的 LLM 具备了令人印象深刻的多语言理解和推理能力，我们发现增加模型规模通常能带来显著的好处。以下是 PaLM 的一些重要收获。

**幂律总是成立吗？** 关于 LLMs 的众多出版物显示，LLM 性能与各种量（如非嵌入模型参数、数据集大小、训练计算量等）之间存在幂律关系。虽然这种趋势在整体性能上成立，但在分别考察每项任务的性能时，情况会更复杂。某些任务从规模中获得的好处不成比例，而其他任务则没有太大收益。因此，规模对 LLMs 通常有帮助，但结果因所解决的下游任务而异。

**我们应该避免流水线并行吗？** PaLM 的主要卖点之一是其使用的高效 Pathways 训练框架。通常，在多个 TPU pods 或计算节点上进行训练需要使用流水线并行，因为内存带宽有限。然而，通过去除流水线并行，仅使用数据和模型并行进行 TPU pods 间的训练，PaLM 实现了突破性的训练效率和吞吐量。这些训练框架的提升使 PaLM 能够在更多数据上进行训练，从而实现了模型的出色表现。

**LLM 规模与推理能力。** 之前关于 LLMs 的研究常常指出其推理能力较差。事实上，LLMs 执行推理任务的能力似乎随着规模的增加而下降。然而，在 PaLM 上我们看到情况并非总是如此。如果将更大的 LLMs 与更多的预训练数据和正确的提示方法（如链式思维提示）结合，我们会看到 LLM 推理能力的显著提升！


## T5


![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63c940a1-8868-4787-a217-4f49361dcd23_1954x1444.png)

BERT 的成功不言而喻（即在几乎所有语言基准上达到了新的最先进性能）。因此，NLP 社区开始深入研究迁移学习这一主题，提出了许多新的扩展和改进。由于该领域的快速发展，各种方法之间的比较变得困难。文本到文本转换器（T5）模型提出了一个统一的框架，用于研究 NLP 中的迁移学习方法，使我们能够分析不同的设置并得出一套最佳实践。这套最佳实践构成了 T5，一个用于语言理解任务的最先进模型和训练框架。

## 一、T5：统一的文本到文本转换器

T5 的贡献不在于新的架构或训练方法。相反，研究完全基于现有技术。T5 考虑了 NLP 迁移学习中的所有方面，如不同的（未标注）数据集、预训练目标、基准测试和微调方法。然而，这些方面都是通过统一的文本到文本格式进行研究的。T5 的目标是：*1）* 分析迁移学习设置，*2）* 确定最有效的方法。

### 1.1 文本到文本框架

T5 将所有文本处理问题转换为“文本到文本”格式（即，输入文本并输出文本）。这种通用结构也被零/少样本学习的大型语言模型所利用，使我们能够以统一的方法建模和解决各种任务。我们可以对每个任务应用相同的模型、目标、训练程序和解码过程！我们只需采用提示方法，让语言模型以文本格式生成答案。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821c34a1-b63b-4ea9-9ced-0c73549288d2_2238x1266.png)

为了更具体地说明，T5 可以将所有任务转换为如下的文本到文本格式：

1. 在原始输入序列前添加任务特定的前缀
2. 将该序列输入到转换器中
3. 将模型的目标设定为一个文本序列

使用这种格式，我们可以轻松执行摘要或翻译等任务（即，目标自然是一个序列）。此外，我们可以通过训练模型生成与正确类别相关的文本来执行分类任务。对于回归问题，这个过程会稍微复杂一些（即，我们必须将实数输出四舍五入到最接近的小数，并将其视为分类问题），但这对大多数语言任务来说效果很好。上图展示了示例。

> “在文本分类任务中，如果模型输出的文本不对应于任何可能的标签，会出现问题……在这种情况下，我们总是将模型的输出视为错误，尽管在我们训练的模型中从未观察到这种行为。” - 来自 [1]

T5 在每个解决的任务上进行微调。这与使用少样本学习的 LLMs 和使用多任务学习解决多个任务的 NLP decathlon 不同。


### 1.2 模型、数据、实验

T5 中的所有分析都使用了上述统一的文本到文本框架，因为它允许将各种不同的语言理解任务转换为统一的格式。此外，T5 的分析使用了相同的基础 transformer 架构和预训练数据集。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f25f11e-1daf-4711-940a-6b09a1f62ae7_2298x1474.png)
*T5 对编码器-解码器 Transformer 架构所做的修改*

**模型**    T5 使用的编码器-解码器架构与原始 Transformer 非常相似。不同之处在于：

1. 在每个注意力和前馈转换之前立即应用 LayerNorm（即，在残差路径之外）
2. 对 LayerNorm 不使用加性偏置（即，仅使用缩放并消除加性偏置）
3. 使用简单的位置嵌入方案，将标量添加到用于计算注意力权重的对数值上
4. 在整个网络中应用 Dropout（例如，注意力权重、前馈网络、跳跃连接等）

这些修改在上图中有所展示。使用此模型（以及其他一些模型），T5 可以测试多种迁移学习设置，以得出一系列最佳实践。

**预训练数据集**    T5 在 Colossal Clean Crawled Corpus (C4) 上进行预训练，这是一个 750GB 的“相对干净”的英语文本语料库。虽然先前的研究提出了各种预训练数据集，但作者选择自行构建数据集，因为先前的数据集不可公开获取，使用的过滤规则有限，范围有限（例如，仅来自 Creative Commons），或仅关注于机器翻译的平行数据（即，同一句话的多个不同语言版本）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F697e8965-6413-4ab1-982b-9f7ca97679fd_1682x572.png)

值得注意的是，C4 后来被用作 MassiveText 数据集的一个子集，用于预训练 Gopher 和 Chinchilla。请参阅上表以了解该数据集的大小指标，这有助于更好地理解 C4 相对于用于训练现代大型语言模型的预训练数据集的规模。对于大型语言模型（LLM），我们已经看到在足够大的数据集上预训练仅使用解码器的模型对其成功至关重要。对于具有不同架构的 Transformer，例如 T5，情况也是如此。在大规模、未标注的数据集上进行广泛的预训练有助于提高下游任务的表现。

**实验设置**   T5 在 C4 上进行预训练，然后微调以解决各种下游任务。然而，在此框架内使用的具体设置是可变的。即，我们可以更改：

- Transformer 架构
- 预训练设置（即，任务或数据量）
- 微调设置
- 模型的大小/规模

通过逐一更改这些设置并评估结果，我们可以为 NLP 中的迁移学习制定一套最佳实践，从而将 BERT 之后的众多提案提炼成一个有效的流程，用于创建高效的语言理解模型。

## 二、从 T5 中学到了什么？

如前所述，T5 的实验旨在发现 NLP 中迁移学习的最佳实践。为此，首先提出了一种基线方法，然后逐一改变该基线的几个方面（如模型架构/规模、数据集和预训练目标），以确定最佳方案。这种方法类似于一种**坐标下降**策略。我们将首先描述基线技术，然后解释 T5 在测试各种迁移学习设置后得出的结论。

### 2.1、T5 基线模型

与编码器-解码器架构相比，仅解码器模型存在一定局限，因为它们仅使用**因果（或掩码）自注意力**。掩码自注意力在计算序列中任一给定标记的表示时，只考虑前面的标记。然而，在某些情况下，我们希望对文本的初始部分或前缀执行完全可见的注意力，然后基于该前缀生成输出（例如翻译任务）。仅解码器模型无法处理这种情况，因为它们在整个输入上执行因果自注意力。

**训练 T5** T5 模型在 C4 语料库的总计 340 亿个标记上进行了预训练。相比之下，BERT 在 1370 亿个标记上训练，而 RoBERTa 在 2.2 万亿个标记上训练。受 BERT 的 MLM 目标启发，T5 使用稍作修改的去噪目标进行预训练，该目标：

1. 随机选择输入序列中 15% 的标记。
2. 用单个“哨兵”标记替换所有连续的选定标记。
3. 为每个哨兵标记分配一个唯一的 ID，适用于当前输入序列。
4. 使用所有选定标记构建目标，并用哨兵标记分隔。

虽然这个任务看起来有些复杂，但我们可以通过下面的简短输入序列的示例来了解其工作原理。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac18ac33-8c04-44ec-a012-d220071b41b3_2170x1118.png)

通过用单个哨兵标记替换整个掩码标记的跨度，我们降低了预训练的计算成本，因为这样操作的输入和目标序列通常较短。

**微调**    在预训练完成后，T5 在评估前会针对每个下游任务进行单独微调。由于 T5 使用了文本到文本的格式，预训练和微调都使用相同的**最大似然目标**！换句话说，我们将正确答案表述为一个文本序列（在预训练和微调过程中），并训练模型输出正确的文本序列。

**基线表现如何**    如下表所示，基线 T5 模型的表现与 BERT 等之前的模型相似，尽管这些模型不能直接比较（即，基线 T5 模型只使用了 BERTBase 计算量的 25%）。此外，我们看到预训练在大多数任务中提供了巨大的优势。唯一的例外是翻译任务，在这些任务中，是否进行预训练的性能相似。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40023bf9-2576-40e8-9260-bba060ef6526_1482x480.png)


### 2.2、更好的方法…

在测试基线架构和训练方法后，作者每次修改该方法的一个方面，例如基础架构、预训练目标或微调策略。通过测试这些不同的迁移学习变体，我们可以找到一种在不同语言理解任务中始终表现最佳的方法。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd824a968-c8f5-41fa-a7cd-91582803bcc1_1522x1124.png)

**架构**    为了研究架构选择对迁移学习结果的影响，我们可以测试不同的 Transformer 架构变体。T5 中测试的架构包括普通的编码器-解码器架构、仅解码器架构和前缀语言模型。前缀语言模型在序列内的固定前缀上执行完全可见的注意力，然后使用因果自注意力生成输出。主要区别在于这些架构在自注意力机制中使用的掩码类型不同。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb29a1ee3-5179-47ea-a8ba-5b2d5900f5c4_2400x558.png)

当测试不同架构（使用因果语言建模和去噪目标进行预训练）时，我们发现编码器-解码器 Transformer 架构（带去噪目标）表现最佳，因此在后续实验中采用了该架构。相较于其他模型，这种编码器-解码器变体总共有 2P 个参数，但与具有 P 个参数的仅解码器模型计算成本相同。为了将总参数数量减少到 P，可以在编码器和解码器之间共享参数，这种方法效果很好。

**预训练目标**    最初，T5 使用三种不同类型的预训练目标进行训练。第一种是 BERT 风格的 MLM 目标。其他目标包括一种去重排策略（即模型尝试将打乱的句子恢复到正确顺序）和基于前缀的语言建模目标。在后者中，文本被分成两个部分，第一部分作为输入传递给编码器，第二部分由解码器预测（即我们使用的是编码器-解码器 Transformer）。比较使用这些目标训练的模型的性能时，我们发现去噪目标明显优于其他策略。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F235c03cd-7ff6-4591-bc5e-d14caf13f1f8_1470x312.png)

从这里开始，作者测试了对 BERT 风格的 MLM 目标的几种修改，如下面的表格所示。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb70fcf3c-2f43-48b8-ab7d-bc3aa483725a_1914x804.png)

这些变体的表现往往相似；见下文。然而，通过选择预训练目标，将整个被破坏的标记序列替换为单个哨兵标记，并且仅尝试预测目标中的破坏标记，我们可以最小化预训练的计算成本。因此，遮蔽连续标记整个序列的基准策略是高效的，因为它生成了更短的目标序列。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa620a46e-df94-4c9f-98d7-d73e1c18fd69_1458x434.png)

作者测试了不同的破坏率，发现破坏率对结果没有显著影响，15%的设置效果良好。另一种预训练目标明确选择标记序列进行破坏（即基准方法是均匀选择标记而不是作为一个整体序列选择，然后将连续标记组合在一起），结果与基准方法表现相似，中测试的不同预训练目标的示意图如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F092ebb5d-a8d4-49cb-84f8-58b9666871ec_1480x890.png)

研究了多种策略，但主要结论是：(i) 去噪目标效果最佳，(ii) 去噪目标的不同变体表现相似，(iii) 最小化目标长度的策略在计算上最为高效。

**数据和模型规模**    最后，研究了规模对 T5 质量的影响。首先，T5 使用多个不同的数据集进行预训练，包括一个未过滤的数据集、一个新闻专用数据集、一个模仿 [GPT-2 的 WebText 语料库](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) 的数据集，以及几个 Wikipedia 语料库的变体。T5 在这些数据集上预训练后的表现如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1a23181-3ac4-47f6-93aa-756cf057869d_1480x466.png)

我们可以看到：(i) 不过滤预训练语料库会极大地影响性能，(ii) 在特定领域的语料库上进行预训练在某些情况下是有帮助的。例如，在新闻语料库上预训练在 ReCoRD 数据集（一个基于新闻文章的阅读理解数据集）上表现最佳。

> “这些发现的主要教训是，在领域内的无标签数据上进行预训练可以提高下游任务的性能。这并不令人惊讶，但如果我们的目标是预训练一个能够快速适应任意领域语言任务的模型，这就不太令人满意。” 

进一步地，T5 使用不同规模的 C4 语料库的截断版本进行预训练。从这些实验中，我们了解到更多的数据（不意外地）更好。在预训练期间多次循环使用较小版本的数据集会导致过拟合，并损害下游性能；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F966563aa-6b21-426e-a6f7-119f2f72fe28_1472x846.png)

为了扩大 T5 模型的规模，作者测试了以下修改：

1. 增加 4 倍的训练迭代次数（或 4 倍更大的批量大小）
2. 增加 2 倍的训练迭代次数和 2 倍更大的模型
3. 增加 4 倍更大的模型
4. 训练一个由 4 个编码器-解码器 Transformer 组成的集成模型

在这里，为了简化，预训练和微调步骤都进行了增加。这些实验的结果如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b71dbb-5ee6-442b-8617-ce153406ae16_1490x608.png)

这些结果大致符合我们的预期。增加训练时间（或批量大小）可以提高性能。将其与更大的模型结合使用，比单独增加训练迭代次数或批量大小带来更大的好处。换句话说，*增加预训练数据量和模型规模在提高性能方面是互补的*。

**其他内容**    T5 还使用不同的多任务训练策略进行微调。总体而言，这些模型的表现略逊于为每个任务单独微调的模型。然而，确实存在一些策略可以缩小任务特定微调和多任务学习之间的性能差距。

许多深度神经网络的微调方法仅训练模型参数的一个子集（例如，“冻结”早期层，只微调模型的最后几层）。作者尝试了几种以这种方式微调 T5 的技术（例如，通过适配器层或逐步解冻），但这些方法的性能不如端到端微调整个模型；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8884f5cb-ecb6-4275-9829-8cd2a8294b07_1496x546.png)


## LLaMA


*什么是 LLaMA？* LLaMA 并非单一模型，而是一系列参数规模从 70 亿到 650 亿不等的 LLM（大型语言模型）。受 Chinchilla 的启发，这些 LLM 的规模比同类模型略小，但进行了大量预训练（即模型更小，处理的词元更多），并且开发目的是提供一组不同的模型，以在性能和推理效率之间实现不同的权衡。LLaMA 模型的表现令人惊讶；例如，130 亿参数的模型大致可与 GPT-3 相媲美，而 650 亿参数的模型其性能往往超过 PaLM。

除了令人印象深刻的性能外，LLaMA 仅使用公开可用的数据进行预训练。在大型语言模型（LLM）领域朝着开源迈出一步（后退），LLaMA 模型可以完全从在线资源中复制。最近的模型如 GPT-4 已知是使用公共数据和专有/私有数据的组合进行训练的。

### 背景信息

#### RMSNorm

通常，Transformer 架构（包括 LLM 所使用的仅解码器的 Transformer 架构）使用层归一化（LayerNorm）来对其各层内的激活值进行归一化。然而，使用不同的归一化技术已被证明可以稳定训练过程并提高泛化性能。例如，均方根层归一化（RMSNorm）定义如下方程所示。$$\overline{a}_i = \frac{a_i}{\texttt{RMS}},~~where~~\texttt{RMS} = \sqrt{ \frac{1}{n}\sum_{i=1}^n a_i^2}$$
RMSNorm 与 LayerNorm 有些相似，但在对神经网络的激活值进行归一化时，它去除了均值居中操作（并且使用了略有修改的分母）。与 LayerNorm 相比，RMSNorm 计算效率更高且更简单，使其能够以 10 - 50% 的效率提升实现相当的性能水平。

#### SwiGLU 激活函数

LLM 仅解码器架构的每个块都包含一个两层前馈神经网络（即不使用偏置，并单独应用于每个标记向量），两层之间有一个非线性函数。最初，这个非线性函数是修正线性单元（ReLU）激活函数。然而，最近的研究表明，这不是最佳选择。$$\texttt{SwiGLU} (x) = \texttt{Swish}(xW) \cdot xV$$
特别是，LLaMA（以及其他像 PaLM 这样的大型语言模型）选择使用上文公式中给出的 SwiGLU 激活函数。在此，我们将 Swish 激活函数定义如下。$$\texttt{Swish}(x) = x \cdot \texttt{Sigmoid}(\beta x)$$
SwiGLU 是输入 x 的两个线性变换的逐元素乘积，其中一个线性变换应用了 Swish 激活函数。这种激活函数需要三次矩阵乘法，但即使计算量保持不变，与其他激活函数相比，它也能提高性能。

#### 重新物化（或重新计算）

重新物化（Rematerialization），也称为重新计算（recomputation），是在大型语言模型（LLMs）（以及其他大型神经网络）的训练中使用的一种技术，它以额外的计算成本来减少内存消耗。通常，在计算神经网络的前向传播时，我们会存储/保留每层的激活值，以便在后向传播期间使用（这对于计算权重更新是必要的！）。但是，这需要大量的内存！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02f2c30b-7e1b-42b0-a18c-6dfea9ada815_1164x814.png)

重新计算的基本思想是在反向传播过程中重新计算某些中间激活值，而不是在前向传播过程中将它们存储在内存中。这有助于减少训练期间的峰值内存使用量，从而允许在可用内存限制内训练更大的模型或使用更大的批量大小。这对于大型语言模型尤为重要，因为它们规模庞大且消耗大量内存。

### LLaMA 套件

简而言之：在更多数据上对较小的大型语言模型进行预训练，在更深入地研究 LLaMA 之前，我们将简要概述这些观点。总体而言，LLaMA 对大规模大型语言模型的趋势提出了强烈质疑，声称（如果进行了足够的预训练！）更小的大型语言模型可以在显著较低的推理预算下实现令人印象深刻的性能。

#### 如何最大化大型语言模型（LLM）的效率？

最近大型语言模型发展历程中一个特别值得关注的时刻是 Chinchilla 的提出。在 GPT-3 之后，深度学习研究界对足够大的语言模型中出现的令人印象深刻的少样本学习能力感到惊叹。因此，我们开始测试比 GPT-3 还要大的模型。但是，结果并不那么理想！

> 霍夫曼等人（2022年）的最新研究表明，在给定的计算预算下，最佳性能并非由最大的模型实现，而是由在更多数据上训练的小型模型实现。

为了创造出比 GPT-3 好得多的大型语言模型（LLMs），我们不能仅仅使用更大的模型。相反，我们需要更多的预训练数据！具体而言，奇努西拉（Chinchilla）的分析表明，如果我们用更广泛的数据对规模稍小的大型语言模型进行预训练，就有可能实现更高的性能水平。

*这是否就是全貌？* 尽管知道如果进行大量预训练，较小的语言模型也能表现良好，但即使是 [3] 中所做的分析也表明，训练相对较大的语言模型是达到高性能的最有效方式。这一说法完全正确，但它只考虑了训练效率。因此，我们必须问自己一个问题：我们关心的仅仅是训练效率吗？对于大多数从业者来说，对这个问题的答案无疑是否定的！

> “这项工作的重点是训练一系列语言模型，通过使用比通常更多的标记进行训练，在各种推理预算下实现尽可能最佳的性能。” 

训练成本仅是与大型语言模型（LLM）相关的全部成本中的一小部分。我们还必须考虑托管问题，这使得推理预算成为一个需要重点考量的因素。LLaMA 秉持这一理念，强调在给定目标性能水平的情况下，对较小的语言模型进行更长时间的预训练，最终在推理阶段会成本更低，并且随着时间的推移能节省大量成本。尽管如果我们需要提升性能可能会使用更大的模型，但应通过大量预训练尽可能减小模型规模（从而降低托管成本）。

#### LLaMA 组件

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa70c9131-98ec-40cb-9bde-51b3772e832c_610x534.png)

*数据集*   我们知道LLaMA的预训练数据集是基于公开数据的，但这些数据究竟来自哪里呢？LLaMA 所用预训练数据集的内容如上所述。可以看出，预训练数据（尽管完全公开）具有相当大的多样性，来源包括从 StackExchange 到 Gutenberg 项目。完整的数据集在经过标记化处理后大约包含 1.4T 个标记。Chinchilla 是在相同数量的标记上进行预训练的；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2eab82f-fe68-4c34-9583-d7e9e6d009d2_1280x504.png)

鉴于 LLaMA 对透明度和可重复性的重视，LLaMA 中提供了大量关于预训练数据集构建的见解。这一讨论最有趣的方面之一是，我们可以利用它来更多地了解在预训练大型语言模型（LLM）之前如何对数据进行过滤。例如，来自 CommonCrawl 的文本数据经过过滤以排除：

- 重复文档（使用 CCNet 管道）
- 非英语数据（通过训练 fastText 线性分类器）
- 低质量内容（使用 n-gram 语言模型）

此外，LLaMA 中的作者训练了一个线性分类器，以区分维基百科中用作参考文献的页面和随机抽样的页面，然后丢弃未被分类为参考文献的页面。所有这些步骤仅仅是为了过滤 CommonCrawl 数据！从之前的研究中，我们知道对预训练数据集进行正确过滤对于大型语言模型（LLM）的性能至关重要。在 LLaMA 中，我们对实施有效的过滤流程的具体细节有了更深入的了解。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F446d37f0-43ab-417a-81c0-ab75b4b5aa5a_1124x840.png)

*架构*   LLaMA 套件采用了许多来自像 GPT-3 和 PaLM 这类流行大型语言模型（LLM）的常见架构技巧。例如，LLaMA 在其每层内部执行预归一化，这意味着在变换器的每层输入而不是输出上应用归一化；见上文。此外，在每个变换器层中使用 RMSNorm、SwiGLU 激活函数以及旋转位置嵌入（RoPE）（即绝对位置嵌入和相对位置嵌入之间的一种混合）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F687c9b33-4dc1-4edc-b7b5-53fbd17f8a3a_996x290.png)

在 LLaMA 中，使用了四种不同大小的模型，参数数量从 67 亿到 652 亿不等；见上文。这些模型构成了被称为 LLaMA 的大型语言模型集合，并在性能与模型大小或推理预算之间提供了多种不同的权衡。最值得注意的是，我们将看到 LLaMA-13B 的性能与 GPT-3 相当，并且可以在单个 V100 GPU 上运行。与以前的模型相比，这是一个巨大的成就，使这些模型对大多数从业者来说更加易得（例如， PaLM 是使用超过 6000 个加速器进行训练的）。

*更高的效率*   LLaMA 中的作者采用了一些有趣的技巧来提高大型语言模型（LLM）的训练效率。首先，我们应该回想一下，基于仅解码器 Transformer 模型的现代大型语言模型，在其各层中使用因果多头注意力机制。

为了提高这种因果多头注意力操作的效率，LLaMA 采用了一种高效的实现方式，即不进行以下操作：i) 存储注意力权重；ii) 计算被掩码标记的标记的键/查询分数。通过这样做，我们可以节省大量通常浪费在对因果自注意力未考虑的被掩码标记上的计算量。这种方法受到了[9]中想法的启发，但我们可以在 xformers 库中找到开源实现。

除了实现高效的因果自注意力机制外，LLaMA 在重新计算（rematerialization）方面的方法与大多数大型语言模型（LLM）的训练策略有所不同。在正向传播过程中，会保存计算成本最高的激活值（例如线性层的输出），从而减少反向传播过程中需要重新计算的激活值数量。这种改变需要手动重新实现大型语言模型的反向传播（而不是使用 PyTorch 中的自动求导功能 autograd），属于一种混合式的重新计算方法，显著提高了整体训练吞吐量 。

>“在训练一个 650 亿参数模型时，我们的代码在配备 80GB 内存的 2048 个 A100 GPU 上每秒每 GPU 处理约 380 个 token。这意味着训练包含 1.4万亿 token 的数据集大约需要 21 天。” 

鉴于 LLaMA 为提高训练效率所采用的改进措施，我们可能会想知道：这实际上会让训练速度提高多少呢？嗯，这在很大程度上取决于训练基础设施。然而，当使用 2048 个 A100 GPU 时，LLaMA-65B 在 1.4T 个标记上进行预训练大约需要 21 天。

### LLaMA vs. SOTA LLMs

虽然开源和可重复性很好，但除非这些模型表现良好，否则没有人会关心 LLaMA！之前已经有过开源LLM的尝试（例如，OPT 和 BLOOM）。但是，这些模型在性能方面与现代 LLM 相比并不具有竞争力。在本节中，我们将分析 LLaMA 模型相对于像 GPT-3 和 PaLM 这样的流行 LLM 的性能。

我们如何进行评估？正如之前的文章中广泛描述的那样，LLaMA 的评估方式与大多数语言基础模型类似：通过零样本和小样本学习。值得注意的是，LLaMA 模型仅作为预训练的基础模型进行评估，这意味着不进行微调（无论是通过 SFT 还是 RLHF）。LLaMA 在自由形式生成和多项选择任务中，与流行的闭源大语言模型（例如 GPT-3、Gopher、Chinchilla 和 PaLM）以及之前的开源大语言模型（例如 OPT、GPT-J 和 GPT-Neo）进行了比较。测试涵盖多个领域（例如常识和数学推理、代码生成、问答等）。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa80ea58d-f1aa-4f7b-97bc-8855c1e086ca_2038x634.png)

*语言理解*   在闭卷问答和阅读理解任务中，我们发现 LLaMA-65B 实现了最先进的零样本和小样本性能，始终超过像 PaLM 和 Chinchilla 这样大得多的模型的性能。更进一步，LLaMA-13B 的表现出人意料地好，甚至在大多数情况下超过了 GPT-3（其规模是它的 10 倍！）的性能。这里的基本结论是：i) 更大的 LLaMA 模型与最先进的模型具有竞争力；ii) 较小的 LLaMA 模型在其规模上表现出人意料地好。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb08837-695b-4b1a-baf7-88e49a6d21cf_1170x516.png)

*推理任务*   LLaMA 系列模型也在常识和数学推理任务上进行了评估。在常识推理任务中，LLaMA 超越了几个强大基线模型的零样本推理性能；详见上文。然而，需要在此指出的是，为了促进推理能力的提升，并未采用特殊的提示方法（例如思维链提示）。先前的研究[5]表明，如果没有正确的提示方法，大型语言模型（LLMs）的“推理”能力可能会随着规模的增长而下降。

![|240](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e6a0fbc-0555-43c3-bfea-1b4141e91b90_592x724.png)

尽管这种分析存在局限性，但与基准模型相比，LLaMA 的推理能力似乎仍然相对令人印象深刻。具体而言，LLaMA 模型在数学推理数据集上与几个基准模型表现相当（甚至在某些情况下表现更好）。事实上，LLaMA-65B 甚至超过了同尺寸的 Minerva 模型，而 Minerva 模型经过了专门针对数学数据的微调以提高其在这些任务上的性能。

>“米涅瓦（Minerva）是一系列在从arXiv和数学网页中提取的385亿个标记（token）上微调过的PaLM模型……在GSM8k上，我们观察到LLaMA65B的表现优于米涅瓦-62B，尽管它未曾在数学数据上进行微调。” 

代码生成。除了基本的推理能力外，代码生成是 LLaMA 模型的另一项技能。尽管 LLaMA 从未针对代码进行微调（即代码在 LLaMA 的预训练数据中所占比例<5%），但 LLaMA-65B 在代码生成任务上优于 PaLM，而 LLaMA-13B 的代码生成性能超过了 GPT-3（不过……GPT-3在生成代码方面确实表现不佳）。

![|240](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F220f0c90-90ac-4082-b90c-fda0093cda56_594x676.png)

*其他细节*   在 MMLU 基准测试中，在大多数情况下，LLaMA模型的性能落后于Chinchilla和PaLM等大型语言模型（LLMs）。这个基准测试是LLaMA性能明显被当前其他替代模型超越的少数情况之一。[1]中的作者声称，这种性能下降是由于LLaMA预训练数据集中书籍和学术论文数量有限（即，与最先进的大型语言模型相比，这类预训练数据减少了超过10倍）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F828381e2-82c9-4307-878f-1b55a4f1e4f6_1206x816.png)

当在整个预训练过程中跟踪 LLaMA 模型的性能时，我们观察到在整个预训练过程中性能有明显且稳定的提升；见上文。尽管并非所有任务的表现都相同，但我们可以看到 LLaMA 采用的预训练策略总体上相对稳定。

### 要点  

长话短说，LLaMA 是一种性能惊人的开源大型语言模型。自 LLaMA 被提出后，研究界已经充分利用了这样一个令人印象深刻的公开可用模型。例如，以下研究工作已经在 LLaMA 的基础上进行了拓展：

- Vicuna：LLaMA 的微调版本，性能（几乎）可与 GPT-4 相媲美 链接
- Koala：在互联网对话数据上微调的 LLaMA 链接
- ChatLLaMA：利用少量计算资源，在您自己的数据上创建个性化版本的 ChatGPT 链接
- ColossalChat：基于 LLaMA 并采用 RLHF 流程的类似 ChatGPT 的模型 链接

LLaMA 的影响可能会显著增加。就我个人而言，看到关于开放大型语言模型（LLMs）的研究不断取得进展，我感到无比兴奋。我希望让这些模型更容易获取，将促使更广泛的研究社区进行更彻底的调查和研究。以下给出了一些基本的要点。

*开源大型语言模型（LLMs）*   目前，大型语言模型生态系统正见证着一场有趣的冲突，在这场冲突中，有两种不同的方法被用于向公众推出这些强大的基础模型。一方面，像 ChatGPT 和 GPT-4 这样的模型仅通过付费应用程序编程接口（API）发布，使得研究界无法详细访问此类模型。像 LLaMA 这样的贡献则逆势而行，为研究界提供了对完整模型的访问权限。

*什么尺寸最好？* LLaMA 并没有发布单一模型，而是提供了一系列不同尺寸的大型语言模型（LLMs）。先前关于大型语言模型的研究倾向于提倡使用更大的模型，因为更大的大型语言模型在训练期间通常可以用更少的总体计算成本达到令人印象深刻的性能水平。然而，如果我们更广泛地预训练一个较小的模型，LLaMA 表明我们可以在实现显著降低推理成本的同时达到可比的性能水平。因此，（至少）考虑使用较小的大型语言模型是有意义的，特别是当我们必须部署它们时。值得注意的是，一些 LLaMA 模型可以在单个 GPU 上运行，这极大地提高了此类大型语言模型的可及性。

*令人印象深刻的性能？*   在 LLaMA 提出之前，许多研究小组试图发布流行的大型语言模型（LLM）的开源版本（例如，OPT 基本上是一个开源的GPT-3）。但是，这些模型的性能比通过API访问的付费模型差得多。尽管在某些情况下LLaMA的表现并非最佳，但它是一个巨大的进步，因为它通常优于流行的最先进的大型语言模型（取决于所使用的模型大小）。