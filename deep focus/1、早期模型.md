

### GPT-2

GPT-2 的提案遵循了与其前身类似的模式。该模型使用语言建模目标进行预训练，但不执行微调，而是选择以零样本方式解决下游任务。简而言之，GPT-2 通过以下方式执行多任务学习：

1. 对原始文本数据进行通用 LM 预训练
2. 使用文本“提示”对各种任务执行零样本推理

虽然性能并不出色，但我们需要记住，_GPT-2 无需进行微调即可解决任何这些任务_。所有这些结果都是通过零样本推理实现的，这使得 GPT 在某些任务上的竞争性能相当令人印象深刻。

### GPT-3

这篇概述中将多次提到 *幂律* 的概念。即一个量的变化会导致另一个量发生相对的、比例不变的变化。

为了更具体地说明，幂律可以通过以下方程表达。$$y=ax^p$$
这里，我们研究的两个量是 $x$ 和 $y$，而 $a$ 和 $p$ 决定了这些量之间幂律的形状/行为。绘制这个幂律（设 $a = 1$，$p = 0.5$，且 $0 < x, y < 1$）会得到下图，其中将两个轴转换为对数刻度会产生幂律特有的线性趋势。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)
幂律简单地告诉我们，一个量随着另一个量的幂变化。在本概述中，我们将看到幂律的逆版本，如下所示。$$y=a\left(\frac{1}{x}\right)^p$$值得注意的是，这与之前的方程相同，只是 $p$ 的指数为负。这个负指数产生的图如下所示，其中一个量随着另一个量的增加而减少。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)

具体来说，LM 的损失通常会根据模型或数据集规模等多个因素的幂律关系而减少。我们将在后面的部分对此进行更详细的讨论。

个词，而是选择前 k 个最可能的生成，根据这些选择维护一个可能的输出序列列表，然后在最后选择这些序列中最可能的一个。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F95e07e0d-4cd6-42f8-b0ed-d933fb49015e_1118x700.png)

### 2、论文

我们现在将概述一些文献，这些文献预测[1]并通过实验证实[2]像GPT-3这样的大型语言模型的惊人实用性。通过这些文献，我们将更好地理解为什么大型语言模型如此强大，并深入分析它们在实际应用中的表现。

#### 2.1 LM 的扩展法则

GPT和GPT-2展示了语言模型作为通用基础模型的巨大潜力，但在转移到下游任务时的表现仍有待提高。因此，我们可能会开始问：_如何让这些模型更好？_

在[1]中，作者研究了一种可能的方向——扩大模型规模。他们训练了一系列仅使用解码器的语言模型，并分析其测试损失（即在保留测试集上的交叉熵语言建模损失），作为几个因素的函数，包括：

- 模型大小
- 数据量
- 训练计算量
- 批量大小
- 架构细节（即模型的宽度/深度、注意力头的数量等）
- 上下文长度（即用于预测下一个词的词数）

这一分析揭示了语言模型训练行为的一些基本特性。例如，如果参数总数固定，调整架构细节对模型性能的影响很小。然而，模型的测试损失随着模型大小、数据量和训练计算量的变化呈现幂律关系；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fda994e00-8afd-4b11-b29e-4a6d074f2a9a_2344x976.png)

为了更清楚地说明这一点，[1]中的作者考虑了三个主要因素：模型大小（N）、数据量（D）和训练计算量（C）。为了研究这些因素的扩展行为，我们首先确保其他两个因素足够大（即，它们不会成为性能的瓶颈），然后测量我们研究的因素在广泛取值范围内的测试损失。例如，为研究C的扩展特性，我们确保模型和数据集足够大，然后在不同的C设置下测量大型语言模型的性能。现在我们将分别考虑这些因素。

**模型大小。** 为了研究模型大小的扩展特性，作者在完整的数据集上训练了不同的语言模型以达到收敛。该数据集是WebText2，是GPT-2的WebTest的扩展版本，大约大10倍。通过采用具有不同总参数数量的多个语言模型，我们可以得到如下图所示的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2af05f5c-d140-47bc-96d0-25469c55f856_2048x830.png)

通过将语言模型的测试损失绘制为仅解码器层（即，不包括嵌入层中的所有参数）中参数总数的函数，我们可以看到模型损失与模型大小 $N$ 呈现平滑的幂律关系。换句话说，*增加语言模型的规模能够稳步提升其性能*。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd73449b-e9e4-4e05-b6cb-4f9a8c47527f_660x610.png)

**数据和计算**    为了研究语言模型性能如何随着训练数据量的变化而扩展，[1]的作者采用了一个足够大的语言模型，并在不同大小的数据集上进行独立的训练试验。对于每次试验，模型训练到测试损失开始增加为止，这表明过拟合的发生。分析再次显示，测试损失随着数据集大小的增加呈幂律下降。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd1b4ffe-d02a-47ed-bd1f-5c7f926065b2_714x628.png)

我们在改变训练计算量时也观察到了非常相似的趋势，计算量定义为 $C = 6NBS$，其中 $B$ 是批大小，$S$ 是训练迭代次数。给定一个足够大的数据集和固定的批大小 $B$，我们可以通过调整多个模型规模 $N$ 来获得上述结果。在这里，我们发现对于每个计算预算 $C$，通过不同的 $N$ 和 $S$ 组合可以达到最佳效果，但最佳模型损失随着训练计算量的增加呈幂律下降。

进一步分析，我们可以从这些结果中看到，语言模型的样本效率（即模型表现良好所需的样本数量）随着 $N$ 的增加而提高。为了更清晰地展示这一点，[1] 的作者分析了不同规模语言模型在训练过程中观察到的样本总数与其性能的关系，得出了下图。我们可以清楚地看到，随着模型变大，语言模型的性能提升得更快。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F72f34bc3-ce21-48e2-a4a9-5101c0b7e1f6_904x824.png)

**成对缩放规律**    除了分别分析 $N$、$D$ 和 $C$ 所观察到的幂律关系外，同时改变这些因素中的两个也能产生可预测的行为。例如，通过同时改变 $N$ 和 $D$，我们可以得到下图。在这里，我们观察到：(i) 较大的模型在较小的数据集上开始过拟合；(ii) 给定足够大的数据集，语言模型的损失相对于 $N$ 遵循严格的幂律关系。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7ca3ed5-acb1-4a5c-96d3-dade2313f900_1372x756.png)

从整体上看，这表明当我们增加语言模型的规模时，必须扩大数据集以避免过拟合。然而，[1] 的作者发现，数据规模的增加可以是次线性的（具体来说，按 $N^{0.74}$ 比例），这就足以避免过拟合。

**要点总结**    尽管我们在高层次上讨论了 [1] 中提出的幂律关系，但该论文实际上将这些规律具体化，并提出了一个准确的预测框架来估计任何语言模型的测试损失。为了简化，我们在此不深入细节，而是专注于以下语言模型训练的要点。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e45145e-6913-4a83-847d-0b8aca7360f6_1436x672.png)

如果我们要扩大语言模型的训练规模，应该：

1. 将大部分额外计算资源投入到增加模型规模中（即，较大的模型在样本效率上更高）。
2. 增加数据集的规模（但不需要像模型规模那样多），以避免过拟合。
3. 略微增加批量大小（即，根据关键批量大小 [3]）。
4. 在模型显著收敛之前停止训练，以优化训练计算的使用。

[1] 中观察到的幂律关系在几个数量级上似乎不受阻碍地持续存在。尽管这种扩展最终会达到极限，但它仍然表明，适当地增加语言模型训练的规模可以带来显著的性能提升，暗示探索大型语言模型（如 GPT-3）可能会非常有益。

> “我们的结果强烈表明，较大的模型将继续表现更好，而且它们的样本效率也比以前认为的要高得多。大模型可能比大数据更重要。” — 摘自 [1]


#### 2.2 GPT-3

先前关于 GPT 和 GPT-2 的研究 [4, 5] 开始揭示通用语言模型在解决文本理解任务中的效用。然而，这些模型仍存在局限性：

- GPT 并非完全任务无关（即，需要针对特定任务进行微调）
- 在零样本情况下，GPT-2 的表现远不如监督学习的最先进技术

现有研究提供了一个“概念验证”，表明语言模型可以通过执行零样本/少样本、任务无关的推理来消除任务指定的需求。然而，语言模型相对于监督技术的表现较差，使其实用性降低。幸运的是，[1] 中观察到的幂律关系提供了希望，即更大的语言模型（即大型语言模型）可以缩小任务无关和任务特定/监督性能之间的差距。

朝着这个方向发展，GPT-3 在现有语言模型的基础上将规模扩大了几个数量级，并与 GPT-2 共享相同的仅解码器架构（除了增加了一些稀疏注意力层 [6]）。具体来说，GPT-3 是一个拥有超过 1750 亿参数的大型语言模型（作为参考，GPT-2 [5] 包含 15 亿参数）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d05fa6b-0603-48b2-ba60-9187a0bd38af_1262x406.png)

随着 GPT-3 的出现，我们终于开始看到大型语言模型在任务无关性能上的潜力，因为模型的少样本性能在多个任务上接近监督基线。与 GPT-2 类似，作者使用语言建模目标进行预训练，但他们采用了基于过滤后的 CommonCrawl 和一些额外高质量语料的大型数据集。用于预训练的完整数据集的组成如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e0630ce-c5b2-4880-a8e7-6859f12a6b17_2210x700.png)

GPT-3 的预训练与 GPT-2 类似，但模型训练时间更长。为了使训练过程在计算上可行，作者采用了一种模型并行分布式训练方法，将每个语言模型层的部分分配到不同的 GPU 上。由于每个 GPU 只存储完整模型的一小部分，因此训练可以在不超出内存限制的情况下进行。

GPT-3 的学习过程包括两个部分：无监督/自监督预训练和上下文学习。这两个部分在下图中进行了说明。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8961e8-7484-44e3-9a78-eb4d5365cf63_2214x1276.png)

简单来说，我们首先在大型无监督文本语料库上预训练通用的语言模型，然后通过上下文学习引导该模型解决下游任务。这种上下文学习过程可以通过任务特定的微调（如在 GPT 中）进行，甚至可以使用无需对模型进行梯度更新的少样本学习等技术。下图展示了微调与零样本、一样本和少样本学习的不同变体之间的区别。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1b83b2-6cb7-4a36-80a2-0d1ac53d013f_1580x1380.png)

与之前的版本不同，GPT-3 仅使用零样本和少样本学习技术进行评估。作者没有对用于评估的下游数据集进行适配或微调，而是通过大规模文本语料库预训练这个非常大的模型，并研究是否可以仅通过包含不同数量“上下文示例”的少样本提示技术准确地进行上下文学习。

通过在一系列语言理解任务上评估 GPT-3，我们立即发现使用更大的模型显著提升了少样本表现。例如，在句子补全任务中，GPT-3 在多个流行数据集上超越了当前的最新水平（包括使用监督训练或微调的方法），并且提供更多的上下文示例似乎进一步提高了性能；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8b6ee1-49db-4981-90aa-17bf68141839_1392x1222.png)

在问答任务中，我们发现 GPT-3 的表现不如 T5 或 RoBERTa 这样的模型。然而，这些模型进行了广泛的监督微调，而 GPT-3 通过与任务无关的少样本推理实现了可比的结果。简单来说，GPT-3 在这些任务上的表现仍然令人印象深刻，因为它是一个完全通用的语言模型，没有专门针对这些任务进行优化。

在评估 GPT-3 的翻译任务时，我们观察到 GPT-3 在将其他语言翻译成英语方面优于最新的无监督神经机器翻译技术。考虑到 GPT-3 的预训练集中只有 7% 是非英语内容，并且没有明确的语言混合或翻译，这样的结果令人惊讶。有趣的是，GPT-3 在将英语翻译成其他语言时效果要差得多；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bc961b3-9741-41b5-9b70-22c5f81253c5_2028x774.png)

作者还在 SuperGLUE 基准上评估了 GPT-3，该基准包含各种语言理解任务。结果总结在下图中，我们可以看到：(i) 使用更多的上下文示例有助于提升 GPT-3 的性能；(ii) GPT-3 甚至可以超越像 BERT 这样经过微调的流行基线模型的表现。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e797415-7032-4ab3-b2d3-7ad2f8385a27_2154x1368.png)

在所有基准测试中，GPT-3 展示了随着模型规模的增长，大型语言模型在任务无关的上下文学习中变得更加有效。我们可以使用上下文示例来提示 LLMs 在各种任务中产生准确的响应，使得 GPT-3 成为第一个无需任务特定修改就能在多种下游任务上进行高精度推理的通用 LLM 的实际例子。

尽管 GPT-3 在创建任务无关的基础模型方面取得了令人难以置信的进步，但这些进展伴随着显著的计算成本。GPT-3 在一个专用的 GPU 集群上进行预训练，其预训练过程所需的计算量远远超过之前研究的任何模型；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaa4655d-d69f-4115-b343-fc294984270a_1584x962.png)

尽管最近的研究已大幅降低了 GPT-3 的训练成本（即从超过 1000 万美元的计算成本降至不到 50 万美元），但这样的基础模型仍然不便宜。如果我们想创建像 GPT-3 这样的基础模型，必须确保其性能良好。

**开源 GPT-3** 在最初提出 GPT-3 后，该模型并未公开发布，而是通过付费 API 提供访问。尽管模型的 API 被广泛使用，但缺乏对模型本身（及其训练代码）的开源访问，阻碍了进一步的分析和实验。

为了解决这个问题，一个名为 OPT-175B 的开源版本被创建并进行了分析。OPT-175B 的发布还包括一个完整的代码库和若干日志，提供了关于 LLM 训练过程的宝贵见解。要了解有关 OPT-175B 的更多信息（以及查看可用于训练类似 GPT-3 的 LLM 的代码），请查看以下概述。

[Learn about OPT-175B](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)


### 3、要点

GPT 模型最初被提出和探索的目标是创建能够解决多种任务的通用语言模型。这些模型的假设是，如果我们能够以非常细致的水平理解语言建模（即预测序列中的下一个词），那么我们可以在很多有用的方面推广这种理解，而无需进行任务特定的微调或适应。

最初，像 GPT 和 GPT-2 这样的语言模型未能实现这一目标。它们的任务无关性能远不如监督基线。然而，通过这次概述，我们了解到，增加这些语言模型的规模是创建高性能、任务无关的语言理解模型的可行路径。最终，这种思路导致了 GPT-3 的提出和分析，作为一个巨大的语言模型（即比 GPT-2 大约 100 倍），其任务无关性能远超之前的语言模型。

**扩展法则**    扩大语言模型（即使用更大的模型、更多的数据和计算资源）可以显著提高其性能。根据研究，我们了解到在扩大语言模型训练规模时，应该大幅增加基础模型的大小，同时适度增加用于预训练的数据量（以及批量大小）。较大的语言模型在样本效率上更高，其性能随着模型大小、数据量和训练计算量的增加呈幂律增长。换句话说，**语言模型越大，效果越好**。

**我们能扩展到多大**    GPT-3（一个拥有 1750 亿参数的语言模型）在前所未有的规模上验证了这些趋势。当我们采用这个庞大的模型并在大量文本语料库上进行预训练时，其任务无关的少样本性能大幅提升。虽然 GPT-3 在多个基准上仍被监督技术超越，但研究提供了明确的证据，表明随着规模的增长，大型语言模型在上下文学习中的表现有所改善。尽管 GPT-3 在技术上与 GPT-2 类似，但训练这样规模的模型是一项工程壮举，展示了语言基础模型的巨大潜力。


## 四、GPT-3 之后


在这篇概述中，我们将探讨在 GPT-3 之后生成的大型语言模型（LLMs）。GPT-3 的惊人成果清楚地表明，增大语言模型（LMs）的规模非常有益。然而，问题在于，*这种趋势何时会趋于平稳？随着参数数量的增加，模型性能是否会继续指数级提升？*

这个问题很快就被后续研究的大型语言模型所回答，这些模型的参数数量多达 5300 亿。尽管这些研究中有许多有趣的发现，但主要结论是，仅仅增大模型规模是不够的。LLM 的性能在某个点之后开始趋于平稳（即不再显著优于 GPT-3）。

然而，我们可以使用其他技术来提高 LLM 的性能。主要是我们可以不再专注于增加模型的规模，而是更多地关注预训练语料库。如果增加预训练语料库的规模和质量，通常会提升模型性能。简单来说，*提升 LLM 性能似乎需要在增加模型和数据规模上共同努力*。

### 1、语言建模入门

关于语言建模的基本概念，我在最近撰写的大型语言模型（LLMs）相关文章中已广泛讨论过。

**概念**   简单来说，语言模型（LMs）和大型语言模型（LLMs）是专注于解决单一任务的深度神经网络：预测文本序列中的下一个词。虽然这个过程还有更多内容，但基本概念确实就是这么简单。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F20cfeca0-efac-4773-b238-a5a66e43913f_1600x899.png)

要训练一个语言模型（LM），我们首先需要获取大量未标注的文本。然后，我们可以通过以下步骤进行自监督学习：

1. 抽取一些文本
2. 尝试预测下一个词
3. 根据“正确”的下一个词更新模型

这个过程称为语言模型的预训练，如上图所示。有趣的是，这种训练方法允许（L）LM从大量数据中学习，因为我们学习的文本数据不需要人工标注！我们可以直接从互联网上下载大量原始文本数据用于预训练。从如此大规模的语料库中学习对发展多样化、全面的语言理解非常有利。

**创建基础模型。** 如果我们对一个大型语言模型（LLM）进行预训练（顺便说一句，这个过程成本很高），我们就能得到一个可以根据给定文本准确预测下一个词的神经网络。起初，这可能看起来没那么有用，但这些 LLM 在自然语言方面拥有令人难以置信的基础知识。

要理解其原因，我们首先需要认识到，预测文本序列中的下一个词是一个困难的问题——即使对人类来说也不简单！准确选择下一个词实际上需要模型对语言进行深入、细致的理解。这种理解非常有益，因为它可以被重新用于解决其他类型的语言任务！

换句话说，这些 LLM 是一种基础模型——这是一个指代可以重新用于解决各种任务的大型神经网络的通用术语。这些基础模型的学习过程分为两个阶段：预训练和上下文学习。预训练过程如上所述，而上下文学习指的是使用通用 LLM 来解决更具体的下游任务的过程。

**等等……我们怎么做到这一点？** 我们可以通过多种方式将 LLM 重新用于解决下游任务。目前，许多研究致力于零样本和少样本推理技术，以使用 LLM 解决各种任务。总体而言，这些技术通过将下游任务重新表述为下一个词预测问题来解决。例如，我们可以向 LLM 提供以下提示：

- “总结以下文档：\<document\> ⇒”
- “将这句话翻译成法语：\<sentence\> ⇒”

然后，使用下一个词预测，我们可以生成一个文本响应来回答我们的预期问题。我们通过提示/请求 LLM 为我们解决任务来完成任务！上述提示是零样本学习的例子。我们还可以进行少样本学习，在提示中提供几个正确输出的示例；请参见下面的示例。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fea590d84-2fb2-40db-8044-cf68a08329b0_614x1122.png)

少样本学习随着 GPT-3 的推出而流行开来，它展示了在一定规模下的语言模型使用这种技术能表现得相对不错。然而，这种表现仍然落后于通过监督学习或微调解决下游任务的基线技术。

![|350](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4648b7d-7118-4d63-89ee-9f6d228258ef_798x1046.png)

与其进行少样本推理，我们可以直接对 LLM 进行微调（即根据输入和期望输出对模型参数进行更新）；参见上图。这种方法表现得相当不错，但也有一些缺点：

- 需要进一步的训练（可能成本高）
- 每个下游任务都需要一个专门的模型

如果能用一个模型准确解决所有任务，那就太好了。事实上，这也是基础模型的最终目标。然而，目前看来，为了达到最佳性能，微调可能是必要的。不过，在这个概述中，我们会看到大多数当前研究通过零样本/少样本推理来衡量 LLM 的性能。

#### 1.1  概述

到目前为止，希望对 LLM 的概念及其工作原理有了一定的了解。在本概述中，我们将重点关注：(i) 训练更大的 LLM 和 (ii) 使用更大的数据集进行预训练。现代 LLM 基于仅解码器的 Transformer 架构。可以通过增加更多层或增加每层的宽度来扩大这些模型的规模。为了获取更多数据来训练这些模型，我们通常使用像 Common Crawl 这样的工具从网络上抓取文本，或者使用像 Pile 数据集这样的大型文本数据源。

我们将研究四篇探索现代 LLM 的论文，并尝试改进 GPT-3 的结果。最初对更大模型的训练尝试未能完全达到 GPT-3 所设定的期望——**我们获得的性能提升并没有我们希望的那么好**。后来的研究发现，使 LLM 成功不仅仅是简单地增大模型规模——我们还需要改进预训练语料库的规模和质量。这导致了更高效的 LLM 的提出，通过在更多数据上训练较小的模型来取得显著成果。让我们来看看吧！

#### 1.2 Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model

尽管 LLM 已经相当大（例如，GPT-3 包含 1750 亿参数），但 MT-NLG 530B 将这一规模提升到了新高度。这项由 Nvidia 和微软合作的工作训练了一个拥有 5300 亿参数的 LLM。像 GPT-3 这样的模型由于其规模已经很难训练，因此训练 MT-NLG 530B（另一个仅解码器的 Transformer 模型，参数量是其 3 倍以上，如下图所示）显然非常困难。实际上，MT-NLG 需要一个专用的计算集群和若干分布式训练创新，才能使训练变得可行和高效。

![|350](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80745355-df2c-4cf5-83bb-95e885e9e5aa_730x724.png)

模型使用数据并行、模型/流水线并行和张量并行的组合进行训练；有关这些概念的快速讨论，请参见[此处](https://cameronrwolfe.substack.com/i/88082618/other-useful-details)。基于[Megatron-LM 库](https://github.com/NVIDIA/Megatron-LM)的分布式训练方法是此工作的主要贡献——仅仅设计一个能够训练具有 5300 亿参数的 LLM 的系统就极具挑战性。

在执行分布式训练时，有两种基本方法可以增加训练过程中的 GPU 数量：

1. 在机器上增加更多 GPU
2. 增加更多具有 GPU 的机器

MT-NLG 的训练过程在这两种情况下采用了不同的分布式训练方法。在每台机器内，我们使用张量切片——一种模型并行形式，将单层分为多个独立的“切片”参数，每个切片分配到不同的 GPU 上进行训练。然后，使用流水线并行将训练分配到不同的机器或计算节点。要了解更多关于这些技术的信息，请查看以下链接：

- 流水线并行 [文档](https://pytorch.org/docs/stable/pipeline.html)[教程](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html)
- 模型并行变体（包括张量切片）[博客](https://huggingface.co/transformers/v4.11.3/parallelism.html)

这种混合分布式训练方法是必要的，因为跨不同机器进行通信的成本更高。由于同一台机器上的 GPU 之间通信速度非常快，张量切片在这种情况下效果很好。然而，不同机器之间的通信时间增加，使得在多个计算节点上进行分布式训练时，流水线并行成为更有效的选择。

MT-NLG 具有 105 层，隐藏维度为 20K，每层有 128 个注意力头。模型在使用 Common Crawl 和 Pile 数据集衍生的大型文本语料库上进行训练。与之前的工作类似，进行了大量去重和匹配，以从预训练语料库中去除重复项和下游训练或测试数据。

这些过滤过程是为了避免“膨胀”模型的测试性能。如果某个下游数据集的测试数据存在于预训练语料库中，那么我们的模型可能会通过简单记忆数据轻松解决该任务，但这并不能真正反映模型的泛化能力。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F785d3762-8ea1-458f-b690-802b0f9d7ac3_1600x978.png)

经过预训练后，MT-NLG 的评估方式与 GPT-3 类似，使用与任务无关的零样本、单样本和少样本推理在大量不同基准上进行评估。评估结果显示，与 GPT-3 相比，MT-NLG 的表现略有提升。例如，在语言建模上，MT-NLG 比 GPT-3 略好（即小于 1% 的提升），在常识推理任务上也有类似结果（即所有任务和样本的最大提升约为 3%）。

在词义消歧、自然语言推理和阅读理解任务上，MT-NLG 的表现比 GPT-3 更显著提升。因此，我们可以看到，增加模型规模可能对某些任务的益处更大。例如，MT-NLG 能够将零样本词义消歧的准确率从 GPT-3 的 0% 提升到 48.59%。然而，我们应该注意，MT-NLG 的结果在所有情况下仍低于有监督的基线。仅仅增加模型规模（至少目前来看）还不足以让 LLM 在任务无关的情况下达到人类水平的表现。

总体而言，MT-NLG 的贡献主要集中在工程方面。虽然 MT-NLG 在性能上有所提升，但并不显著。然而，训练这样规模的模型在训练和使用上带来了显著的复杂性。仅仅存储这样规模的模型的优化器状态在单个 GPU 上都是不可能的！随着这些 LLM 越来越大，支撑它们的核心概念保持不变，但处理如此大规模模型的工程挑战变得越来越困难。

#### 1.3 Scaling Language Models: Methods, Analysis, and Insights from Training Gopher

![500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F88b27be7-4002-4ffc-afa3-1dfb27e31240_1600x497.png)

延续训练比 GPT-3 更大规模的 LLM 的趋势，[2] 的作者仅仅扩大了参数数量、数据集规模和用于 LLM 预训练的计算量。他们训练了一系列大小从 4400 万到 2800 亿参数不等的 LLM。然后，通过分析这些模型在 152 个不同任务上的表现来进行比较。这个评估基准（如上图所示）比以往的工作更全面（例如，以往的工作只研究了大约 120 个任务）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2076e2d3-4290-41eb-8f84-295ccae8f64c_1266x438.png)

为了预训练他们的 LLM，作者构建了一个新的 MassiveText 语料库，包含超过 2.3 万亿个标记。相比之下，用于训练 GPT-3 的 CommonCrawl 语料库包含的标记不到 5000 亿。因此，[2] 中用于预训练的数据集比以往任何工作中使用的语料库都要大得多。

在 [2] 中，具体的训练策略取决于 LLM 的规模，但作者采用了数据并行、模型并行和流水线并行的不同组合，以最大化预训练的吞吐量。除了使用相对位置编码和 RMSNorm（而不是 [LayerNorm](https://leimao.github.io/blog/Layer-Normalization/)）外，LLM 的基础架构与 GPT-3 相同。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F109471a5-1455-49b4-9276-58c0a8ade64f_1596x742.png)

与 MT-NLG [1] 不同，[2] 的结果表明，使用更大的 LLM 可以带来显著的好处。然而，为了实现这些性能提升，**必须在更大且质量更高的语料库上进行预训练**。在评估 [2] 中最大的 LLM——一个拥有 2800 亿参数的模型 Gopher 时，我们看到在 152 个任务中有 81% 的任务性能有所提升。上图提供了这些性能提升的更详细概述。

在语言建模任务中，Gopher 的性能与 GPT-3 相似。在其他任务中，性能提升最大的是知识密集型任务，如阅读理解和事实核查。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F063f78d8-667f-43f8-8564-3e1f7b56eeac_1600x504.png)

虽然 Gopher 在事实核查上优于最先进的基线，但我们应该注意到，在阅读理解方面，LLM 的少样本性能仍远不及人类和有监督学习。仅仅扩大模型和语料库的规模（不幸的是）不足以让基础模型超越任务特定技术的性能——有监督学习仍然占据主导地位。

在需要推理的任务（如数学、逻辑推理、常识推理等）中，我们发现更大的模型没有带来任何好处。事实上，Gopher 在这些任务上甚至被之前的 LLM（以及一些较小的 LLM）超越。相比于基于知识的任务，**推理密集型任务似乎从模型规模中获益更少**。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0c24c01-7fc7-4ddc-8937-ddd9cbed8d70_1590x586.png)

[2] 中的作者广泛研究了 Gopher 是否容易产生偏见或有害行为。有趣的是，结果表明，当模型接收到有害提示时，Gopher 往往会生成有害文本。此外，这种效应随着模型规模的增大而增加，较大的模型对有害提示的反应更具攻击性。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce98e69-e733-4cdf-80b9-455d46907467_1586x608.png)

Gopher 也对某些社会群体的少数群体存在偏见，如下图所示。然而，尽管有这些发现，作者强调当前评估 LLM 偏见和公平性的方法仍然有限。相对于现有社会规范，分析和改善 LLM 的行为是一个活跃且受关注的研究领域。

#### 1.4 Jurassic-1: Technical Details and Evaluation

除了增加像 GPT-3 这样的 LLM 的规模外，我们还可以考虑形状不同但规模相似的模型。例如，[3] 中的研究人员研究了一种名为 Jurassic-1 的 1780 亿参数的仅解码器 LLM。这个模型与 GPT-3 非常相似，但略大且层数较少（即 76 层而非 96 层）。为了弥补层数的减少，每层的宽度（即每个自注意力头的隐藏维度）增加，从而在参数数量上形成一个规模相似的模型。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe5a3b4e-cec4-4eec-8848-8434bb51be00_934x1016.png)

Jurassic-1 的修改架构遵循了之前研究 [6] 中关于 LLM 深度和宽度权衡的建议。该研究考察了不同深度的 LLM，并分析了模型深度和参数总数对性能的影响。有趣的是，分析显示，LLM 的最佳深度会随其规模变化。只有当模型足够大时，使用更深的 LLM 才有意义，并且可以根据参数总数准确预测最佳深度；详见上图。

[3] 中的作者在选择 Jurassic-1 的深度时遵循了 [6] 的经验预测；下面是该模型与 GPT-3 结构的比较。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F968486b4-2767-4555-8d2d-9bec9988b645_1234x316.png)

[3] 中的作者还探索了使用多词标记，并增加了基础分词器的词汇量。这一变化大大提高了标记效率，这意味着可以用更少的标记对给定的句子或文本进行编码。基本思路是，减少模型的输入标记数可以提高其效率——因为我们处理的是更短的输入序列！

此外，更好的标记效率意味着我们实际上可以在提示中提供更多的上下文示例！这是因为像 Jurassic-1 和 GPT-3 这样的模型有一个最大上下文长度，即输入中可以包含的最大标记数。如果标记效率更高，我们就能在相同的上下文长度中容纳更多数据。使用更多上下文示例的影响在下图中有所展示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F488fd4d9-51ef-4e60-a69b-02d43422aaf3_1392x490.png)

提高标记效率在文本生成任务中效果最显著，提升了23%，因为这需要逐个生成每个标记。训练和批量推理（即对一批示例进行一次前向传递）的速度也分别提高了1.5%和7%。

Jurassic-1 的训练过程与 GPT-3 非常相似。正如之前的研究所示，训练模型的优化器状态（即所有模型参数及其在优化过程中使用的相关统计数据）必须分布在多个 GPU 和计算节点上，因为模型的规模太大，无法集中存储这些状态。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79b7d03-7a56-42de-bee2-9d344c59cd72_1390x536.png)

该模型使用[公开的测试套件](https://github.com/AI21Labs/lm-evaluation)进行评估，该套件随论文发布。大多数设置借鉴了 GPT-3 的评估方法，结果显示 Jurassic-1 在大多数情况下与 GPT-3 表现相似。然而，[3] 中的作者主要考虑了零样本评估，声称这种设置比少样本评估更简单且确定性更高。

总体而言，这项工作的主要价值在于对基础分词器的修改。训练一个浅但宽的模型似乎没有带来显著的好处。尽管使用更大的词汇表增加了模型的内存使用（因为必须存储更大的嵌入层的所有参数），但提高的标记效率非常有价值，因为它可以使用更多的上下文示例，并在多个方面提高大型语言模型的效率。


#### 1.5 Training Compute-Optimal LLMs

为了最大化大型语言模型（LLM）的性能，之前的缩放趋势分析[9]表明，我们应该尽可能扩大模型的规模（即非嵌入参数的数量），而基础预训练数据集的规模则应稍微少扩展（具体来说，按模型参数数量 \(N^{0.74}\) 的比例）。这种对大规模语言模型行为的分析启发了后续工作，如 GPT-3，在任务无关的少样本性能上取得了突破性进展。

由于 GPT-3 的巨大实用性，近期研究（如本文讨论的工作）探索了更大的 LLM（例如，MT-NLG 的参数量达到了 5300 亿！）。这些模型通常遵循[9]的建议：使用非常大的模型，但并不相应地大幅增加基础数据集的规模。

有趣的是，[4] 中的研究发现这种扩展 LLM 的方法并不理想。相反，为了以计算最优的方式训练 LLM（即在固定的计算成本下实现最大性能），[4] 发现 LLM 的规模和基础预训练语料库的规模应同时增加。简单来说，这意味着相比现有工作，我们应该在更多的数据上训练 LLM。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F736814c5-e37f-41c1-8b32-8a374e159238_1600x984.png)

直观上，这种方法是合理的。正如我们在 Gopher 的研究[2]中看到的，使用更大的预训练语料库以及更大的模型，与主要关注模型规模的 MT-NLG[1]相比，能带来更显著的性能提升。

为了更具体地说明这种扩展过程，[4] 考虑了不同大小的 LLM（即从 7000 万到 160 亿参数）$N$ 和用于训练的标记数量 $D$。需要注意的是，由于预训练数据量巨大，现代 LLM 通常训练少于 1 个周期（即每个例子只出现一次）。预训练期间观察到的标记数量等于数据集的大小。

通过使用不同的 $N$ 和 $D$ 组合训练 LLM，我们可以像[8]那样，尝试发现一个幂律关系，以预测 LLM 的测试损失作为 $N$ 和 $D$ 的函数。在[4]中，作者训练了超过 400 个 LLM，并进行了这样的分析。通过对这些模型的分析，我们可以找出在不同计算预算下，哪些 $N$ 和 $D$ 的组合效果最佳。


![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8014d6-0f01-4e1b-b13e-c73830bc30bf_1600x872.png)

有趣的是，这些实验表明，训练的最佳方法是使模型规模与训练标记数量保持相同的扩展比例。这与之前建议数据集应比模型规模少扩展的分析[9]相矛盾。然而，作者通过三种不同的分析方法验证了这些发现，这些方法通过不同技术研究扩展行为（见[4]的第 3.1-3.3 节）。所有这些研究都预测数据和模型规模应该同等扩展。

总体而言，这些发现告诉我们现代 LLM _(i)_ 过大，_(ii)_ 数据训练量不足。例如，[4]中的作者预测，与 Gopher 参数数量相同的模型需要用超过 20 倍的数据进行训练才能达到计算最优。因此，如果我们想正确训练 LLM，就需要更多的数据！

> “预计所需的训练数据量远超当前用于训练大模型的数据量，这强调了数据集收集的重要性，除了允许模型扩展的工程改进之外。” - 来自[4]

为了验证这些发现，作者训练了一个名为 Chinchilla 的 700 亿参数的 LLM。与之前的模型相比，Chinchilla 较小，但在预训练期间观察到更多的数据。数据集和评估策略与 Gopher 的研究[2]相同。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9327f694-f351-4f65-9e8b-a4496347e06f_1286x506.png)

在对 Chinchilla 的评估中，我们看到该模型尽管参数量少了四倍，但仍然超越了像 Gopher 这样更大的 LLM。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5d47478-eafe-4d6e-81b8-86d6b9447b68_1284x752.png)

该模型在大量任务上进行了评估，并与其他几种现代 LLM 进行了比较。结果显示，在所有情况下，它的表现都与其他最先进的 LLM 相当或更好。这揭示了模型规模可能没有我们最初认为的那么重要，预训练数据集的大小同样至关重要！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2526d02-a08d-4efb-bd40-2138a6901540_1600x976.png)

### 主要结论

随着 GPT-3 的提出，我们看到通过扩大 LLM 的规模可以获得很多好处。然而，在这篇综述中，我们探讨的问题是，模型规模是否是解决所有问题的答案。总体而言，我们了解到，仅仅扩大 LLM 的规模并不是提升任务无关性能的唯一必要因素。此外，这也带来了一些缺点。以下是主要结论的总结。

**更大的 LLM = 更多的工程努力。** 随着 LLM 规模的增大，处理它们变得越来越困难。我们已经从 GPT-3 中看到了这一点——训练一个拥有 1750 亿参数的模型需要结合多种分布式训练技术，是一项重大的工程壮举。对于更大的模型，如 MT-NLG，训练过程变得更加复杂。最近的努力已经降低了训练 LLM 的成本，但训练和部署这些模型所需的工程工作仍然很大。

**数据很重要。** 最初，扩大 LLM 的规模似乎是实现更好性能的首选方法。GPT-3 很出色，_为什么不让它更大呢_？然而，当我们看到随着模型变大，性能提升趋于平缓时，我们了解到在更多数据上进行训练也非常重要。LLM 性能的最大提升（例如 Gopher 和 Chinchilla [2, 4]）是通过模型和数据集规模的结合（大致成比例）实现的。

**深度还是宽度？** 这虽然不太显著，但当前研究[6]似乎表明，常用的 LLM 架构可能比实际需要的要深。在某些情况下，可能更有意义的是让它们稍微浅一些，并将节省下来的参数投入到每一层的宽度中。

**监督性能仍然占主导地位。** 尽管我们观察到了 LLM 令人难以置信的任务无关性能优势，但我们必须将这些结果放在背景中进行考量。从这篇综述中的研究中我们看到，这些技术仍然不及监督训练性能。在很多情况下，通过任务特定的微调，我们仍然可以获得显著的性能提升。虽然任务无关的基础模型是一种美好的愿景，但在不进行任何任务特定适应的情况下利用这些模型进行实际应用可能还需要一段时间。_如果微调一点能大大提高我们的性能，为什么不这样做呢_？
