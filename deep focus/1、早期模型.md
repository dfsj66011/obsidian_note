
## 一、BERT

#### 3.2 训练 BERT

BERT 的训练过程分为两个步骤：

1. 预训练
2. 微调

这两个步骤的架构几乎相同，尽管可能会使用一些小的、特定任务的模块（例如，MLM 和 NSP 都使用一个额外的分类层）。

它通过将输入序列中 15% 的标记随机替换为特殊的 `[MASK]` 标记来进行掩码。然后，这些 `[MASK]` 标记的最终表示被传递到一个分类层，以预测被掩盖的词。然而，作者并不是总是以这种方式掩盖标记，而是 80% 的情况下用 `[MASK]` 替换，10% 的情况下用随机标记替换，10% 的情况下保持原始标记不变。这样的修改是为了避免 `[MASK]` 标记在预训练中存在但在微调中不存在的问题。

在微调过程中，所有 BERT 参数都是端到端训练的。与预训练相比，BERT 的微调过程成本较低。事实上，论文中的所有结果在单个 GPU 上复现都不到 2 小时。

[在 GLUE 上微调 BERT](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)

BERT 实证评估中的其他一些有趣发现如下：

- BERT Large 和 BERT Base 在所有考虑的任务中都显著优于之前的所有方法。
- BERT Large 在所有任务中显著优于 BERT Base，尤其是在训练数据较少的任务中表现出色。
- 移除 NSP 或 MLM（即使用单向语言建模目标）会显著降低 BERT 的性能。

尽管较大的模型在小数据集上表现更好似乎违反直觉（这似乎是过拟合的配方），但 BERT 的结果表明，使用较大的模型对低资源任务（即训练数据较少的任务）是有益的，只要有足够的预训练。



## 二、GPT、GPT-2

GPT-3 等语言模型彻底改变了 NLP 的现代深度学习应用，获得了广泛的宣传和认可。然而有趣的是，GPT-3 的大部分技术创新都继承自其前身 GPT 和 GPT-2。因此，对 GPT 和 GPT-2 的实用理解有助于更好地掌握当前的 NLP 方法。

GPT 和 GPT-2 模型探索的基本方法很简单。事实上，它可以归结为几个步骤：

1. 使用大量原始文本数据预训练语言模型
2. 调整此预训练模型来解决下游任务

但是描述有点模糊。_语言模型的预训练是如何进行的？我们如何“调整”语言模型来解决不同的任务？_

在本概述中，我们将对语言建模、其在 GPT 和 GPT-2 中的使用以及它如何用于解决不仅仅是生成连贯文本的问题建立基本的了解。尽管由于最近提出了更大、更强大的模型，GPT 和 GPT-2 有些过时，但它们所基于的基本概念仍然与现代深度学习应用高度相关。


### 1、先决条件

GPT 和 GPT-2 背后的基本原理是使用通用的、预训练的语言模型以高精度解决各种语言建模任务。为了充分理解这种方法，我们必须首先介绍一些关于语言模型如何工作以及如何在 GPT 和 GPT-2 中利用它们的基本概念。

#### 1.1 LM

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F89687ad1-ab5d-4c72-840c-343d7fa26ab2_1854x1030.png)


GPT 模型是使用语言建模目标在未标记文本数据的语料库/数据集上进行预训练的。简而言之，这意味着我们通过 _(i)_ 从数据集中抽取一些文本和 _(ii)_ 训练模型来预测下一个单词来训练模型。这种预训练过程是一种自监督学习，因为只需查看数据集中的下一个单词就可以确定正确的“下一个”单词。

**数学中的语言建模**   要理解语言建模，我们只需要掌握上面概述的基本思想。然而，为了使这一点更加严格，我们可以注意到我们的语料库只是一组标记。我们可以将标记视为数据集中的单个单词，但这并不完全正确。实际上，标记可能是子词甚至字符。

我们将组成我们预训练数据集的这组标记（大小为 $N$）表示如下：$$u=\{u_1, u_2, \cdots,u_N\}$$
给定一个具有参数 $\theta$ 的深度学习模型，语言建模目标试图最大化如下所示的可能性。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3430b67c-2d19-4840-9207-09e68a25d03a_1318x444.png)

简而言之，这个表达式描述了模型在给定前 $k$ 个标记作为上下文的情况下预测正确的下一个标记的概率。

使用语言建模损失（它仅表示我们的模型准确预测序列中的下一个标记的能力！），我们可以按照以下步骤预训练我们的模型的参数 $θ$ 以最小化损失：

1. 来自预训练语料库的示例文本
2. 使用我们的模型预测下一个标记
3. 使用随机梯度下降 (SGD) 或任何其他优化器来增加下一个正确的标记的概率

通过多次重复这种（自我监督）训练过程，我们的模型最终将变得非常擅长语言建模（即预测序列中的下一个标记）。

**什么是语言模型？**  使用这种自监督语言建模目标进行预训练的模型通常称为语言模型 (LM)。LM 的规模越大（即层数、参数等越多），其效率就越高。因此，我们经常会看到这些模型的更大版本（例如 GPT-3），它们被称为大型语言模型 (LLM)。 

**为什么 LM 有用？** LM 可以通过迭代预测最有可能的下一个标记来生成连贯的文本，从而实现从文本自动完成到聊天机器人等一系列应用。然而，除了生成能力之外，NLP 领域的先前研究表明，LM 预训练对各种任务都非常有益；例如，预训练的词嵌入在下游任务中很有用，LM 预训练可以提高 LSTM的性能。

除了这些方法之外，GPT 模型还探索了使用 Transformer 进行语言模型预训练。与顺序模型（例如 LSTM）相比，Transformer _(i)_ 具有极强的表达能力（即高表示容量、许多参数等）；_(ii)_ 更适合现代 GPU 的并行计算能力，允许使用更大的模型和更多的数据进行 LM 预训练。这种可扩展性使探索 LLM 成为可能，而 LLM 已经彻底改变了 NLP 应用。

#### 1.2 仅解码器的 transformers

GPT 和 GPT-2 都使用仅解码器的 Transformer 架构。Transformer 架构有两个主要组件：编码器和解码器。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0235fd2f-26f4-47ff-b95e-eddf6a4593b0_782x1152.png)

仅解码器架构从变压器中移除了以下组件：

- 整个编码器模块
- 解码器中的所有编码器-解码器自注意模块

移除这些组件后，解码器的每一层都只包含一个掩蔽的自注意力层和一个前馈神经网络。将多个这样的层堆叠在一起，形成了一个深度的、仅用于解码器的 Transformer 架构，例如用于 GPT 或 GPT-2 的架构。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F91a045da-57be-437d-962c-529ee5bc93fb_1234x828.png)

**为什么是解码器？** 选择使用解码器架构（而不是编码器）作为 LM 并不是任意的。解码器中的掩蔽自注意力层确保模型在制作 token 的表示时不能向前看序列。相反，双向自注意力（如编码器中使用的）允许根据序列中的所有其他 token 调整每个 token 的表示。

语言建模需要使用掩蔽自注意力，因为我们在预测下一个标记时不应该向前看句子。使用掩蔽自注意力会产生一个自回归架构（即，模型在时间的输出 $t$ 被用作时间的输入$t+1$），它可以连续预测序列中的下一个标记。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F83ac8a81-a3b8-42e8-bc53-6ab3a505effc_1880x1010.png)

然而，对于不需要掩蔽自注意力的任务（例如句子分类、标记等），我们应该记住使用双向自注意力确实是有益的。

#### 1.3 创建基础模型

现在我们对语言建模和相关架构有了基本的了解，我们可以理解 GPT LM 背后的灵感，它始于以下观察： 

- 未标记的文本语料库非常丰富
- 标记数据稀缺 

对于大多数深度学习系统来说，需要大量标记数据才能执行判别性语言理解任务。_当前的深度学习系统是狭隘的专家_。该模型只是在大型监督数据集上进行训练，以便它学会准确地执行特定任务。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F51e535d4-8edb-4218-9c41-3298fca62643_1624x1112.png)

尽管这种方法很常用，但它也存在一些主要限制：

1. 有些领域没有太多标记数据
2. 我们必须为每个想要解决的任务训练一个新模型（并且训练深度学习模型的成本很高！）

**基础模型**   GPT和 GPT-2 摆脱了深度学习中狭隘专家的范式。我们不必为每个应用程序训练一个新模型，而是可以预先训练一个 LM，然后以某种方式调整该模型来解决许多任务。用于解决许多任务的通用模型称为基础模型。

这种方法通过在大型、多样化的数据集上进行预训练来缓解数据稀缺问题。此外，这些模型可以重复使用或调整以解决其他任务，从而让我们避免不断训练新模型。将基础模型调整到下游任务的一种方法是在监督数据集上进行微调（即更多训练）。然而，最近，首选方法是通过零次或少量推理。

**通过提示进行零次/少次推理**。GPT 模型接收文本作为输入并产生文本作为输出。我们可以通过提供如下输入来利用这种通用的输入输出结构：

- “将这句话翻译成英语：`<sentence> =>`”
- “总结以下文件：`<document> =>`”。

这些解决任务的“提示”支持使用 LM 进行零样本（即，无需查看正确输出的示例）推理。根据这些提示，LM 的最合适输出应该可以解决该任务（例如，翻译成英文或总结文档）！要执行少样本推理，我们可以构建一个类似的提示，并在开始时提供正确输出的示例。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F20c74320-2996-47ed-9507-08e1967a36d9_736x1262.png)

### 2、论文

现在我们将概述 GPT 和 GPT-2 的细节。这些模型由 OpenAI 的研究人员发布，率先使用通用 LM 来解决下游任务。它们为 GPT-3 等突破性进展奠定了基础。这些模型之间的主要区别仅在于底层 LM 的大小。

#### 2.1 GPT 

GPT 是一种通用语言理解模型，训练分为两个阶段：预训练和微调。

![|200](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F47d302b2-a7e6-4ee9-bf10-7c161a9e4057_342x648.png)

GPT 使用 12 层、仅解码器的 Transformer 架构，与原始 Transformer 解码器 相匹配（除了使用可学习的位置嵌入之外）。GPT 首先在 BooksCorpus 数据集上执行语言模型预训练，然后在各种判别性语言理解任务上分别进行微调（以监督方式）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F60d46502-4340-48d7-8db6-057993f82060_1622x816.png)

我们不会修改 GPT 的架构来解决不同的任务，而是以特定于任务的结构提供输入，然后将模型的输出传递到单独的分类层。例如，在蕴涵任务中，我们将输入的句子连接起来，用特殊的分隔符将它们分开，将此输入提供给 GPT，然后将 GPT 的输出传递到单独的分类层。gpt 的第 3.3 节进一步解释了使用不同监督任务对 GPT 进行微调，并在上文进行了说明。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F636f5316-9d99-4b79-8c91-e4b3c76da2ef_1600x332.png)

GPT 已在上述各种任务上进行了评估。作者发现，在包含长篇连续文本（而不是单个、打乱顺序的句子）的语料库上对 GPT 进行预训练至关重要。在实验设置中，我们看到 GPT 在 12 项任务中的 9 项上实现了最佳性能，甚至始终优于模型集成。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb925c519-b72b-40a7-8d32-2f5c8b3804dc_1968x1078.png)

从这些实验中，我们了解到通用 LM 能够相对较好地理解语言概念，并且能够学习文本数据中的复杂模式（例如长期依赖关系、语言歧义等）。在不使用任何特定于任务的架构或修改的情况下，GPT 的表现远远优于许多基准，包括许多用于解决单个任务的专门解决方案。

#### 2.2 GPT-2

GPT-2 的提案遵循了与其前身类似的模式。该模型使用语言建模目标进行预训练，但不执行微调，而是选择以零样本方式解决下游任务。简而言之，GPT-2 通过以下方式执行多任务学习：

1. 对原始文本数据进行通用 LM 预训练
2. 使用文本“提示”对各种任务执行零样本推理

预训练是在自定义 WebText 数据集上进行的，该数据集是通过从 Reddit 中抓取热门链接构建的，并测试了四种不同大小的 LM。最小的模型与 GPT 的大小相匹配，最大的模型是 GPT-2。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80a786e0-35eb-4216-be14-7e32b88f5ff8_1186x460.png)

该模型架构与 GPT 相同，除了一些细微的差异（例如不同的权重初始化、更大的词汇量、更长的输入序列等）。尽管这些 LM 规模很大，但在预训练期间发现它们与 WebText 数据集的拟合度不足，这表明更大的 LM 表现会更好。

GPT-2 在多个任务（即语言建模、问答、翻译等）上进行了评估，取得了令人鼓舞的结果（但并不总是最先进的）。例如，在下表中，我们可以看到 GPT-2 在语言建模和阅读理解任务上表现良好，但在总结和问答方面远远落后于基准。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F206e161b-36a7-40a1-8dd7-06d73725deb9_1982x586.png)

虽然性能并不出色，但我们需要记住，_GPT-2 无需进行微调即可解决任何这些任务_。所有这些结果都是通过零样本推理实现的，这使得 GPT 在某些任务上的竞争性能相当令人印象深刻。

有趣的是，零样本性能随着底层 LM 的大小而不断提高，这表明增加 LM 的大小/容量可以提高其在预训练期间学习相关特征的能力。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a242e94-54ba-44e1-9f99-663a8330a67d_1966x800.png)

预训练和微调是一种有效的迁移学习范式，但 GPT-2 向我们展示了更简单、更通用的迁移方法。鉴于它们是在足够大的语料库上进行预训练的，LM 似乎能够学习下游任务，即使没有任何架构或参数修改。虽然 GPT-2 的表现并不令人印象深刻，但作者指出，更大的 LM 会好得多。

> “... 具有足够容量的语言模型将开始学习推断和执行自然​​语言序列中所展示的任务，以便更好地预测它们，无论它们的采购方法如何。” -摘自[2]

### 3、总结

GPT 和 GPT-2 教会了我们很多关于深度学习的知识。虽然从准确性的角度来看，它们在下游任务上的有效性并不令人印象深刻，但它们让我们看到了 LM 作为基础模型的巨大潜力，并为 GPT-3 等 LLM 的出现奠定了方法论基础。这些模型的影响是深远的，但我试图在下面总结一些从 GPT 和 GPT-2 研究中得出的最有用的结论和想法。

**语言模型预训练非常棒**  Transformers 能够高效利用计算资源，因此能够大规模执行语言模型预训练。在此预训练过程中学习到的表示法使预训练的 LM 能够很好地推广到解决其他任务。简而言之，_LM 不仅擅长语言建模_，它们还可以解决其他任务！

**尺寸很重要。** 正如我们在从 GPT 到 GPT-2 的过渡中所看到的，增加预训练 LM 的尺寸可以提高学习表征的质量；例如，GPT-2 在零样本/少样本推理方面远远优于 GPT。在（更大的）GPT-3 模型发布后，这一趋势变得更加明显。

**我们应该利用基础模型。** 大多数深度学习模型都是为完成单一、狭窄的任务而训练的。然而，在许多情况下，我们可以从以下方面受益：_(i)_ 通过对未标记数据的自监督学习对更大的模型进行预训练；_(ii)_ 调整此模型来解决许多任务。这种对大型基础模型的重新利用在计算上是高效的（即计算在许多任务之间共享），并且不特定于 LM。我们也可以为计算机视觉等领域训练基础模型！

#### 代码和资源

对于那些有兴趣尝试使用 GPT-2 的应用程序的人来说，代码是[公开的](https://github.com/openai/gpt-2)！但是，预训练这样的模型在计算上非常昂贵。更好的方法是[下载预先训练的语言模型](https://huggingface.co/models?sort=downloads)并对其[进行微调](https://huggingface.co/docs/transformers/v4.14.1/en/training)或执行零次/少次推理（例如，通过使用下面的演示）。


## 三、扩展定律和 GPT-3

语言模型（LM）非常通用——它们以文本为输入并生成文本为输出。最近的研究表明，这种通用的文本到文本结构可以通过提示技术来解决各种任务，而无需进行任务特定的调整（即，无需微调或架构修改），从而实现准确的零样本和少样本推理。简单来说，我们可以在一个大型未标注的文本语料库上对语言模型进行预训练（使用语言建模目标），然后通过文本提示让模型解决问题。这样，预训练模型可以轻松地用于解决不同的问题。

尽管语言模型作为任务无关的基础模型具有巨大潜力，但最初将预训练的语言模型用于下游任务（如 GPT 和 GPT-2）的尝试效果并不理想。在本概述中，我们将了解最近的研究如何在这些初步尝试的基础上进行改进，创建出在任务无关性能上表现更好的语言模型。该研究领域的关键发现是，随着语言模型规模的扩大，它们会变得更加强大。

更具体地说，我们将了解到大型语言模型（LLMs）在以下方面表现更优：(i) 比较小的模型更具样本效率，(ii) 更能无关任务地迁移到下游任务。有趣的是，这些大型语言模型的性能随着各种因素（如模型规模和训练数据量）呈现出可预测的趋势。对这些趋势的实证观察最终促成了 GPT-3 的诞生，这是一款拥有 1750 亿参数的大型语言模型，在任务无关性能上远超其前辈，甚至在某些任务上超过了最先进的监督深度学习技术。

### 1、背景

理解语言模型所需的大部分前置信息已经在我之前的一篇文章中介绍过了。这些前置知识包括语言建模目标、仅解码器的 Transformer 模型，以及如何将这些概念结合以生成强大的基础模型。请查看下面的链接以了解更多信息。

[LM Prerequisites](https://cameronrwolfe.substack.com/i/85568430/prerequisites-for-gpt)

我将在这里快速概述这些概念，并解释一些对理解像 GPT-3 这样的大型语言模型有用的附加概念。

#### 1.1 快速了解语言建模

现代语言模型使用通用的预训练程序来解决各种任务，而无需进行下游适配（即无需架构修改、微调等）。通过使用大量未标注的文本语料库，我们使用语言建模目标对模型进行预训练，具体步骤是：(i) 从语料库中采样一些文本，并且 (ii) 尝试预测下一个出现的单词。这是一种自监督学习形式，因为我们可以通过简单查看语料库中的数据来找到真实的下一个单词；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F26fd2d00-2b35-460f-83c1-4699cc5f23f6_1854x1030.png)

**架构**    现代语言模型使用仅解码器的 Transformer 架构，这种架构对模型输入应用由掩码自注意力和前馈转换组成的层序列。使用掩码自注意力而不是[双向自注意力](https://cameronrwolfe.substack.com/i/76273144/self-attention)，是为了防止模型在序列中“前瞻”来发现下一个单词。

除了这些仅解码器层外，语言模型架构还包含嵌入层，用于存储与固定大小词汇表中所有可能的标记对应的向量。利用这些嵌入层，原始文本可以转换为模型可处理的输入矩阵，具体过程如下：

1. 将原始文本[分词](https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb)为单个标记（即单词或子词）
2. 查找每个输入标记对应的嵌入向量
3. 连接标记嵌入，形成标记向量的矩阵/序列
4. 为每个标记添加位置（及其他）嵌入

请参见下图以了解该过程的示意图。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2eff169-d176-4ce5-9993-25df080dd039_1504x742.png)

LM 中嵌入层和仅解码器层之间的区分非常重要。例如，后续研究中会通过排除嵌入层的参数，仅计算仅解码器层中的参数来研究底层 LM 的参数数量。

**适应性。** 通过在大型语料库上预训练 LM，我们得到一个可以在给定上下文序列的情况下准确预测下一个标记的模型。但是，_我们如何使用这样的模型来解决诸如句子分类和语言翻译等语言理解任务呢？_

对于现代 LM，答案其实很简单——我们不改变 LM。相反，我们通过提供文本“提示”来利用模型的文本到文本输入输出结构的通用性，例如：

* 将这句话翻译成英语：\<sentence\> =>
* 总结以下文档：\<document\> =>

在这些问题解决提示的基础上，一个好的 LM 应该输出一个解决问题的文本序列！对于必须从固定解集中选择的任务（例如多项选择或分类），我们可以使用 LM 来测量生成每个潜在解决方案的概率，并选择最可能的解决方案。

[提示示例](https://www.buildgpt3.com/)

**主要结论**   现代大型语言模型的关键在于，我们可以使用语言模型预训练作为创建通用基础模型的工具，这些模型可以在不需要适应或微调的情况下解决各种问题。虽然像 GPT 和 GPT-2 这样的早期 LM 相较于微调或监督的语言理解技术表现不佳，但这种学习框架非常有前景——正如我们将在 GPT-3 中看到的，当底层 LM 变得更大时，甚至可以表现得相当好。

#### 1.2 幂律

这篇概述中将多次提到[幂律](https://en.wikipedia.org/wiki/Power_law)的概念。例如，某篇论文可能会这样陈述：

> “语言模型的测试损失与模型参数数量呈幂律关系。”

这句话简单地告诉我们，在损失和模型参数数量之间存在一种关系，即一个量的变化会导致另一个量发生相对的、[比例不变](http://felix.physics.sunysb.edu/~allen/540-05/scaling.html#:~:text=scale%20invariance%20of%20power%20law%20functions&text=The%20function%20y%3Dxp,by%20a%20scale%20factor%20a.)的变化。

为了更具体地说明，幂律可以通过以下方程表达。$$y=ax^p$$
这里，我们研究的两个量是 $x$ 和 $y$，而 $a$ 和 $p$ 决定了这些量之间幂律的形状/行为。绘制这个幂律（设 $a = 1$，$p = 0.5$，且 $0 < x, y < 1$）会得到下图，其中将两个轴转换为对数刻度会产生幂律特有的线性趋势。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3b410c6-03f9-4214-8d6a-074b1cbbf6ec_800x300.png)
幂律简单地告诉我们，一个量随着另一个量的幂变化。在本概述中，我们将看到幂律的逆版本，如下所示。$$y=a\left(\frac{1}{x}\right)^p$$值得注意的是，这与之前的方程相同，只是 $p$ 的指数为负。这个负指数产生的图如下所示，其中一个量随着另一个量的增加而减少。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Faeb36a47-1a10-41c1-ad10-dd0a5a7df7d2_800x300.png)

在我们对语言模型（LM）的分析中，将会遇到类似上述图形的幂律关系。具体来说，LM 的损失通常会根据模型或数据集规模等多个因素的幂律关系而减少。我们将在后面的部分对此进行更详细的讨论。

#### 1.3 其他有用的细节

除了语言建模的核心理念，还有一些额外的概念可能对接下来的学习有帮助。

**分布式训练。** 本文概述中的主要思想是扩展像 GPT 和 GPT-2 这样的模型，以提高其性能。然而，随着模型规模的增大，训练变得更加困难，因为计算和内存开销也在增加。为了解决这个问题，我们可以利用分布式训练技术，通过使用更多的硬件（即更多的服务器/GPU）来使大规模训练过程更易于处理和更高效。

分布式训练神经网络的方法有几种。其中一种技术是**数据并行训练**，具体步骤如下：

1. 采用一个大型小批量数据
2. 将这个小批量数据拆分成几个较小的子批量
3. 在不同的 GPU 上并行执行与每个子批量相关的计算
4. 将每个 GPU 的子批量结果汇总到一个集中的模型更新中

这种方法通过在多个 GPU 上并行化大批量数据的模型计算，提高了训练效率。

稍有不同的是，我们可以进行**模型并行训练**，这种方法是在多个 GPU 上拆分模型本身（而不是小批量数据）。例如，我们可以将模型的每一层，甚至是每层的更小部分，分配到不同的 GPU 上。实际上，这意味着前向传播过程分布在几个设备或 GPU 上，每个设备只包含模型的一小部分。这种方法允许训练更大的模型（因为每个 GPU 只存储模型的一小部分），并且通过智能流水线和模型前向传播的并行化来提高训练效率。

在本概述中，我们只需要知道可以通过在多个 GPU 上进行分布来使大型语言模型的训练更易于处理和更高效。数据并行和模型并行训练是流行的分布式训练技术的例子。关于分布式训练，还有许多考虑因素和替代方法——这是深度学习中的一个完整研究领域，产生了许多出色的实际成果。

要了解更多，我建议阅读以下文章：

- 数据和模型并行训练 [博客](https://analyticsindiamag.com/data-parallelism-vs-model-parallelism-how-do-they-differ-in-distributed-training/)

- 提高语言模型训练效率 [LM 博客](https://www.mosaicml.com/blog/billion-parameter-gpt-training-made-easy) [LLM 博客](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)

**关键批量大小**    考虑到使用大批量数据进行数据并行训练可以提高计算效率，我们是不是应该尽可能增大批量呢？其实不然，因为 _(i)_ 较大的批量可能会降低模型性能，_(ii)_ 增加批量大小会增加计算成本并需要额外的硬件。简单来说，过度增加批量大小会导致收益递减；请参见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9f1d8e5-35e1-4edd-a463-ed5a9d88187f_2338x1254.png)

考虑到这一点，我们可能会开始想：**最佳的批量大小是多少？** 这一问题在 [3] 中通过经验研究得到了解答，该研究提出了关键批量大小的概念。此研究使用一种称为**梯度噪声尺度**的指标来估计在不同领域中最大的有效批量大小。超过这个关键批量大小后，性能和计算效率开始出现收益递减。由于采用不同的批量大小会影响训练的效率和质量，因此有些研究——正如我们将在本概述中看到的——将关键批量大小作为资源高效训练的标准实践。

**束搜索**    语言模型通过在收到提示后输出文本序列来解决问题。这些序列可以通过自回归方式生成，即不断预测下一个词，将其添加到输入提示中，再预测下一个词，如此循环；请参见下图。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff759c4d3-def7-4904-b7c7-210e33d4ae08_1880x1010.png)

然而，持续选择最可能的下一个词的贪心方法并不是最优的！这是因为一个序列的概率（假设每个词是独立生成的）是每个词在给定前面词的条件下的概率的乘积（即概率的链式法则）。贪心地选择最可能的下一个词可能不会最大化这个概率；例如，最初选择一个低概率的词可能随后会导致序列中出现更高概率的词。

我们可以使用**束搜索**来找到一个近似解，而不是测试所有可能的输出词组合以找到最佳输出序列。束搜索的想法很简单：在每一步中，不是选择最可能的下一个词，而是选择前 k 个最可能的生成，根据这些选择维护一个可能的输出序列列表，然后在最后选择这些序列中最可能的一个。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F95e07e0d-4cd6-42f8-b0ed-d933fb49015e_1118x700.png)

### 2、论文

我们现在将概述一些文献，这些文献预测[1]并通过实验证实[2]像GPT-3这样的大型语言模型的惊人实用性。通过这些文献，我们将更好地理解为什么大型语言模型如此强大，并深入分析它们在实际应用中的表现。

#### 2.1 LM 的扩展法则

GPT和GPT-2展示了语言模型作为通用基础模型的巨大潜力，但在转移到下游任务时的表现仍有待提高。因此，我们可能会开始问：_如何让这些模型更好？_

在[1]中，作者研究了一种可能的方向——扩大模型规模。他们训练了一系列仅使用解码器的语言模型，并分析其测试损失（即在保留测试集上的交叉熵语言建模损失），作为几个因素的函数，包括：

- 模型大小
- 数据量
- 训练计算量
- 批量大小
- 架构细节（即模型的宽度/深度、注意力头的数量等）
- 上下文长度（即用于预测下一个词的词数）

这一分析揭示了语言模型训练行为的一些基本特性。例如，如果参数总数固定，调整架构细节对模型性能的影响很小。然而，模型的测试损失随着模型大小、数据量和训练计算量的变化呈现幂律关系；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fda994e00-8afd-4b11-b29e-4a6d074f2a9a_2344x976.png)

为了更清楚地说明这一点，[1]中的作者考虑了三个主要因素：模型大小（N）、数据量（D）和训练计算量（C）。为了研究这些因素的扩展行为，我们首先确保其他两个因素足够大（即，它们不会成为性能的瓶颈），然后测量我们研究的因素在广泛取值范围内的测试损失。例如，为研究C的扩展特性，我们确保模型和数据集足够大，然后在不同的C设置下测量大型语言模型的性能。现在我们将分别考虑这些因素。

**模型大小。** 为了研究模型大小的扩展特性，作者在完整的数据集上训练了不同的语言模型以达到收敛。该数据集是WebText2，是GPT-2的WebTest的扩展版本，大约大10倍。通过采用具有不同总参数数量的多个语言模型，我们可以得到如下图所示的结果。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2af05f5c-d140-47bc-96d0-25469c55f856_2048x830.png)

通过将语言模型的测试损失绘制为仅解码器层（即，不包括嵌入层中的所有参数）中参数总数的函数，我们可以看到模型损失与模型大小 $N$ 呈现平滑的幂律关系。换句话说，*增加语言模型的规模能够稳步提升其性能*。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd73449b-e9e4-4e05-b6cb-4f9a8c47527f_660x610.png)

**数据和计算**    为了研究语言模型性能如何随着训练数据量的变化而扩展，[1]的作者采用了一个足够大的语言模型，并在不同大小的数据集上进行独立的训练试验。对于每次试验，模型训练到测试损失开始增加为止，这表明过拟合的发生。分析再次显示，测试损失随着数据集大小的增加呈幂律下降。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd1b4ffe-d02a-47ed-bd1f-5c7f926065b2_714x628.png)

我们在改变训练计算量时也观察到了非常相似的趋势，计算量定义为 $C = 6NBS$，其中 $B$ 是批大小，$S$ 是训练迭代次数。给定一个足够大的数据集和固定的批大小 $B$，我们可以通过调整多个模型规模 $N$ 来获得上述结果。在这里，我们发现对于每个计算预算 $C$，通过不同的 $N$ 和 $S$ 组合可以达到最佳效果，但最佳模型损失随着训练计算量的增加呈幂律下降。

进一步分析，我们可以从这些结果中看到，语言模型的样本效率（即模型表现良好所需的样本数量）随着 $N$ 的增加而提高。为了更清晰地展示这一点，[1] 的作者分析了不同规模语言模型在训练过程中观察到的样本总数与其性能的关系，得出了下图。我们可以清楚地看到，随着模型变大，语言模型的性能提升得更快。

![|300](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F72f34bc3-ce21-48e2-a4a9-5101c0b7e1f6_904x824.png)

**成对缩放规律**    除了分别分析 $N$、$D$ 和 $C$ 所观察到的幂律关系外，同时改变这些因素中的两个也能产生可预测的行为。例如，通过同时改变 $N$ 和 $D$，我们可以得到下图。在这里，我们观察到：(i) 较大的模型在较小的数据集上开始过拟合；(ii) 给定足够大的数据集，语言模型的损失相对于 $N$ 遵循严格的幂律关系。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7ca3ed5-acb1-4a5c-96d3-dade2313f900_1372x756.png)

从整体上看，这表明当我们增加语言模型的规模时，必须扩大数据集以避免过拟合。然而，[1] 的作者发现，数据规模的增加可以是次线性的（具体来说，按 $N^{0.74}$ 比例），这就足以避免过拟合。

**要点总结**    尽管我们在高层次上讨论了 [1] 中提出的幂律关系，但该论文实际上将这些规律具体化，并提出了一个准确的预测框架来估计任何语言模型的测试损失。为了简化，我们在此不深入细节，而是专注于以下语言模型训练的要点。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e45145e-6913-4a83-847d-0b8aca7360f6_1436x672.png)

如果我们要扩大语言模型的训练规模，应该：

1. 将大部分额外计算资源投入到增加模型规模中（即，较大的模型在样本效率上更高）。
2. 增加数据集的规模（但不需要像模型规模那样多），以避免过拟合。
3. 略微增加批量大小（即，根据关键批量大小 [3]）。
4. 在模型显著收敛之前停止训练，以优化训练计算的使用。

[1] 中观察到的幂律关系在几个数量级上似乎不受阻碍地持续存在。尽管这种扩展最终会达到极限，但它仍然表明，适当地增加语言模型训练的规模可以带来显著的性能提升，暗示探索大型语言模型（如 GPT-3）可能会非常有益。

> “我们的结果强烈表明，较大的模型将继续表现更好，而且它们的样本效率也比以前认为的要高得多。大模型可能比大数据更重要。” — 摘自 [1]


#### 2.2 GPT-3

先前关于 GPT 和 GPT-2 的研究 [4, 5] 开始揭示通用语言模型在解决文本理解任务中的效用。然而，这些模型仍存在局限性：

- GPT 并非完全任务无关（即，需要针对特定任务进行微调）
- 在零样本情况下，GPT-2 的表现远不如监督学习的最先进技术

现有研究提供了一个“概念验证”，表明语言模型可以通过执行零样本/少样本、任务无关的推理来消除任务指定的需求。然而，语言模型相对于监督技术的表现较差，使其实用性降低。幸运的是，[1] 中观察到的幂律关系提供了希望，即更大的语言模型（即大型语言模型）可以缩小任务无关和任务特定/监督性能之间的差距。

朝着这个方向发展，GPT-3 在现有语言模型的基础上将规模扩大了几个数量级，并与 GPT-2 共享相同的仅解码器架构（除了增加了一些稀疏注意力层 [6]）。具体来说，GPT-3 是一个拥有超过 1750 亿参数的大型语言模型（作为参考，GPT-2 [5] 包含 15 亿参数）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d05fa6b-0603-48b2-ba60-9187a0bd38af_1262x406.png)

随着 GPT-3 的出现，我们终于开始看到大型语言模型在任务无关性能上的潜力，因为模型的少样本性能在多个任务上接近监督基线。与 GPT-2 类似，作者使用语言建模目标进行预训练，但他们采用了基于过滤后的 CommonCrawl 和一些额外高质量语料的大型数据集。用于预训练的完整数据集的组成如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e0630ce-c5b2-4880-a8e7-6859f12a6b17_2210x700.png)

GPT-3 的预训练与 GPT-2 类似，但模型训练时间更长。为了使训练过程在计算上可行，作者采用了一种模型并行分布式训练方法，将每个语言模型层的部分分配到不同的 GPU 上。由于每个 GPU 只存储完整模型的一小部分，因此训练可以在不超出内存限制的情况下进行。

GPT-3 的学习过程包括两个部分：无监督/自监督预训练和上下文学习。这两个部分在下图中进行了说明。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8961e8-7484-44e3-9a78-eb4d5365cf63_2214x1276.png)

简单来说，我们首先在大型无监督文本语料库上预训练通用的语言模型，然后通过上下文学习引导该模型解决下游任务。这种上下文学习过程可以通过任务特定的微调（如在 GPT 中）进行，甚至可以使用无需对模型进行梯度更新的少样本学习等技术。下图展示了微调与零样本、一样本和少样本学习的不同变体之间的区别。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1b83b2-6cb7-4a36-80a2-0d1ac53d013f_1580x1380.png)

与之前的版本不同，GPT-3 仅使用零样本和少样本学习技术进行评估。作者没有对用于评估的下游数据集进行适配或微调，而是通过大规模文本语料库预训练这个非常大的模型，并研究是否可以仅通过包含不同数量“上下文示例”的少样本提示技术准确地进行上下文学习。

通过在一系列语言理解任务上评估 GPT-3，我们立即发现使用更大的模型显著提升了少样本表现。例如，在句子补全任务中，GPT-3 在多个流行数据集上超越了当前的最新水平（包括使用监督训练或微调的方法），并且提供更多的上下文示例似乎进一步提高了性能；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8b6ee1-49db-4981-90aa-17bf68141839_1392x1222.png)

在问答任务中，我们发现 GPT-3 的表现不如 T5 或 RoBERTa 这样的模型。然而，这些模型进行了广泛的监督微调，而 GPT-3 通过与任务无关的少样本推理实现了可比的结果。简单来说，GPT-3 在这些任务上的表现仍然令人印象深刻，因为它是一个完全通用的语言模型，没有专门针对这些任务进行优化。

在评估 GPT-3 的翻译任务时，我们观察到 GPT-3 在将其他语言翻译成英语方面优于最新的无监督神经机器翻译技术。考虑到 GPT-3 的预训练集中只有 7% 是非英语内容，并且没有明确的语言混合或翻译，这样的结果令人惊讶。有趣的是，GPT-3 在将英语翻译成其他语言时效果要差得多；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bc961b3-9741-41b5-9b70-22c5f81253c5_2028x774.png)

作者还在 SuperGLUE 基准上评估了 GPT-3，该基准包含各种语言理解任务。结果总结在下图中，我们可以看到：(i) 使用更多的上下文示例有助于提升 GPT-3 的性能；(ii) GPT-3 甚至可以超越像 BERT 这样经过微调的流行基线模型的表现。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e797415-7032-4ab3-b2d3-7ad2f8385a27_2154x1368.png)

在所有基准测试中，GPT-3 展示了随着模型规模的增长，大型语言模型在任务无关的上下文学习中变得更加有效。我们可以使用上下文示例来提示 LLMs 在各种任务中产生准确的响应，使得 GPT-3 成为第一个无需任务特定修改就能在多种下游任务上进行高精度推理的通用 LLM 的实际例子。

尽管 GPT-3 在创建任务无关的基础模型方面取得了令人难以置信的进步，但这些进展伴随着显著的计算成本。GPT-3 在一个专用的 GPU 集群上进行预训练，其预训练过程所需的计算量远远超过之前研究的任何模型；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaa4655d-d69f-4115-b343-fc294984270a_1584x962.png)

尽管最近的研究已大幅降低了 GPT-3 的训练成本（即从超过 1000 万美元的计算成本降至不到 50 万美元），但这样的基础模型仍然不便宜。如果我们想创建像 GPT-3 这样的基础模型，必须确保其性能良好。

**开源 GPT-3** 在最初提出 GPT-3 后，该模型并未公开发布，而是通过付费 API 提供访问。尽管模型的 API 被广泛使用，但缺乏对模型本身（及其训练代码）的开源访问，阻碍了进一步的分析和实验。

为了解决这个问题，一个名为 OPT-175B 的开源版本被创建并进行了分析。OPT-175B 的发布还包括一个完整的代码库和若干日志，提供了关于 LLM 训练过程的宝贵见解。要了解有关 OPT-175B 的更多信息（以及查看可用于训练类似 GPT-3 的 LLM 的代码），请查看以下概述。

[Learn about OPT-175B](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15)


### 3、要点

GPT 模型最初被提出和探索的目标是创建能够解决多种任务的通用语言模型。这些模型的假设是，如果我们能够以非常细致的水平理解语言建模（即预测序列中的下一个词），那么我们可以在很多有用的方面推广这种理解，而无需进行任务特定的微调或适应。

最初，像 GPT 和 GPT-2 这样的语言模型未能实现这一目标。它们的任务无关性能远不如监督基线。然而，通过这次概述，我们了解到，增加这些语言模型的规模是创建高性能、任务无关的语言理解模型的可行路径。最终，这种思路导致了 GPT-3 的提出和分析，作为一个巨大的语言模型（即比 GPT-2 大约 100 倍），其任务无关性能远超之前的语言模型。

**扩展法则**    扩大语言模型（即使用更大的模型、更多的数据和计算资源）可以显著提高其性能。根据研究，我们了解到在扩大语言模型训练规模时，应该大幅增加基础模型的大小，同时适度增加用于预训练的数据量（以及批量大小）。较大的语言模型在样本效率上更高，其性能随着模型大小、数据量和训练计算量的增加呈幂律增长。换句话说，**语言模型越大，效果越好**。

**我们能扩展到多大**    GPT-3（一个拥有 1750 亿参数的语言模型）在前所未有的规模上验证了这些趋势。当我们采用这个庞大的模型并在大量文本语料库上进行预训练时，其任务无关的少样本性能大幅提升。虽然 GPT-3 在多个基准上仍被监督技术超越，但研究提供了明确的证据，表明随着规模的增长，大型语言模型在上下文学习中的表现有所改善。尽管 GPT-3 在技术上与 GPT-2 类似，但训练这样规模的模型是一项工程壮举，展示了语言基础模型的巨大潜力。


## 四、GPT-3 之后


在这篇概述中，我们将探讨在 GPT-3 之后生成的大型语言模型（LLMs）。GPT-3 的惊人成果清楚地表明，增大语言模型（LMs）的规模非常有益。然而，问题在于，*这种趋势何时会趋于平稳？随着参数数量的增加，模型性能是否会继续指数级提升？*

这个问题很快就被后续研究的大型语言模型所回答，这些模型的参数数量多达 5300 亿。尽管这些研究中有许多有趣的发现，但主要结论是，仅仅增大模型规模是不够的。LLM 的性能在某个点之后开始趋于平稳（即不再显著优于 GPT-3）。

然而，我们可以使用其他技术来提高 LLM 的性能。主要是我们可以不再专注于增加模型的规模，而是更多地关注预训练语料库。如果增加预训练语料库的规模和质量，通常会提升模型性能。简单来说，*提升 LLM 性能似乎需要在增加模型和数据规模上共同努力*。

### 1、语言建模入门

关于语言建模的基本概念，我在最近撰写的大型语言模型（LLMs）相关文章中已广泛讨论过。

**概念**   简单来说，语言模型（LMs）和大型语言模型（LLMs）是专注于解决单一任务的深度神经网络：预测文本序列中的下一个词。虽然这个过程还有更多内容，但基本概念确实就是这么简单。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F20cfeca0-efac-4773-b238-a5a66e43913f_1600x899.png)

要训练一个语言模型（LM），我们首先需要获取大量未标注的文本。然后，我们可以通过以下步骤进行自监督学习：

1. 抽取一些文本
2. 尝试预测下一个词
3. 根据“正确”的下一个词更新模型

这个过程称为语言模型的预训练，如上图所示。有趣的是，这种训练方法允许（L）LM从大量数据中学习，因为我们学习的文本数据不需要人工标注！我们可以直接从互联网上下载大量原始文本数据用于预训练。从如此大规模的语料库中学习对发展多样化、全面的语言理解非常有利。

**创建基础模型。** 如果我们对一个大型语言模型（LLM）进行预训练（顺便说一句，这个过程成本很高），我们就能得到一个可以根据给定文本准确预测下一个词的神经网络。起初，这可能看起来没那么有用，但这些 LLM 在自然语言方面拥有令人难以置信的基础知识。

要理解其原因，我们首先需要认识到，预测文本序列中的下一个词是一个困难的问题——即使对人类来说也不简单！准确选择下一个词实际上需要模型对语言进行深入、细致的理解。这种理解非常有益，因为它可以被重新用于解决其他类型的语言任务！

换句话说，这些 LLM 是一种基础模型——这是一个指代可以重新用于解决各种任务的大型神经网络的通用术语。这些基础模型的学习过程分为两个阶段：预训练和上下文学习。预训练过程如上所述，而上下文学习指的是使用通用 LLM 来解决更具体的下游任务的过程。

**等等……我们怎么做到这一点？** 我们可以通过多种方式将 LLM 重新用于解决下游任务。目前，许多研究致力于零样本和少样本推理技术，以使用 LLM 解决各种任务。总体而言，这些技术通过将下游任务重新表述为下一个词预测问题来解决。例如，我们可以向 LLM 提供以下提示：

- “总结以下文档：\<document\> ⇒”
- “将这句话翻译成法语：\<sentence\> ⇒”

然后，使用下一个词预测，我们可以生成一个文本响应来回答我们的预期问题。我们通过提示/请求 LLM 为我们解决任务来完成任务！上述提示是零样本学习的例子。我们还可以进行少样本学习，在提示中提供几个正确输出的示例；请参见下面的示例。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fea590d84-2fb2-40db-8044-cf68a08329b0_614x1122.png)

少样本学习随着 GPT-3 的推出而流行开来，它展示了在一定规模下的语言模型使用这种技术能表现得相对不错。然而，这种表现仍然落后于通过监督学习或微调解决下游任务的基线技术。

![|350](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4648b7d-7118-4d63-89ee-9f6d228258ef_798x1046.png)

与其进行少样本推理，我们可以直接对 LLM 进行微调（即根据输入和期望输出对模型参数进行更新）；参见上图。这种方法表现得相当不错，但也有一些缺点：

- 需要进一步的训练（可能成本高）
- 每个下游任务都需要一个专门的模型

如果能用一个模型准确解决所有任务，那就太好了。事实上，这也是基础模型的最终目标。然而，目前看来，为了达到最佳性能，微调可能是必要的。不过，在这个概述中，我们会看到大多数当前研究通过零样本/少样本推理来衡量 LLM 的性能。

#### 1.1  概述

到目前为止，希望对 LLM 的概念及其工作原理有了一定的了解。在本概述中，我们将重点关注：(i) 训练更大的 LLM 和 (ii) 使用更大的数据集进行预训练。现代 LLM 基于仅解码器的 Transformer 架构。可以通过增加更多层或增加每层的宽度来扩大这些模型的规模。为了获取更多数据来训练这些模型，我们通常使用像 Common Crawl 这样的工具从网络上抓取文本，或者使用像 Pile 数据集这样的大型文本数据源。

我们将研究四篇探索现代 LLM 的论文，并尝试改进 GPT-3 的结果。最初对更大模型的训练尝试未能完全达到 GPT-3 所设定的期望——**我们获得的性能提升并没有我们希望的那么好**。后来的研究发现，使 LLM 成功不仅仅是简单地增大模型规模——我们还需要改进预训练语料库的规模和质量。这导致了更高效的 LLM 的提出，通过在更多数据上训练较小的模型来取得显著成果。让我们来看看吧！

#### 1.2 Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model

尽管 LLM 已经相当大（例如，GPT-3 包含 1750 亿参数），但 MT-NLG 530B 将这一规模提升到了新高度。这项由 Nvidia 和微软合作的工作训练了一个拥有 5300 亿参数的 LLM。像 GPT-3 这样的模型由于其规模已经很难训练，因此训练 MT-NLG 530B（另一个仅解码器的 Transformer 模型，参数量是其 3 倍以上，如下图所示）显然非常困难。实际上，MT-NLG 需要一个专用的计算集群和若干分布式训练创新，才能使训练变得可行和高效。

![|350](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80745355-df2c-4cf5-83bb-95e885e9e5aa_730x724.png)

模型使用数据并行、模型/流水线并行和张量并行的组合进行训练；有关这些概念的快速讨论，请参见[此处](https://cameronrwolfe.substack.com/i/88082618/other-useful-details)。基于[Megatron-LM 库](https://github.com/NVIDIA/Megatron-LM)的分布式训练方法是此工作的主要贡献——仅仅设计一个能够训练具有 5300 亿参数的 LLM 的系统就极具挑战性。

在执行分布式训练时，有两种基本方法可以增加训练过程中的 GPU 数量：

1. 在机器上增加更多 GPU
2. 增加更多具有 GPU 的机器

MT-NLG 的训练过程在这两种情况下采用了不同的分布式训练方法。在每台机器内，我们使用张量切片——一种模型并行形式，将单层分为多个独立的“切片”参数，每个切片分配到不同的 GPU 上进行训练。然后，使用流水线并行将训练分配到不同的机器或计算节点。要了解更多关于这些技术的信息，请查看以下链接：

- 流水线并行 [文档](https://pytorch.org/docs/stable/pipeline.html)[教程](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html)
- 模型并行变体（包括张量切片）[博客](https://huggingface.co/transformers/v4.11.3/parallelism.html)

这种混合分布式训练方法是必要的，因为跨不同机器进行通信的成本更高。由于同一台机器上的 GPU 之间通信速度非常快，张量切片在这种情况下效果很好。然而，不同机器之间的通信时间增加，使得在多个计算节点上进行分布式训练时，流水线并行成为更有效的选择。

MT-NLG 具有 105 层，隐藏维度为 20K，每层有 128 个注意力头。模型在使用 Common Crawl 和 Pile 数据集衍生的大型文本语料库上进行训练。与之前的工作类似，进行了大量去重和匹配，以从预训练语料库中去除重复项和下游训练或测试数据。

这些过滤过程是为了避免“膨胀”模型的测试性能。如果某个下游数据集的测试数据存在于预训练语料库中，那么我们的模型可能会通过简单记忆数据轻松解决该任务，但这并不能真正反映模型的泛化能力。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F785d3762-8ea1-458f-b690-802b0f9d7ac3_1600x978.png)

经过预训练后，MT-NLG 的评估方式与 GPT-3 类似，使用与任务无关的零样本、单样本和少样本推理在大量不同基准上进行评估。评估结果显示，与 GPT-3 相比，MT-NLG 的表现略有提升。例如，在语言建模上，MT-NLG 比 GPT-3 略好（即小于 1% 的提升），在常识推理任务上也有类似结果（即所有任务和样本的最大提升约为 3%）。

在词义消歧、自然语言推理和阅读理解任务上，MT-NLG 的表现比 GPT-3 更显著提升。因此，我们可以看到，增加模型规模可能对某些任务的益处更大。例如，MT-NLG 能够将零样本词义消歧的准确率从 GPT-3 的 0% 提升到 48.59%。然而，我们应该注意，MT-NLG 的结果在所有情况下仍低于有监督的基线。仅仅增加模型规模（至少目前来看）还不足以让 LLM 在任务无关的情况下达到人类水平的表现。

总体而言，MT-NLG 的贡献主要集中在工程方面。虽然 MT-NLG 在性能上有所提升，但并不显著。然而，训练这样规模的模型在训练和使用上带来了显著的复杂性。仅仅存储这样规模的模型的优化器状态在单个 GPU 上都是不可能的！随着这些 LLM 越来越大，支撑它们的核心概念保持不变，但处理如此大规模模型的工程挑战变得越来越困难。

#### 1.3 Scaling Language Models: Methods, Analysis, and Insights from Training Gopher

![500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F88b27be7-4002-4ffc-afa3-1dfb27e31240_1600x497.png)

延续训练比 GPT-3 更大规模的 LLM 的趋势，[2] 的作者仅仅扩大了参数数量、数据集规模和用于 LLM 预训练的计算量。他们训练了一系列大小从 4400 万到 2800 亿参数不等的 LLM。然后，通过分析这些模型在 152 个不同任务上的表现来进行比较。这个评估基准（如上图所示）比以往的工作更全面（例如，以往的工作只研究了大约 120 个任务）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2076e2d3-4290-41eb-8f84-295ccae8f64c_1266x438.png)

为了预训练他们的 LLM，作者构建了一个新的 MassiveText 语料库，包含超过 2.3 万亿个标记。相比之下，用于训练 GPT-3 的 CommonCrawl 语料库包含的标记不到 5000 亿。因此，[2] 中用于预训练的数据集比以往任何工作中使用的语料库都要大得多。

在 [2] 中，具体的训练策略取决于 LLM 的规模，但作者采用了数据并行、模型并行和流水线并行的不同组合，以最大化预训练的吞吐量。除了使用相对位置编码和 RMSNorm（而不是 [LayerNorm](https://leimao.github.io/blog/Layer-Normalization/)）外，LLM 的基础架构与 GPT-3 相同。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F109471a5-1455-49b4-9276-58c0a8ade64f_1596x742.png)

与 MT-NLG [1] 不同，[2] 的结果表明，使用更大的 LLM 可以带来显著的好处。然而，为了实现这些性能提升，**必须在更大且质量更高的语料库上进行预训练**。在评估 [2] 中最大的 LLM——一个拥有 2800 亿参数的模型 Gopher 时，我们看到在 152 个任务中有 81% 的任务性能有所提升。上图提供了这些性能提升的更详细概述。

在语言建模任务中，Gopher 的性能与 GPT-3 相似。在其他任务中，性能提升最大的是知识密集型任务，如阅读理解和事实核查。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F063f78d8-667f-43f8-8564-3e1f7b56eeac_1600x504.png)

虽然 Gopher 在事实核查上优于最先进的基线，但我们应该注意到，在阅读理解方面，LLM 的少样本性能仍远不及人类和有监督学习。仅仅扩大模型和语料库的规模（不幸的是）不足以让基础模型超越任务特定技术的性能——有监督学习仍然占据主导地位。

在需要推理的任务（如数学、逻辑推理、常识推理等）中，我们发现更大的模型没有带来任何好处。事实上，Gopher 在这些任务上甚至被之前的 LLM（以及一些较小的 LLM）超越。相比于基于知识的任务，**推理密集型任务似乎从模型规模中获益更少**。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0c24c01-7fc7-4ddc-8937-ddd9cbed8d70_1590x586.png)

[2] 中的作者广泛研究了 Gopher 是否容易产生偏见或有害行为。有趣的是，结果表明，当模型接收到有害提示时，Gopher 往往会生成有害文本。此外，这种效应随着模型规模的增大而增加，较大的模型对有害提示的反应更具攻击性。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce98e69-e733-4cdf-80b9-455d46907467_1586x608.png)

Gopher 也对某些社会群体的少数群体存在偏见，如下图所示。然而，尽管有这些发现，作者强调当前评估 LLM 偏见和公平性的方法仍然有限。相对于现有社会规范，分析和改善 LLM 的行为是一个活跃且受关注的研究领域。

#### 1.4 Jurassic-1: Technical Details and Evaluation

除了增加像 GPT-3 这样的 LLM 的规模外，我们还可以考虑形状不同但规模相似的模型。例如，[3] 中的研究人员研究了一种名为 Jurassic-1 的 1780 亿参数的仅解码器 LLM。这个模型与 GPT-3 非常相似，但略大且层数较少（即 76 层而非 96 层）。为了弥补层数的减少，每层的宽度（即每个自注意力头的隐藏维度）增加，从而在参数数量上形成一个规模相似的模型。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe5a3b4e-cec4-4eec-8848-8434bb51be00_934x1016.png)

Jurassic-1 的修改架构遵循了之前研究 [6] 中关于 LLM 深度和宽度权衡的建议。该研究考察了不同深度的 LLM，并分析了模型深度和参数总数对性能的影响。有趣的是，分析显示，LLM 的最佳深度会随其规模变化。只有当模型足够大时，使用更深的 LLM 才有意义，并且可以根据参数总数准确预测最佳深度；详见上图。

[3] 中的作者在选择 Jurassic-1 的深度时遵循了 [6] 的经验预测；下面是该模型与 GPT-3 结构的比较。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F968486b4-2767-4555-8d2d-9bec9988b645_1234x316.png)

[3] 中的作者还探索了使用多词标记，并增加了基础分词器的词汇量。这一变化大大提高了标记效率，这意味着可以用更少的标记对给定的句子或文本进行编码。基本思路是，减少模型的输入标记数可以提高其效率——因为我们处理的是更短的输入序列！

此外，更好的标记效率意味着我们实际上可以在提示中提供更多的上下文示例！这是因为像 Jurassic-1 和 GPT-3 这样的模型有一个最大上下文长度，即输入中可以包含的最大标记数。如果标记效率更高，我们就能在相同的上下文长度中容纳更多数据。使用更多上下文示例的影响在下图中有所展示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F488fd4d9-51ef-4e60-a69b-02d43422aaf3_1392x490.png)

提高标记效率在文本生成任务中效果最显著，提升了23%，因为这需要逐个生成每个标记。训练和批量推理（即对一批示例进行一次前向传递）的速度也分别提高了1.5%和7%。

Jurassic-1 的训练过程与 GPT-3 非常相似。正如之前的研究所示，训练模型的优化器状态（即所有模型参数及其在优化过程中使用的相关统计数据）必须分布在多个 GPU 和计算节点上，因为模型的规模太大，无法集中存储这些状态。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff79b7d03-7a56-42de-bee2-9d344c59cd72_1390x536.png)

该模型使用[公开的测试套件](https://github.com/AI21Labs/lm-evaluation)进行评估，该套件随论文发布。大多数设置借鉴了 GPT-3 的评估方法，结果显示 Jurassic-1 在大多数情况下与 GPT-3 表现相似。然而，[3] 中的作者主要考虑了零样本评估，声称这种设置比少样本评估更简单且确定性更高。

总体而言，这项工作的主要价值在于对基础分词器的修改。训练一个浅但宽的模型似乎没有带来显著的好处。尽管使用更大的词汇表增加了模型的内存使用（因为必须存储更大的嵌入层的所有参数），但提高的标记效率非常有价值，因为它可以使用更多的上下文示例，并在多个方面提高大型语言模型的效率。


#### 1.5 Training Compute-Optimal LLMs

为了最大化大型语言模型（LLM）的性能，之前的缩放趋势分析[9]表明，我们应该尽可能扩大模型的规模（即非嵌入参数的数量），而基础预训练数据集的规模则应稍微少扩展（具体来说，按模型参数数量 \(N^{0.74}\) 的比例）。这种对大规模语言模型行为的分析启发了后续工作，如 GPT-3，在任务无关的少样本性能上取得了突破性进展。

由于 GPT-3 的巨大实用性，近期研究（如本文讨论的工作）探索了更大的 LLM（例如，MT-NLG 的参数量达到了 5300 亿！）。这些模型通常遵循[9]的建议：使用非常大的模型，但并不相应地大幅增加基础数据集的规模。

有趣的是，[4] 中的研究发现这种扩展 LLM 的方法并不理想。相反，为了以计算最优的方式训练 LLM（即在固定的计算成本下实现最大性能），[4] 发现 LLM 的规模和基础预训练语料库的规模应同时增加。简单来说，这意味着相比现有工作，我们应该在更多的数据上训练 LLM。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F736814c5-e37f-41c1-8b32-8a374e159238_1600x984.png)

直观上，这种方法是合理的。正如我们在 Gopher 的研究[2]中看到的，使用更大的预训练语料库以及更大的模型，与主要关注模型规模的 MT-NLG[1]相比，能带来更显著的性能提升。

为了更具体地说明这种扩展过程，[4] 考虑了不同大小的 LLM（即从 7000 万到 160 亿参数）$N$ 和用于训练的标记数量 $D$。需要注意的是，由于预训练数据量巨大，现代 LLM 通常训练少于 1 个周期（即每个例子只出现一次）。预训练期间观察到的标记数量等于数据集的大小。

通过使用不同的 $N$ 和 $D$ 组合训练 LLM，我们可以像[8]那样，尝试发现一个幂律关系，以预测 LLM 的测试损失作为 $N$ 和 $D$ 的函数。在[4]中，作者训练了超过 400 个 LLM，并进行了这样的分析。通过对这些模型的分析，我们可以找出在不同计算预算下，哪些 $N$ 和 $D$ 的组合效果最佳。


![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8014d6-0f01-4e1b-b13e-c73830bc30bf_1600x872.png)

有趣的是，这些实验表明，训练的最佳方法是使模型规模与训练标记数量保持相同的扩展比例。这与之前建议数据集应比模型规模少扩展的分析[9]相矛盾。然而，作者通过三种不同的分析方法验证了这些发现，这些方法通过不同技术研究扩展行为（见[4]的第 3.1-3.3 节）。所有这些研究都预测数据和模型规模应该同等扩展。

总体而言，这些发现告诉我们现代 LLM _(i)_ 过大，_(ii)_ 数据训练量不足。例如，[4]中的作者预测，与 Gopher 参数数量相同的模型需要用超过 20 倍的数据进行训练才能达到计算最优。因此，如果我们想正确训练 LLM，就需要更多的数据！

> “预计所需的训练数据量远超当前用于训练大模型的数据量，这强调了数据集收集的重要性，除了允许模型扩展的工程改进之外。” - 来自[4]

为了验证这些发现，作者训练了一个名为 Chinchilla 的 700 亿参数的 LLM。与之前的模型相比，Chinchilla 较小，但在预训练期间观察到更多的数据。数据集和评估策略与 Gopher 的研究[2]相同。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9327f694-f351-4f65-9e8b-a4496347e06f_1286x506.png)

在对 Chinchilla 的评估中，我们看到该模型尽管参数量少了四倍，但仍然超越了像 Gopher 这样更大的 LLM。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5d47478-eafe-4d6e-81b8-86d6b9447b68_1284x752.png)

该模型在大量任务上进行了评估，并与其他几种现代 LLM 进行了比较。结果显示，在所有情况下，它的表现都与其他最先进的 LLM 相当或更好。这揭示了模型规模可能没有我们最初认为的那么重要，预训练数据集的大小同样至关重要！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2526d02-a08d-4efb-bd40-2138a6901540_1600x976.png)

### 主要结论

随着 GPT-3 的提出，我们看到通过扩大 LLM 的规模可以获得很多好处。然而，在这篇综述中，我们探讨的问题是，模型规模是否是解决所有问题的答案。总体而言，我们了解到，仅仅扩大 LLM 的规模并不是提升任务无关性能的唯一必要因素。此外，这也带来了一些缺点。以下是主要结论的总结。

**更大的 LLM = 更多的工程努力。** 随着 LLM 规模的增大，处理它们变得越来越困难。我们已经从 GPT-3 中看到了这一点——训练一个拥有 1750 亿参数的模型需要结合多种分布式训练技术，是一项重大的工程壮举。对于更大的模型，如 MT-NLG，训练过程变得更加复杂。最近的努力已经降低了训练 LLM 的成本，但训练和部署这些模型所需的工程工作仍然很大。

**数据很重要。** 最初，扩大 LLM 的规模似乎是实现更好性能的首选方法。GPT-3 很出色，_为什么不让它更大呢_？然而，当我们看到随着模型变大，性能提升趋于平缓时，我们了解到在更多数据上进行训练也非常重要。LLM 性能的最大提升（例如 Gopher 和 Chinchilla [2, 4]）是通过模型和数据集规模的结合（大致成比例）实现的。

**深度还是宽度？** 这虽然不太显著，但当前研究[6]似乎表明，常用的 LLM 架构可能比实际需要的要深。在某些情况下，可能更有意义的是让它们稍微浅一些，并将节省下来的参数投入到每一层的宽度中。

**监督性能仍然占主导地位。** 尽管我们观察到了 LLM 令人难以置信的任务无关性能优势，但我们必须将这些结果放在背景中进行考量。从这篇综述中的研究中我们看到，这些技术仍然不及监督训练性能。在很多情况下，通过任务特定的微调，我们仍然可以获得显著的性能提升。虽然任务无关的基础模型是一种美好的愿景，但在不进行任何任务特定适应的情况下利用这些模型进行实际应用可能还需要一段时间。_如果微调一点能大大提高我们的性能，为什么不这样做呢_？
