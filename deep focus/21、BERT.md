

### 1、什么是 BERT？

Transformer 的双向编码器表示（BERT）是一种流行的深度学习模型，用于多种语言理解任务。BERT 与 Transformer 编码器具有相同的架构，并在大量未标记的文本数据上使用自监督学习目标进行广泛的预训练，然后微调以解决下游任务（如问答、句子分类、命名实体识别等）。在提出时，BERT 在 11 种不同的语言理解任务中取得了新的最先进成果，迅速成名并持续至今。

BERT 的卓越效果源于：

1. 通过自监督学习在大量原始文本数据上进行预训练
2. 为序列中的每个标记构建丰富的双向特征表示

尽管先前的研究表明，语言建模任务从大规模文本语料库的预训练中受益，BERT 通过设计一套简单而有效的自监督预训练任务扩展了这一理念，使相关特征得以学习。此外，BERT 摒弃了常用的单向自注意力机制，这种机制通常用于在语言理解任务中实现语言建模风格的预训练。相反，BERT 在其每一层中利用双向自注意力，揭示了双向预训练对于实现强大的语言表示至关重要。

**BERT 非常有用。** 你可能会想：_为什么要专门为这个模型写一整篇文章？_ 简单的答案是 BERT 非常通用——这种单一的模型架构可以用于解决许多不同的任务，并达到最先进的准确性，包括 token 级别（如命名实体识别）和句子级别（如情感分类）的语言理解任务。此外，其应用已扩展到 NLP 领域之外，用于解决多模态分类、语义分割等问题。

声称某个深度学习模型是“最有用的”有点夸张（尽管这很吸引眼球！）。然而，BERT 无疑是任何深度学习从业者的重要工具之一。简单来说，这种架构只需进行最少的任务特定修改，就可以下载并以低计算成本微调，以解决 NLP 及其他领域的众多潜在问题—— _它是深度学习的 “瑞士军刀 ”_ ！

### 2、BERT 的构建模块

在概述 BERT 架构的具体细节之前，了解 BERT 所基于的核心组件和理念是很重要的。这些主要概念可以归纳为以下几点：

- （双向）自注意力机制
- Transformer 编码器
- 自监督学习

#### 2.1、自注意力

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7f42e9e5-422b-486b-b31d-0f40323ccd74_1031x369.png)

从高层次来看，自注意力是一种非线性变换，它将 “token” 序列（即序列中的单个元素）作为输入，每个标记用向量表示。与此输入序列相关的矩阵如上所示。然后，这些标记表示被转换，返回一个新的标记表示矩阵。

**在这个转换中发生了什么？** 对于每个单独的标记向量，自注意力执行以下操作：

- 将该标记与序列中的每个其他标记进行比较
- 计算每对的注意力得分
- 根据序列中其他标记的注意力得分，调整当前标记的表示

直观地说，自注意力只是根据序列中的其他标记调整每个标记的向量表示，形成一个更具上下文意识的表示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb41d46e-ebea-46c8-9005-472d2983a35b_1164x848.png)

**多头注意力**    自注意力通常以多头的方式实现，即在并行应用多个自注意力模块后，将其输出连接在一起。每个注意力头内部的自注意力机制仍然相同，不过在应用自注意力之前，标记向量会被线性投影到较低维度（以避免计算成本过高）。

这种多头方法的好处在于，多头注意力层中的每个注意力头可以学习序列中不同的注意力模式。因此，模型不会因为单个自注意力层中可以“关注”的其他标记数量而受到限制。相反，不同的头可以捕捉到标记关系的不同模式。

**单向与双向**    在为序列中的每个标记创建上下文感知的表示时，有两种基本选项来定义这个上下文：

- 考虑所有标记
- 考虑当前标记左侧的所有标记

这两种选项，如下图所示，产生了自注意力的两种不同变体：双向和单向。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8c58829-dd45-4b9e-87c3-4cb541e9d5fc_1444x604.png)

单向自注意力确保每个标记的表示仅依赖于序列中它之前的标记（即，通过在自注意力中“屏蔽”序列中后来的标记）。这种修改对于语言建模等应用是必要的，因为不应允许模型“向前看”以预测下一个词。

相反，双向自注意力基于序列中的所有其他标记来构建每个标记的表示。双向自注意力是 BERT 成功的关键，因为许多之前的建模方法：

1. 使用了单向自注意力
2. 通过连接句子在前向和后向的单向表示来构建浅层的双向特征

这些方法远不如 BERT 使用双向自注意力有效，这强调了双向特征表示在超越语言建模的任务中的优势。

#### 2.2 Transformer 编码器

Transformer 架构通常有两个组件——编码器和解码器。然而，BERT 仅使用了 Transformer 的编码器组件。

![|150](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9ec3e8bd-88f4-43fd-a5c4-14c120dd31ee_330x384.png)

可以看到，Transformer 编码器只是几个重复的层，每层包含（双向、多头）自注意力和前馈变换，每个操作后跟随有层归一化和残差连接。非常简单！

**为什么只用编码器？** Transformer 架构的两个组件往往有不同的用途。

- 编码器： 利用双向自注意力将原始输入序列编码为一系列可区分的标记特征。
- 解码器： 接收丰富的编码表示，并将其解码为新的、期望的序列（例如，将原始序列翻译成另一种语言）。

在 BERT 的情况下，我们将在本文的其余部分看到，这种解码过程并不是必需的。BERT 的目的是简单地构建初始的编码表示，然后可以用于解决不同的下游任务。

#### 2.3 自监督学习

![|380](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfe41ce8-3962-4b32-b119-b8302101337d_878x860.png)

BERT 卓越性能的关键之一在于其能够以自监督方式进行预训练。总体而言，这种训练非常有价值，因为可以在原始、未标注的文本上进行。由于此类数据在网上广泛可得（例如，通过在线书籍库或像维基百科这样的网站），可以收集大量的文本数据集进行预训练，使 BERT 能够从比大多数监督/标注数据集大得多的多样化数据集中学习。

虽然存在许多自监督训练目标的例子，但一些例子（我们将在本文中进一步概述）包括：

- 掩码语言模型 (MLM)：在句子中掩盖/移除某些词并尝试预测它们。
- 下一句预测 (NSP)：给定一对句子，预测这些句子在文本语料库中是否相互衔接。

这些任务都不需要人工标注，而是可以用未标注的文本数据来完成。

**这是无监督学习吗？** 这里值得区分的一点是自监督学习和无监督学习的区别。无监督和自监督学习都不利用标注数据。然而，无监督学习专注于发现和利用数据本身的潜在模式，而自监督学习则在数据中找到某种已存在的监督训练信号并利用其进行训练，因此不需要人工干预。

### 3、BERT 的实际工作原理…

虽然我们已经概述了一些 BERT 背后的基本思想，但在本节中，我将更详细地描述 BERT，重点介绍其架构和训练方案。

#### 3.1 BERT 的架构

如前所述，BERT 的架构只是 Transformer 模型的编码器部分（即，仅编码器的 Transformer 架构）。在最初的发布中，提出了两种不同大小的 BERT：

- BERT Base：12 层，768 维隐藏表示，每个自注意力模块中有 12 个注意力头，共 110M 参数。
- BERT Large：24 层，1024 维隐藏表示，每个自注意力模块中有 16 个注意力头，共 340M 参数。

值得注意的是，BERT Base 的大小与 OpenAI 的 GPT 相同，这使得模型之间的比较更加公平。

**这有什么不同？** BERT 与先前提出的语言理解模型（例如 OpenAI GPT）的主要区别在于使用了双向自注意力。尽管先前的工作利用了自监督预训练，但仅使用了单向自注意力，这极大地限制了模型能够学习的标记表示的质量。

**构建输入序列**    直观地说，BERT 将一些文本序列作为输入。特别是，这个序列通常是单个句子或一对连续的句子。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3eaa245c-7a13-4872-90f4-6c748b682f32_1549x792.png)

这个高层次的概念很简单，但你可能会想：如何从原始文本生成 BERT 兼容的输入序列？这个过程可以分为几个步骤：

- 分词：将原始文本数据分解为表示单词或词的一部分的单个标记或元素。
- 插入“特殊”标记：BERT 的输入序列以 `[CLS]` 标记开始，以 `[SEP]` 标记结束，表示句子的开始/结束。如果使用两个连续的句子，则在它们之间放置另一个 `[SEP]` 标记。
- 嵌入：将每个标记转换为其对应的 WordPiece 嵌入向量。
- 加性嵌入：输入数据现在是一个向量序列。可学习的嵌入被添加到序列中的每个元素，表示元素在序列中的位置以及它是第一句还是第二句的一部分。因为自注意力机制无法区分元素在序列中的位置，所以需要这些信息。

通过这些步骤，原始文本数据被转换为 BERT 可以处理的向量序列。分词、插入特殊标记和嵌入过程在上图中展示，而加性嵌入过程在下图中展示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F30b03109-5bcd-4624-8c4e-55806dfd00d9_2138x688.png)


#### 3.2 训练 BERT

BERT 的训练过程分为两个步骤：

1. 预训练
2. 微调

这两个步骤的架构几乎相同，尽管可能会使用一些小的、特定任务的模块（例如，MLM 和 NSP 都使用一个额外的分类层）。

**预训练**    在预训练期间，BERT 模型通过两种不同的任务在未标注数据上进行训练：MLM（也称为 Cloze 任务）和 NSP。值得注意的是，BERT 不能使用以往工作中常用的语言建模目标进行训练，其中模型迭代地尝试预测序列中的下一个词。使用双向自注意力会使 BERT 通过简单地观察和复制下一个标记来作弊。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F984d381d-383b-491b-9422-adcc8d45b7d0_2050x1016.png)

如上图所示，NSP 任务相当简单。来自预训练语料库的连续句子（即句子 A 和 B）被输入到 BERT 中，其中 50% 的情况下，第二个句子会被替换为另一个随机句子。然后，经过 BERT 处理后的 `[CLS]` 标记的最终表示被传递到一个分类模块中，该模块预测输入的句子是否实际匹配。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe697a9b-6a11-489b-9701-15d42d3feb15_2006x1206.png)

如上所示，MLM 不是像 NSP 那样的序列级任务。它通过将输入序列中 15% 的标记随机替换为特殊的 `[MASK]` 标记来进行掩码。然后，这些 `[MASK]` 标记的最终表示被传递到一个分类层，以预测被掩盖的词。然而，作者并不是总是以这种方式掩盖标记，而是 80% 的情况下用 `[MASK]` 替换，10% 的情况下用随机标记替换，10% 的情况下保持原始标记不变。这样的修改是为了避免 `[MASK]` 标记在预训练中存在但在微调中不存在的问题。

通过这些任务，BERT 在由 BooksCorpus 和英文维基百科组成的语料库上进行预训练。有趣的是，使用文档级语料库（而不是打乱的句子语料库）对预训练质量至关重要。你的语料库需要在句子之间具有长距离依赖，以便 BERT 学习最佳特征。后来的研究也证实了这一有趣的发现。事实上，即使是基于 TF-IDF 分数对随机打乱的句子重新排序以形成合成的长期依赖，也能提高预训练质量。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3c76554-5ade-4210-b612-98f2f5062abf_1224x690.png)

**微调**    BERT 的自注意力机制设计得非常简单，以便能够轻松地建模不同类型的下游任务。在大多数情况下，只需将任务的输入输出结构与 BERT 的输入输出结构匹配，然后对所有模型参数进行微调。以下是一些示例：

- 标记级任务：正常处理序列，然后将每个标记的输出表示通过一个单独的模块来预测给定标记。
- 句子/文档级任务：正常处理序列，然后将 `[CLS]` 标记的输出表示（输入序列的聚合嵌入）通过一个单独的模块来进行序列级预测。
- 文本对任务：在 BERT 的输入结构中将文本对的每一部分编码为“句子 A”和“句子 B”，然后将 `[CLS]` 标记的输出表示通过一个单独的模块来基于文本对进行预测。

上述一般任务结构表明 BERT 是一个多功能的模型。许多不同的任务可以通过简单地将它们映射到 BERT 的输入输出结构来解决，相对于预训练所需的架构修改最小。请参见下方 BERT 可解决的不同语言理解任务的示例。

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed88333-36d9-4790-8fa4-62d1cc5d2ae8_500x533.png)

在微调过程中，所有 BERT 参数都是端到端训练的。与预训练相比，BERT 的微调过程成本较低。事实上，论文中的所有结果在单个 GPU 上复现都不到 2 小时。如果不相信，可以自己试试！

[在 GLUE 上微调 BERT](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)

### 4、BERT 可能是自切片面包以来最好的东西！

在结束这个概述之前，我想概述一下 BERT 所取得的一些实证结果。虽然可以轻松阅读论文来查看结果，但我认为值得简要介绍一下，原因是——_强调 BERT 在 NLP 任务中的出色表现_。BERT 在各种不同任务上取得的结果如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F58bcb60d-4e58-4350-a4df-62b319a230a3_1607x1088.png)

你可能会注意到 BERT 在这些实验中的表现很有趣——它从未被超越（除非是人类，但也仅在某些情况下）。在发表时，BERT 在**十一项不同的 NLP 基准测试**中设立了新的记录。此外，这些任务大多以前由特定任务的专用模型解决，而 BERT（如你在概述中所见）是一个通用的语言理解模型，可以应用于许多不同的任务。

BERT 实证评估中的其他一些有趣发现如下：

- BERT Large 和 BERT Base 在所有考虑的任务中都显著优于之前的所有方法。
- BERT Large 在所有任务中显著优于 BERT Base，尤其是在训练数据较少的任务中表现出色。
- 移除 NSP 或 MLM（即使用单向语言建模目标）会显著降低 BERT 的性能。

尽管较大的模型在小数据集上表现更好似乎违反直觉（这似乎是过拟合的配方），但 BERT 的结果表明，使用较大的模型对低资源任务（即训练数据较少的任务）是有益的，只要有足够的预训练。

### 5、要点

尽管 BERT 在当前深度学习研究的快速发展中相对较旧，但我希望这个概述能恰当地强调模型的简单性和深刻性。BERT 是一个非常强大的工具，使用简单且成本低廉。

**是什么让 BERT 如此强大？** BERT 的关键在于两个核心概念：双向自注意力和自监督学习。BERT 改进了先前的方法，部分原因是它摒弃了使用单向自注意力进行语言建模式预训练的常见方法。相反，BERT 利用双向自注意力来制定一组自监督的预训练任务，从而产生更强大的特征表示。最近，研究人员表明，这些自监督任务的制定本身（而不仅仅是用于预训练的大量数据）是 BERT 成功的关键。

**普通从业者可以使用吗？** 使用 BERT，你可以简单地：

1. [下载](https://huggingface.co/transformers/v3.3.1/pretrained_models.html)一个预训练模型
2. [微调](https://huggingface.co/docs/transformers/training)这个模型，以在大量 NLP 任务中实现最先进的性能

微调 BERT 的计算成本低，可以在相对简单的硬件配置（例如单个 GPU）上进行。因此，BERT 是任何深度学习从业者工具库中的一个非常好的工具——你会惊讶于 BERT 是你在许多不同任务中的最佳选择。

**进一步阅读**    我在这个概述中只涵盖了一篇论文，但 BERT 已被无数后续出版物扩展。以下是我最喜欢的一些：

1. ELECTRA 提出了一种新的预训练任务，使得高性能的小型 BERT 模型的训练成为可能
2. ALBERT 提出参数减少技术，使 BERT 预训练更快且内存占用更少
3. Vilbert 是 BERT 的一个推广，用于联合视觉和语言任务）

**个人笔记**    BERT 是第一个让我对深度学习产生兴趣的模型。尽管我目前的研究更专注于计算机视觉（或多模态学习，BERT 在这方面仍然表现很好！），但 BERT 的多功能性至今仍让我印象深刻。_简单而有效的想法是稀有而美丽的_。