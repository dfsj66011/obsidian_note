
> https://cameronrwolfe.substack.com/p/automatic-prompt-optimization


无需人工努力即可提高 prompt 质量的实用技术…


> _“通过依赖自然语言指令，大型语言模型（LLMs）展现出了作为通用计算机的惊人能力。然而，任务的表现很大程度上取决于用于引导模型的提示质量，大多数有效的提示都是由人类精心设计的。”

许多模型对提示的细微变化过于敏感。调整提示的措辞或结构可能导致错误和意外的结果。因此，撰写有效的提示需要一定的领域专业知识，并已成为一种热门技能。大多数提示都是通过人工反复试验的过程创建的。我们不断调整和测试提示，利用我们对提示技术和被提示的 LLM 的了解，指导我们找到表现良好的提示。

**自动改进提示** 提示需要大量的人力，并且本质上是不完美的。为了解决这些问题，最近的研究探索了自动提示优化的想法，通过算法利用数据来提高提示的质量。这种方法有几个关键好处：

1. 需要更少的人工努力来找到好的提示。
2. 提示的搜索和发现是系统化的，能够找到超越人类编写的提示的表现。

提示优化技术允许我们自动提高提示的质量，而不是依赖启发式和领域知识。在本概述中，我们将学习关于这一主题的广泛文献，重点介绍创建更好提示的最实用技术。

### 1、基础知识：提示和优化

#### 1.1 什么是优化？

优化通常意味着找到使函数值最小化（或最大化）的点。通过找到这个最优点，我们就在“优化”这个函数，或者进行优化。

**优化算法的类型**    已经提出了无数的优化算法。即使是专门用于训练神经网络的应用，也有[大量的优化算法](https://www.ruder.io/optimizing-gradient-descent/)可供选择。尽管种类繁多，优化算法可以大致分为两类：

1. 基于梯度的
2. 无梯度的

**基于梯度的优化**反复进行以下步骤：计算函数的梯度，并使用该梯度更新当前解。

**无梯度算法**是一类不使用任何梯度信息的优化算法；例如，暴力搜索和爬山算法是两个基本例子。虽然存在许多无梯度优化算法，但其中最流行的一类算法是进化算法（EA）：

![|400](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8daaba2-727e-4d6f-971b-40373d9c7bce_942x797.png)

进化算法（EAs）维持一个“种群”候选解，并反复进行以下步骤：

- 通过变异和交叉等进化操作修改种群成员（*候选解*），以产生新的种群成员，模拟繁殖行为。
- 根据 *某个目标函数* 选择最佳成员，使其继续进化（即优胜劣汰）。

尽管存在许多进化算法的变体，但最常见的实例是遗传算法和差分进化。通常，进化算法被认为比基于梯度的优化算法效率低。例如，在没有梯度信息的情况下训练大型神经网络（如大型语言模型）是困难的。搜索空间非常大，仅仅调整模型参数、测量损失并期望找到更好的模型效果有限。

> *“进化算法通常从初始种群的 $N$ 个解开始，然后通过对当前种群使用进化操作（如变异和交叉）迭代生成新解，并根据适应度函数更新种群。”* 

然而，进化算法也有许多优点；例如，这些算法在探索和开发之间的权衡上表现良好。基于梯度的算法产生单一解，而进化算法保持整个种群！这一特性对某些类型的问题非常有用，促使进化算法在许多有趣领域得到实际应用，如进化神经网络架构或优化计算机网络拓扑。

**使用 LLMs 作为优化器**    最近，研究人员探索了将 LLMs 用作无梯度优化器的想法。为此，我们向 LLM 提供有关当前问题解的信息，包括解的性能和优化轨迹（即先前解的历史）。然后，我们可以提示 LLM 生成一个新的、希望更好的解。通过测量新解的性能并重复这一过程，我们创建了一种新的优化算法。

#### 1.2 提示工程基础

不幸的是，LLMs 对提示中的细微变化很敏感——我们如何措辞和构建提示的细节会产生很大影响。这一问题可能会使非专家在提示 LLM 时感到困难。因此，提示工程已成为一种合理、流行且有用的技能。

**提示技术**    在之前的概述中，我们深入研究了不同类别的提示技术：

- 实用提示工程：基本概念、零/少样本提示、指令等。
- 高级提示工程：思维链（CoT）提示、CoT 提示的变体、上下文检索等。
- 提示工程的现代进展：最近的提示研究（例如，工具使用、推理、程序辅助提示、长篇写作等）。

除了上述概述，还有无数在线资源可用于深入学习提示工程；例如，[学习提示](https://learnprompting.org/docs/introduction)、[提示指南（来自 The Gradient）](https://thegradient.pub/prompting/)、提示工程综述等。

**编写更好提示的框架**    提示工程是一门主要基于试错的经验科学。为了编写更好的提示，我们应不断地 _i)_ 调整原始提示，_ii)_ 测量新提示的性能，_iii)_ 选择更好的提示。我们还要确保提示不至于过于复杂。原因如下：

- 如果复杂的提示表现不佳，我们如何知道提示的哪个部分出了问题，需要修复什么？
- 复杂的提示通常更长，会消耗更多的 token，从而增加成本。

因此，我们应以简单的提示（例如，少样本或基于指令的提示）开始提示工程过程。然后，我们（缓慢地）增加提示的复杂性——通过添加少样本示例、使用高级技术如 CoT 或仅调整指令——同时监测性能随时间的变化。通过遵循此过程，我们可以改进提示，并通过提高性能来证明复杂性的增加是合理的。一旦提示达到我们用例的可接受性能水平，过程即结束。

**提示工程就是优化**    如果我们仔细审视上述提示工程过程的步骤，会发现这其实是一个优化问题！我们反复调整解决方案——即我们的提示——并分析新方案是否更好。在这个设置中，“优化器”是一个人类提示工程师，他们利用自己的判断力和提示知识来确定下一个最佳提示。然而，由于提示工程需要大量的试错，我们可能会想是否可以用自动化方法或算法来替代人类。

**为什么优化提示很困难**   正如我们所了解的，有两类优化算法可用于自动化提示工程过程。不幸的是，使用基于梯度的算法进行提示优化存在几个困难：

1. 对于许多 LLM，我们只能通过 API 访问，这限制了我们获取/使用任何梯度信息。
2. 提示由离散的标记组成，使用基于梯度的算法来优化离散解是复杂的。

由于上述原因，我们主要依赖于无梯度算法进行提示优化。事实上，进化算法（EAs）实际上是离散优化问题中最成功和广泛使用的算法之一！


### 2、早期的提示优化研究

在本节中，我们将研究一些早期的提示优化工作，这些工作应用于仅编码器和 GPT 风格的语言模型。

#### 2.1 生成合成提示

如上所述，由于提示中的标记是离散的，直接优化提示很困难。我们可以通过训练一个 LLM 来生成提示作为输出来解决这个问题。通过这种方式，我们可以训练该 LLM 的权重，而不是直接优化提示。这是一种实用且常见的技术。然而，大多数研究使用的是预训练的 LLM 来编写提示，并没有明确优化 LLM 以编写更好的提示。

**指令诱导**是最早探索使用预训练 LLM 生成提示的工作之一。这种技术旨在根据几个具体的（上下文中的）任务示例来预测潜在任务。我们设计一个提示，其中包含多个输入-输出示例，并要求 LLM 通过完成 “the instruction was…” 的序列来推断正在解决的任务。

LLM 不能直接解决此任务，例如，预训练的 GPT-3 模型仅能达到 10% 的准确率。然而，较大的模型在经过指令对齐后开始在此任务上表现良好。InstructGPT 模型——一种早期基于 GPT-3 的 LLM 变体，通过 RLHF 对齐以遵循人类指令——在指令诱导任务中达到了近 70% 的准确率。

**Self-Instruct** 是最早提出使用 LLM 生成合成指令微调数据集的框架之一。该框架从一小组种子任务开始，提示 LLM（使用种子任务作为输入）生成可以解决的新任务。之后，我们可以使用同一个 LLM 为每个任务创建具体的示例，并根据质量进行筛选。这个流程的结果是一个由 LLM 生成的指令微调数据集，其中包含大量不同任务的输入-输出示例。

**WizardLM** 设计了一种自我指令风格的框架，专注于创建高度复杂的指令微调数据集。该方法的核心是名为 EvolInstruct 的技术，它使用 LLM 迭代地重写或“进化”指令以增加其复杂性。我们可以通过两种基本方式进化任何给定的指令：

1. *深入发展*：通过增加约束、需要更多推理步骤、复杂化输入等方式使当前指令更复杂（即保持相同指令并使其更复杂）。
2. *广泛发展*：增强指令微调数据集的主题覆盖、技能覆盖和整体多样性（即为数据中尚未覆盖的主题生成指令）。

这两种进化类型都有许多变体。在 [9] 中提供了深入和广泛进化的示例提示，不过在 EvolInstruct 的实现中还使用了许多其他提示。

根据这些进化提示的策略，我们可以采用三个步骤的方法：_i)_ 进化指令，_ii)_ 生成指令响应，_iii)_ 删除或淘汰不可行或未能变得更复杂的提示。通过遵循这些步骤，我们可以引导自我指令框架生成一个包含高度复杂指令的合成数据集：

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447dac7-a51e-4c8d-b12b-866ec00a158f_1062x1030.png)



#### 2.2 Soft Prompts: 和 and Prompt Tuning

如果我们想改进一个提示，可以从简单地调整提示中的措辞或指令入手。这种方法称为“硬”提示微调，因为我们明确地以离散方式更改提示中的词（或标记）。然而，硬提示微调并不是唯一可用的工具——我们可以探索对提示进行软性、连续的更新。

> “与提示不同，前缀完全由不对应于真实标记的自由参数组成。” 

**前缀微调** 是最早探索对 LLM 提示进行连续更新的论文之一。传统的模型微调是在下游数据集上训练模型的所有参数，而前缀微调则几乎固定模型的所有参数。相反，我们在模型输入的开头附加一个“前缀”——即一系列额外的标记向量，并直接训练这个前缀的内容。然而，前缀微调不仅仅是更新输入提示，而是在基础模型的每个 Transformer 块的输入中添加一个可学习的前缀：

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff89a3ad2-941c-4bb7-84bb-e857db672440_2120x1268.png)

在前缀微调中，我们仅训练模型中添加的前缀。与直接训练前缀内容不同，文献 [4] 的作者发现，在与每层输入连接之前，通过前馈变换传递可学习的前缀可以使训练更加稳定。这种技术被称为“重参数化”，它为模型添加了更多可学习的参数。

在训练期间，除前缀外的所有参数都保持不变，这大幅减少了微调过程中需要学习的参数总数，减少了 100 倍或更多。然而，通过前缀微调，我们可以显著提高下游任务的性能；

尽管前缀微调在上述实验中表现良好，但文献 [4] 中用于微调实验的唯一基础模型是 GPT-2，这意味着这些结果尚需在更现代的模型和数据集上验证。

**提示微调** 是前缀微调的简化版本，并且是同时提出的。我们同样从一个预训练的语言模型开始，并冻结模型的参数。不训练模型，而是创建一个“软”提示，即一系列连接到大型语言模型输入开头的标记序列。然后，通过梯度下降在数据集上训练这段软提示，类似于训练其他模型参数

![|350](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b032ac4-9b72-4020-bd7c-960a1c430093_1056x1192.png)

与前缀微调相比，提示微调仅在输入层前置前缀标记，而不是在每一层前置，并且不需要重新参数化来保持训练的稳定性。因此，提示微调中学习的参数总数比前缀微调要少得多。

> “我们……展示了仅靠提示微调（没有中间层前缀或任务特定的输出层）就足以与模型微调媲美。” 

尽管可学习参数数量很少，提示微调依然表现出惊人的效果，随着底层大型语言模型变大，其性能甚至赶上了端到端模型训练。此外，提示微调的性能通常超过人工设计的提示——尽管我们应注意，人类的表现依赖于撰写提示的人。

**软提示真的算是提示吗**    软提示在性能上非常有效，但有人可能会认为这些技术并没有真正改善提示工程过程。提示工程的目标是找到一个在特定应用领域和考虑的语言模型下表现良好的基于文本的提示。提示和前缀微调没有解决这个问题，而是消除了提示必须是离散/基于文本的限制。这些技术发现的“提示”实际上是通过基于梯度的优化学习的连续向量。与其他模型参数类似，这个向量是不可解释的，也不能转移到其他语言模型中使用。

> “随着规模的扩大，这变得不太实用，因为计算梯度变得昂贵，并且对模型的访问转向可能不提供梯度访问的 API。” 

因此，前缀和提示微调更接近于参数高效微调（PEFT）技术，而不是自动提示优化策略。我们不是寻找更好的提示，而是向模型添加少量额外参数，并在一个较小的、特定领域的数据集上直接训练这些参数——同时保持预训练模型不变。这些额外参数只是恰好被连接到我们的提示，而不是注入到模型的内部权重矩阵中。

**基于 API 的访问**    另一个关于提示和前缀微调的问题是，当我们只能通过 API 访问大型语言模型时，这些技术无法使用。进行任何形式的基于梯度的训练都需要完全访问模型的权重，而 LLM 的 API 仅提供对模型输出的访问——这些 LLM 实际上是一个黑箱！因此，提示和前缀微调仅与开源 LLM 兼容，但研究人员已经尝试为此问题创建解决方案。

**InstructZero** 在软提示和 LLM API 之间插入另一个 LLM。这个（开源的）LLM 可以被训练成在给定软提示和附加任务信息的情况下生成基于文本的指令。然后，生成的指令可以正常传递给 LLM API。简单来说，我们使用额外的 LLM 在传递给 API 之前将软提示“解码”成文本提示。为了提高生成指令的质量，我们可以使用贝叶斯优化，生成的指令能够匹配或超过通过其他自动提示方法发现的提示的性能。

**更多关于软提示的工作** 前缀和提示微调是最广为人知的软提示技术，但还有许多其他论文考虑了这一想法。以下是该领域其他值得注意的论文示例：

- Mixtures of Soft Prompts：通过微调提供给语言模型的整个提示——无论是从现有提示还是随机初始化——形成软提示，然后可以与其他软提示混合或组合以实现更好的任务性能。
- WARP：通过学习特定任务的词嵌入来解决语言模型适应多个下游任务的问题，这些词嵌入可以连接到模型的输入以解决不同的任务。
- P-Tuning：通过将一系列可训练的嵌入与模型的提示连接起来解决提示工程的不稳定性，模型可以与添加的嵌入一起微调或保持冻结。
- PADA：通过训练语言模型首先预测解决问题的特定领域提示，然后使用这个生成的提示来实际解决问题，提高了LLM适应新领域的能力。

关于软提示的文章也有很多。[Sebastian Rashka](https://sebastianraschka.com/) 写了关于[提示和前缀微调](https://magazine.sebastianraschka.com/p/understanding-parameter-efficient)的详细解释，而 [Lilian Weng](https://lilianweng.github.io/) 在她的[文本生成](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design)和[提示工程](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#automatic-prompt-design)文章中探讨了这个话题。

#### 2.3 离散提示优化

软提示在可解释性、跨模型的可重用性和对基于 API 的 LLM 的适用性方面存在局限性。不幸的是，这些限制破坏了提示的一些关键优势。考虑到这一点，我们可能会想，是否可以在保持这些特性的一些前提下自动优化提示。在提示的早期阶段，有论文提出寻找离散的“触发”词，可以将其包含在模型的提示中以帮助解决某些问题。

> “编写提示不仅耗时，而且不清楚相同的措辞是否对每个模型都有效，也不清楚哪些标准决定某个特定措辞是否是引出所需信息的最佳方式。” 

**AutoPrompt**   通过对一组离散的词进行梯度引导搜索，以发现一组最佳的附加词来包含在 LLM 的提示中。这种方法应用于探查 LLM 内部知识的任务，我们看到这些附加词在该任务上带来了更可靠和稳定的性能。通过使用触发词，我们可以通过严格的（基于梯度的）搜索程序优化 LLM 的性能，而不是通过手动反复试验来启发式地寻找最佳提示。

由于使用了梯度信息，[11] 中指出 AutoPrompt 用于编辑提示词的训练过程可能不稳定。作为替代方案，后续研究探索了通过 RL 来优化提示。使用RL优化提示的想法非常简单。首先，我们定义一个策略网络，这就是一个生成候选提示作为输出的 LLM。然后，我们可以将这个策略网络的奖励定义为其生成提示的性能！

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c9b20a6-8f19-4607-a9e0-50891f91e1ad_2454x1436.png)

这种设置与通过 RLHF 训练 LLM 非常相似。在这两种情况下，我们的策略都是一个 LLM，该策略通过生成词元来执行动作。一旦生成完整序列，我们就计算该序列的奖励。在 RLHF 中，这个奖励是奖励模型的输出，作为人类偏好的代理。对于提示优化，这个奖励由提示的性能决定

**RLPrompt**  是使用 RL 优化离散提示的最著名的论文之一。生成提示的策略网络实现为一个预训练的 LLM。该模型的参数保持冻结状态，但我们在模型中添加了一个通过 RL 训练的前馈层；

这个策略网络用于生成候选提示，供另一个 LLM 使用以解决任务——策略网络和接收提示的 LLM 不必相同。奖励通过测量使用给定提示所达到的性能（例如，分类准确性或输出对给定风格的遵循程度）来获得。在这里，奖励函数依赖于LLM生成的输出。因此， RLPromp t框架中的奖励信号是不可预测的、复杂的，并且难以优化。尽管存在这一问题，[12] 中的作者表明可以使用各种奖励稳定技术来使优化过程更可靠。

当应用于分类和风格转换任务时，RLPrompt 生成的提示比微调和提示调整策略表现更佳；然而，我们希望自动优化能被人类轻松理解的提示，这一点尚未实现。RLPrompt 发现的提示尽管在其他 LLM 中表现良好，往往是不合语法甚至是“胡言乱语”。根据我们的观点，这一发现可能让我们思考提示是否在某种程度上是一种独特的语言。

> “优化后得到的提示通常是不合语法的胡言乱语；令人惊讶的是，这些胡言乱语的提示在不同的语言模型之间具有可转移性，保留了显著的性能，表明语言模型的提示可能不遵循人类语言模式。” 

**TEMPERA** ——受实例依赖提示优化（即为每个输入动态优化提示）的启发——采用不同的方法，在测试/推理时使用 RL 对提示进行调整。给定初始提示，我们可以通过以下方式编辑此提示的属性：_i)_ 修改指令，_ii)_ 添加或删除少量示例，以及 _iii)_ 修改词汇化器。我们可以通过定义奖励为给定编辑前后性能的差异，使用 RL 训练一个代理来执行这些编辑；

TEMPERA 学习一个函数，该函数将原始提示、LLM 的当前输入查询、一组少样本示例和一个词汇化器池作为输入，然后通过一系列编辑生成最终提示，例如交换示例或编辑词汇化器。从 RL 的角度看，状态由当前提示给出，我们可以通过编辑提示来执行动作。为了表示状态，我们可以直接使用当前提示的文本。然而，[13] 中的作者选择通过预训练的 LLM 嵌入这个文本。通过这样做，我们可以实现一个简单的策略网络，该网络：

1. 以状态/提示嵌入为输入。
2. 生成要执行的动作/编辑作为输出。

TEMPERA 考虑了一组固定的可对提示执行的编辑，因此这个动作空间是离散的！我们可以直接用策略网络预测要执行的提示编辑。通过将奖励定义为给定编辑带来的性能提升，我们可以使用现成的RL算法（如 PPO）优化整个系统。事实证明，这种方法数据效率高，对分类有用，甚至对解决更困难的 LLM 任务有影响。

**为什么这样有效**   通过使用 RL，我们可以最终实现优化离散提示的训练过程。与我们迄今为止看到的其他方法相比，像 RLPrompt 这样的技术进行了两个关键改变，使这成为可能：

1. 使用 LLM 生成提示。
2. 根据生成的提示质量优化该 LLM。

通过使用 LLM 生成提示，我们可以专注于优化 LLM 的（连续）权重，而不是离散提示。这种方法也被 Instruction Induction 和 Self-Instruct 等技术使用，但我们通过使用 RL 来教模型创建更好的提示，超越了这些技术。我们需要 RL 来执行此优化，因为用于生成奖励的系统基于 LLM，因此不可微分！换句话说，我们无法轻松计算用于优化的梯度。

### 3、自动优化 LLM 的提示

现在我们已经了解了早期关于优化提示的工作，接下来我们将学习最近流行的关于自动提示优化的论文。几乎所有这些论文都依赖于 LLM 优化提示的能力。使用 LLM 作为无梯度优化器（可以说）比传统和成熟的优化算法更不严谨。然而，这种方法在概念上简单，易于实施和应用，并且非常有效，这使得基于 LLM 的提示优化算法变得非常流行。

#### 3.1 大型语言模型是具有人类水平的提示工程师

撰写有效的提示需要大量的试验和错误。提示工程是一个黑箱优化问题——我们事先不知道某个提示与任何给定模型的兼容性。在 [1] 中，我们看到首次尝试利用 LLM 提高人工提示工程效率的努力。

> “我们将这个问题称为自然语言程序合成，并提出将其作为一个黑箱优化问题来解决，使用 LLM 生成和搜索启发式可行的候选解决方案。” 

所提出的方法称为自动提示工程师（APE），它在 LLM 提出的提示池中搜索，找到表现最佳的提示。这个设置使用不同的 LLM 来提出和评估提示。为了评估，我们通过零样本推理生成输出，并根据选定的评分函数评估输出。尽管其简单，APE 在 [1] 中显示能够找到与人类编写的提示性能相当或更优的提示，揭示了 LLM 在编写提示任务上实际上表现出色。

**这是如何工作的**    APE 有两个主要操作：**提议**和**评分**。对于提议，我们让 LLM 为某个任务生成新的候选提示；上面提供了一组示例提示。新指令的生成可以采用正向或反向方式。在正向模式中，我们通过下一个词预测生成新指令，这与使用 LLM 生成任何其他类型文本相同。反向生成则基于填充（即在序列中间插入缺失的文本/词元），这对于仅解码器的 LLM 是不可能的。根据任务的不同，我们可能会调整指令生成模板的措辞。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F569eb694-e78d-4614-b27e-a59f83ba04e7_2328x538.png)

在 [1] 中我们看到，LLM 可以从先前的指令中推断出更好的指令。为了确定生成指令的质量，我们可以使用另一个 LLM 以零样本的方式进行评估，可以使用输出准确性或更柔和的指标（例如，正确输出的对数概率）。每个生成提示的评估都在整个提示优化过程中使用的固定训练集上进行。与微调相比，通过 APE 进行提示优化所需的训练示例要少得多。在优化完成后，我们通常在一个保留的测试集上评估最终提示。

为了实现最佳性能，在 [1] 中我们看到应该让 LLM 生成大约 64 个提示以供选择。如果生成更多提示，性能提升会开始递减——**发现的最佳提示在准确性方面没有显著提高**。

> **“虽然这种方法提高了提议集的整体质量，但得分最高的指令在更多阶段中往往保持不变。”** 

**迭代生成**    我们还可以以迭代方式使用 APE 优化提示。在这种设置中，我们仍然让 LLM 提出新指令，为每个指令计算得分，并选择得分最高的指令。更进一步，我们可以重复此过程，将这些生成的指令作为输入，使用它们作为上下文以类似方式提出更多指令变体，并选择表现最佳的提示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13faa491-b94e-4156-9314-1d7dfc5299ff_1822x534.png)

使用这种策略，我们可以探索LLM提出的不同提示变体。然而，在 [1] 中我们看到，迭代 APE 仅提高了被探索提示集的整体质量。APE 发现的最佳提示的性能在任何次数的迭代后都保持不变。因此，**迭代 APE 的额外成本可能是不必要的**。

**APE 是否能发现更好的提示**    APE在多个设置中评估其编写有效提示的能力，包括零样本提示、少样本提示、CoT 提示以及用于引导 LLM 行为（如更真实）的提示。在实验中，我们发现 APE 能够找到比人类编写的提示更优的提示。特别是，在 24 个指令归纳任务中， APE 生成的提示优于人类编写的提示，在 21 个 BIG-Bench 任务中，APE 在其中 17 个任务中表现更佳。

通过研究APE提出的提示，我们甚至可以总结出编写有效提示的有用技巧。在许多情况下，这些技巧在不同任务中表现良好，并教会我们如何正确地为某些模型编写提示。我们还在 [1] 中看到，指令候选的性能随着LLM规模的增大而提高，**这表明在 APE 中使用 LLM 来提出新指令是有帮助的。**

#### 3.2 自动提示优化

虽然 APE 表现良好，但其用于优化提示的过程是随机且无方向性的。我们只是使用 LLM 提出一系列新的提示变体，并选择表现最好的生成提示。该算法中没有迭代优化过程。相反，我们纯粹依赖优化器LLM提出各种有前途的提示变体，并在单次评估中找到更好的提示。

> **“该算法使用数据批次形成自然语言梯度，对当前提示进行批评，就像数值梯度指向误差上升的方向一样。然后通过在与梯度相反的语义方向上编辑提示，将自然语言梯度传播到提示中。”** 

从正确的角度来看，使用 LLM 优化提示类似于传统的基于梯度的优化。在 [2] 中，作者提出了一种称为自动提示优化（APO）的提示优化技术，受到这种类比的启发。APO 是一种通用技术，只需要一个（小型）训练数据集、初始提示和 LLM API 的访问权限即可工作。该算法使用训练数据批次来得出“梯度”——**仅是概述当前提示局限性的文本批评**——以指导对提示的编辑和改进，模拟梯度更新。

**优化框架**    APO 旨在通过自然语言梯度对提示进行离散改进。这些梯度通过以下方式得出：

- 在训练数据集上使用 LLM 执行当前提示。
- 根据某个目标函数衡量提示的性能。
- 使用 LLM 批评该提示在训练数据集上的关键性能局限。

所得到的梯度只是对当前提示中存在的各种问题的文本总结。利用这个总结，我们可以提示 LLM（使用梯度和当前提示作为输入）编辑现有提示，以减少这些问题。APO 迭代地应用这些步骤，以找到最佳提示。

**梯度和编辑**    APO 创建了一个递归反馈循环，通过以下步骤优化提示：_i)_ 收集当前提示在训练数据上的错误，_ii)_ 通过自然语言梯度总结这些错误，_iii)_ 使用梯度生成多个修改版本的提示，_iv)_ 选择最佳编辑提示，_v)_ 重复此过程多次。

> **“我们通过基于文本的对话模拟梯度下降的步骤，用 LLM 反馈代替微分，用 LLM 编辑代替反向传播。”** 

为了生成“梯度”，我们向 LLM 展示当前提示在训练数据集上所犯错误的示例，并要求模型提出这些错误的原因。然后，我们使用这些原因作为编辑提示的背景信息。

在每次迭代中，我们会对当前提示生成多个编辑版本——编辑的数量是优化过程的一个超参数。此外，我们可以通过显式生成每个提示的多种表述来扩展搜索空间，这种技术在 [2] 中被称为蒙特卡罗采样。

**搜索与选择** 通过生成每个提示的多个变体，我们可以通过束搜索寻找最佳提示。换句话说，在每次迭代中，我们保留多个最佳提示候选，针对每个提示提出 $N$ 个编辑，然后通过在训练集上的性能测量选择最佳的 $B$ 个编辑，以在下一次迭代中保留。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbef3e5-65c3-40e2-a125-00c5ae4904a1_1600x900.gif)
正如我们可能预料的，如果我们持续在整个训练数据集上评估所有提示候选，束搜索会变得昂贵。然而，在 [2] 中我们看到，可以使用多臂老虎机技术，特别是与最佳臂识别问题相关的技术，使搜索过程更高效。虽然这种方法的细节超出了本文的范围，但其高层次的想法是，我们可以利用统计方法来选择哪些提示值得全面评估，而不是简单地对每个提示在整个训练数据集上进行评估。

**效果如何**   APO 在四个不同的文本分类任务中对 GPT-3.5 的提示进行了评估，包括越狱检测、仇恨言论检测、假新闻检测和讽刺检测。_为什么只考虑分类问题？_ 选择分类任务来评估 APO 可能是因为这些问题的目标函数定义相对简单——我们可以根据分类准确率来评估每个提示。对于复杂或开放式任务，定义目标函数并不简单，这使得像 APO 这样的技术应用起来不那么直接。

如上所示，APO 在所有数据集上都优于其他最新的提示优化算法。值得注意的是，图中的蒙特卡洛（MC）技术 APE 的表现不如 APO，这表明通过自然语言梯度为提示优化过程添加更明确的方向是有益的。在 [2] 中，我们看到 APO 可以将初始提示的性能提高最多 31%，尽管 APO 需要调用大型语言模型的 API，但其改进幅度仍超过其他技术。

#### 3.3 GRIPS: 无梯度、基于编辑的指令搜索，用于提示大型语言模型

> _“我们提出了无梯度的指令提示搜索（GRIPS），这是一种通过迭代的、本地的、基于编辑的无梯度搜索来改进指令提示的自动化程序。”_ - 摘自[14]

与其在提示优化过程中使用大型语言模型来编辑提示，我们可以使用启发式策略来生成新的提示变体。在 [14] 中，作者提出了 GRIPS，这是一种无梯度的方法，通过迭代应用一组固定的启发式编辑操作来搜索更好的提示。与我们之前看到的方法类似，GRIPS 以人工编写的提示作为输入，并返回改进后的编辑提示作为输出。

**启发式提示搜索**   GRIPS 可以应用于基于指令的提示和包含上下文示例的提示（如下所示），但其重点仅在于改进指令——GRIPS 中不尝试优化示例的选择。为了在可能的提示空间中进行搜索，我们从初始提示开始，迭代地对其进行一系列编辑。然后，我们可以在一组训练示例上测试编辑后的提示性能，以确定哪些编辑后的提示效果最佳。

这个过程与我们之前看到的技术有些相似，但在搜索过程中，我们应用一组启发式编辑来优化提示，而不是依赖大型语言模型生成编辑版本。这种技术可以与任何类型的 LLM 一起使用，包括通过 API 公开的模型。

**搜索细节**   GRIPS 的优化框架如下所示。我们从一个由人类编写的基础指令开始搜索过程。在每次迭代中，我们选择若干编辑操作应用于该提示，生成多个新的提示候选。这些候选会被评分，以便识别出最佳的一个。如果最佳候选的得分超过当前基础指令的得分，则我们将其作为新的基础指令继续使用。否则，我们保持当前的基础指令并重复这个过程。这是一个贪婪的搜索过程，当基础指令的得分在若干次迭代中没有提高时，搜索终止。

类似于 [2]，我们可以通过在每次迭代中维护一组指令，将这种贪婪搜索过程轻松调整为执行束搜索。然而，这种修改会增加搜索过程的成本，因为在每一步需要评估更多的候选提示。

**编辑类型**    在 GRIPS 中，我们总是在短语级别进行编辑，这样可以在保持提示结构的同时进行有意义的修改。为了将提示分解为短语，我们可以使用成分分析器。然后，对提示中的短语进行以下四种编辑操作：

1. **删除**：移除指令中某个短语的所有出现，并存储删除的短语以备添加操作使用。
2. **交换**：给定两个短语，将指令中第一个短语的所有出现替换为第二个短语，反之亦然。 
3. **释义**：将指令中某个短语的所有出现替换为该短语的释义版本。
4. **添加**：取一个在先前迭代中删除的短语，并在随机的短语边界处将其添加回指令中。

**实际应用**    GRIPS 主要在 Natural Instructions 数据集的二分类任务上进行评估。与 [12] 中的观察类似，我们发现 GRIPS 发现的提示往往能提高准确性，但并不总是连贯的。优化后的提示包含语义上尴尬和令人困惑的措辞。然而，在 [14] 中我们看到，使用 GRIPS 优化基础指令能一致地提高各种LLM的准确性，幅度可达 2-10%。此外，GRIPS 通常优于手动重写提示，甚至在某些情况下优于基于梯度的提示调优，尤其是在优化特定任务（而非通用）指令时效果最佳。

#### 3.4 大型语言模型作为优化器

> _“我们不是正式定义优化问题并通过编程求解器推导更新步骤，而是用自然语言描述优化问题，然后指示LLM根据问题描述和先前找到的解决方案迭代生成新解决方案。”_ 

如今最流行和广泛使用的提示优化技术之一是通过提示进行优化（Optimization by Prompting, OPRO）。然而，OPRO 不仅仅能优化提示。它是一种通用的优化算法，其操作过程如下：

1. 用自然语言描述一个优化任务。
2. 向优化器 LLM 展示先前优化任务的解决方案示例及其目标值。
3. 要求优化器 LLM 推断该问题的新/更好的解决方案。
4. 通过评估器 LLM 测试推断出的解决方案。

通过重复上述步骤，我们创建了一个极其灵活的、无梯度的优化算法。我们无需用数学形式正式指定我们要解决的问题，只需用自然语言解释即可。这种描述足以使用 OPRO 进行优化，使得该算法具有广泛的适用性，并易于扩展到新任务。

在正确的采样策略下，这种优化算法的轨迹相对稳定，这再次表明 LLM 能够从过去的解决方案中学习，以提出新的、更好的解决方案；见上文。尽管 OPRO 可以应用于许多问题，但在 [3] 中我们看到，这种算法在自动提示优化中特别有用。首先，我们将了解 OPRO 的工作原理，然后探讨如何用它来优化提示。

**OPRO 框架**    OPRO 的高层步骤如上所述。在优化过程的每一步，OPRO 基于元提示生成新解决方案，该元提示包含优化问题的文本描述和之前提出的解决方案。给定元提示后，我们会同时生成多个新解决方案。通过在每个步骤生成多个候选解决方案，我们可以确保（相对）稳定的优化轨迹，因为 LLM 有多次机会生成可行的解决方案。

接下来，我们根据目标函数（例如准确性）评估每个新解决方案，并将最佳解决方案添加到下一步优化的元提示中。通过这种方式，我们选择最佳解决方案传递到优化过程的下一次迭代。我们的目标是找到一个优化目标函数的解决方案，一旦 LLM 无法提出能在目标上带来改进的新解决方案，优化过程就会终止。

**OPRO 的关键组件**    从框架中可以看出，OPRO 的优化过程包含两个主要组件：

- **优化器**：接受元提示作为输入并提出新解决方案。
- **评估器**：接受解决方案作为输入并计算目标值。
    
优化器始终由 LLM 实现。为了成功应用 OPRO，我们必须选择一个足够强大的 LLM 作为优化器，因为从元提示中提供的上下文中推导最佳解决方案需要复杂的推理能力。而且，我们在 [1] 中看到，更大的 LLM 往往在这项任务中表现更好。

评估器也可以根据所解决的问题由 LLM 实现，*但优化器和评估器不必是同一个 LLM*。例如，线性回归问题有一个简单的评估器（即我们可以直接从解决方案中计算目标值），但提示优化问题需要使用评估器 LLM 来衡量每个提示的输出质量。

> **“虽然传统优化通常需要相当大的训练集，但我们的实验表明，少量或部分训练样本就足够了。”** 

使用 OPRO 进行优化需要训练和测试数据。我们使用训练数据集在优化过程中计算目标值，而测试集用于在优化过程结束后评估生成解决方案的最终性能。与大多数优化算法相比，OPRO 需要的训练数据非常少。

**关于元提示的更多信息。** 元提示提供了优化器 LLM 提出更好解决方案所需的所有上下文。元提示有两个主要组成部分：

1. 优化问题的文本描述。
2. 先前的解决方案及其目标值（即优化轨迹）。
    
优化轨迹排序，使最佳解决方案出现在元提示的末尾。除了这些核心信息，我们还包括从训练数据集中随机选择的示例，以展示预期的输出格式，以及创建新解决方案时需要遵循的一般指令（例如，“简洁”或“生成最大化准确性的指令”）。下面展示了一个用于提示优化任务的元提示示例。

**OPRO 的性能**   在 [3] 中，OPRO 在 GSM8K 和 Big-Bench Hard 上进行了评估。在这些数据集上，OPRO 发现的提示分别比人工编写的提示高出 8% 和 50%。与 APE 等先前方法相比，OPRO 能够找到更复杂的指令，从而实现更好的性能；有趣的是，发现的指令风格往往因使用的优化器 LLM 而异。

**高级推理系统（o1）**    如前所述，从优化轨迹中推导新解决方案是一个复杂的推理问题。我们已经看到，更大的 LLM 在解决此任务时表现更好，这表明使用更强大的优化器 LLM 能带来更好的推理能力。此外，**提示优化是一次性成本**。我们只需优化一次提示，但优化后的提示通常会在下游应用中使用更长时间。因此，使用更昂贵的优化器 LLM 的成本被其生成的提示的使用寿命所摊销。

最近提出的基于 LLM 的推理系统（如 OpenAI 的 o1）为提示优化开启了新的可能性。这些系统经过训练，能够遵循复杂的推理过程，模型可以动态决定是否需要更多计算来解决复杂问题。这种高级推理策略在某些情况下可能成本较高；例如，o1 在响应提示前可能会“思考”超过一分钟。然而，在推理时使用额外的计算资源可能对复杂推理问题是值得的。在自动提示优化的情况下，优化器 LLM 响应的延迟和成本远不如生成提示的质量重要，这使得这些高级推理系统成为此类应用的理想选择。

> “我们推出了 OpenAI o1，这是一种通过强化学习训练的大型语言模型，能够执行复杂推理。o1 在回答前会进行思考——它可以在响应用户前产生长的内部思维链。” 


#### 3.5 通过进化优化提示

目前为止，我们看到的提示优化策略从人类编写的一个或一组提示开始，并通过迭代过程：

1. 生成提示的变体；
2. 从这些变体中选择表现最佳的提示。

在某种程度上，这个优化过程可以被视为一种进化算法（EA）。提示的种群会随着时间推移进行维护、变异和选择。我们通常使用单一种群（除非使用束搜索），并通过提示 LLM（或使用基于编辑的启发式策略）来修改当前提示进行变异。受此工作的启发，研究人员最近探索了更明确的策略，将进化算法应用于提示优化。正如我们将看到的，这些算法与我们之前看到的并没有太大不同！

**遗传提示搜索（GPS）** 使用遗传算法自动发现高性能提示。首先，算法从一组手写提示——即种群——开始。然后，我们可以通过以下操作对这些提示进行变异：

- **反向翻译**：将提示翻译为11种其他语言之一，然后再翻译回英文。
- **完形填空**：在提示中屏蔽几个标记，使用预训练语言模型（如 T5）填充它们，从而生成新提示。
- **句子续写**：提示 LLM“写两句意思相同的句子”，并提供当前提示作为第一句。

这些策略可以变异当前提示种群以生成新的提示变体。然后，我们可以使用保留验证集对生成的提示进行评分，以确定哪些提示应保留到进化的下一迭代或世代。该算法优于手动提示工程，甚至在某些情况下优于提示调优。

> “提示中的短语序列可以被视为典型进化算法中的基因序列，使其与自然进化过程兼容。” 

**EvoPrompt**  同样利用进化算法的概念来创建一个性能优良且收敛快速的提示优化器。EvoPrompt 从手动编写的提示种群开始，使用两种主要的进化操作——变异和交叉——生成新提示。在每一代中，通过在保留验证集上的表现来选择和保留最佳提示以进行进一步进化。

根据定义，进化操作符以序列为输入并输出修改后的序列。传统上，操作符独立应用于序列的每个元素——我们更新每个元素时不考虑其周围元素是否已更改。对于提示优化，这种方法存在问题，因为提示中的所有标记都是相互关联的。

为了解决这个问题，EvoPrompt 通过提示实现进化操作，并依靠LLM的专业知识以逻辑方式进化提示。上面展示了用于提示优化的交叉和变异提示示例。然而，EvoPrompt 不仅支持遗传算法，还支持多种进化算法！例如，下面展示了一组用于实现差分进化策略的提示。EvoPrompt 框架足够灵活，可以以即插即用的方式替换不同的进化算法。

EvoPrompt 在多项任务中使用闭源和开源的 LLM（如 Alpaca 和 GPT-3.5）进行测试，发现其性能优于 APE 和 APO 等算法。在大多数情况下，EvoPrompt 的差分进化变体比遗传算法变体表现更好。

**Prompbreeder** 在 EvoPrompt 之后不久被提出。这两种技术背后的理念相似：

- 它们都从一组人工编写的提示开始。
- 它们都通过对LLM进行提示来实现进化操作，如变异和交叉。
- 它们都通过在验证集上的表现来进行选择。

这些技术的主要区别在于，Promptbreeder 不仅优化任务提示，还优化用于实现进化操作的提示！这种方法允许在提示优化算法中引入自我改进机制。

> “也就是说，Promptbreeder 不仅在改进任务提示，还在改进用于改进这些任务提示的变异提示。” 

Promptbreeder 在推理基准测试中表现优于多种强大的提示策略（如链式思维和计划与解决提示）。虽然其他提示优化算法在扩展优化过程到更多迭代时通常会遇到收益递减和性能饱和的情况，但 Promptbreeder 可以动态调整优化过程，发现更复杂的提示，从而更好地解决困难任务。

### 4、实用技巧和要点

在本次概述中，我们已经看到了各种提示优化技术，包括直接从数据中学习软提示的方法、利用 LLM 作为无梯度优化器、使用强化学习训练更擅长生成提示的模型等。以下是所有这些研究的关键要点。

**为什么提示优化很难**    正如我们所见，提示优化并不是一个普通的优化问题，原因包括：

1. 我们试图优化的提示由离散的标记组成。
2. 在大多数情况下，我们缺乏梯度信息。

如果我们使用的是 LLM API，我们能获得的信息非常有限，难以改进提示。此外，提示是离散的，使得应用基于梯度的优化算法变得困难。成功的提示优化算法通过以下方式避免了这些问题：_i)_ 采用类似进化算法的无梯度优化算法；_ii)_ 依靠 LLM 从已尝试的提示中推断出更好的提示。

**LLM 是优秀的提示工程师**    最近的提示优化论文的主要结论之一是，LLM 在编写提示方面表现出色。假设我们提供正确的信息作为背景，仅通过反复提示LLM来批判和改进提示，就可以创建出意外强大的提示优化算法。更大（更强）的 LLM 在这项任务上表现更好。因此，将先进的模型应用于提示优化是一个尚未被充分探索的有趣机会。

**我们应该使用什么**    由于我们见识了许多提示优化算法，可能不确定该使用哪些技术。我们所见的大部分研究都是有用的。然而，基于 LLM 的提示优化器（如 OPRO）是迄今为止最直接和实用的提示优化技术。我们可以使用这些算法自动改进任何我们编写的提示，生成一个人类仍可理解的更好提示。OPRO 易于应用，这使其在 LLM 从业者中很受欢迎。OPRO 的开源实现可以在[这里](https://github.com/google-deepmind/opro)找到。

**人类是否仍然必要**    尽管提示优化技术可以大幅减少手动提示工程的工作量，但完全消除人类提示工程师是不太可能的。我们所见的所有算法都需要人类提示工程师提供初始提示作为优化过程的起点。此外，像 OPRO 这样的技术在实际应用中需要人类干预和指导以找到最佳提示。简单来说，_自动提示优化技术是辅助性质的_。这些算法自动化了一些基本的手动提示工程工作，但并不消除对人类提示工程师的需求。然而，值得注意的是，LLM 对提示的微妙变化变得不那么敏感，使得提示工程总体上变得不那么必要。

**提示优化的局限性**    我们所见的提示优化算法对于改进提示的措辞和基本结构很有用。然而，构建高性能 LLM 系统时还需要做出许多其他选择。例如，我们可能需要使用检索增强生成（RAG），在提示中添加额外的数据源，找到最佳的少样本例子等。自动化这些更高层次的选择超出了我们所见提示优化算法的能力，但研究人员正在开发像 DSPy 这样的工具，可以自动化 LLM 系统的高层设计和低层实现。
