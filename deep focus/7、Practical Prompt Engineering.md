
由于其文本到文本的格式，大型语言模型（LLMs）能够凭借单一模型解决多种多样的任务。最初，像GPT - 2和GPT - 3 [5, 6]这样的模型通过零样本和少样本学习展示了这种能力。然而，当对大型语言模型进行微调以使其与人类偏好和指令相一致时，它们就变得更具吸引力了，从而使得诸如代码辅助工具、信息检索对话代理以及基于聊天的搜索体验等流行的生成式应用程序成为可能。

由于它们所带来的应用可能性，大型语言模型（LLMs）在研究界和流行文化中迅速崛起。在这一崛起过程中，我们也见证了一个新的互补领域的发展：提示工程（prompt engineering）。从高层次来看，大型语言模型的运作方式是通过 i) 接受文本（即提示）作为输入，并 ii) 生成我们可以从中提取有用信息（例如分类、摘要、翻译等）的文本输出。这种方法的灵活性是有益的。然而，与此同时，我们必须确定如何正确构建输入提示，以便大型语言模型有最大的机会生成所需的输出。

提示工程是一门实证科学，研究如何利用不同的提示策略来优化大语言模型的性能。尽管存在多种方法，但在本次概述中，我们将着重理解提示的基本机制，以及一些基础（但极其有效！）的提示技术，如零/少样本学习和指令提示。在此过程中，我们将学习实用技巧和要点，这些内容可立即付诸实践，助你成为一名更高效的提示工程师和大语言模型从业者。

# Prompting at a Glance

要完成一项任务，我们所需要做的就是：i) 向模型提供包含相关信息的文本输入；ii) 从模型返回的文本中提取输出。这种统一的方法可用于翻译、摘要生成、问答、分类等任务。然而，事情并非（完全）如此简单。也就是说，提供给大语言模型的提示词（即输入的文本）的措辞和结构会显著影响模型的准确性。换句话说，提示工程至关重要。

### What is prompt engineering?

鉴于恰当地构建提示词内容对于利用大型语言模型（LLM）获得有用结果至关重要，近几个月来，提示工程受到了广泛关注。然而，这是一门经验科学——发现最佳提示词通常基于启发式方法，并且需要进行实验。我们可以通过跟踪和版本控制提示词，并随着时间的推移测试不同的想法，以了解哪些方法有效，从而发现更好的提示词。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78f78c42-5ebf-4b2e-a79b-b757d861ec9d_1598x776.png)

提示的组成部分。创建提示有多种方式可供选择。然而，大多数提示都包含以下相同的几个（可选）组成部分：

- 输入数据：这是大语言模型预期处理的实际数据（例如，要翻译或分类的句子、要总结的文档等）。
- 示例：向大语言模型展示正确行为的最佳方式之一是在提示中提供一些具体的输入 - 输出对示例。
- 指令：我们可以在提示中不展示正确行为的具体示例，而是通过指令以文字形式描述需要执行的操作；见上文。
- 指示符：以固定且可预测的结构向大语言模型提供输入是有帮助的，因此我们可以使用指示符分隔提示的不同部分；见下文。
- 上下文：除了上述组成部分之外，我们可能还想以某种方式向大语言模型提供额外的“上下文”或信息。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7df4b8f5-6894-4e63-8057-36db0d19f095_1574x928.png)

通用提示技巧。提示工程的具体细节会因所使用的模型以及我们试图解决的任务而大不相同。然而，有一些被普遍接受的提示工程原则值得牢记[1, 3]。

- 从简入手：先从一个简单的提示开始，然后在跟踪实证结果的同时逐步修改提示。
- 直截了当：如果我们希望大语言模型匹配特定的风格或格式，应清晰直接地说明这一点。明确说出你想要的内容能准确传达信息。
- 具体性：歧义是每个提示工程师的大敌。我们应该让提示详细具体，但不要过度，提供过长（即提示长度是有限制的！）的输入。
- 示例很有用：如果难以描述我们想要的内容，提供几个不同输入的正确输出或行为的具体示例可能会有所帮助。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e3f8a12-a893-4508-95d7-312d37a77ea2_1792x1112.png)


### The Context Window

在考虑不同的提示技巧和方法时，我们需要记住，我们在提示中只能包含有限的信息。所有大型语言模型都有一个预定义的上下文窗口，它限制了一次可以处理的令牌总数（即文本序列中的单词或子词）。不同模型的上下文窗口大小不同，但目前有一个强烈的趋势是增加上下文窗口的大小。例如，GPT-4 的上下文窗口为 32K 个令牌，是 OpenAI 之前任何模型的 4 倍。

# Common Prompting Techniques
![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F526871dd-138b-4f4d-97ce-9033f71c866d_2536x1416.png)

尽管像ChatGPT这样的热门模型使得大型语言模型（LLMs）近期备受关注，但提示（prompting）技术其实已经存在一段时间了。最初，像GPT [4] 这样的模型是通过微调来解决下游任务的。随着GPT - 2 [5] 的提出，我们看到研究人员开始使用零样本学习，用单个基础模型来解决多个下游任务。最后，GPT - 3向我们展示了随着规模的增长，语言模型在少样本学习方面变得非常出色。在本节中，我们将深入探讨这些理念，以便更好地理解零样本学习和少样本学习的工作原理，并详细介绍一些更复杂的提示技术。

### Zero-Shot Learning

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe749d0ff-794c-4fd5-a8bf-d75873bf3e38_924x322.png)

零样本学习背后的理念其实相当简单。我们只需将待解决任务的描述以及相关输入数据提供给大语言模型（LLM），让它生成结果即可；详见上文。由于在预训练阶段接触了海量数据，大语言模型通常能够以这种方式很好地完成任务。也就是说，它们可以利用自身的知识库来解决（相对）较多的任务；具体示例如下（由GPT - 3.5生成）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc2ea13-cf4b-4b6c-955f-835dbbf27be4_2412x752.png)

零样本学习已被像GPT - 2这样的模型广泛探索，在某些情况下表现良好。然而，如果零样本学习不能解决我们的任务，我们该怎么办呢？在许多情况下，通过提供更具体、更详细的信息，我们可以大幅提高大型语言模型的性能。特别是，我们可以开始在提示中添加期望输出的示例，让模型从提示中看到的数据里复制模式。

### Few-Shot Learning

除了单纯的任务描述之外，我们还可以通过高质量的输入 - 输出示例来丰富我们的提示语。这种技术构成了小样本学习的基础，小样本学习试图通过提供正确行为的明确示例来提高大语言模型的性能。如果使用得当并应用于合适的模型，小样本学习会非常有效，正如像GPT - 3 [6] 这样的大语言模型所展现出的突破性能力那样；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50ee0351-5743-4f1a-bdee-2f0e13074579_1590x1044.png)

然而，学习如何正确利用大型语言模型（LLMs）的小样本学习能力可能比较复杂。我们应该在提示中包含哪些示例？提示的结构是否有正确的方式？提示的更改会显著影响大型语言模型吗？

大多数大语言模型（LLM）对提示词的构建方式较为敏感，这使得提示词工程既困难又重要。尽管像GPT - 4这样的近期模型似乎对提示词中的小扰动不太敏感[2]，但研究界[7]还是为我们提供了一些关于正确使用少样本学习的有用建议，这些仍值得我们了解。

- 范例排序很重要，打乱少样本示例的顺序可能会极大地改变大语言模型的性能。增加少样本示例的数量并不能解决这个问题。
- 少样本示例中标签的分布很重要，应与实际数据分布相匹配。令人惊讶的是，标签的正确性并非那么重要。
- 大语言模型倾向于重复少样本示例中的最后一个示例（即近期偏差）。
- 提示中包含的范例应具有多样性且随机排序。

最佳数据采样。选择多样化、随机排序且与测试样本相关的示例是最好的。然而，除了这些基本直觉之外，大量研究已经开展，以确定如何为提示选择最佳的范例。例如，可以通过多样性选择[8]、基于不确定性的选择[9]，甚至基于与测试样本相似性的选择[10]来挑选少样本学习样本。

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a2a491-da4b-4e94-8412-e828fde0a18c_1398x702.png)

小样本学习与微调。在继续之前，我想指出一个容易混淆的重要问题。小样本学习并非微调。小样本学习是在提示词中向大语言模型展示示例，这些示例可作为生成正确输出的相关上下文。这一过程被称为“上下文学习”；详见上文。小样本学习不会改变模型的参数。相比之下，微调会明确地对模型进行训练（即通过反向传播更新其权重），训练基于选定的数据集。

### Instruction Prompting

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48a2a944-9dd7-4c51-af7e-6071111fafa4_1108x638.png)

小样本学习能力极强，但存在一个显著的缺点：示例会消耗大量标记。考虑到大语言模型的上下文窗口有限，我们或许需要探索一种不消耗过多标记的提示方法。例如，我们能否用文字向大语言模型解释正确的行为？答案是肯定的！这种技术仅需在提示中加入书面指令，被称为指令提示，它在特定类型的大语言模型上表现最佳。

指令微调与对齐。近期语言模型的发展重点主要集中在提升遵循指令的能力上。预训练的大型语言模型（LLMs）在初始状态下并不擅长遵循指令。然而，教会这些模型如何遵循指令能使其更好地完成用户的需求（即提升与人类的对齐度）。遵循指令的大型语言模型支撑着多种实用应用，从信息查询对话代理（例如ChatGPT）到代码辅助工具（例如Codex [13]）；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F918b281b-fbae-493d-a131-a566aecdcb01_2460x1138.png)

正如之前的文章中广泛讨论的那样，创建大型语言模型（LLM）的第一步是使用语言建模目标，在大量未标记的文本语料库上对模型进行预训练。在此过程中，模型获取信息并学会准确地进行下一个词元预测。然而，模型的输出并不总是有趣、引人入胜或有帮助的，而且模型通常难以遵循复杂的指令。为了鼓励这种行为，我们需要超越基础预训练。

创建遵循指令的大语言模型（LLMs）。目前有几种不同的方法可以教会大语言模型如何遵循指令。例如，我们可以进行指令微调[12]，或者基于包含指令的对话示例对大语言模型进行微调。一些知名的模型采用了这种方法，如LLaMA（及其变体）[15]、所有FLAN模型[12]、OPT - IML[16]等。另外，我们还可以采用由监督微调（SFT）和基于人类反馈的强化学习（RLHF）组成的三步法；详见下文。这种方法催生了像ChatGPT、GPT - 4、Sparrow[17]等令人惊叹的模型。


![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91b184e2-35a4-431b-96a5-837893544ea4_1690x1206.png)


撰写有用的指令。如果我们能够使用经过训练以遵循指令的大语言模型（LLM），那么通过向模型提供有用且信息丰富的指令，我们可以完成很多任务。以下是使用指令提示的一些关键技巧和思路：

- 就像我们提示中的其他内容一样，指令应具体且详细。
- 我们应避免在提示中告诉大语言模型不要做什么。相反，我们应专注于告诉它要做什么。
- 使用带有明确标识指令的指示符的输入结构会有所帮助；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2644d809-0970-4d6e-a1b4-deba130418b7_1610x874.png)

角色提示（role prompting）。另一种与指令提示有一定关联的有趣提示技术是角色提示，它为模型赋予一个“角色”或人物设定。该角色通过提示中的如下文本片段来指定：

- 你是一位著名且杰出的数学家。
- 你是一名医生。
- 你是一名音乐专家。

有趣的是，最近的LLMs能够在整个对话过程中相当好地假定并维持这样的角色[18]；见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbab9a10c-0bfe-4b94-869c-5222b94ac17e_2484x846.png)

进一步来说，角色提示不仅仅是一个有趣的技巧。为大型语言模型（LLM）设定一个角色实际上可以提高其性能（例如，将GPT - 3提示为“杰出的数学家”可以提升其在基于算术的问题上的表现）。然而，角色提示仅在某些情况下才能提高性能。

> _“在为人工智能分配角色时，我们是在为其提供一些背景信息。这些背景信息有助于人工智能更好地理解问题。对问题有了更好的理解后，人工智能通常会给出更好的答案。——来自learnprompting.org

现实世界中的指令提示。用指令提示大语言模型（LLM）是一种极其强大的工具，可用于多种应用场景。要了解如何利用这一技术，我们只需看看最近发布的ChatGPT插件即可，其中包含一个开源的信息检索应用程序编程接口（API）。在这个API中，提供了两个特定的模块，分别用于从文档中提取元数据以及过滤个人可识别信息（PII）。有趣的是，这些服务完全基于大语言模型，并使用了下面所示的提示语。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff749f634-3395-4e6a-b444-869946faf961_1396x834.png)

在这些提示中，大语言模型会得到关于如何执行其期望任务的具体且详细的指令。这些指令的一些显著特点包括：

- 明确说明了期望的输出格式（JSON 或是真/假）。
- 指令采用结构化格式（即用项目符号分隔的列表）来描述重要信息。
- 在提示中明确说明了大语言模型的任务（即识别个人信息识别信息或提取元数据）。
- 有趣的是，这些提示多次告知模型不应做什么，而通常不建议这样做。
 
鉴于大语言模型存在局限性，信任它们准确执行诸如个人身份信息（PII）检测这类关键任务可能并非良策。然而，这种方法展示了指令提示的巨大潜力。我们或许无需编写一整个程序或服务，只需写一个提示，就能快速解决诸多任务。

# Takeaways

>“为一个聊天机器人角色撰写一个真正出色的提示词是一项极具杠杆效应的技能，也是早期在自然语言中进行少量编程的一个例子。” —— 萨姆·奥特曼

如果我们从这篇概述中学不到其他东西，那么我们应该知道，构建正确的提示（即提示工程）是在实践中成功利用大型语言模型的重要组成部分。由于语言模型的文本到文本结构，它们具有难以置信的通用性，可以用来解决各种任务。然而，我们必须为这些模型提供详细且适当的上下文，以便它们能够表现良好。尽管最佳提示技巧因模型和任务而异，但我们可以借鉴许多高层次的经验教训，以最大化成功的机会。

从零样本学习到小样本学习。鉴于大语言模型进行了大量的预训练（如今还包括微调），其训练数据集极为丰富，这些模型蕴含了大量信息，并且能够即刻解决多种任务。为此，我们只需向模型提供任务描述和相关输入数据，模型就能生成正确的输出结果。然而，由于提供给模型的上下文信息有限，零样本学习的表现也会受限。为了提升零样本学习的性能，我们可以通过在提示中插入示例的方式，利用小样本学习来改进。

指令遵循型大语言模型（LLMs）。尽管其表现良好，但少样本学习通常会消耗大量标记（token），鉴于大多数大语言模型的上下文窗口有限，这成为一个问题。为解决这一问题，我们可以采用指令提示方法，即提供对大语言模型期望行为的精确文本描述，而非通过正确输出的具体示例来捕捉这种行为。指令提示法很强大，但它需要一种经过微调（例如通过指令微调或基于人类反馈的强化学习）的特定形式的大语言模型才能发挥良好效果。预训练的大语言模型在初始状态下并不擅长遵循指令。

提示工程有各种技巧和最佳实践可以采用。通常，这类技术会随着每个新模型的发布而变化（例如，与之前的模型相比，GPT - 4 在处理非结构化提示方面要好得多[2]），但一些原则在相当长的一段时间内一直适用。首先，我们应该始终从一个简单的提示开始，然后慢慢增加复杂性。在编写提示时，我们应力求具体详细，同时避免过于冗长（因为上下文窗口有限）。最后，要真正充分发挥大语言模型的性能，我们通常需要利用少样本学习、指令提示或更复杂的方法。