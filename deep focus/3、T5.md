
![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63c940a1-8868-4787-a217-4f49361dcd23_1954x1444.png)

BERT 的成功不言而喻（即在几乎所有语言基准上达到了新的最先进性能）。因此，NLP 社区开始深入研究迁移学习这一主题，提出了许多新的扩展和改进。由于该领域的快速发展，各种方法之间的比较变得困难。文本到文本转换器（T5）模型提出了一个统一的框架，用于研究 NLP 中的迁移学习方法，使我们能够分析不同的设置并得出一套最佳实践。这套最佳实践构成了 T5，一个用于语言理解任务的最先进模型和训练框架。

## 一、T5：统一的文本到文本转换器

T5 的贡献不在于新的架构或训练方法。相反，研究完全基于现有技术。T5 考虑了 NLP 迁移学习中的所有方面，如不同的（未标注）数据集、预训练目标、基准测试和微调方法。然而，这些方面都是通过统一的文本到文本格式进行研究的。T5 的目标是：*1）* 分析迁移学习设置，*2）* 确定最有效的方法。

### 1.1 文本到文本框架

T5 将所有文本处理问题转换为“文本到文本”格式（即，输入文本并输出文本）。这种通用结构也被零/少样本学习的大型语言模型所利用，使我们能够以统一的方法建模和解决各种任务。我们可以对每个任务应用相同的模型、目标、训练程序和解码过程！我们只需采用提示方法，让语言模型以文本格式生成答案。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821c34a1-b63b-4ea9-9ced-0c73549288d2_2238x1266.png)

为了更具体地说明，T5 可以将所有任务转换为如下的文本到文本格式：

1. 在原始输入序列前添加任务特定的前缀
2. 将该序列输入到转换器中
3. 将模型的目标设定为一个文本序列

使用这种格式，我们可以轻松执行摘要或翻译等任务（即，目标自然是一个序列）。此外，我们可以通过训练模型生成与正确类别相关的文本来执行分类任务。对于回归问题，这个过程会稍微复杂一些（即，我们必须将实数输出四舍五入到最接近的小数，并将其视为分类问题），但这对大多数语言任务来说效果很好。上图展示了示例。

> “在文本分类任务中，如果模型输出的文本不对应于任何可能的标签，会出现问题……在这种情况下，我们总是将模型的输出视为错误，尽管在我们训练的模型中从未观察到这种行为。” - 来自 [1]

T5 在每个解决的任务上进行微调。这与使用少样本学习的 LLMs 和使用多任务学习解决多个任务的 NLP decathlon 不同。


### 1.2 模型、数据、实验

T5 中的所有分析都使用了上述统一的文本到文本框架，因为它允许将各种不同的语言理解任务转换为统一的格式。此外，T5 的分析使用了相同的基础 transformer 架构和预训练数据集。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f25f11e-1daf-4711-940a-6b09a1f62ae7_2298x1474.png)
*T5 对编码器-解码器 Transformer 架构所做的修改*

**模型**    T5 使用的编码器-解码器架构与原始 Transformer 非常相似。不同之处在于：

1. 在每个注意力和前馈转换之前立即应用 LayerNorm（即，在残差路径之外）
2. 对 LayerNorm 不使用加性偏置（即，仅使用缩放并消除加性偏置）
3. 使用简单的位置嵌入方案，将标量添加到用于计算注意力权重的对数值上
4. 在整个网络中应用 Dropout（例如，注意力权重、前馈网络、跳跃连接等）

这些修改在上图中有所展示。使用此模型（以及其他一些模型），T5 可以测试多种迁移学习设置，以得出一系列最佳实践。

**预训练数据集**    T5 在 Colossal Clean Crawled Corpus (C4) 上进行预训练，这是一个 750GB 的“相对干净”的英语文本语料库。虽然先前的研究提出了各种预训练数据集，但作者选择自行构建数据集，因为先前的数据集不可公开获取，使用的过滤规则有限，范围有限（例如，仅来自 Creative Commons），或仅关注于机器翻译的平行数据（即，同一句话的多个不同语言版本）。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F697e8965-6413-4ab1-982b-9f7ca97679fd_1682x572.png)

值得注意的是，C4 后来被用作 MassiveText 数据集的一个子集，用于预训练 Gopher 和 Chinchilla。请参阅上表以了解该数据集的大小指标，这有助于更好地理解 C4 相对于用于训练现代大型语言模型的预训练数据集的规模。对于大型语言模型（LLM），我们已经看到在足够大的数据集上预训练仅使用解码器的模型对其成功至关重要。对于具有不同架构的 Transformer，例如 T5，情况也是如此。在大规模、未标注的数据集上进行广泛的预训练有助于提高下游任务的表现。

**实验设置**   T5 在 C4 上进行预训练，然后微调以解决各种下游任务。然而，在此框架内使用的具体设置是可变的。即，我们可以更改：

- Transformer 架构
- 预训练设置（即，任务或数据量）
- 微调设置
- 模型的大小/规模

通过逐一更改这些设置并评估结果，我们可以为 NLP 中的迁移学习制定一套最佳实践，从而将 BERT 之后的众多提案提炼成一个有效的流程，用于创建高效的语言理解模型。

## 二、从 T5 中学到了什么？

如前所述，T5 的实验旨在发现 NLP 中迁移学习的最佳实践。为此，首先提出了一种基线方法，然后逐一改变该基线的几个方面（如模型架构/规模、数据集和预训练目标），以确定最佳方案。这种方法类似于一种**坐标下降**策略。我们将首先描述基线技术，然后解释 T5 在测试各种迁移学习设置后得出的结论。

### 2.1、T5 基线模型

与编码器-解码器架构相比，仅解码器模型存在一定局限，因为它们仅使用**因果（或掩码）自注意力**。掩码自注意力在计算序列中任一给定标记的表示时，只考虑前面的标记。然而，在某些情况下，我们希望对文本的初始部分或前缀执行完全可见的注意力，然后基于该前缀生成输出（例如翻译任务）。仅解码器模型无法处理这种情况，因为它们在整个输入上执行因果自注意力。

**训练 T5** T5 模型在 C4 语料库的总计 340 亿个标记上进行了预训练。相比之下，BERT 在 1370 亿个标记上训练，而 RoBERTa 在 2.2 万亿个标记上训练。受 BERT 的 MLM 目标启发，T5 使用稍作修改的去噪目标进行预训练，该目标：

1. 随机选择输入序列中 15% 的标记。
2. 用单个“哨兵”标记替换所有连续的选定标记。
3. 为每个哨兵标记分配一个唯一的 ID，适用于当前输入序列。
4. 使用所有选定标记构建目标，并用哨兵标记分隔。

虽然这个任务看起来有些复杂，但我们可以通过下面的简短输入序列的示例来了解其工作原理。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac18ac33-8c04-44ec-a012-d220071b41b3_2170x1118.png)

通过用单个哨兵标记替换整个掩码标记的跨度，我们降低了预训练的计算成本，因为这样操作的输入和目标序列通常较短。

**微调**    在预训练完成后，T5 在评估前会针对每个下游任务进行单独微调。由于 T5 使用了文本到文本的格式，预训练和微调都使用相同的**最大似然目标**！换句话说，我们将正确答案表述为一个文本序列（在预训练和微调过程中），并训练模型输出正确的文本序列。

**基线表现如何**    如下表所示，基线 T5 模型的表现与 BERT 等之前的模型相似，尽管这些模型不能直接比较（即，基线 T5 模型只使用了 BERTBase 计算量的 25%）。此外，我们看到预训练在大多数任务中提供了巨大的优势。唯一的例外是翻译任务，在这些任务中，是否进行预训练的性能相似。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40023bf9-2576-40e8-9260-bba060ef6526_1482x480.png)


### 2.2、更好的方法…

在测试基线架构和训练方法后，作者每次修改该方法的一个方面，例如基础架构、预训练目标或微调策略。通过测试这些不同的迁移学习变体，我们可以找到一种在不同语言理解任务中始终表现最佳的方法。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd824a968-c8f5-41fa-a7cd-91582803bcc1_1522x1124.png)

**架构**    为了研究架构选择对迁移学习结果的影响，我们可以测试不同的 Transformer 架构变体。T5 中测试的架构包括普通的编码器-解码器架构、仅解码器架构和前缀语言模型。前缀语言模型在序列内的固定前缀上执行完全可见的注意力，然后使用因果自注意力生成输出。主要区别在于这些架构在自注意力机制中使用的掩码类型不同。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb29a1ee3-5179-47ea-a8ba-5b2d5900f5c4_2400x558.png)

当测试不同架构（使用因果语言建模和去噪目标进行预训练）时，我们发现编码器-解码器 Transformer 架构（带去噪目标）表现最佳，因此在后续实验中采用了该架构。相较于其他模型，这种编码器-解码器变体总共有 2P 个参数，但与具有 P 个参数的仅解码器模型计算成本相同。为了将总参数数量减少到 P，可以在编码器和解码器之间共享参数，这种方法效果很好。

**预训练目标**    最初，T5 使用三种不同类型的预训练目标进行训练。第一种是 BERT 风格的 MLM 目标。其他目标包括一种去重排策略（即模型尝试将打乱的句子恢复到正确顺序）和基于前缀的语言建模目标。在后者中，文本被分成两个部分，第一部分作为输入传递给编码器，第二部分由解码器预测（即我们使用的是编码器-解码器 Transformer）。比较使用这些目标训练的模型的性能时，我们发现去噪目标明显优于其他策略。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F235c03cd-7ff6-4591-bc5e-d14caf13f1f8_1470x312.png)

从这里开始，作者测试了对 BERT 风格的 MLM 目标的几种修改，如下面的表格所示。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb70fcf3c-2f43-48b8-ab7d-bc3aa483725a_1914x804.png)

这些变体的表现往往相似；见下文。然而，通过选择预训练目标，将整个被破坏的标记序列替换为单个哨兵标记，并且仅尝试预测目标中的破坏标记，我们可以最小化预训练的计算成本。因此，遮蔽连续标记整个序列的基准策略是高效的，因为它生成了更短的目标序列。

![|600](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa620a46e-df94-4c9f-98d7-d73e1c18fd69_1458x434.png)

作者测试了不同的破坏率，发现破坏率对结果没有显著影响，15%的设置效果良好。另一种预训练目标明确选择标记序列进行破坏（即基准方法是均匀选择标记而不是作为一个整体序列选择，然后将连续标记组合在一起），结果与基准方法表现相似，中测试的不同预训练目标的示意图如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F092ebb5d-a8d4-49cb-84f8-58b9666871ec_1480x890.png)

研究了多种策略，但主要结论是：(i) 去噪目标效果最佳，(ii) 去噪目标的不同变体表现相似，(iii) 最小化目标长度的策略在计算上最为高效。

**数据和模型规模**    最后，研究了规模对 T5 质量的影响。首先，T5 使用多个不同的数据集进行预训练，包括一个未过滤的数据集、一个新闻专用数据集、一个模仿 [GPT-2 的 WebText 语料库](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt) 的数据集，以及几个 Wikipedia 语料库的变体。T5 在这些数据集上预训练后的表现如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff1a23181-3ac4-47f6-93aa-756cf057869d_1480x466.png)

我们可以看到：(i) 不过滤预训练语料库会极大地影响性能，(ii) 在特定领域的语料库上进行预训练在某些情况下是有帮助的。例如，在新闻语料库上预训练在 ReCoRD 数据集（一个基于新闻文章的阅读理解数据集）上表现最佳。

> “这些发现的主要教训是，在领域内的无标签数据上进行预训练可以提高下游任务的性能。这并不令人惊讶，但如果我们的目标是预训练一个能够快速适应任意领域语言任务的模型，这就不太令人满意。” 

进一步地，T5 使用不同规模的 C4 语料库的截断版本进行预训练。从这些实验中，我们了解到更多的数据（不意外地）更好。在预训练期间多次循环使用较小版本的数据集会导致过拟合，并损害下游性能；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F966563aa-6b21-426e-a6f7-119f2f72fe28_1472x846.png)

为了扩大 T5 模型的规模，作者测试了以下修改：

1. 增加 4 倍的训练迭代次数（或 4 倍更大的批量大小）
2. 增加 2 倍的训练迭代次数和 2 倍更大的模型
3. 增加 4 倍更大的模型
4. 训练一个由 4 个编码器-解码器 Transformer 组成的集成模型

在这里，为了简化，预训练和微调步骤都进行了增加。这些实验的结果如下所示。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b71dbb-5ee6-442b-8617-ce153406ae16_1490x608.png)

这些结果大致符合我们的预期。增加训练时间（或批量大小）可以提高性能。将其与更大的模型结合使用，比单独增加训练迭代次数或批量大小带来更大的好处。换句话说，*增加预训练数据量和模型规模在提高性能方面是互补的*。

**其他内容**    T5 还使用不同的多任务训练策略进行微调。总体而言，这些模型的表现略逊于为每个任务单独微调的模型。然而，确实存在一些策略可以缩小任务特定微调和多任务学习之间的性能差距。

许多深度神经网络的微调方法仅训练模型参数的一个子集（例如，“冻结”早期层，只微调模型的最后几层）。作者尝试了几种以这种方式微调 T5 的技术（例如，通过适配器层或逐步解冻），但这些方法的性能不如端到端微调整个模型；详见下文。

![|500](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8884f5cb-ecb6-4275-9829-8cd2a8294b07_1496x546.png)
