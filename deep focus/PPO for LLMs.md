
ç†è§£ä¸ºæˆ‘ä»¬å¸¦æ¥ç°ä»£ LLM çš„å¤æ‚ RL ç®—æ³•â€¦

Oct 27, 2025

![](https://substackcdn.com/image/fetch/$s_!PJsw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8db8bc5-f39d-4d1a-be16-26e0c0eb01a7_2502x1398.png)

è¿‡å»å‡ å¹´ä¸­ï¼ŒRL ä¸€ç›´æ˜¯ LLM ç ”ç©¶ä¸­æœ€å…·å½±å“åŠ›çš„é¢†åŸŸä¹‹ä¸€ã€‚æ—©æœŸç ”ç©¶åˆ©ç”¨ RL å°† LLM ä¸äººç±»åå¥½å¯¹é½ï¼Œè€Œè¿™é¡¹å°† RL åº”ç”¨äº LLM çš„åˆæ­¥å·¥ä½œå‡ ä¹å®Œå…¨ä¾èµ–äºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ã€‚è¿™ä¸€é€‰æ‹©ä½¿å¾— PPO å¤šå¹´æ¥æˆä¸º LLM è®­ç»ƒåé»˜è®¤çš„ RL ç®—æ³•â€”â€”*è€ƒè™‘åˆ° LLM ç ”ç©¶çš„å¿«é€Ÿå‘å±•ï¼Œè¿™å¯è°“ç»Ÿæ²»åœ°ä½ç›¸å½“æŒä¹…ï¼* ç›´åˆ°æœ€è¿‘å…³äº LLM æ¨ç†çš„ç ”ç©¶ä¸­ï¼Œç ”ç©¶è€…ä»¬æ‰å¼€å§‹ä½¿ç”¨ GRPO ç­‰æ›¿ä»£ç®—æ³•ã€‚

å°½ç®¡ PPO éå¸¸é‡è¦ï¼Œä½†é™¤äº†é¡¶çº§ç ”ç©¶å®éªŒå®¤ä¹‹å¤–ï¼Œäººä»¬å¯¹å®ƒçš„äº†è§£ç”šå°‘ã€‚è¿™ç§ç†è§£ä¸Šçš„ç¼ºå¤±æ˜¯æœ‰å……åˆ†åŸå› çš„ã€‚*PPO ä¸ä»…æ˜¯ä¸€ç§åŒ…å«å¾®å¦™å®ç°ç»†èŠ‚çš„å¤æ‚ç®—æ³•*ï¼Œè€Œä¸”å…¶é«˜æ˜‚çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ä½¿å¾—åœ¨æ²¡æœ‰å¤§é‡è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹è¿›è¡Œå®éªŒå˜å¾—å›°éš¾ã€‚è¦æˆåŠŸåˆ©ç”¨ PPOï¼Œæ—¢éœ€è¦å¯¹ç®—æ³•æœ‰æ·±åˆ»çš„ç†è§£ï¼Œä¹Ÿéœ€è¦ä¸°å¯Œçš„é¢†åŸŸçŸ¥è¯†æˆ–å®è·µç»éªŒã€‚

æœ¬æ¦‚è¿°å°†ä» RL çš„åŸºæœ¬æ¦‚å¿µå…¥æ‰‹ï¼Œé€æ­¥æ·±å…¥ç†è§£ PPO ç®—æ³•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†é˜è¿°ä½¿ç”¨ PPO çš„å…³é”®å®è·µè¦ç‚¹ï¼ŒåŒ…æ‹¬ PPO çš„ä¼ªä»£ç åŠå…¶å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚æœ€åï¼Œé€šè¿‡åˆ†æå‡ é¡¹åœ¨ LLM é¢†åŸŸæ¨å¹¿ PPO çš„å¼€åˆ›æ€§ç ”ç©¶ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›çŸ¥è¯†èä¼šè´¯é€šã€‚

## ä¸€ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åŸºç¡€

åœ¨æ·±å…¥äº†è§£ PPO ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå­¦ä¹  RL çš„åŸºç¡€çŸ¥è¯†ã€‚æœ¬èŠ‚å°†ä»‹ç»å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬é—®é¢˜è®¾ç½®å’Œæœ¯è¯­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¨å¯¼ä¸€ä¸ªç®€å•çš„ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼ï¼Œè¿™æ˜¯ PPO ç®—æ³•çš„åŸºç¡€ã€‚

### 1.1 é—®é¢˜è®¾ç½®ä¸æœ¯è¯­

åœ¨è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª agent åœ¨æŸä¸ªç¯å¢ƒä¸­æ‰§è¡Œ actionsï¼›å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![|500](https://substackcdn.com/image/fetch/$s_!lQCe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7117e42-c6ab-43c4-8878-5a88cb99c9ae_2203x870.png)

è¿™äº›è¡Œä¸ºæ˜¯ç”±ä¸€ä¸ªç­–ç•¥é¢„æµ‹çš„â€”â€”æˆ‘ä»¬å¯ä»¥å°†ç­–ç•¥è§†ä¸ºæ™ºèƒ½ä½“çš„å¤§è„‘â€”â€”é€šå¸¸è¿™ä¸ªç­–ç•¥æ˜¯å‚æ•°åŒ–çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒ LLM çš„èƒŒæ™¯ä¸‹ï¼Œç­–ç•¥å°±æ˜¯ LLM æœ¬èº«ã€‚æˆ‘ä»¬å¯ä»¥å°†ç­–ç•¥ä¸‹ç»™å®šè¡Œä¸ºçš„æ¦‚ç‡å»ºæ¨¡ä¸º $Ï€_Î¸(a_t | s_t)$ã€‚å½“ç­–ç•¥è¾“å‡ºä¸€ä¸ªè¡Œä¸ºæ—¶ï¼Œç¯å¢ƒçš„çŠ¶æ€ä¼šæ ¹æ®è½¬ç§»å‡½æ•°è¿›è¡Œæ›´æ–°ï¼Œè½¬ç§»å‡½æ•°æ˜¯ç¯å¢ƒçš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†è½¬ç§»å‡½æ•°è¡¨ç¤ºä¸º $P(s_{t+1} | a_t, s_t)$ã€‚ç„¶è€Œï¼Œè½¬ç§»å‡½æ•°å¯¹ LLM æ¥è¯´ä¸å¤ªç›¸å…³ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸æ˜¯ç›´é€šçš„ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å‡è®¾ $s_t = \{x, a_1, a_2, â€¦, a_t\}$ï¼Œå…¶ä¸­ $x$ æ˜¯æç¤ºè¯ã€‚

æœ€åï¼Œä»£ç†è®¿é—®çš„æ¯ä¸ªçŠ¶æ€éƒ½ä¼šä»ç¯å¢ƒä¸­è·å¾—ä¸€ä¸ªå¥–åŠ±ï¼Œå¯èƒ½æ˜¯æ­£æ•°ã€è´Ÿæ•°æˆ–é›¶ï¼ˆå³æ— å¥–åŠ±ï¼‰ã€‚å¦‚å‰å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„ä»£ç†ä¼šè¿­ä»£è¡ŒåŠ¨ï¼Œæ¯ä¸ªåŠ¨ä½œï¼ˆ$a_t$ï¼‰ã€å¥–åŠ±ï¼ˆ$r_t$ï¼‰å’ŒçŠ¶æ€ï¼ˆ$s_t$ï¼‰éƒ½ä¸æ—¶é—´æ­¥é•¿ $t$ ç›¸å…³è”ã€‚å°†è¿™äº›æ—¶é—´æ­¥é•¿ç»„åˆåœ¨ä¸€èµ·å°±å½¢æˆäº†ä¸€ä¸ªè½¨è¿¹ï¼›è§ä¸‹æ–‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‡è®¾ä»£ç†åœ¨è¿™ä¸ªç‰¹å®šè½¨è¿¹ä¸­æ€»å…±åœ¨ç¯å¢ƒä¸­é‡‡å–äº† $T$ æ­¥ã€‚

![|400](https://substackcdn.com/image/fetch/$s_!cjh1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee11fdb-dee8-4d4e-8819-b97642a17129_2008x338.png)


åˆ©ç”¨æ¦‚ç‡çš„é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ç»“åˆä»¥ä¸‹æ¦‚ç‡æ¥è®¡ç®—å®Œæ•´è½¨è¿¹çš„æ¦‚ç‡ï¼š

* æ¯ä¸ªåŠ¨ä½œ $a_t$ éƒ½ç”±ç­–ç•¥ $Ï€_Î¸(a_t | s_t)$ ç»™å‡ºã€‚
* æ¯ä¸ªçŠ¶æ€ $s_{t+1}$ éƒ½ç”±è½¬ç§»å‡½æ•° $P(s_{t+1} | a_t, s_t)$ ç»™å‡º

è½¨è¿¹æ¦‚ç‡çš„å®Œæ•´è¡¨è¾¾å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

![|400](https://substackcdn.com/image/fetch/$s_!YCeT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52061751-cc8a-4f3e-a889-5d4e542b21bf_2092x770.png)

**å¼ºåŒ–å­¦ä¹ ç›®æ ‡**ï¼šåœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ•´ä¸ªè½¨è¿¹ä¸Šçš„ç´¯ç§¯å¥–åŠ±ï¼ˆå³ $r_t$ çš„æ€»å’Œï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸€ç›®æ ‡å­˜åœ¨å‡ ç§å¸¸è§å˜ä½“ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æœ€å¤§åŒ–çš„å¥–åŠ±å¯ä»¥æ˜¯æŠ˜ç°çš„æˆ–éæŠ˜ç°çš„ã€‚é€šè¿‡å¼•å…¥æŠ˜ç°å› å­ $Î³$ï¼Œæˆ‘ä»¬é¼“åŠ±ç­–ç•¥å°½æ—©è·å¾—å¥–åŠ±è€Œéå»¶åè·å–ã€‚*æ¢å¥è¯è¯´ï¼Œå½“ä¸‹è·å¾—çš„å¥–åŠ±æ¯”æœªæ¥è·å–æ›´æœ‰ä»·å€¼*ã€‚

![|350](https://substackcdn.com/image/fetch/$s_!8D_n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbfd6da8-2406-4197-b9d0-d3a1ec301b39_1496x876.png)

æˆ‘ä»¬çš„ç›®æ ‡é€šå¸¸è¢«è¡¨è¿°ä¸ºé¢„æœŸç´¯ç§¯å¥–åŠ±ï¼Œå…¶ä¸­æœŸæœ›å€¼æ˜¯å¯¹è½¨è¿¹è¿›è¡Œçš„ã€‚å±•å¼€è¿™ä¸ªæœŸæœ›å€¼å¯ä»¥å¾—åˆ°ä¸€ä¸ªæŒ‰è½¨è¿¹æ¦‚ç‡åŠ æƒçš„æ€»å’Œã€‚æˆ‘ä»¬å¯ä»¥ç”¨è¿ç»­æˆ–ç¦»æ•£çš„æ–¹å¼æ¥è¡¨è¿°è¿™ä¸€ç‚¹ã€‚

![|350](https://substackcdn.com/image/fetch/$s_!45io!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523baab0-10b4-438e-85d7-e7c5c0681209_1692x884.png)

**çŠ¶æ€ã€ä»·å€¼å’Œä¼˜åŠ¿å‡½æ•°**ï¼šä¸å¼ºåŒ–å­¦ä¹ ç›®æ ‡ç›¸å…³ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å®šä¹‰ä»¥ä¸‹å‡½æ•°é›†ï¼š

* *ä»·å€¼å‡½æ•°* $V(s)$ï¼šå½“ä»çŠ¶æ€ $s$ å¼€å§‹å¹¶æ ¹æ®å½“å‰ç­–ç•¥ $Ï€_Î¸$ è¡ŒåŠ¨æ—¶ï¼Œé¢„æœŸçš„ç´¯ç§¯å¥–åŠ±ã€‚
* *åŠ¨ä½œ-ä»·å€¼å‡½æ•°* $Q(s, a)$ï¼šå½“ä½ ä»çŠ¶æ€ $s$ å¼€å§‹ï¼Œé‡‡å–åŠ¨ä½œ $a$ï¼Œç„¶åæ ¹æ®ç­–ç•¥ $Ï€_Î¸$ è¡ŒåŠ¨æ—¶ï¼Œé¢„æœŸçš„ç´¯ç§¯å¥–åŠ±ã€‚
* *ä¼˜åŠ¿å‡½æ•°* $A(s, a)$ï¼šåŠ¨ä½œä»·å€¼å‡½æ•°ä¸ä»·å€¼å‡½æ•°ä¹‹é—´çš„å·®å€¼ï¼Œå³ $A(s, a) = Q(s, a) - V(s)$ã€‚

ç›´è§‚åœ°è¯´ï¼Œä¼˜åŠ¿å‡½æ•°é€šè¿‡è®¡ç®—åœ¨çŠ¶æ€ $s$ä¸‹é‡‡å–åŠ¨ä½œ $a$ åçš„é¢„æœŸå›æŠ¥ä¸çŠ¶æ€ $s$ çš„ä¸€èˆ¬é¢„æœŸå›æŠ¥ä¹‹é—´çš„å·®å€¼ï¼Œæ¥å‘Šè¯‰æˆ‘ä»¬æŸä¸ªåŠ¨ä½œ $a$ æœ‰å¤šå¤§çš„ç”¨å¤„ã€‚å¦‚æœåŠ¨ä½œ $a$ å¸¦æ¥çš„å›æŠ¥é«˜äºé¢„æœŸï¼Œä¼˜åŠ¿å€¼å°†ä¸ºæ­£å€¼ï¼Œåä¹‹åˆ™ä¸ºè´Ÿå€¼ã€‚ä¼˜åŠ¿å‡½æ•°åœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²â€”â€”*å®ƒä»¬è¢«ç”¨æ¥è®¡ç®—ç­–ç•¥çš„æ¢¯åº¦*ã€‚

> â€œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæœ‰æ—¶æˆ‘ä»¬å¹¶ä¸éœ€è¦ä»ç»å¯¹æ„ä¹‰ä¸Šæè¿°ä¸€ä¸ªåŠ¨ä½œæœ‰å¤šå¥½ï¼Œè€Œåªéœ€çŸ¥é“å®ƒå¹³å‡è€Œè¨€æ¯”å…¶ä»–åŠ¨ä½œå¥½å¤šå°‘ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“è¯¥åŠ¨ä½œçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚æˆ‘ä»¬é€šè¿‡ä¼˜åŠ¿å‡½æ•°æ¥ç²¾ç¡®è¡¨è¾¾è¿™ä¸€æ¦‚å¿µã€‚â€â€”â€”æ‘˜è‡ª [æ·±åº¦å¼ºåŒ–å­¦ä¹ å…¥é—¨](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)


### 1.2 LLMs çš„å¼ºåŒ–å­¦ä¹ å…¬å¼

![|300](https://substackcdn.com/image/fetch/$s_!RBDE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4b8b6b8-fe96-4b70-87d2-038a3b3511cf_1346x1134.png)

æ—¢ç„¶æˆ‘ä»¬å·²ç»ç†è§£äº†å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼Œç°åœ¨éœ€è¦å°†æ‰€å­¦æœ¯è¯­æ˜ å°„åˆ° LLM çš„è®­ç»ƒåœºæ™¯ä¸­ã€‚å…·ä½“å¯¹åº”å…³ç³»å¦‚ä¸‹ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰ï¼š

* æˆ‘ä»¬çš„ *ç­–ç•¥* å°±æ˜¯ LLM æœ¬èº«ã€‚
* æˆ‘ä»¬çš„ *åˆå§‹çŠ¶æ€* å°±æ˜¯ promptã€‚
* LLM çš„è¾“å‡ºâ€”â€”æ— è®ºæ˜¯æ¯ä¸ª token è¿˜æ˜¯æ•´ä¸ªå®Œæˆå†…å®¹â€”â€”éƒ½æ˜¯ actionã€‚
* æˆ‘ä»¬çš„ *çŠ¶æ€* æ˜¯ prompt ä¸ LLM è¾“å‡ºçš„ç»“åˆã€‚
* LLM çš„æ•´ä¸ªè¾“å‡ºè¿‡ç¨‹å½¢æˆäº†ä¸€æ¡ *è½¨è¿¹*ã€‚
* *å¥–åŠ±* æ¥è‡ªéªŒè¯å™¨æˆ–å¥–åŠ±æ¨¡å‹ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸ªè®¾ç½®ä¸­æ²¡æœ‰è½¬ç§»å‡½æ•°ï¼Œå› ä¸ºè½¬ç§»å‡½æ•°æ˜¯å®Œå…¨ç¡®å®šæ€§çš„ã€‚å¦‚æœæˆ‘ä»¬ä»ä¸€ä¸ªæç¤º $x$ å¼€å§‹ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„ LLM æ ¹æ®è¿™ä¸ªæç¤ºè¾“å…¥é¢„æµ‹å‡º token $t_1$ å’Œ $t_2$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ›´æ–°åçš„çŠ¶æ€å°±ç®€å•åœ°å˜ä¸º $s_2 = \{x, t_1, t_2\}$ã€‚æ¢å¥è¯è¯´ï¼Œ*æˆ‘ä»¬çš„çŠ¶æ€åªæ˜¯ LLM é’ˆå¯¹ç»™å®šæç¤º $x$ æ­£åœ¨ç”Ÿæˆçš„è¿è¡Œå®Œæˆå†…å®¹ã€‚*

**MDP å…¬å¼åŒ–**ï¼šå¯¹äº LLMsï¼ŒRL å¯ä»¥é€šè¿‡ä¸¤ç§å…³é”®æ–¹å¼è¿›è¡Œå…¬å¼åŒ–ï¼Œè¿™ä¸¤ç§æ–¹å¼åœ¨å¦‚ä½•å»ºæ¨¡åŠ¨ä½œæ–¹é¢æœ‰æ‰€ä¸åŒã€‚

1. *å¼ºç›—å¼è¡¨è¿°*ï¼šå°† LLM çš„æ•´ä¸ªå®Œæˆæˆ–å“åº”å»ºæ¨¡ä¸ºå•ä¸€åŠ¨ä½œã€‚
2. *é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å»ºæ¨¡*ï¼šå°†å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ª token è§†ä¸ºç‹¬ç«‹åŠ¨ä½œã€‚

æˆ‘ä»¬åœ¨ä¹‹å‰çš„æ¦‚è¿°ä¸­è¯¦ç»†ä»‹ç»äº†è¿™ä¸¤ç§æ–¹æ¡ˆçš„ç»†èŠ‚ã€‚ä¸è¿‡ï¼ŒPPO ä¾èµ–äº MDP æ–¹æ¡ˆï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ­¤å°†ä¸»è¦å…³æ³¨ MDP æ–¹æ¡ˆã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼ŒLLM é€šè¿‡ä¸‹ä¸€ä¸ª token é¢„æµ‹æ¥ç”Ÿæˆè¾“å‡ºï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡ä¾æ¬¡ç”Ÿæˆè¾“å‡ºè¡¥å…¨ä¸­çš„æ¯ä¸ª tokenã€‚è¿™ä¸ªè‡ªå›å½’è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![|450](https://substackcdn.com/image/fetch/$s_!QUg4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b1a8412-5cfb-481f-bd50-473f0a6fd9b5_1992x1037.png)

ä¸‹ä¸€ä¸ª token é¢„æµ‹å¯ä»¥è½»æ¾æ˜ å°„åˆ° RL çš„è®¾ç½®ä¸­â€”â€”æˆ‘ä»¬å¯ä»¥*å°†æ¯ä¸ª token å»ºæ¨¡ä¸ºä¸€ä¸ªåŠ¨ä½œ*ï¼è¿™ç§è®¾ç½®è¢«ç§°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¡†æ¶ã€‚MDP æ˜¯ä¸€ç§ç”¨äºå»ºæ¨¡å†³ç­–çš„æ¦‚ç‡æ¡†æ¶ï¼ŒåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±â€”â€”è¿™æ­£æ˜¯æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢è®¨è®ºçš„å¼ºåŒ–å­¦ä¹ è®¾ç½®ï¼ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ MDP æ¡†æ¶å¦‚ä¸‹æ‰€ç¤ºã€‚

![|400](https://substackcdn.com/image/fetch/$s_!KWz-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52f4f8de-4456-4cbd-935c-a945968b704d_1466x916.png)

åœ¨å°† RL å»ºæ¨¡ä¸º LLMs çš„ MDP æ—¶ï¼Œæˆ‘ä»¬çš„åˆå§‹çŠ¶æ€æ˜¯ promptï¼Œè€Œç­–ç•¥åˆ™é€šè¿‡é¢„æµ‹å•ä¸ª token æ¥æ‰§è¡Œã€‚æˆ‘ä»¬çš„ LLM å½¢æˆäº†ä¸€ç§ï¼ˆéšæœºï¼‰ç­–ç•¥ï¼Œé¢„æµ‹ token çš„æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä»è¯¥åˆ†å¸ƒä¸­é€‰æ‹©ä¸€ä¸ª token æ¥æ‰§è¡ŒåŠ¨ä½œâ€”â€”*æ¯ä¸ª token éƒ½æ˜¯å…¶è‡ªèº«çš„åŠ¨ä½œ*ã€‚å½“ä¸€ä¸ª token è¢«é¢„æµ‹å‡ºæ¥åï¼Œå®ƒä¼šè¢«æ·»åŠ åˆ°å½“å‰çŠ¶æ€ä¸­ï¼Œå¹¶ç”± LLM ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª token â€”â€”è¿™æ­£æ˜¯è‡ªå›å½’çš„ä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼æœ€ç»ˆï¼ŒLLM é¢„æµ‹å‡ºä¸€ä¸ªåœæ­¢ tokenï¼ˆä¾‹å¦‚ `<|end_of_text|>` æˆ– `<eos>`ï¼‰æ¥å®Œæˆç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªå®Œæ•´çš„è½¨è¿¹ã€‚

### 1.3 ç­–ç•¥æ¢¯åº¦åŸºç¡€

åœ¨ RL è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°â€”â€”å³ç´¯ç§¯ï¼ˆå¯èƒ½ç»è¿‡æŠ˜æ‰£çš„ï¼‰å¥–åŠ±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ³•ï¼›å…·ä½“æ–¹æ³•å¦‚ä¸‹ã€‚

![|400](https://substackcdn.com/image/fetch/$s_!slrY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3072897-d905-42be-b385-6186c24ae059_2390x302.png)

å°†è¿™ä¸€ç‚¹æ”¾åœ¨ LLM çš„èƒŒæ™¯ä¸‹ï¼ŒRL è®­ç»ƒéµå¾ªä»¥ä¸‹æ­¥éª¤åºåˆ—ã€‚æˆ‘ä»¬é¦–å…ˆé‡‡æ ·ä¸€æ‰¹æç¤ºè¯ï¼Œå¹¶ç”¨ LLM æˆ–ç­–ç•¥ç”Ÿæˆè¿™äº›æç¤ºè¯çš„è¡¥å…¨å†…å®¹ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—è¿™äº›è¡¥å…¨å†…å®¹çš„å¥–åŠ±ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¥–åŠ±æ¥æ¨å¯¼ç­–ç•¥æ›´æ–°ã€‚*è¿™æœ€åçš„ç­–ç•¥æ›´æ–°æ­¥éª¤æ­£æ˜¯ä½¿ç”¨æ¢¯åº¦ä¸Šå‡çš„åœ°æ–¹*ã€‚

![|350](https://substackcdn.com/image/fetch/$s_!yR8D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b7b374-8bee-45fb-b7ee-a26008aa7259_1267x843.png)

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å®Œæˆæƒ…å†µå’Œå¥–åŠ±æ¥ä¼°ç®— RL è®­ç»ƒç›®æ ‡ç›¸å¯¹äºç­–ç•¥å‚æ•°çš„æ¢¯åº¦â€”â€”è¿™è¢«ç§°ä¸ºâ€œ*ç­–ç•¥æ¢¯åº¦*â€ã€‚å¦‚æœæˆ‘ä»¬èƒ½è®¡ç®—å‡ºè¿™ä¸ªæ¢¯åº¦ï¼Œå°±å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸Šå‡æ³•æ¥è®­ç»ƒç­–ç•¥ã€‚ä½†é—®é¢˜æ˜¯ï¼š*æˆ‘ä»¬è¯¥å¦‚ä½•è®¡ç®—è¿™ä¸ªæ¢¯åº¦å‘¢ï¼Ÿ*

> _â€œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸ºæ™ºèƒ½ä½“æ‰¾åˆ°ä¸€ç§æœ€ä¼˜è¡Œä¸ºç­–ç•¥ï¼Œä»¥è·å¾—æœ€ä¼˜å¥–åŠ±ã€‚ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ—¨åœ¨ç›´æ¥å»ºæ¨¡å’Œä¼˜åŒ–ç­–ç•¥ã€‚â€_Â -Â [Lilian Weng](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)

**ç­–ç•¥æ¢¯åº¦**ï¼šå‡ ä¹æ‰€æœ‰ç”¨äº LLM è®­ç»ƒçš„ RL ä¼˜åŒ–å™¨ï¼ˆå¦‚ PPOã€GRPO å’Œ REINFORCEï¼‰éƒ½å±äºç­–ç•¥æ¢¯åº¦ç®—æ³•ã€‚è¿™ç±»ç®—æ³•çš„è¿ä½œåˆ†ä¸ºä¸¤æ­¥ï¼š*i)* ä¼°ç®—ç­–ç•¥æ¢¯åº¦ï¼›*ii)* åŸºäºä¼°ç®—ç»“æœæ‰§è¡Œæ¢¯åº¦ä¸Šå‡ã€‚ä¸åŒç®—æ³•åœ¨ç­–ç•¥æ¢¯åº¦ä¼°ç®—æ–¹æ³•ä¸Šå„æœ‰å·®å¼‚ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³é«˜åº¦ç›¸ä¼¼â€”â€”æˆ‘ä»¬åªéœ€æ ¹æ®å…·ä½“æŠ€æœ¯å¾®è°ƒç»†èŠ‚ã€‚ä¸ºæ·±å…¥ç†è§£ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæˆ‘ä»¬å°†é¦–å…ˆæ¨å¯¼æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦å½¢å¼ï¼Œéšåæ‰©å±•è¿™ä¸€æ€è·¯ï¼Œæ¨å¯¼å‡ºæ›´å¤æ‚çš„ç®—æ³•å¦‚ä¿¡ä»»åŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ã€‚

**Vanilla Policy Gradientï¼ˆVPGï¼‰**ï¼šç®—æ³•å·²è¢«ä¼—å¤šç½‘ç»œèµ„æºè¯¦ç»†é˜è¿°ã€‚å…¶ä»–å…³äº VPG çš„æœ‰ç”¨è§£é‡Šè¿˜åŒ…æ‹¬ï¼š

- OpenAI ç­–ç•¥ä¼˜åŒ–å…¥é—¨ [link](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)
- [Nathan Lambert](https://natolambert.com/) çš„ RLHF ä¹¦ç± [link](https://rlhfbook.com/c/11-policy-gradients.html)
- [Lilian Weng](https://lilianweng.github.io/) çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³• [link](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)

ç„¶è€Œï¼Œä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å°†å†æ¬¡æ¨å¯¼å‡ºä¸€äº›ç®€å•çš„ç­–ç•¥æ¢¯åº¦å½¢å¼ã€‚æ­£å¦‚æˆ‘ä»¬å·²ç»çŸ¥é“çš„ï¼ŒRL çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚å¦‚æœæˆ‘ä»¬å°è¯•è®¡ç®—è¿™ä¸ªç›®æ ‡ç›¸å¯¹äºç­–ç•¥å‚æ•° $Î¸$ çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºä»¥ä¸‹ç»“æœï¼š

![|500](https://substackcdn.com/image/fetch/$s_!GetI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1685ea69-1b2c-438c-87ed-dba51c4bee65_2406x1065.png)

log æ±‚å¯¼æ­¥éª¤ä¸­ï¼Œ$\ln(y)'=\frac{y'}{y}$ï¼Œæ‰€ä»¥ $y'=y \ln'(y)$ï¼Œ([source](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html))

è¿™ä¸ªæ¨å¯¼è¿‡ç¨‹ä» RL è®­ç»ƒç›®æ ‡ï¼ˆç´¯ç§¯å¥–åŠ±ï¼‰çš„æ¢¯åº¦å¼€å§‹ï¼Œæœ€ç»ˆå¾—å‡ºç­–ç•¥æ¢¯åº¦çš„åŸºæœ¬è¡¨è¾¾å¼ã€‚ä¸Šé¢åˆ—ä¸¾äº†æ¨å¯¼è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ­¥éª¤ã€‚è¿™é‡Œå”¯ä¸€å¤æ‚çš„æ­¥éª¤æ˜¯å¯¹æ•°å¯¼æ•°æŠ€å·§çš„ä½¿ç”¨ä»¥åŠæœ€åä¸€æ­¥ï¼Œè¿™ä¸€æ­¥åˆ©ç”¨äº†æˆ‘ä»¬å¯¹è½¨è¿¹æ¦‚ç‡çš„å®šä¹‰ã€‚åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä»£å…¥è½¨è¿¹æ¦‚ç‡çš„å®šä¹‰ï¼Œå¹¶è§‚å¯Ÿåˆ°åˆå§‹çŠ¶æ€æ¦‚ç‡å’Œè½¬ç§»å‡½æ•°ç›¸å¯¹äºç­–ç•¥å‚æ•°çš„æ¢¯åº¦å§‹ç»ˆä¸ºé›¶ï¼Œå› ä¸ºå®ƒä»¬éƒ½ä¸ä¾èµ–äºç­–ç•¥ï¼›è¯¦è§ä¸‹æ–‡ ([source](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html))ã€‚

![|500](https://substackcdn.com/image/fetch/$s_!Rkmm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0f526be-55f2-4eae-abd8-fa4382d8335a_1564x432.png)

**å®ç°åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦**ï¼šåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºçš„åŸºæœ¬ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼æ˜¯ç†è®ºæ€§çš„â€”â€”å®ƒ*æ¶‰åŠæœŸæœ›å€¼*ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦åœ¨å®é™…ä¸­è®¡ç®—è¿™ä¸ªæ¢¯åº¦ï¼Œå°±å¿…é¡»ç”¨æ ·æœ¬å‡å€¼æ¥è¿‘ä¼¼ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é‡‡æ ·å›ºå®šæ•°é‡çš„è½¨è¿¹ï¼ˆå¯¹äº LLM æ¥è¯´ï¼Œå°±æ˜¯æç¤ºå’Œè¡¥å…¨ï¼‰ï¼Œå¹¶å¯¹æ¯ä¸ªè½¨è¿¹çš„ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼å–å¹³å‡å€¼ã€‚åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼åŒ…å«ä¸¤ä¸ªæˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•è®¡ç®—çš„å…³é”®é‡ï¼š

* å¥–åŠ±ç›´æ¥æ¥è‡ªéªŒè¯è€…æˆ–å¥–åŠ±æ¨¡å‹ã€‚
* åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡å¯ä»¥é€šè¿‡ LLM è®¡ç®—å¾—å‡ºï¼ˆå³è¿™äº›åªæ˜¯ LLM è¾“å‡ºçš„ token æ¦‚ç‡ï¼‰ã€‚

ä¸ºäº†ä½¿è®¡ç®—åŸºæœ¬ç­–ç•¥æ¢¯åº¦çš„è¿‡ç¨‹æ›´åŠ å…·ä½“ï¼Œä¸‹é¢æä¾›äº† PyTorch ä¼ªä»£ç çš„é€æ­¥å®ç°ã€‚
<img src="https://substackcdn.com/image/fetch/$s_!PYzF!,w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e4bdafe-cd71-48b7-8a10-abdc895432f7_1920x1076.gif" width="600">

| ä»£ç æ­¥éª¤                              | ç†è®ºæ¨å¯¼ï¼ˆç­–ç•¥æ¢¯åº¦å®šç†ï¼‰                              | å…·ä½“è§£é‡Š                                             |
| --------------------------------- | ----------------------------------------- | ------------------------------------------------ |
| `completions = LLM(prompts)`      | é‡‡æ ·è½¨è¿¹ $\tau \sim \pi_{\theta}$             | è®©å½“å‰ç­–ç•¥ï¼ˆLLMï¼‰æ ¹æ® prompts ç”Ÿæˆæ–‡æœ¬ï¼ˆcompletionsï¼‰ï¼Œè¿™å°±æ˜¯åœ¨é‡‡æ ·è½¨è¿¹ã€‚ |
| `rewards = RM(completions)`       | è®¡ç®—è½¨è¿¹å›æŠ¥ $R(\tau)$                          | RM ä¸ºç”Ÿæˆçš„æ¯æ¡è½¨è¿¹æ‰“åˆ†ï¼Œå¾—åˆ°å›æŠ¥ $R(\tau)$ã€‚                    |
| `token_logp = F.log_softmax(...)` | è®¡ç®— $\log \pi_{\theta}(a_t \mid s_t)$      | è®¡ç®—æ¯ä¸ªç”Ÿæˆæ­¥éª¤ï¼ˆçŠ¶æ€ $s_t$ ä¸‹é€‰æ‹© token $a_t$ï¼‰çš„ç­–ç•¥å¯¹æ•°æ¦‚ç‡ã€‚       |
| `loss = (- token_logp * rewards)` | æ„å»ºæŸå¤± $-\sum_t \log \pi(a_t\|s_t) R(\tau)$ | å°†è´Ÿçš„å¯¹æ•°æ¦‚ç‡ä¸å›æŠ¥ç›¸ä¹˜ï¼ŒæŸå¤±æœ€å°åŒ–ç­‰ä»·äºç­–ç•¥æ¢¯åº¦ä¸Šå‡ã€‚                     |
| `loss.backward()`                 | è®¡ç®—æ¢¯åº¦ $\nabla_{\theta} J$                  | åå‘ä¼ æ’­è‡ªåŠ¨è®¡ç®—è¿‘ä¼¼ç­–ç•¥æ¢¯åº¦çš„æœŸæœ›ã€‚                               |
å…¶ä¸­ï¼Œ `- token_logp * rewards`ï¼Œè¿™æ˜¯æ ¸å¿ƒã€‚å›é¡¾ç†è®ºï¼Œç­–ç•¥æ¢¯åº¦æ˜¯ $\mathbb{E} [ \sum_t \nabla_{\theta} \log \pi (a_t|s_t) \cdot R(\tau) ]$ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å®šä¹‰æŸå¤±å‡½æ•°ï¼Œç„¶åé€šè¿‡æœ€å°åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰æ¥ä¼˜åŒ–ã€‚ç”±äº `token_logp` æ˜¯æˆ‘ä»¬è¦å¢åŠ çš„é‡çš„å¯¹æ•°ï¼Œæ‰€ä»¥åŠ ä¸Šè´Ÿå·å°†å…¶å˜ä¸ºæŸå¤±ã€‚è¿™æ ·ï¼Œ*æœ€å°åŒ–è¿™ä¸ªæŸå¤±å°±ç­‰ä»·äºæœ€å¤§åŒ–æœŸæœ›å›æŠ¥*ï¼ˆæ¢¯åº¦ä¸‹é™å˜ä¸ºäº†æ¢¯åº¦ä¸Šå‡ï¼‰ã€‚

èšåˆæŸå¤±ï¼šä»£ç æä¾›äº†å‡ ç§é€‰é¡¹ï¼Œé€‰é¡¹ 1 æ˜¯æœ€å¸¸è§çš„åšæ³•ä¹‹ä¸€ã€‚å®ƒå…ˆå¯¹æ¯ä¸ªåºåˆ—çš„ token æŸå¤±æ±‚å’Œï¼Œç„¶åé™¤ä»¥æ¯ä¸ªåºåˆ—çš„æœ‰æ•ˆé•¿åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ€åå¯¹æ‰€æœ‰æ‰¹æ¬¡å†…çš„åºåˆ—æ±‚å¹³å‡ã€‚è¿™ç¡®ä¿äº†ä¸åŒé•¿åº¦çš„åºåˆ—å¯¹æŸå¤±çš„è´¡çŒ®æ˜¯å‡è¡¡çš„ã€‚

åœ¨ä¸Šè¿°å®ç°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„çš„ä¸€ä¸ªå…³é”®ç»†èŠ‚æ˜¯ï¼šæˆ‘ä»¬å¹¶éç›´æ¥è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œä½¿å…¶æ¢¯åº¦ç­‰äºç­–ç•¥æ¢¯åº¦ï¼Œç„¶ååˆ©ç”¨ PyTorch çš„è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ï¼ˆé€šè¿‡ `loss.backward()` å®ç°ï¼‰æ¥é—´æ¥è®¡ç®—ç­–ç•¥æ¢¯åº¦ã€‚ç”¨äºè®¡ç®—ç­–ç•¥æ¢¯åº¦çš„å…·ä½“æŸå¤±å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºã€‚

![|400](https://substackcdn.com/image/fetch/$s_!TwP0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4bb2d85-fdea-4cfc-a46b-e6c5f78ff4f4_1613x593.png)

ç†è§£è¿™ä¸€åŒºåˆ«éå¸¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬å°†é€šè¿‡æŸå¤±å‡½æ•°è€Œéç›´æ¥ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼æ¥æ„å»º PPOï¼ˆä»¥åŠTRPOï¼‰ã€‚

**åŸºæœ¬ç­–ç•¥æ¢¯åº¦çš„é—®é¢˜**ï¼šåŸºæœ¬ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼è™½ç„¶ç®€å•ç›´æ¥ï¼Œä½†ä¹Ÿå­˜åœ¨å‡ ä¸ªæ˜¾è‘—é—®é¢˜ï¼š

* *é«˜æ–¹å·®*ï¼šæ¢¯åº¦ä¼°è®¡å¯èƒ½å…·æœ‰é«˜æ–¹å·®ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
* *ä¸ç¨³å®šçš„ç­–ç•¥æ›´æ–°*ï¼šç›®å‰æ²¡æœ‰æœºåˆ¶æ¥é˜²æ­¢ç­–ç•¥å‘ç”Ÿå¯èƒ½ç ´åç¨³å®šçš„å¤§è§„æ¨¡æ›´æ–°ã€‚

ç”±äºæ–¹å·®è¾ƒå¤§ï¼Œå‡†ç¡®ä¼°è®¡ç­–ç•¥æ¢¯åº¦é€šå¸¸éœ€è¦åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­é‡‡æ ·å¤§é‡è½¨è¿¹ï¼Œè¿™åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨ LLM ç”Ÿæˆå¤§é‡è¡¥å…¨ç»“æœï¼Œå¹¶ä¸ºæ‰€æœ‰è¿™äº›è¡¥å…¨è®¡ç®—å¥–åŠ±å’Œ token å¯¹æ•°æ¦‚ç‡ã€‚

æ­¤å¤–ï¼Œè¿™ç§é«˜æ–¹å·®ä¼šå¢åŠ è®­ç»ƒä¸ç¨³å®šçš„é£é™©â€”â€”*å¤§è€Œä¸å‡†ç¡®çš„æ›´æ–°å¯èƒ½ä¼šå¯¹æˆ‘ä»¬çš„ç­–ç•¥é€ æˆé‡å¤§æŸå®³*ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¤§å¤šæ•°ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¸“æ³¨äºå‡å°‘ç­–ç•¥æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®ï¼Œå¹¶åœ¨ç­–ç•¥æ›´æ–°ä¸Šå¼ºåˆ¶æ‰§è¡Œä¿¡ä»»åŒºåŸŸï¼ˆå³é™åˆ¶ç­–ç•¥åœ¨å•æ¬¡æ›´æ–°ä¸­å¯ä»¥æ”¹å˜çš„ç¨‹åº¦ï¼‰ã€‚

> _â€œæŒ‰ç…§è¿™ä¸ªæ¢¯åº¦è¿ˆå‡ºä¸€æ­¥ï¼Œä¼šæŒ‰æ¯”ä¾‹æå‡æ¯ä¸ªåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼Œæ¯”ä¾‹å› å­ä¸º $R(ğœ)$â€”â€”å³è¿„ä»Šä¸ºæ­¢è·å¾—çš„æ‰€æœ‰å¥–åŠ±ä¹‹å’Œ.â€_Â -Â [Spinning up in Deep RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)

**Reward-to-go.**ï¼šä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬åŸºæœ¬çš„ç­–ç•¥æ¢¯åº¦ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬åŸºäºè½¨è¿¹çš„ç´¯ç§¯å¥–åŠ±æ¥å¢åŠ ç»™å®šåŠ¨ä½œçš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå› ä¸ºåœ¨è¯¥åŠ¨ä½œå‘ç”Ÿä¹‹å‰è§‚å¯Ÿåˆ°çš„å¥–åŠ±è€Œå¢åŠ è¯¥åŠ¨ä½œçš„æ¦‚ç‡ã€‚

![|300](https://substackcdn.com/image/fetch/$s_!Ymws!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b14bade-8617-4bfa-9e4a-59811bbe8de7_1374x218.png)

è¿™ä¸€ç®€å•è§‚å¯Ÿä¿ƒæˆäº†"å¥–åŠ±ç´¯è®¡"ç­–ç•¥æ¢¯åº¦çš„è¯ç”Ÿã€‚è¿™ç§æ”¹è¿›çš„ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼ä»…ç”¨åŠ¨ä½œåè§‚å¯Ÿåˆ°çš„å¥–åŠ±æ€»å’Œæ›¿ä»£äº†ç´¯ç§¯å¥–åŠ±ã€‚è¿ç”¨ [EGLP å¼•ç†](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#expected-grad-log-prob-lemma)ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜è¿™ç§å¥–åŠ±ç´¯è®¡å…¬å¼æ˜¯ç­–ç•¥æ¢¯åº¦çš„æ— åä¼°è®¡é‡ã€‚æ­¤å¤–ï¼Œä¸ä¹‹å‰çš„åŸºç¡€ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼ç›¸æ¯”ï¼Œå¥–åŠ±ç´¯è®¡ç­–ç•¥æ¢¯åº¦è¢«è¯æ˜å…·æœ‰æ›´ä½çš„æ–¹å·®ã€‚

![|450](https://substackcdn.com/image/fetch/$s_!s3m9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92c4ac85-74ac-4c12-8d51-c6c9b3bf22ba_2216x460.png)

**Baselines.** ä¸ºäº†è¿›ä¸€æ­¥é™ä½æ–¹å·®ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åœ¨ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼ä¸­æ·»åŠ ä¸€ä¸ªåŸºçº¿ã€‚ä¸å¥–åŠ±ç´¯ç§¯ç­–ç•¥æ¢¯åº¦ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ EGLP å¼•ç†è¯æ˜ï¼Œå¸¦åŸºçº¿çš„ç­–ç•¥æ¢¯åº¦ç‰ˆæœ¬æ˜¯æ— åçš„ï¼Œä¸”å…·æœ‰æ›´ä½çš„æ–¹å·®ã€‚æ ¹æ® EGLP å¼•ç†ï¼Œè¯¥åŸºçº¿å¿…é¡»ä»…ä¾èµ–äºå½“å‰çŠ¶æ€ï¼ˆå¦åˆ™å°†è¿å EGLP å¼•ç†çš„å‡è®¾ï¼Œå¯¼è‡´è¯æ˜ä¸å†æœ‰æ•ˆï¼‰ã€‚

![|400](https://substackcdn.com/image/fetch/$s_!QhBq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4801db8-b3f3-4ec3-9d3f-624b8ffbd550_1774x344.png)

è¿™ä¸ªè¡¨è¾¾å¼ä¸"å¥–åŠ±ç´¯è®¡"ç­–ç•¥æ¢¯åº¦å‡ ä¹å®Œå…¨ç›¸åŒâ€”â€”æˆ‘ä»¬*åªæ˜¯ä»"å¥–åŠ±ç´¯è®¡"é¡¹ä¸­é¢å¤–å‡å»ä¸€ä¸ªåŸºçº¿*ã€‚åœ¨ç­–ç•¥æ¢¯åº¦ä¼°è®¡ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§å¯èƒ½çš„åŸºçº¿é€‰æ‹©ã€‚*ä¸€ä¸ªå¸¸è§çš„åŸºçº¿æ˜¯ä»·å€¼å‡½æ•°ã€‚ä½¿ç”¨ä»·å€¼å‡½æ•°ä½œä¸ºåŸºçº¿ï¼Œå¯ä»¥æ­£å‘å¼ºåŒ–é‚£äº›è·å¾—é«˜äºé¢„æœŸç´¯ç§¯å¥–åŠ±çš„åŠ¨ä½œ*ã€‚

_æ™®é€šç­–ç•¥æ¢¯åº¦ç®—æ³•çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯æ¢¯åº¦æ›´æ–°çš„é«˜æ–¹å·®â€¦â€¦ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œäººä»¬é‡‡ç”¨äº†å„ç§æŠ€æœ¯æ¥å¯¹ä»·å€¼ä¼°è®¡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œè¿™äº›æŠ€æœ¯è¢«ç§°ä¸ºåŸºçº¿ã€‚åŸºçº¿é€šè¿‡å¤šç§æ–¹å¼å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæœ‰æ•ˆåœ°å°†çŠ¶æ€ä»·å€¼ç›¸å¯¹äºåç»­åŠ¨ä½œè¿›è¡Œå½’ä¸€åŒ–ï¼ˆä¾‹å¦‚ä¼˜åŠ¿å‡½æ•°çš„æƒ…å†µï¼Œå³ Q å€¼ä¸çŠ¶æ€ä»·å€¼ä¹‹é—´çš„å·®å€¼ï¼‰ã€‚æœ€ç®€å•çš„åŸºçº¿å½¢å¼åŒ…æ‹¬å¯¹å¥–åŠ±æ‰¹æ¬¡å–å¹³å‡å€¼æˆ–ä½¿ç”¨ç§»åŠ¨å¹³å‡å€¼ã€‚ -Â [RLHF book](https://rlhfbook.com/c/11-policy-gradients.html)_

**é€šç”¨ç­–ç•¥æ¢¯åº¦**ï¼šåœ¨æ–‡çŒ® [3] ä¸­ï¼Œä½œè€…ç”¨ä¸€ä¸ªæ›´é€šç”¨çš„ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼æ€»ç»“äº†è®¡ç®—ç­–ç•¥æ¢¯åº¦çš„å‡ ç§æ–¹æ³•ï¼›

![|550](https://substackcdn.com/image/fetch/$s_!Vl-C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58aa8bae-6778-4ec0-ac53-3f8b8550390f_2137x836.png)

è¿™ä¸ªè¡¨è¾¾ä¸æˆ‘ä»¬ç›®å‰æ‰€è§çš„è¡¨è¾¾å‡ ä¹å®Œå…¨ç›¸åŒã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬å°†å¥–åŠ±é¡¹ $R(ğœ)$ æ›¿æ¢ä¸ºä¸€ä¸ªé€šç”¨çš„ $Î¨_t$ é¡¹ï¼Œå®ƒå¯ä»¥è¢«è®¾ç½®ä¸ºå‡ ç§ä¸åŒçš„è¡¨è¾¾å¼ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

* å°† $Î¨_t$ è®¾ä¸º $R(ğœ)$ ä»¥æ¢å¤æˆ‘ä»¬çš„åŸºæœ¬ç­–ç•¥æ¢¯åº¦è¡¨è¾¾å¼ã€‚
* å°† $Î¨_t$ è®¾ä¸ºæ—¶é—´ $t$ ä¹‹åè·å¾—çš„å¥–åŠ±ï¼Œä»¥æ¢å¤æˆ‘ä»¬ç­–ç•¥æ¢¯åº¦çš„â€œå¥–åŠ±ç´¯ç§¯â€å˜ä½“ã€‚
* å°† $Î¨_t$ è®¾ç½®ä¸ºå¥–åŠ±çš„åŸºçº¿ç‰ˆæœ¬ï¼›ä¾‹å¦‚ï¼Œç´¯ç§¯å¥–åŠ± $R(ğœ)$ ä¸ä»·å€¼å‡½æ•° $V(s_t)$ ä¹‹é—´çš„å·®å€¼ã€‚
* å°† $Î¨_t$ è®¾ä¸ºçŠ¶æ€-åŠ¨ä½œ $Q$ æˆ–ä¼˜åŠ¿å‡½æ•° $A$ã€‚

å°½ç®¡å­˜åœ¨å¤šç§å¯èƒ½çš„è¡¨è¿°æ–¹å¼ï¼Œ*PPOï¼ˆä»¥åŠå‡ ä¹æ‰€æœ‰ç”¨äº LLM é¢†åŸŸçš„ RL ä¼˜åŒ–å™¨ï¼‰éƒ½è‡´åŠ›äºå°† $Î¨_t$ è®¾å®šä¸ºä¼˜åŠ¿å‡½æ•° $A(s_t, a_t)$ã€‚è¿™ä¸€è®¾å®šè¢«ç§°ä¸ºæ ‡å‡†ç­–ç•¥æ¢¯åº¦ï¼ˆVPGï¼‰*ã€‚ç†è®ºä¸Šï¼ŒVPG èƒ½äº§ç”Ÿæ–¹å·®æœ€å°çš„æ¢¯åº¦ä¼°è®¡ã€‚

![|350](https://substackcdn.com/image/fetch/$s_!1PL6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dbd6ad6-4d9e-4085-b4a7-849b29789350_1662x470.png)

å°½ç®¡ VPG çš„æ–¹å·®è¾ƒä½ï¼Œä½†åœ¨ç­–ç•¥æ›´æ–°è¿‡ç¨‹ä¸­ä»ç¼ºä¹å¼ºåˆ¶ä¿¡ä»»åŒºåŸŸçš„æœºåˆ¶â€”â€”*å¤§è§„æ¨¡ç ´åæ€§ç­–ç•¥æ›´æ–°ä»å¯èƒ½ä½¿è®­ç»ƒè¿‡ç¨‹å¤±ç¨³*ã€‚PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰æ­£æ˜¯ä¸ºè§£å†³è¿™ä¸€é—®é¢˜è€Œè¯ç”Ÿã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼ŒPPO è™½ç„¶æ²¿ç”¨äº†åŸºç¡€ç­–ç•¥æ¢¯åº¦çš„è¡¨è¾¾å¼ï¼Œä½†é¢å¤–å¢åŠ äº†å¯¹ç­–ç•¥æ›´æ–°æ–½åŠ ä¿¡ä»»åŒºåŸŸçš„æœºåˆ¶ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ PPO åŠå…¶å®ç°è¿‡ç¨‹ä¸­æ¶‰åŠçš„è¯¸å¤šå®è·µç»†èŠ‚ã€‚

## äºŒã€è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)

æ—¢ç„¶æˆ‘ä»¬å·²ç»ç†è§£äº† RL çš„åŸºç¡€çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹  PPOã€‚è¿™ä¸€éƒ¨åˆ†çš„è®²è§£å°†åŸºäºæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­æ¨å¯¼å‡ºçš„ VPG è¡¨è¾¾å¼ï¼Œä» PPO çš„å‰èº«â€”â€”ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰å¼€å§‹ã€‚TRPO åœ¨ç¨³å®šè®­ç»ƒæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ä¹Ÿç›¸å¯¹å¤æ‚ã€‚PPO ä½œä¸ºä¸€ç§æ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆè¢«æå‡ºï¼Œå…·æœ‰ç±»ä¼¼çš„ä¼˜åŠ¿ã€‚åœ¨æœ¬èŠ‚çš„æœ€åï¼Œæˆ‘ä»¬è¿˜å°†ä»‹ç»å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰ï¼Œè¿™æ˜¯ PPO ä¸­è®¡ç®—ä¼˜åŠ¿å‡½æ•°æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚

#### [Trust Region Policy Optimization (TRPO)](https://arxiv.org/abs/1502.05477)Â [6]

> _â€œTRPO uses a hard constraint rather than a penalty because it is hard to choose a single value of Î² that performs well across different problemsâ€”or even within a single problem, where the characteristics change over the course of learning.â€_Â - from [1]

Prior to learning about PPO, we need to take a look at its predecessor, Trust Region Policy Optimization (TRPO) [6]. The key motivation behind TRPO is creating an algorithm that is data efficient and does not require too much hyperparameter tuning. To do this, authors in [6] propose the constrained objective below,Â _which is guaranteed to monotonically improve our policy_. This objective enforces a trust region on the policy update, thus eliminating the risk of large and destructive policy updates that could destabilize training.

[

![](https://substackcdn.com/image/fetch/$s_!x5A5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a9c1514-c3dd-4692-bb7a-d63644987d5e_1784x940.png)



](https://substackcdn.com/image/fetch/$s_!x5A5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a9c1514-c3dd-4692-bb7a-d63644987d5e_1784x940.png)

Surrogate objective for TRPO (from [1])

**Surrogate objective.**Â This objective shown above is called the surrogate objective in TRPO. This naming stems from the fact that the surrogate objective is different from the standard RL training objective. In RL, we aim to maximize cumulative reward, butâ€”_as we have seen in our discussion of the VPG_â€”directly maximizing this â€œtrueâ€ objective of RL can lead to training instability. TRPO formulates the surrogate objective to maximize in place of the true objective.

There are a few noticeable differences between the above expression for TRPO and the VPG:

- Action probabilities in the current policy are normalized by the probability of that action in the old policy (i.e., the policy prior to training)â€”_this forms the policy ratio (also called an importance ratio)_. We also use probabilities in this formulation instead of log probabilities.
    
- There is a constraint placed on the objective to ensure that the expected KL divergence between the new and old policies is less than a thresholdÂ `Î´`.
    

Otherwise, the TRPO loss function shares a similar structure to that of VPGâ€”_it includes the advantage function and a sum over token-level probabilities in a trajectory_.

**Policy ratio.**Â The centerpiece of the TRPO loss function is the policy ratio, defined as shown below. The policy ratio tells us how much more likely a given action is in our current policy relative to the probability of that action before the training process startedâ€”_this is denoted as the â€œoldâ€ policy_.

[

![](https://substackcdn.com/image/fetch/$s_!IXsZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a7d1530-a2cc-48c6-9e95-8571b781ba35_1994x792.png)



](https://substackcdn.com/image/fetch/$s_!IXsZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a7d1530-a2cc-48c6-9e95-8571b781ba35_1994x792.png)

The policy (or importance) ratio

This quantity serves the purpose of assigning an importance to different actions within our trajectory. If the new policy assigns a higher probability to an action than the old policy did, this ratio is greater than one, increasing the influence of that actionâ€™s advantage in the objective. Conversely, if the new policy assigns a lower probability, the ratio is less than one, reducing the influence of that action. The policy ratio ensures that the policy update emphasizes actions that the new policy is making more likelyâ€”_especially if those actions have high advantage_â€”while suppressing actions that are becoming less likely under the new policy. By doing this, we ensure that the update is properly weighted according to how the new policy differs from the old, enabling stable and efficient policy improvement.

**Solving the surrogate objective.**Â Although this objective yields stable policy updates, solving it can be quite involved. By introducing an explicit constraint into our objective, we eliminate the ability to solve this objective with simple gradient ascent[3](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-3-175107358). Instead, we have to solve this objective via the more complexÂ [conjugate gradient algorithm](https://en.wikipedia.org/wiki/Conjugate_gradient_method). Alternatively, we could remove this constraint and instead add the KL divergence as a penalty into our loss function; see below. This unconstrained loss is simpler and can again be solved with basic gradient ascent.

[

![](https://substackcdn.com/image/fetch/$s_!fFIz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F301f1d55-7e7c-4c2f-8138-67a3bc162338_1872x388.png)



](https://substackcdn.com/image/fetch/$s_!fFIz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F301f1d55-7e7c-4c2f-8138-67a3bc162338_1872x388.png)

The penalty objective for TRPO

**From TRPO to PPO.**Â Formulating the constraint from TRPO as a penalty allows us to avoid complicated optimization techniques and rely upon basic gradient ascent. However, a new hyperparameter Î² is introduced to the optimization process that makes tuning difficult. Properly setting the value of Î² is essential for this objective to perform well, and finding a single value of Î² that generalizes to many domains is hard. As a result, both of the above objectives have their issues:

- The TRPO surrogate objective is too complex to solve in practice.
    
- The reformulated penalty objective is sensitive to the setting of Î².
    

We want to develop an algorithm that retains the benefits of TRPOâ€”_such as stability, data efficiency, and reliability_â€”while avoiding its complexity. Ideally, the algorithm should be broadly applicable and solvable using basic gradient ascent. These goals led to the proposal of PPO, which is largely inspired by TRPO. PPOâ€™s objective is inspired by the TRPO surrogate objective but replaces the hard KL constraint with a clipping mechanism to enforce a trust region in a simpler way.

#### [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)Â [1]

> _â€œWe propose a new family of policy gradient methods for RL, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent.â€_Â - from [1]

The VPG is simple to compute in practice, but it has poor data efficiency (i.e., the model must be trained over many samples to perform well) and high variance in the policy updates. These problems are largely solved by TRPO but at the cost of significant added complexity. PPO is an algorithm with the data efficiency and reliability benefits of TRPO that is still solvable with gradient ascent. In this way, PPO is a simpler algorithm compared to TRPO. As we will see, however,Â _PPO is still a complex algorithm with many implementation complexities of its own_.

[

![](https://substackcdn.com/image/fetch/$s_!S1nc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc38f9ea3-d07f-4240-898e-de3c75e66878_2264x786.png)



](https://substackcdn.com/image/fetch/$s_!S1nc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc38f9ea3-d07f-4240-898e-de3c75e66878_2264x786.png)

Update procedure in PPO (from [1])

**Training process.**Â Similarly to TRPO, PPO focuses upon optimizing a surrogate objective, but the objective in PPO has no constraint and has been slightly modified. As shown in the algorithm above, PPO performs more than a single policy update in each step, instead alternating between:

1. Sampling new data or trajectories from the policy.
    
2. Performing several epochs of optimization on the sampled data.
    

**The PPO surrogate objective**Â is again based upon the policy ratio between the current policy and the old model (i.e., the policy before any training is performed). To match notation in [1], we will denote the policy ratio asÂ `r_t(Î¸)`, which is similar to theÂ `r_t`Â notation used for the reward for time stepÂ `t`. However,Â _the policy ratio is unrelated to the reward_! To obtain the PPO objective, we start with the surrogate objective being maximized by TRPO with no KL constraint; see below.

[

![](https://substackcdn.com/image/fetch/$s_!fqSm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80447ac5-6fd2-4cbb-b33c-a4e385e7fc2c_1390x478.png)



](https://substackcdn.com/image/fetch/$s_!fqSm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80447ac5-6fd2-4cbb-b33c-a4e385e7fc2c_1390x478.png)

The unclipped PPO objective

We will call this formulation the â€œunclippedâ€ objective. Because it does not have a constraint, this objective can be easily computed to derive the policy gradient byÂ _i)_Â estimating the advantage andÂ _ii)_Â computing the policy ratio. However, if we try to maximize this unconstrained objective, this will potentially lead to large and destructive policy gradient updates that make the training process unstable. To solve this issue, PPO introduces a novel clipping mechanism into the surrogate objective that helps us with maintaining the trust region; see below.

[

![](https://substackcdn.com/image/fetch/$s_!oHJG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6be9f2-f165-4e48-be0c-e63074454d2a_2003x338.png)



](https://substackcdn.com/image/fetch/$s_!oHJG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6be9f2-f165-4e48-be0c-e63074454d2a_2003x338.png)

The PPO surrogate objective

The main term in the objective is unchanged, but there is an added term with a clipped version of the policy ratioâ€”_the policy ratio must fall in the range_Â `[1 - Îµ, 1 + Îµ]`[4](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-4-175107358). The clipping term disincentivizes the RL training process from moving the policy ratio away from a value of one. The PPO surrogate objective takes the minimum of clipped and unclipped objectives. In this way,Â _the PPO objective is a pessimistic (lower) bound for the original, unclipped objective_[5](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-5-175107358).

[

![](https://substackcdn.com/image/fetch/$s_!ovlv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38769a7f-6549-4fed-ab3e-f829185b5069_1544x642.png)



](https://substackcdn.com/image/fetch/$s_!ovlv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38769a7f-6549-4fed-ab3e-f829185b5069_1544x642.png)

(from [1])

Depending upon whether the advantage is positive or negative, the behavior of clipping is slightly different; see above. The use of a minimum in the surrogate objective causes clipping to be applied in only one direction. In particular, we can arbitrarilyÂ _decrease_Â surrogate objective by moving the policy ratio far away from a value of one, but clipping prevents arbitrarilyÂ _increasing_Â the objective via the policy ratio. In this way, PPO de-incentivize large policy ratios so that our policy does not deviate too much from the old policy after training updates.

> _â€œWith this scheme, we only ignore the change in probability ratio when it would make the objective improve, and we include it when it makes the objective worse.â€_Â - from [1]

To more deeply understand the clipping logic of PPO, we can consider each of the four possible cases that can arise when optimizing the surrogate objective:

- Case #1 [`A > 0`,Â `r_t(Î¸) â‰¤ 1 + Îµ`]: advantage is positiveâ€”_this is an action that we want to reinforce_. Our policy ratio is belowÂ `1 + Îµ`, so we perform a normal policy gradient update to increase the probability of this action.
    
- Case #2 [`A > 0`,Â `r_t(Î¸) > 1 + Îµ`]: advantage is positive again, but our policy ratio is greater thanÂ `1 + Îµ`. This means that this action is already more likely in the new policy relative to the old policy. The objective gets clipped, and the gradient with respect to further increases in the policy ratio is zero. This prevents the policy from making the action even more likely
    
- Case #3 [`A < 0`,Â `r_t(Î¸) â‰¥ 1 - Îµ`]: advantage is negativeâ€”_this is an action we want to negatively reinforce (i.e., decrease probability)_. Our policy ratio is aboveÂ `1 - Îµ`, so we perform a normal policy gradient update to decrease the probability of this action.
    
- Case #4 [`A < 0`,Â `r_t(Î¸) < 1 - Îµ`]: advantage is negative again, but our policy ratio is less thanÂ `1 - Îµ`. This means that this action is already less likely in the new policy relative to the old policy. The objective gets clipped, and the gradient with respect to further decreases in the policy ratio is zero. This prevents the policy from making the action even less likely.
    

The policy ratio is computed between the current and old policies. The old policy is updated to match the current policy each time new data is sampled in PPO. In the context of LLMs, we perform 2-4 gradient updates (or sometimes more) [2] for each batch of data,Â _so_Â _the old model is updated frequently_. The clipping operation in PPO, therefore, maintains a trust region for a particular batch of data.

**KL divergence.**Â When training LLMs with PPO, we usually incorporate the KL divergence between the current policy and a reference policyâ€”_usually some policy from before RL training begins (e.g., the SFT model)_â€”into the training process. This added KL divergence term penalizes the policy from becoming too different from the reference policy, which has a regularizing effect. We compute KL divergence per token by comparing the token probability distributions outputted by the two LLMs for each token within the sequence. Details on how exactly the KL divergence is computed in practice can be foundÂ [here](https://cameronrwolfe.substack.com/i/167254905/kullback-leibler-kl-divergence).

[

![](https://substackcdn.com/image/fetch/$s_!MMrI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc3d5004-2390-489f-995a-e0245c174535_2534x530.png)



](https://substackcdn.com/image/fetch/$s_!MMrI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc3d5004-2390-489f-995a-e0245c174535_2534x530.png)

Incorporating KL divergence into the reward

There are two common ways of adding the KL divergence into PPO training. First, we can directly subtract the KL divergence from the reward in RL; see above. Alternatively, we can add the KL divergence as a penalty term to the RL training objective as shown below. In both cases, we simply want to maximize rewards without making our new policy too different from the reference.

[

![](https://substackcdn.com/image/fetch/$s_!kyeM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7464e10-d669-4f6b-ab83-f1980b8918d4_2416x436.png)



](https://substackcdn.com/image/fetch/$s_!kyeM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7464e10-d669-4f6b-ab83-f1980b8918d4_2416x436.png)

Incorporating a KL penalty into the RL training objective

Such a KL divergence term is almost universally used in RL training for LLMs, though the exact implementation varies. Both of the approaches outlined above have been used successfully. However, capturing the KL divergence via a penalty term in the training objective is probably more common (and a bit simpler).

**The critic.**Â Recall that the advantage function is defined as the difference between the state-action value function and the value function. In PPO, we estimate the state-action value functionâ€”_the expected reward for taking a specific action in a given state_â€”by using the actual reward observed for a trajectory. The value function, in contrast, is typically estimated using a learned model; see below.

[

![](https://substackcdn.com/image/fetch/$s_!noKQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55141cda-9010-48ea-ba62-5cd56e9bd814_1772x629.png)



](https://substackcdn.com/image/fetch/$s_!noKQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55141cda-9010-48ea-ba62-5cd56e9bd814_1772x629.png)

For example, we can create a separate copy of our policy, orâ€”_for better parameter efficiency_â€”add a dedicated value head that shares weights with the policy to predict the value function. This learned value function is often referred to as a value model or critic. Taking a partial response as input, the critic predicts the expected final reward for every token position within the sequence; see below.

**Critic versus reward model.**Â In the context of LLMs, we predict the reward with a reward model. Additionally, most LLMs are trained using outcome supervision, meaning that a reward is only assigned after the model has generated a complete response (i.e., after theÂ `<eos>`Â token has been outputted). The critic and reward model are similar in that they are both learned modelsâ€”_usually another copy of our LLM policy_â€”that predict rewards. However, the critic predicts expected rewards given a partial completion as input, while the reward model typically predicts the reward received by an entire response; see below. Going further, the reward model is fixed throughout RL training, while the critic is continually updated.

[

![](https://substackcdn.com/image/fetch/$s_!fXOv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb8133ba-f772-44f5-bfbc-19e800a842cc_1732x570.png)



](https://substackcdn.com/image/fetch/$s_!fXOv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb8133ba-f772-44f5-bfbc-19e800a842cc_1732x570.png)

Value model versus reward model

**Critic training.**Â The value function is on-policyâ€”_it is dependent upon the current parameters of our policy_. UnlikeÂ [reward models](https://cameronrwolfe.substack.com/p/reward-models)Â which are fixed at the beginning of RL training, the critic is trained alongside the LLM in each policy update to ensure that its predictions remain on-policyâ€”_this is called an actor-critic setup_[6](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-6-175107358). This is accomplished by adding an extraÂ [mean-squared error (MSE) loss](https://en.wikipedia.org/wiki/Mean_squared_error)â€”_between the rewards predicted by the critic and actual rewards_â€”to the surrogate loss.

**PPO implementation.**Â To make each of these ideas more complete, we have implemented PPO in PyTorch pseudocode below. In this implementation, we see several of the key ideas we have discussed so far, such as:

- Computing the KL divergence between the current policy and a reference model, then directly subtracting this KL divergence from our reward.
    
- Using a learned critic to compute the advantage (and training this critic via an MSE loss alongside the policy itself).
    
- Computing the policy ratio with respect to the old model. The script below performs a single policy update, but PPO usually performs several (i.e., 2-4 in the case of LLMs [2]) policy updates for each batch of data. The â€œoldâ€ model in the policy ratio is the model from before the first update for a batch.
    
- Computing the full (clipped) PPO loss. We take the negative of this loss because PyTorch performs gradient descent (not ascent) by default.
    
- Aggregating or averaging the token-level PPO loss across a batch of sequences. There are many ways to aggregate the loss in a batch, and the approach used can significantly impact results [2][7](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-7-175107358).
    

One interesting detail we see here is thatâ€”_despite the PPO loss using token probabilities and not log probabilities_â€”we choose to work with token log probabilities and exponentiate them instead of using raw probabilities when computing the policy ratio. This is a commonly-used numerical stability trick.

```
import torch
import torch.nn.functional as F

# constants
kl_beta = 0.1
critic_weight = 0.5
ppo_eps = 0.2

# sample prompt completions and rewards
with torch.no_grad():
    completions = LLM.generate(prompts)  # (B*G, L)
    rewards = RM(completions)  # (B*G, 1)

# create a padding mask from lengths of completions in batch
completion_mask = <... mask out padding tokens ...>

# compute value function / critic output
values = CRITIC(completions)  # (B*G, L) - predicted reward per token!

# get policy logprobs for each action
llm_out = LLM(completions)
per_token_logps = F.log_softmax(llm_out, dim=-1)  # (B*G, L)

# get reference logprobs for each action
ref_out = REF(completions)
ref_per_token_logps = F.log_softmax(ref_out, dim=-1)  # (B*G, L)

# compute KL divergence between policy and reference policy
kl_div = per_token_logps - ref_per_token_logps

# directly subtract KL divergence from rewards
# NOTE: KL div is per token, so reward becomes per token and reward
# for all tokens (besides last token) is just kl divergence.
# Reward for last token is sum of outcome reward and KL div.
rewards -= kl_beta * kl_div # (B*G, L)

# compute the advantage - simple approach
advantage = rewards - values.detach()  # (B*G, L)

# compute the policy ratio
# NOTE: old_per_token_logps must be persisted during first policy
# update for this batch of data and re-used in each subsequent update
policy_ratio = torch.exp(
    per_token_logps - old_per_token_logps,
)  # (B*G, L)
clip_policy_ratio = torch.clamp(
    policy_ratio,
    min=1.0 - ppo_eps,
    max=1.0 + ppo_eps,
)

# compute the ppo loss
ppo_loss = torch.min(
    advantage * policy_ratio,
    advantage * clip_policy_ratio,
)  # (B*G, L)
ppo_loss = -ppo_loss

# combine ppo loss and critic mse loss
critic_loss = ((rewards - values) ** 2)  # (B*G, L)
loss = ppo_loss + critic_weight * critic_loss

# aggregate the loss across tokens (many options exist here)
loss = ((loss * completion_mask).sum(axis=-1) /
        completion_mask.sum(axis=-1)).mean()

# perform policy gradient update
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

**Experiments.**Â The LLM setting is not considered in [1], as PPO was proposed during the heyday ofÂ [DeepRL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)â€”_well before the proliferation of LLMs_. Understanding the experimental results in [1] is nonetheless useful for gaining intuition on the mechanics of PPO. In these experiments, PPO is used to train fully-connectedÂ [multi-layer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron)Â (MLPs) from scratch on a variety of robotics and video game tasks. The policy and critic are kept separate (i.e., no parameter sharing).

First, authors use several simulated robotics tasks from theÂ [OpenAI Gym](https://github.com/Farama-Foundation/Gymnasium)Â to test different formulations of the surrogate loss in PPO:

- The clipped objective (standard for PPO).
    
- The unclipped objective.
    
- The unclipped objective with (adaptive[8](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-8-175107358)) KL divergence.
    

Unlike the typical RL training setup for LLMs, these experiments compute the KL divergence between the current policy and the old model, with the goal of testing whether this approach works better than the standard PPO clipping mechanism. Ordinarily, when training LLMs with PPO, the KL divergence is computed between the current policy and a reference model (e.g., the SFT model), not the old model[9](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-9-175107358). However, in these experiments, using a reference model for the KL divergence is not possible because we are training models from scratchâ€”_there is no pretrained model to serve as a reference_.

The results from testing these different objectives are outlined belowâ€”_the clipped objective for PPO stabilizes training and clearly outperforms the other options_.

[

![](https://substackcdn.com/image/fetch/$s_!CHQh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1cc9a21-11e9-4c34-8d72-0576cde83e94_2086x894.png)



](https://substackcdn.com/image/fetch/$s_!CHQh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1cc9a21-11e9-4c34-8d72-0576cde83e94_2086x894.png)

(from [1])

PPO is also tested on 49 games in theÂ [Atari gameplay domain](https://arxiv.org/abs/1207.4708)Â and compared to strong baseline RL algorithms likeÂ [A2C](https://arxiv.org/abs/1602.01783)Â andÂ [ACER](https://arxiv.org/abs/1611.01224). Performance is measured based on two metrics:

1. Average reward throughout training (favors faster learning).
    
2. Average reward over the last 100 training steps (favors final quality / reward).
    

For each of these metrics, we compute a â€œwin rateâ€, which captures the number of times each algorithm achieves the top score across all Atari games. The results of these experiments are shown below, where we see that baseline algorithms like ACER perform similarly to or better than PPO but learn much slower.Â _PPO stabilizes training, performs well, and yields an improvement in sample complexity_[10](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-10-175107358).

[

![](https://substackcdn.com/image/fetch/$s_!SgN4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79fdf5d-6d9e-4f9c-b87e-885fe063de66_1814x499.png)



](https://substackcdn.com/image/fetch/$s_!SgN4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79fdf5d-6d9e-4f9c-b87e-885fe063de66_1814x499.png)

(from [1])

#### [Generalized Advantage Estimation (GAE)](https://arxiv.org/abs/1506.02438)Â [3]

The advantage tells us how much better a given action is compared to the average action in a given state:Â `A(s_t, a_t) = Q(s_t, a_t) - V(s_t)`. The value function in this formulation is estimated by our critic, but we have not yet discussed in detail how the advantage function can be computed. In PPO, the advantage function is estimated on a per-token (or action) basis. There are two main approaches that can be used to compute the advantage, and these approaches form the basis for most other techniques.

**(1) Monte Carlo (MC).**Â An MC estimate of the advantage relies upon the actual reward observed for the full trajectory. Namely, the advantage is computed as the difference between the cumulative reward for the full trajectoryÂ `R(s_t)`[11](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-11-175107358)Â and the value function for the current stateÂ `V(s_t)`, as predicted by the critic.

So far, our discussions of PPO have assumed an MC approach for estimating the advantage. The MC estimate has low bias because it relies on the actual reward observed for the trajectory (exact information), but MC estimates also have high variance. Therefore, we need to take many samples and make a sufficient number of observations to yield an accurate advantage estimateâ€”_this can be expensive_.

**(2) Temporal Difference (TD).**Â The TD residual uses per-token value predictions from the critic to form a one-step estimate of the advantage, as shown below.

[

![](https://substackcdn.com/image/fetch/$s_!A4K-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c1e98c7-da70-4da6-a365-3b2fe9cd2230_1723x896.png)



](https://substackcdn.com/image/fetch/$s_!A4K-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c1e98c7-da70-4da6-a365-3b2fe9cd2230_1723x896.png)

Temporal difference (TD) residual

This TD residual analyzes how much the expected reward changes after predicting a single token and observing the actual reward for that action[12](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-12-175107358). We subtract the value for the current stateÂ `V(s_t)`Â from the sum of:

1. The observed reward for the current stateÂ `r_t`.
    
2. The (discounted) value of the next stateÂ `V(s_{t+1})`.
    

Similarly toÂ `V(s_t)`, the sum of these two terms captures the expected return at stateÂ `s_t`. However, the reward for the current state is captured via the actual observed rewardÂ `r_t`Â rather than being estimated by the critic. Therefore, the difference between these terms is capturing how much better the actual reward observed at stateÂ `s_t`Â is than expectedâ€”_this is the advantage_!

By using the actual rewardÂ `r_t`, we incorporate some exact information into our advantage estimateâ€”_the terms in the estimate come partly from our critic and partly from real rewards_. Using such token-level rewards to estimate the advantage lowers the variance of the policy gradient. If our value function were exact, then the TD residual would also form an unbiased advantage estimate. Unfortunately, we do not have access to the ground truth value function, so we train a critic to estimate the value function[13](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-13-175107358). Because accurately anticipating final rewards from a partial response is difficult,Â _the TD residual is biased._

**N-step estimators.**Â The TD residual analyzes the difference between actual and expected reward for a single step. However, we can generalize this idea to capture any number of steps. As shown below, anÂ `N`-step advantage estimator has a similar structure to the TD residual, but it incorporates real rewards forÂ `N`Â states, whereÂ `N`Â can be greater than one.

[

![](https://substackcdn.com/image/fetch/$s_!_U8s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ae75ed-997b-4654-b383-dda56a8d9b2e_2298x716.png)



](https://substackcdn.com/image/fetch/$s_!_U8s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ae75ed-997b-4654-b383-dda56a8d9b2e_2298x716.png)

`N`-step advantage estimators

Similarly to the single-step TD residual, advantage estimators with lower values ofÂ `N`Â have low variance but high bias. As we increase the value ofÂ `N`, however, we are incorporating more exact reward information into the advantage estimate, thus lowering the bias (and, in turn, increasing variance).

Taking this further, we can even recover an MC estimate by settingÂ `N`Â equal to the total number of steps in the trajectory! This setting ofÂ `N`Â simply yields the difference between cumulative reward and the value of the current stateÂ `V(s_t)`. Therefore, different settings ofÂ `N`Â yield different tradeoffs in bias and variance, spanning all the way from the single-step TD residual (high bias, low variance) to an MC estimate (high variance, low bias).

_â€œGAE is an alternate method to compute the advantage for policy gradient algorithms that better balances the bias-variance tradeoff. Traditional single-step advantage estimates can introduce too much bias, while using complete trajectories often suffer from high variance. GAE works by combining two ideas â€“ multi-step prediction and weighted running average (or just one of these).â€ - from [2]_

**Generalized Advantage Estimation (GAE)**, which is the most commonly-used approach for estimating the advantage with PPO, makes use ofÂ `N`-step advantage estimates. Instead of choosing a single value ofÂ `N`, however, GAE uses all values ofÂ `N`Â by taking an average ofÂ `N`-step advantage estimates with different values ofÂ `N`. This is done by introducing a mixing parameterÂ `Î»`Â for GAE as shown below[14](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-14-175107358).

[

![](https://substackcdn.com/image/fetch/$s_!v3wn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff11ed641-c3be-442a-ad17-b41072a721a8_2015x843.png)



](https://substackcdn.com/image/fetch/$s_!v3wn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff11ed641-c3be-442a-ad17-b41072a721a8_2015x843.png)

GAE formulation

In this formulation, settingÂ `Î» = 0`Â yields a single-step TD residual because only the first term in the sum receives a non-zero weight. Additionally, a setting ofÂ `Î» = 1`Â recovers the MC estimate. To see this, we can expand the definition of each TD residual in the sum, yielding the difference in cumulative discounted rewards and the value function of the current stateÂ `V(s_t)`; see below.

[

![](https://substackcdn.com/image/fetch/$s_!DRfY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc295ca-a904-4885-85b2-59968c744cc0_2872x674.png)



](https://substackcdn.com/image/fetch/$s_!DRfY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc295ca-a904-4885-85b2-59968c744cc0_2872x674.png)

The benefit of GAE is that the value ofÂ `Î» âˆˆ [0, 1]`Â controls the bias variance tradeoff. As we increase the value ofÂ `Î»`, more exact reward information is used in the advantage estimate, thus lowering the bias (but increasing variance). Similarly, we can use lower values ofÂ `Î»`Â to reduce variance at the cost of higher bias.

**Outcome rewards.**Â When we are working with LLMs, we usually use an outcome reward setup, which simplifies GAE. The reward is always zero[15](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-15-175107358), unless we are at the final step of the trajectory. In this scenario, most of the TD residual terms in our GAE summation are simply the difference in (discounted) value functions between two time stepsÂ `Î³V(s_{t + 1}) - V(s_t)`. The final term in the summation contains the actual outcome reward observed for the trajectory.

**GAE implementation.**Â To make the concept of GAE more concrete, letâ€™s examine a real-world example adapted from AI2â€™sÂ [OpenInstruct](https://github.com/allenai/open-instruct)Â library. The full PPO training script, availableÂ [here](https://github.com/allenai/open-instruct/blob/main/open_instruct/ppo2.py), is a great resource for learning the details of PPO in a production-grade training setting. The GAE component of this script is shown below with some additional comments for clarity. We can efficiently compute the GAE recursion by iterating through the sequence in reverse order.

```
import torch

# store advantages in reverse order while iterating thru sequence
advantages_reversed = []

# iterate backward to compute GAE recursion
lastgaelam = 0
gen_length = responses.shape[1]
for t in reversed(range(gen_length)):
    if t < gen_length - 1:
        # get value model prediction for time t + 1
        nextvalues = values[:, t + 1]
    else:
        # no values predicted beyond end of sequence
        nextvalues = 0.0

    # compute TD residual at time t    
    delta = rewards[:, t] + gamma * nextvalues - values[:, t]

    # add to the discounted sum of TD residuals for GAE    
    lastgaelam = delta + gamma * lam * lastgaelam

    # store the advantage for step t in our list
    advantages_reversed.append(lastgaelam)

# put the list of advantages in the correct order
advantages = torch.stack(advantages_reversed[::-1], axis=1)
```

## Using PPO for LLMs

[

![](https://substackcdn.com/image/fetch/$s_!CJn6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fd3791-df29-4a92-b185-21f6be4f2ddc_2176x642.png)



](https://substackcdn.com/image/fetch/$s_!CJn6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fd3791-df29-4a92-b185-21f6be4f2ddc_2176x642.png)

(from [7])

There are two different types of RL training that are commonly used to train LLMs (shown above):

- _[Reinforcement Learning from Human Feedback (RLHF)](https://cameronrwolfe.substack.com/p/the-story-of-rlhf-origins-motivations)_Â trains the LLM using RL with rewards derived from a human preferenceÂ [reward model](https://cameronrwolfe.substack.com/p/reward-models).
    
- _[Reinforcement Learning with Verifiable Rewards (RLVR)](https://cameronrwolfe.substack.com/i/153722335/reinforcement-learning-with-verifiable-rewards)_Â trains the LLM using RL with rewards derived from rules-based or deterministic verifiers.
    

These RL training techniques differ mainly in how they derive the reward for training, but other details of the algorithms are mostly similar. As depicted below, they both operate by generating completions over a set of prompts, computing the reward for these completions, and using the rewards to derive aÂ [policy update](https://cameronrwolfe.substack.com/p/policy-gradients-the-foundation-of)â€”_or an update to the LLMâ€™s parameters_â€”with an RL optimizer (e.g., PPO).

[

![[animate output image]](https://substackcdn.com/image/fetch/$s_!uPv8!,w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56eba05c-359c-400d-920f-38a36dd4690a_1920x1078.gif "[animate output image]")



](https://substackcdn.com/image/fetch/$s_!uPv8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56eba05c-359c-400d-920f-38a36dd4690a_1920x1078.gif)

Visual walkthrough of RL training for LLMs

RLHF was the original form of RL explored by LLMs like InstructGPT [8], the predecessor to ChatGPT. Early research on RLHF for LLMs used PPO as the default RL optimizer, which ultimately made PPO a standard choice for training LLMs with RL. RLVR was introducedÂ [more recently](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models), and most works in this space useÂ [GRPO](https://arxiv.org/abs/2402.03300)Â as the underlying RL optimizer instead of PPO.

> _â€œPPO has been positioned as the canonical method for RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning.â€_Â - from [9]

**Downsides of PPO.**Â Though it quickly became the default RL optimizer for RLHF, PPO is a complex actor-critic algorithm with high compute and memory overhead, as well as many low-level implementation complexities. The memory overhead of PPO is high because we keep four copies of the LLM in memory:

1. The policy.
    
2. The reference policy.
    
3. The critic.
    
4. The reward model (if we are using a reward model).
    

Additionally, we are updating the parameters of our critic alongside the policy itself and running inference for all of these models simultaneously, leading to high compute costs. Beyond memory and compute overhead, there are also many implementation details that we must carefully consider during PPO training:

- How do we initialize the critic and reward model? What training settings should we adopt for these models?
    
- What value ofÂ `Îµ`Â should we use for clipping in PPO?
    
- Which model should we use as our reference model for the KL divergence?
    
- How many policy updates should we perform for a batch of data?
    
- Do we add the KL divergence as a penalty to the loss or directly incorporate it into the reward function? What scaling factorÂ `Î²`Â should we use?
    
- How should we weight the criticâ€™s loss relative to the main PPO loss?
    
- Should we use GAE? What setting should we use forÂ `Î»`?
    

Each of these choices may impact the results of RL training! PPO is a sensitive algorithm that is prone to instabilityâ€”_we may spend a lot of compute and time on training a model that ultimately performs poorly due to an incorrect hyperparameter setting_. For these reasons, simpler RL algorithms likeÂ [REINFORCE](https://cameronrwolfe.substack.com/p/reinforce)Â andÂ [GRPO](https://arxiv.org/abs/2402.03300)â€”_or even RL-free techniques likeÂ [DPO](https://cameronrwolfe.substack.com/p/direct-preference-optimization)_â€”have become popular alternatives to PPO.

**PPO for LLMs.**Â In this final section, we will take what we have learned and study PPO specifically in the context of LLM training. We will focus particularly on the foundational works that were the first to use PPO for training LLMs [5, 8]â€”_this research laid the groundwork for the modern LLM boom shortly after_. While studying these papers, we will emphasize implementation details and practical lessons that are necessary to obtain a working PPO implementation.

#### **[Learning to Summarize from Human Feedback](https://arxiv.org/abs/2009.01325)Â [5]**

Abstractive summarizationâ€”_or using models to create a human-readable, concise summary of a piece of textâ€”_has been studied for a long time. Prior to the rise of LLMs and RLHF, most papers on this topic trained language models using aÂ [supervised learning](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)Â approach with human-written reference summaries and evaluated these models using traditional metrics like theÂ [ROUGE score](https://cameronrwolfe.substack.com/i/138218863/evaluating-language-models-and-the-rouge-score).

These approaches can work well, but supervised learning and ROUGE are both proxies for what is actually desiredâ€”_a model that writes high-quality summaries_. In [5], authors solve this problem by replacing supervised learning with RLHF. Such an approach allows us to finetune language models to produce better summaries by directly using human feedback on model outputs as a training signal.

**PPO for summarization.**Â Authors in [5] are commonly credited with proposing the first RLHF framework for LLM finetuning. The proposed approach allows us to optimize an LLM based on the quality of its responses, as assessed by human annotators. Beginning with a pretrained LLM, we can iteratively:

1. Collect humanÂ [preference data](https://cameronrwolfe.substack.com/i/166169560/the-bradley-terry-model-of-preference).
    
2. Train aÂ [reward model](https://cameronrwolfe.substack.com/p/reward-models)Â over this preference data.
    
3. Finetune our LLM with RL using this reward model.
    

Notably, authors in [5] adopt PPO as their underlying RL optimizer, which led PPO to become the common choice in subsequent RLHF research. With this RL training strategy, we can train an LLM to produce summaries that surpass the quality of human summaries and are even better than those produced by larger LLMs trained with a supervised learning approach; see below.

[

![](https://substackcdn.com/image/fetch/$s_!bjdU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F377524f4-cff7-44f9-b717-ed1e842b50bb_1612x970.png)



](https://substackcdn.com/image/fetch/$s_!bjdU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F377524f4-cff7-44f9-b717-ed1e842b50bb_1612x970.png)

(from [5])

**SFT stage.**Â In [5], the LLM is first trained usingÂ [supervised finetuning](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)Â over human reference summaries for a single epoch, producing a supervised baseline that is later finetuned via RLHF. The methodology for RLHF proposed in [5]â€”_as illustrated in the figure shown below_â€”is tailored to the summarization task.

[

![](https://substackcdn.com/image/fetch/$s_!oeIY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)



](https://substackcdn.com/image/fetch/$s_!oeIY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc713702e-ca1c-4759-bff4-b1dedfdf1bbf_1650x1016.png)

(from [5])

**Preferences and reward models.**Â In [5], a preference dataset is constructed by:

- Grabbing a textual input to summarizeâ€”_this is our prompt_.
    
- Producing many summaries of the input using several different policiesâ€”_these are different responses to the same prompt_.
    
- Sampling two summaries or responses for the prompt.
    
- Asking a human annotator to identify the better of the two summaries.
    

Authors in [5] collect this preference data in large batches. Once we have finished collecting a new batch of preference data, we train a reward model on the data such that it accurately predicts human preference scores given an LLM-generated summary. Then, we use this reward model to finetune our policy with PPO.

**A**Â **KL divergence**Â term is used for PPO in [5] to minimize divergence from the SFT model. Interestingly, authors in [5] were not the first to use this strategyâ€”_it was actually adopted fromÂ [prior work](https://arxiv.org/abs/1907.00456)._Â The KL divergence is directly subtracted from the rewards instead of being added to the PPO loss as a penalty term. We see in [5] that adding the KL divergence into RL training helps to prevent the modelâ€™s summaries from becoming too different from those seen during training.

[

![](https://substackcdn.com/image/fetch/$s_!ZjlA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc088796c-52eb-45e5-afbc-195116ec5d1f_1612x764.png)



](https://substackcdn.com/image/fetch/$s_!ZjlA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc088796c-52eb-45e5-afbc-195116ec5d1f_1612x764.png)

(from [5])

**Experiments.**Â In [5], large pretrained models matching the style of GPT-3 with 1.3B to 6.7B parameters are finetuned over theÂ [TL;DR dataset](https://huggingface.co/datasets/openai/summarize_from_feedback). This dataset, which contains over three million posts from Reddit with author-written summaries, is filtered to only 120K high-quality examples; see above. Models are first trained using SFTâ€”_these supervised models are also used as baselines across experiments_â€”and then further finetuned with RLHF. Given that summary length can impact the resulting quality score, the authors in [5] constrain generated summaries to 48 tokens and finetune the model accordingly.

Finetuning language models with human feedback outperforms a variety of strong English summarization baselines. Notably, the 1.3B summarization model outperforms a 10Ã— larger model trained with SFT, and the 6.7B summarization model performs even better than the 1.3B model, revealing that summarization quality improves with model scale. Furthermore, we see that summarization models trained via RLHF generalize better to new domains. In particular, the models in [5] are applied to summarizing news articlesâ€”_a domain outside of the training data_â€”and found to perform well without further finetuning; see below.

[

![](https://substackcdn.com/image/fetch/$s_!HYOl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)



](https://substackcdn.com/image/fetch/$s_!HYOl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda0d4ac2-cee0-464b-ba5d-3b278f1b1b9c_1628x846.png)

(from [5])

From here, summarization models are evaluated in terms of:

- _Coverage_: the summary covers all information from the original post.
    
- _Accuracy_: statements in the summary are accurate.
    
- _Coherence_: the summary is easy to read on its own.
    
- _Quality_: the overall quality of the summary is good.
    

When evaluated in this manner, we see that summarization models trained via RLHF benefit the most in terms of coverage, while coherence and accuracy are only slightly improved compared to supervised baseline models; see below.

[

![](https://substackcdn.com/image/fetch/$s_!d5Qe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1f3213a-8fd2-4703-8987-b2cfcbc5880a_662x672.png)



](https://substackcdn.com/image/fetch/$s_!d5Qe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1f3213a-8fd2-4703-8987-b2cfcbc5880a_662x672.png)

(from [5])

**Beyond summarization.**Â Although RLHF was explored only in the context of summarization in [5], the authors of this paper had an incredible amount of foresight about what was to come. The approach proposed in [5] later became a standard part of LLM post-training, as we will soon see with InstructGPT [8].

> _â€œThe methods we present in this paper are motivated in part by longer-term concerns about the misalignment of AI systems with what humans want them to do. When misaligned summarization models make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems become more powerful and are given increasingly important tasks, the mistakes they make will likely become more subtle and safety-critical, making this an important area for further research.â€_Â - from [1]

Interestingly, authors in [5] explicitly state their intent to leverage the proposed methodology to better align LLMs to human desires in the long term. This statement was made over two years prior to the proposal of ChatGPT! Work in [5] was a building block for major advancements in AI that were yet to come.

#### **[The N+ Implementation Details of RLHF with PPO](https://arxiv.org/abs/2403.17031)Â [4]**

[

![](https://substackcdn.com/image/fetch/$s_!Om25!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdf3dce4-738f-47c5-a5e3-f12c75887538_1864x1216.png)



](https://substackcdn.com/image/fetch/$s_!Om25!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdf3dce4-738f-47c5-a5e3-f12c75887538_1864x1216.png)

(from [4])

There are many moving parts in PPO training, including multiple copies of the LLM (i.e., policy, reference, critic, and reward model) and various hyperparameter settings that must be carefully tuned to ensure stable training. For these reasonsâ€”_and due to computational expense_â€”reproducing RL training results is difficult.

> _â€œIt has proven challenging to reproduce OpenAIâ€™s RLHF pipelineâ€¦ for several reasons: 1) RL and RLHF have many subtle implementation details that can significantly impact training stability, 2) the models are challenging to evaluateâ€¦ 3) they take a long time to train and iterate.â€_Â - from [4]

As a starting point for democratizing understanding of RL, authors in [4] focus on a simple setupâ€”_OpenAIâ€™s prior work on RLHF for summarization_Â [5]. Though many details are already provided in the original work, authors in [4] fully reproduce these results while enumerating all implementation details needed to arrive at a working PPO implementation. The TL;DR summarization task is simple relative to most modern RLHF pipelines. However, this studyâ€”_based on Pythia models [10] with 1B, 2.8B, and 6.8B parameters_â€”provides a clear and comprehensive view of key practical considerations when training an LLM with PPO.

**Dataset considerations.**Â Authors in [4] enumerate around 20 practical details needed to obtain a working RLHF pipeline with PPO. Nearly half of these details are not related to PPOâ€”_they focus on the training data_. For those who have worked with LLMs, this data emphasis should not come as a surprise:Â _data quality is the key determinant of success in all forms of LLM training, including RL_.

All experiments in [4] use theÂ [TL;DR summarization dataset](https://huggingface.co/datasets/CarperAI/openai_summarize_tldr)Â from OpenAI, which contains both an SFT and preference dataset. Some notable remarks about the data used for PPO in [4] include:

- There is a misalignment in completion lengths between the SFT and preference portion of the TL;DR datasetâ€”_the preference data tends to have longer completions_.
    
- Data must occasionally be truncated to fit within the fixed sequence length used in [4], but the authors choose to truncate at paragraph boundariesâ€”_determined by newline characters_â€”instead of performing a hard truncation at the maximum sequence length.
    
- All completions are followed by anÂ `<EOS>`Â token. Authors in [4] emphasize that thisÂ `<EOS>`Â token must be different than the padding token used by the LLM. Otherwise, the loss for theÂ `<EOS>`Â token will be masked with the other padding tokens, preventing the model from learning to properly complete each sequence with anÂ `<EOS>`Â token.
    

**Reward model.**Â Several choices exist for initializing the reward model in RLHF. In [4], we initialize with the weights of the SFT model, which matches settings used in [5]. A randomly-initialized linear head that is used to predict the reward is then added to the reward modelâ€™s architecture before the model is trained for a single epoch over the available preference data.

An outcome reward setting is used in [4]. To extract the reward, a forward pass is performed on the full sequence, and we extract the reward prediction from theÂ `<EOS>`Â token only. To teach the policy to consistently output sequences of reasonable length with a correspondingÂ `<EOS>`Â token, theÂ **EOS trick**Â is used, which assigns a reward of -1 to any sequence with noÂ `<EOS>`Â token.

> _â€œIf the padding token does not exist, the extracted reward will then be logits corresponding to the last token of the sequence â€“ if that token is not the EOS token, its reward wonâ€™t be used for PPO trainingâ€_Â - from [4]

After the reward model is trained, authors follow the recommendation in [5] ofÂ **normalizing rewards**Â outputted by the model. Specifically, the reward model is used to predict rewards for the entire SFT dataset. Then, we compute the mean reward across this dataset and use this mean to center the average reward. In other words, this mean is subtracted as a bias from the reward modelâ€™s output, ensuring that rewards predicted over the SFT dataset have an average of zero. Normalizing the reward modelâ€™s output benefits training stability for PPO.

**Critic settings.**Â We must also choose how to initialize the critic. In [4], the critic is initialized with the weights of the reward model at the beginning of PPO training. After all,Â _the value model is effectively a reward model that predicts the reward on a per-token basis_. Authors observe in [4] that the reward modelâ€™s predictions are usually negative for all tokens except theÂ `<EOS>`Â token; see below.

[

![](https://substackcdn.com/image/fetch/$s_!fBTb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4cd7447-83f7-4f34-921a-41672d4c391c_1866x536.png)



](https://substackcdn.com/image/fetch/$s_!fBTb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4cd7447-83f7-4f34-921a-41672d4c391c_1866x536.png)

(from [4])

Therefore, the value estimated by the critic is negative for nearly every token at the start of PPO training. However, we see in [4] that warm starting the critic in this way helps to improve the initial stability of gradients during training.

**Reward and advantage whitening.**Â In addition to normalizing rewards after training the reward model, many PPO implementations perform reward and advantageÂ [whitening](https://joelouismarino.github.io/posts/2017/08/statistical_whitening/). An example implementation of the whitening operation is shown below, where the values can be a list of either rewards or advantages.

[

![](https://substackcdn.com/image/fetch/$s_!XoxA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9646db42-a84e-4dca-99a2-e585c053143c_1722x336.png)



](https://substackcdn.com/image/fetch/$s_!XoxA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9646db42-a84e-4dca-99a2-e585c053143c_1722x336.png)

(from [4])

When whitening rewards, we usually do not shift the mean (i.e.,Â `shift_mean = False`Â in the above code) so that we can retain the magnitude and sign of the rewards. However, the mean is usually shifted when whitening advantages. Based on results in [4],Â _whitening rewards and advantages does not seem to have a huge positive or negative performance impact on the resulting policy_. However, whitening is a common implementation detail in PPO. Usually, whitening is applied over the set of rewards or advantages within a batch of data.

> _â€œWhere normalization bounds all the values from the RM to be between 0 and 1, which can help with learning stability, whitening the rewards or the advantage estimatesâ€¦ can provide an even stronger boost to stability.â€_Â - from [2]

**Beware of dropout.**Â We must also be sure to avoid using dropout in PPO. Dropout adds noise to the modelâ€™s forward pass, making the computation of policy ratios and KL divergence unreliable. This implementation detail can cause optimization issues and tends to be impactfulâ€”_dropout is a perfect example of small but important practical details in PPO_. For example, theÂ [OpenInstruct PPO script](https://github.com/allenai/open-instruct/blob/main/open_instruct/ppo2.py)Â explicitly disables dropout in the policy, critic, reference, and reward models.

**Final results.**Â After enumerating various practical choices and hyperparameter settings, the policies in [4] successfully replicate the original results of [5]. PPO models outperform those trained with SFT, and there are clear scaling trends that can be observed (i.e., larger models achieve better performance metrics) for SFT models, reward models, and the final RL policies. Additionally, the preference rate of the RL policies over human reference summariesâ€”_as predicted by a GPT-3.5-based LLM judge_â€”scales predictably with model size; see below.

[

![](https://substackcdn.com/image/fetch/$s_!y_F0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63af44b0-f8ab-4b8a-9872-276a6d78726f_2462x820.png)



](https://substackcdn.com/image/fetch/$s_!y_F0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63af44b0-f8ab-4b8a-9872-276a6d78726f_2462x820.png)

(from [4])

#### **[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)Â [8]**

Going beyond the summarization domain, authors in [8] explore the use of RLHF for language modelÂ [alignment](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)Â by directly learning from human feedback. The resulting model, called InstructGPT, is the sister model and predecessor to ChatGPT. Since this model is outlined and explained in detail in [8], the work provides significant insight into how early LLMs at OpenAI were trained.

[

![](https://substackcdn.com/image/fetch/$s_!ZdHw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45180b88-a11e-42e8-8910-ceca2c3b447a_1618x980.png)



](https://substackcdn.com/image/fetch/$s_!ZdHw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45180b88-a11e-42e8-8910-ceca2c3b447a_1618x980.png)

(from [8])

Following an approach similar to [5], we start with a set of prompts that are either written by human annotators or collected from OpenAIâ€™s API. We can then have annotators write responses to these prompts and finetune a pretrained LLMâ€”_[GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)Â in particular_â€”over these examples using SFT. Using this model, we can then collect comparison data by asking humans to select their preferred outputs from the LLM and apply the same RLHF process outlined in [5] for finetuning. As shown above, the resulting model is heavily preferred by humans and much better at following detailed instructions provided within the prompt.

> _â€œMaking language models bigger does not inherently make them better at following a userâ€™s intent.â€_Â - from [8]

**The alignment process.**Â Pretrained LLMs have a number of undesirable properties that we want to fix during post-training; e.g., hallucinations or an inability to follow detailed instructions. To fix these issues, we align the LLM in [8] according to the following set of criteria:

- _Helpful_: follows the userâ€™s instructions and infers intention fromÂ [few-shot prompts](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)Â or other patterns.
    
- _Honest_: makes correct factual statements about the world.
    
- _Harmless_: avoids harmful outputs, such as those that denigrate a protected class or contain sexual/violent content.
    

Using RLHF, we can teach an LLM to reflect each of these qualities within its output. Specifically, this is done by constructing preference pairs where the preferred responses are chosen based upon adherence to these criteria.

[

![](https://substackcdn.com/image/fetch/$s_!ddkD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ee233ce-ea11-4928-bcbc-131c5fdc2f2f_1732x930.png)



](https://substackcdn.com/image/fetch/$s_!ddkD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ee233ce-ea11-4928-bcbc-131c5fdc2f2f_1732x930.png)

(from [8])

**More on RLHF.**Â Authors in [8] curate a team of 40 human annotators, who are screened with a test to judge their annotation quality, to collect preference data for the LLM. The approach for RLHF used in [8] matches the approach used in [5] almost completely. Using a pretrained LLM and a set of prompts for finetuning, the alignment process proceeds according to the following steps:

1. Collect human demonstrations of responses for each prompt.
    
2. Train the model in a supervised fashion over human demonstrations.
    
3. Collect preference data.
    
4. Train aÂ [reward model](https://cameronrwolfe.substack.com/p/reward-models).
    
5. Optimize the underlying LLM or policy with PPO.
    
6. Repeat steps 3-5.
    

The distribution of prompts used for finetuning in [8] is outlined in the table below. For SFT, a dataset of over 13K prompt and response pairs is constructed. The reward model is trained over 33K prompts, while a dataset of size 31K is used for finetuning with PPO. Unlike [5], human annotators are shown 4-9 responses to a prompt (i.e., instead of two) when collecting comparison data, allowing them to quickly rank responses and generate larger amounts of comparison data more efficiently. However,Â _later work on RLHF largely abandoned this approach in favor of binary preferences_. The dataset used in [8] is also 96% English.

[

![](https://substackcdn.com/image/fetch/$s_!xMFU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b979ad-bd64-47c4-bfe7-64890b661ba9_1660x724.png)



](https://substackcdn.com/image/fetch/$s_!xMFU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b979ad-bd64-47c4-bfe7-64890b661ba9_1660x724.png)

(from [8])

Similarly to [5], a KL divergence term between the policy and the SFT model is directly subtracted from the reward to avoid drifting too far away from the SFT model. Additionally, extra pretraining updates are â€œmixed inâ€ to the RLHF optimization process, which authors find to help with maintaining the modelâ€™s performance across various benchmarks. These pretraining updates, which use a supervised loss, are simply added to the PPO loss used during RL.

> _â€œWe were able to mitigate most of the performance degradations introduced by our fine-tuning. If this was not the case, these performance degradations would constitute an alignment taxâ€”an additional cost for aligning the model.â€_Â - from [2]

**Experimental findings.**Â In [8], authors train three models with 1.3B, 6B, and 175B (i.e., same asÂ [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)) parameters. From these experiments, we learn that human annotators prefer InstructGPT outputs over those of GPT-3, even for models with 10Ã— fewer parameters; see below. This result is similar to observations in [5], where finetuning via RLHF enables much smaller models to outperform larger models trained in a supervised manner.

[

![](https://substackcdn.com/image/fetch/$s_!BTzq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08415ad7-db55-4f46-8415-2fb3da1c9ab6_1350x1348.png)



](https://substackcdn.com/image/fetch/$s_!BTzq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08415ad7-db55-4f46-8415-2fb3da1c9ab6_1350x1348.png)

(from [8])

Notably, outputs from InstructGPT-1.3B are preferred to those of GPT-3, which has 100Ã— more parameters. Additionally, we see that InstructGPT-175B produces outputs that are preferred to GPT-3 85% of the time. Going further, InstructGPT models are found to more reliably follow explicit constraints and instructions provided by a human user within the modelâ€™s prompt; see below.

[

![](https://substackcdn.com/image/fetch/$s_!JB4X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc9280f9-a159-4e81-ab17-86faf28f47ba_1876x882.png)



](https://substackcdn.com/image/fetch/$s_!JB4X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc9280f9-a159-4e81-ab17-86faf28f47ba_1876x882.png)

(from [8])

Compared to pretrained and supervised models, InstructGPT is also found to be:

- More truthful.
    
- Slightly less toxic.
    
- Generalizable to instructions beyond the training dataset.
    

For example, InstructGPT can answer questions about code and handle prompts written in different languages, despite the finetuning dataset lacking sufficient data within this distribution. Although the model did not receive as much recognition as ChatGPT, InstructGPT was a major step forward in AI that introduced many core concepts used for training modern LLMs.

## Conclusion

PPO is one of the most widely used RL algorithms for LLMs that hasâ€”_through its key role in RLHF pipelines_â€”directly contributed to fundamental advancements in AI. As we learned, research on PPO was an important factor in the creation of models like InstructGPT and ChatGPT. These influential models catalyzed the ongoing boom in LLM research in which we currently find ourselves.

We cannot overstate the impact of PPO on LLM research, and PPO continues to play an important role in LLM post-training pipelines today. However, the barrier to entry for PPO is high due to its memory and compute overhead. Additionally, the results of PPO can vary based on a wide variety of practical implementation details and hyperparameter settings. For these reasons, most research on PPO has been centralized within top frontier labs. Only a small number of groups have sufficient compute resources to empirically tune and obtain a working PPO implementation at scale.

Nonetheless, understanding PPO is essential due to its fundamental role in AI research. The cost and complexity of PPO remains high, but RL researchers have recently expanded and improved upon ideas proposed by PPO. For example, REINFORCE and GRPO are simpler (and more stable) policy gradient algorithms that can be used to train LLMs, which use less memory than PPO by avoiding the critic. A working understanding of PPO makes understanding these new algorithmsâ€”_or even developing our own_â€”much simpler!

#### New to the newsletter?

Hi! Iâ€™mÂ [Cameron R. Wolfe](https://cameronrwolfe.me/), Deep Learning Ph.D. and Senior Research Scientist atÂ [Netflix](https://research.netflix.com/research-area/nlp-and-conversations). This is the Deep (Learning) Focus newsletter, where I help readers better understand important topics in AI research. The newsletter will always be free and open to read. If you like the newsletter, please subscribe, consider a paid subscription, share it, or follow me onÂ [X](https://twitter.com/cwolferesearch)Â andÂ [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/)!

Subscribe

#### Bibliography

[1] Schulman, John, et al. â€œProximal policy optimization algorithms.â€Â _arXiv preprint arXiv:1707.06347_Â (2017).

[2] Lambert, Nathan. â€œReinforcement Learning from Human Feedback.â€ Online (2025). https://rlhfbook.com

[3] Schulman, John, et al. â€œHigh-dimensional continuous control using generalized advantage estimation.â€Â _arXiv preprint arXiv:1506.02438_Â (2015).

[4] Huang, Shengyi, et al. â€œThe n+ implementation details of rlhf with ppo: A case study on tl; dr summarization.â€Â _arXiv preprint arXiv:2403.17031_Â (2024).

[5] Stiennon, Nisan, et al. â€œLearning to summarize with human feedback.â€Â _Advances in neural information processing systems_Â 33 (2020): 3008-3021.

[6] Schulman, John, et al. â€œTrust region policy optimization.â€Â _International conference on machine learning_. PMLR, 2015.

[7] Lambert, Nathan, et al. â€œTulu 3: Pushing frontiers in open language model post-training.â€Â _arXiv preprint arXiv:2411.15124_Â (2024).

[8] Ouyang, Long, et al. â€œTraining language models to follow instructions with human feedback.â€Â _Advances in neural information processing systems_Â 35 (2022): 27730-27744.

[9] Ahmadian, Arash, et al. â€œBack to basics: Revisiting reinforce style optimization for learning from human feedback in llms.â€Â _arXiv preprint arXiv:2402.14740_Â (2024).

[10] Biderman, Stella, et al. â€œPythia: A suite for analyzing large language models across training and scaling.â€Â _International Conference on Machine Learning_. PMLR, 2023.

[1](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-1-175107358)

As we can see, the discounted reward has an infinite horizon in this case. In other words, the total number of steps in the trajectory is infiniteÂ `T = âˆ`. This is known as the infinite-horizon discounted return.

[2](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-2-175107358)

The VPG was also partially covered in my overview of REINFORCE that was released a few weeks ago; seeÂ [here](https://cameronrwolfe.substack.com/p/reinforce).

[3](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-3-175107358)

Specifically, if we wanted to solve a constrained optimization problem like this with gradient ascent, we would have to use constrained gradient ascent. However, this method requires that we project our solution into the space of valid solutions that satisfy the constraint after every optimization step, which would be computationally intractable for neural network parameters. The KL divergence is a very complex constraint for which to perform this projection!

[4](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-4-175107358)

More specifically, if the policy ratio is greater thanÂ `1 + Îµ`, we set it equal toÂ `1 + Îµ`. If the policy ratio is less thanÂ `1 - Îµ`, we set it toÂ `1 - Îµ`. Otherwise, we keep the value of the policy ratio unchanged.

[5](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-5-175107358)

The clipped objective will always be less than or equal to the unclipped objective due to the fact that we are taking the minimum of the unclipped and clipped objectives.

[6](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-6-175107358)

The â€œactorâ€ refers to the LLMâ€”_or the model that is taking actions_â€”and the â€œcriticâ€ refers to the value model. The value model is called a critic due to the fact that it is predicting the reward associated with each action (i.e., effectively critiquing the action).

[7](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-7-175107358)

For more details on loss aggregation in RL, seeÂ [this section](https://rlhfbook.com/c/11-policy-gradients.html#loss-aggregation)Â of the RLHF book, which provides concrete examples of different aggregation strategies and their impact.

[8](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-8-175107358)

The adaptive KL divergence is explained in Section 4 of [1]. Instead of setting a fixed scaling factor for the KL divergence, authors propose dynamically adjusting this factor throughout training such that the KL divergence stays close to a target KL divergenceÂ `d_targ`. Put differently, instead of choosing the scaling factor,Â _we specify what we want our KL divergence to be and dynamically adjust the scaling factor throughout training to keep the KL divergence in this range_. This approach is not commonly used for recent LLMs, and it is much more common to set a fixedÂ `Î²`Â coefficient for the KL divergence.

[9](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-9-175107358)

The reference and old models are different models in PPO! The reference model is the policy parameters before any RL training is performed. For LLMs, the SFT model is usually the reference model. We usually perform multiple updates over a batch of data in PPO,Â _and the old model is the model before the first update_. The old model is updated each time a new batch of data is sampled, whereas the reference model is fixed.

[10](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-10-175107358)

This means that less data is required to achieve a given level of performance (i.e., the learning process is faster).

[11](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-11-175107358)

Specifically, we would use the cumulative reward after stateÂ `s_t`. However, for LLMs this distinction does not usually matter due to the use of outcome rewards.

[12](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-12-175107358)

In fact, this is where the name for the TD residual comes from. We are computing the difference in value between two time steps.

[13](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-13-175107358)

The critic is just a model that imperfectly estimates of the value function. The bias in the TD residual comes from the fact that the critic makes mistakes in estimating the value.

[14](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-14-175107358)

To derive this expression, we begin with the original formula for the GAE showed in the top line, expand the definitions of theÂ `N`-step advantage estimates, rearrange the terms, then use theÂ [geometric series formula](https://en.wikipedia.org/wiki/Geometric_series)Â to derive the final expression.

[15](https://cameronrwolfe.substack.com/p/ppo-llm#footnote-anchor-15-175107358)

This statement assumes that the KL divergence is added to the loss and not directly incorporated into the reward.
