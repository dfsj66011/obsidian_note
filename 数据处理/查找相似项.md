
> [Finding Similar Items](http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf)


我们首先将相似性问题表述为寻找具有相对较大交集的集合的问题。我们展示了如何通过一种称为“指纹”的技术，将查找文本相似文档的问题转化为这样的集合问题。然后，我们介绍了一种称为 “minhashing” 的技术，该技术以这样一种方式压缩大型集合，即我们仍然可以从它们的压缩版本中推断出底层集合的相似性。

即使计算任意一对 item 的相似度变得非常容易，也可能有太多 item 对需要测试，这一担忧促使了一种称为“局部敏感哈希”的技术的发展，该技术专注于最有可能相似的 item 对。最后，我们探讨了无法用集合交集来表达的“相似性”概念。这项研究引导我们考虑任意空间中的距离度量理论。它还激发了一个适用于其他“相似性”定义的局部敏感哈希通用框架。

#### 1. 集合的 Jaccard 相似度

Jaccard 相似度是集合 $S$ 和 $T$ 的交集大小与并集大小的比值，表示为：$$\text{SIM}(S, T) = \frac{|S \cap T|}{|S \cup T|}$$
#### 2. $k$-shingles

Shingling 是将文档表示为集合的一种方式，$k$-Shingles 就是将文本拆分为长度为 $k$ 的字符串子集，例如，假设我们的文本 $D$ 的内容为 ”abcdabd“，则当 $k=2$ 时，拆解的全部子集为 $\{ab, bc, cd, da, bd\}$，其中 $ab$ 在文本 $D$ 中出现了两次，但是在 shingling 集合中只保留一次（另一个版本采用的 bag，而不是 set 则支持重复项）。这里的小字符串就称为 ”shingle“。

##### 2.2 $k$ 的选择

如果 $k$ 选择的比较小，则任意文本相似性很高，例如极限情况下，$k=1$，且只考虑字母+空格情况，则几乎任意文档都是有 27 个元素（26个字母+1个空格）组成，相似度为 $1$；所以 *$K$ 应该足够大，以至于任意给定的 shingle 出现在任意给定的文档中的概率都很低。*

例如当 $k=5$，$27^5=14,348,907$，对于短文本，字符长度远远小于 0.14 亿的长度，因此通常选择 $k=5$ 就挺好，不过实际上文本中的字符可能不止 $27$ 个，同时每个字母出现的概率也不一样，大致可以按 $20^k$ 计算，对于长篇文档（如研究论文），可以考虑用 $k=9$.

#### 2.3 Hashing Shingles

假设 $k=9$，则一个 shingles 长度为 9，则需要 9 个字节空间存储，如果将其 hash 到 0 到 $2^{32}-1$ 范围内，则可以用 4 个字节（32位）的长度表示，可以节省空间。需要注意的是，如果我们使用 9-shingle 并将它们 hash 到 4 个字节，那么我们可以更好地区分文档，而不是使用 4-shingle，即使用于表示 shingle 的空间是相同的。

### 三、MinHash 过程

#### 3.1 集合的矩阵表示  

shingle 集合非常大，即使我们将其都 hash 到 4 个字节，一篇文档的 shingle 集合所需要的空间仍然大概是原文档所需空间的 4 倍。例如原始文档 $D$ 长度为 $ld$，如果不考虑重复 shingle 出现，则一个文档 D 的 shingles 集合大小为 $ld-k+1$，而每个 shingles 需要 4 个字节存储，$k$ 又远远小于 $ld$，所以需要大致 $4ld$ 空间。

所以我们需要想办法将上述大集合替换为规模小得多的”签名“。但这种签名是有要求的，即*通过比较签名之间的相似度就可以估计实际 shingle 集合之间的 Jaccard 相似度*。当然，通过签名无法得到原始 shingle 集合之间 Jaccard相似度的精确值，但是估计结果与真实结果相差不大，并且签名集合越大，估计的精度也越高。例如， 50000 字节文档的 shingle 可能会映射为 200000 字节的哈希结果，然后替换成 1000 字节大小的签名集合。基于最终签名集合得到的原始文档 Jaccard 相似度的估计值与真实值的差异也就在几个百分点之内。

首先将一系列集合表示成特征矩阵形式：$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline a & 1 & 0 & 0 & 1 \\ b & 0 & 0 & 1 & 0 \\ c & 0 & 1 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ e & 0 & 0 & 1 & 0 \\ \hline \end{array}$$例如这里有 4 个集合，$S_1 = \{a, d\}, \,S_2 = \{c\}, \,S_3 = \{b, d, e\}, \, S_4 = \{a, c, d\}$。

#### 3.2 最小哈希（Minhashing）  

为了对特征矩阵每列所表示的集合进行 minhash 计算，首先选择行的一个排列转换。任意列的 minhash 值是该列在排列顺序中首次出现 1 的行号。例如下图中我们把行的顺序按照 `beadc` 顺序排列后，得到$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline b & 0 & 0 & 1 & 0 \\ e & 0 & 0 & 1 & 0 \\ a & 1 & 0 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ c & 0 & 1 & 0 & 1 \\ \hline \end{array}$$虽然从物理上来说，对非常大的特征矩阵进行置换是不可能的，但 MinHash 函数 h 隐式地对原始矩阵的行进行了重新排序，在这个矩阵中，我们可以通过从顶部开始扫描，直到遇到 $1$ 来读出 $h$ 的值，例如 $S_1$ 中第一次出现 1 的位置是 $a$ 行，则 $h(S_1)=a$，同理 $h(S_2) = c, \,h(S_3) = b, \, h(S_4) = a$

#### 3.3 minhash 与 Jaccard 相似度

*两个集合经随机排列转换之后得到的两个 minhash 值相等的概率等于这两个集合的 Jaccard 相似度。*

解释一下，假设只考虑集合 $S_1$ 和 $S_2$ 所对应的列，那么它们所在的行可以按照所有可能的结果分成如下 3 类：

1. 属于 $X$ 类的行，意思是两列的值均为 1；
2. 属于 $Y$ 类的行，意思是其中一列的值为 0, 另一列的值为 1；
3. 属于 $Z$ 类的行，意思两列的值都为 0 。

由于特征矩阵十分稀疏，因此大部分行都属于 $Z$ 类。但我们可以忽略都是 0 的行，实际由 $X$ 和 $Y$ 类行数目的比例决定了 $\text{SIM}(S_1, S_2)$ 及概率 $h(S_1)=h(S_2)$ 的大小。假定 $X$ 类行的数目为 $x$, $Y$ 类的行的数目为 $y$, 则 $\text{SIM}(S_1,S_2) =x/(x+y)$。原因是 $S_1 \cap S_2$ 的大小为 $x$，而 $S_1 \cup S_2$ 的大小为 $x+y$。

接下来我们考虑 $h(S_1)=h(S_2)$ 的概率。设想所有行进行随机排列转换，然后我们从上到下进行扫描处理，在碰到 $Y$ 类行之前碰到 $X$ 行的概率是 $x/(x+y)$。但是如果从上往下扫描遇到的除 $Z$ 类行之外的第一行属于 $X$ 类，那么肯定有 $h(S_1)=h(S_2)$ 。另一方面，如果首先碰到的是 $Y$ 类行，而不是 $Z$ 类行，那么值为 1 的那个集合的 minhash 值为当前行。但值为 0 的那个集合必将会进一步扫描下去。因此，如果首先碰到 $Y$ 类行，那么此时 $h(S_1) \neq h(S_2)$。于是，我们可以得到最终结论，即 $h(S_1)=h(S_2)$ 的概率是 $x/(x+y)$，而这也是两个集合 Jaccard 相似度的计算公式。

### 3.3 相似性保留的集合摘要 


#### 3.3.4 最小哈希签名  

再次考虑由特征矩阵 M 表示的一组集合。为了表示这些集合，我们随机选择若干行置换（例如 100 次或几百次），这些置换确定了最小哈希函数 h₁, h₂, ..., hₙ。对于表示集合 S 的列，构造 S 的最小哈希签名，即向量 [h₁(S), h₂(S), ..., hₙ(S)]。通常将这组哈希值表示为一列。因此，我们可以从矩阵 M 构造一个签名矩阵，其中 M 的第 i 列被第 i 列集合的最小哈希签名所替代。

请注意，签名矩阵的列数与矩阵 M 相同，但行数仅为 n。即使 M 并未以显式形式表示，而是以适合稀疏矩阵的某种压缩形式（例如，通过其 1 的位置）来表示，签名矩阵通常也会比 M 小得多。

#### 3.3.5 计算Minhash签名

显式地对大型特征矩阵进行排列是不可行的。即使对数百万或数十亿行进行随机排列也需要耗费大量时间，而对行进行必要的排序则需要更多时间。因此，如图3.3所示的排列矩阵虽然在概念上很有吸引力，但实际上并不可行。

幸运的是，可以通过一个随机哈希函数来模拟随机排列的效果，该哈希函数将行号映射到与行数相同的多个桶中。一个将整数 0, 1,..., k − 1 映射到桶号 0 至 k−1 的哈希函数通常会将某些整数对映射到同一个桶中，并使其他桶为空。然而，只要 k 足够大且冲突不多，这种差异并不重要。我们可以维持这样一种假设，即我们的哈希函数 h 将行 r “排列”到排列顺序中的位置 h(r)。

因此，我们不是选取 n 个随机行排列，而是选取 n 个随机的行哈希函数 h1, h2,..., hn。我们按给定顺序考虑每一行来构建签名矩阵。设 SIG(i, c) 为第 i 个哈希函数和第 c 列对应的签名矩阵元素。初始时，将所有 i 和 c 的 SIG(i, c) 设为 ∞。我们通过以下步骤处理行 r：
1. 计算 h1(r), h2(r),..., hn(r)。
2. 对每一列 c 执行以下操作：
	(a) 如果行 r 中列 c 的值为 0，则不进行任何操作。
	(b) 然而，如果行 r 中列 c 的值为 1，则对于每个 i = 1, 2,..., n，将 SIG(i, c) 设置为当前 SIG(i, c) 值和 hi(r) 中的较小值。

$$\begin{array}{|c||c|c|c|c||c|c|} \hline \textit{Row} & S_1 & S_2 & S_3 & S_4 & x+1 \mod 5 & 3x+1 \mod 5 \\ \hline 0 & 1 & 0 & 0 & 1 & 1 & 1 \\ 1 & 0 & 0 & 1 & 0 & 2 & 4 \\ 2 & 0 & 1 & 0 & 1 & 3 & 2 \\ 3 & 1 & 0 & 1 & 1 & 4 & 0 \\ 4 & 0 & 0 & 1 & 0 & 0 & 3 \\ \hline \end{array}$$

图3.4：为图3.2的矩阵计算哈希函数

**示例 3.8：** 让我们重新考虑图 3.2 的特征矩阵，我们在图 3.4 中复制了该矩阵并添加了一些额外的数据。我们用整数 0 到 4 替换了命名行的字母。我们还选择了两个哈希函数：h1(x) = x + 1 mod 5 和 h2(x) = 3x + 1 mod 5。这两个函数应用于行号的值在图 3.4 的最后两列中给出。请注意，这些简单的哈希函数是行的真实排列，但只有因为行数 5 是质数，才可能进行真正的排列。一般来说，会发生冲突，即两行得到相同的哈希值。

现在，让我们模拟计算签名矩阵的算法。最初，这个矩阵由全 ∞ 组成。

$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & \infty & \infty & \infty & \infty \\ h_2 & \infty & \infty & \infty & \infty \\ \hline \end{array}$$

首先，我们考虑图3.4的第0行。可以看到，h1(0)和h2(0)的值均为1。编号为0的行在集合S1和S4对应的列中有1，因此只有签名矩阵的这两列可能会发生变化。由于1小于∞，我们实际上会改变S1和S4对应列中的两个值。因此，签名矩阵的当前估计值为：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & \infty & 1 \\ h_2 & 1 & \infty & \infty & 1 \\ \hline \end{array}$$
现在，我们移动到图3.4中编号为1的行。这一行仅在S3中为1，其哈希值分别为h1(1) = 2和h2(1) = 4。因此，我们将SIG(1, 3)设为2，将SIG(2, 3)设为4。所有其他签名条目保持不变，因为它们在编号为1的行中对应的列为0。新的签名矩阵为：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & 2 & 1 \\ h_2 & 1 & \infty & 4 & 1 \\ \hline \end{array}$$

图3.4中编号为2的那一行在S2和S4对应的列中为1，其哈希值分别为h1(2) = 3和h2(2) = 2。我们可以改变S4的签名值，但签名矩阵中这一列的值[1, 1]分别小于对应的哈希值[3, 2]。然而，由于S2对应的列仍然是∞，我们将其替换为[3, 2]，结果如下：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 1 & 2 & 4 & 1 \\ \hline \end{array}$$
接下来是图3.4中编号为3的行。这里，除了S2列之外，所有列的值都是1，哈希值分别为h1(3) = 4和h2(3) = 0。h1的值为4，超过了签名矩阵中所有列当前已有的值，因此我们不会改变签名矩阵第一行的任何值。然而，h2的值为0，小于当前已有的值，因此我们将SIG(2, 1)、SIG(2, 3)和SIG(2, 4)的值降低为0。注意，我们不能降低SIG(2, 2)的值，因为在图3.4中S2列在我们当前考虑的行中的值为0。最终的签名矩阵如下：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
最后，考虑图3.4中编号为4的行。h1(4) = 0且h2(4) = 3。由于第4行仅在S3对应的列有1，我们只需比较该集合当前签名列[2, 0]与哈希值[0, 3]。由于0 < 2，我们将SIG(1, 3)改为0，但由于3 > 0，我们不改变SIG(2, 3)。最终的签名矩阵为：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 0 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
我们可以根据这个签名矩阵估算底层集合的杰卡德相似度。注意到第1列和第4列完全相同，因此我们推测SIM(S1, S4) = 1.0。查看图3.4会发现，S1和S4真实的杰卡德相似度其实是2/3。需要记住的是，签名矩阵中行一致的比例只是对真实杰卡德相似度的估计，而这个例子规模太小，无法依靠大数定律保证估计值的准确性。再举些例子：S1和S3的签名列在半数行中一致（真实相似度为1/4），而S1和S2的签名估计相似度为0（这是正确值）。

### 3.4 面向文档的局部敏感哈希  

尽管我们可以通过最小哈希将大型文档压缩为短小的签名，并保持任意文档对之间的预期相似度，但高效找出相似度最高的文档对仍可能无法实现。原因在于，即便文档总数不多，文档对的数量仍可能过于庞大。

**例 3.9：** 假设我们有一百万个文档，并使用长度为 250 的签名。这样每个文档的签名占用 1000 字节，整个数据可以放入一个千兆字节的空间中——比一台普通笔记本电脑的内存还要小。然而，有 $\frac{1,000,000 \times (1,000,000 - 1)}{2}$ 或者说半万亿对文档。如果计算两个签名的相似度需要一微秒，那么在那台笔记本电脑上计算所有相似度将需要将近六天。

如果我们的目标是计算每对文档之间的相似度，那么除了通过并行计算缩短耗时外，我们无法减少计算量。然而通常我们只需要找出最相似的文档对，或者相似度超过某个阈值的所有文档对。这种情况下，我们就只需重点关注可能相似的文档对，而无需检查所有组合。针对该需求的理论框架称为局部敏感哈希（LSH）或近邻搜索技术。本节我们将探讨专为当前研究场景设计的LSH具体实现方案——该方案适用于通过shingle集合表示文档，再经最小哈希生成短签名的处理流程。关于局部敏感哈希的通用理论体系及其衍生技术和应用场景，我们将在3.6节系统阐述。

#### 3.4.1 基于最小哈希签名的局部敏感哈希  

局部敏感哈希（LSH）的一种通用方法是多次“哈希”项，使得相似项比不相似项更可能被哈希到同一桶中。我们将任何在至少一次哈希中被分配到同一桶中的配对视为候选对，并仅检查这些候选对的相似性。其核心思想是：希望大多数不相似配对永远不会被哈希到同一桶中，因此无需被检查。那些被错误分配到同一桶的不相似配对称为假阳性，我们希望这类情况在所有配对中占比极小。同时期望真正相似的配对在至少一个哈希函数下会落入同一桶中。未能满足这一条件的相似配对称为假阴性，我们希望这类情况在真实相似配对中占比同样极小。

如果我们拥有项目的minhash签名，选择哈希方案的有效方法是将签名矩阵划分为b个带区，每个带区包含r行。对于每个带区，存在一个哈希函数，该函数接收r个整数的向量（即该带区内某列的片段）并将它们哈希到大量桶中。我们可以对所有带区使用相同的哈希函数，但为每个带区单独设置桶数组，因此不同带区中具有相同向量的列不会哈希到同一个桶中。$$\begin{array}{|c|c|}
\hline
\text{band 1} & 
\begin{array}{ccc}
& 1 & 0 & 0 & 0 & 2 \\
\cdots & 3 & 2 & 1 & 2 & 2 & \cdots \\
& 0 & 1 & 3 & 1 & 1 \\
\end{array} \\
\hline
\text{band 2} & \\
\hline
\text{band 3} & \\
\hline
\text{band 4} & \\
\hline
\end{array}
$$
图 3.6：将签名矩阵划分为四个带区，每个带区包含三行

示例3.10：图3.6展示了一个12行签名矩阵的部分内容，该矩阵被划分为四个波段，每个波段包含三行。在明确显示的列中，第二列和第四列的第一个波段都具有列向量[0, 2, 1]，因此它们在第一个波段的哈希处理中必定会被映射到同一个桶中。这意味着无论这两列在其他三个波段的表现如何，这对列都将成为候选对。其他列（例如明确显示的前两列）也可能根据第一个波段的哈希处理被映射到同一个桶中。然而，由于它们的列向量不同（分别为[1, 3, 0]和[0, 2, 1]），且每个哈希处理对应大量桶，我们预计偶然碰撞的概率非常低。通常我们假设，两个向量当且仅当完全相同时才会被哈希到同一个桶中。

在第一个波段不匹配的两列仍有三次机会成为候选对——它们可能在其他任一波段中完全相同。但要注意，两列越相似，它们就越有可能在某个波段中一致。因此，直观上，这种分波段策略使得相似列比不相似列更有可能成为候选对。

#### 3.4.2 分带技术分析 

假设我们使用b个带，每个带有r行，并且假设某对文档的Jaccard相似度为s。回顾3.3.3节，这两个文档的minhash签名在签名矩阵的任意一行中一致的概率为s。我们可以按如下方式计算这对文档（或其签名）成为候选对的概率：

1. 签名在某个特定带的所有行中都一致的概率为 $s^r$。
2. 签名在某个特定带的至少一行中不一致的概率为 $1 − s^r$。
3. 签名在所有带的任意一行中都不一致的概率为 $(1 − s^r)^b$。
4. 签名在至少一个带的所有行中都一致（因而成为候选对）的概率为 $1 − (1 − s^r)^b$。


虽然可能不够直观，但无论选择怎样的常数b和r，该函数都具有如图3.7所示的S形曲线特征。其阈值（即相似度s值上升最陡峭处的临界点）是b和r的函数。阈值的近似值为(1/b)^(1/r)。例如当b=16且r=4时，阈值约为1/2，因为16的四次方根等于2。

**例 3.11：** 我们以b=20且r=5的情况为例。假设我们有一个长度为100的签名，被划分为20个波段，每个波段包含5行。图3.8列出了函数1 − (1 − s^5)^20的部分取值。需要注意的是，曲线的阈值（即函数值上升到一半时对应的s值）略大于0.5。同时可以看到，这条曲线并非理想的阶跃函数（在阈值处从0突变到1），但其中间部分的斜率仍然显著。例如，当s从0.4增加到0.6时，函数值上升超过0.6，这意味着中间区间的斜率大于3。

例如，当s=0.8时，1−(0.8)^5约为0.672。若将该数值进行20次方运算，结果约为0.00035。用1减去该值得出0.99965。这意味着对于相似度为80%的两份文档，在任意一个波段中，它们仅有约33%的概率能在所有五行中都匹配成功从而成为候选对。但由于存在20个波段，就意味着有20次成为候选对的机会。在相似度高达80%的文档对中，大约每3000对中仅有一对会因未能成为候选对而出现假阴性情况。

图 3.7: S 曲线

$$\begin{array}{c|c}
s & 1 - (1 - s^r)^b \\
\hline
0.2 & 0.006 \\
0.3 & 0.047 \\
0.4 & 0.186 \\
0.5 & 0.470 \\
0.6 & 0.802 \\
0.7 & 0.975 \\
0.8 & 0.9996 \\
\end{array}
$$

图 3.8: Values of the S-curve for b = 20 and r = 5 

#### 3.4.3 技术组合应用

我们现在可以给出一种方法，用于寻找相似文档的候选对集合，然后从中识别出真正相似的文档。必须强调的是，这种方法可能会产生假阴性——即某些实际相似的文档对由于未能成为候选对而未被识别出来。同时也会出现假阳性——即被评估的候选对最终被判定为相似度不足的情况。

1. 选择一个k值，为每个文档生成对应的k-shingle集合（可选步骤：将k-shingle哈希映射为更短的桶编号）。
2. 对文档-shingle配对进行排序，按shingle值重新排列。
3. 确定最小哈希签名长度n，根据3.3.5节算法处理排序后的列表，计算所有文档的最小哈希签名。
4. 设定相似度阈值t以定义目标"相似文档对"：选择波段数b和行数r，满足b×r=n，且t≈(1/b)^(1/r)。若需减少漏判，可调低阈值；若需加快速度并减少误判，可调高阈值。
5. 应用3.4.1节的局部敏感哈希(LSH)技术生成候选文档对。
6. 检查每个候选对的签名，确认其相同分量的比例是否≥t。
7. 可选步骤：若签名相似度极高，可进一步校验原始文档是否真实相似（避免因签名巧合导致的误判）。


### 3.5 距离度量  

我们现在稍作转向，探讨距离度量的一般概念。杰卡德相似度用于衡量集合之间的接近程度，尽管它并非严格意义上的距离度量。也就是说，集合越相似，杰卡德相似度越高。实际上，1减去杰卡德相似度才是一种距离度量，后文将对此进行说明；这一结果称为杰卡德距离。

然而，雅卡尔距离并非唯一合理的相似度度量方式。本节我们将探讨其他几种具有实际应用的距离度量方法。随后在3.6节中，我们将看到其中部分距离度量如何通过局部敏感哈希技术实现高效近邻搜索，无需遍历所有数据点。距离度量的其他应用场景将在第7章研究聚类时进一步展开。

#### 3.5.1 距离度量的定义 

假设有一个点的集合，称为空间。该空间上的距离度量是一个函数d(x, y)，它以空间中的两个点作为参数并产生一个实数，且满足以下公理：

1. d(x, y) ≥ 0（距离非负）。
2. d(x, y) = 0 当且仅当 x = y（距离为正，除非是点到自身的距离）。
3. d(x, y) = d(y, x)（距离对称）。
4. d(x, y) ≤ d(x, z) + d(z, y)（三角不等式）。

三角不等式是最复杂的条件。它直观地表明，要从x点到达y点，如果被迫经过某个特定的第三点z，我们将无法获得任何好处。正是三角不等式公理使得所有距离度量都表现得如同距离描述了两点之间最短路径的长度。

#### 3.5.2 欧几里得距离  

最熟悉的距离度量就是我们通常理解的“距离”。n维欧几里得空间中的点是n个实数构成的向量。该空间中的常规距离度量（我们称为L2范数）定义为：$$d\left( [x_1, x_2, \ldots, x_n], [y_1, y_2, \ldots, y_n] \right) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$也就是说，我们在每个维度上对距离进行平方，将平方值相加，然后取正平方根。

很容易验证距离度量的前三个条件是满足的。两点之间的欧氏距离不可能为负，因为取的是正平方根。由于所有实数的平方都是非负的，只要存在某个i使得xi ≠ yi，距离就严格为正。反之，若所有i都满足xi = yi，则距离显然为0。对称性成立是因为(xi - yi)² = (yi - xi)²。三角不等式的验证需要大量代数运算，但这是欧氏空间的固有性质：三角形任意两边长度之和不小于第三边长度。

还存在其他用于欧几里得空间的距离度量。对于任意常数r，我们可以将Lr范数定义为由以下公式给出的距离度量d：$$d\left( [x_1, x_2, \ldots, x_n], [y_1, y_2, \ldots, y_n] \right) = \left( \sum_{i=1}^{n} |x_i - y_i|^r \right)^{1/r}$$
当r=2时，即为前文提到的常规L2范数。另一种常见的距离度量是L1范数，又称曼哈顿距离。该距离计算的是两点在各维度上坐标差值的绝对值之和。其得名源于：若两点间移动必须沿网格线行进（如曼哈顿城区街道的布局方式），则该距离即为实际需要行进的路程长度。

另一种有趣的距离度量是L∞范数，它是当r趋近于无穷大时Lr范数的极限。随着r不断增大，只有差异最大的维度才起决定性作用，因此严格来说，L∞范数被定义为所有维度i上|xi − yi|的最大值。

**例 3.12：** 考虑二维欧几里得空间（常规平面）及点(2,7)和(6,4)。L2范数给出的距离为√[(2−6)²+(7−4)²]=√(4²+3²)=5；L1范数给出的距离为|2−6|+|7−4|=4+3=7；L∞范数给出的距离为max(|2−6|,|7−4|)=max(4,3)=4。

#### 3.5.3 杰卡德距离

如本节开头所述，我们将集合的杰卡德距离定义为 d(x, y) = 1 − SIM(x, y)。也就是说，杰卡德距离等于1减去集合x与y的交集大小与并集大小之比。必须验证该函数是否满足距离度量的条件。

1. d(x, y) 具有非负性，因为交集的大小不可能超过并集的大小。
2. 当 x = y 时，d(x, y) = 0，因为 x ∪ x = x ∩ x = x。然而，若 x ≠ y，则 x ∩ y 的大小严格小于 x ∪ y 的大小，因此 d(x, y) 严格为正。
3. d(x, y) = d(y, x)，因为并集和交集均具有对称性，即 x ∪ y = y ∪ x 且 x ∩ y = y ∩ x。
4. 关于三角不等式，回顾第 3.3.3 节可知，SIM(x, y) 表示随机最小哈希函数将 x 和 y 映射到同一值的概率。因此，Jaccard 距离 d(x, y) 即为随机最小哈希函数不将 x 和 y 映射到同一值的概率。于是，我们可以将条件 d(x, y) ≤ d(x, z) + d(z, y) 转化为以下陈述：对于随机最小哈希函数 h，h(x) ≠ h(y) 的概率不超过 h(x) ≠ h(z) 的概率与 h(z) ≠ h(y) 的概率之和。这一陈述成立的原因是，每当 h(x) ≠ h(y) 时，h(x) 和 h(y) 中至少有一个与 h(z) 不同——它们不可能同时等于 h(z)，否则 h(x) 和 h(y) 就会相同。

#### 3.5.4 余弦距离  

余弦距离适用于具有维度的空间，包括欧几里得空间及其离散版本，例如由整数分量或布尔值（0或1）分量构成的向量空间。在此类空间中，点可被视为方向。我们不区分向量与其倍数之间的差异。两个点之间的余弦距离即为指向这两个点的向量所形成的夹角。无论空间维度多少，该夹角范围始终在0到180度之间。

我们可以通过先计算角度的余弦值，再应用反余弦函数将其转换为0-180度范围内的角度来计算余弦距离。给定两个向量x和y，它们之间夹角的余弦值是点积x.y除以x和y的L2范数（即它们到原点的欧几里得距离）。回忆一下，向量[x1, x2, ..., xn]和[y1, y2, ..., yn]的点积是Σ_{i=1}^n x_i y_i。

**例 3.13：** 设两个向量分别为x = [1, 2, -1]和y = [2, 1, 1]。点积x·y为1×2 + 2×1 + (-1)×1 = 3。两个向量的L2范数都是√6。例如，x的L2范数为√(1² + 2² + (-1)²) = √6。因此，两向量夹角的余弦值为3/(√6×√6)即1/2。余弦值为1/2的夹角是60度，这就是x与y之间的余弦距离。

我们必须证明余弦距离确实是一种距离度量。我们将其定义为取值范围在0到180度之间，因此不存在负距离。当且仅当两个向量方向相同时，它们的夹角为0度。对称性显而易见：向量x与y的夹角等于y与x的夹角。三角不等式最直观的论证方式是通过物理推理：从x旋转到y的一种路径是先旋转到z，再从z旋转到y。这两段旋转角度的总和不可能小于直接从x旋转到y的角度。

#### 3.5.5 编辑距离  

当点表示为字符串时，此距离具有实际意义。两个字符串x = x₁x₂···xₙ与y = y₁y₂···yₘ之间的编辑距离，是指将x转换为y所需的最少单字符插入与删除操作次数。

示例 3.14：字符串 x = abcde 与 y = acfdeg 的编辑距离为 3。将 x 转换为 y 的步骤为：
1. 删除 b；
2. 在 c 后插入 f；
3. 在 e 后插入 g。

要将x转换为y，至少需要进行三次插入和/或删除操作。因此，d(x, y) = 3。

另一种定义和计算编辑距离d(x, y)的方法是计算x和y的最长公共子序列(LCS)。x和y的LCS是通过从x和y中删除若干位置后构造出的字符串，其长度与任何可通过此方式构造的字符串一样长。编辑距离d(x, y)可计算为x的长度加上y的长度减去它们LCS长度的两倍。

**例3.15：** 在例3.14中，字符串x=abcde与y=acfdeg存在唯一的最长公共子序列acde。我们可以确定这是可能的最长子序列，因为它包含了x和y中所有共有的字符。幸运的是，这些共有字符在两个字符串中的出现顺序一致，因此能全部用于构成最长公共子序列。注意到x的长度为5，y的长度为6，它们的最长公共子序列长度为4。因此编辑距离为5 + 6 − 2 × 4 = 3，这与例3.14中的直接计算结果一致。

再举一个例子，设x = aba，y = bab。它们的编辑距离为2。例如，我们可以通过删除第一个a，然后在末尾插入b，将x转换为y。此时存在两个最长公共子序列（LCS）：ab和ba。每个LCS都可以通过从两个字符串中各删除一个符号得到。对于同一对字符串存在多个LCS的情况，这些LCS必然具有相同长度。因此，我们可以通过公式3 + 3 − 2 × 2 = 2计算出编辑距离为2。

> [!tip]
> **非欧式空间**
> 
> 请注意，本节介绍的几种距离度量并不属于欧几里得空间。欧几里得空间的一个重要特性（我们将在第七章讨论聚类时重点提及）是：空间中点的均值始终存在且仍属于该空间。然而，考虑我们定义杰卡德距离的集合空间时，"两个集合的平均值"这一概念毫无意义。同理，在使用编辑距离的字符串空间中，我们也无法计算"字符串的平均值"。
> 
> 我们曾建议采用余弦距离的向量空间可能是欧几里得的，也可能不是。若向量的分量可取任意实数，则该空间是欧几里得的；但若限制分量为整数，则该空间便非欧几里得。请注意，例如在由两个整数分量构成的向量空间中，我们无法找到向量[1, 2]和[3, 1]的平均值——尽管若将它们视为二维欧几里得空间的成员，则可以说其平均值为[2.0, 1.5]。


编辑距离是一种距离度量。显然，任何编辑距离都不可能为负，且只有当两个字符串完全相同时，其编辑距离才为零。要证明编辑距离的对称性，只需注意到插入和删除操作的序列是可逆的——每次插入可转换为删除，反之亦然。三角不等式同样易于理解：将字符串s转换为t的一种方法是先将s转换为某个中间字符串u，再将u转换为t。因此，从s到u的编辑次数加上从u到t的编辑次数，绝不可能少于直接将s转换为t所需的最小编辑次数。

#### 3.5.6 汉明距离  

在向量空间中，我们定义两个向量之间的汉明距离为它们在各分量上不同的数量。显然，汉明距离是一种距离度量。汉明距离不可能为负，若距离为零，则两向量完全相同。该距离与两向量的顺序无关。三角不等式同样成立：若向量x与z有m个分量不同，z与y有n个分量不同，则x与y不同的分量数不超过m+n。汉明距离最常用于布尔向量（仅含0和1），但原则上，向量的分量可以来自任意集合。

**例3.16：** 向量10101与11110之间的汉明距离为3。这两个向量在第二、第四和第五分量上存在差异，而在第一和第三分量上保持一致。


### 3.6 局部敏感函数理论  

第3.4节开发的LSH技术（最小哈希函数族）是一个函数家族的示例，这些函数可以通过分桶技术组合起来，从而有效区分低距离和高距离的数据对。图3.7中S曲线的陡峭程度反映了我们能在候选对中多大程度避免假阳性和假阴性。

现在，我们将探讨除最小哈希函数外，其他能高效生成候选对的函数族。这些函数可应用于集合空间和杰卡德距离，或其他空间及距离度量方式。一个有效的函数族需满足三个条件：

1. 函数必须使相近元素对比疏远元素对更可能成为候选对。我们将在3.6.1节对此概念进行精确阐述。
2. 函数必须具有统计独立性，即可以通过独立事件的乘积规则来估算多个函数同时产生特定响应的概率。
3. 它们必须在两个方面保持高效：  
    (a) 必须能够以远低于全量比对的时间找出候选对。例如最小哈希函数具备这种特性——我们可以在与数据规模成正比的时间内完成集合哈希计算（而非与集合数量的平方成正比）。由于具有相同取值的集合会被分配到同一哈希桶，这意味着我们以远低于集合对数量的时间就隐式生成了单个最小哈希函数的候选对。  
    (b) 必须能够组合构建出更有效规避假阳性/假阴性的函数，且组合函数的时间复杂度仍需远低于全量比对。例如3.4.1节的分桶技术：通过组合多个仅满足条件3a但本身不具备理想S曲线特性的最小哈希函数，最终生成具有S曲线特性的组合函数。

我们的首要步骤是广义地定义"局部敏感函数"。随后将探讨该思想在若干应用场景中的具体实践。最后我们将讨论如何将该理论应用于采用余弦距离或欧氏距离度量的任意数据。


#### 3.6.1 局部敏感函数  

在本节中，我们将讨论用于判断两个项目是否应成为候选对的函数。这类函数通常会对项目进行“哈希”处理，并根据哈希结果是否相同来做出决策。为方便起见，我们用f(x)=f(y)表示函数f(x,y)判定“是，将x和y作为候选对”，并将此作为该含义的简写形式。同时，f(x)≠f(y)表示“除非其他函数判定需要，否则不将x和y作为候选对”。

这种形式的函数集合将被称为函数族。例如，基于特征矩阵行可能排列的每个最小哈希函数，构成一个函数族。

设d1 < d2为根据某距离度量d定义的两种距离。若对于函数族F中的每个函数f满足以下条件，则称F是(d1, d2, p1, p2)-敏感的：

1. 当d(x, y) ≤ d1时，f(x) = f(y)的概率至少为p1；
2. 当d(x, y) ≥ d2时，f(x) = f(y)的概率至多为p2。


图3.9：(d1, d2, p1, p2)-敏感函数的行为特性

图3.9展示了我们对于(d1,d2,p1,p2)-敏感族中给定函数判定两个项目为候选对的概率预期。需要注意的是，当项目间距离严格处于d1与d2之间时，我们并未作任何说明——但可以通过调整使d1和d2无限接近。代价在于这通常会导致p1和p2也随之趋近。后文将说明，在保持d1和d2固定的情况下，实现p1与p2的分离是可行的。


#### 3.6.2 Locality-Sensitive Families for Jaccard Distance

目前，我们只有一种方法来寻找局部敏感函数族：使用最小哈希函数族，并假设距离度量是Jaccard距离。与之前一样，我们将最小哈希函数h解释为当且仅当h(x) = h(y)时，使x和y成为候选对。

* 对于任意满足0 ≤ d1 < d2 ≤ 1的d1和d2，minhash函数族都是一个(d1, d2, 1−d1, 1−d2)-敏感的函数族。

原因是，如果d(x, y) ≤ d1（其中d表示杰卡德距离），那么SIM(x, y) = 1 − d(x, y) ≥ 1 − d1。但我们知道，x和y的杰卡德相似度等于最小哈希函数将x和y映射到同一值的概率。类似的论证也适用于d2或其他任何距离。

**示例3.17：** 我们可以设d1=0.3，d2=0.6。此时可以断言最小哈希函数族是一个(0.3,0.6,0.7,0.4)-敏感族。也就是说，当x和y之间的杰卡德距离不超过0.3时（即SIM(x,y)≥0.7），最小哈希函数将x和y映射为相同值的概率至少为0.7；而当x和y之间的杰卡德距离不小于0.6时（即SIM(x,y)≤0.4），它们被映射为相同值的概率至多为0.4。需要注意的是，我们也可以选择其他d1和d2值来作出相同断言，唯一要求是d1必须小于d2。

#### 3.6.3 局部敏感族的放大  

假设给定一个(d1, d2, p1, p2)-敏感的族F。我们可以通过AND-构造在F上构建一个新族F′，其定义如下：F′的每个成员由F的r个成员组成（r为固定值）。若f属于F′，且f由F的成员集合{f1, f2, ..., fr}构造而成，则当且仅当对所有i = 1, 2, ..., r满足fi(x) = fi(y)时，我们称f(x) = f(y)。注意，该构造模拟了单个波段中r行的作用：当波段内所有r行都判定x和y相等时（即该行认为x和y是候选对），该波段就将x和y标记为候选对。

由于F族的成员是独立选择以构成F'族的成员，因此我们可以断言F'族是一个d1, d2, (p1)^r, (p2)^r敏感的哈希族。也就是说，对于任意点对(p)，若p表示F族成员将(x,y)判定为候选对的概率，那么F'族成员将其判定为候选对的概率就是p^r。

另一种构造方法称为OR-构造，它能将(d1, d2, p1, p2)-敏感的哈希函数族F转化为(d1, d2, 1 − (1 − p1)^b, 1 − (1 − p2)^b)-敏感的哈希函数族F′。F′中的每个函数f由F中的b个函数（记为f1, f2, ..., fb）构成，我们定义f(x) = f(y)当且仅当存在至少一个i值使得fi(x) = fi(y)。这种OR-构造模拟了组合多个波段的效果：只要任意一个波段使x和y成为候选对，它们就会成为候选对。

若p表示F中某个成员将(x, y)判定为候选对的概率，则1−p即为不将其判定为候选对的概率。(1−p)^b表示f1, f2, ..., fb全部不将(x, y)判定为候选对的概率，而1−(1−p)^b则表示至少有一个fi会将其判定为候选对的概率，因此f最终会将(x, y)判定为候选对。

需要注意的是，AND 构造会降低所有概率值，但若合理选择函数族F和参数r，我们能使较小的概率p2无限趋近于0，而较高的概率p1仍显著远离0。类似地，OR构造会使所有概率值上升，但通过合理选择F和参数b，可使较大概率逼近1，同时较小概率仍与1保持距离。我们可以按任意顺序级联AND与OR构造，使得低概率无限接近0、高概率无限接近1。当然，使用的构造次数越多，选取的r和b值越大，所需原始函数族的基数就越大。因此，最终函数族的性能越优越，应用该族函数所需的时间成本就越高。

**示例3.18：** 假设我们初始有一个族F。首先采用r=4的AND构造法生成族F₁，接着对F₁应用b=4的OR构造法产生第三个族F₂。需要注意的是，F₂的每个成员均由F的16个成员构建而成，这种情况类似于初始使用16个最小哈希函数，并将它们划分为四个波段，每个波段包含四行。

$$\begin{array}{|c|c|}
\hline
p & 1 - (1 - p^4)^4 \\
\hline
0.2 & 0.0064 \\
0.3 & 0.0320 \\
0.4 & 0.0985 \\
0.5 & 0.2275 \\
0.6 & 0.4260 \\
0.7 & 0.6666 \\
0.8 & 0.8785 \\
0.9 & 0.9860 \\
\hline
\end{array}$$

**图3.10：** 4路AND构造后接4路OR构造的效果


四路与门函数将任意概率p转换为p⁴。当我们后续采用四路或门结构时，该概率会进一步转化为1−(1−p⁴)⁴。图3.10展示了该函数的部分取值。这个函数呈现S型曲线特征：初始保持低位，随后急剧上升（虽然斜率最高不超过2），最终在高位趋于平缓。与所有S型曲线相同，它具有不动点——即应用该函数时保持不变的p值。本例中，不动点是满足p=1−(1−p⁴)⁴的p值。通过观察可知该不动点介于0.7至0.8之间：低于该值时概率被衰减，高于该值时概率被放大。因此，若选取高于不动点的高概率和低于不动点的低概率，就能实现低概率衰减而高概率放大的预期效果。

假设F是一个最小哈希函数族，被视为(0.2, 0.6, 0.8, 0.4)-敏感族。那么通过4路AND操作后接4路OR操作构建的函数族F2，就是一个(0.2, 0.6, 0.8785, 0.0985)-敏感族，这可以从图3.10中0.2和0.6对应的行读出。通过用F2替换F，我们同时降低了假阴性率和假阳性率，代价是应用这些函数所需时间增加了16倍。
$$\begin{array}{|c|c|}
\hline
p & (1 - (1 - p)^4)^4 \\
\hline
0.1 & 0.0140 \\
0.2 & 0.1215 \\
0.3 & 0.3334 \\
0.4 & 0.5740 \\
0.5 & 0.7725 \\
0.6 & 0.9015 \\
0.7 & 0.9680 \\
0.8 & 0.9936 \\
\hline
\end{array}$$
**图3.11：** 4路OR构造后接4路AND构造的效果

**示例3.19**：在相同成本下，我们可以先应用4路OR构造，再应用4路AND构造。图3.11展示了该构造对概率的转换效果。例如，假设F是一个(0.2, 0.6, 0.8, 0.4)-敏感族，那么构造后的新族将变为(0.2, 0.6, 0.9936, 0.5740)-敏感族。这种选择未必最优——虽然较高概率值更接近1，但较低概率也随之上升，会导致误报数量增加。

**示例3.20**：我们可以根据需要级联多个构造。例如，可以先将示例3.18的构造应用于最小哈希函数族，再对所得函数族应用示例3.19的构造。这样构建的函数族中，每个函数将由256个最小哈希函数组合而成。该构造能够将(0.2, 0.8, 0.8, 0.2)-敏感的函数族转化为(0.2, 0.8, 0.99999996, 0.0008715)-敏感的函数族。

## 3.7 适用于其他距离度量的LSH族  

并非所有距离度量都能保证存在局部敏感哈希函数族。目前我们仅针对杰卡德距离介绍了此类函数族。本节将展示如何为汉明距离、余弦距离以及常规欧氏距离构建局部敏感的哈希函数族。

#### 3.7.1 面向汉明距离的LSH函数族  

为汉明距离构建局部敏感的函数族非常简单。假设我们有一个d维向量空间，h(x, y)表示向量x和y之间的汉明距离。如果我们取向量的任意一个位置，比如第i个位置，我们可以定义函数fi(x)为向量x的第i位。那么fi(x) = fi(y)当且仅当向量x和y在第i位上一致。因此，对于随机选择的i，fi(x) = fi(y)的概率恰好是1 − h(x, y)/d；也就是说，这个概率等于x和y在位置上一致的比例。

这种情况与我们遇到的最小哈希（minhashing）情形几乎完全相同。因此，由函数{f1, f2, ..., fd}构成的族F是一个(d1, d2, 1−d1/d, 1−d2/d)-敏感的哈希函数族，其中d1 < d2。该哈希函数族与最小哈希函数族之间仅存在两点差异。

1. 雅卡尔距离的取值范围是0到1，而维度为d的向量空间上的汉明距离取值范围是0到d。因此需要通过除以d来缩放距离，将其转化为概率值。
2. 虽然最小哈希函数的供应几乎是无限的，但汉明距离对应的函数族F的大小仅为d。

第一点无关紧要，它仅要求我们在适当的时候除以d即可。第二点则更为关键。如果d值较小，那么通过AND和OR结构构建的函数数量将受到限制，从而制约了我们使S曲线变得陡峭的程度。

#### 3.7.2 随机超平面与余弦距离  

回顾3.5.4节可知，两个向量之间的余弦距离就是这两个向量之间的夹角。例如，在图3.12中，我们看到向量x和y之间有一个夹角θ。需要注意的是，这些向量可能位于一个高维空间中，但它们始终会定义一个平面，而它们之间的夹角就是在这个平面中测量的。图3.12是包含x和y的平面的“俯视图”。

[图]

**图3.12**：两个向量之间的夹角为θ

假设我们选择一个通过原点的超平面。该超平面与x和y坐标平面相交于一条直线。图3.12展示了两种可能的超平面：其中一个的交线是虚线，另一个的交线是点线。要随机选择一个超平面，我们实际上是选择该超平面的法向量，记为v。该超平面就是所有与v的点积为零的点构成的集合。

首先，考虑一个向量v，它垂直于图3.12中虚线所表示的投影超平面；也就是说，x和y位于该超平面的两侧。因此，点积v·x和v·y将具有不同的符号。例如，如果我们假设v是一个在x和y所在平面上的投影位于图3.12中虚线上方的向量，那么v·x为正，而v·y为负。反之，若法向量v朝相反方向延伸至虚线下方，则v·x为负而v·y为正，但两者的符号仍然相反。

另一方面，随机选择的向量v可能垂直于某个超平面，如图3.12中的虚线所示。在这种情况下，v·x和v·y具有相同的符号。若v的投影向右延伸，则两个点积均为正；若v向左延伸，则两者均为负。

随机选择的向量与虚线所示的超平面而非点线所示的超平面垂直的概率是多少？由于随机超平面与xy平面相交所形成的直线角度均匀分布，因此该超平面呈现虚线状的概率为θ/180，否则将呈现点线状。

因此，我们局部敏感哈希函数族F中的每个哈希函数f都由一个随机选择的向量vf构建而成。给定两个向量x和y，当且仅当点积vf·x与vf·y的符号相同时，称f(x)=f(y)。此时F就成为余弦距离下的局部敏感函数族。其参数设置与3.6.2节描述的杰卡德距离函数族基本相同，唯一的区别在于距离范围是0-180度而非0-1。具体而言，F是一个(d1,d2,(180-d1)/180,d2/180)敏感的哈希函数族。基于此，我们可以像处理基于最小哈希的函数族那样，对该函数族进行任意程度的扩展放大。


#### 3.7.3 草图法  

相较于从所有可能的向量中随机选取，将选择范围限制在由+1和-1构成的向量上就足以保证随机性。任意向量x与由+1和-1组成的向量v的点积计算方法是：先累加x中对应v为+1的分量，再减去x中对应v为-1的分量。

如果我们选取一组随机向量，例如v₁, v₂, ..., vₙ，可以通过计算v₁·x, v₂·x, ..., vₙ·x将它们应用于任意向量x，然后将正值替换为+1，负值替换为−1。结果称为x的草图。对于零值可以任意处理，例如随机选择+1或−1。由于点积结果为零的概率极小，这种选择基本不会产生影响。

**例3.21：** 假设我们的空间由4维向量构成，随机选取三个向量：v1 = [+1, −1, +1, +1]，v2 = [−1, +1, −1, +1]，以及v3 = [+1, +1, −1, −1]。对于向量x = [3, 4, 5, 6]，其素描结果为[+1, +1, −1]。具体而言，v1·x = 3−4+5+6 = 10。由于结果为正，素描的第一个分量为+1。同理，v2·x = 3且v3·x = −4，因此素描的第二个分量为+1，第三个分量为−1。

考虑向量 y = [4, 3, 2, 1]。我们同样可以计算出其素描值为 [+1, −1, +1]。由于 x 和 y 的素描值在 1/3 的位置上一致，我们估计它们之间的夹角为 120 度。也就是说，随机选择的一个超平面看起来像图 3.12 中虚线的概率是像点线的两倍。

上述结论实际上大错特错。我们可以通过计算向量x与y的夹角余弦值来验证，该值为x·y的点积结果（6×1 + 5×2 + 4×3 + 3×4=40）除以两向量的模长。经计算，x的模长为√(6²+5²+4²+3²)=9.274，y的模长为√(1²+2²+3²+4²)=5.477。因此两向量夹角的余弦值为0.7875，对应角度约为38度。但若考察所有16个由+1和-1组成的四维向量v时，会发现仅有四个向量（v2、v3及其互补向量[+1,-1,+1,-1]和[-1,-1,+1,+1]）与x、y的点积符号相异。这意味着若选用这十六个向量构建草图，估算得到的角度值将是180度除以4，即45度。

#### 3.7.4 面向欧氏距离的LSH函数族  

现在，让我们转向欧氏距离（第3.5.2节），探讨能否为该距离设计一个局部敏感的哈希函数族。我们将从二维欧氏空间开始构建。函数族F中的每个哈希函数f都将与空间中随机选取的一条直线相关联。如图3.13所示，选取常数a并将直线分割成长度为a的若干段（图中"随机"直线被调整为水平方向以便示意）。

[图]

**图3.13**：相距d ≫ a的两点被哈希到同一桶中的概率很小

这条线段被分割为若干桶，函数f将点哈希映射到这些桶中。点的投影落在哪个桶区间，就被哈希到对应的桶。当两点间距离d远小于参数a时，它们有很大概率会被哈希到同一个桶中，此时哈希函数f将判定这两点相等。例如，若d=a/2，则两点落入同一桶的概率至少为50%。事实上，若随机选择的线段与两点连线的夹角θ较大，则两点落入同一桶的概率会更高——当θ达到90度时，两点必定会落入同一个桶中。

然而，假设d大于a。为了使两点有机会落入同一桶中，必须满足d cos θ ≤ a。图3.13的示意图说明了这一条件的必要性。需要注意的是，即使d cos θ ≪ a，两点仍不一定会落入同一桶中。但我们可以确定以下结论：若d ≥ 2a，则两点落入同一桶的概率不超过1/3。其原因是，要使cos θ小于1/2，θ必须处于60度至90度之间；若θ处于0度至60度范围，则cos θ大于1/2。由于θ是平面上两条随机直线之间较小的夹角，θ落在0到60度区间的概率是落在60到90度区间的两倍。

我们得出以下结论：上述构造的哈希函数族F构成一个(a/2, 2a, 1/2, 1/3)-敏感的局部敏感哈希族。具体而言，当两点间距离不超过a/2时，它们被映射到同一哈希桶的概率至少为1/2；而当两点间距离不小于2a时，它们被映射到同一哈希桶的概率至多为1/3。如同我们讨论过的其他局部敏感哈希函数实例，可以通过适当方法对该函数族进行概率增强。

#### 3.7.5 欧氏空间的更多LSH函数族  

第3.7.4节开发的哈希函数族存在一些不尽如人意之处。首先，该技术仅针对二维欧氏空间进行了描述。如果我们的数据是高维空间中的点，该如何处理？其次，对于Jaccard距离和余弦距离，只要满足d1 < d2，我们就能为任意距离对d1和d2开发出局部敏感函数族。而在第3.7.4节中，我们似乎需要更强的条件d1 < 4d2。

然而，我们主张对于任意d1 < d2及任意维度数，都存在一个局部敏感的哈希函数族。该函数族的哈希函数仍源自空间中的随机直线和划分直线的桶大小a。我们仍通过将点投影到直线上进行哈希处理。给定d1 < d2，我们可能无法精确计算距离为d1的两点落入同一桶的概率p1，但可以确定该概率必然大于距离为d2的两点落入同一桶的概率p2。这是因为该概率必然随着距离减小而增大。因此，即便难以计算p1和p2的具体值，我们仍可断言：对于任意d1 < d2及任意给定维度数，都存在一个满足(d1, d2, p1, p2)敏感特性的哈希函数族。

利用第3.6.3节的放大技术，我们可以调整这两个概率，使其围绕任意特定值分布，并拉开任意所需的差距。当然，想要让概率差距越大，所需使用的底层哈希函数集F中的函数数量就越多。

## 3.8 局部敏感哈希的应用 

本节将探讨局部敏感哈希在实际应用中的三个示例。每种情况下，我们所学的技术都需要进行调整，以满足问题的特定约束条件。我们将涵盖的三大主题是：

1. 实体解析：该术语指代将指向同一现实实体的数据记录进行匹配，例如同一个人。此处解决的核心问题是记录间的相似度无法完全契合理论构建所依据的相似集合模型或相似向量模型。
2. 指纹匹配：虽然指纹可被表示为集合形式，但我们将探索与最小哈希不同的局部敏感哈希函数族来实现匹配。
3. 新闻文章匹配：在此场景中，我们采用一种聚焦在线新闻网页核心内容的特殊分片方法，通过忽略广告及报纸专属内容等无关元素来构建匹配模型。

#### 3.8.1 实体解析  

通常我们会拥有多个数据集，并且知道它们指向相同的实体。例如，多个不同的文献数据源可能提供关于相同书籍或论文的信息。一般情况下，我们会遇到描述某类实体（如人或书籍）的记录。这些记录可能具有相同的格式，也可能格式各异，包含不同类型的信息。

关于同一实体的信息可能存在差异，其原因多种多样——即便所涉字段本应相同。例如：不同记录中名称的表述可能因拼写错误、缺少中间名首字母、使用昵称等情况而不同。"Bob S. Jomes"与"Robert Jones Jr."可能是同一人，也可能不是。若记录来源不同，字段设置也会存在差异：某个数据源可能包含"年龄"字段，而另一个则没有；后者可能设有"出生日期"字段，也可能完全不包含出生时间信息。

#### 3.8.2 实体解析示例  

我们将通过一个真实案例来研究LSH如何应用于实体解析问题。公司A受雇于公司B，为其招揽客户。只要客户保持订阅状态，公司B就会每年向公司A支付服务费。后来双方发生争执，对于A究竟为B提供了多少客户存在分歧。双方各自拥有约100万条记录，其中部分记录描述的是同一批人——即A为B提供的客户群。这些记录包含不同的数据字段，但遗憾的是没有任何字段明确标注"该客户由A提供给B"。因此，问题的核心在于匹配两个数据集的记录，以判断哪些记录对代表同一个人。

每条记录都包含人员的姓名、地址和电话号码字段。然而，这些字段中的值可能因多种原因存在差异。不仅存在第3.8.1节提到的拼写错误和其他命名差异，还存在其他可能导致不一致的情况。客户可能向A提供家庭电话，而向B提供手机号码；或者他们可能搬家后只通知B而未告知A（因为他们不再需要与A保持联系）。此外，电话号码的区号有时也会发生变更。

识别记录的策略是通过对三个字段（姓名、地址和电话）的差异进行评分来实现的。为了创建一个描述两条记录（一条来自A，另一条来自B）描述同一人可能性的分数，每个字段被分配了100分，因此所有三个字段完全匹配的记录得分为300分。然而，每个字段的不匹配都会导致扣分。初步方案采用编辑距离（第3.5.5节）计算差异，但扣分随距离增加呈二次方增长。随后，通过某些公开表格在适当情况下降低扣分。例如，“Bill”和“William”被视为仅相差一个字母，尽管它们的编辑距离为5。

然而，对所有一万亿条记录对进行评分并不可行。因此，我们采用了一种简单的局部敏感哈希（LSH）方法来聚焦潜在匹配项。该方法使用了三种“哈希函数”：第一种仅当记录具有完全相同的名称时才将其分配到同一哈希桶；第二种对地址执行相同操作，第三种则针对电话号码。实际操作中并未真正计算哈希值，而是通过排序实现——首先按名称排序，使名称相同的记录连续排列，从而对这些记录的名称、地址和电话字段进行整体相似性评分；接着按地址排序，对地址相同的记录评分；最后按电话号码排序，对号码相同的记录进行评分。

这种方法漏掉了一对确实代表同一个人的记录，但三个字段都没有完全匹配。由于目标是在法庭上证明这两人是同一人，这样的记录对不太可能被法官认为足够相似而予以接受。

#### 3.8.3 验证记录匹配  

剩下的问题是确定多高的分数才能表明两条记录确实代表同一个人。在当前示例中，有一种简单的方法可以做出判断，该技术也可应用于许多类似场景。具体做法是查看相关记录的创建日期，并假设在A公司购买服务后，最多90天内一定会在B公司完成注册。因此，若随机选取两条记录进行匹配（仅限制B记录日期比A记录日期晚0至90天），其平均延迟时间应为45天。

研究发现，在获得满分300分的配对中，平均延迟为10天。若假设300分配对均为正确匹配，则可观察任意分数s对应的配对组，并计算这些配对的平均延迟。设该平均延迟为x，且分数s配对中真实匹配的比例为f，则有等式x = 10f + 45(1 - f)，即x = 45 - 35f。由此可解得：分数s配对中真实匹配的比例为(45 - x)/35。

> [!NOTE]
> 记录匹配何时足够可靠？  
> 
> 虽然每个案例各不相同，但了解3.8.3节的实验在3.8.2节数据上的表现或许具有参考价值。对于分数降至185的情况，x值非常接近10，这意味着这些分数实质上表明两条记录代表同一个人的可能性近乎1。需注意，此例中185分对应的场景是：一个字段完全匹配（这是必要条件，否则记录根本不会被评分），另一个字段完全不同，第三个字段存在微小差异。此外，即使分数低至115，x值仍明显小于45，说明部分低分匹配对确实代表同一人。115分对应的情形是：一个字段匹配，而另外两个字段仅存在微弱相似性。


每当出现以下情况时，均可采用相同技巧：

1. 存在一个评分系统，用于评估两条记录代表同一实体的可能性；
2. 存在某个未参与评分的字段，我们可以从中提取出一个度量指标，该指标在真匹配对和假匹配对之间的平均值存在差异。

例如，假设在我们的示例中，公司A和公司B都记录了“身高”字段。我们可以计算随机记录对之间的平均身高差异，也可以计算获得完美匹配分数（因而必然代表同一实体）的记录之间的平均身高差异。对于给定分数s，我们可以评估具有该分数的记录对的平均身高差异，并估计这些记录代表同一实体的概率。具体而言，若h0表示完美匹配对的平均身高差异，h1表示随机记录对的平均身高差异，h表示分数为s的记录对的平均身高差异，则分数s对应的优质记录对比例为(h1 − h)/(h1 − h0)。

#### 3.8.4 指纹匹配  

当通过计算机进行指纹匹配时，通常的表示方式并非图像，而是一组 minutiae（细节特征点）所在位置的集合。在指纹描述中，minutia 指代指纹中发生异常情况的位置，例如两条纹线交汇或某条纹线终止。若在指纹上覆盖网格，便可通过 minutiae 所在的网格区域集合来表示该指纹。

理想情况下，在叠加网格之前，指纹会经过尺寸和方向归一化处理。这样，如果我们采集同一手指的两幅图像，就能确保细节特征点落在完全相同的网格单元中。本文不探讨图像归一化的最佳方法，而是假定通过综合运用网格尺寸选择、当细节点靠近网格边界时将其分配到相邻单元等技术手段，我们可以认为：来自同一手指的两幅图像的网格单元在细节特征点存在与否的匹配概率，会显著高于来自不同手指的图像网格单元。

因此，指纹可以通过网格方块的集合来表示——这些方块是其特征点所在的位置——并像任何集合一样使用杰卡德相似度或距离进行比较。然而，指纹比对存在两种形式：  
• ​**多对一问题**​ 是常见的场景。例如在枪支上发现一枚指纹，需将其与大型数据库中的所有指纹进行比对，以找出匹配项。  
• ​**多对多问题**​ 则是针对整个数据库，检测是否存在代表同一人的指纹对。

虽然多对多版本与我们寻找相似项时遵循的模型相匹配，但同样的技术也可用于加速多对一问题的解决。

#### 3.8.5 适用于指纹匹配的LSH族  

我们可以对表示指纹的集合进行最小哈希处理，并采用3.4节的标准LSH技术。但由于这些集合是从相对较小的网格点集合（可能为1000个）中选取的，将其最小哈希为更简洁签名的必要性尚不明确。此处我们将研究另一种形式的局部敏感哈希，该技术特别适用于我们讨论的此类数据。

假设在某个示例中，随机指纹的随机网格方块内存在细节特征点的概率为20%。同时假定，若两个指纹来自同一手指，且其中一个指纹在给定网格方块中存在细节特征点，则另一个指纹在同一位置也存在细节特征点的概率为80%。我们可以按如下方式定义一个局部敏感的哈希函数族：该函数族F中的每个函数f由三个网格方块定义。当两个指纹在这三个网格方块中均存在细节特征点时，函数f输出"是"，否则输出"否"。换言之，可以认为函数f将所有在这三个指定网格点均存在细节特征点的指纹映射到同一个存储桶中，而将其他每个指纹分别映射到各自的独立存储桶。在下文中，我们将把前一种存储桶称为函数f的"专属"存储桶，而忽略那些必须作为单例存在的存储桶。

若要解决多对一问题，我们可以使用函数族F中的多个函数，并预先计算这些函数回答"是"的指纹桶。当需要匹配新指纹时，我们确定该指纹属于哪些桶，并与这些桶中的所有指纹进行比对。针对多对多问题，我们需为每个函数计算对应的指纹桶，并逐一比对所有桶中的指纹。

让我们思考需要多少函数才能以合理概率匹配到结果，而无需将枪支上的指纹与数据库中数百万指纹逐一比对。首先，对于函数集F中的任意函数f，来自不同手指的两个指纹落入同一桶的概率为(0.2)^6=0.000064。这是因为只有当两个指纹都在f关联的三个网格点中各有一个特征点时才会被归入同一桶，而每个独立事件发生的概率均为0.2。

现在，考虑来自同一手指的两个指纹最终落入f对应桶中的概率。第一个指纹在f所属的三个方格中均出现细节特征的概率是(0.2)³=0.008。但如果确实如此，那么另一个指纹也满足该条件的概率为(0.8)³=0.512。因此，若两个指纹来自同一手指，它们同时出现在f对应桶中的概率为0.008×0.512=0.004096。这个概率并不高，大约为两百分之一。然而，如果我们使用F中的多个函数（但不过多），就能在避免过多假阳性（即需要比对但不匹配的指纹）的同时，获得较高的同一手指指纹匹配概率。

**例3.22**：具体而言，假设我们随机选取F族的1024个函数进行运算。接着通过对其执行1024路"或"操作构建新函数族F1。此时F1将同一手指的指纹归入至少一个相同桶的概率为1 − (1 − 0.004096)^1024 = 0.985；而不同手指的指纹被误归入同一桶的概率为(1 − (1 − 0.000064)^1024 = 0.063。这意味着我们得到约1.5%的假阴性率和约6.3%的假阳性率。

The result of Example 3.22 is not the best we can do. While it offers only a 1.5% chance that we shall fail to identify the fingerprint on the gun, it does force us to look at 6.3% of the entire database. Increasing the number of functions from F will increase the number of false positives, with only a small benefit of reducing the number of false negatives below 1.5%. On the other hand, we can also use the AND construction, and in so doing, we can greatly reduce the probability of a false positive, while making only a small increase in the false-negative rate. For instance, we could take 2048 functions from F in two groups of 1024. Construct the buckets for each of the functions. However, given a fingerprint P on the gun: 1. Find the buckets from the first group in which P belongs, and take the union of these buckets. 2. Do the same for the second group. 3. Take the intersection of the two unions. 4. Compare P only with those fingerprints in the intersection. 3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 97 Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints can be represented by their integer indices in the database. If we use this scheme, the probability of detecting a matching fingerprint is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the probability of a false positive is (0.063)2 = 0.00397. That is, we only have to examine about 1/250th of the database. 3.8.6 Similar News Articles Our last case study concerns the problem of organizing a large repository of on-line news articles by grouping together Web pages that were derived from the same basic text. It is common for organizations like The Associated Press to produce a news item and distribute it to many newspapers. Each newspaper puts the story in its on-line edition, but surrounds it by information that is special to that newspaper, such as the name and address of the newspaper, links to related articles, and links to ads. In addition, it is common for the newspaper to modify the article, perhaps by leaving off the last few paragraphs or even deleting text from the middle. As a result, the same news article can appear quite different at the Web sites of different newspapers. The problem looks very much like the one that was suggested in Section 3.4: find documents whose shingles have a high Jaccard similarity. Note that this problem is different from the problem of finding news articles that tell about the same events. The latter problem requires other techniques, typically examining the set of important words in the documents (a concept we discussed briefly in Section 1.3.1) and clustering them to group together different articles about the same topic. However, an interesting variation on the theme of shingling was found to be more effective for data of the type described. The problem is that shingling as we described it in Section 3.2 treats all parts of a document equally. However, we wish to ignore parts of the document, such as ads or the headlines of other articles to which the newspaper added a link, that are not part of the news article. It turns out that there is a noticeable difference between text that appears in prose and text that appears in ads or headlines. Prose has a much greater frequency of stop words, the very frequent words such as “the” or “and.” The total number of words that are considered stop words varies with the application, but it is common to use a list of several hundred of the most frequent words. Example 3.23 : A typical ad might say simply “Buy Sudzo.” On the other hand, a prose version of the same thought that might appear in an article is “I recommend that you buy Sudzo for your laundry.” In the latter sentence, it would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words. ✷ 98 CHAPTER 3. FINDING SIMILAR ITEMS Suppose we define a shingle to be a stop word followed by the next two words. Then the ad “Buy Sudzo” from Example 3.23 has no shingles and would not be reflected in the representation of the Web page containing that ad. On the other hand, the sentence from Example 3.23 would be represented by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for your laundry,” and “your laundry x,” where x is whatever word follows that sentence. Suppose we have two Web pages, each of which consists of half news text and half ads or other material that has a low density of stop words. If the news text is the same but the surrounding material is different, then we would expect that a large fraction of the shingles of the two pages would be the same. They might have a Jaccard similarity of 75%. However, if the surrounding material is the same but the news content is different, then the number of common shingles would be small, perhaps 25%. If we were to use the conventional shingling, where shingles are (say) sequences of 10 consecutive characters, we would expect the two documents to share half their shingles (i.e., a Jaccard similarity of 1/3), regardless of whether it was the news or the surrounding material that they shared. 3.8.7 Exercises for Section 3.8 Exercise 3.8.1 : Suppose we are trying to perform entity resolution among bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that all references include a year of publication, and this year is equally likely to be any of the ten most recent years. Further, suppose that we discover that among the pairs of references with a perfect score, there is an average difference in the publication year of 0.1.5 Suppose that the pairs of references with a certain score s are found to have an average difference in their publication dates of 2. What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference in publication date between random pairs is 5 or 5.5. You need to calculate it exactly, and you have enough information to do so. Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80% chance of a second copy of a fingerprint having a minutia in a grid square where the first copy does, and each function in F being formed from three grid squares. In Example 3.22, we constructed family F1 by using the OR construction on 1024 members of F. Suppose we instead used family F2 that is a 2048-way OR of members of F. (a) Compute the rates of false positives and false negatives for F2. 5We might expect the average to be 0, but in practice, errors in publication year do occur. 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 99 (b) How do these rates compare with what we get if we organize the same 2048 functions into a 2-way AND of members of F1, as was discussed at the end of Section 3.8.5? Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F ′ defined like F, but using only two randomly chosen grid squares. Construct another set of functions F ′ 1 from F ′ by taking the n-way OR of functions from F ′ . What, as a function of n, are the false positive and false negative rates for F ′ 1 ? Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.22, but we want to solve the many-many problem. (a) If two fingerprints are from the same finger, what is the probability that they will not be compared (i.e., what is the false negative rate)? (b) What fraction of the fingerprints from different fingers will be compared (i.e., what is the false positive rate)? ! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2, and we construct a new set of functions F3 by an n-way OR of functions in F. For what value of n is the sum of the false positive and false negative rates minimized? 3.9 Methods for High Degrees of Similarity LSH-based methods appear most effective when the degree of similarity we accept is relatively low. When we want to find sets that are almost identical, there are other methods that can be faster. Moreover, these methods are exact, in that they find every pair of items with the desired degree of similarity. There are no false negatives, as there can be with LSH. 3.9.1 Finding Identical Items The extreme case is finding identical items, for example, Web pages that are identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents that fell into the same bucket. That scheme should work well, unless all the documents begin with the same characters, such as an HTML header. Our second thought would be to use a hash function that examines the entire document. That would work, and if we use enough buckets, it would be very rare that two documents went into the same bucket, yet were not identical. The downside of this approach is that we must examine every character of every document. If we limit our examination to a small number of characters, then 100 CHAPTER 3. FINDING SIMILAR ITEMS we never have to examine a document that is unique and falls into a bucket of its own. A better approach is to pick some fixed random positions for all documents, and make the hash function depend only on these. This way, we can avoid a problem where there is a common prefix for all or most documents, yet we need not examine entire documents unless they fall into a bucket with another document. One problem with selecting fixed positions is that if some documents are short, they may not have some of the selected positions. However, if we are looking for highly similar documents, we never need to compare two documents that differ significantly in their length. We exploit this idea in Section 3.9.3. 3.9.2 Representing Sets as Strings Now, let us focus on the harder problem of finding, in a large collection of sets, all pairs that have a high Jaccard similarity, say at least 0.9. We can represent a set by sorting the elements of the universal set in some fixed order, and representing any set by listing its elements in this order. The list is essentially a string of “characters,” where the characters are the elements of the universal set. These strings are unusual, however, in that: 1. No character appears more than once in a string, and 2. If two characters appear in two different strings, then they appear in the same order in both strings. Example 3.24 : Suppose the universal set consists of the 26 lower-case letters, and we use the normal alphabetical order. Then the set {d, a, b} is represented by the string abd. ✷ In what follows, we shall assume all strings represent sets in the manner just described. Thus, we shall talk about the Jaccard similarity of strings, when strictly speaking we mean the similarity of the sets that the strings represent. Also, we shall talk of the length of a string, as a surrogate for the number of elements in the set that the string represents. Note that the documents discussed in Section 3.9.1 do not exactly match this model, even though we can see documents as strings. To fit the model, we would shingle the documents, assign an order to the shingles, and represent each document by its list of shingles in the selected order. 3.9.3 Length-Based Filtering The simplest way to exploit the string representation of Section 3.9.2 is to sort the strings by length. Then, each string s is compared with those strings t that follow s in the list, but are not too long. Suppose the upper bound on Jaccard distance between two strings is J. For any string x, denote its length by Lx. Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 101 A Better Ordering for Symbols Instead of using the obvious order for elements of the universal set, e.g., lexicographic order for shingles, we can order symbols rarest first. That is, determine how many times each element appears in the collection of sets, and order them by this count, lowest first. The advantage of doing so is that the symbols in prefixes will tend to be rare. Thus, they will cause that string to be placed in index buckets that have relatively few members. Then, when we need to examine a string for possible matches, we shall find few other strings that are candidates for comparison. have more than Ls members, while their union has at least Lt members. Thus, the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt. That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt, or equivalently, Lt ≤ Ls/J. Example 3.25 : Suppose that s is a string of length 9, and we are looking for strings with at least 0.9 Jaccard similarity. Then we have only to compare s with strings following it in the length-based sorted order that have length at most 9/0.9 = 10. That is, we compare s with those strings of length 9 that follow it in order, and all strings of length 10. We have no need to compare s with any other string. Suppose the length of s were 8 instead. Then s would be compared with following strings of length up to 8/0.9 = 8.89. That is, a string of length 9 would be too long to have a Jaccard similarity of 0.9 with s, so we only have to compare s with the strings that have length 8 but follow it in the sorted order. ✷ 3.9.4 Prefix Indexing In addition to length, there are several other features of strings that can be exploited to limit the number of comparisons that must be made to identify all pairs of similar strings. The simplest of these options is to create an index for each symbol; recall a symbol of a string is any one of the elements of the universal set. For each string s, we select a prefix of s consisting of the first p symbols of s. How large p must be depends on Ls and J, the lower bound on Jaccard distance. We add string s to the index for each of its first p symbols. In effect, the index for each symbol becomes a bucket of strings that must be compared. We must be certain that any other string t such that SIM(s, t) ≥ J will have at least one symbol in its prefix that also appears in the prefix of s. Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of s. Then the highest Jaccard similarity that s and t can have occurs when t is a suffix of s, consisting of everything but the first p symbols of s. The Jaccard 102 CHAPTER 3. FINDING SIMILAR ITEMS similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not have to compare s with t, we must be certain that J > (Ls − p)/Ls. That is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as possible, so we do not index string s in more buckets than we need to. Thus, we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that gets indexed. Example 3.26 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 = ⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any string t that does not have the first symbol of s in a position such that t is indexed by that symbol will have Jaccard similarity with s that is less than 0.9. Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not begin with b. There are two cases to consider. 1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is abcdefghij. But if that is the case, t will be indexed under both a and b. The reason is that Lt = 10, so t will be indexed under the symbols of its prefix of length ⌊0.1 × 10⌋ + 1 = 2. 2. If t begins with c or a later letter, then the maximum value of SIM(s, t) occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9. In general, with J = 0.9, strings of length up to 9 are indexed by their first symbol, strings of lengths 10–19 are indexed under their first two symbols, strings of length 20–29 are indexed under their first three symbols, and so on. ✷ We can use the indexing scheme in two ways, depending on whether we are trying to solve the many-many problem or a many-one problem; recall the distinction was introduced in Section 3.8.4. For the many-one problem, we create the index for the entire database. To query for matches to a new set S, we convert that set to a string s, which we call the probe string. Determine the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For each symbol appearing in one of the prefix positions of s, we look in the index bucket for that symbol, and we compare s with all the strings appearing in that bucket. If we want to solve the many-many problem, start with an empty database of strings and indexes. For each set S, we treat S as a new set for the many-one problem. We convert S to a string s, which we treat as a probe string in the many-one problem. However, after we examine an index bucket, we also add s to that bucket, so s will be compared with later strings that could be matches. 3.9.5 Using Position Information Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9. Since both strings are of length 10, they are indexed under their first two 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 103 symbols. Thus, s is indexed under a and c, while t is indexed under b and c. Whichever is added last will find the other in the bucket for c, and they will be compared. However, since c is the second symbol of both, we know there will be two symbols, a and b in this case, that are in the union of the two sets but not in the intersection. Indeed, even though s and t are identical from c to the end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11, which is less than 0.9. If we build our index based not only on the symbol, but on the position of the symbol within the string, we could avoid comparing s and t above. That is, let our index have a bucket for each pair (x, i), containing the strings that have symbol x in position i of their prefix. Given a string s, and assuming J is the minimum desired Jaccard distance, we look at the prefix of s, that is, the positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is x, add s to the index bucket for (x, i). Now consider s as a probe string. With what buckets must it be compared? We shall visit the symbols of the prefix of s from the left, and we shall take advantage of the fact that we only need to find a possible matching string t if none of the previous buckets we have examined for matches held t. That is, we only need to find a candidate match once. Thus, if we find that the ith symbol of s is x, then we need look in the bucket (x, j) for certain small values of j. j s t Symbols definitely appearing in only one string i Figure 3.14: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that To compute the upper bound on j, suppose t is a string none of whose first j −1 symbols matched anything in s, but the ith symbol of s is the same as the jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical beyond their ith and jth symbols, respectively, as suggested by Fig. 3.14. If that is the case, the size of their intersection is Ls − i + 1, since that is the number of symbols of s that could possibly be in t. The size of their union is at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and there are also at least j −1 symbols of t that are not in s. The ratio of the sizes of the intersection and union must be at least J, so we must have: Ls − i + 1 Ls + j − 1 ≥ J 104 CHAPTER 3. FINDING SIMILAR ITEMS If we isolate j in this inequality, we have j ≤ Ls(1 − J) − i + 1 + J  /J. Example 3.27 : Consider the string s = acdefghijk with J = 0.9 discussed at the beginning of this section. Suppose s is now a probe string. We already established that we need to consider the first two positions; that is, i can be 1 or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11. Thus, j can be 1 or 2, but nothing higher. Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1), but in no other bucket. In comparison, using the buckets of Section 3.9.4, we would look into the buckets for a and c, which is equivalent to looking to all buckets (a, j) and (c, j) for any j. ✷ 3.9.6 Using Position and Length in Indexes When we considered the upper limit on j in the previous section, we assumed that what follows positions i and j were as in Fig. 3.14, where what followed these positions in strings s and t matched exactly. We do not want to build an index that involves every symbol in the strings, because that makes the total work excessive. However, we can add to our index a summary of what follows the positions being indexed. Doing so expands the number of buckets, but not beyond reasonable bounds, and yet enables us to eliminate many candidate matches without comparing entire strings. The idea is to use index buckets corresponding to a symbol, a position, and the suffix length, that is, the number of symbols following the position in question. Example 3.28 : The string s = acdefghijk, with J = 0.9, would be indexed in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol a, and its suffix is of length 9. The second position has symbol c and its suffix is of length 8. ✷ Figure 3.14 assumes that the suffixes for position i of s and position j of t have the same length. If not, then we can either get a smaller upper bound on the size of the intersection of s and t (if t is shorter) or a larger lower bound on the size of the union (if t is longer). Suppose s has prefix length p and t has prefix length q. Case 1: p ≥ q. Here, the maximum size of the intersection is Ls − i + 1 − (p − q) Since Ls = i + p, we can write the above expression for the intersection size as q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not take suffix length into account. Thus, we require q + 1 Ls + j − 1 ≥ 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 105 whenever p ≥ q. Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as when suffix length was not considered. However, the minimum size of the union is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can replace Ls − p by i and get the formula i + j − 1 + q for the size of the union. If the Jaccard similarity is at least J, then Ls − i + 1 i + j − 1 + q ≥ J whenever p < q. Example 3.29 : Let us again consider the string s = acdefghijk, but to make the example show some details, let us choose J = 0.8 instead of 0.9. We know that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q the suffix length of t. First, consider the case p ≥ q. The additional constraint we have on q and j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for each i between 1 and 3, as follows. i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q: q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2, or j = 3. Note that for j = 4, 10/13 > 0.8. q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2. For j = 3, 9/12 > 0.8. q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality. q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every positive integer j. The same holds for every smaller value of q. i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j ≥ 0.8 does not depend on i, 6 we can use the analysis from the above case, but exclude the case q = 9. Thus, the only possible values of j and q when i = 2 are 1. q = 8; j = 1. 2. q = 8; j = 2. 3. q = 7; j = 1. i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The only option is q = 7 and j = 1. 6Note that i does influence the value of p, and through p, puts a limit on q. 106 CHAPTER 3. FINDING SIMILAR ITEMS Next, we must consider the case p < q. The additional constraint is 11 − i i + j + q − 1 ≥ 0.8 Again, consider each possible value of i. i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible values of q and j are 1. q = 10; j = 1. 2. q = 10; j = 2. 3. q = 11; j = 1. i = 2: Now, p = 10, so we require q ≥ 11 and 9/(q + j + 1) ≥ 0.8. there are no solutions, since j must be a positive integer. i = 3: As for i = 2, there are no solutions. q j = 1 j = 2 j = 3 7 x 8 x x i = 1 9 x x x 10 x x 11 x 7 x i = 2 8 x x 9 x i = 3 7 x Figure 3.15: The buckets that must be examined to find possible matches for the string s = acdefghijk with J = 0.8 are marked with an x When we accumulate the possible combinations of i, j, and q, we see that the set of index buckets in which we must look forms a pyramid. Figure 3.15 shows the buckets in which we must search. That is, we must look in those buckets (x, j, q) such that the ith symbol of the string s is x, j is the position associated with the bucket and q the suffix length. ✷ 3.9.7 Exercises for Section 3.9 Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the order of elements is taken to be the vowels, in alphabetic order, followed by the consonants in reverse alphabetic order. Represent the following sets as strings. a {q, w, e, r, t, y}. 3.10. SUMMARY OF CHAPTER 3 107 (b) {a, s, d, f, g, h, j, u, i}. Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in Section 3.9.3. If s is a string of length 20, with what strings is s compared when J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85 (b) J = 0.95 (c) J = 0.98? Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index its prefix as in Section 3.9.4. (a) How many positions are in the prefix if J = 0.85? (b) How many positions are in the prefix if J = 0.95? ! (c) For what range of values of J will s be indexed under its first four symbols, but no more? Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position pairs will s be compared with if we use the indexing approach of Section 3.9.5, and (a) J = 0.75 (b) J = 0.95? ! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of 100 elements. Assume J = 0.9. What is the probability that s and t will be compared if (a) s and t are both of length 9. (b) s and t are both of length 10. Exercise 3.9.6 : Suppose we use indexes based on both position and suffix length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9? 3.10 Summary of Chapter 3 ✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size of the intersection of the sets to the size of the union. This measure of similarity is suitable for many applications, including textual similarity of documents and similarity of buying habits of customers. ✦ Shingling: A k-shingle is any k characters that appear consecutively in a document. If we represent a document by its set of k-shingles, then the Jaccard similarity of the shingle sets measures the textual similarity of documents. Sometimes, it is useful to hash shingles to bit strings of shorter length, and use sets of hash values to represent documents. 108 CHAPTER 3. FINDING SIMILAR ITEMS ✦ Minhashing: A minhash function on sets is based on a permutation of the universal set. Given any such permutation, the minhash value for a set is that element of the set that appears first in the permuted order. ✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the sequence of minhash values obtained by applying each permutation on the list to that set. Given two sets, the expected fraction of the permutations that will yield the same minhash value is exactly the Jaccard similarity of the sets. ✦ Efficient Minhashing: Since it is not really possible to generate random permutations, it is normal to simulate a permutation by picking a random hash function and taking the minhash value for a set to be the least hash value of any of the set’s members. ✦ Locality-Sensitive Hashing for Signatures: This technique allows us to avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we can eliminate from consideration most of the pairs that do not meet our threshold of similarity. ✦ Distance Measures: A distance measure is a function on pairs of points in a space that satisfy certain axioms. The distance between two points is 0 if the points are the same, but greater than 0 if the points are different. The distance is symmetric; it does not matter in which order we consider the two points. A distance measure must satisfy the triangle inequality: the distance between two points is never more than the sum of the distances between those points and some third point. ✦ Euclidean Distance: The most common notion of distance is the Euclidean distance in an n-dimensional space. This distance, sometimes called the L2-norm, is the square root of the sum of the squares of the differences between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of the magnitudes of the differences between the points in each dimension. ✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure, called the Jaccard distance. ✦ Cosine Distance: The angle between vectors in a vector space is the cosine distance measure. We can compute the cosine of that angle by taking the dot product of the vectors and dividing by the lengths of the vectors. ✦ Edit Distance: This distance measure applies to a space of strings, and is the number of insertions and/or deletions needed to convert one string 3.10. SUMMARY OF CHAPTER 3 109 into the other. The edit distance can also be computed as the sum of the lengths of the strings minus twice the length of the longest common subsequence of the strings. ✦ Hamming Distance: This distance measure applies to a space of vectors. The Hamming distance between two vectors is the number of positions in which the vectors differ. ✦ Generalized Locality-Sensitive Hashing: We may start with any collection of functions, such as the minhash functions, that can render a decision as to whether or not a pair of items should be candidates for similarity checking. The only constraint on these functions is that they provide a lower bound on the probability of saying “yes” if the distance (according to some distance measure) is below a given limit, and an upper bound on the probability of saying “yes” if the distance is above another given limit. We can then increase the probability of saying “yes” for nearby items and at the same time decrease the probability of saying “yes” for distant items to as great an extent as we wish, by applying an AND construction and an OR construction. ✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of basis functions to start a generalized LSH for the cosine distance measure by identifying each function with a list of randomly chosen vectors. We apply a function to a given vector v by taking the dot product of v with each vector on the list. The result is a sketch consisting of the signs (+1 or −1) of the dot products. The fraction of positions in which the sketches of two vectors agree, multiplied by 180, is an estimate of the angle between the two vectors. ✦ LSH For Euclidean Distance: A set of basis functions to start LSH for Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals, and the function answers “yes” to a pair of points that fall into the same interval. ✦ High-Similarity Detection by String Comparison: An alternative approach to finding similar items, when the threshold of Jaccard similarity is close to 1, avoids using minhashing and LSH. Rather, the universal set is ordered, and sets are represented by strings, consisting their elements in order. The simplest way to avoid comparing all pairs of sets or their strings is to note that highly similar sets will have strings of approximately the same length. If we sort the strings, we can compare each string with only a small number of the immediately following strings. ✦ Character Indexes: If we represent sets by strings, and the similarity threshold is close to 1, we can index all strings by their first few characters. The prefix whose characters must be indexed is approximately the length 110 CHAPTER 3. FINDING SIMILAR ITEMS of the string times the maximum Jaccard distance (1 minus the minimum Jaccard similarity). ✦ Position Indexes: We can index strings not only on the characters in their prefixes, but on the position of that character within the prefix. We reduce the number of pairs of strings that must be compared, because if two strings share a character that is not in the first position in both strings, then we know that either there are some preceding characters that are in the union but not the intersection, or there is an earlier symbol that appears in both strings. ✦ Suffix Indexes: We can also index strings based not only on the characters in their prefixes and the positions of those characters, but on the length of the character’s suffix – the number of positions that follow it in the string. This structure further reduces the number of pairs that must be compared, because a common symbol with different suffix lengths implies additional characters that must be in the union but not in the intersection. 3.11 References for Chapter 3 The technique we called shingling is attributed to [10]. The use in the manner we discussed here is from [2]. Minhashing comes from [3]. The original works on locality-sensitive hashing were [9] and [7]. [1] is a useful summary of ideas in this field. [4] introduces the idea of using random-hyperplanes to summarize items in a way that respects the cosine distance. [8] suggests that random hyperplanes plus LSH can be more accurate at detecting similar documents than minhashing plus LSH. Techniques for summarizing points in a Euclidean space are covered in [6]. [11] presented the shingling technique based on stop words. The length and prefix-based indexing schemes for high-similarity matching comes from [5]. The technique involving suffix length is from [12]. 1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117– 122, 2008. 2. A.Z. Broder, “On the resemblance and containment of documents,” Proc. Compression and Complexity of Sequences, pp. 21–29, Positano Italy, 1997. 3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise independent permutations,” ACM Symposium on Theory of Computing, pp. 327–336, 1998. 4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002. 3.11. REFERENCES FOR CHAPTER 3 111 5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering, 2006. 6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004. 7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518– 529, 1999. 8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006. 9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998. 10. U. Manber, “Finding similar files in a large file system,” Proc. USENIX Conference, pp. 1–10, 1994. 11. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM SIGIR Conference, July, 2008, Singapore. 12. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.


主要利用的是 shingling-minhash-LSH 技术：

* shingling：将文本拆解为拆解为长度为 $K$ 的字符串集合
* minhash：文本签名，能够表示集合并反映其相似性的较短长度的整数向量
* LSH：局部敏感哈希的目的是生成测试候选对









#### 3.3 minhash 签名

我们可以把上面介绍的过程随机置换行顺序 $n$ 次，$n$ 大概为几百次的规模，则我们会得到 $h_1,h_2,\cdots,h_n$，而对于集合 $S$，我们现在可以表示为 $[h_1(S),h_2(S),\cdots,h_n(S)]$，这样一来，数据规模就小很多了，原来的特征矩阵需要 len(k-shingles) 行，而现在只需要 $n$ 行。$n$ 越大，则结果与原始的 Jaccard 相似度越接近。

**然而这种做法听起来不错，却根本无法实现**，即使对上百万甚至数十亿的行选择一个随机排列转换也极其消耗时间，而对行进行必要的排序则需要花费更多的时间。幸运的是，我们可以通过一个随机哈希函数来模拟随机排列转换的效果，该函数将行号映射到与行数目大致相等数量的桶中。通常而言，一个将整数 $0,1, …, k-1$ 映射到桶号 $0, 1, …, k-1$ 的哈希函数会将某些整数映射到同一个桶中，而有些桶却没有被任何整数所映射到。然而，只要 $k$ 很大且哈希结果冲突不太频繁的话，差异就不是很重要。于是，我们就可以继续假设哈希函数 "h" 将原来的第 $r$ 行放在排列转换后次序中的第 $h(r)$ 个位置上。

因此，我们就可以不对行选择 $n$ 个随机排列转换，取而代之的是随机选择 $n$ 个哈希函数 $h_1, h_2, ···,h_n$ 作用于行。在上述处理基础上，就可以根据每行在哈希之后的位置来构建签名矩阵。令 $\text{SIG}(i, c)$  为签名矩阵中第 $i$ 个哈希函数在第 $c$ 列上的元素。一开始，对于所有的 $i$ 和 $C$, 将 $\text{SIG}(i, c)$ 都初始化为 $\infty$。然后，对行 $r$ 进行如下处理：

1. 计算 $h_1(r), h_i(r), …, h_n(r)$;
2. 对每列 $c$ 进行如下操作：
   1. 如果 $c$ 在第 $r$ 行为 0, 则什么都不做；
   2. 否则，如果 $c$ 在第 $r$ 行为 1 , 那么对于每个 $i=1, 2, …, n$, 将 $\text{SIG}(i, c)$ 置为原来的 $\text{SIG}(i, c)$ 和 $h_(r)$ 之中的较小值。

例如下图中，我们把上面的特征矩阵加上了行号，以及设定两个哈希函数分别是 $h_1(x) = (x+1) \% 5, h_2(x) = (3x+1) \% 5$，这里的 $x$​ 指的是行号。两个哈希函数产生的结果在最后两列。注意到这里的两个简单哈希函数对应真正的行排列转换，当然这里只有当行数目为质数（这里为 5) 时才有会有真正的排列转换。通常来说，哈希结果都会存在冲突，即至少有两行得到的哈希值相等。

![image-20211016155236323](image-20211016155236323.png)

接下来模拟计算签名矩阵的算法。一开始，签名矩阵全都由 $\infty$ 构成：

![image-20211016121822025](image-20211016121822025.png)

首先，考虑第 0 行。此时，不论是 $h_1(0)$ 还是 $h_2(0)$ 的结果值都是 1 。而只有集合 $S_1$ 和 $S_4$ 在第 0 行为 1, 因此签名矩阵中只有这两列的值需要修改。因为 $1 < \infty$， 所以实际上是对 $S_1$ 和 $S_4$ 的对应列值进行修改，所以当前签名矩阵的估计结果为：

接下来，我们看第 2 行。对于该行，只有 $S_3$ 的值为 1 , 此时其哈希值 $h_1(1) = 2,h_2(1) =4$ ，更新后的签名矩阵为：

![image-20211016121933550](image-20211016121933550.png)

再接下来看第 2 行中只有 $S_2$ 和 $S_4$ 对应的列为 1，且其哈希值 $h_1(2)=3, h_2(2)=2$ 。$S_4$ 对应的签名本应修改，但是签名矩阵中对应列值为 [1, 1] 小于相应的哈希值 [3, 2]，因此其签名最后不会修改（其实仔细想想，在此处为什么不修改，是因为这里的 hash 值充当置换后的新行，如果行数较小的行中已经有 1，则置换后行数较大的行中再出现 1 其实也没啥用了）。而 $S_2$ 对应的列中仍然是初始值 $\infty$。我们将它替换为 [3, 2], 得到：

![image-20211016122008417](image-20211016122008417.png)

同理可以继续进行下去，最后得到的签名矩阵为：

![image-20211016122056725](image-20211016122056725.png)

其实如果你真的按照两次哈希后的顺序，真的去排列后重新找第一次出现 1 的行，得到的也是上面的结果。

基于上述签名矩阵，我们可以估计原始集合之间的 Jaccard 相似度。注意到在签名矩阵中 $S_1$ 和 $S_4$ 对应的列向量完全相同，因此我们可以猜测 $\text{SIM}(S_1, S_4)=1.0$。如果回到原始图中, 我们会发现 $S_1$ 和 $S_4$ 的真实 Jaccard 相似度为 2/3 。需要记住的是，签名矩阵中行之间的一致程度只是真实 Jaccard 相似度的一个估计值，因为本例规模太小，所以并不足以说明在大规模数据情况下估计值和真实值相近的规律。

#### 3.4 加速

上面的计算方法虽然从计算结果上看，可以把不可能实现的置换操作以哈希方式给实现了，可以将矩阵从行数非常巨大的特征矩阵变成只有几百行的签名矩阵。然而在这个过程中计算量非常大，因为它需要考虑整个特征矩阵的所有行。

我们重新回到最原始的特征矩阵上

![image-20211016145825668](image-20211016145825668.png)

假设我们目前看到的已经是置换后的，那我们其实在找第一行为 1 的时候就停下了，例如对于 $S_1$ 而言，我们不会再看 bcde 行，对于 $S_2$ 而言，也不会再看 de 行，所以我们的注意力并没有看完所有的特征矩阵的每一行。所以我们由此可以想象出，对于一个非常巨大的特征矩阵，我们可以只看前 $m$ 行（假设一共 $k$ 行），这样一来就可以大大的减少计算量，例如上图，实际上只看前 3 行完全就可以得到正确答案。

但是这样也会引入新的问题，例如上图中，如果我只看前两行，那么 $S_2$ 这一列将看不到 1，怎么办？用 $\infty$ 表示。这样一来，含 $\infty$ 符号，在计算 Jaccard 的时候，就有 3 种情况：

1. 两列都不含 $\infty$，那就正常比较
2. 一列有，另一列没有，那么前者真实出现第一个 1 的行肯定不在前 $m$ 行，即两列 minhash 不等
3. 两列都有 $\infty$，则既不作为等值处理，也不作为不等值处理

第 3 种情况不是很常见，这种影响将在一定程度上降低我们对 Jaccard 距离估计的准确性，但不会太大。由于我们现在能够比检查所有行更快地计算所有列的 minhash 值，因此我们可以节省时间来应用更多的 minhash 函数。我们得到了比原来更好的准确度，而且比以前更快。

#### 3.5 在 Hash 函数基础上加速

实际上是将上面 3.3 中提到的 hash 方法（并不真的进行置换）与 3.4 中提到的只关注前 $m$ 行方法进行结合。虽然这种方式可能产生全是 0 的列，以至于签名矩阵中出现 $\infty$，但只要 $m$ 足够大，这种情况则很少发生，我们依然可以根据签名矩阵很好地评估原始 Jaccard 值。遇到的话，忽略掉该行。

假设 $T$ 是特征矩阵的前 $m$ 行包含的所有元素集合，那么 $S_1$ 在前 $m$ 行的元素集合为 $S_1 \cap T$，同理 $S_2$ 在前 $m$ 行的集合为 $S_2 \cap T$，现在再算 Jaccard，则，计算公式需要修改为：
$$
\text{Jaccard}(S_1, S_2)=\frac{|S_1 \cap S_2 \cap T|}{|(S_1 \cup S_2) \cap T|}
$$
然而，会有一些随机变化，因为根据 $T$，我们可以在矩阵的前 $m$ 行中可能会找到多于或少于 $X$ 类行（两列中均为 1的行）和/或 $Y$ 类行（一列为 1，另一列为 0 的行）。

为了缓解这种变化，我们对每个 minhashing 不使用相同的集合 $T$。相反，我们将特征矩阵划分为 $k/m$ 组。然后，对于每个 hash 函数，我们通过第一个 $m$ 行来计算一个 minhash 值，通过第二个 $m$ 行计算不同的 minhash 值，依此类推。因此，我们从单个 hash 函数和对 $m$ 的所有行的单次传递中获得 $k/m$ 个 minhash值。事实上，如果 $k/m$ 足够大，我们可以单个 hash 函数通过应用于特征矩阵的每个子集上m$ 的每个行获得整个特征矩阵的签名矩阵。

例如下图中，$k=8,m=4$，$\text{SIM}(S_1, S_2) = 1/2, \text{SIM}(S_1, S_3) = 1/5, \text{SIM}(S_2, S_3) = 1/2$（忽略都是 0 的行）。

![image-20211016173238876](image-20211016173238876.png)

如果只看前 4 行，不管用什么 hash 函数，$S_1$ 和 $S_2$ 的 minhash 值将始终不等，因为 $S_1 \cap S_2 \cap T=\emptyset$，然而只看后四行，则相似度为 $2/3$。所以平均起来是 $(0+2/3)/2=1/3$，与真实的 $1/2$ 有误差，不是太大。在真实场景下，$m$ 远大于 4 行，这种误差将趋于 0。

同理比较 $S_1$ 与 $S_3$ 之间，是 $(0+1/3)/2=1/6$，与真实的 $1/5$ 比较接近，而 $S_2$ 与 $S_3$ 之间完全一致。

### 3.4 LSH

#### 3.4.1 面向最小哈希签名的 LSH

虽然 minhash 可以大幅度压缩特征矩阵，但矩阵的列数并没有变化，那么依然解决不了如何在大量文件之间高效的找到最大相似度的文档。下面介绍局部敏感哈希（LSH）。

LSH 的一个一般性做法就是对目标项进行多次哈希处理，使得相似项会比不相似项更可能哈希到同一桶中。然后将至少有一次哈希到同一桶中的文档对看成是候选对，我们只会检查这些候选对之间的相似度。我们希望大部分不相似的文档对将永远不会哈希到相同的桶中，这样就永远不需要检查它们的相似度。那些哈希到同一个桶中的非相似文档对称为伪正例(false positive) , 我们希望它们在所有对中占的比例越低越好。同时，我们也希望大部分真正相似的文档对会至少被一个哈希函数映射到同一桶中。那些没有映射到相同桶中的真正相似的文档对称为伪反例(false negative) , 我们希望它们在所有真正相似文档对中的比例也很小。

如果拥有目标项的最小哈希签名矩阵，那么一个有效的哈希处理方法是将签名矩阵划分成 $b$ 个行条(band), 每个行条由 $r$ 行组成。对每个行条，存在一个哈希函数能够将行条中的每 $r$ 个整数组成的列向釐（行条中的每一列）映射到某个大数目范围的桶中。可以对所有行条使用相同的哈希函数，但是对每个行条我们都使用一个独立的桶数组，因此即使是不同行条中的相同向量列，它们也不会被哈希到同一桶中。

如下图所示：

![image-20211016175800408](image-20211016175800408.png)

包含 12 行的签名矩阵的一部分，有 4 个行条，每个行条 3 行，图中显式可见的行条1 中第 2 列和第 4 列均包含列向量 [0, 2, 1], 因此它们肯定会哈希到行条l 下的相同桶中。因此，不管这两列在其他 3 个行条下的结果如何，它们都是一个相似候选对。图中显式给出的其他列也有可能会哈希到行条 1 下的同一桶中。但是，由于此时两个列向量 [1, 3, 0J 和 [0, 2, 1] 不同，加上哈希的桶数目也不少，因此偶然冲突的预期概率会非常低。通常我们都假设当且仅当两个向量相等时，它们才会哈希到同一桶中。

在行条1 中不相等的两个列仍然还有另外3次机会成为候选对，只要它们在剩余的 3 个行条中有一次相等即可。然而，我们观察到，如果签名矩阵的两列越相似，那么在多个行条中的向量相等的可能性也越大。因此，直观上看，行条化策略能够使得相似列会比不相似列更有可能成为候选对。

#### 3.4.2 行条化策略分析

假定使用 $b$ 个行条，每个行条由 $r$ 行组成，并假定某对具体文档之间的 Jaccard 相似度为 $s$。则文档的最小哈希签名矩阵中某个具体行中的两个签名相等的概率等于 $s$ 。接下来我们可以计算这些文档（或其签名）作为候选对的概率，具体计算过程如下：

1. 在某个具体行条中所有行的两个签名相等的概率是 $s^r$
2. 在某个具体行条中至少有一对签名不相等的概率是 $1-s^r$
3. 在任何行条中的任意一行的签名对都不相等的概率为 $(1-s^r)^b$
4. 签名至少在一个行条中全部相等的概率，也即成为候选对的概率为 $1-(1-s^r)^b$

虽然有可能并不特别明显，但是不论常数 $b$ 和 $r$ 的取值如何，上述形式的概率函数图像大致为下图中的 S-曲线。

![image-20211016180658409](image-20211016180658409.png)



曲线中候选概率 $1/2$ 处对应的相似度就是所谓的阈值。它是 $b$ 和 $r$ 的函数。阈值对应的大概是上升最陡峭的地方，对于较大的 $b$ 和 $r$, 相似度在阈值之上的对很可能成为候选对，而在阈值之下的对则不太可能成为候选对，这正是我们想要的结果。阈值的一个近似估计是 $(1/b)^{l/r}$。例如，如果 $b=16,r=4$, 那么由于 16  的 4 次方根为 2, 阈值的近似值为 1/2 。

考虑 $b=20, r=5$ 的情况，也就是说假定签名的个数为 100, 分成 20 个行条，每个行条包含 5 行。下表给出了函数 $1-(1-s^5)^{20}$ 的部分值。注意到的是，这里的阈值，也就是曲线中部上升处的 $s$ 值，仅仅比 0.5 稍大一点。另外也注意到，该曲线并非从 0 到 1 在阈值处跳跃的最理想步进函数，但是曲线中部的斜率十分显著。例如， $s$ 从 0.4 变到 0.6, 增加的函数值大于 0.6,  因此中间部分的斜率大于 3 。

![image-20211016181251776](image-20211016181251776.png)



又例如， $s=0.8$ 时， $1-(0.8)^5$ 大约为 0.672。如果再求 20 次方得到大约 0.000 35, 用 1 减去该值以后得 0.999 65 。也就是说，如果认为两篇文档的相似度为 80%, 那么在任意行条中， 5 行中签名对全部相等的可能性，即它们会成为候选对的概率只有约 33%。然而，这里有 20 个行条，因此有 20 次机会成为一个候选对。3000 个对中，大致仅有 1 个相似度为 80% 的对不会成为候选对，即成为伪反例。

#### 3.4.3 上述技术综合

本节将给出一个完整的相似项发现方法：首先找出可能的候选对相似文档集合，然后基于该集合发现真正的相似文档。必须强调的是，这种方法可能会产生伪反例，即某些相似文档对由于没有进入候选对所以最终没有被识别出来。同样，该方法也可能会产生伪正例，即在评估了某些候选对后，发现其相似度不足。

1. 选择某个 $k$, 并对每篇文档构建其 k-shingle 集合。将这些 k-shingle 映射成更短的桶编号（后一步可选）。

2. 将文档-shingle 对按照 shingle 排序，构建特征矩阵。
3. 选择最小哈希签名的长度 $n$ 。计算所有文档的最小哈希签名。
4. 选择阈值 $t$ 来定义应该达到的相似程度使之被看做是预期的”相似对“。选择行条数 $b$ 和每个行条中的行数 $r$, 使得 $br=n$, 而阈值 $t$ 近似等于 $(1/b)^{1/r}$。如果避免伪反例的产生很重要，那么选择合适的 $b$ 和 $r$ 以产生小于 $t$ 的阈值。而如果速度相当重要并且希望限制伪正例的数目，那么选择合适的 $b$和 $r$ 来获得更高的阈值。
5. 应用 LSH 技术来构建候选对。
6. 检查每个候选对的签名，确定它们一致性的比例是否大于 $t$。
7.  (该步可选）如果签名足够相似，则直接检查文档本身看它们是否真正相似。不相似的文档有时碰巧会具有相似的签名。

