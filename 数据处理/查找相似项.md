
> [Finding Similar Items](http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf)


我们首先将相似性问题表述为寻找具有相对较大交集的集合的问题。我们展示了如何通过一种称为“指纹”的技术，将查找文本相似文档的问题转化为这样的集合问题。然后，我们介绍了一种称为 “minhashing” 的技术，该技术以这样一种方式压缩大型集合，即我们仍然可以从它们的压缩版本中推断出底层集合的相似性。

即使计算任意一对 item 的相似度变得非常容易，也可能有太多 item 对需要测试，这一担忧促使了一种称为“局部敏感哈希”的技术的发展，该技术专注于最有可能相似的 item 对。最后，我们探讨了无法用集合交集来表达的“相似性”概念。这项研究引导我们考虑任意空间中的距离度量理论。它还激发了一个适用于其他“相似性”定义的局部敏感哈希通用框架。

#### 1. 集合的 Jaccard 相似度

Jaccard 相似度是集合 $S$ 和 $T$ 的交集大小与并集大小的比值，表示为：$$\text{SIM}(S, T) = \frac{|S \cap T|}{|S \cup T|}$$
#### 2. $k$-shingles

Shingling 是将文档表示为集合的一种方式，$k$-Shingles 就是将文本拆分为长度为 $k$ 的字符串子集，例如，假设我们的文本 $D$ 的内容为 ”abcdabd“，则当 $k=2$ 时，拆解的全部子集为 $\{ab, bc, cd, da, bd\}$，其中 $ab$ 在文本 $D$ 中出现了两次，但是在 shingling 集合中只保留一次（另一个版本采用的 bag，而不是 set 则支持重复项）。这里的小字符串就称为 ”shingle“。

##### 2.2 $k$ 的选择

如果 $k$ 选择的比较小，则任意文本相似性很高，例如极限情况下，$k=1$，且只考虑字母+空格情况，则几乎任意文档都是有 27 个元素（26个字母+1个空格）组成，相似度为 $1$；所以 *$K$ 应该足够大，以至于任意给定的 shingle 出现在任意给定的文档中的概率都很低。*

例如当 $k=5$，$27^5=14,348,907$，对于短文本，字符长度远远小于 0.14 亿的长度，因此通常选择 $k=5$ 就挺好，不过实际上文本中的字符可能不止 $27$ 个，同时每个字母出现的概率也不一样，大致可以按 $20^k$ 计算，对于长篇文档（如研究论文），可以考虑用 $k=9$.

#### 2.3 Hashing Shingles

假设 $k=9$，则一个 shingles 长度为 9，则需要 9 个字节空间存储，如果将其 hash 到 0 到 $2^{32}-1$ 范围内，则可以用 4 个字节（32位）的长度表示，可以节省空间。需要注意的是，如果我们使用 9-shingle 并将它们 hash 到 4 个字节，那么我们可以更好地区分文档，而不是使用 4-shingle，即使用于表示 shingle 的空间是相同的。

### 三、MinHash 过程

#### 3.1 集合的矩阵表示  

shingle 集合非常大，即使我们将其都 hash 到 4 个字节，一篇文档的 shingle 集合所需要的空间仍然大概是原文档所需空间的 4 倍。例如原始文档 $D$ 长度为 $ld$，如果不考虑重复 shingle 出现，则一个文档 D 的 shingles 集合大小为 $ld-k+1$，而每个 shingles 需要 4 个字节存储，$k$ 又远远小于 $ld$，所以需要大致 $4ld$ 空间。

所以我们需要想办法将上述大集合替换为规模小得多的”签名“。但这种签名是有要求的，即*通过比较签名之间的相似度就可以估计实际 shingle 集合之间的 Jaccard 相似度*。当然，通过签名无法得到原始 shingle 集合之间 Jaccard相似度的精确值，但是估计结果与真实结果相差不大，并且签名集合越大，估计的精度也越高。例如， 50000 字节文档的 shingle 可能会映射为 200000 字节的哈希结果，然后替换成 1000 字节大小的签名集合。基于最终签名集合得到的原始文档 Jaccard 相似度的估计值与真实值的差异也就在几个百分点之内。

首先将一系列集合表示成特征矩阵形式：$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline a & 1 & 0 & 0 & 1 \\ b & 0 & 0 & 1 & 0 \\ c & 0 & 1 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ e & 0 & 0 & 1 & 0 \\ \hline \end{array}$$例如这里有 4 个集合，$S_1 = \{a, d\}, \,S_2 = \{c\}, \,S_3 = \{b, d, e\}, \, S_4 = \{a, c, d\}$。

#### 3.2 最小哈希（Minhashing）  

为了对特征矩阵每列所表示的集合进行 minhash 计算，首先选择行的一个排列转换。任意列的 minhash 值是该列在排列顺序中首次出现 1 的行号。例如下图中我们把行的顺序按照 `beadc` 顺序排列后，得到$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline b & 0 & 0 & 1 & 0 \\ e & 0 & 0 & 1 & 0 \\ a & 1 & 0 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ c & 0 & 1 & 0 & 1 \\ \hline \end{array}$$虽然从物理上来说，对非常大的特征矩阵进行置换是不可能的，但 MinHash 函数 h 隐式地对原始矩阵的行进行了重新排序，在这个矩阵中，我们可以通过从顶部开始扫描，直到遇到 $1$ 来读出 $h$ 的值，例如 $S_1$ 中第一次出现 1 的位置是 $a$ 行，则 $h(S_1)=a$，同理 $h(S_2) = c, \,h(S_3) = b, \, h(S_4) = a$

#### 3.3 minhash 与 Jaccard 相似度

*两个集合经随机排列转换之后得到的两个 minhash 值相等的概率等于这两个集合的 Jaccard 相似度。*

解释一下，假设只考虑集合 $S_1$ 和 $S_2$ 所对应的列，那么它们所在的行可以按照所有可能的结果分成如下 3 类：

1. 属于 $X$ 类的行，意思是两列的值均为 1；
2. 属于 $Y$ 类的行，意思是其中一列的值为 0, 另一列的值为 1；
3. 属于 $Z$ 类的行，意思两列的值都为 0 。

由于特征矩阵十分稀疏，因此大部分行都属于 $Z$ 类。但我们可以忽略都是 0 的行，实际由 $X$ 和 $Y$ 类行数目的比例决定了 $\text{SIM}(S_1, S_2)$ 及概率 $h(S_1)=h(S_2)$ 的大小。假定 $X$ 类行的数目为 $x$, $Y$ 类的行的数目为 $y$, 则 $\text{SIM}(S_1,S_2) =x/(x+y)$。原因是 $S_1 \cap S_2$ 的大小为 $x$，而 $S_1 \cup S_2$ 的大小为 $x+y$。

接下来我们考虑 $h(S_1)=h(S_2)$ 的概率。设想所有行进行随机排列转换，然后我们从上到下进行扫描处理，在碰到 $Y$ 类行之前碰到 $X$ 行的概率是 $x/(x+y)$。但是如果从上往下扫描遇到的除 $Z$ 类行之外的第一行属于 $X$ 类，那么肯定有 $h(S_1)=h(S_2)$ 。另一方面，如果首先碰到的是 $Y$ 类行，而不是 $Z$ 类行，那么值为 1 的那个集合的 minhash 值为当前行。但值为 0 的那个集合必将会进一步扫描下去。因此，如果首先碰到 $Y$ 类行，那么此时 $h(S_1) \neq h(S_2)$。于是，我们可以得到最终结论，即 $h(S_1)=h(S_2)$ 的概率是 $x/(x+y)$，而这也是两个集合 Jaccard 相似度的计算公式。

#### 3.4 minhash 签名

我们可以把上面介绍的过程随机置换行顺序 $n$ 次，$n$ 大概为几百次的规模，则我们会得到 $h_1,h_2,\cdots,h_n$，而对于集合 $S$，我们现在可以表示为 $[h_1(S),h_2(S),\cdots,h_n(S)]$，这样一来，数据规模就小很多了，原来的特征矩阵需要 len(k-shingles) 行，而现在只需要 $n$ 行。$n$ 越大，则结果与原始的 Jaccard 相似度越接近。

*然而这种做法听起来不错，却根本无法实现*，即使对上百万甚至数十亿的行选择一个随机排列转换也极其消耗时间，而对行进行必要的排序则需要花费更多的时间。

幸运的是，我们可以通过一个随机哈希函数来模拟随机排列转换的效果，该函数将行号映射到与行数目大致相等数量的桶中。通常而言，一个将整数 $0,1, …, k-1$ 映射到桶号 $0, 1, …, k-1$ 的哈希函数会将某些整数映射到同一个桶中，而有些桶却没有被任何整数所映射到。然而，只要 $k$ 很大且哈希结果冲突不太频繁的话，差异就不是很重要。于是，我们就可以继续假设哈希函数 $h$ 将原来的第 $r$ 行放在排列转换后次序中的第 $h(r)$ 个位置上。

因此，我们就可以不对行选择 $n$ 个随机排列转换，取而代之的是随机选择 $n$ 个哈希函数 $h_1, h_2, ···,h_n$ 作用于行。在上述处理基础上，就可以根据每行在哈希之后的位置来构建签名矩阵。令 $\text{SIG}(i, c)$  为签名矩阵中第 $i$ 个哈希函数在第 $c$ 列上的元素。一开始，对于所有的 $i$ 和 $C$, 将 $\text{SIG}(i, c)$ 都初始化为 $\infty$。然后，对行 $r$ 进行如下处理：

1. 计算 $h_1(r), h_i(r), …, h_n(r)$;
2. 对每列 $c$ 进行如下操作：
   3. 如果 $c$ 在第 $r$ 行为 0，则什么都不做；
   4. 否则，如果 $c$ 在第 $r$ 行为 1，那么对于每个 $i=1, 2, …, n$，将 $\text{SIG}(i, c)$ 置为原来的 $\text{SIG}(i, c)$ 和 $h_i(r)$ 之中的较小值。

例如下图中，我们把上面的特征矩阵加上了行号，以及设定两个哈希函数分别是 $h_1(x) = (x+1) \% 5$ 和 $h_2(x) = (3x+1) \% 5$，这里的 $x$​ 指的是行号。两个哈希函数产生的结果在最后两列。注意到这里的两个简单哈希函数对应真正的行排列转换，当然这里只有当行数目为质数（这里为 5) 时才有会有真正的排列转换。通常来说，哈希结果都会存在冲突，即至少有两行得到的哈希值相等。$$\begin{array}{|c||c|c|c|c||c|c|} \hline \textit{Row} & S_1 & S_2 & S_3 & S_4 & x+1 \mod 5 & 3x+1 \mod 5 \\ \hline 0 & 1 & 0 & 0 & 1 & 1 & 1 \\ 1 & 0 & 0 & 1 & 0 & 2 & 4 \\ 2 & 0 & 1 & 0 & 1 & 3 & 2 \\ 3 & 1 & 0 & 1 & 1 & 4 & 0 \\ 4 & 0 & 0 & 1 & 0 & 0 & 3 \\ \hline \end{array}$$
接下来模拟计算签名矩阵的算法。一开始，签名矩阵全都由 $\infty$ 构成：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & \infty & \infty & \infty & \infty \\ h_2 & \infty & \infty & \infty & \infty \\ \hline \end{array}$$
首先，考虑第 0 行。此时，不论是 $h_1(0)$ 还是 $h_2(0)$ 的结果值都是 1 。而只有集合 $S_1$ 和 $S_4$ 在第 0 行为 1，因此签名矩阵中只有这两列的值需要修改。因为 $1 < \infty$， 所以实际上是对 $S_1$ 和 $S_4$ 的对应列值进行修改，所以当前签名矩阵的估计结果为：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & \infty & 1 \\ h_2 & 1 & \infty & \infty & 1 \\ \hline \end{array}$$
接下来，我们看第 1 行。对于该行，只有 $S_3$ 的值为 1 , 此时其哈希值 $h_1(1) = 2,h_2(1) =4$ ，更新后的签名矩阵为：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & 2 & 1 \\ h_2 & 1 & \infty & 4 & 1 \\ \hline \end{array}$$
再接下来看第 2 行中只有 $S_2$ 和 $S_4$ 对应的列为 1，且其哈希值 $h_1(2)=3, h_2(2)=2$ 。$S_4$ 对应的签名本应修改，但是签名矩阵中对应列值为 \[1, 1\] 小于相应的哈希值 \[3, 2\]，因此其签名最后不会修改（其实仔细想想，在此处为什么不修改，是因为这里的 hash 值充当置换后的新行，如果行数较小的行中已经有 1，则置换后行数较大的行中再出现 1 其实也没啥用了）。而 $S_2$ 对应的列中仍然是初始值 $\infty$。我们将它替换为 \[3, 2\], 得到：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 1 & 2 & 4 & 1 \\ \hline \end{array}$$
再次考虑第 3 行，得到：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
最后考虑第 4 行得到的签名矩阵为：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 0 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
其实如果你真的按照两次哈希后的顺序，真的去排列后重新找第一次出现 1 的行，得到的也是上面的结果。

基于上述签名矩阵，我们可以估计原始集合之间的 Jaccard 相似度。注意到在签名矩阵中 $S_1$ 和 $S_4$ 对应的列向量完全相同，因此我们可以猜测 $\text{SIM}(S_1, S_4)=1.0$。如果回到原始图中, 我们会发现 $S_1$ 和 $S_4$ 的真实 Jaccard 相似度为 2/3 。需要记住的是，签名矩阵中行之间的一致程度只是真实 Jaccard 相似度的一个估计值，因为本例规模太小，所以并不足以说明在大规模数据情况下估计值和真实值相近的规律。

### 3.4 面向文档的局部敏感哈希  

尽管我们可以通过最小哈希将大型文档压缩为短小的签名，并保持任意文档对之间的预期相似度，但高效找出相似度最高的文档对仍可能无法实现。原因在于，即便文档总数不多，文档对的数量仍可能过于庞大。

**例 3.9：** 假设我们有一百万个文档，并使用长度为 250 的签名。这样每个文档的签名占用 1000 字节，整个数据可以放入一个千兆字节的空间中——比一台普通笔记本电脑的内存还要小。然而，有 $\frac{1,000,000 \times (1,000,000 - 1)}{2}$ 或者说半万亿对文档。如果计算两个签名的相似度需要一微秒，那么在那台笔记本电脑上计算所有相似度将需要将近六天。

如果我们的目标是计算每对文档之间的相似度，那么除了通过并行计算缩短耗时外，我们无法减少计算量。然而通常我们只需要找出最相似的文档对，或者相似度超过某个阈值的所有文档对。这种情况下，我们就只需重点关注可能相似的文档对，而无需检查所有组合。针对该需求的理论框架称为局部敏感哈希（LSH）或近邻搜索技术。本节我们将探讨专为当前研究场景设计的LSH具体实现方案——该方案适用于通过shingle集合表示文档，再经最小哈希生成短签名的处理流程。关于局部敏感哈希的通用理论体系及其衍生技术和应用场景，我们将在3.6节系统阐述。

#### 3.4.1 基于最小哈希签名的局部敏感哈希  

局部敏感哈希（LSH）的一种通用方法是多次“哈希”项，使得相似项比不相似项更可能被哈希到同一桶中。我们将任何在至少一次哈希中被分配到同一桶中的配对视为候选对，并仅检查这些候选对的相似性。其核心思想是：希望大多数不相似配对永远不会被哈希到同一桶中，因此无需被检查。那些被错误分配到同一桶的不相似配对称为假阳性，我们希望这类情况在所有配对中占比极小。同时期望真正相似的配对在至少一个哈希函数下会落入同一桶中。未能满足这一条件的相似配对称为假阴性，我们希望这类情况在真实相似配对中占比同样极小。

如果我们拥有项目的minhash签名，选择哈希方案的有效方法是将签名矩阵划分为b个带区，每个带区包含r行。对于每个带区，存在一个哈希函数，该函数接收r个整数的向量（即该带区内某列的片段）并将它们哈希到大量桶中。我们可以对所有带区使用相同的哈希函数，但为每个带区单独设置桶数组，因此不同带区中具有相同向量的列不会哈希到同一个桶中。$$\begin{array}{|c|c|}
\hline
\text{band 1} & 
\begin{array}{ccc}
& 1 & 0 & 0 & 0 & 2 \\
\cdots & 3 & 2 & 1 & 2 & 2 & \cdots \\
& 0 & 1 & 3 & 1 & 1 \\
\end{array} \\
\hline
\text{band 2} & \\
\hline
\text{band 3} & \\
\hline
\text{band 4} & \\
\hline
\end{array}
$$
图 3.6：将签名矩阵划分为四个带区，每个带区包含三行

示例3.10：图3.6展示了一个12行签名矩阵的部分内容，该矩阵被划分为四个波段，每个波段包含三行。在明确显示的列中，第二列和第四列的第一个波段都具有列向量[0, 2, 1]，因此它们在第一个波段的哈希处理中必定会被映射到同一个桶中。这意味着无论这两列在其他三个波段的表现如何，这对列都将成为候选对。其他列（例如明确显示的前两列）也可能根据第一个波段的哈希处理被映射到同一个桶中。然而，由于它们的列向量不同（分别为[1, 3, 0]和[0, 2, 1]），且每个哈希处理对应大量桶，我们预计偶然碰撞的概率非常低。通常我们假设，两个向量当且仅当完全相同时才会被哈希到同一个桶中。

在第一个波段不匹配的两列仍有三次机会成为候选对——它们可能在其他任一波段中完全相同。但要注意，两列越相似，它们就越有可能在某个波段中一致。因此，直观上，这种分波段策略使得相似列比不相似列更有可能成为候选对。

#### 3.4.2 分带技术分析 

假设我们使用b个带，每个带有r行，并且假设某对文档的Jaccard相似度为s。回顾3.3.3节，这两个文档的minhash签名在签名矩阵的任意一行中一致的概率为s。我们可以按如下方式计算这对文档（或其签名）成为候选对的概率：

1. 签名在某个特定带的所有行中都一致的概率为 $s^r$。
2. 签名在某个特定带的至少一行中不一致的概率为 $1 − s^r$。
3. 签名在所有带的任意一行中都不一致的概率为 $(1 − s^r)^b$。
4. 签名在至少一个带的所有行中都一致（因而成为候选对）的概率为 $1 − (1 − s^r)^b$。


虽然可能不够直观，但无论选择怎样的常数b和r，该函数都具有如图3.7所示的S形曲线特征。其阈值（即相似度s值上升最陡峭处的临界点）是b和r的函数。阈值的近似值为(1/b)^(1/r)。例如当b=16且r=4时，阈值约为1/2，因为16的四次方根等于2。

**例 3.11：** 我们以b=20且r=5的情况为例。假设我们有一个长度为100的签名，被划分为20个波段，每个波段包含5行。图3.8列出了函数1 − (1 − s^5)^20的部分取值。需要注意的是，曲线的阈值（即函数值上升到一半时对应的s值）略大于0.5。同时可以看到，这条曲线并非理想的阶跃函数（在阈值处从0突变到1），但其中间部分的斜率仍然显著。例如，当s从0.4增加到0.6时，函数值上升超过0.6，这意味着中间区间的斜率大于3。

例如，当s=0.8时，1−(0.8)^5约为0.672。若将该数值进行20次方运算，结果约为0.00035。用1减去该值得出0.99965。这意味着对于相似度为80%的两份文档，在任意一个波段中，它们仅有约33%的概率能在所有五行中都匹配成功从而成为候选对。但由于存在20个波段，就意味着有20次成为候选对的机会。在相似度高达80%的文档对中，大约每3000对中仅有一对会因未能成为候选对而出现假阴性情况。

图 3.7: S 曲线

$$\begin{array}{c|c}
s & 1 - (1 - s^r)^b \\
\hline
0.2 & 0.006 \\
0.3 & 0.047 \\
0.4 & 0.186 \\
0.5 & 0.470 \\
0.6 & 0.802 \\
0.7 & 0.975 \\
0.8 & 0.9996 \\
\end{array}
$$

图 3.8: Values of the S-curve for b = 20 and r = 5 

#### 3.4.3 技术组合应用

我们现在可以给出一种方法，用于寻找相似文档的候选对集合，然后从中识别出真正相似的文档。必须强调的是，这种方法可能会产生假阴性——即某些实际相似的文档对由于未能成为候选对而未被识别出来。同时也会出现假阳性——即被评估的候选对最终被判定为相似度不足的情况。

1. 选择一个k值，为每个文档生成对应的k-shingle集合（可选步骤：将k-shingle哈希映射为更短的桶编号）。
2. 对文档-shingle配对进行排序，按shingle值重新排列。
3. 确定最小哈希签名长度n，根据3.3.5节算法处理排序后的列表，计算所有文档的最小哈希签名。
4. 设定相似度阈值t以定义目标"相似文档对"：选择波段数b和行数r，满足b×r=n，且t≈(1/b)^(1/r)。若需减少漏判，可调低阈值；若需加快速度并减少误判，可调高阈值。
5. 应用3.4.1节的局部敏感哈希(LSH)技术生成候选文档对。
6. 检查每个候选对的签名，确认其相同分量的比例是否≥t。
7. 可选步骤：若签名相似度极高，可进一步校验原始文档是否真实相似（避免因签名巧合导致的误判）。


### 3.5 距离度量  

我们现在稍作转向，探讨距离度量的一般概念。杰卡德相似度用于衡量集合之间的接近程度，尽管它并非严格意义上的距离度量。也就是说，集合越相似，杰卡德相似度越高。实际上，1减去杰卡德相似度才是一种距离度量，后文将对此进行说明；这一结果称为杰卡德距离。

然而，雅卡尔距离并非唯一合理的相似度度量方式。本节我们将探讨其他几种具有实际应用的距离度量方法。随后在3.6节中，我们将看到其中部分距离度量如何通过局部敏感哈希技术实现高效近邻搜索，无需遍历所有数据点。距离度量的其他应用场景将在第7章研究聚类时进一步展开。

#### 3.5.1 距离度量的定义 

假设有一个点的集合，称为空间。该空间上的距离度量是一个函数d(x, y)，它以空间中的两个点作为参数并产生一个实数，且满足以下公理：

1. d(x, y) ≥ 0（距离非负）。
2. d(x, y) = 0 当且仅当 x = y（距离为正，除非是点到自身的距离）。
3. d(x, y) = d(y, x)（距离对称）。
4. d(x, y) ≤ d(x, z) + d(z, y)（三角不等式）。

三角不等式是最复杂的条件。它直观地表明，要从x点到达y点，如果被迫经过某个特定的第三点z，我们将无法获得任何好处。正是三角不等式公理使得所有距离度量都表现得如同距离描述了两点之间最短路径的长度。

#### 3.5.2 欧几里得距离  

最熟悉的距离度量就是我们通常理解的“距离”。n维欧几里得空间中的点是n个实数构成的向量。该空间中的常规距离度量（我们称为L2范数）定义为：$$d\left( [x_1, x_2, \ldots, x_n], [y_1, y_2, \ldots, y_n] \right) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$也就是说，我们在每个维度上对距离进行平方，将平方值相加，然后取正平方根。

很容易验证距离度量的前三个条件是满足的。两点之间的欧氏距离不可能为负，因为取的是正平方根。由于所有实数的平方都是非负的，只要存在某个i使得xi ≠ yi，距离就严格为正。反之，若所有i都满足xi = yi，则距离显然为0。对称性成立是因为(xi - yi)² = (yi - xi)²。三角不等式的验证需要大量代数运算，但这是欧氏空间的固有性质：三角形任意两边长度之和不小于第三边长度。

还存在其他用于欧几里得空间的距离度量。对于任意常数r，我们可以将Lr范数定义为由以下公式给出的距离度量d：$$d\left( [x_1, x_2, \ldots, x_n], [y_1, y_2, \ldots, y_n] \right) = \left( \sum_{i=1}^{n} |x_i - y_i|^r \right)^{1/r}$$
当r=2时，即为前文提到的常规L2范数。另一种常见的距离度量是L1范数，又称曼哈顿距离。该距离计算的是两点在各维度上坐标差值的绝对值之和。其得名源于：若两点间移动必须沿网格线行进（如曼哈顿城区街道的布局方式），则该距离即为实际需要行进的路程长度。

另一种有趣的距离度量是L∞范数，它是当r趋近于无穷大时Lr范数的极限。随着r不断增大，只有差异最大的维度才起决定性作用，因此严格来说，L∞范数被定义为所有维度i上|xi − yi|的最大值。

**例 3.12：** 考虑二维欧几里得空间（常规平面）及点(2,7)和(6,4)。L2范数给出的距离为√[(2−6)²+(7−4)²]=√(4²+3²)=5；L1范数给出的距离为|2−6|+|7−4|=4+3=7；L∞范数给出的距离为max(|2−6|,|7−4|)=max(4,3)=4。

#### 3.5.3 杰卡德距离

如本节开头所述，我们将集合的杰卡德距离定义为 d(x, y) = 1 − SIM(x, y)。也就是说，杰卡德距离等于1减去集合x与y的交集大小与并集大小之比。必须验证该函数是否满足距离度量的条件。

1. d(x, y) 具有非负性，因为交集的大小不可能超过并集的大小。
2. 当 x = y 时，d(x, y) = 0，因为 x ∪ x = x ∩ x = x。然而，若 x ≠ y，则 x ∩ y 的大小严格小于 x ∪ y 的大小，因此 d(x, y) 严格为正。
3. d(x, y) = d(y, x)，因为并集和交集均具有对称性，即 x ∪ y = y ∪ x 且 x ∩ y = y ∩ x。
4. 关于三角不等式，回顾第 3.3.3 节可知，SIM(x, y) 表示随机最小哈希函数将 x 和 y 映射到同一值的概率。因此，Jaccard 距离 d(x, y) 即为随机最小哈希函数不将 x 和 y 映射到同一值的概率。于是，我们可以将条件 d(x, y) ≤ d(x, z) + d(z, y) 转化为以下陈述：对于随机最小哈希函数 h，h(x) ≠ h(y) 的概率不超过 h(x) ≠ h(z) 的概率与 h(z) ≠ h(y) 的概率之和。这一陈述成立的原因是，每当 h(x) ≠ h(y) 时，h(x) 和 h(y) 中至少有一个与 h(z) 不同——它们不可能同时等于 h(z)，否则 h(x) 和 h(y) 就会相同。

#### 3.5.4 余弦距离  

余弦距离适用于具有维度的空间，包括欧几里得空间及其离散版本，例如由整数分量或布尔值（0或1）分量构成的向量空间。在此类空间中，点可被视为方向。我们不区分向量与其倍数之间的差异。两个点之间的余弦距离即为指向这两个点的向量所形成的夹角。无论空间维度多少，该夹角范围始终在0到180度之间。

我们可以通过先计算角度的余弦值，再应用反余弦函数将其转换为0-180度范围内的角度来计算余弦距离。给定两个向量x和y，它们之间夹角的余弦值是点积x.y除以x和y的L2范数（即它们到原点的欧几里得距离）。回忆一下，向量[x1, x2, ..., xn]和[y1, y2, ..., yn]的点积是Σ_{i=1}^n x_i y_i。

**例 3.13：** 设两个向量分别为x = [1, 2, -1]和y = [2, 1, 1]。点积x·y为1×2 + 2×1 + (-1)×1 = 3。两个向量的L2范数都是√6。例如，x的L2范数为√(1² + 2² + (-1)²) = √6。因此，两向量夹角的余弦值为3/(√6×√6)即1/2。余弦值为1/2的夹角是60度，这就是x与y之间的余弦距离。

我们必须证明余弦距离确实是一种距离度量。我们将其定义为取值范围在0到180度之间，因此不存在负距离。当且仅当两个向量方向相同时，它们的夹角为0度。对称性显而易见：向量x与y的夹角等于y与x的夹角。三角不等式最直观的论证方式是通过物理推理：从x旋转到y的一种路径是先旋转到z，再从z旋转到y。这两段旋转角度的总和不可能小于直接从x旋转到y的角度。

#### 3.5.5 编辑距离  

当点表示为字符串时，此距离具有实际意义。两个字符串x = x₁x₂···xₙ与y = y₁y₂···yₘ之间的编辑距离，是指将x转换为y所需的最少单字符插入与删除操作次数。

示例 3.14：字符串 x = abcde 与 y = acfdeg 的编辑距离为 3。将 x 转换为 y 的步骤为：
1. 删除 b；
2. 在 c 后插入 f；
3. 在 e 后插入 g。

要将x转换为y，至少需要进行三次插入和/或删除操作。因此，d(x, y) = 3。

另一种定义和计算编辑距离d(x, y)的方法是计算x和y的最长公共子序列(LCS)。x和y的LCS是通过从x和y中删除若干位置后构造出的字符串，其长度与任何可通过此方式构造的字符串一样长。编辑距离d(x, y)可计算为x的长度加上y的长度减去它们LCS长度的两倍。

**例3.15：** 在例3.14中，字符串x=abcde与y=acfdeg存在唯一的最长公共子序列acde。我们可以确定这是可能的最长子序列，因为它包含了x和y中所有共有的字符。幸运的是，这些共有字符在两个字符串中的出现顺序一致，因此能全部用于构成最长公共子序列。注意到x的长度为5，y的长度为6，它们的最长公共子序列长度为4。因此编辑距离为5 + 6 − 2 × 4 = 3，这与例3.14中的直接计算结果一致。

再举一个例子，设x = aba，y = bab。它们的编辑距离为2。例如，我们可以通过删除第一个a，然后在末尾插入b，将x转换为y。此时存在两个最长公共子序列（LCS）：ab和ba。每个LCS都可以通过从两个字符串中各删除一个符号得到。对于同一对字符串存在多个LCS的情况，这些LCS必然具有相同长度。因此，我们可以通过公式3 + 3 − 2 × 2 = 2计算出编辑距离为2。

> [!tip]
> **非欧式空间**
> 
> 请注意，本节介绍的几种距离度量并不属于欧几里得空间。欧几里得空间的一个重要特性（我们将在第七章讨论聚类时重点提及）是：空间中点的均值始终存在且仍属于该空间。然而，考虑我们定义杰卡德距离的集合空间时，"两个集合的平均值"这一概念毫无意义。同理，在使用编辑距离的字符串空间中，我们也无法计算"字符串的平均值"。
> 
> 我们曾建议采用余弦距离的向量空间可能是欧几里得的，也可能不是。若向量的分量可取任意实数，则该空间是欧几里得的；但若限制分量为整数，则该空间便非欧几里得。请注意，例如在由两个整数分量构成的向量空间中，我们无法找到向量[1, 2]和[3, 1]的平均值——尽管若将它们视为二维欧几里得空间的成员，则可以说其平均值为[2.0, 1.5]。


编辑距离是一种距离度量。显然，任何编辑距离都不可能为负，且只有当两个字符串完全相同时，其编辑距离才为零。要证明编辑距离的对称性，只需注意到插入和删除操作的序列是可逆的——每次插入可转换为删除，反之亦然。三角不等式同样易于理解：将字符串s转换为t的一种方法是先将s转换为某个中间字符串u，再将u转换为t。因此，从s到u的编辑次数加上从u到t的编辑次数，绝不可能少于直接将s转换为t所需的最小编辑次数。

#### 3.5.6 汉明距离  

在向量空间中，我们定义两个向量之间的汉明距离为它们在各分量上不同的数量。显然，汉明距离是一种距离度量。汉明距离不可能为负，若距离为零，则两向量完全相同。该距离与两向量的顺序无关。三角不等式同样成立：若向量x与z有m个分量不同，z与y有n个分量不同，则x与y不同的分量数不超过m+n。汉明距离最常用于布尔向量（仅含0和1），但原则上，向量的分量可以来自任意集合。

**例3.16：** 向量10101与11110之间的汉明距离为3。这两个向量在第二、第四和第五分量上存在差异，而在第一和第三分量上保持一致。


### 3.6 局部敏感函数理论  

第3.4节开发的LSH技术（最小哈希函数族）是一个函数家族的示例，这些函数可以通过分桶技术组合起来，从而有效区分低距离和高距离的数据对。图3.7中S曲线的陡峭程度反映了我们能在候选对中多大程度避免假阳性和假阴性。

现在，我们将探讨除最小哈希函数外，其他能高效生成候选对的函数族。这些函数可应用于集合空间和杰卡德距离，或其他空间及距离度量方式。一个有效的函数族需满足三个条件：

1. 函数必须使相近元素对比疏远元素对更可能成为候选对。我们将在3.6.1节对此概念进行精确阐述。
2. 函数必须具有统计独立性，即可以通过独立事件的乘积规则来估算多个函数同时产生特定响应的概率。
3. 它们必须在两个方面保持高效：  
    (a) 必须能够以远低于全量比对的时间找出候选对。例如最小哈希函数具备这种特性——我们可以在与数据规模成正比的时间内完成集合哈希计算（而非与集合数量的平方成正比）。由于具有相同取值的集合会被分配到同一哈希桶，这意味着我们以远低于集合对数量的时间就隐式生成了单个最小哈希函数的候选对。  
    (b) 必须能够组合构建出更有效规避假阳性/假阴性的函数，且组合函数的时间复杂度仍需远低于全量比对。例如3.4.1节的分桶技术：通过组合多个仅满足条件3a但本身不具备理想S曲线特性的最小哈希函数，最终生成具有S曲线特性的组合函数。

我们的首要步骤是广义地定义"局部敏感函数"。随后将探讨该思想在若干应用场景中的具体实践。最后我们将讨论如何将该理论应用于采用余弦距离或欧氏距离度量的任意数据。


#### 3.6.1 局部敏感函数  

在本节中，我们将讨论用于判断两个项目是否应成为候选对的函数。这类函数通常会对项目进行“哈希”处理，并根据哈希结果是否相同来做出决策。为方便起见，我们用f(x)=f(y)表示函数f(x,y)判定“是，将x和y作为候选对”，并将此作为该含义的简写形式。同时，f(x)≠f(y)表示“除非其他函数判定需要，否则不将x和y作为候选对”。

这种形式的函数集合将被称为函数族。例如，基于特征矩阵行可能排列的每个最小哈希函数，构成一个函数族。

设d1 < d2为根据某距离度量d定义的两种距离。若对于函数族F中的每个函数f满足以下条件，则称F是(d1, d2, p1, p2)-敏感的：

1. 当d(x, y) ≤ d1时，f(x) = f(y)的概率至少为p1；
2. 当d(x, y) ≥ d2时，f(x) = f(y)的概率至多为p2。


图3.9：(d1, d2, p1, p2)-敏感函数的行为特性

图3.9展示了我们对于(d1,d2,p1,p2)-敏感族中给定函数判定两个项目为候选对的概率预期。需要注意的是，当项目间距离严格处于d1与d2之间时，我们并未作任何说明——但可以通过调整使d1和d2无限接近。代价在于这通常会导致p1和p2也随之趋近。后文将说明，在保持d1和d2固定的情况下，实现p1与p2的分离是可行的。


#### 3.6.2 Locality-Sensitive Families for Jaccard Distance

目前，我们只有一种方法来寻找局部敏感函数族：使用最小哈希函数族，并假设距离度量是Jaccard距离。与之前一样，我们将最小哈希函数h解释为当且仅当h(x) = h(y)时，使x和y成为候选对。

* 对于任意满足0 ≤ d1 < d2 ≤ 1的d1和d2，minhash函数族都是一个(d1, d2, 1−d1, 1−d2)-敏感的函数族。

原因是，如果d(x, y) ≤ d1（其中d表示杰卡德距离），那么SIM(x, y) = 1 − d(x, y) ≥ 1 − d1。但我们知道，x和y的杰卡德相似度等于最小哈希函数将x和y映射到同一值的概率。类似的论证也适用于d2或其他任何距离。

**示例3.17：** 我们可以设d1=0.3，d2=0.6。此时可以断言最小哈希函数族是一个(0.3,0.6,0.7,0.4)-敏感族。也就是说，当x和y之间的杰卡德距离不超过0.3时（即SIM(x,y)≥0.7），最小哈希函数将x和y映射为相同值的概率至少为0.7；而当x和y之间的杰卡德距离不小于0.6时（即SIM(x,y)≤0.4），它们被映射为相同值的概率至多为0.4。需要注意的是，我们也可以选择其他d1和d2值来作出相同断言，唯一要求是d1必须小于d2。

#### 3.6.3 局部敏感族的放大  

假设给定一个(d1, d2, p1, p2)-敏感的族F。我们可以通过AND-构造在F上构建一个新族F′，其定义如下：F′的每个成员由F的r个成员组成（r为固定值）。若f属于F′，且f由F的成员集合{f1, f2, ..., fr}构造而成，则当且仅当对所有i = 1, 2, ..., r满足fi(x) = fi(y)时，我们称f(x) = f(y)。注意，该构造模拟了单个波段中r行的作用：当波段内所有r行都判定x和y相等时（即该行认为x和y是候选对），该波段就将x和y标记为候选对。

由于F族的成员是独立选择以构成F'族的成员，因此我们可以断言F'族是一个d1, d2, (p1)^r, (p2)^r敏感的哈希族。也就是说，对于任意点对(p)，若p表示F族成员将(x,y)判定为候选对的概率，那么F'族成员将其判定为候选对的概率就是p^r。

另一种构造方法称为OR-构造，它能将(d1, d2, p1, p2)-敏感的哈希函数族F转化为(d1, d2, 1 − (1 − p1)^b, 1 − (1 − p2)^b)-敏感的哈希函数族F′。F′中的每个函数f由F中的b个函数（记为f1, f2, ..., fb）构成，我们定义f(x) = f(y)当且仅当存在至少一个i值使得fi(x) = fi(y)。这种OR-构造模拟了组合多个波段的效果：只要任意一个波段使x和y成为候选对，它们就会成为候选对。

若p表示F中某个成员将(x, y)判定为候选对的概率，则1−p即为不将其判定为候选对的概率。(1−p)^b表示f1, f2, ..., fb全部不将(x, y)判定为候选对的概率，而1−(1−p)^b则表示至少有一个fi会将其判定为候选对的概率，因此f最终会将(x, y)判定为候选对。

需要注意的是，AND 构造会降低所有概率值，但若合理选择函数族F和参数r，我们能使较小的概率p2无限趋近于0，而较高的概率p1仍显著远离0。类似地，OR构造会使所有概率值上升，但通过合理选择F和参数b，可使较大概率逼近1，同时较小概率仍与1保持距离。我们可以按任意顺序级联AND与OR构造，使得低概率无限接近0、高概率无限接近1。当然，使用的构造次数越多，选取的r和b值越大，所需原始函数族的基数就越大。因此，最终函数族的性能越优越，应用该族函数所需的时间成本就越高。

**示例3.18：** 假设我们初始有一个族F。首先采用r=4的AND构造法生成族F₁，接着对F₁应用b=4的OR构造法产生第三个族F₂。需要注意的是，F₂的每个成员均由F的16个成员构建而成，这种情况类似于初始使用16个最小哈希函数，并将它们划分为四个波段，每个波段包含四行。

$$\begin{array}{|c|c|}
\hline
p & 1 - (1 - p^4)^4 \\
\hline
0.2 & 0.0064 \\
0.3 & 0.0320 \\
0.4 & 0.0985 \\
0.5 & 0.2275 \\
0.6 & 0.4260 \\
0.7 & 0.6666 \\
0.8 & 0.8785 \\
0.9 & 0.9860 \\
\hline
\end{array}$$

**图3.10：** 4路AND构造后接4路OR构造的效果


四路与门函数将任意概率p转换为p⁴。当我们后续采用四路或门结构时，该概率会进一步转化为1−(1−p⁴)⁴。图3.10展示了该函数的部分取值。这个函数呈现S型曲线特征：初始保持低位，随后急剧上升（虽然斜率最高不超过2），最终在高位趋于平缓。与所有S型曲线相同，它具有不动点——即应用该函数时保持不变的p值。本例中，不动点是满足p=1−(1−p⁴)⁴的p值。通过观察可知该不动点介于0.7至0.8之间：低于该值时概率被衰减，高于该值时概率被放大。因此，若选取高于不动点的高概率和低于不动点的低概率，就能实现低概率衰减而高概率放大的预期效果。

假设F是一个最小哈希函数族，被视为(0.2, 0.6, 0.8, 0.4)-敏感族。那么通过4路AND操作后接4路OR操作构建的函数族F2，就是一个(0.2, 0.6, 0.8785, 0.0985)-敏感族，这可以从图3.10中0.2和0.6对应的行读出。通过用F2替换F，我们同时降低了假阴性率和假阳性率，代价是应用这些函数所需时间增加了16倍。
$$\begin{array}{|c|c|}
\hline
p & (1 - (1 - p)^4)^4 \\
\hline
0.1 & 0.0140 \\
0.2 & 0.1215 \\
0.3 & 0.3334 \\
0.4 & 0.5740 \\
0.5 & 0.7725 \\
0.6 & 0.9015 \\
0.7 & 0.9680 \\
0.8 & 0.9936 \\
\hline
\end{array}$$
**图3.11：** 4路OR构造后接4路AND构造的效果

**示例3.19**：在相同成本下，我们可以先应用4路OR构造，再应用4路AND构造。图3.11展示了该构造对概率的转换效果。例如，假设F是一个(0.2, 0.6, 0.8, 0.4)-敏感族，那么构造后的新族将变为(0.2, 0.6, 0.9936, 0.5740)-敏感族。这种选择未必最优——虽然较高概率值更接近1，但较低概率也随之上升，会导致误报数量增加。

**示例3.20**：我们可以根据需要级联多个构造。例如，可以先将示例3.18的构造应用于最小哈希函数族，再对所得函数族应用示例3.19的构造。这样构建的函数族中，每个函数将由256个最小哈希函数组合而成。该构造能够将(0.2, 0.8, 0.8, 0.2)-敏感的函数族转化为(0.2, 0.8, 0.99999996, 0.0008715)-敏感的函数族。

## 3.7 适用于其他距离度量的LSH族  

并非所有距离度量都能保证存在局部敏感哈希函数族。目前我们仅针对杰卡德距离介绍了此类函数族。本节将展示如何为汉明距离、余弦距离以及常规欧氏距离构建局部敏感的哈希函数族。

#### 3.7.1 面向汉明距离的LSH函数族  

为汉明距离构建局部敏感的函数族非常简单。假设我们有一个d维向量空间，h(x, y)表示向量x和y之间的汉明距离。如果我们取向量的任意一个位置，比如第i个位置，我们可以定义函数fi(x)为向量x的第i位。那么fi(x) = fi(y)当且仅当向量x和y在第i位上一致。因此，对于随机选择的i，fi(x) = fi(y)的概率恰好是1 − h(x, y)/d；也就是说，这个概率等于x和y在位置上一致的比例。

这种情况与我们遇到的最小哈希（minhashing）情形几乎完全相同。因此，由函数{f1, f2, ..., fd}构成的族F是一个(d1, d2, 1−d1/d, 1−d2/d)-敏感的哈希函数族，其中d1 < d2。该哈希函数族与最小哈希函数族之间仅存在两点差异。

1. 雅卡尔距离的取值范围是0到1，而维度为d的向量空间上的汉明距离取值范围是0到d。因此需要通过除以d来缩放距离，将其转化为概率值。
2. 虽然最小哈希函数的供应几乎是无限的，但汉明距离对应的函数族F的大小仅为d。

第一点无关紧要，它仅要求我们在适当的时候除以d即可。第二点则更为关键。如果d值较小，那么通过AND和OR结构构建的函数数量将受到限制，从而制约了我们使S曲线变得陡峭的程度。

#### 3.7.2 随机超平面与余弦距离  

回顾3.5.4节可知，两个向量之间的余弦距离就是这两个向量之间的夹角。例如，在图3.12中，我们看到向量x和y之间有一个夹角θ。需要注意的是，这些向量可能位于一个高维空间中，但它们始终会定义一个平面，而它们之间的夹角就是在这个平面中测量的。图3.12是包含x和y的平面的“俯视图”。

[图]

**图3.12**：两个向量之间的夹角为θ

假设我们选择一个通过原点的超平面。该超平面与x和y坐标平面相交于一条直线。图3.12展示了两种可能的超平面：其中一个的交线是虚线，另一个的交线是点线。要随机选择一个超平面，我们实际上是选择该超平面的法向量，记为v。该超平面就是所有与v的点积为零的点构成的集合。

首先，考虑一个向量v，它垂直于图3.12中虚线所表示的投影超平面；也就是说，x和y位于该超平面的两侧。因此，点积v·x和v·y将具有不同的符号。例如，如果我们假设v是一个在x和y所在平面上的投影位于图3.12中虚线上方的向量，那么v·x为正，而v·y为负。反之，若法向量v朝相反方向延伸至虚线下方，则v·x为负而v·y为正，但两者的符号仍然相反。

另一方面，随机选择的向量v可能垂直于某个超平面，如图3.12中的虚线所示。在这种情况下，v·x和v·y具有相同的符号。若v的投影向右延伸，则两个点积均为正；若v向左延伸，则两者均为负。

随机选择的向量与虚线所示的超平面而非点线所示的超平面垂直的概率是多少？由于随机超平面与xy平面相交所形成的直线角度均匀分布，因此该超平面呈现虚线状的概率为θ/180，否则将呈现点线状。

因此，我们局部敏感哈希函数族F中的每个哈希函数f都由一个随机选择的向量vf构建而成。给定两个向量x和y，当且仅当点积vf·x与vf·y的符号相同时，称f(x)=f(y)。此时F就成为余弦距离下的局部敏感函数族。其参数设置与3.6.2节描述的杰卡德距离函数族基本相同，唯一的区别在于距离范围是0-180度而非0-1。具体而言，F是一个(d1,d2,(180-d1)/180,d2/180)敏感的哈希函数族。基于此，我们可以像处理基于最小哈希的函数族那样，对该函数族进行任意程度的扩展放大。


#### 3.7.3 草图法  

相较于从所有可能的向量中随机选取，将选择范围限制在由+1和-1构成的向量上就足以保证随机性。任意向量x与由+1和-1组成的向量v的点积计算方法是：先累加x中对应v为+1的分量，再减去x中对应v为-1的分量。

如果我们选取一组随机向量，例如v₁, v₂, ..., vₙ，可以通过计算v₁·x, v₂·x, ..., vₙ·x将它们应用于任意向量x，然后将正值替换为+1，负值替换为−1。结果称为x的草图。对于零值可以任意处理，例如随机选择+1或−1。由于点积结果为零的概率极小，这种选择基本不会产生影响。

**例3.21：** 假设我们的空间由4维向量构成，随机选取三个向量：v1 = [+1, −1, +1, +1]，v2 = [−1, +1, −1, +1]，以及v3 = [+1, +1, −1, −1]。对于向量x = [3, 4, 5, 6]，其素描结果为[+1, +1, −1]。具体而言，v1·x = 3−4+5+6 = 10。由于结果为正，素描的第一个分量为+1。同理，v2·x = 3且v3·x = −4，因此素描的第二个分量为+1，第三个分量为−1。

考虑向量 y = [4, 3, 2, 1]。我们同样可以计算出其素描值为 [+1, −1, +1]。由于 x 和 y 的素描值在 1/3 的位置上一致，我们估计它们之间的夹角为 120 度。也就是说，随机选择的一个超平面看起来像图 3.12 中虚线的概率是像点线的两倍。

上述结论实际上大错特错。我们可以通过计算向量x与y的夹角余弦值来验证，该值为x·y的点积结果（6×1 + 5×2 + 4×3 + 3×4=40）除以两向量的模长。经计算，x的模长为√(6²+5²+4²+3²)=9.274，y的模长为√(1²+2²+3²+4²)=5.477。因此两向量夹角的余弦值为0.7875，对应角度约为38度。但若考察所有16个由+1和-1组成的四维向量v时，会发现仅有四个向量（v2、v3及其互补向量[+1,-1,+1,-1]和[-1,-1,+1,+1]）与x、y的点积符号相异。这意味着若选用这十六个向量构建草图，估算得到的角度值将是180度除以4，即45度。

#### 3.7.4 面向欧氏距离的LSH函数族  

现在，让我们转向欧氏距离（第3.5.2节），探讨能否为该距离设计一个局部敏感的哈希函数族。我们将从二维欧氏空间开始构建。函数族F中的每个哈希函数f都将与空间中随机选取的一条直线相关联。如图3.13所示，选取常数a并将直线分割成长度为a的若干段（图中"随机"直线被调整为水平方向以便示意）。

[图]

**图3.13**：相距d ≫ a的两点被哈希到同一桶中的概率很小

这条线段被分割为若干桶，函数f将点哈希映射到这些桶中。点的投影落在哪个桶区间，就被哈希到对应的桶。当两点间距离d远小于参数a时，它们有很大概率会被哈希到同一个桶中，此时哈希函数f将判定这两点相等。例如，若d=a/2，则两点落入同一桶的概率至少为50%。事实上，若随机选择的线段与两点连线的夹角θ较大，则两点落入同一桶的概率会更高——当θ达到90度时，两点必定会落入同一个桶中。

然而，假设d大于a。为了使两点有机会落入同一桶中，必须满足d cos θ ≤ a。图3.13的示意图说明了这一条件的必要性。需要注意的是，即使d cos θ ≪ a，两点仍不一定会落入同一桶中。但我们可以确定以下结论：若d ≥ 2a，则两点落入同一桶的概率不超过1/3。其原因是，要使cos θ小于1/2，θ必须处于60度至90度之间；若θ处于0度至60度范围，则cos θ大于1/2。由于θ是平面上两条随机直线之间较小的夹角，θ落在0到60度区间的概率是落在60到90度区间的两倍。

我们得出以下结论：上述构造的哈希函数族F构成一个(a/2, 2a, 1/2, 1/3)-敏感的局部敏感哈希族。具体而言，当两点间距离不超过a/2时，它们被映射到同一哈希桶的概率至少为1/2；而当两点间距离不小于2a时，它们被映射到同一哈希桶的概率至多为1/3。如同我们讨论过的其他局部敏感哈希函数实例，可以通过适当方法对该函数族进行概率增强。

#### 3.7.5 欧氏空间的更多LSH函数族  

第3.7.4节开发的哈希函数族存在一些不尽如人意之处。首先，该技术仅针对二维欧氏空间进行了描述。如果我们的数据是高维空间中的点，该如何处理？其次，对于Jaccard距离和余弦距离，只要满足d1 < d2，我们就能为任意距离对d1和d2开发出局部敏感函数族。而在第3.7.4节中，我们似乎需要更强的条件d1 < 4d2。

然而，我们主张对于任意d1 < d2及任意维度数，都存在一个局部敏感的哈希函数族。该函数族的哈希函数仍源自空间中的随机直线和划分直线的桶大小a。我们仍通过将点投影到直线上进行哈希处理。给定d1 < d2，我们可能无法精确计算距离为d1的两点落入同一桶的概率p1，但可以确定该概率必然大于距离为d2的两点落入同一桶的概率p2。这是因为该概率必然随着距离减小而增大。因此，即便难以计算p1和p2的具体值，我们仍可断言：对于任意d1 < d2及任意给定维度数，都存在一个满足(d1, d2, p1, p2)敏感特性的哈希函数族。

利用第3.6.3节的放大技术，我们可以调整这两个概率，使其围绕任意特定值分布，并拉开任意所需的差距。当然，想要让概率差距越大，所需使用的底层哈希函数集F中的函数数量就越多。

## 3.8 局部敏感哈希的应用 

本节将探讨局部敏感哈希在实际应用中的三个示例。每种情况下，我们所学的技术都需要进行调整，以满足问题的特定约束条件。我们将涵盖的三大主题是：

1. 实体解析：该术语指代将指向同一现实实体的数据记录进行匹配，例如同一个人。此处解决的核心问题是记录间的相似度无法完全契合理论构建所依据的相似集合模型或相似向量模型。
2. 指纹匹配：虽然指纹可被表示为集合形式，但我们将探索与最小哈希不同的局部敏感哈希函数族来实现匹配。
3. 新闻文章匹配：在此场景中，我们采用一种聚焦在线新闻网页核心内容的特殊分片方法，通过忽略广告及报纸专属内容等无关元素来构建匹配模型。

#### 3.8.1 实体解析  

通常我们会拥有多个数据集，并且知道它们指向相同的实体。例如，多个不同的文献数据源可能提供关于相同书籍或论文的信息。一般情况下，我们会遇到描述某类实体（如人或书籍）的记录。这些记录可能具有相同的格式，也可能格式各异，包含不同类型的信息。

关于同一实体的信息可能存在差异，其原因多种多样——即便所涉字段本应相同。例如：不同记录中名称的表述可能因拼写错误、缺少中间名首字母、使用昵称等情况而不同。"Bob S. Jomes"与"Robert Jones Jr."可能是同一人，也可能不是。若记录来源不同，字段设置也会存在差异：某个数据源可能包含"年龄"字段，而另一个则没有；后者可能设有"出生日期"字段，也可能完全不包含出生时间信息。

#### 3.8.2 实体解析示例  

我们将通过一个真实案例来研究LSH如何应用于实体解析问题。公司A受雇于公司B，为其招揽客户。只要客户保持订阅状态，公司B就会每年向公司A支付服务费。后来双方发生争执，对于A究竟为B提供了多少客户存在分歧。双方各自拥有约100万条记录，其中部分记录描述的是同一批人——即A为B提供的客户群。这些记录包含不同的数据字段，但遗憾的是没有任何字段明确标注"该客户由A提供给B"。因此，问题的核心在于匹配两个数据集的记录，以判断哪些记录对代表同一个人。

每条记录都包含人员的姓名、地址和电话号码字段。然而，这些字段中的值可能因多种原因存在差异。不仅存在第3.8.1节提到的拼写错误和其他命名差异，还存在其他可能导致不一致的情况。客户可能向A提供家庭电话，而向B提供手机号码；或者他们可能搬家后只通知B而未告知A（因为他们不再需要与A保持联系）。此外，电话号码的区号有时也会发生变更。

识别记录的策略是通过对三个字段（姓名、地址和电话）的差异进行评分来实现的。为了创建一个描述两条记录（一条来自A，另一条来自B）描述同一人可能性的分数，每个字段被分配了100分，因此所有三个字段完全匹配的记录得分为300分。然而，每个字段的不匹配都会导致扣分。初步方案采用编辑距离（第3.5.5节）计算差异，但扣分随距离增加呈二次方增长。随后，通过某些公开表格在适当情况下降低扣分。例如，“Bill”和“William”被视为仅相差一个字母，尽管它们的编辑距离为5。

然而，对所有一万亿条记录对进行评分并不可行。因此，我们采用了一种简单的局部敏感哈希（LSH）方法来聚焦潜在匹配项。该方法使用了三种“哈希函数”：第一种仅当记录具有完全相同的名称时才将其分配到同一哈希桶；第二种对地址执行相同操作，第三种则针对电话号码。实际操作中并未真正计算哈希值，而是通过排序实现——首先按名称排序，使名称相同的记录连续排列，从而对这些记录的名称、地址和电话字段进行整体相似性评分；接着按地址排序，对地址相同的记录评分；最后按电话号码排序，对号码相同的记录进行评分。

这种方法漏掉了一对确实代表同一个人的记录，但三个字段都没有完全匹配。由于目标是在法庭上证明这两人是同一人，这样的记录对不太可能被法官认为足够相似而予以接受。

#### 3.8.3 验证记录匹配  

剩下的问题是确定多高的分数才能表明两条记录确实代表同一个人。在当前示例中，有一种简单的方法可以做出判断，该技术也可应用于许多类似场景。具体做法是查看相关记录的创建日期，并假设在A公司购买服务后，最多90天内一定会在B公司完成注册。因此，若随机选取两条记录进行匹配（仅限制B记录日期比A记录日期晚0至90天），其平均延迟时间应为45天。

研究发现，在获得满分300分的配对中，平均延迟为10天。若假设300分配对均为正确匹配，则可观察任意分数s对应的配对组，并计算这些配对的平均延迟。设该平均延迟为x，且分数s配对中真实匹配的比例为f，则有等式x = 10f + 45(1 - f)，即x = 45 - 35f。由此可解得：分数s配对中真实匹配的比例为(45 - x)/35。

> [!NOTE]
> 记录匹配何时足够可靠？  
> 
> 虽然每个案例各不相同，但了解3.8.3节的实验在3.8.2节数据上的表现或许具有参考价值。对于分数降至185的情况，x值非常接近10，这意味着这些分数实质上表明两条记录代表同一个人的可能性近乎1。需注意，此例中185分对应的场景是：一个字段完全匹配（这是必要条件，否则记录根本不会被评分），另一个字段完全不同，第三个字段存在微小差异。此外，即使分数低至115，x值仍明显小于45，说明部分低分匹配对确实代表同一人。115分对应的情形是：一个字段匹配，而另外两个字段仅存在微弱相似性。


每当出现以下情况时，均可采用相同技巧：

1. 存在一个评分系统，用于评估两条记录代表同一实体的可能性；
2. 存在某个未参与评分的字段，我们可以从中提取出一个度量指标，该指标在真匹配对和假匹配对之间的平均值存在差异。

例如，假设在我们的示例中，公司A和公司B都记录了“身高”字段。我们可以计算随机记录对之间的平均身高差异，也可以计算获得完美匹配分数（因而必然代表同一实体）的记录之间的平均身高差异。对于给定分数s，我们可以评估具有该分数的记录对的平均身高差异，并估计这些记录代表同一实体的概率。具体而言，若h0表示完美匹配对的平均身高差异，h1表示随机记录对的平均身高差异，h表示分数为s的记录对的平均身高差异，则分数s对应的优质记录对比例为(h1 − h)/(h1 − h0)。

#### 3.8.4 指纹匹配  

当通过计算机进行指纹匹配时，通常的表示方式并非图像，而是一组 minutiae（细节特征点）所在位置的集合。在指纹描述中，minutia 指代指纹中发生异常情况的位置，例如两条纹线交汇或某条纹线终止。若在指纹上覆盖网格，便可通过 minutiae 所在的网格区域集合来表示该指纹。

理想情况下，在叠加网格之前，指纹会经过尺寸和方向归一化处理。这样，如果我们采集同一手指的两幅图像，就能确保细节特征点落在完全相同的网格单元中。本文不探讨图像归一化的最佳方法，而是假定通过综合运用网格尺寸选择、当细节点靠近网格边界时将其分配到相邻单元等技术手段，我们可以认为：来自同一手指的两幅图像的网格单元在细节特征点存在与否的匹配概率，会显著高于来自不同手指的图像网格单元。

因此，指纹可以通过网格方块的集合来表示——这些方块是其特征点所在的位置——并像任何集合一样使用杰卡德相似度或距离进行比较。然而，指纹比对存在两种形式：  
• ​**多对一问题**​ 是常见的场景。例如在枪支上发现一枚指纹，需将其与大型数据库中的所有指纹进行比对，以找出匹配项。  
• ​**多对多问题**​ 则是针对整个数据库，检测是否存在代表同一人的指纹对。

虽然多对多版本与我们寻找相似项时遵循的模型相匹配，但同样的技术也可用于加速多对一问题的解决。

#### 3.8.5 适用于指纹匹配的LSH族  

我们可以对表示指纹的集合进行最小哈希处理，并采用3.4节的标准LSH技术。但由于这些集合是从相对较小的网格点集合（可能为1000个）中选取的，将其最小哈希为更简洁签名的必要性尚不明确。此处我们将研究另一种形式的局部敏感哈希，该技术特别适用于我们讨论的此类数据。

假设在某个示例中，随机指纹的随机网格方块内存在细节特征点的概率为20%。同时假定，若两个指纹来自同一手指，且其中一个指纹在给定网格方块中存在细节特征点，则另一个指纹在同一位置也存在细节特征点的概率为80%。我们可以按如下方式定义一个局部敏感的哈希函数族：该函数族F中的每个函数f由三个网格方块定义。当两个指纹在这三个网格方块中均存在细节特征点时，函数f输出"是"，否则输出"否"。换言之，可以认为函数f将所有在这三个指定网格点均存在细节特征点的指纹映射到同一个存储桶中，而将其他每个指纹分别映射到各自的独立存储桶。在下文中，我们将把前一种存储桶称为函数f的"专属"存储桶，而忽略那些必须作为单例存在的存储桶。

若要解决多对一问题，我们可以使用函数族F中的多个函数，并预先计算这些函数回答"是"的指纹桶。当需要匹配新指纹时，我们确定该指纹属于哪些桶，并与这些桶中的所有指纹进行比对。针对多对多问题，我们需为每个函数计算对应的指纹桶，并逐一比对所有桶中的指纹。

让我们思考需要多少函数才能以合理概率匹配到结果，而无需将枪支上的指纹与数据库中数百万指纹逐一比对。首先，对于函数集F中的任意函数f，来自不同手指的两个指纹落入同一桶的概率为(0.2)^6=0.000064。这是因为只有当两个指纹都在f关联的三个网格点中各有一个特征点时才会被归入同一桶，而每个独立事件发生的概率均为0.2。

现在，考虑来自同一手指的两个指纹最终落入f对应桶中的概率。第一个指纹在f所属的三个方格中均出现细节特征的概率是(0.2)³=0.008。但如果确实如此，那么另一个指纹也满足该条件的概率为(0.8)³=0.512。因此，若两个指纹来自同一手指，它们同时出现在f对应桶中的概率为0.008×0.512=0.004096。这个概率并不高，大约为两百分之一。然而，如果我们使用F中的多个函数（但不过多），就能在避免过多假阳性（即需要比对但不匹配的指纹）的同时，获得较高的同一手指指纹匹配概率。

**例3.22**：具体而言，假设我们随机选取F族的1024个函数进行运算。接着通过对其执行1024路"或"操作构建新函数族F1。此时F1将同一手指的指纹归入至少一个相同桶的概率为1 − (1 − 0.004096)^1024 = 0.985；而不同手指的指纹被误归入同一桶的概率为(1 − (1 − 0.000064)^1024 = 0.063。这意味着我们得到约1.5%的假阴性率和约6.3%的假阳性率。

示例3.22的结果并非最优解。虽然该方案仅存在1.5%的概率无法识别枪支上的指纹，但却迫使我们筛查整个数据库6.3%的内容。若增加函数集F的数量，虽能略微将漏检率降至1.5%以下，却会导致误报数量大幅上升。另一方面，我们也可以采用AND构造法——这样既能显著降低误报概率，又仅会小幅提高漏检率。例如，我们可以从F中选取2048个函数，将其分为两组各1024个函数，并为每个函数构建哈希桶。不过当遇到枪支上的指纹P时：

1. 在第一组中找到包含P的桶，并取这些桶的并集。
2. 对第二组执行相同操作。
3. 取两个并集的交集。
4. 仅将P与交集中的指纹进行比对。

需要注意的是，我们仍需对大量指纹集合进行并集和交集运算，但实际只需比较其中一小部分。指纹比对消耗了绝大部分时间；而在步骤（1）和（2）中，指纹可通过其在数据库中的整数索引来表示。

若采用此方案，匹配指纹的检测概率为(0.985)²=0.970，即会出现约3%的假阴性。然而，假阳性概率仅为(0.063)²=0.00397。这意味着我们只需检查数据库约1/250的数据量。

#### 3.8.6 相似新闻文章  

我们的最后一个案例研究涉及如何通过将源自同一基础文本的网页归类，来组织大型在线新闻文章库。美联社等新闻机构通常会产生一则新闻并将其分发给多家报社。每家报社都会在自己的网络版上发布这则新闻，但会围绕新闻添加报社特有的信息，例如报社的名称和地址、相关文章的链接以及广告链接。此外，报社通常会对文章进行修改，可能会删去最后几段，甚至删除中间的部分文本。因此，同一则新闻文章在不同报社的网站上可能会呈现出很大的差异。

这个问题看起来与3.4节提出的建议非常相似：寻找那些具有高杰卡德相似度的文档。需要注意的是，这个问题与寻找报道相同事件的新闻文章是不同的。后者需要采用其他技术，通常是检查文档中的重要词汇集（我们在1.3.1节简要讨论过这个概念），并通过聚类将它们分组，以将关于同一主题的不同文章归类在一起。

然而，人们发现针对所述数据类型，一种改进的局部敏感哈希变体更为有效。问题在于3.2节所述的传统局部敏感哈希方法对文档所有部分一视同仁，但我们希望忽略广告或报纸添加的其他文章标题链接等非新闻正文内容。研究发现，正文文本与广告/标题文本存在显著差异：正文中停用词（如"the"或"and"等高频词）出现频率远高于其他部分。实际应用中停用词表规模不尽相同，但通常包含数百个最高频词汇。

**例3.23：** 一则典型广告可能仅简洁地写道"买Sudzo吧"。而同样的意思若以文章形式呈现，则可能表述为"我建议您购买Sudzo洗衣粉"。在后一个句子中，通常会将"我"、"那"、"您"、"为"和"您的"视为停用词处理。

假设我们将一个shingle定义为停用词后接接下来的两个单词。那么示例3.23中的广告"Buy Sudzo"就不包含任何shingles，因此不会反映在包含该广告的网页表示中。另一方面，示例3.23中的句子将由五个shingles表示："I recommend that"、"that you buy"、"you buy Sudzo"、"for your laundry"和"your laundry x"，其中x是该句子后跟随的任意单词。

假设我们有两个网页，每个网页由一半新闻文本和一半广告或其他停用词密度较低的材料组成。如果新闻文本相同但周边材料不同，那么我们会预期这两个页面的大部分片段是相同的，其Jaccard相似度可能达到75%。然而，若周边材料相同而新闻内容不同，则共同片段的数量会很少，可能只有25%。若采用传统的片段化方法（例如将10个连续字符作为一个片段），无论共享的是新闻还是周边材料，我们预计这两个文档会共享一半的片段（即Jaccard相似度为1/3）。

### 3.9 高相似度处理方法  

基于局部敏感哈希（LSH）的方法在可接受的相似度较低时最为有效。当我们需要寻找几乎完全相同的集合时，存在其他更快速的处理方法。此外，这些方法是精确的，能够找出所有符合目标相似度要求的项目对。与LSH可能产生的漏检情况不同，这类方法不存在假阴性结果。

#### 3.9.1 查找相同项  

极端情况是查找完全相同的项目，例如字符级完全一致的网页。直接比较两个文档是否相同很简单，但我们仍需避免对所有文档进行两两比较。最初的想法是根据文档开头的几个字符进行哈希处理，仅比较落入同一哈希桶的文档。这种方法通常有效，除非所有文档都以相同字符开头（例如HTML文档的头部声明）。

我们的第二个想法是使用一个检查整个文档的哈希函数。这种方法可行，而且如果我们使用足够多的桶，那么两个文档进入同一个桶却又不完全相同的情况将非常罕见。这种方法的缺点在于我们必须检查每个文档的每一个字符。如果我们只检查少量字符，就永远不需要检查那些独一无二且落入自己专属桶中的文档。

一种更好的方法是预先为所有文档选取若干固定随机位置，使哈希函数仅依赖于这些位置。这样既能避免所有或多数文档存在共同前缀的问题，也无需检查整个文档内容——除非文档落入与其他文档相同的哈希桶中。固定位置选择的潜在缺陷在于，若某些文档较短，可能不包含部分选定位置。但当我们寻找高度相似的文档时，实际上无需比较长度差异悬殊的文档。我们将在3.9.3节中具体运用这一思想。

#### 3.9.2 将集合表示为字符串  

现在，让我们聚焦于一个更复杂的问题：如何在一个庞大的集合集合中，找出所有具有高杰卡德相似度（例如至少0.9）的集合对。我们可以通过将全集元素按固定顺序排列，并用该顺序列出集合元素来表示一个集合。这种列表本质上是一种由全集元素构成的“字符串”。然而，这些字符串的特殊之处在于：

1. 字符串中不存在重复字符；
2. 若两个字符出现在不同字符串中，则它们在所有字符串中的出现顺序保持一致.

例3.24：假设全集由26个小写字母组成，且采用常规字母顺序。那么集合{d, a, b}对应的字符串表示为abd。

在后续讨论中，我们将默认所有字符串均按上述方式表示集合。因此，当我们谈及字符串的杰卡德相似度时，严格意义上是指这些字符串所代表集合的相似度。同样地，我们将用字符串长度作为其所代表集合元素数量的替代指标。

需要注意的是，第3.9.1节讨论的文档并不完全符合该模型，尽管我们可以将文档视为字符串。为了使文档符合模型要求，我们需要对文档进行分片处理，为分片分配顺序，并按照选定顺序用分片列表来表示每个文档。

> [!NOTE]
> 任务：符号的优化排序方案  
> 
> 相较于直接采用通用集合元素的默认排序（例如对shingles使用字典序），我们可以按照符号的稀有度进行排序——即统计每个元素在集合群中的出现频次，按出现次数从低到高排列。这种排序的优势在于：前缀中的符号通常由稀有元素构成，使得对应字符串会被归类到成员较少的索引桶中。当需要检查字符串的潜在匹配项时，我们只需对比少量候选字符串即可。

#### 3.9.3 基于长度的过滤  

利用3.9.2节所述字符串表示的最简单方法是按长度对字符串进行排序。随后，每个字符串s只需与列表中位于其后且长度不超过一定阈值的字符串t进行比较。假设两个字符串之间杰卡德距离的上界为J。对于任意字符串x，用Lx表示其长度。注意Ls ≤ Lt。由s和t所表示集合的交集元素数量最多为Ls，而并集元素数量至少为Lt。因此，s与t的杰卡德相似度（记为SIM(s,t)）至多为Ls/Lt。也就是说，若要使s和t需要被比较，必须满足J ≤ Ls/Lt，即Lt ≤ Ls/J。

**例3.25：** 假设字符串s的长度为9，我们需要寻找与之Jaccard相似度至少为0.9的字符串。此时只需将s与按长度排序后位于其后且长度不超过9/0.9=10的字符串进行比较。具体而言，就是将s与排序中位于其后且长度为9的字符串，以及所有长度为10的字符串进行对比。无需将s与其他任何字符串进行比较。

假设s的长度变为8。那么s只需与长度不超过8/0.9=8.89的字符串进行比较。这意味着长度为9的字符串因过长而无法与s达到0.9的杰卡德相似度，因此我们仅需将s与排序顺序中紧随其后且长度为8的字符串进行比对。

#### 3.9.4 前缀索引  

除了长度之外，字符串的其他特征也可用于减少识别所有相似字符串对所需的比较次数。其中最简单的方案是为每个符号创建索引（需注意，字符串的符号是指通用集合中的任一元素）。对于每个字符串s，我们选取其前p个符号构成的前缀。p的取值取决于字符串长度Ls和杰卡德距离下限J。我们将字符串s添加到其前p个符号中每个符号对应的索引中。

实际上，每个符号对应的索引就变成了一个必须进行字符串比较的存储桶。我们必须确保，任何满足相似度SIM(s, t) ≥ J的字符串t，其前缀中至少有一个符号与字符串s的前缀中的符号相同。

假设不成立；相反，SIM(s, t) ≥ J，但t不包含s的前p个符号。此时s与t能达到的最高Jaccard相似度出现在t是s的后缀时——即t由s去掉前p个符号后剩余的部分构成。此时二者的Jaccard相似度为(Ls − p)/Ls。为确保无需比较s与t，必须满足J > (Ls − p)/Ls，即p至少为⌊(1 − J)Ls⌋ + 1。为使p尽可能小（避免对字符串s建立过多索引），我们最终将索引前缀长度确定为p = ⌊(1 − J)Ls⌋ + 1。

**示例3.26：** 假设J=0.9。若Ls=9，则p=⌊0.1×9⌋+1=⌊0.9⌋+1=1。这意味着我们只需根据字符串s的首个符号建立索引。任何字符串t若未在对应位置包含s的首个符号，则其与s的Jaccard相似度将小于0.9。假设s为bcdefghij，则该字符串仅以b建立索引。若t不以b开头，则需考虑以下两种情况。

1. 如果t以a开头，且SIM(s, t) ≥ 0.9，那么t只能是abcdefghij。但如果是这种情况，t将会被同时索引在a和b下。原因是Lt = 10，因此t会以其前缀长度为⌊0.1 × 10⌋ + 1 = 2的符号被索引。
2. 如果t以c或更靠后的字母开头，那么SIM(s, t)的最大值出现在t为cdefghij时。但此时SIM(s, t) = 8/9 < 0.9。

通常，当J=0.9时，长度不超过9的字符串按其首字符建立索引，长度为10-19的字符串按其前两个字符建立索引，长度为20-29的字符串按其前三个字符建立索引，以此类推。

我们可以通过两种方式使用该索引方案，具体取决于待解决的是"多对多"问题还是"多对一"问题（该区分方法已在3.8.4节中说明）。针对多对一问题，我们为整个数据库创建索引。当查询与新集合S的匹配项时，先将该集合转换为字符串s（称为探针字符串），然后计算必须考虑的前缀长度⌊(1−J)Ls⌋+1。对于出现在s前缀位置中的每个符号，我们查找该符号对应的索引桶，并将s与该桶中所有字符串进行比对。

要解决多对多问题，首先初始化一个空的字符串及索引数据库。对于每个集合S，我们将其视为多对一问题中的新集合。将S转换为字符串s后，将其作为多对一问题中的探测字符串进行处理。但关键在于：每当检查完某个索引桶后，我们会将s也加入该桶中，这样后续处理的字符串就能与s进行匹配比较。

#### 3.9.5 利用位置信息  

考虑字符串 s = acdefghijk 和 t = bcdefghijk，并假设 J = 0.9。由于两个字符串长度均为10，它们的前两个符号被用于索引。因此，s 的索引符号是 a 和 c，而 t 的索引符号是 b 和 c。后加入的字符串会在 c 的桶中找到另一个字符串，并进行比较。然而，由于 c 是两者的第二个符号，我们知道会有两个符号（本例中为 a 和 b）出现在两个集合的并集中但不在交集中。实际上，尽管 s 和 t 从 c 开始到末尾完全相同，它们的交集为9个符号，并集为11个符号；因此 SIM(s, t) = 9/11，小于0.9。

如果我们构建索引时不仅基于符号本身，还基于该符号在字符串中的位置，就可以避免直接比较字符串s和t。具体而言，我们为每个符号-位置对(x, i)创建一个索引桶，其中包含前缀第i位为符号x的所有字符串。给定字符串s时，假设J是最小期望的Jaccard距离，我们考察s的前缀（即第1位到⌊(1−J)Ls⌋+1位）。若前缀第i位的符号是x，就将s加入对应(x, i)的索引桶中。

现在将字符串s视为探测字符串。它需要与哪些桶进行比较？我们将从左至右遍历s的前缀符号，并利用一个关键事实：只需找到可能匹配的字符串t一次，前提是之前检查过的桶中均未包含t。也就是说，若发现s的第i个符号为x，则只需在特定的小范围j值对应的桶(x, j)中进行查找。

[图]

**图3.14：** 字符串s和t分别以i−1和j−1个唯一符号开头，之后的部分完全一致。

为计算j的上界，假设存在字符串t，其前j-1个符号均未与s中的任何符号匹配，但s的第i个符号与t的第j个符号相同。当s和t在第i个与第j个符号之后完全相同时（如图3.14所示），SIM(s, t)达到最大值。此时两者交集的大小为Ls − i + 1（即s中可能存在于t的符号数量），而并集的大小至少为Ls + j − 1（s必然贡献Ls个符号，且t中至少有j-1个符号不在s中）。交集与并集大小的比值必须至少为J，因此必须满足：$$\frac{L_s - i +1}{L_s +j -1} \geq J$$
若将j从此不等式中分离出来，可得j ≤ Ls(1 − J) − i + 1 + J /J。

**示例3.27：** 考虑本节开头讨论的字符串s = acdefghijk（J=0.9）。假设s现在是探测字符串。前文已论证需要考察前两个位置，即i可取1或2。当i=1时，根据公式j ≤ (10×0.1−1+1+0.9)/0.9，只需在j≤2.11时比较符号a与(a,j)桶中的字符串。因此j仅能取1或2，不可更大。

现在假设i = 2。那么我们需要满足j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9，即j ≤ 1。由此可以得出结论：我们只需检查(a,1)、(a,2)和(c,1)这三个桶，而无需检查其他桶。相比之下，若使用第3.9.4节的桶划分方案，我们将需要检查a和c对应的所有桶，这相当于要检查所有可能的(a,j)和(c,j)桶（j为任意值）。

#### 3.9.6 在索引中使用位置和长度  

在上一节讨论j的上限时，我们假设字符串s和t中位置i和j之后的内容如图3.14所示，即这些位置之后的字符完全匹配。我们并不希望构建一个包含字符串中每个符号的索引，因为这将导致总工作量过大。然而，我们可以为索引添加被索引位置后续内容的摘要信息。这样做虽然会增加桶的数量，但仍保持在合理范围内，同时使我们无需比较整个字符串就能排除许多候选匹配项。具体方法是使用对应于符号、位置和后缀长度（即该位置之后的符号数量）的索引桶。

**示例3.28**：对于字符串s = acdefghijk，当J = 0.9时，它将被索引到(a, 1, 9)和(c, 2, 8)对应的桶中。也就是说，字符串的第一个位置符号为a，其后缀长度为9；第二个位置符号为c，其后缀长度为8。

图3.14假设字符串s的第i个位置的后缀与字符串t的第j个位置的后缀长度相同。若长度不一致，则当t较短时，我们可能得到s与t交集的更小上界；当t较长时，则可能得到并集的更大下界。假设s的前缀长度为p，t的前缀长度为q。

**情况1：** p ≥ q。此时，交集的最大规模为  
Ls − i + 1 − (p − q)  

由于 Ls = i + p，可将上述交集规模表达式改写为 q + 1。并集的最小规模仍为 Ls + j − 1（与我们未考虑后缀长度时的计算一致）。因此，我们需要满足$$\frac{q+1}{L_s+j-1}\geq J$$
这里的 $p \geq q$

**情况2：** p < q。此时，交集的最大规模仍为Ls − i + 1（不考虑后缀长度时的情况）。但并集的最小规模变为Ls + j − 1 + q − p。若再次利用关系式Ls = i + p，可将Ls − p替换为i，从而得到并集规模的公式i + j − 1 + q。若Jaccard相似度至少为J，则当p < q时需满足：  
(Ls − i + 1)/(i + j − 1 + q) ≥ J

**例3.29：** 让我们再次考虑字符串s = acdefghijk，但为了使示例展示更多细节，这次选择J = 0.8而非0.9。已知Ls = 10。由于⌊(1 − J)Ls⌋ + 1 = 3，后续需考察前缀位置i = 1、2和3的情况。与之前一样，设p为s的后缀长度，q为t的后缀长度。

首先，考虑p ≥ q的情况。我们对q和j的附加约束条件是(q + 1)/(9 + j) ≥ 0.8。接下来可以枚举i从1到3时j和q的取值组合，如下所示。

i = 1的情况：此处p = 9，因此q ≤ 9。我们考虑q的可能取值：  
q = 9：必须满足10/(9 + j) ≥ 0.8。因此j可取1、2或3。注意当j=4时，10/13 > 0.8。  
q = 8：必须满足9/(9 + j) ≥ 0.8。因此j可取1或2。当j=3时，9/12 > 0.8。  
q = 7：必须满足8/(9 + j) ≥ 0.8。仅j=1满足该不等式。  
q = 6：不存在满足条件的j值，因为对于所有正整数j，7/(9 + j) > 0.8。对于更小的q值同理。

i = 2的情况：此处p = 8，因此要求q ≤ 8。由于约束条件(q+1)/(9+j) ≥ 0.8不依赖于i，我们可以沿用上例的分析方法，但需排除q = 9的情形。因此当i = 2时，j与q的可能取值组合为：1. q = 8；j = 1；2. q = 8；j = 2；3. q = 7；j = 1。

i = 3的情况：此时p = 7，约束条件为q ≤ 7且(q + 1)/(9 + j) ≥ 0.8。唯一可行的解是q = 7且j = 1。  
接下来需要考虑p < q的情况。附加约束条件为(11 − i)/(i + j + q − 1) ≥ 0.8。同样，需逐一检验i的可能取值。

i = 1时：此时p = 9，因此要求q ≥ 10且10/(q + j) ≥ 0.8。q和j的可能取值为：1. q = 10；j = 1；2. q = 10；j = 2；3. q = 11；j = 1。  
i = 2时：此时p = 10，因此要求q ≥ 11且9/(q + j + 1) ≥ 0.8。由于j必须是正整数，无解。  
i = 3时：与i = 2情况相同，无解。

[图]

图3.15：为查找与字符串s = acdefghijk在J=0.8阈值下可能匹配项而需检查的桶已标记为x。

当我们累加i、j和q的可能组合时，会发现需要检索的索引桶集合形成一个金字塔结构。图3.15展示了必须搜索的存储桶范围。具体而言，我们需要检索满足以下条件的存储桶(x, j, q)：其中x是字符串s的第i个符号，j是与存储桶关联的位置，q表示后缀长度。 

### 3.10 第三章小结  

✦ 杰卡德相似度：集合的杰卡德相似度是指集合交集大小与并集大小的比值。该相似度度量适用于许多应用场景，包括文档的文本相似度计算以及顾客购买习惯的相似性分析。
✦ 分片（Shingling）：k-分片是指文档中连续出现的任意k个字符。若用文档的k-分片集合来表示该文档，则分片集合间的Jaccard相似度可用于衡量文档间的文本相似性。有时将分片哈希为较短长度的位串，并用哈希值集合来表示文档会更为实用。
✦ 最小哈希（Minhashing）：基于全域集合排列的集合最小哈希函数。对于任意排列，集合的最小哈希值是该集合在排列顺序中首个出现的元素。
✦ Minhash签名：我们可以通过选择一组排列，并为每个集合计算其minhash签名来表示集合。该签名是通过将列表中的每个排列应用于该集合所得到的一系列minhash值。给定两个集合，预期会产生相同minhash值的排列比例恰好等于这两个集合的Jaccard相似度。
✦ 高效最小哈希：由于实际无法生成真正的随机排列，通常的做法是通过选取一个随机哈希函数来模拟排列，并将集合的最小哈希值定义为该集合中所有元素哈希值的最小值。
✦ 面向签名的局部敏感哈希技术：该技术能避免计算所有集合对或其最小哈希签名的相似度。当获得集合的签名后，可将签名划分为多个波段，仅当两个集合在至少一个波段中完全相同时，才需计算它们的相似度。通过合理选择波段大小，可筛除大部分不满足相似度阈值的集合对。
✦ 距离度量：距离度量是定义在空间中点对上的函数，需满足特定公理。当两点相同时，其距离为0；若两点不同，则距离大于0。距离具有对称性，即两点顺序不影响计算结果。此外，距离度量必须满足三角不等式：任意两点间的距离不超过它们与第三点距离之和。
✦ 欧几里得距离：最常见的距离概念是n维空间中的欧几里得距离。该距离（有时称为L2范数）是各维度上两点差值平方和的平方根。另一种适用于欧几里得空间的距离称为曼哈顿距离或L1范数，即各维度上两点差值绝对值的总和。
✦ 杰卡德距离：1减去杰卡德相似度即为一种距离度量，称为杰卡德距离。


✦ Cosine Distance: The angle between vectors in a vector space is the cosine distance measure. We can compute the cosine of that angle by taking the dot product of the vectors and dividing by the lengths of the vectors. ✦ Edit Distance: This distance measure applies to a space of strings, and is the number of insertions and/or deletions needed to convert one string 3.10. SUMMARY OF CHAPTER 3 109 into the other. The edit distance can also be computed as the sum of the lengths of the strings minus twice the length of the longest common subsequence of the strings. ✦ Hamming Distance: This distance measure applies to a space of vectors. The Hamming distance between two vectors is the number of positions in which the vectors differ. ✦ Generalized Locality-Sensitive Hashing: We may start with any collection of functions, such as the minhash functions, that can render a decision as to whether or not a pair of items should be candidates for similarity checking. The only constraint on these functions is that they provide a lower bound on the probability of saying “yes” if the distance (according to some distance measure) is below a given limit, and an upper bound on the probability of saying “yes” if the distance is above another given limit. We can then increase the probability of saying “yes” for nearby items and at the same time decrease the probability of saying “yes” for distant items to as great an extent as we wish, by applying an AND construction and an OR construction. ✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of basis functions to start a generalized LSH for the cosine distance measure by identifying each function with a list of randomly chosen vectors. We apply a function to a given vector v by taking the dot product of v with each vector on the list. The result is a sketch consisting of the signs (+1 or −1) of the dot products. The fraction of positions in which the sketches of two vectors agree, multiplied by 180, is an estimate of the angle between the two vectors. ✦ LSH For Euclidean Distance: A set of basis functions to start LSH for Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals, and the function answers “yes” to a pair of points that fall into the same interval. ✦ High-Similarity Detection by String Comparison: An alternative approach to finding similar items, when the threshold of Jaccard similarity is close to 1, avoids using minhashing and LSH. Rather, the universal set is ordered, and sets are represented by strings, consisting their elements in order. The simplest way to avoid comparing all pairs of sets or their strings is to note that highly similar sets will have strings of approximately the same length. If we sort the strings, we can compare each string with only a small number of the immediately following strings. ✦ Character Indexes: If we represent sets by strings, and the similarity threshold is close to 1, we can index all strings by their first few characters. The prefix whose characters must be indexed is approximately the length 110 CHAPTER 3. FINDING SIMILAR ITEMS of the string times the maximum Jaccard distance (1 minus the minimum Jaccard similarity). ✦ Position Indexes: We can index strings not only on the characters in their prefixes, but on the position of that character within the prefix. We reduce the number of pairs of strings that must be compared, because if two strings share a character that is not in the first position in both strings, then we know that either there are some preceding characters that are in the union but not the intersection, or there is an earlier symbol that appears in both strings. ✦ Suffix Indexes: We can also index strings based not only on the characters in their prefixes and the positions of those characters, but on the length of the character’s suffix – the number of positions that follow it in the string. This structure further reduces the number of pairs that must be compared, because a common symbol with different suffix lengths implies additional characters that must be in the union but not in the intersection. 3.11 References for Chapter 3 The technique we called shingling is attributed to [10]. The use in the manner we discussed here is from [2]. Minhashing comes from [3]. The original works on locality-sensitive hashing were [9] and [7]. [1] is a useful summary of ideas in this field. [4] introduces the idea of using random-hyperplanes to summarize items in a way that respects the cosine distance. [8] suggests that random hyperplanes plus LSH can be more accurate at detecting similar documents than minhashing plus LSH. Techniques for summarizing points in a Euclidean space are covered in [6]. [11] presented the shingling technique based on stop words. The length and prefix-based indexing schemes for high-similarity matching comes from [5]. The technique involving suffix length is from [12]. 1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117– 122, 2008. 2. A.Z. Broder, “On the resemblance and containment of documents,” Proc. Compression and Complexity of Sequences, pp. 21–29, Positano Italy, 1997. 3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise independent permutations,” ACM Symposium on Theory of Computing, pp. 327–336, 1998. 4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002. 3.11. REFERENCES FOR CHAPTER 3 111 5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering, 2006. 6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004. 7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518– 529, 1999. 8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006. 9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998. 10. U. Manber, “Finding similar files in a large file system,” Proc. USENIX Conference, pp. 1–10, 1994. 11. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM SIGIR Conference, July, 2008, Singapore. 12. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.


主要利用的是 shingling-minhash-LSH 技术：

* shingling：将文本拆解为拆解为长度为 $K$ 的字符串集合
* minhash：文本签名，能够表示集合并反映其相似性的较短长度的整数向量
* LSH：局部敏感哈希的目的是生成测试候选对









#### 3.4 加速

上面的计算方法虽然从计算结果上看，可以把不可能实现的置换操作以哈希方式给实现了，可以将矩阵从行数非常巨大的特征矩阵变成只有几百行的签名矩阵。然而在这个过程中计算量非常大，因为它需要考虑整个特征矩阵的所有行。

我们重新回到最原始的特征矩阵上

![image-20211016145825668](image-20211016145825668.png)

假设我们目前看到的已经是置换后的，那我们其实在找第一行为 1 的时候就停下了，例如对于 $S_1$ 而言，我们不会再看 bcde 行，对于 $S_2$ 而言，也不会再看 de 行，所以我们的注意力并没有看完所有的特征矩阵的每一行。所以我们由此可以想象出，对于一个非常巨大的特征矩阵，我们可以只看前 $m$ 行（假设一共 $k$ 行），这样一来就可以大大的减少计算量，例如上图，实际上只看前 3 行完全就可以得到正确答案。

但是这样也会引入新的问题，例如上图中，如果我只看前两行，那么 $S_2$ 这一列将看不到 1，怎么办？用 $\infty$ 表示。这样一来，含 $\infty$ 符号，在计算 Jaccard 的时候，就有 3 种情况：

1. 两列都不含 $\infty$，那就正常比较
2. 一列有，另一列没有，那么前者真实出现第一个 1 的行肯定不在前 $m$ 行，即两列 minhash 不等
3. 两列都有 $\infty$，则既不作为等值处理，也不作为不等值处理

第 3 种情况不是很常见，这种影响将在一定程度上降低我们对 Jaccard 距离估计的准确性，但不会太大。由于我们现在能够比检查所有行更快地计算所有列的 minhash 值，因此我们可以节省时间来应用更多的 minhash 函数。我们得到了比原来更好的准确度，而且比以前更快。

#### 3.5 在 Hash 函数基础上加速

实际上是将上面 3.3 中提到的 hash 方法（并不真的进行置换）与 3.4 中提到的只关注前 $m$ 行方法进行结合。虽然这种方式可能产生全是 0 的列，以至于签名矩阵中出现 $\infty$，但只要 $m$ 足够大，这种情况则很少发生，我们依然可以根据签名矩阵很好地评估原始 Jaccard 值。遇到的话，忽略掉该行。

假设 $T$ 是特征矩阵的前 $m$ 行包含的所有元素集合，那么 $S_1$ 在前 $m$ 行的元素集合为 $S_1 \cap T$，同理 $S_2$ 在前 $m$ 行的集合为 $S_2 \cap T$，现在再算 Jaccard，则，计算公式需要修改为：
$$
\text{Jaccard}(S_1, S_2)=\frac{|S_1 \cap S_2 \cap T|}{|(S_1 \cup S_2) \cap T|}
$$
然而，会有一些随机变化，因为根据 $T$，我们可以在矩阵的前 $m$ 行中可能会找到多于或少于 $X$ 类行（两列中均为 1的行）和/或 $Y$ 类行（一列为 1，另一列为 0 的行）。

为了缓解这种变化，我们对每个 minhashing 不使用相同的集合 $T$。相反，我们将特征矩阵划分为 $k/m$ 组。然后，对于每个 hash 函数，我们通过第一个 $m$ 行来计算一个 minhash 值，通过第二个 $m$ 行计算不同的 minhash 值，依此类推。因此，我们从单个 hash 函数和对 $m$ 的所有行的单次传递中获得 $k/m$ 个 minhash值。事实上，如果 $k/m$ 足够大，我们可以单个 hash 函数通过应用于特征矩阵的每个子集上m$ 的每个行获得整个特征矩阵的签名矩阵。

例如下图中，$k=8,m=4$，$\text{SIM}(S_1, S_2) = 1/2, \text{SIM}(S_1, S_3) = 1/5, \text{SIM}(S_2, S_3) = 1/2$（忽略都是 0 的行）。

![image-20211016173238876](image-20211016173238876.png)

如果只看前 4 行，不管用什么 hash 函数，$S_1$ 和 $S_2$ 的 minhash 值将始终不等，因为 $S_1 \cap S_2 \cap T=\emptyset$，然而只看后四行，则相似度为 $2/3$。所以平均起来是 $(0+2/3)/2=1/3$，与真实的 $1/2$ 有误差，不是太大。在真实场景下，$m$ 远大于 4 行，这种误差将趋于 0。

同理比较 $S_1$ 与 $S_3$ 之间，是 $(0+1/3)/2=1/6$，与真实的 $1/5$ 比较接近，而 $S_2$ 与 $S_3$ 之间完全一致。

### 3.4 LSH

#### 3.4.1 面向最小哈希签名的 LSH

虽然 minhash 可以大幅度压缩特征矩阵，但矩阵的列数并没有变化，那么依然解决不了如何在大量文件之间高效的找到最大相似度的文档。下面介绍局部敏感哈希（LSH）。

LSH 的一个一般性做法就是对目标项进行多次哈希处理，使得相似项会比不相似项更可能哈希到同一桶中。然后将至少有一次哈希到同一桶中的文档对看成是候选对，我们只会检查这些候选对之间的相似度。我们希望大部分不相似的文档对将永远不会哈希到相同的桶中，这样就永远不需要检查它们的相似度。那些哈希到同一个桶中的非相似文档对称为伪正例(false positive) , 我们希望它们在所有对中占的比例越低越好。同时，我们也希望大部分真正相似的文档对会至少被一个哈希函数映射到同一桶中。那些没有映射到相同桶中的真正相似的文档对称为伪反例(false negative) , 我们希望它们在所有真正相似文档对中的比例也很小。

如果拥有目标项的最小哈希签名矩阵，那么一个有效的哈希处理方法是将签名矩阵划分成 $b$ 个行条(band), 每个行条由 $r$ 行组成。对每个行条，存在一个哈希函数能够将行条中的每 $r$ 个整数组成的列向釐（行条中的每一列）映射到某个大数目范围的桶中。可以对所有行条使用相同的哈希函数，但是对每个行条我们都使用一个独立的桶数组，因此即使是不同行条中的相同向量列，它们也不会被哈希到同一桶中。

如下图所示：

![image-20211016175800408](image-20211016175800408.png)

包含 12 行的签名矩阵的一部分，有 4 个行条，每个行条 3 行，图中显式可见的行条1 中第 2 列和第 4 列均包含列向量 [0, 2, 1], 因此它们肯定会哈希到行条l 下的相同桶中。因此，不管这两列在其他 3 个行条下的结果如何，它们都是一个相似候选对。图中显式给出的其他列也有可能会哈希到行条 1 下的同一桶中。但是，由于此时两个列向量 [1, 3, 0J 和 [0, 2, 1] 不同，加上哈希的桶数目也不少，因此偶然冲突的预期概率会非常低。通常我们都假设当且仅当两个向量相等时，它们才会哈希到同一桶中。

在行条1 中不相等的两个列仍然还有另外3次机会成为候选对，只要它们在剩余的 3 个行条中有一次相等即可。然而，我们观察到，如果签名矩阵的两列越相似，那么在多个行条中的向量相等的可能性也越大。因此，直观上看，行条化策略能够使得相似列会比不相似列更有可能成为候选对。

#### 3.4.2 行条化策略分析

假定使用 $b$ 个行条，每个行条由 $r$ 行组成，并假定某对具体文档之间的 Jaccard 相似度为 $s$。则文档的最小哈希签名矩阵中某个具体行中的两个签名相等的概率等于 $s$ 。接下来我们可以计算这些文档（或其签名）作为候选对的概率，具体计算过程如下：

1. 在某个具体行条中所有行的两个签名相等的概率是 $s^r$
2. 在某个具体行条中至少有一对签名不相等的概率是 $1-s^r$
3. 在任何行条中的任意一行的签名对都不相等的概率为 $(1-s^r)^b$
4. 签名至少在一个行条中全部相等的概率，也即成为候选对的概率为 $1-(1-s^r)^b$

虽然有可能并不特别明显，但是不论常数 $b$ 和 $r$ 的取值如何，上述形式的概率函数图像大致为下图中的 S-曲线。

![image-20211016180658409](image-20211016180658409.png)



曲线中候选概率 $1/2$ 处对应的相似度就是所谓的阈值。它是 $b$ 和 $r$ 的函数。阈值对应的大概是上升最陡峭的地方，对于较大的 $b$ 和 $r$, 相似度在阈值之上的对很可能成为候选对，而在阈值之下的对则不太可能成为候选对，这正是我们想要的结果。阈值的一个近似估计是 $(1/b)^{l/r}$。例如，如果 $b=16,r=4$, 那么由于 16  的 4 次方根为 2, 阈值的近似值为 1/2 。

考虑 $b=20, r=5$ 的情况，也就是说假定签名的个数为 100, 分成 20 个行条，每个行条包含 5 行。下表给出了函数 $1-(1-s^5)^{20}$ 的部分值。注意到的是，这里的阈值，也就是曲线中部上升处的 $s$ 值，仅仅比 0.5 稍大一点。另外也注意到，该曲线并非从 0 到 1 在阈值处跳跃的最理想步进函数，但是曲线中部的斜率十分显著。例如， $s$ 从 0.4 变到 0.6, 增加的函数值大于 0.6,  因此中间部分的斜率大于 3 。

![image-20211016181251776](image-20211016181251776.png)



又例如， $s=0.8$ 时， $1-(0.8)^5$ 大约为 0.672。如果再求 20 次方得到大约 0.000 35, 用 1 减去该值以后得 0.999 65 。也就是说，如果认为两篇文档的相似度为 80%, 那么在任意行条中， 5 行中签名对全部相等的可能性，即它们会成为候选对的概率只有约 33%。然而，这里有 20 个行条，因此有 20 次机会成为一个候选对。3000 个对中，大致仅有 1 个相似度为 80% 的对不会成为候选对，即成为伪反例。

#### 3.4.3 上述技术综合

本节将给出一个完整的相似项发现方法：首先找出可能的候选对相似文档集合，然后基于该集合发现真正的相似文档。必须强调的是，这种方法可能会产生伪反例，即某些相似文档对由于没有进入候选对所以最终没有被识别出来。同样，该方法也可能会产生伪正例，即在评估了某些候选对后，发现其相似度不足。

1. 选择某个 $k$, 并对每篇文档构建其 k-shingle 集合。将这些 k-shingle 映射成更短的桶编号（后一步可选）。

2. 将文档-shingle 对按照 shingle 排序，构建特征矩阵。
3. 选择最小哈希签名的长度 $n$ 。计算所有文档的最小哈希签名。
4. 选择阈值 $t$ 来定义应该达到的相似程度使之被看做是预期的”相似对“。选择行条数 $b$ 和每个行条中的行数 $r$, 使得 $br=n$, 而阈值 $t$ 近似等于 $(1/b)^{1/r}$。如果避免伪反例的产生很重要，那么选择合适的 $b$ 和 $r$ 以产生小于 $t$ 的阈值。而如果速度相当重要并且希望限制伪正例的数目，那么选择合适的 $b$和 $r$ 来获得更高的阈值。
5. 应用 LSH 技术来构建候选对。
6. 检查每个候选对的签名，确定它们一致性的比例是否大于 $t$。
7.  (该步可选）如果签名足够相似，则直接检查文档本身看它们是否真正相似。不相似的文档有时碰巧会具有相似的签名。

