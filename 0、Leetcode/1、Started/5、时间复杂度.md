
在学习算法和数据结构时，你会经常遇到“时间复杂度”这个术语。这个概念是计算机科学的基础，它能帮助我们了解算法在给定输入规模下完成所需的时间。

### What

时间复杂度表示算法运行所需的时间与输入长度之间的函数关系。它提供了运行时间的上限，帮助我们理解性能方面的最坏情况。


### Why

假设你有两种对数字列表进行排序的方法。虽然两者都能给出正确答案，但一种方法对 1000 个数字的列表需要一分钟，而另一种只需一秒钟。自然，你会选择更快的那种。时间复杂度帮助我们预测这些差异，而无需用实际的大规模输入来运行算法。

关于时间复杂度分析，关键在于我们通常不考虑常数项。我们更关注的是找出最能匹配计算时间增长趋势的"函数族"。例如，`O(2n)` 和 `O(5n)` 可以粗略地归为 `O(n)` 这个函数族。

这是描述算法时间复杂度的标准表示法。当我们说一个算法的时间复杂度为 `O(n)` 时，意味着在最坏情况下，算法的运行时间与输入规模呈线性增长关系。

假设我们有一个程序，使用单个 for 循环打印从 `1` 到 `n` 的所有整数。该程序的时间复杂度为 `O(n)`。即使程序打印从 `-100` 到 `5 * n` 的所有整数，时间复杂度也不会改变，因为运行时间仍然随着 `n` 线性增长。

在某些特定情况下，人们可能希望优化算法以获得更好的常数项。然而在大多数情况下，这是不必要的，基本的运行时分析就可以表明解决方案的效率。

一般来说，编程挑战平台通常允许你的解决方案最多执行 1000 万到 2000 万次操作。利用这一信息，我们可以从问题描述中的约束条件推断出来。

以下是常见时间复杂度的更详细分类、对应的最大输入规模以及实现这些复杂度所需的常见算法：

## `O(1)`

恒定时间复杂度。可能是

- 哈希映射查找
- 数组访问和更新
- 从栈中压入和弹出元素
- 查找并应用数学公式
- 通常适用于 `n > 10⁹`

以下代码的时间复杂度为 `O(1)`，因为它执行的操作数量是固定的：

```python
a = 5
b = 7
c = 4
d = a + b + c + 153
```

## `O(log(N))`

对数函数 `log(N)` 的增长速度极其缓慢。`log(1,000,000)` 的值仅为 20 左右。事实上，在关系型数据库中通过主键查询的时间复杂度就是 `log(N)`（许多主流关系型数据库如 PostgreSQL 默认使用 B 树作为索引结构，而在 B 树中查找的时间复杂度正是 `log(N)）`。

在编程面试中，`log(N)` 通常表示

* 二分查找或其变体
* 平衡二叉搜索树查找
* 处理数字的各位数
* 通常适用于 `n > 10⁸`

注：除非另有说明，我们默认 `log(N)` 指的是以 2 为底的对数，即 `log₂(N)`。

以下代码的时间复杂度为 `O(log(N))`，因为每次迭代都将 `N` 减半，所以工作量是对数级的：


```python
N = 100000000
while N > 0:
  # some constant operation
  N //= 2
```

## `O(N)`

线性时间通常意味着对线性数据结构进行常数次数的循环遍历。最常见的情况是

* 遍历数组/链表
* 双指针
* 某些类型的贪心算法
* 树/图遍历
* 栈/队列
* 通常适用于 `n ≤ 10⁶`

以下示例均为 `O(N)`：

```python
for i in range(1, N + 1):
  # constant time code

i = 0
while i < N:
  # constant time code
  i += 1
```

因为我们同样忽略常数因子和低阶项，以下也是 `O(N)`：

```python
for i in range(1, 5 * N + 17):
  # constant time code

for i in range(1, N + 538238):
  # constant time code
```

## `O(K log(N))`

* 堆的 `K` 次推入/弹出操作。当遇到需要找"前 `K` 个元素"的问题时，通常可以对堆进行 `K` 次推入和弹出操作来解决，时间复杂度为 $O(K \log(N))$。如：[K closest points](https://algo.monster/problems/k_closest_points), [merge K sorted lists](https://algo.monster/problems/merge_k_sorted_lists)。
- 二分查找 K 次。
- 通常适用于 `n ≤ 10⁶`。

## `O(N log(N))`

* 排序。所有主流语言中默认排序算法的预期运行时间均为 `N log(N)`。例如，Java 在对象排序时使用归并排序的变体，而对基本类型排序时采用快速排序的变体。
* 采用线性时间合并操作的分治策略。分解步骤通常为 `log(N)`，若合并操作为 `O(N)`，则整体运行时间为 `O(N log(N))`。典型问题是 [smaller numbers to the right](https://algo.monster/problems/count_of_smaller_numbers_after_self).
* 通常适用于 `n ≤ 10⁶` 的情况

```python
N = int(input())
ar = []
for i in range(N):
  m = int(input())
  ar.append(m)
ar.sort() # nlogn
```

## `O(N^2)`

也称为二次时间。

- 嵌套循环，例如访问每个矩阵条目
- 许多暴力破解解决方案
- 通常适用于 `n ≤ 3000`

对于较小的 `N < 1000`，这对现代计算机来说并不算太糟糕。对于非常小的 $N$ 值，你可能可以通过大多数 Leetcode 测试，即使使用二次时间复杂度。然而，在面试中，如果存在更优解，你的解决方案通常需要优于二次时间复杂度才能给面试官留下深刻印象。

这个例子是 `O(N^2)` 的，因为外层循环运行 `O(N)` 次迭代，内层循环也运行 `O(N)` 次：

```python
for i in range(1, N + 1):
  for j in range(1, N + 1):
    # constant time code
```

在这个例子中，外层循环运行了 `O(N)` 次迭代，而内层循环的迭代次数则在 1 到 N 之间变化。由于大 O 符号计算的是最坏情况下的时间复杂度，我们将内层循环视为 N 的一个因子。因此，这段代码的时间复杂度是 `O(N^2)`。

```python
for a in range(1, N + 1):
  for j in range(a, N + 1):
    # constant time code
```

## `O(2^N)`

生长速度极快。通常需要采用记忆化技术来避免重复计算并降低复杂度。

- 组合问题，回溯法，例如子集问题
- 通常涉及递归，且初次分析时间复杂度较难
- 更多详细代码示例可在回溯法部分找到
- 通常适用于 `n ≤ 20` 的情况

递归实现的斐波那契算法时间复杂度为 `O(2^N)`，因为对于任意 `Fib(i)`（其中 `i>1`），我们都需要调用`Fib(i-1)` 和 `Fib(i-2)`。

```python
def Fib(n):
  if n == 0 or n == 1:
    return 1
  return Fib(n - 1) + Fib(n - 2)
```

## `O(N!)`

增长速度极其迅猛。对于较小的 `N` 值，只能通过计算机求解。通常需要使用记忆化技术来避免重复计算并降低复杂度。

- 组合问题，回溯法，例如排列问题
- 通常涉及递归，且初看时时间复杂度较难分析
- 详细代码示例可参见回溯法部分
- 一般适用于 `n ≤ 12` 的情况

## 摊销时间复杂度

摊销时间的概念是指偶尔执行一项非常耗时的任务。这项耗时任务执行得如此之少，以至于其成本被稀释。例如，如果我们有 `N` 个 `O(1)` 的任务，但只有一个 `O(N)` 的任务，当 `N` 足够大时，我们仍然可以将总成本视为 `O(N)`，而不是 `O(N^2)`。

关键要点在于，摊销时间是每个操作的平均耗时。以下是动态大小数组的 `O(1)` 追加函数的实现：


```python
size = None # number of elements in array
capacity = None # maximum number of elements array can store
arr = []
...
def append(x):
  if size == capacity:
    new_arr = [None] * (2 * capacity)
    capacity *= 2
    for i in range(size):
      new_arr[i] = arr[i]
    arr = new_arr
  arr[size] = x
  size += 1
```

