
从基本原理出发，全面探索 Flash Attention，这不仅意味着我们要编写 Flash Attention 的代码，还要从理论上推导出它的实现过程，因此，我们假设 Flash Attention 这篇论文从未存在过，而是直接审视注意力计算本身及其存在的问题，并一步步尝试解决这些问题，通过这种方式，我们不仅能深入理解其工作原理，还能将理论与实践紧密结合，实践部分则通过编写代码来实现。为了编写 Flash Attention 的代码，我们需要为 GPU 编写一个内核程序，具体来说，是针对我们的需求定制一个内核，不过我们不会直接编写 C++ 代码，而是使用 Triton，它能够将 Python 代码直接转换为可在 GPU 上运行的 CUDA 内核程序，可以把 Triton 看作是一个编译器，它接收 Python 代码并将其转换为能在 GPU 上运行的程序。

本篇文章要讨论的主题包括：

* 多头注意力机制（MHA）
* safe softmax
* online softmax,
* 然后，我们将深入了解 GPU，因为我们要编写一个在 GPU 上运行的内核程序，因此，我们需要理解 CPU 和 GPU 之间的区别，什么事内核程序，以及它与 CPU 编写的普通程序有何不同，
* 我们将研究张量在内存中的布局方式，比如行优先布局、列优先布局，步幅等，
* 我们将探讨分块矩阵乘法，
* Triton 的软件流水线，以及 Triton 对我们代码所做的所有优化
* 最后，我们将能够编写 Flash Attention 的前向传播代码，当然，仅仅编写前向传播代码并不能让我们满足，
* 我们还希望编写反向传播代码，但要编写反向传播代码，我们还需要理解在自定义操作的情况下，自动微分 autograd 和梯度下降是如何工作的，
* 因此，我们需要理解什么是导数、梯度和雅可比矩阵，
* 然后计算我们在 Flash Attention 中使用的常见操作的梯度，
* 最终，我们将掌握足够的知识来编写反向传播代码，

### 0、先决条件

* 高中数学基础（导数）
* 线性代数的基础知识（矩阵乘法、矩阵转置等）
* 注意力机制
* 很多耐心

我们会从基本原理出发，从头开始推导一切。

### 1、多头注意力机制

快速回顾一下 MHA 是什么以及它是如何工作的，公式如下：$$\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}$$
实际上，Flash Attention 主要关注的是 Attention 部分的优化，$Q,K,V$ 的线性层投影以及输出层投影都是常规的矩阵相乘，这一点在 GPU 中已经高度优化。


### 2、缩放点积注意力机制的局限性

#### 2.1 问题一、GPU I/O 限制

一个非常关键的问题是：我们为何需要改进注意力机制的实现方式，如果你查阅 [Flash Attention 1 论文](https://arxiv.org/pdf/2205.14135)，会注意到章节 2.2：

![[Pasted image 20250318151046.png|650]]

GPU 主要由两种内存构成，一种是 HBM，即动态随机存取存储器（DRAM），也就是 GPU 的内存，例如 A100 的 40GB 内存，这是 GPU 中容量最大的内存；此外还存在共享内存。

GPU 面临的问题是，访问 HBM（全局内存）与访问共享内存相比，速度极其缓慢，然而，与 HBM 相比，共享内存的容量要小得多，FlashAttention 论文中指出，*注意力机制的操作是 I/O 受限的*，这意味着，如果我们频繁访问全局内存，那么计算注意力机制的整体操作速度慢，这并不是因为计算这些操作本身慢，而是因为频繁访问速度较慢的全局内存导致的，因此我们可以将这类操作称为 I/O 受限型操作。

因此，改善这一状况的唯一方法是，在 GPU 的共享内存中计算注意力机制，尽管共享内存的容量要小得多，因为共享内存更靠近实际执行计算的 kernel，因此，我们需要将注意力计算拆分为更小的块，以便这些块能够放入共享内存中，然后在那里在那里计算输出矩阵的一部分，再将这部分复制到位于 HBM 中的输出矩阵中，并针对查询、键、和值矩阵划分的所有块，重复这一过程。

在论文中，他们称之为“分块（tiling）”，这是一种在编写 GPU 内核时常用的技术，尤其是在涉及矩阵乘法的情况下。现在我们了解了 FlashAttention 试图解决的核心问题。

#### 2.2 问题二、softmax

这种分块计算的最大难题在于 softmax，因为 softmax 需要访问整个 $S$ 矩阵的所有元素才能完成计算，因为需要计算归一化因子，这个因子是对所有元素逐行计算指数后的总和。


### 3、（Safe）Softmax

#### 3.1 softmax 计算上的问题

$$
\mathbf{S} = \mathbf{QK}^\top \in \mathbb{R}^{N \times N}, \quad \mathbf{P} = \text{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d},$$
这里的 $QKV$ 都是经过相应线性层转换后的矩阵，$Q,K$ 的维度均为 $N \times d$，点积运算后，其输出矩阵 $S$ 的维度为 $N \times N$，softmax 操作按行处理，并不改变矩阵维度，其结果最后与 $V$ 相乘，输出维度为 $N \times d$。

softmax 操作的作用是什么呢？它会将这些点积结果进行转换，使得它们以某种方式变为一种概率分布，*按行计算*，这意味着每个数字都介于 0 到 1 之间，softmax 的定义如下：$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}$$分母是向量所有维度指数的总和，这被称为归一化因子，为的是使所有这些数字介于 0 和 1 之间，使用 softmax 是因为我们希望这些数字都是正数（概率值），这是使用指数函数的原因，

但这里*存在一个问题*，问题在于，想象一下我们的输入向量由许多可能很大的数字组成，比如 100 的指数，会造成计算机结果上溢，即数值不稳定性，在计算机科学中，“数值不稳定性”意味着数字无法用我们现有的位数（通常是 32 位或 16 位）在固定表示形式中表示出来。

#### 3.2 解决方案

为了使这个 softmax 操作在数值上保持稳定，我们希望这些数字不会爆炸或变得太小以至于无法表示，我们需要找到一种解决方案，如下：$$\begin{align}\frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}} 
&= \frac{c \cdot e^{x_i}}{c \cdot \sum_{j=1}^{N} e^{x_j}} = \frac{c e^{x_i}}{\sum_{j=1}^{N} c e^{x_j}} = \frac{e^{\log(c)} e^{x_i}}{\sum_{j=1}^{N} e^{\log(c)} e^{x_j}} \\[1.2ex]
&= \frac{e^{x_i + \log(c)}}{\sum_{j=1}^{N} e^{x_j + \log(c)}} = \frac{e^{x_i - k}}{\sum_{j=1}^{N} e^{x_j - k}} \quad \text{where } k = -\log(c)\end{align}$$
用因子 c 乘以分子和分母，通过上面推导过程，我们可以看到，如果我们巧妙地选择一个值插入到这个指数函数中，就能有效减少指数部分的计算量，我们将选择这个 $k$ 值等于输入向量中需要应用 softmax 的最大元素（$k=\max_i(x_i)$），这样一来，每个指数的参数要么为 0（当 $x_i$ 等于向量中的最大值时），要么小于 0，其指数结果介于 0 到 1 之间，这样的数值用 32 位浮点数就能轻松表示。

#### 3.3 safe softmax 算法

$$\text{softmax}(x_i) = \frac{e^{x_i - x_{\text{max}}}}{\sum_{j=1}^{N} e^{x_j - x_{\text{max}}}}$$
给定一个 $N \times N$ 的矩阵，对于每一行：

1. 寻找每一行的最大值
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
2. 计算分母归一化因子
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
3. 对向量中的每一个元素应用 softmax
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$

伪代码如下：
```python
m_0 = -infty
for i=1 to N
    m_i = max(m_{i-1}, x_i)

l_0 = 0
for J=1 to N
    l_J = l_{J-1} + e^{x_J - m_N}

for K=1 to N
    x_K <- e^{x_K-m_N} / l_N
```

这段伪代码描述的算法相当慢，显而易见，这里存在 3 个 for 循环。所以接下来优化的思路就是寻找一种策略，合并其中的某些操作，减少循环次数。

### 4、Online Softmax

#### 4.1 online softmax

我们尝试将前两个操作融合到一个 for 循环中，这意味着我们只需要遍历数组一次，同时计算 $m_i$，并尝试计算 $l_j$，当然，我们无法在此刻计算 $l_j$，因为无法得知全局最大值，但我们可以尝试使用当前已知的局部最大值作为估算值来进行计算，即我们尝试用 $m_i$ 替代 $m_n$。

当后续迭代过程中发现更大值时，需要对过去计算项进行修正，实际上这个校正因子非常容易计算，以 $x=[3,2,5,1]$ 为例，在前两轮中最大值为 3，第三次迭代时，最大值为 5，即在第三轮迭代中，

* 错误迭代计算：$l_3 = l_2 + e^{5-5}=e^{3-3}+e^{2-3}+e^{5-5}$
* 正确修正方法：$l_3 = l^2 \cdot \textcolor{blue}{e^{3-5}} + e^{5-5}=(e^{3-3}+e^{2-3})\textcolor{blue}{e^{3-5}}+e^{5-5}$

显然这个修正因子的计算方法为过去的最大值与当前新的最大值之间的差。

因此，softmax 新算法如下：
```python
m_0 = -infty
l_0 = 0
for i=1 to N
    m_i = max(m_{i-1}, x_i)
    l_i = l_{i-1}*e^{m_{i-1} - m_i} + e^{x_i - m_i}

for K=1 to N
    x_K <- e^{x_K-m_N} / l_N
```

#### 4.2 数学归纳法证明


1. 证明对于大小为 $N=1$ 的向量，该命题成立：$$\begin{align}
m_1 &= \max(-\infty, x_1) = x_1 = \max_i(x_i) = x_{\max} \\[1.2ex]
l_1 &= 0 \times e^{-\infty} + e^{x_1 - x_1} = \sum_{j=1}^{N} e^{x_j - x_{\max}}\end{align}$$
2. 如果假设该命题对大小为 $N$ 的向量成立，证明它对大小为 $N+1$ 的向量也成立$$\begin{align}
m_{N+1} &= \max(m_N, x_{N+1}) = \max_i(x_i) \\[1.2ex]
l_{N+1} &= l_N \cdot e^{m_N - m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \left(\sum_{j=1}^{N} e^{x_j - m_N}\right)e^{m_N-m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \sum_{j=1}^{N} e^{x_j - m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \sum_{j=1}^{N+1} e^{x_j - m_{N+1}} \end{align}$$
### 5、分块矩阵乘法

![[Pasted image 20250319151749.png|500]]


现在，让我们来看看为什么这一点对我们来说非常重要，那么我们为什么要关注矩阵乘法呢？因为我们正试图计算以下操作，具体来说，就是将查询矩阵与键矩阵的转置相乘，然后对这个操作的结果应用 softmax 函数，最后将 softmax 的输出与值矩阵相乘，

目前我们先暂时忽略 softmax 的部分，假设我们不打算应用任何 softmax 函数，因此，我们取查询矩阵与键矩阵转置相乘的结果，直接将其与值矩阵 V 相乘，得到注意力机制的输出，当然，这种做法是不正确的，但它简化了我们接下来要处理的内容，

所以就目前而言，我们暂且假设不需要应用任务 softmax 函数，于是，我们只需要将查询矩阵与键矩阵的转置相乘，然后直接将这个操作的结果与值矩阵 V 相乘，这样得到的结果将是一个 nxd 的矩阵，其中 n 代表 token 的数量，每个 token 由一个 d 维（小写 d 表示维度）的嵌入向量构成，我们知道 q,k,v 本身也是 nxd 维的矩阵，也就是说，这些 n 个 token 是由 d 维的嵌入向量组成的，想象一下，我们有一个 8x128的查询矩阵，以及同样大小的键矩阵和值矩阵，这意味着我们有 8 个 token，每个 token 由 128 个维度组成，正如我们所看到的，在进行矩阵乘法计算时，我们可以将矩阵进行分块处理，我们可以将矩阵划分成多个块，如何选择分块方式完全取决于我们，只要它们能够正常运算即可，在进行矩阵乘法时，确保分块的形状能够相互匹配即可。例如，在前面的例子中，我们将矩阵 A 划分为块，使得块矩阵（即仅由这些块组成的矩阵）的形状与块矩阵 B 兼容，从而确保这种操作能够顺利进行，因此，这是我们在进行块矩阵乘法时唯一需要注意的要求，块矩阵（即仅由块组成的矩阵）的形状在矩阵乘法中应相互匹配，除此之外，具体如何划分并不重要，想象一下，我们选择将查询矩阵按行划分为块，这是完全可以做的，我们并不一定也需要按列进行划分，我们可以仅按行划分，使得每个 Q 不再是单行，而是由两行组成的一个组，因此，Q1 是 Q 序列中前两行组成的组，Q2 则是 Q 序列中接下来的两行组成的组，以此类推，我们对 V 也采取同样的划分方式，对于 K，我们不做这样的划分，因为实际上我们要与 K 的转置相乘，所以直接在 K 的转置上进行这种细分，因此，我们有被划分为若干行组的 Q，以及 K 的转置，这是一个 128 行 8 列的矩阵，因为它是 8 行 128 列的键矩阵的转置，我们决定将 K 转置矩阵的每一列组划分为一个单独的块，因此 k1 是 K 转置矩阵的前两列，k2 则是 K 转置矩阵中接下来的两列组成的组，以此类推，直到 k4，即 k 转置矩阵中最后两列，我们首先进行的操作是查询，与键转置的乘法，这基本上意味着我们需要将每个查询与所有键相乘，接着是第二个查询与所有键相乘，以此类推。

现在，每个查询并不是 Q 序列中的单一行，它是由 Q 序列 中的两行组成的一个组，而每个 k 也不是 k 转置中的单一列，它是由 k 转置中两列构成的一个组，但这并不重要，因为我们已经看到矩阵乘法：如果将矩阵视为由块组成，我们只需按照常规矩阵乘法的方式进行计算即可，所以我们正在将这个矩阵乘以那个矩阵，就我们所知，这个矩阵由四行组成，每行有 128 个维度，而那个矩阵则由多少行组成呢？128行，4列，我没画那些列，因为太多了无法再这里展示，但你需要想象每个向量都有很多维度，每个向量有 128 个维度，这里你需要想象它有 128 行，当我们进行矩阵乘法时，我们采用常规的矩阵乘法步骤，即每个输出元素，因此，首先，这个矩阵乘法的输出形状将是 4 乘 4，因为这是两个矩阵相乘时的外维度，输出的第一个元素将是这个向量与那个向量的点积，第二个元素，也就是这个位置，将是这个向量与那个向量的点积，然而，这并不是一个向量，那个也不是一个向量，所以实际上这是矩阵乘法，在这种情况下，这里的这个元素不是一个标量，它是一组输出矩阵的元素，因为我们正在进行块矩阵乘法，以及它将包含多少个元素，我们知道，原始的 q1 是一个 2 乘 128 的矩阵，k1 是一个 128 乘 2 的矩阵，因此，这将是输出矩阵中一组 2 乘 2 的元素，我们正在进行 q1 与 k1 的矩阵乘法，然后是 q1 与 k2，接着是 q1 与 k3，q1 与 k4，以此类推，这是第一行的计算，接着，第二行将是 q2 与所有 k 的乘法，然后是 q3 与所有 k 的乘法，以及 q4 与所有 k 的乘法，

正如你所见，当我们进行矩阵乘法时，我们并不关心底层是块、向量还是标量，我们只需遵循相同的步骤即可，首先，将黑色块矩阵的第一行与第二个矩阵的第一列相乘，然后将第一行与第二列相乘，接着是第一行与第三列相乘，以此类推，接下来，按照公式的要求，我们需要将查询和键的转置相乘，然后再与值相乘，这些都是块矩阵，现在，正如你从我使用的颜色中可以看到的，每当我提到原始矩阵时，我是用蓝色，每当我提到块矩阵时，我使用粉色，因此，我们需要先将查询与键的转置相乘，然后将结果与值相乘，这里我们暂时跳过了 softmax 操作，稍后会解释其中的原因，如果我们要进行这个乘法运算，需要按照以下步骤操作。

因此，这个矩阵由多个块组成，而块矩阵乘法会忽略这一事实，直接将其视为普通矩阵进行乘法运算，首先，我们先将第一行与第一列相乘，然后将第一行与第二列相乘，接着是第一行与第三列相乘，以此类推，等等，那么，在这个矩阵乘法的输出矩阵中，第一行的第一个块是如何计算的呢？嗯，它将由第一行的点积计算得出点积，虽然严格来说这并不是真正的点积，实际上，这是第一行的矩阵乘法，但以类似于点积的方式进行，具体来说，是与由 v1、v2、v3 和 v4 组成的第一列相乘，因此，这个元素与 v1 相乘，加上这个元素与 v2 相乘，再加上这个元素与 v3 相乘，最后加上这个元素与 v4 相乘，结果就是第一个输出元素，第二个输出块将是这一行与这一列的乘积，具体来说，这个元素与 v1 相乘，加上这个元素与 v2 相乘，再加上这个元素与 v3 相乘，最后加上这个元素与 v4 相乘，结果就是第二个输出块，以此类推，等等，同样的，第三个和第四个块的输出也是如此，

我们来看看每个块是由什么组成的，每个块由第一个元素组成，即 q1 与 k1 相乘的结果，因为这是 query 与 keys 的乘积，再与第二个矩阵的 v1 相乘得到的，再加上这个元素与这个元素相乘，再加上这个元素与这个元素相乘，最后加上这个元素与这个元素相乘，生成这个输出的伪代码如下（虽然这不不完全是注意力机制，因为我们跳过了 softmax 步骤，但我想让你们习惯以块的思维方式来思考）：我们取每个 query 块，依次处理每个 query，实际上，我们可以看看这个输出是由什么组成的，它由 query 1 与 key 1 相乘的结果再与 v1 相乘组成，然后是 query1 与 k2 相乘的结果与 v2 相乘，接着是 query1 与 k3 相乘的结果与 v3 相乘，最后是 query1 与 k4 相乘的结果与 v4 相乘，这基本上就是我们正在做的事情，计算由块组成的这一行与这一列的点积，因此，生成第一行的伪代码是：首先取 query1，然后依次遍历从 1 到 4 的 key 和 value，并逐步累加结果，因此，对于每个块，基本上是为了生成这个输出矩阵，对于每一行，我们会看到它是不同的 query 与所有 key 和 value 的组合，然后这是 query3 与所有 key 和 value 的组合，这是 query4 与所有 key 和 value 的组合，为了生成这个输出矩阵，我们需要做以下操作：遍历所有的 query，每一行对应输出矩阵的一行，然后对当前遍历的 query i 与第 j 个 key 和 value 进行迭代求和，逐步累加结果，最终生成输出矩阵。

你可以在这里看到这个过程，我知道目前为止我所做的内容对 Flash Attention 来说并不直接有用，但它对我们理解如何以块的方式计算这种乘积非常有帮助，因为后续我们还会结合 softmax 来使用这种方法，好的，我知道我们目前计算的内容并不是真正的 softmax 操作，也不是完整的注意力机制，因为我们跳过了 softmax 步骤，因此，我们需要以某种方式将其恢复进来，接下来的几分钟，我想大概 10 到 20 分钟，将会非常具有挑战性，因为我将进行大量操作，涉及许多不同的块、多种指标、乘法以及 softmax 的各种变体，所以可能会有些难以跟上，不过，请不要放弃，你可以把这一部分多看几遍，每次观看都会有更深的理解，我建议你先看到我们讲解完 Flash Attention 算法，然后再回过头来重新观看这部分内容，因为当你看到 Flash Attention 算法的讲解时，会让你对之前的内容有更好的理解，然后再重新观看这部分内容，可以进一步加深你的理解，另外，我建议你拿起笔和纸，把看到的操作步骤和每个块的形状都写下来，尤其是这些参与矩阵乘法的元素的形状，这样能帮助你更好的理解正在发生的事情，也能在我提到某个特定元素或块时更容易记住，好的，在说完这段小小的激励之后，我们正是开始吧

到目前为止，我们所做的是将 query 与 键的转置相乘，不过，这里的每个查询并不是查询序列中的单一行，而是一个查询块，它是一个由多行组成的块，在我们这个具体例子中，q1 并不是查询序列中的单一行，而是两行，因为我们选择的块大小是两行为一组，同样 K transpose one 并不是 k 转置矩阵中的单列，而是两列，因为我们是这样选择的，如果你记不清了，我们可以回头再看一下，这里我们选择 k one 是两列，而 Q one 是原始查询矩阵的两列，每当我使用蓝色时，指的是原始形状，而每当我使用粉色或紫色时，无论哪种颜色，我指的则是块矩阵，所以，它是原始矩阵中元素的块，好的，现在我们做的第一件事是将查询与键的转置相乘，这会生成一个我们称之为 S 的块矩阵作为输出，其中每个元素 Sij，也就是这个矩阵的 S11 元素，将是查询一与k转置1的乘积，S12 将是查询一与 k 转置二的乘积，S13 则是查询一与 k 转置三的乘积，以此类推，适用于所有行和所有列，

接下来，我们应该应用 softmax，因为如果你还记得公式，它是查询与键转置相乘后的 softmax，然而，我想恢复 softmax 操作，但稍作调整，这意味着，我们将应用简化版的 softmax，并称之为 softmax星号，它只是去除了归一化的 softmax，那么，让我为你写下它的含义，让我们用我选择表示 softmax 的颜色，橙色来书写，所以，如果你还记得，如果我们还记得，它就是 softmax 星号对于一个向量的两个元素，我们逐个元素的应用它，因此输出的 l 向量中第 i 个元素，我们对其应用 softmax 等于输入向量中第 i 个元素的指数，减去输入向量中的最大值，再除以一个归一化因子。这个归一化因子是根据以下求和公式计算得出的，即从 j 等于 1 到 n 的指数求和，即 xi 减去 x_max 的指数，所以基本上我们是对每个元素减去 x_max 后取指数， 如果你记得清楚的话，为什么要减去这个 x_max 呢？这是为了让指数计算在数值上稳定且可计算，否则结果可能会爆炸式增长，因为我们将其应用与于分子，素以也需要咱分母上进行同样的处理，好的，softmax* 操作与 softmax 完全相同，但去掉了归一化部分，也就是说它只保留了 softmax 的分子部分，因此我们将根据这个公式修改应用 softmax* 的向量中的每个元素

让我把它调整的更对齐一些，像这样，所以我们只需要进行逐元素操作，即对每个元素取指数后，减去应用 softmax* 的向量的最大值，好的，那么我为什么要引入这个 softmax* 操作呢？因为我们将把它应用到目前计算得到的矩阵上，也就是这个 S 矩阵，因此，我们对 S 矩阵中的每个元素都应用了 softmax*，但由于 S 矩阵是块矩阵，它的每个元素本身也是一个矩阵，而这个 S 矩阵的每个元素，举个例子，元素 S11 是一个 2x2 矩阵，因为它来自两个矩阵的乘积，这两个矩阵分别时从 Q 和 K 中提取的一组行和一组列，那么，举个例子，这个 S11 是什么呢？让我们实际画出来看看吧，比如这个 S11 将由四个元素组成，我们暂且称它为 S11 的 A 部分吧，让我们选一个更好的命名方式，比如就叫 A、B、C 和 D 吧，这些都是通用的元素名称，当我们对 S11 应用 softmax* 时，它会生成相应的结果，那么，让我们来应用 softmax* 吧。

softmax* 会生成一个矩阵，其中每个元素都是原矩阵中对应元素的指数值减去该行的最大值，现在我们并不知道哪个是最大值，所以先随便选一个吧，假设这一行的最大值是 A，而这一行的最大值是 D，对块 S11 应用 softmax* 后，输出的第一个元素将是其指数值，也就是 a 减去 a 的指数值，因为我们选择 a 作为这一行的最大值，第二个元素则是 b 减去 a 的指数值，因为 a 是这一行的最大值，在底行中，第一个元素是 c 减去 d 的指数值，因为 d 是底行的最大值；第二个元素则是  d 减去 d 的指数值，这就是它的指数计算结果，这就是 softmax* 如何修改这个块矩阵中每个块的方式，让我把这些内容删掉，不然他们会一直留在我的幻灯片里，之后我想把幻灯片分享给你们，这样你们就可以用同样的内容了，所以删掉，删掉，再删掉，

好的，当我们对 S 矩阵中的每个元素应用了 softmax 后，我们会将其称为 P 矩阵，其中的每个元素，比如 p11，仍然是一个 2x2 的块矩阵，因此，p11 就是经过 softmax* 处理后的结果，应用到 s11 上，这里的 s11 是什么呢，就是查询 1 乘上 k 的转置，而 p12 则是 softmax* 应用到 s12 上的结果，这里的 s12 是什么呢，也是查询1 乘上 k 的转置 2，以此类推，等等，对于 s 的所有元素，好的，现在我们已经应用了这个 softmax* 操作，接下来按照注意力机制的公式，我们应该做的操作是：

先对查询与键的转置相乘结果进行 softmax 计算，然后将 softmax 的结果与值 v 相乘，我知道我们用的不是真正的 softmax，而是 softmax*，也就是没有归一化的 softmax，稍后我们会看到如何弥补这个缺失的归一化步骤，因为我们可以把它放到最后再做，这是完全可行的，

好的，那么我们现在取这个 P 矩阵，它是 softmax* 作用于 S 矩阵的结果，然后我们将它与 V 相乘，我们该怎么做呢，嗯，它是一个由矩阵块组成的块矩阵，所以 p11 实际上不是一个标量，而是一个 2x2 的矩阵，我们需要用它来乘以 V，但我们不是用原始的序列 v 来相乘，而是用分块后的序列 v，就像之前一样，这里的每个 v 不是 v 的一行，而是 v 的一组行，它由多少行呢，它是 v 的两行，呃，现在请完全忽略我在这里写的任何内容，因为我稍后会用到它，所以我们需要计算这个由块组成的矩阵的乘积，记住，这个由块组成的矩阵实际上有四组行，这里的每一行并不是真正的一行，而是一组行块，而这个矩阵由 4x4 个元素组成，其中每个元素实际上并不是标量，而是一个矩阵，所以，正如你所记得的，在块矩阵乘法中，计算矩阵乘法的算法与普通矩阵乘法相同，只不过我们使用的是块，所以我们要做的操作是这样的，

让我们把它写下来吧，假设 O 等于 P 乘以 V，明白吗，因此，第一个输出行（由于它实际上不是一个行，而是一个块行），将按如下方式计算，这个块矩阵的第一行与 V 矩阵的第一列相乘，我们将其视为块矩阵，因此它将等于 p11 乘以 v1 加上 p12 乘以 v2 加上 p13 乘以 v3 再加上 p14 乘以 v4，这将生成 O 的第一个输出行，但它实际上并不是一个行，因为它由两行组成，所以这里的输出并不是一行，而是两行，我们可以证明这一点，因为 p11 是什么，p11 是，让我们吧它写在这里，所以 p11 是一个 2x2 的矩阵，是的，2x2，然后我们将其与 v1 相乘，v1 是 v 的一个由两行组成的块，所以它是两行，每行有 128 个维度，因此，它的尺寸是 2x128，所以这里的这个部分是 2x128，所以这里的这个块，也就是你正在计算的输出块，是我们正在计算的输出矩阵中的一个由两行组成的块，

我知道这实际上很难跟上，因为我们涉及到了块的概念，所以我们需要同时将矩阵想象成块的形式和原本的矩阵形式，所以我强烈建议你暂停视频，仔细思考，写下你需要记录的内容，因为仅仅靠记忆这些形状很难跟上的，你确实需要动笔写下来，总之，我们正在计算 O 矩阵的第一个输出块，现在，如果你还记得，这里的输出应该是 softmax 的结果乘以 V，现在，这个 softmax 还没有应用到 S 矩阵的整行上，基本上是为了计算这个 softmax 星号，我们之前所做的，是独立于其他块来计算每个块的 softmax 星号，这意味着我们用来计算每个 softmax 星号的最大值，并不是 S 矩阵这一行的全局最大值，而是每个块的局部最大值，这实际上是错误的，因为当我们计算 softmax 时，我们应用的是全局 softmax，我们应该使用整行的全局值，我想给你举个例子，不使用分块的方式，否则可能不太容易理解，当我们进行常规的注意力计算时，我们会有一个查询与键的转置相乘，这会生成一个 nxn 的矩阵，也就是序列长度乘以序列长度，这个矩阵中的每个元素，假设是 3，4，5，我也不确定具体有多少，好的，这里的这个值应该是第一个查询与第一个键的点积，让我用公式来说明，因为这是查询1的转置乘以键1，这是因为，正如我之前提到的，当我们计算两个向量的乘积时，我们总是将它们视为列向量，所以当你想要表示点积时，你不能直接对两个列向量进行相乘，你需要用一个行向量与一个列向量相乘，这就是为什么我们要对其中一个向量进行转置，如果这让你感到困惑，你也可以直接写成 q1 和 k1，这完全没有问题，只是从数学符号的角度来看，这样写不太规范，总之，第一个值将是查询向量与 .. 的点积，第二个元素将是查询向量与 K2 的点积，第三个元素是查询向量与 k3 的点积，以此类推，因此，这个结果是 q1 与 k1，q1 与 k2，q1 与 k3，q1 与 k4 的点积，无论如何，当我们计算 softmax 时，实际上是在这一整行中求最大值，

然而，我们实际在做的是块矩阵乘法，正如你所记得的，当我们分块计算时，我们会将查询的行和键的行分组处理，在这个特定的例子中，我们将两个查询想象归为一组，形成一个查询块，同时将两个键向量归为一组，形成一个键块，因此，我们需要这个块的另一行数据，所以它是，让我选择，查询1与键1，以及查询2与键1，这里应该是查询2与键1，查询2与键2，查询2与键3，查询2与键4，查询2与键5以及查询2与键6，这里的每个块都在计算原矩阵中 2x2的元素，如果我们从未应用块划分的话，因此，它计算的是这里的这四个元素，如果我们在每个块上应用 softmax*，我们并没有使用这一行中的最大值元素，我们仅使用了每个块内的最大值元素，这意味着在与 V 度量的下游乘积中使用时，我们会累积错误的值，因为这里的每个只都将基于一个非全局最大值的局部最大值，这是该块的局部最大值，而这个块将使用其自身的局部最大值，这个块将使用其自身的局部最大值，以此类推，所以我想说的是，当你将 p11 与 v1 相加时，p11 可能有某个局部最大值，这个局部最大值与 p12 的局部最大值不同，p13 可能有一个不同的局部最大值，与 p11 和 p12 的局部最大值不同，因此，我们需要找到一种方法来修正这里用于计算指数的最大值，以防这里的最大值高于 p11 的局部最大值，

因此，如果我们在这里发现了一个比此处使用的最大值更高的最大值，那么我们需要修正这个和这个，因为 softmax 中的最大值应该是整个行的最大值，而不是每个块的最大值，这引导我们进入下一步，如何修正这个问题，

首先，让我介绍一些用于计算这个输出矩阵的伪代码，这是一个输出块矩阵，稍后我们将使用这个伪代码来调整我们在某些块中产生的错误，以防后续的块，如 P，有比 P 或 P 更好的最大值，因此，为了计算这个输出矩阵 O，我们需要遍历，例如，为了计算第一行，我们选择，嗯，p11 就是它本身，让我们回到前面，p11 是，让我也删除这个，它已经不在需要了，p11 是 q1 和 k1 的 softmax* 结果，p13 是 q1 和 k3 的 softmax* 结果，p14 是 q1 和 k4 的 softmax* 结果，这意味着，要计算这里的这个块，我们首先需要计算 p11，p11 是什么，p11 是 Q 块与 K 块的 softmax* 结果，对于输出矩阵的第一行来说，它表示的是查询与键1的 softmax* 结果，查询1与键2 的 softmax* 结果，查询1与键三的 softmax* 结果，查询1与键4的 softmax* 结果，这意味着我们需要遍历所有键，同时保持查询不变，因此，要计算第一行输出，我们需要进行 softmax* 计算得到 p11，即查询1与键1的 softmax* 结果，并初始化为零，因为我们不知如何初始化输出，所以就用零来初始化，接着，我们把查询1与键二的 softmax* 结果 p12 加进去，再把查询一与键三的 softmax* 结果 p13 加进去，以此类推，这就是为什么我们这里有一个内循环，好的，

所以，我们计算的这个输出是错误的，因为正如我之前所说，我们是用每个块的最大值统计量来计算 softmax* 的，而不是基于整个原始矩阵的每一行的最大值来计算的，如何修正这个问题呢？我们其实有一个工具，