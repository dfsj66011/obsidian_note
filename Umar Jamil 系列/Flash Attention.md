
从基本原理出发，全面探索 Flash Attention，这不仅意味着我们要编写 Flash Attention 的代码，还要从理论上推导出它的实现过程，因此，我们假设 Flash Attention 这篇论文从未存在过，而是直接审视注意力计算本身及其存在的问题，并一步步尝试解决这些问题，通过这种方式，我们不仅能深入理解其工作原理，还能将理论与实践紧密结合，实践部分则通过编写代码来实现。为了编写 Flash Attention 的代码，我们需要为 GPU 编写一个内核程序，具体来说，是针对我们的需求定制一个内核，不过我们不会直接编写 C++ 代码，而是使用 Triton，它能够将 Python 代码直接转换为可在 GPU 上运行的 CUDA 内核程序，可以把 Triton 看作是一个编译器，它接收 Python 代码并将其转换为能在 GPU 上运行的程序。

本篇文章要讨论的主题包括：

* 多头注意力机制（MHA）
* safe softmax
* online softmax,
* 然后，我们将深入了解 GPU，因为我们要编写一个在 GPU 上运行的内核程序，因此，我们需要理解 CPU 和 GPU 之间的区别，什么事内核程序，以及它与 CPU 编写的普通程序有何不同，
* 我们将研究张量在内存中的布局方式，比如行优先布局、列优先布局，步幅等，
* 我们将探讨分块矩阵乘法，
* Triton 的软件流水线，以及 Triton 对我们代码所做的所有优化
* 最后，我们将能够编写 Flash Attention 的前向传播代码，当然，仅仅编写前向传播代码并不能让我们满足，
* 我们还希望编写反向传播代码，但要编写反向传播代码，我们还需要理解在自定义操作的情况下，自动微分 autograd 和梯度下降是如何工作的，
* 因此，我们需要理解什么是导数、梯度和雅可比矩阵，
* 然后计算我们在 Flash Attention 中使用的常见操作的梯度，
* 最终，我们将掌握足够的知识来编写反向传播代码，

### 0、先决条件

* 高中数学基础（导数）
* 线性代数的基础知识（矩阵乘法、矩阵转置等）
* 注意力机制
* 很多耐心

我们会从基本原理出发，从头开始推导一切。

### 1、多头注意力机制

快速回顾一下 MHA 是什么以及它是如何工作的，公式如下：$$\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}$$
实际上，Flash Attention 主要关注的是 Attention 部分的优化，$Q,K,V$ 的线性层投影以及输出层投影都是常规的矩阵相乘，这一点在 GPU 中已经高度优化。


### 2、缩放点积注意力机制的局限性

#### 2.1 问题一、GPU I/O 限制

一个非常关键的问题是：我们为何需要改进注意力机制的实现方式，如果你查阅 [Flash Attention 1 论文](https://arxiv.org/pdf/2205.14135)，会注意到章节 2.2：

![[Pasted image 20250318151046.png|650]]

GPU 主要由两种内存构成，一种是 HBM，即动态随机存取存储器（DRAM），也就是 GPU 的内存，例如 A100 的 40GB 内存，这是 GPU 中容量最大的内存；此外还存在共享内存。

GPU 面临的问题是，访问 HBM（全局内存）与访问共享内存相比，速度极其缓慢，然而，与 HBM 相比，共享内存的容量要小得多，FlashAttention 论文中指出，*注意力机制的操作是 I/O 受限的*，这意味着，如果我们频繁访问全局内存，那么计算注意力机制的整体操作速度慢，这并不是因为计算这些操作本身慢，而是因为频繁访问速度较慢的全局内存导致的，因此我们可以将这类操作称为 I/O 受限型操作。

因此，改善这一状况的唯一方法是，在 GPU 的共享内存中计算注意力机制，尽管共享内存的容量要小得多，因为共享内存更靠近实际执行计算的 kernel，因此，我们需要将注意力计算拆分为更小的块，以便这些块能够放入共享内存中，然后在那里在那里计算输出矩阵的一部分，再将这部分复制到位于 HBM 中的输出矩阵中，并针对查询、键、和值矩阵划分的所有块，重复这一过程。

在论文中，他们称之为“分块（tiling）”，这是一种在编写 GPU 内核时常用的技术，尤其是在涉及矩阵乘法的情况下。现在我们了解了 FlashAttention 试图解决的核心问题。

#### 2.2 问题二、softmax

这种分块计算的最大难题在于 softmax，因为 softmax 需要访问整个 $S$ 矩阵的所有元素才能完成计算，因为需要计算归一化因子，这个因子是对所有元素逐行计算指数后的总和。


### 3、（Safe）Softmax

#### 3.1 softmax 计算上的问题

$$
\mathbf{S} = \mathbf{QK}^\top \in \mathbb{R}^{N \times N}, \quad \mathbf{P} = \text{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d},$$
这里的 $QKV$ 都是经过相应线性层转换后的矩阵，$Q,K$ 的维度均为 $N \times d$，点积运算后，其输出矩阵 $S$ 的维度为 $N \times N$，softmax 操作按行处理，并不改变矩阵维度，其结果最后与 $V$ 相乘，输出维度为 $N \times d$。

softmax 操作的作用是什么呢？它会将这些点积结果进行转换，使得它们以某种方式变为一种概率分布，*按行计算*，这意味着每个数字都介于 0 到 1 之间，softmax 的定义如下：$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}$$分母是向量所有维度指数的总和，这被称为归一化因子，为的是使所有这些数字介于 0 和 1 之间，使用 softmax 是因为我们希望这些数字都是正数（概率值），这是使用指数函数的原因，

但这里*存在一个问题*，问题在于，想象一下我们的输入向量由许多可能很大的数字组成，比如 100 的指数，会造成计算机结果上溢，即数值不稳定性，在计算机科学中，“数值不稳定性”意味着数字无法用我们现有的位数（通常是 32 位或 16 位）在固定表示形式中表示出来。

#### 3.2 解决方案

为了使这个 softmax 操作在数值上保持稳定，我们希望这些数字不会爆炸或变得太小以至于无法表示，我们需要找到一种解决方案，如下：$$\begin{align}\frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}} 
&= \frac{c \cdot e^{x_i}}{c \cdot \sum_{j=1}^{N} e^{x_j}} = \frac{c e^{x_i}}{\sum_{j=1}^{N} c e^{x_j}} = \frac{e^{\log(c)} e^{x_i}}{\sum_{j=1}^{N} e^{\log(c)} e^{x_j}} \\[1.2ex]
&= \frac{e^{x_i + \log(c)}}{\sum_{j=1}^{N} e^{x_j + \log(c)}} = \frac{e^{x_i - k}}{\sum_{j=1}^{N} e^{x_j - k}} \quad \text{where } k = -\log(c)\end{align}$$
用因子 c 乘以分子和分母，通过上面推导过程，我们可以看到，如果我们巧妙地选择一个值插入到这个指数函数中，就能有效减少指数部分的计算量，我们将选择这个 $k$ 值等于输入向量中需要应用 softmax 的最大元素（$k=\max_i(x_i)$），这样一来，每个指数的参数要么为 0（当 $x_i$ 等于向量中的最大值时），要么小于 0，其指数结果介于 0 到 1 之间，这样的数值用 32 位浮点数就能轻松表示。

#### 3.3 safe softmax 算法

$$\text{softmax}(x_i) = \frac{e^{x_i - x_{\text{max}}}}{\sum_{j=1}^{N} e^{x_j - x_{\text{max}}}}$$
给定一个 $N \times N$ 的矩阵，对于每一行：

1. 寻找每一行的最大值
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
2. 计算分母归一化因子
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
3. 对向量中的每一个元素应用 softmax
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$

伪代码如下：
```python
m_0 = -infty
for i=1 to N
    m_i = max(m_{i-1}, x_i)

l_0 = 0
for J=1 to N
    l_J = l_{J-1} + e^{x_J - m_N}

for K=1 to N
    x_K <- e^{x_K-m_N} / l_N
```

这段伪代码描述的算法相当慢，显而易见，这里存在 3 个 for 循环。所以接下来优化的思路就是寻找一种策略，合并其中的某些操作，减少循环次数。

### 4、Online Softmax

#### 4.1 online softmax

我们尝试将前两个操作融合到一个 for 循环中，这意味着我们只需要遍历数组一次，同时计算 $m_i$，并尝试计算 $l_j$，当然，我们无法在此刻计算 $l_j$，因为无法得知全局最大值，但我们可以尝试使用当前已知的局部最大值作为估算值来进行计算，即我们尝试用 $m_i$ 替代 $m_n$。

当后续迭代过程中发现更大值时，需要对过去计算项进行修正，实际上这个校正因子非常容易计算，以 $x=[3,2,5,1]$ 为例，在前两轮中最大值为 3，第三次迭代时，最大值为 5，即在第三轮迭代中，

* 错误迭代计算：$l_3 = l_2 + e^{5-5}=e^{3-3}+e^{2-3}+e^{5-5}$
* 正确修正方法：$l_3 = l^2 \cdot \textcolor{blue}{e^{3-5}} + e^{5-5}=(e^{3-3}+e^{2-3})\textcolor{blue}{e^{3-5}}+e^{5-5}$

显然这个修正因子的计算方法为过去的最大值与当前新的最大值之间的差。

因此，softmax 新算法如下：
```python
m_0 = -infty
l_0 = 0
for i=1 to N
    m_i = max(m_{i-1}, x_i)
    l_i = l_{i-1}*e^{m_{i-1} - m_i} + e^{x_i - m_i}

for K=1 to N
    x_K <- e^{x_K-m_N} / l_N
```

#### 4.2 数学归纳法证明


1. 证明对于大小为 $N=1$ 的向量，该命题成立：$$\begin{align}
m_1 &= \max(-\infty, x_1) = x_1 = \max_i(x_i) = x_{\max} \\[1.2ex]
l_1 &= 0 \times e^{-\infty} + e^{x_1 - x_1} = \sum_{j=1}^{N} e^{x_j - x_{\max}}\end{align}$$
2. 如果假设该命题对大小为 $N$ 的向量成立，证明它对大小为 $N+1$ 的向量也成立$$\begin{align}
m_{N+1} &= \max(m_N, x_{N+1}) = \max_i(x_i) \\[1.2ex]
l_{N+1} &= l_N \cdot e^{m_N - m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \left(\sum_{j=1}^{N} e^{x_j - m_N}\right)e^{m_N-m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \sum_{j=1}^{N} e^{x_j - m_{N+1}} + e^{x_{N+1} - m_{N+1}} \\
&= \sum_{j=1}^{N+1} e^{x_j - m_{N+1}} \end{align}$$
### 5、分块矩阵乘法

![[Pasted image 20250319151749.png|500]]

#### 5.1 忽略 softmax

目前我们先暂时忽略 softmax 的部分，即：$$\mathbf{S} = \mathbf{QK}^\top \in \mathbb{R}^{N \times N},  \quad \mathbf{O} = \mathbf{SV} \in \mathbb{R}^{N \times d},$$当然，这种做法是不正确的，但它简化了我们接下来要处理的内容，

![[Pasted image 20250320101609.png|500]]

现在，每个 query 是由 Q 矩阵中的两行组成的一个组，key 也做相应的分块，在此基础上做分块矩阵乘法，如下所示：

![[Pasted image 20250320101933.png|400]]

以 $S$ 中左上角第一个分块为例，$Q_1$ 的维度为 $(2, 128)$，$K^T$ 的维度是 $(128,2)$，即 $S_{11}$ 实际上是一个 $(2,2)$ 的小矩阵。接下来将 $S$ 矩阵与 $V$ 相乘，其结果也是显而易见的：
![[Pasted image 20250320102445.png|400]]
其运算结果为：

![[Pasted image 20250320102557.png|400]]


伪代码如下：
```python
FOR EACH BLOCK Q_i
    O_i = zeroes(2, 128)  // Output is initially zeroes
    FOR EACH BLOCK K_j
        O_i ← O_i + (Q_i * K_j^T) * V_j
    END FOR
END FOR
```

#### 5.2 softmax$^\star$

softmax$^\star$ 是去除了归一化的 softmax，$$\text{SOFTMAX}^*\left(S_{ij}\right) = \exp\left[S_{ij} - \text{rowmax}\left(S_{ij}\right)\right]
$$
将 softmax$^\star$ 应用于 $S$ 矩阵的每个块上，可以得到：
![[Pasted image 20250320142554.png|500]]

但是需要注意的是，理论上，应用于每个小块内部元素上，我们需要知道该行的最大值，但目前暂时无法获知，举例而言，假设 $S_{11}= [a\, b; c\, d]$，假设第一行的最大值是 $a$，第二行的最大值是 $d$，则 $P_{11}=[e^{a-a}\, e^{b-a}; e^{c-d}\, e^{d-d}]$，接下来，将 $P$ 矩阵与 $V$ 矩阵相乘，得到 $O$ 矩阵如下：
![[Pasted image 20250320152649.png|240]]

*再次强调：这里计算每个 softmax$^\star$ 的最大值，并不是 $S$ 矩阵这一行的全局最大值，而是每个块的局部最大值，这实际上是错误的；*

然而，我们实际在做的是块矩阵乘法，正如你所记得的，当我们分块计算时，我们会将查询的行和键的行分组处理，在这个特定的例子中，我们将两个查询想象归为一组，形成一个查询块，同时将两个键向量归为一组，形成一个键块，因此，我们需要这个块的另一行数据，所以它是，让我选择，查询1与键1，以及查询2与键1，这里应该是查询2与键1，查询2与键2，查询2与键3，查询2与键4，查询2与键5以及查询2与键6，这里的每个块都在计算原矩阵中 2x2的元素，如果我们从未应用块划分的话，因此，它计算的是这里的这四个元素，如果我们在每个块上应用 softmax*，我们并没有使用这一行中的最大值元素，我们仅使用了每个块内的最大值元素，这意味着在与 V 度量的下游乘积中使用时，我们会累积错误的值，因为这里的每个只都将基于一个非全局最大值的局部最大值，这是该块的局部最大值，而这个块将使用其自身的局部最大值，这个块将使用其自身的局部最大值，以此类推，所以我想说的是，当你将 p11 与 v1 相加时，p11 可能有某个局部最大值，这个局部最大值与 p12 的局部最大值不同，p13 可能有一个不同的局部最大值，与 p11 和 p12 的局部最大值不同，因此，我们需要找到一种方法来修正这里用于计算指数的最大值，以防这里的最大值高于 p11 的局部最大值，

因此，如果我们在这里发现了一个比此处使用的最大值更高的最大值，那么我们需要修正这个和这个，因为 softmax 中的最大值应该是整个行的最大值，而不是每个块的最大值，这引导我们进入下一步，如何修正这个问题，

首先，让我介绍一些用于计算这个输出矩阵的伪代码，这是一个输出块矩阵，稍后我们将使用这个伪代码来调整我们在某些块中产生的错误，以防后续的块，如 P，有比 P 或 P 更好的最大值，因此，为了计算这个输出矩阵 O，我们需要遍历，例如，为了计算第一行，我们选择，嗯，p11 就是它本身，让我们回到前面，p11 是，让我也删除这个，它已经不在需要了，p11 是 q1 和 k1 的 softmax* 结果，p13 是 q1 和 k3 的 softmax* 结果，p14 是 q1 和 k4 的 softmax* 结果，这意味着，要计算这里的这个块，我们首先需要计算 p11，p11 是什么，p11 是 Q 块与 K 块的 softmax* 结果，对于输出矩阵的第一行来说，它表示的是查询与键1的 softmax* 结果，查询1与键2 的 softmax* 结果，查询1与键三的 softmax* 结果，查询1与键4的 softmax* 结果，这意味着我们需要遍历所有键，同时保持查询不变，因此，要计算第一行输出，我们需要进行 softmax* 计算得到 p11，即查询1与键1的 softmax* 结果，并初始化为零，因为我们不知如何初始化输出，所以就用零来初始化，接着，我们把查询1与键二的 softmax* 结果 p12 加进去，再把查询一与键三的 softmax* 结果 p13 加进去，以此类推，这就是为什么我们这里有一个内循环，好的，

所以，我们计算的这个输出是错误的，因为正如我之前所说，我们是用每个块的最大值统计量来计算 softmax* 的，而不是基于整个原始矩阵的每一行的最大值来计算的，如何修正这个问题呢？我们其实有一个工具，我们之前已经计算过一种算法，叫做在线 softmax，我不确定之前是否提到过它叫在线 softmax，但它确实叫做在线 softmax，它允许我们在计算当前迭代时修正之前的迭代，基于什么原理呢？

让我们回顾一下在线 softmax，我们开始吧，假设我们正在处理一个单一的向量，我们有一个由 n 个元素组成的向量，我们做的是通过一个 for 循环，迭代计算到当前元素为止的最大值，修正之前迭代中计算的归一化因子，并在当前元素找到更大的最大值时，如果这部分不清楚的话，大家回去看看在线 softmax 的内容，因为这非常重要，我们将用这种方法来修正 P11，和 P12 块，以防在 P13 和 P14 等块中找到更大的最大值，那么，让我们看看如何将在线 softmax 应用到当前场景中，以便进行计算，你可能会想，为什么要费这么大周折呢？我是说，为什么呢？

真正的原因在于，首先，我们为什么要引入块矩阵乘法，因为我们想要并行计算矩阵乘法，你可以认为，由于 P11 等块彼此独立，并且每个块都是用各自块内的最大值，因此它们可以独立计算，然而，我们需要以某种方式聚合它们的值，为了聚合这些值，我们需要修正那些独立计算的值，因为在独立计算时我们没有这样做，我们没有全局视角，只有局部视角，因此，我们计算局部块，比如 P11，P12，P13 等，然后，当我们需要聚合这些值时，必须对它们进行修正，正因如此，我们才尝试构建这套系统，用于修正那些独立计算得出的值，那么，如何解决这个问题呢？

让我们来看一下接下来的算法，首先正如我之前提到的，这里的 O 块是一个包含两行的块，每一行由128个维度组成，这一点我们之前通过检查 P11 和 V1 的维度已经了解，即 P11 与 V1 相乘的结果表明，对于每个输出块，我们需要处理两个最大值和两个归一化因子，到目前为止，我们还没有用到归一化因子，我们说过，正在应用的是 softmax*，即未经归一化的 softmax，但最终我们还是需要计算这个归一化因子，因此，我们的目标是设计一个算法，既能修正用于计算每个 P11 的最大值，又能同时计算归一化因子，最后再应用这个归一化因子，具体实现方法如下所述，

我们首先将最大值初始化为负无穷，每处理一行就初始化一个，由于我们的输出块由两行组成，因此需要为顶行和底行各设一个最大值，同时初始化归一化因子为 0，因此目前尚未进行任何求和操作，以及输出，我们将输出初始化为全零，因为目前还没有向这个输出添加任何内容，我们计算输出行时，需要先进行计算，因此，针对这里的输出块，即这个输出块，这里，我们需要遍历所有键来生成这些值，P11、P12、P13、P14，而查询则是第一个查询，即查询块1，第一步，我们计算第一个块 P11 的最大值，即行最大值，也就是块 q1、k1 中每行的最大值，这里实际上指的是 s1，而不是 p11，抱歉大家，这里应该是 s11，因此，我们计算它的最大值，并称之为 s1，正如你在这里看到的，接下来，我们可以计算 P11，也就是 softmax*，它是查询1与k1的指数乘积，即 s1 减去局部组中的最大值 s1，然后将结果添加到我们的输出中，目前，输出初始化为零，所以现在，暂时忽略这部分，稍后我会解释这部分，因此，目前 O1 应该只等于 P11 乘以 V1，现在，在第二步中，我们可能会在局部组 S12 中找到更好的最大值，这里的 S12 就是新的最大值，我们需要更新顶部行和底部行的最大值，这个最大值是 M2，它可能比之前这两个行的最大值更好，但也可能不是，我们需要找到一种方法，在它更好的情况下进行调整，而在它没有更好的情况喜爱不做任何改动，我们是这样做的，

因此，我们计算当前局部行查询 2 的新最大值，我们计算了 P12，它是 s2 的 softmax*，即 s2 减去局部最大值 m2，然后我们需要将其添加到输出中，然而，在这种情况下，我们可能找到了一个更好的最大值，那么如何修正 o1 呢？它只使用了 s1 的局部最大值，我们知道，可以通过使用指数函数来修正，因为 o1 的每个元素都只是一个指数，没有归一化，因为我们应用的是 softmax*，那么如何用另一个指数来修正一个指数呢？基本上，我们说的是将 o1，即一个矩阵，进行乘法操作，那么让我展示一下，这个矩阵是什么，所以 o1 是一个由两行组成的矩阵，如你所见，这里展示了 o1 的形状，它是一个 2 行 128 列的矩阵，这是第一行，所以 o11,o12 以此类推，直到 o1128，然后是 o21,o22以此类推，直到 o2128，需要修正这个值，如何修正呢？

我们基本上就是使用之前在线 softmax 中用过的指数函数来修正，因此，如果我们用一个对角矩阵乘以这个矩阵，对角矩阵的构造如下：它是由两个元素组成的对角矩阵，因为 m1 减去 m2 的指数结果会是一个包含两个元素的向量，而元素级别的指数运算会生成另一个包含两个元素的向量，这里的 Diag 基本上表示一个对角矩阵，其对角线上的元素，是我们应用 Diag 操作的向量中的元素，也就是说，这里的值将是 m1 第一个元素的指数，让我展示一下如何写它：m1 减去 m2 在减去 m2 的指数，所以第一个元素，我们在这里称它为 1，这里有一个零，这里也是 0，让我们删除这个，然后在这里写一下另一个，即 m1 减去 m2 的指数，但这是向量的第二个元素，简单来说，这里的 diag 表示将向量分布到一个 nxn 的矩阵上，其中 n 是应用该操作的向量的大小，矩阵中所有其他元素都应为零，这就是 diag 的含义，如果我们在这里进行这个操作，我们会看到这个乘法的输出将使用这个指数修正顶行的每个元素，并使用这个指数修正底行的每个元素，这基本上会抵消之前迭代中计算的 m1，并在 O 块矩阵的每个元素中引入当前迭代中计算的 m2，

好的，所以在这个输出将是，这个元素将乘以这个，所以它将用这里的这个因子修正 o11，而 o21 不会被修正，将乘以零，所以它不会对第一个输出元素有贡献，所以这里的这个元素将只依赖于由 m1 减去 m2 的指数修正的 o11，但这是向量的第一个元素，然后 o 也将由这里的这个指数修正，而不是由这个修正。

第一行的所有维度都将由这个指数修正，而这里的第二行的所有维度将由这里的这个指数修正，这个标量，它是向量 exp 的第二个元素，好的，这个确实很有挑战性，所以我们正在做的是计算 p12，并通过乘以这里的这个矩阵，乘以这里的这个因子，矩阵因子，来修正 p1 中的所有元素，当我们计算第三步时，我们将修正第二步，以此类推，现在让我们来谈谈归一化因子，因为当目前为止我们一直忽略了它，归一化因子是我们在计算这些最大值时可以同时计算的，因为它在我们之前看到的在线算法的伪代码中提供了用于 softmax，所以在计算最大值的同时，我们实际上可以通过修正前一次迭代的归一化因子，来计算归一化因子，这正是我们在这里所做的，所以在第一次迭代中，我们使用局部最大值计算了归一化因子，在第二次迭代中，你现在可以忽略这个，因为我们没有用任何东西修正 L0，因为 L0 将是 0，所以我们基本上只是在这里计算这个求和，所以 L0 将是 0，所以这里的这个因子将是 0，嗯，在计算 L2时，也就是归一化步骤，在第二次迭代中，我们将用一个指数来修正 L1，猜猜是什么，正是用来修正最大值的那个指数，即 p11，所以它是前一次对最大值的估计，减去当前对最大值的估计，再加上使用局部最大值的新归一化因子，然后我们继续执行这一过程，最终，我们得到这个块的正确输出

但没有进行归一化，如何应用归一化呢？归一化就是我们需要用归一化因子，除以 O 中的每个元素，但由于我们在遍历这四个循环时，也在计算归一化因子，我们不断累积它，直到迭代结束，然后应用归一化因子，因此我们取最后的输出，只需将其除以 L4，即第四次迭代计算出的归一化因子，这将修正 softmax，好了，各位，现在我们已经推导出如何分块计算注意力模块输出的算法，同时也在每个块中独立修正了 softmax，我们知道，归一化是在最后完成的，我还想证明一下这一点，因此，当我们引入这种在线计算 softmax 的算法时，我们通过归纳法证明了该算法的正确性，因此，在这个算法结束时，最后一次迭代的 L 实际上就是我们可以用来得到 softmax 的归一化因子，因此，在在线计算输出时，我们不会在通过将查询与所有键块相乘的迭代过程中应用归一化，我们在这四次迭代结束时应用它，在这四次迭代结束时，我们将得到最后的输出，我们还知道，最后的 L 将包含我们需要应用于每行的确切归一化因子，因为这个 O 是一个输出行块，如果你还记得注意力机制，注意力的输出与输入查询向量的形状相同，这是一个标记序列，因此，这个 O 是一个我们需要应用归一化的标记序列，我们知道正确的因子是 L4，那么，让我们来证明这个简单的公式，L4 是一个向量，它包含的元素数量与 O4 中的行数相同，所以在这个 O 行块中，假设它包含两行，就像我目前描述的算法中那样，我们假装将两行查询，与两列键组合在一起，因此，输出 O，即块 O 将包含两列输出，所以，我们在这个 L4 向量中会有两个归一化因子，我们通过这个公式所做的是，取这个 L4 向量，并用它创建一个对角线矩阵，然后计算这个对角矩阵的逆矩阵，因此，L4 是一个包含两个归一化因子的向量，因此，它是 L，我不确定，让我们称之为 L4 元素 1 和 L4 元素 2，这就是我们的 L4 向量，然后我们有 o4，o4 是一个矩阵，从形状可以看出，它是一个 2 乘 128 的矩阵，所以 o 是，实际上我们复制它吧，哦不，还是别复制了，

o4 是一个两行 128 列的矩阵，第一行有 128 个元素，第二行也有 128 个元素，我们对 l4 做的第一件事是，将其转换为一个对角矩阵，它将是一个 2 乘 2 的对角矩阵，因为它包含两个元素，因此他将变成类似这样的形式，所以它将是 L4，L4 的第一个元素，然后是 0， 接着是 0，以及这个向量的第二个元素 L4，然后我们计算这个矩阵的逆矩阵，对角矩阵的逆矩阵就是将对角线上的每个元素取倒数后得到的对角矩阵，这是线性代数中的内容，不是我编造的，也不是我发明的，所以这个矩阵的逆矩阵等于，嗯，同样的对角矩阵，但每个元素是 L4 的倒数，即 L4 的第一个元素的倒数，0，0，以及 L4 的第二个元素的倒数，然后我们将这些内容进行相乘，让我删除一些内容，所以这里的内容将与 o 相乘，o 是一个矩阵，是一个 2 乘 128 的矩阵，所以我们在进行这个乘法运算，现在进行乘法运算，现在，这是输出的结果，所以这是二，让我写下来，2 乘 2矩阵与 2 乘 128 矩阵相乘的结果将是一个 2 乘 128 的矩阵，其中运算结果的第一行第一维将是这个的点积，将这一行与第一列相乘，所以基本上我们在这里用 L4 的第一个元素来除这个元素，这里的第二个输出元素将是这一行与第二列的点积，所以我们只进行乘法运算，我们在这里将输入向量的第二个元素除以 L 的第一个元素，因为第二行的所有元素都将乘以 0，所以它们不会对这一输出行产生贡献，而第二个输出行将是点积的结果，这个元素将是这一行与第一列的点积，这里的第一个元素乘以 0，所以它不会对这个输出产生贡献，所以只有第二个元素，即第二行的第一个元素，输入矩阵第二行的第一个元素将除以 L 的第二个元素，这基本上将应用于第二行的所有元素，而第一行的所有元素将除以 L 的第一个元素，从而生成我们这里需要的归一化结果，我们需要应用这个归一化因子，这应该能帮助你更好的理解为什么这个操作最终会归一化输出向量，并且仍然得到相同的结果，

现在让我们继续深入，好了，各位，我们终于可以看看 Flash Attention 的前向传播过程了，同时也可以将其与我们目前推导的内容进行比较，如果你看一下 Flash Attention 的论文，首先，这是 Flash Attention 2 的前向传播过程，稍后我会解释，Flash Attention 1 和 Flash Attention 2 之间的区别，我不想直接跳过这个前向传播过程，因为我相信，即使推导过程有点难懂，它也能让你对正在发生的事情有一些直观的理解，所以即使你只理解了其中的 50%，也足够了，因为稍后我们还会编写代码，你应该能达到 90% 的理解程度，因此，每当我们引入一些新的信息时，他都应该能提高你的理解，基本上，在 Flash Attention 中，特别是在 Flash Attention 2 中，我们以查询、键和值作为输入，他们都是 token 序列，每个 token 由一个 d 维向量组成，其中 d 是小写的 d 维，然后我们将这个查询分为若干块，分成多少块呢？

这取决于参数 br，也就是我们想要选择的查寻块的大小，也就是说，我们想把多少行的查询组合成一个块，我们对 K 和 V 也做了同样的处理，根据参数 BC 将它们分成若干块，然后我们还要初始化输出，也就是我们想要生成的输出，那么 flash attention 到底在计算什么呢？简单来说，flash attention 计算的是以下内容，它计算的是 softmax，

具体来说，就是用查询乘以键的转置，再除以归一化因子，得到 softmax 值，最后乘以 V，这就是它的计算过程，而且是以这种方式来计算的，首先，我们有一个针对查询的外层循环，这与之前看到的伪代码是一致的，以为我们要并行计算，输出矩阵的每个块，说白了，我们希望独立计算这个输出块和那个输出块，这里的这个输出块依赖于第一个查询和所有的键，而这个输出块则依赖于第二个查询和所有的键，这个输出块依赖于第三个查询和所有的键，这里的查询 1 不是指第一个查询，而是指第一组查询或第一个查询块，查询 2 也不是指第二个查询，而是指查询矩阵的第二个块，以此类推，等等，正因为如此，我们需要在所有块之间进行外层迭代，这样才能并行计算输出矩阵的所有块，但要计算每一个输出块，我们需要遍历所有的键，这就是为什么我们在键上设置了一个内层循环，我们执行的正是之前已经完成的操作，手动操作，首先我们计算 s 矩阵，它是每个查询块与对应键块的乘积结果，接着，我们计算当前 s 块的局部最大值，这是局部最大值，我们会将其与前一次迭代的最大值进行比较，因为这是在线 softmax 中的标准做法，随后，我们计算 p 块，即 s 块的 softmax* 值减去 s 块的局部最大值，

接下来，我们计算归一化因子，归一化因子是什么，它是所有 softmax* 指数值的总和，但需要结合上一步的归一化因子进行调整，我们知道如何调整归一化因子，只需乘以一个指数值，即前一次最大值与当前最大值的差值，这就是这个因子的作用，接着，我们使用之前提到的修正因子，精确计算输出，这个因子是一个对角矩阵，其对角线上的元素由这个向量构成，该向量时前一次最大值与当前最大值的差值取指数后的结果，我们将这个结果乘以上一步的输出，因为需要修正上一步的计算结果，它是基于上一步的局部最大值得出的，再加上当前基于局部最大值的 PV 值，而这一结果将在下一次迭代中被修正，好的，最终，在我们遍历完所有的键之后，我们已经计算出了所有输出块，但尚未应用归一化因子，归一化因子在最后应用，是因为在处理每个键的过程中，我们正在计算 softmax 的 L 归一化因子，而在这个循环内部，我们仅计算了 softmax*，因此并未对每个值进行归一化，因此，最后需要对结果进行归一化处理，具体操作如下：使用我们在所有迭代中计算得到的归一化因子，并将其应用于 o 的每个元素，因为 softmax* 与实际 softmax 的区别，仅在于是否除以了归一化因子，这里的操作实际上是将每个 O 除以对应的归一化因子，每个因子对应输出块中的一行，也就是我们正在计算的输出矩阵的每一行，稍后，我们还将探讨接下来的操作步骤，

SRAM 是什么，HBM 是什么，现在，我只需要你专注于我们正在进行的操作，这些操作与我们迄今为止所做的完全一致，稍后我们会详细讲解，此外，我们还需要了解为什么要在这里保存这些内容，等等，但目前，你应该已经掌握了足够的知识，能够理解 Flash Attention 论文中所述的内容，因为，就前向传播算法而言，我们所做的基本上就是分块矩阵乘法，而在计算这个块时，我们通过运用指数的技巧来固定前一个块，好的，现在我们了解了 Flash Attention 的前向传播过程，但在实现它之前，我们还缺少一些知识，因为我们对 GPU 一无所知，对 cuda 也一无所知，对 triton 同样不了解，因此接下来我们将学习这些内容。

好了，各位，现在终于到了我们探索 GPU和 cuda 编程模型的时候了，那么，让我们从比较 cpu 和 gpu 开始，这将帮助我们理解 cuda 的工作原理，首先，什么是 cuda，什么是 gpu，gpu 是我们购买的硬件单元，而 cuda 是由 nvidia 开发的软件堆栈，用于为他们销售的 gpu 编写软件，amd 有自己的软件堆栈，其他制造商也有各自的解决方案，在这个视频中，我们将看到一些 cuda 内核的例子，但你获得的知识同样适用于其他 gpu，首先，cpu 和 gpu 的第一个区别在于它们的用途，你的电脑目前正在 cpu 上运行，操作系统通过所谓的调度器与 cpu 进行交互。