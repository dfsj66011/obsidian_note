
从基本原理出发，全面探索 Flash Attention，这不仅意味着我们要编写 Flash Attention 的代码，还要从理论上推导出它的实现过程，因此，我们假设 Flash Attention 这篇论文从未存在过，而是直接审视注意力计算本身及其存在的问题，并一步步尝试解决这些问题，通过这种方式，我们不仅能深入理解其工作原理，还能将理论与实践紧密结合，实践部分则通过编写代码来实现。为了编写 Flash Attention 的代码，我们需要为 GPU 编写一个内核程序，具体来说，是针对我们的需求定制一个内核，不过我们不会直接编写 C++ 代码，而是使用 Triton，它能够将 Python 代码直接转换为可在 GPU 上运行的 CUDA 内核程序，可以把 Triton 看作是一个编译器，它接收 Python 代码并将其转换为能在 GPU 上运行的程序。

本篇文章要讨论的主题包括：

* 多头注意力机制（MHA）
* safe softmax
* online softmax,
* 然后，我们将深入了解 GPU，因为我们要编写一个在 GPU 上运行的内核程序，因此，我们需要理解 CPU 和 GPU 之间的区别，什么事内核程序，以及它与 CPU 编写的普通程序有何不同，
* 我们将研究张量在内存中的布局方式，比如行优先布局、列优先布局，步幅等，
* 我们将探讨分块矩阵乘法，
* Triton 的软件流水线，以及 Triton 对我们代码所做的所有优化
* 最后，我们将能够编写 Flash Attention 的前向传播代码，当然，仅仅编写前向传播代码并不能让我们满足，
* 我们还希望编写反向传播代码，但要编写反向传播代码，我们还需要理解在自定义操作的情况下，自动微分 autograd 和梯度下降是如何工作的，
* 因此，我们需要理解什么是导数、梯度和雅可比矩阵，
* 然后计算我们在 Flash Attention 中使用的常见操作的梯度，
* 最终，我们将掌握足够的知识来编写反向传播代码，

### 0、先决条件

* 高中数学基础（导数）
* 线性代数的基础知识（矩阵乘法、矩阵转置等）
* 注意力机制
* 很多耐心

我们会从基本原理出发，从头开始推导一切。

### 1、多头注意力机制

快速回顾一下 MHA 是什么以及它是如何工作的，公式如下：$$\begin{align*}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}$$
实际上，Flash Attention 主要关注的是 Attention 部分的优化，$Q,K,V$ 的线性层投影以及输出层投影都是常规的矩阵相乘，这一点在 GPU 中已经高度优化。


### 2、缩放点积注意力机制的局限性

#### 2.1 问题一、GPU I/O 限制

一个非常关键的问题是：我们为何需要改进注意力机制的实现方式，如果你查阅 [Flash Attention 1 论文](https://arxiv.org/pdf/2205.14135)，会注意到章节 2.2：

![[Pasted image 20250318151046.png|650]]

GPU 主要由两种内存构成，一种是 HBM，即动态随机存取存储器（DRAM），也就是 GPU 的内存，例如 A100 的 40GB 内存，这是 GPU 中容量最大的内存；此外还存在共享内存。

GPU 面临的问题是，访问 HBM（全局内存）与访问共享内存相比，速度极其缓慢，然而，与 HBM 相比，共享内存的容量要小得多，FlashAttention 论文中指出，*注意力机制的操作是 I/O 受限的*，这意味着，如果我们频繁访问全局内存，那么计算注意力机制的整体操作速度慢，这并不是因为计算这些操作本身慢，而是因为频繁访问速度较慢的全局内存导致的，因此我们可以将这类操作称为 I/O 受限型操作。

因此，改善这一状况的唯一方法是，在 GPU 的共享内存中计算注意力机制，尽管共享内存的容量要小得多，因为共享内存更靠近实际执行计算的 kernel，因此，我们需要将注意力计算拆分为更小的块，以便这些块能够放入共享内存中，然后在那里在那里计算输出矩阵的一部分，再将这部分复制到位于 HBM 中的输出矩阵中，并针对查询、键、和值矩阵划分的所有块，重复这一过程。

在论文中，他们称之为“分块（tiling）”，这是一种在编写 GPU 内核时常用的技术，尤其是在涉及矩阵乘法的情况下。现在我们了解了 FlashAttention 试图解决的核心问题。

#### 2.2 问题二、softmax

这种分块计算的最大难题在于 softmax，因为 softmax 需要访问整个 $S$ 矩阵的所有元素才能完成计算，因为需要计算归一化因子，这个因子是对所有元素逐行计算指数后的总和。


### 3、（Safe）Softmax

#### 3.1 softmax 计算上的问题

$$
\mathbf{S} = \mathbf{QK}^\top \in \mathbb{R}^{N \times N}, \quad \mathbf{P} = \text{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d},$$
这里的 $QKV$ 都是经过相应线性层转换后的矩阵，$Q,K$ 的维度均为 $N \times d$，点积运算后，其输出矩阵 $S$ 的维度为 $N \times N$，softmax 操作按行处理，并不改变矩阵维度，其结果最后与 $V$ 相乘，输出维度为 $N \times d$。

softmax 操作的作用是什么呢？它会将这些点积结果进行转换，使得它们以某种方式变为一种概率分布，*按行计算*，这意味着每个数字都介于 0 到 1 之间，softmax 的定义如下：$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}$$分母是向量所有维度指数的总和，这被称为归一化因子，为的是使所有这些数字介于 0 和 1 之间，使用 softmax 是因为我们希望这些数字都是正数（概率值），这是使用指数函数的原因，

但这里*存在一个问题*，问题在于，想象一下我们的输入向量由许多可能很大的数字组成，比如 100 的指数，会造成计算机结果上溢，即数值不稳定性，在计算机科学中，“数值不稳定性”意味着数字无法用我们现有的位数（通常是 32 位或 16 位）在固定表示形式中表示出来。

#### 3.2 解决方案

为了使这个 softmax 操作在数值上保持稳定，我们希望这些数字不会爆炸或变得太小以至于无法表示，我们需要找到一种解决方案，如下：$$\begin{align}\frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}} 
&= \frac{c \cdot e^{x_i}}{c \cdot \sum_{j=1}^{N} e^{x_j}} = \frac{c e^{x_i}}{\sum_{j=1}^{N} c e^{x_j}} = \frac{e^{\log(c)} e^{x_i}}{\sum_{j=1}^{N} e^{\log(c)} e^{x_j}} \\[1.2ex]
&= \frac{e^{x_i + \log(c)}}{\sum_{j=1}^{N} e^{x_j + \log(c)}} = \frac{e^{x_i - k}}{\sum_{j=1}^{N} e^{x_j - k}} \quad \text{where } k = -\log(c)\end{align}$$
用因子 c 乘以分子和分母，通过上面推导过程，我们可以看到，如果我们巧妙地选择一个值插入到这个指数函数中，就能有效减少指数部分的计算量，我们将选择这个 $k$ 值等于输入向量中需要应用 softmax 的最大元素（$k=\max_i(x_i)$），这样一来，每个指数的参数要么为 0（当 $x_i$ 等于向量中的最大值时），要么小于 0，其指数结果介于 0 到 1 之间，这样的数值用 32 位浮点数就能轻松表示。

#### 3.3 safe softmax 算法

$$\text{softmax}(x_i) = \frac{e^{x_i - x_{\text{max}}}}{\sum_{j=1}^{N} e^{x_j - x_{\text{max}}}}$$
给定一个 $N \times N$ 的矩阵，对于每一行：

1. 寻找每一行的最大值
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
2. 计算分母归一化因子
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$
3. 对向量中的每一个元素应用 softmax
	* 时间复杂度：$O(n)$
	* 内存占用：$O(n)$

伪代码如下：
```python
m_0 = -infty
for i=1 to N
    m_i = max(m_{i-1}, x_i)

l_0 = 0
for J=1 to N
    l_J = l_{J-1} + e^{x_J - m_N}

for K=1 to N
    x_K <- e^{x_K-m_N} / l_N
```

这段伪代码描述的算法相当慢，显而易见，这里存在 3 个 for 循环。所以接下来优化的思路就是寻找一中策略，合并其中的某些操作，减少循环次数。


-----
好的，各位，我们先来回顾以下我们要解决的问题是什么，问题描述如下，我们能否找到一种更高效的方法来计算 softmax，无需遍历向量三次，因为让我们来看看目前为止我们发现的用于计算 softmax 的算法的伪代码，假设我们有一个由 4 个元素组成的向量，首先，我们需要计算这个向量中的最大元素，这意味着要遍历这个四元素的循环，以便找到向量中的最大元素，具体来说，我们从向量的左侧开始，逐步向右移动，从第一个元素开始，一直遍历到最后一个元素，并将之前找的最大值与当前元素进行比较，从而确定全局最大值，简单来说，这个过程虽然非常基础 ，我确信你可能不需要这个例子，但通过这个例子，我们能更好的理解接下来的步骤，所以请耐心听我讲解，即便我讲的内容看起来非常简单，

好的，一开始，$m_0$ 被初始化为负无穷大，$m_1$ 是第一次循环迭代的结果，这意味着，$m_1$ 将被赋值为当前估计的最大值（即负无穷大）与当前元素（即 3）中的较大者，因此 $m_1$ 最终等于 3，接下来，$m_2$ 将被赋值为之前计算出的最大值与当前元素中的较大者，也就是说，$m_1$ 的值为 3，而当前元素为 2，因此 $m_2$ 依然等于 3，$m_3$ 将被赋值为之前计算出的最大值与当前元素中的较大者，也就是说，之前的最大者为 3，而当前元素为 5，因此，$m_3$ 最终等于 5，接下来，$m_4$ 将被赋值为之前计算出的最大值，与当前元素中的较大者，因此 $m_4$ 依然等于 5，通过这种方式，我们就能计算出整个序列中的最大值元素，因此，在第四次迭代时，无论输入数组的具体内容是什么，我们都能得到全局的最大值，

好的，在我们计算出最大值（已知为 5）之后，就可以计算归一化因子了，那么，我们就从 $l_0$ 开始吧，$l_0$ 的初始值为 0，$l_1$ 则等于 $l_0$ 加上当前元素的指数值，也就是说，3 减去我们在上一个 for 循环中找到的最大值 5，接着，$l_2$ 将等于 $l_1$ 加上当前元素的指数值，即 2 减去最大值，然后 $l_3$ 将等于 $l_2$ 加上当前元素的指数值，5 减去 5，接着 $l_4$ 将等于 $l_3$ 加上 1 减去 5 的指数值，如果展开这个 $l$，它基本上就等于 e 的三次方减 5，加上 e 的二次方减 5，再加上 e 的五次方减 5，最后加上 e 的一次方减 5，计算出归一化因子后，我们可以用它来归一化输入向量中的每个元素，也就是说，新的 $x_1$（记为$x_1'$）将等于 e 的第一个元素 3 减去 5 的幂，再除以我们在前一个 for 循环中计算得到的 $l$，即第四次迭代时的 $l$，新的 $x_2$，因此，$x_2'$ 将等于 e 的 2 减去 5 的幂除以 $l_4$，而 $x_3'$ 则等于 e 的 5 减去 5 的幂除以 $l_4$，以此类推，等等，对所有元素都如此操作，我知道这看起来非常简单，但浙江队我们后续的工作大有帮助，因此，在这个 for 循环中，我们需要遍历向量三次，第一次是为了计算寻找最大值，第二次是为了累加分母，第三次是为了分别计算分子，我们不能打乱这个顺序执行，因为要计算这个 for 循环，必须先知道最大值，因为这里需要用到它，同样，只有在完成前一个 for 循环后，才能计算下一个，因为归一化因子是必要的条件，然而，我们坚持尝试将前两个操作融合到一个 for 循环中，这意味着我们只需要遍历数组一次，同时计算 $m_i$，并在同一迭代中尝试计算 $l_j$，当然，我们无法在此刻计算 $l_j$，因为还未遍历完整个数组，也就无法得知全局最大值，不过，我们可以尝试使用当前已知的局部最大值作为估算值来进行计算，于是，我们尝试用 $m_i$ 替代 $m_n$，也就是用目前计算出的局部最大值来进行估算，如果我们以这种融合的方式对向量应用 softmax，迭代过程如下：

这是我们的数组或向量，第一步是计算 $m_i$，因此，$m_1$ 将等于之前的最大值（即负无穷）与当前元素中的较大值，负无穷与当前元素中的较大值等于 3，$l_1$ 将等于之前的 $l$（即 $l_0$，初始值为 0），加上 e 的（当前元素减去 $m_i$）次方，虽然我们应该使用全局最大值，但目前无法获得，因此暂时使用当前已知的最大值 3，在第二次迭代时，我们处理向量中的这个元素，并计算当前的最大值，因此，当前的最大值是之前最大值与当前元素中的较大者，于是，之前最大值与当前元素中的较大值，也就是 3 和 2 之间的较大值，结果是 3，归一化因子等于之前的归一化因子加上 e 的 2 减 3 次方，即当前元素减去当前最大值的结果，如果数组仅由这两个元素（3 和 2）组成，那么我们计算的结果实际上是正确的，因为找到的最大值 3 就是全局最大值，我们计算的归一化因子也是正确的，因为每个指数值都是基于全局最大值计算的，第一个元素的计算使用了 3 作为参考（即减去 3），第二个元素的计算参数中也减去了 3，而这个 3 正是向量的全局最大值，然而，当我们进行到第三次迭代时，最大值会发生变化，这也会导致我们的归一化因子出错，因为我们处理到第三个元素，也就是这里的 5，然后重新计算最大值，最大值通过比较前一个最大值和当前元素得出，因此新的最大值变为 5，归一化因子则是前一个归一化因子 $l_2$，加上当前元素减去当前估计的最大值（即 5）的指数结果，然而，如果你看这个 $l_3$，它其实是错误的，为什么，因为 $l_3$ 等于，如果展开这个求和式，它等于 e 的 3 减 3 次方加上 e 的 2 减 3 次方再加上 e 的  5 减 5 次方，这里的指数计算使用了 5 作为全局最大值，前面的指数计算使用了 3 作为全局最大值，所以前两个元素在计算时是以 3 作为全局最大值来处理的，但实际上我们后来发现了一个更优的全局最大值，也就是 5，这导致了这个归一化因子是错误的，然而，我们能够在第三地迭代时修正前两次迭代中已经计算出的归一化结果呢？事实上，我们是可以做的，

因为如果我们将其展开，就像这里展示的那样，我们已经完成了展开，这里我们需要的是减去 5，因为这才是目前为止我们找到的全局最大值，而不是之前迭代中使用的 3，因此，我们这里也需要修正这一点，将减去 3 替换为减去 5，我们该如何实现这一点呢？其实，如果我们用校正因子分别乘以这两项，从而将新的最大值巧妙的引入到指数计算中，问题就迎刃而解了，实际上，这个校正因子非常容易计算，在第三次迭代时，如果我们用 $l_2$，即之前计算得到的归一化因子，乘以这个因子，它是之前估计的最大值，与当前估计的最大值（即 5）之差的指数函数，根据指数函数的性质，这里就会变成 e 的 3 减 3 加 3 减 5 次方，这样一来，这个负 3 就会与这个正 3 相互抵消，同样，第二个因子中的这个 3 也会与这个负 3 相互抵消，于是，他们将分别变为 e 的 3 减 5 次方和 e 的 2 减 5 次方，这正是我们想要的结果，因为在第三次迭代时，我们实际上应该使用负 5 作为当前数组中的最大值，所以本质上，我们发现了一种方法，可以在遍历数组的过程中，当发现比当前更好的最大值时，修正之前计算得到的归一化因子，而当无需修正时，这个公式依然适用，因为我们在这里引入的校正因子（即这个表达式）起到了修正的作用，这个校正因子其实就是之前估计的最大值，与当前迭代中，当前估计的最大值（即当前最大值）之差，也就是说，这里的是前一次迭代中的最大值 $m_{i-1}$，而这是当前迭代中的当前最大值 $m_i$，

基本上，当我们处理到最后一个元素时，会发现最大值没有变化，因为我们将之前记录的最大值与当前元素进行比较，而当前元素小于之前记录的最大值，所以最大值保持不变，因此，我们无需进行任何修正，因为之前的 $l_3$（即之前计算得到的归一化因子）已经正确，因为整个过程都是用了负 5 作为最大值，因此，当我们无需进行任何修正时，只需将结果乘以 e 的前一次最大值与当前最大值之差的幂次方，而在这个例子中，这个差值恰好为零，所以 e 的幂次方就是 e 的零次方，所以实际上并没有修正任何内容，因此，我们找到了一种方法，可以在遍历数组的过程中修正之前计算得到的归一化因子，即使当前迭代尚未得到全局最大值，这样一来，每当最大值发生变化时，我们都能进行修正，而当最大值不变时，只需乘以 e 的零次方，这相当于乘以 1，对结果没有影响，因此，我们为 softmax 找到的新算法如下：

首先，我们将 $m_0$ 初始化为负无穷，接着，我们将 $l_0$ 初始化为零，我们遍历数组，计算局部最大值，即从第 0 个元素到当前迭代的第 i 个元素之间的最大值，而之前计算的 $l_i$ 可以通过这个修正因子进行修正，修正因子为 e 的前一次最大值与当前最大值之差的幂次方，再加上当前元素的值减去当前估计的最大值的指数，通过这种方式，我们只需要遍历数组一次，就能同时得到两个值，全局最大值和归一化因子，然后，我们就可以利用这些值来计算 softmax 了，这样一来，我们将原本需要三次遍历数组的过程优化为仅需要两次遍历，这一点至关重要，我们稍后将看到如何实际运用它来推导出 Flash Attention，

到目前为止，我举的这个例子并不能完全证明我们的算法在所有情况下都有效，因为我们只是用了一个由 4 个元素组成的向量作为简单示例，那么我们的新算法是否适用于所有情况，无论数值如何变化呢？这需要我们进行证明，因此，我们将通过数学归纳法来证明这一点。

那么，首先我们要证明的是什么呢？如你所见，我们已经将前两个 for 循环融合成了一个 for 循环，我们期望的是，在这个 for 循环结束时，$m_n$ 也就是最后一次迭代时的 $m$，实际上就是向量中的全局最大值，而 $l_n$ 最后一次迭代时的 $l$ 将等于向量中所有元素减去全局最大值后的指数和。我们需要对此进行证明，因为之前我所做的只是一个示例，并不能算作严格的证明，我们将通过数学归纳法来证明这一点，这是证明这类定理的典型方法，

现在，归纳法的证明基本遵循以下步骤，我们需要证明我们的算法在基础情况下是有效的，假设当 n 等于 1 时，然后我们假设算法在 n 的情况下是有效的，接着需要证明它在 n 加 1 的情况下也同样有效，如果这一条件成立，那么我们就证明了我们的算法对于所有可能得 n 都是有效的，因为它已经在基础情况下得到了验证，例如，当 n 等于 1 时成立，然后通过归纳步骤我们可以说，如果它对 n 有效，那么它对 n 加 1 也有效，这意味着它对 2 也有效，接着，如果它对 2 有效，那么由于我们将证明的归纳步骤，它对 3 也应该有效，如果它对 3 有效，那么它对 4 也有效，以此类推，直到无穷大，让我们从基础情况开始证明，即当 n 等于 1 时，这非常简单，当 n 等于 1 时，这个 for 循环只会执行一次迭代，因此只涉及 $m_1$ 和 $l_1$，$m_1$ 将是前一个 $m$ 的最大值，即负无穷大，因为我们初始化 $m_0$ 为负无穷大，因此，$m_1$ 将等于前一个 $m$ （即负无穷大）与当前元素 $x_1$ 的最大值，所以 $m_1$ 将等于 $x_1$，无论 $x_1$ 是什么值，$x_1$ 通常不会等于负无穷大，$x_1$ 不可能等于负无穷大，因为它是一个固定表示形式的数值，因此，$x_1$ 不可能是负无穷大，所以最终 $m_1$ 的值就是 $x_1$，

因此，由于当前只有一个元素 $x_1$，$m_1$ 既是这个元素的 $m$ 值，也是这个 for 循环中最后一个 $m$ 值，$m_1$ 将等于仅由一个元素组成的向量的全局最大值，而 $l_1$ 将等于前一个 $l$ 值，我们初始化为 0，因此 $l_0$ 乘以一个修正因子，在这个情况下是 e 的负无穷大次方，因为修正因子是前一次估计的最大值与当前估计的最大值之差，但是前一次估计的最大值是负无穷大，减去 $x_1$ 等于负无穷大，所以这一项是没问题的，它会相互抵消，然后加上 e 的 $x_1$ 次方，减去当前的最大值 $x_1$，也就是 $m_1$，如果这一项等于向量中所有元素的和，而向量仅由一个元素组成，减去数组中的最大值 $x_1$，因此，我们已经证明了当 n 等于 1 时，这个方法是成立的，现在，我们假设对于 n 的情况，这个方式是成立的，那么，对于大小为 n 加 1 的向量数组，这个方法是否仍然适用呢？

那么，让我们来看看在第 n 加 1 次迭代时会发生什么，在第 n 加 1 次迭代时，我们将计算之前 m 的估计值（即第 n 次迭代时的 m）与当前元素 $x_n$ 加 1 的最大值，这是根据最大值函数的性质得出的，实际上，这等于全局向量直到 n 加 1 的最大值，因为最大值函数会在之前的估计值和当前的估计值之间选择较大的那个，而 $l_n$ 加 1 是归一化因子，在第 n 加 1 次迭代时，$l_n$ 加 1 将等于 $l_n$，因此，前一个估计值（实际上是前一个归一化因子）在迭代结束时，乘以校正因子，该校正因子是前一个最大值减去当前最大值，再加上当前元素 x 减去当前最大值估计值的指数，但最终结果仍然是 $l_n$，我们假设这个性质，即这个算法，在 n 次迭代时是成立的，因此 $l_n$ 肯定等于向量中前 n 个元素的指数之和，减去向量中的前 n 个元素的局部最大值，即 $m_n$，如果有需要校正的地方，我们乘以校正因子，该因子由前一个最大值减去当前最大值，再加上当前元素的指数减去当前最大值的估计值组成，接下来，通过指数函数的性质，因此，我们可以将其带入求和公式中，并会发现这个 $m_n$ 和这个 $m_n$ 会相互抵消，因为最终表达式会使 $x_j$ 减去 $m_n$ 的指数，再加上 $m_n$ 减去 $m_n$ 加 1，所以这个 mn 和这个 mn 会相互抵消，我们最终得到这个结果加上这里的这个因子保持不变。

然而，你可以看到，这里的内容正好是迭代 n 加 1 时求和项的精确参数，所以这个表达式就是 e 的 xj 次方，其中 j 从 1 到 n，减去 mn 加 1，再加上 e 的 x，n 加 1 减去 m，n 加 1 次方，因此，j 仅出现在这里，其最大值为 n，而这类似于将 j 替换为 n 加 1 的情况，因此，我们可以将这个求和的索引增加 1，结果将保持不变，最终得到的求和结果也相同，因此，我们已经证明，在 n 加 1 迭代时，$l$ 仍然等于数组中所有元素的指数之和，即数组中从第 1 个元素到第 n 加 1 个元素的指数减去这些元素中的最大值，于是，我们已经证明了，如果这个方法对 n 成立，于是它对 n 加 1 也同样适用，这足以证明该方法适用于任意大小的数组，

如果你没完全理解这个归纳证明，也不用担心，如果你是第一次接触这类证明，可能需要一些时间才能完全掌握，如果你想进一步了解归纳证明，我建议你观看一些其他的证明过程，其实归纳证明很简单，只需要调整到正确的思维方式，

好了，我们继续往下讲，好的，我们接下来讨论分块矩阵乘法，我知道你可能想直接跳到代码部分，我们稍后就会讲到那里，实际上我们还需要补充一点理论背景，假设我们正在进行矩阵乘法运算，我们有一个矩阵 A，想要将其与矩阵 B 相乘，结果会生成输出矩阵 C，假设第一个矩阵的维度是 M 乘以 K，第二个矩阵的维度是 K 乘以 N，结果将生成一个 M 乘以 N 的输出矩阵，现在，假设我们希望并行化计算这个输出矩阵，我知道我还没提到 GPU，所以这里我们先不讨论 GPU，我们将讨论在多核 CPU 情况下的并行化，这应该是您非常熟悉的，因为如今在购买电脑时，通常会选择 CPU，而 CPU 又分为单核和多核，比如双核、四核、八核等，这些核心实际上就像是 CPU 内部的小 CPU 能够并行执行操作，

如何并行化矩阵乘法呢？假设你需要并行化这个矩阵乘法，矩阵 C 中的每个输出元素，都是矩阵 A 的一行与矩阵 B 的一列的点积，例如，左上角的这个元素是矩阵 A 的第一行与矩阵 B 的第一列的点积结果，而矩阵 C 右上角的这个元素，则是矩阵 A 的第一行与矩阵 B 最后一列的点积，左下角这个元素是矩阵 A 的最后一行与矩阵 B 的第一列的点积，以此类推，其他所有元素的计算方式也是如此，现在，为了并行化这一计算，如果我们希望完全并行化，就需要与矩阵 C 中元素数量相当的核心数，因此，如果 m 和 n 的值很小，或许我们现有的核心数量就足以应对，但设想一下，当 m 和 n 的数值相当大时，假设矩阵的规模达到 100 乘以 100，目前我们的 CPU 并未配备 1 万个核心，那么，如何在核心数少于矩阵元素数量的情况下，实现矩阵运算的并行化呢？

这便是我们引入分块矩阵乘法概念的时候了，简而言之，分块矩阵乘法指的是将原始矩阵划分为更小的元素块，然后在这些块之间进行矩阵乘法的运算，例如，设想我们有一个 8 行 4 列的矩阵，这意味着它有 8 行和 4 列，总共包含 32 个元素，然后我们将它与另一个 4 行 8 列的矩阵相乘，也就是说这个矩阵有 4 行和 8 列，因此，它同样包含 32 个元素，最终的输出矩阵将包含 64 个元素，我们的处理器并没有 64 个核心，那么，我们该如何实现并行化呢，假设我们仅有 8 个核心可用，现在，利用这 8 个核心，我们可以将原始矩阵 A 划分为四个区块，其中第一个区块位于左上角，包含 4 行 2 列的元素，那么，该如何表达呢？左上角有 8 个元素，接着是矩阵右上角的 8 个元素，然后是左下角的 8 个元素，最后是右下角的 8 个元素，这样就形成了四个区块，随后，我们将矩阵 B 也划分为八个区块，每个区块由四个元素构成，因此 B11 代表原始矩阵中左上角的四个元素，而前面提到的则是原始矩阵中右上角的四个元素，这个 B21 指的是位于矩阵左下角的四个元素，以此类推，等等，我们如何进行这种分块矩阵乘法呢？

我们可以将这些矩阵视为仅由其分块组成的结构，因此这里可以将这个矩阵看作仅由各个分块构成，我们可以将这个矩阵看作仅由其分块组成的结构，这种乘法的输出结果将是一个矩阵，其计算方式与原始矩阵相同，但每个点积的输出不再是输出矩阵的单个元素，而是输出矩阵的一个元素块，例如，这里的左上角分块是这个矩阵的第一行与这个矩阵的第一列的点积，其计算方式如下：它将由 a11 乘以 b11 加上 a12 乘以 b21 得到，而这个输出结果不再是一个单一标量，而是，嗯，让我数一下，它应该是由八个元素组成，所以它应该是由四个元素组成，它应该是一个由四个或八个元素组成的块，让我实际数一下，因为我们有八个分块，所以它应该由八个元素组成，我们可以在这里看到这一点，如何确定这个输出块的维度？

我们可以查看 A11 是什么，A11 是一个 4 行 2 列的矩阵，因此它由 8 个元素组成，这些元素分布在 4 行 2 列的小矩阵中，我们将其与 B11 相乘，B11 是一个比原矩阵更小的 2 行 2 列矩阵，因此包含 4 个元素，因此，当我们将 4 行 2 列的矩阵与 2 行 2 列的矩阵相乘时，会得到一个 4 行 2 列的输出块矩阵，所以，如果我们逐块进行这个计算，它将生成原始矩阵的一个输出元素块，因此，不是单个标量，而是一个输出块，这使得并行化变得的非常容易，因为，如果我们只有八个核心，我们可以将每个输出块分配给一个核心，每个核心不会生成原始矩阵的一个输出元素，而是会生成原始矩阵的八个元素，形成一个 4 行 2 列的矩阵，

嗯，简单来说，块矩阵允许我们以两种方式进行矩阵乘法：一种是逐元素相乘，就像在原矩阵中那样，每一行与每一列对应相乘；另一种是逐块相乘，就像我们进行普通矩阵乘法一样，因为我们在块之间进行的矩阵乘法与我们在原矩阵上进行的矩阵乘法方式相同，只是它生成的不是一个标量，而是一个块

现在，让我们来看看为什么这一点对我们来说非常重要，那么我们为什么要关注矩阵乘法呢？因为我们正试图计算以下操作，具体来说，就是将查询矩阵与键矩阵的转置相乘，然后对这个操作的结果应用 softmax 函数，最后将 softmax 的输出与值矩阵相乘，

目前我们先暂时忽略 softmax 的部分，假设我们不打算应用任何 softmax 函数，因此，我们取查询矩阵与键矩阵转置相乘的结果，直接将其与值矩阵 V 相乘，得到注意力机制的输出，当然，这种做法是不正确的，但它简化了我们接下来要处理的内容，

所以就目前而言，我们暂且假设不需要应用任务 softmax 函数，于是，我们只需要将查询矩阵与键矩阵的转置相乘，然后直接将这个操作的结果与值矩阵 V 相乘，这样得到的结果将是一个 nxd 的矩阵，其中 n 代表 token 的数量，每个 token 由一个 d 维（小写 d 表示维度）的嵌入向量构成，我们知道 q,k,v 本身也是 nxd 维的矩阵，也就是说，这些 n 个 token 是由 d 维的嵌入向量组成的，想象一下，我们有一个 8x128的查询矩阵，以及同样大小的键矩阵和值矩阵，这意味着我们有 8 个 token，每个 token 由 128 个维度组成，正如我们所看到的，在进行矩阵乘法计算时，我们可以将矩阵进行分块处理，我们可以将矩阵划分成多个块，如何选择分块方式完全取决于我们，只要它们能够正常运算即可，在进行矩阵乘法时，确保分块的形状能够相互匹配即可。例如，在前面的例子中，我们将矩阵 A 划分为块，使得块矩阵（即仅由这些块组成的矩阵）的形状与块矩阵 B 兼容，从而确保这种操作能够顺利进行，因此，这是我们在进行块矩阵乘法时唯一需要注意的要求，块矩阵（即仅由块组成的矩阵）的形状在矩阵乘法中应相互匹配，除此之外，具体如何划分并不重要，想象一下，我们选择将查询矩阵按行划分为块，这是完全可以做的，我们并不一定也需要按列进行划分，我们可以仅按行划分，使得每个 Q 不再是单行，而是由两行组成的一个组，因此，Q1 是 Q 序列中前两行组成的组，Q2 则是 Q 序列中接下来的两行组成的组，以此类推，我们对 V 也采取同样的划分方式，对于 K，我们不做这样的划分，因为实际上我们要与 K 的转置相乘，所以直接在 K 的转置上进行这种细分，因此，我们有被划分为若干行组的 Q，以及 K 的转置，这是一个 128 行 8 列的矩阵，因为它是 8 行 128 列的键矩阵的转置，我们决定将 K 转置矩阵的每一列组划分为一个单独的块，因此 k1 是 K 转置矩阵的前两列，k2 则是 K 转置矩阵中接下来的两列组成的组，以此类推，直到 k4，即 k 转置矩阵中最后两列，我们首先进行的操作是查询，与键转置的乘法，这基本上意味着我们需要将每个查询与所有键相乘，接着是第二个查询与所有键相乘，以此类推。