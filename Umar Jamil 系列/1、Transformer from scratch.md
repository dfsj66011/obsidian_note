
在匹配，而且大多数都匹配，实际上，我们也可以说它，几乎过度拟合了这个特定数据，但这就是 transformer 的力量，我没有训练它很多天，我只训练了几个小时，如果我没记错的话，结果真的非常好，

现在让我们制作一个笔记本，用来可视化这个预训练模型的注意力，根据我们之前构建的文件，所以，trainpi 你也可以选择你喜欢的语言来训练你自己的模型，我强烈建议你改变语言，看看模型的表现如何，并尝试诊断为什么模型表现不佳，如果表现不好或表现良好，尝试理解如何进一步改进它，所以让我们尝试可视化注意力，

所以让我们创建一个新的笔记本，让我们称之为，比如说，注意力可视化，好的，我们做的第一件事是导入所有需要的库，我还将使用一个叫做 Altair 的库，这是一个用于图表的可视化库，它实际上与深度学习无关，它只是一个可视化功能，特别是可视化功能，实际上，我在网上找到的，不是我写的，就像大多数可视化功能一样，如果你想构建一个图表或直方图，你可以很容易地在互联网上找到，所以我主要使用这个库，因为我从网上复制了代码来可视化它，但其余的都是我自己的代码，

所以让我们导入它，好的，让我们导入所有这些，当然，当你在电脑上运行代码时，你需要安装这个特定的库，我们还要定义设备，你可以直接从这里复制代码，然后我们加载模型，我们可以像这样从这里复制，好的，让我们粘贴到这里，这个变成了词汇源和词汇目标，

好的，现在让我们创建一个加载批次的功能，我现在将使用 tokenizer 将批次转换为标记，当然，对于解码器，我们使用目标词汇，即目标 tokenizer，所以让我们使用我们的贪心解码算法进行推理，所以我们提供模型，我们返回所有这些信息，

好的，现在我将构建必要的功能来可视化注意力，我将从另一个文件复制一些函数，因为实际上我们要构建的内容从学习角度来看并不有趣，就深度学习而言，它主要是用于可视化数据的函数，所以我会复制它，因为写起来相当长，当然，我会解释其中的关键部分，

这就是那个函数，好的，这个函数是做什么的呢？基本上，我们会从编码器那里得到注意力，如何从编码器获取注意力，例如，我们在三个位置有注意力，第一个在编码器中，第二个在解码器的开始，即解码器的自注意力，然后我们有编码器和解码器之间的交叉注意力，所以我们可以可视化三种注意力，如何获取关于注意力的信息，我们加载模型，我们有编码器，我们选择要从哪个层获取注意力，然后从每个层我们可以获取自注意力块及其注意力分数，这个变量从哪里来？

如果你记得当我们在这里定义注意力计算时，当我们计算注意力时，我们不仅返回输出到下一层，我们还给出这个注意力分数，它是 softmax 的输出，我们将其保存在这个变量中，self.attentionScores，现在我们可以直接获取并可视化它，所以这个函数将根据我们想要从哪一层和哪个头获取注意力，选择正确的矩阵，这个函数构建一个数据框来可视化信息，即从该矩阵中提取的标记和分数，

在这里，我们将从这个矩阵中提取行和列，然后我们还会构建图表，图表是用 Altair 构建的，实际上我们要做的是获取所有头的注意力，我构建了这个方法来获取我们作为输入传递给这个函数的所有层和所有头的注意力，所以现在让我运行这个单元格，

好的，让我们创建一个新单元格，然后运行它，好的，首先我们想要可视化我们正在处理的句子，即批次，其他输入标记，所以我们加载一个批次，然后我们可视化源和目标，还有目标，最后我们还计算了长度，

长度是什么？好的，基本上是所有在填充字符之前的字符，即第一个填充字符的出现，因为这是从数据集中提取的批次，已经是为训练构建的张量，所以它们已经包含了填充，在我们的例子中，我们只想获取句子中实际字符的数量，所以这个，我们可以，句子中实际单词的数量，所以我们可以检查填充之前的单词数量，所以让我们运行这个，这里有一些问题，我忘了这个函数是错误的，所以现在应该可以了，这个句子太短了，让我们找一个更长的，好的，让我检查一下质量，你不能保持现状，尤其是你，我们不能，你们不能继续这样，尤其是现在，好的，看起来不错，好的，让我们打印第 0、1、2 层的注意力，因为我们有六层，如果你记得，参数 n 等于 6，所以我们只可视化三层，并且我们会可视化所有头，每层有八个头，所以头编号：0、1、2、3、4、5、6 和 7

好的，首先让我们可视化编码器自注意力，我们确实得到了所有注意力图，我们想要哪一个，所以编码器的一个，我们想要这些层和这些头，原始标记是什么，编码器输入标记，我们在列中想要什么？因为我们打算构建一个网格，所以如你所知，注意力是一个将行与列相关联的网格，在我们的例子中，我们谈论的是编码器的自注意力，所以这是同一个句子在自我关注，所以我们需要在行和列上都提供编码器的输入句子，我们想要可视化的最大长度是多少？

好的，假设我们想要可视化不超过 20 个，所以是 20 和句子长度的最小值，好的，这是我们的可视化结果，我们可以看到，正如我们所预期的，实际上，当我们可视化注意力时，我们预期对角线上的值会很高，因为这是每个标记与其自身的点积，我们还可以看到其他有趣的关系，

例如，假设句子开始标记和句子结束标记，至少对于头 0 和层0，它们与其他词没有关联，就像我实际预期的那样，但其他头确实学习了一些非常小的映射，我们可以看到，如果我们悬停在每个网格单元格上，我们可以看到自注意力的实际值，例如自注意力的分数，我们可以看到这里的注意力非常强，所以 "especially" 和 "specially“ 这两个词是相关的，所以这是同一个词与自身的关联，但还有 "especially” 和 “now”，我们可以为所有层可视化这种注意力，

所以因为每个头会观察每个词的不同方面，因为我们均匀地分配了词嵌入到各个头，所以每个头会看到词嵌入的不同部分，我们也希望它们学习不同类型的词间映射一一这实际上是事实一一以及层与层之间的不同映射，我们还有不同的 Q、W、K 和 WV 矩阵，所以它们也应该学习不同的关系，

现在我们可能还想可视化解码器的注意力，所以让我们来做这个，让我复制代码并更改参数，好的，这里我们想要解码器 1，我们想要相同的层等，但行和列上的标记将是解码器标记，所以解码器输入标记和解码器输入标记，让我们可视化一一而且我们现在应该看到意大利语，因为我们使用的是解码器自注意力，确实是这样，所以在这里我们看到解码器端不同的注意力类型，而且我们还有多个头，它们应该学习不同的映射，以及不同的层应该学习词之间的不同映射，我发现最有意思的是交叉注意力，

所以让我们来看一下，好的，让我复制代码并再次运行它，好的，所以，如果你记得这个方法，它是编码器、解码器，相同的层，所以在这里，行上我们将显示编码器输入，列上我们将显示解码器输入标记，因为这是编码器和解码器之间的交叉注意力，好的，这就是编码器和解码器之间交互的大致方式以及它是如何发生的，所以这就是我们找到交叉注意力的地方，它是使用来自编码器的键和值计算的，而查询来自解码器，所以这实际上是翻译任务发生的地方，这就是模型如何学习将这两句话相互关联，以实际计算翻译。

所以我邀请你们自己运行代码，我给你们的第一个建议是，和我一起看视频并编写代码，你可以暂停视频，自己编写、运行代码，好的，让我给你们一 些实际的例子，例如，当我编写模型代码时，我建议你先看我编写一个特定层的代码，然后暂停视频，自己编写，花点时间，不要马上看解决方案，尝试找出哪里出错了，如果你真的在一两分钟后还是无法找出问题所在，你可以快速看一下视频，但尽量自己解决，当然，有些事情你无法自己想出来，所以，例如，对于位置编码和所有这些计算，这基本上只是公式的应用，但重点是，你至少应该能够自己构思出一个结构，所以所有层是如何相互作用的，这是我的第一个建议，而在训练循环方面，训练部分实际上是非常标准的，所以它与其他你可能见过的训练循环非常相似，有趣的部分是我们如何计算损失，以及我们如何使用 Transformer 模型，最后真正重要的是我们如何进行模型推理，这在贪心代码中，

所以感谢大家观看视频并陪伴我这么长时间，我可以保证这是值得的，我希望在接下来的视频中，能展示更多我熟悉的 Transformer 和其他模型的例子，并且我也想和你们一起探索，所以如果有什么你不明白的或者你想让我解释得更清楚的，请告诉我，我也一定会关注评论区的，请给我留言，谢谢, 祝你有个美好的一天，