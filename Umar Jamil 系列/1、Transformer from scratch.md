
的是一个验证循环，它将允许我们评估模型，这也意味着我们想从这个模型中推理并检查一些样本句子，看看它们是如何被翻译的：所以让我们开始构建验证循环，

我们做的第一件事是构建个名为 runValidation 的新方法，这个方法将接受一些我们将使用的参数，现在，我只是把它们都写下来，稍后我会解释它们将如何使用，好的，我们运行验证的第一件事是将我们的模型设置为评估模式，所以我们执行 model.eval, 这意味着这告诉 PyTorch 我们将要评估我们的模型，然后我们将推理两个句子，看看模型的输出是什么，所以通过 torch.NoGrad 我们禁用了在这个宽度块内运行的每个张量的梯度计算，而这正是我们想要的，

我们只是想从模型中推理，我们不希望在这个循环中训练它，所以让我们从验证数据集中获取一个批次，因为我们只想推理两个，所以我们记录我们已经处理了多少个，并从这个当前批次中获取输入，我想提醒你，对于验证 ds，我们只有一个批次大小，这是编码器输入，我们也可以获取编码器掩码，

让我们确认一下批次的实际大小确实是一个，现在让我们进入有趣的部分，所以，如你所记得的，当我们想要推理模型时，我们需要只计算一次编码器输出，并将其重用于我们将要处理的每个 token，模型将从解码器输出，所以让我们创建另一个函数，它将在我们的模型上运行贪婪解码，我们将看到它只会运行一次编码器，所以让我们称这个函数为 greedy_decode，

好的，让我们创建一些我们将需要的 token，所以 Sos token，即句子的开始，我们可以从任一tokenizer 中获取，无论是目标还是源，它们都有，eos 好的，然后我们做的是我们预先计算编码器输出，并将其重用于我们从解码器获得的每个 token，所以我们只给出源和源掩码，即编码器输入和编码器掩码，我们也可以称之为编码器输入和编码器掩码，然后我们，好的，我们如何进行推理？我们做的第一件事是给解码器句子的开始 token，以便解码器将输出翻译句子的第一个 token，然后在每次迭代中，就像我在幻灯片中展示的那样，每次迭代我们都将前个 token 添加到解码器输入中，以便解码器可以输出下一个 token，然后我们取下一个 token，再次将其放在解码器输入的前面，并获取后续的 token，所以让我们为第一次送代构建一个解码器输入，它只包含句子的开始 token，

我们用句子的开始 token 填充这个，它与编码器输入具有相同类型，现在，我们将不断要求解码器输出下一个 token，直到我们达到句子的结束 token 或我们在这里定义的最大长度，所以我们可以做个 while True 循环，然后我们的第一个停止条件是，如果解码器输出，即下一步的输入大于或达到最大长度，这里，为什么我们有两个维度？一个是用于批次，另一个是用于解码器输入的 token，现在我们也需要为此创建一个掩码，我们可以使用我们的函数 causal mask 来表示我们不希望输入看到未来的词，我们不需要另一个掩码，因为如你所见，这里没有任何填充 token，现在我们计算输出。

我们为循环的每一次送代重用编码器的输出，我们重用源掩码，即编码器的输入和掩码，然后我们提供解码器输入，以及它的掩码，即解码器掩码，然后我们得到下一个 token，所以我们使用投影层获取下一个 token 的概率，但我们只想要最后一个 token 的投影，即我们在编码器之后给出的最后一个 token 的下一个 token，现在我们可以使用 max，这样我们就能得到具有最大概率的 token，这就是贪婪搜索，

然后我们得到这个词，并将其追加回来，因为它将成为下一次送代的输入，我们进行连接，所以我们取解码器输入并追加下个 token，所以我们创建另一个张量，应该是正确的，好的，如果下个词或 token 等于句子的结束 token，那么我们也停止循环，这就是我们的贪婪搜索，

现在我们可以直接返回输出，所以输出基本上就是解码器输入，因为你每次都将其追加下一个 token，并且我们移除了批次维度，所以进行了压缩，这就是我们的贪婪解码.

现在我们可以在这个函数中使用它，所以在验证函数中，我们终于可以得到模型输出，等于贪婪解码，我们给它所有参数，然后我们想要将这个模型输出与我们预期的结果进行比较，即与标签进行比较，所以让我们把这些全部追加进去， 所以我们给输入的，我们给了模型，模型的输出，即预测的输出，以及我们预期的输出，我们将所有这些保存在这些列表中，然后在循环结束时，我们将在控制台上打印它们，为了获取模型输出的文本，我们需要再次使用 tokenizer 将 token 转换回文本，当然，我们使用目标 tokenizer，因为这是目标语言，

好的，现在我们将所有这些保存到各自的列表中，我们也可以在控制台上打印出来，为什么我们使用这个名为 printMessage 的函数，而不是直接使用 Python 的 print？

因为我们在主循环，即训练循环中使用这里，TKODM，这是一个非常漂亮的进度条，但在进度条运行时不建议直接在控制台上打印，所以在控制台上打印，TKODM 提供了一个名为 print 的方法，我们将这个方法传递给这个函数，以确保输出不会干扰进度部分的打印，我们打印一些条形图，然后打印所有消息，如果我们已经处理了一定数量的示例，那么我们就中断，

那么为什么我们要创建这些列表呢？嗯，实际上我们也可以将所有这些发送到  tensorboard，所以我们可以，例如，如果我们启用了 tensorboard，我们可以将所有这些发送到 tensorboard，为此，实际上我们需要另一个库，它允许我们计算一些指标，我想我们可以跳过这部分，但如果你真的很感兴趣，我在我发布在 GitHub 上的代码中，你会发现我使用了这个名为 torchmatrix 的库，它允许计算字符错误率、BLEU指标，这对翻译任务非常有用，还有词错误率，所以如果你真的感兴趣，你可以在 GitHub 上找到代码，但对于我们的演示，我认为这并不必要，所以，实际上我们可以移除它，因为我们没有做这部分。

好的，现在我们有了 runValidation 方法，我们可以直接调用它，我通常的做法是在每隔几步后运行验证，但由于我们希望尽快看到结果，我们将首先在每次送代时运行它，并且我们还把这个 modeltrain 放在这个循环中，以便每次运行验证后，模型都会回到训练模式，所以现在我们可以直接运行验证，并给它所有运行验证所需的参数，给它模型，好的，用于打印消息，我们在打印任何消息吗？是的，我们在打印，所以让我们创建一个 lambda，我们只需做，这是要写的消息与 TQDM，然后我们需要给出全局步骤和 writer，我们不会使用它，但好的，现在我想我们可以再次运行训练，看看验证是否有效，好的，看起来它在工作，所以模型没问题，它在每一步都运行验证，这完全不是我们想要的，但至少我们知道贪婪搜索在工作，而且它不是，至少看起来它在工作，模型没有预测任何有用的东西，实际上它只是在预测一堆逗号，因为它完全没有训练，但如果我们训练模型一段时间后，

我们应该会看到在几个 epoch 之后，模型会变得越来越好，所以让我们停止这个训练，并把这个放回它该在的地方，即每个 epoch 的末尾，是的，这个我们可以留在这里，没问题，是的。

我现在将快进到一个已经预训练好的模型，我预先训练了它几个小时，这样我们就可以进行推理并可视化注意力，我已经复制了预先计算好的预训练权重，我还创建了这个笔记本，重用了我们在训练文件中之前定义的函数，代码非常简单，

实际上，我只是从训练文件中复制粘贴了代码，我只是加载模型并运行验证，使用我们刚刚写的方法，然后我在预训练上运行了验证，让我们再运行一次，例如，如你所见，模型正在推理 10 个例子，句子，结果还不错，我的意思是，我们可以看到 11 个微笑，11 个故事，它在匹配，而且大多数都匹配，实际上，我们也可以说它，几乎过度拟合了这个特定数据，但这就是 transformer 的力量，我没有训练它很多天，我只训练了几个小时，如果我没记错的话，结果真的非常好，

现在让我们制作一个笔记本，用来可视化这个预训练模型的注意力，根据我们之前构建的文件，所以，trainpi 你也可以选择你喜欢的语言来训练你自己的模型，我强烈建议你改变语言，看看模型的表现如何，并尝试诊断为什么模型表现不佳，如果表现不好或表现良好，尝试理解如何进一步改进它，所以让我们尝试可视化注意力，

所以让我们创建一个新的笔记本，让我们称之为，比如说，注意力可视化，好的，我们做的第一件事是导入所有需要的库，我还将使用一个叫做 Altair 的库，这是一个用于图表的可视化库，它实际上与深度学习无关，它只是一个可视化功能，特别是可视化功能，实际上，我在网上找到的，不是我写的，就像大多数可视化功能一样，如果你想构建一个图表或直方图，你可以很容易地在互联网上找到，所以我主要使用这个库，因为我从网上复制了代码来可视化它，但其余的都是我自己的代码，

所以让我们导入它，好的，让我们导入所有这些，当然，当你在电脑上运行代码时，你需要安装这个特定的库，我们还要定义设备，你可以直接从这里复制代码，然后我们加载模型，我们可以像这样从这里复制，好的，让我们粘贴到这里，这个变成了词汇源和词汇目标，

好的，现在让我们创建一个加载批次的功能，我现在将使用 tokenizer 将批次转换为标记，当然，对于解码器，我们使用目标词汇，即目标 tokenizer，所以让我们使用我们的贪心解码算法进行推理，所以我们提供模型，我们返回所有这些信息，

好的，现在我将构建必要的功能来可视化注意力，我将从另一个文件复制一些函数，因为实际上我们要构建的内容从学习角度来看并不有趣，就深度学习而言，它主要是用于可视化数据的函数，所以我会复制它，因为写起来相当长，当然，我会解释其中的关键部分，

这就是那个函数，好的，这个函数是做什么的呢？基本上，我们会从编码器那里得到注意力，如何从编码器获取注意力，例如，我们在三个位置有注意力，第一个在编码器中，第二个在解码器的开始，即解码器的自注意力，然后我们有编码器和解码器之间的交叉注意力，所以我们可以可视化三种注意力，如何获取关于注意力的信息，我们加载模型，我们有编码器，我们选择要从哪个层获取注意力，然后从每个层我们可以获取自注意力块及其注意力分数，这个变量从哪里来？

如果你记得当我们在这里定义注意力计算时，当我们计算注意力时，我们不仅返回输出到下一层，我们还给出这个注意力分数，它是 softmax 的输出，我们将其保存在这个变量中，self.attentionScores，现在我们可以直接获取并可视化它，所以这个函数将根据我们想要从哪一层和哪个头获取注意力，选择正确的矩阵，这个函数构建一个数据框来可视化信息，即从该矩阵中提取的标记和分数，

在这里，我们将从这个矩阵中提取行和列，然后我们还会构建图表，图表是用 Altair 构建的，实际上我们要做的是获取所有头的注意力，我构建了这个方法来获取我们作为输入传递给这个函数的所有层和所有头的注意力，所以现在让我运行这个单元格，

好的，让我们创建一个新单元格，然后运行它，好的，首先我们想要可视化我们正在处理的句子，即批次，其他输入标记，所以我们加载一个批次，然后我们可视化源和目标，还有目标，最后我们还计算了长度，

长度是什么？好的，基本上是所有在填充字符之前的字符，即第一个填充字符的出现，因为这是从数据集中提取的批次，已经是为训练构建的张量，所以它们已经包含了填充，在我们的例子中，我们只想获取句子中实际字符的数量，所以这个，我们可以，句子中实际单词的数量，所以我们可以检查填充之前的单词数量，所以让我们运行这个，这里有一些问题，我忘了这个函数是错误的，所以现在应该可以了，这个句子太短了，让我们找一个更长的，好的，让我检查一下质量，你不能保持现状，尤其是你，我们不能，你们不能继续这样，尤其是现在，好的，看起来不错，好的，让我们打印第 0、1、2 层的注意力，因为我们有六层，如果你记得，参数 n 等于 6，所以我们只可视化三层，并且我们会可视化所有头，每层有八个头，所以头编号：0、1、2、3、4、5、6 和 7

好的，首先让我们可视化编码器自注意力，我们确实得到了所有注意力图，我们想要哪一个，所以编码器的一个，我们想要这些层和这些头，原始标记是什么，编码器输入标记，我们在列中想要什么？因为我们打算构建一个网格，所以如你所知，注意力是一个将行与列相关联的网格，在我们的例子中，我们谈论的是编码器的自注意力，所以这是同一个句子在自我关注，所以我们需要在行和列上都提供编码器的输入句子，我们想要可视化的最大长度是多少？

好的，假设我们想要可视化不超过 20 个，所以是 20 和句子长度的最小值，好的，这是我们的可视化结果，我们可以看到，正如我们所预期的，实际上，当我们可视化注意力时，我们预期对角线上的值会很高，因为这是每个标记与其自身的点积，我们还可以看到其他有趣的关系，

例如，假设句子开始标记和句子结束标记，至少对于头 0 和层0，它们与其他词没有关联，就像我实际预期的那样，但其他头确实学习了一些非常小的映射，我们可以看到，如果我们悬停在每个网格单元格上，我们可以看到自注意力的实际值，例如自注意力的分数，我们可以看到这里的注意力非常强，所以 "especially" 和 "specially“ 这两个词是相关的，所以这是同一个词与自身的关联，但还有 "especially” 和 “now”，我们可以为所有层可视化这种注意力，

所以因为每个头会观察每个词的不同方面，因为我们均匀地分配了词嵌入到各个头，所以每个头会看到词嵌入的不同部分，我们也希望它们学习不同类型的词间映射一一这实际上是事实一一以及层与层之间的不同映射，我们还有不同的 Q、W、K 和 WV 矩阵，所以它们也应该学习不同的关系，

现在我们可能还想可视化解码器的注意力，所以让我们来做这个，让我复制代码并更改参数，好的，这里我们想要解码器 1，我们想要相同的层等，但行和列上的标记将是解码器标记，所以解码器输入标记和解码器输入标记，让我们可视化一一而且我们现在应该看到意大利语，因为我们使用的是解码器自注意力，确实是这样，所以在这里我们看到解码器端不同的注意力类型，而且我们还有多个头，它们应该学习不同的映射，以及不同的层应该学习词之间的不同映射，我发现最有意思的是交叉注意力，

所以让我们来看一下，好的，让我复制代码并再次运行它，好的，所以，如果你记得这个方法，它是编码器、解码器，相同的层，所以在这里，行上我们将显示编码器输入，列上我们将显示解码器输入标记，因为这是编码器和解码器之间的交叉注意力，好的，这就是编码器和解码器之间交互的大致方式以及它是如何发生的，所以这就是我们找到交叉注意力的地方，它是使用来自编码器的键和值计算的，而查询来自解码器，所以这实际上是翻译任务发生的地方，这就是模型如何学习将这两句话相互关联，以实际计算翻译。

所以我邀请你们自己运行代码，我给你们的第一个建议是，和我一起看视频并编写代码，你可以暂停视频，自己编写、运行代码，好的，让我给你们一 些实际的例子，例如，当我编写模型代码时，我建议你先看我编写一个特定层的代码，然后暂停视频，自己编写，花点时间，不要马上看解决方案，尝试找出哪里出错了，如果你真的在一两分钟后还是无法找出问题所在，你可以快速看一下视频，但尽量自己解决，当然，有些事情你无法自己想出来，所以，例如，对于位置编码和所有这些计算，这基本上只是公式的应用，但重点是，你至少应该能够自己构思出一个结构，所以所有层是如何相互作用的，这是我的第一个建议，而在训练循环方面，训练部分实际上是非常标准的，所以它与其他你可能见过的训练循环非常相似，有趣的部分是我们如何计算损失，以及我们如何使用 Transformer 模型，最后真正重要的是我们如何进行模型推理，这在贪心代码中，

所以感谢大家观看视频并陪伴我这么长时间，我可以保证这是值得的，我希望在接下来的视频中，能展示更多我熟悉的 Transformer 和其他模型的例子，并且我也想和你们一起探索，所以如果有什么你不明白的或者你想让我解释得更清楚的，请告诉我，我也一定会关注评论区的，请给我留言，谢谢, 祝你有个美好的一天，


---------

## Attention is all you need

大家好，欢迎来到我的视频，今天我们要聊的是 Transformer，这实际上是我 Transformer 系列视频的 2.0 版本，我之前做过一个关于 Transformer 的视频，但音频质量不太好，根据观众的建议，那个视频反响非常好，所以观众建议我改进音频质量，这就是我制作这个视频的原因。你不需要看之前的系列，因为我这次基本上会做同样的内容，但会有一些改进，我会弥补之前的一些错误，并加入一些可以提升的地方，看完这个视频后，我建议你接着看看我另一个关于如何从零开始编写 Transformer 模型的视频，所以我会降到如何编写模型本身，如何在数据上训练它，以及如何进行推理。跟我一起坚持下去，虽然这会是一段稍长的旅程，但绝对值得。

现在，在我们讨论 Transformer 之前，我想先谈谈循环神经网络，在引入 Transformer 之前，用于大多数序列到序列任务的网络，让我们来回顾一下它们，

循环神经网络在 Transformer 出现之前就已经存在很久了，它们能够将一个输入序列映射到另一个输出序列，在这种情况下，我们的输入是 $X$，我们希望得到输出序列 $Y$，我们之前的方法是将序列拆分成多个项目，所以我们先给循环神经网络输入第一给项目 $x_1$, 
