

$$\text{FFN}(x) = \max(0, xW_1+b_1)W_2 + b_2$$
让我们先看看论文，了解一下论文中前馈层的细节，前馈层基本上是两所以第一个是从 d_model 到 d_ff，第二个是从 d_ff 到 d_model，所以 d_ff 是 2048，d_model 是 512，让我们来构建它，

类前馈块，我们也在这个情况下构建构造函数，在构造函数中我们需要定义我们在论文中看到的这两个值，所以 d_model、dff，还有在这种情况下 dropout，我们定义第一个矩阵，即 W1 和 B1 为线性层，并且它从 d_model 到 d_ff，然后我们应用 dropout，实际上，我们定义了 dropout，然后我们定义第二个矩阵 W2 和 B2，所以让我写注释，d_ff 到 d_model，这是 w2 和 b2，为什么我们有 b2？因为实际上，正如你在这里看到的，偏置默认是 true，所以它已经为我们定义了一个偏置矩阵，好的，让我们定义前向方法，在这种情况下，我们要做的是我们有一个输入句子，它是批次的，它是一个维度为批次、序列长度和 d_model 的张量，首先我们将使用线性层将其转换为另一个张量，从批次到序列计划到 d_ff，因为如果我们应用这个线性层，它会将 d_model 转换为d_ff，然后我们应用线性层到，这将把它转换回模型，我们在中间应用 dropout，这就是我们的前馈块，让我们看看下一个块。

我们的下一个块是最重要和最有趣的，它是多头注意力，我们在上一视频中详细地看到了多头注意力是如何工作的，所以我现在将再次打开幻灯片来复习它是如何工作的，然后我们将通过编码来实际操作，如你所记，在编码器中我们有多头注意力，它三次使用编码器的输入，一次称为查询，一次称为键，一次称为值，你也可以认为这是输入的三次复制，或者简单地说，这是同一个输入应用了三次，多头注意力的基本工作原理如下：我们有一个输入序列，它是序列长度乘以 d_model，我们将其转换为三个矩阵：q、k 和 V，它们与输入完全相同，在这种情况下，因为我们讨论的是编码器，我们会发现在解码器中它略有不同一一然后我们将其乘以称为 W_q、w_k 和 w_v 的矩阵，这会产生一个新的维度为序列乘以 d_model 的矩阵，然后我们将这些矩阵分成 H 个矩阵，模矩阵，为什么是 H？因为这是我们想要的多头注意力的头数，我们沿着嵌入维度而不是序列维度分割这些矩阵，这意味着每个头将能够访问整个句子，但每个词的不同嵌入部分，我们使用这个公式对每个模矩阵应用注意力，这将给我们作为结果的较小矩阵，然后我们将它们组合回来，就像论文所说的那样，因此，将头 1 到头 H 连接起来，最后我们乘以 w_o 以得到多头注意力的输出，这同样是一个矩阵，其维度与输入矩阵相同，如你所见，多头注意力的输出在本幻灯片中也是序列乘以模型，实际上，我没有展示批量维度，因为我们正在讨论一个句子，但在我们编写 Transformer 时，我们不仅处理一个句子，而是多个句子，因此，我们需要考虑这里还有一个维度，即批量，好的，让我们来编写这个多头注意力的代码，我会稍微放慢速度，这样我们可以详细地看到每一步是如何完成的，但我真的希望你能再次了解它是如何工作的以及我们为什么要做这些，那么，让我们开始编写代码吧。

此外，在这种情况下，我们定义了构造函数以及我们需要提供给这个多头注意力的内容，作为参数，当然有模型的 d_model，在我们的例子中是 512，头数，我们在论文中称为 h，所以 h 表示我们想要多少个头，然后是 dropout 值，我们保存这些值，如你所见，我们需要将这个嵌入向量分成 H 个头，这意味着这个 d_modeI 应该能被 H 整除，否则我们无法将代表嵌入的相同向量平均分配给每个头的矩阵，所以我们确保 d_model 基本上能被 H 整除，这会进行检查，如果我们再看一遍我的幻灯片，我们可以看到 d_model 除以 h 的值被称为 d_k，正如我们在这里看到的，如果我们将 d_model 除以 h 个头，我们会得到一个新值，称为 d_k，并且为了与论文中的命名保持一致，我们也将其称为 d_k，所以 d_k 是 d_model 除以 h，好的，让我们也定义我们将用于乘以查询、键和值的矩阵以及输出矩阵 Wo，这同样是一个线性的，从 d_model 到 d_model，为什么从 d_model 到 d_model？因为如你所见，我的幻灯片中这是 d_model 乘以 d_model，所以输出将是序列乘以 d_model，所以这是 WQ，这是 wk，这是 WV，最后，我们还有一个输出矩阵，这里称为 Wo，这个 Wo 是 h 乘以 dv，乘以 d_model，所以 h 乘以 dv，dv 实际上等于 dk，因为它是 d_model 除以 h，但为什么这里称为 dv，这里称为 dk？因为这个头实际上是结果，这个头来自这个乘法，最后的乘法是乘以 V，在论文中他们称这个值为 dv，但在实际层面上它等于 dk，所以我们的 Wo 也是一个矩阵，是 d_model 乘以 d_model，因为 h 乘以 dv 等于 d_model，这是 Wo，最后，我们创建了 dropout。

让我们实现前向方法，并看看在编码过程中多头注意力是如何详细工作的，我们定义了查询、键和值，还有一个掩码，那么这个掩码是什么？掩码基本上是如果我们希望某些词不与其他词交互，我们就屏蔽它们，我们在我之前的视频中看到了，但现在让我们回到那些幻灯片，看看掩码在做什么，如你所记，当我们使用这个公式计算注意力时，即 softmax (q 乘以 kt 除以根号 dk) 再乘以 V，我们得到这个头矩阵，但在我们乘以 v 之前，只有这里的 q 乘以 k 的乘法，我们得到这个矩阵，它是每个词与每个其他词的组合，它是一个序列乘以序列的矩阵，如果我们不希望某些词与其他词交互，我们基本上将它们的值，即注意力分数，替换为非常小的值，在我们应用 softmax 之前，当我们应用 softmax 时，这些值将变为零，因为如你所记，softmax 的分子是 e 的 x 次方，所以如果 x 趋向于负无穷，即非常小的数，e 的负无穷次方将变得非常小，即非常接近零，所以我们基本上隐藏了这两个词的注意力，这就是掩码的工作，按照我的幻灯片，我们一个接一个地进行乘法，所以，如我们所记，我们首先计算查询乘以 wq，所以 self.dot wq 乘以查询给我们一个新的矩阵，称为 q'，在我的幻灯片中，我只是在这里称为查询，我们对键和值做同样的事情，让我也写下维度，所以我们从批量序列长度到 d_model，通过这个乘法，我们将得到另一个矩阵，它是批量序列长度和 d_model，你可以在幻灯片中看到，所以当我们做序列乘以 d_model 再乘以 d_model，我们得到一个与初始矩阵相同维度的新矩阵，即序列乘以 d_model，这对它们三个都是一样的，现在我们想做的是，我们想将这个查询、键和值分成更小的矩阵，以便我们可以将每个小矩阵分配给不同的头，让我们来做吧。

我们将使用 pytorch 的 view 方法进行分割，这意味着我们保持批量维度，因为我们不想分割句子，我们想将嵌入分割成各个部分，我们也想保持第二个维度，即序列，因为我们不想分割它，还有第三个维度，即 d_model，我们想将其分割成两个更小的维度，即 H 乘以 DK，所以 self.H, self.DK，如你所记，DK基本上是 d_model 除以 H，所以这个乘以这个，给你 d_model，然后我们转置，为什么要转置？因为我们更喜欢有边缘维度，而不是作为第三个维度，我们希望它成为第二个维度，这样每个头都能看到整个句子，所以我们会看到这个维度，所以序列长度乘以 dk一一让我也在这里写上注释，所以我们从批量序列长度，d_model 到批量序列长度，边缘，衰减，然后通过使用转置，我们将到批量边缘序列长度和衰减，这非常重要，因为我们希望，我们希望每个头都能观察到这些内容。所以序列长度乘以 dk，这意味着每个头我们会看到完整的句子，所以句子中的每个词，但只是嵌入的一小部分，我们对键和值做同样的事情，好的，现在我们有了这些更小的矩阵，让我回到幻灯片，这样我可以向你展示我们在哪里，

所以我们做了这个乘法，我们得到了查询、键和值，我们将其分割成更小的矩阵，现在我们需要使用这里的公式计算注意力，在我们计算注意力之前，让我们创建一个函数来计算扩展，所以如果我们创建一个新函数，也可以在以后使用，所以自注意力，让我们将其定义为静态方法，所以静态方法基本上意味着你可以在没有这个类实例的情况下，调用这个函数，你可以直接说多头注意力块点注意力，而不需要这个类的实例。我们还给了它一个 dropout 层，我们做的是我们得到衰减，什么是衰减？它是查询、键和值的最后一个维度，我们将使用这里的这个函数，让我先调用它，这样你可以理解我们将如何使用它，然后我们再定义它，所以我们希望从这个函数中得到两样东西：输出和我们想要的注意力分数，所以 softmax 注意力分数的输出，我们将这样调用它：所以我们给它查询、键、值、掩码和 dropout 层，现在让我们回到这里，所以我们有了 dk，现在我们做的是首先应用公式的前半部分，即查询乘以键的转置，除以 dk 的平方根，所以这些是我们的注意力分数：查询，矩阵乘法，所以这个 @ 表示矩阵乘法，在 PyTorch 中，我们转置最后两个维度，-2，-1 意味着转置最后两个维度，所以这将成为最后一个维度，是序列乘以序列长度乘以 dk，它将变成 dk 乘以序列长度，然后我们将其除以 √(dk)，我们在应用 softmax 之前，正如我们之前看到的，需要应用掩码，所以我们想要隐藏一些词之间的交互，我们应用掩码，然后应用 softmax，所以 softmax 会处理我们替换的值，我们如何应用掩码？我们只需将所有我们想要掩码的值替换为非常非常小的值，这样 softmax 就会将它们替换为零，所以如果定义了掩码，就应用它，这意味着基本上将所有满足此条件的值替换为这个值，我们将这样定义掩码，使得在这个值、这个表达式为真时，我们希望它被替换为这个值，稍后我们还将看到如何构建掩码，现在，姑且认为这些都是我们不想要的值，在注意力中，所以我们不希望，例如，某个词去关注未来的词，例如当我们构建解码器时，或者我们不希望填充值与其他值交互，因为它们只是为了达到序列长度而填充的词，我们将用负 10 的 9 次方替换它们，这是一个在负数范围内非常大的数，基本上代表负无穷大，然后当我们应用 softmax 时，它将被替换为零，应用到这个维度，好吧，让我写一些注释，所以在这种情况下，我们有个批次乘以边, 所以每个头都会，然后是序列长度和序列长度，如果我们也有 dropout 的话，所以如果 dropout 没有错，我们也应用 dropout，最后，正如我们在原始幻灯片中看到的，我们将 softmax 的输出与 V 矩阵相乘，所以我们返回注意力得分乘以值，以及注意力得分本身，那么我们为什么要返回一个元组呢？因为我们想要这个，当然，我们需要它用于模型，因为我们需要将其传递给下一层，但这将用于可视化，所以自注意力的输出，在这种情况下是多头注意力的输出，实际上会在这里，我们将用它来进行可视化，所以为了可视化，模型对这个特定交互给出的分数是多少？我也要在这里写一些注释，所以这里我们这样做批次，让我们回到这里，所以现在我们有多头注意力，多头注意力的输出，我们最终要做的是，好吧，让我们回到幻灯片。

首先，我们在哪里，我们计算了这里较小的矩阵，所以我们应用了 softmax，k 乘以 kt 除以 dv 的平方根，然后我们也乘以 V 一一我们可以在这里看到——我们这里的小矩阵，头一、头二、头三和头四，现在我们需要将它们组合在一起，连接，就像论文中的公式所说，最后乘以 wo，所以让我们来做吧，我们转置是因为在我们将矩阵转换为序列长度乘以之前，我们将序列长度作为第三维度，我们最初想要将它们组合在一起，因为结果张量，我们希望序列长度在第二位置，所以让我先写下来，我们想要做什么，批次，我们从这一个序列长度开始，首先我们做一个转置，然后我们想要的是这个：所以这个转置带我们到这里，然后我们做一个视图，但我们不能这样做，我们需要使用连续的，这基本上意味着 PyTorch，为了改变张量的形状，需要我们将内存设置为连续的，这样他就可以就地进行操作，减去一，并乘以 self.h 乘以 self.dk，正如你记得的，这是模型，因为我们在这里定义了 dk，模型乘以h，除以 h，最后我们将这个 x 乘以 wo，这是我们的输出矩阵，这将给我们一一我们从批次到，这就是我们的多头注意力块，

我们现在有了所有必要的成分，可以将它们全部组合在一起，我们只是漏掉了一层，让我们先去看看它，我们还需要构建最后一层，就是这里看到的连接，例如，这里我们有一些这层的输出，所以在这里加上一个带有这个连接的归一化，这一部分被发送到这里，然后这个的输出被发送到归一化，然后通过这层组合在一起，所以我们需要创建这个管理跳跃连接的层，所以我们取输入，我们让它跳过一层，我们取前一层的输出，所以在这种情况下，多头注意力，我们把它给这层，但也与这部分结合，所以让我们构建这层，我称之为残差连接，因为它基本上是一个跳跃连接，好的，让我们构建这个残差连接，

像往常一样，我们定义构造函数，在这种情况下，我们只需要一个 dropout，如你所记，脚本连接是在加和归一化与前一层之间，所以我们还需要一个归一化，这是我们之前定义的层归一化，然后我们定义前向方法和子层，这是前一层，我们所做的是我们取 x，并将其与下一层的输出结合，在这种情况下，称为子层，我们应用 dropout，所以这是加和归一化的定义，实际上，有一个细微的差别：我们首先应用归一化，然后应用子层，在论文的情况下，他们首先应用子层，然后是归一化，我看到了很多实现，其中大多数实际上是这样做的，所以我们也会坚持这一点，如你所记，这些块通过这里更大的块组合在一起，我们有 n 个这样的块，所以这个大块我们称之为编码器块，每个编码器块重复 n 次，前一个的输出发送到下一个，最后一个的输出发送到解码器，所以我们需要创建这个块，它将包含一个多头注意力，两个加和归一化，以及一个前馈，所以让我们来做吧。

我们称这个块为编码器块，因为解码器内部有三个块，而编码器只有两个，并且，如我之前所见，我们内部有自注意力块，即多头注意力，我们称之为自注意力，因为在编码器的情况下，它应用于具有三种不同角色的相同输入：查询、键和值的角色，这是我们的前馈，然后我们有一个 dropout，这是一个浮点数，然后我们定义，然后我们定义两个残差连接，我们使用模块列表，这是一种组织模块列表的方式，在这种情况下，我们需要两个，好的，让我们定义前向方法，我定义了源掩码，源掩码是什么？它是我们想要应用于编码器输入的掩码，为什么我们需要为编码器的输入设置掩码？因为我们想要隐藏填充词与其他词的交互，我们不希望填充词与其他词交互，所以我们应用掩码，让我们进行第一个残差连接，让我们回去查看视频，实际上是查看幻灯片，这样我们可以理解我们现在在做什么，所以第一个跳跃连接是这样的：x 从这里到这里，但在添加之前，我们需要先应用多头注意力，所以我们取这个 x，我们把它发送到多头注意力，同时我们也把它发送到这里，然后我们结合这两者，

所以第一个跳跃连接是在 x 之间，然后另一个 x 来自自注意力，所以这是函数，所以我将使用 lambda 定义子层，所以这基本上意味着首先应用自注意力，自注意力，其中我们给出查询、键和值是我们的 x，即我们的输入，这就是为什么它被称为自注意力，因为查询、键和值的角色是 x 本身，即输入本身，所以句子在观察自己，因此，一个句子中的每个词都在与同一句子中的其他词交互，我们将在解码器中看到这一点不同，因为我们在解码器中有交叉注意力，所以来自解码器的键在观察一一抱歉, 来自解码器的查询，在观察来自编码器的键和值，我们给它源掩码，所以这是什么？基本上，我们调用这个函数为多头注意力块的前向函数，所以我们给出查询、键、值和掩码，这将通过使用残差连接与这个结合，然后我们再次进行第二个，第二个是前馈一一我实际上需要在这里使用 lambda，然后我们返回 x，所以这意味着结合前馈和 x 本身，即前一层的输出，也就是这个，然后应用残差连接，这定义了我们的编码器块，

现在我们可以定义编码器对象，因为编码器由许多编码器块组成，根据论文，我们可以有最多 n 个，所以让我们定义编码器：我们将有多少层？我们将有 N 层，所以我们有很多层，它们一个接一个地应用，所以这是一个模块列表，最后我们将应用层归一化，所以我们一层接一层地应用，前一层的输出成为下一层的输入，最后我们应用归一化，这结束了我们对编码器的探索之旅，

让我们简要回顾一下我们所做的事情，我们已经获取了输入，目前还没有将所有块组合在一起，我们刚刚在这里构建了这个名为编码器的大块，其中包含两个较小的块，即跳跃连接，第一个跳跃连接是在多头注意力和发送到这里的这个 x 之间，第二个是在这个前馈和发送到这里的这个 x 之间，我们有 n 个这样的块，一个接一个，最后一个的输出将被发送到解码器，但在应用归一化之前，现在我们会，我们构建了解码器部分，

现在在解码器中，输出嵌入与输入嵌入相同，我的意思是，我们需要定义的类是相同的，所以我们只需要初始化两次，位置编码也是如此，我们可以使用与编码器相同的值，也用于解码器，我们需要定义的是这里的大块，它由掩码多头注意力和加法归一化组成，所以这里有一个跳跃连接, 另一个多头注意力与另一个跳跃连接，以及前馈与这里的跳跃连接，我们定义多头注意力类的方式实际上已经考虑到了掩码，所以我们不需要为解码器重新发明轮子，我们可以定义解码器块，即这里由三个子层组成的大块，然后我们使用这个 n 个解码器块来构建解码器，让我们开始吧，

首先让我们定义解码器块，解码器中，我们有自注意力，即：让我们回到这里，这是自注意力，因为我们有这个输入在多头注意力中被使用了三次，所以这被称为自注意力，因为相同的输入扮演了查询、键和值的角色，这意味着句子中的每个词都与同一句子中的其他词相匹配，但在这里的部分，我们将使用来自解码器的查询来计算注意力，而键和值将来自编码器，所以这不是自注意力，这被称为交叉注意力，因为我们正在将两种不同类型的对象交叉在一起，并以某种方式匹配它们来计算它们之间的关系，好吧，让我们定义，

这是交叉注意力块，基本上是多头注意力，但我们会给它不同的参数，这是我们的前馈层，然后我们有一个 dropout，好的，我们还需要找到残差连接，在这种情况下，我们有三个，太棒了，好的，让我们构建前向方法，它与编码器非常相似，但有一点不同，我会强调需要 x，x 是解码器的输入，但我们还需要编码器的输出，我们需要源掩码，即应用于编码器的掩码，以及目标掩码，即应用于解码器的掩码，为什么它们被称为源掩码和目标掩码？因为在这种特定情况下，我们处理的是翻译任务，所以我们有一个源语言，在这种情况下是英语，我们有一个目标语言，在我们的例子中是意大利语，所以你可以称之为编码器掩码或解码器掩码，但基本上我们有两个掩码：一个是来自编码器的，一个是来自解码器的，所以在我们的例子中我们称之为源掩码，源掩码是来自编码器的，即源语言，而目标掩码是来自解码器的，即目标语言，而且，就像之前一样，我们首先计算自注意力，这是解码器块的第一部分，其中查询、键和值是相同的输入，但带有解码器的掩码, 因为这是解码器的自注意力块，然后我们需要结合，我们需要计算交叉注意力，这是我们的第二个残差连接，我们给它 一一好的，在这种情况下，我们给它来自解码器的查询，所以 x，键和值来自编码器，以及编码器的掩码，最后是前馈块，就像之前一样，就是这样，

我们现在实际上已经有了构建解码器的所有成分，它只是这个块的 n 次重复，一个接一个，就像我们对编码器所做的那样，在这种情况下我们也会提供多层，所以层 一一这只是一个模块列表，我们最后还会有一个归一化，就像之前一样，我们将输入应用于一层，然后使用前一层的输出作为下一层的输入，嗯，每一层都是一个解码器块，所以我们需要给它 x，我们需要给它编码器输出，然后是源掩码和目标掩码，所以每一个都是这样的：我们在这里调用前向方法，所以没有什么不同，最后我们应用归一化，这就是我们的解码器，

还有一个最后的成分，我们需要有一个完整的 Transformer，让我们来看看它，我们需要的最后一个成分是这里的这个线性层，所以，正如你从我的幻灯片中记得的那样，多头注意力的输出是按 D 模块序列化的，所以在这里，如果我们不考虑批次维度，我们期望输出是按 D 模块序列化的，然而，我们希望将这些词映射回词汇表，这就是为什么我们需要这个线性层，它将嵌入转换为词汇表中的位置，我将称这个层为投影层，因为它将嵌入投影到词汇表中，让我们来构建它。

我们需要这个层的 D 模型，所以 D 模型是一个整数，还有词汇表大小，这基本上是一个从 D 模型转换到词汇表大小的线性层，所以投影层是..，让我们定义前向方法，好的，我们想要做什么？让我写一个小注释，我们想要将批次序列长度转换为批次序列长度词汇表大小，在这种情况下，我们还将应用 softmax：实际上我们将应用 log softmax 以获得数值稳定性，就像我之前展示的那样，到最后一维，就这样，这是我们的投影层，现在我们有了 Transformer 所需的所有成分，所以让我们在 Transformer 中定义我们的 Transformer 块，我们有一个编码器，也就是我们的编码器，我们有一个解码器，也就是我们的解码器，我们有一个源嵌入，为什么我们需要源嵌入和目标嵌入？因为我们处理的是多种语言，所以我们有一个源语言的输入嵌入和一个目标语言的输入嵌入，我们还有目标嵌入，然后我们有源位置和目标位置，实际上它们是一样的，然后我们有了投影层，我们刚刚保存了这个，

现在我们定义了三种方法：一种用于编码，一种用于解码，一种用于投影，我们将依次应用它们，为什么？为什么我们不直接构建一个前向方法？因为，正如我们在推理过程中将看到的，我们可以重用编码器的输出，我们不需要每次都计算它，而且我们也倾向于将这个输出分开保存，也为了可视化注意力，所以对于编码器，我们有源，因为我们有源语言和源掩码，所以我们首先应用嵌入，然后我们应用位置编码，最后我们应用编码器，然后我们定义解码方法，它接收编码器输出(即张量)、源掩码（即张量）、目标以及目标掩码，我们做的是目标，我们首先将目标嵌入应用于目标句子，然后对目标句子应用位置编码，最后，通过代码，这基本上是解码器的前向方法，所以我们有相同的参数顺序，

是的，最后，我们定义了投影方法，在其中我们只是应用投影，从嵌入到词汇表大小，好的，这也是我们最后需要构建的模块，但我们还没有创建一个方法来将所有这些模块组合在一起，所以我们构建了许多模块，我们需要一个方法，给定 Transformer 的超参数，为我们构建一个单一的 Transformer，初始化所有的编码器、解码器、嵌入等，所以让我们构建这个函数，我们称之为 buildTransformer，给定所有超参数，它将为我们构建 Transformer，并用一些初始值初始化参数，我们需要定义 Transformer 的哪些部分？

当然，在这种情况下，我们谈论的是翻译，好的，我们正在构建的这个模型，我们将用于翻译，但你可以将其用于任何任务，所以我使用的命名基本上是翻译任务中常用的那些，稍后你可以更改命名，但结构是相同的，因此, 你可以将其用于 Transformer 适用的任何其他任务，所以我们首先需要的是源语言和目标语言的词汇表大小，因为我们需要构建嵌入，因为嵌入需要将词汇表中的标记转换为大小为 512 的向量，所以它需要知道词汇表有多大，所以它需要知道要创建多少个向量，然后是目标语言，这也是一个整数，然后我们需要告诉它源序列长度和目标序列长度，这一点非常重要，它们也可以相同，在我们的例子中，它们将是相同的，但它们也可以不同，例如，如果你使用的是处理两种非常不同语言的 Transformer，比如翻译，其中源语言所需的标记数远高于或远低于另一种语言，那么你就不需要保持相同的长度，你可以使用不同的长度，

下一个超参数是解码器模块，我们将其初始化为 512，因为我们希望保持与论文相同的值，然后我们定义超参数 n，即层数，因此我们将使用的编码器块和解码器块的数量，根据论文，是六个，然后我们定义超参数 h，即我们想要的头数，根据论文它是八个，dropout 率为 0.1，最后，我们有一个前馈层的隐藏层 dff，如我们在论文中看到的，它是 2048，这样就构建了一个 Transformer，首先，我们创建嵌入层，即源嵌入和目标嵌入，然后我们创建位置编码层，我们不需要创建两个位置编码层，因为实际上它们做的是同样的工作，它们也不会增加任何参数，但由于它们有 dropout，而且我也想让它更直观，这样你可以在不做任何优化的情况下理解每个部分，我认为这实际上是可以的，因为这是出于教育目的，所以我不想优化代码，我只想让它尽可能易懂，所以我做了我需要的每一个部分，没有走捷径，

然后我们创建了编码器块，我们有 n 个，所以让我们定义，让我们创建一个空数组，所以我们有 n 个，每个编码器块都有自注意力机制，所以编码器自注意力，这是一个多头注意力块，多头注意力需要d_model 、h 和 dropout 值，然后我们有一个前馈块，正如你所见，我使用的名称相当长，主要是为了尽可能让每个人都容易理解，所以每个编码器块由一个自注意力和一个前馈组成，最后，我们告诉它 dropout 率是多少，最后，我们添加这个编码器块，然后我们可以创建解码器块，我们也有用于解码器块的交叉注意力，我们也有前馈，就像编码器一样，然后我们定义解码器块本身：包括解码器块交叉注意力和最终的前馈与 dropout，最后将其保存到数组中，现在我们可以创建编码器和解码器了，我们给它所有的块，包括 N 和解码器，我们创建一个投影层，将模型转换为词汇大小，具体是哪个词汇，当然是目标语言，因为我们想从源语言转换到目标语言，所以我们希望将输出投影到目标词汇中，然后我们构建 Transformer，它需要什么呢？它需要一个编码器、一个解码器、源嵌入、目标嵌入，然后是源位置编码、目标位置编码，最后是投影层，就这样了。

现在我们可以使用 Xavier Uniform 初始化参数，这是一种初始化参数的方法，以加快训练速度，使它们不从随机值开始，有很多算法可以做到这一点，我看到很多实现使用了 Xavier，所以我认为这对模型来说是一个很好的起点来学习，最后返回我们心爱的 Transformer，就是这样，这就是你构建模型的方法，现在我们已经构建了模型，我们将进一步使用它。

所以我们将创建，我们首先会看一下数据集，然后我们将构建训练循环，在训练循环之后，我们还将构建推理部分和可视化注意力的代码，所以请耐心等待，喝点咖啡或茶，因为这会稍微长一些，但绝对值得，现在我们已经完成了模型的代码，下一步是构建训练代码，但在我们开始之前，我们先来复查一下代码，因为可能会有一些拼写错误，我实际上已经做了这个检查，代码中有一些错误，我对比了旧代码和新代码，这些都是非常小的问题，所以我们在这里写的是 "feedforward "而不是"feedforward ", 同样的问题也出现在所有提到 "feedforward "的地方，以及我们在构建解码器块时，另一个问题是，在这里构建解码器块时，我们只写了 nnmodule，应该改为"nn module list "然后"feedforward "也应该在这里和构建 Transformer 方法中修正，现在我可以删除旧的，我们不再需要它，让我检查一下模型它是正确的，包含 "feedforward "，是的，好的：我们的下一步是构建训练代码，但在构建训练代码之前，我们必须看一下数据，我们将使用什么样的数据？如我之前所说，我们处理的是翻译任务，我选择了这个名为 opus books 的数据集, 可以在 Hugging Face 上找到我们还将使用 Hugging Fakes 的 库来为我们下载这个数据集，这是我们将使用的唯一一个除了 PyTorch 之外的库，因为当然，我们不能自己重新发明数据集，因此，我们将使用这个数据集，并且我们还将使用 Huggingface 的 tokenizer 库将文本转换为词汇，因为我们的目标是构建 Transformer，所以不是要重新发明所有的轮子，我们将只专注于构建和训练 Transformer，在我个人的情况下，我将使用英语到意大利语的子集，但我们将以这样的方式来构建代码，你可以选择语言，代码会相应的运行，

如果我们看一下数据，我们可以看到每个数据项都是一对英语和意大利语的句子，例如，”那天不可能散步“，在意大利语中是 xxx，训练我们的 Transformer 从源语言（英语）翻译到目标语言（意大利语），让我们一步一步来做，首先，我们将编写代码来下载这个数据集并创建 tokenizer，那么，什么是 tokenizer 呢？让我们回到幻灯片，简要概述一下我们将如何处理这些数据，tokenizer 是在输入嵌入之前使用的，所以我们有一个英语句子, 例如,"你的猫是一只可爱的猫”，但这个句子将来自我们的数据集，tokenizer 的目标是创建这些标记，即将句子分割成单个单词，这有很多策略，如你所见，

这里我们有一个句子："你的猫是一只可爱的猫", tokenizer 的目标是将这个句子分割成单个单词，这可以通过多种方式完成，有 BPE tokenizer，有词级 tokenizer，有子词级词部分组织器，有很多种 tokenizer，我们将使用最简单的一种，称为词级 tokenizer，所以词级 tokenizer 基本上会通过空格来分割这个句子，所以每个空格定义了一个单词的边界，从而分割成单个单词，每个单词将被映射到一个数字，所以这是 tokenizer 的工作：构建词汇表和这些数字，并将每个单词映射到一个数字，当我们构建 tokenizer 时，我们还可以创建一些特殊标记，这些标记将用于 Transformer，例如称为填充的标记、句子开始的标记标记将用于 Transformer，例如称为填充的标记、 句子开始的标记、句子结束的标记一一这些对于训练 Transformer 是必要的，但我们将一步一步来做，所以让我们先编写构建tokenizer 和下载数据集的代码，好的，

让我们创建一个新文件，我们称之为 train.py，好的，让我们导入我们常用的库，即 torch，我们还将导入torchnn, 并且因为我们使用的是来自 Hugging Face 的库，我们还需要导入这两个库，我们将使用，我们将使用 datasets 库，你可以通过 pip 安装，即 datasets，实际上我们将使用 load_dataset，我们还将使用来自 HuggingFace 的 tokenizers 库，也可以通过 pip 安装，我们还需要确定我们需要哪种 tokenizer，所以我们使用词级 tokenizer，还有 trainers，即用于训练 tokenizer 的类，它将根据句子列表创建词汇表，我们将根据空格分割单词，我将一步一步地构建方法，首先，我将构建创建 tokenizer 的方法，并描述每个参数，现在你可能还没有整体的认识，但稍后当我们把这些方法结合起来时，你就会有更全面的理解，

所以让我们首先创建构建 tokenizer 的方法，我们称之为 getOrBuildTokenizer，这个方法接受配置，即我们模型的配置，我们稍后会定义它，数据集和我们将为其构建 tokenizer 的语言，我们定义 tokenizer 路径，即我们将保存此 tokenizer 的文件，我们通过配置路径来实现，好的，让我定义一些东西，首先，这个路径来自 pathlib，所以从 pathlib，这是一个允许你根据相对路径创建绝对路径的库，我们假设有一个名为 tokenizer_file 的配置，它是 tokenizer 文件的路径，并且这个路径可以使用语言进行格式化，例如，我们可以这样写，例如，像这样：

并且这将根据语言创建，例如，英语 tokenizer 或意大利语 tokenizer，因此，如果 tokenizer 不存在，我们就创建它，我实际上是从 HuggingFace 那里获取了所有这些代码，这并不复杂，我只是快速浏览了他们的 tokenizer 库，使用起来非常简单，并且能节省大量时间，因为构建 tokenizer 真的是在重新发明轮子，我们还将引入未知词 unknown，那么这是什么意思呢?

如果我们的 tokenizer 在其词汇表中遇到不认识的词，它将用这个 unknown 词替换它，它将映射到与这个 "unknown" 词对应的数字，预  tokenizer 基本上意味着我们按空白字符分割，然后我们构建训练器来训练我们的 tokenizer，好的，这是训练器，这是什么意思呢？

这意味着它将是一个词级别的训练器，因此它将使用空白字符和单个词来分割词，并且还将有四个特殊标记，一个是 "unknown"，这意味着如果在词汇表中找不到那个特定的词，就将其替换为 "unknown"，它还将有填充标记，我们将用它来训练 Transformer，以及句子的开始和结束特殊标记，平均频率意味着一个词要想出现在我们的词汇表中，它的频率必须至少为两次，现在我们可以训练 tokenizer 了，

我们使用这种方法，这意味着我们首先构建一个方法，该方法从我们的数据集中获取所有句子，我们稍后会构建它，好的，所以让我们也构建一个名为 getAlISentence 的方法，这样我们就可以遍历数据集，获取与我们要为其创建 tokenizer 的特定语言，对应的所有句子，如你所记，数据集中的每个项目都是一对句子，一个是英语的，一个是意大利语的，我们只想提取一种特定的语言，这是代表这对句子的项目，从这对句子中我们只提取我们想要的那种语言，这是构建 tokenizer 的代码，

现在让我们编写代码来加载数据集，然后构建 tokenizer，我们将调用这个方法 getDataset，它还接受我们稍后将定义的模型配置，所以让我们加载数据集，我们称之为 dsrow，好的，Hugging Face 让我们可以非常容易地下载它的数据集，我们只需要告诉它数据集的名称，然后告诉它我们想要的子集是什么，我们想要的是英意翻译的子集，但我们还想让它对你们来说是可配置的，以便你们能快速更改语言，所以让我们动态地构建这个子集，我们将在配置中有两个参数，一个是名为 language source，另一个是名为 language target.

稍后，我们还可以定义我们想要的数据集的分片，在我们的例子中，HuggingFace 的原始数据集中只有训练分片，但我们将自己将其分为验证数据和训练数据，所以让我们构建一个 tokenizer，这是原始数据集，我们也有目标数据，好的，现在因为我们只有从 HuggingFace 获得的训练分片，我们可以自己将其分为训练和验证，我们将 90

random_split 方法允许，这是 PyTorch 的一个方法，允许我们使用作为输入给出的尺寸来分割数据集，所以在这种情况下，这意味着将这个数据集分割成两个较小的数据集，一个具有这个尺寸，另一个具有这个尺寸，但让我们从 Torch 导入这个方法，我们也导入稍后会需要的那一个，总加载器和随机分割，现在我们需要创建数据集，我们的模型将使用的数据集，以便直接访问张量，因为现在我们刚刚创建了 tokenizer，而我们刚刚加载了数据，但我们还需要创建模型将使用的张量，那么，让我们创建数据集，我们称之为双语数据集，为此，我们创建了一个新文件，

这里我们也导入了 torch，就这样，我们将调用数据集，我们称之为双语数据集，OK，像往常一样，我们定义了构造函数，在这个构造函数中，我们需要给它从 Hugging Face 下载的数据集，源语言的tokenizer，目标语言的 tokenizer，源语言的名称，目标语言的名称，以及我们将使用的序列长度，好的，我们保存所有这些值，

我们还可以保存用于为模型创建张量的特定标记，所以我们需要句子开始、句子结束和填充标记，那么，我们如何将句子开始的标记转换为数字，即输入 ID 呢？tokenizer 有一个专门的方法来完成这个任务，所以让我们来做吧，

这是句子开始的标记，我们希望将其构建为张量，这个张量将只包含一个数字，由.. 给出. 我们可以使用源语言或目标语言的 tokenizer，这无关紧要，因为它们都包含这些特定标记，这是将标记转换为数字的方法，所以句子开始，这个标记的类型，这个张量的类型是：我们希望它是长整型，因为词汇表可能超过 32 位长，即词汇表的大小，所以我们通常使用 64 位长整型，对句子结束和填充标记也做同样处理，

我们还需要定义这个数据集的长度方法，它告诉数据集本身的长度，基本上就是 HuggingFace 数据集的长度，然后我们需要定义 getltem 方法，好的，首先我们将从 HuggingFace 数据集中提取原始对，然后我们提取源文本和目标文本，最后，我们将每个文本转换为标记，然后转换为输入 ID，这是什么意思？tokenizer 首先将句子拆分为单个单词，然后将其映射到词汇表中对应的数字，并且它只进行一次遍历，这是通过 encode 方法的 ids 完成的，这为我们提供了输入 ID，即原始句子中每个单词对应的数字，并以数组形式给出，我们对解码器也做了同样的事情，现在，如你所记，我们还需要将句子填充到序列长度，这非常重要，因为我们希望模型始终能够工作，我的意思是，模型总是以固定长度的序列工作，但我们并不是每个句子都有足够的单词，所以我们使用填充标记，这里的 pad 作为填充标记，直到句子达到序列长度，所以我们计算需要为编码器侧和解码器侧添加多少个填充标记，这基本上是我们需要达到序列长度的数量，减去二，为什么这里要减去二？所以我们已经有这么多标记，我们需要达到这个数量，但我们还会在编码器侧添加句子的起始标记和结束标记，所以这里也要减去二

这里只减去一，如果你还记得我之前的视频，我们在训练时，只在解码器侧添加句子的起始标记，然后在标签中只添加句子的结束标记，所以在这种情况下，我们只需要为句子添加一个特殊标记，我们还确保我们选择的序列长度足以表示数据集中的所有句子，如果我们选择得太小，我们希望抛出一个异常，所以基本上，这个填充标记的数量永远不应该变为负数，

好的，现在让我们为编码器输入、解码器输入以及标签构建两个张量，所以一个句子将被发送到编码器的输入，一个句子将被发送到解码器的输入，还有一个句子是我们期望的解码器输出，而这个输出我们称之为标签，通常它被称为目标或标签，我称之为标签，我们可以截取起始张量，好的，我们可以截取三个张量，首先是这个句子的起始标记，然后是源文本的标记，然后是句子的结束标记，然后添加足够的填充标记以达到序列长度，我们已经计算出需要为这个句子添加多少个填充标记，所以让我们开始吧，

这是编码器输入，所以让我在这里写一些注释，这是在源文本中添加起始标记，然后我们构建解码器，这也是标记的连接，在这种情况下，我们没有句子的起始标记，我们只有句子的结束标记，最后，我们添加了足够的填充标记以达到序列长度，我们已经计算出需要多少个，现在使用这个值，然后我们构建标签，在标签中，我们只添加句子的结束标记，让我复制一下，这样更快，是的，因为我们需要的填充标记数量与解码器输入相同，让我们再检查一遍，只是为了调试，确保我们确实达到了序列长度，

好的，既然我们已经进行了这个检查，让我在这里也写一些注释，这里我们只添加 eos，不，这里添加 sos 到解码器输入，这里是：在标签中添加 eos，这是我们期望从解码器得到的输出，现在我们可以返回所有这些张量，以便我们的训练可以使用它们，我们返回一个由编码器输入组成的字典，编码器输入是什么？它基本上是偏移的序列长度，然后我们有解码器输入，这也是一个由序列长度数量的标记组成的，我这里漏了一个逗号，然后我们有编码器掩码，那么编码器掩码是什么？如你所记，我们通过添加填充标记来增加编码器输入句子的长度，但我们不希望这些填充标记参与自注意力机制，所以我们需要构建一个掩码，表示我们不希望这些标记被自注意力机制看到，因此我们为编码器构建掩码，我们如何构建这个掩码？我们只需说明所有非填充标记都是正常的，所有填充标记都是不正常的，我们也解冻以添加这个序列维度，稍后还会添加批次维度，并将其转换为整数，所以这是一个序列长度为 1，因为这将在自注意力机制中使用，然而，对于解码器，我们需要一个特殊的掩码，即因果掩码，这意味着每个词只能看到前面的词，并且每个词只能看到非填充词，所以我们再次不希望填充标记参与自注意力机制，我们只希望真正的词参与其中，并且我们也不希望每个词看到它后面的词，而只希望看到它前面的词，因此，我将在这里使用一种称为因果掩码的方法，稍后将构建它，我们也将构建它，

所以现在我只是调用它来展示它的使用方式，然后我们也将继续构建它，在这种情况下，我们不希望填充标记，我们添加必要的维度，并且我们还进行布尔与因果掩码的操作，这是一个我们将立即构建的方法，这个因果掩码需要构建一个大小为序列长度乘以序列长度的矩阵，序列长度基本上是我们解码器输入的大小，这个——让我为你写一个注释，所以这是一个，两个序列长度组合在一起，所以这个布尔与一个序列长度，序列和，这个可以广播，让我们去定义这个方法：因果掩码，那么什么是因果掩码，

因果掩码基本上意味着我们想要，让我们回到幻灯片，实际上，如你在幻灯片中记得的，我们希望解码器中的每个词只看到它前面的词，所以我们想要的是让这个矩阵对角线以上的所有值，这个矩阵表示自注意力机制中查询与键的乘积，我们想要隐藏所有这些值，这样你就不能看到"猫是一只可爱的猫". 它只能看到它自己，但这里的这个词，例如"可爱的"，可以看它前面的一切，所以从你到“可爱的”本身，但不包括它后面的“猫”这个词，所以我们所做的是我们希望这里的所有这些值都被屏蔽掉，这也意味着我们希望这个对角线以上的所有值都被屏蔽掉，在 PyTorch 中有一个非常实用的方法来做到这一点，所以让我们来做，让我们去构建这个方法，

所以掩码基本上是 torch.triu，这意味着给我所有我告诉你的对角线以上的值，所以我们想要一个矩阵，哪个矩阵？由所有 1 组成的矩阵，这个方法将返回对角线以上的所有值，其他所有值将变为零，所以我们想要对角线 1 类型，我们希望它是整数，我们所做的是返回掩码等于零，所以这将返回对角线以上的所有值，而对角线以下的所有值将变为零，但实际上我们想要相反的效果，所以我们说：好吧，所有为零的值将通过这个表达式变为真，所有不为零的值将变为假，因此我们在这里应用它来构建这个掩码，所以这个掩码将是一个序列长度乘以序列长度的矩阵，这正是我们想要的，

好的，让我们也添加标签，标签也在上面，我忘了逗号，序列长度，然后我们有源文本，只是为了可视化，我们可以发送源文本，然后是自标文本，这是我们的数据集，现在让我们回到我们的训练方法，继续编写训练循环，所以现在我们有了数据集，我们可以创建它，我们可以创建两个数据集，一个用于训练，一个用于验证，然后将它们发送到数据加载器，最后到我们的训练循环，哦，我们忘记导入数据集了，所以让我们在这里导入它，我们还导入了因果掩码，我们稍后会用到它，我们的源语言是什么？它在配置中，我们的目标语言是什么？我们的序列长度是多少？它也在配置中，我们对验证也做同样的操作，但唯一的区别是我们现在使用这个，其余的都一样，我们也只是为了选择最大序列长度，我们还希望观察我们在这里创建的两个分割中，源语言和目标语言中每句话的最大长度，这样如果我们选择一个非常小的序列长度，我们就会知道

基本上我们做的是，我从源语言和目标语言加载每句话，使用 tokenizer 将其转换为 ID，并检查长度，如果长度是，比如说，180，我们可以选择 200 作为序列长度，因为它将覆盖我们在这个数据集中所有可能的句子，如果是，比如说，500，我们可以使用 510 或类似的值，因为我们还需要在这些句子中添加句子的开始和结束标记，这是源 ID，然后让我们也创建目标 ID，这是目标语言，然后我们只是说源的最大长度是当前句子的最大长度，目标是目标，目标 ID 是，然后我们打印这两个值，我们也对目标做同样的事情。

就是这样，现在我们可以继续创建数据加载器了，我们根据配置定义批量大小，我们还没有定义，但你可以猜到它的值，我们希望它是打乱的，好的，对于验证，我将使用批量大小为 1，因为我希望逐句处理每句话，这个方法返回训练的数据加载器、验证的数据加载器，源语言的 tokenizer 和目标语言的 tokenizer，现在我们可以开始构建模型了，

所以让我们定义一个新方法，叫做 getModel，它将根据我们的配置、词汇表大小构建模型，即 Transformer 模型，所以模型是，我们还没有导入模型，所以让我们导入它，构建 transformer，第一个是什么？源词汇表大小，和目标词汇表大小，然后我们有序列长度，我们有源语言的序列长度和目标语言的序列长度，我们将使用相同的，对于两者，然后我们有 dmodule，即嵌入的大小，我们可以保持其余的默认值，就像在论文中一样，如果模型对于你的 GPU 来说太大，无法训练，你可以尝试减少头数或层数，当然，这会影响模型的性能，但我认为，考虑到数据集并不大且不复杂，这应该不是个大问题，因为我们无论如何都不会构建一个庞大的数据集，好的，现在我们有了模型，可以开始构建训练循环了，但在构建训练循环之前，让我先定义这个配置，因为它经常出现，我认为现在定义结构会更好，所以让我们创建一个名为 config.py 的新文件，在其中定义两个方法，一个是 getConfig，另一个是获取我们将保存模型权重的路径，好的，让我们定义批量大小，我选择 8，如果你的电脑允许，你可以选择更大的值，我们将训练的轮数，我认为 20 轮就足够了，学习率，我使用的是 10 的负 4 次方，你可以使用其他值，我认为这个学习率是合理的，在训练过程中改变学习率是可能的，实际上，给予一个非常高的学习率，然后随着每个轮次逐渐降低它，这是相当常见的做法，我们不会使用这种方法，因为它会让代码稍微复杂一些，而这并不是本视频的真正目的。

本视频的目的是教授 Transformer 的工作原理，我已经检查过，对于这个特定的从英语到意大利语的数据集，序列长度为 350 已经足够了，我们将使用的 D 模型是默认的 512，源语言是英语，所以我们是从英语开始的，目标语言是意大利语，我们将翻译成意大利语，我们将把模型保存到名为 weights 的文件夹中，模型文件名将是 tmodel，即 Transformer 模型，我还编写了代码，以便在可能需要重新启动训练时预加载模型，例如在模型崩溃后，这是 tokenizer 文件，所以它将这样保存，根据语言分别保存为 tokenizer_en 和 tokenizer_it，这是用于 TensorBoard 的实验名称，我们将在训练过程中保存损失，我想这里有一个逗号，

好的，现在让我们定义另一个方法，用于找到保存权重的路径，我创建如此复杂的结构是因为我还将提供在 Google Colab 上运行此训练的笔记本，因此，我们只需更改这些参数，使其在 Google Colab 上运行并直接将权重保存到您的 Google Drive 中，我已经编写了这段代码，它将在 GitHub 上提供，我也会在视频中提供链接，

好的，文件是根据模型基本名称构建的，然后是 epoch.pt，让我们在这里也导入路径库，好的，现在让我们回到训练循环，好的，我们终于可以构建训练循环了，所以，训练模型，根据配置，首先我们需要定义将所有张量放置在哪个设备上，所以，定义设备，如果我的电脑有冷却时间，我们也会打印出来，我们确保创建了 weights 文件夹，然后加载我们的数据集。

我们可以直接使用这里的值，并将其设置为 getDS(config)，我们还创建了模型一获取词汇表大小，有一个名为 getVocabSize 的方法，我想我们没有其他参数了，最后，我们将模型转移到我们的设备上，我们还启动了 TensorBoard，TensorBoard 允许可视化损失、图形和图表，让我们也导入 TensorBoard，好的，我们回去吧，让我们也创建优化器，我将使用 Adam 优化器，好的，由于我们还有配置允许在模型崩溃或某些东西崩溃时，恢复训练，让我们实现这一点，这将允许我们恢复模型的状态和优化器的状态，

让我们导入我们在数据集中定义的方法，我们加载文件，这里有一个拼写错误，好的，我们将使用的损失函数是交叉熵损失，我们需要告诉他忽略索引是什么，所以我们不希望他忽略填充标记，基本上，我们不希望填充标记的损失对总损失有贡献，我们还将使用标签平滑，标签平滑基本上允许我们的模型对其决策不那么自信，所以，打个比方，想象一下我们的模型告诉我们选择第三个词，并且概率非常高，因此，我们将通过标签平滑来取走一小部分那个概率，并将其分配给其他标记，从而使我们的模型对其选择不那么确定，这样就不会过度拟合，这实际上提高了模型的准确性，因此，我们将使用 0.1 的标签平滑，这意味着从每个最高概率的标记中，取 0.1，好的，最后让我们构建训练循环，

我们告诉模型进行训练，我使用 TKODM 为数据加载器构建了一个批次迭代器，这将显示一个非常漂亮的进度条，我们需要导入 TQDM，好的，最后我们得到了张量，编码器输入，这个张量的大小是多少？它是批次到序列长度，解码器输入是解码器输入的批次，我们也将其移动到我们的设备上,批次到序列长度，我们还得到了两个掩码，这是大小，然后是解码器掩码，好的，为什么这两个掩码不同？因为在一种情况下，我们只告诉他隐藏填充标记，在另一种情况下，我们还告诉他隐藏每个词的所有后续词，以隐藏所有后续词来掩盖它们，

好的，现在我们运行，让我们让张量通过 Transformer，因此，首先我们计算编码器的输出，并使用编码器输入和编码器掩码进行编码，然后我们使用编码器输出、编码器掩码、解码器输入和解码器掩码计算解码器输出，好的，正如我们所知，这个的结果，因此，模型编码的输出将是批次序列长度-d 模型，同样，解码器的输出将是批次序列长度，d 模型，但我们想将其映射回词汇表，所以我们需要投影，所以让我们获取投影输出，这将产生一个，B，即批次序列长度和目标词汇表大小，好的，现在我们有了模型的输出，我们想将其与我们的标签进行比较，所以首先让我们从批次中提取标签，我们也将其放在我们的设备上，那么标签是什么？

它是 B，即批次到序列长度，其中每个位置告诉，因此，标签已经为每个 b 和序列长度，因此对于每个维度，告诉我们该特定词在词汇表中的位置，我们希望这两者是可比较的，所以我们首先需要计算损失到这个，我现在展示给你投影输出视图减一，好的，这是做什么的？这基本上转换了... 我会在这里展示给你，这个大小变成这个大小，P 乘以序列长度，然后是目标词汇表大小，好的。因为我们想将其与这个进行比较，这就是交叉熵希望张量的样子，还有标签，好的，现在我们可以，我们已经计算了损失，我们可以更新我们的进度条，这个用我们计算的损失，这将在我们的进度条上显示损失，我们也可以在 TensorBoard 上记录它，让我们也刷新它，

好的，现在我们可以反向传播损失，所以 loss backward，最后我们更新模型的权重，这就是优化器的工作，最后我们可以将梯度清零，并将全局步数加一，全局步数主要用于 TensorBoard 来跟踪损失，我们可以在每个 epoch 保存模型，好的，模型文件名，我们从我们的特殊方法中获取，这个，我们告诉他我们的配置和我们文件的名称，这是 epoch，但前面有零，然后我们保存我们的模型，当我们希望能够恢复训练时，保存不仅模型的状态，还有优化器的状态，这是一个非常好的主意，因为优化器也会跟踪一些统计数据，每个权重一个，以理解如何独立移动每个权重，通常实际上，我发现优化器字典相当大，所以即使它很大，如果你想让你的训练可恢复，你需要保存它，否则优化器总是会从零开始，并且必须从零开始，即使你从个之前的 epoch 开始一一如何移动每个权重，所以每次我们保存一些快照时，我总是包含它模型的状态，这是模型的所有权重，我们也想保存优化器，让我们也保存全局步数，我们想把所有这些保存到文件名中，所以模型，文件名，就这样，现在让我们编写代码来运行这个，

所以如果名字是，我真的发现警告很烦人，所以我想过滤掉它们，因为我有很多库，尤其是 CUDA，我已经知道内容是什么，所以我不希望每次都看到它们，但对你来说，我建议至少看一次，以了解是否有任何大问题，否则它们只是在抱怨 CUDA，

好的，让我们尝试运行这段代码，看看是否一切正常，应该没问题，我们期望的是，代码应该在第一次下载数据集，然后创建 tokenizer 并将其保存到文件中，并且它还应该开始训练模型 30 个 epoch，当然，它永远不会完成，但让我们开始吧，让我再检查一下配置，好的，让我们运行它，

好的，它在构建 tokenizer。 我们这里有一些问题，序列长度，好的，最终模型开始训练了，我给你们回顾一下我犯的错误，首先，序列长度写错了，这里有一个大写的 L，还有在数据集中，我忘了在这里保存它，这里我也写成了大写，所以 L 是大写的，现在训练正在进行中，你可以看到训练相当快，至少在我的电脑上是这样，实际上并不那么快，由于我选择了 8 的批量大小，我可以尝试增加它，它正在 CUDA 上进行，损失在减少，权重将保存在这里，

所以如果我们到达 epoch 的末尾，它将在这里创建第一个权重，所以让我们等到 epoch 结束，看看权重是否真的创建了，在实际完成模型训练之前，让我们再做一件事，我们还希望在训练过程中可视化模型的输出，这被称为验证，所以我们想看看我们的模型在训练过程中是如何演变的，所以我们想要构建的是一个验证循环，它将允许我们评估模型，这也意味着我们想从这个模型中推理并检查一些样本句子，看看它们是如何被翻译的：所以让我们开始构建验证循环，

我们做的第一件事是构建个名为 runValidation 的新方法，这个方法将接受一些我们将使用的参数，现在，我只是把它们都写下来，稍后我会解释它们将如何使用，好的，我们运行验证的第一件事是将我们的模型设置为评估模式，所以我们执行 model.eval, 这意味着这告诉 PyTorch 我们将要评估我们的模型，然后我们将推理两个句子，看看模型的输出是什么，所以通过 torch.NoGrad 我们禁用了在这个宽度块内运行的每个张量的梯度计算，而这正是我们想要的，

我们只是想从模型中推理，我们不希望在这个循环中训练它，所以让我们从验证数据集中获取一个批次，因为我们只想推理两个，所以我们记录我们已经处理了多少个，并从这个当前批次中获取输入，我想提醒你，对于验证 ds，我们只有一个批次大小，这是编码器输入，我们也可以获取编码器掩码，

让我们确认一下批次的实际大小确实是一个，现在让我们进入有趣的部分，所以，如你所记得的，当我们想要推理模型时，我们需要只计算一次编码器输出，并将其重用于我们将要处理的每个 token，模型将从解码器输出，所以让我们创建另一个函数，它将在我们的模型上运行贪婪解码，我们将看到它只会运行一次编码器，所以让我们称这个函数为 greedy_decode，

好的，让我们创建一些我们将需要的 token，所以 Sos token，即句子的开始，我们可以从任一tokenizer 中获取，无论是目标还是源，它们都有，eos 好的，然后我们做的是我们预先计算编码器输出，并将其重用于我们从解码器获得的每个 token，所以我们只给出源和源掩码，即编码器输入和编码器掩码，我们也可以称之为编码器输入和编码器掩码，然后我们，好的，我们如何进行推理？我们做的第一件事是给解码器句子的开始 token，以便解码器将输出翻译句子的第一个 token，然后在每次迭代中，就像我在幻灯片中展示的那样，每次迭代我们都将前个 token 添加到解码器输入中，以便解码器可以输出下一个 token，然后我们取下一个 token，再次将其放在解码器输入的前面，并获取后续的 token，所以让我们为第一次送代构建一个解码器输入，它只包含句子的开始 token，

我们用句子的开始 token 填充这个，它与编码器输入具有相同类型，现在，我们将不断要求解码器输出下一个 token，直到我们达到句子的结束 token 或我们在这里定义的最大长度，所以我们可以做个 while True 循环，然后我们的第一个停止条件是，如果解码器输出，即下一步的输入大于或达到最大长度，这里，为什么我们有两个维度？一个是用于批次，另一个是用于解码器输入的 token，现在我们也需要为此创建一个掩码，我们可以使用我们的函数 causal mask 来表示我们不希望输入看到未来的词，我们不需要另一个掩码，因为如你所见，这里没有任何填充 token，现在我们计算输出。

我们为循环的每一次送代重用编码器的输出，我们重用源掩码，即编码器的输入和掩码，然后我们提供解码器输入，以及它的掩码，即解码器掩码，然后我们得到下一个 token，所以我们使用投影层获取下一个 token 的概率，但我们只想要最后一个 token 的投影，即我们在编码器之后给出的最后一个 token 的下一个 token，现在我们可以使用 max，这样我们就能得到具有最大概率的 token，这就是贪婪搜索，

然后我们得到这个词，并将其追加回来，因为它将成为下一次送代的输入，我们进行连接，所以我们取解码器输入并追加下个 token，所以我们创建另一个张量，应该是正确的，好的，如果下个词或 token 等于句子的结束 token，那么我们也停止循环，这就是我们的贪婪搜索，

现在我们可以直接返回输出，所以输出基本上就是解码器输入，因为你每次都将其追加下一个 token，并且我们移除了批次维度，所以进行了压缩，这就是我们的贪婪解码.

现在我们可以在这个函数中使用它，所以在验证函数中，我们终于可以得到模型输出，等于贪婪解码，我们给它所有参数，然后我们想要将这个模型输出与我们预期的结果进行比较，即与标签进行比较，所以让我们把这些全部追加进去， 所以我们给输入的，我们给了模型，模型的输出，即预测的输出，以及我们预期的输出，我们将所有这些保存在这些列表中，然后在循环结束时，我们将在控制台上打印它们，为了获取模型输出的文本，我们需要再次使用 tokenizer 将 token 转换回文本，当然，我们使用目标 tokenizer，因为这是目标语言，

好的，现在我们将所有这些保存到各自的列表中，我们也可以在控制台上打印出来，为什么我们使用这个名为 printMessage 的函数，而不是直接使用 Python 的 print？

因为我们在主循环，即训练循环中使用这里，TKODM，这是一个非常漂亮的进度条，但在进度条运行时不建议直接在控制台上打印，所以在控制台上打印，TKODM 提供了一个名为 print 的方法，我们将这个方法传递给这个函数，以确保输出不会干扰进度部分的打印，我们打印一些条形图，然后打印所有消息，如果我们已经处理了一定数量的示例，那么我们就中断，

那么为什么我们要创建这些列表呢？嗯，实际上我们也可以将所有这些发送到  tensorboard，所以我们可以，例如，如果我们启用了 tensorboard，我们可以将所有这些发送到 tensorboard，为此，实际上我们需要另一个库，它允许我们计算一些指标，我想我们可以跳过这部分，但如果你真的很感兴趣，我在我发布在 GitHub 上的代码中，你会发现我使用了这个名为 torchmatrix 的库，它允许计算字符错误率、BLEU指标，这对翻译任务非常有用，还有词错误率，所以如果你真的感兴趣，你可以在 GitHub 上找到代码，但对于我们的演示，我认为这并不必要，所以，实际上我们可以移除它，因为我们没有做这部分。

好的，现在我们有了 runValidation 方法，我们可以直接调用它，我通常的做法是在每隔几步后运行验证，但由于我们希望尽快看到结果，我们将首先在每次送代时运行它，并且我们还把这个 modeltrain 放在这个循环中，以便每次运行验证后，模型都会回到训练模式，所以现在我们可以直接运行验证，并给它所有运行验证所需的参数，给它模型，好的，用于打印消息，我们在打印任何消息吗？是的，我们在打印，所以让我们创建一个 lambda，我们只需做，这是要写的消息与 TQDM，然后我们需要给出全局步骤和 writer，我们不会使用它，但好的，现在我想我们可以再次运行训练，看看验证是否有效，好的，看起来它在工作，所以模型没问题，它在每一步都运行验证，这完全不是我们想要的，但至少我们知道贪婪搜索在工作，而且它不是，至少看起来它在工作，模型没有预测任何有用的东西，实际上它只是在预测一堆逗号，因为它完全没有训练，但如果我们训练模型一段时间后，

我们应该会看到在几个 epoch 之后，模型会变得越来越好，所以让我们停止这个训练，并把这个放回它该在的地方，即每个 epoch 的末尾，是的，这个我们可以留在这里，没问题，是的。

我现在将快进到一个已经预训练好的模型，我预先训练了它几个小时，这样我们就可以进行推理并可视化注意力，我已经复制了预先计算好的预训练权重，我还创建了这个笔记本，重用了我们在训练文件中之前定义的函数，代码非常简单，

实际上，我只是从训练文件中复制粘贴了代码，我只是加载模型并运行验证，使用我们刚刚写的方法，然后我在预训练上运行了验证，让我们再运行一次，例如，如你所见，模型正在推理 10 个例子，句子，结果还不错，我的意思是，我们可以看到 11 个微笑，11 个故事，它在匹配，而且大多数都匹配，实际上，我们也可以说它，几乎过度拟合了这个特定数据，但这就是 transformer 的力量，我没有训练它很多天，我只训练了几个小时，如果我没记错的话，结果真的非常好，

现在让我们制作一个笔记本，用来可视化这个预训练模型的注意力，根据我们之前构建的文件，所以，trainpi 你也可以选择你喜欢的语言来训练你自己的模型，我强烈建议你改变语言，看看模型的表现如何，并尝试诊断为什么模型表现不佳，如果表现不好或表现良好，尝试理解如何进一步改进它，所以让我们尝试可视化注意力，

所以让我们创建一个新的笔记本，让我们称之为，比如说，注意力可视化，好的，我们做的第一件事是导入所有需要的库，我还将使用一个叫做 Altair 的库，这是一个用于图表的可视化库，它实际上与深度学习无关，它只是一个可视化功能，特别是可视化功能，实际上，我在网上找到的，不是我写的，就像大多数可视化功能一样，如果你想构建一个图表或直方图，你可以很容易地在互联网上找到，所以我主要使用这个库，因为我从网上复制了代码来可视化它，但其余的都是我自己的代码，

所以让我们导入它，好的，让我们导入所有这些，当然，当你在电脑上运行代码时，你需要安装这个特定的库，我们还要定义设备，你可以直接从这里复制代码，然后我们加载模型，我们可以像这样从这里复制，好的，让我们粘贴到这里，这个变成了词汇源和词汇目标，

好的，现在让我们创建一个加载批次的功能，我现在将使用 tokenizer 将批次转换为标记，当然，对于解码器，我们使用目标词汇，即目标 tokenizer，所以让我们使用我们的贪心解码算法进行推理，所以我们提供模型，我们返回所有这些信息，

好的，现在我将构建必要的功能来可视化注意力，我将从另一个文件复制一些函数，因为实际上我们要构建的内容从学习角度来看并不有趣，就深度学习而言，它主要是用于可视化数据的函数，所以我会复制它，因为写起来相当长，当然，我会解释其中的关键部分，

这就是那个函数，好的，这个函数是做什么的呢？基本上，我们会从编码器那里得到注意力，如何从编码器获取注意力，例如，我们在三个位置有注意力，第一个在编码器中，第二个在解码器的开始，即解码器的自注意力，然后我们有编码器和解码器之间的交叉注意力，所以我们可以可视化三种注意力，如何获取关于注意力的信息，我们加载模型，我们有编码器，我们选择要从哪个层获取注意力，然后从每个层我们可以获取自注意力块及其注意力分数，这个变量从哪里来？

如果你记得当我们在这里定义注意力计算时，当我们计算注意力时，我们不仅返回输出到下一层，我们还给出这个注意力分数，它是 softmax 的输出，我们将其保存在这个变量中，self.attentionScores，现在我们可以直接获取并可视化它，所以这个函数将根据我们想要从哪一层和哪个头获取注意力，选择正确的矩阵，这个函数构建一个数据框来可视化信息，即从该矩阵中提取的标记和分数，

在这里，我们将从这个矩阵中提取行和列，然后我们还会构建图表，图表是用 Altair 构建的，实际上我们要做的是获取所有头的注意力，我构建了这个方法来获取我们作为输入传递给这个函数的所有层和所有头的注意力，所以现在让我运行这个单元格，

好的，让我们创建一个新单元格，然后运行它，好的，首先我们想要可视化我们正在处理的句子，即批次，其他输入标记，所以我们加载一个批次，然后我们可视化源和目标，还有目标，最后我们还计算了长度，

长度是什么？好的，基本上是所有在填充字符之前的字符，即第一个填充字符的出现，因为这是从数据集中提取的批次，已经是为训练构建的张量，所以它们已经包含了填充，在我们的例子中，我们只想获取句子中实际字符的数量，所以这个，我们可以，句子中实际单词的数量，所以我们可以检查填充之前的单词数量，所以让我们运行这个，这里有一些问题，我忘了这个函数是错误的，所以现在应该可以了，这个句子太短了，让我们找一个更长的，好的，让我检查一下质量，你不能保持现状，尤其是你，我们不能，你们不能继续这样，尤其是现在，好的，看起来不错，好的，让我们打印第 0、1、2 层的注意力，因为我们有六层，如果你记得，参数 n 等于 6，所以我们只可视化三层，并且我们会可视化所有头，每层有八个头，所以头编号：0、1、2、3、4、5、6 和 7

好的，首先让我们可视化编码器自注意力，我们确实得到了所有注意力图，我们想要哪一个，所以编码器的一个，我们想要这些层和这些头，原始标记是什么，编码器输入标记，我们在列中想要什么？因为我们打算构建一个网格，所以如你所知，注意力是一个将行与列相关联的网格，在我们的例子中，我们谈论的是编码器的自注意力，所以这是同一个句子在自我关注，所以我们需要在行和列上都提供编码器的输入句子，我们想要可视化的最大长度是多少？

好的，假设我们想要可视化不超过 20 个，所以是 20 和句子长度的最小值，好的，这是我们的可视化结果，我们可以看到，正如我们所预期的，实际上，当我们可视化注意力时，我们预期对角线上的值会很高，因为这是每个标记与其自身的点积，我们还可以看到其他有趣的关系，

例如，假设句子开始标记和句子结束标记，至少对于头 0 和层0，它们与其他词没有关联，就像我实际预期的那样，但其他头确实学习了一些非常小的映射，我们可以看到，如果我们悬停在每个网格单元格上，我们可以看到自注意力的实际值，例如自注意力的分数，我们可以看到这里的注意力非常强，所以 "especially" 和 "specially“ 这两个词是相关的，所以这是同一个词与自身的关联，但还有 "especially” 和 “now”，我们可以为所有层可视化这种注意力，

所以因为每个头会观察每个词的不同方面，因为我们均匀地分配了词嵌入到各个头，所以每个头会看到词嵌入的不同部分，我们也希望它们学习不同类型的词间映射一一这实际上是事实一一以及层与层之间的不同映射，我们还有不同的 Q、W、K 和 WV 矩阵，所以它们也应该学习不同的关系，

现在我们可能还想可视化解码器的注意力，所以让我们来做这个，让我复制代码并更改参数，好的，这里我们想要解码器 1，我们想要相同的层等，但行和列上的标记将是解码器标记，所以解码器输入标记和解码器输入标记，让我们可视化一一而且我们现在应该看到意大利语，因为我们使用的是解码器自注意力，确实是这样，所以在这里我们看到解码器端不同的注意力类型，而且我们还有多个头，它们应该学习不同的映射，以及不同的层应该学习词之间的不同映射，我发现最有意思的是交叉注意力，

所以让我们来看一下，好的，让我复制代码并再次运行它，好的，所以，如果你记得这个方法，它是编码器、解码器，相同的层，所以在这里，行上我们将显示编码器输入，列上我们将显示解码器输入标记，因为这是编码器和解码器之间的交叉注意力，好的，这就是编码器和解码器之间交互的大致方式以及它是如何发生的，所以这就是我们找到交叉注意力的地方，它是使用来自编码器的键和值计算的，而查询来自解码器，所以这实际上是翻译任务发生的地方，这就是模型如何学习将这两句话相互关联，以实际计算翻译。

所以我邀请你们自己运行代码，我给你们的第一个建议是，和我一起看视频并编写代码，你可以暂停视频，自己编写、运行代码，好的，让我给你们一 些实际的例子，例如，当我编写模型代码时，我建议你先看我编写一个特定层的代码，然后暂停视频，自己编写，花点时间，不要马上看解决方案，尝试找出哪里出错了，如果你真的在一两分钟后还是无法找出问题所在，你可以快速看一下视频，但尽量自己解决，当然，有些事情你无法自己想出来，所以，例如，对于位置编码和所有这些计算，这基本上只是公式的应用，但重点是，你至少应该能够自己构思出一个结构，所以所有层是如何相互作用的，这是我的第一个建议，而在训练循环方面，训练部分实际上是非常标准的，所以它与其他你可能见过的训练循环非常相似，有趣的部分是我们如何计算损失，以及我们如何使用 Transformer 模型，最后真正重要的是我们如何进行模型推理，这在贪心代码中，

所以感谢大家观看视频并陪伴我这么长时间，我可以保证这是值得的，我希望在接下来的视频中，能展示更多我熟悉的 Transformer 和其他模型的例子，并且我也想和你们一起探索，所以如果有什么你不明白的或者你想让我解释得更清楚的，请告诉我，我也一定会关注评论区的，请给我留言，谢谢, 祝你有个美好的一天，


---------

## Attention is all you need

大家好，欢迎来到我的视频，今天我们要聊的是 Transformer，这实际上是我 Transformer 系列视频的 2.0 版本，我之前做过一个关于 Transformer 的视频，但音频质量不太好，根据观众的建议，那个视频反响非常好，所以观众建议我改进音频质量，这就是我制作这个视频的原因。你不需要看之前的系列，因为我这次基本上会做同样的内容，但会有一些改进，我会弥补之前的一些错误，并加入一些可以提升的地方，看完这个视频后，我建议你接着看看我另一个关于如何从零开始编写 Transformer 模型的视频，所以我会降到如何编写模型本身，如何在数据上训练它，以及如何进行推理。跟我一起坚持下去，虽然这会是一段稍长的旅程，但绝对值得。

现在，在我们讨论 Transformer 之前，我想先谈谈循环神经网络，在引入 Transformer 之前，用于大多数序列到序列任务的网络，让我们来回顾一下它们，

循环神经网络在 Transformer 出现之前就已经存在很久了，它们能够将一个输入序列映射到另一个输出序列，在这种情况下，我们的输入是 $X$，我们希望得到输出序列 $Y$，我们之前的方法是将序列拆分成多个项目，所以我们先给循环神经网络输入第一给项目 $x_1$, 
