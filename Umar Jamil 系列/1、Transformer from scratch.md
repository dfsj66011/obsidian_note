
词汇表大小，和目标词汇表大小，然后我们有序列长度，我们有源语言的序列长度和目标语言的序列长度，我们将使用相同的，对于两者，然后我们有 dmodule，即嵌入的大小，我们可以保持其余的默认值，就像在论文中一样，如果模型对于你的 GPU 来说太大，无法训练，你可以尝试减少头数或层数，当然，这会影响模型的性能，但我认为，考虑到数据集并不大且不复杂，这应该不是个大问题，因为我们无论如何都不会构建一个庞大的数据集，好的，现在我们有了模型，可以开始构建训练循环了，但在构建训练循环之前，让我先定义这个配置，因为它经常出现，我认为现在定义结构会更好，所以让我们创建一个名为 config.py 的新文件，在其中定义两个方法，一个是 getConfig，另一个是获取我们将保存模型权重的路径，好的，让我们定义批量大小，我选择 8，如果你的电脑允许，你可以选择更大的值，我们将训练的轮数，我认为 20 轮就足够了，学习率，我使用的是 10 的负 4 次方，你可以使用其他值，我认为这个学习率是合理的，在训练过程中改变学习率是可能的，实际上，给予一个非常高的学习率，然后随着每个轮次逐渐降低它，这是相当常见的做法，我们不会使用这种方法，因为它会让代码稍微复杂一些，而这并不是本视频的真正目的。

本视频的目的是教授 Transformer 的工作原理，我已经检查过，对于这个特定的从英语到意大利语的数据集，序列长度为 350 已经足够了，我们将使用的 D 模型是默认的 512，源语言是英语，所以我们是从英语开始的，目标语言是意大利语，我们将翻译成意大利语，我们将把模型保存到名为 weights 的文件夹中，模型文件名将是 tmodel，即 Transformer 模型，我还编写了代码，以便在可能需要重新启动训练时预加载模型，例如在模型崩溃后，这是 tokenizer 文件，所以它将这样保存，根据语言分别保存为 tokenizer_en 和 tokenizer_it，这是用于 TensorBoard 的实验名称，我们将在训练过程中保存损失，我想这里有一个逗号，

好的，现在让我们定义另一个方法，用于找到保存权重的路径，我创建如此复杂的结构是因为我还将提供在 Google Colab 上运行此训练的笔记本，因此，我们只需更改这些参数，使其在 Google Colab 上运行并直接将权重保存到您的 Google Drive 中，我已经编写了这段代码，它将在 GitHub 上提供，我也会在视频中提供链接，

好的，文件是根据模型基本名称构建的，然后是 epoch.pt，让我们在这里也导入路径库，好的，现在让我们回到训练循环，好的，我们终于可以构建训练循环了，所以，训练模型，根据配置，首先我们需要定义将所有张量放置在哪个设备上，所以，定义设备，如果我的电脑有冷却时间，我们也会打印出来，我们确保创建了 weights 文件夹，然后加载我们的数据集。

我们可以直接使用这里的值，并将其设置为 getDS(config)，我们还创建了模型一获取词汇表大小，有一个名为 getVocabSize 的方法，我想我们没有其他参数了，最后，我们将模型转移到我们的设备上，我们还启动了 TensorBoard，TensorBoard 允许可视化损失、图形和图表，让我们也导入 TensorBoard，好的，我们回去吧，让我们也创建优化器，我将使用 Adam 优化器，好的，由于我们还有配置允许在模型崩溃或某些东西崩溃时，恢复训练，让我们实现这一点，这将允许我们恢复模型的状态和优化器的状态，

让我们导入我们在数据集中定义的方法，我们加载文件，这里有一个拼写错误，好的，我们将使用的损失函数是交叉熵损失，我们需要告诉他忽略索引是什么，所以我们不希望他忽略填充标记，基本上，我们不希望填充标记的损失对总损失有贡献，我们还将使用标签平滑，标签平滑基本上允许我们的模型对其决策不那么自信，所以，打个比方，想象一下我们的模型告诉我们选择第三个词，并且概率非常高，因此，我们将通过标签平滑来取走一小部分那个概率，并将其分配给其他标记，从而使我们的模型对其选择不那么确定，这样就不会过度拟合，这实际上提高了模型的准确性，因此，我们将使用 0.1 的标签平滑，这意味着从每个最高概率的标记中，取 0.1，好的，最后让我们构建训练循环，

我们告诉模型进行训练，我使用 TKODM 为数据加载器构建了一个批次迭代器，这将显示一个非常漂亮的进度条，我们需要导入 TQDM，好的，最后我们得到了张量，编码器输入，这个张量的大小是多少？它是批次到序列长度，解码器输入是解码器输入的批次，我们也将其移动到我们的设备上,批次到序列长度，我们还得到了两个掩码，这是大小，然后是解码器掩码，好的，为什么这两个掩码不同？因为在一种情况下，我们只告诉他隐藏填充标记，在另一种情况下，我们还告诉他隐藏每个词的所有后续词，以隐藏所有后续词来掩盖它们，

好的，现在我们运行，让我们让张量通过 Transformer，因此，首先我们计算编码器的输出，并使用编码器输入和编码器掩码进行编码，然后我们使用编码器输出、编码器掩码、解码器输入和解码器掩码计算解码器输出，好的，正如我们所知，这个的结果，因此，模型编码的输出将是批次序列长度-d 模型，同样，解码器的输出将是批次序列长度，d 模型，但我们想将其映射回词汇表，所以我们需要投影，所以让我们获取投影输出，这将产生一个，B，即批次序列长度和目标词汇表大小，好的，现在我们有了模型的输出，我们想将其与我们的标签进行比较，所以首先让我们从批次中提取标签，我们也将其放在我们的设备上，那么标签是什么？

它是 B，即批次到序列长度，其中每个位置告诉，因此，标签已经为每个 b 和序列长度，因此对于每个维度，告诉我们该特定词在词汇表中的位置，我们希望这两者是可比较的，所以我们首先需要计算损失到这个，我现在展示给你投影输出视图减一，好的，这是做什么的？这基本上转换了... 我会在这里展示给你，这个大小变成这个大小，P 乘以序列长度，然后是目标词汇表大小，好的。因为我们想将其与这个进行比较，这就是交叉熵希望张量的样子，还有标签，好的，现在我们可以，我们已经计算了损失，我们可以更新我们的进度条，这个用我们计算的损失，这将在我们的进度条上显示损失，我们也可以在 TensorBoard 上记录它，让我们也刷新它，

好的，现在我们可以反向传播损失，所以 loss backward，最后我们更新模型的权重，这就是优化器的工作，最后我们可以将梯度清零，并将全局步数加一，全局步数主要用于 TensorBoard 来跟踪损失，我们可以在每个 epoch 保存模型，好的，模型文件名，我们从我们的特殊方法中获取，这个，我们告诉他我们的配置和我们文件的名称，这是 epoch，但前面有零，然后我们保存我们的模型，当我们希望能够恢复训练时，保存不仅模型的状态，还有优化器的状态，这是一个非常好的主意，因为优化器也会跟踪一些统计数据，每个权重一个，以理解如何独立移动每个权重，通常实际上，我发现优化器字典相当大，所以即使它很大，如果你想让你的训练可恢复，你需要保存它，否则优化器总是会从零开始，并且必须从零开始，即使你从个之前的 epoch 开始一一如何移动每个权重，所以每次我们保存一些快照时，我总是包含它模型的状态，这是模型的所有权重，我们也想保存优化器，让我们也保存全局步数，我们想把所有这些保存到文件名中，所以模型，文件名，就这样，现在让我们编写代码来运行这个，

所以如果名字是，我真的发现警告很烦人，所以我想过滤掉它们，因为我有很多库，尤其是 CUDA，我已经知道内容是什么，所以我不希望每次都看到它们，但对你来说，我建议至少看一次，以了解是否有任何大问题，否则它们只是在抱怨 CUDA，

好的，让我们尝试运行这段代码，看看是否一切正常，应该没问题，我们期望的是，代码应该在第一次下载数据集，然后创建 tokenizer 并将其保存到文件中，并且它还应该开始训练模型 30 个 epoch，当然，它永远不会完成，但让我们开始吧，让我再检查一下配置，好的，让我们运行它，

好的，它在构建 tokenizer。 我们这里有一些问题，序列长度，好的，最终模型开始训练了，我给你们回顾一下我犯的错误，首先，序列长度写错了，这里有一个大写的 L，还有在数据集中，我忘了在这里保存它，这里我也写成了大写，所以 L 是大写的，现在训练正在进行中，你可以看到训练相当快，至少在我的电脑上是这样，实际上并不那么快，由于我选择了 8 的批量大小，我可以尝试增加它，它正在 CUDA 上进行，损失在减少，权重将保存在这里，

所以如果我们到达 epoch 的末尾，它将在这里创建第一个权重，所以让我们等到 epoch 结束，看看权重是否真的创建了，在实际完成模型训练之前，让我们再做一件事，我们还希望在训练过程中可视化模型的输出，这被称为验证，所以我们想看看我们的模型在训练过程中是如何演变的，所以我们想要构建的是一个验证循环，它将允许我们评估模型，这也意味着我们想从这个模型中推理并检查一些样本句子，看看它们是如何被翻译的：所以让我们开始构建验证循环，

我们做的第一件事是构建个名为 runValidation 的新方法，这个方法将接受一些我们将使用的参数，现在，我只是把它们都写下来，稍后我会解释它们将如何使用，好的，我们运行验证的第一件事是将我们的模型设置为评估模式，所以我们执行 model.eval, 这意味着这告诉 PyTorch 我们将要评估我们的模型，然后我们将推理两个句子，看看模型的输出是什么，所以通过 torch.NoGrad 我们禁用了在这个宽度块内运行的每个张量的梯度计算，而这正是我们想要的，

我们只是想从模型中推理，我们不希望在这个循环中训练它，所以让我们从验证数据集中获取一个批次，因为我们只想推理两个，所以我们记录我们已经处理了多少个，并从这个当前批次中获取输入，我想提醒你，对于验证 ds，我们只有一个批次大小，这是编码器输入，我们也可以获取编码器掩码，

让我们确认一下批次的实际大小确实是一个，现在让我们进入有趣的部分，所以，如你所记得的，当我们想要推理模型时，我们需要只计算一次编码器输出，并将其重用于我们将要处理的每个 token，模型将从解码器输出，所以让我们创建另一个函数，它将在我们的模型上运行贪婪解码，我们将看到它只会运行一次编码器，所以让我们称这个函数为 greedy_decode，

好的，让我们创建一些我们将需要的 token，所以 Sos token，即句子的开始，我们可以从任一tokenizer 中获取，无论是目标还是源，它们都有，eos 好的，然后我们做的是我们预先计算编码器输出，并将其重用于我们从解码器获得的每个 token，所以我们只给出源和源掩码，即编码器输入和编码器掩码，我们也可以称之为编码器输入和编码器掩码，然后我们，好的，我们如何进行推理？我们做的第一件事是给解码器句子的开始 token，以便解码器将输出翻译句子的第一个 token，然后在每次迭代中，就像我在幻灯片中展示的那样，每次迭代我们都将前个 token 添加到解码器输入中，以便解码器可以输出下一个 token，然后我们取下一个 token，再次将其放在解码器输入的前面，并获取后续的 token，所以让我们为第一次送代构建一个解码器输入，它只包含句子的开始 token，

我们用句子的开始 token 填充这个，它与编码器输入具有相同类型，现在，我们将不断要求解码器输出下一个 token，直到我们达到句子的结束 token 或我们在这里定义的最大长度，所以我们可以做个 while True 循环，然后我们的第一个停止条件是，如果解码器输出，即下一步的输入大于或达到最大长度，这里，为什么我们有两个维度？一个是用于批次，另一个是用于解码器输入的 token，现在我们也需要为此创建一个掩码，我们可以使用我们的函数 causal mask 来表示我们不希望输入看到未来的词，我们不需要另一个掩码，因为如你所见，这里没有任何填充 token，现在我们计算输出。

我们为循环的每一次送代重用编码器的输出，我们重用源掩码，即编码器的输入和掩码，然后我们提供解码器输入，以及它的掩码，即解码器掩码，然后我们得到下一个 token，所以我们使用投影层获取下一个 token 的概率，但我们只想要最后一个 token 的投影，即我们在编码器之后给出的最后一个 token 的下一个 token，现在我们可以使用 max，这样我们就能得到具有最大概率的 token，这就是贪婪搜索，

然后我们得到这个词，并将其追加回来，因为它将成为下一次送代的输入，我们进行连接，所以我们取解码器输入并追加下个 token，所以我们创建另一个张量，应该是正确的，好的，如果下个词或 token 等于句子的结束 token，那么我们也停止循环，这就是我们的贪婪搜索，

现在我们可以直接返回输出，所以输出基本上就是解码器输入，因为你每次都将其追加下一个 token，并且我们移除了批次维度，所以进行了压缩，这就是我们的贪婪解码.

现在我们可以在这个函数中使用它，所以在验证函数中，我们终于可以得到模型输出，等于贪婪解码，我们给它所有参数，然后我们想要将这个模型输出与我们预期的结果进行比较，即与标签进行比较，所以让我们把这些全部追加进去， 所以我们给输入的，我们给了模型，模型的输出，即预测的输出，以及我们预期的输出，我们将所有这些保存在这些列表中，然后在循环结束时，我们将在控制台上打印它们，为了获取模型输出的文本，我们需要再次使用 tokenizer 将 token 转换回文本，当然，我们使用目标 tokenizer，因为这是目标语言，

好的，现在我们将所有这些保存到各自的列表中，我们也可以在控制台上打印出来，为什么我们使用这个名为 printMessage 的函数，而不是直接使用 Python 的 print？

因为我们在主循环，即训练循环中使用这里，TKODM，这是一个非常漂亮的进度条，但在进度条运行时不建议直接在控制台上打印，所以在控制台上打印，TKODM 提供了一个名为 print 的方法，我们将这个方法传递给这个函数，以确保输出不会干扰进度部分的打印，我们打印一些条形图，然后打印所有消息，如果我们已经处理了一定数量的示例，那么我们就中断，

那么为什么我们要创建这些列表呢？嗯，实际上我们也可以将所有这些发送到  tensorboard，所以我们可以，例如，如果我们启用了 tensorboard，我们可以将所有这些发送到 tensorboard，为此，实际上我们需要另一个库，它允许我们计算一些指标，我想我们可以跳过这部分，但如果你真的很感兴趣，我在我发布在 GitHub 上的代码中，你会发现我使用了这个名为 torchmatrix 的库，它允许计算字符错误率、BLEU指标，这对翻译任务非常有用，还有词错误率，所以如果你真的感兴趣，你可以在 GitHub 上找到代码，但对于我们的演示，我认为这并不必要，所以，实际上我们可以移除它，因为我们没有做这部分。

好的，现在我们有了 runValidation 方法，我们可以直接调用它，我通常的做法是在每隔几步后运行验证，但由于我们希望尽快看到结果，我们将首先在每次送代时运行它，并且我们还把这个 modeltrain 放在这个循环中，以便每次运行验证后，模型都会回到训练模式，所以现在我们可以直接运行验证，并给它所有运行验证所需的参数，给它模型，好的，用于打印消息，我们在打印任何消息吗？是的，我们在打印，所以让我们创建一个 lambda，我们只需做，这是要写的消息与 TQDM，然后我们需要给出全局步骤和 writer，我们不会使用它，但好的，现在我想我们可以再次运行训练，看看验证是否有效，好的，看起来它在工作，所以模型没问题，它在每一步都运行验证，这完全不是我们想要的，但至少我们知道贪婪搜索在工作，而且它不是，至少看起来它在工作，模型没有预测任何有用的东西，实际上它只是在预测一堆逗号，因为它完全没有训练，但如果我们训练模型一段时间后，

我们应该会看到在几个 epoch 之后，模型会变得越来越好，所以让我们停止这个训练，并把这个放回它该在的地方，即每个 epoch 的末尾，是的，这个我们可以留在这里，没问题，是的。

我现在将快进到一个已经预训练好的模型，我预先训练了它几个小时，这样我们就可以进行推理并可视化注意力，我已经复制了预先计算好的预训练权重，我还创建了这个笔记本，重用了我们在训练文件中之前定义的函数，代码非常简单，

实际上，我只是从训练文件中复制粘贴了代码，我只是加载模型并运行验证，使用我们刚刚写的方法，然后我在预训练上运行了验证，让我们再运行一次，例如，如你所见，模型正在推理 10 个例子，句子，结果还不错，我的意思是，我们可以看到 11 个微笑，11 个故事，它在匹配，而且大多数都匹配，实际上，我们也可以说它，几乎过度拟合了这个特定数据，但这就是 transformer 的力量，我没有训练它很多天，我只训练了几个小时，如果我没记错的话，结果真的非常好，

现在让我们制作一个笔记本，用来可视化这个预训练模型的注意力，根据我们之前构建的文件，所以，trainpi 你也可以选择你喜欢的语言来训练你自己的模型，我强烈建议你改变语言，看看模型的表现如何，并尝试诊断为什么模型表现不佳，如果表现不好或表现良好，尝试理解如何进一步改进它，所以让我们尝试可视化注意力，

所以让我们创建一个新的笔记本，让我们称之为，比如说，注意力可视化，好的，我们做的第一件事是导入所有需要的库，我还将使用一个叫做 Altair 的库，这是一个用于图表的可视化库，它实际上与深度学习无关，它只是一个可视化功能，特别是可视化功能，实际上，我在网上找到的，不是我写的，就像大多数可视化功能一样，如果你想构建一个图表或直方图，你可以很容易地在互联网上找到，所以我主要使用这个库，因为我从网上复制了代码来可视化它，但其余的都是我自己的代码，

所以让我们导入它，好的，让我们导入所有这些，当然，当你在电脑上运行代码时，你需要安装这个特定的库，我们还要定义设备，你可以直接从这里复制代码，然后我们加载模型，我们可以像这样从这里复制，好的，让我们粘贴到这里，这个变成了词汇源和词汇目标，

好的，现在让我们创建一个加载批次的功能，我现在将使用 tokenizer 将批次转换为标记，当然，对于解码器，我们使用目标词汇，即目标 tokenizer，所以让我们使用我们的贪心解码算法进行推理，所以我们提供模型，我们返回所有这些信息，

好的，现在我将构建必要的功能来可视化注意力，我将从另一个文件复制一些函数，因为实际上我们要构建的内容从学习角度来看并不有趣，就深度学习而言，它主要是用于可视化数据的函数，所以我会复制它，因为写起来相当长，当然，我会解释其中的关键部分，

这就是那个函数，好的，这个函数是做什么的呢？基本上，我们会从编码器那里得到注意力，如何从编码器获取注意力，例如，我们在三个位置有注意力，第一个在编码器中，第二个在解码器的开始，即解码器的自注意力，然后我们有编码器和解码器之间的交叉注意力，所以我们可以可视化三种注意力，如何获取关于注意力的信息，我们加载模型，我们有编码器，我们选择要从哪个层获取注意力，然后从每个层我们可以获取自注意力块及其注意力分数，这个变量从哪里来？

如果你记得当我们在这里定义注意力计算时，当我们计算注意力时，我们不仅返回输出到下一层，我们还给出这个注意力分数，它是 softmax 的输出，我们将其保存在这个变量中，self.attentionScores，现在我们可以直接获取并可视化它，所以这个函数将根据我们想要从哪一层和哪个头获取注意力，选择正确的矩阵，这个函数构建一个数据框来可视化信息，即从该矩阵中提取的标记和分数，

在这里，我们将从这个矩阵中提取行和列，然后我们还会构建图表，图表是用 Altair 构建的，实际上我们要做的是获取所有头的注意力，我构建了这个方法来获取我们作为输入传递给这个函数的所有层和所有头的注意力，所以现在让我运行这个单元格，

好的，让我们创建一个新单元格，然后运行它，好的，首先我们想要可视化我们正在处理的句子，即批次，其他输入标记，所以我们加载一个批次，然后我们可视化源和目标，还有目标，最后我们还计算了长度，

长度是什么？好的，基本上是所有在填充字符之前的字符，即第一个填充字符的出现，因为这是从数据集中提取的批次，已经是为训练构建的张量，所以它们已经包含了填充，在我们的例子中，我们只想获取句子中实际字符的数量，所以这个，我们可以，句子中实际单词的数量，所以我们可以检查填充之前的单词数量，所以让我们运行这个，这里有一些问题，我忘了这个函数是错误的，所以现在应该可以了，这个句子太短了，让我们找一个更长的，好的，让我检查一下质量，你不能保持现状，尤其是你，我们不能，你们不能继续这样，尤其是现在，好的，看起来不错，好的，让我们打印第 0、1、2 层的注意力，因为我们有六层，如果你记得，参数 n 等于 6，所以我们只可视化三层，并且我们会可视化所有头，每层有八个头，所以头编号：0、1、2、3、4、5、6 和 7

好的，首先让我们可视化编码器自注意力，我们确实得到了所有注意力图，我们想要哪一个，所以编码器的一个，我们想要这些层和这些头，原始标记是什么，编码器输入标记，我们在列中想要什么？因为我们打算构建一个网格，所以如你所知，注意力是一个将行与列相关联的网格，在我们的例子中，我们谈论的是编码器的自注意力，所以这是同一个句子在自我关注，所以我们需要在行和列上都提供编码器的输入句子，我们想要可视化的最大长度是多少？

好的，假设我们想要可视化不超过 20 个，所以是 20 和句子长度的最小值，好的，这是我们的可视化结果，我们可以看到，正如我们所预期的，实际上，当我们可视化注意力时，我们预期对角线上的值会很高，因为这是每个标记与其自身的点积，我们还可以看到其他有趣的关系，

例如，假设句子开始标记和句子结束标记，至少对于头 0 和层0，它们与其他词没有关联，就像我实际预期的那样，但其他头确实学习了一些非常小的映射，我们可以看到，如果我们悬停在每个网格单元格上，我们可以看到自注意力的实际值，例如自注意力的分数，我们可以看到这里的注意力非常强，所以 "especially" 和 "specially“ 这两个词是相关的，所以这是同一个词与自身的关联，但还有 "especially” 和 “now”，我们可以为所有层可视化这种注意力，

所以因为每个头会观察每个词的不同方面，因为我们均匀地分配了词嵌入到各个头，所以每个头会看到词嵌入的不同部分，我们也希望它们学习不同类型的词间映射一一这实际上是事实一一以及层与层之间的不同映射，我们还有不同的 Q、W、K 和 WV 矩阵，所以它们也应该学习不同的关系，

现在我们可能还想可视化解码器的注意力，所以让我们来做这个，让我复制代码并更改参数，好的，这里我们想要解码器 1，我们想要相同的层等，但行和列上的标记将是解码器标记，所以解码器输入标记和解码器输入标记，让我们可视化一一而且我们现在应该看到意大利语，因为我们使用的是解码器自注意力，确实是这样，所以在这里我们看到解码器端不同的注意力类型，而且我们还有多个头，它们应该学习不同的映射，以及不同的层应该学习词之间的不同映射，我发现最有意思的是交叉注意力，

所以让我们来看一下，好的，让我复制代码并再次运行它，好的，所以，如果你记得这个方法，它是编码器、解码器，相同的层，所以在这里，行上我们将显示编码器输入，列上我们将显示解码器输入标记，因为这是编码器和解码器之间的交叉注意力，好的，这就是编码器和解码器之间交互的大致方式以及它是如何发生的，所以这就是我们找到交叉注意力的地方，它是使用来自编码器的键和值计算的，而查询来自解码器，所以这实际上是翻译任务发生的地方，这就是模型如何学习将这两句话相互关联，以实际计算翻译。

所以我邀请你们自己运行代码，我给你们的第一个建议是，和我一起看视频并编写代码，你可以暂停视频，自己编写、运行代码，好的，让我给你们一 些实际的例子，例如，当我编写模型代码时，我建议你先看我编写一个特定层的代码，然后暂停视频，自己编写，花点时间，不要马上看解决方案，尝试找出哪里出错了，如果你真的在一两分钟后还是无法找出问题所在，你可以快速看一下视频，但尽量自己解决，当然，有些事情你无法自己想出来，所以，例如，对于位置编码和所有这些计算，这基本上只是公式的应用，但重点是，你至少应该能够自己构思出一个结构，所以所有层是如何相互作用的，这是我的第一个建议，而在训练循环方面，训练部分实际上是非常标准的，所以它与其他你可能见过的训练循环非常相似，有趣的部分是我们如何计算损失，以及我们如何使用 Transformer 模型，最后真正重要的是我们如何进行模型推理，这在贪心代码中，

所以感谢大家观看视频并陪伴我这么长时间，我可以保证这是值得的，我希望在接下来的视频中，能展示更多我熟悉的 Transformer 和其他模型的例子，并且我也想和你们一起探索，所以如果有什么你不明白的或者你想让我解释得更清楚的，请告诉我，我也一定会关注评论区的，请给我留言，谢谢, 祝你有个美好的一天，


---------

## Attention is all you need

大家好，欢迎来到我的视频，今天我们要聊的是 Transformer，这实际上是我 Transformer 系列视频的 2.0 版本，我之前做过一个关于 Transformer 的视频，但音频质量不太好，根据观众的建议，那个视频反响非常好，所以观众建议我改进音频质量，这就是我制作这个视频的原因。你不需要看之前的系列，因为我这次基本上会做同样的内容，但会有一些改进，我会弥补之前的一些错误，并加入一些可以提升的地方，看完这个视频后，我建议你接着看看我另一个关于如何从零开始编写 Transformer 模型的视频，所以我会降到如何编写模型本身，如何在数据上训练它，以及如何进行推理。跟我一起坚持下去，虽然这会是一段稍长的旅程，但绝对值得。

现在，在我们讨论 Transformer 之前，我想先谈谈循环神经网络，在引入 Transformer 之前，用于大多数序列到序列任务的网络，让我们来回顾一下它们，

循环神经网络在 Transformer 出现之前就已经存在很久了，它们能够将一个输入序列映射到另一个输出序列，在这种情况下，我们的输入是 $X$，我们希望得到输出序列 $Y$，我们之前的方法是将序列拆分成多个项目，所以我们先给循环神经网络输入第一给项目 $x_1$, 
