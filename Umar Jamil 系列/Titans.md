
[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)

环模型更有效。它们还能够在大于 200 万的上下文窗口大小中，以更高的准确性在“大海捞针”任务中超越基线模型。

### 1、引言

线性模型来增强可扩展性和效率（线性 vs. 二次复杂性），其优势在于处理非常长的上下文；另一方面，非常长的上下文无法在小的向量值或矩阵值状态中得到适当压缩。

#### 1.1 记忆视角

RNNs 可以被定义为具有向量值记忆模块 $M$（也称为隐藏状态）的模型，包含两个主要步骤：在时间  $t$ 给定新输入 $x_t$，模型（1）使用函数 $f(M_{t-1}, x_t)$ 更新记忆（带压缩）；（2）使用函数 $g(M_t, x_t)$ 检索输入的相应记忆（详见 §2.1）。

类似地，Transformers 可以被视为具有增长记忆的架构，并包含两个类似步骤。即，键和值矩阵对充当模型的记忆，模型：（1）通过将键和值附加到记忆中来更新记忆（无压缩）；（2）通过找到查询和键向量的相似性来检索查询向量的相应记忆，然后使用该相似性加权值向量以获得输出。

这种视角可以帮助我们更好地理解现有范式、它们的关键差异，并设计更有效的架构。例如，Transformers 和线性 Transformers 之间的主要区别在于记忆结构以及记忆更新步骤，其中线性 Transformers 将历史数据压缩到固定大小的矩阵值记忆中，而 Transformers 保留所有历史数据（在上下文长度内）而不进行压缩。虽然线性 Transformers 和线性 RNNs（包括状态空间模型）在记忆更新步骤中压缩信息，但关键区别在于记忆的结构，其中线性 RNNs（与线性 Transformers 相比）使用向量值记忆（与矩阵值记忆相比）。因此，这种视角促使我们提出以下问题：
**（Q1）什么构成了良好的记忆结构？（Q2）什么是适当的记忆更新机制？（Q3）什么是良好的记忆检索过程？（Q4）如何设计一种有效的架构来整合不同的互联记忆模块（Q5）是否需要一个深度记忆模块来有效地存储/记住过去的长久历史？**

#### 1.2 贡献和路线图

**神经记忆（§3**）我们提出了一种（深度）神经长时记忆，它作为一个元上下文模型，在测试时学习如何将数据存储到其参数中。受到人类长时记忆系统的启发，我们设计了这个记忆模块，使得违反预期（令人惊讶）的事件更容易被记住。为此，我们通过关联记忆损失中神经网络相对于输入的梯度来衡量输入的惊讶度（详见 §3.1）。为更好地处理有限记忆，我们提出了一种衰减机制，考虑到记忆大小和数据惊讶度的比例，从而实现更好的记忆管理。我们展示了这种衰减机制实际上是现代递归模型中遗忘机制的推广。有趣的是，我们发现这种机制相当于用小批量梯度下降、动量和权重衰减优化一个元神经网络。基于将小批量梯度下降张量化以使用更多矩阵乘法操作，我们提出了一种快速且可并行化的算法来训练我们的深度神经长时记忆。

**Titans 架构（§4）**  在设计长时神经记忆后，一个重要的问题是如何有效地将记忆集成到深度学习架构中。我们提出了 Titans，一系列深度模型，包括三个超头： (1) 核心：该模块包含短时记忆，负责数据处理的主要流程（我们使用有限窗口大小的注意力）；(2) 长时记忆：这一分支是我们的神经长时记忆模块，负责存储/记住过去的长时信息；(3) 持久记忆：这是一组可学习但与数据无关的参数，用于编码任务的知识。最后，作为概念验证，我们提出了 Titans 的三个变体，其中我们将记忆作为：(i) 上下文，(ii) 层，和 (iii) 门控分支。

**实验结果（§5）**  我们在语言建模、常识推理、召回密集型、针尖查找、时间序列预测和 DNA 建模任务上进行了实验评估。我们观察到，Titans 架构在全面的基准测试中优于所有现代递归模型及其混合变体（结合滑动窗口注意力）。此外，Titans 在相同上下文窗口下优于 Transformers，并在使用整个上下文的 Transformers 中表现出竞争力。与 Transformers 相反，Titans 可以扩展到大于 200 万的上下文窗口大小。





我们希望注意力层可以利用递归神经网络压缩的信息来预测下一个标记。如果我们这样做，想象一下我们有这种架构，也就是注意力加上递归网络的混合架构。这个架构的问题在于，当你训练它时，由于深度学习的特性，我们强迫模型学习任何目标，它将被迫以某种方式压缩信息，以便注意力层可以使用它，而注意力层将被迫从递归神经网络压缩的状态中提取任何信息。

这很好，所以当你训练它时，损失降低，你会发现它表现得相当好。然而，当你在实际使用中输入提示时，模型可能没有在过去见过这样的数据，我们称之为分布外数据。模型可能不知道如何很好地压缩它，不知道该保留什么，不该保留什么。在这种情况下，递归网络将在压缩数据的任务中失败，因为预测下一个标记所需的数据没有被很好地压缩，注意力层将无法利用这些数据来预测下一个标记。

因此，在训练时，我们看到这种混合架构表现得很好，但在测试时，也就是在实际使用中，我们发现它们并不那么有效。这是原因之一：它们只学会压缩在训练时见过的信息。比如，如果它看到一段长的Python源代码，它会知道不应该关注一些可能重复的注释，而应该关注代码本身，或者在看到C代码时，不应该关注可能只是冗余的括号，而应该关注表达式等等。

它实际上学会了压缩信息，但仅限于在训练时见过的信息。现在我们可以谈谈这篇论文。论文声称，我们有这些需要某种记忆的模型。在变换器模型中，我们有这个K缓存。K缓存的问题在于它会增长。K缓存增长的问题在于它需要大量内存。实际上，大多数模型的限制在于我们无法在当前模型中拥有非常大的上下文窗口，因为这些模型的推理成本非常高。

这些模型非常昂贵，因为我们需要保留每一层的K缓存，而更大的模型有很多层，所以你需要为模型的每一层保留所有标记，以预测每个标记。因此，它非常昂贵。解决这个不断增长的无限记忆的方法是拥有一个压缩记忆，但这种压缩记忆仅在训练时效果很好。

论文的主张是，我们能否拥有一个在测试时训练的记忆模块，并且在检索时有效，因为记忆的目标是在测试时检索出模型需要的重要信息，而不仅仅是在训练时见过的信息。这就是我们试图用 Titans 解决的问题。

他们的做法如下：

他们说，好吧，想象一下我们有一个模块，我们称之为M模块。把它想象成模块中的一层。让我画一下，实际上画出来会更容易理解。假设我们有一个非常长的序列。我们已经看到递归网络的任务是压缩这个非常长的序列，以便变换器可以使用它。现在让我们看看Titans是如何不同的，然后我们会检查所有的细节。

我们有这个输入，再次来看一下，我们将其转换为嵌入。然后，我会以不同的方式画出来，稍后我会解释为什么。假设我们再次有一个变换器和递归层的混合架构，但我不会画递归层。这是第一层，我想这是第一层，我们称之为L1。第一层有注意力，第二层有注意力，第三层有注意力，然后我们有输出，即logits。我想现在更清楚了，对吧？

假设在这个架构中，我们有另一个模块，我们称之为记忆模块。我们称之为神经记忆，因为他们在这里是这样称呼的。我会把它画成一个外部模块——神经记忆。现在我想向你展示它如何与神经记忆一起工作，然后我们检查它是如何实际训练的。

通常我们如何训练模型呢？假设我们给它一个序列，想象一下100万个标记。假设一个非常大的序列，比如说100万个标记。你将这个标记序列转换为嵌入，然后在神经网络中运行这些嵌入，递归神经网络会将这100万个标记压缩成可能1000个标记，因为它的目标是压缩信息。

因为注意力的问题是它的计算复杂度是平方的，所以更小的输入会带来更好的计算效果。我们将这1000个压缩标记输入到注意力层，然后强制它仅利用这1000个压缩标记来预测下一个标记。我们输入100万个标记，但我们强制注意力层仅利用更少的信息来预测下一个标记。我们希望递归网络擅长选择保留哪些标记，而丢弃哪些标记。

实际上，这不是一个标记修剪机制，而是一个标记压缩机制。不过，你可以把它想象成一个标记修剪过程：它接收100万个标记，只保留最重要的1000个标记用于预测下一个标记。这是在训练时进行的。我们输入100万个标记，在训练时计算输出，我们知道下一个标记应该是什么，因为在训练时我们知道下一个标记是什么。我们计算损失与我们认为应该是下一个标记的差异，然后反向传播以更新模型的参数，并对我们拥有的所有序列重复这个过程。

对于Titans，它会以不同的方式工作。想象一下，你再次有100万个标记，你会进行两个步骤。首先，我们有这个输入，将其转换为嵌入。在训练循环中，想象我们正在训练这个Titans架构，我们首先训练这个神经模块来学习记忆我们的100万个标记，然后要求它检索预测下一个标记所需的信息，并将其提供给注意力层。

所以这是一个注意力层，这是一个注意力层，这是一个注意力层。看看这里的区别：之前我们有一个输入，预测输出，计算损失，反向传播并更新模型的所有参数。在这里，我们会做一些不同的事情。我们有一个输入，即100万个标记，将它们转换为嵌入，等等。

例如，我们在整个维基百科、整个网络、大量书籍等上训练语言模型。模型几乎见过世界上所有可能的数据。然而，我们希望当我们有一个混合模型时，比如一个结合了 Transformer 和递归神经网络的模型，

假设这里有一个注意力层（Transformer 层），我们称之为注意力层，而这里是一个递归神经网络。假设这是一个可以并行化的新型递归网络架构，但问题在于，递归神经网络会产生一个固定大小的记忆。如果输入 1000 个标记，它会输出一个记忆，这个记忆会被注意力层利用，但不会是 1000 个标记，因为递归网络的目标是将信息压缩成固定大小的记忆，以便 Transformer 模型（即这里的注意力层）利用。

注意力层非常擅长利用输入的数据，但这些数据并不是整个序列，因为我们用递归神经网络压缩了它。我们希望注意力层能利用递归网络压缩的信息来预测下一个标记。如果我们这样做，想象一下我们有一个注意力加递归网络的混合架构，这种架构的问题在于，当你训练它时，由于深度学习的特性，我们强迫模型学习任何目标，它会被迫学习以某种方式压缩信息，以便注意力层可以使用它，而注意力层会被迫提取递归神经网络压缩状态中的信息。

这很好，所以当你训练它时，损失会减少，你会发现它表现得相当好。然而，当你在实际使用中输入提示时，可能不是语言模型过去见过的数据，我们称之为分布外数据。模型可能不知道如何很好地压缩它，哪些信息该保留，哪些不该保留。在这种情况下，递归网络在压缩数据的任务上会失败，因为预测下一个标记所需的数据没有被很好地压缩，注意力层将无法利用这些数据来预测下一个标记。因此，在训练时我们看到这种混合架构效果很好，但在测试时（即实际使用时），我们发现它们效果不佳。这是其中一个原因：它们学会了很好地压缩它们见过的数据。

例如，如果它看到一段长的 Python 源代码，它知道不应该关注可能重复的注释，而应该关注代码；或者当它看到 C 代码时，不应该关注括号，因为它们只是冗余的，而应该关注表达式等等。因此，它实际上学会了压缩信息，但仅限于训练时见过的信息。

现在我们可以谈谈这篇论文。论文声称我们有这些需要某种记忆的模型。在 Transformer 模型中，我们有这个 K 缓存，问题是 K 缓存会增长。K 缓存增长的问题在于它需要大量内存。实际上，大多数模型的限制在于我们无法在当前模型中拥有很大的上下文窗口，因为这些模型的推理成本非常高。因为我们需要保留 K 缓存，K 缓存是每一层都有的，对于较大的模型，它们有很多层，所以你需要为模型的每一层保留所有标记，以预测每个标记。这非常昂贵。解决这个无限增长的记忆问题的方法是使用压缩记忆，但这种压缩记忆仅在训练时效果很好。

所以这个观点是，我们能不能有一个在测试时训练的记忆模块，这就是为什么我们要讨论在测试时学习记忆，这个模块要能高效检索信息，因为记忆的目标就是检索那些重要的、当前模块需要的信息，它要能有效检索那些在测试时实时输入的信息，而不仅仅是训练时见过的数据。这就是我们试图用Titans解决的问题。  

他们的做法是这样的：想象我们有一个模块，我们叫它M，这个模块可以看作是网络中的某一层。让我画出来可能更清楚，我们新建一页来画。假设我们有一个很长的序列，我们知道循环神经网络的任务就是压缩这个长序列，这样Transformer才能处理它。现在看看Titans有什么不同，然后再看细节。  

我们有这个输入，转换成嵌入向量，然后我稍微换个方式画。假设我们有一个混合架构，又是Transformer和循环层，但我不画循环层。这是Titans的第一层，叫它L1，第一层带注意力，第二层带注意力，第三层带注意力，然后输出logits。这样应该更清晰了。  

现在想象在这个架构里还有另一个模块，我们叫它记忆模块，或者神经记忆，因为论文里是这么叫的。我把它画成一个外部模块——神经记忆。现在我要展示它如何工作，然后再看具体是怎么训练的。  

通常我们训练模型的方式是：假设输入一个很长的序列，比如100万token，把它转换成嵌入向量，然后用循环神经网络压缩，比如压缩到1000个token，因为它的目标就是压缩信息。这样注意力层处理的序列就更短，因为注意力机制的计算复杂度是平方级的，输入越小计算效率越高。  

然后我们强迫模型只用这1000个压缩后的token去预测下一个token。输入是100万token，但注意力层只能基于这1000个token做预测。我们希望循环网络能学会保留重要的token，丢弃不重要的。其实它更像是一种token压缩机制，但你可以理解为token剪枝，比如从100万token里选出最重要的1000个。  

这是在训练时做的：输入100万token，计算输出，因为训练时我们知道下一个token应该是什么，所以我们可以计算损失，然后反向传播更新模型参数，对所有序列都这样训练。  

Titans的做法不同：假设还是100万token，我们分两步走。首先，输入转换成嵌入向量，然后在训练循环中做两件事


所以想象一下我们在训练这个Titans架构 我们首先训练这个Neal模块让它学会记住我们的100万个token 然后我们要求它检索预测下一个token所需的信息 并将其输入到注意力层 所以这个我们称之为注意力层 这是一个注意力层 这是一个注意力层 这是一个注意力层 看看这里的区别 之前我们有一个输入 我们预测输出 计算损失 反向传播 然后更新模型的所有参数 这里我们会做一些不同的事情 我们有一个100万个token的输入 我们将它们转换成嵌入向量等等 我们在这里训练这个单独的模型 在论文中他们称之为训练的内循环 我们训练这个神经记忆 稍后我们会看到如何训练它 唯一目的就是让这个神经记忆学习关于这些数据的所有信息 这样它就能在需要时轻松检索这些数据 所以我们获取这100万个token 将它们转换成嵌入向量 我们在内循环中训练这个神经记忆 然后我们获取这个已经训练好记住这些数据的神经记忆 然后我们要求它从它见过的所有信息中检索出任何重要的信息 并将其作为注意力层的输入 这样注意力层就可以利用这个压缩的记忆来产生输出并预测下一个token 这不仅仅是在训练时 在测试时也是如此 所以当我们使用混合架构的注意力时 比如注意力加循环神经网络 在测试时也就是推理时 我们通常有一个提示 想象这个提示非常大 因为你要求比如ChatGPT分析一个非常大的GitHub仓库的整个代码库 会发生的情况是 这100万个token会被输入到现在已经固定的循环神经网络中 所以我们是在使用模型 不再改变它的参数了 循环神经网络的工作是压缩数据 所以它会将这些token压缩成一个更短的序列 我们会将其输入到注意力层 然后它会输出logits 然而可能我们输入到这个循环神经网络的信息有些超出分布 循环神经网络从未见过类似的东西 它可能会在压缩这些数据时表现非常糟糕 因为它不知道该保留什么不该保留什么 注意力层就无法利用最重要的信息 然后它就无法很好地预测下一个token 所以会导致糟糕的输出 而使用Titans 即使在测试时也就是推理时 我们实际上是在训练一个模型 现在我来展示具体做法 想象现在我们又有一个GitHub仓库 它非常大 导致我们想让语言模型分析的100万个token 我们将其转换成嵌入向量 然后我们获取这100万个token 即时训练这个神经记忆 它的工作就是尽可能多地学习关于这100万个token的信息 检索最重要的信息 因为记忆的工作就是压缩信息 所以现在在我们在这个内循环中训练它之后 我们检索这些信息 将其输入到注意力层 然后注意力层应该能够利用神经记忆检索到的信息 所以基本上使用Titans 我们不仅仅有一个在训练时训练好之后就再也不训练的RNN作为我们的记忆 每次它看到从未见过的东西就会表现失常 我们有一个可以在推理时即时训练的神经记忆 唯一目的就是压缩东西 因为我们在推理时训练它 我们希望它即使在从未见过的数据上也能表现更好 现在根据他们在论文中发布的基准测试 不过这在所有论文中都会发生 所以你永远不要相信基准测试 看起来它现在做得不错 现在让我们看看细节 我想提醒你们我们正在解决的问题是长上下文建模 长上下文建模有一个问题 就是使用Transformer时 对长上下文进行推理非常昂贵 使用RNN时 我们遇到的问题是我们在一些数据上训练它们 但是当你用它们处理从未见过的东西时 它们不知道如何压缩 不知道该保留什么不该保留什么 所以它们会表现失常 因为它们表现失常 它们做不好这项工作 注意力层无法利用这些信息 所以它们只会产生非常糟糕的输出 使用神经记忆 我们想在推理模型的同时即时训练一个记忆 唯一目的就是压缩输入给它的任何数据 现在我们可以看看细节

好的 嗯 好的 这里他们先做了些初步的 呃 怎么说 嗯 关于记忆或线性注意力等的概述 我们现在暂时不关心这个 他们说 好吧 假设我们有个只有两种操作的内存模块 一个是写入操作 一个是读取操作 嗯 我们想在推理时和训练时都对这个内存进行读写 该如何训练这个内存呢 首先 这个神经记忆本身是个神经网络 也就是说你可以把它看作独立于其他架构的外部神经网络 嗯 那个架构会使用这个 呃 神经记忆 所以你得想象有个Transformer模型正在利用这个 呃 神经记忆 嗯 现在问题来了 怎么在推理时训练这个神经记忆 因为训练时我们知道怎么做 我们只要 呃 计算输出 反向传播 就搞定了 但在推理时该怎么做 这就是他们这里探讨的 他们说 好吧 假设我们有这个内存 呃 首先 我们想怎么更新它的信息 他们想更新内存里的信息 呃 好吧 再退一步 我们想让这个内存做什么 我们想让这个内存学会 呃 提取它该记住的任何信息 为此他们用了非常特殊的损失函数 就是那种重建损失 假设我们有这个内存 如果我们要求它记住 呃 好吧 假设我们有个输入序列 就叫它X吧 这个XT XT这里 我们用两个线性投影WK和W来映射它 基本上就相当于注意力机制里用的那种 这个 呃 内存怎样才能很好地工作呢 只有当它学会重建见过的数据时才行 呃 这就是你在这里看到的损失函数 就是这里的L2损失 嗯 它学习记住键投影和V投影之间的映射关系 所以算是学会重建相同的数据 嗯 这就是内存的职责 所以如果我存入某些东西 就应该能取出同样的东西 应该能尽可能还原存入的内容 怎么训练它呢 他们说 好吧 我有这个内存 想通过类似梯度下降的方法来更新 梯度下降怎么运作的 呃 想象我们有个普通网络 梯度下降的基本版本 呃 是这样运作的 我们有个带参数的网络 参数就叫θ吧 比如θ 嗯 在训练第i步时的参数θ 是用模型前一步的参数来更新的 所以是前一个时刻的参数 减去我们称为γ的学习率 乘以损失函数对模型参数的梯度 这个梯度告诉我们该如何改变 呃 参数才能最大化损失 但我们朝着梯度相反方向更新 所以你会看到减号 我们朝着最小化损失的方向更新参数 这里就是这么做的 我们说想这样更新内存 使得最小化这个损失 就是前面看到的记忆损失 也就是重建损失 这个损失表示 如果我让内存通过数据的键投影来检索信息 它就该重建这个数据 嗯 在论文里他们把这个内存建模成线性层

线性层本质上只是一个带有权重矩阵的矩阵乘法运算。因此这个记忆模块M，它不过就是一个线性层的权重矩阵W。我们以这样的方式修改这个权重矩阵W，使其能够最小化数据的重构损失——就像我们训练神经网络时调整参数来降低损失函数一样。这些参数的计算方式使得它们能够产生尽可能小的损失值。同样地，我们更新这个作为记忆的W矩阵，使其能够产生最小的信息损失，因为这就是我们优化的目标——重构损失。

当我们计算这个损失时，他们称之为"surprise"（意外程度）。这个记忆矩阵W相对于损失的梯度（∂L/∂W）被称为"surprise"，因为损失越大，模型重构数据的难度就越大，这意味着模型对这个数据感到越"意外"。

如果你曾经研究过优化器的工作原理，你会记得深度学习中有一个叫做"动量"的概念。通常我们不会简单地直接这样更新模型参数，因为：

1. 首先，损失是通过小批量梯度下降计算的
2. 这意味着我们不是在整个数据集上计算，而是在数据的小批量上计算
3. 这个梯度的方向实际上是随机的，不是真实的梯度方向
4. 它会不断波动，平均而言会指向正确的梯度方向，但每一步都是带有噪声的

因为我们不想在每一步训练中过于自信地采取更新步骤，所以我们加入这个动量项。动量项基本上创建了所有梯度的指数移动平均值，这样我们就能保留一些关于过去计算过的梯度的信息，从而平滑权重的变化，避免更新幅度过大。

他们引入"surprise"概念的思路是这样的：  
他们说：如果我训练我的记忆来重建数据，那么在看到一些新数据后，它可能会错过这些新数据。也许有一些新的数据模型应该记住，但梯度在一段时间后会消失，导致模型会错过它。为了避免这种机制，他们使用了动量——就像我们在模型训练中使用的那样。他们称之为"past surprise"（历史意外），这个"past surprise"不过就是我们在优化器（比如Adam优化器）中使用的动量项中的历史梯度。而"momentary surprise"（瞬时意外）则是指相对于当前输入的梯度。

总结一下我们目前所说的：  
我们有一个记忆，它就是一个W矩阵，我们希望通过优化这个矩阵，使其能够

所以我们希望随着接收到的每个token不断更新这个W，让它能封装输入中的所有信息。我们怎么知道它是否捕获了输入中的所有信息呢？因为我们要求它最小化输入的重建损失。现在的问题是，我们不仅想在训练时训练这个神经记忆，还想在推理时也这样做。因为如果只在训练时做，推理时遇到从未见过的新信息时，它的压缩效果会很差，就无法正常工作。

那么在推理时具体怎么做呢？实际操作如下：在推理时，假设我们有输入——比如第一个输入，让我写下这些公式以便参考（这里粘贴公式）。同时复制这个损失函数。

假设我们有100万个token，但实际上可能从单个token开始生成。比如提示词只有一个token时会发生什么？我们称这个token为a₁，将其输入模型转换为嵌入表示（只有一个嵌入向量），然后要在这一单个token上训练神经记忆，让它学会重建这个token。

具体操作是：

1. 首先用矩阵WK和WV将这个嵌入向量投影为键（key）和值（value）。
2. 计算记忆检索——因为记忆被建模为线性层的W矩阵，检索信息就是W乘以输入（这里输入称为QT，是输入通过WQ矩阵的投影）。
    - KT来自WK * X
    - VT来自WV * X
    - QT来自WQ * X
3. 这里的W是记忆的参数，同时也是记忆本身。我们需要更新这个W。

如何更新？

- 用WV和WK投影单token信息
- 计算W乘以这个项（公式中的检索项）
- 计算损失函数
- 计算其梯度（论文中给出了梯度公式，稍后可以推导）

梯度计算：

- 计算损失函数对模型参数W的梯度
- 更新W时，用学习率乘以梯度（假设初始状态没有动量项，暂设为0）
- 用这个更新项调整W

更新记忆后，如何从中检索信息？

- 直接取W乘以输入X（继续计算后续步骤）


所以我们用矩阵WQ将单个token投影为QT，再乘以W，现在检索到的信息会被送入模型的第一层作为压缩后的历史信息，接着传递到第二层、第三层等等，最终预测输出token。通常我们会把这个输出token重新放回输入序列以生成下一个token。但这里我们讨论的不是单纯的Transformer模型，而是一个混合架构——既有注意力层，又有神经记忆模块。因此，我们需要用新生成的token来更新神经记忆。

这个新生成的token会被再次用于更新记忆模块。记忆不会单纯被新token覆盖，而是希望它能同时保留之前第一个token和当前token的信息。具体操作是：

1. 将模型输出的新token通过WV投影为VT
2. 通过WK投影为KT
3. 计算损失项及其梯度
4. 像之前那样更新神经记忆

但这次我们有了历史信息（Surpise项），因为此时记忆不仅包含当前token，还包含之前token#1的信息。可以看到，由于我们在测试时（推理阶段）训练神经记忆，相比仅在训练时更新的记忆，它的表现应该更好——因为此时记忆的每次更新都针对当前实时数据优化，而不仅是训练集数据。

我知道信息量很大，但现在应该更清楚内循环（inner loop）和外循环（outer loop）的概念了：训练大模型时，我们调整参数使其能利用记忆模块的输出；而记忆模块不仅在训练时学习压缩信息，还会在推理时针对实时输入数据持续优化。

这个记忆模块的问题在于：每次都要对单个token运行梯度下降。想象处理100万个token时，如果无法并行化，就必须逐个token更新记忆（先处理token#1更新记忆，再token#2更新...重复100万次），这无法发挥GPU的并行计算能力。论文提出了一种折中方案——按数据块（chunk）并行：例如将100万token分成1000长度的块，先并行处理前1000个token，再处理下一块，这样总计算量从100万步降到1000步。

关于如何使用这个神经记忆模块：它可以作为上下文记忆（contextual memory）集成到混合架构中。比如在同时包含注意力和神经记忆的架构中，我们可以：

1. 取出用户输入的序列
2. 将记忆模块压缩的信息拼接到序列前端（无需纠结论文提到的persistent memory tokens，系统即使不用这些也能工作）
3. 将拼接后的数据输入注意力模块
4. 用注意力模块的输出同时更新记忆和生成预测结果



所以我们来看这个架构 基本上意味着假设我们已经给这个内存输入了10个token 现在要预测第11个token 具体操作是 我会拿这个第11个token 转换成嵌入表示 然后从神经内存中检索信息 因为它的工作是压缩 即使我输入了10个token 它也不一定要返回10个token 而是给出这些token的压缩版本 假设压缩比例是5个token 那么我会把这5个token和我当前的单个token拼接 变成6个token 输入到第一个注意力层 用注意力层的输出更新内存 并和注意力输出结合得到这一层的最终输出 再传给下一层 这就是神经内存作为上下文的使用方式

另一种用法是"内存作为门控" 就是这里的架构 这种情况下 我们有第11个token 不用考虑持久内存 我说过这个设计过度了 其实不用持久内存这个机制也能工作 他们把第11个token放入内存 先更新内存 同时也输入给注意力层 然后结合神经内存的输出(虽然内存包含11个token但只返回5个token)和注意力层的输出(只输入了1个token)来生成最终输出

或者可以完全不用注意力层 只使用内存模块 这意味着跳过所有这些部分 直接拿输入(可以是1个token或100万个token)持续更新内存 取出内存的压缩版本直接输入给线性层生成logits 这就是他们说的"内存作为层"的用法

说实话 这个架构可以有上百万种变体 关键不在于怎么使用 而在于它的工作原理 我想强调的是 我们在测试时训练这个模块 这和循环神经网络的做法不同 循环神经网络是在训练时训练 它们的任务是压缩数据 但正因为它们擅长压缩见过的数据 在推理时遇到新数据可能表现不佳 而这种可以在推理时训练的内存 配合可并行化的算法 有望解决这个问题 因为内存的唯一任务就是存储和检索

我确实喜欢这篇论文 因为这是个新颖的想法 我之前没想到过 这属于测试时训练的研究领域 但这个架构在这个领域有点创新 还有什么需要了解的？我认为现在你已经掌握了阅读这篇论文所需的信息 我们讨论了如何更新这个内存 以及这个内存是什么 在论文中他们提到这个内存不一定要是单层线性层 也可以是多层感知机 比如带激活函数的两层结构 他们设计的并行算法同样适用于多层内存

我们没有讨论持久内存 但持久内存就是那些预置在输入前的token 它们不属于神经内存 而是属于外循环 外循环就是这个模型 内循环是内存更新 但系统完全可以不用持久token 这是我的观点 从基准测试来看 相比Mamba等当前网络架构 这个方法的综合表现更好 我认为这是个有前景的研究方向 我很期待他们即将发布的代码 因为理解新架构最好的方式就是看代码实现 感谢大家花时间听我讲解 希望至少给了你们足够的直观理解 我也非常期待看到代码 祝大家晚安