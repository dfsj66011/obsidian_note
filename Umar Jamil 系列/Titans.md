
[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)


#### 1.1 记忆视角

RNNs 被定义为具有向量值记忆模块 $M$（也称为隐藏状态）的模型，包含两个主要步骤：在时间  $t$ 给定新输入 $x_t$，模型（1）使用函数 $f(M_{t-1}, x_t)$ 更新记忆（带压缩）；（2）使用函数 $g(M_t, x_t)$ 检索输入的相应记忆（详见 §2.1）。

类似地，Transformers 可以被视为具有增长记忆的架构，并包含两个类似步骤。即，键值矩阵对充当模型的记忆，模型：（1）通过将键和值附加到记忆中来更新记忆（无压缩）；（2）通过找到查询和键向量的相似性来检索查询向量的相应记忆，然后使用该相似性加权值向量以获得输出。

Transformers 和线性 Transformers 之间的主要区别在于记忆结构以及记忆更新步骤，其中线性 Transformers 将历史数据压缩到固定大小的矩阵值记忆中，而 Transformers 保留所有历史数据（在上下文长度内）而不进行压缩。虽然线性 Transformers 和线性 RNNs（包括状态空间模型）在记忆更新步骤中压缩信息，但关键区别在于记忆的结构，其中线性 RNNs（与线性 Transformers 相比）使用向量值记忆（与矩阵值记忆相比）。因此，这种视角促使我们提出以下问题：
**（Q1）什么构成了良好的记忆结构？（Q2）什么是适当的记忆更新机制？（Q3）什么是良好的记忆检索过程？（Q4）如何设计一种有效的架构来整合不同的互联记忆模块（Q5）是否需要一个深度记忆模块来有效地存储/记住过去的长久历史？**

### 2、预备知识

令 $x \in \mathbb{R}^{N \times d_{\text{in}}}$ 为输入，$M$ 为神经网络（神经记忆模块），$Q$、$K$、$V$ 分别为注意力机制中的查询、键和值，$M$ 为注意力掩码。在对序列进行分段时，我们用 $S^{(i)}$ 表示第 $i$ 个段。在本文中，使用下标来表示矩阵、向量或段中特定的元素。例如，我们用 $S^{(i)}_j$ 表示第 $i$ 个段中的第 $j$ 个标记。唯一的例外是下标 $t$，我们保留用于索引时间上的递归，或神经网络在时间 $t$ 的状态。给定一个神经网络 $N$ 和一个数据样本 $x$，我们用 $N(x)$（或 $N^*(x)$）表示有（或无）权重调整的前向传递。同样地，我们使用 $N^{(k)}$ 表示神经网络的第 $k$ 层。接下来，我们首先讨论注意力及其高效变体的背景，然后回顾现代线性 RNN。最后，我们从记忆的角度讨论这些架构，这促使我们设计出 Titans。

#### 2.1 背景知识

**注意力机制**    作为多数深度学习模型事实上的核心架构，Transformer 基于注意力机制。给定输入 $x \in \mathbb{R}^{N \times d_{\text{in}}}$，因果注意力通过输入相关的键、值、查询矩阵的 softmax 运算生成输出 $y \in \mathbb{R}^{N \times d_{\text{in}}}$：  
$$
\begin{align}  
\mathbf{Q} = x \mathbf{W}_{\mathbf{Q}}, \qquad \mathbf{K} = x \mathbf{W}_{\mathbf{K}}, \qquad \mathbf{V} = x \mathbf{W}_{\mathbf{V}}, \\  
\mathbf{y}_i = \sum_{j = 1}^{i} \frac{ \exp\left( \mathbf{Q}_i^{\top} \mathbf{K}_j/\sqrt{d_{\text{in}}}\right) \mathbf{V}_j }{\sum_{\ell = 1}^{i} \exp\left( \mathbf{Q}_i^{\top} \mathbf{K}_{\ell}/\sqrt{d_{\text{in}}}\right)},  
\end{align}$$

尽管 Transformer 具有强大的记忆召回能力，但其计算输出至少需要 $N×d$ 次运算，导致长序列场景下内存消耗大、吞吐量低。

**高效注意力机制**    为提升长序列处理的存储效率与吞吐量，现有研究主要聚焦三类改进：注意力矩阵的 I/O 感知实现、通过稀疏化注意力矩阵设计高效机制、softmax 近似或开发基于核的线性注意力。本节重点讨论最后一类——用核函数 $ϕ(.,.)$ 替代标准注意力中的 softmax，满足 $ϕ(x,y)=ϕ(x)ϕ(y)$，从而将注意力改写为：  
$$
\begin{align}  
\mathbf{y}_i = \sum_{j = 1}^{i} \frac{\phi(Q_i^\top K_j)}{\sum_{\ell = 1}^{i} \phi(Q_i^{\top} K_{\ell})} V_j = \sum_{j = 1}^{i} \frac{\phi(Q_i)^\top \phi(K_j)}{\sum_{\ell = 1}^{i} \phi(Q_i)^{\top} \phi(K_{\ell})} V_j = \frac{\phi(Q_i)^{\top} \sum_{j=1}^{i} \phi(K_j) V_j}{\phi(Q_i)^{\top} \sum_{\ell = 1}^{i} \phi(K_{\ell})},  
\end{align}$$

从而实现更高的吞吐量，因为每一步都重用了 $\sum_{j=1}^{i} \phi (K_j)$ 和 $\sum_{\ell=1}^{i} \phi (K_\ell)$。当选择核为单位矩阵时，上述公式也可以写成递归形式：$$\begin{align}
M_t &= M_{t-1} + K_t^\top V_t \\
y_t &= Q_t M_t \end{align}$$这使得线性注意力的推理更加高效。

\head{现代线性模型的内存视角}  
如前所述，学习可视为获取有效记忆的过程。基于此，循环神经网络（RNN）的隐藏状态可视为信息压缩的记忆单元。广义RNN的递归过程可分解为记忆单元的\textcolor{c1}{\emph{读取}}与\textcolor{c1}{\emph{写入}}操作：设输入x∈RN×dinx∈RN×din​，记忆单元\M∈Rd\M∈Rd，输出\mby∈Rdin\mby∈Rdin​，则通用递归形式定义为：  
\begin{align}  
&\qquad \qquad \qquad \qquad \M_t = f(\M_{t-1}, x_t) , \qquad \qquad & \textcolor{c1}{\text{写入操作}}\  
&\qquad \qquad \qquad \qquad \mb{y}_{t} = g(\M_t, x_t), \qquad \qquad &\textcolor{c1}{\text{读取操作}}  
\end{align}  
其中f(.,.)f(.,.)对应\textcolor{c1}{\emph{写入}}，g(.,.)g(.,.)对应\textcolor{c1}{\emph{读取}}。此处\Mt\Mt​下标表示tt时刻的记忆状态。

在此视角下，线性Transformer的递归公式（见\autoref{eq:linear-transformer}）等价于将键值对(Kt,Vt)(Kt​,Vt​)以加法形式压缩写入矩阵值记忆单元\Mt\Mt​。处理长上下文数据时，这种累加特性会导致记忆溢出，严重损害模型性能。现有研究通过两种方向解决该问题：(1) 添加遗忘机制：如GLA~\citep{yang2024gatedattn}、LRU~\citep{orvieto2023resurrecting}、Griffin~\citep{de2024griffin}、xLSTM~\citep{beck2024xlstm}和Mamba2~\citep{dao2024transformers}等模型采用数据依赖的遗忘门机制，其中Mamba2还与离散化传统状态空间模型~\citep{gu2024mamba}相关联。(2) 改进写入操作：\citet{widrow1988adaptive}提出Delta规则，在添加新记忆（键值对）前先移除旧值；\citet{yang2024parallelizing}设计快速并行化算法；\citet{yang2024gated}近期通过增加遗忘门改进DeltaNets。

\head{记忆模块} 记忆始终是神经网络设计的核心要素~\citep{schmidhuber1992learning, LSTM, graves2014neuralturingmachines, zhang2024memory}。将线性层视为键值（关联）记忆系统的思想源于快速权重编程，即通过动态快速程序为RNN提供可写记忆~\citep{schmidhuber1992learning}。Hebbian~\citep{hebb2005organization}与delta~\citep{prados1989neural}规则是快速权重编程最常用的学习规则，已被广泛研究~\citep{munkhdalai2017neural, schmidhuber1992learning, munkhdalai2019metalearned, schlag2021linear, irie2021going, yang2024parallelizing, yang2024gated}。但这些模型均基于瞬时信号，忽略了序列中的词元流动（见\autoref{sec:long-memory}），且多数缺乏遗忘门导致内存管理低效。

更多与近期模型的关联分析见\autoref{app:MAS}，其他相关工作见\autoref{app:rw}。


\section{学习在测试时记忆}\label{sec:mem-module}  
\lettrine[lines=3]{为}{}了克服长期记忆的缺失，并使模型能够学习、遗忘和检索信息，本节提出一种神经长期记忆模块——这是一种在测试时学习记忆的元模型。在\autoref{sec:long-memory}中，我们首先讨论神经记忆的动机与设计；在\autoref{sec:fast-training}中，阐述我们的架构设计如何通过快速并行化训练获益；最后在\autoref{sec:persistent-memory}中，我们使用持久记忆模块增强架构，其中采用可学习但与数据无关的参数来获取任务的元信息。

\subsection{长期记忆}\label{sec:long-memory}  
设计神经长期记忆模块需要模型能将历史信息的抽象表征编码至参数中。典型案例如大型语言模型（LLMs），研究表明其能记忆训练数据~\citep{staab2024beyond, schwarzschild2024rethinking, leybzon2024learning}。因此，直观思路是训练神经网络使其记忆训练数据。然而记忆现象通常被视为神经网络的不良特性，因其会限制模型泛化能力~\citep{bayat2024pitfalls}、引发隐私担忧~\citep{staab2024beyond}，并导致测试时性能下降。更重要的是，训练数据的记忆在测试时可能无益——尤其是面对分布外数据时。我们主张需要一种在线元模型，在测试时动态学习记忆/遗忘机制。该框架下，模型学习的是具备记忆能力但不陷入训练数据过拟合的函数，从而提升测试时的泛化性能。

\head{学习过程与惊奇度量}  
训练长期记忆的核心在于将其视为在线学习问题，目标是将历史信息x1,…,xt−1x1​,…,xt−1​压缩至长期神经记忆模块MtMt​的参数中。如前所述，违反预期的事件（即惊奇事件）对人类更具记忆性~\citep{mandler2014structure}。受此启发，模型的惊奇度可简单定义为输入数据的梯度幅值——梯度越大，表明当前数据与历史数据的差异越显著。基于该惊奇评分，记忆更新可表示为：  
\begin{align}\label{eq:GD}  
\M_{t} = \M_{t-1} - \theta_t : \undermath{\text{惊奇度}}{\textcolor{c4}{\nabla \ell(\M_{t-1}; x_{t})}}.  
\end{align}  
但该度量存在缺陷：重大惊奇事件后可能出现重要信息遗漏。即连续多个惊奇步骤后梯度会急剧衰减，导致陷入平坦区域（局部极小值），从而丢失序列部分信息。从人类记忆角度看，事件虽具记忆性，其惊奇度未必长期持续。这是因为初始时刻的惊奇度足以在长时间跨度中吸引注意力，从而形成完整时间段的记忆。为改进上述惊奇度量（\autoref{eq:GD}），我们将其分解为：(1) \emph{历史惊奇度}——衡量近期历史的惊奇程度；(2) \emph{瞬时惊奇度}——衡量新到数据的惊奇程度：  
\begin{align}\label{eq:GD-momentum}  
& \M_{t} = \M_{t-1} + \textcolor{c4}{S_{t}},\  
& S_{t} = \eta_t \undermath{\text{历史惊奇度}}{\textcolor{c4}{S_{t-1}}} - \theta_t: \undermath{\text{瞬时惊奇度}}{\textcolor{c4}{\nabla \ell\left(M_{t-1}; x_{t} \right)}}.  
\end{align}  
值得注意的是，该公式与带动量的梯度下降法相似，其中StSt​即动量项。此处动量项实际充当了跨时间（序列长度）的惊奇记忆。公式中ηtηt​为数据依赖的惊奇衰减系数（xtxt​的函数），控制惊奇度随时间衰减的速率；θtθt​则以数据依赖方式控制瞬时惊奇度对最终度量的贡献程度。这种数据依赖性至关重要：虽然前序标记的惊奇度可能影响后续标记，但该机制仅在所有标记相关且同属相同上下文时有效。因此数据依赖的ηη可控制记忆模块：(1) 当ηt→0ηt​→0时忽略历史惊奇度（可能因上下文切换）；(2) 当ηt→1ηt​→1时完全继承历史惊奇度（可能因标记与近期历史高度相关）。






我想提醒你们我们正在解决的问题是长上下文建模 长上下文建模有一个问题 就是使用Transformer时 对长上下文进行推理非常昂贵 使用RNN时 我们遇到的问题是我们在一些数据上训练它们 但是当你用它们处理从未见过的东西时 它们不知道如何压缩 不知道该保留什么不该保留什么 所以它们会表现失常 因为它们表现失常 它们做不好这项工作 注意力层无法利用这些信息 所以它们只会产生非常糟糕的输出 使用神经记忆 我们想在推理模型的同时即时训练一个记忆 唯一目的就是压缩输入给它的任何数据 现在我们可以看看细节

好的 嗯 好的 这里他们先做了些初步的 呃 怎么说 嗯 关于记忆或线性注意力等的概述 我们现在暂时不关心这个 他们说 好吧 假设我们有个只有两种操作的内存模块 一个是写入操作 一个是读取操作 嗯 我们想在推理时和训练时都对这个内存进行读写 该如何训练这个内存呢 首先 这个神经记忆本身是个神经网络 也就是说你可以把它看作独立于其他架构的外部神经网络 嗯 那个架构会使用这个 呃 神经记忆 所以你得想象有个Transformer模型正在利用这个 呃 神经记忆 嗯 现在问题来了 怎么在推理时训练这个神经记忆 因为训练时我们知道怎么做 我们只要 呃 计算输出 反向传播 就搞定了 但在推理时该怎么做 这就是他们这里探讨的 他们说 好吧 假设我们有这个内存 呃 首先 我们想怎么更新它的信息 他们想更新内存里的信息 呃 好吧 再退一步 我们想让这个内存做什么 我们想让这个内存学会 呃 提取它该记住的任何信息 为此他们用了非常特殊的损失函数 就是那种重建损失 假设我们有这个内存 如果我们要求它记住 呃 好吧 假设我们有个输入序列 就叫它X吧 这个XT XT这里 我们用两个线性投影WK和W来映射它 基本上就相当于注意力机制里用的那种 这个 呃 内存怎样才能很好地工作呢 只有当它学会重建见过的数据时才行 呃 这就是你在这里看到的损失函数 就是这里的L2损失 嗯 它学习记住键投影和V投影之间的映射关系 所以算是学会重建相同的数据 嗯 这就是内存的职责 所以如果我存入某些东西 就应该能取出同样的东西 应该能尽可能还原存入的内容 怎么训练它呢 他们说 好吧 我有这个内存 想通过类似梯度下降的方法来更新 梯度下降怎么运作的 呃 想象我们有个普通网络 梯度下降的基本版本 呃 是这样运作的 我们有个带参数的网络 参数就叫θ吧 比如θ 嗯 在训练第i步时的参数θ 是用模型前一步的参数来更新的 所以是前一个时刻的参数 减去我们称为γ的学习率 乘以损失函数对模型参数的梯度 这个梯度告诉我们该如何改变 呃 参数才能最大化损失 但我们朝着梯度相反方向更新 所以你会看到减号 我们朝着最小化损失的方向更新参数 这里就是这么做的 我们说想这样更新内存 使得最小化这个损失 就是前面看到的记忆损失 也就是重建损失 这个损失表示 如果我让内存通过数据的键投影来检索信息 它就该重建这个数据 嗯 在论文里他们把这个内存建模成线性层

线性层本质上只是一个带有权重矩阵的矩阵乘法运算。因此这个记忆模块M，它不过就是一个线性层的权重矩阵W。我们以这样的方式修改这个权重矩阵W，使其能够最小化数据的重构损失——就像我们训练神经网络时调整参数来降低损失函数一样。这些参数的计算方式使得它们能够产生尽可能小的损失值。同样地，我们更新这个作为记忆的W矩阵，使其能够产生最小的信息损失，因为这就是我们优化的目标——重构损失。

当我们计算这个损失时，他们称之为"surprise"（意外程度）。这个记忆矩阵W相对于损失的梯度（∂L/∂W）被称为"surprise"，因为损失越大，模型重构数据的难度就越大，这意味着模型对这个数据感到越"意外"。

如果你曾经研究过优化器的工作原理，你会记得深度学习中有一个叫做"动量"的概念。通常我们不会简单地直接这样更新模型参数，因为：

1. 首先，损失是通过小批量梯度下降计算的
2. 这意味着我们不是在整个数据集上计算，而是在数据的小批量上计算
3. 这个梯度的方向实际上是随机的，不是真实的梯度方向
4. 它会不断波动，平均而言会指向正确的梯度方向，但每一步都是带有噪声的

因为我们不想在每一步训练中过于自信地采取更新步骤，所以我们加入这个动量项。动量项基本上创建了所有梯度的指数移动平均值，这样我们就能保留一些关于过去计算过的梯度的信息，从而平滑权重的变化，避免更新幅度过大。

他们引入"surprise"概念的思路是这样的：  
他们说：如果我训练我的记忆来重建数据，那么在看到一些新数据后，它可能会错过这些新数据。也许有一些新的数据模型应该记住，但梯度在一段时间后会消失，导致模型会错过它。为了避免这种机制，他们使用了动量——就像我们在模型训练中使用的那样。他们称之为"past surprise"（历史意外），这个"past surprise"不过就是我们在优化器（比如Adam优化器）中使用的动量项中的历史梯度。而"momentary surprise"（瞬时意外）则是指相对于当前输入的梯度。

总结一下我们目前所说的：  
我们有一个记忆，它就是一个W矩阵，我们希望通过优化这个矩阵，使其能够

所以我们希望随着接收到的每个token不断更新这个W，让它能封装输入中的所有信息。我们怎么知道它是否捕获了输入中的所有信息呢？因为我们要求它最小化输入的重建损失。现在的问题是，我们不仅想在训练时训练这个神经记忆，还想在推理时也这样做。因为如果只在训练时做，推理时遇到从未见过的新信息时，它的压缩效果会很差，就无法正常工作。

那么在推理时具体怎么做呢？实际操作如下：在推理时，假设我们有输入——比如第一个输入，让我写下这些公式以便参考（这里粘贴公式）。同时复制这个损失函数。

假设我们有100万个token，但实际上可能从单个token开始生成。比如提示词只有一个token时会发生什么？我们称这个token为a₁，将其输入模型转换为嵌入表示（只有一个嵌入向量），然后要在这一单个token上训练神经记忆，让它学会重建这个token。

具体操作是：

1. 首先用矩阵WK和WV将这个嵌入向量投影为键（key）和值（value）。
2. 计算记忆检索——因为记忆被建模为线性层的W矩阵，检索信息就是W乘以输入（这里输入称为QT，是输入通过WQ矩阵的投影）。
    - KT来自WK * X
    - VT来自WV * X
    - QT来自WQ * X
3. 这里的W是记忆的参数，同时也是记忆本身。我们需要更新这个W。

如何更新？

- 用WV和WK投影单token信息
- 计算W乘以这个项（公式中的检索项）
- 计算损失函数
- 计算其梯度（论文中给出了梯度公式，稍后可以推导）

梯度计算：

- 计算损失函数对模型参数W的梯度
- 更新W时，用学习率乘以梯度（假设初始状态没有动量项，暂设为0）
- 用这个更新项调整W

更新记忆后，如何从中检索信息？

- 直接取W乘以输入X（继续计算后续步骤）


所以我们用矩阵WQ将单个token投影为QT，再乘以W，现在检索到的信息会被送入模型的第一层作为压缩后的历史信息，接着传递到第二层、第三层等等，最终预测输出token。通常我们会把这个输出token重新放回输入序列以生成下一个token。但这里我们讨论的不是单纯的Transformer模型，而是一个混合架构——既有注意力层，又有神经记忆模块。因此，我们需要用新生成的token来更新神经记忆。

这个新生成的token会被再次用于更新记忆模块。记忆不会单纯被新token覆盖，而是希望它能同时保留之前第一个token和当前token的信息。具体操作是：

1. 将模型输出的新token通过WV投影为VT
2. 通过WK投影为KT
3. 计算损失项及其梯度
4. 像之前那样更新神经记忆

但这次我们有了历史信息（Surpise项），因为此时记忆不仅包含当前token，还包含之前token#1的信息。可以看到，由于我们在测试时（推理阶段）训练神经记忆，相比仅在训练时更新的记忆，它的表现应该更好——因为此时记忆的每次更新都针对当前实时数据优化，而不仅是训练集数据。

我知道信息量很大，但现在应该更清楚内循环（inner loop）和外循环（outer loop）的概念了：训练大模型时，我们调整参数使其能利用记忆模块的输出；而记忆模块不仅在训练时学习压缩信息，还会在推理时针对实时输入数据持续优化。

这个记忆模块的问题在于：每次都要对单个token运行梯度下降。想象处理100万个token时，如果无法并行化，就必须逐个token更新记忆（先处理token#1更新记忆，再token#2更新...重复100万次），这无法发挥GPU的并行计算能力。论文提出了一种折中方案——按数据块（chunk）并行：例如将100万token分成1000长度的块，先并行处理前1000个token，再处理下一块，这样总计算量从100万步降到1000步。

关于如何使用这个神经记忆模块：它可以作为上下文记忆（contextual memory）集成到混合架构中。比如在同时包含注意力和神经记忆的架构中，我们可以：

1. 取出用户输入的序列
2. 将记忆模块压缩的信息拼接到序列前端（无需纠结论文提到的persistent memory tokens，系统即使不用这些也能工作）
3. 将拼接后的数据输入注意力模块
4. 用注意力模块的输出同时更新记忆和生成预测结果



所以我们来看这个架构 基本上意味着假设我们已经给这个内存输入了10个token 现在要预测第11个token 具体操作是 我会拿这个第11个token 转换成嵌入表示 然后从神经内存中检索信息 因为它的工作是压缩 即使我输入了10个token 它也不一定要返回10个token 而是给出这些token的压缩版本 假设压缩比例是5个token 那么我会把这5个token和我当前的单个token拼接 变成6个token 输入到第一个注意力层 用注意力层的输出更新内存 并和注意力输出结合得到这一层的最终输出 再传给下一层 这就是神经内存作为上下文的使用方式

另一种用法是"内存作为门控" 就是这里的架构 这种情况下 我们有第11个token 不用考虑持久内存 我说过这个设计过度了 其实不用持久内存这个机制也能工作 他们把第11个token放入内存 先更新内存 同时也输入给注意力层 然后结合神经内存的输出(虽然内存包含11个token但只返回5个token)和注意力层的输出(只输入了1个token)来生成最终输出

或者可以完全不用注意力层 只使用内存模块 这意味着跳过所有这些部分 直接拿输入(可以是1个token或100万个token)持续更新内存 取出内存的压缩版本直接输入给线性层生成logits 这就是他们说的"内存作为层"的用法

说实话 这个架构可以有上百万种变体 关键不在于怎么使用 而在于它的工作原理 我想强调的是 我们在测试时训练这个模块 这和循环神经网络的做法不同 循环神经网络是在训练时训练 它们的任务是压缩数据 但正因为它们擅长压缩见过的数据 在推理时遇到新数据可能表现不佳 而这种可以在推理时训练的内存 配合可并行化的算法 有望解决这个问题 因为内存的唯一任务就是存储和检索

我确实喜欢这篇论文 因为这是个新颖的想法 我之前没想到过 这属于测试时训练的研究领域 但这个架构在这个领域有点创新 还有什么需要了解的？我认为现在你已经掌握了阅读这篇论文所需的信息 我们讨论了如何更新这个内存 以及这个内存是什么 在论文中他们提到这个内存不一定要是单层线性层 也可以是多层感知机 比如带激活函数的两层结构 他们设计的并行算法同样适用于多层内存

我们没有讨论持久内存 但持久内存就是那些预置在输入前的token 它们不属于神经内存 而是属于外循环 外循环就是这个模型 内循环是内存更新 但系统完全可以不用持久token 这是我的观点 从基准测试来看 相比Mamba等当前网络架构 这个方法的综合表现更好 我认为这是个有前景的研究方向 我很期待他们即将发布的代码 因为理解新架构最好的方式就是看代码实现 感谢大家花时间听我讲解 希望至少给了你们足够的直观理解 我也非常期待看到代码 祝大家晚安