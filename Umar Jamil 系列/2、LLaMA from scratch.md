
> https://www.bilibili.com/video/BV1xe4peAESX/?spm_id_from=333.1387.collection.video_card.click&vd_source=aced32e35ad9cff83fe98c60854f183c

文章内容主要包括：

* Vanilla Transformer 和 LLaMA 模型之间的架构差异
* RMS 归一化
* 旋转位置编码
* KV 缓存
* 多查询注意力
* 分组多查询注意力
* 前馈层的 SwiGLU 激活函数。

### 1、Transformer vs LLaMA

* 首先，*在 LLaMA 中只有解码器*；
* 其次，在嵌入层之后，没有位置编码，而是 RMS 归一化，实际上，所有的*归一化都移到了块之前*；
* 位置编码不再是 Transformer 的位置编码，而是*旋转位置编码*，并且他们只应用于 $Q$ 和 $K，$不包含 $V$；
* 自注意力机制*带有 $KV$ 缓存*；
* 自注意机制为*分组多查询注意力*；
* 前馈层中激活函数由 ReLU 变为 *SwiGLU 函数*

### 2、LLaMA 简介

LLaMA 于 2023 年 2 月发布，他们为这个模型设定了四个维度：

![[Pasted image 20250318203939.png|550]]

在原始的 Transformer 中，$\text{dimension} = 512, n\text{ head}=8, n\text{ layers}=6$，在 LLaMA 2 中，大多数参数都翻倍了，
![[Pasted image 20250318204234.png|550]]

上下文长度（即，序列长度），是指模型能处理的最长序列，模型训练所用的 token 数量也翻倍了，从 1T 到 2T，每个模型的大小都有所增加，而参数量大致保持不变，最后两个模型使用了 GQA 技术表示的模型，稍后会介绍其工作原理。

### 3、嵌入层

对句子进行分词，将其转换为 token，分词通常是通过 BPE 分词器完成的，在 LLaMA 中，token 向量大小为 4096，这些嵌入向量是可学习的，因此它们是模型的参数，在模型训练过程中，这些嵌入向量会发生变化，以便捕捉它们所映射单词的含义。

### 4、归一化层


这是紧接着嵌入层之后的层，我们将从数学层面分析理解归一化是如何工作的，假设我们有一个线性层，信息的流动控制公式为：$$O=XW^T+b$$一个神经元对某个数据项的输出，取决于输入数据项的特征和神经元的参数，我们可以将 $X$ 视为前一层的输出，如果前一层由于梯度下降更新了其权重，导致输出 $X$ 发生巨大变化，将产生与以往大不相同的输出，下一层也将因此大幅改变其输出，在梯度下降的下一步，它将被迫大幅调整其权重，这种导致神经元内部节点分布发生变化的现象被称为*内部协变量偏移（Internal Covariate Shift）*，我们希望避免这种现象，它会使网络训练变慢，因为神经元被迫因前一层输出的剧烈变化而在一个方向或另一个方向上大幅调整其权重。

#### 4.1 Layer Norm
层正则化公式如下：$$y = \frac{x - \mathbb{E}[x]}{\sqrt{\text{Var}[x]} + \epsilon} \times \gamma + \beta $$
- 每个 item 用其标准化值更新，这将使其变为均值为 0、方差为 1 的正态分布。
- 两个参数 $\gamma$ 和 $\beta$ 是可学习参数，它们允许模型根据损失函数的需要“放大”每个特征的尺度或对特征进行平移。

#### 4.2 均方根 RMS Norm
![[Pasted image 20250320211350.png|600]]
大致翻译：LayerNorm 成功的一个著名解释是其 *重新中心化* 和 *重新缩放的不变性* 特性，什么意思呢？无论特征是什么，它们都将被重新中心化到均值为 0，并重新缩放到方差为 1，前者使模型对输入和权重的偏移噪声不敏感，而后者在输入和权重随机缩放时保持输出表示不变，*在本文中，我们假设 LayerNorm 成功的原因是重新缩放不变性，而非重新中心化不变性*

他们所做的基本上是说，能否找到另一个不依赖于均值的统计量，因为他们认为重新中心化不是必要的，所以他们使用均方根统计量，定义如下：$$
\bar{a}_i = \frac{a_i}{\text{RMS}(a)} g_i, \quad \text{where} \quad \text{RMS}(a) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}.$$其中，$g \in \mathbb{R}^n$ 是用于重新缩放标准化后的输入总和的增益参数，初始值设为 1。可以看到，这里不再使用均值信息，而在之前计算方差的过程中需要均值信息：$$
\mu = \frac{1}{n} \sum_{i=1}^{n} a_i, \quad \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (a_i - \mu)^2}. $$
**那为什么要选择使用 RMSNorm？** 与层归一化相比，计算量更少，只需要计算一个均方根统计量，不需要计算均值和标准差，这带来了计算上的优势，而且在实践中效果很好，所以实际上，*论文作者的假设是正确的*，不需要重新中心化，至少在 LLaMA 中是这样的。

### 5、旋转位置编码

绝对位置编码和相对位置编码之间的区别：

- 绝对位置编码是固定向量，添加到标记的嵌入中以表示其在句子中的绝对位置。因此，它*一次处理一个标记*。
- 相对位置编码则一次处理两个标记，并在计算注意力时涉及其中。由于注意力机制捕捉两个词之间相关程度的“强度”，相对位置编码告诉注意力机制这两个词之间的*距离*。因此，给定*两个* 标记，我们创建一个表示其距离的向量。

![[Pasted image 20250320214214.png|400]]

相对位置编码首次在 Paper：Self-Attention with Relative Position Representations 这篇来自谷歌的论文中被引入，Vaswani 是 Transformer 模型的作一。

在原始的注意力机制中，计算两个 token 之间的点积，其位置信息已经包含在各自的 token 中，因此可在两个向量中直接计算：$$
e_{ij} = \frac{(x_i W^Q)(x_j W^K)^T}{\sqrt{d_z}}$$在相对位置编码中，则有三个向量，其中向量 $a_{ij}$ 表示这两个 token 之间的距离：$$ 
e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_z}}$$旋转位置编码在 RoFormer 中被引入，论文的作者想要做的是，能否找到一种内积，只依赖于注意力机制中使用的两个向量（Q 和 K）本身以及它们所代表的 token 的相对距离，也就是说，给定 Q 和 K，它们只包含它们所代表的词的嵌入以及它们在句子中的位置，$$q_m = f_q(x_q, m), \quad k_n = f_k(x_k, n)$$这里的 $m, n$ 都是它们在句子中的绝对位置，我们能否找到一个内积，使得：$$\mathbf{q}_m^\top \mathbf{k}_n = \langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, n - m),$$是的，我们可以找到这样一个函数，这个函数定义在复数空间中，并且可以通过使用欧拉公式转换，我们可以定义一个函数 $g$，它仅依赖于两个嵌入向量 $\mathbf{q}$ 和 $\mathbf{k}$ 及其相对距离：$$\begin{align}
  &f_q(x_m, m) = (W_q x_m) e^{im\theta} \quad 
  f_k(x_n, n) = (W_k x_n) e^{in\theta} \\[1.2ex]
  &g(x_m, x_n, m-n) = \Re\left[(W_q x_m) (W_k x_n)^* e^{i(m-n)\theta}\right] \end{align}$$ (公式中的星号：复数的共轭)，使用欧拉公式，我们可以将其写成矩阵形式：$$
  f_{\{q,k\}}(x_m, m) = 
  \begin{pmatrix}
  \cos m\theta & -\sin m\theta \\
  \sin m\theta & \cos m\theta
  \end{pmatrix}
  \begin{pmatrix}
  W^{(11)}_{\{q,k\}} & W^{(12)}_{\{q,k\}} \\
  W^{(21)}_{\{q,k\}} & W^{(22)}_{\{q,k\}}
  \end{pmatrix}
  \begin{pmatrix}
  x_m^{(1)} \\
  x_m^{(2)}
  \end{pmatrix}$$等号右侧第一项表示在二维空间中的旋转矩阵，因此称为旋转位置嵌入。

![[Pasted image 20250320224246.png|200]]
示例：旋转矩阵为$$
\mathbf{R}_\theta = 
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix},$$我们有：$$
\mathbf{v}' = \mathbf{R}_\theta \mathbf{v}_0.$$
考虑到嵌入维度并不是 2 维，在 LLaMA 中为 4096 维度，则采用如下稀疏矩阵 
![[Pasted image 20250320224607.png|600]]
但由于其稀疏性，这会使 GPU 计算效率很低，为此需要转换一个格式：

![[Pasted image 20250321202640.png]]
与绝对位置编码一样，只计算一次，可以对我们将要训练模型的所有句子重复使用；

另一个有趣特性是*长期衰减*，作者通过改变两个标记之间的距离计算了内积的上限，并证明了随着相对距离的增加，该内积会衰减。这意味着，使用旋转位置嵌入编码的两个标记之间的“关系强度”会随着它们之间距离的增加而数值变小，这是我们希望从旋转位置嵌入中得到的理想特性。

**旋转位置嵌入仅应用于 $Q$ 和 $K$，不包含 $V$，为什么？**  因为这仅仅用于计算注意力分数，在注意力机制中，旋转位置嵌入是在向量 $\mathbf{q}$ 和 $\mathbf{k}$ 被 $W$ 矩阵相乘之后应用的，而在普通的 Transformer 中，它们是在相乘之前应用的。

---------

### 6、KV 缓存的动机

* 在推理的每一步中，我们只关注模型输出的最后一个标记，因为之前的标记已经获得。但模型需要访问所有先前标记才能决定输出哪个标记，这些标记构成了其上下文（即"提示"）。  
* 能否减少模型对已见标记的计算量？  可以！解决方案就是 KV 缓存！

![[Pasted image 20250520092637.png]]

1. 我们已经在之前的步骤中计算了这些点积。能否将它们缓存起来？
2. 由于该模型是因果性的，我们并不关心一个 token 与其后续 tokens 之间的注意力，而只关注它与之前 tokens 之间的注意力。
3. 我们并不关心这些，因为我们想要预测下一个 token，而之前的 token 我们已经预测过了。
4.  *我们只对最后这一行感兴趣！*


<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <img src="kv_1.png" alt="Image 1" style="width: 100%;">
  <img src="kv_2.png" alt="Image 2" style="width: 100%;">
  <img src="kv_3.png" alt="Image 3" style="width: 100%;">
  <img src="kv_4.png" alt="Image 4" style="width: 100%;">
</div>


这就是为什么它被称为 KV 缓存的原因，因为我们保留了键和值的缓存，正如你所见，KV 缓存让我们节省了很多计算，因为我们不再进行以前需要做的大量点积运算，这使得推理速度更快。


### 7、分组多查询注意力

* 近年来，GPU 的计算性能已变得极其强大，其运算速度远超内存带宽或内存区域间的数据传输速度。例如英伟达 A100 显卡可实现 19.5 TFLOPs，但内存带宽仅为 2TB/秒。  
* 这意味着性能瓶颈有时不在于运算量大小，而取决于计算过程中所需的数据传输量——该传输量由参与计算的张量尺寸和数量决定。  
* 举例说明：对同一张量重复进行 $N$ 次相同运算，可能比对 $N$ 个不同张量（即使尺寸相同）执行相同运算更快，因为 GPU 可能需要频繁移动这些张量。  
* *由此可见，我们不仅要优化运算次数，更应尽量减少内存访问/数据传输操作*。

Paper：Introducing Multi-Query Attention, 作者 Noam Shazeer，他也是 attention is all you need 这篇论文的作者之一，在这篇论文中，他提出了这个问题，

**批量多头注意力**：即原始论文 “Attention is all you need” 中介绍的多头注意力。
- 通过设置 $m = n$（查询的序列长度 = 键和值的序列长度）。
- 执行的算术操作数量是 $O(bnd^2)$。
- 所涉及操作的总内存，由计算中涉及的所有张量的总和（包括派生的张量）给出，是 $O(bnd + bhn^2 + d^2)$。
- 总内存与算术操作数量之间的比率是 $O(\frac{1}{k} + \frac{1}{bn})$。
- 在这种情况下，比率远小于 1，这意味着我们进行的内存访问数量远小于算术操作的数量，因此内存访问不是这里的瓶颈。

**引入 KV 缓存后**：
- 使用 KV 缓存来减少执行的操作数量。
- 通过设置 $m = n$（查询的序列长度 = 键和值的序列长度）。
- 执行的算术操作数量是 $O(bnd^2)$。
- 所涉及操作的总内存，由计算中涉及的所有张量的总和（包括派生的张量）给出，是 $O(bn^2d + nd^2)$。
- 总内存与算术操作数量之间的比率是 $O(\frac{1}{k} + \frac{1}{b})$。
- 当 $n \approx d$（序列长度接近嵌入向量的大小）或 $b \approx 1$（批次大小为 1）时，比率变为 1，内存访问成为算法的瓶颈。对于批次大小来说，这不是问题，因为通常远大于 1，而对于 $\frac{1}{k}$ 项，我们需要减少序列长度。但有一个更好的方法……

**多查询注意力**：
- 我们从 $K$ 和 $V$ 中移除 $h$ 维度，同时保留在 $Q$ 中。这意味着所有不同的查询头将共享相同的键和值。
- 执行的算术操作数量是 $O(bnd^2)$。
- 所涉及操作的总内存，由计算中涉及的所有张量的总和（包括派生的张量）给出，是 $O(bnd + bn^2k + nd^2)$。
- 总内存与算术操作数量之间的比率是 $O(\frac{1}{d} + \frac{n}{dh} + \frac{1}{b})$。
- 与之前的方法相比，我们通过一个因子 $h$ 减少了昂贵的项 $\frac{n}{d}$。
- 性能提升很重要，而模型的质量仅有一点点下降。

**分组多查询注意力**：

在多查询中，我们只为查询设置了多个头，但键和值只有一个头，而分组多查询中键和值则分组有多个头。

### 8、SwiGLU 激活函数

Paper：GLU Variants Improve Transformer，又是 Noam Shazeer，

* Transformer：$\text{FFN}(x)=\max(0, xW_{1}+b_{1})W_{2}+b_{2}$，一些后续版本没有偏置项；
* LLaMA：$\text{FFN}_{\text{SwiGLU}}(x,W,V,W_{2})=(\text{Swish}_{1}(xW) \otimes xV)W_{2}$
	* 这里使用的 swish 函数是 $\beta=1$，这种情况下叫做 Sigmoid Linear Unit（SiLU）函数
	* $\text{swish}(x)=x\, \text{sigmoid}(\beta x)=\frac{x}{1+e^{-\beta x}}$

为什么 SwiGLU 激活函数表现的如此出色呢？论文中并没有解释为什么这种架构似乎有效，我们将其成功归因于，神的恩典，实际上，这有点好笑，但也有些真实。


