
> https://www.bilibili.com/video/BV1xe4peAESX/?spm_id_from=333.1387.collection.video_card.click&vd_source=aced32e35ad9cff83fe98c60854f183c

在本篇文章中，我们将了解什么是 LLaMA，它在结构上与 Transformer 有何不同，并逐一构建其各个模块，不仅从概念上解释每个模块的功能，还将从数学角度和编程角度进行探讨，以便将理论与实践结合起来。

文章内容主要包括：

* Vanilla Transformer 和 LLaMA 模型之间的架构差异
* RMS 归一化
* 旋转位置编码
* KV 缓存
* 多查询注意力
* 分组多查询注意力
* 前馈层的 SwiGLU 激活函数。


### 0、先决条件

* Transformer 的结构、注意力机制工作原理
* Transformer 的训练与推理
* 线性代数：矩阵乘法、点乘
* 复数：欧拉公式，$e^{ix}=\cos x+ i\sin x$


### 1、Transformer vs LLaMA

* 首先，在 LLaMA 中只有解码器；
* 其次，在嵌入层之后，没有位置编码，而是 RMS 归一化，实际上，所有的归一化都移到了块之前；
* 位置编码不再是 Transformer 的位置编码，而是旋转位置编码，并且他们只应用于 $Q$ 和 $K，$不包含 $V$；
* 自注意力机制带有 $KV$ 缓存；
* 自注意机制为分组多查询注意力；
* 前馈层中激活函数由 ReLU 变为 SwiGLU 函数


我们将从底层开始构建每一个子模块，并详细展示这些块具体做什么，它们如何工作，如何相互作用，背后的数学原理是什么，以及它们试图解决的问题是什么。

### 2、LLaMA 简介

LLaMA 于 2023 年 2 月发布，他们为这个模型设定了四个维度：

![[Pasted image 20250318203939.png|550]]

在原始的 Transformer 中，$\text{dimension} = 512, n\text{ head}=8, n\text{ layers}=6$，在 LLaMA 2 中，大多数参数都翻倍了，
![[Pasted image 20250318204234.png|550]]

上下文长度（即，序列长度），是指模型能处理的最长序列，模型训练所用的 token 数量也翻倍了，从 1T 到 2T，每个模型的大小都有所增加，而参数量大致保持不变，最后两个模型使用了 GQA 技术表示的模型，稍后会介绍其工作原理。

### 3、嵌入层

对句子进行分词，将其转换为 token，分词通常是通过 BPE 分词器完成的，在 LLaMA 中，token 向量大小为 4096，这些嵌入向量是可学习的，因此它们是模型的参数，在模型训练过程中，这些嵌入向量会发生变化，以便捕捉它们所映射单词的含义。

### 4、归一化层

这是紧接着嵌入层之后的层，我们将从数学层面分析理解归一化是如何工作的，假设我们有一个线性层，信息的流动控制公式为：$$O=XW^T+b$$一个神经元对某个数据项的输出，取决于输入数据项的特征和神经元的参数，我们可以将 $X$ 视为前一层的输出，如果前一层由于梯度下降更新了其权重，导致输出 $X$ 发生巨大变化，将产生与以往大不相同的输出，下一层也将因此大幅改变其输出，在梯度下降的下一步，它将被迫大幅调整其权重，这种导致神经元内部节点分布发生变化的现象被称为*内部协变量偏移（Internal Covariate Shift）*，我们希望避免这种现象，它会使网络训练变慢，因为神经元被迫因前一层输出的剧烈变化而在一个方向或另一个方向上大幅调整其权重。

#### 4.1 Layer Norm
层正则化公式如下：$$y = \frac{x - \mathbb{E}[x]}{\sqrt{\text{Var}[x]} + \epsilon} \times \gamma + \beta $$
- 每个 item 用其标准化值更新，这将使其变为均值为 0、方差为 1 的正态分布。
- 两个参数 $\gamma$ 和 $\beta$ 是可学习参数，它们允许模型根据损失函数的需要“放大”每个特征的尺度或对特征进行平移。


-----------


现在让我们来谈谈均方根归一化，这是 llama 使用的，均方根归一化是在这篇论文中引入的，即这两位研究者提出的均方根层归一化，让我们一起读这篇论文，层归一化成功的众所周知解释是其重新中心化和重新缩放的不变性特性，那么，他们是什么意思呢？重新中心化和重新缩放的不变性是什么，特征的事实是，无论它们是什么，它们都将被重新中心化到均值为 0，并重新缩放到方差为 1，前者使模型对输入和权重的偏移噪声不敏感，而后者在输入和权重的偏移噪声不敏感，而后者在输入和权重随机缩放时保持输出表示不变，在这篇论文中，我们假设重新缩放方差是层归一化成功的原因，而不是重新中心化的不变性，所以他们在论文中声称，层归一化的成功基本上不是因为重新中心化和重新缩放，而主要是因为重新缩放，所以基本上是除以方差，所以方差为一。

他们所做的基本上是说，好吧，我们能否找到另一个不依赖于均值的统计量，因为我们认为它不是必要的，嗯，是的，他们使用的这个均方根统计量，所以这里定义的这个统计量$$
\bar{a}_i = \frac{a_i}{\text{RMS}(a)} g_i, \quad \text{where} \quad \text{RMS}(a) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}.$$
正如你从这个统计量的表达式中看到的，我们不再使用均值来计算它，因为之前的统计量，即方差，需要均值来计算，因为，如果你记得，方差的计算需要均值，所以方差等于 x 减去 $\mu$ 的平方和除以 n，所以我们需要均值来计算方差，因此，作者在这篇论文中想要做的是，他们说，好吧，因为我们不需要重新中心化，因为我们假设，重新中心化不是获得层归一化效果所必需的，所以我们想要找到一个不依赖于均值的统计量，而均方根统计量不依赖于均值，所以他们做了与层归一化中完全相同的事情，他们通过行计算均方根统计量，每行一个，然后根据这里的公式进行归一化，所以他们只是除以均方根统计量，然后乘以这个 $\gamma$ 参数，它是可学习的，为什么呢，为什么，均方根归一化，嗯，与层归一化相比，它需要的计算量更少，因为我们只计算一个统计量，所以我们不计算均值和标准差，我们只计算一个，所以它给你带来了计算上的优势，而且在实践中效果很好，所以实际上，论文作者的假设是正确的，我们只需要不变性来获得层归一化的效果，我们不需要重新中心化，至少在 llama 中是这样的，

下一个我们要讨论的话题是位置编码，但在介绍旋转位置编码之前，我们先回顾一下原始 Transformer 中的位置编码，如你所记，在我们将我们的标记转换为嵌入（即大小为 512 的向量）后，在原始 Transformer 中，我们会将另一个表示句子中每个标记位置的向量加到这些嵌入上，而这些位置嵌入是固定的，它们不是由模型学习到的，它们在计算一次后，在训练和推理过程中对每个句子都会重复使用，每个词都有自己大小为 512 的向量，我们有一种新的位置编码，叫作旋转位置编码，所以，绝对位置编码是固定的向量，他们被加到标记的嵌入上，以表示其在句子中的绝对位置，所以第一个标记有自己的向量，第二个标记有自己的向量，第三个标记也有自己的向量，所以绝对位置编码一次处理一个标记，你可以把它想象成地图上的经纬度，地球上的每个点都有其独特的经纬度，这就是地球上每个点位置的绝对指示，这与原始 Transformer 中的绝对位置编码是相同的，我们有一个向量精确的表示那个位置，它被加到那儿特定位置的标记上，

而相对位置编码则一次处理两个标记，并且在计算注意力时起作用，由于注意力机制捕捉了两个词之间关系的强度，相对位置编码告诉注意力机制这两个词在注意力机制中的距离，因此，给定两个标记，我们创建一个表示他们距离的向量，这就是为什么它被称为相对的，因为它相对于两个标记之间的距离，相对位置编码首次在以下这篇来自谷歌的论文中被引入，你可以注意到，Vaswani，我想，是 Transformer 模型的同一位作者，现在使用绝对位置编码，所以在 Attention is all you need 中，当我们计算注意力机制中的点积时，所以如果你记得注意力机制的公式，让我写下来，注意力等于查询乘以键的转置，再除以模型维度的平方根，然后对所有这些进行 softmax，再乘以 V，等等，但我们只关注这个情况下的 Q 乘以 K 的转置，这就是我们在这里看到的，因此当我们计算这个点积时，注意力机制正在计算已经包含绝对位置编码的两个标记之间的点积，因为我们已经将绝对位置编码添加到每个标记中，所以在原始 Transformer 的注意力机制中，我们有两个标记和注意力机制，而在相对位置编码中，我们有三个向量，我们有标记一、标记二，然后这里有一个向量，表示这两个标记之间的距离，因此，在这个注意力机制中，我们涉及三个向量，我们希望注意力机制根据这里的向量以不同的方式匹配这个标记，因此，这个向量将指示注意力机制，即点积，如果关联这两个处于特定距离的词。

我们使用旋转位置嵌入，我们做类似的工作，他们在这篇论文中被引入，及 RoFormer，它们来自一家中国公司，注意力机制中使用的点积是一种内积，所以如果你还记得线性代数，点积是一种具有某些特性的运算，这些特性是每个内积必须具备的特性，因此，内积可以被视为点积的泛化，论文的作者想要做的是，我们能否找到一种内积，它只依赖于注意力机制中使用的两个向量（查询和键）本身以及它们所代表的标记的相对距离，也就是说，给定两个向量，查询和键，它们只包含它们所代表的词的嵌入以及它们在句子中的位置，所以这个 $m$ 实际上是一个绝对数，是一个标量，它表示词在句子中的位置，这个 $n$ 表示句子中第二个词的位置，他们想要说的是，我们能否找到一个内积，所以，我们在这里看到的这个特定括号，是这两个向量之间的内积，它表现的像这个函数 $g$，只依赖于 $x_m$ 的嵌入（即第一个标记）、$x_n$ 的嵌入（即第二个标记）以及他们之间的相对距离，而不依赖其他信息，因此，这个函数将只给出：第一个标记的嵌入、第二个标记的嵌入和一个表示这两个标记相对位置的数字，即这两个标记的相对距离，

是的，我们可以找到这样一个函数，这个函数就是这里定义的那个，可以定义一个像下面这样的函数 g：它只需要依赖于两个嵌入向量 q 和 k 以及相对距离，这个函数定义在复数空间中，并且可以通过使用欧拉公式转换成这种形式，还有一点需要注意的是，我们正在看的这个函数，是为维度为 2 的向量定义的，当然，稍后我们会看到当维度更大时会发生什么，当我们通过欧拉公式将这个表达式（在复数空间中）转换为矩阵形式时，我们可以识别出这个矩阵为旋转矩阵，所以这个矩阵基本上表示向量的旋转，例如这里的这个向量，所以这里的乘积将是一个向量，而这俄国旋转矩阵将把这个向量旋转到由 $m_{\theta}$ 描述的空间中，也就是角度 $m_\theta$，让我们看一个例子，

所以想象一下，我们有一个向量 $v_0$，我们想将其旋转 $\theta$ 角度，即这里的角度 $\theta$，到达向量 $V'$，所以我们所做的是将向量 $V_0$ 乘以这个矩阵，正是这个矩阵，其中的值是这样计算的，$\cos\theta, -\sin\theta, \sin\theta, \cos\theta$，结果向量将是同一个向量，即相同的长度，但旋转了这个角度，这就是为什么它们被称为旋转位置嵌入，因为这个向量代表了一个旋转，现在，当向量不是二维的，而是我们有 $n$ 维时，例如，在原始的 transformer 模型中，我们的嵌入大小是 512，而在 llama 中是 4096，我们需要使用这种形式，现在我想让你注意的是，不是这个矩阵中的数字是什么，而是这个矩阵是稀疏的这一事实，因此，使用它来计算位置嵌入并不方便，因为如果我们将这个嵌入乘以我们的转换流，我们的 GPU，我们的计算机会做很多无用的操作，因为我们已经知道大部分乘积将为零，那么有没有更好的、计算效率更高的方法来完成这个计算呢？是的，有

这种形式，因此，给定一个带有嵌入向量 x 和句子中位置 m 的标记，这就是我们如何计算该标记的位置嵌入，我们取标记的维度，乘以这个矩阵，这里计算如下：其中 $\theta$ 是固定的，$m$ 是标记的位置，$x_1, x_2, x_3$ 是嵌入的维度，所以嵌入的第一个维度，嵌入的第二个维度，等等，加上或减去第二个嵌入向量，计算如下：减去 $x_2$，即向量 x 的第二个嵌入维度的负值，乘以这里的这个矩阵，所以我们没有什么需要学习的，在这个矩阵里，一切都是固定的，因为如果我们看之前的幻灯片，我们可以看到这个 $\theta$ 实际上是像这样为每个维度计算的，所有没有什么需要学习的，所以基本上它们就像绝对位置编码一样，我们计算一次，然后可以对我们将要训练模型的所有句子重复使用，

旋转位置嵌入的另一个有趣特性是长期衰减，所以作者做了什么，他们计算了我们之前看到的内积的上限，所以通过改变两个标记之间的距离来计算 G 函数，然后他们证明了无论这两个标记是什么，都存在一个随着两个标记之间距离增加而减小的上限，如果你还记得我们计算的内积或点积是为了注意力计算，这个点积代表了我们在计算注意力的两个标记之间的关系强度，而旋转位置嵌入所做的，他们基本上会衰减这种关系，这两个标记之间的这种关系的强度，如果我们匹配的两个标记彼此相距很远，而这实际上是我们想要的，所以我们希望相距很远的两个词有较弱的关系，而相距较近的两个词有较强的关系，这是我们希望从旋转位置嵌入中得到的理想特性，

现在，旋转位置嵌入仅应用于查询和键，而不是值，让我们看看为什么，首先，它们基本上在我们计算注意力时发挥作用，所以当我们计算注意力时，是注意力机制会改变分数，如你所记，注意力机制是一种分数，它告诉我们两个标记之间的关系有多强，因此，这种关系会更强或更弱，或者会根据这两个标记在句子中的位置以及它们之间的相对距离而变化，另一件事是，旋转位置嵌入是在向量 Q 和 K 在注意力机制中乘以 W 矩阵之后应用的，而在原始的 Transformer 中，他们是在之前应用的，所以在原始的 Transformer 中，位置嵌入是在我们将标记转换为嵌入之后立即应用的，但在旋转位置嵌入中，即在 llama 中，我们不这样做，我们基本上在注意力机制中乘以 W 矩阵之后，如果你记得，W 矩阵是每个注意力头所拥有的参数矩阵，所以在 llama 中，我们基本上在将向量 q 和 k 乘以 w 矩阵之后应用旋转位置编码，

现在到了有趣的部分，我们将看看 llama 中的自注意力是如何工作的，但在我们讨论 llama 中使用的自注意力之前，我们需要至少简要回顾一下原始 Transformer 中的自注意力，我们从矩阵 Q 开始，这是一个序列乘以模型的矩阵，这意味着我们在行上开始，这是一个序列乘以模型的矩阵，这意味着我们在行上有标记，在列上有嵌入向量的维度，所以我们可以这样想，

我们可以想象它有六行，每一行都是 512 维的向量，表示该标记的嵌入，现在让我删除，然后我们根据这个公式进行乘法运算，所以 Q 乘以 K 的转置，即 K 的转置除以 512 的平方根，这是嵌入向量的维度，其中 K 等于 Q、V 也等于 Q，因为这是自注意力，所以这三个矩阵实际上是相同的序列，
然后我们应用 softmax，得到这个矩阵，所以我们有一个 6 乘 512 的矩阵乘以另一个 512 乘 6 的矩阵，我们将得到一个 6 乘 6 的矩阵，其中每个元素代表第一个标记与自身的点积，然后是第一个标记与第二个标记的点积，第一个标记与第三个标记的点积，第一个标记与第四个标记的点积，等等，所以这个矩阵捕捉了两个标记之间的关系强度，这就是这个 softmax 的输出，乘以 V 矩阵以获得注意力序列，所以自注意力的输出是另一个与初始矩阵维度相同的矩阵，因此，它将生成一个序列，其中嵌入不仅捕捉每个标记的含义，不仅捕捉每个标记的位置，还捕捉该标记与所有其他标记之间的关系，如果你不理解这个概念，请去看我之前关于 Transformer 的视频，我在那里非常仔细且详细的解释了它

现在让我们来看看多头注意力，非常简单的说，多头注意力基本上意味着我们有一个输入序列，我们将其复制到 q、k 和 v 中，所以它们是相同的矩阵，我们乘以参数矩阵，然后将其分割成多个较小的矩阵，每个头一个，并计算这些头之间的注意力，所以头一，头二，头三，头四，然后我们将这些头的输出连接起来，乘以输出矩阵 $W^O$，最后得到多头注意力的输出，

让我们看看什么是第一个 KV 缓存，因此，在介绍 KV 缓存之前，我们需要了解 llama 是如何训练的，以及什么是下一个标记预测任务，因此，llama 与大多数 llms 一样，已经在下一个标记预测任务上进行了训练，这意味着，给定一个序列，它将尝试预测下一个标记，即最有可能继续提示的一个标记，因此，例如，如果我们告诉他一首诗，例如，没有最后一个词，它很可能会想出那首诗中缺失的最后一个词，在这种情况下，我将使用但丁·阿利吉耶里的一段非常著名的段落，我将不使用意大利语翻译器，而是使用这里的英语翻译器，所以我只处理第一行，你可以在这里看到：爱能迅速俘获温柔的心，所以让我们在这个句子中训练 llama，训练是如何进行的，好吧，我们将输入提供给模块输入是这样构想的，我们首先添加句子开始标记，然后目标是这样构建的，我们附加一个句子结束标记，为什么，因为模型，这个 transformer 模型，是一个序列到序列模型，它将输入序列中的每个位置映射到输出序列中的另一个位置，所以基本上，输入序列的第一个标记将映射到输出序列的第二个标记，等等，这也意味着，如果我们给模型输入 sos，它将输出第一个标记，所以是爱，然后如果我们给出前两个标记，它将输出第二个标记，所以是爱能，如果我们给出前三个标记，它将输出第三个标记，当然，模型也会为前两个标记生成输出，但让我们通过一个例子来看，所以如果你还记得我之前的视频，我在其中进行推理，当我们训练模型时，我们只做一步，所以我们给出输入，给出目标，计算损失，我们没有 for 循环来训练一个单一的句子模型，但对于推理，我们需要逐个标记的进行，所以在这种推理中，我们从时间戳 1 开始，我们只给出输入 sos，即句子开始，输出是爱，然后我们在这里取输出标记，爱，并将其附加到输入中，然后我们再次将其提供给模型，模型将生成下一个标记，爱能，然后我们取模型输出的最后一个标记，我们再次将其附加到输入中，模型将生成下一个标记，然后我们再次取下一个标记，即 can，我们将其附加到输入中，并再次将其提供给模型，模型将快速输出下一个标记，我们为所有必要的步骤进行这一过程，知道我们达到句子结束标记，那时我们就知道模型已经完成了输出，现在，这实际上并不是 llama 的训练方式，但这是一个很好的例子，向你展示下一个标记预测任务是如何工作的，

现在，这种方法存在一个问题，让我们看看为什么，在推理的每一步，我们只对模型输出的最后一个标记感兴趣，因为我们已经有了之前的标记，然而，模型需要访问所有之前的标记来决定输出哪个标记，因为它们构成了其上下文或提示，所以我的意思是，例如，要输出单词 D，模型必须看到这里所有的输入，我们不能只给出 seize，模型需要看到所有的输入才能输出这个最后的标记 D，但关键是，这是一个序列到序列的模型，所以他会生成这个序列作为输出，即使我们只关心最后一个标记，所以我们做了很多不必要的计算，重新计算了这些标记，而这些标记我们实际上在之前的步骤中已经有了，所以让我们找到一种方法，避免这种无用的计算，这就是我们使用 KV 缓存所做的事情，因此，KV 缓存是一种在推理过程中减少对已见标记计算量的方法，因此，它仅在 Transformer 模型的推理过程中应用，它不仅适用于 LLaMA 中的 Transformer，还适用于所有 Transformer 模型，因为所有 Transformer 模型都是这样工作的

这是一个描述，展示了在下一个标记预测任务中自注意力机制的工作原理，所以，正如你再我的前几张幻灯片中看到的，我们在这里有一个包含 n 个标记的查询矩阵，然后是键的转置，因此，查询可以被认为是向量行，其中第一个向量代表第一个标记，第二个向量代表第二个标记，等等，然后键的转置是相同的标记，但转置了，所以行变成了列，这产生了一个 n 乘 n 的矩阵，所以如果初始输入矩阵是 9，输出矩阵将是 9 乘 9,然后我们将其乘以 v 矩阵，这将产生注意力，然后，注意力被输入到 Transformer 的线性层，然后线性层将生成 logits，logits 被输入到 softmax 中，softmax 帮助我们决定从词汇表中选择哪个标记，再次提醒，如果你不熟悉这个过程，请观看我之前关于 Transformer 推理的视频，你会更清楚的理解，所以这是一个在自注意力机制中一般层面上发生的事情的描述，现在让我们一步步来看，

所以在推理的第一步，我们只有一个标记，如果你还记得之前，我们只使用了句子的起始标记，所以我们取句子的起始标记，将其与自身相乘，所以转置后，它将产生一个 1 乘 1 的矩阵，所以这个矩阵是 1 乘 4096，乘以另一个 4096 乘 1 的矩阵，这将产生一个 1 乘 1 的矩阵，为什么是 4096，因为 LLaMA 中的嵌入向量是 4096，然后输出，这个 1 乘 1 的矩阵乘以 V，将在这里产生输出标记，而这将成为我们输出的第一个标记，然后我们取这个输出标记，将其附加到下一步的输入中，所以现在我们有两个标记作为输入，它们与自身的转置版本相乘，将产生一个 2 乘 2 的矩阵，然后该矩阵乘以 V 矩阵，将产生两个输出标记，但我们只对模型输出的最后一个标记感兴趣，就是这个注意力，也是，然后将其附加到时间步 3 的输入矩阵中，所以在时间步 3，我们有 3 个标记，它们与自身的转置版本相乘，将产生一个 3 乘 3 的矩阵，然后该矩阵乘以 V 矩阵，我们得到这 3 个输出标记，但我们只对模型输出的最后一个标记感兴趣，所以我们再次将其作为输入附加到 Q 矩阵，现在有 4 个标记，它们与自身的转置版本相乘，将产生一个 4 乘 4 的输出矩阵，然后该矩阵乘以这里的矩阵，将产生这里的注意力矩阵，但我们只对最后一个注意力感兴趣，它将被再次添加到下一步的输入中，但我们已经注意到一些东西，

首先，我们已经在计算这个标记与这个、这个标记与这个、这个标记与这个之间的点积的矩阵中，所以这个矩阵是这两个矩阵之间所有点积的结果，我们可以看到一些东西，第一件事是，我们已经在之前的步骤中计算了这些点积，我们可以缓存它们吗？所以让我们回到之前，如你所见，这个矩阵在增长，二、三、四，看，有很多注意力，因为每次我们推理变换器时，我们都在给它，给变换器一些输入，所以它在重新计算所有这些点积，这很不方便，因为我们实际上已经在之前的步骤中计算过它们了，所以有没有办法不再计算它们，我们可以缓存他们吗，是的，我们可以，然后，由于模型是因果性的，我们不关心一个标记与其前驱的注意力，而只关心它与前一个标记的注意力，所以，如你所记得的，在自注意力中，我们应用了一个掩码，对吧，所以掩码基本上是我们不希望一个词与它后面的词进行点积，而只希望与它前面的词进行点积，所以基本上我们不希望这个矩阵主对角线上方的所有数值，这就是为什么我们在自注意力中应用了掩码，但重点是我们不需要计算所有这些点积，我们唯一感兴趣的点积是这最后一行，所以因为我们在上一步的基础上增加了标记四作为输入，所以我们只有这个新标记，标记四，我们想要知道标记四如何与所有其他标记交互，所以基本上，我们只对这里的最后一行感兴趣，而且，因为我们只关心最后一个标记的注意力，因为我们想从词汇表中选择单词，所以我们只关心最后一行，我们不关心在自注意力输出序列中生成这三个注意力分数，我们只关心最后一个，那么有没有办法去除所有这些冗余计算呢？

是的，我们可以用 KV 缓存来实现，让我们看看如何实现，所以使用 KV 缓存，我们基本上是缓存了，键和值，每次我们有一个新标记时，我们将其附加到键和值上，而查询仅是前一步的输出，所以在开始时，我们没有任何前一步的输出，所以我们只使用第一个标记，所以推理的第一步与没有缓存时是一样的，我们有标记一，它与自身生成一个 1 乘 1 的矩阵，乘以一个标记，然后产生一个注意力，然而，在第二步时，我们不将其附加到之前的查询中，我们只是用这里的新标记替换之前的标记，然而，我们保留键的缓存，所以我们保留键中的前一个标记，并将最后一个输出附加到这里的键和值中，如果你进行这个乘法，他会生成一个 1 乘 2 的矩阵，其中第一个元素是标记二与标记一的点积，以及标记二与标记二的点积，这实际上是我们想要的，如果我们在乘以 V 矩阵，它只会产生一个注意力分数，这正是我们想要的，我们再次这样做，所以我们取这个注意力，这将成为下一步推理的输入，这个标记三，我们将其附加到之前缓存的 K 矩阵和 V 矩阵中，这个乘法将产生一个输出矩阵，我们可以在这里看到，这个输出矩阵与 V 矩阵的乘法将产生一个输出标记，就是这个，我们知道使用这个来选择哪个标记，然后我们将其作为下一步推理的的输入，通过将其附加到缓存的键和缓存的 V 矩阵中，我们进行这个乘法，我们将得到这个矩阵，它是 1 乘 4 的，这是标记 4 与标记 1、标记 4 与标记 2、标记 4 与标记 3 以及标记 4 与自身的点积，我们乘以 V 矩阵，这将只产生一个注意力，这正是我们选择输出标记所需的，

这就是为什么它被称为 KV 缓存的原因，因为我们保留了键和值的缓存，正如你所见，KV 缓存让我们节省了很多计算，因为我们不再进行以前需要做的大量点积运算，这使得推理速度更快，

接下来，我们要讨论的是分组多查询注意力，但在讨论分组多查询注意力之前，我们需要先介绍它的前身，多查询注意力，让我们来看看，所以让我们从问题开始，问题是 GPU 太快了，如果你看这份数据表，这是来自英伟达 A100 GPU，我们可以看到 GPU 在计算和执行计算方面非常快，但在从其内存传输数据方面并不那么快，这意味着，例如，A100 可以使用 32 位精度每秒执行 19.5万亿次浮点运算，而它每秒只能传输 1.9 千兆字节，在数据传输方面，它的速度几乎是计算速度的 10 倍慢，这意味着有时瓶颈不在于我们执行了多少操作，而在于我们操作需要多少数据传输，而这取决于我们计算中涉及的张量的大小和数量，例如，如果我们在同一个张量上计算相同的操作 n 次，可能比在 n 个不同标记上计算相同的操作更快，即使它们的大小相同，这是因为 GPU 可能需要移动这些张量，因此，这意味着我们的目标不仅应该是优化我们算法执行的操作数量，还应该尽量减少算法执行的内存访问和内存传输，因为与计算相比，内存访问和内存传输在时间上更为昂贵，这在软件中也会发生，当我们进行 I/O 操作时，例如，如果我们复制，在 CPU 中进行一些乘法运算，或者从硬盘读取一些数据，从硬盘读取数据比在 CPU 上进行大量计算要慢的多，这是一个问题，

现在，在这篇论文中，我们介绍了多查询注意力，这篇论文来自 Noam Shazeer，他也是 attention is all you need 这篇论文的作者之一，在这篇论文中，他提出了这个问题，他说，好吧，让我们看看多头注意力，也就是批量多头注意力，这是在原论文 attention is all you need 中提出的多头注意力，让我们看看这个算法，并计算一下执行的算数操作数量以及这些操作涉及的总内存，他计算出算数操作的数量是 O(bnd^2)，其中 b 是批量大小，n 是序列长度，d 是嵌入向量的大小，而操作中涉及的总内存，包括所有参与计算的张量（包括派生的张量）等于 O(bnd+bhn^2+d^2)，其中 h 是多头注意力中的头数，现在，如果我们计算总内存与算数操作数量之间的比率，我们得到这个表达式：O(1/k+1/bn)，在这种情况下，比率远小于 1,这意味着我们执行的内存访问次数远小于算数操作的数量，因此，在这种情况下，内存访问不是瓶颈，我的意思是，这个算法的瓶颈不是内存访问，实际上是计算的数量，正如你之前看到的，当我们引入 KV 缓存时，我们试图解决的问题是计算的数量，但通过引入 KV 缓存，我们创造了一个新问题，