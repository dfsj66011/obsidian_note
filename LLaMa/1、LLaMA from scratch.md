
> https://www.bilibili.com/video/BV1xe4peAESX/?spm_id_from=333.1387.collection.video_card.click&vd_source=aced32e35ad9cff83fe98c60854f183c

欢迎观看我关于 LLaMA 的新视频，在这个视频中，我们将了解什么是 llama，它是如何制作的，它在结构上与 Transformer 有何不同，并且我们将逐一构建构成 llama 的每个模块，因此，我不仅会从概念上解释每个模块的功能，还会从数学角度和编程角度进行探讨，以便我们将理论与实践结合起来，我可以保证，如果你观看这个视频，你将对 llama 模型背后的核心原理有深刻的理解，这样，你不仅能理解各个模块是如何相互作用的，还能了解它们的功能以及为什么最初需要这些模块。

在这个视频中，我们将回顾许多主题，首先从 Vanilla Transformer 和 llama 模型之间的架构差异开始，我们将探讨新的归一化方法，RMS 归一化、旋转位置编码、KV 缓存、多查询注意力、分组多查询注意力、以及前馈层的 SwiGLU 激活函数。

当然，我假设你已经具备一定的背景知识，首先，我强烈建议你观看我之前关于 Transformer 的视频，因为你需要了解 Transformer 的工作原理，在我之前的视频中，我还探讨了训练和推理 Transformer 模块的概念，这个视频大约 45 分钟，我认为值的一看，因为它将真正让你深入理解 Transformer，在你掌握了这些知识后，就可以观看这个视频了，无论如何，对于那些已经看过视频但忘记了一些内容的人，我会在我们探讨各个主题的过程中回顾大部分概念，我也假设你具备一些基本的线性代数知识，比如矩阵、乘法、点积等基础内容，此外，由于我们将使用旋转位置编码，对复数有一些了解也是有益的，即使它不是最基础的部分，所以，如果你不记得复数或它们的工作原理，或者欧拉公式，那也没关系，你将理解这个概念，而不是数学细节，这并不是非常基础的。

------------

有时我会回顾你可能已经熟悉的主题，所以请随意跳过这些部分，我们通过回顾 Vanilla Transformer 和 llama 之间的架构差异来开始我们的旅程。

这张图是我自己制作的，在右边，因为我找不到论文中的架构图，那么，让我们来看看这些差异，如你所记，在 Vanilla Transoformer 中，我们有编码器部分和解码器部分，而在 llama 中，我们只有编码器，首先，因为 llama 是一个 LLMs，它已经接受了下一个预测 token 任务的训练，因此，基本上我们只需要自注意力机制来预测下一个 token，我们会看到所有这些概念，因此我们将了解下一个预测任务是什么，它是如何工作的，以及这种新的自注意力机制是如何工作的。

从这些图片中，我们可以看到的第二个区别是，一开始我们有嵌入层，而在原始的 Transformer 中也有嵌入层，但在嵌入层之后，我们没有位置编码，而是有一个 RMS 归一化，实际上，所有的归一化都移到了块之前，所以之前我们有多头注意力机制，然后是 Add 和归一化，也就是这里的加号，所以这是一个残差连接，多头注意力的输出和归一化的组合，在每个块之后都有；但在这里的 llama 中，我们在每个块之前进行归一化，我们将回顾什么是归一化，以及为什么它以这种方式工作，在归一化之后，我们为自注意力机制提供了 Q，K，V 的输入，我们可以注意到的一点是，位置编码不再是 Transformer 的位置编码，而是变成了旋转位置编码，并且他们只应用于 Q 和 K，而不是值，我们也会看到为什么。

另一件事是自注意力机制现在是有 KV 缓存的自注意力，我们将了解什么是 KV 缓存以及它是如何工作的，还有这种分组多头查询注意力，

另一个变化是这个前馈层，在 Vanilla Transformer 的原始前馈层中，我们为前馈块使用了 relu 激活函数，但在 llama 中，我们使用的是 SwiGLU 函数，我们将会看到原因，

右侧的 Nx 表示虚线框中的这个块被重复了 N 次，一个接一个，使得最后一层的输出被送入这个 RMS 归一化，然后是线性层，最后是 softmax。

我们将从底层开始构建每一个这样的块，所以我会详细展示这些块具体做什么，它们如何工作，如何相互作用，背后的数学原理是什么，以及它们视图解决的问题是什么，因此，我们将对这些模型有深入的了解，让我们从回顾 llama 引入的模型开始我们的旅程，

----

llama 与 2023 年 2 月发布，他们为这个模型设定了四个维度，6.7B、13B、32B、65B，dimension 基本上是将每个 token 转换为这个 dimension 所指示大小的向量，n heads 即注意力机制有多少个头，以及 n layers

如果你还记得原始的 transformer，dimension 是 512，heads 是 8 个，layers 是 6 层，然后我们有每个模型训练所用的 token 数量，是 1T（万亿）和 1.4T，在 llama2 中，大多数数字都翻倍了，所以上下文长度，就是序列长度，模型能处理的最长序列是多少，然后，模型训练所用的 token 数量也翻倍了，所以从 1T 到 2T，每个模型的大小都有所增加，而参数量大致保持不变，然后我们有 GQA 这一列，它表示这两个大小的模型，即 34B 和 70B 参数的模型，使用了 GQA，我们会看到它是如何工作的。

---------

让我们从回顾这里的嵌入层开始，为此，我将使用我之前视频中的幻灯片，如果你还记得我之前的视频，我们是这样介绍嵌入的，我们有一个由六个词组成的句子，我们所做的是对句子进行分词，将其转换为 token，分词通常不是通过空格来完成的，而是通过 BPE 分词器，所以实际上每个词会被拆分成子词，但为了清晰和简便起见，我们只是通过空格来分词，因此，每个 token 之间都用空格隔开，然后，每个 token 都会被映射到词汇表中的相应位置，因此，词汇表就是模型能够识别的所有单词的列表，当然，它们不一定是单词，它们可以是任何东西，它们只是 tokens，因此，每个 token 在词汇表中占据一个位置，输入 id 则表示每个 token 在词汇表中所占据的编号，然后，我们将每个输入 id 映射到一个大小为 512 的向量，但在 llama 中，这个向量大小变为 4096，这些嵌入向量是可学习的，因此它们是模型的参数，在模型训练过程中，这些嵌入向量会发生变化，以便捕捉它们所映射单词的含义，因此，我们希望 cat 和 dog 这两个词的嵌入向量相似，因为它们在某种程度上映射了相似的概念，至少它们属于同一个语义组，同样的，如果我们检查这两个向量，房子和建筑这两个词的嵌入向量也会非常接近，这就是嵌入背后的理念，

-----------

现在让我们看看什么是归一化，因为这是嵌入层之后的层，为此，我们先回顾一下神经网络及其工作原理，假设我们有一个前馈神经网络，它有一个输入层，一个由神经元组成的隐藏层，另一个由五个神经元组成的隐藏层，最后映射到一个输出层，我们通常有一个目标，并通过将输出与目标进行比较来产生损失，然后，损失通过反向传播的方式传递回两个隐藏层，因此，我们所做的是计算损失相对于这两个隐藏层中每个权重的梯度，并根据学习率调整这些隐藏层的权重，以检查为什么需要归一化以及归一化的必要性，我将简化神经网络，以便更好的说明这一点，

---------

所以，假设我们的神经网络实际上是一个制造手机的工厂，因此，为了制作手机，我们从一些原材料开始，这些原材料被交给硬件团队，他们会利用这些原材料生产出一些硬件，例如，他们可能会选择蓝牙设备、显示屏、麦克风、摄像头等，并将这些不见组装成手机的硬件。然后硬件团队将这个原型交给软件团队，软件团队随后为这个硬件开发软件。然后软件团队的输出就是带有硬件和软件的完整手机，作为最终的输出，然后将这个输出与手机的原始设计进行比较，然后我们计算损失，那么，我们设定的手机目标与我们实际生产的产品之间有什么差异呢

假设损失就是我们的 CEO，假设损失相当大，我们的 CEO 会与硬件团队和软件团队沟通，并告诉他们调整策略，以便下次更接近目标，假设硬件成本过高，因此，CEO 会告诉硬件团队使用更小的显示屏，选择更便宜的摄像头，将蓝牙改为低范围的，将 Wi-Fi 改为低能耗的，更换电池等等，我们还会与软件团队讨论，调整他们的策略，可能告诉软件团队减少重构的投入，减少培训，招聘更多的实习生，不太在意员工，因为成本太高了，等等，这将调整软件和硬件团队的策略，所以下次我们再次从原材料开始，让我们回到起点，我们再次从原材料开始，根据 CEO 设定的新策略，硬件团队将生产新的硬件，现在问题出现了，软件团队现在将收到一个他们从未见过的硬件，因为显示屏、蓝牙、Wi-Fi 等所有东西都变了，因此，软件团队需要重新做很多工作，特别是他们需要大幅调整策略，因为他们面对的是从未见过的东西，因此，软件团队的输出将与他们之前的输出大不相同，而且可能离目标更远，因为软件团队没有准备好做出所有这些调整，所以他们可能浪费了很多时间，可能浪费了很多资源，甚至可能无法达到目标，甚至无法接近目标，这次可能损失更大，

因此，如你所见，问题在于损失函数修改了硬件和软件团队的权重，但在下一次迭代中，软件团队收到了一个从未见过的输入，这个输入导致它产生的输出与之前大相径庭，这会使模型在损失中产生波动，并使训练变得非常缓慢，

现在让我们从数学层面看看发生了什么，以理解归一化是如何工作的，让我们回顾一些数学知识，假设我们有一个线性层，定义为 `nn.Linear` 具有 3 个输入特征和 5 个输出特征，并带有偏置，这是 PyTorch 中定义的线性层，线性层将创建两个矩阵，一个称为 W 权重，另一个称为 B 偏置，假设我们有一个形状为 10 行 3 列的输入，这个线性层对这个输入 x 的输出将是 10 行 5 列，但这是如何从数学上实现的呢，让我们回顾一下，想象一下，我们有一个输入，它是 10 行 3 列的，这意味着我们有 10 个 items，每个 item 有 3 个特征，线性层创建的 W 矩阵将是 5 行 3 列的，即由 3 个输入特征生成 5 个输出特征，我们可以把每一行看作一个神经元，每个神经元有 3 个权重，每个输入特征 x 对应一个权重，然后我们有一个偏置向量，偏置向量对每个神经元有一个权重，因为每个神经元都有一个偏置，这将产生一个 10 行 5 列的输出，意味着我们有 10 个 items，每个 items 有 5 个特征，让我们试着理解这些矩阵中信息的流动过程，信息的流动由这个表达式控制：$$O=XW^T+b$$
因此，输出等于 x 乘以 w 矩阵的转置在加上 b，假设我们有一个输入 x，并且有一个 item，该 item 有三个特征 a1、a2 和 a3，W 的转置是这里的这个矩阵，我们交换了行和列，因为根据公式，我们需要对这个矩阵进行转置，所以我们有一个神经元，它有三个权重 w1、w2、w3，我们把它们相乘，得到了这个矩阵，因此，x 乘以 w 的转置产生这个矩阵，其中第一行是这个行向量与这个列向量的点积，然后我们加上 b 行向量，正如你所见，要相加两个矩阵，它们需要有相同的维度，但在 PyTorch 中，由于广播机制，这一行会被加到这里的这一行，然后独立的加到这一行和这一行，等等，由于广播机制，然后我们将得到这个输出，这里的第一个 item 将是 z1，z1 是什么，z1 = r1 + b1，但 r1 是什么，r1 是这个列与这个行或这个行与这个列的点积，所以是这里的这个表达式，因此 item1 的神经元 1 的输出仅取决于 item 1 的特征，通常在这个输出之后，我们还会应用一个非线性函数，比如 ReLU 函数，ReLU 函数的参数被称为神经元 1 的激活，现在，正如我们所见，神经元 1 的输出仅依赖于每个 item 的输入特征

因此一个神经元对某个数据项的输出取决于输入数据项的特征和神经元的参数，我们可以将输入到神经元的数据视为前一层的输出，所以，例如，我们之前看到的那个输入 x，它也可以是前一层的输出，如果前一层，由于梯度下降更新了其权重，导致输出发生巨大变化，就像我们之前所做的那样，例如，因为 CEO 重新调整了硬件团队的策略，所以前一层，即硬件团队，将产生与以往大不相同的输出，下一层也将因此大幅改变其输出，因此在梯度下降的下一步，它将被迫大幅调整其权重，所以我们不喜欢的是，前一层的输出权重变化太大，以至于下一层也不得不大幅改变其输出，以符合损失函数定义的策略，因此，这种导致神经元内部节点分布发生变化的现象被称为内部协变量偏移（Internal Covariate Shift），我们希望避免这种现象，因为它会使网络训练变慢，因为神经元被迫因前一层输出的剧烈变化而在一个方向或另一个方向上大幅调整其权重，

那么我们应该怎么做，我们在普通的 Transformer 中进行层归一化，那么，让我们回顾一下层归一化是如何工作的，假设我们仍然有一个由 10 行 3 列定义的输入 x，并且对于这些 item 中的每一个，我们独立计算两个统计量，一个是 $\mu$ 即均值，另一个是 $\sigma$ 即方差，然后我们根据这个公式对这个矩阵中的值进行归一化，所以我们基本上取 x 减去 $\mu$，即每个 item 减去 $\mu$，然后除以方差加上 $\varepsilon$ 的平方根，其中 $\varepsilon$ 是一个非常小的数，这样即使方差很小，我们也永远不会通过这种方式除以零，然后，这些数字中的每一个都乘以两个参数，一个是 $\gamma$，一个是 $\beta$，这两个参数都是由模块学习的，并且它们很有用，因为模块可以通过调整 $\gamma$ 和 $\beta$ 来放大它需要的值，所以在我们使用层归一化之前，我们通常使用批量归一化进行归一化，而使用批量归一化时，唯一的区别是我们不是按行计算统计，而是按列计算，所以特征1、特征2、和特征3，使用层归一化时，我们按行进行，因此，每一行都会有自己的 $\mu$ 和 $\sigma$，因此，通过使用层归一化，我们基本上将特征的初始分布（无论它们是什么）转换为均值为零、方差为一的归一化数值，所以这个公式实际上来自概率统计，如果你还记得，让我用笔写一下

好的，如果你还记得，基本上，如果我们有一个变量 x，它想正态分布的变量一样分布，均值为 5，方差为 36，如果我们做 x 减去其均值，即 5，再除以方差的平方根，即 36，这个变量，这里我们称之为 z，将会像 $z \sim N(0, 1)$ 一样，即它将成为一个标准高斯分布，这正是我们在这里所做的，所以我们将其转换为标准高斯分布，这样这个值，大多数时候，将接近于零，我的意思是将围绕零分布，

现在让我们来谈谈均方根归一化，这是 llama 使用的，均方根归一化是在这篇论文中引入的，即这两位研究者提出的均方根层归一化，让我们一起读这篇论文，层归一化成功的众所周知解释是其重新中心化和重新缩放的不变性特性，那么，他们是什么意思呢？重新中心化和重新缩放的不变性是什么，特征的事实是，无论它们是什么，它们都将被重新中心化到均值为 0，并重新缩放到方差为 1，前者使模型对输入和权重的偏移噪声不敏感，而后者在输入和权重的偏移噪声不敏感，而后者在输入和权重随机缩放时保持输出表示不变，在这篇论文中，我们假设重新缩放方差是层归一化成功的原因，而不是重新中心化的不变性，所以他们在论文中声称，层归一化的成功基本上不是因为重新中心化和重新缩放，而主要是因为重新缩放，所以基本上是除以方差，所以方差为一。

他们所做的基本上是说，好吧，我们能否找到另一个不依赖于均值的统计量，因为我们认为它不是必要的，嗯，是的，他们使用的这个均方根统计量，所以这里定义的这个统计量$$
\bar{a}_i = \frac{a_i}{\text{RMS}(a)} g_i, \quad \text{where} \quad \text{RMS}(a) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}.$$
正如你从这个统计量的表达式中看到的，我们不再使用均值来计算它，因为之前的统计量，即方差，需要均值来计算，因为，如果你记得，方差的计算需要均值，所以方差等于 x 减去 $\mu$ 的平方和除以 n，所以我们需要均值来计算方差，因此，作者在这篇论文中想要做的是，他们说，好吧，因为我们不需要重新中心化，因为我们假设，重新中心化不是获得层归一化效果所必需的，所以我们想要找到一个不依赖于均值的统计量，而均方根统计量不依赖于均值，所以他们做了与层归一化中完全相同的事情，他们通过行计算均方根统计量，每行一个，然后根据这里的公式进行归一化，所以他们只是除以均方根统计量，然后乘以这个 $\gamma$ 参数，它是可学习的，为什么呢，为什么，均方根归一化，嗯，与层归一化相比，它需要的计算量更少，因为我们只计算一个统计量，所以我们不计算均值和标准差，我们只计算一个，所以它给你带来了计算上的优势，而且在实践中效果很好，所以实际上，论文作者的假设是正确的，我们只需要不变性来获得层归一化的效果，我们不需要重新中心化，至少在 llama 中是这样的，

下一个我们要讨论的话题是位置编码，但在介绍旋转位置编码之前，我们先回顾一下原始 Transformer 中的位置编码，如你所记，在我们将我们的标记转换为嵌入（即大小为 512 的向量）后，在原始 Transformer 中，我们会将另一个表示句子中每个标记位置的向量加到这些嵌入上，而这些位置嵌入是固定的，它们不是由模型学习到的，它们在计算一次后，在训练和推理过程中对每个句子都会重复使用，每个词都有自己大小为 512 的向量，我们有一种新的位置编码，叫作旋转位置编码，所以，绝对位置编码是固定的向量，他们被加到标记的嵌入上，以表示其在句子中的绝对位置，所以第一个标记有自己的向量，第二个标记有自己的向量，第三个标记也有自己的向量，所以绝对位置编码一次处理一个标记，你可以把它想象成地图上的经纬度，地球上的每个点都有其独特的经纬度，这就是地球上每个点位置的绝对指示，这与原始 Transformer 中的绝对位置编码是相同的，我们有一个向量精确的表示那个位置，它被加到那儿特定位置的标记上，

而相对位置编码则一次处理两个标记，并且在计算注意力时起作用，由于注意力机制捕捉了两个词之间关系的强度，相对位置编码告诉注意力机制这两个词在注意力机制中的距离，因此，给定两个标记，我们创建一个表示他们距离的向量，这就是为什么它被称为相对的，因为它相对于两个标记之间的距离，相对位置编码首次在以下这篇来自谷歌的论文中被引入，你可以注意到，Vaswani，我想，是 Transformer 模型的同一位作者，现在使用绝对位置编码，所以在 Attention is all you need 中，当我们计算注意力机制中的点积时，所以如果你记得注意力机制的公式，让我写下来，注意力等于查询乘以键的转置，再除以模型维度的平方根，然后对所有这些进行 softmax，再乘以 V，等等，但我们只关注这个情况下的 Q 乘以 K 的转置，这就是我们在这里看到的，因此当我们计算这个点积时，注意力机制正在计算已经包含绝对位置编码的两个标记之间的点积，因为我们已经将绝对位置编码添加到每个标记中，所以在原始 Transformer 的注意力机制中，我们有两个标记和注意力机制，而在相对位置编码中，我们有三个向量，我们有标记一、标记二，然后这里有一个向量，表示这两个标记之间的距离，因此，在这个注意力机制中，我们涉及三个向量，我们希望注意力机制根据这里的向量以不同的方式匹配这个标记，因此，这个向量将指示注意力机制，即点积，如果关联这两个处于特定距离的词。

我们使用旋转位置嵌入，我们做类似的工作，他们在这篇论文中被引入，及 RoFormer，它们来自一家中国公司，注意力机制中使用的点积是一种内积，所以如果你还记得线性代数，点积是一种具有某些特性的运算，这些特性是每个内积必须具备的特性，因此，内积可以被视为点积的泛化，论文的作者想要做的是，我们能否找到一种内积，它只依赖于注意力机制中使用的两个向量（查询和键）本身以及它们所代表的标记的相对距离，也就是说，给定两个向量，查询和键，它们只包含它们所代表的词的嵌入以及它们在句子中的位置，所以这个 $m$ 实际上是一个绝对数，是一个标量，它表示词在句子中的位置，这个 $n$ 表示句子中第二个词的位置，他们想要说的是，我们能否找到一个内积，所以，我们在这里看到的这个特定括号，是这两个向量之间的内积，它表现的像这个函数 $g$，只依赖于 $x_m$ 的嵌入（即第一个标记）、$x_n$ 的嵌入（即第二个标记）以及他们之间的相对距离，而不依赖其他信息，因此，这个函数将只给出：第一个标记的嵌入、第二个标记的嵌入和一个表示这两个标记相对位置的数字，即这两个标记的相对距离，

是的，我们可以找到这样一个函数，这个函数就是这里定义的那个，可以定义一个像下面这样的函数 g：它只需要依赖于两个嵌入向量 q 和 k 以及相对距离，这个函数定义在复数空间中，并且可以通过使用欧拉公式转换成这种形式，还有一点需要注意的是，我们正在看的这个函数，是为维度为 2 的向量定义的，当然，稍后我们会看到当维度更大时会发生什么，当我们通过欧拉公式将这个表达式（在复数空间中）转换为矩阵形式时，我们可以识别出这个矩阵为旋转矩阵，所以这个矩阵基本上表示向量的旋转，例如这里的这个向量，所以这里的乘积将是一个向量，而这俄国旋转矩阵将把这个向量旋转到由 $m_{\theta}$ 描述的空间中，也就是角度 $m_\theta$，让我们看一个例子，

所以想象一下，我们有一个向量 $v_0$，我们想将其旋转 $\theta$ 角度，即这里的角度 $\theta$，到达向量 $V'$，所以我们所做的是将向量 $V_0$ 乘以这个矩阵，正是这个矩阵，其中的值是这样计算的，$\cos\theta, -\sin\theta, \sin\theta, \cos\theta$，结果向量将是同一个向量，即相同的长度，但旋转了这个角度，这就是为什么它们被称为旋转位置嵌入，因为这个向量代表了一个旋转，现在，当向量不是二维的，而是我们有 $n$ 维时，例如，在原始的 transformer 模型中，我们的嵌入大小是 512，而在 llama 中是 4096，我们需要使用这种形式，现在我想让你注意的是，不是这个矩阵中的数字是什么，而是这个矩阵是稀疏的这一事实，因此，使用它来计算位置嵌入并不方便，因为如果我们将这个嵌入乘以我们的转换流，我们的 GPU，我们的计算机会做很多无用的操作，因为我们已经知道大部分乘积将为零，那么有没有更好的、计算效率更高的方法来完成这个计算呢？是的，有

这种形式，因此，给定一个带有嵌入向量 x 和句子中位置 m 的标记，这就是我们如何计算该标记的位置嵌入，我们取标记的维度，乘以这个矩阵，这里计算如下：其中 $\theta$ 是固定的，$m$ 是标记的位置，$x_1, x_2, x_3$ 是嵌入的维度，所以嵌入的第一个维度，嵌入的第二个维度，等等，加上或减去第二个嵌入向量，计算如下：减去 $x_2$，即向量 x 的第二个嵌入维度的负值，乘以这里的这个矩阵，所以我们没有什么需要学习的，在这个矩阵里，一切都是固定的，因为如果我们看之前的幻灯片，我们可以看到这个 $\theta$ 实际上是像这样为每个维度计算的，所有没有什么需要学习的，所以基本上它们就像绝对位置编码一样，我们计算一次，然后可以对我们将要训练模型的所有句子重复使用，

旋转位置嵌入的另一个有趣特性是长期衰减，所以作者做了什么，他们计算了我们之前看到的内积的上限，所以通过改变两个标记之间的距离来计算 G 函数，然后他们证明了无论这两个标记是什么，都存在一个随着两个标记之间距离增加而减小的上限，如果你还记得我们计算的内积或点积是为了注意力计算，这个点积代表了我们在计算注意力的两个标记之间的关系强度，而旋转位置嵌入所做的，他们基本上会衰减这种关系，这两个标记之间的这种关系的强度，如果我们匹配的两个标记彼此相距很远，而这实际上是我们想要的，所以我们希望相距很远的两个词有较弱的关系，而相距较近的两个词有较强的关系，这是我们希望从旋转位置嵌入中得到的理想特性，

现在，旋转位置嵌入仅应用于查询和键，而不是值，让我们看看为什么，首先，它们基本上在我们计算注意力时发挥作用，所以当我们计算注意力时，是注意力机制会改变分数，如你所记，注意力机制是一种分数，它告诉我们两个标记之间的关系有多强，因此，这种关系会更强或更弱，或者会根据这两个标记在句子中的位置以及它们之间的相对距离而变化，另一件事是，旋转位置嵌入是在向量 Q 和 K 在注意力机制中乘以 W 矩阵之后应用的，而在原始的 Transformer 中，他们是在之前应用的，所以在原始的 Transformer 中，位置嵌入是在我们将标记转换为嵌入之后立即应用的，但在旋转位置嵌入中，即在 llama 中，我们不这样做，我们基本上在注意力机制中乘以 W 矩阵之后，如果你记得，W 矩阵是每个注意力头所拥有的参数矩阵，所以在 llama 中，我们基本上在将向量 q 和 k 乘以 w 矩阵之后应用旋转位置编码，

现在到了有趣的部分，我们将看看 llama 中的自注意力是如何工作的，但在我们讨论 llama 中使用的自注意力之前，我们需要至少简要回顾一下原始 Transformer 中的自注意力，我们从矩阵 Q 开始，这是一个序列乘以模型的矩阵，这意味着我们在行上开始，这是一个序列乘以模型的矩阵，这意味着我们在行上有标记，在列上有嵌入向量的维度，所以我们可以这样想，

我们可以想象它有六行，每一行都是 512 维的向量，表示该标记的嵌入，现在让我删除，然后我们根据这个公式进行乘法运算，所以 Q 乘以 K 的转置，即 K 的转置除以 512 的平方根，这是嵌入向量的维度，其中 K 等于 Q、V 也等于 Q，因为这是自注意力，所以这三个矩阵实际上是相同的序列，
然后我们应用 softmax，得到这个矩阵，所以我们有一个 6 乘 512 的矩阵乘以另一个 512 乘 6 的矩阵，我们将得到一个 6 乘 6 的矩阵，其中每个元素代表第一个标记与自身的点积，然后是第一个标记与第二个标记的点积，第一个标记与第三个标记的点积，第一个标记与第四个标记的点积，等等，所以这个矩阵捕捉了两个标记之间的关系强度，这就是这个 softmax 的输出，乘以 V 矩阵以获得注意力序列，所以自注意力的输出是另一个与初始矩阵维度相同的矩阵，因此，它将生成一个序列，其中嵌入不仅捕捉每个标记的含义，不仅捕捉每个标记的位置，还捕捉该标记与所有其他标记之间的关系，如果你不理解这个概念，请去看我之前关于 Transformer 的视频，我在那里非常仔细且详细的解释了它

现在让我们来看看多头注意力，非常简单的说，多头注意力基本上意味着我们有一个输入序列，我们将其复制到 q、k 和 v 中，所以它们是相同的矩阵，我们乘以参数矩阵，然后将其分割成多个较小的矩阵，每个头一个，并计算这些头之间的注意力，所以头一，头二，头三，头四，然后我们将这些头的输出连接起来，乘以输出矩阵 $W^O$，最后得到多头注意力的输出，

让我们看看什么是第一个 KV 缓存，因此，在介绍 KV 缓存之前，我们需要了解 llama 是如何训练的，以及什么是下一个标记预测任务，因此，llama 与大多数 llms 一样，已经在下一个标记预测任务上进行了训练，这意味着，给定一个序列，它将尝试预测下一个标记，即最有可能继续提示的一个标记，因此，例如，如果我们告诉他一首诗，例如，没有最后一个词，它很可能会想出那首诗中缺失的最后一个词，在这种情况下，我将使用但丁·阿利吉耶里的一段非常著名的段落，我将不使用意大利语翻译器，而是使用这里的英语翻译器，所以我只处理第一行，你可以在这里看到：爱能迅速俘获温柔的心，所以让我们在这个句子中训练 llama，训练是如何进行的，好吧，我们将输入提供给模块输入是这样构想的，我们首先添加句子开始标记，然后目标是这样构建的，我们附加一个句子结束标记，为什么，因为模型，这个 transformer 模型，是一个序列到序列模型，它将输入序列中的每个位置映射到输出序列中的另一个位置，所以基本上，输入序列的第一个标记将映射到输出序列的第二个标记，等等，这也意味着，如果我们给模型输入 sos，它将输出第一个标记，所以是爱，然后如果我们给出前两个标记，它将输出第二个标记，所以是爱能，如果我们给出前三个标记，它将输出第三个标记，当然，模型也会为前两个标记生成输出，但让我们通过一个例子来看，所以如果你还记得我之前的视频，我在其中进行推理，当我们训练模型时，我们只做一步，所以我们给出输入，给出目标，计算损失，我们没有 for 循环来训练一个单一的句子模型，但对于推理，我们需要逐个标记的进行，所以在这种推理中，我们从时间戳 1 开始，我们只给出输入 sos，即句子开始，输出是爱，然后我们在这里取输出标记，爱，并将其附加到输入中，然后我们再次将其提供给模型，模型将生成下一个标记，爱能，然后我们取模型输出的最后一个标记，我们再次将其附加到输入中，模型将生成下一个标记，然后我们再次取下一个标记，即 can，我们将其附加到输入中，并再次将其提供给模型，模型将快速输出下一个标记，我们为所有必要的步骤进行这一过程，知道我们达到句子结束标记，那时我们就知道模型已经完成了输出，现在，这实际上并不是 llama 的训练方式，但这是一个很好的例子，向你展示下一个标记预测任务是如何工作的，

现在，这种方法存在一个问题，让我们看看为什么，在推理的每一步，我们只对模型输出的最后一个标记感兴趣，因为我们已经有了之前的标记，然而，模型需要访问所有之前的标记来决定输出哪个标记，因为它们构成了其上下文或提示，所以我的意思是，例如，要输出单词 D，模型必须看到这里所有的输入，我们不能只给出 seize，模型需要看到所有的输入才能输出这个最后的标记 D，但关键是，这是一个序列到序列的模型，所以他会生成这个序列作为输出，即使我们只关心最后一个标记，所以我们做了很多不必要的计算，重新计算了这些标记，而这些标记我们实际上在之前的步骤中已经有了，所以让我们找到一种方法，避免这种无用的计算，

