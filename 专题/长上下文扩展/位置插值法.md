
* https://arxiv.org/pdf/2306.15595
* meta （2023）

**摘要**    我们提出位置插值法（Position Interpolation，PI），该方法通过极少量微调（1000 步以内）即可将基于 RoPE 的预训练大语言模型（如 LLaMA 系列模型）的上下文窗口扩展至 32768，同时在需要长上下文的任务中展现出强劲性能表现——从 7B 到 65B 参数的 LLaMA 模型在密钥检索、语言建模及长文档摘要等任务中均取得优异结果。经位置插值扩展的模型在其原始上下文窗口范围内的任务上仍能保持较好质量。该方法通过线性压缩输入位置索引使其适配原始上下文窗口尺寸，而非外推超出训练长度的位置索引（后者可能导致灾难性高注意力分数，彻底破坏自注意力机制）。理论研究表明，插值法的误差上界至少比外推法小 600 倍，进一步验证了其稳定性。通过位置插值扩展的模型保持原有架构，可复用绝大多数既有优化方案与基础设施。

## 1、引言

LLMs 通常具有预定义的上下文窗口大小。例如，LLaMA 模型的输入必须少于 2048 个标记。在诸如进行长对话、总结长文档或执行长期规划等应用中，经常超出这个预设的上下文窗口限制。对于这些应用，具有更长上下文窗口的 LLMs 更为理想。然而，从头开始训练一个具有长上下文窗口的 LLM 需要大量投入。这自然引出了一个问题：我们能否扩展现有预训练 LLM 的上下文窗口？

一种直接的方法是对现有的预训练 Transformer 进行微调，使其具有更长的上下文窗口。然而，通过实验我们发现，以这种方式训练的模型对长上下文窗口的适应速度非常缓慢。在训练超过 10000 个批次后，有效的上下文窗口仅从 2048 增加到 2560（表 4），这表明这种方法在扩展到更长的上下文窗口时效率低下。

尽管某些技术能够实现 Transformer 的长度外推，即在较短的上下文窗口上进行训练，在较长的窗口上进行推理，但许多现有的预训练 LLM（包括 LLaMA）使用的位置编码具有较弱的外推特性（例如RoPE）。因此，这些技术在扩展此类 LLM 的上下文窗口大小方面的适用性仍然有限。

在这项工作中，我们引入了位置插值法（Position Interpolation），为包括 LLaMA 在内的某些现有预训练大语言模型实现上下文窗口扩展。其核心思想是：不同于外推法，我们直接对位置索引进行降尺度处理，使最大位置索引与预训练阶段原有的上下文窗口限制相匹配（如图 1 所示）。换言之，为了容纳更多输入标记，我们利用位置编码可应用于非整数位置的特性，对相邻整数位置的位置编码进行插值——这相比在已训练位置范围外进行外推（可能导致灾难性数值）更具优势。我们从理论上验证了该方法的有效性：在 LLaMA 7B 模型设定中，插值后的注意力分数上界比外推法小约 600 倍，因而更加稳定。这使得模型更容易适应插值后的位置编码。

[图 1]

**图 1**：我们的位置插值方法示意图。假设一个 Llama 模型预训练时的上下文窗口长度为 2048。左上角展示了 LLM 模型的常规用法：输入位置索引（蓝色圆点）位于预训练范围内。右上角展示了长度外推的情况，此时模型需要处理未见过的位置（红色圆点），直至 4096。左下角展示了位置插值方法，我们将位置索引本身（蓝色和绿色圆点）从 [0, 4096] 线性压缩至 [0, 2048]，强制使其落在预训练范围内。

根据实证研究，我们发现位置插值法非常高效且有效，仅需极短时间的微调即可使模型完全适应大幅扩展的上下文窗口。我们展示了使用位置插值法将 7B 至 65B 参数的 LLaMA 模型上下文窗口从初始的 2048 扩展到最高 32768 的实验结果。研究结果表明：

1. 位置插值法能够轻松实现超长上下文窗口（例如 32768），仅需在 Pile 数据集上进行 1000 步微调即可获得良好效果。与预训练成本相比，微调代价几乎可以忽略不计。这验证了我们的假设：模型相对容易适应插值式位置编码。
2. 位置插值法生成的强大模型能有效利用大幅扩展的上下文窗口。我们证明，通过位置插值扩展的模型在文本建模中，因上下文窗口极大扩展而获得显著的困惑度提升，且困惑度随上下文窗口扩大而平稳下降。我们还将位置插值应用于长文本摘要任务，并展现出具有竞争力的性能。
3. 位置插值法在原始上下文窗口大小的任务中能较好保持模型质量。我们针对扩展后的 LLaMA 模型在原始 LLaMA 基准测试中呈现了多样化的评估结果。与原始 LLaMA 模型相比，扩展后的 LLaMA 模型在 2048 个 token 限制内的若干标准基准测试中仅出现轻微性能下降。

我们的研究结果突显了 Transformer 模型“能够外推至训练时未见过的更长序列长度”的先天能力。我们重申了这一假设，并指出先前已知的语言模型在长序列外推方面的弱点可能是由于直接外推位置编码所致，而通过改为插值位置编码可以在很大程度上缓解这一问题。

**并行工作**。就在我们发布前夕，我们获悉了一篇并行的博客文章（Super-HOT kaiokendev (2023)），该文章同样通过在 RoPE 中插值位置编码，将上下文窗口从 2K 扩展到 8K。最近，开源社区在 Reddit 帖子和 Github Issues 中对此进行了讨论，表明使用 LoRA 进行微调似乎也能取得良好效果。我们的论文展示了在高达 65B 参数的模型上，通过位置插值进行完整微调的效果良好，并且我们还从理论上解释了为什么插值比外推能获得更稳定的结果——通过证明插值注意力分数的上界远低于外推的上界。

--------

## 2、方法

### 2.1 背景：旋转位置嵌入（ROPE）

Transformer 模型需要显式地注入位置信息（通常以位置编码的形式）来表示输入序列的顺序。我们采用 RoPE——该编码方案被应用于 LLaMA 模型。给定位置索引 $m \in [0, c)$ 和嵌入向量 $\mathbf{x} := [x_0, x_1, \ldots, x_{d-1}]^\top$（其中 $d$ 表示注意力头的维度），RoPE 定义了如下向量值复变函数 $\mathbf{f}(\mathbf{x}, m)$：
$$
\begin{equation}
    \mathbf{f}(\mathbf{x},m) = [(x_0 + \mathrm{i} x_1) e^{\mathrm{i} m \theta_0}, (x_2 + \mathrm{i} x_3) e^{\mathrm{i} m \theta_1}, \ldots, (x_{d-2} + \mathrm{i} x_{d-1})e^{\mathrm{i} m \theta_{d/2-1}}]^\top
\end{equation} \tag{1}$$
其中 $\mathrm{i}:= \sqrt{-1}$ 为虚数单位，$\theta_j = 10000^{-2j/d}$。采用 RoPE 后，自注意力得分

$$\begin{eqnarray}
a(m,n) &=& \mathrm{Re}\langle \mathbf{f}(\mathbf{q}, m), \mathbf{f}(\mathbf{k}, n)\rangle \nonumber \\
&=& \mathrm{Re}\left[\sum_{j=0}^{d/2-1} (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1}) e^{\mathrm{i} (m-n)\theta_j}\right] \nonumber \\
&=& \sum_{j=0}^{d/2-1} (q_{2j} k_{2j} + q_{2j+1}k_{2j+1})\cos((m-n)\theta_j) + (q_{2j} k_{2j+1} - q_{2j+1}k_{2j})\sin((m-n)\theta_j) \nonumber \\
&=:& a(m-n)
\end{eqnarray} $$

仅通过三角函数依赖于相对位置 $m−n$。这里的 $\mathbf{q}$ 和 $\mathbf{k}$ 是特定注意力头的查询向量和键向量。在每一层中，RoPE 都被应用于查询和键嵌入以计算注意力分数。


### 2.2 直接外推

虽然 RoPE 中的注意力分数仅取决于相对位置（这正是我们想要的），但其外推性能并不理想。特别是当直接扩展到训练中未见过的更大上下文窗口时，困惑度可能会飙升到非常高的数值（即 $>10^3$），与未经训练的模型相当。

理想情况下，我们希望看到在 $L=2048$ 大小的上下文窗口上训练的模型在更长的上下文窗口上仍然能合理工作，但可能无法利用超过 $L$ 范围之外的信息。例如，要回答位于 3000 位置的问题，在最大窗口大小为 $L=2048$ 训练的模型无法利用位置 0 提供的证据，但仍然可以利用位置 2900 提供的证据。然而现实中我们观察到灾难性的行为，即位于 3000 位置的问题无法被正确回答，即使证据位于 2900 位置。

原因何在？根据 RoPE 论文第 3.4.3 节的描述，注意力分数 $a_{m-n}$ 应随着相对距离 $|m-n|$ 的增加而衰减，那么来自很远距离的内容应该影响不大才对。然而，事实证明 RoPE 第 3.4.3 节推导出的上界可能过于宽松：虽然它确实会随着 $|m-n|$ 的增加而衰减，但这个界限仍然可能相当大（即界限可能严重依赖于 $v_j$ 的大小），因此显得空洞。实际上，如果我们把所有三角函数都视为基函数（即 $\phi_j(s):=e^{\mathrm{i} s \theta_j}$），并将方程 2 视为如下的基展开：

$$\begin{equation}
    a(s) = \mathrm{Re}\left[\sum_{j=0}^{d/2-1} h_j e^{\mathrm{i} s \theta_j}\right] \tag{3}
\end{equation}$$

查询与键之间的 $h_j := (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1})$ 是依赖于 $\mathbf{q}$ 和 $\mathbf{k}$ 的复系数（此处 $h_j$ 的定义与 RoPE 第 3.4.3 节中的定义完全相同）。现在问题变得清晰：如图 2 所示，在 [0,2048] 区间内其幅值可能很小，但在该区间外会产生巨大数值。根本原因在于三角函数族 $\{\phi_j\}$（当 $d$ 足够大时）是通用逼近器，可以拟合任意函数。因此对于任意函数，总存在对应的系数 $\{h_j\}$（即键与查询），使得其在 [0,2048] 区间内函数值较小，而在区间外显著增大。


[图 2]

**图 2：** 外推与内插的对比。​**左图：​**​ 一条拟合的注意力分数函数（红色曲线），形式为公式（3），其中$d= d_\mathrm{model} / n_\mathrm{head} = 4096 / 32 = 128$（LLaMA 7B 的设置）。圆点是待拟合的随机输入点，红色曲线是通过最小二乘法拟合的分数函数，大致在 [−1,1] 范围内。​**中图：​**​ 虽然拟合函数在 \[0,$L$\]（$L=2048$）内看起来有良好的边界，但超出此区域后，函数值可能超过 8000，导致注意力计算中出现灾难性问题。请注意，这里我们完全没有刻意挑选：几乎所有从 \[0,$L$\] 内随机生成的输入点，集合中学习到的曲线都存在外推问题。​**右图：​**​ 另一方面，内插则稳定得多。在垂直虚线之间（即整数位置差）的曲线平滑且表现良好。生成该图的源代码请参见附录 C.1。


### 2.3 建议方法：位置插值（PI）

在图 2 中，由于基函数 $\phi_j$ 的光滑性，插值方法更加稳定，不会产生异常值。因此，我们建议不再将式 3 中的注意力分数外推到 $s > L$，而是定义一个新的注意力分数 $\tilde a(s) = a(Ls/L')$，其中 $L'$ 表示更长的上下文窗口。正式来说，我们用以下方式重新定义 RoPE 向量函数 $\mathbf{f'}$ 来替代原来的 $\mathbf{f}$：

$$\begin{equation}
    \mathbf{f'}(\mathbf{x}, m)= \mathbf{f}\left(\mathbf{x}, \frac{mL}{L'} \right). \tag{4}
\end{equation}$$

我们将这种位置编码的变换称为 **位置插值**。在这一步骤中，我们将位置索引从 \[0,$L′$) 缩减至 \[0, $L$)，以匹配计算 RoPE 前的原始索引范围。因此，作为 RoPE 的输入，任意两个标记之间的最大相对距离已从 $L′$ 降至 $L$。由于我们在扩展前后对齐了位置索引和相对距离的范围，从而减轻了因上下文窗口扩展对注意力分数计算的影响，这使得模型更容易适应。为了进一步证明这一点，我们在以下定理中展示了插值后的注意力分数表现良好：

**定理 2.1（插值边界）**  对于注意力分数 $a(s) = \mathrm{Re}\left[\sum_{j=0}^{d/2-1} h_j e^{\mathrm{i} s \theta_j}\right]$，其中 $\theta_j = c^{-2j/d}$，其在区间 $s \in [s_1, s_2]$ 内的插值 $a(s)$ 有如下界限：

$$\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| \le d\left(\max_j |h_j|\right) \frac{(s-s_1)(s_2-s)}{8\ln c} \tag{5}
\end{equation}$$
其中 $a_{\mathrm{linear}}(s)$ 是两个网格点 $a(s_1)$ 和 $a(s_2)$ 的线性插值，这两个点已知表现良好，这是通过 LLM 预训练强化的：

$$\begin{equation}
    a_{\mathrm{linear}}(s) := (1-\lambda(s)) a(s_1) + \lambda(s) a(s_2), \quad\quad \lambda(s) := \frac{s-s_1}{s_2-s_{1}  }  \tag{6}
\end{equation}$$

请查看附录 A 以获取证明。直观上，在 LLM 预训练中，我们知道注意力分数 $a(s)$ 在整数网格 $s_1$ 和 $s_2$ 上表现良好。因此，对于任何插值 $s\in [s_1,s_2]$，我们有 $(s-s_1)(s_2-s) \le 1/4$。注意 $c=10000$，边界变为：
$$\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| \le \frac{d}{32 \ln c}\max_j |h_j| \approx \frac{d \max_j |h_j|}{294.73}    \tag{7}
\end{equation}$$

相比之下，RoPE 中的第 3.4.3 节给出了一个外推边界（即它适用于所有位置距离 $s$）：
$$\begin{equation}
 |a(s)| \le \left(\max_{j} |h_j - h_{j+1}|\right) \sum_{k=0}^{d/2-1} |A_{k+1}(s)| \le 2\left(\max_{j} |h_j|\right) \sum_{k=0}^{d/2-1} |A_{k+1}(s)|  \tag{8}
\end{equation}$$
其中 $A_k(s) := \sum_{j=0}^{k-1} e^{\mathrm{i} s \theta_j}$。虽然 $B(s) := \sum_{k=0}^{d/2-1} |A_{k+1}(s)|$ 没有闭合形式，但数值上至少大于 $d$，并且对于许多位置差异 $s$，$B(s)$ 远大于 $d$（具体图示参见附录 B）。因此，插值边界至少比外推边界小 $2 \cdot 294.73\sim 600\times$ 倍，这意味着插值注意力分数比外推分数稳定得多。

值得注意的是，我们重新调整位置索引的方法既不会引入额外的权重，也不会以任何方式修改模型架构。这使得该方法在实际应用中极具吸引力，因为扩展后原始模型的大部分基础设施和优化方案都能直接复用。

**微调**  我们可以使用下一个词预测任务，在扩展的上下文窗口大小上使用插值位置编码，进一步微调插值模型，例如使用 Pile 这样的预训练语料库。在下一节中，我们将展示我们的微调过程仅需要数万到数十万个示例。我们还发现，微调的结果对示例的选择并不敏感。原因可能是模型在微调阶段只是适应新的上下文窗口，从一个良好的初始化开始，而不是获取新知识。

**其他降低插值/外推误差边界的方法**  从插值误差边界（公式 5）和外推误差边界（公式 8）的表达式可以看出，它们共有的一个项是 $\max_j |h_j|$，即查询/键乘积的最大幅度。如果我们在大型语言模型训练过程中对 $|h_j|$ 施加正则化约束，就有可能减轻甚至解决灾难性外推误差。实际上，如果我们采用带适当正则化项的岭回归来拟合图 2 中的曲线，当 $s > L$ 时外推得到的 $a(s)$ 幅值可以与 \[0, $L$] 区间内的幅值相当。据我们所知，目前还没有现有的大型语言模型预训练技术利用这种正则化方法，这将留待未来研究。

## 3、实验

我们证明位置插值法能有效将上下文窗口扩展至原始大小的 32 倍，且仅需数百次训练步骤即可完成。研究表明，所得模型是具备完全有效长上下文窗口的强大大语言模型。我们通过语言建模、密钥检索和长文档摘要等多项任务验证其性能，同时展示了扩展模型在原始 LLaMA 评估基准上的测试结果。

### 3.1 设置

**模型变体**   我们基于预训练的 7B、13B、33B 和 65B LLaMA 模型，通过直接微调或位置插值方法将其上下文窗口扩展至最高 32768。除对采用位置插值扩展的模型进行位置索引重缩放外，我们未对 LLaMA 模型架构进行任何其他修改。

**训练流程**   我们采用下一词元预测目标对所有模型变体进行微调。优化器选用 AdamW，参数设为 $\beta_1=0.9$ 和 $\beta_2=0.95$。学习率采用线性预热策略，从最大学习率的 $10\%$ 开始，经过 20 步达到峰值。7B 和 13B 模型的学习率设为 $2\times 10^{-5}$，33B 和 65B 模型设为 $10^{-5}$。权重衰减率设为零。在将 7B、13B 和 33B 模型的上下文窗口扩展至 8192 时，我们使用 32 块 A100 显卡和 64 的全局批次大小；其他情况均使用 128 块 A100 显卡和 128 的全局批次大小。需要说明的是，增加显卡数量主要是为了突破微调时的显存限制，特定场景下可减少显卡用量。所有模型训练均基于 PyTorch 框架，采用全分片数据并行技术和 Flash 注意力机制实现。

如无特别说明，对于位置插值方法，我们对模型进行了 1000 步的微调。对于直接微调方法，我们使用了 10000 步。我们主要使用 Pile 训练数据集进行微调。在章节 3.4 中，我们还比较了 RedPajama 数据集上的微调性能。

### 3.2 长序列语言模型

我们评估了扩展模型和基线模型在两个数据集上的长序列语言建模性能：书籍语料库（PG-19）和经过清洗的 Arxiv Math proof-pile 数据集。

我们使用了 PG19 数据集和 proof-pile 数据集的测试集。对于 PG19，我们使用了包含 100 份文档的完整测试集。对于 proof-pile 数据集，我们随机抽取了 128 份文档作为子样本，每份文档至少包含 32768 个 SentencePiece 分词单元，并将每份测试文档截取至前 32768 个分词单元。我们采用滑动窗口方法以步长 $S=256$ 来评估不同上下文窗口大小下的困惑度。

在表 1 和表 2 中，我们报告了模型及基线在各数据集上的困惑度结果。通过分析发现，采用我们方法扩展的模型随着上下文窗口长度的增加，困惑度得到了显著改善。当上下文窗口从 2048 扩展到 16384 时，LLaMA 7B 模型在两个数据集上的困惑度分别降低了 0.28 和 0.5，LLaMA 13B 模型降低了 0.27 和 0.48，LLaMA 33B 模型降低了 0.14 和 0.42。对于 LLaMA 65B 模型，当扩展至 8192 上下文窗口时，其困惑度降低了 0.12 和 0.3。

总体而言，我们观察到模型在更长上下文窗口下的困惑度表现呈现持续提升趋势。这表明我们的模型能有效利用长上下文窗口，在语言建模任务中更准确地预测下一个词元。值得注意的是，在 PG19 数据集上，LLaMA 7B 和 13B 模型在 32768 长度的上下文窗口中仍保持这种提升趋势，这意味着我们的方法可能支持扩展到更长的上下文窗口。

相比之下，我们发现通过直接微调方法扩展的模型在更长上下文窗口上的困惑度出现了退化（最高达 +0.48）或仅有微小改进（最高达 -0.12）。这表明以此方式扩展的模型利用超出预训练设定长度的上下文窗口的能力有限。

在某些情况下，我们发现扩展模型在原始 2048 上下文窗口上的困惑度略有下降。例如，在 Proof-pile 数据集上，所有采用位置插值扩展的模型都出现了 0.01 到 0.05 范围内的性能下降。由于位置插值强制将原始上下文窗口中的位置编码压缩到更窄的区域内，这可能会对语言模型的性能产生负面影响，因此在原始评估上下文窗口中出现小幅性能下降是预期之中的。关于原始上下文窗口尺寸的更多基准测试结果，我们将在第 3.4 节中详细展示。

在表 3 中，我们报告了 LLaMA 7B 模型在 PG19 数据集上使用位置插值法将上下文窗口扩展至 8192 和 16384 时，困惑度与微调步数之间的关系。可以看到未经微调（第 0 步）时，模型已展现出一定的语言建模能力——扩展到 8192 上下文窗口时困惑度 $<20$（相比之下直接外推法会导致 $>10^3$ 的困惑度）。通过微调，我们发现困惑度快速改善：200 步时模型在 2048 窗口尺寸上的表现已超越原始模型，表明其获得了有效利用长于预训练设置的序列进行语言建模的能力；到 1000 步时，模型稳步提升并实现了显著更优的困惑度指标。

[表 1、2、3]

**表 1：** PG19数据集上的评估困惑度。FT：直接微调。PI：位置插值。使用 PI 微调的模型在上下文窗口更长时显示出逐渐降低的困惑度，表明 PI 能很好地利用长上下文，而 FT 的困惑度在窗口更长时会增加。请注意，由于 PG19 的写作风格非常不同，整体困惑度比表 2 要高。

**表 2：** 在 Arxiv 数学 Proof-pile 数据集上评估困惑度。FT：直接微调。PI：位置插值。

**表 3：** 在 PG19 数据集上使用位置插值法评估困惑度随微调步数的变化关系


### 3.3 通过密钥检索测量有效上下文窗口大小

我们研究了模型在扩展后的有效上下文窗口大小，即在推理过程中一个标记能够有效关注的最大距离。为了测量这一点，我们采用了 Mohtashami & Jaggi (2023) 提出的密码检索合成评估任务。在该任务中，模型需要从长文档中恢复隐藏的随机密码。文档格式参见图 3。

给定一个语言模型，我们按如下方式估算有效上下文窗口的上下界。假设随机密钥位于输入末尾的 $k$ 个标记处。当模型在多次独立尝试中持续无法检索到正确的密钥值时，表明该模型的有效上下文窗口大小小于 $k$。反之，若模型能持续成功检索到正确的密钥值，则可推断该模型的有效上下文窗口大小至少为 $k$。

我们评估了通过位置插值或直接微调扩展的 7B 和 33B LLaMA 模型变体。对于每个模型，我们在目标上下文窗口 $L'$ 内均匀选取 32 个不同的 $k$ 值，并对每个 $k$ 值运行上述测试 10 次，每次使用一个由 5 位随机数字组成的随机密码。在表 4 中，我们将 $k_{\max}$ 报告为微调步数的函数，其中 $k_{\max}$ 定义为满足以下条件的最大 $k$ 值：对于所有 $k' \le k$，模型在 $k'$上的成功率至少为 20%。

我们可以看到，通过位置插值扩展的模型都成功地实现了它们在有效上下文窗口大小方面的预期扩展目标，这表现为有效上下文窗口大小在仅经过 200 步微调后即达到最大值 $k_{\max}=L'$，这一现象在 7B 和 33B 模型规模以及高达 32768 的上下文窗口中都保持一致。相比之下，通过直接微调扩展的 LLaMA 模型即使经过超过 10000 步的微调，其有效上下文窗口大小 $k_{\max}$ 仅从 2048 略微增加到 2560，且没有明显迹象表明窗口大小的增长会加速。

[表 4]

**表 4：** 微调后的有效上下文窗口大小。FT：直接微调。PI：位置插值。

[图 3]

**图 3：** 用于检索密码的提示格式。我们采用了与 Mohtashami & Jaggi（2023）提出的完全相同的提示。在测试过程中，这里的密码 12345 会被替换为一个随机的 5 位数。

### 3.4 原始上下文窗口大小的基准测试

我们在原始上下文窗口大小为 2048 的标准基准任务上评估了通过位置插值扩展的模型。其他任务如闭卷问答、数学推理和代码生成通常需要的上下文窗口不超过 2048，因此留待未来评估。评估结果列于表 5 中。从结果来看，扩展到 8192 的模型在原本为更小上下文窗口设计的基准测试中产生了可比较的结果，对于 7B 和 33B 模型规模，基准任务的性能下降最多为 2%。扩展到更长上下文窗口的模型在基准测试上表现有所下降，但对于大多数任务仍在合理范围内。我们还注意到，微调数据集的选择似乎并未导致基准性能的显著差异，这可能是由于我们方法中使用的微调步骤数量有限。基准任务的性能下降与我们在第 3.2 节中对困惑度下降的观察一致。

[表 5、6]

**表 5：** LLaMA 基准测试子集上的零样本性能。通过位置插值扩展的模型表现与原始模型相当，但 BoolQ 数据集除外，该数据集可能需要模型密切注意短参考段落中的词序。

**表 6：** GovReport 数据集上的 ROUGE 评分。


### 3.5 长文档摘要

在本任务中，我们评估了模型在长文档摘要任务上的表现。具体而言，我们采用了 GovReport 数据集，该数据集包含 17,457 篇训练文档和 972 篇评估文档。每篇文档均附有人工撰写的摘要。我们将所有输入文档截断至前 15,000 个词元。

我们对 LLaMA 模型进行了微调，通过位置插值技术将上下文窗口扩展至 16384。需要注意的是，在此微调阶段仍需对位置索引进行重新缩放。首先，我们使用图 4 中的提示模板格式化原始文档，然后将提示信息与每个文档对应的真实摘要（截断至 1000 个标记）进行拼接。基于上述设置，我们采用下一个标记预测任务对模型进行了 10 个 epoch 的微调。在微调过程中，我们排除了训练样本中输入提示部分产生的损失。

我们采用生成温度为 0.5 和 $\text{top}_p = 0.95$ 作为推理参数，为测试集中的每份文档生成摘要。最终输出截断为 1000 个词元。我们使用 ROUGE-1/ROUGE-2/ROUGE-L 评分作为评估指标，将模型输出与真实摘要进行对比评估。

在表 6 中我们报告了评估结果，同时纳入了现有 SCROLLS 排行榜中两个基线的结果。总体而言，在仅进行少量超参数调整的情况下，我们的模型取得了具有竞争力的 R1 分数。这一结果表明，我们采用 16384 上下文窗口的模型能够有效处理长文档摘要任务。

[图 4]

**图 4：** 长文档摘要的输入格式。


## 4、相关工作

**检索增强型 LLM：** 一类研究通过为大语言模型添加检索模块进行扩展，这些模块能获取相关文档并将检索结果纳入大语言模型的输入上下文环境。我们的工作与这些研究具有互补性，因为扩展的上下文窗口允许在输入中包含更多文档。此外，由于采用未经修改的注意力机制和模型架构，我们的方法可能更具通用性，能够原生处理检索导向任务之外的其他任务，例如长文档摘要、小样本学习等。

**循环 Transformer 与记忆 Transformer：** 多项研究通过循环机制为 Transformer 模型增添了记忆功能，从而提升了模型处理超长序列的能力。这些工作的一个局限在于，它们只能关注到过去输入的有损压缩版本。mu2023 指出，这可能导致模型无法记住过去输入中的特定细节。相比之下，我们的方法允许关注所有历史标记，在无需压缩的情况下保留全部细节，尽管推理成本较高。mohtashami2023 提出了地标注意力机制，通过引入地标标记实现对输入任意区块的完全随机访问。而我们的工作通过未经修改的注意力机制实现对完整输入的全局访问，这对摘要生成等任务可能更具实用价值。

**近似多头注意力机制：** 大量研究致力于通过近似或稀疏化方法降低多头注意力机制（MHA）的内存和计算复杂度。虽然 LLaMA 模型未采用这些方法（这并非本文研究重点），但我们注意到：由于本方法仅针对位置编码进行改进而不涉及注意力机制本身，因此与大多数近似方案具有兼容性。

**长度外推性：** 近期一系列研究致力于让 Transformer 模型在短序列上训练，而在更长的序列上进行推理。然而这些方法尚未应用于 LLaMA 或 OPT 等某些最大规模的语言模型，导致无法实现众多已有预训练语言模型的长度外推能力。我们的工作聚焦于扩展现有大语言模型，这能节省大量预训练成本。此外，由于我们的方法没有大幅偏离现有位置编码或注意力机制的定义，因此即使在小型上下文窗口任务中，也能保持原始模型的质量。

**插值法：** 与我们的方法最相关的是 dosovitskiy2021 在视觉 Transformer 研究中提出的线性插值学习位置嵌入技术。该方法通过在微调阶段对学习到的位置嵌入进行线性插值，以支持更高分辨率（即增加输入嵌入数量）。插值后的位置嵌入权重被用作新增位置在微调过程中的初始化参数。我们的工作与其存在以下差异：(1) 我们插值的是位置索引而非位置嵌入，这种方法更适合 RoPE 类位置编码，且由于不添加可训练参数可能减少训练量；(2) 我们成功将上下文窗口扩展至32倍，而 dosovitskiy2021 仅探索了 4 倍扩展，这推进了通过插值探索上下文窗口扩展上限的研究；(3) 我们验证了位置插值法在扩展语言模型上下文窗口方面的有效性。

我们认为，结合 dosovitskiy2021 的研究成果，我们的结果为 Transformer 模型处理远超训练序列长度的卓越能力提供了实证依据。此外，我们推测类似他们的方法可直接应用于具有可学习位置嵌入的大型语言模型（如 OPT），并计划在未来对此展开深入研究。

## 5、总结

位置插值法能够通过极少量微调，有效扩展 LLaMA 模型的上下文窗口至远超原有长度。经扩展的模型不仅能出色完成各种长上下文任务，还能相对完好地保留其在原始上下文范围内的能力，使其成为处理长短输入皆宜的通用语言模型优选方案。此外，采用位置插值法扩展的模型可兼容绝大多数现有基础设施与优化方案，这使该方法在实际应用中极具吸引力。我们认为位置插值是一种通用技术，未来可应用于其他类型的位置编码方式，从而拓展更多大语言模型的适用场景，近期我们计划就此展开深入研究。


## 6、附录 A：证明



\appendix 
\section{Proof}
\label{sec:proof}
\interp*
\begin{proof}
Using Taylor expansion, we have:
\begin{eqnarray}
    a(s_1) &=& a(s) + a'(s)(s-s_1) + \frac12 a''(\xi_1)(s-s_1)^2 \label{eq:xi1} \\
    a(s_2) &=& a(s) + a'(s)(s-s_2) + \frac12 a''(\xi_2)(s-s_2)^2 \label{eq:xi2}
\end{eqnarray}
where $\xi_1\in [s_1,s]$ and $\xi_2\in [s,s_2]$. Multiplying Eqn.~\ref{eq:xi1} with $s-s_2$ and Eqn.~\ref{eq:xi2} with $s-s_1$ and subtract, we get:
\begin{equation}
    a(s) - a_{\mathrm{linear}}(s) = R(s) := -\frac{(s-s_1)(s-s_2)}{2(s_1-s_2)}\left[a''(\xi_1)(s-s_1) - a''(\xi_2)(s-s_2)\right] 
\end{equation}
Now we bound the second order derivative $a''(s)$. Note that for any complex number $x$, $|\mathrm{Re}(x)| \le |x|$ so we have:
\begin{eqnarray}
    |a''(s)| &\le& \sum_{j=0}^{d/2-1} |h_j| |\phi''_j(s)| \le \sum_{j=0}^{d/2-1} |h_j| \theta^2_j \\
    &\le& \left(\max_j |h_j|\right) \sum_{j=0}^{d/2-1} c^{-4j/d} = \left(\max_j |h_j|\right) \frac{1}{1 - c^{-4/d}}
\end{eqnarray}
Note that when $x < 0$ and $c > 1$, $c^x \le 1 + x\ln c$, therefore $c^{-4/d} \le 1 - 4/d\ln c$ and we have:
\begin{equation}
    \frac{1}{1-c^{-4/d}} \le \frac{1}{4/d \ln c} = \frac{d}{4\ln c}
\end{equation}
So 
\begin{equation}
    |a''(s)| \le \left(\max_j |h_j|\right)\frac{d}{4\ln c} =: M
\end{equation}
Let the above bound to be $M$, we have:
\begin{equation}
    |R(s)| \le \frac{(s-s_1)(s_2-s)}{2(s_2-s_1)}\left[M(s-s_1) + M(s_2-s)\right] = \frac{M}{2}(s-s_1)(s_2-s)
\end{equation}
As a result:
\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| = |R(s)| \le d\left(\max_j |h_j|\right) \frac{(s-s_1)(s_2-s)}{8\ln c}
\end{equation}
\end{proof}

\section{Visualization of quantities in extrapolation bound}
\label{sec:visualization}
As shown in Eqn.~\ref{eq:extra-bound}, the extrapolation bound contains the term  $B(s) := \sum_{k=0}^{d/2-1} |A_{k+1}(s)|$ where $A_k(s) := \sum_{j=0}^{k-1} e^{\di s\theta_j}$. Here we check how large the bound is. We use $\theta_j = c^{-2j/d}$ with $c = 10000$ and $d = 4096 / 32 = 128$ (LLaMA-7B setting), and Fig.~\ref{fig:b_bound}  shows that $B(s)/d$ almost always larger than $1$ and in many places it is much larger than $1$. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{B_decay-crop.pdf}
    \caption{\small The bound $B(s) / d$ decays with $s$. While the bounds goes down with large positional difference $s$, numerically $B(s)/d \ge 1$ and at many $s$ much larger than $1$ (the dotted horizontal line). Please check Appendix~\ref{sec:b-bound} for the source code used to draw the figure.}
    \label{fig:b_bound}
\end{figure}

\section{Code}
\subsection{Code for Fig.~\ref{fig:interp-vs-extrap}}
\label{sec:code-inter-extra}
{\small
\begin{verbatim}
# build basis function
d = 4096 // 32
theta = 10000
# Frequency computation, 
freqs = 1.0 / (theta ** (torch.arange(0, d, 2)[: (d // 2)].float() / d))

# construct basis function 
L = 2048

x = torch.zeros(L)
x[:L] = torch.arange(0, L)

# basis functions
xfreq = torch.outer(x, freqs)

y = torch.randn(x.shape[0])

# do linear regression
X = torch.cat([xfreq.sin(), xfreq.cos()], dim=1)

eps = 0.000
coeffs = torch.linalg.solve(X.t() @ X + torch.eye(X.shape[1]) * eps, X.t() @ y)

x2 = torch.arange(0, 2*L)
xfreq2 = torch.outer(x2, freqs)
X2 = torch.cat([xfreq2.sin(), xfreq2.cos()], dim=1)

y2 = X2 @ coeffs

x3 = torch.arange(25, 75, 0.125)
xfreq3 = torch.outer(x3, freqs)
X3 = torch.cat([xfreq3.sin(), xfreq3.cos()], dim=1)

y3 = X3 @ coeffs

plt.figure(figsize=(16,5))

plt.subplot(1, 3, 1)
plt.plot(x2[:L], y2[:L], "r")
plt.scatter(x, y)
plt.ylabel("attention score $a(s)$")
plt.xlabel("Positional difference $s$")

plt.subplot(1, 3, 2)

plt.plot(x2, y2, "r")
plt.scatter(x, y)
plt.axvline(L, color="k", linestyle="--", linewidth=0.5)

plt.title("Effect of Extrapolation")
plt.xlabel("Positional difference $s$")


plt.subplot(1, 3, 3)
plt.plot(x3, y3, "r")
for i in range(25,75):
    plt.axvline(i, color="k", linestyle="--", linewidth=0.5)
plt.title("Effect of Interpolation")
plt.xlabel("Positional difference $s$")
plt.show()
\end{verbatim}
}

\subsection{Code for Fig.~\ref{fig:b_bound}}
\label{sec:b-bound}
{\small
\begin{verbatim}
L = 2048
x = torch.arange(0, 2*L)
d = 4096 // 32
theta = 10000
freqs = 1.0 / (theta ** (torch.arange(0, d, 2)[: (d // 2)].float() / d))

xfreq = torch.outer(x, freqs)

mags = (xfreq.sin().cumsum(dim=1).pow(2) + xfreq.cos().cumsum(dim=1).pow(2)).sqrt()

plt.plot(mags.sum(dim=1)/d)
plt.axhline(1.0, color='k', linestyle="--")
plt.xlabel("Positional difference $s$")
plt.ylabel("$B(s)/d$")
plt.show()
\end{verbatim}
}

% \include{appendix_7_news_summary}

\end{document}

