
* https://arxiv.org/pdf/2306.15595
* meta （2023）

**摘要**    我们提出位置插值法（Position Interpolation，PI），该方法通过极少量微调（1000 步以内）即可将基于 RoPE 的预训练大语言模型（如 LLaMA 系列模型）的上下文窗口扩展至 32768，同时在需要长上下文的任务中展现出强劲性能表现——从 7B 到 65B 参数的 LLaMA 模型在密钥检索、语言建模及长文档摘要等任务中均取得优异结果。经位置插值扩展的模型在其原始上下文窗口范围内的任务上仍能保持较好质量。该方法通过线性压缩输入位置索引使其适配原始上下文窗口尺寸，而非外推超出训练长度的位置索引（后者可能导致灾难性高注意力分数，彻底破坏自注意力机制）。理论研究表明，插值法的误差上界至少比外推法小 600 倍，进一步验证了其稳定性。通过位置插值扩展的模型保持原有架构，可复用绝大多数既有优化方案与基础设施。

```python
import torch  
  
  
# --- 1. RoPE Implementation ---  
  
"""  
在苏剑林的 [blog](https://spaces.ac.cn/archives/8265) 中的公式 （13）  
关于 sin 的系数，排列顺序是 (-1, 0, -3, 2, -5, 4)而这里的系数排列是 (-3, -4, -5, 0, 1, 2)实际上是一样的，关键在于 `_set_cos_sin_cache()` 函数中 emb 实际上拼接了两个 freqs这保证了 -3 和 0 实际上看到的 sin 值是一样的  
"""  
  
  
def rotate_half(x):  
    """Rotates half the hidden dims of the input."""  
    x1 = x[..., : x.shape[-1] // 2]  
    x2 = x[..., x.shape[-1] // 2:]  
    return torch.cat((-x2, x1), dim=-1)  
  
  
def apply_rotary_pos_emb(tensor, cos, sin):  
    """Applies rotary position embedding to the input tensor."""  
    # The 'tensor' is q or k, with shape [batch, seq_len, num_heads, head_dim]  
    # cos/sin are [seq_len, head_dim]    # We unsqueeze cos/sin to make them broadcastable with the tensor    cos = cos.unsqueeze(0).unsqueeze(2)  # -> [1, seq_len, 1, head_dim]  
    sin = sin.unsqueeze(0).unsqueeze(2)  # -> [1, seq_len, 1, head_dim]  
    return (tensor * cos) + (rotate_half(tensor) * sin)  
  
  
class RotaryEmbedding(torch.nn.Module):  
    """  
    The Rotary Position Embedding (RoPE) module.    This module now correctly applies the embedding in its forward pass.    """  
    def __init__(self, dim, max_position_embeddings=2048, base=10000.0, scaling_factor=1.0):  
        super().__init__()  
        self.dim = dim  
        self.max_position_embeddings = max_position_embeddings  
        self.base = base  
        self.scaling_factor = scaling_factor  
  
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))   # 计算逆频 (64, )        self.register_buffer("inv_freq", inv_freq)  
  
        self._set_cos_sin_cache(max_position_embeddings)  
  
    def _set_cos_sin_cache(self, seq_len):  
        self.max_seq_len_cached = seq_len  
        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)  
        t = t / self.scaling_factor  
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)      # 计算外积 (2048/4096, 64)        emb = torch.cat((freqs, freqs), dim=-1)               # 堆叠相同的频率，是为了保证后续的旋转向量计算  
        self.register_buffer("cos_cached", emb.cos(), persistent=False)  
        self.register_buffer("sin_cached", emb.sin(), persistent=False)  
  
    def forward(self, q, k, seq_len):  
        """  
        This forward method now takes q and k, and applies the rotation.        """        if seq_len > self.max_seq_len_cached:  
            self._set_cos_sin_cache(seq_len)  
  
        cos = self.cos_cached[:seq_len, ...]               # (2048/4096, 128)  
        sin = self.sin_cached[:seq_len, ...]               # (2048/4096, 128)  
  
        # <<< The crucial step: applying the rotation >>>        q_embed = apply_rotary_pos_emb(q, cos, sin)  
        k_embed = apply_rotary_pos_emb(k, cos, sin)  
        return q_embed, k_embed  
  
  
# --- 2. Setup for Demonstration ---  
  
old_max_len = 2048  
new_max_len = 4096  
d_model_head = 128  
num_heads = 4  # Example number of attention heads  
batch_size = 1  # Example batch size  
  
num_scaling_factor = new_max_len / old_max_len  
print(f"从 {old_max_len} 扩展到 {new_max_len}，线性缩放因子为: {num_scaling_factor}\n")  
  
# 1. Original RoPE  
original_rope = RotaryEmbedding(  
    dim=d_model_head,  
    max_position_embeddings=old_max_len,  
    scaling_factor=1.0  
)  
  
# 2. Scaled RoPE  
scaled_rope = RotaryEmbedding(  
    dim=d_model_head,  
    max_position_embeddings=new_max_len,  
    scaling_factor=num_scaling_factor  
)  
  
# --- 3. 最终的、更严格的验证方法 ---# 我们可以测试多个位置  
test_positions = [0, 3, 100, 1024, 2000]  
shift = 0    # 可以调整，查看非对应位置上  
  
for pos_original in test_positions:  
    # 计算出在扩展模型中对应的位置  
    pos_scaled = int(pos_original * num_scaling_factor) + shift  
  
    print(f"--- 验证: 原始位置 {pos_original} vs 扩展位置 {pos_scaled} ---")  
  
    # 每次测试都用新的随机向量，以避免巧合  
    dummy_q_original = torch.randn(batch_size, old_max_len, num_heads, d_model_head)  
    dummy_k_original = torch.randn(batch_size, old_max_len, num_heads, d_model_head)  
    dummy_q_scaled = torch.randn(batch_size, new_max_len, num_heads, d_model_head)  
    dummy_k_scaled = torch.randn(batch_size, new_max_len, num_heads, d_model_head)  
  
    # <<< 确保我们正在比较的两个位置，其原始输入向量是相同的！>>>  
    dummy_q_scaled[:, pos_scaled, :, :] = dummy_q_original[:, pos_original, :, :]  
  
    # 应用 RoPE    rotated_q_original, _ = original_rope(dummy_q_original, dummy_k_original, seq_len=old_max_len)  
    rotated_q_scaled, _ = scaled_rope(dummy_q_scaled, dummy_k_scaled, seq_len=new_max_len)  
  
    # 提取对应位置的、经过旋转后的向量  
    rotated_q_vec_original = rotated_q_original[:, pos_original, :, :]  
    rotated_q_vec_scaled = rotated_q_scaled[:, pos_scaled, :, :]  
  
    # 计算余弦相似度  
    similarity = torch.nn.functional.cosine_similarity(  
        rotated_q_vec_original.flatten(),  
        rotated_q_vec_scaled.flatten(),  
        dim=0  
    )  
  
    print(f"  - 相似度为: {similarity.item():.6f}")  
    print("\n结论：所有测试位置的相似度均非常高，严格证明了线性插值的有效性。")
```

## 1、引言

LLMs 通常具有预定义的上下文窗口大小。例如，LLaMA 模型的输入必须少于 2048 个 tokens。我们能否扩展现有预训练 LLM 的上下文窗口？

一种直接的方法是*对现有的预训练 Transformer 进行微调*，使其具有更长的上下文窗口。然而，通过实验我们发现，以这种方式训练的模型对长上下文窗口的*适应速度非常缓慢*。在训练超过 10000 个批次后，有效的上下文窗口仅从 2048 增加到 2560（表 4），这表明这种方法在扩展到更长的上下文窗口时效率低下。

尽管某些技术能够实现 Transformer 的长度外推，即在较短的上下文窗口上进行训练，在较长的窗口上进行推理，但许多现有的预训练 LLM（包括 LLaMA）使用的*位置编码具有较弱的外推特性（例如RoPE）*。因此，这些技术在扩展此类 LLM 的上下文窗口大小方面的适用性仍然有限。

位置插值法核心思想是：*不同于外推法，我们直接对位置索引进行降尺度处理，使最大位置索引与预训练阶段原有的上下文窗口限制相匹配（如图 1 所示）*。换言之，为了容纳更多输入标记，我们利用位置编码可应用于非整数位置的特性，对相邻整数位置的位置编码进行插值——这相比在已训练位置范围外进行外推（可能导致灾难性数值）更具优势。我们从理论上验证了该方法的有效性：在 LLaMA 7B 模型设定中，插值后的注意力分数上界比外推法小约 600 倍，因而更加稳定。这使得模型更容易适应插值后的位置编码。

[图 1]

**图 1**：我们的位置插值方法示意图。假设一个 Llama 模型预训练时的上下文窗口长度为 2048。左上角展示了 LLM 模型的常规用法：输入位置索引（蓝色圆点）位于预训练范围内。右上角展示了长度外推的情况，此时模型需要处理未见过的位置（红色圆点），直至 4096。左下角展示了位置插值方法，我们将位置索引本身（蓝色和绿色圆点）从 \[0, 4096\] 线性压缩至 \[0, 2048\]，强制使其落在预训练范围内。

根据实证研究，我们发现位置插值法非常高效且有效，仅需极短时间的微调即可使模型完全适应大幅扩展的上下文窗口。我们展示了使用位置插值法将 7B 至 65B 参数的 LLaMA 模型上下文窗口从初始的 2048 扩展到最高 32768 的实验结果。研究结果表明：

1. 位置插值法能够轻松实现超长上下文窗口（例如 32768），仅需在 Pile 数据集上 *进行 1000 步微调* 即可获得良好效果。与预训练成本相比，微调代价几乎可以忽略不计。这验证了我们的假设：模型相对容易适应插值式位置编码。
2. 位置插值法生成的强大模型能有效利用大幅扩展的上下文窗口。我们证明，通过位置插值扩展的模型在文本建模中，因上下文窗口极大扩展而获得显著的困惑度提升，且困惑度随上下文窗口扩大而平稳下降。我们还将位置插值应用于长文本摘要任务，并展现出具有竞争力的性能。
3. 位置插值法在原始上下文窗口大小的任务中能较好保持模型质量。我们针对扩展后的 LLaMA 模型在原始 LLaMA 基准测试中呈现了多样化的评估结果。与原始 LLaMA 模型相比，扩展后的 LLaMA 模型在 2048 个 token 限制内的若干标准基准测试中仅出现轻微性能下降。

我们的研究结果突显了 Transformer 模型“能够外推至训练时未见过的更长序列长度”的先天能力。我们重申了这一假设，并指出先前已知的语言模型在长序列外推方面的弱点可能是由于直接外推位置编码所致，而通过改为插值位置编码可以在很大程度上缓解这一问题。


## 2、方法

### 2.1 背景：旋转位置嵌入（ROPE）

给定位置索引 $m \in [0, c)$ 和嵌入向量 $\mathbf{x} := [x_0, x_1, \ldots, x_{d-1}]^\top$（其中 $d$ 表示注意力头的维度），RoPE 定义了如下向量值复变函数 $\mathbf{f}(\mathbf{x}, m)$：
$$
\begin{equation}
    \mathbf{f}(\mathbf{x},m) = [(x_0 + \mathrm{i} x_1) e^{\mathrm{i} m \theta_0}, (x_2 + \mathrm{i} x_3) e^{\mathrm{i} m \theta_1}, \ldots, (x_{d-2} + \mathrm{i} x_{d-1})e^{\mathrm{i} m \theta_{d/2-1}}]^\top
\end{equation} \tag{1}$$
其中 $\mathrm{i}:= \sqrt{-1}$ 为虚数单位，$\theta_j = 10000^{-2j/d}$。采用 RoPE 后，自注意力得分

$$\begin{eqnarray}
a(m,n) &=& \mathrm{Re}\langle \mathbf{f}(\mathbf{q}, m), \mathbf{f}(\mathbf{k}, n)\rangle \nonumber \\
&=& \mathrm{Re}\left[\sum_{j=0}^{d/2-1} (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1}) e^{\mathrm{i} (m-n)\theta_j}\right] \nonumber \\
&=& \sum_{j=0}^{d/2-1} (q_{2j} k_{2j} + q_{2j+1}k_{2j+1})\cos((m-n)\theta_j) + (q_{2j} k_{2j+1} - q_{2j+1}k_{2j})\sin((m-n)\theta_j) \nonumber \\
&=:& a(m-n)
\end{eqnarray} $$

仅通过三角函数依赖于相对位置 $m−n$。这里的 $\mathbf{q}$ 和 $\mathbf{k}$ 是特定注意力头的查询向量和键向量。在每一层中，RoPE 都被应用于查询和键嵌入以计算注意力分数。


### 2.2 直接外推

虽然 RoPE 中的注意力分数仅取决于相对位置（这正是我们想要的），但其外推性能并不理想。特别是当直接扩展到训练中未见过的更大上下文窗口时，困惑度可能会飙升到非常高的数值（即 $>10^3$），与未经训练的模型相当。

理想情况下，我们希望看到在 $L=2048$ 大小的上下文窗口上训练的模型在更长的上下文窗口上仍然能合理工作，但可能无法利用超过 $L$ 范围之外的信息。例如，要回答位于 3000 位置的问题，在最大窗口大小为 $L=2048$ 训练的模型无法利用位置 0 提供的证据，但仍然可以利用位置 2900 提供的证据。然而现实中我们观察到灾难性的行为，即位于 3000 位置的问题无法被正确回答，即使证据位于 2900 位置。

原因何在？根据 RoPE 论文第 3.4.3 节的描述，注意力分数 $a_{m-n}$ 应随着相对距离 $|m-n|$ 的增加而衰减，那么来自很远距离的内容应该影响不大才对。然而，事实证明 RoPE 第 3.4.3 节推导出的上界可能过于宽松：虽然它确实会随着 $|m-n|$ 的增加而衰减，但这个界限仍然可能相当大（即界限可能严重依赖于 $v_j$ 的大小），因此显得空洞。实际上，如果我们把所有三角函数都视为基函数（即 $\phi_j(s):=e^{\mathrm{i} s \theta_j}$），并将方程 2 视为如下的基展开：

$$\begin{equation}
    a(s) = \mathrm{Re}\left[\sum_{j=0}^{d/2-1} h_j e^{\mathrm{i} s \theta_j}\right] \tag{3}
\end{equation}$$

查询与键之间的 $h_j := (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1})$ 是依赖于 $\mathbf{q}$ 和 $\mathbf{k}$ 的复系数（此处 $h_j$ 的定义与 RoPE 第 3.4.3 节中的定义完全相同）。现在问题变得清晰：如图 2 所示，在 \[0,2048\] 区间内其幅值可能很小，但在该区间外会产生巨大数值。根本原因在于三角函数族 $\{\phi_j\}$（当 $d$ 足够大时）是通用逼近器，可以拟合任意函数。因此对于任意函数，总存在对应的系数 $\{h_j\}$（即键与查询），使得其在 \[0,2048\] 区间内函数值较小，而在区间外显著增大。

> [!tip]
> **解释：** 由于 $\{e^{\mathrm{i} s \theta_j}\}$ 是一组通用基函数，可以逼近任意函数，这意味着只要选取合适的系数 $\{h_j\}$（而这些系数由查询与键控制），我们就可以“刻意”设计一个注意力分数函数 $a(s)$，使其在给定区间 $[0, 2048]$ 内，$a(s)$ 的幅值可以非常小（即远距离 token 的注意力衰减得很好）。  但是在这个区间外，函数 $a(s)$ 的值却可以突然变得非常大。
> 
> 这揭示了一个问题：尽管推导上的上界证明了理论上衰减的性质，但由于基函数的通用性，模型（或说参数 $\{h_j\}$ 的选择）有“能力”在一个有限区间内控制注意力分数，而在区间外却产生极端大的值。这种情况说明那些理论上“衰减”的上界可能在实际应用中并不能有效保证模型在所有距离上都表现稳定。
> 


[图 2]

**图 2：** 外推与内插的对比。​**左图：​**​ 一条拟合的注意力分数函数（红色曲线），形式为公式（3），其中$d= d_\mathrm{model} / n_\mathrm{head} = 4096 / 32 = 128$（LLaMA 7B 的设置）。圆点是待拟合的随机输入点，红色曲线是通过最小二乘法拟合的分数函数，大致在 [−1,1] 范围内。​**中图：​**​ 虽然拟合函数在 \[0,$L$\]（$L=2048$）内看起来有良好的边界，但超出此区域后，函数值可能超过 8000，导致注意力计算中出现灾难性问题。请注意，这里我们完全没有刻意挑选：几乎所有从 \[0,$L$\] 内随机生成的输入点，集合中学习到的曲线都存在外推问题。​**右图：​**​ 另一方面，内插则稳定得多。在垂直虚线之间（即整数位置差）的曲线平滑且表现良好。

（图 2 的生成代码如下：）

```python
import torch  
import matplotlib  
import matplotlib.pyplot as plt  
  
matplotlib.use('TkAgg')  
  
d = 4096 // 32
theta = 10000  
  
# 频率计算  
freqs = 1.0 / (theta ** (torch.arange(0, d, 2)[: (d // 2)].float() / d))  
  
L = 2048  
x = torch.zeros(L)  
x[:L] = torch.arange(0, L)  
  
# basis functions  
xfreq = torch.outer(x, freqs)  
y = torch.randn(x.shape[0])  
  
  
# do linear regression  
X = torch.cat([xfreq.sin(), xfreq.cos()], dim=1)  
eps = 0.000  
coeffs = torch.linalg.solve(X.t() @ X + torch.eye(X.shape[1]) * eps, X.t() @ y)  
  
  
x2 = torch.arange(0, 2*L)  
xfreq2 = torch.outer(x2, freqs)  
X2 = torch.cat([xfreq2.sin(), xfreq2.cos()], dim=1)  
y2 = X2 @ coeffs  

x3 = torch.arange(25, 75, 0.125)  
xfreq3 = torch.outer(x3, freqs)  
X3 = torch.cat([xfreq3.sin(), xfreq3.cos()], dim=1)  
y3 = X3 @ coeffs  

plt.figure(figsize=(16,5))  
plt.subplot(1, 3, 1)  
plt.plot(x2[:L], y2[:L], "r")  
plt.scatter(x, y)  
plt.ylabel("attention score $a(s)$")  
plt.xlabel("Positional difference $s$")  

plt.subplot(1, 3, 2)  
plt.plot(x2, y2, "r")  
plt.scatter(x2, y2)  
plt.axvline(L, color="k", linestyle="--", linewidth=0.5)  
plt.title("Effect of Extrapolation")  
plt.xlabel("Positional difference $s$")  

plt.subplot(1, 3, 3)  
plt.plot(x3, y3, "r")  
  
for i in range(25, 75):  
    plt.axvline(i, color="k", linestyle="--", linewidth=0.5)  
  
plt.title("Effect of Interpolation")  
plt.xlabel("Positional difference $s$")  
plt.show()
```




### 2.3 建议方法：位置插值（PI）

**(推导证明，略，代码见开头)**

在图 2 中，由于基函数 $\phi_j$ 的光滑性，插值方法更加稳定，不会产生异常值。因此，我们建议不再将式 3 中的注意力分数外推到 $s > L$，而是定义一个新的注意力分数 $\tilde a(s) = a(Ls/L')$，其中 $L'$ 表示更长的上下文窗口。正式来说，我们用以下方式重新定义 RoPE 向量函数 $\mathbf{f'}$ 来替代原来的 $\mathbf{f}$：

$$\begin{equation}
    \mathbf{f'}(\mathbf{x}, m)= \mathbf{f}\left(\mathbf{x}, \frac{mL}{L'} \right). \tag{4}
\end{equation}$$

我们将这种位置编码的变换称为 **位置插值**。在这一步骤中，我们将位置索引从 \[0,$L'$) 缩减至 \[0, $L$)，以匹配计算 RoPE 前的原始索引范围。因此，作为 RoPE 的输入，任意两个标记之间的最大相对距离已从 $L'$ 降至 $L$。由于我们在扩展前后对齐了位置索引和相对距离的范围，从而减轻了因上下文窗口扩展对注意力分数计算的影响，这使得模型更容易适应。为了进一步证明这一点，我们在以下定理中展示了插值后的注意力分数表现良好：

*结论是，插值边界至少比外推边界小 $2 \cdot 294.73\sim 600\times$ 倍，这意味着插值注意力分数比外推分数稳定得多。*

值得注意的是，*我们重新调整位置索引的方法既不会引入额外的权重，也不会以任何方式修改模型架构。* 这使得该方法在实际应用中极具吸引力，因为扩展后原始模型的大部分基础设施和优化方案都能直接复用。

**微调**  我们可以使用下一个词预测任务，在扩展的上下文窗口大小上使用插值位置编码，进一步微调插值模型，例如使用 Pile 这样的预训练语料库。在下一节中，我们将展示我们的微调过程仅需要数万到数十万个示例。我们还发现，微调的结果对示例的选择并不敏感。原因可能是模型在微调阶段只是适应新的上下文窗口，从一个良好的初始化开始，而不是获取新知识。

## 3、实验

我们证明位置插值法能有效将上下文窗口扩展至原始大小的 32 倍，且仅需数百次训练步骤即可完成。研究表明，所得模型是具备完全有效长上下文窗口的强大大语言模型。我们通过语言建模、密钥检索和长文档摘要等多项任务验证其性能，同时展示了扩展模型在原始 LLaMA 评估基准上的测试结果。

### 3.1 设置

**模型变体**   我们基于预训练的 7B、13B、33B 和 65B LLaMA 模型，通过直接微调或位置插值方法将其上下文窗口扩展至最高 32768。*除对采用位置插值扩展的模型进行位置索引重缩放外，我们未对 LLaMA 模型架构进行任何其他修改*。

**训练流程**   我们采用下一词元预测目标对所有模型变体进行微调。优化器选用 AdamW，参数设为 $\beta_1=0.9$ 和 $\beta_2=0.95$。学习率采用线性预热策略，从最大学习率的 $10\%$ 开始，经过 20 步达到峰值。7B 和 13B 模型的学习率设为 $2\times 10^{-5}$，33B 和 65B 模型设为 $10^{-5}$。权重衰减率设为零。在将 7B、13B 和 33B 模型的上下文窗口扩展至 8192 时，我们使用 32 块 A100 显卡和 64 的全局批次大小；其他情况均使用 128 块 A100 显卡和 128 的全局批次大小。*需要说明的是，增加显卡数量主要是为了突破微调时的显存限制，特定场景下可减少显卡用量。所有模型训练均基于 PyTorch 框架，采用全分片数据并行技术和 Flash 注意力机制实现。*

如无特别说明，对于位置插值方法，我们对模型进行了 *1000 步的微调*。对于直接微调方法，我们使用了 10000 步。我们主要使用 Pile 训练数据集进行微调。在章节 3.4 中，我们还比较了 RedPajama 数据集上的微调性能。

### 3.2 长序列语言模型

我们评估了扩展模型和基线模型在两个数据集上的长序列语言建模性能：书籍语料库（PG-19）和经过清洗的 Arxiv Math proof-pile 数据集。

我们使用了 PG19 数据集和 proof-pile 数据集的测试集。对于 PG19，我们使用了包含 100 份文档的完整测试集。对于 proof-pile 数据集，我们随机抽取了 128 份文档作为子样本，每份文档至少包含 32768 个 SentencePiece 分词单元，并将每份测试文档截取至前 32768 个分词单元。我们采用滑动窗口方法以步长 $S=256$ 来评估不同上下文窗口大小下的困惑度。

在表 1 和表 2 中，我们报告了模型及基线在各数据集上的困惑度结果。通过分析发现，采用我们方法扩展的模型随着上下文窗口长度的增加，困惑度得到了显著改善。当上下文窗口从 2048 扩展到 16384 时，LLaMA 7B 模型在两个数据集上的困惑度分别降低了 0.28 和 0.5，LLaMA 13B 模型降低了 0.27 和 0.48，LLaMA 33B 模型降低了 0.14 和 0.42。对于 LLaMA 65B 模型，当扩展至 8192 上下文窗口时，其困惑度降低了 0.12 和 0.3。

总体而言，我们观察到模型在更长上下文窗口下的困惑度表现呈现持续提升趋势。这表明我们的模型能有效利用长上下文窗口，在语言建模任务中更准确地预测下一个词元。值得注意的是，在 PG19 数据集上，LLaMA 7B 和 13B 模型在 32768 长度的上下文窗口中仍保持这种提升趋势，这意味着我们的方法可能支持扩展到更长的上下文窗口。

相比之下，我们发现通过直接微调方法扩展的模型在更长上下文窗口上的困惑度出现了退化（最高达 +0.48）或仅有微小改进（最高达 -0.12）。这表明以此方式扩展的模型利用超出预训练设定长度的上下文窗口的能力有限。

在某些情况下，我们发现扩展模型在原始 2048 上下文窗口上的困惑度略有下降。例如，在 Proof-pile 数据集上，所有采用位置插值扩展的模型都出现了 0.01 到 0.05 范围内的性能下降。由于位置插值强制将原始上下文窗口中的位置编码压缩到更窄的区域内，这可能会对语言模型的性能产生负面影响，因此在原始评估上下文窗口中出现小幅性能下降是预期之中的。关于原始上下文窗口尺寸的更多基准测试结果，我们将在第 3.4 节中详细展示。

在表 3 中，我们报告了 LLaMA 7B 模型在 PG19 数据集上使用位置插值法将上下文窗口扩展至 8192 和 16384 时，困惑度与微调步数之间的关系。可以看到未经微调（第 0 步）时，模型已展现出一定的语言建模能力——扩展到 8192 上下文窗口时困惑度 $<20$（相比之下直接外推法会导致 $>10^3$ 的困惑度）。通过微调，我们发现困惑度快速改善：200 步时模型在 2048 窗口尺寸上的表现已超越原始模型，表明其获得了有效利用长于预训练设置的序列进行语言建模的能力；到 1000 步时，模型稳步提升并实现了显著更优的困惑度指标。

[表 1、2、3]

**表 1：** PG19数据集上的评估困惑度。FT：直接微调。PI：位置插值。使用 PI 微调的模型在上下文窗口更长时显示出逐渐降低的困惑度，表明 PI 能很好地利用长上下文，而 FT 的困惑度在窗口更长时会增加。请注意，由于 PG19 的写作风格非常不同，整体困惑度比表 2 要高。

**表 2：** 在 Arxiv 数学 Proof-pile 数据集上评估困惑度。FT：直接微调。PI：位置插值。

**表 3：** 在 PG19 数据集上使用位置插值法评估困惑度随微调步数的变化关系


### 3.3 通过密钥检索测量有效上下文窗口大小

我们研究了模型在扩展后的有效上下文窗口大小，即在推理过程中一个标记能够有效关注的最大距离。为了测量这一点，我们采用了 Mohtashami & Jaggi (2023) 提出的密码检索合成评估任务。在该任务中，模型需要从长文档中恢复隐藏的随机密码。文档格式参见图 3。

给定一个语言模型，我们按如下方式估算有效上下文窗口的上下界。假设随机密钥位于输入末尾的 $k$ 个标记处。当模型在多次独立尝试中持续无法检索到正确的密钥值时，表明该模型的有效上下文窗口大小小于 $k$。反之，若模型能持续成功检索到正确的密钥值，则可推断该模型的有效上下文窗口大小至少为 $k$。

我们评估了通过位置插值或直接微调扩展的 7B 和 33B LLaMA 模型变体。对于每个模型，我们在目标上下文窗口 $L'$ 内均匀选取 32 个不同的 $k$ 值，并对每个 $k$ 值运行上述测试 10 次，每次使用一个由 5 位随机数字组成的随机密码。在表 4 中，我们将 $k_{\max}$ 报告为微调步数的函数，其中 $k_{\max}$ 定义为满足以下条件的最大 $k$ 值：对于所有 $k' \le k$，模型在 $k'$上的成功率至少为 20%。

我们可以看到，通过位置插值扩展的模型都成功地实现了它们在有效上下文窗口大小方面的预期扩展目标，这表现为有效上下文窗口大小在仅经过 200 步微调后即达到最大值 $k_{\max}=L'$，这一现象在 7B 和 33B 模型规模以及高达 32768 的上下文窗口中都保持一致。相比之下，通过直接微调扩展的 LLaMA 模型即使经过超过 10000 步的微调，其有效上下文窗口大小 $k_{\max}$ 仅从 2048 略微增加到 2560，且没有明显迹象表明窗口大小的增长会加速。

[表 4]

**表 4：** 微调后的有效上下文窗口大小。FT：直接微调。PI：位置插值。

[图 3]

**图 3：** 用于检索密码的提示格式。我们采用了与 Mohtashami & Jaggi（2023）提出的完全相同的提示。在测试过程中，这里的密码 12345 会被替换为一个随机的 5 位数。

### 3.4 原始上下文窗口大小的基准测试

我们在原始上下文窗口大小为 2048 的标准基准任务上评估了通过位置插值扩展的模型。其他任务如闭卷问答、数学推理和代码生成通常需要的上下文窗口不超过 2048，因此留待未来评估。评估结果列于表 5 中。从结果来看，扩展到 8192 的模型在原本为更小上下文窗口设计的基准测试中产生了可比较的结果，对于 7B 和 33B 模型规模，基准任务的性能下降最多为 2%。扩展到更长上下文窗口的模型在基准测试上表现有所下降，但对于大多数任务仍在合理范围内。我们还注意到，微调数据集的选择似乎并未导致基准性能的显著差异，这可能是由于我们方法中使用的微调步骤数量有限。基准任务的性能下降与我们在第 3.2 节中对困惑度下降的观察一致。

[表 5、6]

**表 5：** LLaMA 基准测试子集上的零样本性能。通过位置插值扩展的模型表现与原始模型相当，但 BoolQ 数据集除外，该数据集可能需要模型密切注意短参考段落中的词序。

**表 6：** GovReport 数据集上的 ROUGE 评分。


### 3.5 长文档摘要

在本任务中，我们评估了模型在长文档摘要任务上的表现。具体而言，我们采用了 GovReport 数据集，该数据集包含 17,457 篇训练文档和 972 篇评估文档。每篇文档均附有人工撰写的摘要。我们将所有输入文档截断至前 15,000 个词元。

我们对 LLaMA 模型进行了微调，通过位置插值技术将上下文窗口扩展至 16384。需要注意的是，在此微调阶段仍需对位置索引进行重新缩放。首先，我们使用图 4 中的提示模板格式化原始文档，然后将提示信息与每个文档对应的真实摘要（截断至 1000 个标记）进行拼接。基于上述设置，我们采用下一个标记预测任务对模型进行了 10 个 epoch 的微调。在微调过程中，我们排除了训练样本中输入提示部分产生的损失。

我们采用生成温度为 0.5 和 $\text{top}_p = 0.95$ 作为推理参数，为测试集中的每份文档生成摘要。最终输出截断为 1000 个词元。我们使用 ROUGE-1/ROUGE-2/ROUGE-L 评分作为评估指标，将模型输出与真实摘要进行对比评估。

在表 6 中我们报告了评估结果，同时纳入了现有 SCROLLS 排行榜中两个基线的结果。总体而言，在仅进行少量超参数调整的情况下，我们的模型取得了具有竞争力的 R1 分数。这一结果表明，我们采用 16384 上下文窗口的模型能够有效处理长文档摘要任务。

[图 4]

**图 4：** 长文档摘要的输入格式。


## 5、总结

位置插值法能够通过极少量微调，有效扩展 LLaMA 模型的上下文窗口至远超原有长度。经扩展的模型不仅能出色完成各种长上下文任务，还能相对完好地保留其在原始上下文范围内的能力，使其成为处理长短输入皆宜的通用语言模型优选方案。此外，采用位置插值法扩展的模型可兼容绝大多数现有基础设施与优化方案，这使该方法在实际应用中极具吸引力。我们认为位置插值是一种通用技术，未来可应用于其他类型的位置编码方式，从而拓展更多大语言模型的适用场景，近期我们计划就此展开深入研究。


