
* https://arxiv.org/pdf/2306.15595
* meta （2023）

**摘要**    我们提出位置插值法（Position Interpolation，PI），该方法通过极少量微调（1000 步以内）即可将基于 RoPE 的预训练大语言模型（如 LLaMA 系列模型）的上下文窗口扩展至 32768，同时在需要长上下文的任务中展现出强劲性能表现——从 7B 到 65B 参数的 LLaMA 模型在密钥检索、语言建模及长文档摘要等任务中均取得优异结果。经位置插值扩展的模型在其原始上下文窗口范围内的任务上仍能保持较好质量。该方法通过线性压缩输入位置索引使其适配原始上下文窗口尺寸，而非外推超出训练长度的位置索引（后者可能导致灾难性高注意力分数，彻底破坏自注意力机制）。理论研究表明，插值法的误差上界至少比外推法小 600 倍，进一步验证了其稳定性。通过位置插值扩展的模型保持原有架构，可复用绝大多数既有优化方案与基础设施。

## 1、引言

LLMs 通常具有预定义的上下文窗口大小。例如，LLaMA 模型的输入必须少于 2048 个标记。在诸如进行长对话、总结长文档或执行长期规划等应用中，经常超出这个预设的上下文窗口限制。对于这些应用，具有更长上下文窗口的 LLMs 更为理想。然而，从头开始训练一个具有长上下文窗口的 LLM 需要大量投入。这自然引出了一个问题：我们能否扩展现有预训练 LLM 的上下文窗口？

一种直接的方法是对现有的预训练 Transformer 进行微调，使其具有更长的上下文窗口。然而，通过实验我们发现，以这种方式训练的模型对长上下文窗口的适应速度非常缓慢。在训练超过 10000 个批次后，有效的上下文窗口仅从 2048 增加到 2560（表 4），这表明这种方法在扩展到更长的上下文窗口时效率低下。

尽管某些技术能够实现 Transformer 的长度外推，即在较短的上下文窗口上进行训练，在较长的窗口上进行推理，但许多现有的预训练 LLM（包括 LLaMA）使用的位置编码具有较弱的外推特性（例如RoPE）。因此，这些技术在扩展此类 LLM 的上下文窗口大小方面的适用性仍然有限。

在这项工作中，我们引入了位置插值法（Position Interpolation），为包括 LLaMA 在内的某些现有预训练大语言模型实现上下文窗口扩展。其核心思想是：不同于外推法，我们直接对位置索引进行降尺度处理，使最大位置索引与预训练阶段原有的上下文窗口限制相匹配（如图 1 所示）。换言之，为了容纳更多输入标记，我们利用位置编码可应用于非整数位置的特性，对相邻整数位置的位置编码进行插值——这相比在已训练位置范围外进行外推（可能导致灾难性数值）更具优势。我们从理论上验证了该方法的有效性：在 LLaMA 7B 模型设定中，插值后的注意力分数上界比外推法小约 600 倍，因而更加稳定。这使得模型更容易适应插值后的位置编码。

[图 1]

**图 1**：我们的位置插值方法示意图。假设一个 Llama 模型预训练时的上下文窗口长度为 2048。左上角展示了 LLM 模型的常规用法：输入位置索引（蓝色圆点）位于预训练范围内。右上角展示了长度外推的情况，此时模型需要处理未见过的位置（红色圆点），直至 4096。左下角展示了位置插值方法，我们将位置索引本身（蓝色和绿色圆点）从 [0, 4096] 线性压缩至 [0, 2048]，强制使其落在预训练范围内。

根据实证研究，我们发现位置插值法非常高效且有效，仅需极短时间的微调即可使模型完全适应大幅扩展的上下文窗口。我们展示了使用位置插值法将 7B 至 65B 参数的 LLaMA 模型上下文窗口从初始的 2048 扩展到最高 32768 的实验结果。研究结果表明：

1. 位置插值法能够轻松实现超长上下文窗口（例如 32768），仅需在 Pile 数据集上进行 1000 步微调即可获得良好效果。与预训练成本相比，微调代价几乎可以忽略不计。这验证了我们的假设：模型相对容易适应插值式位置编码。
2. 位置插值法生成的强大模型能有效利用大幅扩展的上下文窗口。我们证明，通过位置插值扩展的模型在文本建模中，因上下文窗口极大扩展而获得显著的困惑度提升，且困惑度随上下文窗口扩大而平稳下降。我们还将位置插值应用于长文本摘要任务，并展现出具有竞争力的性能。
3. 位置插值法在原始上下文窗口大小的任务中能较好保持模型质量。我们针对扩展后的 LLaMA 模型在原始 LLaMA 基准测试中呈现了多样化的评估结果。与原始 LLaMA 模型相比，扩展后的 LLaMA 模型在 2048 个 token 限制内的若干标准基准测试中仅出现轻微性能下降。

我们的研究结果突显了 Transformer 模型“能够外推至训练时未见过的更长序列长度”的先天能力。我们重申了这一假设，并指出先前已知的语言模型在长序列外推方面的弱点可能是由于直接外推位置编码所致，而通过改为插值位置编码可以在很大程度上缓解这一问题。

**并行工作**。就在我们发布前夕，我们获悉了一篇并行的博客文章（Super-HOT kaiokendev (2023)），该文章同样通过在 RoPE 中插值位置编码，将上下文窗口从 2K 扩展到 8K。最近，开源社区在 Reddit 帖子和 Github Issues 中对此进行了讨论，表明使用 LoRA 进行微调似乎也能取得良好效果。我们的论文展示了在高达 65B 参数的模型上，通过位置插值进行完整微调的效果良好，并且我们还从理论上解释了为什么插值比外推能获得更稳定的结果——通过证明插值注意力分数的上界远低于外推的上界。

--------

## 2、方法

### 2.1 背景：旋转位置嵌入（ROPE）

Transformer 模型需要显式地注入位置信息（通常以位置编码的形式）来表示输入序列的顺序。我们采用 RoPE——该编码方案被应用于 LLaMA 模型。给定位置索引 $m \in [0, c)$ 和嵌入向量 $\mathbf{x} := [x_0, x_1, \ldots, x_{d-1}]^\top$（其中 $d$ 表示注意力头的维度），RoPE 定义了如下向量值复变函数 $\mathbf{f}(\mathbf{x}, m)$：
$$
\begin{equation}
    \mathbf{f}(\mathbf{x},m) = [(x_0 + \mathrm{i} x_1) e^{\mathrm{i} m \theta_0}, (x_2 + \mathrm{i} x_3) e^{\mathrm{i} m \theta_1}, \ldots, (x_{d-2} + \mathrm{i} x_{d-1})e^{\mathrm{i} m \theta_{d/2-1}}]^\top
\end{equation} \tag{1}$$
其中 $\mathrm{i}:= \sqrt{-1}$ 为虚数单位，$\theta_j = 10000^{-2j/d}$。采用 RoPE 后，自注意力得分

$$\begin{eqnarray}
a(m,n) &=& \mathrm{Re}\langle \mathbf{f}(\mathbf{q}, m), \mathbf{f}(\mathbf{k}, n)\rangle \nonumber \\
&=& \mathrm{Re}\left[\sum_{j=0}^{d/2-1} (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1}) e^{\mathrm{i} (m-n)\theta_j}\right] \nonumber \\
&=& \sum_{j=0}^{d/2-1} (q_{2j} k_{2j} + q_{2j+1}k_{2j+1})\cos((m-n)\theta_j) + (q_{2j} k_{2j+1} - q_{2j+1}k_{2j})\sin((m-n)\theta_j) \nonumber \\
&=:& a(m-n)
\end{eqnarray} $$

仅通过三角函数依赖于相对位置 $m−n$。这里的 $\mathbf{q}$ 和 $\mathbf{k}$ 是特定注意力头的查询向量和键向量。在每一层中，RoPE 都被应用于查询和键嵌入以计算注意力分数。


### 2.2 直接外推

虽然 RoPE 中的注意力分数仅取决于相对位置（这正是我们想要的），但其外推性能并不理想。特别是当直接扩展到训练中未见过的更大上下文窗口时，困惑度可能会飙升到非常高的数值（即 $>10^3$），与未经训练的模型相当。

理想情况下，我们希望看到在 $L=2048$ 大小的上下文窗口上训练的模型在更长的上下文窗口上仍然能合理工作，但可能无法利用超过 $L$ 范围之外的信息。例如，要回答位于 3000 位置的问题，在最大窗口大小为 $L=2048$ 训练的模型无法利用位置 0 提供的证据，但仍然可以利用位置 2900 提供的证据。然而现实中我们观察到灾难性的行为，即位于 3000 位置的问题无法被正确回答，即使证据位于 2900 位置。

原因何在？根据 RoPE 论文第 3.4.3 节的描述，注意力分数 $a_{m-n}$ 应随着相对距离 $|m-n|$ 的增加而衰减，那么来自很远距离的内容应该影响不大才对。然而，事实证明 RoPE 第 3.4.3 节推导出的上界可能过于宽松：虽然它确实会随着 $|m-n|$ 的增加而衰减，但这个界限仍然可能相当大（即界限可能严重依赖于 $v_j$ 的大小），因此显得空洞。实际上，如果我们把所有三角函数都视为基函数（即 $\phi_j(s):=e^{\mathrm{i} s \theta_j}$），并将方程 2 视为如下的基展开：

$$\begin{equation}
    a(s) = \mathrm{Re}\left[\sum_{j=0}^{d/2-1} h_j e^{\mathrm{i} s \theta_j}\right] \tag{3}
\end{equation}$$

查询与键之间的 $h_j := (q_{2j} +\mathrm{i} q_{2j+1})(k_{2j} - \mathrm{i} k_{2j+1})$ 是依赖于 $\mathbf{q}$ 和 $\mathbf{k}$ 的复系数（此处 $h_j$ 的定义与 RoPE 第 3.4.3 节中的定义完全相同）。现在问题变得清晰：如图 2 所示，在 [0,2048] 区间内其幅值可能很小，但在该区间外会产生巨大数值。根本原因在于三角函数族 $\{\phi_j\}$（当 $d$ 足够大时）是通用逼近器，可以拟合任意函数。因此对于任意函数，总存在对应的系数 $\{h_j\}$（即键与查询），使得其在 [0,2048] 区间内函数值较小，而在区间外显著增大。


[图 2]

**图 2：** 外推与内插的对比。​**左图：​**​ 一条拟合的注意力分数函数（红色曲线），形式为公式（3），其中$d= d_\mathrm{model} / n_\mathrm{head} = 4096 / 32 = 128$（LLaMA 7B 的设置）。圆点是待拟合的随机输入点，红色曲线是通过最小二乘法拟合的分数函数，大致在 [−1,1] 范围内。​**中图：​**​ 虽然拟合函数在 \[0,$L$\]（$L=2048$）内看起来有良好的边界，但超出此区域后，函数值可能超过 8000，导致注意力计算中出现灾难性问题。请注意，这里我们完全没有刻意挑选：几乎所有从 \[0,$L$\] 内随机生成的输入点，集合中学习到的曲线都存在外推问题。​**右图：​**​ 另一方面，内插则稳定得多。在垂直虚线之间（即整数位置差）的曲线平滑且表现良好。生成该图的源代码请参见附录 C.1。


### 2.3 建议方法：位置插值（PI）

在图 2 中，由于基函数 $\phi_j$ 的光滑性，插值方法更加稳定，不会产生异常值。因此，我们建议不再将式 3 中的注意力分数外推到 $s > L$，而是定义一个新的注意力分数 $\tilde a(s) = a(Ls/L')$，其中 $L'$ 表示更长的上下文窗口。正式来说，我们用以下方式重新定义 RoPE 向量函数 $\mathbf{f'}$ 来替代原来的 $\mathbf{f}$：

$$\begin{equation}
    \mathbf{f'}(\mathbf{x}, m)= \mathbf{f}\left(\mathbf{x}, \frac{mL}{L'} \right). \tag{4}
\end{equation}$$

我们将这种位置编码的变换称为 **位置插值**。在这一步骤中，我们将位置索引从 \[0,$L′$) 缩减至 \[0, $L$)，以匹配计算 RoPE 前的原始索引范围。因此，作为 RoPE 的输入，任意两个标记之间的最大相对距离已从 $L′$ 降至 $L$。由于我们在扩展前后对齐了位置索引和相对距离的范围，从而减轻了因上下文窗口扩展对注意力分数计算的影响，这使得模型更容易适应。为了进一步证明这一点，我们在以下定理中展示了插值后的注意力分数表现良好：

**定理 2.1（插值边界）**  对于注意力分数 $a(s) = \mathrm{Re}\left[\sum_{j=0}^{d/2-1} h_j e^{\mathrm{i} s \theta_j}\right]$，其中 $\theta_j = c^{-2j/d}$，其在区间 $s \in [s_1, s_2]$ 内的插值 $a(s)$ 有如下界限：

$$\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| \le d\left(\max_j |h_j|\right) \frac{(s-s_1)(s_2-s)}{8\ln c} \tag{5}
\end{equation}$$
其中 $a_{\mathrm{linear}}(s)$ 是两个网格点 $a(s_1)$ 和 $a(s_2)$ 的线性插值，这两个点已知表现良好，这是通过 LLM 预训练强化的：

$$\begin{equation}
    a_{\mathrm{linear}}(s) := (1-\lambda(s)) a(s_1) + \lambda(s) a(s_2), \quad\quad \lambda(s) := \frac{s-s_1}{s_2-s_{1}  }  \tag{6}
\end{equation}$$

请查看附录 A 以获取证明。直观上，在 LLM 预训练中，我们知道注意力分数 $a(s)$ 在整数网格 $s_1$ 和 $s_2$ 上表现良好。因此，对于任何插值 $s\in [s_1,s_2]$，我们有 $(s-s_1)(s_2-s) \le 1/4$。注意 $c=10000$，边界变为：
$$\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| \le \frac{d}{32 \ln c}\max_j |h_j| \approx \frac{d \max_j |h_j|}{294.73}    \tag{7}
\end{equation}$$

相比之下，RoPE 中的第 3.4.3 节给出了一个外推边界（即它适用于所有位置距离 $s$）：
$$\begin{equation}
 |a(s)| \le \left(\max_{j} |h_j - h_{j+1}|\right) \sum_{k=0}^{d/2-1} |A_{k+1}(s)| \le 2\left(\max_{j} |h_j|\right) \sum_{k=0}^{d/2-1} |A_{k+1}(s)|  \tag{8}
\end{equation}$$
其中 $A_k(s) := \sum_{j=0}^{k-1} e^{\mathrm{i} s \theta_j}$。虽然 $B(s) := \sum_{k=0}^{d/2-1} |A_{k+1}(s)|$ 没有闭合形式，但数值上至少大于 $d$，并且对于许多位置差异 $s$，$B(s)$ 远大于 $d$（具体图示参见附录 B）。因此，插值边界至少比外推边界小 $2 \cdot 294.73\sim 600\times$ 倍，这意味着插值注意力分数比外推分数稳定得多。

值得注意的是，我们重新调整位置索引的方法既不会引入额外的权重，也不会以任何方式修改模型架构。这使得该方法在实际应用中极具吸引力，因为扩展后原始模型的大部分基础设施和优化方案都能直接复用。

**微调**  我们可以使用下一个词预测任务，在扩展的上下文窗口大小上使用插值位置编码，进一步微调插值模型，例如使用 Pile 这样的预训练语料库。在下一节中，我们将展示我们的微调过程仅需要数万到数十万个示例。我们还发现，微调的结果对示例的选择并不敏感。原因可能是模型在微调阶段只是适应新的上下文窗口，从一个良好的初始化开始，而不是获取新知识。

**其他降低插值/外推误差边界的方法**  从插值误差边界（公式 5）和外推误差边界（公式 8）的表达式可以看出，它们共有的一个项是 $\max_j |h_j|$，即查询/键乘积的最大幅度。如果我们在大型语言模型训练过程中对 $|h_j|$ 施加正则化约束，就有可能减轻甚至解决灾难性外推误差。实际上，如果我们采用带适当正则化项的岭回归来拟合图 2 中的曲线，当 $s > L$ 时外推得到的 $a(s)$ 幅值可以与 \[0, $L$] 区间内的幅值相当。据我们所知，目前还没有现有的大型语言模型预训练技术利用这种正则化方法，这将留待未来研究。

## 3、实验

我们证明位置插值法能有效将上下文窗口扩展至原始大小的 32 倍，且仅需数百次训练步骤即可完成。研究表明，所得模型是具备完全有效长上下文窗口的强大大语言模型。我们通过语言建模、密钥检索和长文档摘要等多项任务验证其性能，同时展示了扩展模型在原始 LLaMA 评估基准上的测试结果。

### 3.1 设置

**模型变体**   我们基于预训练的 7B、13B、33B 和 65B LLaMA 模型，通过直接微调或位置插值方法将其上下文窗口扩展至最高 32768。除对采用位置插值扩展的模型进行位置索引重缩放外，我们未对 LLaMA 模型架构进行任何其他修改。

**训练流程**   我们采用下一词元预测目标对所有模型变体进行微调。优化器选用 AdamW，参数设为 $\beta_1=0.9$ 和 $\beta_2=0.95$。学习率采用线性预热策略，从最大学习率的 $10\%$ 开始，经过 20 步达到峰值。7B 和 13B 模型的学习率设为 $2\times 10^{-5}$，33B 和 65B 模型设为 $10^{-5}$。权重衰减率设为零。在将 7B、13B 和 33B 模型的上下文窗口扩展至 8192 时，我们使用 32 块 A100 显卡和 64 的全局批次大小；其他情况均使用 128 块 A100 显卡和 128 的全局批次大小。需要说明的是，增加显卡数量主要是为了突破微调时的显存限制，特定场景下可减少显卡用量。所有模型训练均基于 PyTorch 框架，采用全分片数据并行技术和 Flash 注意力机制实现。

如无特别说明，对于位置插值方法，我们对模型进行了 1000 步的微调。对于直接微调方法，我们使用了 10000 步。我们主要使用 Pile 训练数据集进行微调。在章节 3.4 中，我们还比较了 RedPajama 数据集上的微调性能。


\subsection{Long Sequence Language Modeling}
\label{sec:perplexity}

We evaluate the long sequence language modeling performance of our extended models and baselines on two datasets: book corpus (PG-19) \citep{Rae2020Compressive} and cleaned Arxiv Math proof-pile dataset \citep{proofpile}. 

We use the test splits of PG19 \citep{Rae2020Compressive} and proof-pile \citep{proofpile}. 
For PG19, we use the whole test split consisting of 100 documents. 
For the proof-pile dataset, we use a random subsample of 128 documents with at least 32768 SentencePiece \citep{kudo-richardson-2018-sentencepiece} tokens and truncate to the first 32768 tokens for each test document. 
We evaluate perplexity at various context window size by using a sliding window approach following \citet{press2022train} with stride $S=256$.

In Table~\ref{table:perplexity-pg19} and Table~\ref{table:perplexity-pile}, we report the perplexity results for our models and baselines on the datasets. From the results, we found that models extended with our method enjoy a significantly 
improved perplexity from longer context window sizes. 
By increasing the context window size from 2048 to 16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on both datasets,
-0.27 and -0.48 reductions for extending LLaMA 13B models, and -0.14 and -0.42 reductions for extending LLaMA 33B models. 
For LLaMA 65B models, we observed -0.12 and -0.3 reductions of perplexity by extending to the 8192 context window size.

In general, we observed a consistent trend of our models achieving better perplexity with
longer context windows.  
This indicates our models can effectively make use of the longer context windows to
better predict next tokens in language modeling tasks.
Moreover, we found this trend extends to 32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. 
This indicates that our method may enable extension to even longer context windows.

In contrast, we observed that models extended via the direct fine-tuning method has shown 
regression (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.
This indicates that models extended this way have limited capability of making
use of context windows longer than their pre-trained settings.

We saw a minor degradation of the perplexity on the original context window of 2048 for our extended models in some cases.
For example, on the Proof-pile dataset, we saw a degradation ranging from 0.01 to 0.05 across all models with extended with Position Interpolation. 
A small degradation of performance within original evaluation context window is expected since Position Interpolation forces position encodings in original context window to reside in a much  narrower region, which may negatively affect the language model's performance.
We present more benchmark results on the original context window size in Section~\ref{sec:llama}.

In Table~\ref{tab:perplexity-finetune} we report the relationship between perplexity and the number of fine-tuning steps
for LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation evaluated
on the PG19 dataset.
We can see without fine-tuning (at step 0) the model can exhibit certain language modeling capability, as 
indicated by $<20$ perplexity for extending to 8192 context window (in contrast, the direct extrapolation method leads to $>10^3$ perplexity).
With fine-tuning, we observed that the perplexity improves quickly. 
At 200 steps the models surpassed the original model's perplexity on 2048 context window size, indicating the models
gaining ability of effectively using sequences longer  than the pre-training settings for language modeling.
At 1000 steps, we can see the models have improved steadily and achieve a significantly better perplexity.


\begin{table}[thbp]
\centering
\begin{tabular}{cccccccc}
\toprule
\multicolumn{3}{c}{Model} & \multicolumn{5}{c}{Evaluation Context Window Size}\\
Size &  Context Window & Method &  2048 & 4096 & 8192 & 16384 & 32768 \\
\midrule
   7B &            2048 &   None &  7.20 &    $>10^3$ &  $>10^3$ &    $>10^3$ &     $>10^3$ \\
   7B &            8192 &     FT &  7.21 & 7.34 & 7.69 &     - &     - \\
\midrule
   7B &            8192 &     PI&  7.13 & 6.96 & 6.95 &     - &     - \\
   7B &           16384 &     PI &  7.11 & 6.93 & 6.82 &  6.83 &     - \\
   7B &           32768 &     PI&  7.23 & 7.04 & 6.91 &  6.80 &  6.77 \\
\midrule
   13B &            2048 &   None &  6.59 &    - &  - &    - &     -\\
   13B &            8192 &     FT &  6.56 & 6.57 & 6.69 &     - &     - \\
\midrule
   13B &            8192 &     PI&  6.55 & 6.42 & 6.42 &     - &     - \\
   13B &           16384 &     PI &  6.56 & 6.42 & 6.31 &  6.32 &     - \\
   13B &           32768 &     PI&  6.54 & 6.40 & 6.28 &  6.18 &  6.09 \\
\midrule
  33B &            2048 &   None &  5.82 &    - &    - &     - &     - \\
  33B &            8192 &     FT &  5.88 & 5.99 & 6.21 &     - &     - \\
\midrule
  33B &            8192 &     PI &  5.82 & 5.69 & 5.71 &     - &     - \\
  33B &           16384 &     PI &  5.87 & 5.74 & 5.67 &  5.68 &     - \\
\midrule
65B & 2048 & None & 5.49 & - & - & - & -\\
\midrule
65B & 8192 & PI & 5.42 & 5.32 & 5.37 & - & - \\  
\bottomrule
\end{tabular}
\caption{\small Evaluation perplexity on PG19 dataset \citep{Rae2020Compressive}. FT: Direct Fine-tuning. PI: Position Interpolation. Model fine-tuned with PI shows progressively lower perplexity with longer context window, showing that PI can leverage long context well, while the perplexity of FT increases over longer window. Note that overall the perplexity is higher compared to Table~\ref{table:perplexity-pile} since PG19 has very different writing styles.}
\label{table:perplexity-pg19}
\end{table}


\begin{table}[thbp]
\centering
\begin{tabular}{cccccccc}
\toprule
\multicolumn{3}{c}{Model} & \multicolumn{5}{c}{Evaluation Context Window Size}\\
Size & Context Window & Method & 2048 & 4096 & 8192 & 16384 & 32768 \\
\midrule
7B & 2048 & None & 2.77 & - & - & - & -\\
7B & 8192 & FT &  2.85 & 2.74 & 2.73 & - & -\\
\midrule
7B & 8192 & PI & 2.79 & 2.57 & 2.39 & - & -\\
7B & 16384 & PI & 2.79 & 2.57 & 2.37 & 2.25 & -\\
7B & 32768 & PI & 2.82 & 2.59 & 2.39 & 2.24 & 2.48 \\
\midrule
13B & 2048 & None & 2.66 & - & - & - & -\\
13B & 8192 & FT &  2.71 & 2.56 & 2.50 & - & -\\
\midrule
13B & 8192 & PI & 2.67 & 2.47 & 2.30 & - & -\\
13B & 16384 & PI & 2.68 & 2.47 & 2.29 & 2.18 & -\\
13B & 32768 & PI & 2.68 & 2.46 & 2.28 & 2.15 & 2.35 \\
\midrule
33B & 2048 & None & 2.49 & - & - & - & -\\
33B & 8192 & FT &  2.56 & 2.48 & 2.47 & - & -\\
\midrule
33B & 8192 & PI & 2.50 & 2.32 & 2.18 & - & - \\
33B & 16384 & PI & 2.53 & 2.34 &2.18 & 2.07 & - \\
\midrule
65B & 2048 & None & 2.42 & - & - & - & -\\
\midrule
65B & 8192 & PI & 2.43 & 2.26 & 2.12 & - & - \\
\bottomrule
\end{tabular}
\caption{\small Evaluation perplexity on Arxiv Math Proof-pile dataset \citep{proofpile}. FT: Direct Fine-tuning. PI: Position Interpolation.}
\label{table:perplexity-pile}
\end{table}

\begin{table}[thbp]
\centering
\begin{tabular}{cccccccc}
\toprule
\multicolumn{2}{c}{Model} & \multicolumn{6}{c}{Number of fine-tuning steps}\\
Size &  Context Window  &     0 &  200 &  400 &  600 &  800 &  1000 \\
\midrule
  7B &                 8192 & 16.10 & 7.12 & 7.10 & 7.02 & 6.99 &  6.95 \\
  7B &                 16384 & 112.13 & 7.05 & 6.93 & 6.88 & 6.84 &  6.83 \\
\bottomrule
\end{tabular}
\caption{\small 
Evaluation perplexity on PG19 dataset \citep{Rae2020Compressive} with respect to the number of fine-tuning steps using Position Interpolation.
}
\label{tab:perplexity-finetune}
\end{table}


\subsection{Measuring Effective Context Window Size through Passkey Retrieval}
\label{sec:passkey}
We study the effective context window size, i.e. the maximum distance of a token can \emph{effectively} attend to during inference,  of our models after extension. 
To measure this, we follow a synthetic evaluation task of passkey retrieval proposed by \citet{mohtashami2023landmark}.
In this task, the models are asked to recover a random passkey hidden in a long document. 
See Figure~\ref{fig:passcode-prompt} for the format of the document.

Given a language model, we estimate the upper and lower bounds of effective context windows as follows.
Suppose the random passkey is $k$ tokens away from the end of the input. 
When a model persistently fails to retrieve the correct passkey value across several independent attempts, 
it suggests that the effective context window size of the model is less than $k$. 
Conversely, if a model consistently succeeds in retrieving the correct passkey value, 
we deduce that the effective context window size of the model is at least $k$. 

We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or direct fine-tuning. 
For each model, we use 32 different $k$ uniformly spaced in the targeted context window $L'$ and run the above tests 
for 10 times for each $k$, where each time a random passkey of 5 random digits is used. 
In Table~\ref{fig:passcode}, we report $k_{\max}$ as a function of the number of fine-tuning steps, where $k_{\max}$ is defined as the maximum $k$ such that, for all  $k' \le k$, the model has a success rate of at least 20\% on $k'$.

We can see that models extended via Position Interpolation all successfully attain their desired extension objectives in terms of effective context window sizes, indicating by the effective context window size reaching maximum $k_{\max}=L'$, after merely fine-tuning for 200 steps, consistently across both 7B and 33B model sizes and up to 32768 context windows.
In contrast, LLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context window size $k_{\max}$ from 2048 to 2560, even after
fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size. 

\ignore{
This result highlights the effectiveness of our methods: Position Interpolation 

This also suggested the inefficiency of direct fine-tuning for extending LLaMA models to 
much longer context windows.
}

\begin{table}[th]
    \centering
\begin{tabular}{lclrrllll}
\toprule
    \multicolumn{3}{c}{Model} & \multicolumn{6}{c}{Fine-tuning steps}\\
  Size &  Context Window & Method &    200 &   400 &    600 &    800 &    1000 &   10000 \\
\midrule
 7B &  8192 &   FT &  1792 &    2048 &   2048 &  2048  & 2304 & 2560 \\
 33B &  8192 &   FT &  1792 &    2048 &   1792 &  2048  & 2304 & - \\
\midrule
 7B &  8192 &   PI &  8192 &  8192 &8192 &        8192 &       8192 &       - \\
 7B & 16384 &   PI  & 16384 & 16384 &  16384 &    16384 &       16384 &       - \\
 7B & 32768 &   PI & 32768 & 32768 &  18432 &  32768 &       32768 &       - \\
33B &  8192 &   PI &  8192 &  8192 &   8192 &   8192 &       8192 &       - \\
33B &  16384 &   PI&  16384 &  16384 &   16384 &   16384 &       16384 &       - \\
\bottomrule
\end{tabular}
    \caption{\small Effective context window sizes after fine-tuning. FT: Direct fine-tuning. PI: Position Interpolation.}
    \label{fig:passcode}
\end{table}

\begin{figure}[th]
    \small
	\texttt{\noindent
		There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there.\\
		The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. {\color{blue}{(repeat X times)}} \\
		The pass key is {\color{red}{12345}}. Remember it. {\color{red}{12345}} is the pass key.\\
		The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. {\color{blue}{(repeat Y times)}} \\
		What is the pass key? The pass key is\\
	}
	\caption{\small Prompt format for passkey retrieval. We use the exact same prompt as proposed by \citet{mohtashami2023landmark}. Here the passkey 12345 is replaced with a random 5-digit numbers during test.}
	\label{fig:passcode-prompt}
\end{figure}


\subsection{Benchmarks on Original Context Window Size}
\label{sec:llama}

We evaluate the models extended by Position Interpolation on several standard benchmark tasks within the original context window size of 2048.
% Other tasks such as closed book QA, math reasoning and code generation, often require a context window no more than 2048, and are left for future evaluation. 
The evaluation results are listed in Table~\ref{tab:llama_original}.
From the results, we saw that models extended to 8192 produce comparable results on 
the original benchmark which is designed for a much smaller context window, with
a degradation of up to 2\% on the benchmark tasks, for both 7B and 33B model sizes. 
Models extended to longer context windows regressed more on the benchmarks, but still in reasonable ranges for most tasks. 
We also note that the choice of fine-tuning datasets does not seem to lead significant difference in the
benchmark performances, which may be due to the limited number of fine-tuning steps used in our method.
The regression on benchmark tasks is consistent with our observation on perplexity regression in Section~\ref{sec:perplexity}.

\ignore{
There are two sources w.r.t performance degradation: 
\begin{enumerate}
\item Performance regression while aligning to specific fine-tuning dataset, this is also observed in \cite{ouyang2022training} which can be mitigated by ``rehearsing" on pre-trained dataset during finetuning. As shown in Table~\ref{tab:llama_original}, we observe
less regression when fine-tuning on Redpajama dataset which is more aligned with orignal LLaMA training set. We notice the 
performance degradation worsened when extending to longer context windows. 

\item Performance regression due to position interpolation. This is the price we pay due to loss of language model's ``resolution". We observed worse benchmark results when interpolating (extending to) longer context windows -- this is also observed in Passkey Retrieval task where we see the retrieval accuracy degrading as we extend to longer window size.

\end{enumerate}
}


\ignore{
This is encouraging since the results suggest the models still preserve relatively well their ability
in their original context window sizes even they are fine-tuned on much larger context windows.
This also suggests that the fact that Position
Interpolation puts position encodings into a much narrower region seem does not hurt
the modeling quality significantly.
}

\begin{table}[]
    \centering
    \begin{tabular}{cccccccccc}
       \toprule
       Model Size & Context Window & Fine-tune on & BoolQ & PIQA & Race-M & Race-H & WinoGrande \\
       \midrule
       7B & 2048 & None & 76.1 & 78.9 & 55.7 & 42.2 & 69.6 \\
       \midrule
       7B & 8192 & Pile & 73.2 & 78.2 & 53.8 & 41.7 & 69.0 \\
       7B & 16384 & Pile & 69.8   & 77.6 & 53.3 & 40.9  & 67.8 \\
       7B & 32768 & Pile & 64.7   & 77.2 & 50.1 & 39.6  & 66.9 \\
       7B & 8192 & RedPajama & 75.5 & 77.4 & 54.5 & 41.5 & 68.1 \\
       \midrule
       33B & 2048 & None & 81.6   & 80.2 & 61.1 & 45.9 & 76.2 \\
       \midrule
       33B & 8192 & Pile & 80.2   & 80.7 & 60.2 & 45.7 & 75.9 \\       
       \bottomrule
    \end{tabular}
    \caption{\small Zero-shot performance on a subset of LLaMA Benchmarks. Models extended by Position Interpolation comparable performance as the original models, except for BoolQ dataset that may require models to pay close attention to word ordering in a short reference paragraph.}
    \label{tab:llama_original}
\end{table}


\subsection{Long Document Summarization}

In this task, we evaluate our models' performance on the long document summarization task. 
In particular, we consider the GovReport \citep{huang-etal-2021-efficient} dataset, which contains 17457 documents for training and 972 documents for evaluation.
Each document comes with a human generated summary.
We truncate all input documents to their first 15000 tokens.

We fine-tune the LLaMA models extended with Position Interpolation with a context window
of 16384. Note the rescaling of position indices are still required during this fine-tuning step.
We first format the raw document using the prompt template in Figure~\ref{fig:summ-format}, and then concatenate the prompt
with the ground-truth summary (truncate to 1000 tokens) associated with each document. 
We fine-tune the model using the next token prediction task with the above setup for 10 epochs.
The losses from the input prompt proportion of training examples are excluded during our fine-tuning.

We use a generation temperature of 0.5 and $\text{top}_p = 0.95$ as our inference parameter to generate a summarization of each document in the test set. 
The final output is truncated at 1000 tokens.
We used the ROUGE-1/ROUGE-2/ROUGE-L scores \citep{lin-2004-rouge} as the evaluation metrics to evaluate the models' outputs vs the ground-truth summaries.

%\clj{As shown on Table \ref{table:govreport} our model on-par with the }
In Table~\ref{table:govreport} we report our evaluation results. 
We have also included results from two baselines in existing SCROLLS Leaderboard \citep{shaham-etal-2022-scrolls, ainslie2023colt5}. 
% To our knowledge, this is the first work to extend LLaMA model's window size to apply to long document summarization. 
In general, we have obtained competitive R1 score among other models with minimal tuning of hyper-parameters. 
% We note that LLaMA-7B is a general purpose language model with summarization only being one of its capabilities.
This result suggests our models with 16384 context window can effectively handle the long document summarization task.
%We leave extending to beyond 32768 context window to summarize even longer document as future works. 
%We found that our methods produced highly competitive results, with 33B-16384 holding a clear lead over all
% other models. In Appendix~\ref{appendix:govreport} we show a few examples of the generation result on the validation set.
\begin{figure}[th]
	\texttt{\noindent
        Read the following article and then summarize it. \\
        \# .... Document goes here \\
        Now summarize the above article. \\
        Summary:
    }
	\caption{\small Input format for long doc summarization.}
	\label{fig:summ-format}
\end{figure}


\begin{table}[]
    \centering
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{Model} & \multicolumn{3}{c}{Evaluation Score}\\
Model & Context Window & ROUGE-1 & ROUGE-2 & ROUGE-L \\
\midrule
CoLT5 Base \citep{ainslie2023colt5} & 16K & 58.7 & 29.6 & 31.4 \\
CoLT5 XL \citep{ainslie2023colt5} & 16K &  61.3 & 32.2 & 33.8 \\
\midrule
LLaMA-7B Extended & 16K &  60.0 & 28.0 & 29.5 \\
\bottomrule
\end{tabular}
\caption{\small ROUGE Score on GovReport Dataset.}
\label{table:govreport}
\end{table}

\section{Related Work}

{\noindent \bf Retrieval-augmented LLM.} One line of work extends LLMs by augmenting it with retrieval modules which fetch related documents and include the retrieval results into the input context of an LLM \citep{karpukhin2020dense, guu2020realm, izacard2022atlas, jiang2022retrieval, khattab2021relevance, Santhanam2022colbertv2}. Our work is complementary to these works as our extended context window allows more documents being included in the input. In addition, with an unmodified attention mechanism and model architecture, our method may be more versatile as it can natively handle tasks beyond retrieval oriented ones, such as long document summarization, few-shots learning, etc.

{\noindent \bf Recurrent Transformers and Memory Transformers.} Several works add memory capabilities to Transformers through recurrence, which increase the models’ capability of handling very long sequences \citep{bulatov2022recurrent, wu2020memformer, dai2019transformerxl, wu2022memorizing, martins2021inftyformer, mu2023learning}. One limitation of these works is that they only allow attending to a lossy compressed version of past inputs. \citet{mu2023learning} suggested that this may prevent models from remembering specific details in the past inputs. In contrast, our work allows attending to all previous tokens, preserving all details without compression, albeit with higher inference costs. \citet{mohtashami2023landmark} proposed landmark attention which allows full random access to any chunk of the input through introducing landmark tokens. Our work allows full access of the entire input through unmodified attention, which may be useful for tasks such as summarization.

{\noindent \bf Approximated Multi-head Attention.} There is a large body of research that focuses on decreasing the memory and computational complexity of the multi-head attention (MHA) mechanism through approximation or sparsification \citep{child2019generating, zaheer2020bigbird, beltagy2020longformer, wang2020linformer, choromanski2021rethinking, kitaev2020reformer, ren2021combiner}. Although not the focus of this work, as these methods are not used in LLaMA \citep{touvron2023llama}, we note that our method is compatible with most of them since our changes are restricted to position encodings, and not attention mechanisms.

{\noindent \bf Length Extrapolation.} A recent line of research aims to train Transformers models on short sequences and inference on longer \citep{press2022train, sun2022lengthextrapolatable, haviv2022transformer}. However, these methods have not been applied in some of the largest language models such as LLaMA \citep{touvron2023llama}, or OPT \citep{zhang2022opt}.  This has prevented them from enabling length extrapolation of many pre-existing pre-trained language models. Our work focuses on extending existing LLMs, which can save substantial pre-training costs. In addition, our method preserves the quality of the original models, even for small context window tasks, since it does not deviate far from existing definitions of position encoding or attention mechanisms.

{\noindent \bf Interpolation.} The most related technique to ours is proposed by \citet{dosovitskiy2021an} in their work on Vision Transformers, where the authors proposed to linearly interpolate learnt position embeddings to support higher resolution, which translates to an increased number of input embeddings, in the fine-tuning stage. The interpolated position embedding weights are used as initialization in the fine-tuning process for the newly added positions. Our work differs from their work in several ways (1) Instead of interpolating position embeddings, our method interpolates position indices, which is more suitable for RoPE like position encodings and may require less training since no trainable parameters are added. (2) We report successful results of extending the context window to 32 times while \citet{dosovitskiy2021an} explored up to 4 times. Our results extend theirs in exploring the upper limit of context window extension via interpolation. (3) We evaluated and confirmed the effectiveness of Position Interpolation for extending context windows for language models.

\ignore{
{\noindent \bf Extending Large Language Model's Context Window} Recently there have been a few works trying to extend LLM's context window length w/o re-running pre-training which can be prohibitively expensive. \cite{ratner2022parallel} directly extend LLM's context window length at inference time by chunking the input to smaller windows; \cite{mohtashami2023landmark} extended to 32k size by finetuning "landmark" tokens. \cite{kazemnejad2023impact} proposed to remove positional encoding completely to bypass the window size limit. Our work aims at extending LLM's context window without any assumption on specific downstream tasks; in other words, we shoot for extending context length while preserving LLM's general-purpose capabilities.  
}
We believe our results, in conjunction with \citep{dosovitskiy2021an}, provide empirical evidence on Transformer's remarkable ability of handling significantly longer sequences beyond training. 
Further, we conjecture that a method similar to theirs is directly applicable in LLMs with learnable position embeddings such as OPT \citep{zhang2022opt} and we plan to investigate this in the future.

\section{Conclusions}
Position Interpolation can effectively extend LLaMA models' context window to be significantly larger, using minimal fine-tuning.
The extended models are fully capable to perform a variety of tasks on the extended context windows, and preserve its original 
ability relatively well for tasks within the original extended models, making them good choices of generic language models 
for both long and short input prompts.
Further, models extended by Position Interpolation can reuse most pre-existing infrastructure and optimization, making this method
attractive in many practical applications.
We believe that Position Interpolation is a general method that could be apply to other 
types of position encodings, which can allow extension for more types of LLMs, and we plan to investigate in such directions in the near future.

\section*{Acknowledgements}
We thank Mike Lewis for his input on evaluation.

\bibliography{reference}
\bibliographystyle{iclr}

\def\dd{\mathrm{d}}

\newpage
\begin{center}
\textbf{\Large{Appendix}}    
\end{center}

\appendix 
\section{Proof}
\label{sec:proof}
\interp*
\begin{proof}
Using Taylor expansion, we have:
\begin{eqnarray}
    a(s_1) &=& a(s) + a'(s)(s-s_1) + \frac12 a''(\xi_1)(s-s_1)^2 \label{eq:xi1} \\
    a(s_2) &=& a(s) + a'(s)(s-s_2) + \frac12 a''(\xi_2)(s-s_2)^2 \label{eq:xi2}
\end{eqnarray}
where $\xi_1\in [s_1,s]$ and $\xi_2\in [s,s_2]$. Multiplying Eqn.~\ref{eq:xi1} with $s-s_2$ and Eqn.~\ref{eq:xi2} with $s-s_1$ and subtract, we get:
\begin{equation}
    a(s) - a_{\mathrm{linear}}(s) = R(s) := -\frac{(s-s_1)(s-s_2)}{2(s_1-s_2)}\left[a''(\xi_1)(s-s_1) - a''(\xi_2)(s-s_2)\right] 
\end{equation}
Now we bound the second order derivative $a''(s)$. Note that for any complex number $x$, $|\mathrm{Re}(x)| \le |x|$ so we have:
\begin{eqnarray}
    |a''(s)| &\le& \sum_{j=0}^{d/2-1} |h_j| |\phi''_j(s)| \le \sum_{j=0}^{d/2-1} |h_j| \theta^2_j \\
    &\le& \left(\max_j |h_j|\right) \sum_{j=0}^{d/2-1} c^{-4j/d} = \left(\max_j |h_j|\right) \frac{1}{1 - c^{-4/d}}
\end{eqnarray}
Note that when $x < 0$ and $c > 1$, $c^x \le 1 + x\ln c$, therefore $c^{-4/d} \le 1 - 4/d\ln c$ and we have:
\begin{equation}
    \frac{1}{1-c^{-4/d}} \le \frac{1}{4/d \ln c} = \frac{d}{4\ln c}
\end{equation}
So 
\begin{equation}
    |a''(s)| \le \left(\max_j |h_j|\right)\frac{d}{4\ln c} =: M
\end{equation}
Let the above bound to be $M$, we have:
\begin{equation}
    |R(s)| \le \frac{(s-s_1)(s_2-s)}{2(s_2-s_1)}\left[M(s-s_1) + M(s_2-s)\right] = \frac{M}{2}(s-s_1)(s_2-s)
\end{equation}
As a result:
\begin{equation}
    |a(s) - a_{\mathrm{linear}}(s)| = |R(s)| \le d\left(\max_j |h_j|\right) \frac{(s-s_1)(s_2-s)}{8\ln c}
\end{equation}
\end{proof}

\section{Visualization of quantities in extrapolation bound}
\label{sec:visualization}
As shown in Eqn.~\ref{eq:extra-bound}, the extrapolation bound contains the term  $B(s) := \sum_{k=0}^{d/2-1} |A_{k+1}(s)|$ where $A_k(s) := \sum_{j=0}^{k-1} e^{\di s\theta_j}$. Here we check how large the bound is. We use $\theta_j = c^{-2j/d}$ with $c = 10000$ and $d = 4096 / 32 = 128$ (LLaMA-7B setting), and Fig.~\ref{fig:b_bound}  shows that $B(s)/d$ almost always larger than $1$ and in many places it is much larger than $1$. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{B_decay-crop.pdf}
    \caption{\small The bound $B(s) / d$ decays with $s$. While the bounds goes down with large positional difference $s$, numerically $B(s)/d \ge 1$ and at many $s$ much larger than $1$ (the dotted horizontal line). Please check Appendix~\ref{sec:b-bound} for the source code used to draw the figure.}
    \label{fig:b_bound}
\end{figure}

\section{Code}
\subsection{Code for Fig.~\ref{fig:interp-vs-extrap}}
\label{sec:code-inter-extra}
{\small
\begin{verbatim}
# build basis function
d = 4096 // 32
theta = 10000
# Frequency computation, 
freqs = 1.0 / (theta ** (torch.arange(0, d, 2)[: (d // 2)].float() / d))

# construct basis function 
L = 2048

x = torch.zeros(L)
x[:L] = torch.arange(0, L)

# basis functions
xfreq = torch.outer(x, freqs)

y = torch.randn(x.shape[0])

# do linear regression
X = torch.cat([xfreq.sin(), xfreq.cos()], dim=1)

eps = 0.000
coeffs = torch.linalg.solve(X.t() @ X + torch.eye(X.shape[1]) * eps, X.t() @ y)

x2 = torch.arange(0, 2*L)
xfreq2 = torch.outer(x2, freqs)
X2 = torch.cat([xfreq2.sin(), xfreq2.cos()], dim=1)

y2 = X2 @ coeffs

x3 = torch.arange(25, 75, 0.125)
xfreq3 = torch.outer(x3, freqs)
X3 = torch.cat([xfreq3.sin(), xfreq3.cos()], dim=1)

y3 = X3 @ coeffs

plt.figure(figsize=(16,5))

plt.subplot(1, 3, 1)
plt.plot(x2[:L], y2[:L], "r")
plt.scatter(x, y)
plt.ylabel("attention score $a(s)$")
plt.xlabel("Positional difference $s$")

plt.subplot(1, 3, 2)

plt.plot(x2, y2, "r")
plt.scatter(x, y)
plt.axvline(L, color="k", linestyle="--", linewidth=0.5)

plt.title("Effect of Extrapolation")
plt.xlabel("Positional difference $s$")


plt.subplot(1, 3, 3)
plt.plot(x3, y3, "r")
for i in range(25,75):
    plt.axvline(i, color="k", linestyle="--", linewidth=0.5)
plt.title("Effect of Interpolation")
plt.xlabel("Positional difference $s$")
plt.show()
\end{verbatim}
}

\subsection{Code for Fig.~\ref{fig:b_bound}}
\label{sec:b-bound}
{\small
\begin{verbatim}
L = 2048
x = torch.arange(0, 2*L)
d = 4096 // 32
theta = 10000
freqs = 1.0 / (theta ** (torch.arange(0, d, 2)[: (d // 2)].float() / d))

xfreq = torch.outer(x, freqs)

mags = (xfreq.sin().cumsum(dim=1).pow(2) + xfreq.cos().cumsum(dim=1).pow(2)).sqrt()

plt.plot(mags.sum(dim=1)/d)
plt.axhline(1.0, color='k', linestyle="--")
plt.xlabel("Positional difference $s$")
plt.ylabel("$B(s)/d$")
plt.show()
\end{verbatim}
}

% \include{appendix_7_news_summary}

\end{document}

