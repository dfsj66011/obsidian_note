
两种类型的智能框架：无代码框架和基于代码的框架。无代码框架，比如 n8n，基于编码的框架，比如 LangChain、LangGraph 和 LlamaIndex。

两种评估智能体的框架：Langfuse 和 arize；

四篇论文：CoT，ReAct，Toolformer，Generative Agents

从 A 点到 B 点的价值、工具、决策、规划、外部环境、记忆以及人在循环中的反馈。

---------

所以，我们所有人都在2022年和2023年与GPT-3.5互动过。我相信，这个模型既经过了预训练，也经过了微调。这两个步骤都需要完成，而预训练的成本非常高。仅GPT-3的预训练就花费了大约460万美元，而GPT-4以及参数更多的模型成本会更高。因此，目前预训练基础模型的工作主要由那些拥有足够资金和远见的实验室和初创公司来完成。这就是从头开始构建一个大型语言模型的基本流程。在整个工作流程中，我们没有在任何地方教授语言模型语法知识。它完全是通过训练数据量自行掌握的。因此，这个经过预训练和微调的大语言模型（LLM）就成为了智能体的"大脑"。为什么它能胜任这个角色？因为这个经过预训练和微调的LLM现在对各种文本相关任务都表现出色——无论是文本摘要还是分类任务都游刃有余。比方说对文本进行情感分类分析，然后翻译各种内容对吧。在上一节课中，当我们研究不同类型的智能体及其需要执行的任务时，所有这些任务都涉及处理语言。因此我们需要某种擅长理解和处理语言的模型，这种模型就是大语言模型。我先暂停一下，看看大家有没有任何疑问或问题。你们可以在聊天区继续提问，我来回答一些问题。



if if you feel it's important enough if web scraping is the source to knowledge and nowadays everyone uses chat gpt yeah so one more thing is that as we are interacting with the LLMs right that itself can serve as the data for further training and i think some of you are also interested in what actually happens in the training procedure right so in the training procedure let me break it down a bit what what really happens and those of you who are not interested in this or who think it's a bit complex don't worry about this but i'm just showing the whole workflow let's say if you have huge amounts of data right for the sake of simplicity let's assume we have to build a harry potter specific LLM we have to build a harry potter specific LLM which understands and can answer all questions related to harry potter right so what we will do first is that we source harry potter data let's say i take this pdf just as a reference um and i use this data and i collect it right once we have all of this data then what we are going to do is that this data which we have that's tokenized by tokenization i mean it's broken broken down into smaller and smaller parts and then each of these tokens pass through the entire LLM architecture and the LLM architecture consists of the attention layer there are many layers which are stacked together the normalization layer and then there is an output layer let's say so you can think of this block as the LLM architecture block and then what comes out of this is the next token prediction so this is the predicted next token and then we compare with the actual next token so we have the predicted next token and the actual next token there is a loss term which is calculated between this that's the categorical cross-entropy loss and then we do back propagation so then we do back propagation to minimize this loss and what this means is that let's say there are 175 billion parameters in this model all of these parameters are optimized during back propagation so that the next tokens are predicted correctly that is the entire so when i say pre training it's this entire workflow which happens so if any of you are actually interested in this right then this build build LLM from scratch playlist which i have it's this one this playlist has 43 videos which teach you exactly this workflow which i right now showed so my main purpose in today's lecture was to give you high level overview of what large language models are so that when we when we use the term LLMs in future lectures you know exactly what i mean but if any one of you want to dive deeper into this and actually know how LLMs are pre-trained then you will need to spend a significantly more amount of time understanding this um so there is a question 175 billion parameters what are these parameters these parameters are actually all kind of neural network layers then if you have queries keys values matrices there are several places at which these parameters are present the simplest way to think about it is y equal to x plus b right here a and b are parameters so here the input input data let's say is your x and the next token is y all of these parameters which come in between x and y those are your 175 billion trainable parameters there is a question that are agents built on top of generative ai or ml layers that's an interesting question so see now the reason we covered this is because if you see uh yeah so let's say uh let's say the same example of trip planning right and what we want to do is that let's say on day number one we are going to barcelona and i want to decide where we are going to on day number two we need a model which can understand what is mentioned on websites we need a model which can calculate distances so it needs to have access to certain tools and it should essentially understand what's written on web pages so there has to be a language layer so you asked about machine learning right the reason why agents use a language modeling layer is because all these other so if you consider deep learning right deep learning is a very vast field which also has many things that have nothing to do with language but agents agents are specifically built on top of large language models because language is the thing which helps you to reason and to plan so if we look at this definition of agents right this definition of agents agents can plan and take decisions how is this possible how will they be able to plan and take decisions if they don't understand what's written on web pages if they don't understand what's written on booking websites if they don't understand human feedback so as a human let's say if you've given feedback through voice or text the agent needs to understand what's your feedback right they cannot do all of these things unless they have a language layer underneath so that's why we say that the brain or foundation of an agent is a large language model and now when you say large language model don't just think of chat gpt people who have not understood llms just associate llms with chat gpt but when i say when i show you this figure and when i say brain or foundation of an agent is a large language model right i specifically mean language models which are pre-trained and fine-tuned and when i say pre-trained it means that whole workflow of tokenization predicting the next token etc and then fine-tune and when you think of llms you should know that they are trained to predict the next token but as a by-product they have become very good at understanding form of the language they have also understood meaning of the language and as a result they are able to draft emails they are able to understand the sentiment of a given piece of paragraph or a given piece of text etc so this lecture is kind of like peeking behind the curtain and really trying to understand what llms are in fact in our life course build large language models from scratch this is our first lecture or rather the first two lectures that's a 25 lecture course but in this lecture i always like to give this because here we teach things from first principles actually one more book which i would like to share with all of you is by wolfram and it's called how chat gpt works it's one of the best books i've read on how gpt works and in fact so let me share this book this is that book it's freely available and i really encourage all of you to read this book because it's absolutely incredible so some of the ideas which i've got at the start of this lecture right like more options than the number of particles in the universe etc it's been inspired from this book it's just 79 pages and it's a two hour read but it really breaks down what chat gpt is from first principles i think that if all of you have this knowledge you will just get better at building agents and understanding agents okay let's see so i can take a few more questions now from the chat parameters are basically weights and biases yeah that's correct but they show up at various places how come deep seek with 13 billion parameters perform as good as 175 billion gpt model what other factors are involved in the performance that's actually a very complicated question because the answer to that would require so essentially the reason deep seek is so good is because they have several things so first they have kv cache optimizations they have something called multi-head latent attention they have rotary positional encodings integrated with latent attention all of these things are there so i won't get into this right now but the short answer is that they have implemented a lot of architectural innovations actually that's what makes deep seek so much better so there is a question on small language models right that's actually an interesting question because let me try to explain my thinking about the way the llm space has evolved right so there was a scaling law at the start there was a scaling law and everyone was interested in this that the more you parameters you have the more better you get slowly start that started reaching a plateau then what happened is that businesses started figuring out that if i have a specific use case i don't need the all the data in the world to train my language model i will start with a small amount of data i will start with a small amount of data and i will use a language model which has only 10 to 50 million parameters and then they figured out that even such a small model leads to great results if you have specific small amount of data these language models which are much smaller than their larger counterparts are called as small language models and small language models are extremely effective in enterprises the reason is because enterprises generally want their own corporate brain so they cannot use chat gpt they want to train the language model which has knowledge about their company about their data and their specific operations which need to be performed so small language models are the future in that space i believe if any one of you is further interested you can you can watch this three-hour workshop

This is how to build a small language model from scratch. So I've recorded. I recorded this three hour workshop three weeks back, but before doing that, I really encourage you to watch this playlist because this starts at slightly higher level, but it teaches you how to build an SLM from scratch actually.Um, okay. Let's see what other questions are there. Moore's law.Yeah. You can think of scaling law as slightly similar to Moore's law, just in the reverse direction. How much math is required.So for these agents bootcamp in particular, I'll try to cover everything from first principles. So I'll tell you the plan going forward. Um, until now we have covered the introductory lectures, right? Which are, uh, LLMs and LLM agent.In the next lecture, what we are going to do is that we are going to see how an agent works. And we are going to write our first code in the next lecture for building an agent. Um, and there, what we'll do is that we peek into the inner workings of an AI agent actually.Because when I say that agents have large language model as their brain, you might be thinking, what does this exactly mean? Right? You, you might have understood what LLMs are, uh, but you, you would not currently see the integration of an LLM into an agent, uh, for that. We need to understand something called as the react framework. So react framework, I shared this paper actually in the previous lecture, if, uh, if you saw, but yeah, this is that paper we need to understand this in this paper.We'll actually learn how language models are integrated with agents or how language models can be equipped to plan and reason. So there is something called reasoning and acting, which is react, which we'll be looking at in the next lecture. Uh, okay.So next lecture, now this lecture has come to an end. What I'm going to do is that next lecture will be held tomorrow, which is Tuesday. And in the next lecture, we are going to cover how an AI agent works.We are going to cover this react framework. We are actually going to, uh, see our first agent in code. Now for the rest of this lecture series, it's very important that all of you are regularly going to be attending lectures.You should have an open AI. Uh, you should have an open AI API key. Um, so that is the purpose of this.And mostly what I'll be trying to do is that many of the lectures I've constructed with open source elements, but a huge number of participants in this call are industry professionals. At least those participants who are, uh, who are from under enterprises, you can, you need API keys for students for students. I would say go ahead with free API keys, but people who are joining from enterprises or people who are industry professionals, uh, you definitely need to have your open AI API key.So that's going to be, uh, extremely important moving forward. Other all other API keys, which we'll create will be free. So we need API keys for several other platforms like hugging face.We'll need API keys for LLM and agent monitoring platforms. Those are free. Uh, but, but please have them ready so that you can follow along the different coding lectures, which we are going to conduct tomorrow's lecture.We'd also have coding in Google collab. Um, so make sure that you have collab. We don't need any pro plan subscription, but don't join tomorrow's lecture from mobile phone.And let me iterate this once more tomorrow's lecture. We'll deal with some code. So many of you who might be joining through mobile phones, please don't do that because then it's not the best effective medium for understanding.It's very difficult to run code files on phone. Um, so just make sure to avoid that tomorrow's lecture. You need a Google collab open API key might not be needed in tomorrow's lecture, but it will be needed in one of the subsequent lectures, but just have it ready tomorrow also.Um, also, uh, people who are on the pro plan, you will receive three things after every lecture. You will receive the substack. You will receive the notes and, um, assignments.So I've shared the first assignment with all of you on discord, right? Uh, so it's important that you do that assignment and submit the solution on the discord group itself. And plus after each lecture, you may have certain doubts, right? So those doubts you can ask on discord and I'll clarify them. Um, to summarize the key takeaways from today's lecture, here, here is what you should take away from today's lecture.First language models are next token prediction engines. Um, language models are next token prediction engines, and they are not deterministic prediction engines, but they are probabilistic. Um, so they have a certain confidence score of what the next token should be.And they choose the next token with the highest confidence or the highest probability of being the next token. Now, why we retrain a language model for predicting the next token. They learn the language itself as a by-product, which is the form and the meaning of the language.And language models are called large language models because they have huge number of parameters. And why do they have a huge number of parameters? First is because they get emergent properties. As we increase the parameter size, language models magically start becoming better at new tasks.Second reason is that as kids, uh, certain neuronal connections are formed in our minds as we learn the language. Similarly, for a language model, learning a language is not easy. That's why they need so many parameters, which are flexible, which can be tweaked.Uh, so that they can effectively learn the language. Then we saw that the core of language models is of course, this transformer architecture. And we saw the difference between LLMs, generative AI, deep learning and machine learning.And finally, we saw that when we look at LLMs for any large language model, which we are going to use, it has two stages. It has pre-training and fine-tuning. Fine-tuning comes after pre-training.The reason it's called pre-trained LLMs, it's a bit misleading because we already trained it before. The reason it's called pre-trained is because there is also one more stage, which is the fine-tuning stage, which is going to come afterwards. So it's in the anticipation of the next stage, which is going to come.That's why it's called pre-training. I also shared this Google Colab notebook with all of you. Those of you who are students, just replace the models with Gemini models and use free API keys, but industry professionals and people joining from enterprises, please have your API key ready so that we can do tomorrow's lecture efficiently.We'll need to create accounts on many different things and I'll do that on the go. So you will need an account on Hugging Face. So just make sure that you have an account, which is created on Hugging Face.If you don't have it right now, it's fine. I'll take you through it as we are going through the lecture. Then what else I might be forgetting some things, but I'll cover it as and when we go through that particular lecture.And then finally, I hope many of you will now start making the link between lecture number one and lecture number two. In lecture number one, I talked about this foundational layer of agents, right, which is a language model. And in lecture number two, we have seen what that foundational layer is.And I hope you think beyond that GPT, when you think of a language model, try to think from first principles, like here is how I relate it to physics, right? Because physics is what comes naturally to me. So I look at physical models and then I try to relate to language models. I think that's always a better way to learn rather than learning just directly from applications.OK, so now that we know agents and LLMs independently, I will look forward to seeing all of you in the next lecture, in which we learn how agents actually work and the React framework in particular. And again, let me repeat one more thing, if any of you is actually interested in going deep, make notes along as you are learning, because one thing which I have learned myself is that as I make notes, I get better at the material. Second thing which I've learned is what I also covered in the Discord group assignments is teaching, the benefit of teaching.So when I say teaching, what I mean is that if you really have understood something, you should be able to teach it to someone who has no clue about that topic. And if you are not able to teach it, it means you have not understood it. That's why I started explaining LLMs from very simple terms.So if you want to teach, let's say language models to a 10 year old, how will you teach you? You cannot use difficult words, right? So that's one of the best, best ways to teach or best ways to learn rather. So I call this learn by teaching, and I don't know why this method is not implemented in universities, right? In all of our college, during undergraduate, graduate, I gave exams and they judged me for my marks, but instead of there were teaching assignments. So if there was an exam where you teach your students and we check how well the students have understood it, I think that's a better indicator of whether you have actually learned the concept or not.So as an exercise, try explaining language models to someone who has no clue about this field. And if you are successfully, you are able to explain it successfully, then maybe you have understood it well. Thanks, everyone.

I will look forward to seeing you in the next lecture, which will be at the same time, 2 p.m. tomorrow, same Zoom link. So I look forward to seeing you then. Thanks, everyone.

Bye.

All right, so let's get started with today's lecture. Today is day number three of the Vijuara AI Agents Bootcamp and today is going to be an extremely important lecture because today we are actually going to look at the theory behind agents and we'll also be seeing our first AI agent on Google Colab like I mentioned to all of you yesterday. So before we get started actually let me do a quick recap of what all we have covered on day number one and day number two.

So let's go to day number one. The main takeaway from day number one was that agents take you from point A to point B and they add value to your life value being time and money. Agents can also plan and take decisions and they do that through the usage of tools.

Furthermore agents can interact with their environment and they have memory and in the previous lecture we saw that the foundation of agents is something which is called as a large language model. So for the people who have registered for the pro plan I have summarized all of the lecture content in of the previous lecture in this substack article. So we started the lecture by assuming that if an alien enters town and if they want to predict the next token how will the alien go about doing this.

Statistical or probabilistic methods are quite difficult here so that's why we turn towards models. So large language models essentially take sequences of words as input and they predict the next token. That's the main idea and as they are trained to predict the next token they also learn about the form and the meaning of the language itself.LLMs have this magical property that after a certain number of parameters they start developing emergent properties and that's why people continuously chased increasing the number of parameters in a language model. And then finally we saw that there are actually two stages of building a large language model. The first is pre-training and the second is fine tuning and once both of these stages are completed we have a language model which can completely understand human language which can do translation tasks summarization tasks essentially all kind of language related tasks which we want.But if you are following along closely all of you might be thinking that what is the exact link between LLMs and agents right. So I'm bringing this screenshot over here right now. All of you might be thinking that what does it mean that the brain or a foundation of an agent is a large language model.

How do we assemble an agent with the help of large language models. Today we are going to see exactly this. We are going to see how you start with a large language model and then how you can augment the large language model so that it can plan take decisions and it has access to tools.

We'll also see how you can augment large language models so that they interact with their environment and have memory. So you can think of this lecture number three as a lecture number one plus lecture number two and here is where everything will come together. So for now you might be thinking of this aspect and this aspect a bit separately.

In this lecture we'll merge it together and I'll actually show you how LLMs can be augmented to give them all the functionalities of an agent. But before we understand how agents are actually constructed from the ground up and how LLMs can be augmented to give agents. I first want to show you an AI agent in action which means we are going to code an AI agent ourselves right now and I'm going to share this Google Colab code file with all of you right now.

I've mentioned this as Vijuwara agents course code number one. We'll eventually have eight to nine code files like this and so in today's lecture our aim will not be to understand the code itself. I'll do that in the later lectures.

Our goal will be to run the different code cells and I'll give you a brief overview of what these code cells mean and see the output. In the later lectures we are going to dive into code based frameworks and no code frameworks where I'll actually explain how the code really works. But in today's lecture all of you should be able to run this Google Colab code file with me.

So I'm going to share this right now with all of you on the chat and if some people are joining late you can share this link with those people who are joining later. So Google Colab link is this one. All of you can click on this link and open it.There is one thing which all of you will need which I mentioned in the previous lecture as well and that's the OpenAI API key. So those of you who have their OpenAI API key handy you can paste it over here in between the inverted commas and let's get started with executing the code step by step. So let's run the first code cell right now.

So I've already run it. So I'm just going to wait for a couple of minutes because this first cell takes some time to run. So as long as so if anyone finishes running this first code block can you just let me know in the chat if you have finished running this first code cell and when I say finish you should see a green tick which is appearing over here.

In the first code cell we are just installing some packages such as langchain, OpenAI etc. Don't worry if you don't know what these packages mean I'll cover that in one of the later lectures. As I mentioned today's aim is to just run this code and see an agent in action so that you get a feel of what's really going on over here.So there is a question in the chat. Do we need to change the imports for Google Gemini API? So we are going to use this package called chatOpenAI which is specifically for chatGPT. For Gemini the code needs to be changed slightly so I'll mention those details after the class is done but for now we have to stick with chatGPT and you have to install these required modules before we get started.

So just to clarify langchain is the main framework in which we are going to develop our first agent today and if you can how many of you are aware about langchain in the first place? If you have heard of langchain or if you have used langchain before you can just type yes or no in the chat. So yeah so it's one of the most famous platforms for not just building agents but essentially building applications on top of large language models. I'm going to cover this in detail in lecture number 5. All right so how many of you have finished running the first code cell? Can you type in the chat if you see a green tick over here.

I can see that four to five people have finished running it that's very good. So this part of the code will take the longest time rest everything will be a bit straightforward. Now as we move next we will need to put in the API key over here.

So if you directly I've already put in here so it does not show an error but for others you will need to put your OpenAI API key here and then just run this code cell. Has anyone successfully run this second code cell by inputting their API key? Yes okay good. I can see that many have run this.

For those who are joining late I'm just sharing this Google Colab code file link once more so you can follow along with the rest of us. Great so then we are going to move forward there is something which is called as react template and something called as prompt template. Again don't worry about this right now just run this currently.

Here we are essentially defining the prompt which needs to be given to the large language model and in this code cell what we are essentially doing is that we are giving tools. We are giving two tools to the large language model the first tool is called DuckDuckGo search. How many of you are aware of DuckDuckGo? So if you search DuckDuckGo right it's a privacy first company but it's you can think of it as Google search but with your privacy protected.

So the first tool which we are giving to this agent is that it can go to any website and search what's mentioned on that website that's the DuckDuckGo search tool and the second tool which we are going to give this agent is LLM math which essentially means that we are equipping this large language model with a calculator and other mathematical operations and the large language model which we are using here is GPT 3.5 turbo. You can change this to whichever one you want but we can stick with this for now and then what we are doing is that we have to we are essentially loading these tools so we are collecting the tools together so essentially we have the search tool and we have the mathematics tool we have two tools which have been given to the language model and then we have to just create an agent. So here we have to mention that this is the agent it has access to these tools etc and then finally the question which we'll ask to the agent is what is the current price of a macbook pro in usd how much would it cost so this is the question which we are asking to the agent so let me actually take a screenshot of this and bring it over here so if this is the question what is the search what is the current price of macbook pro in usd and how much would it cost in euros if the exchanger it is 0.85 so let me ask this question to all of you what tools would you need to solve this question let's say you have the large language model which is chat gpt or any language model what are the tools you will need to answer this question what will you need to do if i ask you that what is the current price of a macbook pro and how much would it cost in euro if the exchange rate is this much as we can see already right we need essentially we definitely need a web search we we essentially need a web search because we have to go online and we have to type what is the current price of macbook pro this is what a human would do right if this is the question asked to me i would go and search on the web macbook price in usd um so that's the first that's the first tool which is needed and then what's the second tool which is needed let's say i get the price as thousand dollars the second tool which will be needed is a calculator why would a calculator be needed because i have to convert this thousand dollars into euros and for that i will be multiplying it with 0.85 so to convert this large language model to an agent i essentially need i need two tools i need a web search and i need a calculator so that's exactly what we are doing here right the tools which we have defined are the mathematics operations and a duckduckgo web search engine these are the tools passed to the and if you click on this agent executor dot invoke so i think i have to i did not put my openai api key over here so that's why i got this error so let me just go ahead and so now i have put my api key and you can see here now let me go and invoke this once more so now you see what's going on over here here we can see that the agent let's let's see what the agent is doing the agent first figures out that i should use a web search engine to find the current price of a macbook pro in usd and then i should use a calculator to convert it into euros the agent then takes an action the first tool which it uses is duckduckgo it uses this tool and it goes online to find the current price of a macbook pro and then it understands that the prices of a macbook pro start at one nine nine nine one nine nine nine dollars and then the second tool is a calculator which the agent figures out that's the second tool which we should use and then you multiply this one nine nine nine with 0.85 to get the price in euros right so this is now the price of the macbook pro in euros so let me again hide this once more so one simple thing which we can do actually is create something called secret keys in google collab the reason i am not doing this here is because it's sometimes confusing to some people so here you can go and create secret keys like this so your api will be stored as a secret key but first how many of you were able to run until here and see the output which i am seeing on the screen right now yes so you can mention how many of you okay so i can see that many were able to run it but just got a different answer right and that's totally fine it's fine if you got a different answer as long as you are able to run this code um all right so just see what's going on here right now just like how humans are thinking here we have an agent who is thinking along who is thinking along very similar lines the agent first uses the web search engine to search about the current price of a macbook pro it gets that price and then it realizes that i need to convert this into uh euros um and then uh it uses the calculator tool and converts the pricing into euros and then it says that i now know the final answer uh the current price of a macbook pro is one nine nine nine one nine nine nine dollars and it would cost approximately 16.99 euros with an exchange rate of 0.85 so i deliberately went a bit fast through the code i just wanted you to get a feel of how agents are working but now we'll again jump back to our miro whiteboard and we'll try to see what exactly did we see currently in code what what really happened right now currently all of you would see that we had a large language model in the code and that language model was chat gpt so now all of you have run the code it's great all of you have not it's fine you can do it after the lecture is over now it's time to focus on the main theory part okay so pay a lot of attention to what's going on right now because i'm slowly going to explain what we just saw in the code all of you saw that we used a large language model and that was actually gpt 3.5 okay then we also saw that there were tools which were used the first tool which was used was a web search and the second tool which was used was mathematical operations like let's say calculator this is what we saw right but this is the output which was produced and it seems like there is really a lot of thought which is going on the agent thinks like a human the agent formulates its thoughts and then it takes actions so what exactly is going on here and why do we get descriptive answers like this where exactly is a large language model used in this process did we see something magical let's deconstruct it now once we understand it it will no longer be magical we'll now take a step back and we'll understand two things which have led to this answer the first concept which we need to understand is tools and what exactly are tools and the second concept we need to understand is something which is called as a react framework so until now in this bootcamp i've used the term tools very vaguely but it's actually one of the most important things which convert a large language model into an agent um okay so let us understand what tools are and how we can supply tools to a large language model so that it becomes an agent okay so the simplest way i like to think about it is tools essentially are just weapons which a large language model receives so that it can interact with its environment and tools can come in various forms there can be a web search tool there can be a tool for image generation there can even be a tool for let's say code generation uh code evaluation there can be a tool which retrieves documents um there can be a tool which interacts with an external api such as github youtube spotify etc so essentially tool is any such thing which allows an llm to interact with its external um environment okay uh tools so we have only seen web search and the calculator tool but subsequently in this bootcamp we will see all of these tools which will be utilized but then let me ask you the question that okay these are the tools and we need to pass a tool to the llm how do we let the large language model know that it has to use the tool um how do we tell the large language model that this is a tool which um which you have to use so lang chain so someone has answered lang chain that's not uh so always think from first principles right lang chain is a package which is developed but the question which i am asking is the answer is not react framework also i have not taught these things yet so try to think from first principles uh let's say you have a large language model how do you tell the large language model that it has to use the tool and the answer is actually very simple i i hope someone answers that in the chat yeah exactly uh now i think we are getting to the correct answer right the answer is only one word and it's as simple as the prompt so agents the more you learn about agents the more you will understand that um llms and prompt engineering are extremely important to really understand agents and here is why so if you have a language model right now let's say jat gpt right what is a prompt a prompt is anything which i type so if i type hey llm here are the tools which you have access to i want you to use these tools and then we provide the list of tools to the language model and this is the simplest way i can explain how large language models can acquire tools and then they can execute actions based on these tools and i'm going to show you now how such a prompt is actually augmented but please keep in mind that a good tool complements the power of a language model if an llm struggles with math you give it the calculator tool if the language model cannot find latest information you can provide it with a web search tool if a large language model is struggling so let me ask you a question if the language model is struggling with finding the correct weather what's the tool which you will give if you have a language model and when you ask the weather of a place and that weather is answered incorrectly by the language model what's the tool which you will give to the language model in such a case you will probably give a weather api right what i mean by weather api is that you will access to some api which the llm can request the answer from and then get the answer from that api if you don't know what an api means just think of it as websites like let's say acuweather so websites such as acuweather essentially have apis which you can use so the llm can ask this website that hey can you help me to get the answer which is requested by the user because i am not able to answer on my own so that's how tools complement the power of a large language model but now i want to show you how do tools actually work so as we saw yesterday large language models can receive input texts and generate output right these input texts which a large language model receives it's called a prompt these input texts which a large language model receives it's called a prompt and unless we mention some things in the prompt large language models cannot call tools on their own so we need to tell the language models that hey llm this particular tool exists and here's how you have to use this particular tool so the way agents or any package like lang chain or any agentic framework is actually written is there is a lot of prompt engineering which goes on in the background so let's say if you want to make an agent which answers this question about weather right what is the weather in paris so you first have a language model and then the language model has access to the weather tool now in the prompt it will be written that you have access to these tools then what the language model will do is that the language model will make a tool call what tool call means is that it just calls this particular weather tool or write this text that this weather tool needs to be called the language model it has now become an agent because it's equipped with a tool call so the agent reads this response and the agent executes the tool call on the language model's behalf and then we get the weather data and then the language model uses the weather data for further uh for getting further responses so essentially uh if you see it's the whole process is actually quite simple you have a language model at the initial stage then the language model if the question is about weather right then there is some text which is generated which calls that particular tool let's say that text is something like call the weather tool now it has become an agent because it has access to this tool and then that api call will be made and then we receive the response that response is again received by the language model and then the language model prints the answer so tools the way tools are integrated into language models is through prompt engineering we mention it in the prompt itself that this is a tool which which has to be used and the way it's done is that so here i've taken a screenshot of how lang lang chain or other packages actually work in the prompt we have to mention that you are an ai assistant so let me mark this with the yellow color in the prompt it's mentioned that you are an ai assistant designed to help users efficiently and accurately your primary goal is to provide helpful precise and clear responses and it's also mentioned that you have access to the following tools and then you mention a description about the tool that's very important you have to mention that what the tool really does and what are the inputs which the particular tool expects so this is how language models actually use tools and even when you think about tools there can be two types of tools if a certain tool does not exist so i mentioned a list of tools over here if a certain tool does not exist you can create a custom tool and by custom tool i mean you can create a simple functionality which is a tool which you can provide the language model in the prompt itself so tools are actually one of the simplest concepts of ai agents because it's nothing but prompt engineering so if you go back to the start of the lecture now and see what we we started thinking about how are large language models augmented to achieve all these other things right so until now we have seen that we start with an llm and through the prompt itself we give the language model access to tools so the first part we have now understood how does how do language models have access to tools we mentioned it in the prompt itself that's step number one so my takeaway after so now i have built and deployed so many ai agents that the simple takeaway after understanding how tool works is that prompt engineering is extremely important because it's how you convey in the prompt that hey llm here are the tools which you have to use and if you go back to your code right now if you scroll up um just look at the start of the prompt template which we have to the language model which we have defined here we have mentioned that answer the following questions as best as you can and you have access to the following tools and then we have provided tools to the language model this is the first aspect of how language models became or have or use tools to become an agent that's part number one so let me pause here for a while and ask does anyone have any doubts or questions in the first part so remember i mentioned that we need to understand two things to answer the code the first to understand the code the first is tools um and if there are no doubts in this section then i'm going to move to the second section which is react framework but i'm waiting here for a while to to see if there are any questions um there is a question here can we make our own tools yeah we can define custom tools is llm able to handle all formats in which data comes from the api so in this case


The response, which is the format in which the response is going to come, or if your LLM can only handle specific format, we have to ensure that the response is in that format before augmenting the prompt. Can the large language model handle tools without us including it in the prompt? So it has to be mentioned somewhere, otherwise the language model does not know really what to do. It's only what we mentioned in the prompt, when we mentioned in the prompt, the language model knows that, okay, these are the tools which I have to use.Okay, let's see, are there scenarios where we need, do we have to specify system prompt for the agent? Yeah, we have to specify system prompt to the agent. Then how does chatGPT know how to use the weather tool then? Because probably it's internally configured, that if the user asks questions about weather, make a call to the API, but here we are actually looking how that internal thing works, right? So if chatGPT should not be your basis for understanding agents, we should look beneath that, okay, if chatGPT is making a weather API call, how would they have coded it internally? So what I'm mentioning right now is we don't have to write this for every single agent, that's why we use frameworks such as langchain. So in langchain, when we, langchain already has defined some tools for us, so we don't have to write too many custom tools.And langchain already comes with predefined templates like react agent, etc. So that makes our job easy. Is NLP a part of it? So NLP is the broader umbrella, right, under which we have large language models.So definitely, this, this, all, all these lectures come under natural language processing by default. Okay, now let's move to the second part, which is understanding the react framework. And why do we need a framework like react? Because if you see very carefully, we started with this screenshot, right? And until now, I've told you that agents have access to tools, and how agents use tools, but we have not learned about two things.How can agents plan and take decisions? And how can agents interact with their environment and have memory? How do we go about doing this? Because until now, I just explained to you how an LLM can receive tools in its prompt and use those tools. But I did not explain how can agents plan and take decisions, and how they can interact with their environment and have memory. So the underlying framework to help agents do this is called react framework.And I mentioned this paper at the start of the first lecture itself. But let me share this paper in the chat. This is one of the pioneering papers from I think it came out in 2022 or something.Yeah, react. So this is the paper. I would encourage all of you to just take a look at it.So if you see, it's called react because it's reasoning and action. So re stands for reasoning and act stands for action. And let me deconstruct this framework to you step by step.This framework will eventually help agents interact with the external environment and help them to plan and reason. So here is how the react framework was actually born. We initially started with something called chain of thought prompting.And chain of thought prompting essentially helps language models to answer as if they are thinking and as if they are planning. So let's say if you have a question that Roger has five tennis balls, he buys two more cans of tennis balls and each can can have three tennis balls. How many tennis balls does Roger have? So actually to do this calculation, we all know that it's five plus two multiplied by three, right? That's our reasoning.So the answer is 11. But in the first case, we only say that the answer is 11. And then we ask a new question.The cafeteria had 23 apples. If they use 20 to make lunch and bring six more, how many apples do they have? So in the first case, which is standard prompting, the LLM answers incorrectly. But then people figured out a method which is chain of thought prompting, which means that in your prompt itself, in your prompt itself, if you tell the LLM how reasoning is done, the language model mimics that in its responses.So for example, if the same question which we have, Roger has five tennis balls, he buys two more cans of tennis balls, each can has three balls. How many tennis balls does he have now? And we actually show the calculation that Roger started with five and then two cans of three, each is six. So five plus six equal to 11, the answer is 11.We include this in the prompt itself. And then we ask the new question. And you will see that when the LLM answers now, the cafeteria had 23 apples, you'll see the answer is first of all, much more detailed.And it's as if the LLM is planning. The language model is planning as it answers. So this clearly shows that if we actually include some thought process in our prompt itself, the language model will answer as if it's planning.And we want planning capabilities in our agent. We want planning capabilities in our agent. That's one of the most important things which we have seen in an agentic definition, right? We want agents which can plan and take decisions.So chain of thought, this paper, a chain of thought prompting was a huge success. And this technique actually proved that language models are capable of generating reasoning traces or thoughts. The answer here, which you are seeing here, these are also called reasoning traces, because it looks like the language model has its own thoughts and its reasoning through those thoughts and coming to an actual answer.But chain of thought prompting had some issues. It does not have access to any external world, so it cannot act, which means it cannot take actions. We have reasoning traces, that's good, but we also want a language model to take actions on top of this.There are some questions, few short prompting, so basically single short prompt. So this is, you can call this as a few short prompting, because we just give one single prompt. And in this case, even a single prompt is sufficient.A single, I mean, a single additional example is sufficient in this particular case. But the disadvantage of chain of thought is that it lacks access to the external world and it cannot act. That's where the act of react comes into the picture.It builds on top of chain of thought, react builds on top of chain of thought, but on top of that, it adds this acting capability. And the way react framework does that is through something called the T-A-O principle. What the T-A-O principle means is, it's thought, action and observation.And actually it's a loop. So I should actually draw it as follows, T, thought, action and observation. It's in a full loop like this.So you may be confused that what does this exactly mean, right? Let me show you with the same example which we saw. Let's say the question is the same question which we start with, right? This question that what is the current price of a MacBook Pro? How much would it cost? Let's say that's the question given to an agent. The thought action of the thought action observation framework goes like this.The first thought which the agent will have is I have to check the MacBook prices. Based on that, the agent will take an action. That action is let me do a web search and then it receives an observation.Which is the price seems to be in dollars. Now here is the most important thing about agents. Agents don't stop here after one loop.They keep on repeating this loop until they get the correct answer. So price seems to be in dollars. That's the current observation.On top of that, I will have my second thought. Which is the price now needs to be converted into euro. My second action is let me use the calculator tool.And my final observation is the price is in euro. I am now satisfied. So you see this loop.This loop keeps on happening until the agent is satisfied. And this is one of the biggest differences between language models and agents. A language model will just generate an answer and stop over there.But in this thought action and observation framework, the agent keeps repeating this loop until it has an answer which it thinks is right. If it's not, it will go to the loop again. So for example, the things which I've shown in the red here right now.On or let me mark it with the first circle in red. So the first circle which I'm marking right now, this first circle, that's the first loop of the agent. Thought, action and observation.I have to check MacBook prices. That's the thought. Let me do web search.That's the action. Price seems to be in dollars. That's the observation.It does not stop here. We go to the second loop. Thought.So the second loop is essentially the outside loop, which is this loop. So the thought is the price needs to be converted into euro. Let me use that calculator tool.That's the action. And then the final observation is price in euro. The agent is satisfied here and then it stops.So this is the main difference between chain of thought prompting and the react framework. Because if you see in chain of thought prompting, there is no thought action observation framework loop like this. There is only thinking and the answer.React essentially builds on top of this because it adds reasoning and acting. Where does the acting come into the picture? It's the action part. And these actions are actually performed through the usage of tools.So now see, this is where everything actually comes together. The actions are performed using tools. And we already looked at tools before, right? The tools are actually used in this thought action observation framework.That's the reasoning plus acting. And here you see the whole framework is such that the agent continuously interacts with the external environment. So this observation which it has, it's through its interaction with the external environment.In this observation, the agent can also receive human feedback. Remember in the first lecture, we talked about human feedback. We can even give feedback in the user in the observation.So then the agent again goes into this loop. So essentially agents are LLM equipped with tools. That's fine.But using the react framework, agents can interact with external environment to add additional information into their reasoning. So there is reasoning here, there is action here, and there is observation and this entire loop. And this diagram which you are seeing right now, that is the main difference between LLM and agents.So it's a simple addition that these people actually thought of this loop, right? That the thought action observation framework needs to be set into a loop. And they got huge benefits through this simple framework. You can call this a react framework.You can call this agentic framework. You can call this agentic AI, etc. But broken down into its simplest form, agents just have this thought process in their minds.So they have this framework which guides them and they have access to tools. Now the simplest analogy which I can think of is, let's say that there is a kid, right? Which is left out into the environment. And there are five chairs here.And the kid needs to find that chair which has the candy. So here you will see the thought action observation framework, right? The kid first goes to chair number one. It sees that there is nothing over here.That is the observation. So the thought is now there is nothing in chair number one. So the action is let me go to chair number two.Again, the observation is there is nothing here. So then again, the thought is that there is nothing in chair two. Let me go to chair three.So similarly, when kids are in their external environment, when they receive feedback from the external environment, we are also in this thought action observation cycle continuously. In fact, if you think about all of the decisions we humans make, it can be boiled down to this simple framework where we are in an external environment where nothing is fixed. We are continuously receiving feedback.And through that feedback, we are improving. We are learning. And if you observe very closely for this to work, agents need to have memory.Why am I saying that? Because let's say the kid here right after going to chair number three, if the kid suddenly forgot what has what has come before, it might go back. Right. So you need to have memory for this to proceed.So the thought action observation framework definitely needs the agent to have memory of the interactions which it has had in the past. So for example, you're right. We need to have memory of what happened in my first loop.I saw that the price is in USD because if this memory does not come to the next loop, then this framework is useless. That's why now if you go ahead and if you go to the top of the lecture. And now if we try to deconstruct how can agents plan and take decisions and how can they interact with their environment and have memory? That is because of the React framework.And where is this React framework actually implemented? The React framework is also implemented in the prompt itself. So right in the prompt, we give the agents access to tools and the React is also there in the prompt. So if you see here, if you closely see the prompt, there is the thought, there is a thought, there is action, and there is observation which is mentioned in the prompt itself.And lang chain by default has something called create React agent. And this is a package, but it will use this thought, action and observation, which we have defined over here to go into that loop. So see here the thought is you should always think about what to do.Action is the action to take should be one of the tool names. See here is where I mentioned that when you want to take an action here, here is where you have to use the tools. So the action to take should be one of the tool names.The action input is the input to the action. So if it's, let's say, weather API, the input is maybe the city where you want the weather. The observation is the result of the action.And see what's mentioned here. This thought, action, observation framework is repeated n times. So this thought, action, observation is repeated n times.And finally, when the LLM thinks that, okay, now I know the final answer. That's when we come out of this loop. That's when we come out of the thought loop.So it's all in the prompt. So if you might be thinking, okay, I understood the React template and I understood the tools. Where is the place where we give that to the LLM? Everything is given to the language model in the prompt itself.So in the prompt itself, we tell the LLM to go through this thought, action, observation loop. And only stop when you think you know the final answer. So see, we are relying so much on the power of an LLM to understand language.Because if your model did not understand language, this entire framework is useless. This framework is based on the premise that your language model is amazing and it can understand everything about human language. Which is probably true with the latest GPT models.So now when you look back at this code, you should look at it in a completely different light. You should now think that, oh, okay. The reason I have this React template here is because I have to give a much more detailed prompt to the language model.I want the language model to follow this thought, action, observation framework. If I don't have any framework, my language model won't plan. And not just planning.We want the language model to continuously question itself, right? That's why we have this loop. We want the language model to question itself and we want it to repeat n times until you know the final answer. So all the important learning of agents actually lies in how you make the prompts which are passed to the LLM.Once you have the prompt, so you actually need two things. You need the prompt and in the prompt, you need tools. So first we define the prompt template.Then we actually define the tools which we need. And then we created an agent. So the agent needed these three things.It needs the language model. The language model needs a prompt and the prompt needs tools. So that's why this create React agent has three main arguments.You need a language model. The language model needs prompts and the prompts need access to tools. Why do you need access to tools? Because you cannot perform any action in the thought action observation framework without tools.That's it. And then you just do agent executor.invoke. So at its very basic, what we are doing here is just calling the large language model, right? We are actually just asking some question to a large language model. The reason it's been called as an agent is because now the framework is such that it has access to the external environment and it can reason and plan and it has access to tools.But all what we are doing is just making changes in the prompt really. So one way to think of agents is glorified language models who have access to tools, who have access to environment, who can reason and plan with memory. But I hope that you are not intimidated by codes like these now.The way to start understanding this code is from the bottom up. So first, look at this thing that, OK, I have these three things, right? LLM, tools and prompt. What's the LLM which I'm using? It's GPT 3.5. What are the tools which I'm using? DuckDuckGo for web search and math.And what's the prompt which I'm using? It's the thought action observation framework. OK, that's the react template. All right.So this is the main. The core of today's lecture, which I wanted to that the first take away from today's lecture, it's the React framework and this loop which I've shown over here, which is the main highlight of how LLMs are converted into agents. We are going to take a look at one last code today, but before that, let me pause and take some questions in the chat.Can someone share the links? So the links which have been shared are this Google Colab notebook. And let me share this link once more. Yeah, so this is the Google Colab notebook, which you can open.Let me see if there are any other questions. Are we going to have a class on prompts? No, we will not have a separate class on prompts, but as we develop agents in subsequent lectures, I'll show you how you write effective prompts. There's a good question that what if the agent goes in an endless loop? So usually when we define agents, there is a max iterations which we set.So that's the maximum number of times it can go in that loop. What's the difference between the reasoning models and agents? The reasoning models, again, are not equipped to take actions with tools. Agents are equipped with tools.So the prompt specifically mentions usage of tools. The main thing of agents you can think of is reasoning models in a loop. And also reasoning models reason and stop.They don't self-evaluate themselves many, many, many, many times. We can define an agent to enter a loop countless number of times and then take action. Can agents end up overthinking and give wrong answer? Yeah, probably.We are already seeing that with some reasoning models, right? But we need a good amount of thinking for planning. So if you take a look at the first lecture and the example, which we took in the first lecture regarding the Spain trip, right? That needs a lot of planning. So we need, we would want our agent to think.Does the React agent created in the notebook have memory? That's actually quite a good question. So by default, React agents have memory. But they don't have infinite memory.So for example, there is something called conversation window buffer or window memory buffer, which we can set, which defines the amount of memory which a React agent has. How does the agent know its last end loop? That's another very good question. So how does the agent know its last end loop, right? Take a close look at what we have mentioned in the prompt.We have mentioned in the prompt that whenever you think you know the final answer, you have to stop. So we are relying on the language model judgment that, OK, now I've got the final answer. I'm going to stop over here.And if it still keeps on going, then we can put a limit on the number of maximum loops which we can have. Can the agent evaluate itself? Actually, it is evaluating itself, right? Because the moment it enters into a loop, we know that some kind of self-evaluation is going on. We can even put human feedback to give additional evaluation points.OK, now let's go to the last point in today's lecture, which is essentially one more code, which we are going to explicitly do with agents. Now, this is the second React demo code number two. So what I'm going to do over here now is that you can first refresh your notebook entirely.And we need one more API key. That's essentially the Serper API key. So what I'm going to do is that I'm going to refresh here once more.And meanwhile, let me show you the Serper. How many of you know what Serper is? There is a question about, can we create AI agents with Olama locally? That, yeah, I think it's possible. So for example, if you download a local Olama model and you do the prompt engineering yourself, then you can deploy an agent locally.So what I just mentioned, the thought-action observation, you have to create a prompt template and you have to feed that prompt template to the language model and use tools. The reason normally we use packages like langchain or langgraph is because they have functions out of the box. So for example, they have functions such as this agent executor, then create react agent.These functions are given out of the box by Lama index. You can even do it locally. So you can use these packages locally.And the LLM, you can use the one which you have locally downloaded from Olama. But in that case, it needs to be a small language model, I think, because otherwise it will be very slow. Or you can check DeepSeq, trying to download it locally.Yeah, so what I was talking about was the surfer. So surfer is essentially the reason why we need search APIs is because we'll frequently need to go to Google and we'll need to do some sort of a search, right? That's why we need search API. And there are multiple search APIs.I want to expose you to as many search APIs as possible. So you can think of it as a essentially Google search API. There are many competitors in this space, but these are some of the widely used competitors.So just go here, click on sign in. And I think these API keys you will get for free. So you can just create an account over here at surfer and you can get their API keys for free.You can create an account login using Google and then it will directly prompt you towards creating an API key. So I have already got my API key here on the free tier. So you can now start running the rest.There is a question that tried to make this code work with perplexity but failing due to a weird error. Yeah, so perplexity, I have not actually tested it out with this code. So I need to see how that actually works.Okay, so let me run these packages once more. Meanwhile, let me explain what's going on in this code. So what we are going to do in this code right now is that it's a simple thing.We are just going to ask a question that some random question and then we are going to use the Google search or the server API. And we are going to use it along with another tool, which is the calculator tool. So let's see.Meanwhile, let me take some questions in the chat. The API key for OpenAI has a rate limit. Yeah, so that's you'll have to update your balance to move past the rate limit on OpenAI right now.But in the second code, let me quickly explain what's happening. So we have the OpenAI API key and we have the server API key. And see again, what we are doing here is that we are initializing an agent.And we are using tools. The tools are the server API tool and the LLM math, which is essentially calculated, right? And then the agent, which we have mentioned is zero short react description. So now all of you should intuitively understand what this really means.What does it mean? Agent equal to zero short react description. It means that we are using the react framework and it's a zero short react framework, which means we are not passing any examples in the prompt. And then finally, what I'm going to ask over here is that let's say who are Dr. Raj Dandekar's co-founders and what is their current age raised to the power of one.So here again, what are the.

If you want to find the co-founders of a person, what tools will be needed? And let's say we want to take the current age raised to number one power, whatever. Yeah, we need SERPR for web search, right? And we need to use the math tool. So here again, I'm using the SERPR API, but the answers which we get is not very good because there is not too much information about this online, I think.You can replace this with anything which you want. You can put your name over here. But the point here is that take a look at this thought action observation framework over here.The first thought is that I can use a search engine to find information. That's the first thought. The action is that I'm going to use the Google SERPR tool.The action input is Dr. Raj Dandekar co-founders. And the observation is whatever that the current age is this, this, this. So the thought action observation framework, which we learned about on the Miro, that's the exact same thought action observation framework, which is now happening on the screen.Let's see. I tried using language chain with Gemini models. However, I was unable to use that react agent.Yeah. So with language chain, OpenAI libraries are very much compatible. So if you see there is the import chat OpenAI package, that's extremely compatible with language chain.All right. So now let's see how many of you are able to run this second demo code. So can you mention in the chat if you are able to run this and what what are the answers which you are getting? So I think Manish is able to run this code.

Good. I ran into errors where the output is tuple. Yes.

So Madhavan is also able to run this code. That's that's quite good. So as you can see, the answer might be a bit hallucinating of it.And we are going to look at some examples later to reduce hallucinations and to ground the LLM output in reality. So the reason I took this example is to mention that we should not blindly trust the answers given by an agent because an agent at the heart of it is ultimately a large language model. Right.

And large language models can hallucinate. So that means that agents can also hallucinate. But some takeaways, which I want all of you to remember after this lecture is that.Agents essentially are large language models which have access to tools. They are built on top of this framework, which is called as the React framework. The React framework is a thought action observation loop.So we have a thought, we go into the action and we see the observation. That's the thought action observation loop. In the action, we have access to tools.And I've already exposed you to some tools and I'm deliberately exposing you to more and more tools which are used in industry. So, for example, the Surfer API, which I just showed you right now. The Surfer API is a very widely used search API in industry settings because it's extremely fast and it's also cheaper.So I'll introduce you to many such API calls like these. OK, can someone share the Google Colab code file? So let me share the Colab code file once more. Meanwhile, are there any doubts or questions in this lecture? I can be happy to take any of those questions.How do we find out the different tools which are available like Surfer API? So what one of my main goals in this bootcamp is that I'll eventually introduce you to all of the type of tools which I've mentioned here. I'll introduce all of you to these categories of tools, I'll introduce you to the web search tool, which you already saw that is Surfer. Then I'll introduce you to image generation, retrieval and different API tools.So in those lectures, I try to cover as many as you can. Are all packages for agents from types configured in line chain? So let me actually now explain one more thing. Now, what we saw is we saw the React framework, right? In the next lecture, we are going to actually start looking at frameworks for building agents.So the next lecture is about frameworks and there we are going to see code based frameworks and no code frameworks. And the reason I explained the React framework to you is that almost all of these frameworks are built on top of the React logic, which is the reasoning and acting. So it's very important for you to know what's going on when we take a look at these codes.Most of the underlying thing will be abstracted away at that time. We'll just use a package. So it will be difficult for you then to understand what's going on.

That's why this lecture was very important. So when you see something like, let's say. If you see something like this.Create React agent. You should know what this means instead of blindly using this in the packages. We are going to have a separate lecture on lang, graph, llama index and small agents where all of them have possibilities of creating React agents like this.They all use different code, different code, but the underlying thing is the same thing. Let's see. There are many questions.

Is there any way to make the agent choose the LLL model? We can. There are some agents which choose the model automatically. For example, we have a routing layer and the layer chooses the model on its own based on if you want to prioritize on cost, etc.But mostly, mostly it's it's given to the LLM. So three is given to the agent. Three things need to be given to an agent.Mostly the language model needs to be given. The tools need to be given and the prompt needs to be given. If the agent evaluates itself during the loop, how is it trained to do so? So the training lies in the prompt itself, because in the prompt itself, we have mentioned that, OK, here is how you should think about this question.Here is how you should go into the loop. If the answer is not correct, go into the loop once more. So all the details have been mentioned in the prompt.It seems like the real magic is the emergent properties of LLM. And now we use LLM in different ways to get the answer. Yeah.So again, if you think about it, the reason agents have become so popular is because they rely completely on LLMs. And if LLMs did not have emergent properties, the LLMs were not so good at many tasks. Agents will naturally be performing worse.So can I say that the whole code is an agent which solves the particular problem with LLM as its brain? Yeah, that's probably a simplest way to think about it, that the LLM is the brain of an agent. The tools you can think of as arms and limbs of the agent. But don't forget about the thought action observation loop.That's probably the most important. Are multiple iterations always necessary for accurate predictions? Mostly because if you want to plan something, planning never happens in a single shot. So if you want to plan and make decisions, you always need to go in a loop.Otherwise, planning is not good. And if you think about humans, when we plan, we also go into multiple loops. We don't immediately get to the correct answer.That's why I think this thought action observation framework is so intuitive because even humans think and plan like this. As I gave an example of this kid who is looking for this candy, right? As the kid navigates the external environment, we automatically are in the thought action observation kind of a bubble. Would we need to include any guardrails? Yeah, we do need to include guardrails if the project is sensitive.So, for example, if usually when we build agents for enterprises, there is a guardrail step and evaluation step, which are the final steps. Because you cannot have responses which contain sensitive information, etc. So enterprises have some rules about the kind of responses they want and the kind of responses they need to filter out.So for that, we have to add a guardrail layer on top of the output of the agent. So I hope that after this lecture, all of you realize that agents are not something quite extremely complex. At the heart of it, it's just an LLM with prompt engineering.But it's just that the prompt engineering is done so wisely. It's based on this research paper, the thought action observation framework. The prompt engineering is done through this TAO framework and the actions need access to tools.And that's how tools are provided. And actually, to provide tools, there are two ways to provide tools. One is you directly give API or access to tools.The other way is something called code. So you can also provide a tool as a code. So let's say if you want to create a custom tool, it can be a custom code function which you have written.I deliberately did not go into those details because it might have made the lecture a bit more complex. Can we get a link to the research paper? So I already shared the research paper link, but I'm sharing it once more. How do we apply the human feedback loop? That's one of the most important pieces of the puzzle, which is ultimately not covered quite well.So there has to be an evaluation framework, a manual evaluation framework where a human is in the loop. And there are multiple dashboards where even human feedback can be collected along with the agent's own feedback. So then that feedback is added to the observation and the agent continues in its loop.I will be showing some evaluation frameworks like these in subsequent lectures. How does it know it's not the right answer? So the right answer is one thing, right? But some questions itself are multi-step. So if I ask an agent to give me a travel itinerary, first it has to go through web search.That's the first thought-action observation. Then it has to do something else. That is the second thought-action observation.So the TAO frameworks are also needed in multi-step planning. So it's not just about whether the answer is correct or wrong. If the LLM feels that its answer is not good, it will again go in a loop.But even if the answer is good, there may be multiple planning steps involved in the question. That also might require the LLM to enter this TAO loop. So there are some questions like, will the bootcamp cover production grade agents in further lectures? Yeah, so we will have special projects for production grade agents for the pro-registered candidates.All other lectures which will be covering will be building 5 to 6 agentic projects in these 10 lectures. But there are 5 industrial projects which I will be sharing with people on the Discord group essentially. How do we apply? Okay, this question I already covered.Where does RAG help in agents? Actually, this is a very good question. We have a separate lecture called agentic RAG. So agentic RAG essentially, I will cover that in the lecture.But there is this thing called agentic RAG which basically augments the traditional RAG framework. In traditional RAG, you don't have this TAO loop. In agentic RAG, you have this loop.So agentic RAG is better for some things than traditional RAG. When we learn about LAMA index, we will learn about agentic RAG. I think that's in lecture number 6. Alright everyone, so this brings us to the end of this lecture.In the next lecture, we will look at frameworks for agents. So we will look at the introduction to the frameworks. And then the day after the next, we will actually start building through frameworks.So next lecture will be on introduction to code coding frameworks and no-code frameworks. But now that you understand the theory, all the frameworks which come after this will be based on the theory. Next lecture will be held tomorrow at the same time.So 2 p.m. IST on Wednesday, 25th June. That will be the next lecture. And I will also upload the recordings.The recordings of all previous lectures are on the dashboard. And those of you on the pro plan, the recording lecture notes are also on the dashboard. And on Discord, I will be uploading some new assignments.Thanks everyone for attending. And I look forward to seeing you in the next lecture. Please make sure that you don't drop off after 3 lectures because it's good that all of you have stayed for 3. Now only 7 more are remaining.Thanks everyone. I'll see you in the next lecture. Bye.