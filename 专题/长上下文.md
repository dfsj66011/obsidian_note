
LOGAN KILPATRICK: 这可能是个奇怪的问题，但模型质量会随着上下文长度的变化而波动吗？比如在处理 10 万 token 输入和 12.8 万 token 时，或者 5 万 tokens 时——质量表现是基本一致的线性变化，还是存在某些异常情况？我在想或许最终模型会将这些差异都泛化掉，变得没有区别。但这个观点是否存在什么微妙之处？我们有没有做过能说明这类问题的评估？

NIKOLAY SAVINOV: 是的，内部我们也看过一些评估结果。我想你的问题可能涉及到人们过去观察到的一些现象。其中非常有名的是"中间迷失"效应。回答你的问题，关于"中间迷失"效应——即在上下文中间出现性能下降的情况，我们的模型确实没有观察到这种现象。但我们观察到的现象是，如果任务难度较高（不像简单的单针操作，而是存在复杂干扰项的任务），随着上下文信息增加，任务完成质量会轻微下降。这正是我们想要改进的问题。

------------

LOGAN KILPATRICK: 是的。为了帮助我理解，当我考虑将十万个标记放入模型的上下文窗口时，从开发者或实际使用长上下文功能的用户角度来看，我是否应该假设模型确实在关注所有不同的上下文？我知道它肯定能处理单个关键信息，可以提取出来。但它是否真的在推理上下文窗口中的所有标记？我对模型在拥有如此大量上下文时背后的运作机制还不太清楚。

NIKOLAY SAVINOV: 是的，这个问题问得很好。有一点需要记住的是：注意力机制本质上存在一个缺陷，因为各 token 之间会相互竞争。当某个标记获得更多注意力时，其他标记获得的注意力就会相应减少。关键在于，如果存在强干扰项(distractor)，其中某个干扰项可能与你寻找的目标信息高度相似，从而吸引大量注意力——这会导致你真正需要的信息获得的注意力减少。标记数量越多，竞争就越激烈。所以这既取决于干扰项的强度，也取决于上下文规模。

--------

LOGAN KILPATRICK: 是的。这又是一个愚蠢的后续问题，但注意力的大小总是固定的吗？比如说，有没有可能获得更多的注意力，还是说它就是一个固定的值 1，然后分散在上下文窗口中的所有标记上，所以标记越多，注意力就越少，这是无法改变的吗？

NIKOLAY SAVINOV: 通常来说，情况确实如此，注意力的总量是有限的。

-------

LOGAN KILPATRICK: 是的。从你举的那个例子来看，困难的干扰项会导致模型做更多的工作，并分散注意力。你的团队或应用方面的其他团队是否探索过预过滤机制之类的东西？比如，你希望长上下文在生产中表现得非常好。听起来最好的结果是上下文窗口中的数据非常不相似。如果存在大量相似数据，而你提出的问题可能与所有这些数据相关，那么在这种使用场景下，性能通常会较差。那么，从这一角度来看，这是开发者或整个行业需要解决的问题吗？还是说，你对人们应该如何应对这个问题有任何建议？

NIKOLAY SAVINOV: 作为一名研究者，我认为这种做法可能偏离了正确方向。与其费心设计各种过滤技巧，我们更应该着力提升内容质量和系统稳健性。不过有个实用建议：尽量避免纳入完全无关的内容。既然明知某些信息毫无价值，何必还要将其纳入上下文？至少这样做会增加成本，实在得不偿失。

---

LOGAN KILPATRICK: 是的，这很有趣，因为我觉得在某种程度上，这与人们使用长文本的核心方式相悖——我在网上看到的例子大多是人们直接把各种随机数据塞进模型的上下文窗口，让它自己筛选出有用的信息。按理说，考虑到剔除无关内容的重要性，模型应该会自行预过滤，只保留相关内容——倒不是说人类懒惰，但我觉得这本来就是卖点之一：用户不需要费心考虑该往上下文窗口里放什么数据。

那么你认为有没有这样一种可能，即模型——你知道的，它更像是一个多部分的系统或类似的东西，其中模型的某个版本实际上正在执行部分工作——根据用户的查询剔除无关数据，然后确保上下文真正传递给模型，这样会稍微容易一些吗？

NIKOLAY SAVINOV: 随着时间的推移，随着模型质量不断提高且成本降低，你就不再需要考虑这个问题了。我只是说——目前的现实情况是，如果你想现在就充分利用它，那么，好吧，我们得现实一点。只是不要放无关的内容。不过我也同意你的观点，如果花太多时间手动筛选或精心挑选哪些内容该放进去，确实很烦人。所以我觉得这两者之间应该有个好的平衡。

LOGAN KILPATRICK: Yeah.

NIKOLAY SAVINOV: 我认为上下文的意义在于简化你的生活并使其更加自动化，而不是让它更耗时或让你花时间手工制作东西。

-----------

LOGAN KILPATRICK: 是的。关于这个问题，我有个后续跟进，主要是从长上下文质量的角度来看评估以及你们正在考虑的评估方案。"大海捞针"显然是最初我们放入 1.5 技术报告中的那个。对于不熟悉的人来说，"大海捞针"就是要求模型在 200 万、100 万甚至 1000 万个标记的上下文中找出特定的信息片段。

这些模型在这方面表现得非常出色。你觉得另一组长期——比如，是否存在一套标准——我认为人们通常——我觉得"大海捞针"这个话题被讨论得有点多，但从长上下文的角度来看，你是否在考虑另一套标准基准？

NIKOLAY SAVINOV: 那么让我们来看看。我认为评估基本上是大型语言模型研究的基石。特别是当你有一个庞大的团队时，评估为整个团队提供了一种协调一致、朝着共同方向努力的方式。

因此，长文本也是如此。如果你想取得进展，就必须有出色的评估结果。现在，大海捞针的问题已经解决，尤其是在干扰项简单的情况下。比如，如果是保罗·格雷厄姆的文章，你插入一句话，像这样——巴塞罗那的神奇数字是37。

告诉我巴塞罗那的神奇数字。这其实已经是一个被解决的问题了。但现在，能力的边界在于处理那些棘手的干扰项。比如说，如果你把整个上下文都塞满了，比如 CTX 的神奇数字是 Y，并且你塞入了上百万个这样的键值对，那么任务就变得困难多了，因为那些干扰项看起来和你想要检索的内容非常相似。

对于大语言模型来说，另一项困难的任务是检索多个关键信息。因此我认为这两点——干扰项的难度和多关键信息检索——正是当前的前沿挑战。但除此之外，评估体系还需要考虑其他因素。

你可能会考虑的一个问题是，那些“大海捞针”式的评估，即使加入了高干扰项，也显得相当人为化，因此你可能想要更贴近现实的测试方式。这种观点确实有道理，但需要记住的是，一旦提高评估的真实性，你可能反而会失去衡量核心长文本理解能力的机会。

例如，如果你向一个非常大的代码库提问，而这个问题基本上只需该代码库中的一个文件就能回答，然后任务是实际实现一些复杂的功能，那么你实际上并没有真正发挥长上下文能力。相反，你是在锻炼编码能力。这样一来，它会给你一个错误的信号来进行爬山算法。

它基本上会在编码上进行爬坡优化，而不是在上下文上。这是需要考虑的一点。另一个考虑因素是人们所说的检索与合成评估。理论上，如果你只需要从大海捞针中检索出一根针，这也可以通过 RAG 来解决。但我们真正应该关注的任务是那些在整个上下文中整合信息的任务。

例如，摘要任务就是这样一项任务，而 RAG 在这方面会面临困难。但现在这些任务听起来方向正确且前景美好，实际上却难以用于自动评估。以摘要指标为例，我们知道像 Rouge 这样的评估标准并不完美。如果你正在进行优化改进，实际上使用那些更不易被"钻空子"的指标会更为合适——我该怎么说呢？就是更难以被取巧的评估标准。

-----------

LOGAN KILPATRICK: 然后简单跟进一下，是什么让它们不那么有用？比如以摘要生成为例，是不是因为判断什么是好摘要更主观，没有一个绝对真实的标准？还是说这个用例本身有什么难点？

NIKOLAY SAVINOV: 是的，那些评估结果会相当嘈杂，因为即便是人类评分者之间的一致性也会相对较低。当然，这并不是说我们不应该研究摘要任务或评估摘要质量。这些都是重要的工作。我只是想说，作为研究者，我个人更倾向于在有很强信号的方向上不断优化。

--------

LOGAN KILPATRICK: 是的，这很有道理。关于长上下文能力，尤其是对 Gemini 来说，它是我们向世界展示的核心能力之一，也是 Gemini 的关键差异化优势。但与此同时，长上下文似乎一直是一个独立的工作方向，并非所有内容都涉及长上下文。你认为是否存在这样一种情况：我们有大量其他团队在攻克各种其他随机问题，比如事实准确性、推理能力等等。

你认为从研究和建模的角度来看，长上下文是否应该融入其他所有工作流程？还是说它仍需要作为一个独立的工作流程存在？毕竟，让模型在长上下文中有效运作与推理相比，其根本方法可能截然不同——打个比方来说，这或许就像处理一个必然的衍生问题那样需要特殊对待？

NIKOLAY SAVINOV: 所以，我的回答有两个方面。首先，我认为为每项重要能力指定一个负责人是有帮助的。但其次，我认为工作流也需要为工作流之外的人提供工具，以便他们能够做出贡献。

--------

LOGAN KILPATRICK: 是的，这非常有道理。我还有一个关于推理的后续问题，我很好奇推理和长上下文之间的相互作用——我们之前请过杰克·雷，昨晚我们还和他一起吃了晚饭，聊了一些随机的推理话题。你觉得——如果我说错了请纠正我——推理能力实际上让长上下文变得更有用了，这一点让你感到惊讶吗？这是一个正常的、预期的结果，仅仅是因为模型花了更多时间思考，还是推理能力和长上下文之间存在某种内在的深层联系，使其更加有效？

NIKOLAY SAVINOV：我认为存在更深层次的联系。其关联在于，如果随着上下文长度的增加，下一个标记预测任务的表现有所提升，那么可以从两个角度来解读。一种理解是：嘿，我要在输入中加载更多上下文，这样对我简短答案的预测也会随之改善。但另一种视角则认为：嘿，你看这些输出标记与输入标记其实非常相似。

因此，如果你允许模型将输出反馈到自己的输入中，那么它某种程度上就变成了输入。所以理论上，如果你具备非常强大的长上下文能力，它也应该有助于你的推理。另一个论点是，长上下文对于推理非常重要，因为如果你只是通过生成一个标记来做决定，即使答案是二元的并且只生成一个标记完全没有问题，可能更倾向于首先生成一个思考轨迹。原因很简单，它们是架构性的。

比如说，当你需要通过上下文进行多次逻辑跳跃来做出预测时，就会受到网络深度的限制，因为这大致相当于注意力层的数量。这就是在上下文跳跃方面制约你的因素。所以你是受限的。但现在，如果你想象把输出反馈到输入中，就不再受限制了。本质上，你可以写入自己的记忆，从而执行比单纯利用网络深度时更复杂的任务。

-----------

LOGAN KILPATRICK: 这真是太有趣了。你和我都与这种推理方式以及长上下文故事有关。我们俩都一直在努力推动让模型能够输出更长的内容，我认为开发者们也很需要这个功能。

我经常看到各种提示。现在我要开始把这些提示发给你，这样你就必须回答这个问题。不过很多人都说，嘿，我们希望输出标记能超过 8000 个。目前我们在一定程度上通过推理标记或推理模型实现了这一点。

它们的输出令牌数为 65,000，但需要注意的是，其中很大一部分输出令牌实际上是用于模型自身思考，而非生成最终的用户响应。长上下文输入能力与长上下文输出能力之间有何关联？这两者是否存在某种相互作用？

因为我觉得对于很多核心用例来说，人们想要的可能是输入百万级 token 然后重构这些 token。你认为我们最终会实现这两种能力合二为一吗？在你看来它们是同一种能力，还是从研究角度而言根本就是两回事？

NIKOLAY SAVINOV: 不，我不认为它们有本质上的不同。我认为重要的是要理解，在预训练之后，模型本身并没有任何限制来生成大量标记。你可以输入，比如说，五十万个标记，然后告诉它——我也不知道——复制这五十万个标记，它实际上会照做。

我们确实尝试过这种方法，而且效果不错。但这种能力在训练后阶段需要格外谨慎地处理。之所以需要如此小心，是因为在训练后阶段存在一个特殊的序列结束标记。如果你的监督微调数据较短，模型就会在序列早期频繁遇到这个结束标记。

而这正是学习的过程，就像，嘿，你总是在X上下文中向我展示这个标记。所以是的，我会在X上下文中生成这个标记并停止生成。这就是你在教我的。这实际上是一个对齐问题。但我想指出的一点是，我觉得推理只是众多长输出任务中的一种。

例如，翻译是另一种类型。而推理则具有非常特殊的格式。它将推理过程封装在某些分隔符中，模型实际上知道我们要求它在其中进行推理。但对于翻译来说，整个输出（不仅仅是推理过程）将会很长。这是我们希望模型能够鼓励产生的另一种能力。因此，这只是一个正确调整模型的问题。实际上，我们正在研究长文本输出。

------------

LOGAN KILPATRICK: 我很兴奋。人们对此非常渴望。我认为这触及了一个更广泛的要点，即开发者应该如何考虑长上下文的最佳实践，以及可能还包括 RAG 的最佳实践。你对这方面有没有一个总体的感觉——我知道你在长达一小时的开发者文档中提供了很多反馈，所以我们已经记录了一些这方面的内容。但对于开发者来说，在考虑如何最有效地利用长上下文时，你对这些建议的总体看法是什么？

NIKOLAY SAVINOV: 所以我认为第一个建议是尽量依赖上下文缓存。让我来解释一下上下文缓存的概念。当你第一次向模型提供长上下文并提出问题时，处理时间会更长，成本也更高；而如果你在同一个上下文的基础上提出第二个问题，就可以利用上下文缓存来使回答既更便宜又更快。

这是我们目前为部分模型提供的功能之一。所以，尽量多依赖这个功能。尝试将用户上传的文件缓存到上下文中，因为这不仅能加快处理速度，而且平均下来，输入令牌的成本会降低四倍。

--------------

LOGAN KILPATRICK: 举个例子来说，最常见的情况——如果我理解有误或与你的思维模型不同，请纠正我——这种技术真正发挥作用的典型应用场景是“与我的文档聊天”、“与PDF聊天”或“与我的数据聊天”这类应用程序。在这些场景中，正如你所指出的，原始输入上下文是相同的。这也是使用上下文缓存的要求之一——再次说明，如果我的理解有误请指正——即你提供的原始上下文必须保持一致。如果由于某些原因，输入上下文在每个请求中都发生变化，那么上下文缓存实际上并不会产生太大效果，因为你需要为存储一组原始输入上下文付出成本，而这些上下文必须在用户逐个请求之间保持持久性。

NIKOLAY SAVINOV: 是的，我想这两个问题的答案都是肯定的。当你需要与一系列文档或某个大型视频进行交互时，这一点很重要。你可能想对其提出一些问题，或者针对一个代码库。你提到这些知识不应该改变是正确的。如果确实需要改变，最佳的改变时机是在最后阶段。因为这样，我们在底层要做的是找到与缓存前缀匹配的部分前缀，然后舍弃其余部分。

有时候，开发者会问一个问题，比如：我们应该把问题放在上下文之前还是之后？这就是答案。你应该把它放在上下文之后，因为如果你想依赖缓存并从中节省成本，那么这就是放置它的地方。因为如果你把它放在开头，并且你打算把所有问题都放在开头，那么你的缓存就要从头开始。

----------

LOGAN KILPATRICK: 是的，太棒了。这很有帮助。还有其他建议吗？除了上下文缓存之外，从开发者的角度还应该考虑些什么？

NIKOLAY SAVINOV: 我们已经提到的一点是与 RAG 的结合。如果你需要处理数十亿 token 的上下文，那么就需要结合 RAG。此外，在某些需要检索多个"针"（关键信息）的应用中，即使需要的上下文较短，结合 RAG 可能仍然有益。我们之前讨论过的另一点是：不要在上下文中塞入无关内容。

这会影响多针检索。另一个有趣的点是，我们探讨了权重内记忆与上下文内记忆的交互作用。必须提到的是，如果你想通过上下文内记忆来更新权重内的知识，那么网络必然需要依赖两种知识。因此，这两者之间可能存在矛盾。我认为通过精心设计的提示来明确解决这一矛盾是有益的。例如，你可以在问题开头说"根据上述信息"等等。当你说"根据上述信息"时，你实际上是在暗示模型必须依赖上下文记忆，而不是权重记忆。这样就消除了模型的模糊性。

------

LOGAN KILPATRICK: 我很赞同。这是个很棒的建议。你提到的关于"内嵌权重与非内嵌权重之间的张力"——我们之前也讨论过一些。但从开发者角度，你如何看待这个微调的问题？唯一比"长上下文是否会取代RAG"更具争议性的话题，可能就是"人们到底该不该进行微调"了。Simon Willison对此发表过很多讨论：真的有人在微调模型吗？最终效果如何？你认为——针对相似知识库同时采用微调和长上下文技术是否有价值？还是说微调本身就能带来更好的泛化效果？你如何看待这两者的相互作用？

NIKOLAY SAVINOV: 是的，让我详细说明一下如何在实际的知识库上进行微调。人们有时会这样做：他们获取额外的知识。比如说，你有一个庞大的企业知识库，比如有数十亿个标记。然后，你可以继续训练网络，就像我们在预训练阶段所做的那样。也就是说，你可以应用语言建模损失函数。你可以让模型学习如何在这个知识库上预测下一个标记。但你要记住，这种信息整合方式虽然有效，但也有其局限性。其中一个限制是，由于你实际上要训练网络而不仅仅是提供上下文，你应该为各种问题做好准备。比如，你需要调整超参数。

你需要知道何时停止训练。你必须处理过拟合问题。一些真正尝试过这种方法的人报告说，使用这个过程会增加幻觉。他们暗示，也许这不是向网络提供知识信息的最佳方式。

但显然，这种技术也有其优势。特别是在推理阶段，它会非常便宜且快速，因为知识已经存在于权重中，你只需要进行采样。但这也带来了一些隐私问题，因为现在知识被固化在网络权重中。如果你真的想更新这些知识，那么你又回到了最初的问题。这些知识不容易更新。它们存在于权重中。那么你打算怎么做呢？你将不得不再次通过上下文提供这些知识。

------


LOGAN KILPATRICK: 是的，我认为从开发者的角度来看，这是一个非常有趣的权衡问题，关键在于你希望以多快的速度更新信息。我觉得成本方面确实不低——持续投入资金去……实际上，我认为 RAG 方案相当合理。你只需为向量数据库付费，而市面上有很多选择，规模化运作时效率也相当不错。但持续微调新模型的成本往往不菲，这确实——嗯，有很多值得考量的有趣维度。

我很好奇从微调——或者也许不是微调——从长上下文的角度来看的长期方向。从体验的角度来看，未来三年人们可以期待长上下文方面的哪些进展？但三年后我们还会讨论长上下文吗？会不会变成这样：模型自动处理这些，我不需要操心，它就能正常工作？或者，嗯，你对这个问题怎么看？



NIKOLAY SAVINOV: 那么我来做几个预测。我认为首先会发生的是，当前 100 万到 200 万上下文的处理质量将大幅提升，我们很快就能在几乎所有检索类任务上达到极限水平。我认为这将是第一步的原因在于，虽然你可以说“嘿，为什么不扩展上下文呢？为什么止步于 100 万或 200 万呢？”但关键在于，目前的百万级上下文还远未达到完美。既然它还不够完美，这就引出了一个问题。

你为什么想要扩展它？因为我认为，当我们接近实现百万级完美上下文时，它将开启完全不可思议的应用，比如一些我们从未想象过会发生的事情。比如，处理信息和连接点的能力将大幅提升。这个东西已经可以同时吸收比人类更多的信息。比如，我不知道，看一个一小时的视频，然后立即回答关于该视频的某个特定问题，比如某人在哪一秒掉了一张纸。作为人类，你很难做到如此精确。

因此我认为，这些超人类能力将会变得更加普遍。我们拥有的长上下文能力越强，就能解锁更多我们从未想象过的功能。这将是第一步。质量会不断提升，我们将实现近乎完美的信息检索能力。之后，长上下文的成本将会降低。

我认为这可能需要更多一点时间，但它终将实现。随着成本的降低，更长的上下文也将被解锁。因此，我认为在不久的将来，我们将看到 1000 万上下文窗口成为标配，或者说服务提供商提供 1000 万上下文窗口将成为常态，而目前情况并非如此。

当这种情况发生时，对于某些应用（如编程）来说将成为一个决定性障碍，因为我认为 100 万到 200 万的上下文容量，只能容纳中小型代码库。但 1000 万的容量实际上就能让大型编程项目完整地包含在上下文中。到那时，我们将实现创新技术，使整个上下文的记忆召回近乎完美。这东西对编程应用来说将非常不可思议，因为人类编程的方式是，你得尽可能多地记住东西才能高效编程，还得不停在各个文件之间跳转。人的注意力总是很有限，但大语言模型将彻底解决这个问题。

他们会一次性将所有信息都记在脑子里，并且能够精确地复现其中的任何部分。不仅如此，他们还能真正将线索串联起来。他们会发现文件之间的联系，因此会成为非常高效的编码员。

我想我们很快就会有超强的 AI 编程助手。它们将无人能敌，基本上会成为全球每个程序员的新工具。所以当这 1000 万实现时，那只是第二步。而要达到比如 1 亿，那就更有争议了。我认为这会发生，只是不知道多久能实现。我也觉得我们可能需要更多深度学习方面的创新才能达成这个目标。

-------

LOGAN KILPATRICK: 是的，我非常赞同这一点。关于这三个方面，我想快速追问一下：在你看来，硬件或基础设施方面的考量相对于模型本身的故事占多大比重？显然，要实现大规模的长上下文服务需要做大量工作，这也是为什么长上下文处理成本更高的原因，诸如此类。你是从研究的角度来思考这个问题的吗？还是说，硬件方面会自行解决？TPU会完成它们的工作，我只需要专注于研究部分？

NIKOLAY SAVINOV: 嗯，是的。我的意思是，仅仅拥有芯片是不够的。你还需要非常有才华的推理工程师。我对我们推理团队的工作印象非常深刻。他们在百万级上下文上取得的成就，简直令人难以置信。如果没有这么强大的推理工程师，我认为我们不可能为客户提供一百万或两百万的上下文支持。所以，这也是一个相当大的推理工程投入。不，我不认为这个问题会自行解决。

------


LOGAN KILPATRICK: 是的，我们的推理工程师一直在努力工作，因为我们总是希望这些模型具备长上下文能力。确实，实现这一点并不容易。您如何看待这些具有长上下文的智能体用例之间的相互作用？这是否从根本上实现了与以往不同的智能体体验？或者说，这两者之间是如何相互影响的？

NIKOLAY SAVINOV: 嗯，这是个有趣的问题。我认为智能体在长上下文场景中，既可以被视为消费者也可以被视为供应者。让我详细解释一下：智能体要高效运作，就必须持续追踪最新状态——包括它们之前采取的行动、观察到的结果等等，当然还有当前状态。为了在内存中保存所有这些历史交互记录，就需要更长的上下文支持。这正是长上下文对智能体的价值所在。这就是智能体作为长上下文消费者的应用场景。

但还存在另一个截然不同的视角——智能体同时也是长上下文内容的供给者。因为人工整理长上下文内容极其繁琐：每次都要手动上传所有需要的文档，或是上传视频，又或者从网页复制粘贴某些内容到指定位置，这些操作都非常耗时费力。你不想手动操作。你希望模型能自动完成。而实现这一点的方法之一就是通过自主工具调用。因此，如果模型能自行决定："嘿，现在我需要获取更多信息"，它就会自动整合上下文。从这个意义上说，智能体就是长上下文的供应者。

-------

LOGAN KILPATRICK: 是的，这个例子太典型了。我的看法是——我已经和很多人讨论过这个问题——我认为这实际上是人与 AI 系统交互的主要局限之一，就像你说的，这个过程太繁琐了。最糟糕的是，每次使用 AI 时，我都得去搜集所有可能相关的背景信息，然后亲自把这些上下文提供给它。而多数情况下，这些信息明明就在我的屏幕上或电脑里。

或者我手头有上下文，但感觉就像所有繁重的工作都得我来做。所以我特别期待——我们应该开发一个长上下文代理系统，它能自动从各处获取你的上下文。我觉得这会非常非常有意思，而且我认为这不仅解决了开发者的一个根本性问题，从 AI 系统终端用户的角度来看也是如此。真希望这些模型能主动获取我的上下文，而不需要我亲力亲为。

NIKOLAY SAVINOV: 是的，MCP 必胜。

---------------

Long-Context LLM （韩松）

大家下午好，我们开始吧，今天，我们将讨论长上下文，大语言模型的高效训练与推理技术，是的，长上下文确实非常有趣，它有助于我们理解长文档中的内容，此外，长上下文还能帮助我们更好的理解大量视频内容以及多模态模型。为了节省内存，我们将学习许多技术来减少内存消耗，从而在长上下文的训练和推理过程中实现更快的运行速度。

这是今天的议程。首先我们将回顾上下文扩展方法，如使用 RoPE，然后介绍用于训练的 LongLoRA 技术，我们还将讨论长上下文大语言模型的推理评估，如中间迷失现象（Lost-in-the-Middle）、大海捞针（needle-in-the-haystack,NIAH）和 LongBench，我们还将探讨长上下文推理中的高效检测机制，包括流式大语言模型、双重注意力机制和查询感知稀疏性，最后，如果有时间，我们将讨论状态空间模型（SSMs）。

那么，让我们从回顾旋转位置嵌入开始，旋转位置嵌入是相对位置嵌入的一种流行实现方式。因此，每个标记不仅拥有自己的嵌入向量，还包含位置嵌入，我们在之前的课程中了解到，RoPE 基本上将输入分成两个部分并配对，这样我们可以在第一个元素和 $\frac{d}{2}$ 元素之间形成一个 XY 对，这就在二维空间中形成了一个 XY 对，然后我们可以通过 $m_{\theta}$ 对其进行旋转，$m$ 与其位置相关，$\Theta$ 是一个相当大的数值，使得在旋转 $2\pi$ 之前，你可以区分许多数字，然后我们可以在右侧看到，当我们在 Q 和 K 之间进行张量计算时，它变成了它们幅度的乘积，和它们幅角 $m-n$ 的差值，因此，两个复向量内积的相位角实际上是这两个复向量之间的相位差，即 $m-n$，因此，结果仅与相对位置 $m-n$ 有关，如果你想扩展上下文窗口，应该怎么做呢？

之前，这是原始数据，它能处理的上下文长度最高可达 2K，如果你想将其扩展到 4K，应该怎么做呢？我们不应该仅仅继续外推，这是未见的范围，相反，我们应该将频率加倍，从每一个，这里我们有这个距离，有两只狗，现在同一业务中有四个成年人，因此基本上我们通过工具将数据分割，这样我们仍然可以使用相同的范围来表示 4k 的范围，因此，在扩展上下文之后，我们的模型已经需要微调了。最后但同样重要的是，微调及其长上下文处理相当昂贵，因此，那么，我们如何使其更加高效呢？

因此，我们引入了一项技术，但长上下文处理仅需要使用单个 GPU 来支持一个 70B 参数的模型及其长上下文，例如这是一本《哈利·波特》的书，以下是书中的部分内容，你把所有内容都放在这里，现在，这本书的内容结束了，请告诉我作者在这本书中想要表达的高层次思想是什么，总的来说，我们大约有 3 万个标记，如果你用原始 LLama 2 的值来输入，它的上下文长度只有 4K，远低于 3 万个标记，那么结果会非常不理想，另一项研究，长上下文模型 LongLLaMA，他们使用了 128 个 TPUv3，这需要大量的计算资源，成本实在太高，难以承受，

我们将介绍一种名为 LongLoRA 的技术，它能够高效的进行微调，一个拥有 70B 参数的模型，其上下文扩展至 32K，且仅需一个计算节点即可完成，同时有效减少了上下文损失。好的，一个包含 8 张显卡的节点，仅需一个这样的节点，他们使用了 A100 GPU，那么，我们开看看这是如何实现的。

那么在长上下文场景下，性能瓶颈究竟在哪里呢？注意力机制确实是瓶颈所在，对吧，既然 Transformer 模型由两个主要部分组成，其一是注意力机制，它负责建模不同 token 之间的信息交互，另一部分是 FFN，它对单个 token 进行非线性变换，这恰恰是长上下文场景下的性能瓶颈所在，注意力机制确实是瓶颈所在，因为它的计算量和复杂度呈二次方增长，随着 token 数量的增加，情况会变得更加糟糕，因此，当上下文长度变得非常大时，注意力机制就成为了性能瓶颈。那我们该如何简化注意力机制呢？

以前，我们需要为因果掩码运行整个下三角矩阵，但在这里，我们发现只需要采用这种稀疏的、分块稀疏的注意力机制就足够了，仅在这四个 token 组成的小组内，这四个 token 只关注组内彼此的信息，