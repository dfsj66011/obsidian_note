
LOGAN KILPATRICK: 嗯，有意思。这真的非常有意思。我觉得这实际上让我开始更广泛地讨论上下文窗口了，而且我认为有很多可以讨论的地方。显然，我们在谈论长上下文，这某种程度上假设你知道什么是上下文窗口。但你能大致介绍一下，人们应该如何理解上下文窗口到底是什么吗？作为一个大型语言模型的用户，或者一个使用人工智能模型进行开发的人，我为什么需要关心上下文窗口？


NIKOLAY SAVINOV: 上下文窗口，本质上就是我们输入到 LLM 中的这些上下文 tokens。它可以是当前的提示，也可以是之前与用户的互动记录；可以是用户上传的文件，比如视频或 PDF。当你向模型提供上下文时，模型实际上拥有两个来源的知识：一个来源是我称之为“权重内”或预训练记忆的部分。所以这是一种认知：LLM 是基于互联网数据片段训练的，它从中学习到了一些知识。它不需要额外提供上下文信息来记住某些事实，即使没有上下文，模型内部也存在着某种记忆机制。

但另一种记忆则是你明确提供给模型的上下文记忆。理解这两者的区别非常重要，因为上下文记忆比权重记忆更容易修改和更新。对于某些类型的知识，权重记忆可能已经足够。如果你需要记忆一些简单的事实，比如物体向下落而非向上，这是非常基础、普遍的常识，因此这些知识来自预训练也没问题。但有些事实在预训练时是正确的，到了推理阶段却已过时，这时你就需要以某种方式更新这些信息。而上下文就为你提供了这种更新的机制。这不仅仅关乎知识的时效性，还包括不同类型的知识，比如私人信息。

网络对你个人一无所知，也无法读懂你的心思。因此，如果你希望它真正对你有帮助，你需要将自己的私人信息放入上下文中，这样它才能为你提供个性化的服务。没有这种个性化，它只会给出对任何人都适用的通用答案，而不是为你量身定制的回答。最后需要插入上下文的一类知识是一些罕见的事实，基本上就是那些在互联网上极少被提及的信息。我必须说，我怀疑这类知识可能会随着时间的推移而消失。也许未来的模型会完全记住互联网上的所有内容，我们就不必再担心这些问题了。

但目前的现实情况是，如果某个信息在整个互联网上只被提及一两次，模型实际上不太可能记住这些事实，它们会产生幻觉般的答案。因此你可能需要将这些信息明确插入到上下文中。我们面临的权衡在于：对于短上下文模型，你提供额外上下文的能力有限。本质上，这会形成知识源之间的竞争。而如果上下文足够大，你就可以不那么挑剔插入的内容，从而获得更高的相关知识召回率和覆盖率。当上下文覆盖率越高，就意味着你能缓解权重记忆带来的所有问题。


LOGAN KILPATRICK: 是的。我认为可以从很多角度来探讨。刚才的描述非常到位。接下来要讨论的一个延伸话题是——我们之前提到了权重内记忆（in-weight memory），也谈到了上下文内记忆（in-context memory）或者说广义的上下文学习。第三类方法涉及如何通过 RAG 系统（检索增强生成）引入上下文。能否请您简要概述一下 RAG？之后我还准备了一些关于 "RAG 与长上下文对比" 的尖锐问题要请教您。


NIKOLAY SAVINOV: Yeah, sure. So what RAG does is, well, it's a simple engineering technique.
It's an additional step before you've packed the information into an LLM context.
So imagine you have a knowledge corpus and you chunk this knowledge corpus into, well,
small textual chunks, and then you use some special embedding model to turn every chunk into a real-value vector.
And then based on those real-value vectors, if you get the query at the test time,
you can embed the query as well. And then you can compare this real-value vector for a query
to those chunks from the corpus. And for the chunks which are close to the query,
you're going to say, hey, I found something relevant. So I'm going to pack those chunks into context,
and now I'm running LLM on this. So that's how RAG works.
LOGAN KILPATRICK: And why-- and this is maybe a silly question. RAG, my sense has always been, like, lets you-- obviously,
there's very hard limits on context that you can pass to the model. We have 1 million. We have 2 million. That's awesome.
But actually, if you look at internet scale, Wikipedia has many trillions of tokens, or whatever--
or maybe not trillions, maybe billions of tokens, whatever it is. Why is RAG, as this notion of bringing in the right context
to the model, not just baked into the model itself? Is it just that, to the point of the conversation, the model
just not working well for-- it's just, like, the wrong research direction to go in? Or why don't you think we build that mechanism in?
Because my face-value perspective is, it seems like that would kind of be useful if the model could just do RAG, and if I could
pass a billion tokens and then let the model sort of figure out heuristically, or through whatever mechanism,
what the right tokens are. Or is that just a problem somewhere else in the stack that should be solved
and the model shouldn't have to think about that? NIKOLAY SAVINOV: Well, one thing I want to say is that after we released the 1.5 Pro model,
there were a lot of debates on social media like, is RAG becoming obsolete?
And well, from my perspective, not really. Say enterprise knowledge bases--
they constitute billions of tokens and not millions. And so for this use case, for this scale, you still need RAG.
What I think is going to happen in practice is that it's not like RAG is going to be eliminated right
now, but rather, long context and RAG are going to work together.
And the benefit of long context for RAG is that you will be able to retrieve more relevant needles
from the context by using RAG. And by doing that, you're going to increase the recall
of the useful information. So if previously, you were setting some rather conservative
thresholds and cutting out many potentially relevant chunks, then now you're going to say, hey, I have a long context,
so I'm going to be more generous, so I'm going to pull more facts.
And so I think there's a pretty good synergy between those. And the real limitation is the latency requirements
of your application. So if you need real-time interactions, then well, you'll have to use shorter context.
But if you can afford to wait a little bit more, then yeah,
you're going to use long context, just because you can increase the recall by doing that.
LOGAN KILPATRICK: Why-- is 1 million just, like, a marketing number? Or is there something intrinsic that
after 1 million or 2 million-- like, is there actually something technically happening around the million-token mark, from a long context perspective?
Or is it literally just, we found a number that sounds good and then made the technology work from a research perspective?
NIKOLAY SAVINOV: Well, when I started working on long context, the competition at the time, I think it was about 128k or maybe
200k tokens, at most. So I was thinking how to set the goals for a long context
project. And while it was-- at the time, it was a small, small part of Gemini.
And I originally thought, well, I mean, just matching competitors doesn't sound very exciting.
So I thought, let's set an ambitious bar. So I thought, well, 1 million is an ambitious enough step
forward. It was, like, compared to 200k, that's, like, 5x.
And very soon after we released 1 million, we also actually shipped 2 million, which was about 10x larger.
And I guess one order of magnitude larger than the previous state of the art, that's a good goal.
That's what makes it exciting for people to work on. LOGAN KILPATRICK: Yeah.
I love that. And my follow-up, spicier question from that is like-- we shipped 1 million.
We shipped 2 million rapidly after that. What's the limitation of continuing to scale up beyond 1 to 2 million?
Is it like, from a serving perspective, it's too costly or too expensive? Or is it just, the architecture that
makes 1 to 2 million work just fundamentally breaks down when you go larger than that? Or how come we haven't seen the frontier for long context
continue to push? NIKOLAY SAVINOV: Yeah. So when we released the 1.5 Pro model,
we actually ran some inference tests at 10 million,
and we got some quality numbers as well. And for, say, single-needle retrieval, it was almost perfect for the whole 10 million context.
We could have shipped this model. But it's pretty expensive to run this inference.
So I guess we weren't sure if people are ready to pay a lot of money for this, so we
started with something more reasonable,
in terms of the price. But I think, in terms of quality,
that's also a good question. Because it was so expensive to run, we didn't run many tests.
And so just bringing up this server again, it's also quite costly. So unless we want to ship it to a lot of customers right now--
you know, we don't have chips to do that. LOGAN KILPATRICK: Yeah. Do you think that that will continue to hold,
that it's like this-- I don't know if it's an exponential increase in capacity
that's needed as we do more long context stuff, but do you have an intuition that that will--
do we need fundamental breakthroughs, from a research perspective, for that to change, to make it so that we can actually keep scaling up beyond?
Or is it like, 1 to 2 million is going to be what we stick with, and if you want more than that, do RAG, and then
be really smart about bringing context in and out of the context window, from the model perspective?
NIKOLAY SAVINOV: So my feeling is that we actually need more innovations.
So it's not just a matter of brute-force scaling. To actually have close to perfect 10 million context,
we need to learn more innovations. But then in terms of the RAG and which paradigm
will be more powerful going into the future, I think that the cost of those models is going to decrease over time and we're
going to try to pack more and more context retrieved
with RAG into those models. And because the quality is also going to increase,
then it's going to be more and more beneficial to do that. LOGAN KILPATRICK: Yeah.
That makes sense. Can you take us back to when we originally landed long context? My understanding of the story is, for 1.5 Pro,
it wasn't like it had been built for a long context to begin with. I think you had, obviously, tried to kick off that workstream with others inside of DeepMind,
and it ended up just being that the pace of research progress was super fast and we--
I think my loose understanding of the story is, like, we had the breakthroughs, we realized it worked, and then it was shortly thereafter,
it ended up actually landing in the model side. Or what was the timeline from the effort starting
to actually landing it into a model that was available externally to the world? NIKOLAY SAVINOV: Oh, I think that was, indeed, pretty quick.
And just to clarify, we didn't really-- like, we were wishing to go long and achieve, say, 1 million
or 2 million contexts, but we kind of didn't expect ourselves to get there that fast.
And when it actually happened, then we thought, like, hey,
this is really impressive. Like, we actually made some strides on this task,
so now we need to ship it. And then we actually managed to assemble a pretty awesome team
very quickly, and the team worked really hard. To be honest, in my life, I've never
seen people working this hard. I was really impressed. LOGAN KILPATRICK: I love that.
That's awesome. And that was for the original 1.5 Pro Series. We landed it for 1.5 Flash as well.
We now have it for 2.0 Flash. We have 2.5 Pro. And I think the-- can you sort of give us the lay of the land,
of what's been happening, from a long context perspective, from that original launch, when we know long context is
possible, we released the technical report for 1.5 Pro, which showed the "needle in the haystack" results,
a bunch of stuff like that, to today, where I think a lot of what's actually making this 2.5 Pro
model blow people's mind is actually how strong it is at long context, which has been awesome for coding use cases
and stuff like that. So what's happened in the long context world, from original launch to today?
NIKOLAY SAVINOV: Yeah, so I think the biggest improvement was actually the quality.
And we made strides both on the quality at, say,
128k context and also at 1 million context. So if we look at the benchmark results for 2.5 Pro model,
we observed that it's better compared to many strong baselines, like GPT 4.5, Claude 3.7,
and also o3-mini-high and some of the DeepSeek models.
So the quality, to actually compare it to those models, we had to run the evals at 128k so that they were all
comparable. And we saw quite a big improvement for 2.5 Pro.
And now, in terms of 1 million contexts, we compared it with 1.5 Pro, and we also
saw some significant advantages. LOGAN KILPATRICK: This is maybe a weird question, but does the quality ebb and flow at different context sizes?
Do you see-- is it almost, like, linear quality, like, on a 100,000-token input versus 128,000, or a 50,000 versus--
is it pretty consistent across, or is there weird-- I'm trying to imagine maybe it all
generalizes when you make it into the final model and there's no there's no difference. But is there any nuance in that perspective?
Have we done evals that show anything like that? NIKOLAY SAVINOV: Yeah, internally, we looked at some of those evals.
I guess maybe your question goes into these effects that people observed in the past.
A very popular one was the "lost in the middle" effect. And to answer your question, the "lost in the middle" effect,
where you have a dip in the middle of the context, we don't really observe this with our models.
But what we do observe is that if it's a hard task, not like a single needle, but some task with hard distractors,
then the quality slightly decreases with the increasing context.
And that's something we want to improve on. LOGAN KILPATRICK: Yeah. And just for my own mental model,
when I think about putting a hundred thousand tokens into the context window of the model, from a developer perspective or a user
who's actually using the long context functionality, should I assume that the model is actually
attending to all of the different contexts? I know it can definitely do the one needle.
It can pull that out. But is it actually reasoning over all those tokens
in the context window? I have just a bad mental model of what's happening behind the scenes when you
have that much context in the context window of the model. NIKOLAY SAVINOV: Yeah, I think that's a good question.
So one thing you need to keep in mind is that attention, in principle, has
a bit of a drawback because there's a competition happening
between tokens. So if one token gets more attention,
then other tokens will get less attention. The thing is, if you have hard distractors,
then one of the distractors might look very similar to the information that you are looking for and it might
attract a lot of attention. And now the piece of information that you are actually
looking for is going to receive less attention. The more tokens you have, the harder competition becomes.
So it depends on the hardness of distractors, and also on the context size.
LOGAN KILPATRICK: Yeah. This is another silly follow-up question, but is the amount of attention always fixed?
Like, there's no-- is it possible to have more attention, or is it just, whatever, it's a value of 1, and then there's--
spread across all of the tokens in the context window, and so the more tokens you have, literally, the less attention
there is, there's no way for that to change? NIKOLAY SAVINOV: Normally, that's the case, that the whole pool of attention is limited.
LOGAN KILPATRICK: Yeah. From that example you gave about the hard distractors causing
the model to do a lot more work and sort of split the attention, has your team explored, or other teams on the applied side
explored, like, pre-filtering mechanisms, any of that type of stuff? So, like, you want long context to work really
well in production. It sounds like the best outcome is you have very dissimilar data that's in the context window.
If there's a lot of similar data, and you're asking a question that could be relative to all of it, you'd expect the performance to be worse in general
in that use case. So have you-- is that just something that developers or the world needs to figure out,
from that perspective? Or do you have any suggestions of how folks should approach that problem? NIKOLAY SAVINOV: For me, as a researcher,
I think it would kind of be a move in the wrong direction. I think we should work more on improving
the quality and robustness instead of coming up with some hacks for filtering.
One practical recommendation, though, is, of course, try not to include totally irrelevant contexts.
Like, if you know that something is not useful, then what's the goal of including it into the context?
Because in the very minimum, it's going to be more expensive, so why would you do it?
LOGAN KILPATRICK: Yeah, it's interesting, because I feel like, in some sense, that
goes against the core way that people use long-- I think the examples I see online is just,
like, people being like, oh, just take all this random data and throw it into the context window of the model and have it sort of figure out what's useful for me.
So you'd almost expect the model to do-- given how important it sounds like it
is to remove some of that stuff-- the model to do, like, the pre-filtering itself, almost, to only include the relevant-- because I think
that-- not that humans are lazy, but I feel like that's been one of the selling points, is, like, I don't need to think about what data I'm putting
into the context window. So do you think there's a world where the model-- you know, it's, like, a multi-part system or something
like that, where the model is actually doing-- some version of the model is doing some of that, eliminating the extraneous data based
on what the user's query is, and then making sure the context actually goes to the model, it's a little bit easier?
NIKOLAY SAVINOV: Over time, as the models get better-quality and they get cheaper, you'll just-- you will not need
to think about this anymore. I'm just talking about-- the current reality is like, if you want to make good use of it
right now, then, well, let's be realistic.
Just don't put irrelevant contexts. But also, I agree with your point, that if you spend too much time manually filtering
it or handcrafting which things to put into context,
that's annoying. So I guess there should be a good balance between those. LOGAN KILPATRICK: Yeah. NIKOLAY SAVINOV: I think the point of context
is to simplify your life and make it more automatic, not to make it more time-consuming
or make you spend time handcrafting things. LOGAN KILPATRICK: Yeah. I've got a follow-up on this around, like,
evals and the evals that you're thinking about from a long context quality perspective. Needle in a haystack, obviously, that
was the original one that we put into the 1.5 technical report. And for folks who aren't familiar, needle in haystack
is just asking the model to find, like, one piece of context in 2 million, 1 million, 10 million tokens of context.
The models are extremely good at this. How do you think about the other set of long--
like, is there a set of standard-- I think folks generally-- I feel like needle in a haystack gets talked about a little bit,
but are there another set of standard benchmarks that you're thinking about from a long context perspective?
NIKOLAY SAVINOV: So let's see. I think the evaluation is pretty much the cornerstone of the LLM
research. Especially if you have a large team,
evaluation provides a way for the whole team to align and push in a common direction.
So the same applies to long context. If you want to make progress, you need to have great evaluations.
Now, single needle in a haystack, it's a solved problem, especially
with easy distractors. So if it's, like, Paul Graham's essays and you put a phrase,
here is my-- like, a magic number for the city of Barcelona is 37.
Give me the magic number for the city of Barcelona. So this is really a solved problem.
But now, the frontier of capabilities is handling hard distractors.
If you, for example, packed your whole context with, like, a magic number for CTX is Y
and you pack, say, the whole million contexts with these key value pairs,
well, that's a much harder task, because then distractors are actually looking very similar to what you want to retrieve.
Another thing which is hard for LLMs is retrieving multiple needles. So I feel like these two things, the hardness of distractors
and multiple needles, they are the frontier. But also, there are additional considerations for the evals.
One consideration you might have is, oh, well, those needle in a haystack
evals, even with hard distractors, they're pretty artificial, so maybe I want something more realistic.
This is a valid argument, but the thing you need to keep in mind is that once you increase
the realism of the eval, you might actually lose the ability to measure the core long-context capability.
For example, if you are asking a question to a very large codebase and the question basically
can be answered by just one file in this codebase, and then the task is to actually implement something complicated,
then you're not really going to be exercising the long-context capability. Instead, you are going to be exercising
the coding capability. And then it will give you a wrong signal for hill climbing.
It will basically hill-climb on coding instead of on context. So that's one consideration.
Another consideration is something which people call retrieval versus synthesis evals.
So theoretically, if you need to just retrieve one needle from the haystack, that
can be solved by RAG as well. But the tasks that we should really be interested in
are the tasks which integrate information over the whole context.
And for example, well, summarization is one such task,
and RAG would have a hard time dealing with this. But now, these tasks, they--
it sounds nice and the right direction to go, but they're actually not so easy to use for automatic evaluation.
For example, the metrics for summarization, we know that they are-- metrics like Rouge, et cetera, they are imperfect.
And if you're doing hill climbing, then actually, you're better off using something which is more--
how do I say it? Less gameable metrics.
LOGAN KILPATRICK: And then just a quick follow-up, what makes them less useful? Like, for summarization, as an example is it
just that it's more subjective of what a good summary is versus what isn't, and it doesn't have a ground-truth source of truth?
Or what makes that use case hard? NIKOLAY SAVINOV: Yeah, those evals, they are going to be pretty noisy, because there
will be a relatively low agreement between the-- even between the human raters.
Of course, this is not to give an impression that we shouldn't work on summarization and we shouldn't measure summarization.
These are important tasks. I'm just talking about, my personal preferences
as a researcher is to hill-climb on something which has a very strong signal. LOGAN KILPATRICK: Yeah.
That makes sense. How do you see sort of, as long context, especially for Gemini, it's just a core part of the capability story
that we're telling the world. It's a core differentiator for Gemini.
And yet at the same time, it feels like long context has always been like an independent workstream, of,
like, everything isn't long context. Do you think there's a world where we have-- there's a ton of other teams hill-climbing
on a bunch of other random stuff, factuality, whatever it is, reasoning, et cetera, et cetera.
Do you think the directional-- from a research perspective, from a modeling perspective,
is that long context is just fused into every other workstream? Or do you think there's still--
you know, it needs to be an independent workstream because it's just fundamentally different in how you get
the model to do useful stuff with long context versus reasoning, as a corollary example, perhaps?
NIKOLAY SAVINOV: So I guess my answer will be twofold. First of all, I find it helpful to have an owner
for every important capability. But second, I think it's important for the workstream
to also provide tools for people outside of this workstream to contribute.
LOGAN KILPATRICK: Yeah, that makes a ton of sense. I have another follow-up around reasoning stuff,
and I'm curious how the interplay between reasoning and long context-- we had Jack Ray on, and we were both at dinner with Jack last night,
and we were talking about random reasoning stuff. Do you think the--
have you been surprised by how much it feels like-- and you can correct me if this is wrong--
like, the reasoning capability actually makes long context much more useful? Is that just a normal, expected outcome just because the model
is spending more time thinking, or is there some inherent, deep connection between reasoning capabilities
and long context to make it much more effective? NIKOLAY SAVINOV: I would say there's a deeper connection.
The connection is that if the next token prediction task improves with the increasing context length,
then you can interpret this in two ways. One way is to say, hey, I'm going
to load more context into the input, and predictions for my short answer
are going to improve as well. But another way to look at this is say, hey, well,
the output tokens, they are very similar to input tokens. So if you allow the model to feed
the output into its own input, then it kind of becomes like input. So theoretically, if you have a very strong long context
capability, it should also help you with the reasoning. Another argument is that long context
is pretty important for the reasoning, because if you are just going to make a decision by generating
one token, even if the answer is binary and it's totally fine to generate just one token,
it might be preferable to first generate a thinking trace. And the reason is simply, they are architectural.
Like, if you need to make many logical jumps
through the context when making a prediction, then you are limited by the network depth,
because that's roughly the number of attention layers. That's what's going to limit you in terms of the jumps
through the context. So you are limited. But now, if you imagine that you are feeding the output into the input,
then you are not limited anymore. Basically, you can write into your own memory
and you can perform much harder tasks than you could
by just utilizing the network depth. LOGAN KILPATRICK: That's super interesting.
You and I have also both related to this reasoning plus long context story. You and I have both been pushing for a long time
to try to get long outputs landed into the models, and I think developers want this.
I see pings all the time. I'm going to start sending them to you now so that you have to answer this question. But lots of people saying, hey, we want longer than 8,000 output
tokens. We have this to a certain extent now with reasoning token or with the reasoning models.
They have 65,000 output tokens, with the caveat that a large portion of those output tokens is actually for the model to do
the thinking itself versus generating some final response to the user. How connected are the long context input
versus long context output capabilities? Is there any interplay between those two things?
Because I feel like for a lot of-- the core use case I think that people want is like dump in a million tokens
and then refactor that million tokens. Do you think we'll get to a world where those two things are actually the same capability?
Do you look at them as the same capability, or is it two like completely fundamentally different things from a research perspective?
NIKOLAY SAVINOV: No, I don't think they are fundamentally different. I think the important thing to understand
is that straight out of pre-training, there isn't really any limitation from the model side
to generate a lot of tokens. You can just put, say, half a million and tell it,
I don't know, copy this half a million tokens and it will actually do it.
And we actually tried it. It works. But this capability, it requires very careful
handling in the post-training. And the reason why it requires a careful handling is because in the post-training, you
have this special end-of-sequence token. And if your SFT data is short, then what's going to happen
is the model is going to see this end-of-sequence token pretty early in the sequence.
And that is just going to learn, like, hey, you're always showing me this token within the context X.
So yeah, I'm going to generate this token within context X and stop generation.
That's what you are teaching me. This is actually an alignment problem. But one point I want to make is that I
feel like reasoning is just one kind of long output tasks.
And for example, translation is another kind. And reasoning, it has a very special format.
It packs the reasoning trace into some delimiters,
and the model actually knows that we are asking it to do the reasoning in there. But for translation, the whole output, not just the reasoning
trace, is going to be long. And this is another kind of capability that we want the model to encourage to produce.
So it's just a matter of properly aligning the model. And we are actually working on long output.
LOGAN KILPATRICK: I'm excited. People want it very badly. I think that gets to a broader point around just how developers
should be thinking about best practices for long context, and also for RAG, potentially, as well. Do you have a general sense of-- and I
know you give a bunch of feedback on hour long context developer documentation, so we have some of this stuff sort of documented already.
But what's your general sense of what the suggestions are for developers as they're thinking about how to most
effectively use long context? NIKOLAY SAVINOV: So I think suggestion number one is try to rely heavily on context caching.
So let me explain the concept of context caching. The first time you supply a long context to the model
and you're asking a question, it's going to take longer and it's going to cost more, while if you're
asking the second question after the first one on the same context, then you can rely on context caching to make it both cheaper and faster
to answer. That's one of the features that we are currently providing for some of the models.
And so yeah, try to rely heavily on this thing. Try to cache the files that the user uploaded into context,
because it's not only faster to process, but it's going to cost you, on average, four times less
for the input token price. LOGAN KILPATRICK: And just to give an example of this,
the most common-- and you correct me if this is wrong or not the same mental model that you have,
but the most common application where this ends up being really useful is the chat with my docs, or chat with PDF,
or chat with my data type of applications, where the actual original input context to your point
is the same. And that's one of the-- again, correct me if my mental model is wrong-- that's one of the requirements of using context caching,
is that the original context you supply has to be the same. If, for some reason, that input context
was changing on a request by request basis, context caching doesn't actually end up being that affected because you're
paying to store some set of original input context that has to persist from a user request-by-user request basis.
NIKOLAY SAVINOV: Yeah, I guess answer is yes to both. It's important for cases where you
want to chat to a collection of your documents or some large video.
You want to ask some questions on it. Or a code base. And you are correct to mention that this knowledge,
it shouldn't change. Or if it changes, then the best way for it to change
is at the very end. Because then, what we're going to do under the hood
is we're going to find the prefix which matches the cached prefix, and we're just
going to throw away the rest. And sometimes, developers ask a question like, where should we
put the question, before the context or after the context? Well, this is the answer.
You want to put it after the context, because if you want to rely on caching and profit
from cost saving, then that's the place to put it. Because if you put it at the beginning,
and if you are intending to put all your questions at the beginning, then your caching
is going to start from scratch. LOGAN KILPATRICK: Yeah, that's awesome. That's helpful.
Other tips? Anything else besides context caching that folks should be thinking about from a developer
perspective? NIKOLAY SAVINOV: One thing we already touched on, and that's combination with RAG. So if you need to go into billions of tokens of context,
then you need to combine with RAG. But also, in some applications where
you need to retrieve multiple needles, it might still be beneficial to combine with RAG,
even if you need much shorter contexts. Another thing which we already discussed is that, well, don't pack the context with irrelevant stuff.
It's going to affect this multi-needle retrieval. Another interesting thing is, we touched
on the interaction between in-weight and in-context memory.
So one thing I must mention is that if you want to update your in-weight knowledge using
in-context memory, then the network will necessarily get two kinds of knowledge to rely on.
So there might be a contradiction between those two. And I think it's beneficial to resolve
this contradiction explicitly by careful prompting. So for example, you might start your question with saying,
"based on the information above," et cetera. And when you say this, "based on the information above,"
you give a hint to the model that it actually has to rely on in-context memory instead of in-weight memory.
So it results this ambiguity for the model. LOGAN KILPATRICK: I love that.
That's a great suggestion. And your comment about this tension between in-weight versus not-- and again, we talked a little bit about this.
But how do you think about, from a developer perspective, the fine-tuning angle of this? And the only thing that's maybe more controversial
than, is long context going to kill RAG, is should people be fine-tuning at all? And Simon Willison has a bunch of threads about this.
It's like, does anyone actually fine-tune models? Does it end up helping them? How do you think about this from a--
would it be useful to do fine-tuning and long context for a similar corpus of knowledge,
or does the fine-tuning piece potentially lead to better general outcomes for fine-tuning?
How do you think about that interplay? NIKOLAY SAVINOV: Yeah, so let me maybe elaborate on how fine-tuning could actually be used on the knowledge corpus.
So what people sometimes do is, well, they get additional knowledge.
Let's say you have a big enterprise knowledge corpus,
say, billion of tokens. And well, you could continue training the network,
just like we were doing with pre-training. So you could apply language modeling loss.
And you can ask the model to learn how to predict the next token on this knowledge corpus.
But you should keep in mind that this way of integrating information, it actually works, but it has limitations.
And one limitation is, because you're actually going to train the network instead of just supply
the context, you should be prepared for various problems. Like, you will need to tune hyperparameters.
You will need to know when to stop the training. You'll have to deal with the overfitting.
Some people who actually tried to do that, they reported increased hallucinations from using this process.
And they hinted that maybe it's not the best way to supply knowledge information into the network.
But obviously, this technique also has advantages. In particular, it's going to be pretty cheap and fast
at inference time because, well, the knowledge is in the weights, so you're just sampling.
But there are also some privacy implications, because now, the knowledge is cemented into the weights
of the network. And if you actually want to update this knowledge, then you are back to the original problem.
This knowledge is not easy to update. It's in the weights. So how are you going to do it?
You will have to, again, supply this knowledge through the context. LOGAN KILPATRICK: Yeah, I think it's
such an interesting trade-off problem from a developer perspective, about how rapidly you
want to be able to update the information. I think the cost piece of it, it's not cheap to just keep paying to--
I feel like RAG is actually pretty reasonable. You're paying for a vector database, which, I feel like there's a lot of offerings,
and that's reasonably efficient to do at scale. But I think continuously fine-tuning new models is oftentimes potentially not cheap,
which is super-- yeah, a lot of interesting dimensions to take into account. I'm curious about the long-term direction from a fine-tuning--
or maybe not from a fine-tuning-- from a long context perspective. What can folks look forward to in the next three
years for long context, from maybe an experience perspective? But will we even talk about long context in three years?
Will it just be like, the model does this thing and I don't need to care about it, and it just works?
Or, yeah, how are you thinking about this? NIKOLAY SAVINOV: So I'll make a few predictions.
What I think is going to happen first is, the quality of the current 1 or 2 million contexts
is going to increase dramatically, and we are going to max out pretty
much all the retrieval-like tasks quite soon.
And the reason I think it's going to be the first step is because, well, you could say, like, hey,
but why don't we extend the context? Why stop at 1 million or 2 million?
But the point is that the current million context, it's
not close to perfect yet. And while it's not close to perfect, there's a question.
Why do you want to extend it? Because what I think is going to happen is when we achieve close to perfect million context,
then it's going to unlock totally incredible applications,
like something we could never imagine would happen. Like, the ability to process information and connect
the dots, it will increase dramatically. This thing, it already can simultaneously
take in more information than a human can. Like, I don't know, go watch a one-hour video,
and then immediately after that, answer some particular question on that video, like at what second someone is
dropping a piece of paper. You can't really do that very precisely as a human.
So what I think is going to happen is these superhuman abilities, they are going to be more pervasive.
The better long context we have, the more capabilities that we could never imagine are going to be unlocked.
So that's going to be step number one. The quality is going to increase and we're
going to get nearly perfect retrieval. After that, what's going to happen is the cost of long context is going to decrease.
And I think it will take maybe a little bit more time, but it's going to happen.
And as the cost decreases, the longer context
also gets unlocked. So I think reasonably soon, we will see that 10 million
context window, which is a commodity, or it will basically be normal for the providers
to give a 10 million context window, which is currently not the case.
When this happens, that's going to be a deal-breaker for some applications, like coding,
because I think for 1 or 2 million, you can only fit somewhere between small and medium-sized
code base in the context. But 10 million actually unlocks large coding projects
to be included in the context completely. And by that point, we'll have the innovations
which enable near-perfect recall for the entire context.
This thing is going to be incredible for coding applications, because the way humans are coding,
well, you need to hold in memory as much as possible to be effective as a coder,
and you need to jump between the files all the time. And you always have this narrow attention span,
but LLMs are going to circumvent this problem completely.
They're going to hold all this information in their memory at once, and they're
going to reproduce any part of this information precisely. Not only that, they will also be able to really connect the dots.
They will find the connections between the files, and so they will be very effective coders.
I imagine we will very soon get superhuman coding AI assistants.
They will be totally unrivaled and they will basically become the new tool for every coder in the world.
And so when this 10 million happens, that's the second step. And going to, say, 100 million, well, it's more debatable.
I think it's going to happen. I don't know how soon it's going to come.
And I also think we will probably need more deep learning innovations to achieve this.
LOGAN KILPATRICK: Yeah, I love that. One sort of quick follow-up across all three of those dimensions is, like, how much from your mind
is this hardware story or the infrastructure story
relative to the model story? There's obviously a lot of work that has to happen to actually serve long context at scale, which
is why it costs more money to do long contexts, et cetera, et cetera. Do you think about this from a research perspective?
Or is it like, hey, the hardware is going to take care of itself? The TPUs will do their job and I can just focus
on the research side of things? NIKOLAY SAVINOV: Well, yeah. I mean, just having the chips is not enough.
You also need very talented inference engineers.
And I'm really impressed by the work of our inference team.
What they pulled off with the million context, that was incredible.
And without such strong inference engineers,
I don't think we would have delivered 1 or 2 million contexts to customers.
So it's a pretty big inference engineering investment as well.
And no, I don't think it's going to resolve itself. LOGAN KILPATRICK: [LAUGHS] Yeah, our inference engineers
are always working hard because we always want long context on these models.
And yeah, it's not easy to make it happen. How do you think about the sort of interplay
of a bunch of these agentic use cases with long contexts? Is it like a fundamental enabler of different agent experiences
than you could have before? Or what's the interplay between those two dynamics? NIKOLAY SAVINOV: Well, this is an interesting question.
I think agents can be considered both consumers
and suppliers for long context. So let me explain this. So the agents, to operate effectively,
they need to keep track of the last state, like the previous actions that they took, the observations that they made, et cetera.
And of course, the current state as well. So to keep all these previous interactions in memory,
you need longer context. So that's where longer context is helping agents.
That's where the agents are the consumers of long context.
But there is also another orthogonal perspective, is that agents are actually suppliers of long context
as well. And this is because [? packing ?] long context by hand is incredibly tedious.
If you have to upload all the documents that you want
by hand every time, or upload a video or I don't know,
copy/paste some content somewhere from the web, this is really tedious.
You don't want to do that. You want the model to do it automatically. And one way to achieve this is through the agentic tool calls.
So if the model can decide on its own, hey, at this point, I'm going to fetch some more information,
then it's going to just pack the context on its own. And so yeah, in that sense, agents
are the suppliers of long context. LOGAN KILPATRICK: Yeah, that's such a great example.
My two cents-- and I've had many conversations with folks about this-- I think this is actually one of the main limitations of how
people interact with AI systems, is your example of, it's tedious. It's so tedious. The worst part about doing anything with AI
is, I have to go and find all the context that might be relevant for the model and personally
bring that context in. And in many cases, the context is already on my screen
or on my computer. Or I have the context somewhere, but it's like, I have to do all the heavy lifting. So I'm excited for--
we should build some long context agent system that just goes and gets your context from everywhere.
I think that would be super, super interesting, and I feel like solves a very fundamental problem, not only for developers, but from an end user of AI systems
perspective. I wish the models could just go and fetch my context and I didn't have to do it all. NIKOLAY SAVINOV: Yeah, MCP for the win.
LOGAN KILPATRICK: [LAUGHS] I love that. Nicolay, this was an awesome conversation. Thank you for taking the time. I'm glad we got to do this in person
and appreciate all the hard work from you and the long context teams. And hopefully, we'll have lots more exciting,
long context stuff to share with folks in the future. NIKOLAY SAVINOV: Yeah, thanks for inviting me. It was fun to have this conversation.
LOGAN KILPATRICK: Yeah, I love it.