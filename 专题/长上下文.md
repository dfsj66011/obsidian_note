
LOGAN KILPATRICK: 是的。我认为可以从很多角度来探讨。刚才的描述非常到位。接下来要讨论的一个延伸话题是——我们之前提到了权重内记忆（in-weight memory），也谈到了上下文内记忆（in-context memory）或者说广义的上下文学习。第三类方法涉及如何通过 RAG 系统（检索增强生成）引入上下文。能否请您简要概述一下 RAG？之后我还准备了一些关于 "RAG 与长上下文对比" 的尖锐问题要请教您。

NIKOLAY SAVINOV: 好的，当然。RAG 的作用其实是一种简单的工程技术。它是在将信息打包进 LLM 上下文之前的一个额外步骤。想象一下，你有一个知识库，你把这个知识库分割成小的文本块，然后使用某种特定的嵌入模型将每个文本块转换成一个实值向量。然后，基于这些实值向量，如果在测试时得到查询，你也可以将查询嵌入。接着，你可以将这个查询的实值向量与语料库中的那些块进行比较。对于那些与查询接近的块，你会说，嘿，我找到了一些相关的内容。于是，我会把这些块打包到上下文中，现在我要在这个基础上运行 LLM。这就是 RAG 的工作原理。

-----------

LOGAN KILPATRICK: 为什么——这可能是个有点傻的问题。关于 RAG，我的理解一直是，它让你——显然，你能传递给模型的上下文有非常严格的限制。我们有 100 万、200 万的 token 容量，这很棒。但实际上，如果看互联网规模，维基百科有数万亿（或数十亿）token 的数据。为什么 RAG 作为一种向模型提供正确上下文的方案，没有被直接内建到模型本身？是否就像我们讨论的那样，模型在这方面效果不佳——或者说这个研究方向本身就是错的？又或者，为什么我们不直接构建这种机制？因为从表面看，如果模型能自主进行RAG，允许我传入 10 亿 token 然后让它通过启发式或其他机制自行筛选关键内容，似乎会很有用。还是说这其实是技术栈其他环节该解决的问题，本就不该让模型来处理？

NIKOLAY SAVINOV: 嗯，我想说的是，在我们发布 1.5 Pro 模型后，社交媒体上有很多争论，比如 RAG 是否过时了？在我看来，并非如此。以企业知识库为例，它们包含数十亿而非数百万的 token。因此，对于这种用例和规模，你仍然需要 RAG。我认为实际会发生的情况是，RAG 不会立即被淘汰，而是长上下文与 RAG 将协同工作。长上下文对 RAG 的好处在于，通过使用 RAG，你将能够从上下文中检索到更多相关的关键信息。这样做可以提高有用信息的召回率。因此，如果之前你设置了一些较为保守的阈值，并剔除了许多可能相关的文本块，那么现在你会说，嘿，我有一个长上下文，所以我会更宽松一些，从而提取更多事实。因此我认为这两者之间存在相当不错的协同效应。真正的限制在于应用程序对延迟的要求。如果你需要实时交互，那就必须使用较短的上下文。但如果可以多等一会儿，那么你就可以使用长上下文，因为这样做可以提高召回率。

----------

LOGAN KILPATRICK: 为什么——100 万只是一个营销数字吗？还是说从长上下文的角度来看，在 100 万或 200 万 tokens 之后，技术上确实会发生一些本质性的变化？或者实际上只是我们找到了一个听起来不错的数字，然后从研究角度让技术实现了这一点？

NIKOLAY SAVINOV:  嗯，当我刚开始研究长上下文时，当时的竞争对手，我记得大概是 128k 或者最多 200k tokens。所以我就在想，该如何为长上下文项目设定目标。当时，这只是 Gemini 项目中非常小的一部分。我最初觉得，仅仅与竞争对手持平听起来并不太令人兴奋。于是我想，不如设定一个雄心勃勃的目标。我认为 100 万就是一个足够有野心的进步。相比于 20 万，这相当于翻了 5 倍。而就在我们推出 100 万后不久，实际上我们又实现了 200 万，大约是之前的 10 倍。我认为比现有技术领先一个数量级是一个不错的目标。正是这一点让人们为之兴奋并投入工作。

----------

LOGAN KILPATRICK: 是的，我很喜欢这个问题。接下来我要问一个更尖锐的问题——我们交付了 100 万，之后很快就交付了 200 万。继续扩大到 100 万到 200 万以上的限制是什么？是从服务角度来看成本太高、太昂贵吗？还是说，让 100 万到 200 万可行的架构在规模更大时就会从根本上崩溃？又或者为什么我们还没有看到长上下文的前沿继续推进？

NIKOLAY SAVINOV: 是的。当我们发布 1.5 Pro 模型时，我们实际上进行了 1000 万的推理测试，并得到了一些质量数据。比如说，在单针检索方面，在整个 1000 万上下文中几乎是完美的。我们本可以发布这个模型。但运行这种推理的成本相当高昂。所以我猜我们当时不确定人们是否愿意为此支付高昂费用，于是我们从价格更合理的方案起步。不过就质量而言，这同样是个值得探讨的问题——由于运行成本过高，我们未能进行充分测试。光是重新启动这个服务器就耗费不菲。除非我们现在就想面向大批客户发布——要知道，我们目前没有足够的芯片来支撑这种规模的应用。

----------

LOGAN KILPATRICK: 是的。你认为这种情况会持续下去吗？就是说——我不确定随着我们处理更多长上下文任务，是否需要呈指数级增长的能力，但你有直觉认为——从研究角度来看，我们需要根本性的突破才能改变现状，让我们能够继续扩大规模吗？还是说，100 万到 200 万可能就是我们的上限，如果想要更多，就做 RAG，然后从模型的角度，非常聪明地管理上下文的进出？

NIKOLAY SAVINOV: 因此，我认为我们实际上需要更多的创新。这不仅仅是蛮力扩展的问题。要真正实现接近完美的 1000 万上下文，我们需要学习更多的创新。但就 RAG 以及未来哪种范式会更强大而言，我认为这些模型的成本会随着时间的推移而降低，我们将尝试将越来越多的通过 RAG 检索到的上下文塞进这些模型中。由于质量也会提高，这样做的好处会越来越大。

---

LOGAN KILPATRICK: 是的，这说得通。你能带我们回顾一下最初实现长上下文的过程吗？据我所知，1.5 Pro 并不是一开始就为长上下文设计的。显然，你曾尝试与 DeepMind 内部其他团队启动这项工作，但最终因为研究进展速度极快——根据我粗略的了解，情况大概是：我们取得了突破，意识到方案可行，随后不久就真正将其整合到了模型中。那么从开始这项研究到最终将其落地成一个对外发布的模型，具体时间线是怎样的？

NIKOLAY SAVINOV: 哦，我觉得那确实相当快。需要澄清的是，我们原本——其实我们是希望能长期发展，达到比如 100 万或 200 万的规模，但真没想到能这么快实现。当它真的发生时，我们就想，哇，这真的很了不起。我们在这项任务上确实取得了进展，所以现在需要把它推出去了。然后我们实际上很快就组建了一支非常棒的团队，而且团队工作非常努力。说实话，我这辈子从没见过有人这么拼命工作。我真的很感动。

----

LOGAN KILPATRICK: 我很喜欢这个。太棒了。那是针对最初的 1.5 Pro 系列。我们也为 1.5 Flash 实现了这个功能。现在 2.0 Flash 也有了。还有 2.5 Pro。我想——你能从更宏观的角度给我们梳理一下发展脉络吗？从最初发布时我们知道长上下文是可能的，到我们发布了 1.5 Pro 的技术报告，展示了"大海捞针"等多项成果，再到现在，我认为 2.5 Pro 模型真正让人惊叹的地方其实就是它在长上下文方面的强大表现，这对编码等使用场景来说简直太棒了。那么从最初发布到现在，长上下文领域都发生了哪些变化？

NIKOLAY SAVINOV: 是的，我认为最大的改进实际上是质量。我们在 128k 上下文和 100 万上下文的质量上都取得了进步。如果我们看一下 2.5 Pro 模型的基准测试结果，我们发现它比许多强大的基线模型都要好，比如 GPT 4.5、Claude 3.7，还有 o3-mini-high 以及一些 DeepSeek 模型。为了真正与这些模型进行比较，我们必须在 128k 上下文中运行评估，这样它们才具有可比性。我们看到 2.5 Pro 有了相当大的改进。而现在，在 100 万上下文的环境中，我们将其与 1.5 Pro 进行了比较，也发现了一些显著的优势。

----------

LOGAN KILPATRICK: 这可能是个奇怪的问题，但模型质量会随着上下文长度的变化而波动吗？比如在处理 10 万 token 输入和 12.8 万 token 时，或者 5 万 tokens 时——质量表现是基本一致的线性变化，还是存在某些异常情况？我在想或许最终模型会将这些差异都泛化掉，变得没有区别。但这个观点是否存在什么微妙之处？我们有没有做过能说明这类问题的评估？

NIKOLAY SAVINOV: 是的，内部我们也看过一些评估结果。我想你的问题可能涉及到人们过去观察到的一些现象。其中非常有名的是"中间迷失"效应。回答你的问题，关于"中间迷失"效应——即在上下文中间出现性能下降的情况，我们的模型确实没有观察到这种现象。但我们观察到的现象是，如果任务难度较高（不像简单的单针操作，而是存在复杂干扰项的任务），随着上下文信息增加，任务完成质量会轻微下降。这正是我们想要改进的问题。

------------

LOGAN KILPATRICK: 是的。为了帮助我理解，当我考虑将十万个标记放入模型的上下文窗口时，从开发者或实际使用长上下文功能的用户角度来看，我是否应该假设模型确实在关注所有不同的上下文？我知道它肯定能处理单个关键信息，可以提取出来。但它是否真的在推理上下文窗口中的所有标记？我对模型在拥有如此大量上下文时背后的运作机制还不太清楚。

NIKOLAY SAVINOV: 是的，这个问题问得很好。有一点需要记住的是：注意力机制本质上存在一个缺陷，因为各 token 之间会相互竞争。当某个标记获得更多注意力时，其他标记获得的注意力就会相应减少。关键在于，如果存在强干扰项(distractor)，其中某个干扰项可能与你寻找的目标信息高度相似，从而吸引大量注意力——这会导致你真正需要的信息获得的注意力减少。标记数量越多，竞争就越激烈。所以这既取决于干扰项的强度，也取决于上下文规模。

--------

LOGAN KILPATRICK: 是的。这又是一个愚蠢的后续问题，但注意力的大小总是固定的吗？比如说，有没有可能获得更多的注意力，还是说它就是一个固定的值 1，然后分散在上下文窗口中的所有标记上，所以标记越多，注意力就越少，这是无法改变的吗？

NIKOLAY SAVINOV: 通常来说，情况确实如此，注意力的总量是有限的。

-------

LOGAN KILPATRICK: 是的。从你举的那个例子来看，困难的干扰项会导致模型做更多的工作，并分散注意力。你的团队或应用方面的其他团队是否探索过预过滤机制之类的东西？比如，你希望长上下文在生产中表现得非常好。听起来最好的结果是上下文窗口中的数据非常不相似。如果存在大量相似数据，而你提出的问题可能与所有这些数据相关，那么在这种使用场景下，性能通常会较差。那么，从这一角度来看，这是开发者或整个行业需要解决的问题吗？还是说，你对人们应该如何应对这个问题有任何建议？

NIKOLAY SAVINOV: 作为一名研究者，我认为这种做法可能偏离了正确方向。与其费心设计各种过滤技巧，我们更应该着力提升内容质量和系统稳健性。不过有个实用建议：尽量避免纳入完全无关的内容。既然明知某些信息毫无价值，何必还要将其纳入上下文？至少这样做会增加成本，实在得不偿失。

---

LOGAN KILPATRICK: 是的，这很有趣，因为我觉得在某种程度上，这与人们使用长文本的核心方式相悖——我在网上看到的例子大多是人们直接把各种随机数据塞进模型的上下文窗口，让它自己筛选出有用的信息。按理说，考虑到剔除无关内容的重要性，模型应该会自行预过滤，只保留相关内容——倒不是说人类懒惰，但我觉得这本来就是卖点之一：用户不需要费心考虑该往上下文窗口里放什么数据。

那么你认为有没有这样一种可能，即模型——你知道的，它更像是一个多部分的系统或类似的东西，其中模型的某个版本实际上正在执行部分工作——根据用户的查询剔除无关数据，然后确保上下文真正传递给模型，这样会稍微容易一些吗？

NIKOLAY SAVINOV: 随着时间的推移，随着模型质量不断提高且成本降低，你就不再需要考虑这个问题了。我只是说——目前的现实情况是，如果你想现在就充分利用它，那么，好吧，我们得现实一点。只是不要放无关的内容。不过我也同意你的观点，如果花太多时间手动筛选或精心挑选哪些内容该放进去，确实很烦人。所以我觉得这两者之间应该有个好的平衡。

LOGAN KILPATRICK: Yeah.

NIKOLAY SAVINOV: 我认为上下文的意义在于简化你的生活并使其更加自动化，而不是让它更耗时或让你花时间手工制作东西。

-----------

LOGAN KILPATRICK: 是的。关于这个问题，我有个后续跟进，主要是从长上下文质量的角度来看评估以及你们正在考虑的评估方案。"大海捞针"显然是最初我们放入 1.5 技术报告中的那个。对于不熟悉的人来说，"大海捞针"就是要求模型在 200 万、100 万甚至 1000 万个标记的上下文中找出特定的信息片段。

这些模型在这方面表现得非常出色。你觉得另一组长期——比如，是否存在一套标准——我认为人们通常——我觉得"大海捞针"这个话题被讨论得有点多，但从长上下文的角度来看，你是否在考虑另一套标准基准？

NIKOLAY SAVINOV: 那么让我们来看看。我认为评估基本上是大型语言模型研究的基石。特别是当你有一个庞大的团队时，评估为整个团队提供了一种协调一致、朝着共同方向努力的方式。

因此，长文本也是如此。如果你想取得进展，就必须有出色的评估结果。现在，大海捞针的问题已经解决，尤其是在干扰项简单的情况下。比如，如果是保罗·格雷厄姆的文章，你插入一句话，像这样——巴塞罗那的神奇数字是37。

告诉我巴塞罗那的神奇数字。这其实已经是一个被解决的问题了。但现在，能力的边界在于处理那些棘手的干扰项。比如说，如果你把整个上下文都塞满了，比如 CTX 的神奇数字是 Y，并且你塞入了上百万个这样的键值对，那么任务就变得困难多了，因为那些干扰项看起来和你想要检索的内容非常相似。

对于大语言模型来说，另一项困难的任务是检索多个关键信息。因此我认为这两点——干扰项的难度和多关键信息检索——正是当前的前沿挑战。但除此之外，评估体系还需要考虑其他因素。

你可能会考虑的一个问题是，那些“大海捞针”式的评估，即使加入了高干扰项，也显得相当人为化，因此你可能想要更贴近现实的测试方式。这种观点确实有道理，但需要记住的是，一旦提高评估的真实性，你可能反而会失去衡量核心长文本理解能力的机会。

例如，如果你向一个非常大的代码库提问，而这个问题基本上只需该代码库中的一个文件就能回答，然后任务是实际实现一些复杂的功能，那么你实际上并没有真正发挥长上下文能力。相反，你是在锻炼编码能力。这样一来，它会给你一个错误的信号来进行爬山算法。

它基本上会在编码上进行爬坡优化，而不是在上下文上。这是需要考虑的一点。另一个考虑因素是人们所说的检索与合成评估。理论上，如果你只需要从大海捞针中检索出一根针，这也可以通过 RAG 来解决。但我们真正应该关注的任务是那些在整个上下文中整合信息的任务。

例如，摘要任务就是这样一项任务，而 RAG 在这方面会面临困难。但现在这些任务听起来方向正确且前景美好，实际上却难以用于自动评估。以摘要指标为例，我们知道像 Rouge 这样的评估标准并不完美。如果你正在进行优化改进，实际上使用那些更不易被"钻空子"的指标会更为合适——我该怎么说呢？就是更难以被取巧的评估标准。

-----------

LOGAN KILPATRICK: 然后简单跟进一下，是什么让它们不那么有用？比如以摘要生成为例，是不是因为判断什么是好摘要更主观，没有一个绝对真实的标准？还是说这个用例本身有什么难点？

NIKOLAY SAVINOV: 是的，那些评估结果会相当嘈杂，因为即便是人类评分者之间的一致性也会相对较低。当然，这并不是说我们不应该研究摘要任务或评估摘要质量。这些都是重要的工作。我只是想说，作为研究者，我个人更倾向于在有很强信号的方向上不断优化。

--------

LOGAN KILPATRICK: 是的，这很有道理。关于长上下文能力，尤其是对 Gemini 来说，它是我们向世界展示的核心能力之一，也是 Gemini 的关键差异化优势。但与此同时，长上下文似乎一直是一个独立的工作方向，并非所有内容都涉及长上下文。你认为是否存在这样一种情况：我们有大量其他团队在攻克各种其他随机问题，比如事实准确性、推理能力等等。

你认为从研究和建模的角度来看，长上下文是否应该融入其他所有工作流程？还是说它仍需要作为一个独立的工作流程存在？毕竟，让模型在长上下文中有效运作与推理相比，其根本方法可能截然不同——打个比方来说，这或许就像处理一个必然的衍生问题那样需要特殊对待？

NIKOLAY SAVINOV: 所以，我的回答有两个方面。首先，我认为为每项重要能力指定一个负责人是有帮助的。但其次，我认为工作流也需要为工作流之外的人提供工具，以便他们能够做出贡献。

--------

LOGAN KILPATRICK: 是的，这非常有道理。我还有一个关于推理的后续问题，我很好奇推理和长上下文之间的相互作用——我们之前请过杰克·雷，昨晚我们还和他一起吃了晚饭，聊了一些随机的推理话题。你觉得——如果我说错了请纠正我——推理能力实际上让长上下文变得更有用了，这一点让你感到惊讶吗？这是一个正常的、预期的结果，仅仅是因为模型花了更多时间思考，还是推理能力和长上下文之间存在某种内在的深层联系，使其更加有效？

NIKOLAY SAVINOV：我认为存在更深层次的联系。其关联在于，如果随着上下文长度的增加，下一个标记预测任务的表现有所提升，那么可以从两个角度来解读。一种理解是：嘿，我要在输入中加载更多上下文，这样对我简短答案的预测也会随之改善。但另一种视角则认为：嘿，你看这些输出标记与输入标记其实非常相似。

因此，如果你允许模型将输出反馈到自己的输入中，那么它某种程度上就变成了输入。所以理论上，如果你具备非常强大的长上下文能力，它也应该有助于你的推理。另一个论点是，长上下文对于推理非常重要，因为如果你只是通过生成一个标记来做决定，即使答案是二元的并且只生成一个标记完全没有问题，可能更倾向于首先生成一个思考轨迹。原因很简单，它们是架构性的。

比如说，当你需要通过上下文进行多次逻辑跳跃来做出预测时，就会受到网络深度的限制，因为这大致相当于注意力层的数量。这就是在上下文跳跃方面制约你的因素。所以你是受限的。但现在，如果你想象把输出反馈到输入中，就不再受限制了。本质上，你可以写入自己的记忆，从而执行比单纯利用网络深度时更复杂的任务。

-----------

LOGAN KILPATRICK: 这真是太有趣了。你和我都与这种推理方式以及长上下文故事有关。我们俩都一直在努力推动让模型能够输出更长的内容，我认为开发者们也很需要这个功能。

我经常看到各种提示。现在我要开始把这些提示发给你，这样你就必须回答这个问题。不过很多人都说，嘿，我们希望输出标记能超过 8000 个。目前我们在一定程度上通过推理标记或推理模型实现了这一点。

它们的输出令牌数为 65,000，但需要注意的是，其中很大一部分输出令牌实际上是用于模型自身思考，而非生成最终的用户响应。长上下文输入能力与长上下文输出能力之间有何关联？这两者是否存在某种相互作用？

因为我觉得对于很多核心用例来说，人们想要的可能是输入百万级 token 然后重构这些 token。你认为我们最终会实现这两种能力合二为一吗？在你看来它们是同一种能力，还是从研究角度而言根本就是两回事？

NIKOLAY SAVINOV: 不，我不认为它们有本质上的不同。我认为重要的是要理解，在预训练之后，模型本身并没有任何限制来生成大量标记。你可以输入，比如说，五十万个标记，然后告诉它——我也不知道——复制这五十万个标记，它实际上会照做。

我们确实尝试过这种方法，而且效果不错。但这种能力在训练后阶段需要格外谨慎地处理。之所以需要如此小心，是因为在训练后阶段存在一个特殊的序列结束标记。如果你的监督微调数据较短，模型就会在序列早期频繁遇到这个结束标记。

而这正是学习的过程，就像，嘿，你总是在X上下文中向我展示这个标记。所以是的，我会在X上下文中生成这个标记并停止生成。这就是你在教我的。这实际上是一个对齐问题。但我想指出的一点是，我觉得推理只是众多长输出任务中的一种。

例如，翻译是另一种类型。而推理则具有非常特殊的格式。它将推理过程封装在某些分隔符中，模型实际上知道我们要求它在其中进行推理。但对于翻译来说，整个输出（不仅仅是推理过程）将会很长。这是我们希望模型能够鼓励产生的另一种能力。因此，这只是一个正确调整模型的问题。实际上，我们正在研究长文本输出。

------------

LOGAN KILPATRICK: 我很兴奋。人们对此非常渴望。我认为这触及了一个更广泛的要点，即开发者应该如何考虑长上下文的最佳实践，以及可能还包括 RAG 的最佳实践。你对这方面有没有一个总体的感觉——我知道你在长达一小时的开发者文档中提供了很多反馈，所以我们已经记录了一些这方面的内容。但对于开发者来说，在考虑如何最有效地利用长上下文时，你对这些建议的总体看法是什么？

NIKOLAY SAVINOV: 所以我认为第一个建议是尽量依赖上下文缓存。让我来解释一下上下文缓存的概念。当你第一次向模型提供长上下文并提出问题时，处理时间会更长，成本也更高；而如果你在同一个上下文的基础上提出第二个问题，就可以利用上下文缓存来使回答既更便宜又更快。

这是我们目前为部分模型提供的功能之一。所以，尽量多依赖这个功能。尝试将用户上传的文件缓存到上下文中，因为这不仅能加快处理速度，而且平均下来，输入令牌的成本会降低四倍。

--------------

LOGAN KILPATRICK: 举个例子来说，最常见的情况——如果我理解有误或与你的思维模型不同，请纠正我——这种技术真正发挥作用的典型应用场景是“与我的文档聊天”、“与PDF聊天”或“与我的数据聊天”这类应用程序。在这些场景中，正如你所指出的，原始输入上下文是相同的。这也是使用上下文缓存的要求之一——再次说明，如果我的理解有误请指正——即你提供的原始上下文必须保持一致。如果由于某些原因，输入上下文在每个请求中都发生变化，那么上下文缓存实际上并不会产生太大效果，因为你需要为存储一组原始输入上下文付出成本，而这些上下文必须在用户逐个请求之间保持持久性。

NIKOLAY SAVINOV: 是的，我想这两个问题的答案都是肯定的。当你需要与一系列文档或某个大型视频进行交互时，这一点很重要。你可能想对其提出一些问题，或者针对一个代码库。你提到这些知识不应该改变是正确的。如果确实需要改变，最佳的改变时机是在最后阶段。因为这样，我们在底层要做的是找到与缓存前缀匹配的部分前缀，然后舍弃其余部分。

有时候，开发者会问一个问题，比如：我们应该把问题放在上下文之前还是之后？这就是答案。你应该把它放在上下文之后，因为如果你想依赖缓存并从中节省成本，那么这就是放置它的地方。因为如果你把它放在开头，并且你打算把所有问题都放在开头，那么你的缓存就要从头开始。

----------

LOGAN KILPATRICK: 是的，太棒了。这很有帮助。还有其他建议吗？除了上下文缓存之外，从开发者的角度还应该考虑些什么？



NIKOLAY SAVINOV: One thing we already touched on, and that's combination with RAG. So if you need to go into billions of tokens of context,
then you need to combine with RAG. But also, in some applications where
you need to retrieve multiple needles, it might still be beneficial to combine with RAG,
even if you need much shorter contexts. Another thing which we already discussed is that, well, don't pack the context with irrelevant stuff.
It's going to affect this multi-needle retrieval. Another interesting thing is, we touched
on the interaction between in-weight and in-context memory.
So one thing I must mention is that if you want to update your in-weight knowledge using
in-context memory, then the network will necessarily get two kinds of knowledge to rely on.
So there might be a contradiction between those two. And I think it's beneficial to resolve
this contradiction explicitly by careful prompting. So for example, you might start your question with saying,
"based on the information above," et cetera. And when you say this, "based on the information above,"
you give a hint to the model that it actually has to rely on in-context memory instead of in-weight memory.
So it results this ambiguity for the model. LOGAN KILPATRICK: I love that.
That's a great suggestion. And your comment about this tension between in-weight versus not-- and again, we talked a little bit about this.
But how do you think about, from a developer perspective, the fine-tuning angle of this? And the only thing that's maybe more controversial
than, is long context going to kill RAG, is should people be fine-tuning at all? And Simon Willison has a bunch of threads about this.
It's like, does anyone actually fine-tune models? Does it end up helping them? How do you think about this from a--
would it be useful to do fine-tuning and long context for a similar corpus of knowledge,
or does the fine-tuning piece potentially lead to better general outcomes for fine-tuning?
How do you think about that interplay? NIKOLAY SAVINOV: Yeah, so let me maybe elaborate on how fine-tuning could actually be used on the knowledge corpus.
So what people sometimes do is, well, they get additional knowledge.
Let's say you have a big enterprise knowledge corpus,
say, billion of tokens. And well, you could continue training the network,
just like we were doing with pre-training. So you could apply language modeling loss.
And you can ask the model to learn how to predict the next token on this knowledge corpus.
But you should keep in mind that this way of integrating information, it actually works, but it has limitations.
And one limitation is, because you're actually going to train the network instead of just supply
the context, you should be prepared for various problems. Like, you will need to tune hyperparameters.
You will need to know when to stop the training. You'll have to deal with the overfitting.
Some people who actually tried to do that, they reported increased hallucinations from using this process.
And they hinted that maybe it's not the best way to supply knowledge information into the network.
But obviously, this technique also has advantages. In particular, it's going to be pretty cheap and fast
at inference time because, well, the knowledge is in the weights, so you're just sampling.
But there are also some privacy implications, because now, the knowledge is cemented into the weights
of the network. And if you actually want to update this knowledge, then you are back to the original problem.
This knowledge is not easy to update. It's in the weights. So how are you going to do it?
You will have to, again, supply this knowledge through the context. LOGAN KILPATRICK: Yeah, I think it's
such an interesting trade-off problem from a developer perspective, about how rapidly you
want to be able to update the information. I think the cost piece of it, it's not cheap to just keep paying to--
I feel like RAG is actually pretty reasonable. You're paying for a vector database, which, I feel like there's a lot of offerings,
and that's reasonably efficient to do at scale. But I think continuously fine-tuning new models is oftentimes potentially not cheap,
which is super-- yeah, a lot of interesting dimensions to take into account. I'm curious about the long-term direction from a fine-tuning--
or maybe not from a fine-tuning-- from a long context perspective. What can folks look forward to in the next three
years for long context, from maybe an experience perspective? But will we even talk about long context in three years?
Will it just be like, the model does this thing and I don't need to care about it, and it just works?
Or, yeah, how are you thinking about this? NIKOLAY SAVINOV: So I'll make a few predictions.
What I think is going to happen first is, the quality of the current 1 or 2 million contexts
is going to increase dramatically, and we are going to max out pretty
much all the retrieval-like tasks quite soon.
And the reason I think it's going to be the first step is because, well, you could say, like, hey,
but why don't we extend the context? Why stop at 1 million or 2 million?
But the point is that the current million context, it's
not close to perfect yet. And while it's not close to perfect, there's a question.
Why do you want to extend it? Because what I think is going to happen is when we achieve close to perfect million context,
then it's going to unlock totally incredible applications,
like something we could never imagine would happen. Like, the ability to process information and connect
the dots, it will increase dramatically. This thing, it already can simultaneously
take in more information than a human can. Like, I don't know, go watch a one-hour video,
and then immediately after that, answer some particular question on that video, like at what second someone is
dropping a piece of paper. You can't really do that very precisely as a human.
So what I think is going to happen is these superhuman abilities, they are going to be more pervasive.
The better long context we have, the more capabilities that we could never imagine are going to be unlocked.
So that's going to be step number one. The quality is going to increase and we're
going to get nearly perfect retrieval. After that, what's going to happen is the cost of long context is going to decrease.
And I think it will take maybe a little bit more time, but it's going to happen.
And as the cost decreases, the longer context
also gets unlocked. So I think reasonably soon, we will see that 10 million
context window, which is a commodity, or it will basically be normal for the providers
to give a 10 million context window, which is currently not the case.
When this happens, that's going to be a deal-breaker for some applications, like coding,
because I think for 1 or 2 million, you can only fit somewhere between small and medium-sized
code base in the context. But 10 million actually unlocks large coding projects
to be included in the context completely. And by that point, we'll have the innovations
which enable near-perfect recall for the entire context.
This thing is going to be incredible for coding applications, because the way humans are coding,
well, you need to hold in memory as much as possible to be effective as a coder,
and you need to jump between the files all the time. And you always have this narrow attention span,
but LLMs are going to circumvent this problem completely.
They're going to hold all this information in their memory at once, and they're
going to reproduce any part of this information precisely. Not only that, they will also be able to really connect the dots.
They will find the connections between the files, and so they will be very effective coders.
I imagine we will very soon get superhuman coding AI assistants.
They will be totally unrivaled and they will basically become the new tool for every coder in the world.
And so when this 10 million happens, that's the second step. And going to, say, 100 million, well, it's more debatable.
I think it's going to happen. I don't know how soon it's going to come.
And I also think we will probably need more deep learning innovations to achieve this.
LOGAN KILPATRICK: Yeah, I love that. One sort of quick follow-up across all three of those dimensions is, like, how much from your mind
is this hardware story or the infrastructure story
relative to the model story? There's obviously a lot of work that has to happen to actually serve long context at scale, which
is why it costs more money to do long contexts, et cetera, et cetera. Do you think about this from a research perspective?
Or is it like, hey, the hardware is going to take care of itself? The TPUs will do their job and I can just focus
on the research side of things? NIKOLAY SAVINOV: Well, yeah. I mean, just having the chips is not enough.
You also need very talented inference engineers.
And I'm really impressed by the work of our inference team.
What they pulled off with the million context, that was incredible.
And without such strong inference engineers,
I don't think we would have delivered 1 or 2 million contexts to customers.
So it's a pretty big inference engineering investment as well.
And no, I don't think it's going to resolve itself. LOGAN KILPATRICK: [LAUGHS] Yeah, our inference engineers
are always working hard because we always want long context on these models.
And yeah, it's not easy to make it happen. How do you think about the sort of interplay
of a bunch of these agentic use cases with long contexts? Is it like a fundamental enabler of different agent experiences
than you could have before? Or what's the interplay between those two dynamics? NIKOLAY SAVINOV: Well, this is an interesting question.
I think agents can be considered both consumers
and suppliers for long context. So let me explain this. So the agents, to operate effectively,
they need to keep track of the last state, like the previous actions that they took, the observations that they made, et cetera.
And of course, the current state as well. So to keep all these previous interactions in memory,
you need longer context. So that's where longer context is helping agents.
That's where the agents are the consumers of long context.
But there is also another orthogonal perspective, is that agents are actually suppliers of long context
as well. And this is because [? packing ?] long context by hand is incredibly tedious.
If you have to upload all the documents that you want
by hand every time, or upload a video or I don't know,
copy/paste some content somewhere from the web, this is really tedious.
You don't want to do that. You want the model to do it automatically. And one way to achieve this is through the agentic tool calls.
So if the model can decide on its own, hey, at this point, I'm going to fetch some more information,
then it's going to just pack the context on its own. And so yeah, in that sense, agents
are the suppliers of long context. LOGAN KILPATRICK: Yeah, that's such a great example.
My two cents-- and I've had many conversations with folks about this-- I think this is actually one of the main limitations of how
people interact with AI systems, is your example of, it's tedious. It's so tedious. The worst part about doing anything with AI
is, I have to go and find all the context that might be relevant for the model and personally
bring that context in. And in many cases, the context is already on my screen
or on my computer. Or I have the context somewhere, but it's like, I have to do all the heavy lifting. So I'm excited for--
we should build some long context agent system that just goes and gets your context from everywhere.
I think that would be super, super interesting, and I feel like solves a very fundamental problem, not only for developers, but from an end user of AI systems
perspective. I wish the models could just go and fetch my context and I didn't have to do it all. NIKOLAY SAVINOV: Yeah, MCP for the win.
LOGAN KILPATRICK: [LAUGHS] I love that. Nicolay, this was an awesome conversation. Thank you for taking the time. I'm glad we got to do this in person
and appreciate all the hard work from you and the long context teams. And hopefully, we'll have lots more exciting,
long context stuff to share with folks in the future. NIKOLAY SAVINOV: Yeah, thanks for inviting me. It was fun to have this conversation.
LOGAN KILPATRICK: Yeah, I love it.