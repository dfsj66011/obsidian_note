

## 一、线性回归

### 概述

线性回归是一种基础且广泛使用的统计技术，在机器学习和统计分析中都很重要。其目的是根据一个或多个自变量（解释变量）来预测因变量（目标变量）。由于其简单性、可解释性以及在理解更复杂模型中的基础作用，这种方法非常受欢迎。尽管机器学习算法的复杂性不断增加，线性回归仍然是各个领域预测建模和解释性分析的重要工具。

线性回归是一种用于建模因变量（结果变量）与一个或多个自变量（预测变量）之间关系的统计方法。为了使线性回归产生有效和可靠的结果，需要满足几个假设。以下是线性回归的主要假设：

* *线性关系*：假设自变量（预测变量）与因变量（结果变量）之间的关系是线性的。具体来说，因变量的期望值是自变量的线性函数。
* *误差独立性*：残差（观察值与预测值之间的差异）应该彼此独立。这意味着不应存在自相关，尤其是在时间序列数据中很重要。
* *同方差性（误差的恒定方差）*：残差在所有自变量水平上应该具有恒定的方差。这意味着残差的分布在预测值范围内应相似。如果存在异方差性，表明模型在某些值上的表现优于其他值。
* *误差的正态性*：残差应该是正态分布的。此假设在进行假设检验（如计算 $p$ 值或置信区间）时尤为重要。
* *无多重共线性*：自变量之间不应高度相关。如果存在多重共线性，难以确定每个预测变量的单独影响。
* *无内生性（误差与预测变量无相关）*：自变量不应与误差项相关。此假设确保预测变量真正独立于误差项，并未捕获任何遗漏变量偏差。

### 目标、损失

线性回归的核心是表示目标变量与一个或多个预测变量之间关系的方程：$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$$
- 其中：
  - $y$：我们要预测的因变量（目标变量）。
  - $x_1, x_2, \ldots, x_p$：自变量（解释变量）。
  - $\beta_0$：截距项。
  - $\beta_1, \beta_2, \ldots, \beta_p$：系数（或权重），量化每个预测变量 $x_i$ 对 $y$ 的影响。
  - $\epsilon$：误差项，表示不可简化的噪声或未建模的数据方面。

线性回归的目标是估计系数（$\beta$），以最小化基于给定自变量集的因变量的预测误差。

**损失函数：** 均方误差 (MSE) 是回归模型常用的损失函数。通过计算实际值与预测值之间平方差的平均值来获得：$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$较低的 MSE 表示模型的预测值更接近实际值。

### 优化方法

**梯度下降**  是一种迭代优化算法，用于最小化代价函数（如 MSE）。它通过计算误差函数的梯度（或斜率），并更新系数，使误差最小化。对于大数据集或多特征问题，梯度下降非常高效。线性回归中梯度下降的更新规则是：$$\beta_j = \beta_j - \alpha \frac{\partial}{\partial \beta_j} \text{MSE}$$其中：
    - $\alpha$ 是学习率（一个控制步长的小正数）。
    - $\frac{\partial}{\partial \beta_j} \text{MSE}$ 表示代价函数对系数 $\beta_j$ 的导数。

**正规方程**   对于较小的数据集或计算复杂度不重要的问题，正规方程提供了一个封闭解来找到最佳系数。正规方程通过最小化残差平方和（RSS）得出，公式为：$$\hat{\beta} = (X^TX)^{-1}X^Ty$$其中 $X$ 是输入特征矩阵，$y$ 是目标值向量。

### 扩展

线性回归可以通过多种方式扩展，包括：

  - **正则化回归**：在代价函数中添加惩罚项以防止过拟合（例如，Lasso 回归和 Ridge 回归）。
  - **多项式回归**：通过添加自变量的多项式项来建模非线性关系。
  - **多元回归**：用于多个自变量预测一个因变量的情况。
  
  在这些扩展模型中解释系数遵循相同的原则，但由于存在多个预测变量或转换，可能需要对模型行为进行更细致的解释。

### FAQs

1. **线性回归是否假设特征与结果变量线性相关？**

是的，线性回归假设自变量（特征）与结果变量之间的关系是线性的。然而，这并不意味着个别预测变量必须以收集到的形式出现（即不进行转换）。你可以在线性回归模型中包含预测变量的非线性转换。例如，可以在模型中包含多项式项（如 $x^2$、$x^3$）或交互（交叉）项（如 $x_1 \times x_2$）。只要模型在系数上是线性的，它仍然是“线性回归”模型。

2. **非线性（交叉）特征如何符合线性假设？**

在回归模型中包含非线性项或交叉特征（交互项）并不违反线性回归的假设。关键在于模型在参数上保持线性。例如，考虑模型中的交互项 $x_1 \times x_2$：$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3(x_1 \times x_2) + \epsilon$$尽管 $x_1$ 和 $x_2$ 之间存在非线性交互，模型仍被认为是线性的，因为结果 $y$ 是参数 $\beta_0, \beta_1, \beta_2, \beta_3$ 的线性组合。

3. **什么是多重共线性？**

多重共线性发生在线性回归模型中，当两个或多个自变量高度相关时，导致信息重叠或冗余。在这种情况下，模型难以分离每个预测变量对因变量的独立影响，导致系数估计不可靠、标准误差膨胀以及解释困难。这对线性回归是一个重要的限制，因为它削弱了模型准确估计预测变量效应的能力。多重共线性可以通过方差膨胀因子（VIF）等诊断工具识别，并通过消除相关变量、应用正则化技术或使用降维方法（如主成分分析，PCA）来解决。

4. **多重共线性如何影响线性回归？**

* 不稳定的系数（膨胀的标准误差）：多重共线性使回归系数的估计对模型或数据的微小变化非常敏感，导致标准误差膨胀，使系数不可靠。因此，一个可能与因变量有显著关系的变量可能显得统计上不显著。
* 解释困难：当预测变量高度相关时，解释单个系数变得更加困难。例如，如果两个变量几乎完全相关，模型难以为每个变量分配适当的权重，导致单个系数不可靠或不直观。
* 降低统计功效：多重共线性的存在降低了估计系数的精确性，从而减少假设检验的统计功效。这使得检测真正存在的显著效应变得更加困难。
* 高方差膨胀因子（VIF）：方差膨胀因子（VIF）常用于检测多重共线性。如果任何预测变量的 VIF 很大（通常大于 5 或 10），则表明存在多重共线性。高 VIF 意味着该变量与其他预测变量高度相关。

4. **多重共线性是线性回归的限制吗？**

是的，多重共线性被认为是线性回归的限制。然而，这不是模型本身的固有限制，而是数据中的问题，可能影响线性回归模型的性能和可解释性。




## 二、逻辑回归

### 概述

逻辑回归是一种基本且强大的监督学习算法，广泛用于二分类任务。在机器学习中，监督学习涉及在输入-输出对上训练模型，以学习能够在未见数据上进行预测的模式。逻辑回归专门用于根据输入特征预测分类结果的概率，其中结果属于两个类别之一。尽管逻辑回归可以扩展到多类分类，但其最常见的应用是二分类。

逻辑回归的核心是估计给定观测值属于某一特定类别的概率。这个概率是通过一个 S 型函数（sigmoid 函数）得出的，该函数将线性方程的输出转换为介于 0 和 1 之间的概率值。通过这个 S 型函数建模输入特征与二元响应变量之间的关系，使得输出可以被解释为观测值属于某一特定类别的可能性。

逻辑回归利用梯度下降或最大似然估计（MLE）等技术来寻找这种关系的最佳参数，目标是最小化预测误差。这些优化方法调整模型的系数以最佳拟合数据，使得逻辑回归成为机器学习中既灵活又可解释的分类工具。

### 目标、损失

对于二分类任务，逻辑回归模型预测概率 $P(y=1 \mid X)$，其中 $y$ 是二元结果（0 或 1），$X = (x_1, x_2, \ldots, x_k)$ 表示输入特征。逻辑回归模型基于以下方程：$$P(y=1 \mid X) = \text{sigmoid}(z) = \frac{1}{1 + e^{-z}}$$其中：
* $z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k$
* $\beta_0, \beta_1, \ldots, \beta_k$ 是模型的系数（参数）
* Sigmoid 函数确保输出是介于 0 和 1 之间的概率

线性预测器 $z$ 是输入特征的线性组合，类似于线性回归。然而，逻辑回归通过应用 sigmoid 函数来确保输出可以解释为概率，而不是预测连续结果。

**损失函数**：对于二分类，合适的损失函数是对数损失（也称为二元交叉熵）。给定数据集有 $n$ 个样本时，对数损失定义为：$$\text{Log-Loss} = -\sum_{i=0}^{n} (y_i \cdot \log(p_i) + (1-y_i) \cdot \log(1-p_i))$$其中：
* $y_i$ 是第 $i$ 个观测的真实标签。
* $p_i$ 是第 $i$ 个观测的预测概率。

对数损失惩罚预测概率与实际标签之间的较大偏差，当模型自信但错误时（即预测概率接近 1 而结果为 0，或相反情况），惩罚更大。最小化对数损失函数可以提高预测的准确性。

### 优化方法

**梯度下降**：根据以下规则更新参数：$$\beta_j^{(t+1)} = \beta_j^{(t)} - \alpha \frac{\partial \text{Log-Loss}}{\partial \beta_j}$$其中：
* $\alpha$ 是学习率（步长）。
* $\frac{\partial \text{Log-Loss}}{\partial \beta_j}$ 是对数损失关于 $\beta_j$ 的偏导数。

**MLE** 是估计逻辑回归系数的另一种方法。它通过最大化给定模型预测的情况下观察到的数据发生的可能性来实现。对数似然函数是似然函数的对数，*最大化对数似然等同于最小化对数损失*。这可以通过将对数似然函数关于参数的偏导数设为零并求解所得的方程组来实现。


### FAQs

1. **为什么逻辑回归中使用 Sigmoid 函数？**

Sigmoid 函数在逻辑回归中起着核心作用，因为它将预测值从 $-\infty$ 到 $+\infty$ 映射到 0 到 1 之间的范围，非常适合概率估计。逻辑回归是一种分类算法，旨在预测观测值属于特定类别（如二分类：0 或 1）的概率。

通过应用 Sigmoid 函数，将输入特征的无界线性组合转换为表示观测值属于正类的概率值。该函数在事件的对数几率与其概率之间提供了自然的联系。此外，其平滑和可微的性质确保了模型训练过程中的高效优化，使其非常适合该任务。

2. **为什么不使用其他激活函数？**

其他函数如阶跃函数或双曲正切（tanh）也可能被考虑，但它们有局限性：
  - 阶跃函数：输出离散值（0 或 1），不适合基于梯度的优化。
  - 双曲正切：输出值在 -1 到 1 之间，不适合概率估计。

相比之下，Sigmoid 函数简单、计算效率高，非常适合逻辑回归中的概率解释。
