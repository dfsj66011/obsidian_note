[Distilled AI](https://aman.ai/primers/ai/)[Back to aman.ai](https://aman.ai/)

# Primers • Naive Bayes

- [Overview](https://aman.ai/primers/ai/naive-bayes/#overview)
- [From Bayes’ Theorem →→ Naive Bayes](https://aman.ai/primers/ai/naive-bayes/#from-bayes-theorem-rightarrow-naive-bayes)
- [Example: Applying Bayes’ Theorem with All Terms](https://aman.ai/primers/ai/naive-bayes/#example-applying-bayes-theorem-with-all-terms)
    - [Given Data](https://aman.ai/primers/ai/naive-bayes/#given-data)
    - [Calculating Posterior Probabilities](https://aman.ai/primers/ai/naive-bayes/#calculating-posterior-probabilities)
    - [Conclusion](https://aman.ai/primers/ai/naive-bayes/#conclusion)
- [Key Concepts](https://aman.ai/primers/ai/naive-bayes/#key-concepts)
    - [Bayesian Probability](https://aman.ai/primers/ai/naive-bayes/#bayesian-probability)
    - [Conditional Independence Assumption](https://aman.ai/primers/ai/naive-bayes/#conditional-independence-assumption)
    - [Types of Naive Bayes Classifiers](https://aman.ai/primers/ai/naive-bayes/#types-of-naive-bayes-classifiers)
    - [Parameter Estimation](https://aman.ai/primers/ai/naive-bayes/#parameter-estimation)
- [Loss Function](https://aman.ai/primers/ai/naive-bayes/#loss-function)
- [Applications](https://aman.ai/primers/ai/naive-bayes/#applications)
    - [Text Classification and Sentiment Analysis](https://aman.ai/primers/ai/naive-bayes/#text-classification-and-sentiment-analysis)
    - [Medical Diagnosis](https://aman.ai/primers/ai/naive-bayes/#medical-diagnosis)
    - [Email Spam Filtering](https://aman.ai/primers/ai/naive-bayes/#email-spam-filtering)
    - [Recommendation Systems](https://aman.ai/primers/ai/naive-bayes/#recommendation-systems)
    - [Real-time Prediction](https://aman.ai/primers/ai/naive-bayes/#real-time-prediction)
- [Pros and Cons](https://aman.ai/primers/ai/naive-bayes/#pros-and-cons)
    - [Pros](https://aman.ai/primers/ai/naive-bayes/#pros)
    - [Cons](https://aman.ai/primers/ai/naive-bayes/#cons)
- [Example](https://aman.ai/primers/ai/naive-bayes/#example)
    - [Problem: Spam Detection](https://aman.ai/primers/ai/naive-bayes/#problem-spam-detection)
        - [Step 1: Compute Priors](https://aman.ai/primers/ai/naive-bayes/#step-1-compute-priors)
        - [Step 2: Compute Likelihoods](https://aman.ai/primers/ai/naive-bayes/#step-2-compute-likelihoods)
        - [Step 3: Compute Posterior](https://aman.ai/primers/ai/naive-bayes/#step-3-compute-posterior)
        - [Step 4: Classification](https://aman.ai/primers/ai/naive-bayes/#step-4-classification)
- [Citation](https://aman.ai/primers/ai/naive-bayes/#citation)

## Overview

- Naive Bayes is a family of probabilistic algorithms based on Bayes’ theorem, with the “naive” assumption of conditional independence between every pair of features given the class label. It is widely used for classification tasks due to its simplicity, efficiency, and interpretability.
- Naive Bayes algorithms are primarily used in supervised learning tasks for classification problems. The approach applies Bayes’ theorem to calculate the probability of a given sample belonging to each possible class and assigns the sample to the class with the highest posterior probability.
- Despite its simplifying assumptions, Naive Bayes remains a robust and efficient tool for a wide range of practical applications, particularly in domains like text analysis and medical diagnostics. Its simplicity and interpretability make it a popular choice for baseline models and quick analyses.

## From Bayes’ Theorem →→ Naive Bayes

- Bayes’ theorem is expressed as:
    
    P(C|X)=P(X|C)P(C)P(X)P(C|X)=P(X|C)P(C)P(X)
    
    - where:
        - P(C∣X)P(C∣X) is the posterior probability of the class CC given the data XX.
        - P(X∣C)P(X∣C) is the likelihood of data XX given class CC.
        - P(C)P(C) is the prior probability of class CC.
        - P(X)P(X) is the marginal probability of data XX.
- The “naive” assumption simplifies the likelihood P(X∣C)P(X∣C) by assuming that features are conditionally independent given the class. For nn features x1,x2,…,xnx1,x2,…,xn, this assumption allows us to express:
    

P(X|C)=P(x1,x2,…,xn|C)=∏i=1nP(xi|C)P(X|C)=P(x1,x2,…,xn|C)=∏i=1nP(xi|C)

## Example: Applying Bayes’ Theorem with All Terms

- To understand the terms in Bayes’ theorem, consider a simple spam email classification problem. Suppose we want to classify an email as either “Spam” (S) or “Not Spam” (¬S¬S) based on a feature XX, which is the presence of the word “offer” in the email.

### Given Data

1. **Prior probabilities**:
    
    - P(S)=0.2P(S)=0.2 (20% of emails are spam).
    - P(¬S)=0.8P(¬S)=0.8 (80% of emails are not spam).
2. **Likelihoods**:
    
    - P(X∣S)=0.9P(X∣S)=0.9 (90% of spam emails contain the word “offer”).
    - P(X∣¬S)=0.1P(X∣¬S)=0.1 (10% of non-spam emails contain the word “offer”).
3. **Marginal probability**:
    
    - P(X)=P(X∣S)P(S)+P(X∣¬S)P(¬S)P(X)=P(X∣S)P(S)+P(X∣¬S)P(¬S): P(X)=(0.9⋅0.2)+(0.1⋅0.8)=0.18+0.08=0.26.P(X)=(0.9⋅0.2)+(0.1⋅0.8)=0.18+0.08=0.26.

### Calculating Posterior Probabilities

- Using Bayes’ theorem:

1. **For Spam**: P(S|X)=P(X|S)P(S)P(X)=0.9⋅0.20.26=0.180.26≈0.692.P(S|X)=P(X|S)P(S)P(X)=0.9⋅0.20.26=0.180.26≈0.692.
    
2. **For Not Spam**: P(¬S|X)=P(X|¬S)P(¬S)P(X)=0.1⋅0.80.26=0.080.26≈0.308.P(¬S|X)=P(X|¬S)P(¬S)P(X)=0.1⋅0.80.26=0.080.26≈0.308.
    

### Conclusion

- The posterior probabilities indicate that the email is more likely to be spam (P(S∣X)≈0.692P(S∣X)≈0.692) than not spam (P(¬S∣X)≈0.308P(¬S∣X)≈0.308). Thus, the email would be classified as spam based on this model.
- This example demonstrates the roles of all terms in Bayes’ theorem:
    - **Prior probabilities** (P(S)P(S) and P(¬S)P(¬S)) represent the base rates of each class.
    - **Likelihoods** (P(X∣S)P(X∣S) and P(X∣¬S)P(X∣¬S)) quantify the probability of observing the feature given each class.
    - **Marginal probability** (P(X)P(X)) ensures proper normalization of posterior probabilities.
    - **Posterior probability** (P(S∣X)P(S∣X)) provides the final classification decision.

## Key Concepts

### Bayesian Probability

- The foundation of Naive Bayes lies in Bayesian probability, where we update our prior beliefs based on new evidence.

### Conditional Independence Assumption

- The algorithm assumes that features are independent given the class. This assumption simplifies computation but may not always hold in real-world data.

### Types of Naive Bayes Classifiers

- Depending on the type of data, different variations of Naive Bayes are used:
    - **Gaussian Naive Bayes**: Assumes continuous data is normally distributed.
    - **Multinomial Naive Bayes**: Suitable for discrete count data, such as word frequencies in text classification.
    - **Bernoulli Naive Bayes**: Suitable for binary/boolean features.

### Parameter Estimation

- Parameters P(C)P(C) (prior) and P(xi∣C)P(xi∣C) (likelihood) are estimated using Maximum Likelihood Estimation (MLE), which counts the occurrences in the training data to compute probabilities.

## Loss Function

- The goal of Naive Bayes is to maximize the posterior probability. Alternatively, we minimize a loss function based on the log-likelihood. The negative log-likelihood (NLL) is commonly used as the loss function:
    
    NLL=−∑i=1NlogP(Ci|Xi)NLL=−∑i=1Nlog⁡P(Ci|Xi)
    
    - where NN is the number of training samples, CiCi is the true class label for sample ii, and P(Ci∣Xi)P(Ci∣Xi) is the predicted probability for the correct class.

## Applications

### Text Classification and Sentiment Analysis

- Naive Bayes is frequently used in Natural Language Processing (NLP) tasks such as spam detection, sentiment analysis, and document classification due to its efficiency with high-dimensional data.

### Medical Diagnosis

- Useful for predicting diseases based on symptoms, assuming conditional independence between symptoms.

### Email Spam Filtering

- Detects spam emails by analyzing the frequency of specific words in the email content.

### Recommendation Systems

- Predicts user preferences based on past behaviors.

### Real-time Prediction

- Due to its computational efficiency, Naive Bayes is effective in real-time applications.

## Pros and Cons

### Pros

1. **Simplicity**: Easy to implement and interpret.
2. **Efficiency**: Computationally fast, especially with large datasets.
3. **Scalability**: Handles high-dimensional data well, such as text data.
4. **No Iterative Parameter Estimation**: Training involves simple computations.

### Cons

1. **Conditional Independence Assumption**: Rarely holds in practice, which may degrade performance.
2. **Limited Expressiveness**: Assumes simple relationships between features and the target variable.
3. **Data Imbalance**: Sensitive to imbalanced datasets, as probabilities can be dominated by frequent classes.
4. **Feature Engineering**: May require careful preprocessing and feature selection for optimal performance.

## Example

### Problem: Spam Detection

- Suppose we have two classes: C1C1 (Spam) and C2C2 (Not Spam). The features are words in an email (X={x1,x2,x3,…,xn}X={x1,x2,x3,…,xn}).

#### Step 1: Compute Priors

P(C1)=Number of Spam EmailsTotal EmailsP(C1)=Number of Spam EmailsTotal Emails

P(C2)=Number of Not Spam EmailsTotal EmailsP(C2)=Number of Not Spam EmailsTotal Emails

#### Step 2: Compute Likelihoods

- For each word xixi, compute P(xi∣C1)P(xi∣C1) and P(xi∣C2)P(xi∣C2) using word frequencies.

#### Step 3: Compute Posterior

- For a new email, compute:

P(C1|X)∝P(C1)∏i=1nP(xi|C1)P(C1|X)∝P(C1)∏i=1nP(xi|C1)

P(C2|X)∝P(C2)∏i=1nP(xi|C2)P(C2|X)∝P(C2)∏i=1nP(xi|C2)

#### Step 4: Classification

- Assign the email to the class with the highest posterior probability.

## Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledNaiveBayes,   title   = {Naive Bayes},   author  = {Chadha, Aman},   journal = {Distilled AI},   year    = {2020},   note    = {\url{https://aman.ai}} }`

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)| 

[www.amanchadha.com](https://www.amanchadha.com/)
