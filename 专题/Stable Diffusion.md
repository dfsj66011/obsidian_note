
Stable Diffusion 是一种基于扩散模型的文本生成图像深度学习模型，由慕尼黑路德维希-马克西米利安大学的 CompViz 团队于 2022 年开发。项目地址：[https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)

#### 生成模型

生成模型学习数据集的概率分布，从而我们可以从该分布中采样以创建新的数据实例。例如，如果我们有许多猫的图片并在其上训练生成模型，那么我们可以从该分布中采样以创建新的猫的图像。

想象你是一名罪犯，想要生成成千上万的虚假身份。每个虚假身份都由变量组成，代表一个人的特征（年龄、身高）。你可以向政府统计部门索取有关人口年龄和身高的统计数据，然后从这些分布中进行抽样。例如，把人口的年龄建模为一个高斯分布，均值为 40，方差为 30；身高的均值为 120 厘米，方差为 100。

起初，你可以从每个分布中独立采样以创建一个虚假身份，但这会产生不合理的（年龄，身高）组合。要生成合理且可信的虚假身份信息，必须考虑 *联合分布*，否则可能会出现（3 岁，130cm）这样不合理的组合。我们还可以通过 *条件概率* 和/或 *边缘化* 一个变量来评估其中一个变量的概率。

生成模型的工作原理：我们将数据建模为一个极其庞大的联合分布，然后学习这个分布的参数。由于这个分布非常复杂，我们让神经网络来学习这些参数。我们有一个由图像组成的数据集，我们想学习一个非常复杂的分布，然后可以用来从中 *采样*。

#### 扩散模型

我们将通过包含一些潜在变量来将我们的系统建模为一个联合分布。

**前向过程**：逐步添加噪声，逐步得到多元高斯噪声 $\mathcal{N}\left(\mathbf{x}_{T} ; \mathbf{0}, \mathbf{I}\right)$，例如 $T=1000$。这个*前向扩散过程是预先固定的*，有一个具体的分析公式来说明如何向图像添加噪声，定义了如何基于前一幅图像逐步构建出加噪版本。

**反向过程**：问题在于我们没有这一过程的反向解析公式，不知道如何直接去除噪声，所以只能通过训练神经网络来学习并执行这个逆向过程，从而从含有噪声的内容中去除噪声。

在正向过程中，图像添加的噪声，均值以之前的图像为中心，而方差则由 $\beta$ 参数（由我们设定）决定，它表示在这个过程的每一步中我们想要添加多少噪声。这里有一个有趣的闭环公式 (4)，可以直接从原始图像计算出时间步 $T$ 时的图像，而无需计算所有中间图像，使用的是这种特定的参数化方法。这里的均值依赖于参数 $\bar{\alpha}$，而 $α$ 实际上又依赖于 $β$，这意味着这些参数是我们已知的，无需额外学习。同样，方差实际上也依赖于 $α$，而 $α$ 又是 $β$ 的函数来定义的。

在反向过程中，我们想要去除噪声，并且我们也将其定义为具有均值 $μ_θ$ 和方差 $Σ_θ$ 的高斯分布，这的均值和方差是未知的，必须学习它们。实际上，方差我们会设为固定值——我们会以某种方式参数化它，使得这个方差实际上是固定的。因此网络仅需要学习这个分布的均值。

#### 训练

那么我们实际上如何训练一个模型来完成这个任务呢？我们的初始目标实际上是学习整个数据集上的概率分布。求边缘概率的方法在理论上是可能的，但实际上需要无限的时间，因此我们无法在这里使用这个方法。

我们想要学习参数 $\theta$ 来最大化似然函数。在这里我们所做的是定义了一个下界，为这个量找到了一个下限，ELBO，如果最大化这个下界，也就相当于最大化似然函数。这表明我们需要训练一个 $\epsilon_{\theta}$ 网络，给定一张带有噪声的图像，以及添加噪声的时间步，该网络必须预测图像中有多少噪声（即被噪声化的图像）。采用梯度下降方法进行优化参数。

这里有很多概念需要理解，目前只需记住有一个正向过程和一个反向过程。为了训练这个网络执行反向过程，我们需要训练一个网络来检测在时间步 $t$ 时图像噪声版本中的噪声量。

#### 采样

假设已经有一个训练好的网络，我们的做法是从纯噪声开始，然后让网络检测其中有多少噪声，我们去除这个噪声，并进行迭代，直到得到一个清晰的图片，但目前我们无法控制具体会生成什么样的新图像。

要找到一种方法，告诉模型在这个生成过程中我们想要什么。想法是，从纯噪声开始，在这一系列去除噪声的过程中，引入一个信号（prompt），也可以称为“条件信号”（conditioning signal），或者“上下文”（context）。即我们通过这种方式影响模型如何去除噪声，从而使输出朝着我们想要的方向发展。

我们让模型学习数据的分布，但这个分布并不包含任何告诉模型什么是猫、什么是狗等信息。模型只是在学习如何生成看起来合理、与初始训练数据相似的图片，但它并不知道这些图片与提示词之间的关系。

所以一个想法可能是：我们能否学习一个关于初始数据的分布或联合分布，也就是所有图像和条件信号（即提示）的分布？但这也是我们不想要的，因为我们实际上是想学习这个分布，以便能够采样并生成新的数据。我们不想学习联合分布，因为它会受到上下文太大的影响，模型可能无法学习数据的生成过程。核心思想是：我们训练一个模型，在训练过程中，有时给它提示，有时不给，这样模型就学会了既忽略提示又关注提示。

当我们从这个模型中进行采样时，我们会执行两个步骤：首先，我们给它提供我们想要的提示；然后，我们再次输入相同的噪声，但不包含我们想要的提示。接着，我们将这两个输出——有条件的和无条件的——通过一个权重线性组合起来，这个权重表示我们希望输出在多大程度上接近我们的条件提示。这个值越高，输出就越像我们的提示；这个值越低，输出就越不像我们的提示。

这就是无分类器引导背后的理念：为了真正提供提示，我们需要给模型某种嵌入表示。模型需要理解这个提示，而要理解提示，模型需要某种嵌入——嵌入意味着我们需要一些向量来表示提示的含义。这些嵌入是通过 CLIP 文本编码器提取的。


（37100）


他们构建了这个矩阵——你可以看到它由第一张图像嵌入的点积组成，与所有可能的字幕相乘。比如第一张图配第一段文本、第一张图配第二段文本、第一张图配第三段文本，然后第二张图配第一段文本、第二张图配第二段文本等等。训练原理在于：我们知道图像与文本的对应关系位于矩阵对角线上，因为第一张图对应第一段文本，第二张图对应第二段文本，第三张图对应第三段文本。

那么他们是怎么训练的呢？基本上，他们说他们构建了一个损失函数，希望这个对角线上的值最大，而其他所有数字都为零，因为这些数字不匹配，不是这些图像的对应描述。通过这种方式，模型学会了如何将图像的描述与图像本身结合起来。而在稳定扩散中，我们所做的就是在这里使用这个文本编码器。

所以只有这部分剪辑用于编码我们的提示，以获取一些嵌入向量，然后这些嵌入向量被用作我们单元的条件信号，将图像去噪成我们想要的样子。嗯，好的，还有另一件事我们需要理解，正如我之前所说，我们有一个正向过程，向图像添加噪声，然后我们有一个反向过程，从图像中去除噪声，而这个反向过程可以通过使用无分类器指导来进行条件化。

而这个反向过程意味着我们需要进行多次去噪步骤才能得到新图像，这也意味着每一步都需要将带有噪声的图像输入单元，并输出该图像中的噪声量。但如果图像非常大，假设这里的图像是512乘以512，那就意味着每次在单元上我们都需要处理一个非常大的矩阵通过这个单元。

这可能非常慢，因为它是一个非常大的数据矩阵，单元必须处理。如果我们能以某种方式将这张图像压缩成更小的东西，使得通过单元的每一步所需的时间更少，那会怎样呢？嗯，想法是，是的，我们可以用所谓的变分自动编码器来压缩这张图像。

让我们来看看变分自编码器是如何工作的。嗯，好的，稳定扩散实际上被称为潜在扩散模型，因为我们学习的不是数据的概率分布PX，而是通过变分自编码器学习数据的潜在表示。简单来说，我们压缩数据，回到刚才说的，我们将数据压缩成更小的形式，然后利用这个压缩版本的数据（而非原始数据）学习扩散过程，之后我们可以解压缩以重建原始数据。让我给...

让我来实际演示一下它的工作原理。假设你有一些数据，想通过互联网发送给朋友，你会怎么做？你可以发送原始文件，也可以发送压缩文件。比如，你可以用WinZip之类的工具压缩文件，然后发送给朋友。朋友收到后解压，就能重建原始数据。这正是自动编码器的工作方式。

自动编码器是一种网络，例如给定一张图像，经过编码器处理后，会转换成一个维度远小于原始图像的向量。如果我们使用这个向量并通过解码器运行它，就能重建出原始图像。我们可以对许多图像进行这样的操作，每张图像都会在这个被称为“编码”的向量空间中有一个对应的表示。

现在自动编码器的问题在于，从语义角度来看，该模型学习到的编码没有任何意义。例如，与猫相关的编码可能与披萨或建筑物的编码非常相似。这些编码之间没有语义关联。为了克服自动编码器的这一局限性，我们引入了变分自动编码器，通过学习以某种方式压缩数据。

但与此同时，这些数据是按照多元分布来分布的，大多数情况下是高斯分布，我们学习这个非常复杂的分布的均值和西格玛，给定潜在表示，我们总是可以通过解码器来重建原始数据，这也是我们在稳定扩散中使用的想法。现在，我们终于可以将所有这些我们见过的东西结合起来，看看稳定扩散的架构是什么。

那么让我们从文本生成图像的原理开始讲起。想象一下，文本生成图像的基本工作原理是这样的：假设你想生成一张戴眼镜的狗的图片。首先，你当然要从一个提示词开始，比如"戴眼镜的狗"。接下来我们要做的是采样一些噪声，这里是从标准正态分布N(0,1)中采样的噪声。我们用变分编码器对这些噪声进行编码，这会得到一个潜在空间的噪声表示，我们称之为Z。这当然还是纯粹的噪声，但已经被编码器压缩过了。然后我们把它送入UNet网络。UNet的目标是检测其中有多少噪声。

也是因为对于这个单元，我们还提供了调节信号。该单元必须检测噪声，即我们需要去除多少噪声，才能将其转化为符合提示的图像，也就是一张狗的图片。因此，我们将初始时间步（如1000）与该单元一起传递，该单元将在输出端进行检测。

我们的调度程序有多少噪音？我们稍后会看到调度程序将如何消除这些噪音，然后再次将其发送到单元进行第二步去核化。这次我们发送的时间步长不是91000，而是例如980，因为我们跳过了一些步骤。然后我们再次检测有多少噪音，调度程序将消除这些噪音并再次发送回去。我们多次重复这个过程，持续进行多步去核化，直到图像中不再存在噪音。

在我们完成这一系列步骤的循环后，我们得到了输出 Z'，它仍然是一个潜在变量，因为这个单元只处理数据的潜在表示，而不处理原始数据。我们将其通过解码器以获得输出图像，这就是为什么它被称为潜在扩散模型，因为去噪过程始终与数据的潜在表示一起工作。

这就是我们如何生成文本到图像的方法，我们同样可以做到图像到图像的转换。图像到图像意味着，例如我有一张狗的照片，我想通过使用提示词来修改这张图片，比如我想让模型给这只狗加上眼镜。那么我可以在这里输入这张图片，然后输入“一只戴眼镜的狗”，希望模型能给这只狗加上眼镜。它的工作原理是，我们使用变分编码器的编码器对图像进行编码。


and we   get the latent representation of our image then we  add the noise to this latent because the unit as   we saw before his job is to denoise an image but  of course we need to have some noise to the noise   so we add the noise to this image and the amount  of noise that we add to this image so this   starting image here indicates how much Freedom  the unit has into building the output image   because the more noise we add the more the unit  has freedom to alter the image but the less noise   we add the less Freedom the model has to alter the  image because it cannot change radically the the   um if you we start from Pure Noise the the unit  can do anything it wants but if we start with   less noise the unit is forced to modify just a  little bit of the output image so the there are   the amount of noise that we start from indicates  how much we want the model to pay attention to the   initial image here and then we give the prompt  for many steps we keep the noising the noising   the noising the noising and after there is no more  noise we take this latent representation we pass   it through the decoder and we get the output image  here and this is how image to image works now   let's go to the last part which is how in painting  works in painting Works similar way to the image   to image but with a mask so in painting means  first of all that we have an image and we want to   cut some part of this image for example the legs  of this dog and we want to the model to generate   new legs for this dog that are maybe a little  different so as you can see this that this feet   here are a little different from the the the legs  of the dog here so what we do is we start from   our initial image of the dog we pass it through  the encoder it becomes a latent representation   we add some noise to this latent representation we  give some prompt on to tell the model what we want   the model to generate so I just say a dog running  because I want to generate new legs for this dog   and then we pass the nullified input to the  unit the unit will produce an output Here For   the First Time step but then of course nobody  told the model to only predict this area the   model of course here at the output predicted  and modified the denois the old image but we   take this output here and we remove we we don't  care what the the noise predicted for this area   of the image the area that we already know we  replace it with the image that we already know   and we pass it again through the unit basically  what we do with at every step at every output of   the unit we replace the areas that are already  known with the areas of the original image so to   basically to fool the model into believing that  it was the model itself that came up with this   details of the image not as so every time here in  this area before we send it back to the unit here   here we combine the output of the unit with the  existing image by replacing whatever output the   unit give us for this area here with what is  the original image and then we give it back to   the unit unit and we keep doing it this way the  model will be only be able to work on this area   here because this is the one we never replace  in the output of the unit and then after there   is no more noise we take the output we send  it to the decoder and then it will build the   image we can see here okay this is how the  stable diffusion work from an architecture   point of view I know it has been a long journey  I had to introduce many Concepts but it's very   important that we know these Concepts before  we start building the unit because the the   model otherwise we we we don't even know  how to start building the stable diffusion   here we are finally coding our stable diffusion  and the first thing that we will code is the   variational autoencoder because it's external to  the unit so it's external to the diffusion model   so the one that will detect will predict how much  noise is present in the image and let's review   it actually let's review the architecture  and let me go to this slide here okay oops this one here okay the first thing  that we will build is this part here   the encoder and the decoder of our variational  of the encoder the job of the encoder and the   decoder of the version of decoder is to encode  an image or noise into a compressed version of   the image or the noise itself such that then we  can take this latent and run it through the unit   and then after the last step of the notification  we take this compressed version or latent and we   pass it through the decoder and to get the  original the output image not the original   and um so the encoder actually his job is to  reduce the dimension of the data into a smaller   data into the data with the smaller Dimension  and the idea is very similar to the one of the   units so we start with the picture that is very  big and at each step there are multiple levels   we keep reducing the size of the image but at the  same time we keep increasing the features of the   image what does it mean that initially each pixel  of the image will be represented by three channels   so red green and blue RGB at each step by using  convolutions we will reduce the size of the image   but at the same time we will increase the number  of features that are that each pixel represents   so each pixel will be represented not by three  channels but maybe but but by more channels this   means that each pixel will actually capture more  data more data of the area to which that pixel   belongs and this is thanks to the convolutions but  I will see I will show you later with an animation   and so let's start building the first  thing we do is open Visual Studio okay   and we create three folders the first is  called data and later we downloaded the   pre-trained weights that you can also find  on my GitHub another folder called images   in which we put images as input and output  and then another fold called SD which is our   model let's create two files one called encoder  dot pi and one called decoder dot Pi these are   these are the encoder and the decoder over of  our Virtual Router encoder let's start with   the encoder and then the encoder is quite  simple so let's start by importing torch and all the other stuff oops let me also select The Interpreter okay then we need to import two other blocks  that we will Define later in the decoder let's   call them for now Port VA attention block and VA  residual block for those who are familiar with   the computer vision models the resistor block  is very similar to the residual block that is   used in the resnet so later you will see the  structure it's very similar but if those who   are not familiar don't worry I will explain  it later so let's start building this encoder and this will inherit from the sequential module   which means basically our encoder  is a sequence of models supermodels okay it's a sequence of submodels in which each  module is something that reduces the dimension   of the data but at the same time increases its  number of features I will write the um I will   write the blocks one by one and as soon as we  encounter a block that we didn't Define we go   to Define it and then we Define also the shapes so  the first thing we do just like in the unit is a   redefine a convolution convolution 2D and  initially our image will be have will have   three channels and we convert it to 128 Channels  with the kernel size of 3 and the padding of one   for those who are not familiar with convolutions  let's go have a look at how convolutions work here here we can see that a convolution basically it's  a kernel so it's made of a matrix of a size that   we can decide which is defined by the parameter  kernel size which is run through the image as in   the following animation so block by block as you  can see and at each block each of the pixel below   the kernel is multiplied by the value of the  kernel in that position so in this for example   this pixel here which is in position let's call  the let's say this this one here so the first row   and the First Column is multiplied by the this red  value of the kernel the second column first row is   multiplied by the green value of the kernel and  then all of these multiplications are summed up   to produce one output so this output here comes  from four multiplications that we do in this   area each one with the corresponding number of the  kernel this way basically by running this kernel   to the image we capture local informations about  the image and this pixel here combines somehow   the information of 4 pixels not only one and  that's it then we can also increase the kernel   size for example and the kernel size will be  increasing the kernel means that we capture   more Global Information so each pixel represents  the information of more pixel from the original   picture so the output is smaller and then we can  introduce for example The Stride which means that   we don't do it every successive pickles pixel  but we skip some pixels as you can see here so   we skip every second pixel here and if the number  is uh the the kernel size is even and the input   is the size is other we will also never touch for  example here the Border as you can see we can also   implement the deletion which means that it becomes  the with the same kernel size the information   becomes even more Global because we don't watch  a consecutive pixel but we skip some pixels etc   etc so the kernels basically in the convolutions  allow us to capture information from a local area   of the picture of the image and combine it using  a kernel and this is the idea behind convolutions   so this convolution here for example we'll  start with our okay let's define some shapes   our variational autoencoder so the encoder  of the variational auto encoder will start   with batch size and the three into three channels  let's define it as Channel then we this the image   will have a height and the width which will  be 512 by 512 as we will see later and this   convolution will convert it into batch size 128  features with the same height and the same width   um why in this case the height and the  width doesn't change because even if we   have a kernel size of size 3 because we had  padding basically we add something to the   right side something to the top side something  to the bottom and the left of the image so the   image with the padding becomes bigger but then  the output of the convolution makes it smaller   and matches the original size of the image this  is the the reason we have the padding here but   we will see later that with the next blocks  the image size will start becoming smaller   the next block is called the residual  block and the Bae residual block   which is from 188 to 28 channels to 128 channels  this is a combination this residual block is a   combination of convolutions and normalization so  it's just a bunch of convolutions that we will   Define later and this one indicates how many input  channels we have and how many output channels we   have and the resistor block will not change the  size of the image so we Define it so our input   image is 128 so right size 128 hate and width  and it becomes it Remains the Same basically oops okay we have another one another residual  block with so the same transformation   then we have another convolution and  this time the convolution will change   the size of the image and we will see  why so we have a convolution to the 128 to 128 because the output channels  of the last and the last block is 128   so the input channel is 128 the output is 128.   the kernel size is three The Stride is two and  the padding is zero this will basically introduce   kernel size 3 stride 2 that's watch so imagine  the batch size is six by six kernel size is   3 stride is 2 without the deletion and  this is the output let me make it bigger okay something yeah so as you can see with the  stride of two uh need to make it okay with the   stride of two and the kernel size of three  this is the behavior so we skip every two   pixels before calculating the output and this  makes the output smaller than the input because   of this stride and also because of the kernel  size and we don't have any padding so this ah   transformation here will have the following  shapes so we are starting from batch size 128 width so the original height  and the width of the input image   but this time it will become batch size 128 the head will become half and  the width will become half Etc then we have two more  residual blocks with the same as before but by this time by  increasing the number of features and also here we don't increase any here  by increasing the feature means that we   don't increase the size of the image or  we reduce the size of the image we just   increase the number of features so this  one becomes 256. and here we start from oops 256 and we remain to 56. now you may  be confused of why we are doing all of this   okay the idea is we start with the initial  image and we keep decreasing the size of   the image so later you will see that the  image will become divided by 4 divided by   8 but at the same time we keep increasing  the features so each pixel represents more   information but the number of pixels is  diminishing is is reducing at every step so let's go forward then  we have another convolution   and this time the the size will become divided  by 4 and the convolution is let me copy this one 256 by 256 because the previous output is 256  the kernel size is 3. let's try these two and   the padding is zero so just like before also  in this case the size of the image will become   half of what is it now so the image is already  divided by two so it will become divided by 4 now then we have another um digital block in  which we increase the number of features   this time from 256 to 512 so we start  from 256 and the image is divided by four   and we go to 512 and the image size doesn't change then we have another one from  512 to 512 in this case oops   we will see later what is the residual block but  the result block you have to think of it as just   a convolution with the normalization we will see  later and this one is 512 and that goes into 512.   and then we have another convolution that  will make it uh even smaller so let's copy   this convolution here this one will go from 512  to 512 the same kernel size and the same stride   and the same padding as before so the image  will become even smaller so our last Dimension   was this let me copy it so we start with an  image that is 512 four times smaller than the   original image and with the four times smaller  width it will it will become eight times smaller   and that's it and then we have a residual blocks  also here we have three of them in this case let me copy one two two three I just write the one  for the last one so anyway the size is   the shape changes here it doesn't change the  shape of the image or the number of features   so here we are going from divide by 8 and  512 here 512 and we go to same Dimension 512 divided by 8 and divide by 8. then we have  annotation block and later we will see what   is the attention block basically it will run  a self-attention over each pixel so each pixel   will become kind of as you remember that the  attention is a way to relate tokens to each   other in a sentence so if we have an image made  of pixels the attention can be thought of as   a sequence of peak cells and the attention  as a way to relate the pixel to each other   so this is the goal of the attention block and because this way the the each pixel is  related to each other is not independent   from each other even if the convolution already  actually related relates close pixels to each   other but the attention will be a global so even  the last pixel can be related to the first pixel   this is the goal of the attention block and also  in this case we don't reduce the size because   the attention is the Transformer's attention is a  sequence to sequence model so we don't reduce the   size of the sequence and the the image Remains  the Same finally we have another visitor block s let me copy here also no change in shape or size  of the image then we have a normalization and we   will see what is this normalization it's the group  normalization which also doesn't change the size   just like any normalization by the way with the number of groups being 32 and the  number of channels being 512 because it's   the number of features finally we have  an activation function called the silu   The Silo is a function okay it's derived from  the sigmoid linear unit and it's a function just   like the relu there is nothing special they just  saw that this one works better for this kind of   application but there is now particular reason to  choose one over another except that they so that   practically this one works fine for this kind of  models and if you watch my previous video about   the Llama for example in which we analyze why we  they chose the Ziggler function if you read the   paper at the end of the paper they say that there  is no particular reason they chose this Igloo they   just saw that practically it works better I  mean it's very difficult to describe why a   activation function works better than the others  so this is why they use the Zillow here because   practically it works well now we have another two  convolutions and then we are done with the encoder convolution 512 8 current size and then heading this will uh not change the size of the  model because just like before we have   the kernel size S3 but we have the padding  that compensates for the reduction given   by the kernel size but we are decreasing the  number of features and this is the bottleneck   of the encoder and I will show you later  on the architecture what is the bottleneck and finally we have another convolution which is eight by eight with kernel size  equal to one and the padding is equal to zero   which also doesn't change the the the  size of the image because if you watch   here if you have a kernel size of  one means that each without Strider   each kernel basically is running over each  pixel so each output actually captures   the information of only one pixel so the  output has the same Dimension as the input   and this is why here also we don't change  the but here we need to change the number of it becomes eight and here from 8 to 8. and this is  the list of models that will make up our encoder   before building the residual block and the  attention block so this attention block let's   write the forward method and then we build  the the reservo block so this is the init   I Define it like this let me review it if it's correct okay yeah now let's define the forward method X is the image for which we wanted  that we want to encode so it's a tensor   torch dot answer and the noise we  need some noise and later I will   show you why we need some noise that has  the same size as the output of the encoder this returns a tensor okay our input X will be of size batch size   with some channels and initially it  will be three because it's an image   height and width which will be 512 by 512 and  then some noise this noise has the same size   as the output of the encoder and we will see  that it's jelly batch size then output channels   height divide by 8 and width divide by 8. then  we just run sequentially all of these modules and then there is one little thing here that  um in the convolutions that have destroyed   we need to apply a special embed padding  and I will show you why and how it works so if the model has a stride attribute and  it's equal to 2 2 which basically means this   convolution here this convolution here and this  convolution here we don't apply the padding here   because the padding here is applied to the top of  the image bottom left and right but we want to do   in a symmetrical padding so we do it manually  and this is applied like this F dot padding basically this says can you add a layer of  pixels on the right side of the image and   on the bottom side of the image only  because when you apply the padding   it's padding left padding right padding top padding bottom this means add a layer of  pixels in the right side of the image and   on the top side of the image and and this is  It's a asymmetrical padding and then if it's   we apply it only for this convolutions  that have this stride equal to two   and then we X is equal to module of X and okay now  you may be wondering why are we building this kind   of structure why it's made like this okay usually  in deep learning communities um especially during   research we don't reinvent the wheel every time so  the people who made the stable diffusion but also   the people before them every time we want to use  a model we check what models similar to the one we   want to build are already out there and they are  working fine so very probably the people who build   stable diffusion they saw that a model like this  is working very well for some previous project as   a variational autoencoder they just modified it a  little bit and kept it like it so for most choices   actually there is no reason there is a historical  reason because it works well in practice and we   know that convolutions work well in practice for  image segmentation for example or anything related   to computer vision and this is why they they made  the model like this so most encoders actually work   like this that we reduce the size of the image  but each we keep increasing the features of the   image the channels the number of channels of the  image so the number of pixels becomes smaller but   each pixels is represented by more than more than  three channels so more channels at every step now   what we do is here we are running our image into  sequentially in one by one um through all of these   um models here so first through this convolution  then through this residual block which is also   some convolutions then this residual block then  again convolution convolution convolution until we   run it through this attention block and Etc this  will transform the image into something smaller so   a compressed version of the image but as I saw  you before this is not an auto encoder this is   a variational auto encoder so the variational  auto encoder let me show you again the picture here we are not learning how to compress data  we are learning a latent space and this latent   space is a are the parameters of a multivariate  gaussian distribution so actually the variational   autoencoder is trained to learn the MU and  the sigma so the mean as the variance of this   distribution and this is actually what we will  get from the output of this variational out encode   or not directly the compressed image and if this  is not clear guys I made a previous video of the   about the variational encoder in which I show  you also why the history of why we do it like   this although the parameters the parametrization  trick Etc but for now just remember that this is   not just a compressed version of the image  it's actually a distribution and then we   can sample from this distribution and I will  show you how so the output of the variational   encoder is actually the mean and the variance and  actually it's actually not the variance but the   log variance so the mean as the log variance is  equal to torch dot chunk x 2 Dimension equal 1.   we will see also what is the chunk function so  I will show you so this basically converts batch   size eight channels height all right divide by  eight with divided by 8 which is the output of   the last layer of this encoder so this one  and we divide it into two sensors so this   chunk basically means divided into two tensors  along this Dimension so along this Dimension   it will become two tensors of size along this  dimension of size 4. so two then source of shape   batch size four then height divide by  eight and with oops with divided by eight and this basically the output of this  actually represents the mean and the variance   and what we do we don't want the um the log  variance we wanted the variance actually so to   transform the log bar the the log variance  into variance we do the exponentiation   so the first thing actually we also need to do  is to clamp this variance because otherwise it   will become very small so clamping means that  if the variance is too small or too big we want   it to become within some ranges that we are  acceptable for us so this clamping function   log variance tells the pi torch that if the value  is too small or too big make it within this range   and this doesn't change the shape of the tensors  so this Still Remains this tensor here and   then we transform the log variance into variances  so the variance is equal to the log variance dot   X which means to make the exponential of this so  you delete the log and it becomes the variance   and this also doesn't change the size of  the the shape of the tensor and then to   calculate the standard deviation  from the variance as you know the   standard deviation is the square root  of the variance so standard deviation is the variance Dot sqrt and also this doesn't  change the size of the tensor okay now what we   want as I showed you before this is the latent  space it's a multivariate gaussian which has its   own mean and its own variance and we know the mean  and the variance this mean and this variance how   do we convert how do we sample from it well what  we can sample from is basically we can sample from   n01 this is we if we have a sample from n01 how  do we convert it into a sample of a given mu so   a given mean and the given variance this as if  you remember from probability and statistics if   you have a sample from n01 you can convert it into  any other sample of a gaussian with a given mean   and the variance through this transformation so if  Z let's call it this one Z is equal to n01 we can   transform into another n let's call it X through  this transformation X is equal to Z uh well the   mean of the new distribution plus the standard  deviation of the new distribution multiplied by   Z this is the transformation this is the formula  from probability statistics basically means   transform this distribution into this one this  has this mean and this variance which basically   means sample from this distribution this is why  we are given also the noise as input because the   noise we want it to come from with a particular  seed of the noise generator so we ask is as a   input and we sample from this distribution like  this x is equal to mean plus standard deviation   multiplied by noise finally there is also another  step that we need to scale the output by a   constant this constant I found it in the original  repository so I'm just writing it here without any   explanation on why because I actually I also don't  know it's just a scaling constant that they use at   the end I don't know if it's there for historical  reason because they use some previous model that   had this constant or they introduced it for some  particular reason but it's a constant that I saw   it in the original repository and actually if  you check the original parameters of the stable   diffusion model there is also this constant so I'm  also scaling the output by this constant and then   we return X so now what we built so far except  that we didn't build the residual block and the   attention block here we built the encoder part of  the variational auto encoder and also the sampling   part so we take the image we run it through the  encoder it becomes very small it will tell us   the mean and the variance and then we sample from  that distribution given the mean and the variance   now we need to build the decoder along with the  resistor block and the attention block and what we   will see is that in the decoder we do the opposite  of what we did in the in the encoder so we will   reduce the number of channels at the end at the  same time we will increase the size of the image so let's go to the decoder let  me review if everything is fine looks like it did so let's go to the decoder again import torch we also need to define the attention we need to define the self  attention later we Define it Define first the residual  block the one we defined before   so that you understand what is this residual block and then we Define the attention block that we  defined before and finally we build the attention   so okay this is made up of normalization  convolutions like I said before   there is a two normalization  which is the group Norm one and then there is another group normalization these remote channels to Old channels and then we have a skip connection  skip connection basically means that   you take the input you skip some layers and  then you connect it there with the output of   the last layer and we also need this residual  connection if the two channels are different   we need to create another intermediate  layer now I created later I explain it okay let's create the forward method which is the torch.tensor and Returns the torch dot tensor okay the  input of this residual layer as you saw before   is something that has a batch with some  channels and then height and width which   can be different it's not always the same  sometimes it's 512 by 512 sometimes it's   half of that sometimes one-fourth of that  Etc so suppose it's a X is a batch size in channels height with what we do is we create  the script connection so we save the initial   input we call it the residual or residue  is equal to X we apply the normalization the first one and this doesn't  change the shape of the the tensor   the normalization doesn't change  then we apply the zero function and this also doesn't change the size of the  tensor then we apply the first convolution this also doesn't change the size of the tensor  because as you can see here we have kernel size   3 yes but with a padding of one with the padding  of one actually it will not change the size of the   tensor so it will still remain this one then  we apply again the group normalization Tool this again doesn't change the size of  the tensor then we apply The Silo again then we apply the convolution number two and finally we apply the residual connection  which basically means that we take X plus the   residual residual but if the number of output  channels is not equal to the input channels   you cannot add this one with this one because  this Dimension will not match between the two   so what we do we create this layer here to convert  the input channels to the output channels of X   such that this sum can be done so what we do is  we apply this residual layer residual layer of   residue like this and this is our residual  block so as I told you it's just a bunch   of convolutions and group normalization  and for those who are familiar with the   computer vision models especially in resnet  we use a lot of it it's a very common block   let's go build the rotation block that we used  also before in the encoder this one here and to   define the attention we also need to define the  self-attention so let's first build the attention   block which is using the variational autoencoder  and then we Define what is this self-attention it has a group normalization again  the channel is always 32 here in   in stable diffusion but you also may be wondering  what is group normalization right so let's go to   review it actually since we are here and okay  if you remember from my previous slides on   llama let's go here when we use the where we use  a layer normalization and um also in the vanilla   Transformer actually we used normalization so  first of all what is normalization normalization   is basically when we have a deep neural network  each layer of the network produces some output   that is fed to the next layer now what happens  is that if the output of a layer is a varying   in distribution so sometimes for example the  output of a layer is between 0 and 1 but the   next step maybe is between 3 and 5 and that the  next step maybe is between the 10 and 15 Etc so   the distribution of the output of a layer changes  then the next layer also will see some input that   it it's very different from what the layer is used  to see this will basically push the output of the   next layer into a new distribution itself which  in turn will push the loss function into basically   the output of the model to oscillate between these  three to change very frequently in distribution so   sometimes this will be very big number sometime it  will be a very small number sometimes it will be   negative sometime it will be positive Etc and this  basically makes the loss function oscillates too   much and it makes the training slower so what we  do is we normalize the values before feeding them   into layers such that each layer always see the  same distribution of the data so it will always   see numbers that are distributed around 0 with a  variance of one and this is the job of the layer   normalization so imagine you are a layer and you  have some input which is a batch of 10 items each   item have some features so feature one feature  two feature three layer normalization calculates   a mean and the variance over these features here  so over this distribution here and then normalizes   this value according to this formula so each each  value basically becomes distributed between 0   and 1. with batch normalization we normalize by  columns so the statistics mean and the sigma is   calculated by columns with layer normalization it  is calculated by rows so each item independently   from the others with group normalization on the  other hand it is like layer normalization but not   all of the features of the item but grouped so  for example imagine you have four features here   so here you have a F1 F2 F3 F4 and you have  two groups then the first group will be F1   and F2 and the second group will be F3 and F4 so  you will have two means and two variants one for   the first group one for the second group but why  we use it like this why do we want to group this   kind of features because these features actually  they come from convolutions and as we saw before   let's go back to the website imagine you have  a kernel of five here each output here actually   comes from local area of the image so the two  close features for example two things that are   close to each other may be related to each other  so two things that are far from each other are not   related to each other this is why we can group  we can use a group normalization in this case   because closer features to each other will have  a kind of the same distribution or we make them   have the same distribution and things that  are far from each other may not this is the   basic idea behind group normalization but  the whole idea behind the normalization is   that we don't want these things to oscillate  too much otherwise the loss of function will   oscillate and will make the training slower  with normalization we make the training faster   so let's go back to coding so we were coding the  attention block so now the attention block has   this group normalization and also an attention  which is a self-attention and later we Define it   and channels okay this one have a forward method tensor the terms of course the torch.penser okay  what is the input of this block the input of this   block is something where is where is it here it's  something in the form of a batch size number of   channels height and width but because it will  be used in many positions this attention block   we don't Define a specific size so we just  say that X is something that is a batch size   features or channels if you want height and  width again we create a residual connection and the first thing we do is we extract  the shape so n is the batch star is the   number of channels the height and the width  is equal to x dot shape then as I told you   before we do the self attention between all the  pixels of this image and I will show you how this will transform distance over here into this tensor here height multiplied by  width so now we have a sequence where each   item represents a pixel because we multiplied  height by width and then we transpose it so   put it back a little before transpose the  -1 with -2 this will transform this shape into this shape so we put back this one so this  one comes before and features becomes the last one   until like this and okay so as you can see  from this sensor here this is like when we   do the attention in the Transformer model so  in the Transformer model we have a sequence of   tokens so each tokens representing for example  a word and the attention basically calculates   the attention between each token so how do two  tokens are related to each other in this case   we can think of it as a sequence of pixels  each pixel with its own embedding which is   the features of that pixels and we relate pictures  pixels to each other and then we do the attention which is the self-attention in which  cell potential means that the query   key and values are the same input  and this doesn't change the shape   so this one Remains the  Same then we transpose back and we do the inverse transformation  so because we we put it in this form   only to do the attention so now  we transpose so we take this one and we convert it into features and  then height and width and then again   we remove this multiplication by  viewing again the tensor so nchw so we go from here to here then we add the residual connection   and we return X that's it the visual connection  will not change the size of the of the input and   we return a tensor of this shape here let me check  also the result of reaction here is correct okay   now with that have also been the attention block  let's build also the self attention since we are   building the attentions and the attentions  because we have two kind of attention in the   um stable diffusion 1 is called the  self-attention and one is the cross   attention and we need to build both so let's go  build it in a separate class called attention and okay so again import torch okay I think you guys maybe want to review the  attention before building it so let's go review   it I have here open my slides from my video about  the attention model for the Transformer model   so the self attention basically it's a way for um  to talk to in a especially in a language model is   a way for uh for us to relate tokens to each  other so we start with a sequence of tokens   each one of them having an embedding of size  D model and we transform it to into queries   key and values in which query can values in  the self-attention are the same Matrix same   sequence we multiply them by WQ Matrix so wqw  K and WV which are parameter matrices then we   split them along the D model Dimension into number  of heads so we can specify how many heads we want   in our case the one attention that we will do here  is actually only one head I will show you later   and then we calculate the attention for each of  this head then we combine back by concatenating   this head together we multiply this output  Matrix of the concatenation with another   Matrix called wo which is the output Matrix  and then this is the output of the multi-head   attention if we have only had one head instead  of being a multi-head then we will not do this   splitting operation we will just do this  multiplication with the w and with the w o   and okay this is how the self attention works  so in the self-attention we have this query   key and values coming from the same Matrix  input and this is what we are going to build so we have the number of heads then we have  the embedding so how what is the embedding of   each token but in our case we are not talking  about tokens we will talk about pixels and   we can think that the number of channels of  each pixel is the embedding of the pixel so   the the embedding just like in the original  Transformer the embeddings are the kind of   vectors that capture the meaning of the word  in this case we have the channels each Channel   um each pixel represented by many channels  that capture the information about that pixel here we have also the bias for the W matrices  which we don't have in the original Transformer okay now uh let's define the W matrices  so wqwk and WV we will represent it as   one big linear layer instead of representing  it as three different matrices it's possible   we just say that it's a big  Matrix 3 by the embedding   and the bias is if we want it so in projection in  projection bias so this stands for in projection   because we it's the projection of the input  before we apply the attention and then there   is auto projection which is after we  apply the attention so the wo Matrix so as you remember here the wo  Matrix is actually D model by   the model the input is also the  bottle by the model and this is   exactly what we did but we have three  of them here so it's three by T model and then we save the number of heads and then we saved the dimension of each head  the dimension of each head basically means   that if we have multi-head each head will  watch a part of the embedding of each token   so we need to save how much is this size so  the D model divided by the number of heads   but we divide by the number of  heads let's implement the forward we can also apply a mask as you remember the mask  is a way to avoid relating tokens that token one   particular token with the tokens that come after  it but only with the token that come before it   and this is the called the causal mask if you really are not understanding what  is happening here in the attention I   highly recommend you watch my previous video  because it's explained very well and I will   if you watch it will take not so much  time and I think you will learn a lot so the first thing we do is extract the shape then we extract but size the sequence length and the embedding is equal to input shape then we say that we will convert it into  another shape that I will show you later why   this is called the interim  shape intermediate shape then we apply the query key and value we split we   apply the in projection so the wqw  K and W V Matrix to the input and we   convert it into query key and values  so query key and values are equal to we multiply it and then we divide it so with  chunk as I showed you before what is chunk   basically we will multiply the input with  the big Matrix that represent WQ WQ and W   key but then with the we split it back into  three smaller matrices and this is the same   as applying a three different projections  instead of is the same as applying a three   separator in projections but it's also  possible to combine it in one big Matrix   this for what we will do basically  it will convert batch size sequence length Dimension into that size it's length Dimension  multiplied by three and then by   using Chunk we split it along the last  Dimension into three different than sources   of shape butt size sequence length and dimension okay now we can um   split the query key and values in the number  of heads according to the number of Heads This   Is Why We Built This shape which means split  the dimension the last Dimension into n heads and values V Dot View wonderful this will convert okay let's write  it that size sequence length Dimension into   that size sequence length then H so the  number of heads and each the dimension   divided by the number of heads so each head  will watch the full sequence but only a part   of the embedding of each token in this case  pixel and we'll watch this uh this part of   the head so the the full the dimension the  embedding divided by the number of heads and then this will convert it because we are also  transposing this will convert it into batch size   add sequence length and then  Dimension H so each head will   watch the all the sequence but  only a part of the embedding we then calculate the the attention just  like the formula so query multiplied by the   transpose of the keys so is the query matrix  multiplication with the transpose of the keys   this will return a matrix of size   batch size and sequence length by  sequence length we can then apply the mask as you remember the mask is something that we  apply when we calculate the attention if we   don't want two tokens to relate to each other  we basically substitute their value the the in   this Matrix we substitute the interaction with  minus infinity before applying the soft Max so   that the soft Max will make it zero so this is  what we are doing here we first build the mask this will create a causal mask basically  a mask where the upper triangle   so above the principal diagonal is made up of one once a lot of ones and then we  fill it up with minus infinity musket oops not a must but wait muskete fill but with mask  and we put minus infinity   like this as you remember the formula of  the Transformer is a query multiplied by   the transpose of the keys and then divided by  the square root of the model so this is what we   will do now so divided by the square root of the  model set of d add and then we apply the softmax we multiplied by the W or Matrix we transpose back so we want to remove now we want  to remove the head Dimension so output is equal to   let me write some shapes so what is this this  is equal to uh batch size sequence by sequence multiplied so matrix  multiplication with batch size this will result into that size h   silver slang and dimension divided by h this we  then multiplied by them we just then transpose and this will result into  so we start with this one and it becomes wait I put too many parentheses  here a batch size sequence length H and dimensions okay then we can reshape as the input like the initial shape so this one   and then we apply the output projection  so we multiply it by the w o Matrix okay this is the self-attention now let's go  back to continue building the decoder for   now we have built the attention block and the  residual block but we need to build the decoder and also this one is a sequence of models that  we will apply one after another we start with   the convolution just like before now I will  not write again the shade the shapes change   but you got the idea in the encoder we  uh in the encoder let me show you here here in the encoder we keep reducing the  size of the image until it becomes small   in the decoder we we need to return to the  original size of the image so we start with   the latent Dimension and we return to  the original dimension of the image convolution so we start with four channels and we output  four channels then we have another convolution we go to 500 then we have a residual block just like before then we have an attention block then we have a bunch of resistor  blocks and we have four of them let me copy okay now the residual  blocks let me write some shapes here   here we arrived to a situation in which we have  batch size we have a 512 features and the size of   the image still didn't grow because we didn't  have any convolution that will make it grow this one of course will remain the  same because it's a residual block   and Etc now to increase the size of the images  so now the image is actually height divided   by 8 which height as you remember is 512 the  size of the image that we are working with so   this Dimension here is a 64 by 64. how can we  increase it we use one model called up sample the up sample we have to think of it like  um when we resize an image so imagine you   have an image that is 64 by 64 and you  want to transform into 128 by 128 the the   app sample will do it just like when we resize  an image so it will replicate the pixels will   um twice so along the dimensions right and down  for example twice so that the total amount of   pixels that the height and the width actually  doubles and um this is the same up sample   basically it will just replicate each pixel so  that by this scale factor along each dimension so this one becomes side but size divided by 8 with divide  by 8 becomes as we see here to divide by 4 and with divided by 4. then we have a convolution resistor  blocks so we have convolutions 2D 512 to 512 then we have digital blocks of 512 500 but in  this case we have three of them two three then   we have another up sample that this will again  double the size of the image so we have another   one that will double the size of the image and by  a scale factor of two so now our image which was   divide by 4 with 512 channels so let's write  this like this will become divided by 2 now   so it will double the size of the  image so now our image is 256 by 256. then again we have a convolution and then we have three residual blocks again  but this time we reduce the number of features so 256 and then it's 256 to 256.   okay then we have another up samplings which  will again double the size of the image and this time we will go from divide by  two to divide by 2 up to the original size and because the number of channels has  changed we are not 512 anymore okay and   then we have another convolution this case with  the 256 because it's the new number of features then we have another bunch of residual blocks  that will decrease the number of features so we go to 256 to 128 we have finally a group normal 32 is the group sizes so we group features by in  groups of 32 before calculating the MU and the   sigma before normalizing and we said they find the  number of channels as 128 which is the number of   features that we have so this group normalization  will divide this 128 features into groups of 32. then we apply the silo and then we have a convolution   the final convolution that will transform  into an image with the three channels so RGB   by applying this convolutions here which  doesn't change the size of the output   so we'll go from an image that is batch size 128  yeah it width why height width because after the   last up sampling we become of the original  size into an image with only three channels and this is our decoder now we  can write the forward method I'm sorry if I'm putting a lot of spaces between  here but otherwise it's easy to get lost and not   understand where we are so here the input of  the decoder is our latent so it's batch size 4.   8 divided by 8 with divide by 8. as you  remember here in the encoder the last   thing we do is we scale by this constant so we  nullify this scaling so we reverse this scaling   two one five and then we  run it through the decoder and then return X which is a  batch size 3 height and width   let me also write the input of  this decoder which is this one   we already have it okay this is our variational  Auto encoder so for so far let's go review we are building our architecture of the stable  diffusion so far we have built the encoder and   the decoder but now we have to build the unit and  then we have to build the clip text encoder and   finally we have to build the pipeline that will  connect all of these things so it's going to be   a long journey but this is it's fun actually  to build things because you learn every every   detail of how they work so the next thing that  we are going to build is the text encoder so   this is a clipping encoder here that will allow  us to encode The Prompt into embeddings that we   can then feed to this unit model here so let's  build this clip encoder and we will of course   use a pre-trained version so by downloading  the vocabulary and I will show you how it   works so let's start we go to visual studio code  we create a new file in SD folder called clip.pi   and here and we start importing the usual stuff and we also import self-attention because we will  using it so basically clip is a layer is very   similar to the encoder layer of the Transformer  so as you remember the Transformer let me show you   here the Transformer this is the encoder layer of  the Transformer is made of attention and then feed   forwards and there are many blocks like this one  after another that are applied one after another   we also have something that tells the position of  each token inside of the sentence and we will also   have something similar in clip so we need to build  something very similar to this one and actually   this is why I mean the the Transformer model was  very successful so that's why they use the same   structure of course also for this purpose and so  let's go to build it the first thing we will build   I will build first the skeleton of the model and  then we will build each block so let's build clip and this has some embeddings the embeddings allow  us to convert the tokens so as you remember in   when you have a sentence made up of text first  you convert it into numbers where each number   indicates the position of the token inside of the  vocabulary and then you convert it into embeddings   where each embedding represents a vector of size  512 in the original Transformer but here in clip   it's the size is 768 and each Vector represents  kind of the meaning of the word or the token   captures so this is an embedding and let's  redefine it we need the vocabulary size   the vocabulary size is 49408 I took it directly  from the file this is the embedding size and the   sequence length the maximum sequence length that  we can have because we need to use the padding is   the 77 because the we should use actually use  some configuration file to save butter because   we will be using with the pre-trained stable  diffusion model the size already fixed for us but   in the future I will refactor the code to add some  configuration actually to make it more extensible this is the list of layers which we'll call it  the clip layer we have this 12 which indicates the   number of head of the attention of the multi-head  attention and then the embedding size which is 768 and we have 12 of these layers then  we have the layer normalization layer Norm can you tell him how many features so 768 and then we Define the forward method this is a pencil and this one returns not  transfer why long tensor because   the I input IDs are usually numbers that  indicate the position of each token inside   of the vocabulary also this concept please if  it's not clear we'll go watch my previous video   about the Transformer because it's very clear  there when we work with the textual models okay first we convert each token into embeddings and then so what is the size here we are  going from batch size sequence length into   batch size sequence length and dimension where  the dimension is 768 then we apply one after uh   one after another all the layers of this encoder just like in the Transformer model and the last one we apply the layer normalization oh and finally we return the  output where the output is of course it's a simplest to sequence  model just like the transformer so the   input should match the size the shape of the  input should match the shape of the output   so we always obtain SQL select by  the model okay now let's define these   two blocks the first one is the clip  embedding so let's go clip embedding how much is the vocabulary  size what is the embedding size and uh number of token okay so  the sequence length basically and okay we Define the embedding itself using  an end dot embedding just like always we need to tell him what is the number of  embeddings or the vocabulary size and what is the   dimension of each factor of the embedding token  then we Define some positional encoding so now   as you remember the positional encoding  in the original Transformer are given by   sinusoidal functions but here in clip  they actually don't use them they use   some learned parameters so they have  these parameters that are learned   by the model during training that tell  the position of the token to the model applicants and and beddings like this we apply them so first we apply the embedding  so we go from patch size sequence length to   patch size so you can select dimension and then just like in the original Transformer we   add the positional encodings to the each position  to each token but in this case as I told you the   positional embeddings are not not fixed like not  a sinusoidal functions but they are learned by   the model so they are learned and then later we  will load these parameters when we load the model and then we return this X then we have  the clip layer which is just like the   layer of the Transformer model the  encoder of the Transformer model so returns nothing actually  and this one is wrong in it okay we have just like in the Transformer  block we have a um the pre Norm then we have   the attention then we have a post norm and then  we have the feed former so layer normalization can have the attention which is a self-attention later we will build the cross attention  and I will show you what is it and we have another layer normalization then we have two feed forward layers and finally we have the forward method finally so this one takes tensor under Transit  tensor so let me write it answer okay just like the transformer model okay let's  go have a look we have some a bunch of residual   connection as you can see here one residual  connection here one digital connection here   we have two normalization one here one here  the feed forward as just like in the original   Transformer we have two linear layers and  then we have this multi head attention which   is actually self-attention because it's the  same input that becomes query key and values   so let's do it the first residual connection  X so what is the input of this forward method   it's a batch size sequence length demo and  the dimension of the embedding which is 768. the first thing we do is we apply the  self attention but before applying this   of attention we apply the layer normalization  so layer number one then we apply the attention but with the causal mask uh as you remember here self-attention we  have the causal mask which basically means   that every token cannot watch the next tokens  so cannot be related to Future tokens but only   the one on the left of it and this is what  we want from a text model actually we don't   want the one word to watch the words that come  after it but only the words that come before it   then we do this residual connection so now  we are now we are doing this connection here then we do the feed forward layer  again we have a residual connection we apply the normalization I'm not writing all the shapes if you  watch my code online I have written all   of them but mostly to save time because  here the we are already familiar with   the structure of the Transformer hopefully  so I am not repeating all the shapes here yeah apply the first linear of the feed forward  them then as the as activation function we use   the glue function and that's what we call the  quick glow function which is defined like this   X multiplied by torch dot  sigmoid of 1.702 multiplied by X and that's it should be like this   so this is called the quick chilu activation  function also here there is no justification on   why we should use this one and not another one  they just saw that in practice this one works   better for this kind of application so that's  why we are using this function here so now and then we apply the residual connection  finally return X this is exactly like the   fifth forward layer of the Transformer except that  in the Transformer we don't have this activation   function but we have the relu function and if  you remember in llama we don't have the relu   function we have the zuiglo function but here we  are doing the quick glue function which I actually   know I'm not so familiar with but I I think that  it works good for this model and I just kept it   so now we have built our our text encoder here  clip which is very small as you can see and   our next thing to build is our unit so we have  built the variational route encoder the coder   part and the decoder part now the best and then  the next thing we have to build is this unit as   you remember the unit is the the network that will  give in some noisified image and the amount and   we also indicated the the to the network what is  the amount of noise that we added to this image   the model has to predict how much image  the noise is there and how to remove it   and this unit is a bunch of convolutions that  will reduce the size of the image as you can   see with each step by but by increasing the  number of features so we reduce the size but   we increase exactly what we did in the encoder of  the variational auto encoder and then we do the   reverse steps just like we did with the decoder  of the original encoder so now again we will work   with some convolutions with the residual blocks  with attentions Etc the one big difference is   that we need to tell our unit not only the image  that is already not in so how what is the image   with noise not only the amount of noises so the  time Step At which this noise was added but also   the prompt because as you remember we need to also  tell this um this unit what is our prompt because   we need to tell him how we want our output image  to be because there are many ways to denoise the   initial noise so if we want the initial noise to  become a dog we need to tell him we want a dog if   we want the initial noise to become a cat we need  to install him we want a cat so the unit has to   know what is the prompt and also he has to relate  this prompt with the rest of the information and   what is the best way to combine two different  stuff so for example an image with text we will   use what is called the cross attention cross  attention basically allows us to calculate the   attention between two sequences in which the  query is the first sequence and the keys and   the values are coming from another sequence so  let's go build it and let's see how this works now the first thing we will do is create  a new class new file here called diffusion   because this is will be our diffusion  model and I think also here I will build   um from top down so we first Define the diffusion  class and then we build each block one by one   let's start by importing the  usual libraries so import torch from perch and then we have import the attention self attention but also we will need the cross  attention and then shown a letter we will build it then we let's create the class diffusion the class division is basically our unit this is made of time embedding so something  that we will Define it later time embedding   320 which is the size of the time embedding so  because um we need to give the unit not only the   noisified image but also the time Step At which it  was notified so the image the unit has needs some   way to to understand this time step so this is why  this time step which is a number will be converted   into an embedding by using this particular model  called that I'm embedding and later we will see it   then we build the unit and then the output layer of the unit and later  we will see what is it this output layer put layer   and we will see how to build  it let's do the forward as you remember the unit will receive the latent  so this Z which is the latent is the output of the   variational autoencoder so this latent which is  the torch dot tensor it will receive the context   what is the context is our prompt which is also  a torch dot tensor and it will receive the time   at which this latent was notified which is also  uh I don't remember I think it's a tensor also   later I Define it okay yeah it's tensor okay let's define the sizes so  the latent here is a batch size   4 because 4 is the output of the encoder if  you remember correctly here four closing okay read and read divide by eight then we  have the context which is our prompt   which we already converted with the clip encoder  here which will be a bit size by sequence length   by Dimension where the dimension is 768 like  we defined before and the time would be another   um we will Define it later how it's defined  how it's built but it's each embedding but   it's a number with an embedding of  size it's a vector of a size of 320. the first thing we do is we  convert this time into an embedding and actually this time we will  see later that it's actually   um just like the positional encoding of the  Transformer model it's actually a number that   is multiplied by signs and cosines just like  in the Transformer because it's a they saw   that it works for the Transformer so they we  can also use the same positional encoding to   convey the information of the time which  is actually kind of an information about   position so it tells the model at which  step we arrived in the denoisification so this one will convert tensor of 1 320 into  a tensor of one one two eight zero one thousand the unit will convert our latent into another  latent so it will not um change the size batch for height this is the output of the variational decoder  which first becomes batch 320 features through this the unit so so why here we have more features than the  starting because let's review here as you   can see the last layer of the unit actually  then we need to go back to the same number of   um the features you can see here so here we start  actually the dimensions here don't match what we   will be using so this is the original unit but  the one used by stable diffusion is a modified   unit so in the last one we built the decoder  the decoder will not build the final number   of features that we need which is four but we  need an additional output layer to go back to   the original size of features and this is the job  of this output layer so later we will see when we   build this this layer so output is equal to self  dot final this one will go from this size here to back to this original size of the unit  because the unit his job is to take in   latents predict how much noise is it then take  again the same latent predict how much noise   we remove it we remove the noise then again we  give another latent we predict how much noise   we remove the noise we give another latent we  predict the noise we remove the noise etc etc   etc so the output Dimension must match the input  dimension and then we return the output which is   the latent like this let's build the first  the time embedding I think it's easy to build   so something that encodes information  about the time step in which we are okay it is made of two linear layers  so nothing fancy here the nut one which will map it to four by and  embedding and then linear two four by and embedding into   buy and embedding and now you understand  why it becomes a 1280 which is 4 times 320. this one returns to tensor  so the input size is 1 320.   what we do is first we apply this first layer  linear one then we apply The Silo function then we apply again the second linear layer and then we'll turn it nothing special here  the last the output Dimension is 1 by 1280. 280 okay the next thing we need to build is the  unit the unit will require many blocks so let's   first build the unit itself and then we build each  of the blocks that it will require so class unit as you can see the unit is made up of one encoder  Branch so this this is like the encoder of the   variation route encoder so things go down so  become the image becomes smaller smaller smaller   but the channels keep increasing the features  keep increasing then we have this bottleneck   layer here it's called the bottleneck and then we  have a decoder part here so it becomes original   size the image from the very small size becomes  the original size and then we have this skip   connections between the encoder and the decoder  so the output of each layer of each layer of which   step of the encoder is connected to the same step  of the decoder on the other side and you will see   this one here so we start building the left side  which is the encoders which is a list of models and to build these encoders we need  to define a special um a special um   layer basically that will apply okay let's build  it and then I will describe it switch sequential and basically this sequence is a switch sequential  given a list of layers we'll apply them one by one   so we can think of it as a sequential but it can  recognize what are the parameters of each of them   and we'll apply accordingly so after I Define it  it will be more clear so first we have just like   before a convolution because we want to increase  the number of channels so as you can see at the   beginning we increase the number of channels of  the image here it's 64 but we go directly to 320.   and then we have another one of these switch  sequentials which is a unit residual block   we Define it later but it's very similar to the  residual block that we built already for the   operational autoencoder and then we  have an attention block which is also   very similar to the attention block that  we built for the evolution Auto encoder then we have a okay I think it's better to  build this switch sequential otherwise we   have too many yeah let's build it it's very simple as you can see it's a sequence but given X so which is our latent which is the third top sensor our context so  our prompt and the time which is also a tensor we'll apply them one by one but based on what they are so if the layer  is a unit attention block for example it will apply it like this so layer of  X and context why because this attention   block basically will compute the cross attention  between our latents and the prompt this is why this residual block will compute um   will match our latent with its time step and  then if it's any other layer we just apply it and then we return but after the full while yeah   so this is now in the CDC we just need to Define  this residual block and this attention block then we have another sequence  sequential switch this one here so the code I'm writing actually is based on  a repository uh or upon which actually most   of the code I rotor is based on which is in turn  based on another repository which was originally   written for tensorflow if I remember correctly  so actually the the code for stable diffusion   it uh because it's a model that we is built by  completes group at the LMU University of course   it cannot be different from that code so most of  the code are actually similar to each other I mean   you cannot create the same model and change  the code of course the code will be similar   so we again use this one which sequential so here   we are building the encoder side so  we are reducing the size of the image let me check where we are so we have  the resistor block of 320 to 64.   and then we have an attention block of 8 to 80.  and this attention block takes the number of Heads   This 8 indicates the number of head and this  indicates the embedding size we will see later   how we transform this the output of this into  a sequence so that we can run attention on it   um okay we have this sequential  and then we have another one then we have another convolution let me just copy convolution of size from 6 640  to 640 channels kernel size 3 stride 2 padding one then we have another residual block that will again increase the  features so from 640 to 1280 and then we have an attention block of  8 heads and 160 is the embedding size   then we have another digital block of 1 to 80  and 1 to 80 and 8 and 160. so as you can see   just like in the encoder of the variational  encoder we with these convolutions we keep   decreasing the size of the image so actually here  we started with the latent representation which   was a height divided by 8 and height divided by  eight so let me write some shapes here at least   you need to understand this size changes so batch  size 4 height divided by 8 and width divide by 8   when we apply this convolution  it will become divided by 16 so it will become divided by 16 so it will become a very small image and after we  apply the second one it will become divided by 32   so here we start from 16 here it will become  divided by 32 so what does it mean divided   by 32 that if the initial image was of  size 512 the latent is of size 64 by 64.   then it becomes a 32 by 32 now it has become 16  by 16. and then we apply this residual connections and then we apply another convolutional layer  which will reduce the size of the image further from 32 here divide by 32 and divide by 32  to divide by 64. every time we divided the we   divided the size of the image by two and the number of features we want  to eight zero one two eight zero then we have a unit this is your  block so let me copy also this one of 1 to 80 and 1380 and then we have a last  one which is another one of the same size   so now we have an image that is 64 divided by 64  and divide by 64 but with much more channels I   forgot to change the channel numbers here so here  is 1280 channels and divide by 64 divided by 64s   and this one Remains the Same because the  residual connections don't change the size   here should be 1280. to 1280 here should be 640 to  640 and here it should be 320.   to 320. so as I said before we keep reducing  the size of the image but we keep increasing   this number of features of each pixel  basically then we build the bottleneck which is this part here of the unit this is a sequence of a residual block then we have the attention block which will make self-attention  sorry not serve the cross attention   and then we have another residual block then we have the decoder so in the decoder  we will do the opposite of what we did in   the encoder so we will reduce the number  of features but increase the image size again let's start with our  beautiful switch sequential so we have a 2560 to 1280. y here is what 2560  even if after the bottleneck we have at 1280 so   we are talking about this part here so after the  the input the of the decoder so this side here   of the unit is the output of the bottleneck but  the bottleneck is outputting 1280 features while   the encoder is expecting 2560 so double the amount  why because we need to consider that we have this   skip connection here so this skip connection  will double the amount at each layer here   and this is why the input we expect here  is double the size of what is the output   of the previous layer let me write  some shapes also here so back size   2560 the image is very small so height part  and read divide by 64. and it will become individual okay then we apply another switch sequential of the same size then we apply another one  with an up sample just like we did in the   traditional encoder so if you remember in the  additional encoder to increase the size of   the image we do up sampling and this is  what we do exactly here we do up sample   but this is not the up sample that we did exactly  the same but it's this the concept is similar and we will Define it later also this one so we have another residual with  attention so we have a residual of 2000   and then we have an attention block 8 by 160 then we have again this one then we have another one with  up sampling so we have 90 20 and then we have an up sample so this one is small so I know that I'm  not writing all the shapes but otherwise   it's really a tiring job and very long so just  remember that we are keep increasing the size   of the image but we we will decrease the number  of features later we will see that this number   here will become very small and the size of  the image will become nearly to the normal then we have another one with attention so as you can see we are  decreasing the features here   then we have 8 by 80 and we are increasing  also here the size and then we have another one and 880. then we have another one with up  sampling so we increase the size of the image so 960 640 8 heads with the dimensions betting size  of 80 and the up sampling with 640 features   and then we have another  residual block with attention 40 then we have another one which is a 600 40  320 840 and finally the last one we have 640 by 320 and 8 and 40. this Dimension here  is the same that will be applied by the   uh there is the output of  the unit as you can see here   this one here and then we will give it to the  final layer to build the original latent size okay let's build all these blocks that we didn't  build before so first let's build the up sample   let's build it here which is exactly the same as the true okay you have this convolution without changing the number of features and this is also doesn't change  the size of the image actually so we will go from batch channels or features  let's call it features height width to batch size features height multiplied by  two and width multiplied by 2 y because   we are going to use the up sampling  this interpolation that we will do now X scale factor equal to mode is equal to  nearest is the same operation that we did here   the same operation here it  will double the size basically and then we apply a convolution now um we have we have to define the final block   and we also have to Define so the output layer  and we also have to define the attention block   and the residual block so let's build first  this output layer it's easier to build so that's this one also has a group normalization  again with the 32 size of the group 32 also has a convolution and a padding of one okay the final layer needs to convert this  shape into this shape so 320 features into   four we have a so we have a input which  is back size of 320 features the height   is divided by 8 and the width is divided by  eight we first apply a group normalization then we apply the silu then we apply the convolution and then we return  this will basically the convolution let me write   also why we are reducing the size this convolution  will change the number of channels from in to out   and when we will declare it we say that we want  to convert from 320 to 4 here so this one will be   of say of shape batch size 4. height divided by 4  I divided by 4 and width divide by 4 divide by 8. then we need to go build this residual block  and this attention attention block here so   let's build it here let's start with the  residual block which is very similar to   the residual block that we built for the  variational autoencoder so tune it lock this is the embedding of the time the the  time the time step as you remember we put   the time embedding we transform into  an embedding of size of one to 1280. we have this group normalization  it's always this group Norm then we have a convolution and we have a linear for the time embedding then we have another group of normalization  we will see later what is this merged and another convolution oops kernel size three typing one again  just like before we have if the in channels   is equal to the old channels we can connect  them directly with the resistor connection otherwise we create a convolution to  connect them to convert the size of the   input into the output otherwise  we cannot add the two tensors zero okay yeah so it takes in as input this feature  tensor which is actually the latent   batch size in channels then we have hate and  width and then also the time embedding which is   one by one two eight zero just like here and  we build first of all a resistor connection then we do apply the group normalization so  the usually there is a lot of connection the   result of blocks are more or less always  the same so there is a normalization an   activation function then we can  have some skip connection etc etc then we have the time here we are merging the latents with the  time embedding but the time embedding   doesn't have the batch and the channels  Dimension so we add it here with unsqueeze   and we merge them then we  normalize this merged connection it's why it's called merged we  apply the activation function then we apply this convolution and  finally we apply the residual connection so why are we doing this well the idea is  that here we have three input we have the   time embedding we have the latent we have the  prompt we need to find a way to combine the   three information together so the unit needs  to learn to detect the noise present in the   noisified image at a particular time step using  a particular prompt as a condition which means   that the model needs to recognize this this time  embedding and needs to relate this time embedding   with the latency and this is exactly what we are  doing in this resistor block here we are relating   the latent with the time embedding so that the  output will depend on the combination of both   not on the single noise or in the single time step  and this will also be done with the context using   cross attention in the attention block that we  will build now so neural net attention block 768 okay okay I will Define some layers that  for now will not make much sense   but later they will make sense  when we make the forward method so my cat is asking for food I think he already  have food but maybe he wants to eat something   special today so let me finish this  attention block and the unit and then all his why everyone wants attention  self-attention and head channels   here we don't have any buyers as you remember  the self attention we can have the buyers   for the W matrices here we don't have any  bias just like in the vanilla Transformer   so we have this attention then we have a  layer normalization self-taught layer Norm two which is along the same number of  features then we have another attention   we will see later why we need all  this attention but this is not a   self attention it's a cross attention  and we will see later how it works then we have the layer Norm tree this is because we are using a  function that is called the check   activation function so we need these matrices here okay now we can build the forward method so our X  is our latency so we have a batch size we have a   feature so we have height we have a width then  we have our context which is our prompt which   is a batch size sequence length Dimension  the dimension is size 768 as we saw before   so the first thing we will do is we will do the  normalization so just like in the Transformer   we will take the input so our latents and we  apply the normalization and the convolution   in actually in the Transformer there is  no convolution but only the normalization so residue this is called the long residual  because it will be applied at the end okay so we have this we are applying  the normalization which doesn't change   the size of the tensor then we have a convolution X is equal to self.com input of X which  also doesn't change the size of the tensor   then we have a we take the shape which is the size of the the batch size the  number of features the height and the width   we transpose because we want  to apply cross attention   first we apply self attention  then we apply cross attention so   so we do normalization Plus self attention  up s itself attention with skip connection so X is the x dot transpose of minus  1 minus 2 so we are going from this uh wait I forgot something here first of  all we need to do X is equal to x dot U   and C H multiplied by W so  we are going from this to um bed size features and then H multiplied  by W so this one multiplied by this   then we transpose these two  Dimensions so now we get from here to here so the features become the last one now  we apply this normalization plus cell potential so   and so we have a first short residual connection  that will apply right after the attention so we   say that X is equal to layer Norm 1 so X then  we apply the attention so self dot attention one   and then we apply the residual connection so X  is plus equal to residual sure short the first   residual connection then we say that the residual  short is again equal to 6 because we are doing to   apply now the cross attention so now we apply the  normalization cross attention with skip connection so what we did here is what we do in any  Transformer so let me show you here what   we do in any Transformer so we apply some  normalization we calculate the attention   and then we combine it with the skip connection  here and now we will instead of translating a self   attention we will do a cross attention which we  still didn't defined we will Define it later so   short and then first we calculate  we apply the normalization then the cross attention between the latent  and the prompt this is the cross attention   so this is cross attention and we will see  how and X Plus or equal to residual short okay and then again equal to X  finally just like with the attention   um the Transformer we have a feed forward  layer with the J glue activation function and this is actually if you watch  the original implementation of the   Transformer of the stable division  it's implemented exactly like this   so it's uh basically later we do a mult  um we do element wise multiplication so these are special activation functions  that involve a lot of parameters but why we use one and not the other  I told you just like before they just   saw that this one works better for this  kind of application there is no other then we apply the skip connection so  we apply the cross attention then we   Define another one here so this  one is basically normalization Plus forward layer with J  glue and script connection   in which the script connection is defined here so  at the end we always apply the script connection   finally we change back to our tensor to  not be a sequence of pixels anymore so we reverse the previous transposition transpose two so basically we go from batch size   with multiplied by eight eight multiplied by  width and features into batch size features eight multiplied by width then we remove this  multiplication so we reverse this multiplication and chw finally we will apply the long skip connection  that we defined here at the beginning so   only if the size match if the sizes don't  match we apply the here this one we have here turn self.com output and this is all of our unit we have defined  everything I think except for the cross   attention which is very fast so we go to the  attention that we defined before and we I put   it in the wrong folder it should be script  changes let me check if I put it correctly yeah we only need to Define this cross  attention here okay attention so let's go   and let's define this cross attention so class  it will be very similar to the not very similar   actually same as the self-attention except that  the keys come from one side and the query and   the sorry the query come from one side and  the key and the values from Another Side this is the dimension of the embedding of the  keys and the values this is the one of the queries this is the WQ Matrix in this case we will  Define the instead of one big Matrix made   of three wqw K and WV will Define three  different matrices both systems are fine   you can Define it as one big mattress or three  separately it doesn't change anything actually so the cross is from the the keys and the values oops linear then we save the number of heads of this cross  attention and also the dimension of each how much   information each head will see and oops d head is  equal to the embed divided by the number of heads   let's define the forward method X is  our query and Y is our keys and values so we are relating X which is our  latents which is of size batch size   it will have a sequence length its own sequence  length to Q let's call it q and its own dimension   and the Y which is the context or  the prompt which will be batch size sequence length of the Kiwi because the  prompt is will become the key and the   values and each of them will have its  own embedding size the dimension of KV   we can already say that this will  be a batch size of 77 because our   sequence length of the prompt is  77 and it embedding is of size 768. so let's build this one this is  input shape is equal to export shape okay then we have the interim  shape like the same as before so this is the sequence length then the N number of  heads and how much information each head will see   the head the first thing we do is multiply  queries by WQ Matrix so query is equal to then we do the same for the keys and the  values but by using the other matrices and as I told you before the key and  the values are the Y and not the X again we split them into H  heads so H number of heads very transpose I will not write  the shifts because they're they   match the same transformation that we do here okay again we calculate the   weight as which is the attention as a query  multiplied by the transpose of the keys and then we divide it by the dimension  of each head or the square root then we do the soft Max in this case we don't have  any causal mask so we don't need to apply the mask   like before because here we are trying to relate  the tokens so that the prompt with the pixels so   the each pixel can watch any word of the token  and every any token can watch any pixel basically so we don't need any mask we are to obtain the object  we multiplied by the V Matrix   and then the output again is  transposed just like before   so now we are doing exactly the same things  that we did here so transpose reshape Etc and then return output and this ends  our building of the let me show you   now we have built all the building blocks for  the stable diffusion so now we can finally   combine them together so the next thing that  we are going to do is to create the system that   taking the noise taking the text taking the time  embedding will run for example if we want to do   text to image we'll run this noise many times  through the unit according to a schedule so we   will build the scheduler which means that because  the unit is trained to predict how much noise is   there but we then need to remove this noise so to  go from a noisy version to go to to obtain a less   noisy version we need to remove the noise that is  predicted by the unit and this job is done by the   scheduler and now we will build the scheduler we  will build the decoder to load the weights of the   pre-trained model and then we combine all these  things together and we actually built the what is   called the pipeline so the pipeline of text to  image image to image Etc and let's go now that   we have built all the the structure of the unit  or we have built the variational auto encoder we   have built a clip we have built the attention  blocks Etc now we it's time to combine all it   all together so the first thing I kindly ask you  to do is to actually download the pre-trained   weights of the stable division because we need to  inference it later so if you go to the repository   I shared this one python stable diffusion you can  download the the pre-trained weights of the stable   diffusion 1.5 directly from the website of hugging  face so you download this file here which is the   EMA which means exponentially moving average  which means that it's it's a model that has been   trained but they didn't change the weights at each  iteration but with an exponentially moving average   schedule so this is good for inferencing it means  that the weights are more stable but if you want   to fine tune later the model you need to download  this one and we also need to download the files   of the tokenizer because we of course we will give  some prompt to the model to generate an image and   the prompt needs to be tokenized by a tokenizer  which will convert the words into tokens and   the tokens into numbers the numbers will then be  mapped into embeddings by our clip embedding here   so we need to download two files for the tokenizer  so first of all the weights of the this one file   here then when they tokenize the folder we find  the merges.txt and devocab.json if we look at the   vocab.json file which I already downloaded it's  basically a vocabulary so each token mapped to a   number that's it just like what the tokenizer does  and then I also prepared the picture of a dog that   I will be using for image to image but you can use  any image you don't have to use the one I am using   of course so now let's first build the pipeline  so how we will inference this stable diffusion   model and then while building the pipeline I will  also explain you how the scheduler will work and   we will build the scheduler later I will explain  all the formulas all the mathematics behind it   so let's start let's create a new file let's call  it pipeline dot pi and we import the usual stuff numpy Ops empty we will also use a tqdm to show the progress  bar and later we will build this sampler the   ddpm sampler and we will build it later and I will  also explain what is this sampler doing and how it   works etc etc so first of all let's define some  constants we the stable diffusion can only accept   the produce images of size 512 by 512 so height  is 500 by 512. the latent size latent Dimension   is the the size of the latent tensor of the  variational autoencoder and as we saw before   if we go check the size the encoder of the  variational encoder will convert something that   is 512 by 512 into something that is 512 divided  by eight so the latent Dimension is 512 divided by   8 and the same goes on for the height 512 divided  by 8 we can also call it width divided by 8 and 8   divided by 8. then we create a function called the  generator this will be the main function that will   be allow us to do text to image and also image to  image which accepts a parameter which is a string   and unconditional prompt so unconditional prompt  this is also called the negative prompt if you   ever used stable diffusion for example with the  hugging phase Library you will know that there   is a you can also specify a negative prompt  which tells that uh you want for example you   want a picture of a cat but you don't want  the cat to be on the sofa so for example you   can put the word sofa in the negative prompt so  it will try to go away from the concept of sofa   when generating the image something like this  and this is connected with the classifier free   guidance that we saw before so but don't worry I  will repeat all the concepts while we are building   it so this is also a string we can have an input  image in case we are building an image to image   and then we have the strength strength I will  show you later what is it but it's related to   if we have an input image and how much if we start  from an image to generate another image how much   attention we want to pay to the initial starting  image and we can also have a parameter called do   CFG which means do classifier free guidance we  set it to yes CFG scale which is the weight of   how much we want the model to pay attention to  our prompt it's a value that goes from 1 to 14   we start with 7.5 the sampler name we will  only Implement one so it's called adpm how   many inference steps we want to do and we will do  50 I think it's quite common to do 50 steps which   produces actually not bad results the models are  the pre-trained models the seed is how we want to   initialize our random number generator let me put  a new line otherwise we become crazy reading this okay new line so seed then we have the  device where we want to create our tensor   we have an idle device which means basically  if we load some model on Cuda and then we don't   need the model we move it to the CPU and  then the tokenizer that we will load later   recognizer is not okay this is our method this is  our main pipeline that given all this information   will generate one picture so it will pay attention  to the prompter it will pay attention to the input   image if there is according to the weights that  we have specified so the strength and the CFG   scale I will repeat all this Concepts don't  worry later I will explain them actually how   they work also on the code level so let's start  so the first thing we do is um we disable okay   torch Dot dot no grad because  we are inferencing the model   the first thing we make sure is the strength  should be uh between zero and one so if then we raise an error where is value error must be between zero and one this Idol device   it was basically if we want to move things  to the CPU we create this Lambda function otherwise okay then we create the oops I think I okay  then we created a random number generator   that we will use I think I made some mess  with this this one should be like here okay and the generator is a random number  generator that we will use to generate the noise   and if we the we we want to  start it with the seed so if seed then we generate with the condom seed  otherwise we specify one manually let me fix this formatting because I don't  know format document okay now at least today then we Define clip the clip is a model that  we take from the pre-trained models so it will   have the clip model inside so this model here  basically this one here we move it to our device okay as you remember with the classifier  free guidance so let me go back to my slides when we do classifier free guidance we inference  the model twice first with by specifying the   conditions for the prompt and another time  by not specifying the condition so without   the prompt and then we combine the output of the  model linearly with a weight this weight w is our   this weight here CFG scale it indicates how much  we want to pay attention to the conditioned output   with respect to the unconditional output which  also means that how much we want the model to   pay attention to the condition that we have  specified what is the condition The Prompt   the textual prompt that we have written and and  the unconditioned actually is also will use the   negative prompt so the negative prompt that you  use in stable diffusion which is this parameter   here so unconditioned prompt this is the  unconditional output so we we will sample   the we will inference from the model twice one  with the prompt one without with the one with the   prompter one with the unconditioned prompt which  is usually an empty text an empty string and then   we combine the two by this and this will tell the  model by using this weight we will combine the   output in such a way that we can decide how much  we want the model to pay attention to the prompter   so let's do it if we want to do classifier for  guidance first convert The Prompt into tokens using the tokenizer we didn't specify what is the tokenizer  lens yet but later we will define it   so the conditional tokens tokenizer batch   code Plus we want to encode the prompter we want  to append the padding up to the maximum length   which means that the prompt if it's too short  it will fill up it with the paddings and the   max length as you remember is 77 because we have  also defined it here the sequence length is 77 and we take the input IDs of this tokenizer  then we convert this this tokens which are input   IDs into a tensor which will be of size batch  size and sequence length so conditional tokens let me put it in the right device now we  run it through clip so it will convert the   batch size sequence length so this input  is easy will be converted into embeddings of size 768 each Vector of  size 768 so let's call it dim   and what we do is conditional context is equal  to clip of conditional tokens so we are taking   these tokens and we are running them through  Clips so this forward method here which will   return batch size sequence length Dimension  and this is exactly what I have written here   we do the same for the unconditioned tokens  so the negative prompt which if you don't   want to specify we will use the empty string  which means the unconditional output of the   model so the uh the model the what would the  model um produce without any condition so if   we start with random noise and we ask the model  to produce an image it will produce an image but   without any condition so the model will output  anything that it wants based on the initial noise we've convert it into tensor then we pass it  through Clips just like the conditional tokens   so they will it will become tokens yes   so it will also become a tensor of the size but  size sequence length Dimension where the sequence   length is actually always 77 and also in this case  it was always 77 because it's the max length here   but I forgot to write the code to convert it into  so unconditional tokens is equal izer batch Plus so the unconditional prompt so also the negative  front the padding is the same as before so max   length and the max length is defined as 77. and we  take the input IDs from here so now we have these   two prompts what we do is we concatenate them they  will become the batch of our input to the unit okay so basically what we are doing is we  are taking the conditional and unconditional   input and we are combining them into  one single tensor so they will become   a tensor of size batch size too so  two sequence length and dimension   where sequence length is actually we can already  write it it will become 2 by 77 by 768 because   77 is the sequence length and the dimension  is 768. if we don't want to do a conditional   um classifier classified free guidance we  only need to use the prompt and that's it   so we just we do only one step through the unit  and only with the prompt without combining the   unconditional input with the conditional input  but in this case we cannot decide how much the   model pays attention to the the the prompt  because we don't have anything to combine it with so again we take the just  a prompt just like before you can take it let's call it just tokens and then we transform this into a tensor okay so long we put it in the right device we calculate the context which is  one big tensor we pass it through   clip but this case it will be only one only one so  the batch size will be one so the batch Dimension   the sequence is again 77 and the dimension is  768. so here we are combining two prompts here   we are combining one why because we will run  through the model two prompts one unconditioned   one conditioned so one with the prompt that we  want one with the empty string and the model will   produce two output because the models take care of  the batch size that's why we have the batch size since we have finished using the clip we can  move it to the idle device this is very useful   actually if you're you have a very limited GPU  and you want to offload the models after using   them you can offload them back to the CPU  by moving them to the CPU again and then we   load the sampler for now we didn't Define the  sampler but we we use it and later on we build   it because it's better to build it after you know  how it is used if we built it before I think it's   easy to get lost and what is it happening what's  happening actually so if the sampler name is ddpm ddpm then we build the sampler ddpm  sampler we pass it to the noise generator   and we tell the sampler how many steps  we want to do for the inferencing and I will show you later what why if the  sampler is not edpm then we raise an error   because we didn't Implement any other any  other sampler so no sampler let me do f sampler name okay why we need to tell him how  many steps because as you remember let's go here here this scheduler needs to do many steps how  many we tell him exactly how many we want to   do so in this case we do to the the notification  steps will be 50. we even if during the training   we have maximum of 1000 steps during inferencing  we don't need to do 1000 steps we can do less of   course usually the more steps you do the better  the quality because the more the the the more   noise you can remove uh but with the different  Samplers they work in different way and with ddpm   usually 50 is good enough to get a nice result  for some other sampler for example ddim you can   do less steps for some other Samplers that work on  with differential equations you can do even less   depends on which sampler you use and how how lucky  you are with the particular prompt actually also this is the latents that will run through the unit  and as you know it's of size latents are hate and   latents width which we defined before so it's a  512 divided by 8 by 512 divided by 8 so 64 by 64.   and now let's do what what happens if the user  specifies an input image so if we have a prompt   we can take care of the prompt by either running a  classifier free guidance which means combining uh   um you know the output of the model with the  prompt and without the prompt according to this   scale here or we can directly just ask the model  to Output only one image only using the prompt   but then we cannot combine the two output with  this scale what happens however if we don't want   to do text to image but we want to do image to  image if we do image to image as we saw before   we start with an image we encode it with the  encoder and then we add the noise to it and   then we ask the scheduler to remove noise noise  noise but since the unit will also be conditioned   by the text prompt we hope that while the unit  will denoise this image it will move towards it   will move it towards this prompt so this is what  we will do first of things we load the image and   we encode it and we add the noise to it so if  an input image is specified we load the encoder we move it to the device in case we are using Cuda  for example then we load the tensor of the image we resize it we make sure that it's 512 by 512 with eight and then we transform it into  a numpy array and then into a tensor so what will be the size here it will be height  by width by Channel and the channel will be 3.   the next thing we do is we rescale this image what  does it mean that they are the input of this unit   should be normalized between should be sorry  rescaled between -1 and plus one because if we   load the image it will have three channels each  channel will be between 0 and 255 so each pixel   have three channels RGB and each number is between  0 and 255 but this is not what the unit wants as   input the unit wants every channel every pixel  to be between -1 and plus one so we will do this we will build it later this  function it's called the rescale to transform anything from  that is from between 0 and   255 into something that is between -1 and plus one and this is the will not change the size of  the um of the tensor we add the batch dimension on squeeze this adds the batch dimension batch size okay and then we change  the order of the dimensions which is 0 and 3 1 2. why because as you know  the encoder of the variational autoencoder wants   batch size Channel height and width while  we have pet size height width channel so we   permute them so to obtain the correct input  for the encoder uh this one go into Channel and height and width and then this part we  can delete okay this is the input then what   we do is we sample some noise because as you  remember the encoder to run the encoder we   need some noise and then he will sample from this  particular gaussian that we have defined before   so encoder Noise We sample it from our generator so as you we have defined this  generator so that we can Define   only one seed and we can also make the output  deterministic if we never change the seed   uh and this is why we use  the generator latent shape okay and now let's run it through  the decoder around the image through the of the VA this will produce latents for input image tensor and then we give it some  noise now we are exactly here we produced this   this is our latency so we give the image to  the encoder along with some noise it will   produce a latent representation of this image  now we need to tell our um as you can see here   we need to add some noise to this latent  how can we add noise we use our scheduler   the strength basically tells us the strength  parameter that we defined here tells us how much   we want the model to pay attention to the input  image when generating the output image the more   the strength the more the noise we add so the  the more the strength the more the strong the   noise so the model will be more creative because  the model will have more noise to remove and can   create a different image but if we add less noise  to this initial image the model cannot be very   creative because the most of the image is already  defined so there is not much noise to remove so   we expect that the output will resemble more or  less the input so this strength here basically   means the more noise you how much noise to add  the more noise we add the less the output will   resemble the input the less noise we add the more  the output will resemble the input because the the   scheduler the the unit sorry has less possibility  of changing the image because there is less noise so let's do it first we tell the sampler what  is the strength that we have defined and later   we will see what is this method doing but  for now we just write it and then we ask   the sampler to add the noise to our latents here  according to the strength that we have defined and Noise basically the the sampler will CR by setting  the strength will create a Time step schedule   later we will see it and by defining these time  steps schedule it will we will start what is the   initial noise level we will start with because  if we set the noise level to be for example the   strength to be one we will start with the maximum  noise level but if we set the strength to be 0.5   we will start with half noise not all completely  noise and later this will be more clear when we   actually visit the sampler so now just remember  that we are exactly here so we have the image we   transform the compress it with the encoder became  a latent we added some noise to it according to   the strength level and then we need to pass it  to the model to the diffusion model so now we   don't need the encoder anymore we can set it to  the idle device if the user didn't specify any   image then how can we start the denoising  it means that we want to do text to image   so we start with random noise so we start with  random noise let's sample some randomize then generator and devices device so let  me write some comments if we are doing   the text to image start with random noise  randomize defined as n01 for n0i actually um we then finally load the  diffusion model which is our unit   diffusion its models diffusion later we  see what is this model then how to load it   will take it to our device where we are working  so for example Cuda and then our sampler will   Define some time steps time steps basically  means that as you remember to train the model   we have maximum of 1000 time steps but when we  inference we don't need to do 1001 steps we in   our case we will be doing for example 50 steps of  inferencing if the maximum strength level is 1000   for example if the maximum level is 1000 the  minimum level will be 1 or if the maximum level is   999 the minimum will be zero and this is a linear  time steps if we do only 50 it means that we need   to do for example we start within 1000 and then  we do every 20 so 980 then 960 940 920 900 10 800   um what 880 860 840 820 etc etc  until we arrive to the zeroth level   basically each of these time steps indicates  a noise level so we with the noise when we   denoise the the image or the initial noise  in case we are doing the text to image we   can tell the scheduler to remove noise  according to particular time steps which   are defined by how many inference steps we want  and this is exactly what we are going to do now   when we initialize the sampler we tell him how  many steps we want to do and He will create these   times time step schedule so according to how  many we want and now we just go through it so   we test the time steps we create tqdm which  is a progress bar we take the time steps and for each of these time  steps we denoise the image so we have 1 300 this is our we need to tell the  unit as you remember diffusion the unit has as   input the time embedding so what is the time step  we want to denoise the context which is the prompt   or in case we are doing a classifier free guidance  also the unconditional prompt and the latent the   current state of the latent because we will  start with some latent and then keep denoising   it and keep denizing it you keep denizing it  according to the time embedding to the time step   so we calculate first that I'm embedding  which is an embedding of the current timestamp   and we will obtain it from this function later we Define it this function basically  will convert a number so the time step into   a vector one of size what 320 that  describes this particular time step   and as you will see later it's basically  just equal to the positional encoding   that we did for the Transformer model so in  the Transformer model we use the signs and   cosines to define the position here we use  the signs and codon to define the time step   and let's build the model input which is the  latency which is of shape at size 4 because it's   the the input of the encoder of the variational  encoder which is of size 4. sorry which has   four channels and then has latents height and  the latents with width which is a 64 by 64. now if we do if we do this one we need to send  basically we are sending the conditioned what   is it here we send the conditional input but also  the unconditional input if we do the classifier   free guidance which means that we need to send the  same latent with the prompt and without the prompt   and so what we can do is we can repeat this latent  twice if we are doing the classifier for guidance it will become a model input by repeat on one this will basically  transform batch size four Leones so this is um I'm going to be twice the size  of the initial batch size which is one actually   and the four channels and latent's height and  latent switch so basically we are repeating   this Dimension twice we are making two copies  of the latents one will be used with the prompt   one without the prompt so now we do we need to  check the model output what is the model output   is this the predicted Noise by the unit so the  model output is the predicted Noise by the unit we do diffusion model input context and time embedding and if we do a  classifier free guidance we need to combine   the conditional output and the unconditional  output because we are passing the input of   the model if we are doing classified free  guidance we are giving a batch size of   two the model will produce an output that has  batch size of two so we can then split it into   two different tensor one will be the conditional  and one will be the unconditional so the output   conditional and the output unconditional  are splitted in this way using chunk the dimension is the along the zeroth dimension  so by default it's the zeroth dimension   and then we combine them according  to this formula here what is the   I miss out according to this formula  here so unconditional output minus the   sorry the condition at output minus  the unconditional output multiplied   by the scale that we defined plus the  unconditioned output so the model output   will be condition at scale multiplied by  the output conditioned minus the output   unconditioned plus the output unconditioned  and then what we do is basically okay now   comes the let's say the clue part so we have a  model that is able to predict the noise in the   current latency so we start for example imagine  we are doing text to image so let me go back here you are going um text to image is a   here so we start with some random noise and  we transform into latency then According to   some scheduler According to some time  step we keep denoising it now our unit   will predict the noise in the latency but how can  we remove this noise from the image to obtain a   less noisy image this is done by the scheduler so  at each step we ask the unit how much noise is in   the image we remove it and then we give it again  to the unit and ask how much noise is there and   we remove it and then I ask again how much noise  is there and then we remove it and then how much   noise is there and when will until we finish  all these time steps after we have finished   this time steps we take the latent give it to  the decoder which will build our image and this   is exactly what we are doing here so imagine we  don't have an input image so we have some random   noise we Define sometime steps on this sampler  based on how many inference steps we want to do   we do all this time step we give the latents  to the unit the unit will tell us how much   is the predicted noise but then we need  to remove this noise so let's do it so   let's remove this noise so the latents are  equal to sampler dot step time step latents   model output this basically means take  the image from a more noisy version   okay let me write it better remove noise  predicted by the unit okay and this is   our Loop of the noising then we can do two idle  diffusion now we have our denoised image because   we have done it for many steps now what we do  is we load the decoder which is models decoder and then our images is run our image is run  through the decoder so we run the latents   through the decoder so we do this step here  so we run this latent through the decoder this   will give the image it actually will be only  one image because we only specify one image then we do images is equal to because the  image was initially as you remember here it   was rescaled so from 0 to 255 in in a new scale  that is between 0 minus one and plus one now we   do the opposite step so rescale again from -1  to 1 into 0 to 255 with clamp equal through   little we will see this function it's  very easy it's just a rescaling function   we permute because um to save the image on the CPU  we want the channel Dimension to be the last one   permute zero two three one so this  one basically will take the batch size a channel 8 width into batch size oops tight with Channel and then we move the image to the CPU and then we convert it into a numpy  array and then we return the image   voila let's build this rescale method so what is the old scale old range  what is the new range and the clamp so let's define the old minimum old maximum  is the old range new minimum and new maximum new range minus equal to Old mean multiply equal  to new Max minus new mean divided by   all the max minus all mean  X Plus equal to new mean   we are just rescaling so convert something  that is within this range into this range   and if it's clamped then X is equal to x dot  clamp new mean new marks and then we'll turn X then you have the time embeddings the the  method that we didn't Define here this get   time embedding this means basically take  the time step which is a number so which   is a an integer and convert it into a vector  of size 320 and this is will be done exactly   using the same system that we use for the  transformer for the positional embeddings so we first Define the frequencies of  our cosines and the signs exactly using   the same formula of the Transformer so  if you remember the formula is equal to   the ten thousand one over ten thousand to the  power of something of I I remember correctly so it's power of ten thousand  and minus torch dot arrange so   I am referring to this formula just in case  you forgot let me find it using the slides I am talking about this formula here so the  formula that defines the positional encodings here here we just use a different  dimension dimension of the embedding this one will produce  something that is a 160 numbers and then we multiply it we we created the   we multiplied with the time step  so we create a shape of size one so X is equal to torch dot tensor  which is a single time step of T type take everything we have the  one dimension so we add one   dimension here this is like a doing  unsqueezed multiplied by the frequency and then we multiply this by the signs in   the cosine just like we did  in the original Transformer this one will return a tensor of  size 100 by 62 so which is 320.   because we are concatenating to tensors not cosine but sine of x and then I concatenated  along the dimension the last dimension   and this is our time embedding so now  let's review what we have built here   we built basically a system a method that takes  the prompt the unconditional prompt also called   the negative prompt The Prompt or empty string  because if we don't want to use any negative   prompt the input image so what is the image we  want to start from in case we want to do an image   to image the strength is how much attention we  want to pay to this input image when we denoise   the image or how much noise we want to add it  to it basically and the more noise we add the   less the output will resemble the input image  the if we want to do classifier free guidance   which means that if we want the model to Output 2  output one is the output with the prompt and one   without the prompt and then we can adjust how much  we want to pay attention to the prompt according   to this scale and then we defined the scheduler  which is only one ddpm and we will Define it now   and how many steps we want to do the first thing  we do is we create a generator which is just a   random number generator then the second thing we  do is if we want to do classifier free guidance   as we need to do the basically we need to go  through the units twice one with the prompt one   without the prompt the the thing we do is that  actually we create a batch size of two one with   the prompt and one without the prompt or using  the unconditional prompt or the negative prompt   in case we don't do the classifier free guidance  we only build one tensor that only includes the   prompt the second thing we do is we load if  there is an input image we load it so instead   of starting from random noise we start from  an image which is to which we add the noise   according to the strength we have defined then  for the number of steps defined by the sampler   which are actually defined by the number of  inference steps we have defined here we do a   loop for Loop that for each for Loop the uh let  me go here the unit will predict some noise and   the scheduler will remove this noise and give  a new latent then this new latent is fed again   to the unit which will predict some noise and we  remove this noise according to the scheduler then   we again predict some noise and we remove the  Some Noise the only thing we need to understand   is how we remove the noise from the image now  because we know that the unit is trained to   predict the noise but how do we actually remove  it and this is the job of the scheduler so now   we need to go build this scheduler here so  let's go build it let's start building our   um tdpm scheduler so ddpm.py oops I forgot to put  it inside the folder and let me review one yeah this is wrong okay sorry import storage import  Moon pi and let's close the class tdpm sampler   okay I didn't call it scheduler because I  don't want you to be confused with the beta   schedule which we will Define later so I call  it scheduler here Oops why I opened this one   I call it scheduler here but actually I I  mean the sampler because there are there   is the beta schedule that we will Define now  what is the beta schedule which indicates the   amount of noise at each time step and then  there is what is known as the scheduler or   the sampler from now on I will refer it to  as sampler so this scheduler here actually   means a sampler I'm sorry for the confusion I  will update the slides when the video is out so how much were the training steps   which is one thousand the beta is um okay now  I Define two constant and later I did I Define   them where what are they and where they come from  0 85 and beta and it's a floating point of 0.0120 okay the parameter Beta start and beta  end basically if you go to the paper   if we look at the forward process we can see that  the forward process is the the process that makes   the image more noisy we add noise to the image  so given an image that don't have a have less   noise how to get a more noisy image according  to this gaussian distribution which is actually   a chain of gaussian distribution which is called  a Markov chain of gaussian distributions and the   noise that we add varies according to a schedule  variance schedule beta 1 beta 2 Beta 3 beta 4 beta   T So beta basically it's a series of numbers that  indicates the variance of the noise that we add   with each of these steps and as in the latent  in the stable diffusion they use a Beta start   so the first value of beta is 0.0085 and the  last variance so this the beta that will turn   the image into complete noise is equal to 0.010 D  it's a choice made by the authors and and the it's   a linear we will use a linear schedule actually  there are other schedules which are for example   the cosine schedule Etc but we will be using  the linear one and um we need to Define this   better schedule which is actually 1000 numbers  between Beta start and Beta And so let's do it so this is defined using the linear space where  the starting number is a better start actually   to the power to the square root of Beta start  so square root of beta starts because this is   how they Define it in the stable diffusion  if you check the official repository they   will also have these numbers and defined in  exactly the same way 0.5 then the number of   training steps so in how many pieces we  want to divide this linear space beta and and then the the type is torch dot float32 I  think and then to the power of 2 because they do   um they divide it into 1000 and then  to the power of 2. and this is in the   diffusers libraries from hugging phase I think  this is called the scaled linear schedule   now we need to Define an other constant that are  needed for our forward and our backward process so   our forward process depends on this beta schedule  but actually this is only for the single step so   if you want to go from for example the original  image by one step forward of more noise we need   to apply this formula here but there is a closed  formula here called this one here that allows you   to go from the original image to any noisified  version of the image at any time step between   0 and 1000 using this one here which depends on  Alpha bar that you can see here so the square root   of this Alpha bar and the variance also depends  on this Alpha bar what is Alpha bar Alpha bar is   the product of alpha going from 1 up to T so if we  are for example we want to go from the time step   0 which is the image without any noise to the  time step 10 which is the image with some noise   and remember that time step 1000 means that it's  only noise so we're going to go to timestart 10   which means that we need to calculate this as of  1 as1 as2 S3 S2 and up until a S10 and we multiply   them together this is the productory and this  a what is this Alpha this Alpha actually is 1   minus beta so let's calculate this Alphas  first so Alpha is actually 1 minus beta beta self.betas so it becomes floating and  then we need to calculate the product of   this Alphas from 1 to T and this is easily done  with the pi torture we pre-compute them basically   this is also come prod self-taught Alphas   this will create basically the an array where  the first element is the first Alpha so Alpha   for example zero the second element is Alpha 0  multiplied by Alpha One the third element is Alpha   zero multiplied by alpha 1 multiplied by Alpha  2 Etc so it's a cumulative product let's we say   then we create one tensor that represents  the number one and later we will use it and so 1.0 okay we save the generator save the number of training steps and then we create the time step schedule  the time step basically because we want to   reverse the noise we want to remove noise we  will start from the more noisy to less noise   so we will go from 1000 to 0. initially so  let's say time steps is equal to torch from we reverse this so this is from zero to one  thousand but actually we want one thousand to zero and this is our initial schedule in case we  want to do 1000 step but later because here   we actually specify how many inference  steps we want to do we will change   these time steps here so if the user later  specifies less than 1000 we will change it   so let's do let's create the  method that will change this   time steps based on how many actual steps we  want to make so set in France step time steps as I said before we usually perform 50 which is   also actually the one they use normally  for for example in hugging face Library let's save this value because we read it later  now if we have a number for example we go from   1000 actually it's not from this is not from zero  to one thousand but it's from zero to one thousand   minus one because this is excluded so it will  be from 99 999 998 997 996 Etc up to zero so we   have one thousand numbers but we don't want 1000  numbers we want less we want 50 of them so what   we do is basically we space them every 20 so we  start with 999 then 999 minus 20 then 999 minus 40   etc etc until we arrived to zero but in total here  will be 1000 steps and here will be 1000 50 steps   why minus 20 because 20 is 1000  divided by 50 if I'm not mistaken so this is exactly what we are going  to do so we can make this step ratio   which is self taught training step  divided by how many we actually want and we redefine the time steps according  to how many we actually want to make zero no inference steps  multiplied by this step ratio and round it will reverse it just like before because  this is from zero so this is actually   means 0 then 20 then 40 then 60 Etc  until we reach 999 then we reverse it then copy as type print MP Dot and 64 so  a long one and then we Define as tensor uh now the code looks very different from each  other because actually I have been copying the   code from multiple sources maybe one of them I  think I copied from the hugging face Library so   I didn't change it I I kept it to the original  one okay but the idea is the one I showed you   before so now we set the exact number of time  steps we want and we redefine these time steps   array like this let's define the next method  which basically tells us let's define the method   on how to add noise to something so imagine we  have the image as you remember to do image to   image we need to add noise to this latent how do  we add noise to something well we need to apply   the formula as defined in the paper so let's go in  the paper here we need to apply this formula here   and that's it this is it means that given  this image you need I want to go to the noise   noisified version of this image at time step t  which means that I need to take um we need to   have a sample from this gaussian but we don't  okay let's build it and we will apply the same   trick that we did for the variational autoencoder  as you remember in the variational auto encoder   I actually already showed how we sample from  a distribution of which we know the mean and   the variance here we will do the same here but  we of course we need to build the mean and the   variance what is the mean of this distribution  it's this one and what is the variance it's this   one so we need to build the mean and the variance  and then we sample from this so let's do it ddpm so we take the original samples which is a flow tensor and then the time steps so this is actually time step not time steps  it indicates at what time step we want to add   the noise because you can add the time Step at  the denoise at time step one two three four up   to one thousand and with each level the noise  increases so the noisified version at the time   step one will be not so noisy but at one at  the time step 1000 will be complete noise this returns a float tensor okay we get let's calculate first let me check  what we need to calculate first we can calculate   first the mean and then the variance so to  calculate the mean we need this Alpha Chrome   product so the cumulative process of the alpha  which stands for Alpha bar so the alpha bar as   you can see is the cumulative product of all  the alphas which is each Alpha is 1 minus beta   so we take this Alpha bar which we will call  Alpha complete so it's already defined here   Alpha is self-taught 1.2 device we move it to the same device because  we need to later combine it with it and of the same type this is a tensor that we also move to the same device of  the other tensor now we need to calculate   the square root of alpha bar so let's  do it square root of alpha [ __ ] prod at the time step t to the power of 0.5 y to the  power of 0.5 because having a number to the power   of 0.5 means doing its the the square root of  the number because the square root of one half   which becomes the square sorry to the power of  one half which becomes the square root and then   we flatten this array and then basically because we need to  combine this Alpha comprot which doesn't   have Dimensions it only has one dimension  which is the number itself but we need to   combine it with the latents we need to  add some Dimensions so one trick is to   just keep adding Dimensions with unsqueeze  until you have the same number of dimensions shape is less than most of this code I have taken from  the hugging phase libraries samplers so we keep our animation until this one and  this tensor and this sensor have the same   dimensions this is because otherwise we cannot  do broadcasting when we multiply them together   the other thing that we need to calculate this  formula is this part here 1 minus Alpha bar so   let's do it so s Square t of 1 minus Alpha  prod as the name implies is 1 minus Alpha   comproed at the time step t to the power of 0.5  y 0.5 because we don't want the variance we want   the standard deviation just like we did with the  encoder of the variational auto encoder we want   the standard deviation because as you remember  if you have an N 0 1 and you want to transform   into an N with a given mean and the variance the  formula is X is equal to mean plus the standard   deviation multiplied by the n01 let's go back  so this is the standard deviation maybe shown and we also flatten this one and then again we keep adding the dimensions  until they have the same dimension otherwise you cannot multiply them  together or sand them together and squeeze so we keep adding Dimensions  now as you remember we our method should   add the noise to an image so we need to add  noise means we need to sample some noise   so we need to sample some noise from the n01 using this generator that we have I think my cat is very angry today with  me because I didn't play with him enough   so later if you guys excuse me I need to later  play with him I think we will be dying we will   be done very soon so so let's get the noisy  samples using the noise and the mean and   the variance that we have calculated according  exactly to this formula here so we do the mean actually no the mean is this  one multiplied by X zero   so the mean is this one multiplied by x 0  is the mean so we need to take this square   root of alpha comprot multiplied by X zero  and this will be the mean so the mean is   the square root of alpha product multiplied by  the original latent so x 0 so the input image or   whatever we want to noise if I plus the standard  deviation which is the square root of this one   multiplied by a sample of the from the n01 so  the noise and this is how we notify an image this is how we add the noise to an image  so this one let me write it down so all   of this is according according to the equation  4 of the DDM paper and also according to this okay now that we know how to add noise   we need to understand how to remove noise  so as you remember let's review again   here imagine we are doing the text to text or  email text to image or image to image it doesn't   matter the point is other unit as you remember  is trained to only predict the amount of noise   given the latent with noise given the prompt  and the time Step At which this noise was added   um so what we do is we have this predicted noise  from the unit we need to remove this noise so   the unit will predict the noise but we need some  way of removing the noise to get the next latent   what I mean by this is um you can see this uh  reverse process here so the reverse process is   defined here we want to go from x t so something  more noisy to something less noisy based on the   moon or anyway based on the noise not on the moon  sorry based on the noise that was predicted by the   unit but here in this formula you don't see any  relationship to the noise predicted by the unit   actually here it just says if you have a met  if you have a network that can evaluate this   mean and this variance you know how to remove  the noise to how to go from XT to XT minus one   but we don't have a method that actually predicts  the mean and the variance we have a method that   tells us how much noise is there so the formula  we should be looking at is actually here so here   here because we have um we trained our  networkers our unit as a Epsilon Theta as   you remember our Training Method was this we  do gradient descent on this loss in which we   train a network to predict the noise in a noisy  image so we need to use this Epsilon Theta now   to remove the noise so this predicted noise  to remove the noise and if we read the paper   it's written here that to sample x t minus 1  given x t is to compute x t minus 1 is equal to   this formula here this tells us how to go  from x t to x t minus 1. and this is the   so basically we sample Some Noise We multiplied  by this Sigma and this basically reminds us on   how to move go from the n01 to any distribution  with a particular mean and a particular variance   so we will be working according to this formula  here actually because we have a model that   predicts noise here this Epsilon Theta and this  is our unit the unit is trained to predict noise   so let's build this part now and I will while  building it I will also tell you which formula I'm   referring to at each step so you can also follow  the paper so now let's build the method let's call   step method that given the time Step at which the  noise was added or we think it was added because   when we do the reverse process we can also Skip  it's not we think it was other but we can skip   some time steps so we need to tell him what is  the time Step at width it should remove the noise   the latents so as you know the unit works with  the latent so with this disease here so this is z   and it keeps the noising so the latents and then  what is the model output so the predicted all   noise of the unit so the the model output is  the predicted noise torch dot then sort this   model output corresponds to this Epsilon Theta of  X DT so this is the predicted noise at time step t   this latent is our XT and what else we need the  alpha we have the beta we have we have everything   okay let's go so T is equal to time step  the previous T is equal to self dot get   previous time step this is a function that  given this time step calculates the previous   one later we will build it actually we  can build it now it's very simple um okay get previous time step the self I start  which is an integer we return another integer   previous time step is equal to the time step   minus self minus basically this quantity here  step ratio so self dot num training steps divided by itself dot num inference steps  return previously this one will return basically   um given for example the number 999 it will  return number 999 minus 20. because the the   time steps for example the initial time step will  be suppose it's 1000 the training steps we are   doing is 1000 divided by the number of inference  step which is we will be doing is 50. so this is   means 1000 minus 20 because a thousand y divided  by 50 is 20. so it will return 980. when we give   him a 980 as input he will return 960. so what is  the next step that we will be doing uh in our for   Loop or what is the previous step of the denoising  so we are going from the image noise at the time   step 1000 to an image noise that time step 980  for example this is the meaning of previous term   then we retrieve some data later  we will use it so Alpha for the   t is equal to self dot Alpha for now if  you don't understand don't worry because   later I will write I will just collect  some data that we need to calculate the   formula and then I will tell you exactly  which formula we are going to calculate um Alpha prod if we don't have any previous step then we don't  know which Alpha to return so we just returned one and actually there is a paper that came out  I think from by dance that was complaining   that this method of doing is not correct because  the the last time step doesn't have this is not   doesn't have the signal to noise ratio about  equal to zero but okay this is something we   don't need to care about now actually if you're  interested I will link the paper in the comments prev current Alpha Team divided by Alpha graph current  also this code I took it from   hugging face the diffusers Library because I mean  we are applying formula so even if I wrote it by   myself it wouldn't be any different because we  are just applying formulas from the paper so   the first thing we need to do is to compute  the original sample according to the formula   15 of the paper what do I mean by this as  you can see where is it this one where is it here so actually let me show  you another formula here   as you can see we can calculate the previous step  so the less noise the the forward process sorry   the reverse process we can calculate the less  noisy image giving a more noisy image and the   predicted image As Time step 0 according to this  formula here where the mean is defined in this   way and the variance is defined in this way but  what is the predicted x0 so given an image given   um a noisy image at time step T how can we  predict what is the X zero of course this is   the predicted x0 not what will be the X zero  so this predicted x 0 we can also retrieve it   using the formula number 15 if I remember  correctly it's here so this x 0 is given as x   t minus 1 minus Alpha multiplied by the predicted  noise at time step T divided by the square root   of alpha all these quantities we have so actually  there are two ways which are equivalent to each   other actually numerically of going from more  noisy to less noisy one way is this one this one   here which is the algorithm tool of the sampling  and one is uh this one here so the equation number   seven that allows you to go from more noises to  less noisy but the two are numerically equivalent   they just in the in the effect they are equivalent  it's just um they have different parameterization   so they have different formulas so as a matter  of fact for example here in the code they say to   go from x t to x t minus one you need to do this  calculation here but as you can see for example   this is this numerator of this multiplied by  this Epsilon Theta is different from the one   in the algorithm here but actually they are  the same thing because BT is equal to equal   to 1 minus Alpha t as beta Alpha is defined as 1  minus beta as you remember so there are multiple   ways of obtaining the same thing so what we will  do is we actually we will apply this formula here   in which we need to calculate the mean and we  need to calculate the variance according to this   formulas here in which we know Alpha we know beta  we know Alpha bar we know all the other Alphas we   know because there are parameters that depend  on beta what we don't know is x0 but x 0 can be   calculated as in the formula 15 here so first  we will calculate this x0 predicted X is zero first compute the predicted original  sample using formula 15 of the ddpm paper or predicted original sample latents minus  while so we do latents minus the square   root of 1 minus Alpha T what is the square  root of 1 minus Alpha T is equal to Beta   so I have here beta t which is  already 1 minus Alpha t as you can see   Alpha bar 1 minus Alpha bar at the time step  T because I already retrieve it from here so   1 minus sorry beta to the power 2 to the power  of one half or the square root of beta so we do   latents minus beta prod at density to the power of  0.5 which it means basically square root of beta   and then we multiply this by the predicted  noise of the image of the latent at time   step T so what is the predicted noise  it's the model output because of our   unit predicts the noise model output and  then we need to divide this by let me check square root of alpha t which we  have I think here Alpha T here   so the square root of alpha t  Alpha prod t to the power of 0.5 here I have something on this one I  don't need this one I don't need okay   because otherwise it's wrong right yeah before  first there is a product between these two terms   and then there is the difference here okay this is  how we compute the prediction the X zero now let's   go back to the formula number seven uh seven seven  okay now we have this x 0 so we can compute this   term and we can compute this term and this we can  compute this term and all the other terms we also   can compute so we calculate this mean and this  variance and then we sample from this distribution so compute the coefficients for red original  sample and the current sample XT this is the   same committee that you can find on the diffusers  Library which basically means we need to compute   this one this is this coefficient for the  predicted sample and this is the coefficient   for x t this one here so predicted original  sample coefficient which is equal to what   Alpha product T minus 1 so the previous Alpha  prod t which is Alpha protein previous which   means the alpha probability but at the previous  time step under the square root so to the power   of 0.5 multiplied by the current beta T so  the beta at the time step T So current beta t   which is we Define it here current beta T  we retrieve it from alpha we could have a okay and then we divide it by beta product because  1 minus Alpha bar is actually equal to Beta bar   beta product t then we have the this  coefficient here so this one here   so this is current sample coefficient is  equal to current Alpha t to the power of 0.5   which means the square root of this time this  this thing here so the square root of alpha t   and then we multiply it by Beta at the  previous time step because it's 1 minus   Alpha at the previous time step corresponds  to Beta as at the previous time steps time   step multiplied by Beta prod divided by  beta at the time step T So beta prod t   now we can compute the mean so the mean  is the sum of these two terms sample so let me write some here compute the  predicted previous sample mean modity is equal to predicted original sample  coefficient multiplied by what by x 0   what is X 0 is this one that we obtained  by the formula number 15 so the prediction   predicted original sample so x0 plus this term  here what is this term is this one here so the   current sample coefficient multiplied by x t  what is x t is the latent at the time step t now this we have computed the mean for  now we need to compute also the variance   let's create another method  to compute the variance that get variance 12 time step and okay we obtained the previous time test T  because we need to do four letter calculations again we calculate the alpha  prod T so all the terms that   we need to calculate this particular terms here and the current beta T is equal to 1 minus  Alpha probability division Alpha from this one   what is current beta T is equal to 1 minus  Alpha protein yeah 1 minus Alpha protein   divided by Alpha property zero okay so the variance according to the formula  number six and seven so this formula here is given as 1 minus Alpha for the D  Prime so 1 minus Alpha probability Prime divided by 1 minus Alpha prod which is 1 minus  Alpha prod y prod because this is the alpha bar   and multiplied by the current beta  beta t and beta T is defined I don't   remember where it's 1 minus Alpha and  this is our barbariance we clamp it oops third dot clamp variance and the  minimum that we want is one my equal   to minus 20. to make sure that it doesn't  reach 0. and then we return the variance and now that we have the mean and the  variance so this variance has also been   computed using I mean right here computed  using formula serving of the ddpm paper and now we go back to our step motion so what  we do is equal to zero so because we only need   to add the variance if we are not at the last last  time step if you are at the last time step we have   no noise so we don't add any uh we we don't add  we don't need to add any noise actually because   um the point is we are going to sample from  this distribution and just like we did before   we actually sample from the n01 and then  we shift it according to the formula so   uh the N gaussian with a particular mean and  the particular variance is equal to the uh   the gaussian at the zero one multiplied by the  standard deviation plus the um plus the mean so you sample the noise okay let me sample some  noise you can do the variance actually this is the variance already multiplied  by the noises so it's actually the standard   deviation because we will see um self Dot get  variance let's see the time step t to the power   of 0.5 so this is 0.5 so this one becomes the  standard deviation we multiply it by the n01   so what we are doing is  basically we are going from n01   to n with a particular move and a particular  Sigma using the usual trick of going from X   is equal to the move plus the sigma actually not  yeah this is the sigma Square then because this   is the variance Sigma multiplied by the Z where  Z where Z is distributed according to the N 0 1   this is the same thing that we always done  also for the variation of the encoder also   for adding the noise the same thing that  we did before this is how you sample from   a distribution how you actually shift the  parameter of the gaussian distribution so predicted prev sample is equal to the predicted  prep sample plus the variance this variance term   here already includes the sigma multiplied  by Z and then we return predicted prep sample   oh okay now we have also built the the sampler let  me check if we have everything no we missed still   something which is the set strength method  as you remember once we want when we want   to do image to image so let's go back to check  our slides if we want to do image to image we   convert the image using the vae to a latent then  we need to add noise to this latent but how much   noise we can decide the more noise we add the  more freedom the unit will have to change this   image the last noise we add the less Freedom  it will have to change the image so what we   do is basically by setting the strength we make  our sampler start from a particular noise level   and this is exactly what the method we want  to implement so I made some s okay so for   example as soon as we load the image we  set the strength which will shift the   noise level from which we start from and then  we add the noise to our latent to create the   image to image here so let's go here and  we create this method called set strength strength okay the start step because  we will skip some steps   is equal to self-taught new inference  steps minus end of cell phone inference this basically means that if we have a  50s inference steps and then we set the   strength to let's say 0.8 it means that we  will skip 20 of the steps so when we will   add we will start from image to image  for example we will not start from a   Pure Noise image but we will start from 80  percent of noise in this image so the unit   will still have freedom to change this  image but not as much as with 100 noise uh we redefined the timestamps  because we are altering the schedule   so basically we skip sometimes steps and self dot start step is equal to  start step so actually what we do   here is suppose we have the strength of 80  we are actually fooling the method the the   unit into believing that he came up with  this image which is now with this level   of strength and now he needs to keep  denoising it this is how we do image   to image so we start with an image we noise  it and then we make the unit believe that he   came up with this image with this particular  noise level and now he has to keep the noising it   until according of course also to The Prompt  until we reach the clean image without any noise now we have the pipeline that we can call we  have the ddpm sampler we have the model built   of course we need to create the function to load  the weights of this model so let's create another   file we will call it the model loader uh here  model loader because now we are nearly close   to sampling from this finally from this stable  diffusion so now we need to create the method   to load the pre-trained the pre-trained weights  that we have downloaded before so let's create it import clip coder or VA encoder then from  decoder import the decoder Fusion import Fusion of a  division model which is our unit   now let me first Define it then I  tell you what we need to do so Prelude from standard weights okay as usual we load the  weights using torture but   we use we will create another function model  converter dot load from standard weights this is a method that we will create later  to to load the weights the pre-trained   weights and I will show you why we need  this method then we create our encoder and we load the state detector  load status from our set addict and we'll set strict to to oops predict strict true then we have the decoder and it's strict also so this strict parameter  here basically tells that when you load a model   from pytorch this for example this ckp ckpd file  here it is a dictionary that contains many keys   and each key corresponds to one Matrix of  our model so for example this uh self this   group normalization has some parameters and  the the hole can torch load this parameters   exactly in this group Norm by using the name  of the variables that we have defined here and   he will when we load the model from PI torch he  will actually load the dictionary and then we load   this dictionary into our models and he will match  by names now the problem is the pre-trained model   actually they don't use the same name that I have  used and actually this code is based on another   code that I have seen so actually the the names  that we use are not the same as the pre-trained   model also because the names in the pre-trained  model not always very friendly for learning this   is why I changed the names and also other people  changed the names of the methods but this also   means that the automatic mapping between the names  of the pre-train model and the names defined in   other classes here cannot happen because it  cannot happen automatically because the names   do not match for this reason there is a script  that I have created in my GitHub Library here   that you need to download to convert these names  it's just a script that Maps one name into another   so if the name is this one map it into this if  the name is this one mapping into this there is   nothing special about the script it's just a very  big mapping of the names and this is actually done   by most models because if you want to change  the name of the classes and or the variables   then you need to do this kind of mapping so I  will also uh I will basically copy it I don't   need to download the file so this is called the  model converter dot pi model converter dot pi and   that's it it's just a very big mapping of names  and I take it from this comment here on GitHub so this is model converter so we need to import  this model converter import model com converter this model converter basically will convert  the names and then we can use the load State   dictator and this will actually map all the  names now now the names will map with each   other and the district makes sure that if there  is even one name that doesn't map then through   an exception which is what I want because  I want to make sure that all the names map so we Define the diffusion  and we load it's statistic diffusion and strict equal to true and let me check up right then we do clip is equal to clip.2 device  so we move it to device where we want to   work and then we load also his State  addict so the parameters of the weights and then we return a dictionary clip clip and then we have the encoder is the encoder  we have the decoder the decoder and then we have   the diffusion we have the diffusion Etc now  we have all the ingredients to run finally the   inference guys so thank you for being patient  so much and it's really finally we have we can   see the light coming so let's build our notebook  so we can visualize the image that we will build   okay let's select the kernel stable  diffusion I already created it   in my repository you will also find the  requirements that you need to install in   order to run this so let's import everything  we need so the model loader the pipeline pill import image this is how to load the image  from python so partly import or actually this   one we don't need transformers this is the only  library that we will be using because the is the   tokenizer of the clip so how to tokenize the  the text into tokens before sending it to the   clip embeddings otherwise we need also need to  build the tokenizer and it's really a lot of job I don't allow Cuda and I also don't  allow MPS but you can activate these two   variables if you want to use good or MPS viable and hello Cuda then the  device becomes Cuda of course and then we printed the device we are using okay let's load the tokenizer tokenizer is  the clip tokenizer we need to tell him what   is the vocabulary file so which is already  saved here in the data data vocabulary.json   and then also the merges file maybe one day I  will make a video on how the tokenizer works   so we can build also the tokenizer but this is  something that requires a lot of time I mean   and it's not really related to the diffusion  model so that's why I didn't want to build it   the model file is I will use the data and then  this file here then below the model so the models   are model loader dot preload model from the model  file into this device that we have selected okay   let's build from text to image but we need  to define the prompt for example I want a cat sitting or stretching let's  say stretching on the floor   highly detailed we need to create  a prompt that will create a good   image so we need to add some lot of details  Ultra sharp cinematic etc etc 8K resolution the unconditioned prompt I keep it blank this you can also use it  as a negative member you can use it as a   negative prompter so if you don't want the  some you don't want the more the output to   have some how to say some characteristics  you can Define it in the negative prompt   of course I like to do CFG so the classifier  freak items which we set true CFG scale is a   number between 1 and 14 which indicates how much  attention we want the model to pay to this prompt   14 means pay very much attention or one means  pay very little attention I use seven because   then we can Define also the  parameter for image to image so input image is equal to none image path  is equal to I will Define it with my image   of the dog which I already have here and but  for now I don't want to load it so if you want   to load it we need to do input image is equal  to image dot open image part but for now I will I will not use it so now let's comment it and  if we use it we need to define the strength so   how much noise we want to add to this image but  for now let's not use it the sampler we will be   using of course is the only one we have is the  ddpm the number of inference steps is equal to 15 and the seed is equal to 42 because it's a  lucky number at least According to some books   output image is equal to pipeline generate okay  The Prompt is the prompt that we have defined   The unconditioned Prompt is the unconditional  pump that we have defined input image is the   input image that we have defined if it's not  commented of course the strength for the image and the CFG scale is the one we have defined sampler name is the sampler name we have defined   the number of inference steps is  the number of inputs the seed models device idle devices of our CPU so when we don't want to   use something we move it to the CPU  and the tokenizer is the tokenizer and then image Dot from array output image if  everything is done well if all the code has   been written correctly you can always go back to  my repository and download the code if you don't   want to write it by yourself let's run the code  and let's see what is the result of my computer   will take a while so it will take some time  so let's run it so if we run the code it will   generate an image according to our prompt in my  computer it took really a long time so I cut the   video and I actually already replaced the code  with the one from my GitHub because now I want   to actually explain you the code without while  showing you all the code together how does it   work so now we we generated an image using only  the prompt I use the CPU that's why it's very   slow because my GPU is not powerful enough and we  set a unconditional prompt to zero we are using   the classifier free guidance and with a scale  of seven so let's go in the pipeline and let's   see what happens so basically because we are doing  the classifier free guidance we will generate two   conditioning signals one with the prompt and one  with empty text which is the unconditioned prompt   which is also called the negative prompt this  will result in a batch size of two that will   run through the unit so let's go back to here  suppose we are doing text to image so now our   unit has two latents that he's doing at the same  time because we have the batch size equal to two   and for each of them it is predicting the noise  level but how can we move remove this noise from   the predict the predicted noise from the initial  noise so because to generate an image we start   from random noise and the prompt initially  we encode it with our vae so it becomes a   latent which is still noise and with the unit we  predict we predict how much noise is it according   to a schedule so according to 50 steps that of  inferencing that we will be doing at the beginning   the first step will be 1000 The Next Step will  be 980 The Next Step will be 960 Etc so this   time will change according to this schedule so  that at the 50th step we are to the time step 0.   and how can we then with the predicted noise go  to the next latent so we remove this noise that   was predicted by the unit well we do it with the  sampler and in particular we do it with the sample   method of the sampler uh step method sorry  of the sampler which basically will calculate   the previous sample given the current sample  according to the formula number 7 here so which   basically calculates the previous sample given  the current one so the less noisy one given the   current one and the predicted x0 so this is not x0  because we don't have X zero so we don't have the   um the noise the sample without any noise so  but we can predict it given the values of the   current noise and the beta schedule another way  of denoising is to do the sampling like this if   you watch my other repository about the ddpm  paper I actually implemented it like this if   you want to see this version here and this is how  we remove the noise to get a less noisy version so   once we get the less noisy version we keep doing  this process until there is no more noises so we   are set the time step 0 in which we have no  more noise we give this latent to the decoder   which will turn it into an image this is how the  texture image works the image to image on the   other side so let's try to do the image to image  so to do the image to image we need to go here   and we uncomment this code here this allows us to  start with the dog and then give for example some   prompt for example we want a this talk here we  want to say okay we want a dog stretching on the   floor highly detailed Etc we can run it I will not  run it because it will take another five minutes   and if we do this we can set the strength of  let's say 0.6 which means that let's go here   so we set a strength of 0.6 so we have  this input image strength of 0.6 means   that we will add we will encode it with the  virational autoencoder will become a latent   We'll add some noise but how much noise not  all the noises so that it becomes completely   noise but less noise than that so as a let's  let's say 60 noise is not really true because   um because it depends on the schedule in our case  it's linear so it can be considered 60 of noise   we then give this image to the scheduler which  will start not from the 1000 step it will start   before so if we set the strength to 0.6 it will  start from the 600th step and then move by 20 uh   will keep going um 600 and 580 then 560 then 100  540 Etc until it reaches 20. so in total it will   do less steps because we start from a less noisy  example but at the same time because we start with   less noise the the unit also has less freedom  to change the uh to alter the image because he   already have the image so he cannot change it too  much so how do you adjust the noise the the noise   level depends if you want the unit to pay very  much attention to the input image and do not   change it too much then you add less noise if you  want to change completely the original image then   you can add all the possible noises so you set the  strength to one and this is how the image to image   works I did it implement the impainting because  the reason is that the pre-trained model here so   the model that we are using is not fine-tuned  for in painting So if you got on the website   and you look at the model card they have another  model for impainting which has different weights   here the this one here but this the structure of  this model is also little different because they   have in the unit they have five additional input  channels for the mask I will of course implement   it in my repository directly so I will modify the  code and also implement the code for impainting so   that we can support this model but unfortunately  I don't have the time now because in China here is   the goch India and I'm going to lautia with my  my wife so we are a little short of time but I   hope that with my video guys you you got really  into stable diffusion and you understood what   is happening under the hood instead of just using  the hugging face library and also notice that the   model itself is not so particularly sophisticated  if you check the decoder and the encoder they are   just a bunch of convolutions and up sampling and  the normalizations just like any other computer   vision model and the same goes on for the unit of  course there are very smart choices in how they   do it okay but that's not the important thing  of the diffusion and actually if we study the   diffusion models like score models you will see  that it doesn't even matter the structure of the   model as long as the model is expressive it will  actually learn the score function in the same way   but this is not our case in this video I will talk  about score model in future videos what I want   you to understand is that how this all mechanism  works together so how can we just learn a model   that predicts the noise and then we come up with  images and let me rehearse again the idea so we   started by training a model that needs to learn  a probability distribution as you remember P of   theta here we we cannot learn this one directly  because we don't know how to marginalize here so   what we did is we found some lower Bound for this  quantity here and we maximize this lower bound   how do we maximize this lower Bound by training  a model by running the gradient descent on this   loss this loss produces a model that allow us to  that predicts the noise then how do we actually   use this model with the predicted noise to  go back in time with the noise because the   forward process we know how to go it's defined  by us how to add noise but in back in time so   how to remove noise we don't know and we do it  according to the formulas that I have described   in the sampler so the formula number seven and the  formula number also this one actually we can use   actually I will show you in my other um here  I have another Repository I think it's called   the python in which I implemented the dbm paper  but by using this algorithm here so if you are   interested in this version of the denoising you  can check my other repository here this one ddpm   and I also wanted to show you how The impainting  Works how the how the image to image and how the   text image works of course the possibilities are  Limitless it all depends on the powerfulness of   the model and how you use it and I hope you use  it in a clever way to build the amazing products   I also want to thank very much many repositories  that I have used as a self-studding material so   because of course I didn't make up all this by  myself I studied a lot of papers I read I think   to study these division models I read more than  30 papers in the last few weeks so it took me a   lot of time but I was really passionate about this  kind of models because they're complicated and I   really like to study things that can generate new  stuff so I want to really thank uh in particularly   some resources that I have used let me see these  ones here so the official code the this guy Diva   Gupta this other repository from um this person  here which I used very much actually as a base   and the diffusers library from this hugging  face upon which I based most of the code of   my sampler because I think it's better to  use because we are actually just applying   some formulas there is no point in writing it  from zero the point is actually understanding   what is happening with these formulas and why we  are doing it the things we are doing and as usual   the full code is available I will also make all  the slides available for you guys and I hope if   you are in China you also have a great holiday  with me and if you're not in China I hope you   have a great time with your family and friends  and everyone else so welcome back to my channel   anytime and please feel free to comment on send  me a comment or if you didn't understand something   or if you want me to explain something better  because I'm always available for explanation   and guys I do this not as my full-time job of  course I do it as a part-time and lately I'm   I'm doing consulting so I'm very busy but  sometimes I take time to record videos   and so please share my channel Share my video  with people if you like it and so that my kind   channel can grow and I have more motivation to  keep doing this kind of videos which take really   a lot of time because to prepare a video like this  I spend around many weeks of research but this is   okay I do it for as a passion I don't do it as a  job and I spend a really a lot of time preparing   all the slides and preparing all the speeches  and preparing and the code and cleaning it and   commending it etc etc I always do it for free  so if you would like to support me the best way   is to subscribe like my video and share it with  other people thank you guys and have a nice day