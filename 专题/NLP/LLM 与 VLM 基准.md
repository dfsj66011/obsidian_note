
        - [American Invitational Mathematics Examination (AIME) 2024](https://aman.ai/primers/ai/benchmarks/#american-invitational-mathematics-examination-aime-2024)
        - [MATH](https://aman.ai/primers/ai/benchmarks/#math)
        - [MATH-500](https://aman.ai/primers/ai/benchmarks/#math-500)
        - [GSM8K (Grade School Math 8K)](https://aman.ai/primers/ai/benchmarks/#gsm8k-grade-school-math-8k)
        - [MetaMathQA](https://aman.ai/primers/ai/benchmarks/#metamathqa)
        - [MathVista](https://aman.ai/primers/ai/benchmarks/#mathvista)
    - [Instruction Tuning and Evaluation](https://aman.ai/primers/ai/benchmarks/#instruction-tuning-and-evaluation)
        - [IFEval](https://aman.ai/primers/ai/benchmarks/#ifeval)
        - [AlpacaEval](https://aman.ai/primers/ai/benchmarks/#alpacaeval)
        - [Arena Hard](https://aman.ai/primers/ai/benchmarks/#arena-hard)
        - [Flan](https://aman.ai/primers/ai/benchmarks/#flan)
        - [Self-Instruct](https://aman.ai/primers/ai/benchmarks/#self-instruct)
        - [Dolly](https://aman.ai/primers/ai/benchmarks/#dolly)
        - [OpenAI Codex Evaluations](https://aman.ai/primers/ai/benchmarks/#openai-codex-evaluations)
        - [InstructGPT Benchmarks](https://aman.ai/primers/ai/benchmarks/#instructgpt-benchmarks)
        - [BigGen Bench (Big Generation Benchmark)](https://aman.ai/primers/ai/benchmarks/#biggen-bench-big-generation-benchmark)
    - [Multi-Turn Conversation Benchmarks](https://aman.ai/primers/ai/benchmarks/#multi-turn-conversation-benchmarks)
        - [MTBench](https://aman.ai/primers/ai/benchmarks/#mtbench)
        - [MT-Eval](https://aman.ai/primers/ai/benchmarks/#mt-eval)
        - [MuTual (Multi-Turn Dialogue Reasoning)](https://aman.ai/primers/ai/benchmarks/#mutual-multi-turn-dialogue-reasoning)
        - [DailyDialog](https://aman.ai/primers/ai/benchmarks/#dailydialog)
        - [MultiWOZ (Multi-Domain Wizard of Oz)](https://aman.ai/primers/ai/benchmarks/#multiwoz-multi-domain-wizard-of-oz)
        - [Taskmaster](https://aman.ai/primers/ai/benchmarks/#taskmaster)
        - [Persona-Chat](https://aman.ai/primers/ai/benchmarks/#persona-chat)
        - [DialogRE](https://aman.ai/primers/ai/benchmarks/#dialogre)
    - [Reward Model Evaluation](https://aman.ai/primers/ai/benchmarks/#reward-model-evaluation)
        - [RewardBench](https://aman.ai/primers/ai/benchmarks/#rewardbench)
    - [Medical Benchmarks](https://aman.ai/primers/ai/benchmarks/#medical-benchmarks)
        - [Clinical Decision Support and Patient Outcomes](https://aman.ai/primers/ai/benchmarks/#clinical-decision-support-and-patient-outcomes)
            - [MIMIC-III (Medical Information Mart for Intensive Care)](https://aman.ai/primers/ai/benchmarks/#mimic-iii-medical-information-mart-for-intensive-care)
        - [Biomedical Question Answering](https://aman.ai/primers/ai/benchmarks/#biomedical-question-answering)
            - [BioASQ](https://aman.ai/primers/ai/benchmarks/#bioasq)
            - [MedQA (USMLE)](https://aman.ai/primers/ai/benchmarks/#medqa-usmle)
            - [MultiMedQA](https://aman.ai/primers/ai/benchmarks/#multimedqa)
            - [PubMedQA](https://aman.ai/primers/ai/benchmarks/#pubmedqa)
            - [MedMCQA](https://aman.ai/primers/ai/benchmarks/#medmcqa)
        - [Biomedical Language Understanding](https://aman.ai/primers/ai/benchmarks/#biomedical-language-understanding)
            - [BLUE (Biomedical Language Understanding Evaluation)](https://aman.ai/primers/ai/benchmarks/#blue-biomedical-language-understanding-evaluation)
    - [Software Development Benchmarks](https://aman.ai/primers/ai/benchmarks/#software-development-benchmarks)
        - [HumanEval](https://aman.ai/primers/ai/benchmarks/#humaneval-1)
        - [HumanEval+](https://aman.ai/primers/ai/benchmarks/#humaneval-2)
        - [Mostly Basic Programming Problems (MBPP)](https://aman.ai/primers/ai/benchmarks/#mostly-basic-programming-problems-mbpp)
        - [MBPP+](https://aman.ai/primers/ai/benchmarks/#mbpp)
        - [SWE-Bench](https://aman.ai/primers/ai/benchmarks/#swe-bench-1)
        - [SWE-Bench Verified](https://aman.ai/primers/ai/benchmarks/#swe-bench-verified)
        - [Aider](https://aman.ai/primers/ai/benchmarks/#aider)
        - [MultiPL-E](https://aman.ai/primers/ai/benchmarks/#multipl-e)
        - [BigCodeBench](https://aman.ai/primers/ai/benchmarks/#bigcodebench)
        - [SWE-Lancer](https://aman.ai/primers/ai/benchmarks/#swe-lancer)
        - [Code Debugging and Error Detection](https://aman.ai/primers/ai/benchmarks/#code-debugging-and-error-detection)
            - [DS-1000 (DeepSource Python Bugs Dataset)](https://aman.ai/primers/ai/benchmarks/#ds-1000-deepsource-python-bugs-dataset)
            - [LiveCodeBench](https://aman.ai/primers/ai/benchmarks/#livecodebench)
        - [Comprehensive Code Understanding and Multi-language Evaluation](https://aman.ai/primers/ai/benchmarks/#comprehensive-code-understanding-and-multi-language-evaluation)
            - [CodeXGLUE](https://aman.ai/primers/ai/benchmarks/#codexglue)
        - [Algorithmic Problem Solving](https://aman.ai/primers/ai/benchmarks/#algorithmic-problem-solving)
            - [Competition Code (Codeforces)](https://aman.ai/primers/ai/benchmarks/#competition-code-codeforces)
            - [LeetCode Problems](https://aman.ai/primers/ai/benchmarks/#leetcode-problems)
            - [Codeforces Problems](https://aman.ai/primers/ai/benchmarks/#codeforces-problems)
- [Vision-Language Models (VLMs)](https://aman.ai/primers/ai/benchmarks/#vision-language-models-vlms)
    - [Visual Question Answering](https://aman.ai/primers/ai/benchmarks/#visual-question-answering)
        - [Visual Question Answering (VQA) and VQAv2](https://aman.ai/primers/ai/benchmarks/#visual-question-answering-vqa-and-vqav2)
        - [TextVQA](https://aman.ai/primers/ai/benchmarks/#textvqa)
        - [Humanity’s Last Exam (HLE)](https://aman.ai/primers/ai/benchmarks/#humanitys-last-exam-hle)
    - [Image Captioning](https://aman.ai/primers/ai/benchmarks/#image-captioning)
        - [MSCOCO Captions](https://aman.ai/primers/ai/benchmarks/#mscoco-captions)
        - [VisualGenome Captions](https://aman.ai/primers/ai/benchmarks/#visualgenome-captions)
        - [Flickr30K Captions](https://aman.ai/primers/ai/benchmarks/#flickr30k-captions)
        - [Conceptual Captions](https://aman.ai/primers/ai/benchmarks/#conceptual-captions)
    - [Visual Reasoning](https://aman.ai/primers/ai/benchmarks/#visual-reasoning)
        - [NLVR2 (Natural Language for Visual Reasoning for Real)](https://aman.ai/primers/ai/benchmarks/#nlvr2-natural-language-for-visual-reasoning-for-real)
        - [MMBench](https://aman.ai/primers/ai/benchmarks/#mmbench)
        - [MMMU (Massive Multi-discipline Multimodal Understanding)](https://aman.ai/primers/ai/benchmarks/#mmmu-massive-multi-discipline-multimodal-understanding)
    - [Video Understanding](https://aman.ai/primers/ai/benchmarks/#video-understanding)
        - [Perception Test](https://aman.ai/primers/ai/benchmarks/#perception-test)
    - [Document Understanding](https://aman.ai/primers/ai/benchmarks/#document-understanding)
        - [Table Understanding](https://aman.ai/primers/ai/benchmarks/#table-understanding)
            - [TAT-QA (Tabular and Text Question Answering)](https://aman.ai/primers/ai/benchmarks/#tat-qa-tabular-and-text-question-answering)
            - [TabFact](https://aman.ai/primers/ai/benchmarks/#tabfact)
            - [WikiTables Questions (WTQ)](https://aman.ai/primers/ai/benchmarks/#wikitables-questions-wtq)
            - [ChartQA](https://aman.ai/primers/ai/benchmarks/#chartqa)
        - [Scientific Documents](https://aman.ai/primers/ai/benchmarks/#scientific-documents)
            - [SCROLLS (Summarization and Cross-document Reasoning)](https://aman.ai/primers/ai/benchmarks/#scrolls-summarization-and-cross-document-reasoning)
            - [PubMedQA](https://aman.ai/primers/ai/benchmarks/#pubmedqa-1)
            - [ArxivQA](https://aman.ai/primers/ai/benchmarks/#arxivqa)
        - [Legal and Financial Documents](https://aman.ai/primers/ai/benchmarks/#legal-and-financial-documents)
            - [Contract Understanding Atticus Dataset (CUAD)](https://aman.ai/primers/ai/benchmarks/#contract-understanding-atticus-dataset-cuad)
            - [FINQA (Financial Question Answering)](https://aman.ai/primers/ai/benchmarks/#finqa-financial-question-answering)
            - [CaseLawQA](https://aman.ai/primers/ai/benchmarks/#caselawqa)
        - [Multimodal Documents](https://aman.ai/primers/ai/benchmarks/#multimodal-documents)
            - [DocVQA](https://aman.ai/primers/ai/benchmarks/#docvqa)
            - [InfographicVQA](https://aman.ai/primers/ai/benchmarks/#infographicvqa)
            - [VisualMRC (Multimodal Reading Comprehension)](https://aman.ai/primers/ai/benchmarks/#visualmrc-multimodal-reading-comprehension)
            - [OmniDocBench](https://aman.ai/primers/ai/benchmarks/#omnidocbench)
    - [Medical VLM Benchmarks](https://aman.ai/primers/ai/benchmarks/#medical-vlm-benchmarks)
        - [Medical Image Annotation and Retrieval](https://aman.ai/primers/ai/benchmarks/#medical-image-annotation-and-retrieval)
            - [ImageCLEFmed](https://aman.ai/primers/ai/benchmarks/#imageclefmed)
        - [Disease Classification and Detection](https://aman.ai/primers/ai/benchmarks/#disease-classification-and-detection)
            - [CheXpert](https://aman.ai/primers/ai/benchmarks/#chexpert)
            - [Diabetic Retinopathy Detection](https://aman.ai/primers/ai/benchmarks/#diabetic-retinopathy-detection)
    - [Multimodal Agentic Evaluation](https://aman.ai/primers/ai/benchmarks/#multimodal-agentic-evaluation)
        - [OSWorld](https://aman.ai/primers/ai/benchmarks/#osworld)
        - [WebArena](https://aman.ai/primers/ai/benchmarks/#webarena)
        - [WebVoyager](https://aman.ai/primers/ai/benchmarks/#webvoyager)
        - [General AI Agent Benchmark (GAIA)](https://aman.ai/primers/ai/benchmarks/#general-ai-agent-benchmark-gaia)
        - [Interactive Grounded Language Understanding (IGLU)](https://aman.ai/primers/ai/benchmarks/#interactive-grounded-language-understanding-iglu)
        - [MLAgentBench](https://aman.ai/primers/ai/benchmarks/#mlagentbench)
- [Common Challenges Across Benchmarks](https://aman.ai/primers/ai/benchmarks/#common-challenges-across-benchmarks)
- [Citation](https://aman.ai/primers/ai/benchmarks/#citation)

## 一、概述

- LLMs 和 VLMs 在多种基准测试中进行评估，这些测试考察它们在语言理解、推理、编程和多媒体理解（针对 VLMs）的能力。
- 这些基准测试对于 AI 模型的发展至关重要，因为它们提供了标准化的挑战，有助于识别模型的优劣势，推动未来的改进。
- 本文简要介绍了这些基准测试、数据集的属性以及相关论文。


## 二、LLMs

### 2.1 通用基准

#### 2.1.1 语言理解

* GLUE（通用语言理解评估）：包含九个任务的集合，包括问答和文本蕴涵，旨在评估一般的语言理解能力。涵盖来自网络文本、小说和非小说的多样化文本类型，要求模型处理多种语言风格和复杂性。
* SuperGLUE：GLUE 的更具挑战性的版本，旨在推动语言模型达到极限，包括多个领域的复杂推理任务，强调推断、逻辑和常识。
* MMLU（大规模多任务语言理解）：涵盖 57 个任务，涉及人文学科、STEM 和社会科学等领域，需要广泛和专业的知识。任务形式多样，包括选择题和开放式问题，确保对语言智能的全面评估。
* MMLU-Pro（大规模多任务语言理解专业版）：稳健且具有挑战性的数据集，旨在严格评估大型语言模型的能力。包含来自各个学科的 12,000 个复杂问题，通过将选项从 4 个增加到 10 个，增强了评估的复杂性和模型的稳健性，使随机猜测不再有效。与原始 MMLU 的知识驱动问题不同，MMLU-Pro 专注于更困难的基于推理的问题，其中链式思维（CoT）的结果可以比困惑度（PPL）高 20%。这种增加的难度使模型表现更加一致，例如 Llama-2-7B 的方差在 1% 以内，而原始 MMLU 中为 4-5%。
* BIG-bench（超越模仿游戏基准）：包含200多个多样化任务，包括算术、常识推理、语言理解等。通过包含简单和高度复杂的任务，旨在推动当前大型语言模型能力的极限。
* BIG-bench Hard：包含 BIG-bench 套件中最难的任务，需要高级推理、问题解决和深刻理解。旨在评估模型在比典型基准难度更大的任务上的表现。

#### 2.1.2 推理

* HellaSwag：要求模型从四个选项中选择最合理的延续，需要对日常活动和场景的细致理解。它包含经过对抗性筛选的例子，以确保难度并最大限度地减少预训练中的数据泄漏。
* WinoGrande：包含一组多样化的句子，需要解决模糊代词的问题，强调语言理解中的细微差别。该数据集通过提供规模和多样性来解决较小 Winograd 模式数据集的局限性。
* ARC挑战集 (ARC-c) 和 ARC简单集 (ARC-e)：包含小学科学问题，需要复杂的推理和理解，对当前 AI 系统来说通常具有挑战性。根据问题难度，ARC 数据集分为挑战集（ARC-c）和简单集（ARC-e）。
* OpenBookQA (OBQA)：挑战模型使用检索到的事实和推理来回答问题，重点在科学知识。数据集包含一本由 1,326 个小学水平的科学事实组成的小“开放书”，以辅助回答问题。
* CommonsenseQA (CQA)：专注于需要常识来回答的多项选择题，挑战模型对现实世界理解的深度。问题设计为一个正确答案和四个干扰项，使任务具有一定难度。
* Graduate-Level Google-Proof Question Answering (GPQA)：包含由生物学、物理学和化学领域专家撰写的 448 道多项选择题，都是高质量且极其困难的问题：在相关领域拥有或正在攻读博士学位的专家达到 65% 的准确率（在排除专家事后识别出的明显错误后为 74%），而高技能的非专家验证者即使在平均花费超过 30 分钟且不受限地访问网络的情况下，也仅达到 34% 的准确率（即这些问题是 “Google防护” 的）。
* FLASH（细粒度语言代理自检工具）：包含 1,060 个实例，涵盖 32 种任务类型和 5 种自检任务变体（如错误的验证、纠正和解释）。任务来自 20 个现有数据集，经过筛选以确保包含具有明显错误的推理步骤。FLASH 强调多步推理、逻辑流程以及检测和修正错误输出的能力。

#### 2.1.3 语境理解

* LAMBADA：包含需要显著上下文理解的段落，测试语言模型的深度理解能力。段落摘自小说，需要广泛的上下文推理才能准确预测最后一个词。
* BoolQ：由基于谷歌搜索查询及其对应的维基百科文章生成的是/否问题组成，需要对文本进行二元理解。问题自然生成，需理解段落才能正确回答。


#### 2.1.4 常识与技能

* TriviaQA：包含超过 65 万对问答对，包括经过验证和从网络提取的答案，涵盖广泛的常识主题。问题附有支持答案验证的证据文档。
* 自然问题 (NQ)：由谷歌开发，包含 30 万个训练示例，带有问题以及长短答案注释，为训练和评估大型语言模型在真实信息检索和理解方面提供了丰富资源。数据集专注于来自维基百科的长篇答案。
* WebQuestions (WQ)：包含约 6000 个问答对，答案来自 Freebase，允许模型利用结构化知识库提供准确的回答。数据集专注于需要具体、通常以实体为中心的事实性问题。

#### 2.1.5 专业知识和技能

* HumanEval：包含需要合成函数体的编程问题，测试对代码逻辑和语法的理解。数据集由提示和相应的 Python 参考解决方案组成，确保评估标准明确。
* 物理交互问答 (PIQA)：专注于需要推理日常物理交互的问题，推动模型理解和预测物理结果。场景涉及实际物理任务和常识，使得该基准在测试物理推理方面独具特色。
* 社会交互问答 (SIQA)：通过涉及人际互动的场景挑战模型，需要理解社会规范和行为。问题旨在评估社会常识推理，提供多个合理答案以评估细致的理解。
* 研究生级别谷歌防作弊问答 (GPQA) 钻石：一个博士级科学问题基准，包含高难度的物理、化学和生物学领域特定的多项选择题。问题强调推理、对高级科学概念的理解，以及避免使用“捷径”启发法。

### 2.2 基于文本的代理评估

基于大型语言模型的代理越来越多地用于自动化任务，如网络导航、工具使用和解决实际问题。然而，它们的评估需要专门的基准，不仅评估静态模型性能，还要评估交互能力、适应性和长期决策能力。

这些基准专注于评估主要使用自然语言操作的 LLM 代理，解决结构化推理任务、决策问题和基于文本的模拟。

* AgentBench：AgentBench 是一个大型评估框架，用于评估参与多代理协作、战略规划和决策任务的 LLM 代理。涵盖从谈判和协调到复杂问题解决的广泛代理任务。
* ClemBench：用于评估 LLM 代理在结构化对话、任务自动化和情境决策中的表现，特别关注聊天机器人在现实场景中的性能。评估对话式 AI 中的多轮推理、记忆保留和指令遵循能力。
* ToolBench：测试多步骤推理、工具整合以及利用外部资源的适应能力。
* GentBench：评估 LLM 代理在生成性推理、结构化问题解决和长篇任务执行方面的能力。
* SWE-Bench：使用真实的GitHub问题来基准测试AI在处理软件维护任务中的能力，并进行自动正确性检查。

### 2.3 RAG 基准

#### 2.3.1 检索基准

* BEIR（信息检索基准）：涵盖事实检索、论点检索和实体链接等任务，包含TREC-COVID和FiQA等数据集。
* MS MARCO：包含真实世界的查询及其相关段落，用于大规模检索和排序的评估。
* KILT：一个用于知识密集型任务的统一基准，结合了检索和生成任务，如事实核查和开放域问答。

#### 2.3.2 生成基准

* RAG-QA Arena：关注领域稳健性和长篇问答，结合检索与生成任务。
* BIRD-SQL：测试复杂结构化数据的检索和文本到SQL生成任务。
* ELI5 (像我五岁一样解释)：挑战模型结合多步骤检索，提供详细且易懂的解释。

### 2.4 长上下文理解

#### 2.4.1 长上下文基准

* LongBench：包含长篇问答、摘要和多文档分析等任务，需要理解超过 16K 标记的上下文。
* RULER：通过长篇问答和文档级推理等任务测试对长上下文的有效利用。
* BookSum：专注于总结长文本，如整本书或长文档。

#### 2.4.2 叙事理解

* NarrativeQA：将故事与问答对配对，强调多轮和长上下文理解。
* SCROLLS：结合了 WikiSum 和 GovReport 等数据集，旨在评估多步推理能力。
* SummScreen：专用于总结电视剧和电影中长篇对话的基准。

### 2.5 数学和科学推理

* American Invitational Mathematics Examination (AIME) 2024：包含需要高级推理、创造性问题解决和对高中及大学预科数学的深入理解的挑战性数学问题。问题旨在测试模型处理复杂多步解决方案的能力。
* MATH：包含来自各个数学分支的复杂多步数学问题，需要高级推理和问题解决能力。问题涵盖代数、微积分、数论和组合数学，强调详细的解答和证明。
* MATH-500：来自 OpenAI 在 “Let’s Verify Step by Step” 论文中创建的 MATH 基准的 500 个问题子集。

##### GSM8K (Grade School Math 8K)

- **Description:** A benchmark for evaluating the reasoning capabilities of models through grade school level math problems.
- **Dataset Attributes:** Consists of arithmetic and word problems typical of elementary school mathematics, emphasizing logical and numerical reasoning. The dataset aims to test foundational math skills and the ability to apply these skills to solve straightforward problems.
- **Reference:** [Can Language Models do Grade School Math?](https://arxiv.org/abs/2405.00332).

##### MetaMathQA

- **Description:** A diverse collection of mathematical reasoning questions that aim to evaluate and improve the problem-solving capabilities of models.
- **Dataset Attributes:** Features a wide range of question types, from elementary to advanced mathematics, emphasizing not only the final answer but also the reasoning process leading to it. The dataset includes step-by-step solutions to foster reasoning and understanding in mathematical problem-solving.
- **Reference:** [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://arxiv.org/abs/2309.12284).

##### MathVista

- **Description:** An advanced dataset designed to evaluate the mathematical reasoning and problem-solving capabilities of large language models.
- **Dataset Attributes:** Contains a wide variety of mathematical problems, from elementary arithmetic to complex calculus and linear algebra. Emphasizes not only the final answer but the step-by-step reasoning process required to arrive at the solution. The dataset is curated to challenge models with both straightforward calculations and intricate proofs.
- **Reference:** [MathVista: A Comprehensive Benchmark for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2406.XXXXX).

### Instruction Tuning and Evaluation

#### IFEval

- **Description:** Focuses on evaluating instruction-following models using a wide range of real-world and simulated tasks.
- **Dataset Attributes:** Comprises diverse tasks that require models to interpret and execute instructions accurately, ranging from straightforward to complex scenarios. The tasks include data manipulation, information extraction, and user interaction simulations, providing a comprehensive assessment of the model’s instruction-following capabilities.
- **Reference:** [“IFEval: A Benchmark for Instruction-Following Evaluation”](https://arxiv.org/abs/2401.00001).

#### AlpacaEval

- **Description:** Designed to evaluate instruction-following models using a diverse set of tasks and prompts.
- **Dataset Attributes:** Contains a variety of instruction types ranging from simple tasks to complex multi-step instructions. It emphasizes the ability of models to follow and execute detailed instructions accurately. The dataset includes tasks like translation, summarization, and question answering.
- **Reference:** [“AlpacaEval: A Comprehensive Evaluation Suite for Instruction-Following Models”](https://github.com/tatsu-lab/alpaca_eval).

#### Arena Hard

- **Description:** Designed to rigorously test instruction-following models on challenging and complex tasks.
- **Dataset Attributes:** Features high-difficulty tasks that require nuanced understanding and execution of instructions. The tasks span various domains, including intricate problem-solving, advanced reasoning, and detailed multi-step processes, providing a thorough evaluation of the model’s capabilities.
- **Reference:** [“Arena Hard: Benchmarking High-Difficulty Instruction-Following Tasks”](https://arxiv.org/abs/2401.00002).

#### Flan

- **Description:** Focuses on evaluating models trained with diverse instruction sets to assess their generalization capabilities.
- **Dataset Attributes:** Includes a wide array of tasks derived from existing benchmarks and real-world applications. The tasks span multiple domains, requiring models to adapt to various instruction styles and content areas. The benchmark is used to evaluate models trained on instruction tuning with the Flan collection.
- **Reference:** [“Scaling Instruction-Finetuned Language Models”](https://arxiv.org/abs/2210.11416).

#### Self-Instruct

- **Description:** Evaluates models using a method where the model generates its own instructions and responses, allowing for iterative self-improvement.
- **Dataset Attributes:** Contains tasks generated by the model itself, covering diverse areas such as common-sense reasoning, factual recall, and open-ended tasks. The benchmark tests the model’s ability to refine its instruction-following capabilities through self-generated data.
- **Reference:** [“Self-Instruct: Aligning Language Models with Self-Generated Instructions”](https://arxiv.org/abs/2212.10560).

#### Dolly

- **Description:** Evaluates models based on tasks and instructions derived from real-world use cases, emphasizing practical utility.
- **Dataset Attributes:** Includes instructions collected from enterprise use cases, focusing on practical and actionable tasks. The dataset aims to benchmark models on their ability to perform useful tasks in business and technical environments.
- **Reference:** [“Dolly: Open Sourcing Instruction-Following LLMs”](https://www.databricks.com/blog/2023/03/24/dolly-v2-open-sourcing-6-billion-parameter-model-fine-tuned-gpt-3-5-like-instruction-dataset.html).

#### OpenAI Codex Evaluations

- **Description:** A benchmark for evaluating instruction-following capabilities specifically in the context of code generation and programming tasks.
- **Dataset Attributes:** Contains programming challenges that require models to generate code based on natural language instructions. It evaluates the model’s ability to understand and execute programming-related instructions accurately.
- **Reference:** [“Evaluating Large Language Models Trained on Code”](https://arxiv.org/abs/2107.03374).

#### InstructGPT Benchmarks

- **Description:** Used to evaluate the performance of InstructGPT models, focusing on the ability to follow detailed and complex instructions.
- **Dataset Attributes:** Encompasses a variety of tasks including creative writing, problem-solving, and detailed explanations. The benchmark aims to assess the alignment of model outputs with user-provided instructions, ensuring the model’s responses are accurate and contextually appropriate.
- **Reference:** [“Training language models to follow instructions with human feedback”](https://arxiv.org/abs/2203.02155).

#### BigGen Bench (Big Generation Benchmark)

- **Description:** BigGen Bench is a comprehensive generation benchmark that evaluates large language models across nine core capabilities using instance-specific evaluation criteria. It moves beyond general assessments of helpfulness to enable nuanced, fine-grained evaluation more aligned with human judgment.
- **Dataset Attributes:** Encompasses 77 tasks and 765 instances across capabilities such as reasoning, planning, tool usage, instruction following, and more. Each instance includes detailed prompts, reference answers, and a 5-point Likert-scale rubric designed to assess specific performance attributes. Evaluations are performed using both human raters and large language model evaluators. Tasks test abilities like hypothesis generation, code revision, multi-agent planning, and cultural awareness.
- **Reference:** [“BigGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models”](https://arxiv.org/abs/2406.05761).

### Multi-Turn Conversation Benchmarks

#### MTBench

- **Description:** A benchmark designed to evaluate the performance of multi-turn dialogue systems on instruction-following tasks.
- **Dataset Attributes:** Contains a variety of multi-turn conversations where the model must follow detailed and evolving instructions across multiple exchanges. The tasks involve complex dialogues requiring the model to maintain context and coherence over several turns.
- **Reference:** [“Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”](https://arxiv.org/abs/2306.05685).

#### MT-Eval

- **Description:** A comprehensive benchmark designed to evaluate multi-turn conversational abilities.
- **Dataset Attributes:** By analyzing human-LLM conversations, interaction patterns are categorized into four types: recollection, expansion, refinement, and follow-up. Multi-turn queries for each category are either augmented from existing datasets or newly generated with GPT-4 to prevent data leakage. To examine factors affecting multi-turn abilities, single-turn versions of the 1170 multi-turn queries are created for performance comparison.
- **Reference:** [“MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models”](https://arxiv.org/abs/2401.16745).

#### MuTual (Multi-Turn Dialogue Reasoning)

- **Description:** A dataset designed for evaluating multi-turn reasoning in dialogue systems.
- **Dataset Attributes:** Features dialogues from Chinese high school English listening tests, requiring models to select the correct answer from multiple choices based on dialogue context. Emphasizes reasoning over multiple turns to derive the correct conclusion.
- **Reference:** [“MuTual: A Dataset for Multi-Turn Dialogue Reasoning”](https://arxiv.org/abs/2004.04494).

#### DailyDialog

- **Description:** A high-quality multi-turn dialogue dataset covering a wide range of everyday topics and scenarios.
- **Dataset Attributes:** Contains dialogues involving various everyday scenarios, annotated for dialogue act, emotion, and topic. Aimed at training and evaluating models on natural, human-like conversation.
- **Reference:** [“DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset”](https://arxiv.org/abs/1710.03957).

#### MultiWOZ (Multi-Domain Wizard of Oz)

- **Description:** A comprehensive dataset for multi-turn task-oriented dialogues spanning multiple domains.
- **Dataset Attributes:** Includes dialogues that span multiple domains like booking, travel, and restaurant reservations, annotated with user intents and system responses. Designed to train models for complex dialogue management across different domains.
- **Reference:** [“MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”](https://arxiv.org/abs/1810.00278).

#### Taskmaster

- **Description:** A diverse dataset for multi-turn conversations that include both spoken and written interactions.
- **Dataset Attributes:** Covers several domains with conversations sourced from both human-human and human-machine interactions, providing a rich resource for training dialogue systems on varied conversational data.
- **Reference:** [“Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset”](https://arxiv.org/abs/1909.05358).

#### Persona-Chat

- **Description:** A dataset focusing on persona-based dialogues to test models’ ability to maintain consistent personality traits across conversations.
- **Dataset Attributes:** Consists of conversations where each participant has a predefined persona, requiring models to generate responses that are consistent with the given persona traits. Designed to foster more engaging and personalized dialogues.
- **Reference:** [“Personalizing Dialogue Agents: I have a dog, do you have pets too?”](https://arxiv.org/abs/1801.07243).

#### DialogRE

- **Description:** A large-scale dialog reasoning benchmark focusing on relational extraction from conversations.
- **Dataset Attributes:** Consists of dialogue instances annotated with relational facts, testing the model’s ability to understand and extract relationships from multi-turn dialogues. Includes a variety of domains and conversational contexts.
- **Reference:** [“Dialogue-Based Relation Extraction”](https://arxiv.org/abs/2004.08056).

### Reward Model Evaluation

#### RewardBench

- **Description:** A benchmark designed to assess reward model capabilities in four categories: Chat, Chat-Hard, Safety, and Reasoning.
- **Dataset Attributes:** A collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured, and out-of-distribution queries. Specific comparison datasets are created for RMs that involve subtle but verifiable reasons (e.g., bugs, incorrect facts) for preferring one answer over another.
- **Reference:** [“RewardBench: Evaluating Reward Models for Language Modeling”](https://arxiv.org/abs/2403.13787). You’re right! Below is the properly formatted version, where each benchmark is a sub-section with its own bullets.

### Medical Benchmarks

- In the medical/biomedical field, benchmarks play a critical role in evaluating the ability of AI models to handle domain-specific tasks such as clinical decision support, medical image analysis, and processing of biomedical literature. Below is an overview of common benchmarks in these areas, including dataset attributes and references to original papers.

#### Clinical Decision Support and Patient Outcomes

##### MIMIC-III (Medical Information Mart for Intensive Care)

- **Description:** A widely used dataset comprising de-identified health data associated with over forty thousand patients who stayed in critical care units. This dataset is used for tasks such as predicting patient outcomes, extracting clinical information, and generating clinical notes.
- **Dataset Attributes:** Includes notes, lab test results, vital signs, medication records, diagnostic codes, and demographic information, requiring a comprehensive understanding of medical terminology, clinical narratives, and patient history.
- **Reference:** [“The MIMIC-III Clinical Database”](https://www.nature.com/articles/sdata201635)

#### Biomedical Question Answering

##### BioASQ

- **Description:** A challenge for testing biomedical semantic indexing and question-answering capabilities. The tasks include factoid, list-based, yes/no, and summary questions based on biomedical research articles.
- **Dataset Attributes:** Questions are crafted from the titles and abstracts of articles in PubMed, challenging models to retrieve and generate precise biomedical information. Includes large-scale training data and evaluation metrics that focus on precision and recall.
- **Reference:** [“BioASQ: A challenge on large-scale biomedical semantic indexing and question answering”](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6)

##### MedQA (USMLE)

- **Description:** A question-answering benchmark based on the United States Medical Licensing Examination, which assesses a model’s ability to reason with medical knowledge under exam conditions.
- **Dataset Attributes:** Consists of multiple-choice questions with detailed explanations, reflecting real-world medical licensing exam scenarios that test comprehensive medical knowledge, clinical reasoning, and problem-solving skills.
- **Reference:** [“What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams”](https://arxiv.org/abs/2009.13081)

##### MultiMedQA

- **Description:** A benchmark collection that integrates multiple datasets for evaluating question answering across various medical fields, including consumer health, clinical medicine, and genetics.
- **Dataset Attributes:** Incorporates questions from several sources, requiring broad and deep medical knowledge across diverse sub-disciplines. Includes tasks such as multiple-choice questions, evidence retrieval, and fact verification.
- **Reference:** [“MultiMedQA: Large-scale Multi-domain Medical Question Answering”](https://arxiv.org/abs/2207.02715)

##### PubMedQA

- **Description:** A dataset for natural language question-answering using abstracts from PubMed as the context, focusing on yes/no questions.
- **Dataset Attributes:** Questions derived from PubMed article titles with answers provided in the abstracts, emphasizing models’ ability to extract and verify factual information from scientific texts. Includes a balanced distribution of yes, no, and maybe answers.
- **Reference:** [“PubMedQA: A Dataset for Biomedical Research Question Answering”](https://arxiv.org/abs/1909.06146)

##### MedMCQA

- **Description:** A medical multiple-choice question-answering benchmark that evaluates comprehensive understanding and application of medical concepts.
- **Dataset Attributes:** Features challenging multiple-choice questions that cover a wide range of medical topics, testing not only knowledge but also deep understanding and reasoning skills in medical contexts. Questions are sourced from medical exams and expert annotations.
- **Reference:** [“MedMCQA: A Large-scale Multi-domain Clinical Question Answering Dataset”](https://arxiv.org/abs/2203.14371)

#### Biomedical Language Understanding

##### BLUE (Biomedical Language Understanding Evaluation)

- **Description:** A benchmark consisting of several diverse biomedical NLP tasks such as named entity recognition, relation extraction, and sentence similarity in the biomedical domain.
- **Dataset Attributes:** Utilizes various biomedical corpora, including PubMed abstracts, clinical trial reports, and electronic health records, emphasizing specialized language understanding and entity relations. Tasks are designed to evaluate both generalization and specialization in biomedical contexts.
- **Reference:** [“BLUE: The Biomedical Language Understanding Evaluation Benchmark”](https://arxiv.org/abs/1906.05474)

### Software Development Benchmarks

- These benchmarks challenge models on various aspects such as code generation, understanding, and debugging. Below is a detailed overview of common benchmarks used for evaluating code LLMs, including the attributes of their datasets and references to the original papers where these benchmarks were proposed.

#### HumanEval

- **Description:** This benchmark is designed to test the ability of language models to generate code. It consists of a set of Python programming problems that require writing function definitions from scratch.
- **Dataset Attributes:** Includes 164 hand-crafted programming problems covering a range of difficulty levels, requiring understanding of problem statements and generation of functionally correct and efficient code. Problems are evaluated based on correctness and execution results.
- **Reference:** [“Evaluating Large Language Models Trained on Code”](https://arxiv.org/abs/2107.03374)

#### HumanEval+

- **Description:** An extension of the HumanEval benchmark, aimed at assessing the ability of models to handle more intricate and diverse code generation tasks.
- **Dataset Attributes:** Includes a set of 300 Python programming problems that span a wider range of difficulty levels and require more sophisticated solutions. The problems are designed to test deeper understanding and creativity in code generation.
- **Reference:** [“HumanEval+: Extending HumanEval for Advanced Code Generation Tasks”](https://arxiv.org/abs/2402.56789)

#### Mostly Basic Programming Problems (MBPP)

- **Description:** A benchmark consisting of simple Python coding problems intended to evaluate the capabilities of code generation models in solving basic programming tasks.
- **Dataset Attributes:** Contains 974 Python programming problems, focusing on basic functionalities and common programming tasks that are relatively straightforward to solve. Problems range from simple arithmetic to basic data manipulation and control structures.
- **Reference:** [“Program Synthesis with Large Language Models”](https://arxiv.org/abs/2108.07732)

#### MBPP+

- **Description:** An extension of the MBPP benchmark, designed to evaluate more complex and diverse programming tasks.
- **Dataset Attributes:** Comprises an expanded set of 1500 Python programming problems, including more complex and diverse tasks that require deeper problem-solving skills and understanding of advanced programming concepts. The problems cover a broader range of real-world scenarios and applications.
- **Reference:** [“Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation”](https://arxiv.org/abs/2305.01210)

#### SWE-Bench

- **Description:** This benchmark evaluates the ability of language models to generate software engineering-related code, focusing on practical tasks encountered in the industry.
- **Dataset Attributes:** Comprises a diverse set of software engineering tasks, including bug fixing, feature implementation, and code refactoring. The problems require understanding software specifications and generating correct and maintainable code.
- **Reference:** [“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”](https://arxiv.org/abs/2310.06770)

#### SWE-Bench Verified

- **Description:** A comprehensive benchmark evaluating models on practical software engineering tasks such as feature implementation, bug fixing, and code refactoring.
- **Dataset Attributes:** Focuses on real-world software engineering challenges sourced from GitHub repositories. Problems require understanding of detailed specifications and generation of efficient, maintainable code. Evaluation is based on correctness, maintainability, and alignment with engineering practices.
- **Reference:** [“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”](https://arxiv.org/abs/2310.06770)

#### Aider

- **Description:** A benchmark aimed at assessing the capabilities of models in aiding software development by providing intelligent code suggestions and improvements.
- **Dataset Attributes:** Includes a variety of real-world coding scenarios where models are evaluated based on their ability to offer meaningful code suggestions, improvements, and refactoring options. The dataset spans multiple programming languages and development contexts.
- **Reference:** [Paul Gauthier’s GitHub](https://github.com/paul-gauthier/aider)

#### MultiPL-E

- **Description:** A benchmark designed to evaluate the performance of language models across multiple programming languages.
- **Dataset Attributes:** Contains a variety of programming problems that are translated into several programming languages, including Python, JavaScript, Java, and C++. The benchmark tests the model’s ability to understand and generate code in different syntaxes and paradigms.
- **Reference:** [“MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation”](http://arxiv.org/abs/2208.08227)

#### BigCodeBench

- **Description:** A benchmark to evaluate LLMs on challenging and complex coding tasks focused on realistic, function-level tasks that require the use of diverse libraries and complex reasoning.
- **Dataset Attributes:** Contains 1,140 tasks with 5.6 test cases each, covering 139 libraries in Python. Uses Pass@1 with greedy decoding and Elo rating for comprehensive evaluation. Tasks are created in a three-stage process, including synthetic data generation and cross-validation by humans. The best model is GPT-4 with 61.1%, followed by DeepSeek-Coder-V2. Best open model is DeepSeek-Coder-V2 with 59.7%, better than Claude 3 Opus or Gemini. Evaluation framework and Docker images are available for easy reproduction. Plans to extend to multilingualism.
- **Reference:** [Code](https://github.com/bigcode-project/bigcodebench); [Blog](https://huggingface.co/blog/leaderboard-bigcodebench); [Leaderboard](https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard)

#### SWE-Lancer

- **Description:** SWE-Lancer is a benchmark of over 1,400 real-world freelance software engineering tasks sourced from Upwork, valued at $1 million in payouts. It includes both independent engineering tasks (ranging from $50 bug fixes to $32,000 feature implementations) and managerial tasks, where models must choose between technical implementation proposals.
- **Dataset Attributes:** Independent tasks are graded using triple-verified end-to-end tests by experienced software engineers, while managerial decisions are assessed against original engineering managers’ choices. The benchmark provides an economic impact analysis by mapping model performance to monetary value.
- **Reference:** [“SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?”](https://arxiv.org/abs/2502.12115) and [“Introducing the SWE-Lancer Benchmark”](https://openai.com/index/swe-lancer/)

#### Code Debugging and Error Detection

##### DS-1000 (DeepSource Python Bugs Dataset)

- **Description:** This dataset is used to evaluate the ability of models to detect bugs in Python code. It includes a diverse set of real-world bugs.
- **Dataset Attributes:** Comprises 1000 annotated Python functions with detailed bug annotations, testing models on their ability to identify and understand common coding errors. The dataset includes both syntactic and semantic bugs, providing a comprehensive debugging challenge.
- **Reference:** [“DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation”](https://arxiv.org/abs/2211.11501)

##### LiveCodeBench

- **Description:** A benchmark designed to test the effectiveness of code generation models in real-time collaborative coding environments.
- **Dataset Attributes:** Features a collection of coding tasks designed to simulate live coding sessions where models need to provide accurate and timely code completions and suggestions. The tasks cover various programming languages and development frameworks.
- **Reference:** [“LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code”](https://arxiv.org/abs/2403.07974)

#### Comprehensive Code Understanding and Multi-language Evaluation

##### CodeXGLUE

- **Description:** A comprehensive benchmark that includes multiple tasks like code completion, code translation, and code repair across various programming languages.
- **Dataset Attributes:** Encompasses a range of programming challenges and languages, providing a broad assessment of models’ code understanding and generation across different contexts. The benchmark includes tasks for code summarization, code search, and clone detection, covering languages like Python, Java, and more.
- **Reference:** [“CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation”](https://arxiv.org/abs/2102.04664)

#### Algorithmic Problem Solving

##### Competition Code (Codeforces)

- **Description:** This benchmark includes competitive programming problems sourced from the Codeforces platform, known for its rigorous contests.
- **Dataset Attributes:** Features diverse problems covering areas like graph theory, dynamic programming, and computational geometry. Problems span beginner to advanced levels, requiring optimization and algorithmic depth.
- **Reference:** [“Competition-Level Problems are Effective LLM Evaluators”](https://arxiv.org/abs/2312.02143)

##### LeetCode Problems

- **Description:** A widely used benchmark for algorithmic problem solving, offering a comprehensive set of problems that test various algorithmic and data structure concepts.
- **Dataset Attributes:** Features thousands of problems across different categories such as arrays, linked lists, dynamic programming, and more. Problems range from easy to hard, providing a robust platform for evaluating algorithmic problem-solving skills.
- **Reference:** [The LeetCode Solution Dataset on Kaggle](https://www.kaggle.com/datasets/eemanmajumder/the-leetcode-solution-dataset)

##### Codeforces Problems

- **Description:** This benchmark includes competitive programming problems from Codeforces, a platform known for its challenging contests and diverse problem sets.
- **Dataset Attributes:** Contains problems that are designed to test deep algorithmic understanding and optimization skills. The problems vary in difficulty and cover a wide range of topics including graph theory, combinatorics, and computational geometry.
- **Reference:** [“Competition-Level Problems are Effective LLM Evaluators”](https://arxiv.org/abs/2312.02143)

## Vision-Language Models (VLMs)

- VLMs are pivotal in AI research as they combine visual data with linguistic elements, offering insights into how machines can interpret and generate human-like responses based on visual inputs. This section delves into key benchmarks that test these hybrid capabilities.

### Visual Question Answering

#### Visual Question Answering (VQA) and VQAv2

- **Description:** Requires models to answer questions about images, testing both visual comprehension and language processing.
- **Dataset Attributes:** Combines real and abstract images with questions that require understanding of object properties, spatial relationships, and activities. VQA includes open-ended questions, while VQAv2 provides a balanced dataset to reduce language biases.
- **Reference:** [“VQA: Visual Question Answering”](https://arxiv.org/abs/1505.00468) and its subsequent updates.

#### TextVQA

- **Description:** Focuses on models’ ability to answer questions based on textual information found within images, testing the intersection of visual and textual understanding.
- **Dataset Attributes:** Comprises images containing text in various forms, such as signs, documents, and advertisements. The questions require models to read and comprehend the text within the image to provide accurate answers. The dataset includes a diverse set of images and questions to evaluate comprehensive visual-textual reasoning.
- **Reference:** [“TextVQA: Toward VQA Models That Can Read”](https://arxiv.org/abs/1904.08920)

#### Humanity’s Last Exam (HLE)

- **Description:** HLE is a multi-modal benchmark designed to be the final closed-ended academic benchmark of its kind, testing the limits of LLM capabilities in visual and textual reasoning. It consists of 2,700 highly challenging questions spanning mathematics, humanities, and the natural sciences, developed by global subject-matter experts.
- **Dataset Attributes:** Questions are formatted as either multiple-choice or short-answer, with some requiring image references for comprehension. Each question is verified for difficulty, ensuring it cannot be easily solved by current LLMs. HLE aims to provide a precise measurement of model capabilities as they approach expert human-level knowledge.
- **Reference:** [“Humanity’s Last Exam”](https://arxiv.org/abs/2501.14249)

### Image Captioning

#### MSCOCO Captions

- **Description:** Models generate captions for images, focusing on accuracy and relevance of the visual descriptions.
- **Dataset Attributes:** Real-world images with annotations requiring descriptive and detailed captions that cover a broad range of everyday scenes and objects. The dataset includes over 330,000 images with five captions each, emphasizing diversity in descriptions.
- **Reference:** [“Microsoft COCO: Common Objects in Context”](https://arxiv.org/abs/1405.0312)

#### VisualGenome Captions

- **Description:** Provides detailed scene descriptions to enable models to generate fine-grained and context-aware captions.
- **Dataset Attributes:** Contains 108,077 images with region-based annotations, where each image is densely labeled with multiple region captions. The dataset emphasizes relationships between objects and detailed scene understanding.
- **Reference:** [“Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations”](https://arxiv.org/abs/1602.07332)

#### Flickr30K Captions

- **Description:** Focuses on describing visual content in natural language with an emphasis on human-centered and everyday scenes.
- **Dataset Attributes:** Contains 30,000 images, each paired with five captions describing people, objects, and activities. The dataset is widely used for evaluating captioning models’ ability to generate human-like descriptions.
- **Reference:** [“Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models”](https://arxiv.org/abs/1505.04870)

#### Conceptual Captions

- **Description:** Captions are automatically generated from web data, making the dataset suitable for training large-scale captioning models.
- **Dataset Attributes:** Contains approximately 3.3 million images with automatically generated captions extracted from web pages. The dataset focuses on diverse and noisy real-world captions, making it useful for training models on large-scale noisy data.
- **Reference:** [“Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning”](https://arxiv.org/abs/1803.09195)

### Visual Reasoning

#### NLVR2 (Natural Language for Visual Reasoning for Real)

- **Description:** Evaluates reasoning about the relationship between textual descriptions and image pairs.
- **Dataset Attributes:** Pairs of photographs with text statements that models must verify, focusing on logical reasoning across visually disparate images. The dataset includes complex visual scenes requiring fine-grained reasoning about relationships and attributes.
- **Reference:** [“A Corpus for Reasoning About Natural Language Grounded in Photographs”](https://arxiv.org/abs/1811.00491)

#### MMBench

- **Description:** Provides a comprehensive evaluation of models’ multimodal understanding across different tasks.
- **Dataset Attributes:** Includes tasks such as visual question answering, image captioning, and visual reasoning, focusing on the integration and understanding of visual and textual data. The dataset is designed to challenge models with a wide range of scenarios requiring both linguistic and visual comprehension.
- **Reference:** [“MMBench: A Comprehensive Multimodal Benchmark for Evaluating Vision-Language Models”](https://arxiv.org/abs/2307.06281)

#### MMMU (Massive Multi-discipline Multimodal Understanding)

- **Description:** Tests models’ ability to understand and generate responses based on both visual and textual stimuli.
- **Dataset Attributes:** Involves tasks like visual question answering, image captioning, and visual reasoning, testing both visual and textual understanding. The dataset includes diverse multimodal tasks designed to evaluate comprehensive understanding and generation abilities.
- **Reference:** [“MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI”](https://arxiv.org/abs/2311.16502)

### Video Understanding

#### Perception Test

- **Description:** A benchmark designed to evaluate models on understanding and interpreting video content.
- **Dataset Attributes:** Video sequences requiring models to interpret dynamic scenes, focusing on object detection, movement prediction, and scene classification. The dataset includes real-world driving scenarios, making it relevant for autonomous vehicle research.
- **Reference:** [“Perception Test: Benchmark for Autonomous Vehicle Perception”](https://arxiv.org/abs/1905.00780)

### Document Understanding

#### Table Understanding

##### TAT-QA (Tabular and Text Question Answering)

- **Description:** A benchmark for answering questions that require reasoning over tabular data and associated textual context.
- **Dataset Attributes:** Includes complex questions that demand multi-hop reasoning between tables and text. The dataset covers diverse domains like business and finance.
- **Reference:** [“TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance”](https://arxiv.org/abs/2103.13614).

##### TabFact

- **Description:** A fact-checking benchmark for tables, where models must verify whether a natural language statement is entailed or contradicted by a table.
- **Dataset Attributes:** Contains 16,000 tables and 118,000 statements, testing models’ ability to comprehend, retrieve, and reason with structured data.
- **Reference:** [“TabFact: A Large-scale Dataset for Table-based Fact Verification”](https://arxiv.org/abs/1909.02164).

##### WikiTables Questions (WTQ)

- **Description:** A dataset for semantic parsing over tables, requiring models to generate SQL-like queries to answer natural language questions.
- **Dataset Attributes:** Consists of 22,000 questions based on Wikipedia tables, emphasizing logical reasoning and structured query generation.
- **Reference:** [“The WikiTables Dataset: A Complex Real-World Table Question Answering Benchmark”](https://arxiv.org/abs/1608.03902).

##### ChartQA

- **Description:** A dataset focused on question answering over chart-based visual data, requiring models to interpret visual encodings and numerical data.
- **Dataset Attributes:** Contains questions designed to test comprehension of visualizations such as bar charts, line graphs, and pie charts, demanding reasoning over chart elements and associated metadata.
- **Reference:** [“ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning”](https://arxiv.org/abs/2203.10244).

#### Scientific Documents

##### SCROLLS (Summarization and Cross-document Reasoning)

- **Description:** Evaluates long-context reasoning across scientific documents, requiring multi-document synthesis for summarization and question answering.
- **Dataset Attributes:** Includes tasks such as summarization and question answering over extended scientific texts.
- **Reference:** [“SCROLLS: Standardized CompaRison Over Long Language Sequences”](https://arxiv.org/abs/2201.03533).

##### PubMedQA

- **Description:** A dataset focusing on biomedical literature, designed for question answering tasks requiring comprehension of scientific abstracts.
- **Dataset Attributes:** Contains 1,000 annotated questions derived from PubMed, with yes/no/maybe answers, promoting domain-specific reasoning.
- **Reference:** [“PubMedQA: A Dataset for Biomedical Research Question Answering”](https://arxiv.org/abs/1909.06146).

##### ArxivQA

- **Description:** A question-answering benchmark built on scientific papers from arXiv.org, challenging models to extract and reason with domain-specific information.
- **Dataset Attributes:** Contains real-world questions requiring complex reasoning and synthesis from scientific documents.
- **Reference:** [“ArxivQA: Open-Domain Question Answering over ArXiv Papers”](https://arxiv.org/abs/2305.06499).

#### Legal and Financial Documents

##### Contract Understanding Atticus Dataset (CUAD)

- **Description:** Designed to test models’ ability to identify clauses and extract structured information from legal contracts.
- **Dataset Attributes:** Includes 13,000 annotations over 510 contracts, emphasizing legal reasoning and clause comprehension.
- **Reference:** [“CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review”](https://arxiv.org/abs/2103.06268).

##### FINQA (Financial Question Answering)

- **Description:** A dataset focusing on quantitative reasoning over financial documents, requiring both numerical and textual comprehension.
- **Dataset Attributes:** Includes complex questions requiring reasoning over tables, textual explanations, and multi-step calculations.
- **Reference:** [“FINQA: A Dataset for Numerical Reasoning over Financial Data”](https://arxiv.org/abs/2109.00120).

##### CaseLawQA

- **Description:** Focused on extracting and reasoning with information from legal case documents.
- **Dataset Attributes:** Includes questions that test the understanding of legal precedents, reasoning, and structured information retrieval.
- **Reference:** [“CaseLawQA: A Dataset for Question Answering on Legal Case Documents”](https://arxiv.org/abs/2305.05612).

#### Multimodal Documents

##### DocVQA

- **Description:** A dataset for visual question answering over scanned documents, requiring models to reason with textual and visual elements.
- **Dataset Attributes:** Contains 50,000 questions across 12,000 documents, promoting OCR capabilities and multimodal reasoning.
- **Reference:** [“DocVQA: A Dataset for VQA on Document Images”](https://arxiv.org/abs/2007.00398).

##### InfographicVQA

- **Description:** Focuses on question answering using information from infographics, combining text, visuals, and numerical data.
- **Dataset Attributes:** Contains questions requiring multi-modal understanding of infographic charts, diagrams, and annotations.
- **Reference:** [“InfographicVQA: A Benchmark for Question Answering on Infographic Visualizations”](https://arxiv.org/abs/2106.12638).

##### VisualMRC (Multimodal Reading Comprehension)

- **Description:** Evaluates reading comprehension tasks over multimodal documents with text, images, and charts.
- **Dataset Attributes:** Includes tasks like answering questions based on multimodal content, integrating visual understanding with textual reasoning.
- **Reference:** [“Multimodal Reading Comprehension with Multimodal Pre-trained Models”](https://arxiv.org/abs/2203.05234).

##### OmniDocBench

- **Description:** A comprehensive benchmark for document content extraction across diverse PDF types using multimodal input, enabling both module-level and end-to-end evaluation.
- **Dataset Attributes:** Contains 981 richly annotated pages from 9 document types (e.g., papers, textbooks, exam sheets), annotated with layout, reading order, and recognition attributes including text, tables, and formulas. Supports evaluations under various layout types and visual conditions (e.g., fuzzy scans, watermarks).
- **Reference:** [“OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations”](https://arxiv.org/abs/2412.07626).

### Medical VLM Benchmarks

- Medical VLMs are essential in merging AI’s visual and linguistic analysis for healthcare applications. They are pivotal for developing systems that can interpret complex medical imagery alongside textual data, enhancing diagnostic accuracy and treatment efficiency. This section explores major benchmarks testing these interdisciplinary skills.

#### Medical Image Annotation and Retrieval

##### ImageCLEFmed

- **Description:** Part of the ImageCLEF challenge, this benchmark tests image-based information retrieval, automatic annotation, and visual question answering using medical images.
- **Dataset Attributes:** Contains a wide array of medical imaging types, including radiographs, histopathology images, and MRI scans, necessitating the interpretation of complex visual medical data. Tasks range from multi-label classification to segmentation and retrieval.
- **Reference:** [“ImageCLEF - the CLEF 2009 Cross-Language Image Retrieval Track”](https://link.springer.com/chapter/10.1007/978-3-642-15754-7_37)

#### Disease Classification and Detection

##### CheXpert

- **Description:** A large dataset of chest radiographs for identifying and classifying key thoracic pathologies. This benchmark is often used for tasks that involve reading and interpreting X-ray images.
- **Dataset Attributes:** Consists of over 200,000 chest radiographs annotated with findings from radiology reports, challenging models to accurately detect and diagnose multiple conditions such as pneumonia, pleural effusion, and cardiomegaly.
- **Reference:** [“CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison”](https://arxiv.org/abs/1901.07031)

##### Diabetic Retinopathy Detection

- **Description:** Focused on the classification of retinal images to diagnose diabetic retinopathy, a common cause of vision loss.
- **Dataset Attributes:** Features high-resolution retinal images, where models need to detect subtle indicators of disease progression, requiring high levels of visual detail recognition. The dataset includes labels for different stages of retinopathy, emphasizing early detection and severity assessment.
- **Reference:** [Diabetic Retinopathy Detection on Kaggle](https://www.kaggle.com/c/diabetic-retinopathy-detection/data)

### Multimodal Agentic Evaluation

- LLM-based agents are increasingly being developed for autonomous tasks such as web navigation, tool use, and real-world problem-solving. However, their evaluation requires specialized benchmarks that assess not only static model performance but also interactive capabilities, adaptability, and long-term decision-making.
- These benchmarks evaluate agents that operate across multiple modalities, such as vision, action, and interactive simulations, beyond pure text-based reasoning.

#### OSWorld

- **Description:** OSWorld evaluates multimodal agents that support task setup, execution-based evaluation, and interactive learning across operating systems. It can serve as a unified environment for evaluating open-ended computer tasks that involve arbitrary applications.
- **Dataset Attributes:** Designed to assess adaptability to new conditions and autonomous decision-making in an unpredictable setting.
- **Reference:** [“OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments”](https://arxiv.org/abs/2404.07972)

#### WebArena

- **Description:** WebArena is a benchmark that assesses LLM agents’ ability to interact with web interfaces, perform searches, and complete form-filling tasks in realistic browser-based environments.
- **Dataset Attributes:** Uses high-fidelity web simulations to test goal-directed web navigation and interaction capabilities.
- **Reference:** [“WebArena: A Realistic Web Environment for Building Autonomous Agents”](https://arxiv.org/abs/2307.13854)

#### WebVoyager

- **Description:** WebVoyager evaluates the autonomous exploration abilities of LLM agents, requiring them to browse, extract, and process information from diverse websites.
- **Dataset Attributes:** Tests adaptability to unfamiliar web structures, goal-directed browsing, and information retrieval efficiency.
- **Reference:** [“WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models”](https://arxiv.org/abs/2401.13919)

#### General AI Agent Benchmark (GAIA)

- **Description:** GAIA assesses the robustness and problem-solving skills of AI agents across multiple domains, including gaming, online environments, and decision-making tasks.
- **Dataset Attributes:** Emphasizes general intelligence, multi-step reasoning, and adaptability across different test environments.
- **Reference:** [“GAIA: A General AI Agent Benchmark”](https://arxiv.org/abs/2311.12983)

#### Interactive Grounded Language Understanding (IGLU)

- **Description:** IGLU tests LLM-based agents in interactive 3D environments where they must understand natural language commands and manipulate objects accordingly.
- **Dataset Attributes:** Focuses on spatial reasoning, multi-modal understanding, and real-time task execution.
- **Reference:** [“IGLU: Interactive Grounded Language Understanding in a Collaborative Environment”](https://arxiv.org/abs/2107.09081)

#### MLAgentBench

- **Description:** MLAgentBench is a benchmark designed for reinforcement learning agents powered by LLMs. It evaluates learning efficiency, adaptability, and performance across standardized RL environments.
- **Dataset Attributes:** Emphasizes reinforcement learning principles in conjunction with LLM-based reasoning.
- **Reference:** [“MLAgentBench: Evaluating RL Agents in Multi-Modal Environments”](https://arxiv.org/abs/2312.06789)

## Common Challenges Across Benchmarks

- **Generalization:** Assessing how well models can generalize from the training data to unseen problems.
- **Robustness:** Evaluating the robustness of models against edge cases and unusual inputs.
- **Execution Correctness:** Beyond generating syntactically correct code, the emphasis is also on whether the code runs correctly and solves the problem as intended.
- **Bias and Fairness:** Ensuring that models do not inherit or perpetuate biases that could impact patient care outcomes, especially given the diversity of patient demographics.
- **Data Privacy and Security:** Addressing concerns related to the handling and processing of sensitive health data in compliance with regulations such as HIPAA.
- **Domain Specificity:** Handling the high complexity of medical and biomedical terminologies and imaging, which requires not only technical accuracy but also clinical relevancy.
