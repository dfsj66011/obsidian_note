[Distilled AI](https://aman.ai/primers/ai/)[Back to aman.ai](https://aman.ai/)

# Primers • Loss Functions

- [Overview](https://aman.ai/primers/ai/loss/#overview)
- [Multi-Class Classification](https://aman.ai/primers/ai/loss/#multi-class-classification)
- [Multi-Label Classification](https://aman.ai/primers/ai/loss/#multi-label-classification)
- [Classification Loss Functions](https://aman.ai/primers/ai/loss/#classification-loss-functions)
    - [Cross-Entropy Loss Function](https://aman.ai/primers/ai/loss/#cross-entropy-loss-function)
        - [Binary Cross-entropy Loss](https://aman.ai/primers/ai/loss/#binary-cross-entropy-loss)
        - [Focal Loss](https://aman.ai/primers/ai/loss/#focal-loss)
        - [Categorical Cross Entropy](https://aman.ai/primers/ai/loss/#categorical-cross-entropy)
    - [Kullback–Leibler (KL) Divergence](https://aman.ai/primers/ai/loss/#kullbackleibler-kl-divergence)
        - [Intuition](https://aman.ai/primers/ai/loss/#intuition)
        - [Mathematical Treatment](https://aman.ai/primers/ai/loss/#mathematical-treatment)
        - [KL Divergence vs. Cross-entropy Loss](https://aman.ai/primers/ai/loss/#kl-divergence-vs-cross-entropy-loss)
    - [Hinge Loss / Multi-class SVM Loss](https://aman.ai/primers/ai/loss/#hinge-loss--multi-class-svm-loss)
    - [PolyLoss](https://aman.ai/primers/ai/loss/#polyloss)
    - [Generalized End-to-End Loss](https://aman.ai/primers/ai/loss/#generalized-end-to-end-loss)
    - [Additive Angular Margin Loss](https://aman.ai/primers/ai/loss/#additive-angular-margin-loss)
    - [Dice Loss](https://aman.ai/primers/ai/loss/#dice-loss)
    - [Summary](https://aman.ai/primers/ai/loss/#summary)
- [Regression Loss Functions](https://aman.ai/primers/ai/loss/#regression-loss-functions)
    - [Mean Absolute Error (MAE) / L1 Loss](https://aman.ai/primers/ai/loss/#mean-absolute-error-mae--l1-loss)
    - [Mean Squared Error (MSE) / L2 Loss](https://aman.ai/primers/ai/loss/#mean-squared-error-mse--l2-loss)
        - [Why is MSE Not Used for Binary Classification?](https://aman.ai/primers/ai/loss/#why-is-mse-not-used-for-binary-classification)
    - [Root Mean Squared Error (RMSE)](https://aman.ai/primers/ai/loss/#root-mean-squared-error-rmse)
    - [Normalized Mean Absolute Error (NMAE)](https://aman.ai/primers/ai/loss/#normalized-mean-absolute-error-nmae)
    - [Huber Loss (Smooth L1 Loss / Smooth Mean Absolute Error)](https://aman.ai/primers/ai/loss/#huber-loss-smooth-l1-loss--smooth-mean-absolute-error)
        - [Asymmetric Huber Loss](https://aman.ai/primers/ai/loss/#asymmetric-huber-loss)
    - [Summary](https://aman.ai/primers/ai/loss/#summary-1)
- [Ranking Loss](https://aman.ai/primers/ai/loss/#ranking-loss)
    - [Bayesian Personalized Ranking (BPR) Loss](https://aman.ai/primers/ai/loss/#bayesian-personalized-ranking-bpr-loss)
        - [Definition](https://aman.ai/primers/ai/loss/#definition)
        - [Characteristics](https://aman.ai/primers/ai/loss/#characteristics)
        - [Applications](https://aman.ai/primers/ai/loss/#applications)
        - [Advantages](https://aman.ai/primers/ai/loss/#advantages)
        - [Limitations](https://aman.ai/primers/ai/loss/#limitations)
    - [Multiple Negative Ranking Loss](https://aman.ai/primers/ai/loss/#multiple-negative-ranking-loss)
    - [Soft Pairwise Loss](https://aman.ai/primers/ai/loss/#soft-pairwise-loss)
    - [Pairwise Logistic Loss](https://aman.ai/primers/ai/loss/#pairwise-logistic-loss)
    - [Pairwise Ranking Loss](https://aman.ai/primers/ai/loss/#pairwise-ranking-loss)
        - [Triplet Loss](https://aman.ai/primers/ai/loss/#triplet-loss)
        - [Margin Ranking Loss](https://aman.ai/primers/ai/loss/#margin-ranking-loss)
- [Contrastive Loss](https://aman.ai/primers/ai/loss/#contrastive-loss)
    - [InfoNCE Loss](https://aman.ai/primers/ai/loss/#infonce-loss)
- [Losses in Deep Learning-based Reinforcement Learning](https://aman.ai/primers/ai/loss/#losses-in-deep-learning-based-reinforcement-learning)
    - [Q-Value Loss](https://aman.ai/primers/ai/loss/#q-value-loss)
    - [Policy Gradient Loss](https://aman.ai/primers/ai/loss/#policy-gradient-loss)
- [Further Reading](https://aman.ai/primers/ai/loss/#further-reading)
- [References](https://aman.ai/primers/ai/loss/#references)
- [Citation](https://aman.ai/primers/ai/loss/#citation)

## Overview

- Loss functions, or cost/error functions, are indispensable in machine learning. They calculate the distance between a model’s predicted output and the expected (or true) output. By providing a measurement of how well a model is capturing patterns in the dataset, they serve as a critical feedback mechanism for the learning algorithm. Broadly speaking, if the predictions are largely inaccurate, the loss value will be high; if they are closer to the actual results, the loss value will be low.
- Different machine learning tasks often require distinct loss functions, each suited to the specific nature of the problem at hand. We’ll begin by discussing the typical tasks involved in machine learning and then delve into the commonly employed loss functions in each context. The loss functions are divided into those used for classification and regression tasks.
- The following figure summarizes a few common loss functions and their use cases [image source: AiEdge.io](https://theaiedge.io/).

![](https://aman.ai/primers/ai/assets/loss/23.png)

- To get a quick refresher on activation functions such as softmax and sigmoid, check out the [Activations](https://aman.ai/primers/ai/activation) primer here.
- In this article, we will explore the distinct categories of tasks in machine learning and subsequently delve into the corresponding loss functions utilized to optimize these tasks.

## Multi-Class Classification

- Multi-class classification is sometimes referred to as a “one-of-many” classification. In this scenario, each instance or sample can belong exclusively to one of CC classes. The model generates CC output neurons, which can be consolidated into a scores vector ss, then fed as input to the softmax activation function. The ground truth, or target vector tt, is a one-hot vector—this means that it features a positive class (1) and C−1C−1 negative classes (0).
- Multi-class classification is treated as a singular problem of classifying samples into one of CC classes.
- An appropriate loss function for multi-class classification is the categorical cross-entropy loss, computed as follows:
    
    L=−1N∗ΣΣtij∗log(sij)L=−1N∗ΣΣtij∗log(sij)
    
    - where NN is the total number of samples, tijtij denotes the true label (0 or 1), and sijsij is the predicted score for each class.

## Multi-Label Classification

- In multi-label classification, each sample can belong to more than one class, contrasting with multi-class classification where each sample belongs to a single class. The model still outputs CC neurons, similar to multi-class classification. However, the target vector tt can contain more than one positive class, resulting in a multi-hot vector with CC dimensionality. This differs from the one-hot vector utilized in multi-class classification, which contains a single positive class.
- This task is interpreted as CC independent binary classification problems, each deciding whether a sample belongs to a specific class or not. The binary cross-entropy loss, averaged over all classes, is commonly used for this scenario.

![](https://aman.ai/primers/ai/assets/loss/mlc.png)

- The image above [(source)](https://gombru.github.io/2018/05/23/cross_entropy_loss/) does a great job at illustrating the differences from multi-class and multi-label.

## Classification Loss Functions

- Below, we will look at a few classification loss functions.

### Cross-Entropy Loss Function

- Cross-entropy loss or Negative Log Loss (NLL) measures the performance of a classification model whose output is a probability value between 0 and 1.
- Cross-entropy loss increases as the predicted probability value moves further away from the actual label. A perfect model would have a loss of 0 because the predicted value would match the actual value.
- For binary classification problems, binary cross-entropy is used, and for multi-class classification problems, categorical cross-entropy is used.
- The cross-entropy loss calculates the error between the model’s predicted probabilities (from the softmax or sigmoid function) and the actual class labels (one-hot encoded in case of multi-class).
    
    > While cross-entropy seeks to maximize the predicted probability distribution to match the target/label distribution, i.e., maximize the negative log likelihood. However, since loss functions are typically minimized (rather than maximized, in which case they’re referred to as utility functions), we end up with a negative sign in the front of the summation to delineate minimizing the negative log likelihood.
    
- Let’s look at the formula for cross-entropy loss:
    
    - For a binary classification problem where the number of classes MM equals 2:
    
    CrossEntropyLoss=−(ylog(p)+(1−y)log(1−p))CrossEntropyLoss=−(ylog(p)+(1−y)log(1−p))
    
    ![](https://aman.ai/primers/ai/assets/loss/2.jpg)
    
    - where,
        - MM: The number of classes or output we want to predict.
        - yy: (One-hot) ground truth label (0 or 1).
        - pp: Predicted probability.
    - Note that some literature in the field denotes the prediction as ŷ y^ so the same equation then becomes:
    
    CrossEntropyLoss=−(yilog(ŷ i)+(1−yi)log(1−ŷ i))CrossEntropyLoss=−(yilog⁡(y^i)+(1−yi)log⁡(1−y^i))
    
    - Below we see the formula for when our number of classes MM is greater than 2.
    
    CrossEntropyLoss=−∑c=1Myo,clog(po,c)CrossEntropyLoss=−∑c=1Myo,clog⁡(po,c)
    
    - where,
        - MM: The number of classes or output we want to predict.
        - yo,cyo,c: One-hot ground truth label serving as a binary indicator if the class cc is the correct classification for observation oo (0 for incorrect classes c≠oc≠o, 1 for the correct class c=oc=o).
        - pp: Predicted probability for observation oo with class cc being the correct classification.
- Effectively, the cross-entropy loss “pulls” the predicted probability of the correct class towards 1 during training. This is accomplished by calculating gradients of the loss function w.r.t. the model’s weights; with the model’s sigmoid/softmax output (in case of binary/multiclass classification) serving as the prediction (i.e., the pre-argmax output is utilized since argmax is not differentiable).
- Chris Olah’s [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information) post is a good intro to the topic and motivates the concept of cross entropy pretty well.
    
- We will look at the many flavors of cross-entropy loss below.

#### Binary Cross-entropy Loss

- In machine learning, binary classification is a supervised learning algorithm that categorizes new observations into one of two classes. The model has a single output (which is fed as input to the sigmoid function) in the range [0,1]. If the output > 0.5, then class 1 (positive class), else 0 (negative class).
- Binary cross-entropy loss, also known as sigmoid cross-entropy loss, is the sigmoid activation with a cross entropy loss as shown below [(source)](https://gombru.github.io/2018/05/23/cross_entropy_loss/).

![](https://aman.ai/primers/ai/assets/loss/sigmoid_CE_pipeline.png)

- For binary classification, the binary cross-entropy loss can be computed as:

L=−1N∗Σ[y∗log(yhat)+(1−y)∗log(1−yhat)]L=−1N∗Σ[y∗log(yhat)+(1−y)∗log(1−yhat)]

- Typical binary classification problems include:
    - Medical testing to determine if a patient has certain disease or not;
    - Quality control in industry, deciding whether a specification has been met;
    - In information retrieval, deciding whether a page should be in the result set of a search or not.

#### [Focal Loss](https://arxiv.org/abs/1708.02002)

- Introduced by Facebook AI in [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) for object detection tasks, it is a variant of Cross Entropy Loss designed to address class imbalance problem in a classification task.
- The main idea behind Focal Loss is to give more weight to harder, easily misclassified examples, and less weight to easier examples. It adds a modulating factor to the standard cross entropy criterion, with the aim of focusing on the misclassified instances.
- One of the most common choices when training deep neural networks for object detection and classification problems in general.
- Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases.

FL(pt)=−(1−pt)γlog(pt)FL(pt)=−(1−pt)γlog⁡(pt)

![](https://aman.ai/primers/ai/assets/loss/8.jpg)

#### Categorical Cross Entropy

- Categorical Cross-Entropy loss is also referred to as Softmax Loss, but let’s make a quick clarification here on how it is different from Softmax activation. Softmax Loss is a Softmax activation plus a Cross Entropy loss as displayed below [(source)](https://gombru.github.io/2018/05/23/cross_entropy_loss/).

![](https://aman.ai/primers/ai/assets/loss/softmax_CE_pipeline.png)

- It is used for multi-class classification tasks where each instance can belong to only one class out of multiple classes. It is a direct application of cross entropy loss in a multi-class classification setting.

### Kullback–Leibler (KL) Divergence

- The Kullback–Leibler divergence, denoted DKL(P∣∣Q)DKL(P∣∣Q), is a type of statistical distance: a measure of how one probability distribution PP is different from a second, reference probability distribution QQ.
- A simple interpretation of the KL divergence of PP from QQ is the expected excess surprise from using QQ as a model when the actual distribution is PP.
- Note that KL divergence is commonly used as a difference (loss) and not a metric since it is not symmetric in the two distributions, i.e., DKL(P‖Q)≠DKL(Q‖P)DKL(P‖Q)≠DKL(Q‖P).

#### Intuition

- The following section has been contributed by [Garvit Suri](https://www.linkedin.com/in/lordgavy01/) and [Sanskar Soni](https://www.linkedin.com/in/sonisanskar/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BuHfMUjcySYmXxJ7uLJVvvw%3D%3D).
- While it’s common to use the KL Divergence loss function while training ChatGPT with RLHF or teaching a smaller model to imitate a larger one during Knowledge Distillation, it’s not always the easiest concept to grasp.
- That is why, in this blog, I intend to explain the intuition behind KL divergence without using any mathematical notations.
- But before diving immediately into deciphering KL divergence, we first need to grasp two fundamental concepts from Information Theory: information and entropy. These two can be jointly understood with this – the less probable an event, the more surprising it is, and therefore, the more entropy it contains, providing more information to gain from it.
- Simply, if I tell you a fact you already knew or had an idea about, you gained no information from our conversation as there was no “surprise” and, correspondingly, our exchange had little entropy.
- Now let’s clarify the mystery surrounding KL Divergence using Lego blocks.
- Imagine: two friends, AA and BB, both have a box of Lego blocks. AA’s box contains a variety of blocks in diverse colours and sizes. In contrast, BB’s box only contains red and yellow blocks, all of the same size.
- Here, AA’s box has a higher entropy, owing to the unpredictability every time you reach inside. On the other hand, BB’s box contains less entropy with only two possible outcomes whenever a block is drawn.
- Now, we let BB play with AA’s box. BB’s already accustomed to her box so she has a certain expectation as to which block she’d pull out. But AA’s box is much less predictable for her. Every time she pulls out a new block, it’s a surprise!
- This difference between BB’s expectations from pulling a block out of the box (based on her box) and what happens when she pulls a block out of AA’s box (the actual “surprise”) can be visualized as KL Divergence.
- KL Divergence compares the “surprisingness” (entropy) of BB’s box and AA’s box and tells us how different the two boxes are in terms of their predictability or the surprise they offer.
- Intuitively, the greater the KL Divergence, the more divergent your expectations are from the ground truth.
- Coming back from the world of abstractions, here are some quick pointers every AI enthusiast should know about KL-Divergence:
    - Formally, KL Divergence measures how much a probability distribution QQ diverges from a true probability distribution PP.
    - Often, KL Divergence is confused with Cross-Entropy in machine learning. However coincidentally, these terms point to the same value while classifying because the entropy of the true distribution is zero as the class labels are one-hot vectors. (AA data point belongs to the class indicated by the ‘1’ in the vector. It doesn’t have any probability of belonging to any other class. So in this case, the entropy, or the measure of uncertainty, is 0 which can be verified mathematically as well.)
    - KL divergence is used when both the predicted and true values are probability distributions, whereas cross-entropy is used when the actual values are one-hot encoded labels, not probability distributions.

#### Mathematical Treatment

- For discrete probability distributions PP and QQ defined on the same probability space, X, the relative entropy from QQ to PP is defined to be:
    
    DKL(P‖Q)=∑x∈P(x)log(P(x)Q(x))DKL(P‖Q)=∑x∈XP(x)log⁡(P(x)Q(x))
    
    - which is equivalent to
    
    DKL(P‖Q)=−∑x∈P(x)log(Q(x)P(x))DKL(P‖Q)=−∑x∈XP(x)log⁡(Q(x)P(x))
    
- In other words, it is the expectation of the logarithmic difference between the probabilities PP and QQ, where the expectation is taken using the probabilities PP.
- [Kullback-Leibler Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) offers a walk-through of KL divergence using an example.

#### KL Divergence vs. Cross-entropy Loss

- **Explanation 1:**
    
    - You will need some conditions to claim the equivalence between minimizing cross entropy and minimizing KLKL divergence. I will put your question under the context of classification problems using cross entropy as loss functions.
        
    - Let us first recall that entropy is used to measure the uncertainty of a system, which is defined as,
        
        S(v)=−∑ip(vi)logp(vi)S(v)=−∑ip(vi)log⁡p(vi)
        
        - for p(vi)p(vi) as the probabilities of different states vivi of the system. From an information theory point of view, S(v)S(v) is the amount of information is needed for removing the uncertainty.
    - For instance, the event II `I will die within 200 years` is almost certain (we may solve the aging problem for the word almost), therefore it has low uncertainty which requires only the information of `the aging problem cannot be solved` to make it certain. However, the event IIII `I will die within 50 years` is more uncertain than event II, thus it needs more information to remove the uncertainties. Here entropy can be used to quantify the uncertainty of the distribution `When will I die?`, which can be regarded as the expectation of uncertainties of individual events like II and IIII.
    - Now look at the definition of KL divergence between distributions AA and BB,
    
    DKL(A‖B)=∑ipA(vi)logpA(vi)−pA(vi)logpB(vi)DKL(A‖B)=∑ipA(vi)log⁡pA(vi)−pA(vi)log⁡pB(vi)
    
    - where the first term of the right hand side is the entropy of distribution AA, the second term can be interpreted as the expectation of distribution BB in terms of AA. And the DKLDKL describes how different BB is from AA from the perspective of AA. It’s worth of noting AA usually stands for the data, i.e. the measured distribution, and BB is the theoretical or hypothetical distribution. That means, you always start from what you observed.
        
    - To relate cross entropy to entropy and KL divergence, we formalize the cross entropy in terms of distributions AA and BB as,
        
    
    H(A,B)=−∑ipA(vi)logpB(vi)H(A,B)=−∑ipA(vi)log⁡pB(vi)
    
    - From the definitions, we can easily see,
    
    H(A,B)=DKL(A‖B)+SAH(A,B)=DKL(A‖B)+SA
    
    - If SASA is a constant, then minimizing H(A,B)H(A,B) is equivalent to minimizing DKL(A‖B)DKL(A‖B).
        
    - A further question follows naturally as how the entropy can be a constant. In a machine learning task, we start with a dataset (denoted as P()P(D)) which represent the problem to be solved, and the learning purpose is to make the model estimated distribution (denoted as P(model)P(model)) as close as possible to true distribution of the problem (denoted as P(truth)P(truth)). P(truth)P(truth) is unknown and represented by P()P(D). Therefore in an ideal world, we expect
        
        P( model )≈P()≈P( truth )P( model )≈P(D)≈P( truth )
        
        - and minimize DKL(P()‖P(model))DKL(P(D)‖P(model)). And luckily, in practice D is given, which means its entropy S(D)S(D) is fixed as a constant.
- **Explanation 2:**
    
    - Considering models usually work with the samples packed in mini-batches, for KLKL divergence and Cross-Entropy, their relation can be written as:
        
        H(q,p)=DKL(p,q)+H(p)=−∑ipilog(qi)H(q,p)=DKL(p,q)+H(p)=−∑ipilog⁡(qi)
        
    - which gives:
        
        DKL(p,q)=H(q,p)−H(p)DKL(p,q)=H(q,p)−H(p)
        
    - From the equation, we can see that KL divergence can depart into a Cross-Entropy of pp and qq (KL(p,q)KL(p,q), which is the first part), and a global entropy of ground truth pp (H(p)H(p), which is the second part).
    - In many machine learning projects, mini-batch is involved to expedite training, where the p′p′ of a minibatch may be different from the global pp. In such a case, Cross-Entropy is relatively more robust in practice while KLKL divergence needs a more stable H(p)H(p) to finish her job.

### Hinge Loss / Multi-class SVM Loss

- The hinge loss is used for “maximum-margin” classification, most notably for support vector machines (SVMs).
- The hinge loss is a convex function, so many of the usual convex optimizers used in machine learning can work with it.
- For an intended output t=±1t=±1 and a classifier score y, the hinge loss of the prediction y is defined as:

ℓ(y)=max(0,1−t⋅y)ℓ(y)=max(0,1−t⋅y)

- The hinge loss is a specific type of cost function that incorporates a margin or distance from the classification boundary into the cost calculation.
- Even if new observations are classified correctly, they can incur a penalty if the margin from the decision boundary is not large enough. The hinge loss increases linearly.

![](https://aman.ai/primers/ai/assets/loss/5.jpg)

### [PolyLoss](https://openreview.net/forum?id=gSdSJoenupI)

- Proposed in [PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions](https://arxiv.org/abs/2204.12511) by Leng et al. in 2022.
- Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems.
- Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets.
- PolyLoss is a generalized form of Cross Entropy loss.
- The paper proposes a framework to view and design loss functions as a linear combination of polynomial functions, motivated by how functions can be approximated via Taylor expansion. Under polynomial expansion, focal loss is a horizontal shift of the polynomial coefficients compared to the cross-entropy loss.
- Motivated by this new insight, they explore an alternative dimension, i.e., vertically modify the polynomial coefficients.

 PolyLoss =∑i=1nϵi(1−pt)ii+CE Loss  PolyLoss =∑i=1nϵi(1−pt)ii+CE Loss 

### [Generalized End-to-End Loss](https://arxiv.org/abs/1710.10467)

- Proposed in [Generalized End-to-End Loss for Speaker Verification](https://arxiv.org/abs/1710.10467) by Wan et al. in ICASSP 2018.
- GE2E makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function.
- Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process.
- Additionally, the GE2E loss does not require an initial stage of example selection.

L(eji)=−Sji,j+log∑k=1Nexp(Sji,k)L(eji)=−Sji,j+log⁡∑k=1Nexp⁡(Sji,k)

### [Additive Angular Margin Loss](https://arxiv.org/abs/1801.07698)

- Proposed in [ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/1801.07698) by Deng et al. in 2018.
- AAM has been predominantly utilized in for face recognition but has recently found applications in other areas such as speaker verification.
- One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power.
    - Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness.
    - SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way.
    - Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability.
- Additive Angular Margin (AAM) Loss (ArcFace) obtains highly discriminative features with a clear geometric interpretation (better than other loss functions) due to the exact correspondence to the geodesic distance on the hypersphere.
- ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.
- Specifically, the proposed ArcFace cos(θ+m)cos⁡(θ+m) directly maximises the decision boundary in angular (arc) space based on the L2 normalised weights and features.
    
    −1N∑i=1NlogeS∗(cos(θyi+m))es∗(cos(θyi+m))+∑nj=1,j≠yies∗cosθj−1N∑i=1Nlog⁡eS∗(cos⁡(θyi+m))es∗(cos⁡(θyi+m))+∑j=1,j≠yines∗cos⁡θj
    
    - where,
        - θjθj is the angle between the weight WjWj and the feature xixi.
        - ss: feature scale, the hypersphere radius.
        - mm: angular margin penalty.

### [Dice Loss](https://ieeexplore.ieee.org/document/9338261)

- Proposed in [Rethinking Dice Loss for Medical Image Segmentation](https://ieeexplore.ieee.org/document/9338261) by Zhao et al. in ICDM 2020.
- Dice loss originates from Sørensen–Dice coefficient, which is a statistic developed in 1940s to gauge the similarity between two samples.
- It was brought to computer vision community by [Milletari et al. in 2016](https://arxiv.org/abs/1606.04797) for 3D medical image segmentation.

D=2∑Nipigi∑Nip2i+∑Nig2iD=2∑iNpigi∑iNpi2+∑iNgi2

![](https://aman.ai/primers/ai/assets/loss/11.jpeg)

- The image above another view of the Dice coefficient mentioned above, from the perspective of set theory, in which the Dice coefficient (DSC) is a measure of overlap between two sets.
- For example, if two sets A and B overlap perfectly, DSC gets its maximum value to 1. Otherwise, DSC starts to decrease, getting to its minimum value to 0 if the two sets don ‘t overlap at all.
- Therefore, the range of DSC is between 0 and 1, the larger the better. Thus we can use 1-DSC as Dice loss to maximize the overlap between two sets.

### Summary

- For classification problems, cross-entropy loss tends to be almost universally adopted.
- With [focal cross-entropy](https://aman.ai/primers/ai/loss/#focal-loss), samples where the model is less confident is weighed more giving more focus on the “hard” samples to classify.
- The KL divergence is another information theoretic metric and can be less stable than cross-entropy in small batches due to the more fluctuating averages of the logs.
- The hinge loss is the original loss of the SVM algorithm. The squared hinge is simply the square of the hinge loss and the soft margin is simply a softer differentiable version of it.

## Regression Loss Functions

### Mean Absolute Error (MAE) / L1 Loss

- As the name suggests, MAE takes the average sum of the absolute differences between the actual and the predicted values.
- Regression problems may have variables that are not strictly Gaussian in nature due to the presence of outliers (values that are very different from the rest of the data).
- Mean Absolute Error would be an ideal option in such cases because it does not take into account the direction of the outliers (unrealistically high positive or negative values).
    
    MAE=1m∑i=1m∣∣h(x(i))−y(i)∣∣MAE=1m∑i=1m|h(x(i))−y(i)|
    
    - where,
        - MAE: mean absolute error
        - mm: number of samples
        - x(i)x(i): ithith sample from dataset
        - h(x(i))h(x(i)): prediction for i-th sample (thesis)
        - y(i)y(i): ground truth label for ii-th sample
    - A quick note here on L1 and L2, these are both used for regularization.
    - L1 Loss Function is used to minimize the error which is the sum of the all the absolute differences between the true value and the predicted value.
    - L1 is not affected by outliers and thus is preferrable if the dataset contains outliers.

### Mean Squared Error (MSE) / L2 Loss

- Formally, the Mean Squared Error (MSE) is given by,
    
    MSE=1m∑i=1m(y(i)−ŷ (i))2MSE=1m∑i=1m(y(i)−y^(i))2
    
    - where,
        - mm: number of samples
        - y(i)y(i): ground truth label for i-th sample
        - ŷ (i)y^(i): predicted label for i-th sample
    - Mean Squared Error is the average of the squared differences between the actual and the predicted values.
    - L2 Loss Function is used to minimize the error which is the sum of the all the squared differences between the true value and the predicted value. It is also the more preferred loss function compared to L1.
    - However, when outliers are present in the dataset, L2 will not perform as well because the squared differences will lead to a much larger error.

#### Why is MSE Not Used for Binary Classification?

- First, using MSE means that we assume that the underlying data has been generated from a normal distribution (a bell-shaped curve). In Bayesian terms, this means we assume a [Gaussian prior](https://en.wikipedia.org/wiki/Conjugate_prior). While in reality, a dataset that can be classified into two categories (i.e., binary) is usually not from a normal distribution but a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
- Secondly, the MSE function is non-convex for binary classification.
    
    - As a reminder, a function is non-convex if the function is not a convex function. Non-convex functions are those functions that have many minimum points, in the form of local and global minimum points. The following figure [(source)](https://www.quora.com/Why-is-nonconvex-optimization-so-difficult-compared-to-convex-optimization) shows the difference between convex and non-convex functions.
    
    ![](https://aman.ai/primers/ai/assets/loss/convex.webp)
    
    - Note that the loss functions that are applied in the context of machine learning models are convex functions, while those applied in the context of neural networks are non-convex functions.
    - Put simply, if a binary classification model is trained with the MSE cost function, it is not guaranteed to minimize the cost function. This is because MSE function expects real-valued inputs in range (−∞,∞)(−∞,∞), while binary classification models outputs discrete probabilities in range (0,1)(0,1) through the sigmoid/logistic function.
- [Why Using Mean Squared Error (MSE) Cost Function for Binary Classification is a Bad Idea?](https://towardsdatascience.com/why-using-mean-squared-error-mse-cost-function-for-binary-classification-is-a-bad-idea-933089e90df7) offers a great overview of this topic.

### Root Mean Squared Error (RMSE)

- Root Mean Squared Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data. It represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation, and they are called errors (or prediction errors) when calculated out-of-sample. RMSE is commonly used in climatology, forecasting, and regression analysis to verify experimental results.
- The RMSE of an estimator θ̂ θ^ with respect to an estimated parameter θθ is defined as the square root of the mean square error:
    
    RMSE(θ̂ )=MSE(θ̂ )‾‾‾‾‾‾‾‾√RMSE(θ^)=MSE(θ^)
    
    - where MSE is the Mean Squared Error, given by:
    
    MSE(θ̂ )=1n∑i=1n(θ̂ i−θi)2MSE(θ^)=1n∑i=1n(θ^i−θi)2
    
    - where, θ̂ iθ^i is the predicted value, θiθi is the observed value, and nn is the number of observations.

### Normalized Mean Absolute Error (NMAE)

- Normalized Mean Absolute Error (NMAE) is another measure of prediction accuracy in a regression analysis. It is a normalized version of the Mean Absolute Error (MAE), which scales the error to make it relative, rather than absolute. This normalization can be done in several ways, such as by dividing by the range of the data or the mean of the observed values, making the NMAE more interpretable, especially when comparing across different datasets.
- The NMAE is calculated as:
    
    NMAE=1norm⋅1n∑i=1n‖θ̂ i−θi‖NMAE=1norm⋅1n∑i=1n‖θ^i−θi‖
    
    where:
    
    - |   |   |   |
        |---|---|---|
        |$$|\hat{\theta}_i - \theta_i|$$ is the absolute error for each prediction,|
        
    - nn is the number of observations,
    - and normnorm is the normalization factor, which could be, for example, the range of θθ (i.e., max(θ)−min(θ)max(θ)−min(θ)) or the mean of θθ (i.e., mean(θ)mean(θ)).
    - The choice of normalization factor depends on the specific context and requirements of the analysis.

### Huber Loss (Smooth L1 Loss / Smooth Mean Absolute Error)

- Huber loss is a loss function used in regression, that is less sensitive to outliers in data than the squared error loss.
- Huber loss is the combination of MSE and MAE. It takes the good properties of both the loss functions by being less sensitive to outliers and differentiable at minima.
- When the error is smaller, the MSE part of the Huber is utilized and when the error is large, the MAE part of Huber loss is used.
- A new hyper-parameter δδ is introduced which tells the loss function where to switch from MSE to MAE. Additional δδ terms are introduced in the loss function to smoothen the transition from MSE to MAE.
- The Huber loss function describes the penalty incurred by an estimation procedure ff. Huber loss defines the loss function piecewise by:
    
    Lδ(a)={12a2δ⋅(|a|−12δ), for |a|≤δ otherwise Lδ(a)={12a2 for |a|≤δδ⋅(|a|−12δ), otherwise 
    
    - where:
        - Lδ(a)Lδ(a) is the Huber loss for a given error aa (which can be the difference between the predicted value and the actual value).
        - δδ is a threshold parameter that defines the limit where the loss changes from quadratic to linear.
- For errors smaller than δδ, the loss is quadratic, making it less sensitive to small errors (like mean squared error). For errors larger than δδ, the loss becomes linear, which reduces the impact of large errors (like mean absolute error). This makes Huber loss robust to outliers.
    
- The above equation is quadratic for small values of aa, and linear for large values, with equal values and slopes of the different sections at the two points where ‖a‖=δ‖a‖=δ. The variable a often refers to the residuals, that is to the difference between the observed and predicted values a=y−f(x)a=y−f(x), so the former can be expanded to:

Lδ(y,f(x))={12(y−f(x))2δ⋅(|y−f(x)|−12δ), for |y−f(x)|≤δ otherwise Lδ(y,f(x))={12(y−f(x))2 for |y−f(x)|≤δδ⋅(|y−f(x)|−12δ), otherwise 

- The below diagram ([source](https://aman.ai/primers/ai/loss/www.evergreeninnovations.co)) compares Huber loss with squared loss and absolute loss:

![](https://aman.ai/primers/ai/assets/loss/huber.png)

#### Asymmetric Huber Loss

- Asymmetric Huber loss is a variation of the traditional Huber loss, designed to handle asymmetry in errors differently. In standard Huber loss, the loss function behaves like mean squared error for small errors and like mean absolute error for large errors. The asymmetric version introduces a way to handle overestimations and underestimations differently by applying different thresholds or scaling factors to positive and negative errors. This is particularly useful in scenarios where the cost of overestimating is different from the cost of underestimating, such as in ETA predictions where overestimation and underestimation of time may have different implications.
- The equation for asymmetric Huber loss, which adjusts differently for overestimation and underestimation, can be written as follows:
    
    Lδ(a)=⎧⎩⎨⎪⎪12a2δ(|a|−12δ)αδ(|a|−12δ)for |a|≤δ,for a>δ,for a<−δ.Lδ(a)={12a2for |a|≤δ,δ(|a|−12δ)for a>δ,αδ(|a|−12δ)for a<−δ.
    
    - where:
        - aa is the prediction error (predicted value minus actual value).
        - δδ is a threshold defining where the loss function shifts from quadratic to linear.
        - αα is an asymmetry parameter that scales the loss differently for underestimations compared to overestimations.
- This modification allows the loss function to weigh overestimations and underestimations differently, which can be crucial in scenarios where these have different impacts.

### Summary

- When it comes to regression problems, the Huber and Smooth L1 losses are the best of both worlds between MSE and MAE being differentiable at 0 and limiting the weight of outliers for large values. The LogCosh has the same advantage, having a similar shape to the Huber one.
- The mean absolute percentage error and the mean squared logarithmic error greatly mitigate the effects of outliers.
- Poisson regression is widely used for count targets that can only be positive.

## Ranking Loss

- Ranking loss is known by several terminologies:
    - **Ranking Loss**: This term originates from the field of information retrieval, where the objective is to train models to rank items in a specific order.
    - **Margin Loss**: This term is derived from the use of a margin to compare the distances between sample representations.
    - **Contrastive Loss**: This term refers to the computation of losses by contrasting the representations of two or more data points. It is commonly used for Pairwise Ranking Loss, although its application in triplet setups is rare.
    - **Triplet Loss**: This term is used when triplet training pairs are utilized in the loss computation.
    - **Hinge Loss**: Also referred to as the max-margin objective, this loss is traditionally employed for training Support Vector Machines (SVMs) for classification. Its formulation is similar to that of Ranking Losses, as it optimizes up to a margin, which explains its occasional association with Ranking Loss.
- Ranking loss functions are widely applied in machine learning tasks that aim to learn rankings or measure similarity between instances. In this context, three widely used ranking loss functions are discussed: **margin ranking loss**, **soft pairwise loss**, and **pairwise logistic loss**.
    
- Ranking loss functions offer significant flexibility in terms of training data requirements. Specifically, they only necessitate a similarity score between data points, which can be binary (e.g., similar/dissimilar). For instance, in a face verification dataset, where it is known which face images belong to the same individual (similar) and which do not (dissimilar), a Ranking Loss function can be utilized to train a Convolutional Neural Network (CNN) to determine whether two face images represent the same person.
    
- To utilize a Ranking Loss function, the process begins by extracting features from two (or three) input data points to generate embedded representations for each. Subsequently, a metric function, such as Euclidean distance, is employed to evaluate the similarity between these representations. The feature extractors are then trained to yield similar representations for inputs deemed similar and dissimilar representations for inputs that are not alike. The focus is not on the absolute values of the representations but rather on the distances between them. This training methodology has been shown to produce highly effective representations for various tasks.

### [Bayesian Personalized Ranking (BPR) Loss](https://arxiv.org/abs/1205.2618)

- Bayesian Personalized Ranking (BPR) loss was proposed by Rendle et al. in the paper [“BPR: Bayesian Personalized Ranking from Implicit Feedback”](https://arxiv.org/abs/1205.2618).
- BPR loss is widely used in recommendation systems, particularly for implicit feedback scenarios. It is designed to optimize the ranking of items for a given user based on their implicit feedback (e.g., clicks, views, or purchases).
- BPR is a pairwise ranking loss function that assumes users prefer observed interactions over unobserved interactions. Its goal is to maximize the difference between the predicted scores of observed and unobserved items.

#### Definition

- The BPR loss is defined as:
    
    BPR=−∑(u,i,j)∈Dlog(σ(ŷ uij))+λ‖Θ‖2LBPR=−∑(u,i,j)∈Dlog⁡(σ(y^uij))+λ‖Θ‖2
    
    - where:
        - σσ is the sigmoid function.
        - ŷ uij=ŷ ui−ŷ ujy^uij=y^ui−y^uj is the difference between the predicted scores for user uu for the observed item ii and the unobserved item $$j$.
        - ΘΘ represents the model parameters.
        - λλ is the regularization parameter.
- Alternatively, BPR loss can be extended to incorporate multiple negative samples:
    
    Ls=−1NS∑j=1NSlog(σ(r̂ s,i−r̂ s,j))Ls=−1NS∑j=1NSlog⁡(σ(r^s,i−r^s,j))
    
    - where:
        - NSNS is the sample size.
        - r̂ s,kr^s,k is the score on item kk at the given point in the session.
        - ii is the desired item (e.g., the next item in the session).
        - jj refers to the negative samples.

#### Characteristics

- BPR assumes pairwise preferences between items, and it optimizes the model to ensure that observed items (positive samples) have higher scores than unobserved items (negative samples).
- The sigmoid function ensures that the loss is bounded and smooth, making optimization more stable.
- Regularization (λ‖Θ‖2λ‖Θ‖2) helps prevent overfitting by penalizing large parameter values.
- The extended formulation with multiple negative samples allows for a more robust comparison by averaging the contributions of multiple negatives.

#### Applications

- BPR loss is extensively used in matrix factorization techniques for recommendation systems. For example, it can be employed with algorithms like latent factor models to learn user and item embeddings that maximize personalized rankings.
- It is particularly suited for tasks where explicit ratings are unavailable, and only implicit feedback data is available.

#### Advantages

- BPR directly optimizes ranking, making it well-suited for recommendation tasks.
- It works well with implicit feedback datasets, which are more common in real-world applications than explicit feedback.

#### Limitations

- The pairwise approach increases the computational complexity, particularly for large datasets.
- The quality of the negative sampling strategy significantly affects the performance of models trained with BPR loss.

### [Multiple Negative Ranking Loss](https://arxiv.org/pdf/1705.00652.pdf)

- Proposed in [Efficient Natural Language Response Suggestion for Smart Reply](https://arxiv.org/pdf/1705.00652.pdf) by Henderson et al. from Google in 2017.
- Multiple Negative Ranking (MNR) Loss is a great loss function if you only have positive pairs, for example, only pairs of similar texts like pairs of paraphrases, pairs of duplicate questions, pairs of `(query, response)`, or pairs of `(source_language, target_language)`.
- This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. `(query, relevant_doc)`) as it will sample `n-1` negative docs in each batch randomly.The performance usually increases with increasing batch sizes.
- This is because with MNR loss, we drop all rows with neutral or contradiction labels — keeping only the positive entailment pairs ([source](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)).
- Models trained with MNR loss outperform those trained with softmax loss in high-performing sentence embeddings problems.
- Below is a code sample referenced from [sbert.net](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss):

![](https://aman.ai/images/copy.png)

`from sentence_transformers import SentenceTransformer, losses, InputExample from torch.utils.data import DataLoader  model = SentenceTransformer('distilbert-base-uncased') train_examples = [InputExample(texts=['Anchor 1', 'Positive 1']),     InputExample(texts=['Anchor 2', 'Positive 2'])] train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32) train_loss = losses.MultipleNegativesRankingLoss(model=model)`

- For more, please refer [Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) and [SBert MNR Loss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss).

### Soft Pairwise Loss

- Soft pairwise loss is another ranking loss function that aims to learn a similarity ranking between instances. It assigns higher scores to similar instances and lower scores to dissimilar instances while considering the pairwise relationships between instances. The soft pairwise loss can be defined as:

soft-pairwise=∑i=1N∑j=1Nlog(1+exp(−yi⋅(si−sj)))Lsoft-pairwise=∑i=1N∑j=1Nlog⁡(1+exp⁡(−yi⋅(si−sj)))

- where:
    - soft-pairwiseLsoft-pairwise is the soft pairwise loss.
    - sisi is the score for instance ii.
    - yiyi is the label indicating the similarity (1) or dissimilarity (-1) of instance ii.
- The soft pairwise loss encourages similar instances to have higher scores than dissimilar instances, and it penalizes large score differences between instances of different labels.

### Pairwise Logistic Loss

- Pairwise logistic loss is a ranking loss function commonly used in binary classification tasks where instances need to be ranked based on their likelihood of belonging to a certain class. It considers pairs of instances and encourages the model to assign higher probabilities to positive instances than negative instances. The pairwise logistic loss can be defined as:

pairwise-logistic=∑i=1N∑j=1Nlog(1+exp(−(yi−yj)⋅(si−sj)))Lpairwise-logistic=∑i=1N∑j=1Nlog⁡(1+exp⁡(−(yi−yj)⋅(si−sj)))

- where:
    - pairwise-logisticLpairwise-logistic is the pairwise logistic loss.
    - sisi is the score for instance ii.
    - yiyi is the label indicating the positive (1) or negative (0) class of instance ii.
- The pairwise logistic loss encourages the model to assign higher scores to positive instances compared to negative instances and penalizes large score differences between instances of different labels.
- These ranking loss functions are used in various applications, including information retrieval, recommender systems, and natural language processing tasks, to learn effective rankings or similarities between instances based on their characteristics and labels.
- Both pairwise logistic loss and pairwise ranking loss are commonly used in tasks where instances need to be ranked or compared pairwise. However, they differ in their underlying formulations and objectives.

### Pairwise Ranking Loss

- The terminology around “pairwise loss” and “ranking loss” can sometimes be a bit confusing, and there can be overlap between them. However, the key difference often lies in the structure of the learning task.
- **Pairwise loss functions**, such as the Triplet Loss, operate on pairs (or in this case, triplets) of samples at a time. The main objective of these loss functions is to learn a representation such that the distance between similar samples is less than the distance between dissimilar samples. The key focus is on the relative distances between individual samples or pairs of samples.
- On the other hand, **ranking loss functions** are typically concerned with the ordering of a set of items, often more than just pairs or triplets. They are used in tasks where items need to be sorted in some type of order, and the loss is typically related to the number of inversions in this order (i.e., pairs of items that are out of order).
- So, the Triplet Loss is generally considered a pairwise loss because it operates on triplets of samples, comparing the distances between an anchor sample and a positive sample (similar), and an anchor sample and a negative sample (dissimilar). It doesn’t directly consider the overall ranking among a larger set of samples, which is typically the concern of ranking loss functions.
- However, note that these categories are not mutually exclusive, and the exact classification can depend on the specific context and implementation. For instance, pairwise methods can be used as a part of a larger ranking problem, and some loss functions can be used both for pairwise comparisons and for ranking a set of items.
- Thus,ranking is the broader concept that encompasses the overall task of ordering or sorting items, while pairwise refers to a specific approach or technique used to compare and rank items based on pairwise comparisons. Pairwise methods are a common approach within the broader field of ranking.

![](https://aman.ai/primers/ai/assets/loss/pairwise_ranking_loss_faces.png)

- As illustrated by the image above [(source)](https://gombru.github.io/2019/04/03/ranking_loss/), pairwise ranking loss is a loss function used to train models that can rank instances based on their similarity or preference. It is often employed in tasks where the goal is to learn a ranking or preference order among instances.
- There are different formulations of pairwise ranking loss, and one common approach is to use the hinge loss formulation. The pairwise ranking loss can be defined as:

pairwise-ranking=∑i=1N∑j=1Nmax(0,m−yi⋅(si−sj))Lpairwise-ranking=∑i=1N∑j=1Nmax(0,m−yi⋅(si−sj))

- where:
    - pairwise-rankingLpairwise-ranking is the pairwise ranking loss.
    - sisi is the score for instance ii.
    - yiyi is the label indicating the preference or ranking position of instance ii.
    - mm is the margin that controls the desired separation between instances with different ranks.
- The pairwise ranking loss encourages the model to assign higher scores to instances that should be ranked higher and penalizes violations of the ranking order by imposing a margin constraint. It focuses on capturing the relative ranking positions of instances.

#### [Triplet Loss](https://arxiv.org/abs/1503.03832)

![](https://aman.ai/primers/ai/assets/loss/triplet_loss_faces.png)

- As illustrated by the image above [(source)](https://gombru.github.io/2019/04/03/ranking_loss/), triplet loss uses triplets of training samples instead of pairs.
- Proposed in [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832) by Schroff et al. in CVPR 2015.
- Triplet loss was orginally used to learn face recognition of the same person at different poses and angles.
- Triplet loss is a loss function for machine learning algorithms where a reference input (called anchor) is compared to a matching input (called positive) and a non-matching input (called negative).

L=max(d(A,P)−d(A,N)+α,0)L=max(d(A,P)−d(A,N)+α,0)

- where,
    - AA is an anchor input.
    - PP is a positive input of the same class as AA.
    - NN is a negative input of a different class from AA.
    - dd is a function to measure the distance between these three samples.
    - αα is a margin between positive and negative pairs.
- Consider the task of training a neural network to recognize faces (e.g. for admission to a high security zone).
- A classifier trained to classify an instance would have to be retrained every time a new person is added to the face database.
- This can be avoided by posing the problem as a similarity learning problem instead of a classification problem.
- Here the network is trained (using a contrastive loss) to output a distance which is small if the image belongs to a known person and large if the image belongs to an unknown person.
- However, if we want to output the closest images to a given image, we would like to learn a ranking and not just a similarity. A triplet loss is used in this case.

#### [Margin Ranking Loss](https://arxiv.org/pdf/1907.05336.pdf)

- Proposed in [Adaptive Margin Ranking Loss for Knowledge Graph Embeddings via a Correntropy Objective Function](https://arxiv.org/pdf/1907.05336.pdf) by Nayyeri et al. in 2019.
- As the name suggests, Margin Ranking Loss (MRL) is used for ranking problems.
- MRL calculates the loss provided there are inputs X1X1, X2X2, as well as a label tensor, yy containing 1 or -1.
- When the value of yy is 1 the first input will be assumed as the larger value and will be ranked higher than the second input.
- Similarly, if y=−1y=−1, the second input will be ranked as higher. It is mostly used in ranking problems.

=∑(h,r,t)∈S+∑(h′,r′,t′)∈S−[fr(h,t)+γ−fr(h′,t′)]+L=∑(h,r,t)∈S+∑(h′,r′,t′)∈S−[fr(h,t)+γ−fr(h′,t′)]+

- Let’s look at the code for this below from [analyticsindiamag](https://analyticsindiamag.com/all-pytorch-loss-function/#h-9-margin-ranking-loss-nn-marginrankingloss):

![](https://aman.ai/images/copy.png)

`first_input = torch.randn(3, requires_grad=True) Second_input = torch.randn(3, requires_grad=True) target = torch.randn(3).sign()  ranking_loss = nn.MarginRankingLoss() output = ranking_loss(first_input, Second_input, target) output.backward() print('input one: ', first_input) print('input two: ', Second_input) print('target: ', target) print('output: ', output)`

- Ranking losses tend to be extensions of the pointwise ones, penalizing the losses when two samples are misaligned compared to the ground truth. The margin ranking, the soft pairwise hinge and the pairwise logistic losses are extensions of the hinge losses. However, the downside of ranking loss functions is that they are painfully slow to compute as the time complexity is O(N2)O(N2) where NN is the number of samples within a batch. [(source)](https://www.linkedin.com/feed/update/urn:li:activity:7080658763547869184/?utm_source=share&utm_medium=member_desktop).
- Margin ranking loss is often used in tasks where instances need to be ranked based on their similarity or dissimilarity. It encourages the model to assign higher scores to similar instances and lower scores to dissimilar instances. The margin ranking loss can be defined as:

margin=∑i=1Nmax(0,m−s+i+s−i)Lmargin=∑i=1Nmax(0,m−si++si−)

- where:
    - marginLmargin is the margin ranking loss,
    - s+isi+ is the score for a positive (similar) instance,
    - s−isi− is the score for a negative (dissimilar) instance,
    - mm is the margin that defines the desired separation between positive and negative scores.
- The margin ranking loss encourages the positive instance scores to be higher than the negative instance scores by at least the margin $m$. This loss function is often used in tasks such as image retrieval and recommendation systems, where instances need to be ranked based on their similarity.

## [Contrastive Loss](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)

- A quick note here to differentiate ranking vs contrastive loss:
    - Contrastive learning is a method used in machine learning, specifically in self-supervised learning, to learn representations from unlabeled data. It operates on the principle of learning to tell apart different data samples (contrasting them) while pushing together, or attracting, transformations of the same data sample.
        1. **Multiple Negative Ranking Loss:** This can be considered a type of contrastive loss. In this method, a positive sample is contrasted with multiple negative samples to learn the representations.
        2. **Soft Pairwise Loss and Pairwise Logistic Loss:** While these are used for pairwise ranking, they are not typically categorized under contrastive learning. Pairwise ranking losses generally aim to optimize the rank order of items rather than learning representations that contrast different samples.
    - However, the boundary isn’t always clear-cut, and it could depend on the specific context and use case. Some methods and loss functions can be considered as contrastive under certain conditions or when used in certain ways, even if they might not be traditionally categorized as such.
    - For instance, the Triplet Loss, despite being a pairwise loss, is a contrastive loss function because it tries to learn representations that contrast an anchor sample with a positive sample (which should be similar) and a negative sample (which should be different). In this context, it’s doing a type of contrastive learning.
- Contrastive learning often involves pairs of samples: a positive pair that is similar, and a negative pair that is different. In this context, the contrastive loss can be defined as follows:
    
- Given:
    - x+ixi+ as the positive sample similar to xixi.
    - x−ixi− as the negative sample different from xixi.
    - D(.)D(.) as the distance function between two samples.
    - mm as the margin, a hyperparameter to tune.
- The Contrastive Loss (`L`) for a single observation can be defined as:

L(xi,x+i,x−i)=D(xi,x+i)+max(0,m−D(xi,x−i))L(xi,xi+,xi−)=D(xi,xi+)+max(0,m−D(xi,xi−))

- In this formula, the first term D(xi,x+i)D(xi,xi+) pushes the positive pair to be closer (minimizes the distance), and the second term max(0,m−D(xi,x−i))max(0,m−D(xi,xi−)) pushes the negative pair to be farther apart, but at least a distance of mm.
- For the overall loss, you typically average this over all observations in your dataset.
- Proposed in [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf) by Hadsell et al. (with Yann LeCun) in IEEE CVPR 2006.
- Contrastive loss is a distance-based loss as opposed to more conventional error-prediction losses. This loss is used to learn embeddings in which two “similar” points have a low Euclidean distance and two “dissimilar” points have a large Euclidean distance.
- Contrastive learning is a very simple way to learn aligned semantic representations of multimodal data. For example, triplet margin loss was used in [FaceNet](https://arxiv.org/pdf/1503.03832.pdf) and cosine embedding loss in [CLIP](https://arxiv.org/pdf/2103.00020.pdf). The hinge embedding loss is similar but we replace the cosine similarity with the Euclidean distance.
- Contrastive loss takes the output of the network for a positive example and calculates its distance to an example of the same class and contrasts that with the distance to negative examples.
- Two samples are either similar or dissimilar. This binary similarity can be determined using several approaches:
    - In this work, the NN closest neighbors of a sample in input space (e.g. pixel space) are considered similar; all others are considered dissimilar. (This approach yields a smooth latent space; e.g. the latent vectors for two similar views of an object are close)
    - To the group of similar samples to a sample, we can add transformed versions of the sample (e.g. using data augmentation). This allows the latent space to be invariant to one or more transformations.
    - We can use a manually obtained label determining if two samples are similar. (For example, we could use the class label. However, there can be cases where two samples from the same class are relatively dissimilar, or where two samples from different classes are relatively similar. Using classes alone does not encourage a smooth latent space.)
- Put simply, clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In other words, contrastive loss calculates the distance between positive example (example of the same class) and negative example (example not of the same class). So loss can be expected to be low if the positive examples are encoded (in this embedding space) to similar examples and the negative ones are further away encoded to different representations. This behavior is illustrated in the image below:

![](https://aman.ai/primers/ai/assets/loss/21.jpg)

- Formally, if we consider X⃗ X→ as the input data and GW(X⃗ )GW(X→) the output of a neural network, the interpoint distance is given by,

DW(X⃗ 1,X⃗ 2)=‖‖GW(X⃗ 1)−GW(X⃗ 2)‖‖2DW(X→1,X→2)=‖GW(X→1)−GW(X→2)‖2

- The contrastive loss is simply,
    
    (W)L(W,(Y,X⃗ 1,X⃗ 2)i)=∑i=1PL(W,(Y,X⃗ 1,X⃗ 2)i)=(1−Y)LS(DiW)+YLD(DiW)L(W)=∑i=1PL(W,(Y,X→1,X→2)i)L(W,(Y,X→1,X→2)i)=(1−Y)LS(DWi)+YLD(DWi)
    
    - where Y=0Y=0 when X1X1 and X2X2 are similar and Y=1Y=1 otherwise, and LSLS is a loss for similar points and LDLD is a loss for dissimilar points.
- More formally, the contrastive loss is given by,
    
    L(W,Y,X⃗ 1,X⃗ 2)=(1−Y)12(DW)2+(Y)12{max(0,m−DW)}2L(W,Y,X→1,X→2)=(1−Y)12(DW)2+(Y)12{max(0,m−DW)}2
    
    - where mm is a predefined margin.
- The gradient is given by the simple equations:
    

∂LS∂W=DW∂DW∂W∂LD∂W=−(m−DW)∂DW∂W∂LS∂W=DW∂DW∂W∂LD∂W=−(m−DW)∂DW∂W

- Contrastive Loss is often used in image retrieval tasks to learn discriminative features for images. During training, an image pair is fed into the model with their ground truth relationship: equals 1 if the two images are similar and 0 otherwise. The loss function for a single pair is:
    
    yd2+(1−y)max(margin−d,0)2yd2+(1−y)max(margin−d,0)2
    
    - where dd is the Euclidean distance between the two image features (suppose their features are f1f1 and f2f2): d=‖f1−f2‖2d=‖f1−f2‖2. The marginmargin term is used to “tighten” the constraint: if two images in a pair are dissimilar, then their distance should be at least marginmargin, or a loss will be incurred.
- Shown below are the results from the [paper](https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping) which are quite convincing:
    

![](https://aman.ai/primers/ai/assets/loss/conloss.png)

- Note that while this is one of the earliest of the contrastive losses, this is not the only one. For instance, the contrastive loss used in [SimCLR](https://aman.ai/primers/ai/loss/#a-simple-framework-for-contrastive-learning-of-visual-representations) is quite different.

### [InfoNCE Loss](https://arxiv.org/pdf/1807.03748v2.pdf)

- Proposed in [Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748v2.pdf) by van den Oord et al. in 2018.
- InfoNCE, where NCE stands for Noise-Contrastive Estimation, is a type of contrastive loss function used for self-supervised learning.
- The InfoNCE loss, inspired by NCE, uses categorical cross-entropy loss to identify the positive sample amongst a set of unrelated noise samples.

N=−𝔼X[logfk(xt+k,ct)∑xj∈Xfk(xj,ct)]LN=−EX[log⁡fk(xt+k,ct)∑xj∈Xfk(xj,ct)]

## Losses in Deep Learning-based Reinforcement Learning

- Deep Learning had a profound effect on Reinforcement Learning, allowing us to train models with high state and action dimensionalities.
- For Deep Q-learning, the loss can simply take the form of the MSE for the residuals of the Bellman equation. In the case of Policy gradient-based reinforcement learning algorithms, the loss is the cross-entropy of the action probabilities weighted by the Q-value.
- Deep learning has significantly impacted reinforcement learning, enabling the training of models with high-dimensional state and action spaces. In deep reinforcement learning, various loss functions are employed to train models effectively. Let’s explore two commonly used loss functions: Q-value loss and policy gradient loss.

### Q-Value Loss

- In deep Q-learning, the goal is to learn a Q-value function that estimates the expected cumulative rewards for different state-action pairs. The loss function used in Q-learning is the mean squared error (MSE) of the temporal difference (TD) error, which represents the difference between the current Q-value estimate and the target Q-value derived from the Bellman equation.
    
- The Bellman equation can be written as follows:
    

Q(s,a)=r+γmaxa′Q(s′,a′)Q(s,a)=r+γmaxa′Q(s′,a′)

- where:
    - Q(s,a)Q(s,a) is the Q-value for a given state-action pair.
    - rr is the immediate reward obtained after taking action aa in state ss.
    - γγ is the discount factor that determines the importance of future rewards.
    - s′s′ is the next state after taking action aa in state ss.
- The Q-value loss can be defined as:

Q=𝔼[(Q(s,a)−(r+γmaxa′Q(s′,a′)))2]LQ=E[(Q(s,a)−(r+γmaxa′Q(s′,a′)))2]

- The Q-value loss drives the network to minimize the TD error, thus improving the accuracy of the Q-value estimates and ultimately leading to better action selection.

### Policy Gradient Loss

- Policy gradient methods directly optimize a policy function that maps states to actions without explicitly estimating Q-values. The objective is to maximize the expected cumulative rewards by updating the policy parameters using gradient ascent.
    
- The loss function in policy gradient methods is typically the negative expected return (or a variant of it) weighted by a baseline, which helps reduce the variance of the gradient estimates. The return is defined as the sum of rewards obtained from a starting state following a policy. One common variant is the advantage function, which measures the advantage of taking a particular action over the average action value.
    
- The policy gradient loss can be written as:
    

PG=−𝔼[∑t=0T∇θlogπ(at|st)⋅A(st,at)]LPG=−E[∑t=0T∇θlog⁡π(at|st)⋅A(st,at)]

- where:
    - PGLPG is the policy gradient loss.
    - π(at‖st)π(at‖st) is the probability of taking action atat given state stst.
    - ∇θlogπ(at‖st)∇θlog⁡π(at‖st) is the gradient of the log probability of the action.
    - A(st,at)A(st,at) is the advantage function.
- The policy gradient loss encourages actions with higher advantages to have higher probabilities, pushing the policy towards more favorable actions.
    
- These loss functions, Q-value loss and policy gradient loss, are fundamental in training deep reinforcement learning models, enabling them to learn effective policies for complex decision-making tasks.
    
- Please note that there are various extensions and modifications to these loss functions based on specific algorithms and problem settings.

## Further Reading

- [PyTorch Losses Documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [SBERT: Losses](https://www.sbert.net/docs/package_reference/losses.html)
- [Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)
- [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)
- [Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
- [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)

## References

- [Machine Learning Mastery](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
- [ML CheatSheet](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)
- [Neptune.ai](https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning)
- [Section.ai](https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/)
- [After Academy](https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions)
- [Programmathically](https://programmathically.com/understanding-hinge-loss-and-the-svm-cost-function/)
- [PapersWithCode: Focal Loss](https://paperswithcode.com/method/focal-loss)
- [ArcFace Additive Angular Margin Loss for Deep Face Recognition](https://www.researchgate.net/publication/322674945_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition)
- [Medium AnalyticsVidhya](https://medium.com/analytics-vidhya/face-recognition-and-arcface-additive-angular-margin-loss-for-deep-face-recognition-44abc56916c)
- [PolyLoss](https://openreview.net/forum?id=gSdSJoenupI)
- [Generalized End-to-End Loss](https://arxiv.org/abs/1710.10467)
- [Wikipedia article on Huber loss](https://en.wikipedia.org/wiki/Huber_loss)
- [Wikipedia article on Triplet loss](https://en.wikipedia.org/wiki/Triplet_loss#:~:text=Triplet%20loss%20is%20a%20loss,matching%20input%20\(called%20negative\).)
- [Towards Data Science](https://towardsdatascience.com/contrastive-learning-in-3-minutes-89d9a7db5a28)
- [Papers With Code infoNCE](https://paperswithcode.com/method/infonce#:~:text=InfoNCE%2C%20where%20NCE%20stands%20for,used%20for%20self%2Dsupervised%20learning.)
- [Lilian Weng: Contrastive learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
- [Dice Loss by Shuchen Du](https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b)
- [Margin Ranking Loss](https://analyticsindiamag.com/all-pytorch-loss-function/#h-9-margin-ranking-loss-nn-marginrankingloss)
- [Margin Ranking Loss Official Paper](https://arxiv.org/pdf/1907.05336.pdf)
- [Wikipedia: Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
- [Kullback-Leibler Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
- [What is the difference Cross-entropy and KL divergence?](https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence)

## Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledLossFunctions,   title   = {Loss Functions},   author  = {Chadha, Aman and Jain, Vinija},   journal = {Distilled AI},   year    = {2020},   note    = {\url{https://vinija.ai}} }`

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)| 

[www.amanchadha.com](https://www.amanchadha.com/)




[Distilled AI](https://aman.ai/primers/ai/)[Back to aman.ai](https://aman.ai/)

# Primers • Training Loss > Validation Loss?

- [Overview](https://aman.ai/primers/ai/train-val-loss/#overview)
- [Theories](https://aman.ai/primers/ai/train-val-loss/#theories)
- [Remedies](https://aman.ai/primers/ai/train-val-loss/#remedies)
- [References](https://aman.ai/primers/ai/train-val-loss/#references)
- [Citation](https://aman.ai/primers/ai/train-val-loss/#citation)

## Overview

- Sometimes, you’ll notice the training loss being more than the validation loss. Ever wondered why?

![](https://aman.ai/primers/ai/assets/train-val-loss/1.jpg)

## Theories

- Here are some theories as to why that might be the case.
- **Regularization:** The most common reason is **regularization** (e.g., dropout), since it applies during training, but not during validation and testing. If we add the regularization loss to the validation loss, here’s how things look:

![](https://aman.ai/primers/ai/assets/train-val-loss/2.jpg)

- **Epoch delta between training and validation loss:** The training loss is measured **during** each epoch, while the validation loss is measured **after** each epoch, so on average the training loss is measured **half an epoch** earlier. If we **shift the training loss by half an epoch** to the left (where it should be), things look much different:

![](https://aman.ai/primers/ai/assets/train-val-loss/3.jpg)

- **Easier validation set:** Perhaps, the validation set is easier than the training set! This can happen by chance if the validation set is too small, or if it wasn’t properly sampled (e.g., too many easy classes).
    
- **Data leaks:** It might also be possible that the training set **leaked** into the validation set.
    
- **Data augmentation:** Using **data augmentation** during training might also cause this.
    - As an example, suppose that the augmentation algorithm involves randomly cropping the images, and 10% of the time, the resulting cropped image misses the main object in the image. Classifying the training images will be **more difficult** than classifying the validation images.
    - Another common case is when the augmentation algorithm involves many transformations, and the resulting images have more **diversity** (in lighting, rotation, scale, etc.) than the validation images. Again, the training images would be harder to classify than the validation images.
    - To validate this theory, using the same augmentation procedure (as used in training) for validation to compare the training and validation losses, would make sense.
        - However, make sure to **not do this** if you’re using early stopping or comparing different models, since in these cases, you are really only interested in the test performance, not the train performance.
- Note that even if the validation loss is close to the train loss, your model may still be **overfitting**.

## Remedies

- Account for the regularization loss when comparing training and validation losses.
- Shift the training loss by half an epoch.
- Make sure the validation set is large enough.
- Sample the validation set from the same distribution as train, without leaks.

## References

- [Aurélien Geron’s Twitter](https://twitter.com/aureliengeron/status/1110839223878184960) for the great inputs.

## Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledTrainingLossValidationLoss,   title   = {Training Loss > Validation Loss?},   author  = {Chadha, Aman},   journal = {Distilled AI},   year    = {2020},   note    = {\url{https://aman.ai}} }`

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)| 

[www.amanchadha.com](https://www.amanchadha.com/)

