

## 随机搜索和网格搜索

考虑如下函数 $f(x,y)=g(x)+h(y)$ 关于参数 $x,y$ 的最大化问题。$$\max_{x,y} f(x,y)$$
假设我们只能通过一个“预言机”（oracle）来访问函数 $f(x,y)$（即我们可以在某个点 $(x,y)$ 处计算 $f$ 的值，但我们不知道 $f$ 的函数形式）。  

问题在于……我们如何找到 $x$ 和 $y$ 的最优值？一个自然的想法是为 $x$ 和 $y$ 的值选择一个范围，并在这个范围内采样一个网格点。  

我们还可以在超参数空间中计算数值梯度。这种方法的一个挑战是，与模型训练的一次迭代不同，每次超参数的评估都非常昂贵和耗时，这使得尝试多种超参数组合变得不可行。

现在假设我们知道：$$f(x,y)=g(x)+h(y)≈g(x)$$
在这种情况下，网格搜索仍然是一个好的策略吗？函数 $f$ 主要依赖于 $x$。因此，网格搜索策略会浪费大量迭代来测试 $y$ 的不同值。  

如果我们对 $(x, y)$ 的评估次数有限，更好的策略是在一定范围内随机采样 $x$ 和 $y$，这样每个样本都会测试每个超参数的不同值。  

随机搜索如何改进网格搜索超参数的示例。“*网格搜索的这种失败在高维超参数优化中是常态* 而非例外。”（Bergstra & Bengio, 2011）
[![|500](https://aman.ai/primers/ai/assets/hyperparam-tuning-and-tensorboard/random-grid.png)](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
随机搜索的弱点和假设是什么？随机搜索假设超参数之间没有相关性。理想情况下，我们会从一个考虑到这种关系的联合分布中采样超参数。

此外，它不会利用之前迭代的结果来指导我们如何为未来的迭代选择参数值。这就是贝叶斯优化背后的动机。

### 贝叶斯优化

贝叶斯推断是一种统计推断形式，在进行估计时利用贝叶斯定理纳入先验知识。贝叶斯定理是一个简单却极其强大的公式，用于关联随机变量的条件分布和联合分布。设 $M$ 为表示我们模型质量的随机变量，$θ$ 表示我们超参数的随机变量。那么贝叶斯规则将分布 $P(θ∣M)$（后验）、$P(M∣θ)$（似然）、$P(θ)$（先验）和 $P(M)$（边际）关联起来，如下所示：$$P(\theta | M) = \frac{P(M|\theta)P(\theta)}{P(M)}$$
那么下一个问题是：我们如何利用贝叶斯规则来改进随机搜索？

通过对超参数施加先验分布，我们可以将先验知识融入优化器中。通过从后验分布而非均匀分布中采样，我们可以利用先前样本的结果来改进搜索过程。

让我们重新考虑寻找函数 $f(x,y)$ 最大值的优化问题。贝叶斯优化策略将会：

1. 对参数 $x$ 和 $y$ 初始化一个先验；
2. 采样一个点 $(x,y)$ 来对 $f$ 进行求值。
3. 利用 $f(x,y)$ 的结果来更新关于 $x,y$ 的后验分布。
4. 重复 2 和 3.

目标是猜测函数，即使我们无法知道其真实形式。通过在每次迭代中添加一个新的数据点，算法可以更准确地猜测函数，并更智能地选择下一个要评估的点以改进其猜测。高斯过程用于从其输入和输出的样本中推断函数。它还根据观察到的数据提供了对可能函数的分布。

让我们考虑一个例子：假设我们想要找到某个表达式未知的函数的最小值。该函数有一个输入和一个输出，我们已经采集了四个不同的样本（蓝色点）。

给定四个蓝色采样数据点的高斯过程分布：
![|500](https://aman.ai/primers/ai/assets/hyperparam-tuning-and-tensorboard/bayes.png)
高斯过程提供了一组连续函数的分布，这些函数能够拟合这些点，用绿色表示。阴影越深，真实函数落在该区域的可能性就越大。绿线是“真实”函数的平均估计值，每条绿色带表示高斯过程分布的半个标准差。

现在，基于这个有用的猜测，我们接下来应该评估哪个点呢？我们有两个可能的选择：

* 利用（Exploitation）：评估一个点，基于我们当前对可能函数的模型，这个点将产生较低的输出值。例如，在上述图表中，1.0 可能是一个选择。
* 探索（Exploration）：在我们最不确定的区域获取一个数据点。在上述图表中，我们可以关注 0.65 到 0.75 之间的区域，而不是 0.15 到 0.25 之间的区域，因为我们对后一区域的情况已经有了相当好的了解。这样，我们将能够降低未来猜测的方差。

平衡这两者就是探索 - 利用权衡。我们通过一个获取函数在这两种策略之间进行选择。

随着每次迭代，“算法平衡其对探索和利用的需求”（诺盖拉）。
![|600](https://aman.ai/primers/ai/assets/hyperparam-tuning-and-tensorboard/bayesopt.gif)
如果你有兴趣了解更多或尝试使用该优化器，[这里](https://github.com/bayesian-optimization/BayesianOptimization)有一个很好的 Python 代码库，可用于结合高斯过程进行贝叶斯优化。

## References

- [Structuring Machine Learning Projects on Coursera](https://www.coursera.org/learn/machine-learning-projects)
- [CS230 code examples](https://github.com/cs230-stanford/cs230-code-examples)
