
- [Case Study for Long Skip Connections: U-Nets](https://aman.ai/primers/ai/skip-connections/#case-study-for-long-skip-connections-u-nets)
- [Conclusion](https://aman.ai/primers/ai/skip-connections/#conclusion)
- [References](https://aman.ai/primers/ai/skip-connections/#references)
- [Citation](https://aman.ai/primers/ai/skip-connections/#citation)

## 引言

为了理解在众多研究中看到的构建深度神经网络（如跳跃连接）所涉及的众多设计选择，了解一点反向传播的机制至关重要。

如果你在2014年尝试训练神经网络，你肯定会遇到所谓的梯度消失问题。简单来说：你盯着屏幕查看网络的训练过程，却发现训练损失停止下降，但性能指标仍远未达到预期值。你整夜检查所有代码行，试图找出问题所在，却一无所获。相信我，这绝不是什么愉快的体验！想知道原因吗？因为促进学习的梯度无法一直传播到网络的初始层！从而导致“梯度消失”！


## 梯度消失问题

那么，让我们回顾一下没有动量的梯度下降的更新规则，其中 $L$ 是损失函数，$λ$ 是学习率。$$w_{\text{new}} = w_{\text{current}} - \alpha \cdot \frac{\partial L}{\partial w_{\text{current}}}
$$
基本上发生的事情是，你尝试通过一个小量 $\alpha \cdot \frac{\partial L}{\partial w_{\text{current}}}$ 来更新参数，该小量是基于梯度计算的。例如，假设对于一个早期层，平均梯度 $\frac{\partial L}{\partial w_{\text{current}}} = 1e^{-15}$。给定学习率 $\alpha = 1e^{-4}$，你实际上是通过引用的量（$\alpha \cdot \frac{\partial L}{\partial w_{\text{current}}}$）的乘积来改变层参数，这个乘积是 $1e^{-19}$，因此对权重几乎没有改变。因此，你实际上无法训练你的网络。这就是消失梯度问题。

## 前奏：反向传播

人们可以通过反向传播算法轻松理解梯度消失问题。我们将从链式法则的角度简要审视反向传播算法，从基础微积分入手，以深入了解残差连接。简而言之，反向传播是深度学习架构背后的“优化魔法”。鉴于深度网络由我们想要学习的有限数量的参数组成，我们的目标是利用损失函数 $L$ 相对于网络参数的梯度迭代优化这些参数。

正如你所见，每种架构都有某种输入（比如一张图像），并产生一个输出（预测）。损失函数在很大程度上取决于我们想要解决的任务。目前，你需要知道的是，损失函数是对两个张量之间距离的定量度量，这两个张量可以表示图像标签、图像中的边界框、另一种语言中的翻译文本等。通常你需要某种形式的监督来将网络的预测与期望的结果（真实值）进行比较。

因此，反向传播这一美妙的思想是通过更新网络的参数来逐渐最小化这个损失。但是，你如何在整个网络中传播所测量的标量损失呢？这正是反向传播发挥作用的地方。

## 反向传播与偏导数

简单来说，反向传播是通过计算偏导数来理解改变网络中的权重（参数）对损失函数的影响。对于后者，我们利用链式法则这一简单思想来最小化预期预测结果的偏差。换句话说，反向传播的核心在于计算损失函数的梯度，同时考虑神经网络内的不同权重，这实际上就是计算损失函数相对于模型参数的偏导数。通过多次重复这一过程，我们将持续最小化损失函数，直到其不再降低，或者满足其他预定义的终止条件。

### 链式法则

链式法则主要描述了一个函数相对于某个输入变量的梯度（变化率）。设该函数为神经网络的损失函数 $z$，而 $x$ 和 $y$ 为神经网络的参数，它们又是前一层参数 $t$ 的函数。此外，设 $f$、$g$、$h$ 为网络上执行输入向量非线性操作的不同层。因此，$$z=f(x,y) \quad x=g(t) \quad y=h(t)$$
利用多元微积分的链式法则来表示 $z$ 关于输入 $t$ 的梯度。$$\frac{\partial z}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
$$
有趣的是，这个著名的算法执行的是完全相同的操作，但方式相反：它从输出 $z$ 开始，计算每个参数的偏导数，并且仅基于后续层的梯度来表示这些偏导数。

值得注意的是，所有这些值通常都小于 1，与符号无关。为了将梯度传播到前面的层，反向传播使用偏导数的乘积（如链式法则所示）。对于我们在网络中向后回溯的每一层，由于上游梯度的绝对值小于 1，在每一层计算下游梯度时（因为下游梯度 = 局部梯度 × 上游梯度），网络的梯度会变得越来越小。


## 跳跃连接助力成功

跳跃连接在许多卷积架构中是标准配置。通过使用跳跃连接，我们为梯度（通过反向传播）提供了另一条路径。实验验证表明，这些额外的路径通常对训练期间模型的收敛有益。顾名思义，在深度架构中的跳跃连接会跳过神经网络中的某些层，并将某一层的输出作为后续多层（而不仅仅是下一层）的输入。

正如之前所解释的，使用链式法则时，我们必须一边向后传播一边不断乘上误差梯度。然而，在长长的连乘过程中，如果相乘的多个因子都小于 1，那么最终的梯度会变得非常小。因此，在深层架构中，当我们接近前面的层时，梯度会变得极小。在某些情况下，梯度甚至会变为零，这意味着我们完全不更新前面的层。

一般来说，通过不同非顺序层使用跳跃连接有两种基本方式：

* 如残差架构中的加法，
* 如密集连接架构中的拼接。

让我们首先对通过加法的跳跃连接进行一个概述，这种跳跃连接通常被称为残差跳跃连接。

## ResNet: 通过加法的跳跃连接

核心思想是通过向量加法，沿恒等函数进行反向传播。这样梯度只需乘以 1，其值就能在前面的层中得以保留。这就是残差网络（ResNets）背后的主要思想：如下面的图所示，它们将这些跳跃残差块堆叠在一起。我们使用恒等函数来保留梯度。

![|300](https://aman.ai/primers/ai/assets/skip-connections/skip-connection.png)

从数学上讲，我们可以表示残差块，并在给定损失函数的情况下计算其偏导数（梯度），如下所示：$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial H} \frac{\partial H}{\partial x} = \frac{\partial L}{\partial H} \left( \frac{\partial F}{\partial x} + 1 \right) = \frac{\partial L}{\partial H} \frac{\partial F}{\partial x} + \frac{\partial L}{\partial H}
$$
其中 $H$ 是上述网络片段的输出，由 $F(x) + x$ 给出。

除了梯度消失问题外，我们常用残差连接还有另一个原因。对于众多任务（如语义分割、光流估计等），初始层捕获的信息可被后续层用于学习。研究发现，在较浅的层中，学到的特征对应于从输入中提取的低级语义信息。如果没有跳跃连接，这些信息会变得过于抽象。

## DenseNet: 通过拼接的跳跃连接

正如所述，对于许多密集预测问题，输入和输出之间存在共享的低级信息，直接通过网络传递这些信息是可取的。另一种实现跳跃连接的方法是通过连接之前的特征图。最著名的深度学习架构是DenseNet。下面你可以看到一个通过连接五个卷积层实现特征重用的示例（图片来自 DenseNet）。

![|400](https://aman.ai/primers/ai/assets/skip-connections/densenet-architecture-skip-connections.png)

这种架构大量使用特征拼接，以确保网络各层之间的信息流动最大化。与 ResNets 不同，它是通过将所有层直接以拼接的方式相连来实现的。实际上，你基本上所做的就是拼接特征通道维度。这会带来以下结果：

* 网络最后层上有大量特征通道
* 更紧凑的模型以及
* 极端特征可重用性。
   
![|350](https://aman.ai/primers/ai/assets/skip-connections/resnet-concatenation.png)

## 深度学习中的短跳连接和长跳连接

更实际地说，在深度学习模型中引入加性跳跃连接时必须小心。在进行加法和拼接操作时，除了选定的通道维度外，其他维度的大小必须相同。这就是为什么你会看到加性跳跃连接被用于两种设置中的原因：

* 短跳连接
* 长跳连接

短跳连接与 *不改变输入维度* 的连续卷积层一起使用（参见 ResNet），而长跳过连接通常存在于编码器 - 解码器架构中。众所周知，全局信息（图像的形状和其他统计数据）解决“是什么”的问题，而局部信息解决“在哪里”的问题（图像块中的小细节）。

长跳连接通常存在于 *对称的架构* 中，在这种架构中，*编码器* 部分的 *空间维度逐渐减小*，而 *解码器* 部分的空间维度则 *逐渐增大*，如下图所示。在解码器部分，可以通过转置卷积（ConvT）层来增加特征图的维度。转置卷积操作形成的连接性与普通卷积相同，但方向相反。



## 长跳连接案例研究：U 型网络

从数学上讲，如果我们将卷积表示为矩阵乘法，那么转置卷积就是逆序的乘法（$B\times A$ 而不是 $A \times B$）。上述编码器-解码器架构结合长跳连接通常被称为 U-net。长跳连接用于预测与输入具有相同空间维度的任务，例如图像分割、光流估计、视频预测等。

长跳连接可以以下图所示的对称方式形成。

![|500](https://aman.ai/primers/ai/assets/skip-connections/long-skip-connection.jpeg)

通过在编码器-解码器架构中引入跳跃连接，可以在预测中恢复细粒度细节。尽管没有理论依据，但对称的长跳连接在密集预测任务（医学图像分割）中效果惊人地好。

## 结论

总结来说，跳跃连接背后的动机是它们能够在训练过程中实现不间断的梯度流动，这有助于解决梯度消失问题。拼接式跳跃连接提供了一种替代方法，以确保从较早层中重用相同维度的特征，并且在对称架构中被广泛使用。

另一方面，长跳连接用于将编码路径中的特征传递到解码路径，以恢复下采样过程中丢失的空间信息。短跳连接似乎能够稳定深度架构中的梯度更新。总体而言，跳跃连接因此实现了特征的重复利用，并稳定了训练和收敛过程。

在 Li et al. (2017) 发表的《可视化神经网络的损失景观》（*Visualizing the Loss Landscape of Neural Nets*）一文中，通过实验验证了引入跳跃连接（skip connections）时损失景观会发生显著变化，如下所示。

![|500](https://aman.ai/primers/ai/assets/skip-connections/skip-connections-visualizing-landscape.png)

