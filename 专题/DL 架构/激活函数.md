
- [Overview](https://aman.ai/primers/ai/activation-functions/#overview)
- [Types of Activation Functions](https://aman.ai/primers/ai/activation-functions/#types-of-activation-functions)
    - [Sigmoid](https://aman.ai/primers/ai/activation-functions/#sigmoid)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros)
    - [Softmax](https://aman.ai/primers/ai/activation-functions/#softmax)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-1)
    - [ReLU (Rectified Linear Unit)](https://aman.ai/primers/ai/activation-functions/#relu-rectified-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-2)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons)
    - [ELU (Exponential Linear Unit)](https://aman.ai/primers/ai/activation-functions/#elu-exponential-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-3)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-1)
    - [Leaky ReLU (Leaky Rectified Linear Unit)](https://aman.ai/primers/ai/activation-functions/#leaky-relu-leaky-rectified-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-4)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-2)
    - [SELU (Scaled Exponential Linear Unit)](https://aman.ai/primers/ai/activation-functions/#selu-scaled-exponential-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-5)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-3)
    - [GELU (Gaussian Error Linear Unit)](https://aman.ai/primers/ai/activation-functions/#gelu-gaussian-error-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-6)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-4)
    - [Citation](https://aman.ai/primers/ai/activation-functions/#citation)

## Overview

- The idea behind activation functions â€˜s use an example. Letâ€™s say you are working in a car parts manufacturing company as a quality control operator. Your job is to test each part coming out of the machine against the quality standards, and if the part is up to the mark, you send the part to the next stage. If not, either you discard it or label it for a further touch-up.
- Now, what would happen if the quality control operator was not there? Every part coming out of the machine is going to move to the next stage. What if that part is the car brakes? In this scenario, you would be getting a car with brakes that have never been tested for quality standards. No one would ever want a car like that! Would you?
- Activation functions work just like a quality control operator. They check the output of every neuron before pushing it forward to the next layer or neuron. If there is no activation function, then you simply get a linear model as a result, which we donâ€™t want, because if we want a linear model, then linear regression should be enough. Why bother with neural networks at all?
- As an example of the ReLU function as an activation function, which lets the output of a neuron pass as it is to the next layer if its value is bigger than zero, but if it is less or equal to zero, then it makes the output of the neuron zero and pushes that zero forward. This adds non-linearity to the linear output from the neuron, and this is what makes neural networks shine.

## Types of Activation Functions

- Letâ€™s go over the different activation functions that we can use in different scenarios.

![](https://aman.ai/primers/ai/assets/activation-functions/act.jpeg)

### Sigmoid

- The Sigmoid function is used for binary classification. It squashes a vector in the range (0, 1). It is applied independently to each element ofÂ ss. It is also called the logistic function (since it is used in logistic regression for binary classification).

![](https://aman.ai/primers/ai/assets/activation-functions/sigmoid.png)

f(si)=11+eâˆ’sif(si)=11+eâˆ’si

#### Pros

- Utilized in binary classification.
- Offers an output that can be interpreted as a probability value since it is non-negative and in the range (0, 1).

### Softmax

- The Softmax function is a generalization of the sigmoid function for multi-class classification. In other words, use sigmoid for binary classification and softmax for multiclass classification. Softmax is a function, not a loss. It squashes a vector in the range (0, 1) and all the resulting elements sum up to 1. It is applied to the output scoresÂ ss. As elements represent a class, they can be interpreted as class probabilities.
- The Softmax function cannot be applied independently to eachÂ sisi, since it depends on all elements ofÂ ss. For a given classÂ sisi, the Softmax function can be computed as:
    
    f(s)i=esiâˆ‘Cjesjf(s)i=esiâˆ‘jCesj
    
    - whereÂ sjsjÂ are the scores inferred by the net for each class inÂ CC. Note that the Softmax activation for a classÂ sisiÂ depends on all the scores inÂ ss.
- Activation functions are used to transform vectors before computing the loss in the training phase. In testing, when the loss is no longer applied, activation functions are also used to get the CNN outputs.

#### Pros

- Utilized in multi-class classification.

### ReLU (Rectified Linear Unit)

#### Pros

- Due to sparsity, there is less time and space complexity compared to the sigmoid.
- Avoids the vanishing gradient problem.

#### Cons

- Introduces the concept of the â€œdead ReLU problem,â€ which refers to network elements that are probably never updated with new values. This can also cause issues from time to time. In a way, this is also an advantage.
- Does not avoid the exploding gradient problem.

### ELU (Exponential Linear Unit)

#### Pros

- Avoids the dead ReLU problem.
- Enables the network to nudge weights and biases in the desired directions by producing negative outputs.
- When calculating the gradient, create activations rather than having them be zero.

#### Cons

- Increases computing time due to the use of an exponential operation.
- Does not avoid the exploding gradient problem.
- The alpha value is not learned by the neural network.

### Leaky ReLU (Leaky Rectified Linear Unit)

#### Pros

- Since we allow a tiny gradient when computing the derivative, we avoid the dead ReLU problem, just like ELU.
- Faster to compute than ELU, because no exponential operation is included.

#### Cons

- Does not avoid the exploding gradient problem.
- The alpha value is not learned by the neural network.
- When differentiated, it becomes a linear function, whereas ELU is partially linear and partially nonlinear.

### SELU (Scaled Exponential Linear Unit)

- If utilized, keep in mind that the LeCun Normal weight initialization approach is necessary for the SELU function and that Alpha Dropout is a unique variant that must be used if dropout is desired.

#### Pros

- The SELU activation is self-normalizing, hence the neural network converges more quickly than external normalization.
- Vanishing and exploding gradient problems are impossible.

#### Cons

- Works best for sequential network architectures. If your architecture has skipped connections, self-normalization will not be guaranteed, hence better performance is not guaranteed.

### GELU (Gaussian Error Linear Unit)

#### Pros

- Appears to be cutting-edge in NLP, particularly in Transformer models.
- Avoids vanishing gradient problem

#### Cons

- Fairly new in practical use, although introduced in 2016.

### Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledActFunctions,   title   = {Activation Functions},   author  = {Chadha, Aman and Jain, Vinija},   journal = {Distilled AI},   year    = {2020},   note    =


### SwiGLU

**ReLU**ï¼š

FFN æ¥æ”¶ä¸€ä¸ªå‘é‡ $x$ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªå­¦ä¹ å¾—åˆ°çš„çº¿æ€§å˜æ¢ï¼ˆç”±çŸ©é˜µ $ğ‘Š_1$ã€$ğ‘Š_2$ å’Œåç½®å‘é‡ $ğ‘_1$ã€$ğ‘_2$ è¡¨ç¤ºï¼‰è¿›è¡Œå¤„ç†ã€‚åœ¨ä¸¤ä¸ªçº¿æ€§å˜æ¢ä¹‹é—´åº”ç”¨äº†ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰æ¿€æ´»å‡½æ•°ã€‚$$\text{FFN}(x, W_{1}, W_{2}, b_{1}, b_{2}) = \max(0, xW_{1}+b_{1})W_{2}+b_{2}$$
æˆ–è€…å¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºä¸ºï¼š$$f ( x ) = x ^ { + } = \max ( 0 , x) = \frac { x + | x | } { 2 } = \begin {cases} x , & \text { if } x > 0 \\ 0 , & \text { otherwise } \end {cases}$$
![|350](https://miro.medium.com/v2/resize:fit:554/1*PUbMngenNS-UCLzn2GEt9A.png)

**GELU**ï¼šGELU çš„è®¾è®¡æ—¨åœ¨è§£å†³ ReLU çš„ä¸€äº›å±€é™æ€§ï¼Œå…¶æ–¹æ³•æ˜¯é€šè¿‡ä¾æ®æ ‡å‡†é«˜æ–¯åˆ†å¸ƒå¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå¤„ç†ã€‚

GELU çš„å…¬å¼ä¸ºï¼š$$
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$
- $\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚
- $\text{erf}$ æ˜¯è¯¯å·®å‡½æ•°ï¼Œç”¨äºè®¡ç®—é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡ã€‚

è¿™ä¸ªå…¬å¼æ„å‘³ç€è¾“å…¥ $x$ ä¼šæ ¹æ®å…¶åœ¨æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸‹çš„æ¦‚ç‡è¿›è¡ŒåŠ æƒã€‚

$$x \cdot \Phi(x) \times Ix + (1 - \Phi(x)) \times 0x = x \cdot \Phi(x)$$
è¿™ä¸ªè¡¨è¾¾å¼å±•ç¤ºäº† GELU æ¿€æ´»å‡½æ•°çš„è®¡ç®—é€»è¾‘ã€‚å®ƒå°†è¾“å…¥ $x$ è§†ä¸ºäºŒé¡¹åˆ†å¸ƒçš„åŠ æƒå’Œï¼š

- $\Phi(x)$ æ˜¯è¾“å…¥ $x$ åœ¨æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸‹çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å€¼ã€‚
- $I$ æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œå–å€¼ä¸º 1ï¼Œè¡¨ç¤ºè¯¥éƒ¨åˆ†è¢«æ¿€æ´»ã€‚
- $0x$ è¡¨ç¤ºæœªæ¿€æ´»éƒ¨åˆ†ï¼Œå¯¹åº”äºè¾“å…¥è¢«æŠ‘åˆ¶çš„æƒ…å†µã€‚

è¿™ä¸ªå…¬å¼çš„æ„ä¹‰åœ¨äºï¼šå½“ $\Phi(x)$ è¾ƒå¤§æ—¶ï¼Œ$x$ çš„å½±å“ä¹Ÿè¾ƒå¤§ï¼›å½“ $\Phi(x)$ è¾ƒå°æ—¶ï¼Œè¾“å…¥çš„å½±å“è¢«å‰Šå¼±ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒGELU èƒ½å¤Ÿæ ¹æ®è¾“å…¥å€¼çš„å¤§å°åŠ¨æ€è°ƒæ•´æ¿€æ´»å¼ºåº¦ï¼Œä»è€Œå®ç°æ›´æŸ”å’Œçš„éçº¿æ€§å¤„ç†ã€‚

**GELU çš„è¿‘ä¼¼**ï¼šGELU å¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼è¿‘ä¼¼ï¼š$$
0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right]\right)$$æˆ–è€…ä½¿ç”¨å¦ä¸€ç§è¿‘ä¼¼ï¼š$$x \cdot \sigma(1.702x)$$
```python
import micropip
await micropip.install("numpy")
await micropip.install("matplotlib")
await micropip.install("scipy")

import numpy as np  
import matplotlib.pyplot as plt  
from scipy.stats import norm  
  
def gelu(x):  
    return x * norm.cdf(x)  
  
def relu(x):  
    return np.maximum(0, x)  
  
x_values = np.linspace(-5, 5, 500)  
y_values = gelu(x_values)  
  
gelu_values = gelu(x_values)  
relu_values = relu(x_values)  
  
plt.plot(x_values, gelu_values, label='GELU')  
plt.plot(x_values, relu_values, label='ReLU')  
plt.title("GELU and ReLU Activation Functions")  
plt.xlabel("x")  
plt.ylabel("Activation")  
plt.grid()  
plt.legend()  
plt.show()
```

<img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XSgtsQWrWQ7XMHYSXI2UWQ.png" width="500">

**GELU çš„ä¼˜åŠ¿ï¼š**  

* éçº¿æ€§ï¼šä¸ SwiGLU ç±»ä¼¼ï¼ŒGELU ä¸ºæ¨¡å‹å¼•å…¥äº†éçº¿æ€§ç‰¹æ€§ï¼Œè¿™å¯¹äºå­¦ä¹ æ•°æ®ä¸­çš„å¤æ‚å…³ç³»è‡³å…³é‡è¦ã€‚  
* å¹³æ»‘æ€§ï¼šGELU æ˜¯ä¸€ä¸ªå¹³æ»‘å‡½æ•°ï¼Œé¿å…äº† ReLU åœ¨é›¶ç‚¹å¤„çš„çªå˜ã€‚è¿™ç§å¹³æ»‘æ€§æœ‰åŠ©äºè®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦çš„æ›´å¥½æµåŠ¨ã€‚  
* æ¦‚ç‡åŒ–è§£é‡Šï¼šGELU åŸºäºæ¦‚ç‡å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒï¼Œä¸ºæ¿€æ´»è¿‡ç¨‹å¢æ·»äº†ç‹¬ç‰¹çš„æ¦‚ç‡åŒ–è§†è§’ã€‚

### é—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰

å¼•å…¥äº†é—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œå±‚ï¼Œå®šä¹‰ä¸ºå¯¹è¾“å…¥è¿›è¡Œä¸¤ç§çº¿æ€§å˜æ¢åçš„é€å…ƒç´ ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸€ç§å˜æ¢ç»è¿‡ Sigmoid å‡½æ•°æ¿€æ´»ã€‚ä»–ä»¬è¿˜å»ºè®®çœç•¥æ¿€æ´»å‡½æ•°ï¼Œç§°ä¹‹ä¸ºâ€œåŒçº¿æ€§â€å±‚ã€‚å…¬å¼åˆ†åˆ«ä¸ºï¼š$$\begin{align}
\text{GLU}(x, W, V, b, c) &= \sigma(xW + b) \otimes (xV + c) \\
\text{Bilinear}(x, W, V, b, c) &= (xW + b) \otimes (xV + c)
\end{align}$$
è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æœ‰ä¸¤ä¸ªå¯è®­ç»ƒçŸ©é˜µ $W$ å’Œ $V$ï¼Œå…¶ä¸­ $V$ ç”¨äºè®¡ç®—é—¨æ§å•å…ƒã€‚è¯¥é—¨æ§åœ¨æ¿€æ´»åæä¾›äº†ä¸€ä¸ªé¢å¤–çš„å¯å­¦ä¹ è¿‡æ»¤å™¨ï¼Œå…¶å‚æ•°å–å†³äºè¾“å…¥æœ¬èº«ã€‚$âŠ—$ è¿ç®—ç¬¦è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ã€‚

åœ¨ä¸è€ƒè™‘åç½®çŸ©é˜µ $b$ å’Œ $c$ çš„æƒ…å†µä¸‹ï¼Œç”¨çŸ©é˜µè¿ç®—å¯è§†åŒ– GLUã€‚

![|500](https://miro.medium.com/v2/resize:fit:675/0*BqdKcITC0ydoNriQ.png)

```python
import numpy as np  
import matplotlib.pyplot as plt  
from scipy.stats import norm  
  
def gelu(x):  
     return x * norm.cdf(x)  
  
def relu(x):  
     return np.maximum(0, x)  
  
def swish(x, beta=1):  
     return x * (1 / (1 + np.exp(-beta * x)))  
  
x_values = np.linspace(-5, 5, 500)  
gelu_values = gelu(x_values)  
relu_values = relu(x_values)  
swish_values = swish(x_values)  
swish_values2 = swish(x_values, beta=0.5)  
  
plt.plot(x_values, gelu_values, label='GELU')  
plt.plot(x_values, relu_values, label='ReLU')  
plt.plot(x_values, swish_values, label='Swish')  
plt.plot(x_values, swish_values2, label='Swish (beta=0.5)')  
plt.title("GELU, ReLU, and Swish Activation Functions")  
plt.xlabel("x")  
plt.ylabel("Activation")  
plt.grid()  
plt.legend()  
plt.show()
```

<img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*WohYvQmfbeH-yFbprE6jCQ.png" width="500">
