
- [Overview](https://aman.ai/primers/ai/activation-functions/#overview)
- [Types of Activation Functions](https://aman.ai/primers/ai/activation-functions/#types-of-activation-functions)
    - [Sigmoid](https://aman.ai/primers/ai/activation-functions/#sigmoid)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros)
    - [Softmax](https://aman.ai/primers/ai/activation-functions/#softmax)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-1)
    - [ReLU (Rectified Linear Unit)](https://aman.ai/primers/ai/activation-functions/#relu-rectified-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-2)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons)
    - [ELU (Exponential Linear Unit)](https://aman.ai/primers/ai/activation-functions/#elu-exponential-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-3)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-1)
    - [Leaky ReLU (Leaky Rectified Linear Unit)](https://aman.ai/primers/ai/activation-functions/#leaky-relu-leaky-rectified-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-4)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-2)
    - [SELU (Scaled Exponential Linear Unit)](https://aman.ai/primers/ai/activation-functions/#selu-scaled-exponential-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-5)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-3)
    - [GELU (Gaussian Error Linear Unit)](https://aman.ai/primers/ai/activation-functions/#gelu-gaussian-error-linear-unit)
        - [Pros](https://aman.ai/primers/ai/activation-functions/#pros-6)
        - [Cons](https://aman.ai/primers/ai/activation-functions/#cons-4)
    - [Citation](https://aman.ai/primers/ai/activation-functions/#citation)

## Overview

- The idea behind activation functions ‘s use an example. Let’s say you are working in a car parts manufacturing company as a quality control operator. Your job is to test each part coming out of the machine against the quality standards, and if the part is up to the mark, you send the part to the next stage. If not, either you discard it or label it for a further touch-up.
- Now, what would happen if the quality control operator was not there? Every part coming out of the machine is going to move to the next stage. What if that part is the car brakes? In this scenario, you would be getting a car with brakes that have never been tested for quality standards. No one would ever want a car like that! Would you?
- Activation functions work just like a quality control operator. They check the output of every neuron before pushing it forward to the next layer or neuron. If there is no activation function, then you simply get a linear model as a result, which we don’t want, because if we want a linear model, then linear regression should be enough. Why bother with neural networks at all?
- As an example of the ReLU function as an activation function, which lets the output of a neuron pass as it is to the next layer if its value is bigger than zero, but if it is less or equal to zero, then it makes the output of the neuron zero and pushes that zero forward. This adds non-linearity to the linear output from the neuron, and this is what makes neural networks shine.

## Types of Activation Functions

- Let’s go over the different activation functions that we can use in different scenarios.

![](https://aman.ai/primers/ai/assets/activation-functions/act.jpeg)

### Sigmoid

- The Sigmoid function is used for binary classification. It squashes a vector in the range (0, 1). It is applied independently to each element of ss. It is also called the logistic function (since it is used in logistic regression for binary classification).

![](https://aman.ai/primers/ai/assets/activation-functions/sigmoid.png)

f(si)=11+e−sif(si)=11+e−si

#### Pros

- Utilized in binary classification.
- Offers an output that can be interpreted as a probability value since it is non-negative and in the range (0, 1).

### Softmax

- The Softmax function is a generalization of the sigmoid function for multi-class classification. In other words, use sigmoid for binary classification and softmax for multiclass classification. Softmax is a function, not a loss. It squashes a vector in the range (0, 1) and all the resulting elements sum up to 1. It is applied to the output scores ss. As elements represent a class, they can be interpreted as class probabilities.
- The Softmax function cannot be applied independently to each sisi, since it depends on all elements of ss. For a given class sisi, the Softmax function can be computed as:
    
    f(s)i=esi∑Cjesjf(s)i=esi∑jCesj
    
    - where sjsj are the scores inferred by the net for each class in CC. Note that the Softmax activation for a class sisi depends on all the scores in ss.
- Activation functions are used to transform vectors before computing the loss in the training phase. In testing, when the loss is no longer applied, activation functions are also used to get the CNN outputs.

#### Pros

- Utilized in multi-class classification.

### ReLU (Rectified Linear Unit)

#### Pros

- Due to sparsity, there is less time and space complexity compared to the sigmoid.
- Avoids the vanishing gradient problem.

#### Cons

- Introduces the concept of the “dead ReLU problem,” which refers to network elements that are probably never updated with new values. This can also cause issues from time to time. In a way, this is also an advantage.
- Does not avoid the exploding gradient problem.

### ELU (Exponential Linear Unit)

#### Pros

- Avoids the dead ReLU problem.
- Enables the network to nudge weights and biases in the desired directions by producing negative outputs.
- When calculating the gradient, create activations rather than having them be zero.

#### Cons

- Increases computing time due to the use of an exponential operation.
- Does not avoid the exploding gradient problem.
- The alpha value is not learned by the neural network.

### Leaky ReLU (Leaky Rectified Linear Unit)

#### Pros

- Since we allow a tiny gradient when computing the derivative, we avoid the dead ReLU problem, just like ELU.
- Faster to compute than ELU, because no exponential operation is included.

#### Cons

- Does not avoid the exploding gradient problem.
- The alpha value is not learned by the neural network.
- When differentiated, it becomes a linear function, whereas ELU is partially linear and partially nonlinear.

### SELU (Scaled Exponential Linear Unit)

- If utilized, keep in mind that the LeCun Normal weight initialization approach is necessary for the SELU function and that Alpha Dropout is a unique variant that must be used if dropout is desired.

#### Pros

- The SELU activation is self-normalizing, hence the neural network converges more quickly than external normalization.
- Vanishing and exploding gradient problems are impossible.

#### Cons

- Works best for sequential network architectures. If your architecture has skipped connections, self-normalization will not be guaranteed, hence better performance is not guaranteed.

### GELU (Gaussian Error Linear Unit)

#### Pros

- Appears to be cutting-edge in NLP, particularly in Transformer models.
- Avoids vanishing gradient problem

#### Cons

- Fairly new in practical use, although introduced in 2016.

### Citation

If you found our work useful, please cite it as:

![](https://aman.ai/images/copy.png)

`@article{Chadha2020DistilledActFunctions,   title   = {Activation Functions},   author  = {Chadha, Aman and Jain, Vinija},   journal = {Distilled AI},   year    = {2020},   note    =


### SwiGLU

**ReLU**：

FFN 接收一个向量 $x$，并通过两个学习得到的线性变换（由矩阵 $𝑊_1$、$𝑊_2$ 和偏置向量 $𝑏_1$、$𝑏_2$ 表示）进行处理。在两个线性变换之间应用了修正线性单元（ReLU）激活函数。$$\text{FFN}(x, W_{1}, W_{2}, b_{1}, b_{2}) = \max(0, xW_{1}+b_{1})W_{2}+b_{2}$$
或者可以用数学表达式表示为：$$f ( x ) = x ^ { + } = \max ( 0 , x) = \frac { x + | x | } { 2 } = \begin {cases} x , & \text { if } x > 0 \\ 0 , & \text { otherwise } \end {cases}$$
![|350](https://miro.medium.com/v2/resize:fit:554/1*PUbMngenNS-UCLzn2GEt9A.png)

**GELU**：GELU 的设计旨在解决 ReLU 的一些局限性，其方法是通过依据标准高斯分布对输入进行加权处理。

GELU 的公式为：$$
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$
- $\Phi(x)$ 是标准正态分布的累积分布函数。
- $\text{erf}$ 是误差函数，用于计算高斯分布的概率。

这个公式意味着输入 $x$ 会根据其在标准正态分布下的概率进行加权。

$$x \cdot \Phi(x) \times Ix + (1 - \Phi(x)) \times 0x = x \cdot \Phi(x)$$
这个表达式展示了 GELU 激活函数的计算逻辑。它将输入 $x$ 视为二项分布的加权和：

- $\Phi(x)$ 是输入 $x$ 在标准高斯分布下的累积分布函数值。
- $I$ 是指示函数，取值为 1，表示该部分被激活。
- $0x$ 表示未激活部分，对应于输入被抑制的情况。

这个公式的意义在于：当 $\Phi(x)$ 较大时，$x$ 的影响也较大；当 $\Phi(x)$ 较小时，输入的影响被削弱。通过这种设计，GELU 能够根据输入值的大小动态调整激活强度，从而实现更柔和的非线性处理。

**GELU 的近似**：GELU 可以通过以下公式近似：$$
0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right]\right)$$或者使用另一种近似：$$x \cdot \sigma(1.702x)$$
```python
import micropip
await micropip.install("numpy")
await micropip.install("matplotlib")
await micropip.install("scipy")

import numpy as np  
import matplotlib.pyplot as plt  
from scipy.stats import norm  
  
def gelu(x):  
    return x * norm.cdf(x)  
  
def relu(x):  
    return np.maximum(0, x)  
  
x_values = np.linspace(-5, 5, 500)  
y_values = gelu(x_values)  
  
gelu_values = gelu(x_values)  
relu_values = relu(x_values)  
  
plt.plot(x_values, gelu_values, label='GELU')  
plt.plot(x_values, relu_values, label='ReLU')  
plt.title("GELU and ReLU Activation Functions")  
plt.xlabel("x")  
plt.ylabel("Activation")  
plt.grid()  
plt.legend()  
plt.show()
```

<img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XSgtsQWrWQ7XMHYSXI2UWQ.png" width="500">

**GELU 的优势：**  

* 非线性：与 SwiGLU 类似，GELU 为模型引入了非线性特性，这对于学习数据中的复杂关系至关重要。  
* 平滑性：GELU 是一个平滑函数，避免了 ReLU 在零点处的突变。这种平滑性有助于训练过程中梯度的更好流动。  
* 概率化解释：GELU 基于概率对输入进行加权，为激活过程增添了独特的概率化视角。

### 门控线性单元（GLU）

引入了门控线性单元（GLU），这是一种神经网络层，定义为对输入进行两种线性变换后的逐元素乘积，其中一种变换经过 Sigmoid 函数激活。他们还建议省略激活函数，称之为“双线性”层。公式分别为：$$\begin{align}
\text{GLU}(x, W, V, b, c) &= \sigma(xW + b) \otimes (xV + c) \\
\text{Bilinear}(x, W, V, b, c) &= (xW + b) \otimes (xV + c)
\end{align}$$
这里我们看到有两个可训练矩阵 $W$ 和 $V$，其中 $V$ 用于计算门控单元。该门控在激活后提供了一个额外的可学习过滤器，其参数取决于输入本身。$⊗$ 运算符表示逐元素相乘。

在不考虑偏置矩阵 $b$ 和 $c$ 的情况下，用矩阵运算可视化 GLU。

![|500](https://miro.medium.com/v2/resize:fit:675/0*BqdKcITC0ydoNriQ.png)

```python
import numpy as np  
import matplotlib.pyplot as plt  
from scipy.stats import norm  
  
def gelu(x):  
     return x * norm.cdf(x)  
  
def relu(x):  
     return np.maximum(0, x)  
  
def swish(x, beta=1):  
     return x * (1 / (1 + np.exp(-beta * x)))  
  
x_values = np.linspace(-5, 5, 500)  
gelu_values = gelu(x_values)  
relu_values = relu(x_values)  
swish_values = swish(x_values)  
swish_values2 = swish(x_values, beta=0.5)  
  
plt.plot(x_values, gelu_values, label='GELU')  
plt.plot(x_values, relu_values, label='ReLU')  
plt.plot(x_values, swish_values, label='Swish')  
plt.plot(x_values, swish_values2, label='Swish (beta=0.5)')  
plt.title("GELU, ReLU, and Swish Activation Functions")  
plt.xlabel("x")  
plt.ylabel("Activation")  
plt.grid()  
plt.legend()  
plt.show()
```

<img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*WohYvQmfbeH-yFbprE6jCQ.png" width="500">
