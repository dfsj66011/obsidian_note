[Distilled AI](https://aman.ai/primers/ai/)[Back to aman.ai](https://aman.ai/)

# Primers • Gradient Descent and Backprop

- [Introduction](https://aman.ai/primers/ai/gradient-descent-and-backprop/#introduction)
- [Univariate Regression](https://aman.ai/primers/ai/gradient-descent-and-backprop/#univariate-regression)
    - [Forward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#forward-propagation)
    - [Backward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#backward-propagation)
- [Multivariate Regression](https://aman.ai/primers/ai/gradient-descent-and-backprop/#multivariate-regression)
    - [Forward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#forward-propagation-1)
    - [Backward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#backward-propagation-1)
- [Two Layer Linear Network](https://aman.ai/primers/ai/gradient-descent-and-backprop/#two-layer-linear-network)
    - [Forward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#forward-propagation-2)
    - [Backward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#backward-propagation-2)
- [Two Layer Nonlinear Network](https://aman.ai/primers/ai/gradient-descent-and-backprop/#two-layer-nonlinear-network)
    - [Forward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#forward-propagation-3)
    - [Backward Propagation](https://aman.ai/primers/ai/gradient-descent-and-backprop/#backward-propagation-3)
- [Further Reading](https://aman.ai/primers/ai/gradient-descent-and-backprop/#further-reading)

## Introduction

- The goal of this topic is to foster an understanding of gradient descent and backpropagation. We will go through four different neural network examples and explicitly compute the backpropagation equations.
- When datasets are large and high-dimensional, it is computationally very expensive (sometimes impossible!) to find an analytical solution for the optimal parameters of your network. Instead, we use optimization methods. A vanilla optimization approach would be to sample different combinations of parameters and choose the one with the lowest loss value.
- **Is this a good idea?**
- **Would it be possible to extract another piece of information to direct our search towards the optimal parameters?**
    
- The trajectory through a loss landscape for a linear regression model trained with gradient descent is shown below. The red dot indicates the value of the loss function corresponding to the initial parameter values.

[![](https://aman.ai/primers/ai/assets/gradient-descent-and-backprop/optimization.png)](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/optimization/index.html)

- This is exactly what _gradient descent_ does! Apart from the loss value, gradient descent computes the local gradient of the loss when evaluating potential parameters. This information is used to decide which direction the search should go to find better parameter values. This extra piece of information (the local gradient) can be computed relatively easily using _backpropagation_. This recursive algorithm breaks up complex derivatives into smaller parts through the _chain rule_.
- To help understand gradient descent, let’s **[visualize](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/optimization/index.html)** the setup.

## Univariate Regression

- Let’s consider a linear regression. You have a data set (x,y)(x,y) with mm examples. In other words, x=(x1,…,xm)x=(x1,…,xm) and y=(y1,…,ym)y=(y1,…,ym) are row vectors of mm scalar examples . The goal is to find the scalar parameters ww and bb such that the line y=wx+by=wx+b optimally fits the data. This can be achieved using gradient descent.

![](https://aman.ai/primers/ai/assets/gradient-descent-and-backprop/model.png)

### Forward Propagation

- The first step of gradient descent is to compute the loss. To do this, define your model’s output and loss function. In this regression setting, we use the mean squared error loss.

ŷ =wx+b=1m||ŷ −y||2y^=wx+bL=1m||y^−y||2

### Backward Propagation

- The next step is to compute the local gradient of the loss with respect to the parameters (i.e. ww and bb). This means you need to calculate derivatives. Note that values stored during the forward propagation are used in the gradient equations.

∂∂w∂∂b=2m(ŷ −y)x⊺=2m(ŷ −y)1⃗ ∂L∂w=2m(y^−y)x⊺∂L∂b=2m(y^−y)1→

## Multivariate Regression

- Now, consider the case where XX is a matrix of shape (n,m)(n,m) and yy is still a row vector of shape (1,m)(1,m). Instead of a single scalar value, the weights will be a vector (one element per feature) of shape (1,n)(1,n). The bias parameter is still a scalar.

![](https://aman.ai/primers/ai/assets/gradient-descent-and-backprop/model1.png)

### Forward Propagation

ŷ =wX+b=1m||ŷ −y||2y^=wX+bL=1m||y^−y||2

### Backward Propagation

∂∂w∂∂b=2m(ŷ −y)X⊺=2m(ŷ −y)1⃗ ∂L∂w=2m(y^−y)X⊺∂L∂b=2m(y^−y)1→

## Two Layer Linear Network

- Consider stacking two linear layers together. You can introduce a hidden variable ZZ of shape (k,m)(k,m), which is the output of the first linear layer. The first layer is parameterized by a weight matrix W1W1 of shape (k,n)(k,n) and bias b1b1 of shape (k,1)(k,1) broadcasted to (k,m)(k,m). The second layer will be the same as in the multivariate regression case, but its input will be ZZ instead of XX.

![](https://aman.ai/primers/ai/assets/gradient-descent-and-backprop/model2.png)

### Forward Propagation

Zŷ =W1X+b1=w2Z+b2=1m||ŷ −y||2Z=W1X+b1y^=w2Z+b2L=1m||y^−y||2

### Backward Propagation

∂∂W1∂∂b1∂∂w2∂∂b2=w⊺22m(ŷ −y)X⊺=w⊺22m(ŷ −y)1⃗ =2m(ŷ −y)Z⊺=2m(ŷ −y)1⃗ ∂L∂W1=w2⊺2m(y^−y)X⊺∂L∂b1=w2⊺2m(y^−y)1→∂L∂w2=2m(y^−y)Z⊺∂L∂b2=2m(y^−y)1→

## Two Layer Nonlinear Network

- In this example, before sending ZZ as the input to the second layer, you will pass it through the sigmoid function. The output is denoted AA and is the input of the second layer.

![](https://aman.ai/primers/ai/assets/gradient-descent-and-backprop/model3.png)

### Forward Propagation

ZAŷ =W1X+b1=σ(Z)=w2A+b2=1m||ŷ −y||2Z=W1X+b1A=σ(Z)y^=w2A+b2L=1m||y^−y||2

### Backward Propagation

∂∂W1∂∂b1∂∂w2∂∂b2=((w⊺22m(ŷ −y))⊙A⊙(1−A))X⊺=((w⊺22m(ŷ −y))⊙A⊙(1−A))1⃗ =2m(ŷ −y)A⊺=2m(ŷ −y)1⃗ ∂L∂W1=((w2⊺2m(y^−y))⊙A⊙(1−A))X⊺∂L∂b1=((w2⊺2m(y^−y))⊙A⊙(1−A))1→∂L∂w2=2m(y^−y)A⊺∂L∂b2=2m(y^−y)1→

## Further Reading

- [Matrix derivatives “cheat” sheet](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf)
- [CS229 Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf)
- [CS229 Backpropagation](http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf)

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)| 

[www.amanchadha.com](https://www.amanchadha.com/)