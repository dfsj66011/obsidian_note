

## Soft prompts

è®­ç»ƒå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éå¸¸è€—æ—¶ä¸”è®¡ç®—å¯†é›†ã€‚éšç€å®ƒä»¬çš„è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œäººä»¬å¯¹æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ *prompting*ï¼‰è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æç¤ºå­¦ä¹ é€šè¿‡åœ¨è¾“å…¥ä¸­åŠ å…¥æè¿°ä»»åŠ¡ç”šè‡³å±•ç¤ºä»»åŠ¡ç¤ºä¾‹çš„æ–‡æœ¬æç¤ºï¼Œæ¥æ¿€æ´»å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ä»¥å®Œæˆç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚å€ŸåŠ©æç¤ºå­¦ä¹ ï¼Œä½ å¯ä»¥é¿å…ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å®Œå…¨è®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨åŒä¸€ä¸ªå†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ç®€å•å¾—å¤šï¼Œå› ä¸ºä½ å¯ä»¥ç”¨åŒä¸€ä¸ªæ¨¡å‹å®Œæˆå¤šä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œè€Œä¸”è®­ç»ƒå’Œå­˜å‚¨å°‘é‡æç¤ºå‚æ•°æ¯”è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¦é«˜æ•ˆå¾—å¤šã€‚

æç¤ºæ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼š

- *ç¡¬æç¤º* æ˜¯æ‰‹åŠ¨ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºï¼ŒåŒ…å«ç¦»æ•£çš„è¾“å…¥æ ‡è®°ï¼›ç¼ºç‚¹æ˜¯éœ€è¦èŠ±è´¹å¤§é‡ç²¾åŠ›æ¥åˆ›å»ºä¸€ä¸ªå¥½çš„æç¤ºã€‚
- *è½¯æç¤º* æ˜¯ä¸è¾“å…¥åµŒå…¥è¿æ¥çš„ã€å¯å­¦ä¹ çš„å¼ é‡ï¼Œå¯ä»¥é’ˆå¯¹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼›ç¼ºç‚¹æ˜¯å®ƒä»¬ä¸å¯è¢«äººè¯»å–ï¼Œå› ä¸ºä½ æ— æ³•å°†è¿™äº›â€œè™šæ‹Ÿæ ‡è®°â€ä¸çœŸå®å•è¯çš„åµŒå…¥ç›¸åŒ¹é…ã€‚

æœ¬æ¦‚å¿µæŒ‡å—ç®€è¦ä»‹ç»äº† ğŸ¤— PEFT ä¸­åŒ…å«çš„è½¯æç¤ºæ–¹æ³•ï¼šprompt tuning, prefix tuning, P-tuning å’Œ multitask prompt tuningã€‚


### 1ã€Prompt tuning

![|500](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prompt-tuning.png)
åªéœ€è®­ç»ƒå¹¶å­˜å‚¨ä¸€ç»„æ˜¾è‘—æ›´å°çš„ç‰¹å®šä»»åŠ¡æç¤ºå‚æ•°ï¼ˆ[å›¾åƒæ¥æº](https://huggingface.co/papers/2104.08691)ï¼‰ã€‚

æç¤ºè°ƒä¼˜ï¼ˆPrompt tuningï¼‰æ˜¯ä¸º T5 æ¨¡å‹ä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å¼€å‘çš„ï¼Œæ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡éƒ½è¢«è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåºåˆ—åˆ†ç±»é€šå¸¸ä¸ºä¸€æ®µæ–‡æœ¬åˆ†é…ä¸€ä¸ªå•ä¸€çš„ç±»åˆ«æ ‡ç­¾ã€‚é€šè¿‡å°†å…¶è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ„æˆç±»åˆ«æ ‡ç­¾çš„æ ‡è®°ä¼šè¢«ç”Ÿæˆã€‚æç¤ºä»¥ä¸€ç³»åˆ— tokens çš„å½¢å¼æ·»åŠ åˆ°è¾“å…¥ä¸­ã€‚é€šå¸¸ï¼Œæ¨¡å‹å‚æ•°æ˜¯å›ºå®šçš„ï¼Œè¿™æ„å‘³ç€æç¤º tokens ä¹Ÿç”±æ¨¡å‹å‚æ•°å›ºå®šã€‚

æç¤ºè°ƒä¼˜èƒŒåçš„å…³é”®æ€æƒ³æ˜¯ï¼Œæç¤ºæ ‡è®°æ‹¥æœ‰ç‹¬ç«‹æ›´æ–°çš„è‡ªèº«å‚æ•°ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å†»ç»“ï¼Œä»…æ›´æ–°æç¤ºæ ‡è®°åµŒå…¥çš„æ¢¯åº¦ã€‚å…¶ç»“æœä¸è®­ç»ƒæ•´ä¸ªæ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ï¼Œå¹¶ä¸”æç¤ºè°ƒä¼˜çš„æ€§èƒ½ä¼šéšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œæå‡ã€‚

### 2ã€Prefix tuning

![|500](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prefix-tuning.png)

ä¼˜åŒ–æ¯ä¸ªä»»åŠ¡çš„å‰ç¼€å‚æ•°ï¼ˆ[å›¾ç‰‡æ¥æº](https://hf.co/papers/2101.00190)ï¼‰

å‰ç¼€è°ƒæ•´ï¼ˆPrefix tuningï¼‰æ˜¯ä¸º GPT æ¨¡å‹ä¸Šçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ä»»åŠ¡è€Œè®¾è®¡çš„ã€‚å®ƒä¸æç¤ºè°ƒæ•´ï¼ˆprompt tuningï¼‰éå¸¸ç›¸ä¼¼ï¼›å‰ç¼€è°ƒæ•´åŒæ ·æ˜¯åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ç³»åˆ—ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ï¼Œè¿™äº›å‘é‡å¯ä»¥è®­ç»ƒå’Œæ›´æ–°ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å…¶ä½™å‚æ•°å†»ç»“ä¸å˜ã€‚

ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå‰ç¼€å‚æ•°è¢«æ’å…¥åˆ°æ¨¡å‹çš„æ‰€æœ‰å±‚ä¸­ï¼Œè€Œæç¤ºè°ƒä¼˜ä»…å°†æç¤ºå‚æ•°æ·»åŠ åˆ°æ¨¡å‹è¾“å…¥åµŒå…¥ä¸­ã€‚å‰ç¼€å‚æ•°è¿˜é€šè¿‡å•ç‹¬çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨è½¯æç¤ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ä¸ç¨³å®šå¹¶å½±å“æ€§èƒ½ã€‚åœ¨æ›´æ–°è½¯æç¤ºåï¼Œå‰é¦ˆç½‘ç»œä¼šè¢«ä¸¢å¼ƒã€‚

å› æ­¤ï¼Œä½œè€…å‘ç°ï¼Œå°½ç®¡å‰ç¼€è°ƒæ•´çš„å‚æ•°æ¯”å®Œå…¨å¾®è°ƒæ¨¡å‹å°‘ 1000 å€ï¼Œä½†å…¶è¡¨ç°ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨æ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚

### 3ã€P-tuning

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/p-tuning.png)

æç¤ºè¯å¯ä»¥æ’å…¥åˆ°è¾“å…¥åºåˆ—çš„ä»»æ„ä½ç½®ï¼Œå¹¶ç”±æç¤ºç¼–ç å™¨è¿›è¡Œä¼˜åŒ–ï¼ˆ[å›¾ç‰‡æ¥æº](https://hf.co/papers/2103.10385)ï¼‰ã€‚

P-tuning ä¸“ä¸ºè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡å’Œæ‰€æœ‰è¯­è¨€æ¨¡å‹è€Œè®¾è®¡ã€‚å®ƒæ˜¯ä¸€ç§è½¯æç¤ºæ–¹æ³•çš„å¦ä¸€ç§å˜ä½“ï¼›P-tuning åŒæ ·æ·»åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„åµŒå…¥å¼ é‡ï¼Œè¯¥å¼ é‡å¯ä»¥è¢«ä¼˜åŒ–ä»¥æ‰¾åˆ°æ›´å¥½çš„æç¤ºï¼Œå¹¶ä¸”å®ƒä½¿ç”¨ä¸€ä¸ªæç¤ºç¼–ç å™¨ï¼ˆåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæˆ– LSTMï¼‰æ¥ä¼˜åŒ–æç¤ºå‚æ•°ã€‚ä¸è¿‡ï¼Œä¸ prefix tuning ä¸åŒçš„æ˜¯ï¼š

- æç¤ºè¯æ ‡è®°å¯ä»¥æ’å…¥åˆ°è¾“å…¥åºåˆ—çš„ä»»ä½•ä½ç½®ï¼Œå¹¶ä¸é™äºä»…åœ¨å¼€å¤´æ’å…¥ã€‚
- æç¤ºè¯æ ‡è®°ä»…æ·»åŠ åˆ°è¾“å…¥ä¸­ï¼Œè€Œä¸æ˜¯æ·»åŠ åˆ°æ¨¡å‹çš„æ¯ä¸€å±‚ã€‚
- å¼•å…¥ *auchor* æ ‡è®°å¯ä»¥æé«˜æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬æŒ‡ç¤ºäº†è¾“å…¥åºåˆ—ä¸­æŸä¸ªç»„ä»¶çš„ç‰¹å¾ã€‚

ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒP-tuning æ¯”æ‰‹åŠ¨è®¾è®¡æç¤ºè¯­æ›´é«˜æ•ˆï¼Œå¹¶ä¸”å®ƒä½¿ç±»ä¼¼ GPT çš„æ¨¡å‹èƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šä¸ç±»ä¼¼ BERT çš„æ¨¡å‹ç›¸ç«äº‰ã€‚


## [](https://huggingface.co/docs/peft/conceptual_guides/prompting#multitask-prompt-tuning)Multitask prompt tuning

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png)

[Multitask prompt tuning enables parameter-efficient transfer learning](https://hf.co/papers/2303.02861).

[Multitask prompt tuning (MPT)](https://hf.co/papers/2303.02861)Â learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt-decomposition.png)

[Prompt decomposition](https://hf.co/papers/2103.10385).

## [](https://huggingface.co/docs/peft/conceptual_guides/prompting#context-aware-prompt-tuning-cpt)Context-Aware Prompt Tuning (CPT)

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/cpt.png)

CPT optimizing only specific token embeddings while keeping the rest of the model frozenÂ [(image source)](https://huggingface.co/papers/2410.17222).

[Context-Aware Prompt Tuning (CPT)](https://huggingface.co/papers/2410.17222)Â is designed to enhance few-shot classification by refining only context embeddings. This approach combines ideas from In-Context Learning (ICL), Prompt Tuning (PT), and adversarial optimization, focusing on making model adaptation both parameter-efficient and effective. In CPT, only specific context token embeddings are optimized, while the rest of the model remains frozen. To prevent overfitting and maintain stability, CPT uses controlled perturbations to limit the allowed changes to context embeddings within a defined range. Additionally, to address the phenomenon of recency biasâ€”where examples near the end of the context tend to be prioritized over earlier onesâ€”CPT applies a decay loss factor.

Take a look atÂ [Example](https://github.com/huggingface/peft/blob/main/examples/cpt_finetuning/README.md)Â for a step-by-step guide on how to train a model with CPT.

[<>UpdateÂ on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/prompting.md)

Soft prompts

[â†Adapters](https://huggingface.co/docs/peft/conceptual_guides/adapter)[IA3â†’](https://huggingface.co/docs/peft/conceptual_guides/ia3)

[Soft prompts](https://huggingface.co/docs/peft/conceptual_guides/prompting#soft-prompts)[Prompt tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#prompt-tuning)[Prefix tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#prefix-tuning)[P-tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#p-tuning)[Multitask prompt tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#multitask-prompt-tuning)[Context-AwareÂ PromptÂ Tuning (CPT)](https://huggingface.co/docs/peft/conceptual_guides/prompting#context-aware-prompt-tuning-cpt)