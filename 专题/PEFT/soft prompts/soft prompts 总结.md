

## Soft prompts

è®­ç»ƒå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éå¸¸è€—æ—¶ä¸”è®¡ç®—å¯†é›†ã€‚éšç€å®ƒä»¬çš„è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œäººä»¬å¯¹æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ *prompting*ï¼‰è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æç¤ºå­¦ä¹ é€šè¿‡åœ¨è¾“å…¥ä¸­åŠ å…¥æè¿°ä»»åŠ¡ç”šè‡³å±•ç¤ºä»»åŠ¡ç¤ºä¾‹çš„æ–‡æœ¬æç¤ºï¼Œæ¥æ¿€æ´»å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ä»¥å®Œæˆç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚å€ŸåŠ©æç¤ºå­¦ä¹ ï¼Œä½ å¯ä»¥é¿å…ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å®Œå…¨è®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨åŒä¸€ä¸ªå†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ç®€å•å¾—å¤šï¼Œå› ä¸ºä½ å¯ä»¥ç”¨åŒä¸€ä¸ªæ¨¡å‹å®Œæˆå¤šä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œè€Œä¸”è®­ç»ƒå’Œå­˜å‚¨å°‘é‡æç¤ºå‚æ•°æ¯”è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¦é«˜æ•ˆå¾—å¤šã€‚

æç¤ºæ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼š

- *ç¡¬æç¤º* æ˜¯æ‰‹åŠ¨ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºï¼ŒåŒ…å«ç¦»æ•£çš„è¾“å…¥æ ‡è®°ï¼›ç¼ºç‚¹æ˜¯éœ€è¦èŠ±è´¹å¤§é‡ç²¾åŠ›æ¥åˆ›å»ºä¸€ä¸ªå¥½çš„æç¤ºã€‚
- *è½¯æç¤º* æ˜¯ä¸è¾“å…¥åµŒå…¥è¿æ¥çš„ã€å¯å­¦ä¹ çš„å¼ é‡ï¼Œå¯ä»¥é’ˆå¯¹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼›ç¼ºç‚¹æ˜¯å®ƒä»¬ä¸å¯è¢«äººè¯»å–ï¼Œå› ä¸ºä½ æ— æ³•å°†è¿™äº›â€œè™šæ‹Ÿæ ‡è®°â€ä¸çœŸå®å•è¯çš„åµŒå…¥ç›¸åŒ¹é…ã€‚

æœ¬æ¦‚å¿µæŒ‡å—ç®€è¦ä»‹ç»äº† ğŸ¤— PEFT ä¸­åŒ…å«çš„è½¯æç¤ºæ–¹æ³•ï¼šprompt tuning, prefix tuning, P-tuning å’Œ multitask prompt tuningã€‚


### 1ã€Prompt tuning

![|500](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prompt-tuning.png)
åªéœ€è®­ç»ƒå¹¶å­˜å‚¨ä¸€ç»„æ˜¾è‘—æ›´å°çš„ç‰¹å®šä»»åŠ¡æç¤ºå‚æ•°ï¼ˆ[å›¾åƒæ¥æº](https://huggingface.co/papers/2104.08691)ï¼‰ã€‚

æç¤ºè°ƒä¼˜ï¼ˆPrompt tuningï¼‰æ˜¯ä¸º T5 æ¨¡å‹ä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å¼€å‘çš„ï¼Œæ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡éƒ½è¢«è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåºåˆ—åˆ†ç±»é€šå¸¸ä¸ºä¸€æ®µæ–‡æœ¬åˆ†é…ä¸€ä¸ªå•ä¸€çš„ç±»åˆ«æ ‡ç­¾ã€‚é€šè¿‡å°†å…¶è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ„æˆç±»åˆ«æ ‡ç­¾çš„æ ‡è®°ä¼šè¢«ç”Ÿæˆã€‚æç¤ºä»¥ä¸€ç³»åˆ— tokens çš„å½¢å¼æ·»åŠ åˆ°è¾“å…¥ä¸­ã€‚é€šå¸¸ï¼Œæ¨¡å‹å‚æ•°æ˜¯å›ºå®šçš„ï¼Œè¿™æ„å‘³ç€æç¤º tokens ä¹Ÿç”±æ¨¡å‹å‚æ•°å›ºå®šã€‚

æç¤ºè°ƒä¼˜èƒŒåçš„å…³é”®æ€æƒ³æ˜¯ï¼Œæç¤ºæ ‡è®°æ‹¥æœ‰ç‹¬ç«‹æ›´æ–°çš„è‡ªèº«å‚æ•°ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å†»ç»“ï¼Œä»…æ›´æ–°æç¤ºæ ‡è®°åµŒå…¥çš„æ¢¯åº¦ã€‚å…¶ç»“æœä¸è®­ç»ƒæ•´ä¸ªæ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ï¼Œå¹¶ä¸”æç¤ºè°ƒä¼˜çš„æ€§èƒ½ä¼šéšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œæå‡ã€‚

### 2ã€Prefix tuning

![|400](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prefix-tuning.png)

ä¼˜åŒ–æ¯ä¸ªä»»åŠ¡çš„å‰ç¼€å‚æ•°ï¼ˆ[å›¾ç‰‡æ¥æº](https://hf.co/papers/2101.00190)ï¼‰

å‰ç¼€è°ƒæ•´ï¼ˆPrefix tuningï¼‰æ˜¯ä¸º GPT æ¨¡å‹ä¸Šçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ä»»åŠ¡è€Œè®¾è®¡çš„ã€‚å®ƒä¸æç¤ºè°ƒæ•´ï¼ˆprompt tuningï¼‰éå¸¸ç›¸ä¼¼ï¼›å‰ç¼€è°ƒæ•´åŒæ ·æ˜¯åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ç³»åˆ—ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ï¼Œè¿™äº›å‘é‡å¯ä»¥è®­ç»ƒå’Œæ›´æ–°ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å…¶ä½™å‚æ•°å†»ç»“ä¸å˜ã€‚

ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå‰ç¼€å‚æ•°è¢«æ’å…¥åˆ°æ¨¡å‹çš„æ‰€æœ‰å±‚ä¸­ï¼Œè€Œæç¤ºè°ƒä¼˜ä»…å°†æç¤ºå‚æ•°æ·»åŠ åˆ°æ¨¡å‹è¾“å…¥åµŒå…¥ä¸­ã€‚å‰ç¼€å‚æ•°è¿˜é€šè¿‡å•ç‹¬çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨è½¯æç¤ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ä¸ç¨³å®šå¹¶å½±å“æ€§èƒ½ã€‚åœ¨æ›´æ–°è½¯æç¤ºåï¼Œå‰é¦ˆç½‘ç»œä¼šè¢«ä¸¢å¼ƒã€‚

å› æ­¤ï¼Œä½œè€…å‘ç°ï¼Œå°½ç®¡å‰ç¼€è°ƒæ•´çš„å‚æ•°æ¯”å®Œå…¨å¾®è°ƒæ¨¡å‹å°‘ 1000 å€ï¼Œä½†å…¶è¡¨ç°ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨æ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚

### 3ã€P-tuning

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/p-tuning.png)

æç¤ºè¯å¯ä»¥æ’å…¥åˆ°è¾“å…¥åºåˆ—çš„ä»»æ„ä½ç½®ï¼Œå¹¶ç”±æç¤ºç¼–ç å™¨è¿›è¡Œä¼˜åŒ–ï¼ˆ[å›¾ç‰‡æ¥æº](https://hf.co/papers/2103.10385)ï¼‰ã€‚

P-tuning ä¸“ä¸ºè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡å’Œæ‰€æœ‰è¯­è¨€æ¨¡å‹è€Œè®¾è®¡ã€‚å®ƒæ˜¯ä¸€ç§è½¯æç¤ºæ–¹æ³•çš„å¦ä¸€ç§å˜ä½“ï¼›P-tuning åŒæ ·æ·»åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„åµŒå…¥å¼ é‡ï¼Œè¯¥å¼ é‡å¯ä»¥è¢«ä¼˜åŒ–ä»¥æ‰¾åˆ°æ›´å¥½çš„æç¤ºï¼Œå¹¶ä¸”å®ƒä½¿ç”¨ä¸€ä¸ªæç¤ºç¼–ç å™¨ï¼ˆåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæˆ– LSTMï¼‰æ¥ä¼˜åŒ–æç¤ºå‚æ•°ã€‚ä¸è¿‡ï¼Œä¸ prefix tuning ä¸åŒçš„æ˜¯ï¼š

- æç¤ºè¯æ ‡è®°å¯ä»¥æ’å…¥åˆ°è¾“å…¥åºåˆ—çš„ä»»ä½•ä½ç½®ï¼Œå¹¶ä¸é™äºä»…åœ¨å¼€å¤´æ’å…¥ã€‚
- æç¤ºè¯æ ‡è®°ä»…æ·»åŠ åˆ°è¾“å…¥ä¸­ï¼Œè€Œä¸æ˜¯æ·»åŠ åˆ°æ¨¡å‹çš„æ¯ä¸€å±‚ã€‚
- å¼•å…¥ *auchor* æ ‡è®°å¯ä»¥æé«˜æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬æŒ‡ç¤ºäº†è¾“å…¥åºåˆ—ä¸­æŸä¸ªç»„ä»¶çš„ç‰¹å¾ã€‚

ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒP-tuning æ¯”æ‰‹åŠ¨è®¾è®¡æç¤ºè¯­æ›´é«˜æ•ˆï¼Œå¹¶ä¸”å®ƒä½¿ç±»ä¼¼ GPT çš„æ¨¡å‹èƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šä¸ç±»ä¼¼ BERT çš„æ¨¡å‹ç›¸ç«äº‰ã€‚


### 4ã€å¤šä»»åŠ¡ prompt tuning

![|400](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png)

å¤šä»»åŠ¡æç¤ºè°ƒä¼˜ï¼ˆMPTï¼‰ä»æ•°æ®ä¸­å­¦ä¹ å•ä¸ªæç¤ºï¼Œè¯¥æç¤ºé€‚ç”¨äºå¤šç§ä»»åŠ¡ç±»å‹ï¼Œå¹¶ä¸”å¯ä»¥å…±äº«ä»¥é€‚åº”ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ã€‚å…¶ä»–ç°æœ‰æ–¹æ³•åˆ™ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ä¸€ä¸ªå•ç‹¬çš„è½¯æç¤ºï¼Œè¿™äº›æç¤ºéœ€è¦è¢«æ£€ç´¢æˆ–èšåˆä»¥é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚MPT åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š

1. æºè®­ç»ƒâ€”â€”å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œå…¶è½¯æç¤ºè¢«åˆ†è§£ä¸ºç‰¹å®šäºä»»åŠ¡çš„å‘é‡ã€‚è¿™äº›ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ç›¸ä¹˜å½¢æˆå¦ä¸€ä¸ªçŸ©é˜µ $W$ï¼Œå¹¶åœ¨ $W$ å’Œä¸€ä¸ªå…±äº«æç¤ºçŸ©é˜µ $P$ ä¹‹é—´ä½¿ç”¨å“ˆè¾¾ç›ç§¯ï¼Œä»¥ç”Ÿæˆç‰¹å®šäºä»»åŠ¡çš„æç¤ºçŸ©é˜µã€‚ç‰¹å®šäºä»»åŠ¡çš„æç¤ºè¢«æç‚¼æˆä¸€ä¸ªåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å…±äº«çš„å•ä¸€æç¤ºçŸ©é˜µã€‚è¿™ä¸ªæç¤ºé€šè¿‡å¤šä»»åŠ¡è®­ç»ƒè¿›è¡Œè®­ç»ƒã€‚
2. ç›®æ ‡é€‚é…â€”â€”ä¸ºäº†é’ˆå¯¹ç›®æ ‡ä»»åŠ¡é€‚é…å•ä¸ªæç¤ºï¼Œåˆå§‹åŒ–ä¸€ä¸ªç›®æ ‡æç¤ºï¼Œå¹¶å°†å…¶è¡¨ç¤ºä¸ºå…±äº«æç¤ºçŸ©é˜µä¸ä»»åŠ¡ç‰¹å®šä½ç§©æç¤ºçŸ©é˜µçš„å“ˆè¾¾ç›ç§¯ã€‚

![|300](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt-decomposition.png)


## [](https://huggingface.co/docs/peft/conceptual_guides/prompting#context-aware-prompt-tuning-cpt)Context-Aware Prompt Tuning (CPT)

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/cpt.png)

CPT optimizing only specific token embeddings while keeping the rest of the model frozenÂ [(image source)](https://huggingface.co/papers/2410.17222).

[Context-Aware Prompt Tuning (CPT)](https://huggingface.co/papers/2410.17222)Â is designed to enhance few-shot classification by refining only context embeddings. This approach combines ideas from In-Context Learning (ICL), Prompt Tuning (PT), and adversarial optimization, focusing on making model adaptation both parameter-efficient and effective. In CPT, only specific context token embeddings are optimized, while the rest of the model remains frozen. To prevent overfitting and maintain stability, CPT uses controlled perturbations to limit the allowed changes to context embeddings within a defined range. Additionally, to address the phenomenon of recency biasâ€”where examples near the end of the context tend to be prioritized over earlier onesâ€”CPT applies a decay loss factor.

Take a look atÂ [Example](https://github.com/huggingface/peft/blob/main/examples/cpt_finetuning/README.md)Â for a step-by-step guide on how to train a model with CPT.

[<>UpdateÂ on GitHub](https://github.com/huggingface/peft/blob/main/docs/source/conceptual_guides/prompting.md)

Soft prompts

[â†Adapters](https://huggingface.co/docs/peft/conceptual_guides/adapter)[IA3â†’](https://huggingface.co/docs/peft/conceptual_guides/ia3)

[Soft prompts](https://huggingface.co/docs/peft/conceptual_guides/prompting#soft-prompts)[Prompt tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#prompt-tuning)[Prefix tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#prefix-tuning)[P-tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#p-tuning)[Multitask prompt tuning](https://huggingface.co/docs/peft/conceptual_guides/prompting#multitask-prompt-tuning)[Context-AwareÂ PromptÂ Tuning (CPT)](https://huggingface.co/docs/peft/conceptual_guides/prompting#context-aware-prompt-tuning-cpt)



----
å‚è€ƒï¼š[Is there a difference between p-tuning and prefix tuning ?](https://www.reddit.com/r/MachineLearning/comments/14pkibg/d_is_there_a_difference_between_ptuning_and/)

- Prompt Tuning:  å¯¹ä¸€ç»„è¿æ¥çš„è¾“å…¥åµŒå…¥å‘é‡è¿›è¡Œè°ƒæ•´ã€‚æœ€åˆåº”ç”¨äº T5-LM æ¨¡å‹ã€‚
- Prefix Tuning: å¯¹æ¯ä¸€å±‚çš„ KV ç¼“å­˜ï¼ˆè½¯å‰ç¼€ï¼‰è¿›è¡Œè°ƒæ•´ï¼Œå¯ä»¥é€šä¿—åœ°æè¿°ä¸ºâ€œåœ¨æ¯ä¸€å±‚è¿›è¡Œæç¤ºè°ƒä¼˜â€ï¼Œå°½ç®¡è¿™ç§è¯´æ³•ç•¥æœ‰ä¸å‡†ç¡®ã€‚å®é™…ä¸Šï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªè¾…åŠ©å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ¥ç”Ÿæˆè½¯å‰ç¼€ä»¥è¾…åŠ©è®­ç»ƒã€‚æœ€åˆåº”ç”¨äº GPT-2 å’Œ BART æ¨¡å‹ã€‚
- P-Tuning: ä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç”Ÿæˆè½¯æç¤ºï¼ˆè€Œéå‰ç¼€ï¼‰ã€‚æœ€åˆåº”ç”¨äº GPT-2 ä»¥åŠ BERT/RoBERTa/MegatronLM æ¨¡å‹ã€‚
- P-Tuning v2:  æœ¬è´¨ä¸Šæ˜¯ Prefix Tuningï¼Œåº”ç”¨äº BERT ç±»æ¨¡å‹ã€‚
- LLaMA-Adapter: é‡‡ç”¨æ›´åˆç†çš„åˆå§‹åŒ–æ–¹å¼ï¼Œåœ¨å­¦ä¹ åˆ°çš„å‰ç¼€ä¸Šè¿›è¡Œå•ç‹¬çš„ softmax æ“ä½œã€‚åº”ç”¨äº LLaMA æ¨¡å‹ï¼Œè¿˜è®¨è®ºäº†å°†å¤šæ¨¡æ€ä¿¡æ¯æ³¨å…¥å‰ç¼€çš„æ–¹æ³•ã€‚


é‡è¦çš„æ˜¯ï¼ŒP-Tuning å’Œ P-Tuning v2 æ˜¯ä¸åŒçš„æ–¹æ³•ã€‚ä½† Prefix Tuning å’Œ P-Tuning v2 æœ¬è´¨ä¸Šæ˜¯ç›¸åŒçš„ã€‚


