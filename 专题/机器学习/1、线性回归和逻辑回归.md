
> source: https://aman.ai/primers/ai/linear-logistic-regression/


- [The Role of the Sigmoid Function](https://aman.ai/primers/ai/linear-logistic-regression/#the-role-of-the-sigmoid-function)

- [Connecting Linear Regression to Probabilities](https://aman.ai/primers/ai/linear-logistic-regression/#connecting-linear-regression-to-probabilities)

- [Log-Odds Transformation](https://aman.ai/primers/ai/linear-logistic-regression/#log-odds-transformation)

- [Probabilistic Interpretation](https://aman.ai/primers/ai/linear-logistic-regression/#probabilistic-interpretation)

- [Gradient-Based Optimization](https://aman.ai/primers/ai/linear-logistic-regression/#gradient-based-optimization)

- [Why Not Other Activation Functions?](https://aman.ai/primers/ai/linear-logistic-regression/#why-not-other-activation-functions)

- [Further Reading](https://aman.ai/primers/ai/linear-logistic-regression/#further-reading)

- [Citation](https://aman.ai/primers/ai/linear-logistic-regression/#citation)

  

## 一、线性回归

### 1.1 概述

线性回归是机器学习和统计分析中一种基础且广泛应用的统计技术。其目的是基于一个或多个自变量（或解释变量）来预测因变量（或目标变量）。该方法因其简单性、可解释性以及在理解更复杂模型中的基础作用而广受欢迎。尽管机器学习算法日益复杂，线性回归仍然是跨领域预测建模和解释分析的重要工具。

线性回归是一种统计方法，用于模拟因变量（结果变量）与一个或多个自变量（预测变量）之间的关系。为了使线性回归产生有效且可靠的结果，必须满足若干假设条件。以下是线性回归的主要假设：

### 1.2 线性回归的假设

1. **线性**：自变量（预测变量）与因变量（结果）之间的关系被假定为线性关系。具体而言，因变量的期望值是自变量的线性函数。
2. **误差独立性**：残差（观测值与预测值之间的差异）应当相互独立。这一假设意味着不应存在自相关性（在时间序列数据中尤为重要）。
3. **同方差性（误差的恒定方差）**：残差应在自变量的所有水平上具有恒定的方差。这意味着残差的分布在预测值的整个范围内应相似。如果存在异方差性，则表明模型在某些值上的表现优于其他值。
4. **误差的正态性**：残差应呈正态分布。这一假设在进行假设检验（如计算 p 值或置信区间）时尤为重要。
5. **无多重共线性**：自变量之间不应高度相关。如果存在多重共线性，将难以确定每个预测变量的单独影响。
6. **无内生性（误差与预测变量之间无相关性）**：自变量不应与误差项相关。这一假设确保预测变量真正独立于误差项，且不会捕捉到任何遗漏变量偏差。

### 1.3 线性回归的一般方程

线性回归的核心在于一个方程，它表示目标变量与一个或多个预测变量之间的关系：$$y=\beta_0 + \beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\epsilon$$这里：
- $y$: 我们旨在预测的因变量（或目标变量）。
- $x_1,x_2,\cdots, x_p$ ：自（或解释性）变量。
- $β_0$：截距项。
- $β_1,β_2,\cdots,β_p$：量化每个预测变量 $x_i$ 对 $y$ 影响的系数（或权重）。
- $\epsilon$：代表数据中不可约噪声或未建模方面的误差项。

线性回归的目标是估计系数（$β$），以最小化基于给定自变量集对因变量的预测误差。

### 1.4 关键概念

* **监督学习算法**: 线性回归是一种监督学习技术，这意味着它从训练数据中学习输入变量与标记输出变量之间的关系。
* **预测**: 一旦线性回归模型拟合了适当的系数，就可以通过简单地代入预测变量的值来预测新数据的 $y$ 值。
- **简单回归 vs 多元回归**:
	- **简单线性回归** 涉及一个自变量。
	- **多元线性回归** 涉及两个或更多自变量。

例如，根据平方英尺预测房价是简单线性回归的一个例子，而根据身高和年龄预测体重则属于多元回归的情况。

  

### 1.5 模型拟合：估计系数

线性回归的主要任务是找到一组系数 $(\beta_0,\beta_1,\cdots,\beta_p)$，使预测值与实际值之间的差异最小化。这是通过最小化平方误差和（SSE）或残差来实现的，即观测数据点与回归线预测值之间的差异。然后可以使用拟合模型通过以下方程进行预测：
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots + \hat{\beta}_p x_p$$
其中 $\hat{y}$ 代表预测值，$\hat{\beta}_i$ 为估计系数。

### 1.6 线性回归模型评估

评估线性回归模型需要考察模型对数据的拟合程度及其预测的准确性。

#### 1.6.1 均方误差 (MSE)

均方误差（MSE）是回归模型常用的损失函数，其计算方法是对实际值与预测值之差的平方取平均值：$$\text{MSE}=\frac{1}{n}\sum_{i=1}^n(y_{i}-\hat{y}_{i})^2$$**解释**: 均方误差（MSE）越低，表明模型的预测值越接近实际值

  
#### 1.6.2 R 方（决定系数）

R 方衡量的是因变量的方差中可由自变量预测的比例。其取值范围为 0 到 1，其中 1 表示模型完美地解释了数据中的方差。$$R^{2} = 1 - \frac{\sum\limits_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}}{\sum\limits_{i=1}^{n} (y_{i} - \bar{y})^{2}}$$例如，$R^2$ 为 0.8 意味着目标变量 80% 的方差由模型解释。



### 1.7 优化技巧：寻找最佳系数

#### 1.7.1 梯度下降

梯度下降是一种迭代优化算法，用于最小化成本函数（均方误差）。其工作原理是通过计算误差函数的梯度（或斜率），并沿最小化误差的方向更新系数。在处理大型数据集或众多特征时，该算法尤其有用，因为它的计算效率很高。

线性回归中梯度下降的更新规则是：$$\beta_{j}=\beta_{j}-\alpha\frac{ \partial}{\partial \beta_{j}}\text{MSE}$$这里：
* $α$ 是学习率（一个控制步长的较小正数）。
* $\frac{ \partial}{\partial \beta_{j}}\text{MSE}$ 表示损失函数对系数 $β_j$ 的导数。



#### 1.7.2 正则方程

对于较小数据集或计算复杂度不成问题的情况，正规方程提供了一种闭式解来寻找最佳系数。正规方程通过最小化残差平方和（RSS）推导得出，其表达式为：$$\hat{\beta}=(X^TX)^{-1}X^Ty$$其中 $X$ 是输入特征矩阵，$y$ 是目标值向量。


### 1.8  扩展与诠释

线性回归可以通过多种方式进行扩展，包括：

* **正则化回归：** 在成本函数中添加惩罚项以防止过拟合（例如，Lasso 回归和岭回归）。
* **多项式回归：** 通过添加自变量的多项式项来建模非线性关系。
* **多元回归分析：** 用于当存在多个自变量预测一个因变量时。

在这些扩展模型中解释系数遵循相同的原则，但多个预测变量或转换的存在可能需要更细致地解释模型的行为。

  

## 二、FAQs

### 2.1 线性回归是否假设特征与结果变量呈线性关系？

是的，线性回归假设自变量（特征）与结果变量之间的关系是线性的。然而，这并不一定意味着单个预测变量本身必须始终以其收集的形式出现（即未经转换）。你可以在线性回归模型中包含预测变量的非线性变换。例如：可以在模型中包含多项式项（如 $x^2, x^3$）或交互（交叉）项（如 $x_1x_2$）。只要模型在系数上是线性的，它就仍然是一个“线性回归”模型。例如，如果关系是二次的（$y=\beta_0+β_1x+β_2x^2+ϵ$），这仍然被视为线性回归模型，因为它在参数 $β_0, β_1, β_2$ 上是线性的。

### 2.2 线性假设如何适用于非线性（交叉）特征？

在回归模型中包含非线性项或交叉特征（交互项）并不违反线性回归的假设。关键因素在于模型在其参数上保持线性。例如，考虑模型中的交互项 $x_1x_2$：$$y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{3}(x_{1}x_{2})+\epsilon$$
尽管 $x_1$ 和 $x_2$ 之间存在非线性交互作用，但由于结果 $y$ 被表示为参数 $β_0$、$β_1$、$β_2$ 和 $β_3$ 的线性组合，该模型仍被视为线性模型。

线性回归中的线性假设涉及模型参数的线性关系，而非原始特征或预测变量本身。因此，只要预测变量与参数之间的关系保持线性，就可以引入变换或非线性特征，例如多项式项或交互项。

总之，线性回归能够容纳非线性特征，因为这些特征并不会影响模型系数的基本线性性质。这使得在遵循线性回归框架的同时，能够灵活捕捉复杂的关系。
  
  

### 2.3 什么是多重共线性？

当线性回归模型中的两个或多个自变量高度相关时，就会出现多重共线性问题，导致信息重叠或冗余。在这种情况下，模型难以区分每个预测变量对因变量的独立影响，从而导致系数估计不可靠、标准误差增大以及解释困难。这给线性回归带来了一个重大限制，因为它削弱了模型准确估计预测变量影响的能力。

可以通过诊断工具（如方差膨胀因子 VIF）来识别多重共线性，并通过消除相关变量、应用正则化技术或采用降维方法（如主成分分析 PCA）等策略来解决。

#### 2.3.1 多重共线性如何影响线性回归？

1. 不稳定系数（膨胀标准误差）：多重共线性会导致回归系数的估计对模型或数据的微小变化非常敏感。这会导致标准误差膨胀，使系数变得不可靠。因此，一个可能与因变量存在显著关系的变量可能在统计上显得不显著。
2. 解释困难：当预测变量高度相关时，解释单个系数变得更加困难。例如，如果两个变量几乎完全相关，模型很难为每个变量分配适当的权重，导致单个系数不可靠或违反直觉。
3. 降低统计功效：多重共线性的存在降低了估计系数的精确度，从而可能减弱假设检验的统计功效。这使得更难检测出真实存在的显著效应。
4. 高方差膨胀因子（VIF）：方差膨胀因子（VIF）常用于检测多重共线性。若任一预测变量的 VIF 值较大（通常大于 5 或 10），则表明存在多重共线性。较高的 VIF 值意味着该变量与其他预测变量高度相关。

#### 2.3.2 多重共线性是线性回归的一个限制吗？

是的，多重共线性被认为是线性回归的一个局限。然而，这并不是模型本身固有的局限，而是数据中可能影响线性回归模型性能和可解释性的问题。
  
以下是它对线性回归的限制：

* 不可靠的系数：高度多重共线性会使回归系数不可靠。这会削弱模型的预测能力，并导致难以得出有意义的结论。
* 解释困难：当变量高度相关时，很难判断哪一个变量对因变量产生影响。这使得理解预测变量与结果之间的关系变得困难。
* 预测方差膨胀：多重共线性会增加系数估计的变异性，进而可能增加预测的变异性，使模型的泛化能力下降。

### 2.4 如何检测和处理多重共线性？

1. 方差膨胀因子（VIF）：一种常见的诊断工具是方差膨胀因子（VIF），它用于衡量回归系数的方差由于多重共线性而被放大的程度。VIF 值大于 5 或 10 表明存在高度多重共线性。
2. 相关矩阵：检查自变量之间的相关矩阵可以揭示高度相关的变量对（相关系数接近 1 或 -1）。这表明可能存在多重共线性问题。
3. 删除其中一个相关变量：如果两个或多个变量高度相关，可以考虑删除其中一个。这样可以简化模型并减少多重共线性。
4. 主成分分析：PCA 可以将相关变量转换为一组不相关的成分，可用于回归分析。这降低了数据的维度并避免了多重共线性。
5. 岭回归或 Lasso 回归：这些是能够帮助减轻多重共线性影响的正则化技术。岭回归在系数大小上增加了一个惩罚项，从而降低它们对多重共线性的敏感度。而套索回归更进一步，可以将某些系数压缩至零，实际上就是选择预测变量的一个子集。
  

## 三、逻辑回归

### 3.1 概述

逻辑回归是一种基础且强大的监督学习算法，广泛应用于二分类任务。在机器学习中，监督学习涉及通过输入-输出对训练模型，以学习能够对未见数据进行预测的模式。逻辑回归专门用于基于输入特征预测分类结果的概率，其中结果属于两个类别之一（例如“雨天”与“晴天”或“成功”与“失败”）。虽然逻辑回归可以扩展到多类别，但其最常见的应用还是在二分类中。

逻辑回归的核心在于估计某个给定观测值属于特定类别的概率。该概率通过 sigmoid 函数计算得出，该函数将线性方程的输出转换为 0 到 1 之间的概率值。输入特征与二元响应变量之间的关系正是通过这个 sigmoid 函数建模的，从而使得输出结果可以被解释为观测值属于某个特定类别的可能性。

逻辑回归通过梯度下降或最大似然估计（MLE）等技术找到这种关系的最优参数，这两种方法都旨在最小化预测误差。这些优化方法调整模型的系数以最佳拟合数据，使得逻辑回归成为机器学习分类任务中既灵活又可解释的工具。

### 3.2 逻辑回归方程

在二分类任务中，逻辑回归模型预测的是概率 $P(y=1∣X)$，其中 $y$ 代表二元结果（0 或 1），而 $X=(x_1,x_2,…,x_k)$ 表示输入特征。逻辑回归模型基于以下方程：$$P(y=1|X)=\text{sigmoid}(z)=\frac{1}{1+e^{-z}}$$这里的 $z=\hat{\beta}_{0}+\hat{\beta}_{1}x_{1}+\hat{\beta}_{2}x_{2}+\cdots \hat{\beta}_{k}x_{k}$，Sigmoid 函数确保输出是一个概率值，范围限定在 0 到 1 之间。

线性预测变量 $z$ 是输入特征的线性组合，这与线性回归非常相似。然而，逻辑回归不是预测连续结果，而是应用 sigmoid 函数来确保输出可以被解释为概率。
  
### 3.3 逻辑回归的工作原理：一个实际案例

要理解逻辑回归在实际中如何运作，设想一个场景：你想预测西雅图的天气，具体来说是晴天还是雨天。结果可以用一个二元变量表示：晴天为 1，雨天为 0。假设你用来预测的特征是温度。

对此数据拟合线性回归模型并不合适，因为预测值可能超出 0 到 1 的范围，导致不合理的结果（例如，预测晴天概率为 1.5）。相反，逻辑回归拟合的是逻辑（S 型）函数，确保预测概率介于 0 和 1 之间。你可以将这些概率解释为在特定温度下晴天的可能性。

一旦模型输出概率值，就会应用一个分类阈值（通常为 0.5）。例如，如果预测概率大于 0.5，模型将预测为晴天；若小于 0.5，则预测为雨天。根据具体场景调整阈值，可以实现更谨慎或更具风险承受能力的预测。

### 3.4 模型评估：对数损失函数

逻辑回归模型通过损失函数来评估其预测真实结果的能力。对于二分类问题，合适的损失函数是对数损失（也称为二元交叉熵）。给定包含 $n$ 个样本的数据集，其对数损失定义为：$$\text{Log-Loss} = -\sum_{i=0}^{n}\left(y_{i} \cdot \log(p_{i}) + (1-y_{i}) \cdot \log(1-p_{i})\right)$$这里的 $y_{i}$ 是第 $i$ 个观测值的真实标签，$p_i$ 是第 $i$ 个观测值的预测概率。

对数损失函数会对预测概率与实际标签之间的较大偏差进行惩罚，尤其是当模型自信但错误时（例如，预测概率接近 1 而实际结果为 0，或反之），惩罚力度更大。最小化对数损失函数有助于提高预测的准确性。


### 3.5 估计系数：梯度下降与最大似然估计

在逻辑回归中，目标是找到使对数损失函数最小的系数 $\hat{\beta}_{0},\hat{\beta}_{1},\cdots,\hat{\beta}_{k}$ 的最优值。估计这些系数主要有两种方法：梯度下降法和最大似然估计法（MLE）。

#### 3.5.1 梯度下降

梯度下降是一种迭代优化算法，用于最小化对数损失函数。该过程首先为参数选择初始值，然后迭代更新它们。每次更新的方向和大小由对数损失函数的梯度决定，梯度指向增长最快的方向。通过沿着梯度的相反方向移动，算法试图最小化损失函数。每次迭代根据以下规则更新参数：$$\beta_{j}^{(t+1)}=\beta_{j}^{(t)}-\alpha\frac{\partial\log\text{-Loss}}{\partial\beta_{j}}$$
#### 3.5.2 最大似然估计（MLE）

MLE（最大似然估计）是另一种估计逻辑回归系数的方法。该方法通过最大化在给定模型预测下观测数据发生的似然性来实现。对数似然函数是似然函数的对数形式，最大化对数似然函数等同于最小化对数损失（Log-Loss）。具体实现时，需要将对数似然函数对各参数的偏导数设为零，并求解由此得到的方程组。

### 3.6 解读逻辑回归系数

逻辑回归中系数的解释与线性回归不同，因为它们是以对数几率而非概率的形式表示的。几率的计算公式为： $$\text{Odds}=\frac{p}{1-p}$$其中，$p$ 代表正类别的概率（例如晴天）。逻辑回归系数表示对应预测变量每增加一个单位时，结果的对数几率发生的变化。

为了使系数更易于解释，我们可以对其进行指数化处理，将其从对数几率尺度转换为几率尺度。例如，如果某个特征的系数 $\hat{\beta}_{1}$ 为0.7，那么 $e^{0.7}≈2$，这意味着该特征每增加一个单位，正类的几率大约会乘以 2。


## 四、FAQ: 为什么 Sigmoid 函数在逻辑回归中扮演重要角色？

Sigmoid函数在逻辑回归中扮演着核心角色，因为它能将预测值（范围从负无穷到正无穷）映射到 0 到 1 之间，使其成为概率估计的理想选择。逻辑回归是一种分类算法，旨在预测观测值属于特定类别的概率，例如在二分类问题中（如 0 或 1）。
  
通过应用 sigmoid 函数，输入特征的无界线性组合被转化为一个概率值，该值表示观测样本属于正类的可能性。此函数在事件的对数几率与其概率之间建立了自然的联系。此外，其平滑可微的特性确保了模型训练过程中能进行高效优化，使其非常适用于该任务。
  
### 4.1 问题性质：二元分类

逻辑回归主要用于二元分类任务，其中因变量（输出）是分类变量（例如，0 或 1，是或否，真或假）。概率自然介于 0 和 1 之间，因此模型需要一个符合此范围的输出函数。



### The Role of the Sigmoid Function

  

- The sigmoid function is defined as: σ(z)=11+e−z

- **Range**: The output of the sigmoid function is always in the interval (0, 1).

- **Shape**: The S-shaped curve ensures smooth transitions between 0 and 1. Small values of z approach 0, and large values of z approach 1.

- This makes it ideal for interpreting z, the linear combination of inputs and weights, as a probability.

  

#### Connecting Linear Regression to Probabilities

  

- Logistic regression starts with a linear model:

z=w0+w1x1+w2x2+⋯+wnxn

- where z can take any real value (−∞ to +∞).

- Directly interpreting z as a probability would not make sense because probabilities are bounded between 0 and 1.

- Applying the sigmoid function to z transforms it into a probability:

P(y=1|x)=σ(z)=11+e−z

  

#### Log-Odds Transformation

  

- The sigmoid function naturally corresponds to the concept of log-odds (logit):

- Log-odds represent the logarithm of the odds of an event occurring:

logit(p)=ln(p1−p)

- The inverse of the logit function is the sigmoid function:

p=11+e−z

- This means that logistic regression models the log-odds as a linear combination of the input features.

  

#### Probabilistic Interpretation

  

- After applying the sigmoid function, the output P(y=1∣x) can be interpreted as:

- The probability of the positive class (1) for given input x.

- 1−P(y=1∣x) as the probability of the negative class (0).

- This probabilistic output is useful for decision-making, thresholding, and downstream tasks.

  

#### Gradient-Based Optimization

  

- Logistic regression uses maximum likelihood estimation (MLE) to find the parameters (w0,w1,…).

- The sigmoid function’s mathematical properties (smooth gradient and bounded output) ensure that the cost function (cross-entropy loss) is differentiable and convex, making it easier to optimize using gradient descent.

  

#### Why Not Other Activation Functions?

  

- Other functions like the step function or hyperbolic tangent (tanh) might also be considered, but they have limitations:

- **Step Function**: Outputs discrete values (0 or 1), making it unsuitable for gradient-based optimization.

- **Hyperbolic Tangent**: Outputs values between -1 and 1, which isn’t ideal for probability estimation.

- The sigmoid function, on the other hand, is simple, computationally efficient, and perfectly suited for probabilistic interpretations in the context of logistic regression.

  

## Further Reading

  

- [MLU-Explain: Linear Regression](https://mlu-explain.github.io/linear-regression/)

- [MLU-Explain: Logistic Regression](https://mlu-explain.github.io/logistic-regression/)

  

## Citation

  

If you found our work useful, please cite it as:

  

![](https://aman.ai/images/copy.png)

  

`@article{Chadha2020DistilledLinearLogisticRegression, title = {Linear and Logistic Regression}, author = {Chadha, Aman}, journal = {Distilled AI}, year = {2020}, note = {\url{https://aman.ai}} }`

  

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)|

  

[www.amanchadha.com](https://www.amanchadha.com/)