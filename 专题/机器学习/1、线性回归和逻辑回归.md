
> source: https://aman.ai/primers/ai/linear-logistic-regression/




- [FAQs](https://aman.ai/primers/ai/linear-logistic-regression/#faqs)

- [Does Linear Regression Assume That Features are Linearly Related to the Outcome Variable?](https://aman.ai/primers/ai/linear-logistic-regression/#does-linear-regression-assume-that-features-are-linearly-related-to-the-outcome-variable)

- [How Does the Linearity Assumption Hold with Non-Linear (Cross) Features?](https://aman.ai/primers/ai/linear-logistic-regression/#how-does-the-linearity-assumption-hold-with-non-linear-cross-features)

- [What is Multicollinearity?](https://aman.ai/primers/ai/linear-logistic-regression/#what-is-multicollinearity)

- [How Does Multicollinearity Affect Linear Regression?](https://aman.ai/primers/ai/linear-logistic-regression/#how-does-multicollinearity-affect-linear-regression)

- [Is Multicollinearity a Limitation of Linear Regression?](https://aman.ai/primers/ai/linear-logistic-regression/#is-multicollinearity-a-limitation-of-linear-regression)

- [How to Detect and Address Multicollinearity?](https://aman.ai/primers/ai/linear-logistic-regression/#how-to-detect-and-address-multicollinearity)

- [Logistic Regression](https://aman.ai/primers/ai/linear-logistic-regression/#logistic-regression)

- [Overview](https://aman.ai/primers/ai/linear-logistic-regression/#overview-1)

- [The Logistic Regression Equation](https://aman.ai/primers/ai/linear-logistic-regression/#the-logistic-regression-equation)

- [How Logistic Regression Works: a Practical Example](https://aman.ai/primers/ai/linear-logistic-regression/#how-logistic-regression-works-a-practical-example)

- [Model Evaluation: the Log-Loss Function](https://aman.ai/primers/ai/linear-logistic-regression/#model-evaluation-the-log-loss-function)

- [Estimating Coefficients: Gradient Descent and Maximum Likelihood Estimation](https://aman.ai/primers/ai/linear-logistic-regression/#estimating-coefficients-gradient-descent-and-maximum-likelihood-estimation)

- [Gradient Descent](https://aman.ai/primers/ai/linear-logistic-regression/#gradient-descent-1)

- [Maximum Likelihood Estimation (MLE)](https://aman.ai/primers/ai/linear-logistic-regression/#maximum-likelihood-estimation-mle)

- [Interpreting Logistic Regression Coefficients](https://aman.ai/primers/ai/linear-logistic-regression/#interpreting-logistic-regression-coefficients)

- [FAQ: Why is the Role of the Sigmoid Function in Logistic Regression?](https://aman.ai/primers/ai/linear-logistic-regression/#faq-why-is-the-role-of-the-sigmoid-function-in-logistic-regression)

- [Nature of the Problem: Binary Classification](https://aman.ai/primers/ai/linear-logistic-regression/#nature-of-the-problem-binary-classification)

- [The Role of the Sigmoid Function](https://aman.ai/primers/ai/linear-logistic-regression/#the-role-of-the-sigmoid-function)

- [Connecting Linear Regression to Probabilities](https://aman.ai/primers/ai/linear-logistic-regression/#connecting-linear-regression-to-probabilities)

- [Log-Odds Transformation](https://aman.ai/primers/ai/linear-logistic-regression/#log-odds-transformation)

- [Probabilistic Interpretation](https://aman.ai/primers/ai/linear-logistic-regression/#probabilistic-interpretation)

- [Gradient-Based Optimization](https://aman.ai/primers/ai/linear-logistic-regression/#gradient-based-optimization)

- [Why Not Other Activation Functions?](https://aman.ai/primers/ai/linear-logistic-regression/#why-not-other-activation-functions)

- [Further Reading](https://aman.ai/primers/ai/linear-logistic-regression/#further-reading)

- [Citation](https://aman.ai/primers/ai/linear-logistic-regression/#citation)

  

## 一、线性回归

### 1.1 概述

线性回归是机器学习和统计分析中一种基础且广泛应用的统计技术。其目的是基于一个或多个自变量（或解释变量）来预测因变量（或目标变量）。该方法因其简单性、可解释性以及在理解更复杂模型中的基础作用而广受欢迎。尽管机器学习算法日益复杂，线性回归仍然是跨领域预测建模和解释分析的重要工具。

线性回归是一种统计方法，用于模拟因变量（结果变量）与一个或多个自变量（预测变量）之间的关系。为了使线性回归产生有效且可靠的结果，必须满足若干假设条件。以下是线性回归的主要假设：

### 1.2 线性回归的假设

1. **线性**：自变量（预测变量）与因变量（结果）之间的关系被假定为线性关系。具体而言，因变量的期望值是自变量的线性函数。
2. **误差独立性**：残差（观测值与预测值之间的差异）应当相互独立。这一假设意味着不应存在自相关性（在时间序列数据中尤为重要）。
3. **同方差性（误差的恒定方差）**：残差应在自变量的所有水平上具有恒定的方差。这意味着残差的分布在预测值的整个范围内应相似。如果存在异方差性，则表明模型在某些值上的表现优于其他值。
4. **误差的正态性**：残差应呈正态分布。这一假设在进行假设检验（如计算 p 值或置信区间）时尤为重要。
5. **无多重共线性**：自变量之间不应高度相关。如果存在多重共线性，将难以确定每个预测变量的单独影响。
6. **无内生性（误差与预测变量之间无相关性）**：自变量不应与误差项相关。这一假设确保预测变量真正独立于误差项，且不会捕捉到任何遗漏变量偏差。

### 1.3 线性回归的一般方程

线性回归的核心在于一个方程，它表示目标变量与一个或多个预测变量之间的关系：$$y=\beta_0 + \beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\epsilon$$这里：
- $y$: 我们旨在预测的因变量（或目标变量）。
- $x_1,x_2,\cdots, x_p$ ：自（或解释性）变量。
- $β_0$：截距项。
- $β_1,β_2,\cdots,β_p$：量化每个预测变量 $x_i$ 对 $y$ 影响的系数（或权重）。
- $\epsilon$：代表数据中不可约噪声或未建模方面的误差项。

线性回归的目标是估计系数（$β$），以最小化基于给定自变量集对因变量的预测误差。

### 1.4 关键概念

* **监督学习算法**: 线性回归是一种监督学习技术，这意味着它从训练数据中学习输入变量与标记输出变量之间的关系。
* **预测**: 一旦线性回归模型拟合了适当的系数，就可以通过简单地代入预测变量的值来预测新数据的 $y$ 值。
- **简单回归 vs 多元回归**:
	- **简单线性回归** 涉及一个自变量。
	- **多元线性回归** 涉及两个或更多自变量。

例如，根据平方英尺预测房价是简单线性回归的一个例子，而根据身高和年龄预测体重则属于多元回归的情况。

  

### 1.5 模型拟合：估计系数

线性回归的主要任务是找到一组系数 $(\beta_0,\beta_1,\cdots,\beta_p)$，使预测值与实际值之间的差异最小化。这是通过最小化平方误差和（SSE）或残差来实现的，即观测数据点与回归线预测值之间的差异。然后可以使用拟合模型通过以下方程进行预测：
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots + \hat{\beta}_p x_p$$
其中 $\hat{y}$ 代表预测值，$\hat{\beta}_i$ 为估计系数。

### 1.6 线性回归模型评估

评估线性回归模型需要考察模型对数据的拟合程度及其预测的准确性。

#### 1.6.1 均方误差 (MSE)

均方误差（MSE）是回归模型常用的损失函数，其计算方法是对实际值与预测值之差的平方取平均值：$$\text{MSE}=\frac{1}{n}\sum_{i=1}^n(y_{i}-\hat{y}_{i})^2$$**解释**: 均方误差（MSE）越低，表明模型的预测值越接近实际值

  
#### 1.6.2 R 方（决定系数）

R 方衡量的是因变量的方差中可由自变量预测的比例。其取值范围为 0 到 1，其中 1 表示模型完美地解释了数据中的方差。$$R^{2} = 1 - \frac{\sum\limits_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}}{\sum\limits_{i=1}^{n} (y_{i} - \bar{y})^{2}}$$例如，$R^2$ 为 0.8 意味着目标变量 80% 的方差由模型解释。



### 1.7 优化技巧：寻找最佳系数

#### 1.7.1 梯度下降

梯度下降是一种迭代优化算法，用于最小化成本函数（均方误差）。其工作原理是通过计算误差函数的梯度（或斜率），并沿最小化误差的方向更新系数。在处理大型数据集或众多特征时，该算法尤其有用，因为它的计算效率很高。

线性回归中梯度下降的更新规则是：$$\beta_{j}=\beta_{j}-\alpha\frac{ \partial}{\partial \beta_{j}}\text{MSE}$$这里：
* $α$ 是学习率（一个控制步长的较小正数）。
* $\frac{ \partial}{\partial \beta_{j}}\text{MSE}$ 表示损失函数对系数 $β_j$ 的导数。



#### 1.7.2 正则方程

对于较小数据集或计算复杂度不成问题的情况，正规方程提供了一种闭式解来寻找最佳系数。正规方程通过最小化残差平方和（RSS）推导得出，其表达式为：$$\hat{\beta}=(X^TX)^{-1}X^Ty$$其中 $X$ 是输入特征矩阵，$y$ 是目标值向量。


### 1.8  扩展与诠释

线性回归可以通过多种方式进行扩展，包括：

* **正则化回归：** 在成本函数中添加惩罚项以防止过拟合（例如，Lasso 回归和岭回归）。
* **多项式回归：** 通过添加自变量的多项式项来建模非线性关系。
* **多元回归分析：** 用于当存在多个自变量预测一个因变量时。

在这些扩展模型中解释系数遵循相同的原则，但多个预测变量或转换的存在可能需要更细致地解释模型的行为。

  

## 二、FAQs

### 2.1 线性回归是否假设特征与结果变量呈线性关系？

是的，线性回归假设自变量（特征）与结果变量之间的关系是线性的。然而，这并不一定意味着单个预测变量本身必须始终以其收集的形式出现（即未经转换）。你可以在线性回归模型中包含预测变量的非线性变换。例如：可以在模型中包含多项式项（如 $x^2, x^3$）或交互（交叉）项（如 $x_1x_2$）。只要模型在系数上是线性的，它就仍然是一个“线性回归”模型。例如，如果关系是二次的（$y=\beta_0+β_1x+β_2x^2+ϵ$），这仍然被视为线性回归模型，因为它在参数 $β_0, β_1, β_2$ 上是线性的。

### 2.2 How Does the Linearity Assumption Hold with Non-Linear (Cross) Features?


- The inclusion of non-linear terms or cross features (interaction terms) in a regression model does not violate the assumptions of linear regression. The key factor is that the model remains linear in its parameters. For example, consider an interaction term x1×x2 in the model:

  

y=β0+β1x1+β2x2+β3(x1×x2)+ϵ.

  

- Although x1 and x2 interact non-linearly, the model is still considered linear because the outcome y is expressed as a linear combination of the parameters β0,β1,β2, and β3.

- The linearity assumption in linear regression pertains to the linearity of the model’s parameters, not necessarily the raw features or predictors. As a result, you can incorporate transformations or non-linear features, such as polynomial terms or interaction terms, as long as the relationship between the predictors and the parameters remains linear.

- In summary, linear regression accommodates non-linear features because these do not affect the fundamental linearity in the model’s coefficients. This allows for flexibility in capturing complex relationships while adhering to the framework of linear regression.

  

### What is Multicollinearity?

  

- Multicollinearity arises when two or more independent variables in a linear regression model are highly correlated, resulting in overlapping or redundant information. In such cases, the model struggles to isolate the individual effects of each predictor on the dependent variable, leading to unreliable coefficient estimates, inflated standard errors, and challenges in interpretation.

- This presents a significant limitation in linear regression, as it compromises the model’s capacity to accurately estimate the effects of the predictors.

- Multicollinearity can be identified using diagnostic tools such as the Variance Inflation Factor (VIF) and addressed through strategies like eliminating correlated variables, applying regularization techniques, or employing dimensionality reduction methods, such as Principal Component Analysis (PCA).

  

#### How Does Multicollinearity Affect Linear Regression?

  

1. **Unstable Coefficients (Inflated Standard Errors)**:

- Multicollinearity causes the estimates of the regression coefficients to become very sensitive to small changes in the model or data. This leads to inflated standard errors, making the coefficients unreliable. As a result, a variable that might have a significant relationship with the dependent variable could appear statistically insignificant.

2. **Difficult Interpretation**:

- When predictors are highly correlated, it becomes harder to interpret the individual coefficients. For instance, if two variables are almost perfectly correlated, the model struggles to assign appropriate weight to each variable, making the individual coefficients unreliable or counterintuitive.

3. **Reduces Statistical Power**:

- The presence of multicollinearity reduces the precision of the estimated coefficients, which can decrease the statistical power of hypothesis tests. This makes it harder to detect significant effects that truly exist.

4. **High Variance Inflation Factor (VIF)**:

- The **Variance Inflation Factor (VIF)** is often used to detect multicollinearity. If the VIF for any predictor is large (usually greater than 5 or 10), it indicates the presence of multicollinearity. A high VIF means the variable is highly correlated with other predictors.

  

#### Is Multicollinearity a Limitation of Linear Regression?

  

- Yes, **multicollinearity is considered a limitation of linear regression**. However, it is not an inherent limitation of the model itself but rather a problem in the data that can affect the performance and interpretability of the linear regression model.

- Here’s how it limits linear regression:

- **Unreliable Coefficients**: High multicollinearity can make the regression coefficients unreliable. This undermines the predictive power of the model and leads to difficulties in drawing meaningful conclusions.

- **Difficulty in Interpretation**: When variables are highly correlated, it becomes hard to tell which one is driving the effect on the dependent variable. This makes it difficult to understand the relationship between predictors and the outcome.

- **Inflated Variance of Predictions**: Multicollinearity increases the variability of the coefficient estimates, which in turn can increase the variability in predictions, making the model less generalizable.

  

### How to Detect and Address Multicollinearity?

  

1. **Variance Inflation Factor (VIF)**:

- A common diagnostic tool is the VIF, which measures how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF greater than 5 or 10 indicates high multicollinearity.

2. **Correlation Matrix**:

- Examining a correlation matrix of the independent variables can reveal pairs of variables that are highly correlated (correlation close to 1 or -1). This is an indication of potential multicollinearity.

3. **Drop One of the Correlated Variables**:

- If two or more variables are highly correlated, consider dropping one of them. This can simplify the model and reduce multicollinearity.

4. **Principal Component Analysis (PCA)**:

- PCA can transform the correlated variables into a set of uncorrelated components, which can be used in regression analysis. This reduces the dimensionality of the data and avoids multicollinearity.

5. **Ridge Regression or Lasso Regression**:

- These are regularization techniques that can help mitigate the effects of multicollinearity. Ridge regression adds a penalty to the size of the coefficients, which reduces their sensitivity to multicollinearity. Lasso regression goes a step further and can shrink some coefficients to zero, effectively selecting a subset of the predictors.

  

## Logistic Regression

  

### Overview

  

- Logistic regression is a fundamental and powerful supervised learning algorithm widely used for binary classification tasks. In machine learning, supervised learning involves training a model on input-output pairs to learn patterns that enable predictions on unseen data. Logistic regression specifically predicts the probability of a categorical outcome based on input features, where the outcome belongs to one of two classes (e.g., “rainy” vs. “sunny” or “success” vs. “failure”). Although logistic regression can be extended to multiple classes, its most common application is in binary classification.

- At its core, logistic regression estimates the probability that a given observation falls into a particular class. This probability is derived using a sigmoid function, which converts the output of a linear equation into a probability value constrained between 0 and 1. The relationship between the input features and the binary response variable is modeled through this sigmoid function, allowing for the output to be interpreted as the likelihood of an observation belonging to a particular class.

- Logistic regression finds the optimal parameters for this relationship using techniques like gradient descent or maximum likelihood estimation (MLE), both of which aim to minimize prediction errors. These optimization methods adjust the model’s coefficients to best fit the data, making logistic regression both a flexible and interpretable tool for classification tasks in machine learning.

  

### The Logistic Regression Equation

  

- For a binary classification task, the logistic regression model predicts the probability P(y=1∣X), where y is the binary outcome (0 or 1) and X=(x1,x2,…,xk) represents the input features.

- The logistic regression model is based on the following equation:

P(y=1∣X)=sigmoid(z)=11+e−z

- where:

- z=β0^+β1^x1+β2^x2+⋯+βk^xk

- β0^,β1^,…,βk^ are the coefficients (parameters) of the model.

- The sigmoid function ensures that the output is a probability, constrained between 0 and 1.

- The linear predictor z is a linear combination of the input features, much like in linear regression. However, instead of predicting continuous outcomes, logistic regression applies the sigmoid function to ensure the output can be interpreted as a probability.

  

### How Logistic Regression Works: a Practical Example

  

- To understand how logistic regression works in practice, consider a scenario where you want to predict the weather in Seattle, specifically whether a day will be sunny or rainy. The outcome can be represented as a binary variable: assign 1 to a sunny day and 0 to a rainy day. Suppose the feature you use to make this prediction is the temperature.

- Fitting a linear regression model to this data would be inappropriate because the predicted values could fall outside the range of 0 and 1, leading to nonsensical results (e.g., a prediction of 1.5 for sunny days). Instead, logistic regression fits a logistic (sigmoid) function, ensuring that predicted probabilities are between 0 and 1. You can interpret these probabilities as the likelihood of a sunny day, given a specific temperature.

- Once the model outputs a probability, a classification threshold (often 0.5) is applied. For example, if the predicted probability is greater than 0.5, the model will predict a sunny day. If it’s less than 0.5, it will predict a rainy day. Adjusting the threshold allows for more cautious or risk-tolerant predictions depending on the context.

  

### Model Evaluation: the Log-Loss Function

  

- Logistic regression models are evaluated using a loss function that measures how well the model predicts the true outcomes. For binary classification, the appropriate loss function is the Log-Loss (also known as binary cross-entropy). The Log-Loss for a given dataset with n samples is defined as:

Log-Loss=−∑i=0n(yi⋅log(pi)+(1−yi)⋅log(1−pi))

- where:

- yi is the true label for the i-th observation.

- pi is the predicted probability for the i-th observation.

- The Log-Loss penalizes large deviations between the predicted probability and the actual label, with larger penalties when the model is confident but wrong (i.e., when a predicted probability is near 1 for an outcome of 0, or vice versa). Minimizing the Log-Loss function leads to more accurate predictions.

  

### Estimating Coefficients: Gradient Descent and Maximum Likelihood Estimation

  

- In logistic regression, the goal is to find the optimal values of the coefficients β0^,β1^,…,βk^ that minimize the Log-Loss. There are two primary methods to estimate these coefficients: gradient descent and maximum likelihood estimation (MLE).

  

#### Gradient Descent

  

- Gradient descent is an iterative optimization algorithm used to minimize the Log-Loss function. The process begins by selecting initial values for the parameters and then updating them iteratively. The direction and magnitude of each update are determined by the gradient of the Log-Loss function, which points in the direction of the steepest increase. By moving in the opposite direction of the gradient, the algorithm seeks to minimize the loss function.

- Each iteration updates the parameters according to the following rule:

β(t+1)j=β(t)j−α∂Log-Loss∂βj

- where:

- α is the learning rate (step size).

- ∂Log-Loss∂βj is the partial derivative of the Log-Loss with respect to βj.

  

#### Maximum Likelihood Estimation (MLE)

  

- MLE is another method to estimate the coefficients in logistic regression. It involves maximizing the likelihood that the observed data occurred given the model’s predictions. The log-likelihood function is the log of the likelihood function, and maximizing the log-likelihood is equivalent to minimizing the Log-Loss. This can be achieved by setting the partial derivatives of the log-likelihood function with respect to the parameters equal to zero and solving the resulting system of equations.

  

### Interpreting Logistic Regression Coefficients

  

- Interpreting the coefficients in logistic regression differs from linear regression because they are expressed in terms of log-odds rather than probabilities. - The odds are calculated as:

Odds=p1−p

- where p is the probability of the positive class (e.g., sunny day). The logistic regression coefficients represent the change in the log-odds of the outcome for a one-unit increase in the corresponding predictor variable.

- To make the coefficients easier to interpret, we can exponentiate them, transforming them from the log-odds scale to the odds scale. For example, if the coefficient β1^ for a feature is 0.7, then e0.7≈2, meaning that a one-unit increase in that feature multiplies the odds of the positive class by approximately 2.

  

## FAQ: Why is the Role of the Sigmoid Function in Logistic Regression?

  

- The sigmoid function plays a central role in logistic regression because it maps the predicted values, which can range from −∞ to +∞, to a range between 0 and 1, making it ideal for probability estimation. Logistic regression is a classification algorithm designed to predict the probability of an observation belonging to a specific class, such as in binary classification (e.g., 0 or 1).

- By applying the sigmoid function, the unbounded linear combination of input features is transformed into a probability value that represents the likelihood of the observation being in the positive class. This function provides a natural link between the log-odds of an event and its probability. Additionally, its smooth and differentiable nature ensures efficient optimization during the model training process, making it well-suited for the task.

  

### Nature of the Problem: Binary Classification

  

- Logistic regression is primarily used for binary classification tasks where the dependent variable (output) is categorical (e.g., 0 or 1, yes or no, true or false).

- Probabilities are naturally bounded between 0 and 1, so the model needs an output function that conforms to this range.

  

### The Role of the Sigmoid Function

  

- The sigmoid function is defined as: σ(z)=11+e−z

- **Range**: The output of the sigmoid function is always in the interval (0, 1).

- **Shape**: The S-shaped curve ensures smooth transitions between 0 and 1. Small values of z approach 0, and large values of z approach 1.

- This makes it ideal for interpreting z, the linear combination of inputs and weights, as a probability.

  

#### Connecting Linear Regression to Probabilities

  

- Logistic regression starts with a linear model:

z=w0+w1x1+w2x2+⋯+wnxn

- where z can take any real value (−∞ to +∞).

- Directly interpreting z as a probability would not make sense because probabilities are bounded between 0 and 1.

- Applying the sigmoid function to z transforms it into a probability:

P(y=1|x)=σ(z)=11+e−z

  

#### Log-Odds Transformation

  

- The sigmoid function naturally corresponds to the concept of log-odds (logit):

- Log-odds represent the logarithm of the odds of an event occurring:

logit(p)=ln(p1−p)

- The inverse of the logit function is the sigmoid function:

p=11+e−z

- This means that logistic regression models the log-odds as a linear combination of the input features.

  

#### Probabilistic Interpretation

  

- After applying the sigmoid function, the output P(y=1∣x) can be interpreted as:

- The probability of the positive class (1) for given input x.

- 1−P(y=1∣x) as the probability of the negative class (0).

- This probabilistic output is useful for decision-making, thresholding, and downstream tasks.

  

#### Gradient-Based Optimization

  

- Logistic regression uses maximum likelihood estimation (MLE) to find the parameters (w0,w1,…).

- The sigmoid function’s mathematical properties (smooth gradient and bounded output) ensure that the cost function (cross-entropy loss) is differentiable and convex, making it easier to optimize using gradient descent.

  

#### Why Not Other Activation Functions?

  

- Other functions like the step function or hyperbolic tangent (tanh) might also be considered, but they have limitations:

- **Step Function**: Outputs discrete values (0 or 1), making it unsuitable for gradient-based optimization.

- **Hyperbolic Tangent**: Outputs values between -1 and 1, which isn’t ideal for probability estimation.

- The sigmoid function, on the other hand, is simple, computationally efficient, and perfectly suited for probabilistic interpretations in the context of logistic regression.

  

## Further Reading

  

- [MLU-Explain: Linear Regression](https://mlu-explain.github.io/linear-regression/)

- [MLU-Explain: Logistic Regression](https://mlu-explain.github.io/logistic-regression/)

  

## Citation

  

If you found our work useful, please cite it as:

  

![](https://aman.ai/images/copy.png)

  

`@article{Chadha2020DistilledLinearLogisticRegression, title = {Linear and Logistic Regression}, author = {Chadha, Aman}, journal = {Distilled AI}, year = {2020}, note = {\url{https://aman.ai}} }`

  

-  [](https://github.com/amanchadha)|  [](https://citations.amanchadha.com/)|  [](https://twitter.com/i_amanchadha)|  [](mailto:hi@aman.ai)|

  

[www.amanchadha.com](https://www.amanchadha.com/)