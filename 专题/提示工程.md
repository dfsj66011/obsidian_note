


### ReAct Prompting

[Yao et al., 2022](https://arxiv.org/abs/2210.03629) 中引入，LLM 被用来以交错的方式生成 *推理轨迹* 和 *特定任务* 的动作。

生成推理痕迹使模型能够归纳、跟踪和更新行动计划，甚至处理异常情况。行动步骤允许与知识库或环境等外部来源进行交互并从中收集信息。ReAct框架能够让大型语言模型*与外部工具交互*，从而获取额外信息，进而生成更可靠且基于事实的回应。

**工作原理：**

ReAct 的灵感来源于“行动”与“推理”之间的协同作用，正是这种协同作用使人类能够学习新任务并做出决策或推理。

ReAct是一种结合推理与大型语言模型（LLMs）行动的通用范式。它通过提示LLMs生成针对任务的语言推理轨迹和行动，使系统能够进行动态推理，从而创建、维护和调整行动计划，同时还能与外部环境（如维基百科）互动，将额外信息融入推理过程。下图（来源）展示了ReAct的一个实例，以及执行问答任务所涉及的不同步骤。

![|600](https://aman.ai/primers/ai/assets/prompt/react.png)

请注意，上下文示例也会被添加到提示中，但为了简化起见，我们在此将其排除。可以看到，模型生成了任务解决轨迹（思考、行动）。obs 对应的是与交互环境（例如搜索引擎）的观察结果。本质上，ReAct 能够检索信息以支持推理，而推理则有助于确定下一步要检索的内容。

第一步是从训练集中选择案例，并编写 ReAct 格式的轨迹。这些轨迹作为提示中的少量示例使用。轨迹由多个思考-行动-观察步骤组成，如上图所示。自由形式的思考用于完成不同任务，如分解问题、提取信息、进行常识/算术推理、指导搜索公式的制定以及综合最终答案。

*缺点：* 但是 ReAct 的结构性约束降低了其在制定推理步骤时的灵活性，它高度依赖其检索到的信息；无意义的搜索结果会干扰模型推理，导致难以恢复和重构思路。一般来说，结合并支持在 ReAct 和 CoT+自洽性之间切换的提示方法，通常优于所有其他提示方法。



### ReAct Usage with LangChain

以下是 ReAct 提示方法在实际应用中的高层次示例。我们将使用 OpenAI 作为大语言模型（LLM），并采用 LangChain，因为它已内置了利用 ReAct 框架的功能，通过结合大语言模型和不同工具的能力来构建执行任务的智能代理。

```python
 # update or install the necessary libraries 
 !pip install --upgrade openai 
 !pip install --upgrade langchain 
 !pip install --upgrade python-dotenv 
 !pip install google-search-results  
 
 # import libraries 
 import openai 
 import os 
 from langchain.llms import OpenAI 
 from langchain.agents import load_tools 
 from langchain.agents import initialize_agent 
 from dotenv import load_dotenv 
 
 load_dotenv()  
 
 # load API keys; you will need to obtain these if you haven't yet 
 os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") 
 os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")
```
`
现在我们可以配置 LLM、将要使用的工具以及允许我们结合 LLM 和工具利用 ReAct 框架的代理。请注意，我们正在使用搜索 API 来搜索外部信息，并将 LLM 作为数学工具使用。

```python
llm = OpenAI(model_name="text-davinci-003" ,temperature=0) 
tools = load_tools(["google-serper", "llm-math"], llm=llm) 
agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)
```
`
配置完成后，我们现在就可以运行代理程序并输入所需的查询/提示。请注意，正如论文中所述，此处我们无需提供少量示例。

```python
agent.run(
	"Who is Olivia Wilde's boyfriend?"
	"What is his current age raised to the 0.23 power?"
)
```

链式执行如下所示：

```
> Entering new AgentExecutor chain...
 I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
Action: Search
Action Input: "Olivia Wilde boyfriend"
Observation: Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.
Thought: I need to find out Harry Styles' age.
Action: Search
Action Input: "Harry Styles age"
Observation: 29 years
Thought: I need to calculate 29 raised to the 0.23 power.
Action: Calculator
Action Input: 29^0.23
Observation: Answer: 2.169459462491557

Thought: I now know the final answer.
Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.

> Finished chain.
```


### Instruction Prompting and Tuning

指令微调通过使用（任务指令、输入、真实输出）的元组对预训练模型进行微调，使模型更好地与用户意图对齐并遵循指令。“在与指令模型交互时，我们应详细描述任务要求，力求具体和精确，明确说明要做什么（而不是说不要做什么）”。 [(source)](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#instruction-prompting).




-----------------

## 概述

与传统机器学习模型不同，LLMs 具有无需重新训练即可提供新颖见解的独特能力。这一创新催生了一场变革浪潮，使人们能够通过简单的文本提示轻松地对计算机进行编程。

提示工程是一种技术，它通过策略性的上下文提示，引导 LLM 产生特定结果，而无需改变模型的权重或参数。它涵盖了与人工智能有效沟通以获得预期结果的艺术。该方法适用于从问答到算术推理等一系列任务，旨在探索大语言模型的边界与潜力。

## Prompts

要理解提示工程的概念，首先必须了解什么是提示及其作用。提示词是提供给模型的初始文本输入，模型利用这些输入生成回答或完成任务。这些是提供给 AI 用于执行任务的指令集。它们的性质各不相同，包括摘要、算术问题解答，以及更常见的问答功能。

因此，提示工程的目标是优化这些提示，以提高模型输出的准确性和相关性。在接下来的部分，我们将探讨一些常见的提示类型，重点介绍两种最广泛使用的提示方法：零样本提示和小样本提示。

## Zero-shot Prompting

零样本学习是指在不提供任何示例的情况下，将任务输入模型以获取期望输出，因此得名“零样本”。例如，我们可以输入一个句子给模型，并期望它输出该句子的情感倾向。

```python
Classify the text into neutral, negative, or positive. 
Text: I think the vacation is okay.
```

Output：`Neutral`

## Few-shot Prompting

另一方面，Few-shot learning 需要为模型提供少量高质量示例，这些示例包含目标任务的输入和期望输出。通过观察这些优质样例，模型能更清晰地理解人类意图和生成准确输出的标准。因此，小样本学习通常比 Zero-shot learning 表现更优。不过这种方法会消耗更多 token，在处理长文本输入输出时可能会遇到上下文长度限制。

大型语言模型（如 GPT-3）在零样本学习方面表现出色。然而，当面对性能下降的复杂任务时，小样本学习就能派上用场！为了提高性能，我们通过在小样本提示中提供示范来指导模型执行任务，从而实现上下文学习。换句话说，通过让模型接触一系列特定任务的示例，有助于提升其表现。

```python
A "whatpu" is a small, furry animal native to Tanzania. 
An example of a sentence that uses the word whatpu is: 
We were traveling in Africa and we saw these very cute whatpus.
To do a "farduddle" means to jump up and down really fast. 
An example of a sentence that uses the word farduddle is:
```

翻译：“whatpu” 是一种原产于坦桑尼亚的小型毛茸茸动物。使用该词的例句为：我们在非洲旅行时看到了这些非常可爱的 whatpus。而 “farduddle” 意为快速上下跳跃。使用该词的例句为：

Output:  `When we won the game, we all started to farduddle in celebration.`

从上面的提示可以看出，模型被给了一个例子后，就能为下一个问题生成答案。

## 思维链 (CoT) Prompting

[Wei et al. (2022)](https://arxiv.org/abs/2201.11903) 提出的“思维链”（CoT）方法，使大型语言模型能够通过将复杂任务分解为必要的组成步骤来应对。简而言之，CoT 提示通过中间推理步骤实现了复杂的推理能力：
![|600](https://aman.ai/primers/ai/assets/prompt/cot.png)
你可以将其与少量样本提示相结合，在需要先进行推理再回答的更复杂任务上获得更好的结果。

```python
The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. 
A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.  

The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24. 
A: Adding all the odd numbers (17, 19) gives 36. The answer is True.  

The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24. 
A: Adding all the odd numbers (11, 13) gives 24. The answer is True.  

The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2. 
A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.  

The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.  
A:
```

Output：`Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.`

哇！当我们提供推理步骤时，可以看到一个完美的结果。实际上，我们甚至可以通过提供更少的例子来解决这个任务，也就是说，似乎一个例子就足够了。

请记住，作者声称这是 *随着语言模型规模足够大* 而涌现出的一种能力。

### Zero-shot CoT

[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916) 提出的 *“Let’s think step by step”*

![|600](https://aman.ai/primers/ai/assets/prompt/zero-cot.png)

```python
I went to the market and bought 10 apples. 
I gave 2 apples to the neighbor and 2 to the repairman. 
I then went and bought 5 more apples and ate 1. 
How many apples did I remain with?
```

Output：`11 apples`

答案不正确！现在让我们尝试使用特殊提示。

```python
...(同上)
Let's think step by step.
```

Output：`First, you started with 10 apples. You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left. Then you bought 5 more apples, so now you had 11 apples. Finally, you ate 1 apple, so you would remain with 10 apples.`

这个简单的提示能如此有效地完成任务，令人印象深刻。在缺乏大量示例用于提示的情况下，这种方法尤其实用。思维链（CoT）提示法会生成一系列被称为推理链的短句，并附带解释和预测对。这种方法不需要额外的训练或微调。这些描述了逐步推理的逻辑，最终得出答案，对于复杂的推理任务和更大的模型，可以看到更多的好处。我们将探讨两种基本的思维链提示技术，并在下文中进行详细说明。

### Few-shot CoT

Few-shot CoT 让模型能够看到一些高质量推理链的示范。

```python
Question: Tom and Elizabeth have a competition to climb a hill. 
Elizabeth takes 30 minutes to climb the hill. 
Tom takes four times as long as Elizabeth does to climb the hill. 
How many hours does it take Tom to climb up the hill? 
Answer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill. 
It takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill. 
So the answer is 2. 
=== 
Question: Jack is a soccer player. 
He needs to buy two pairs of socks and a pair of soccer shoes. 
Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. 
How much more money does Jack need? 
Answer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19. 
The total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111. 
Jack need $111 - $40 = $<<111-40=71>>71 more. 
So the answer is 71. 
=== 
Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. 
Each of the cut parts must be divided into 5 equal parts. 
How long will each final cut be? 
Answer:
```


## 自动思维链 (Auto-CoT)

在使用思维链提示法配合示例演示时，通常需要人工精心设计高效且多样化的示例。这种手工操作可能导致解决方案不够理想。[Zhang et al. (2022)](https://arxiv.org/abs/2210.03493) 提出了一种消除人工干预的方法——通过 LLM 结合 “Let’s think step by step” 的提示语，逐个生成演示所需的推理链条。但*这种自动化流程仍可能导致生成的推理链出现错误。为降低错误影响，示例的多样性至关重要*。本研究提出的 Auto-CoT 方案，通过 *筛选具有多样性的问题* 并生成相应推理链，以此构建演示示例。

Auto-CoT 包含两个主要阶段：

* 第一阶段：*问题聚类* ——将给定数据集中的问题划分为若干类别
* 第二阶段：*示例抽样* ——从每个集群中选取一个具有代表性的问题，并使用零样本思维链结合简单启发式方法生成其推理链。

简单的启发式方法可以是问题的长度（例如 60 个 tokens）和推理步骤的数量（例如 5 个推理步骤）。这鼓励模型使用简单而准确的演示。

![|600](https://aman.ai/primers/ai/assets/prompt/auto-cot.png)


## 自洽性 (Self-Consistency)

或许提示工程中更先进的技术之一是自洽性。由 [Wang et al. (2022)](https://arxiv.org/pdf/2203.11171.pdf) 提出的自洽性旨在“取代思维链提示中使用的简单贪婪解码”。其 *核心思路是通过少量示例的思维链，采样多条多样化的推理路径，并利用生成结果选择最一致的答案*。这种方法有助于提升思维链提示在算术和常识推理任务中的表现。

让我们尝试以下算术推理的例子：

```python
When I was 6 my sister was half my age. Now I’m 70 how old is my sister?
```

Output：`35`

输出是错误的！我们通过自洽性来改进这一点：

```python
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. 
After they are done, there will be 21 trees. 
How many trees did the grove workers plant today? 
A: We start with 15 trees. Later we have 21 trees. 
The difference must be the number of trees they planted. 
So, they must have planted 21 - 15 = 6 trees. The answer is 6.  

...(More exampeles)  

Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister? 
A:
```

Output 1：`When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.`

Output 2:  `When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.`

Output 3:  `When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70/2 = 35. The answer is 35.`

计算最终答案涉及几个步骤（详情请参阅论文），但为了简单起见，我们可以看到已经出现了一个多数答案，这基本上将成为最终答案。



## 思维树（Tree-of-Thoughts (ToT) Prompting）

对于需要探索或战略前瞻的复杂任务，传统的或简单的提示技术往往力不从心。[Yao et el. (2023)](https://arxiv.org/abs/2305.10601) 和 [Long (2023)](https://arxiv.org/abs/2305.08291)最近提出了"思维树"（ToT）框架，该框架推广了思维链提示法，并鼓励探索作为语言模型通用问题解决中间步骤的思维。简而言之，以 ToT 方式将大语言模型的思维组织成节点，使我们能够将思维与符号树搜索算法相结合。这为最优"思维路径"的选择打开了大门，将大语言模型的规划提升到了更复杂的水平。

ToT 维护着一个思维树结构，其中每个思维代表连贯的语言序列，作为解决问题的中间步骤。这种方法使语言模型能够通过深思熟虑的推理过程，自我评估中间思维在解决问题过程中的进展。语言模型生成和评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，从而实现对思维的前瞻性和回溯性系统探索。

![|600](https://aman.ai/primers/ai/assets/prompt/ToT1.webp)

在使用 ToT 时，不同的任务需要定义候选方案的数量和思考/步骤的数量。例如，论文中展示的 24 点游戏被用作数学推理任务，需要将思考分解为 3 个步骤，每个步骤都涉及一个中间方程。在每一步中，保留最佳的 $b=5$ 个候选方案

在“24点游戏”任务中执行树状思维广度优先搜索（BFS）时，语言模型会被提示将每个候选思路评估为“确定/可能/不可能”达到 24。正如作者所述：“目的是推广那些经过少量前瞻性试验即可验证的正确局部解，基于‘过大/过小’的常识排除不可能的局部解，并将其余解保留为‘可能’状态。”每个思路会进行 3 次数值采样。该流程如下图所示：

![|600](https://aman.ai/primers/ai/assets/prompt/ToT2.webp)

根据下图所示的结果，思维树（ToT）方法显著优于其他提示方法：

![|600](https://aman.ai/primers/ai/assets/prompt/ToT3.webp)

从高层次来看，[Yao et el. (2023)](https://arxiv.org/abs/2305.10601) 和 [Long (2023)](https://arxiv.org/abs/2305.08291) 的核心思想是相似的。两者都通过多轮对话的树搜索来增强 LLM 解决复杂问题的能力。主要区别之一是，Yao 利用了深度优先搜索（DFS）/广度优先搜索（BFS）/束搜索（beam search），而 Long 提出的树搜索策略（即何时回溯以及回溯多少层等）是由通过强化学习训练的"思维树控制器"（ToT Controller）驱动的。DFS/BFS/束搜索是通用的解决方案搜索策略，没有针对特定问题进行适配。相比之下，通过强化学习训练的思维树控制器可能能够从新数据集或通过自我对弈（如 AlphaGo 与暴力搜索的对比）中学习，因此基于强化学习的思维树系统即使在固定的大语言模型下也能持续进化并学习新知识。


### ToT 与 CoT 有何区别？哪种更好？为什么？

“思维链”提示法和“思维树”提示法是用于提升 GPT-3 或 GPT-4 等大型语言模型性能的方法，尤其适用于需要多步推理或问题解决的复杂任务。

**思维链提示：**

* *原则*：在“思维链”提示中，用户会写出可能导致答案的中间步骤或推理过程。这种方法帮助模型“出声思考”或遵循一系列逻辑步骤来得出结论。
* *用途*：它特别适用于解决数学应用题等复杂问题，仅陈述问题本身无法为模型提供足够的指导。通过引入推理链条，可以鼓励模型遵循类似的逐步推理方法。
* *示例*：对于数学问题，提示将包括问题和如何解决它的详细顺序解释，引导模型完成推理过程。

**思维树提示：**

* *原则：​*​ 思维树提示是一种更为复杂的方法，其中并行考虑多条推理线索。这就像创建一个决策树，每个分支代表不同的思维路径或问题的不同方面。
* *用法*：此方法适用于可能存在多种有效解决方案的问题，或当问题领域涉及处理分支可能性和结果时。
* *示例*：在一个涉及多个变量或可能结果的复杂场景中，提示内容会包含对这些不同路径的探讨，例如在科学问题中考虑某一现象的不同可能原因。

**比较与效果：**

* *复杂度：​*​ 思维树本质上更为复杂，因为它需要同时考虑多条推理线索。虽然更全面，但有效构建起来也更具挑战性。
* *适用性*：思维链通常更为直接，适用于广泛的问题，尤其是在需要线性、逐步方法的情况下。思维树更适合具有分支可能性的场景，需要考虑多种因素或结果。
* *效率*：对于较简单的问题，思维链通常更高效，因为它更直接。而对于更复杂、多层面的问题，思维树可能会更全面地探索问题空间。
* *哪个更好？*：选择取决于任务的性质。对于大多数直接的解决问题任务，思维链已经足够且更易于管理。思维树更适合需要评估不同假设或场景的复杂、多维问题。

总之，这两种方法都旨在通过引导语言模型遵循更有条理的思维过程来提升其推理能力。具体选择哪一种方法，应根据当前问题的具体需求来决定。


## 思维图（Graph-of-Thought (GoT) Prompting）

随着 LLMs 在自然语言处理任务中的广泛应用，研究人员发现思维链（CoT）通过生成中间步骤来帮助 LLMs 完成复杂推理任务的潜力。然而，人类的思维过程往往是非线性的，而不仅仅是简单的顺序思维链。

[Yao et al. (2023)](https://arxiv.org/abs/2305.16582) 提出了思维图（GoT）推理方法，该方法将人类思维过程建模不仅为链式结构，还扩展为图结构。通过将思维单元表示为节点、单元间的关联表示为边，该研究捕捉了人类思维的非线性特征，实现了更贴近现实的思维过程建模。与多模态思维链（Multimodal-CoT）类似，他们将 GoT 推理建模为两阶段框架：首先生成推理依据，随后产生最终答案。具体而言，他们采用额外的思维图编码器进行表征学习，并通过门控融合机制将思维图表征与原始输入表征相结合。

> 有向无环图（DAG）通过建立无循环依赖关系的流程图，彻底革新了数据流水线编排工具。与树状结构不同，DAG 能够对分叉后又重新汇聚的路径进行建模，这使得它在处理复杂任务时比树状结构更具优势。

思维图谱方法超越了纯粹的编排，将所有信息以图结构呈现，其中节点代表概念或实体，边表示它们之间的关系。每个节点包含可由大语言模型处理的信息，而节点间的连接则捕捉了上下文关联和依赖关系。这种图结构使模型能够遍历和探索概念间的联系，从而促进对输入信息更细致的理解，并有助于制定更具逻辑一致性的计划（类似于人类大脑中的 [System One and System Two thinking](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555)）。

他们在 T5 预训练模型上实现了 GoT 推理模型，并评估了其在纯文本推理任务（GSM8K）和多模态推理任务（ScienceQA）上的表现。

他们的模型在 GSM8K 测试集上分别采用 T5-base 和 T5-large 架构时，相较强大的思维链基线实现了 3.41% 和 5.08% 的显著提升。此外，在 ScienceQA 测试集上，我们的模型将 T5-base 模型的准确率从 84.91% 提升至 91.54%，T5-large 模型从 91.68% 提升至 92.77%，超越了当前最先进的多模态思维链方法。实验表明，尽管 GoT 主干模型参数不足 2.5 亿，却能取得与参数超 7 亿的多模态思维链（大型）相媲美的效果，充分证明了 GoT 的高效性。

![|600](https://aman.ai/primers/ai/assets/prompt/GoT.jpg)

### ToT 提示与 GoT 提示有何区别？哪种更好，为什么？

1. CoT 的缺点：CoT 提示法无法解决那些需要规划、战略前瞻、回溯以及并行探索多种解决方案的问题。
2. 结构方法：
	* 思维树（ToT）提示法将推理过程构建为一棵树结构，其中每个节点代表一个"思维"或从复杂问题中分解出的更简单子问题。简而言之，ToT 提示法将复杂问题拆解为一系列简单问题（或称"思维"）。解决过程会分支出这些简单问题，允许进行策略规划、前瞻和回溯。大语言模型会生成众多思维，并通过自然语言（即提示）持续评估向最终解决方案的进展。通过利用模型对解题进度的自我评估，我们可以借助广泛使用的搜索算法（如广度优先搜索或深度优先搜索）来驱动探索过程，从而在解决问题时实现前瞻/回溯功能。
	* 另一方面，GoT 提示将 ToT 提示的研究推广至基于图的推理策略。因此，GoT 采用图结构而非树结构。这意味着思维路径并非严格线性或层级化的——思考节点可被重新访问、重复利用，甚至形成递归循环。该方法认识到推理过程可能需要回溯早期阶段，或以非线性方式整合思维节点。
3. 灵活性与复杂性：
	* ToT 的进展在某种程度上是线性的，尽管它允许分支。这使得它适用于那些可以通过结构化分解为子问题，从而逻辑性地从一个步骤推进到另一个步骤，直到找到解决方案的问题。
	* GoT 提供了更大的灵活性，因为它可以容纳思想之间更复杂的关系，从而实现更动态和相互关联的推理。这在问题解决过程并非直截了当、且可能受益于重新审视和重新评估先前思想的场景中尤为有用。因此，GoT 并不假设用于生成解决方案的思想路径是线性的。在推导解决方案时，我们可以重复使用思想，甚至通过一系列思想进行递归。- *哪个更好，为什么*？选择ToT 还是 GoT 提示取决于手头问题的性质：
		* 对于适合线性或分层分解的问题，其中每个子问题都明确导向下一个问题时，ToT 提示法可能更有效且高效。它通过保持清晰的结构和路径来简化问题解决过程。
		* 对于更复杂的问题，其解决方案可能需要重新审视先前的想法或以非线性方式结合见解，GoT 提示法可能更具优势。它通过利用图中思想的灵活性和相互关联性，能够更全面地探索潜在解决方案。
	* 需要注意的是，最终这两种方法本身并无绝对的“优劣”之分，因为它们的有效性取决于具体情境。在选择时应以所解决问题的具体需求和特性为指导。此外，针对两种方法因可能需要大量推理步骤而导致实用性受限的批评表明，要高效部署这些技术仍存在需要克服的挑战。
	* 值得一提的是，这些*基于树形/图结构的提示技术因缺乏实用性而备受争议*。使用思维树/思维图提示法解决推理问题时，可能需要大语言模型*执行海量的推理步骤*！

## 思维骨架（Skeleton-of-Thought Prompting）

由清华大学与微软研究院等人提出 [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://arxiv.org/abs/2307.15337)。SoT 旨在通过解决最先进 LLMs 固有的顺序解码过程，来降低其端到端生成延迟。SoT 鼓励 LLMs 首先勾勒出答案的骨架，然后并行填充每个要点的细节，利用 API 调用或批量解码以提高效率。

SoT 的方法论在构建和阐述回答时模拟了人类的认知过程，旨在使大型语言模型的处理更加直观和高效。该方法在 12 个模型上实现了显著的速度提升（最高达 2.39倍），展示了效率的改善，并且在许多情况下，提高了包括知识类、通用类、常识类、角色扮演类和反事实类问题在内的多种问题类型的回答质量。

该方法在两个数据集（Vicuna-80 和 WizardLM）上使用多种大语言模型进行了评估，不仅显示出效率提升，还展现了答案多样性和相关性的潜在改进，表明 SoT 能够引导大语言模型实现更接近人类的推理和表达方式。

论文中的下图展示了 SoT 的示意图。与传统方法按顺序生成答案不同，SoT 通过并行生成答案的不同部分来加速这一过程。具体来说，给定问题后，SoT 首先提示大语言模型生成框架，然后进行批量解码或并行 API 调用来同时扩展多个要点，最后将输出结果汇总以得到最终答案。

![|600](https://aman.ai/images/papers/SoT.jpg)

SoT 在保持（甚至提升）多种问题类型的回答质量的同时，能显著提高处理速度。然而，其最大局限在于*不适用于需要逐步推理的问题*。为推动 SoT 的实际应用，研究者探索了仅在适合场景下自适应触发 SoT 的可能性。为此，他们提出了路由模块——带路由器的 SoT（SoT-R），该模块能判断是否对用户请求应用 SoT，并相应调用 SoT 或常规解码流程。简言之，SoT-R 通过路由机制识别适用问题，实现 SoT 的自适应触发。这种范式也符合当前组合多模型解决复杂任务的趋势。在路由器实现方面，他们探索了两种方案：无需模型训练的 LLM 提示路由，以及基于 RoBERTa 训练的路由器。SoT-R 通过集成路由机制，选择性对适用问题应用 SoT，从而优化响应速度与质量。这一扩展展现了基于问题特征进行自适应应用的潜力，进一步巩固了 SoT 作为大语言模型推理效率数据级优化先驱策略的地位。

与传统模型和系统级优化不同，SoT 和 SoT-R 代表了提升大语言模型能力的创新举措，强调生成响应的效率和质量，并突显了使大语言模型流程更贴近人类思维模式的潜力。[Blog](https://www.microsoft.com/en-us/research/blog/skeleton-of-thought-parallel-decoding-speeds-up-and-improves-llm-output/)

## 验证链（Chain-of-Verification (CoVe)）

大型语言模型中存在的“幻觉”现象，即生成看似合理实则错误的事实信息，仍是一个悬而未决的问题。
[Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495) by Dhuliawala et al. from Meta AI 和 ETH Zurich 研究语言模型通过反思自身给出的回应来纠正错误的能力。

他们开发了 CoVe 方法，具体流程如下：(i) 模型首先生成初步回答；(ii)设计验证性问题来核查草稿的准确性；(iii)独立回答这些问题以避免其他答案的干扰；(iv) 最终生成经过验证的确定答案。

论文中的下表展示了 CoVe 方法。给定用户查询时，大型语言模型会生成可能包含不准确信息（如事实性幻觉）的基线回答。此处我们展示了一个 ChatGPT 未能正确回答的查询案例（详见第 9 节）。为改进这一点，CoVe 首先生成一组验证问题的计划，随后通过回答这些问题来执行验证流程并检查一致性。我们发现单个验证问题的回答准确率通常高于原始长文本生成中事实的初始准确率。最终修订后的回答会整合验证结果。CoVe 的分解版本在回答验证问题时不会受原始回答影响，从而避免重复并提升性能。

![|600](https://aman.ai/images/papers/CoVe.jpg)

通过实验，他们证明 CoVe 在各种任务中减少了幻觉现象，包括来自维基数据的列表式问题、闭卷 MultiSpanQA 以及长文本生成任务。


## 多模态 CoT Prompting

大型语言模型（LLMs）通过利用思维链（CoT）提示生成中间推理链作为推断答案的依据，在复杂推理任务上展现出令人印象深刻的性能。然而，现有的 CoT 研究主要集中在语言模态上。[Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) by Zhang et al. from Shanghai Jiao Tong University and Amazon Web Services, 多模态思维链通过融合语言（文本）和视觉（图像）模态，解决了当前大语言模型中思维链研究的局限性。

Multimodal-CoT 是一种创新的两阶段框架，旨在增强大型语言模型（LLMs）的复杂推理能力。该方法首先利用文本和图像共同生成推理依据，随后借助这些增强后的依据进行更精确的答案推导。这一技术路径显著区别于现有仅关注语言模态的思维链（CoT）研究。

![|500](https://aman.ai/images/papers/MultimodalCoT.jpg)

论文中的下图展示了其多模态思维链框架的概览。该框架包含两个阶段：（i）依据生成阶段和（ii）答案推理阶段。两个阶段共享相同的模型架构，但在输入输出上存在差异。第一阶段中，模型接收语言和视觉输入来生成推理依据；第二阶段则将原始语言输入与第一阶段生成的依据相结合，再将更新后的语言输入与原始视觉输入共同馈入模型，最终推导出答案。

![|600](https://aman.ai/images/papers/MultimodalCoT2.jpg)

作者们证明，他们的模型虽然参数不足 10 亿，但在 ScienceQA 基准测试中显著超越了当前最先进的大型语言模型 GPT-3.5。通过实现 16 个百分点的准确率提升（从 75.17% 提高到 91.68%），多模态思维链不仅超越了GPT-3.5，甚至超过了人类的表现水平。

该论文详细分析了模型的架构，重点介绍了如何利用微调的语言模型有效融合视觉与语言表征。这是为后续推理阶段生成更具信息量的解释的关键组成部分。实证评估证明了该模型在理由生成和答案准确性两方面的有效性，展示了其在传统思维链推理可能失效的场景中的优越性。

作者们将 Multimodal-CoT 与其他模型和基线进行了比较，强调了它为多模态推理任务带来的显著进步。本文还探讨了 Multimodal-CoT 的潜在应用和未来改进方向，特别是在增强语言与视觉特征的交互以及整合更先进的视觉提取技术方面。总的来说，这篇论文代表了大型语言模型在多模态推理领域的重大突破，展示了语言与视觉模态的融合如何显著提升推理与理解能力。[Code](https://github.com/amazon-science/mm-cot)

## 思维可视化提示

微软研究院吴等人团队在 [Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2404.03622) 一文中提出的思维可视化（VoT）提示法，是一种旨在增强 LLMs 空间推理能力的新颖方法。研究者在自然语言导航、视觉导航及二维网格世界中的视觉平铺等多跳空间推理任务中应用了 VoT 技术。该方法通过可视化 LLMs 的推理轨迹，引导模型完成后续推理步骤。研究表明，这种使 LLMs 生成辅助空间推理的内心意象的方法，能高度模拟人类"心灵之眼"的认知过程，在空间推理任务中显著超越现有多模态大语言模型的表现。

论文中的下图说明，人类在空间推理过程中通过构建心理图像可以增强空间感知能力并辅助决策。类似地，大型语言模型（LLMs）也能生成内部心理图像。我们提出的视觉思维链（VoT）提示法，通过让 LLMs 在每个中间步骤可视化其思维过程，从而激发其空间推理所需的"心灵之眼"。

主要贡献包括从认知角度揭示大语言模型的心理意象能力，开发了两个模拟人类感官感知的合成数据集任务，并提出了 VoT 提示法，该方法在实证中优于其他提示方法。值得注意的是，VoT 采用零样本提示，与少样本演示或基于 CLIP 的可视化方法形成对比，从而更好地匹配人类抽象可视化空间信息的认知能力。

在实施方面，作者采用了零样本方法，其中 VoT 通过视觉空间画板增强大型语言模型，无需事先对类似任务进行明确训练即可可视化推理步骤。该方法使用 GPT-4 和 GPT-4V 等模型进行了测试，实验通过 Azure OpenAI API 进行，重点关注自然和视觉导航任务以及复杂的视觉平铺场景。

结果证实了 VoT 的有效性，与其他方法相比，它持续引导大语言模型（LLMs）可视化其推理步骤，并提高了空间任务的表现。论文总结指出，虽然 VoT 在增强 LLMs 的空间推理能力方面展现出显著潜力，但它也可能将这些能力扩展到多模态大语言模型（MLLMs）中，这表明它在需要高级认知和推理技能的任务中具有更广泛的适用性。



## Least-to-Most Prompting

[Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625v3) 中提出的"最少到最多提示法"是一种新颖的提示方法，旨在增强 GPT-3 等大型语言模型的问题解决能力。这种借鉴教育心理学的方法，通过将复杂问题分解为更简单的顺序子问题，并利用先前子问题的答案来促进后续问题的解决。

最小到最多提示的实施不需要模型训练或微调。它完全通过少量示例提示来执行，在符号操作、组合泛化和数学推理等任务中得到了有效展示。

论文中的下图展示了分步提示法在两个阶段解决数学应用题的过程：(1) 查询语言模型将问题分解为子问题；(2) 查询语言模型依次解决子问题。第二个子问题的答案建立在第一个子问题答案的基础上。本示意图省略了每个阶段提示的演示示例。

![|600](https://aman.ai/images/papers/LTM.jpg)

在实证评估中，最少到最多提示法显著优于传统的思维链提示法，尤其是在处理需要从简单到复杂问题解决场景进行泛化的任务时。例如，在组合泛化基准测试 SCAN 中，最少到最多提示法仅用 14 个示例就达到了 99% 的准确率，而传统方法的准确率仅为 16%。

值得注意的是，该方法在长度泛化任务中表现出显著的效率，随着测试案例复杂度的增加，它仍能保持较高的性能，而其他提示方法则表现出性能急剧下降的情况。该论文还探讨了将这种方法与自我一致性等其他提示技术相结合的方式，强调了其在多样化问题解决场景中的灵活性和有效性，而无需额外的计算成本或复杂的模型修改。


## ReAct Prompting

[Yao et al., 2022](https://arxiv.org/abs/2210.03629)提出，该框架以交错方式利用大语言模型生成 *推理轨迹* 和 *任务特定动作*。生成推理轨迹使模型能够归纳、跟踪和更新行动计划，甚至处理异常情况。行动步骤允许 *与知识库或环境等外部来源交互* 并从中收集信息。结果表明，ReAct 在语言和决策任务上的表现优于多个最先进的基线模型。ReAct 还提升了大型语言模型的可解释性和可信度，使其更易于人类理解与信任。总体而言，作者发现最佳方法是结合思维链（CoT）使用 ReAct，这种方法能够在推理过程中同时利用内部知识和外部获取的信息。

### ReAct  工作原理

ReAct 的灵感来源于“行动”与“推理”之间的协同作用，这种协同作用使人类能够学习新任务并做出决策或推理。思维链（CoT）提示技术已证明大语言模型具备通过推理追踪来生成涉及算术与常识推理等问题答案的能力。但其无法接入外部世界或更新知识的缺陷，可能导致事实幻觉和错误传播等问题。

ReAct 是一种结合推理与行动的大型语言模型通用范式。该框架通过提示语言模型生成任务相关的语言推理轨迹和具体行动，使系统能够动态执行推理以创建、维护和调整行动计划，同时支持与外部环境（如维基百科）交互，从而将额外信息融入推理过程。下图（来源）展示了 ReAct 在问答任务中涉及的各个步骤示例。

![|500](https://aman.ai/primers/ai/assets/prompt/react.png)

请注意，上下文示例也被添加到提示中，但为了简单起见，我们在此将其排除。我们可以看到模型生成了*任务解决轨迹*（思考、行动）。Obs 对应于与之交互的环境的观察（例如，搜索引擎）。本质上，ReAct 可以检索信息以支持推理，而推理则有助于确定下一步要检索的内容。

### ReAct Prompting

为了展示 ReAct 提示法的工作原理，让我们以论文中的一个例子为例。第一步是从训练集（如HotPotQA）中选择案例，并编写 ReAct 格式的轨迹。这些轨迹将作为提示中的少量示例。如上图所示，轨迹由多个思考-行动-观察步骤组成。自由形式的思考用于完成不同任务，如分解问题、提取信息、进行常识/算术推理、指导搜索公式化以及综合最终答案。

以下是 ReAct 提示示例的一个例子（从论文中获取并简化为一个示例以便理解）：

```python
Question: What is the elevation range for the area that the eastern sector 
          of the Colorado orogeny extends into? 
Thought 1: I need to search Colorado orogeny, find the area that the eastern sector of 
          the Colorado orogeny extends into, then find the elevation range of the area.
Action 1: Search[Colorado orogeny] 
Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in 
          Colorado and surrounding areas. 

Thought 2: It does not mention the eastern sector. So I need to look up eastern sector. 
Action 2: Lookup[eastern sector] \
Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is 
         called the Central Plains orogeny. 

Thought 3: The eastern sector of Colorado orogeny extends into the High Plains. 
         So I need to search High Plains and find its elevation range. 
Action 3: Search[High Plains] 
Observation 3: High Plains refers to one of two distinct land regions 

Thought 4: I need to instead search High Plains (United States). 
Action 4: Search[High Plains (United States)] 
Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the 
         High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3] 

Thought 5: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer 
         is 1,800 to 7,000 ft. 
Action 5 Finish[1,800 to 7,000 ft] ...
```

请注意，不同类型的任务会采用不同的提示设置。对于以推理为主要需求的任务（如 HotpotQA），任务解决轨迹会包含多个思考-行动-观察步骤。而对于涉及大量行动步骤的决策任务，则较少使用思考环节。


### Results on Knowledge-Intensive Tasks

- The paper first evaluates ReAct on knowledge-intensive reasoning tasks such as question answering (HotPotQA) and fact verification ([Fever](https://fever.ai/resources.html)), as shown in the figure below ([source](https://arxiv.org/abs/2210.03629)). PaLM-540B is used as the base model for prompting.

![](https://aman.ai/primers/ai/assets/prompt/table1.png)

- The prompting results on HotPotQA and Fever using different prompting methods show that ReAct generally performs better than Act (involves acting only) on both tasks.
    
- We can also observe that ReAct outperforms CoT on Fever and lags behind CoT on HotpotQA. A detailed error analysis is provided in the paper. In summary:
    
    - CoT suffers from fact hallucination
    - ReAct’s structural constraint reduces its flexibility in formulating reasoning steps
    - ReAct depends a lot on the information it’s retrieving; non-informative search results derails the model reasoning and leads to difficulty in recovering and reformulating thoughts
- Prompting methods that combine and support switching between ReAct and CoT+Self-Consistency generally outperform all the other prompting methods.
    

### Results on Decision Making Tasks

- The paper also reports results demonstrating ReAct’s performance on decision making tasks. ReAct is evaluated on two benchmarks called [ALFWorld](https://alfworld.github.io/) (text-based game) and [WebShop](https://webshop-pnlp.github.io/) (online shopping website environment). Both involve complex environments that require reasoning to act and explore effectively.
    
- Note that the ReAct prompts are designed differently for these tasks while still keeping the same core idea of combining reasoning and acting. Below is an example ([source](https://arxiv.org/abs/2210.03629)) for an ALFWorld problem involving ReAct prompting.
    

![](https://aman.ai/primers/ai/assets/prompt/alfworld.png)

- ReAct outperforms Act on both ALFWorld and Webshop. Act, without any thoughts, fails to correctly decompose goals into subgoals. Reasoning seems to be advantageous in ReAct for these types of tasks but current prompting-based methods are still far from the performance of expert humans on these tasks.

### ReAct Usage with LangChain

- Below is a high-level example of how the ReAct prompting approach works in practice. We will be using OpenAI for the LLM and [LangChain](https://python.langchain.com/en/latest/index.html) as it already has built-in functionality that leverages the ReAct framework to build agents that perform tasks by combining the power of LLMs and different tools.
    
- First, let’s install and import the necessary libraries:
    

![](https://aman.ai/images/copy.png)

`%%capture # update or install the necessary libraries !pip install --upgrade openai !pip install --upgrade langchain !pip install --upgrade python-dotenv !pip install google-search-results  # import libraries import openai import os from langchain.llms import OpenAI from langchain.agents import load_tools from langchain.agents import initialize_agent from dotenv import load_dotenv load_dotenv()  # load API keys; you will need to obtain these if you haven't yet os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")`

- Now we can configure the LLM, the tools we will use, and the agent that allows us to leverage the ReAct framework together with the LLM and tools. Note that we are using a search API for searching external information and LLM as a math tool.

![](https://aman.ai/images/copy.png)

`llm = OpenAI(model_name="text-davinci-003" ,temperature=0) tools = load_tools(["google-serper", "llm-math"], llm=llm) agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)`

- Once that’s configured, we can now run the agent with the desired query/prompt. Notice that here we are not expected to provide few-shot exemplars as explained in the paper.

![](https://aman.ai/images/copy.png)

`agent.run("Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?")`

- The chain execution looks as follows:

![](https://aman.ai/images/copy.png)

`> Entering new AgentExecutor chain...  I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power. Action: Search Action Input: "Olivia Wilde boyfriend" Observation: Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline. Thought: I need to find out Harry Styles' age. Action: Search Action Input: "Harry Styles age" Observation: 29 years Thought: I need to calculate 29 raised to the 0.23 power. Action: Calculator Action Input: 29^0.23 Observation: Answer: 2.169459462491557  Thought: I now know the final answer. Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.  > Finished chain.`

- The output we get is as follows:

![](https://aman.ai/images/copy.png)

`"Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557."`

- We adapted the example from the [LangChain documentation](https://python.langchain.com/docs/modules/agents/agent_types/react), so credit goes to them. We encourage the learner to explore different combination of tools and tasks.

## Active-Prompt

- Chain-of-thought (CoT) methods rely on a fixed set of human-annotated exemplars. The problem with this is that the exemplars might not be the most effective examples for the different tasks. To address this, [Diao et al., (2023)](https://arxiv.org/pdf/2302.12246.pdf) recently proposed a new prompting approach called Active-Prompt to adapt LLMs to different task-specific example prompts (annotated with human-designed CoT reasoning).
    
- Below is an illustration ([source](https://arxiv.org/abs/2302.12246)) of the approach. The first step is to query the LLM with or without a few CoT examples. _k_ possible answers are generated for a set of training questions. An uncertainty metric is calculated based on the _k_ answers (disagreement used). The most uncertain questions are selected for annotation by humans. The new annotated exemplars are then used to infer each question.
    

![](https://aman.ai/primers/ai/assets/prompt/active-prompt.png)

## Instruction Prompting and Tuning

- Instruction prompting is by far the most common usecase of LLMs, especially chatbots such as ChatGPT. As an example of instruction prompting:
    
- _Prompt:_
    

![](https://aman.ai/images/copy.png)

`Define Onomatopoeia in one sentence.`

- _Output:_

![](https://aman.ai/images/copy.png)

`Onomatopoeia is the use of words that imitate or suggest the natural sound of a thing or action.`

- Instruction tuning seeks to offer instruction prompt examples to the LLM so it can close the train-test discrepancy (where the model was trained on web-scale corpora and tested mostly on instructions) and mimic the real-world usage scenario of chatbots. Stanford’s [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) is a recent example that uses instruction tuning to offer performance similar to OpenAI’s GPT3.5 (without performing RLHF, unlike GPT3.5).
- Instruction tuning finetunes a pretrained model with tuples of (task instruction, input, ground truth output) to enables the model to be better aligned to user intention and follow instructions. “When interacting with instruction models, we should describe the task requirement in detail, trying to be specific and precise, clearly specifying what to do (rather than saying not to do something)” [(source)](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#instruction-prompting).

## Recursive Prompting

- Recursive prompting refers to a method of problem-solving that involves breaking down a complex problem into smaller, more manageable sub-problems, which are then solved recursively through a series of prompts.
- This approach can be particularly useful for tasks that require compositional generalization, where a language model must learn to combine different pieces of information to solve a problem.
- In the context of natural language processing, recursive prompting can involve using a few-shot prompting approach to decompose a complex problem into sub-problems, and then sequentially solving the extracted sub-problems using the solution to the previous sub-problems to answer the next one. This approach can be used for tasks such as math problems or question answering, where a language model needs to be able to break down complex problems into smaller, more manageable parts to arrive at a solution.
- The language model can then solve each sub-problem independently or sequentially, using the solution to the previous sub-problem to answer the next one. For example:
    
- _Prompt:_

![](https://aman.ai/images/copy.png)

`Calculate the product of the length and width: prompt: "What is the product of 8 and 6?" answer: 48  Substitute the given values for length and width into the equation: prompt: "What is the area of a rectangle with length 8 and width 6?"`

- _Output_:

![](https://aman.ai/images/copy.png)

`answer: "The area of a rectangle with length 8 and width 6 is 48."`

- The following image [(source)](https://arxiv.org/pdf/2302.07842.pdf) shows multiple examples of recursive prompting:

![](https://aman.ai/primers/ai/assets/prompt/1.png)

## Automatic Prompt Engineer (APE)

- [Zhou et al., (2022)](https://arxiv.org/abs/2211.01910) propose automatic prompt engineer (APE) a framework for automatic instruction generation and selection, as illustrated in the figure below [(source)](https://arxiv.org/abs/2211.01910).

![](https://aman.ai/primers/ai/assets/prompt/APE.png)

- The instruction generation problem is framed as natural language synthesis addressed as a black-box optimization problem using LLMs to generate and search over candidate solutions.
    
- The first step involves a large language model (as an inference model) that is given output demonstrations to generate instruction candidates for a task. These candidate solutions will guide the search procedure. The instructions are executed using a target model, and then the most appropriate instruction is selected based on computed evaluation scores.
    
- APE discovers a better zero-shot CoT prompt than the human engineered “Let’s think step by step” prompt ([Kojima et al., 2022](https://arxiv.org/abs/2205.11916)).
    
- The prompt “Let’s work this out in a step by step way to be sure we have the right answer.” elicits chain-of-thought reasoning and improves performance on the MultiArith and GSM8K benchmarks [(source)](https://arxiv.org/abs/2211.01910):
    

![](https://aman.ai/primers/ai/assets/prompt/ape-zero-shot-cot.png)

- This paper touches on an important topic related to prompt engineering which is the idea of automatically optimizing prompts. While we don’t go deep into this topic in this guide, here are a few key papers if you are interested in the topic:
    - [AutoPrompt](https://arxiv.org/abs/2010.15980) - proposes an approach to automatically create prompts for a diverse set of tasks based on gradient-guided search.
    - [Prefix Tuning](https://arxiv.org/abs/2101.00190) - a lightweight alternative to fine-tuning that prepends a trainable continuous prefix for NLG tasks.
    - [Prompt Tuning](https://arxiv.org/abs/2104.08691) - proposes a mechanism for learning soft prompts through backpropagation.

## Automatic Reasoning and Tool-use (ART)

- Combining CoT prompting and tools in an interleaved manner has shown to be a strong and robust approach to address many tasks with LLMs. These approaches typically require hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. [Paranjape et al., (2023)](https://arxiv.org/abs/2303.09014) propose a new framework that uses a frozen LLM to automatically generate intermediate reasoning steps as a program.
    
- ART works as follows:
    - given a new task, it select demonstrations of multi-step reasoning and tool use from a task library
    - at test time, it pauses generation whenever external tools are called, and integrate their output before resuming generation
- ART encourages the model to generalize from demonstrations to decompose a new task and use tools in appropriate places, in a zero-shot fashion. In addition, ART is extensible as it also enables humans to fix mistakes in the reasoning steps or add new tools by simply updating the task and tool libraries. The process is demonstrated below [(source)](https://arxiv.org/abs/2303.09014):

![](https://aman.ai/primers/ai/assets/prompt/ART.png)

- ART substantially improves over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and exceeds performance of hand-crafted CoT prompts when human feedback is incorporated.
- Below is a table [(source)](https://arxiv.org/abs/2303.09014) demonstrating ART’s performance on BigBench and MMLU tasks:

![](https://aman.ai/primers/ai/assets/prompt/ART2.png)

## Retrieval Augmented Generation (RAG)

- General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don’t require additional background knowledge.
- For more complex and knowledge-intensive tasks, it’s possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of “hallucination”.
- Meta AI researchers introduced a method called [Retrieval Augmented Generation (RAG)](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and it’s internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.
- RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs’s parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.
- Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is an overview [(source)](https://arxiv.org/pdf/2005.11401) of how the approach works:

![](https://aman.ai/primers/ai/assets/prompt/rag.png)

- RAG performs strong on several benchmarks such as [Natural Questions](https://ai.google.com/research/NaturalQuestions), [WebQuestions](https://paperswithcode.com/dataset/webquestions), and CuratedTrec. RAG generates responses that are more factual, specific, and diverse when tested on MS-MARCO and Jeopardy questions. RAG also improves results on FEVER fact verification.
- This shows the potential of RAG as a viable option for enhancing outputs of language models in knowledge-intensive tasks.
- More recently, these retriever-based approaches have become more popular and are combined with popular LLMs like ChatGPT to improve capabilities and factual consistency.
- You can find a [simple example of how to use retrievers and LLMs for question answering with sources](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html) from the LangChain documentation.
- For a detailed discourse on RAG, please refer to our [RAG](https://aman.ai/primers/ai/RAG) primer.

### The “Needle in a Haystack” Test

- To understand the in-context retrieval ability of long-context LLMs over various parts of their prompt, a simple ‘needle in a haystack’ analysis could be conducted. This method involves embedding specific, targeted information (the ‘needle’) within a larger, more complex body of text (the ‘haystack’). The purpose is to test the LLM’s ability to identify and utilize this specific piece of information amidst a deluge of other data.
- In practical terms, the analysis could involve inserting a unique fact or data point into a lengthy, seemingly unrelated text. The LLM would then be tasked with tasks or queries that require it to recall or apply this embedded information. This setup mimics real-world situations where essential details are often buried within extensive content, and the ability to retrieve such details is crucial.
- The experiment could be structured to assess various aspects of the LLM’s performance. For instance, the placement of the ‘needle’ could be varied—early, middle, or late in the text—to see if the model’s retrieval ability changes based on information location. Additionally, the complexity of the surrounding ‘haystack’ can be modified to test the LLM’s performance under varying degrees of contextual difficulty. By analyzing how well the LLM performs in these scenarios, insights can be gained into its in-context retrieval capabilities and potential areas for improvement.
- This can be accomplished using the [Needle In A Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) library. The following plot shows OpenAI’s GPT-4-128K’s (top) and (bottom) performance with varying context length.

![](https://aman.ai/primers/ai/assets/RAG/GPT4_haystack.jpg)

![](https://aman.ai/primers/ai/assets/RAG/Claude_haystack.jpg)

- However, in their [Long context prompting for Claude 2.1](https://www.anthropic.com/index/claude-2-1-prompting) blog, Anthropic noted that adding “Here is the most relevant sentence in the context:” to the start of Claude’s response raised the score from 27% to 98% on the original evaluation!
- The figure below from the blog shows that Claude 2.1’s performance when retrieving an individual sentence across its full 200K token context window. This experiment uses a prompt technique to guide Claude in recalling the most relevant sentence.

![](https://aman.ai/primers/ai/assets/RAG/Claude_haystack1.jpg)

## [Chain-of-Note (CoN) Prompting](https://arxiv.org/abs/2311.09210)

- Proposed in [Chain-of-Note (CoN): Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210) by Yu et al. from Tencent AI Lab explores improving the robustness of Retrieval-Augmented Language Models (RALMs). It introduces the Chain-of-Note (CoN) framework to address two main robustness challenges in RALMs: noise and unknown scenarios.
- The core of CoN involves generating sequential reading notes for retrieved documents, enabling thorough evaluation of their relevance to the query and integrating this information to formulate a final answer.
- CoN focuses on improving the robustness of RALMs in handling irrelevant or noisy information and responding appropriately when faced with queries outside its knowledge scope.
- The framework was tested on various open-domain question answering datasets. Notably, CoN achieved an average improvement of +7.9 in exact match scores with entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions beyond the pre-training knowledge scope.
- The following image from the paper shows that compared with the current RALMs, the core idea behind CoN is to generate sequential reading notes for the retrieved documents, ensuring a systematic assessment of their relevance to the input question before formulating a final response.

![](https://aman.ai/images/papers/CoN1.jpg)

- The following image from the paper shows an illustration of the CoN framework with three distinct types of reading notes. Type (a) depicts the scenario where the language model identifies a document that directly answers the query, leading to a final answer formulated from the retrieved information. Type (b) represents situations where the retrieved document, while not directly answering the query, provides contextual insights, enabling the language model to integrate this context with its inherent knowledge to deduce an answer. Type (c) illustrates instances where the language model encounters irrelevant documents and lacks the necessary knowledge to respond, resulting in an “unknown” answer. This figure exemplifies the CoN framework’s capability to adaptively process information, balancing direct information retrieval, contextual inference, and the recognition of its knowledge boundaries.

![](https://aman.ai/images/papers/CoN2.jpg)

- ChatGPT was used to generate training data for CoN, which was then trained on a LLaMa-2 7B model, demonstrating the practical approach for implementing the framework.
- The evaluation on overall QA performance, noise robustness, and unknown robustness across multiple datasets indicated that RALMs equipped with CoN significantly outperform standard RALMs.
- Case studies demonstrated the enhanced capability of CoN in understanding and integrating information from multiple sources, leading to more accurate conclusions compared to standard RALMs.
- In conclusion, the paper presents a novel approach to enhance the robustness of RALMs, showing significant improvements in handling noise and unknown scenarios, which is crucial for practical applications of language models in open-domain settings.

## [Chain-of-Knowledge (CoK) Prompting](https://arxiv.org/abs/2305.13269)

- Introduced in [Chain-of-Knowledge (CoK): Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](https://arxiv.org/abs/2305.13269) by Li et al. from DAMO Academy Alibaba Group, NTU, and Singapore University of Technology and Design proposes Chain-of-Knowledge (CoK), a framework that enhances large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. The framework aims to produce more factual rationales and reduce hallucination in generation.
- CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Initially, CoK prepares preliminary rationales and answers for a knowledge-intensive question while identifying relevant knowledge domains. It then corrects these rationales step by step by adapting knowledge from identified domains, thereby providing a better foundation for the final answer.
- The following figure from the paper shows a comparison of different methods: (a) chain-of-thought with self-consistency, (b) verify-and-edit, and (c) CoK. CoK incorporates heterogeneous sources for knowledge retrieval and performs dynamic knowledge adapting. For clarity and succinct presentation, only pivotal steps are shown in the figure.

![](https://aman.ai/images/papers/CoK.jpg)

- The following figure from the paper shows our proposed chain-of-knowledge (CoK) framework, consisting of (I) Reasoning preparation, (II) Dynamic knowledge adapting, and (III) Answer consolidation. n.s.: natural sentence.

![](https://aman.ai/images/papers/CoK2.jpg)

- A key aspect of CoK is its use of both unstructured and structured knowledge sources, such as Wikidata and tables, for more reliable factual information. To access these varied sources, the framework introduces an adaptive query generator that generates queries for different query languages, including SPARQL, SQL, and natural sentences.
- CoK corrects rationales progressively, using preceding corrected rationales to generate and correct subsequent ones, minimizing error propagation. Extensive experiments demonstrate CoK’s consistent improvement in LLMs’ performance across different domains on knowledge-intensive tasks.
- The paper also details the challenges and limitations of existing methods in augmenting LLMs with external knowledge and how CoK addresses these by its design. It provides a comprehensive approach to improve factual correctness and reasoning capabilities of LLMs for a wide range of applications.

## [Chain-of-Code (CoC) Prompting](https://arxiv.org/abs/2312.04474)

- Proposed in [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://arxiv.org/abs/2312.04474).
- This paper by Li et al. introduces the Chain of Code (CoC), an approach that enhances language models’ (LMs) reasoning capabilities by integrating code-writing with an LM-augmented code emulator (LMulator), which executing code with a language model that simulates the execution if the code is not executable.
- CoC leverages code writing by LMs for improved reasoning in logic, arithmetic, and semantic tasks, often blending these aspects.
- The LMulator acts as a pseudo-interpreter, selectively emulating code execution for parts of the program that are not executable by a standard interpreter, like “detect_sarcasm(string)” function in semantic tasks.
- This approach allows LMs to format semantic sub-tasks as flexible pseudocode, with the LMulator catching undefined behaviors to simulate expected outputs.
- CoC outperforms other methods like Chain of Thought, particularly in benchmarks like BIG-Bench Hard, where it achieved an 84% score, a 12% gain.
- The following figure from the paper depicts various prompt engineering methods to solve advanced problems, (a) Chain of Thought prompting breaks the problem down into intermediate steps, (b) Program of Thoughts prompting writes and executes code, and (c) ScratchPad prompting simulates running already written code by tracking intermediate steps through a program state. Our reasoning method: Chain of Code first (d) generates code or psuedocode to solve the question and then (e) executes the code with a code interpreter if possible, and with an LMulator (language model emulating code) otherwise. Blue highlight indicates LM generation, red highlight indicates LM generated code being executed, and purple highlight indicates LMulator simulating the code via a program state in green.

![](https://aman.ai/images/papers/CoC.jpg)

- CoC’s performance is scalable across different LM sizes and broadens the scope of reasoning questions LMs can accurately answer by “thinking in code.”
- [Code](https://chain-of-code.github.io/).

## [Chain-of-Symbol (CoS) Prompting](https://arxiv.org/abs/2305.10276)

- Proposed in [Chain-of-Symbol Prompting for Spatial Relationships in Large Language Models](https://arxiv.org/abs/2305.10276) by Hu et al. from Westlake University, The Chinese University of Hong Kong, and University of Edinburgh.
- Chain-of-Symbol (CoS) prompting is a novel method for representing spatial relationships in Large Language Models (LLMs) using condensed symbols.
- The authors found that conventional Chain-of-Thought (CoT) prompting in natural language is less effective for spatial understanding and planning tasks, as LLMs like ChatGPT struggle with spatial relationships in texts. CoS addresses this by replacing natural language descriptions of spatial relationships with symbolic representations, leading to improved performance and efficiency.
- The following image from the paper illustrates an example for comparison between Chain-of-Thought (CoT) and Chain-of-Symbol (CoS) that elicits large language models in tackling complex planning tasks with higher performance and fewer input tokens. They let the model generate CoT/CoS during inference in a few-shot manner. Results were taken in May 2023 with ChatGPT and can be subject to change.

![](https://aman.ai/images/papers/CoS.jpg)

- The following image from the paper shows `<input, Chain of Symbol, output>` example triples for our three proposed tasks: Brick World, NLVR-based Manipulation, and Natural Language Navigation, and SPARTUN dataset (Mirzaee and Kordjamshidi, 2022). Chains of Symbols are highlighted.

![](https://aman.ai/images/papers/CoS2.jpg)

- CoS was evaluated using three spatial planning tasks (Brick World, NLVR-based Manipulation, and Natural Language Navigation) and a spatial question-answering dataset (SPARTUN). The method showed significant performance gains, for instance, up to 60.8% accuracy improvement in the Brick World task for ChatGPT, and reduced the number of tokens in prompts by up to 65.8%.
- The authors also demonstrated the robustness of CoS across different LLMs and languages, showing that it consistently outperforms CoT in accuracy and token efficiency. This indicates the potential of symbolic representations in enhancing LLMs’ spatial reasoning capabilities.

## [Structured Chain-of-Thought (SCoT) Prompting](https://arxiv.org/abs/2305.06599)

- Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.
- Proposed in [Structured Chain-of-Thought Prompting for Code Generation](https://arxiv.org/abs/2305.06599) by Li et al. from Peking University proposes Structured CoTs (SCoTs) and present a novel prompting technique for code generation, named SCoT prompting.
- Their motivation is that source code contains rich structural information and any code can be composed of three program structures (i.e., sequence, branch, and loop structures). Intuitively, structured intermediate reasoning steps make for structured source code. Thus, they ask LLMs to use program structures to build CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.
- Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think about how to solve requirements from the view of source code and further the performance of LLMs in code generation.
- The following figure from the paper shows a comparison of Chain-of-Thoughts (CoT) and our Structured Chain-of-Thought (SCoT).

![](https://aman.ai/images/papers/SCoT.jpg)

- They apply SCoT prompting to two LLMs (i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline - CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to examples and achieves substantial improvements.

## [Contrastive Chain-of-Thought (CCoT) Prompting](https://arxiv.org/abs/2311.09277)

- Proposed in [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277) by Chia et al. from DAMO, Singapore University of Technology and Desing, and NTU Singapore.
- This paper introduces a novel method to enhance the reasoning capabilities of large language models (LLMs). This method, termed Contrastive Chain of Thought (CCoT), involves providing both valid and invalid reasoning demonstrations, inspired by the way humans learn from both correct and incorrect methods.
- The concept of CCoT is based on the idea that adding contrastive examples, comprising both valid and invalid reasoning, can significantly improve the performance of LLMs in reasoning tasks.
- The process of using CCoT involves preparing a prompt, providing a valid chain of thought (CoT) explanation, generating contrastive invalid CoT explanations from the valid one, and then introducing a new user prompt.
- The following image from the paper shows an overview of contrastive chain-of-thought (right), with comparison to common prompting methods.

![](https://aman.ai/images/papers/CCoT.jpg)

- CCoT has demonstrated improvements of approximately 4-16% over traditional CoT on evaluations focused on strategic and mathematical reasoning. When combined with self-consistency techniques, CCoT becomes even more effective, showing an additional improvement of about 5%.
- A novel approach for generating invalid CoT explanations is introduced. This involves identifying key entities (like numbers, equations, persons) in the valid explanation and then shuffling them to create an invalid explanation.
- The authors identify five different categories of negative rationales to enhance learning. These include using irrelevant entities, erroneous order of logic, and incorrect logic.
- To validate the effectiveness of CCoT, the method was tested on GPT-3.5 using 500 samples across seven different datasets. The results showed that CCoT outperforms standard CoT across all datasets.
- This work represents a significant advancement in the field of natural language processing and reasoning, offering an innovative method to enhance the reasoning abilities of LLMs by learning from a mix of correct and incorrect reasoning examples.

## [Logical Chain-of-Thought (LogiCoT) Prompting](https://arxiv.org/abs/2309.13339)

- Proposed in [Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic](https://arxiv.org/abs/2309.13339) by Zhao et al. from the University of Hamburg.
- Logical Chain-of-Thought (LogiCoT) is aimed at enhancing the zero-shot reasoning capabilities of large language models (LLMs) by incorporating principles from symbolic logic. Recognizing the limitations of LLMs in performing multi-step reasoning tasks without losing coherence or succumbing to hallucinations, LogiCoT presents a neurosymbolic framework that systematically verifies and revises reasoning steps to ensure logical consistency and correctness.
- The methodology behind LogiCoT involves a two-fold process: first, applying reductio ad absurdum to identify and correct logical fallacies within the reasoning chain; second, structuring the reasoning process to allow for systematic verification and revision of each reasoning step, based on logical principles. This process is complemented by the introduction of a chain growth mechanism that selectively revises implausible reasoning steps, thus enhancing the model’s reasoning accuracy without unnecessary computational overhead.
- The figure below from the paper shows an overview of chain-of-thought (CoT) prompting and LogiCoT. In CoT, the failure of entailment (red) makes the rest of the deduction untrustworthy (gray), consequently impeding the overall success of the deduction. In contrast, LogiCoT is designed to think-verify-revise: it adopts those who pass the verification (green) and revise (blue) those who do not, thereby effectively improving the overall reasoning capability.

![](https://aman.ai/images/papers/LogiCoT1.jpg)

- The figure below from the paper shows an arithmetic example when applying LogiCoT verification and revision on CoT reasoning paths. Every reasoning step has to undergo a verification procedure, which is mainly directed by two post hoc reviews generated by the LLM (yellow) independently. In this example, step #1 fails (red) the verification because the discriminator agrees with the “Review Y” which correctly points out the error in this step. As a result, the LLM further revises (blue) the original step into a new step #1 and re-generates the trailing paths based on the revision. The procedure unrolls till every step is verified to be valid ( ). Key snippets of prompts used to achieve each procedure are shown in dotted boxes.

![](https://aman.ai/images/papers/LogiCoT2.jpg)

- Experimental evaluations demonstrate LogiCoT’s effectiveness across a variety of domains, including arithmetic, commonsense reasoning, causal inference, and social interaction tasks. The experiments, conducted on datasets such as GSM8K, AQuA, and others, utilizing models ranging from Vicuna-7b to GPT-4, highlight LogiCoT’s ability to significantly improve reasoning performance, especially as model size increases. Notably, the transition from a composing to an adopting strategy in error detection further accentuates LogiCoT’s advantages in enhancing reasoning accuracy and coherence.
- Moreover, the research delves into the impacts of logical revision on reasoning cases, distinguishing between worsening and improving rates to quantify the efficacy of LogiCoT interventions. The findings suggest that while larger models benefit more from LogiCoT’s revisions, there is a nuanced balance between improving reasoning accuracy and avoiding unnecessary interventions.
- In conclusion, the paper posits that LogiCoT represents a significant step forward in leveraging logical principles to refine the reasoning processes of LLMs. By enabling systematic verification and revision of reasoning steps, LogiCoT not only improves the accuracy and logical consistency of LLM outputs but also opens new avenues for research into neurosymbolic AI and its applications in enhancing the reasoning capabilities of generative models.

## [System 2 Attention Prompting](https://arxiv.org/abs/2311.11829)

- Proposed in [System 2 Attention (is something you might need too)](https://arxiv.org/abs/2311.11829) by Weston and Sukhbaatar from Meta.
- This paper introduces a novel attention mechanism for Large Language Models (LLMs) named System 2 Attention (S2A). This concept is inspired by the human cognitive process of deliberate attention (System 2 reasoning) and aims to solve the problem of standard soft attention in Transformers, which often struggles with filtering out irrelevant information from the input context.
- S2A targets the challenge in standard soft attention mechanisms where irrelevant information in the input context leads to degraded performance in tasks like opinion analysis, question answering, and longform content generation. The central issue is the model’s inability to discern and focus only on the relevant context portions.
- S2A introduces a method where the LLM first regenerates the input context, eliminating irrelevant parts. This approach leverages the LLM’s natural language understanding and instruction-following capabilities to improve the quality of attention and the responses by focusing only on the regenerated, relevant context. Thus, S2A involves a two-step process to improve attention and response quality by focusing only on regenerated, relevant context:
    1. **Context Regeneration**: Given a context x, S2A regenerates this context to x′, removing irrelevant parts that could adversely affect the output. This is denoted as x′∼S2A(x).
    2. **Response Generation with Refined Context**: The final response is produced using the regenerated context x′ instead of the original, leading to more accurate and factual responses. This step is represented as y∼LLM(x′).
- **Implementation Details**:
    
    - S2A is implemented as a class of techniques using general instruction-tuned LLMs. The process is executed as an instruction via prompting.
    - Specifically, S2A(x)=LLM(PS2A(x)), where PS2A is a function generating a zero-shot prompt instructing the LLM to perform the System 2 Attention task over x.
    - An example prompt, PS2A, used in the experiments, instructs the LLM to regenerate the context by extracting parts beneficial for providing relevant context for a given query, shown below in the figure from the paper.
    
    ![](https://aman.ai/images/papers/S2A_2.jpg)
    
    - Post-processing is applied to the output of step 1 to structure the prompt for step 2, as instruction-following LLMs produce additional reasoning and comments.
- The following image from the paper shows an example from the GSM-IC task where a distracting sentence (“Max has 1000 more books than Mary”) makes LLaMA-2-70B-chat (left) make a mistake. System 2 Attention (S2A) regenerates the portion of the context it decides to pay attention to, successfully removing the distracting sentence (right), then hence answering correctly.

![](https://aman.ai/images/papers/S2A.jpg)

- The authors assess S2A across factual QA, longform generation, and math word problems. In factual QA, S2A achieves 80.3% accuracy, significantly improving factuality. In longform generation, it enhances objectivity, scoring 3.82 out of 5. In math word problems, S2A shows improved accuracy, indicating its effectiveness in focusing on relevant context.
- The paper explores different S2A variants, offering insights into its robustness and flexibility.
- The success of S2A in enhancing factuality and objectivity while reducing irrelevant content suggests its potential for high precision tasks like automated news reporting, academic research assistance, or legal document analysis. Future work could refine this approach for specific domains or integrate it with other advanced techniques to further enhance LLM capabilities.
- This research represents a significant advancement in the attention mechanisms of LLMs, particularly in handling context relevance, factuality, and objectivity.

## [Emotion Prompting](https://arxiv.org/abs/2307.11760v7)

- Introduced in [Large Language Models Understand and Can Be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760v7) by Li et al. from CAS, Microsoft, William&Mary, Beijing Normal University and HKUST.
- This paper proposes an idea to enhance the performance of Large Language Models (LLMs) using emotional stimuli, a concept termed “EmotionPrompt.” It explores the influence of emotional intelligence on LLMs and demonstrates how adding emotional cues to prompts significantly improves the LLMs’ performance in various tasks.
- The study introduces EmotionPrompt, a method that combines standard prompts with emotional stimuli. This approach leverages human-like emotional responses to enhance the LLMs’ reasoning and problem-solving abilities.
- The paper conducts automatic experiments using several LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Tasks span deterministic and generative applications, offering a comprehensive evaluation scenario.
- The following image from the paper shows an overview of the process from generating to evaluating EmotionPrompt.

![](https://aman.ai/images/papers/EmotionPrompt1.jpg)

- The following image from the paper illustrates the fact that building upon psychological theories, we developed different sets of emotional stimuli.

![](https://aman.ai/images/papers/EmotionPrompt2.jpg)

- Results show a notable improvement in LLM performance with EmotionPrompt, with a relative performance improvement of 8.00% in Instruction Induction and 115% in BIG-Bench tasks.
- A human study involving 106 participants assessed the quality of generative tasks using both vanilla and emotional prompts. This study indicated a 10.9% average improvement in performance, truthfulness, and responsibility metrics with EmotionPrompt.
- The paper delves into why EmotionPrompt is effective, discussing factors that may influence its performance and providing insights into the integration of emotional intelligence in LLMs.
- Example use case: Imagine an educational application where a language model assists students in learning a new topic. Normally, a prompt might simply ask the model to explain a concept. However, with EmotionPrompt, the query might include a statement like “It’s crucial for my upcoming exam to understand this topic.” This emotional addition motivates the LLM to generate more thoughtful, comprehensive, and engaging explanations, potentially improving the student’s understanding and retention of the material.

## [Thread of Thought (ThoT) Prompting](https://arxiv.org/abs/2311.08734)

- Proposed in [Thread of Thought (ThoT): Unraveling Chaotic Contexts](https://arxiv.org/abs/2311.08734) by Zhou et al. from University of Macau, Microsoft, and University of Technology Sydney.
- This paper introduces the Thread of Thought (ThoT) strategy, a novel technique designed to enhance the reasoning capabilities of Large Language Models (LLMs) in handling chaotic contexts. ThoT draws inspiration from human cognitive processes and aims to systematically segment and analyze extended contexts for better comprehension and accuracy.
- ThoT is developed to address challenges in chaotic contexts, where LLMs struggle to sift through and prioritize relevant information amidst a plethora of data.
- The following image from the paper shows the strategy involves a two-step process where the first step guides the LLM through the context analytically, breaking it down into manageable parts for summarization and analysis. The second step refines this into a definitive answer. Thread of Thought prompting enables large language models to tackle chaotic context problems. In the output depicted, green text denotes the correct answer, while red text indicates the erroneous prediction.

![](https://aman.ai/images/papers/ThoT.jpg)

- The following image from the paper shows Thread of Thought for zero-shot reasoning.

![](https://aman.ai/images/papers/ThoT2.jpg)

- The efficacy of ThoT is demonstrated using PopQA and EntityQ datasets, and a Multi-Turn Conversation Response dataset (MTCR) developed by the authors. ThoT shows significant improvements in reasoning performance over other prompting techniques.
- ThoT can be seamlessly integrated with various pre-trained language models and prompting strategies, acting as a versatile “plug-and-play” module.
- Example Use Case: Consider an LLM being used in a customer service application to handle complex customer inquiries involving multiple issues. Traditional models might struggle to extract and focus on relevant details from a lengthy customer interaction history. With ThoT, the model could systematically break down the customer’s history into segments, analyze each part to identify key issues, and then synthesize this information to provide a comprehensive and accurate response. This method not only improves the quality of the response but also enhances the efficiency and effectiveness of the customer service process.

## [Program of Thoughts (PoT) Prompting](https://arxiv.org/abs/2211.12588)

- Proposed in [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks](https://arxiv.org/abs/2211.12588) by Chen et al. from the University of Waterloo, Vector Institute Toronto, University of California Santa Barbara, and Google Research in TMLR 2023 introduces Program of Thoughts (PoT) prompting.
- PoT improves numerical reasoning in language models. PoT leverages language models, mainly Codex, to generate programming language statements alongside text, which are then executed by a program interpreter. PoT thus decouples complex computation from reasoning and language understanding.
- The following figure from the paper shows a comparison between Chain of Thoughts and Program of Thoughts.

![](https://aman.ai/images/papers/PoT.jpg)

- PoT was evaluated on math word problem and financial QA datasets, showing an average performance gain of around 12% compared to Chain-of-Thoughts prompting.
- The paper demonstrates that PoT, particularly when combined with self-consistency decoding, significantly reduces offensive content and enhances robustness to adversarial prompts.

## [Optimization by Prompting (OPRO)](https://arxiv.org/pdf/2309.03409.pdf)

- Proposed in [Large Language Models as Optimizers](https://arxiv.org/pdf/2309.03409.pdf) by Google DeepMind.
- This paper introduces Optimization by PROmpting (OPRO), a novel method to use LLMs as optimizers in various tasks described in natural language.
- In OPRO, each optimization step involves the LLM generating new solutions from a prompt that includes previously generated solutions and their evaluations. These new solutions are then assessed and added to the prompt for subsequent optimization steps.
- The method is first demonstrated on linear regression and traveling salesman problems, and then extended to prompt optimization. The goal here is to find instructions that maximize task accuracy.
- The study showcases that OPRO significantly enhances performance, with the best prompts optimized by OPRO outperforming human-designed prompts by up to 8% on the GSM8K dataset and by up to 50% on Big-Bench Hard tasks.
- OPRO presents a simple and effective approach for leveraging LLMs in optimization tasks, expanding the potential applications of these models in various real-world scenarios.
- Example Use Case: In a scenario involving route optimization for logistics, traditional methods may struggle with complex constraints and dynamic variables. By describing the task in natural language and applying OPRO, an LLM can iteratively generate and refine routes, taking into account various factors to find the most efficient path. This approach can lead to more optimal and practical solutions compared to conventional methods.

## [Rephrase and Respond (RaR) Prompting](https://arxiv.org/abs/2311.04205)

- Proposed in[Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205) by Deng et al. from UCLA.
- This paper introduces a novel method, ‘Rephrase and Respond’ (RaR), aimed at enhancing the performance of Large Language Models (LLMs) in understanding and responding to human-posed questions. RaR allows LLMs to autonomously rephrase and expand questions before providing responses, addressing the challenge of misunderstanding seemingly unambiguous questions due to discrepancies in interpretation between humans and LLMs.
- RaR is implemented in two forms:
    1. **One-step RaR**: Here, LLMs rephrase and respond to a question within a single prompt. This method is based on the human communication strategy of rephrasing for clarity and coherence. It’s shown to be effective, especially with datasets that present ambiguous questions to LLMs.
    2. **Two-step RaR**: This approach involves a more intricate process where a ‘rephrasing LLM’ first rephrases the question, which is then combined with the original question to prompt a ‘responding LLM’. This method is beneficial for using rephrased questions across different models, with experiments showing that a question rephrased by a more advanced LLM, like GPT-4, can significantly aid a less sophisticated LLM in producing more accurate responses.
- The paper conducts extensive experiments to validate the efficacy of RaR, revealing that:
    - Both One-step and Two-step RaR significantly improve LLM performance across various tasks.
    - One-step RaR is a straightforward and effective method to enhance LLM responses, outperforming Two-step RaR in 6 out of 10 tasks.
    - Two-step RaR consistently improves the quality of responses, particularly in tasks where LLMs initially show poor performance. This method also demonstrates the ability to rephrase questions autonomously, leading to significant accuracy improvements.
- The following figure from the paper depicts Two-step RaR examples where the question is rephrased and the rephrased question is responded to.

![](https://aman.ai/images/papers/RAR.jpg)

- The effectiveness of RaR was tested across multiple benchmark tasks, including Knowledge Classification, Knowledge Comparison, CommonSense QA, Date Understanding, Last Letter Concatenation, Coin Flip, and Sports Understanding. These tasks were designed to evaluate various aspects of LLM capabilities like commonsense reasoning, symbolic reasoning, and sports knowledge.
- The performance of RaR was also examined across different LLMs, including GPT-3.5 and Vicuna. It was found that all LLMs tested showed enhanced performance with Two-step RaR. Moreover, the study confirms that the rephrased questions are transferable across different LLMs, demonstrating that rephrased questions by a model like GPT-4 can significantly benefit other models like Vicuna.
- Additionally, the paper explores the concept of multiple rephrasings, where iterative self-rephrasing by GPT-4 is used to achieve consistent clarifications. This method shows that GPT-4 can progressively clarify concepts, even if it fails to do so in the initial attempt, with the questions becoming more elaborate after each rephrasing.
- Lastly, RaR is compared with the Chain-of-Thought (CoT) method, demonstrating that RaR offers improvements in scenarios where zero-shot CoT is ineffective, and also addresses the shortcomings inherent in few-shot CoT.

## [Scratchpad Prompting](https://arxiv.org/abs/2112.00114)

- Proposed in [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114).
- This paper by Nye et al. from MIT and Google Research, presented at NeurIPS 2021, introduces the concept of “scratchpads” to improve the ability of large Transformer-based language models to perform complex, multi-step computations.
- The authors address the issue that while these models excel at tasks requiring single-step computation, they struggle with multi-step algorithmic tasks, like long addition or program execution. The proposed solution involves training models to use a scratchpad for intermediate computation steps.
- The paper demonstrates that using scratchpads allows models to successfully perform long addition, polynomial evaluation, and execution of arbitrary Python code.
- The following figure from the paper shows an overview of the proposed scratchpad approach applied to predicting code execution and comparison to direct execution prediction. (Top) Previous work has shown that large pre-trained models achieve poor performance when asked to directly predict the result of executing given computer code. (Bottom) In this work, we show that training models to use a scratchpad and predict the program execution trace line-by-line can lead to large improvements in execution prediction performance. N.B. Although the example below only has one loop iteration for each loop, all loops are unrolled across time.

![](https://aman.ai/images/papers/SyW.jpg)

- Empirical results show that the scratchpad method leads to significant improvements in task performance, including out-of-distribution generalization and execution prediction of Python programs.
- The authors conclude that scratchpads offer a simple yet effective way to enhance the computational abilities of Transformer models without altering their underlying architecture.

## [Take a Step Back Prompting](https://arxiv.org/abs/2310.06117)

- Proposed in [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117).
- Imagine you’re asked a detailed physics question. Instead of diving straight in, you first understand the fundamental law or principle that applies. Then, you use this understanding to tackle the specific question. This is the underpinning principle behind the proposal in this paper.
- This paper by Zheng et al. from Google DeepMind introduces a novel prompting technique named Step-Back Prompting. This method enables Large Language Models (LLMs) like PaLM-2L to perform abstractions, deriving high-level concepts and first principles from detailed instances, thus significantly enhancing their reasoning capabilities.
- Step-Back Prompting is a two-step process comprising Abstraction and Reasoning. In the abstraction phase, LLMs are prompted to ask high-level, broader, generic step-back questions about concepts or principles relevant to the task. The reasoning phase then uses these concepts and principles to guide the LLMs towards the solution of the original questions.
- The technique is exemplified in the paper with two illustrations. The following image from the paper illustrates Step-Back Prompting with two steps of Abstraction and Reasoning guided by concepts and principles. Top: an example of MMLU high-school physics where the first principle of Ideal Gas Law is retrieved via abstraction. Bottom: an example from TimeQA where the high-level concept of education history is a result of the abstraction. Left: PaLM-2L fails to answer the original question. Chain-of-Thought prompting ran into errors during intermediate reasoning steps (highlighted as red). Right: PaLM-2L successfully answers the question via Step-Back Prompting.

![](https://aman.ai/images/papers/step-back-prompting.jpg)

- The authors conduct extensive experiments with Step-Back Prompting on PaLM-2L models across various challenging reasoning-intensive tasks, including STEM, Knowledge QA, and Multi-Hop Reasoning. Notably, this technique improves performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
- The effectiveness of Step-Back Prompting is empirically validated, outperforming other methods like Chain of Thought (CoT) prompting and Take a Deep Breath (TDB) prompting, with significant improvements over baseline models.
- An error analysis indicates that most errors in Step-Back Prompting occur during the reasoning step, suggesting that while LLMs can be effectively taught abstraction skills, enhancing their reasoning capabilities remains a challenge.
- The paper positions Step-Back Prompting as a simple yet powerful method to significantly improve the reasoning ability of LLMs, especially in tasks that demand complex and deep reasoning.

## [Ask Me Anything Prompting](https://arxiv.org/abs/2210.02441)

- Proposed in [Ask Me Anything: A Simple Strategy for Prompting Language Models](https://arxiv.org/abs/2210.02441) by Arora et al. from Stanford University, Numbers Station, and UW-Madison.
- Ask Me Anything Prompting (AMA) is a novel prompting method for LLMs.
- AMA aims to overcome the brittleness of traditional prompting methods by aggregating multiple effective yet imperfect prompts to enhance model performance across various tasks. It exploits question-answering (QA) prompts for their open-ended nature, encouraging models to generate more nuanced responses than restrictive prompt types.
- The approach uses the LLM itself to recursively transform task inputs into effective QA formats, collecting several noisy votes for an input’s true label. These votes are then aggregated using weak supervision, a technique for combining noisy predictions without additional labeled data.
- AMA first recursively uses the LLM to reformat tasks and prompts to effective formats, and second aggregates the predictions across prompts using weak-supervision. The reformatting is performed using prompt-chains, which consist of functional (fixed, reusable) prompts that operate over the varied task inputs. Here, given the input example, the prompt-chain includes a question()-prompt through which the LLM converts the input claim to a question, and an answer() prompt, through which the LLM answers the question it generated. Different prompt-chains (i.e., differing in the in-context question and answer demonstrations) lead to different predictions for the input’s true label.

![](https://aman.ai/images/papers/AMA.jpg)

- AMA was evaluated across multiple open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance improvement of 10.2% over a few-shot baseline. Remarkably, it enabled the GPT-J-6B model to match or exceed few-shot GPT-3-175B performance on 15 out of 20 popular benchmarks.
- The paper concludes that AMA not only facilitates the use of smaller, open-source LLMs by reducing the need for perfect prompting but also suggests a scalable and effective method for prompt aggregation.
- [Code](https://github.com/HazyResearch/ama_prompting)

## [Promptbreeder](https://arxiv.org/abs/2309.16797)

- Proposed in [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution](https://arxiv.org/abs/2309.16797) by Fernando et al. from Google DeepMind, Promptbreeder is an innovative system designed to evolve and adapt prompts for Large Language Models (LLMs) autonomously, enhancing their reasoning capabilities across a range of tasks without manual prompt engineering. The system utilizes evolutionary algorithms to mutate a population of task-prompts and mutation-prompts generated by the LLM itself, demonstrating a unique self-referential improvement mechanism.
- Promptbreeder outperforms existing prompt strategies such as Chain-of-Thought and Plan-and-Solve on arithmetic and commonsense reasoning benchmarks and proves its efficiency in evolving domain-specific prompts for complex tasks like hate speech classification, showcasing its adaptability and scalability.
- The evolution process features a diverse set of mutation operators, including direct mutation, estimation of distribution, hypermutation, Lamarckian mutation, and prompt crossover with context shuffling. These operators facilitate the exploration of a wide range of cognitive strategies and promote diversity in prompt evolution.
- The following figure from the paper shows an overview of Promptbreeder. Given a problem description and an initial set of general “thinking-styles” and mutation-prompts, Promptbreeder generates a population of units of evolution, each unit consisting of typically two task-prompts and a mutation-prompt. We then run a standard binary tournament genetic algorithm (Harvey, 2011). To determine the fitness of a task-prompt we evaluate its performance on a random batch of training data. Over multiple generations, Promptbreeder subsequently mutates task-prompts as well as mutation-prompts using five different classes of mutation operators. The former leads to increasingly domain-adaptive task-prompts whereas the latter evolves increasingly useful mutation-prompts in a self-referential way.

![](https://aman.ai/images/papers/Promptbreeder.jpg)

- Experiments highlight Promptbreeder’s effectiveness in evolving intricate task-prompts that significantly outperform state-of-the-art methods, underscoring its potential to automate the generation of effective, domain-specific prompts for improving LLMs’ performance across various tasks.

## Visual Summary

- The following infographic from [Aishwarya Naresh Reganti](https://www.linkedin.com/in/areganti/) offers a visual summary of some of the most popular prompting techniques.

![](https://aman.ai/primers/ai/assets/prompt/summary.jpg)

## Resources

### [Anthropic’s Prompt Generator](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator)

- Anthropic’s [Prompt Generator](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator) automatically generates high-quality first draft prompt templates tailored to your specific tasks, following prompt engineering best practices.
- The prompt generator is particularly useful as a tool for solving the “blank page problem” to give you a jumping-off point for further testing and iteration.

## Further Reading

- [Prompt Engineering Overview](https://www.youtube.com/watch?v=dOxUroR57xs) by Elvis Saravia
- [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) by Elvis Saravia
    - It organizes all the best guides and resources in one single place.
- [Prompt Engineering Guide](https://www.promptingguide.ai/) by Elvis Saravia
    - The Prompt Engineering Guide is a project by [DAIR.AI](https://github.com/dair-ai).
- [Learn Prompting](https://learnprompting.org/)
    - An open-source course on prompt engineering.
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)
    - A comprehensive repository consisting of prompt examples and guides.
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)
    - A survey paper formally introducing prompt engineering. Note that it was published in 2021.
- [Stanford CS224n - NLP with Deep Learning: Prompting, Instruction Finetuning, and RLHF](http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf)
    - A concise summary of some of the latest prompting techniques.
- [Prompt Injection](https://simonwillison.net/series/prompt-injection/)
    - A collection of blog posts on prompt injection that aims to document vulnerabilities with LLMs like GPT-3.
- [CS11-711 - Advanced NLP: Prompting](https://www.youtube.com/watch?v=5ef83Wljm-M)
    - An excellent lecture by Graham Neubig formalizing the paradigm of prompt engineering.
- [Prompt Engineering with OpenAI’s GPT-3 and other LLMs](https://www.youtube.com/watch?v=BP9fi_0XTlw)
    - Great short introduction on Prompt Engineering by James Briggs. Includes notebook with examples.
- [LangChain](https://github.com/hwchase17/langchain)
    - A powerful tool to build applications with LLMs in a composable way. It has excellent guides on how to work with prompts.
- [Prompt Datasets](https://github.com/dair-ai/Prompt-Engineering-Guide#datasets)
    - You can find all sorts of prompt datasets for different purposes here:
- [Prompt Engineering 101 - Introduction and resources](https://www.linkedin.com/pulse/prompt-engineering-101-introduction-resources-amatriain)
    - A quick intro to prompt engineering with examples.

## References

- [Dair-AI’s Prompt Engineering post](https://github.com/dair-ai/Prompt-Engineering-Guide)
- [Lilian Weng’s blog](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#instruction-prompting)
- [Learn Prompting](https://learnprompting.org/docs/basics/instructions)
- [OpenAI](https://github.com/openai/grade-school-math)
- [Cohere-Prompt Engineering](https://txt.cohere.ai/how-to-train-your-pet-llm-prompt-engineering/)