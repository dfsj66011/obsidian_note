

## 一、教学目的的 Open R1

欢迎来到激动人心的开源 AI 与强化学习世界之旅！本章旨在帮助学生理解强化学习及其在大型语言模型中的作用。

我们还将探索 [Open R1](https://github.com/huggingface/open-r1) 这一开创性的社区项目，它让每个人都能接触到先进的人工智能技术。具体来说，本课程旨在帮助学员学习使用 [Open R1](https://github.com/huggingface/open-r1) 并为其做出贡献。

### 1.1 你将学到什么

在本章中，我们将把复杂的概念分解成易于理解的部分，并向你展示如何参与这个令人兴奋的项目，使大型语言模型能够推理复杂问题。

大语言模型在许多生成任务上表现出色。然而，直到最近，它们在需要推理的复杂问题上仍存在困难。例如，它们难以处理需要多步推理的谜题或数学问题。

Open R1 是一个旨在让大型语言模型（LLM）对复杂问题进行推理的项目。它通过使用强化学习来鼓励 LLM 进行“思考”和推理。简单来说，该模型经过训练可以生成想法和输出，并将这些想法和输出结构化，以便用户能够分别处理它们。

让我们来看一个例子。假设我们给自己布置了解决以下问题的任务，我们可能会这样思考：

```python
Problem: "我有 3 个苹果和 2 个橙子。我一共有多少水果？"

Thought: "我需要把苹果和橙子的数量加起来，得到水果的总数。"

Answer: "5"
```

然后我们可以构建这种思路和答案，以便用户能够分别处理它们。对于推理任务，可以训练大语言模型按照以下格式生成思路和答案：

```python
<think>我需要把苹果和橙子的数量加起来，得到水果的总数。</think>
5
```

作为用户，我们可以从模型的输出中提取思路和答案，并用它们来解决问题。


## [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#why-this-matters-for-students)Why This Matters for Students

As a student, understanding Open R1 and the role of reinforcement learning in LLMs is valuable because:

- It shows you how cutting-edge AI is developed
- It gives you hands-on opportunities to learn and contribute
- It helps you understand where AI technology is heading
- It opens doors to future career opportunities in AI

## [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#chapter-overview)Chapter Overview

This chapter is divided into four sections, each focusing on a different aspect of Open R1:

### [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#1-introduction-to-reinforcement-learning-and-its-role-in-llms)1️⃣ Introduction to Reinforcement Learning and its Role in LLMs

We’ll explore the basics of Reinforcement Learning (RL) and its role in training LLMs.

- What is RL?
- How is RL used in LLMs?
- What is DeepSeek R1?
- What are the key innovations of DeepSeek R1?

### [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#2-understanding-the-deepseek-r1-paper)2️⃣ Understanding the DeepSeek R1 Paper

We’ll break down the research paper that inspired [Open R1](https://huggingface.co/open-r1):

- Key innovations and breakthroughs
- The training process and architecture
- Results and their significance

### [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#3-implementing-grpo-in-trl)3️⃣ Implementing GRPO in TRL

We’ll get practical with code examples:

- How to use the Transformer Reinforcement Learning (TRL) library
- Setting up GRPO training

### [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#4-practical-use-case-to-align-a-model)4️⃣ Practical use case to align a model

We’ll look at a practical use case to align a model using Open R1.

- How to train a model using GRPO in TRL
- Share your model on the [Hugging Face Hub](https://huggingface.co/models)

## [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#prerequisites)Prerequisites

To get the most out of this chapter, it’s helpful to have:

- Solid understanding of Python programming
- Familiarity with machine learning concepts
- Interest in AI and language models

Don’t worry if you’re missing some of these – we’ll explain key concepts as we go along! 🚀

If you don’t have all the prerequisites, check out this [course](https://huggingface.co/course/chapter1/1) from units 1 to 11

## [](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#how-to-use-this-chapter)How to Use This Chapter

1. **Read Sequentially**: The sections build on each other, so it’s best to read them in order
2. **Share Notes**: Write down key concepts and questions and discuss them with in the community in [Discord](https://discord.gg/F3vZujJH)
3. **Try the Code**: When we get to practical examples, try them yourself
4. **Join the Community**: Use the resources we provide to connect with other learners

Let’s begin our exploration of Open R1 and discover how you can be part of making AI more accessible to everyone! 🚀

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/1.mdx)

[←Exam Time!](https://huggingface.co/learn/llm-course/chapter11/7?fw=pt)[Reinforcement Learning on LLMs→](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)

[Open R1 for Students](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#open-r1-for-students)[What You’ll Learn](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#what-youll-learn)[Why This Matters for Students](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#why-this-matters-for-students)[Chapter Overview](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#chapter-overview)[1️⃣ Introduction to Reinforcement Learning and its Role in LLMs](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#1-introduction-to-reinforcement-learning-and-its-role-in-llms)[2️⃣ Understanding the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#2-understanding-the-deepseek-r1-paper)[3️⃣ Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#3-implementing-grpo-in-trl)[4️⃣ Practical use case to align a model](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#4-practical-use-case-to-align-a-model)[Prerequisites](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#prerequisites)[How to Use This Chapter](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt#how-to-use-this-chapter)


-----------


[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

# [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#introduction-to-reinforcement-learning-and-its-role-in-llms)Introduction to Reinforcement Learning and its Role in LLMs

Welcome to the first page!

We’re going to start our journey into the exciting world of Reinforcement Learning (RL) and discover how it’s revolutionizing the way we train Language Models like the ones you might use every day.

In this chapter, we are focusing on reinforcement learning for language models. However, reinforcement learning is a broad field with many applications beyond language models. If you’re interested in learning more about reinforcement learning, you should check out the [Deep Reinforcement Learning course](https://huggingface.co/courses/deep-rl-course/en/unit1/introduction).

This page will give you a friendly and clear introduction to RL, even if you’ve never encountered it before. We’ll break down the core ideas and see why RL is becoming so important in the field of Large Language Models (LLMs).

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#what-is-reinforcement-learning-rl)What is Reinforcement Learning (RL)?

Imagine you’re training a dog. You want to teach it to sit. You might say “Sit!” and then, if the dog sits, you give it a treat and praise. If it doesn’t sit, you might gently guide it or just try again. Over time, the dog learns to associate sitting with the positive reward (treat and praise) and is more likely to sit when you say “Sit!” again. In reinforcement learning, we refer to this feedback as a **reward**.

That, in a nutshell, is the basic idea behind Reinforcement Learning! Instead of a dog, we have a **language model** (in reinforcement learning, we call it an **agent**), and instead of you, we have the **environment** that gives feedback.

![RL terms Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/3.jpg)

Let’s break down the key pieces of RL:

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#agent)Agent

This is our learner. In the dog example, the dog is the agent. In the context of LLMs, the LLM itself becomes the agent we want to train. The agent is the one making decisions and learning from the environment and its rewards.

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#environment)Environment

This is the world the agent lives in and interacts with. For the dog, the environment is your house and you. For an LLM, the environment is a bit more abstract – it could be the users it interacts with, or a simulated scenario we set up for it. The environment provides feedback to the agent.

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#action)Action

These are the choices the agent can make in the environment. The dog’s actions are things like “sit”, “stand”, “bark”, etc. For an LLM, actions could be generating words in a sentence, choosing which answer to give to a question, or deciding how to respond in a conversation.

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#reward)Reward

This is the feedback the environment gives to the agent after it takes an action. Rewards are usually numbers.

**Positive rewards** are like treats and praise – they tell the agent “good job, you did something right!“.

**Negative rewards** (or penalties) are like a gentle “no” – they tell the agent “that wasn’t quite right, try something else”. For the dog, the treat is the reward.

For an LLM, rewards are designed to reflect how well the LLM is doing at a specific task – maybe it’s how helpful, truthful, or harmless its response is.

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#policy)Policy

This is the agent’s strategy for choosing actions. It’s like the dog’s understanding of what it should do when you say “Sit!“. In RL, the policy is what we’re really trying to learn and improve. It’s a set of rules or a function that tells the agent what action to take in different situations. Initially, the policy might be random, but as the agent learns, the policy becomes better at choosing actions that lead to higher rewards.

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#the-rl-process-trial-and-error)The RL Process: Trial and Error

![RL Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/1.jpg)

Reinforcement Learning happens through a process of trial and error:

|Step|Process|Description|
|---|---|---|
|1. Observation|The agent observes the environment|The agent takes in information about its current state and surroundings|
|2. Action|The agent takes an action based on its current policy|Using its learned strategy (policy), the agent decides what to do next|
|3. Feedback|The environment gives the agent a reward|The agent receives feedback on how good or bad its action was|
|4. Learning|The agent updates its policy based on the reward|The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards|
|5. Iteration|Repeat the process|This cycle continues, allowing the agent to continuously improve its decision-making|

Think about learning to ride a bike. You might wobble and fall at first (negative reward!). But when you manage to balance and pedal smoothly, you feel good (positive reward!). You adjust your actions based on this feedback – leaning slightly, pedaling faster, etc. – until you learn to ride well. RL is similar – it’s about learning through interaction and feedback.

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#role-of-rl-in-large-language-models-llms)Role of RL in Large Language Models (LLMs)

Now, why is RL so important for Large Language Models?

Well, training really good LLMs is tricky. We can train them on massive amounts of text from the internet, and they become very good at predicting the next word in a sentence. This is how they learn to generate fluent and grammatically correct text, as we learned in [chapter 2](https://huggingface.co/course/chapter2/1).

However, just being fluent isn’t enough. We want our LLMs to be more than just good at stringing words together. We want them to be:

- **Helpful:** Provide useful and relevant information.
- **Harmless:** Avoid generating toxic, biased, or harmful content.
- **Aligned with Human Preferences:** Respond in ways that humans find natural, helpful, and engaging.

Pre-training LLM methods, which mostly rely on predicting the next word from text data, sometimes fall short on these aspects.

Whilst supervised training is excellent at producing structured outputs, it can be less effective at producing helpful, harmless, and aligned responses. We explore supervised training in [chapter 11](https://huggingface.co/course/chapter11/1).

Fine-tuned models might generate fluent and structured text that is still factually incorrect, biased, or doesn’t really answer the user’s question in a helpful way.

**Enter Reinforcement Learning!** RL gives us a way to fine-tune these pre-trained LLMs to better achieve these desired qualities. It’s like giving our LLM dog extra training to become a well-behaved and helpful companion, not just a dog that knows how to bark fluently!

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#reinforcement-learning-from-human-feedback-rlhf)Reinforcement Learning from Human Feedback (RLHF)

A very popular technique for aligning language models is **Reinforcement Learning from Human Feedback (RLHF)**. In RLHF, we use human feedback as a proxy for the “reward” signal in RL. Here’s how it works:

1. **Get Human Preferences:** We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question “What is the capital of France?” and ask them “Which answer is better?“.
    
2. **Train a Reward Model:** We use this human preference data to train a separate model called a **reward model**. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences.
    
3. **Fine-tune the LLM with RL:** Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we’re training the LLM to produce text that our reward model (which learned from human preferences) thinks is good.
    

![RL Basic Concept](https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg)

From a general perspective, let’s look at the benefits of using RL in LLMs:

|Benefit|Description|
|---|---|
|Improved Control|RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise.|
|Enhanced Alignment with Human Values|RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It’s hard to write down rules for “what makes a good answer,” but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments.|
|Mitigating Undesirable Behaviors|RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them.|

Reinforcement Learning from Human Feedback has been used to train many of the most popular LLMs today, such as OpenAI’s GPT-4, Google’s Gemini, and DeepSeek’s R1. There are a wide range of techniques for RLHF, with varying degrees of complexity and sophistication. In this chapter, we will focus on Group Relative Policy Optimization (GRPO), which is a technique for RLHF that has been shown to be effective at training LLMs that are helpful, harmless, and aligned with human preferences.

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#why-should-we-care-about-grpo-group-relative-policy-optimization)Why should we care about GRPO (Group Relative Policy Optimization)?

There are many techniques for RLHF but this course is focused on GRPO because it represents a significant advancement in reinforcement learning for language models.

Let’s briefly consider two of other popular techniques for RLHF:

- Proximal Policy Optimization (PPO)
- Direct Preference Optimization (DPO)

Proximal Policy Optimization (PPO) was one of the first highly effective techniques for RLHF. It uses a policy gradient method to update the policy based on the reward from a separate reward model.

Direct Preference Optimization (DPO) was later developed as a simpler technique that eliminates the need for a separate reward model using preference data directly. Essentially, framing the problem as a classification task between the chosen and rejected responses.

DPO and PPO are complex reinforcement learning algorithms in their own right, which we will not cover in this course. If you’re interested in learning more about them, you can check out the following resources:

- [Proximal Policy Optimization](https://huggingface.co/docs/trl/main/en/ppo_trainer)
- [Direct Preference Optimization](https://huggingface.co/docs/trl/main/en/dpo_trainer)

Unlike DPO and PPO, GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods.

GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function.

GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn’t strictly require one. This is because GRPO can incorporate reward signals from any function or model that can evaluate the quality of responses.

For example, we could use a length function to reward shorter responses, a mathematical solver to verify solution correctness, or a factual correctness function to reward responses that are more factually accurate. This flexibility makes GRPO particularly versatile for different types of alignment tasks.

---

Congratulations on completing Module 1! You’ve now got a solid introduction to Reinforcement Learning and its crucial role in shaping the future of Large Language Models. You understand the basic concepts of RL, why it’s used for LLMs, and you’ve been introduced to GRPO, a key algorithm in this field.

In the next module, we’ll get our hands dirty and dive into the DeepSeek R1 paper to see these concepts in action!

## [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#quiz)Quiz

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#1-what-are-the-key-components-of-reinforcement-learning)1. What are the key components of Reinforcement Learning?

 Agent, Environment, Action, Reward, and Policy Model, Data, Loss Function, and Optimizer Input, Output, and Hidden Layers

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#2-what-is-the-main-advantage-of-rlhf-for-training-language-models)2. What is the main advantage of RLHF for training language models?

 It helps align models with human preferences and values It makes models generate text faster It reduces the model's memory usage

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#3-in-the-context-of-rl-for-llms-what-represents-an-action)3. In the context of RL for LLMs, what represents an “action”?

 Generating words or choosing responses in a conversation Updating model weights Processing input tokens

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#4-what-is-the-role-of-the-reward-in-rl-training-of-language-models)4. What is the role of the reward in RL training of language models?

 To provide feedback on how well the model's responses align with desired behavior To measure the model's vocabulary size To determine the model's training speed

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#5-what-is-a-reward-in-the-context-of-rl-for-llms)5. What is a reward in the context of RL for LLMs?

 A numerical score that measures the quality of a response A function that generates responses A model that evaluates the quality of responses

Submit

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/2.mdx)

[←Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[The Aha Moment in the DeepSeek R1 Paper→](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)

[Introduction to Reinforcement Learning and its Role in LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#introduction-to-reinforcement-learning-and-its-role-in-llms)[What is Reinforcement Learning (RL)?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#what-is-reinforcement-learning-rl)[Agent](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#agent)[Environment](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#environment)[Action](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#action)[Reward](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#reward)[Policy](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#policy)[The RL Process: Trial and Error](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#the-rl-process-trial-and-error)[Role of RL in Large Language Models (LLMs)](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#role-of-rl-in-large-language-models-llms)[Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#reinforcement-learning-from-human-feedback-rlhf)[Why should we care about GRPO (Group Relative Policy Optimization)?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#why-should-we-care-about-grpo-group-relative-policy-optimization)[Quiz](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#quiz)[1. What are the key components of Reinforcement Learning?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#1-what-are-the-key-components-of-reinforcement-learning)[2. What is the main advantage of RLHF for training language models?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#2-what-is-the-main-advantage-of-rlhf-for-training-language-models)[3. In the context of RL for LLMs, what represents an “action”?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#3-in-the-context-of-rl-for-llms-what-represents-an-action)[4. What is the role of the reward in RL training of language models?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#4-what-is-the-role-of-the-reward-in-rl-training-of-language-models)[5. What is a reward in the context of RL for LLMs?](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt#5-what-is-a-reward-in-the-context-of-rl-for-llms)


---------


[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

# [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#understanding-the-deepseek-r1-paper)Understanding the DeepSeek R1 Paper

This chapter is a crash course paper reading. We will walk through the paper in simple terms, and then we will break down the key concepts and takeaways.

DeepSeek R1 represents a significant advancement in language model training, particularly in developing reasoning capabilities through reinforcement learning. The paper introduces a new reinforcement learning algorithm called Group Relative Policy Optimization (GRPO).

![DeepSeek R1 Overview](https://huggingface.co/reasoning-course/images/resolve/main/grpo/4.png)

In the next chapter, we will build on this knowledge and implement GRPO in practice.

The initial goal of the paper was to explore whether pure reinforcement learning could develop reasoning capabilities without supervised fine-tuning.

Up until that point, all the popular LLMs required some supervised fine-tuning, which we explored in [chapter 11](https://huggingface.co/course/chapter11/1).

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-breakthrough-aha-moment)The Breakthrough ‘Aha’ Moment

![The 'Aha Moment'](https://huggingface.co/reasoning-course/images/resolve/main/grpo/9.png)

One of the most remarkable discoveries in R1-Zero’s training was the emergence of a phenomenon known as the “Aha Moment.” This phenomenon is somewhat similar to how humans experience sudden realizations while problem-solving. Here’s how it works:

1. Initial Attempt: The model makes an initial attempt at solving a problem
2. Recognition: It recognizes potential errors or inconsistencies
3. Self-Correction: It adjusts its approach based on this recognition
4. Explanation: It can explain why the new approach is better

This breakthrough resonates with learners and feels like a “Eureka” moment. It demonstrates learning rather than mere memorization, so let’s take a moment to imagine what it feels like to have an “Aha” moment.

For example, imagine you’re trying to solve a puzzle:

- First try: “This piece should go here based on the color”
- Recognition: “But wait, the shape doesn’t quite fit”
- Correction: “Ah, it actually belongs over there”
- Explanation: “Because both the color and shape pattern match in this position”

This ability emerged naturally from RL training, without being explicitly programmed, demonstrating learning rather than mere memorization of a process from the training data.

The easiest way to understand the ‘Aha’ moment is to see it in action. Let’s take a look at an example. In the chat below, we ask the model to solve a problem and the UI shows the model’s thought process as it solves the problem.

If you want to try Deepseek’s R1, you can also check out [Hugging Chat](https://huggingface.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B).

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-training-process)The Training Process

Training R1 was a multi-phase process. Let’s break down the phases and the key innovations in each phase.

The final process results in two models:

- DeepSeek-R1-Zero: A model trained purely using reinforcement learning.
- DeepSeek-R1: A model that builds on the foundation of DeepSeek-R1-Zero and adds supervised fine-tuning.

|Feature|DeepSeek-R1-Zero|DeepSeek-R1|
|---|---|---|
|Training Approach|Pure RL|Multi-phase (SFT + RL)|
|Fine-tuning|None|Supervised fine-tuning|
|Reasoning Capability|Emergent|Enhanced|
|AIME Performance|71.0%|79.8%|
|Key Characteristics|Strong reasoning but readability issues|Better language consistency and readability|

While DeepSeek-R1-Zero demonstrates the potential of pure reinforcement learning for developing reasoning capabilities, DeepSeek-R1 builds upon this foundation with a more balanced approach that prioritizes both reasoning performance and usability.

The training process involves four phases:

1. Cold Start Phase
2. Reasoning RL Phase
3. Rejection Sampling Phase
4. Diverse RL Phase

Let’s break down each phase:

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#cold-start-phase-quality-foundation)Cold Start Phase (Quality Foundation)

![Cold Start Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/5.png)

This phase is designed to establish a strong foundation for the model’s readability and response quality. It uses a small dataset of high-quality samples from R1-Zero to fine-tune the V3-Base model. Starting with the DeepSeek-V3-Base model, the team used thousands of validated, high-quality samples from R1-Zero for supervised fine-tuning. This innovative approach uses a small but high quality dataset to establish strong baseline readability and response quality.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#reasoning-rl-phase-capability-building)Reasoning RL Phase (Capability Building)

![Reasoning RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/6.png)

The Reasoning RL Phase focuses on developing core reasoning capabilities across domains including mathematics, coding, science, and logic. This phase employs rule-based reinforcement learning, with rewards directly tied to solution correctness.

Crucially, all the tasks in this phase are ‘verifiable’ so we can check if the model’s answer is correct or not. For example, in the case of mathematics, we can check if the model’s answer is correct by using a mathematical solver.

What makes this phase particularly innovative is its direct optimization approach that eliminates the need for a separate reward model, streamlining the training process.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#rejection-sampling-phase-quality-control)Rejection Sampling Phase (Quality Control)

![Rejection Sampling Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/7.png)

During the Rejection Sampling Phase, the model generates samples which are then filtered through a quality control process. DeepSeek-V3 serves as the quality judge, evaluating outputs across a broad scope that extends beyond pure reasoning tasks. The filtered data is then used for supervised fine-tuning. This phase’s innovation lies in its ability to combine multiple quality signals to ensure high-standard outputs.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#diverse-rl-phase-broad-alignment)Diverse RL Phase (Broad Alignment)

![Diverse RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/8.png)

The final Diverse RL Phase tackles multiple task types using a sophisticated hybrid approach. For deterministic tasks, it employs rule-based rewards, while subjective tasks are evaluated through LLM feedback. This phase aims to achieve human preference alignment through its innovative hybrid reward approach, combining the precision of rule-based systems with the flexibility of language model evaluation.

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-algorithm-group-relative-policy-optimization-grpo)The Algorithm: Group Relative Policy Optimization (GRPO)

Now that we have a good understanding of the training process, let’s look at the algorithm that was used to train the model.

The authors describe GRPO as a breakthrough in model fine-tuning:

![GRPO Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/10.png)

GRPO’s novelty lies in its capacity to “directly optimize for preference rectification.” This signifies a more direct and efficient route to aligning the model with desired outputs, contrasting with traditional Reinforcement Learning algorithms such as PPO. Let’s break down how GRPO works through its three main components.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#group-formation-creating-multiple-solutions)Group Formation: Creating Multiple Solutions

The first step in GRPO is remarkably intuitive - it’s similar to how a student might solve a difficult problem by trying multiple approaches. When given a prompt, the model doesn’t just generate one response; instead, it creates multiple attempts at solving the same problem (usually 4, 8, or 16 different attempts).

Imagine you’re teaching a model to solve math problems. For a question about counting chickens on a farm, the model might generate several different solutions:

- One solution might break down the problem step by step: first counting total chickens, then subtracting roosters, and finally accounting for non-laying hens
- Another might use a different but equally valid approach
- Some attempts might contain mistakes or less efficient solutions

All these attempts are kept together as a group, much like having multiple students’ solutions to compare and learn from.

![Group Formation](https://huggingface.co/reasoning-course/images/resolve/main/grpo/11.jpg)

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#preference-learning-understanding-what-makes-a-good-solution)Preference Learning: Understanding What Makes a Good Solution

This is where GRPO really shines in its simplicity. Unlike other methods for RLHF that need always require a separate reward model to predict how good a solution might be, GRPO can use any function or model to evaluate the quality of a solution. For example, we could use a length function to reward shorter responses or a mathematical solver to reward accurate mathematical solutions.

The evaluation process looks at various aspects of each solution:

- Is the final answer correct?
- Did the solution follow proper formatting (like using the right XML tags)?
- Does the reasoning match the answer provided?

What makes this approach particularly clever is how it handles the scoring. Instead of just giving absolute scores, GRPO normalizes the rewards within each group. It uses a simple but effective formula for group relative advantage estimation:

Copied

Advantage = (reward - mean(group_rewards)) / std(group_rewards)

![Preference Learning](https://huggingface.co/reasoning-course/images/resolve/main/grpo/12.jpg)

This normalization is like grading on a curve, but for AI. It helps the model understand which solutions within the group were better or worse compared to their peers, rather than just looking at absolute scores.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#optimization-learning-from-experience)Optimization: Learning from Experience

The final step is where GRPO teaches the model to improve based on what it learned from evaluating the group of solutions. This process is both powerful and stable, using two main principles:

1. It encourages the model to produce more solutions like the successful ones while moving away from less effective approaches
2. It includes a safety mechanism (called KL divergence penalty) that prevents the model from changing too drastically all at once

This approach proves more stable than traditional methods because:

- It looks at multiple solutions together rather than comparing just two at a time
- The group-based normalization helps prevent issues with reward scaling
- The KL penalty acts like a safety net, ensuring the model doesn’t forget what it already knows while learning new things

GRPO’s key innovations are:

- Learning directly from any function or model, eliminating the reliance on a separate reward model.
- Group-based learning, which is more stable and efficient than traditional methods like pairwise comparisons.

This breakdown is complex, but the key takeaway is that GRPO is a more efficient and stable way to train a model to reason.

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#grpo-algorithm-in-pseudocode)GRPO Algorithm in Pseudocode

Now that we understand the key components of GRPO, let’s look at the algorithm in pseudocode. This is a simplified version of the algorithm, but it captures the key ideas.

Copied

Input: 
- initial_policy: Starting model to be trained
- reward_function: Function that evaluates outputs
- training_prompts: Set of training examples
- group_size: Number of outputs per prompt (typically 4-16)

Algorithm GRPO:
1. For each training iteration:
   a. Set reference_policy = initial_policy (snapshot current policy)
   b. For each prompt in batch:
      i. Generate group_size different outputs using initial_policy
      ii. Compute rewards for each output using reward_function
      iii. Normalize rewards within group:
           normalized_advantage = (reward - mean(rewards)) / std(rewards)
      iv. Update policy by maximizing the clipped ratio:
          min(prob_ratio * normalized_advantage, 
              clip(prob_ratio, 1-epsilon, 1+epsilon) * normalized_advantage)
          - kl_weight * KL(initial_policy || reference_policy)
          
          where prob_ratio is current_prob / reference_prob

Output: Optimized policy model

This algorithm shows how GRPO combines group-based advantage estimation with policy optimization while maintaining stability through clipping and KL divergence constraints.

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#results-and-impact)Results and Impact

Now that we’ve explored the algorithm, let’s look at the results. DeepSeek R1 achieves state-of-the-art performance across multiple domains:

|Domain|Key Results|
|---|---|
|Mathematics|• 79.8% on AIME 2024  <br>• 97.3% on MATH-500|
|Coding|• Codeforces Rating: 2029  <br>• LiveCodeBench: 65.9%|
|General Knowledge|• MMLU: 90.8%  <br>• GPQA Diamond: 71.5%|
|Language Tasks|• AlpacaEval 2.0: 87.6% win rate  <br>• FRAMES: 82.5%|

The model’s practical impact extends beyond benchmarks through its cost-effective API pricing ($0.14 per million input tokens) and successful model distillation across various sizes (1.5B to 70B parameters). Notably, even the 7B model achieves 55.5% on AIME 2024, while the 70B distilled version approaches o1-mini performance on MATH-500 (94.5%), demonstrating effective capability preservation at different scales.

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#limitations-and-challenges-of-grpo)Limitations and Challenges of GRPO

While GRPO represents a significant advancement in reinforcement learning for language models, it’s important to understand its limitations and challenges:

- **Generation Cost**: Generating multiple completions (4-16) for each prompt increases computational requirements compared to methods that generate only one or two completions.
- **Batch Size Constraints**: The need to process groups of completions together can limit effective batch sizes, adding complexity to the training process and potentially slowing down training.
- **Reward Function Design**: The quality of training heavily depends on well-designed reward functions. Poorly designed rewards can lead to unintended behaviors or optimization for the wrong objectives.
- **Group Size Tradeoffs**: Choosing the optimal group size involves balancing diversity of solutions against computational cost. Too few samples may not provide enough diversity, while too many increase training time and resource requirements.
- **KL Divergence Tuning**: Finding the right balance for the KL divergence penalty requires careful tuning - too high and the model won’t learn effectively, too low and it may diverge too far from its initial capabilities.

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#conclusion)Conclusion

The DeepSeek R1 paper represents a significant milestone in language model development. The Group Relative Policy Optimization (GRPO) algorithm has demonstrated that pure reinforcement learning can indeed develop strong reasoning capabilities, challenging previous assumptions about the necessity of supervised fine-tuning.

Perhaps most importantly, DeepSeek R1 has shown that it’s possible to balance high performance with practical considerations like cost-effectiveness and accessibility. The successful distillation of the model’s capabilities across different sizes, from 1.5B to 70B parameters, demonstrates a path forward for making advanced AI capabilities more widely available.

---

In the next section, we’ll explore practical implementations of these concepts, focusing on how to leverage GRPO and RFTrans in your own language model development projects.

## [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#quiz)Quiz

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#1-what-is-the-main-innovation-of-the-deepseek-r1-paper)1. What is the main innovation of the DeepSeek R1 paper?

 The GRPO algorithm that enables learning from preferences with and without a reward model Using more GPUs for training than any previous model Creating a larger language model than existing ones

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#2-what-are-the-four-phases-of-the-deepseek-r1-training-process)2. What are the four phases of the DeepSeek R1 training process?

 Cold Start, Reasoning RL, Rejection Sampling, and Diverse RL Pre-training, Fine-tuning, Testing, and Deployment Data Collection, Model Training, Evaluation, and Optimization

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#3-what-is-the-aha-moment-phenomenon-in-r1-zeros-training)3. What is the ‘Aha Moment’ phenomenon in R1-Zero’s training?

 A process where the model recognizes errors, self-corrects, and explains its corrections The point where the model reaches human-level performance When the model completes its training process

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#4-how-does-grpos-group-formation-work)4. How does GRPO’s group formation work?

 It generates multiple solutions (4-16) for the same problem and evaluates them together It combines multiple models into one ensemble It splits the training data into different groups

Submit

### [](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#5-what-is-the-key-difference-between-deepseek-r1-zero-and-deepseek-r1)5. What is the key difference between DeepSeek-R1-Zero and DeepSeek-R1?

 R1-Zero uses pure RL while R1 combines RL with supervised fine-tuning R1-Zero is smaller than R1 R1-Zero was trained on less data

Submit

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/3.mdx)

[←Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath→](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)

[Understanding the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#understanding-the-deepseek-r1-paper)[The Breakthrough ‘Aha’ Moment](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-breakthrough-aha-moment)[The Training Process](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-training-process)[Cold Start Phase (Quality Foundation)](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#cold-start-phase-quality-foundation)[Reasoning RL Phase (Capability Building)](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#reasoning-rl-phase-capability-building)[Rejection Sampling Phase (Quality Control)](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#rejection-sampling-phase-quality-control)[Diverse RL Phase (Broad Alignment)](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#diverse-rl-phase-broad-alignment)[The Algorithm: Group Relative Policy Optimization (GRPO)](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#the-algorithm-group-relative-policy-optimization-grpo)[Group Formation: Creating Multiple Solutions](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#group-formation-creating-multiple-solutions)[Preference Learning: Understanding What Makes a Good Solution](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#preference-learning-understanding-what-makes-a-good-solution)[Optimization: Learning from Experience](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#optimization-learning-from-experience)[GRPO Algorithm in Pseudocode](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#grpo-algorithm-in-pseudocode)[Results and Impact](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#results-and-impact)[Limitations and Challenges of GRPO](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#limitations-and-challenges-of-grpo)[Conclusion](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#conclusion)[Quiz](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#quiz)[1. What is the main innovation of the DeepSeek R1 paper?](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#1-what-is-the-main-innovation-of-the-deepseek-r1-paper)[2. What are the four phases of the DeepSeek R1 training process?](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#2-what-are-the-four-phases-of-the-deepseek-r1-training-process)[3. What is the ‘Aha Moment’ phenomenon in R1-Zero’s training?](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#3-what-is-the-aha-moment-phenomenon-in-r1-zeros-training)[4. How does GRPO’s group formation work?](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#4-how-does-grpos-group-formation-work)[5. What is the key difference between DeepSeek-R1-Zero and DeepSeek-R1?](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt#5-what-is-the-key-difference-between-deepseek-r1-zero-and-deepseek-r1)


---------


[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

# [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#advanced-understanding-of-group-relative-policy-optimization-grpo-in-deepseekmath)Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath

This section dives into the technical and mathematical details of GRPO. It was authored by [Shirin Yamani](https://github.com/shirinyamani).

Let’s deepen our understanding of GRPO so that we can improve our model’s training process.

GRPO directly evaluates the model-generated responses by comparing them within groups of generation to optimize policy model, instead of training a separate value model (Critic). This approach leads to significant reduction in computational cost!

GRPO can be applied to any verifiable task where the correctness of the response can be determined. For instance, in math reasoning, the correctness of the response can be easily verified by comparing it to the ground truth.

Before diving into the technical details, let’s visualize how GRPO works at a high level:

![deep](https://huggingface.co/reasoning-course/images/resolve/main/grpo/16.png)

Now that we have a visual overview, let’s break down how GRPO works step by step.

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#the-grpo-algorithm)The GRPO Algorithm

The core innovation of GRPO is its approach to evaluating and learning from multiple generated responses simultaneously. Instead of relying on a separate reward model, it compares outputs within the same group to determine which ones should be reinforced.

Let’s walk through each step of the algorithm in detail:

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-1-group-sampling)Step 1: Group Sampling

The first step is to generate multiple possible answers for each question. This creates a diverse set of outputs that can be compared against each other.

For each questionqq, the model will generate GG outputs (group size) from the trained policy: {o1,o2,o3,…,oGπθoldo1​,o2​,o3​,…,oG​πθold​​ },G=8G=8 where eachoioi​ represents one completion from the model.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example)Example

To make this concrete, let’s look at a simple arithmetic problem:

**Question** qq :Calculate 2+2×6Calculate 2+2×6

**Outputs** (G=8)(G=8):{o1:14 (correct),o2:16 (wrong),o3:10 (wrong),…,o8:14 (correct)}{o1​:14 (correct),o2​:16 (wrong),o3​:10 (wrong),…,o8​:14 (correct)}

Notice how some of the generated answers are correct (14) while others are wrong (16 or 10). This diversity is crucial for the next step.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-2-advantage-calculation)Step 2: Advantage Calculation

Once we have multiple responses, we need a way to determine which ones are better than others. This is where the advantage calculation comes in.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#reward-distribution)Reward Distribution

First, we assign a reward score to each generated response. In this example, we’ll use a reward model, but as we learnt in the previous section, we can use any reward returning function.

Assign a RM score to each of the generated responses based on the correctnessriri​ _(e.g. 1 for correct response, 0 for wrong response)_ then for each of theriri​ calculate the following Advantage value.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#advantage-value-formula)Advantage Value Formula

The key insight of GRPO is that we don’t need absolute measures of quality - we can compare outputs within the same group. This is done using standardization:Ai=ri−mean({r1,r2,…,rG})std({r1,r2,…,rG})Ai​=std({r1​,r2​,…,rG​})ri​−mean({r1​,r2​,…,rG​})​

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example)Example

Continuing with our arithmetic example for the same example above, imagine we have 8 responses, 4 of which is correct and the rest wrong, therefore;

|Metric|Value|
|---|---|
|Group Average|mean(ri)=0.5mean(ri​)=0.5|
|Standard Deviation|std(ri)=0.53std(ri​)=0.53|
|Advantage Value (Correct response)|Ai=1−0.50.53=0.94Ai​=0.531−0.5​=0.94|
|Advantage Value (Wrong response)|Ai=0−0.50.53=−0.94Ai​=0.530−0.5​=−0.94|

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)Interpretation

Now that we have calculated the advantage values, let’s understand what they mean:

This standardization (i.e.AiAi​ weighting) allows the model to assess each response’s relative performance, guiding the optimization process to favorable responses that are better than average (high reward) and discourage those that are worse. For instance ifAi>0Ai​>0, then theoioi​ is better response than the average level within its group; and ifAi<0Ai​<0, then theoioi​ then the quality of the response is less than the average (i.e. poor quality/performance).

For the example above, ifAi=0.94(correct output)Ai​=0.94(correct output) then during optimization steps its generation probability will be increased.

With our advantage values calculated, we’re now ready to update the policy.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-3-policy-update)Step 3: Policy Update

The final step is to use these advantage values to update our model so that it becomes more likely to generate good responses in the future.

The target function for policy update is:JGRPO(θ)=[1G∑i=1Gmin⁡(πθ(oi∣q)πθold(oi∣q)Aiclip(πθ(oi∣q)πθold(oi∣q),1−ϵ,1+ϵ)Ai)]−βDKL(πθ∥∥πref)JGRPO​(θ)=[G1​i=1∑G​min(πθold​​(oi​∣q)πθ​(oi​∣q)​Ai​clip(πθold​​(oi​∣q)πθ​(oi​∣q)​,1−ϵ,1+ϵ)Ai​)]−βDKL​(πθ​∥∥πref​)

This formula might look intimidating at first, but it’s built from several components that each serve an important purpose. Let’s break them down one by one.

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#key-components-of-the-target-function)Key Components of the Target Function

The GRPO update function combines several techniques to ensure stable and effective learning. Let’s examine each component:

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#1-probability-ratio)1. Probability Ratio

The probability ratio is defined as: (πθ(oi∣q)πθold(oi∣q))(πθold​​(oi​∣q)πθ​(oi​∣q)​)

Intuitively, the formula compares how much the new model’s response probability differs from the old model’s response probability while incorporating a preference for responses that improve the expected outcome.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)Interpretation

- If ratio>1ratio>1, the new model assigns a higher probability to responseoioi​ than the old model.
- If ratio<1ratio<1, the new model assigns a lower probability tooioi​

This ratio allows us to control how much the model changes at each step, which leads us to the next component.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#2-clip-function)2. Clip Function

The clipping function is defined as: clip(πθ(oi∣q)πθold(oi∣q),1−ϵ,1+ϵ)clip(πθold​​(oi​∣q)πθ​(oi​∣q)​,1−ϵ,1+ϵ)

Limit the ratio discussed above to be within[1−ϵ,1+ϵ][1−ϵ,1+ϵ] to avoid/control drastic changes or crazy updates and stepping too far off from the old policy. In other words, it limit how much the probability ratio can increase to help maintaining stability by avoiding updates that push the new model too far from the old one.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example-%CE%B5--02)Example (ε = 0.2)

Let’s look at two different scenarios to better understand this clipping function:

- **Case 1**: if the new policy has a probability of 0.9 for a specific response and the old policy has a probabiliy of 0.5, it means this response is getting reinforeced by the new policy to have higher probability, but within a controlled limit which is the clipping to tight up its hands to not get drastic -Ratio:πθ(oi∣q)πθold(oi∣q)=0.90.5=1.8→Clip 1.2Ratio:πθold​​(oi​∣q)πθ​(oi​∣q)​=0.50.9​=1.8→Clip 1.2 (upper bound limit 1.2)
- **Case 2**: If the new policy is not in favour of a response (lower probability e.g. 0.2), meaning if the response is not beneficial the increase might be incorrect, and the model would be penalized. -Ratio:πθ(oi∣q)πθold(oi∣q)=0.20.5=0.4→Clip 0.8Ratio:πθold​​(oi​∣q)πθ​(oi​∣q)​=0.50.2​=0.4→Clip 0.8 (lower bound limit 0.8)

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)Interpretation

- The formula encourages the new model to favour responses that the old model underweighted **if they improve the outcome**.
- If the old model already favoured a response with a high probability, the new model can still reinforce it **but only within a controlled limit[1−ϵ,1+ϵ][1−ϵ,1+ϵ],(e.g., ϵ=0.2, so [0.8−1.2])(e.g., ϵ=0.2, so [0.8−1.2])**.
- If the old model overestimated a response that performs poorly, the new model is **discouraged** from maintaining that high probability.
- Therefore, intuitively, By incorporating the probability ratio, the objective function ensures that updates to the policy are proportional to the advantageAiAi​ while being moderated to prevent drastic changes. T

While the clipping function helps prevent drastic changes, we need one more safeguard to ensure our model doesn’t deviate too far from its original behavior.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#3-kl-divergence)3. KL Divergence

The KL divergence term is: βDKL(πθ∥∥πref)βDKL​(πθ​∥∥πref​)

In the KL divergence term, theπrefπref​ is basically the pre-update model’s output, `per_token_logps` andπθπθ​ is the new model’s output, `new_per_token_logps`. Theoretically, KL divergence is minimized to prevent the model from deviating too far from its original behavior during optimization. This helps strike a balance between improving performance based on the reward signal and maintaining coherence. In this context, minimizing KL divergence reduces the risk of the model generating nonsensical text or, in the case of mathematical reasoning, producing extremely incorrect answers.

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)Interpretation

- A KL divergence penalty keeps the model’s outputs close to its original distribution, preventing extreme shifts.
- Instead of drifting towards completely irrational outputs, the model would refine its understanding while still allowing some exploration

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#math-definition)Math Definition

For those interested in the mathematical details, let’s look at the formal definition:

Recall that KL distance is defined as follows:DKL(P∥∥Q)=∑x∈XP(x)log⁡P(x)Q(x)DKL​(P∥∥Q)=x∈X∑​P(x)logQ(x)P(x)​In RLHF, the two distributions of interest are often the distribution of the new model version, P(x), and a distribution of the reference policy, Q(x).

#### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#the-role-of-%CE%B2-parameter)The Role of β Parameter

The coefficientββ controls how strongly we enforce the KL divergence constraint:

- **Higher β (Stronger KL Penalty)**
    - More constraint on policy updates. The model remains close to its reference distribution.
    - Can slow down adaptation: The model may struggle to explore better responses.
- **Lower β (Weaker KL Penalty)**
    - More freedom to update policy: The model can deviate more from the reference.
    - Faster adaptation but risk of instability: The model might learn reward-hacking behaviors.
    - Over-optimization risk: If the reward model is flawed, the policy might generate nonsensical outputs.
- **Original** [DeepSeekMath](https://arxiv.org/abs/2402.03300) paper set thisβ=0.04β=0.04

Now that we understand the components of GRPO, let’s see how they work together in a complete example.

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#worked-example-with-grpo)Worked Example with GRPO

To solidify our understanding of GRPO, let’s walk through a complete example from start to finish.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example-problem)Example Problem

Q: Calculate 2+2×6Q: Calculate 2+2×6

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-1-group-sampling)Step 1: Group Sampling

First, we generate multiple responses from our model.

Generate (G=8)(G=8) responses,44 of which are correct answer (\( 14, \text{reward=} 1 \)) and44 incorrect(reward= 0)(reward= 0), Therefore:o1:14(correct),o2:10(wrong),o3:16(wrong),...oG:14(correct)o1​:14(correct),o2​:10(wrong),o3​:16(wrong),...oG​:14(correct)

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-2-advantage-calculation)Step 2: Advantage Calculation

Next, we calculate the advantage values to determine which responses are better than average:

|Statistic|Value|
|---|---|
|Group Average|mean(ri)=0.5mean(ri​)=0.5|
|Standard Deviation|std(ri)=0.53std(ri​)=0.53|
|Advantage Value (Correct response)|Ai=1−0.50.53=0.94Ai​=0.531−0.5​=0.94|
|Advantage Value (Wrong response)|Ai=0−0.50.53=−0.94Ai​=0.530−0.5​=−0.94|

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-3-policy-update)Step 3: Policy Update

Finally, we update our model to reinforce the correct responses:

- Assuming the probability of old policy (\( \pi_{\theta_{old}} \)) for a correct outputo1o1​ is0.50.5 and the new policy increases it to0.70.7 then:Ratio:0.70.5=1.4→after Clip 1.2 (ϵ=0.2)Ratio:0.50.7​=1.4→after Clip 1.2 (ϵ=0.2)
- Then when the target function is re-weighted, the model tends to reinforce the generation of correct output, and theKL DivergenceKL Divergence limits the deviation from the reference policy.

With the theoretical understanding in place, let’s see how GRPO can be implemented in code.

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#implementation-example)Implementation Example

Let’s put everything together in a practical example. The following code demonstrates how to implement GRPO in PyTorch.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#1-loading-the-model-and-generating-responses)1. Loading the Model and Generating Responses

First, we need to load a model and generate multiple responses for a given question:

Copied

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model_name = "Qwen/Qwen2-Math-1.5B"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model.eval()

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Input prompt
prompt = "Solve y = 2x + 1 for x = 2, y = "  # Correct answer: 5
inputs = tokenizer(prompt, return_tensors="pt", padding=True)
input_ids = inputs["input_ids"].to(device)  # Shape: (1, prompt_len)
attention_mask = inputs["attention_mask"].to(device)

# Step 1: Generate 8 responses (B = 2 groups, G = 4 responses per group)
batch_size, num_generations = 2, 4
outputs = model.generate(
    input_ids=input_ids,  # Shape: (1, prompt_len)
    attention_mask=attention_mask,
    max_new_tokens=1,  # seq_len = 1 (single token per response)
    num_return_sequences=batch_size * num_generations,  # 8 responses total
    do_sample=True,
    top_k=10,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id,
    return_dict_in_generate=True,
    output_scores=True,
)

this initial Generation (Before Any Steps) will output sth like this:

Copied

Output 1: 5.0
Output 2: 6.0
Output 3: 7.0
Output 4: 5.0
Output 5: 10.0
Output 6: 2.0
Output 7: 5.0
Output 8: 5.0

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#2-calculating-rewards)2. Calculating Rewards

Now, we need to determine which responses are correct and assign rewards accordingly:

With GRPO, with the same sample prompt, we generate multiple completions. So for instance, for our prompts of `"Solve y = 2x + 1 for x = 2, y = "` and `Solve y = 2x + 1 for x = 4, y = "` we have two group of generated outputs for the given prompt one is say

- `[5, 6, 7, 5]` and the other is
- `[10, 2, 9, 9]` while the correct answer is 5 and 9.

Note that in practice these reward scores are achieved by a rule-based reward function that assigns rewards based on the correctness of the response or a more complex neural network-based model that can be trained to assign rewards based on the correctness of the response or a mixed of both. But for sake of simplicity let’s say our reward per response is 1 if the response is correct and 0 if it is wrong, therefore;

Copied

reward_1 = [1, 0, 0, 1]
reward_2 = [0, 0, 1, 1]

next we get the group_wise mean and std of the rewards;

Copied

# Shape: (B * G,) = (8,) bc we have 2 groups of 4 generations that we flatten
rewards = torch.tensor([1, 0, 0, 1, 0, 0, 1, 1], dtype=torch.float32)
num_generations = 4

# Group rewards: Shape (B, G) = 2, 4)
rewards_grouped = rewards.view(-1, num_generations)

# Mean per group: Shape (B,) = (2,)
mean_grouped_rewards = rewards_grouped.mean(dim=1)

# Std per group: Shape (B,) = (2,)
std_grouped_rewards = rewards_grouped.std(dim=1)

# Broadcast to match rewards and normalize: Shape (B * G,) = (8,)
# why we need to broadcast? because we need to calculate the advantage values for each response within the group
mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_generations, dim=0)
std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_generations, dim=0)

this will output:

Copied

Grouped Rewards: tensor([[1., 0., 0., 1.],
                        [0., 0., 1., 1.]])
Mean per group: tensor([0.5000, 0.5000])
Std per group: tensor([0.5774, 0.5774])
Broadcasted Mean: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000])
Broadcasted Std: tensor([0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774])

Now we can calculate the advantage values for each response:

Copied

# Advantages: Shape (B * G,) = (8,)
advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-8)

this will output:

Copied

Advantages: tensor([ 0.8659, -0.8660, -0.8660,  0.8659, -0.8660, -0.8660,  0.8659,  0.8659])

which is coming from the Advantage formula above, so:

Copied

For reward_1 = [1, 0, 0, 1]:
1 - 0.5 / 0.5774 ≈ 0.8659
0 - 0.5 / 0.5774 ≈ -0.8660
For reward_2 = [0, 0, 1, 1]: Same pattern.

however, the shape here is `(B*G,) = (8,)` but in practice, we need to have the shape of `(B, G) = (2, 4)` to match the logits shape, right? Therefore, we need to unsqueeze the advantages tensor to have the shape of `(B*G, 1) = (8, 1)` to match the logits shape.

Copied

# Shape (B * G, 1) = (8, 1) to match the logits shape
advantages = advantages.unsqueeze(1)

which will output:

Copied

Advantages: tensor([[ 0.8659],
                    [-0.8660],
                    [-0.8660],
                    [ 0.8659],
                    [-0.8660],
                    [-0.8660],
                    [ 0.8659],
                    [ 0.8659]])

now we are good, let’s move to the next step of updating the policy model based on the advantage values.

### [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#3-updating-the-policy)3. Updating the Policy

Finally, we use the advantage values to update our model:

Copied

# Compute probability ratio between new and old policies
ratio = torch.exp(
    new_per_token_logps - per_token_logps
)  # Shape: (B*G, seq_len) seq_len is the length of the output i.e. the num of generated tokens so here for simplicity let's assume it is 1 # (8, 1)

Note that the `per_token_logps` can be achieved by passing the generated outputs to the model and get the logits and then apply the softmax function to get the probabilities `F.softmax(logits, dim=-1)`.

Copied

# Clipping Function
eps = self.cliprange  # e.g. 0.2
pg_losses1 = -advantages * ratio  # Shape: (B*G, seq_len)  #(8, 1)
pg_losses2 = -advantages * torch.clamp(
    ratio, 1.0 - eps, 1.0 + eps
)  # Shape: (B*G, seq_len) #(8, 1)
pg_loss_max = torch.max(pg_losses1, pg_losses2)  # Shape: (B*G, seq_len) #(8, 1)

# Now Combine with KL penalty # Shape: (B*G, seq_len) #(8, 1)
per_token_loss = pg_loss_max + self.beta * per_token_kl

`per_token_kl` can also be calculated as follows:

Copied

# Shape: (B*G, seq_len) #(8, 1)
per_token_kl = F.kl_div(
    F.log_softmax(new_per_token_logps, dim=-1),
    F.softmax(per_token_logps, dim=-1),
    reduction="none",
).sum(dim=-1, keepdim=True)

Complete example can be found [here](https://huggingface.co/learn/llm-course/chapter12/basic_example.py). GRPO is also implemented by the excellent TRL team, you can check the implementation [TRL/GRPO_trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py) for more details.

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#summary-and-next-steps)Summary and Next Steps

Congratulations! You’ve now learned about Group Relative Policy Optimization (GRPO). To recap what we’ve covered:

1. GRPO compares multiple outputs within a group to determine which ones are better than others, without requiring a separate value model.
2. The advantage calculation standardizes rewards to identify which responses are above or below average.
3. The policy update uses a clipped objective function with a KL divergence penalty to ensure stable learning.

This approach is particularly powerful for mathematical reasoning tasks, where correctness can be objectively verified. The GRPO method allows for more efficient training compared to traditional RLHF approaches that require a separate critic model.

As you continue exploring GRPO, consider experimenting with different group sizes, reward functions, and KL penalty coefficients to see how they affect your model’s performance.

Happy training! 🚀

## [](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#references)References

1. [RLHF Book by Nathan Lambert](https://github.com/natolambert/rlhf-book)
2. [DeepSeek-V3 Technical Report](https://huggingface.co/papers/2412.19437)
3. [DeepSeekMath](https://huggingface.co/papers/2402.03300)

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/3a.mdx)

[←The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Implementing GRPO in TRL→](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)

[Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#advanced-understanding-of-group-relative-policy-optimization-grpo-in-deepseekmath)[The GRPO Algorithm](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#the-grpo-algorithm)[Step 1: Group Sampling](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-1-group-sampling)[Example](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example)[Step 2: Advantage Calculation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-2-advantage-calculation)[Reward Distribution](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#reward-distribution)[Advantage Value Formula](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#advantage-value-formula)[Example](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example)[Interpretation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)[Step 3: Policy Update](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-3-policy-update)[Key Components of the Target Function](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#key-components-of-the-target-function)[1. Probability Ratio](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#1-probability-ratio)[Interpretation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)[2. Clip Function](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#2-clip-function)[Example (ε = 0.2)](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example-%CE%B5--02)[Interpretation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)[3. KL Divergence](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#3-kl-divergence)[Interpretation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#interpretation)[Math Definition](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#math-definition)[The Role of β Parameter](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#the-role-of-%CE%B2-parameter)[Worked Example with GRPO](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#worked-example-with-grpo)[Example Problem](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#example-problem)[Step 1: Group Sampling](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-1-group-sampling)[Step 2: Advantage Calculation](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-2-advantage-calculation)[Step 3: Policy Update](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#step-3-policy-update)[Implementation Example](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#implementation-example)[1. Loading the Model and Generating Responses](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#1-loading-the-model-and-generating-responses)[2. Calculating Rewards](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#2-calculating-rewards)[3. Updating the Policy](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#3-updating-the-policy)[Summary and Next Steps](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#summary-and-next-steps)[References](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt#references)


---------


[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

# [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#implementing-grpo-in-trl)Implementing GRPO in TRL

In this page, we’ll learn how to implement Group Relative Policy Optimization (GRPO) using the Transformer Reinforcement Learning (TRL) library. We’ll focus on practical implementation with minimal code.

We’ll explore the core concepts of GRPO as they are embodied in TRL’s GRPOTrainer, using snippets from the official TRL documentation to guide us.

This chapter is aimed at TRL beginners. If you are already familiar with TRL, you might want to also check out the [Open R1 implementation](https://github.com/huggingface/open-r1/blob/main/src/open_r1/grpo.py) of GRPO.

First, let’s remind ourselves of some of the important concepts of GRPO algorithm:

- Group Formation: The model generates multiple completions for each prompt.
- Preference Learning: The model learns from a reward function that compares groups of completions.
- Training Configuration: The model uses a configuration to control the training process.

What do we need to do to implement GRPO?

- Define a dataset of prompts.
- Define a reward function that takes a list of completions and returns a list of rewards.
- Configure the training process with a GRPOConfig.
- Train the model using the GRPOTrainer.

Here’s a minimal example to get started with GRPO training:

Copied

from trl import GRPOTrainer, GRPOConfig
from datasets import load_dataset

# 1. Load your dataset
dataset = load_dataset("your_dataset", split="train")

# 2. Define a simple reward function
def reward_func(completions, **kwargs):
    """Example: Reward longer completions"""
    return [float(len(completion)) for completion in completions]

# 3. Configure training
training_args = GRPOConfig(
    output_dir="output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    logging_steps=10,
)

# 4. Initialize and train
trainer = GRPOTrainer(
    model="your_model",  # e.g. "Qwen/Qwen2-0.5B-Instruct"
    args=training_args,
    train_dataset=dataset,
    reward_funcs=reward_func,
)
trainer.train()

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#key-components)Key Components

### [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#1-dataset-format)1. Dataset Format

Your dataset should contain prompts that the model will respond to. The GRPO trainer will generate multiple completions for each prompt and use the reward function to compare them.

### [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#2-reward-function)2. Reward Function

The reward function is crucial - it determines how the model learns. Here are two practical examples:

Copied

# Example 1: Reward based on completion length
def reward_length(completions, **kwargs):
    return [float(len(completion)) for completion in completions]

# Example 2: Reward based on matching a pattern
import re

def reward_format(completions, **kwargs):
    pattern = r"^<think>.*?</think><answer>.*?</answer>$"
    return [1.0 if re.match(pattern, c) else 0.0 for c in completions]

### [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#3-training-configuration)3. Training Configuration

Key parameters to consider in `GRPOConfig`:

Copied

training_args = GRPOConfig(
    # Essential parameters
    output_dir="output",
    num_train_epochs=3,
    num_generation=4,  # Number of completions to generate for each prompt
    per_device_train_batch_size=4,  # We want to get all generations in one device batch
    # Optional but useful
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    logging_steps=10,
    # GRPO specific (optional)
    use_vllm=True,  # Speed up generation
)

The `num_generation` parameter is particularly important for GRPO as it defines the group size - how many different completions the model will generate for each prompt. This is a key differentiator from other RL methods:

- Too small (e.g., 2-3): May not provide enough diversity for meaningful comparisons
- Recommended (4-16): Provides good balance between diversity and computational efficiency
- Larger values: May improve learning but significantly increases computational cost

The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16).

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#tips-for-success)Tips for Success

1. **Memory Management**: Adjust `per_device_train_batch_size` and `gradient_accumulation_steps` based on your GPU memory.
2. **Speed**: Enable `use_vllm=True` for faster generation if your model is supported.
3. **Monitoring**: Watch the logged metrics during training:
    - `reward`: Average reward across completions
    - `reward_std`: Standard deviation within reward groups
    - `kl`: KL divergence from reference model

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#reward-function-design)Reward Function Design

The DeepSeek R1 paper demonstrates several effective approaches to reward function design that you can adapt for your own GRPO implementation:

### [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#1-length-based-rewards)1. Length-Based Rewards

One of the easiest rewards to implement is a length-based reward. You can reward longer completions:

Copied

def reward_len(completions, **kwargs):
    ideal_length = 20
    return [-abs(ideal_length - len(completion)) for completion in completions]

This reward function penalizes completions that are too short or too long, encouraging the model to generate completions that are close to the ideal length of 20 tokens.

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#2-rule-based-rewards-for-verifiable-tasks)2. Rule-Based Rewards for Verifiable Tasks

For tasks with objectively correct answers (like mathematics or coding), you can implement rule-based reward functions:

Copied

def problem_reward(completions, answers, **kwargs):
    """Reward function for math problems with verifiable answers
    completions: list of completions to evaluate
    answers: list of answers to the problems from the dataset
    """

    rewards = []
    for completion, correct_answer in zip(completions, answers):
        # Extract the answer from the completion
        try:
            # This is a simplified example - you'd need proper parsing
            answer = extract_final_answer(completion)
            # Binary reward: 1 for correct, 0 for incorrect
            reward = 1.0 if answer == correct_answer else 0.0
            rewards.append(reward)
        except:
            # If we can't parse an answer, give a low reward
            rewards.append(0.0)

    return rewards

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#3-format-based-rewards)3. Format-Based Rewards

You can also reward proper formatting, which was important in the DeepSeek R1 training:

Copied

def format_reward(completions, **kwargs):
    """Reward completions that follow the desired format"""
    # Example: Check if the completion follows a think-then-answer format
    pattern = r"<think>(.*?)</think>\s*<answer>(.*?)</answer>"

    rewards = []
    for completion in completions:
        match = re.search(pattern, completion, re.DOTALL)
        if match:
            # Check if there's substantial content in both sections
            think_content = match.group(1).strip()
            answer_content = match.group(2).strip()

            if len(think_content) > 20 and len(answer_content) > 0:
                rewards.append(1.0)
            else:
                rewards.append(
                    0.5
                )  # Partial reward for correct format but limited content
        else:
            rewards.append(0.0)  # No reward for incorrect format

    return rewards

These examples demonstrate how you can implement reward functions inspired by the DeepSeek R1 training process, focusing on correctness, formatting, and combined signals.

## [](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#thats-it)That’s it!

In the next section, you will follow an exercise to implement GRPO in TRL.

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/4.mdx)

[←Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Practical Exercise to Fine-tune a model with GRPO→](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)

[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#implementing-grpo-in-trl)[Key Components](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#key-components)[1. Dataset Format](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#1-dataset-format)[2. Reward Function](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#2-reward-function)[3. Training Configuration](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#3-training-configuration)[Tips for Success](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#tips-for-success)[Reward Function Design](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#reward-function-design)[1. Length-Based Rewards](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#1-length-based-rewards)[2. Rule-Based Rewards for Verifiable Tasks](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#2-rule-based-rewards-for-verifiable-tasks)[3. Format-Based Rewards](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#3-format-based-rewards)[That’s it!](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt#thats-it)


----------

[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

[![Ask a Question](https://img.shields.io/badge/Ask%20a%20question-ffcb4c.svg?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgLTEgMTA0IDEwNiI+PGRlZnM+PHN0eWxlPi5jbHMtMXtmaWxsOiMyMzFmMjA7fS5jbHMtMntmaWxsOiNmZmY5YWU7fS5jbHMtM3tmaWxsOiMwMGFlZWY7fS5jbHMtNHtmaWxsOiMwMGE5NGY7fS5jbHMtNXtmaWxsOiNmMTVkMjI7fS5jbHMtNntmaWxsOiNlMzFiMjM7fTwvc3R5bGU+PC9kZWZzPjx0aXRsZT5EaXNjb3Vyc2VfbG9nbzwvdGl0bGU+PGcgaWQ9IkxheWVyXzIiPjxnIGlkPSJMYXllcl8zIj48cGF0aCBjbGFzcz0iY2xzLTEiIGQ9Ik01MS44NywwQzIzLjcxLDAsMCwyMi44MywwLDUxYzAsLjkxLDAsNTIuODEsMCw1Mi44MWw1MS44Ni0uMDVjMjguMTYsMCw1MS0yMy43MSw1MS01MS44N1M4MCwwLDUxLjg3LDBaIi8+PHBhdGggY2xhc3M9ImNscy0yIiBkPSJNNTIuMzcsMTkuNzRBMzEuNjIsMzEuNjIsMCwwLDAsMjQuNTgsNjYuNDFsLTUuNzIsMTguNEwzOS40LDgwLjE3YTMxLjYxLDMxLjYxLDAsMSwwLDEzLTYwLjQzWiIvPjxwYXRoIGNsYXNzPSJjbHMtMyIgZD0iTTc3LjQ1LDMyLjEyYTMxLjYsMzEuNiwwLDAsMS0zOC4wNSw0OEwxOC44Niw4NC44MmwyMC45MS0yLjQ3QTMxLjYsMzEuNiwwLDAsMCw3Ny40NSwzMi4xMloiLz48cGF0aCBjbGFzcz0iY2xzLTQiIGQ9Ik03MS42MywyNi4yOUEzMS42LDMxLjYsMCwwLDEsMzguOCw3OEwxOC44Niw4NC44MiwzOS40LDgwLjE3QTMxLjYsMzEuNiwwLDAsMCw3MS42MywyNi4yOVoiLz48cGF0aCBjbGFzcz0iY2xzLTUiIGQ9Ik0yNi40Nyw2Ny4xMWEzMS42MSwzMS42MSwwLDAsMSw1MS0zNUEzMS42MSwzMS42MSwwLDAsMCwyNC41OCw2Ni40MWwtNS43MiwxOC40WiIvPjxwYXRoIGNsYXNzPSJjbHMtNiIgZD0iTTI0LjU4LDY2LjQxQTMxLjYxLDMxLjYxLDAsMCwxLDcxLjYzLDI2LjI5YTMxLjYxLDMxLjYxLDAsMCwwLTQ5LDM5LjYzbC0zLjc2LDE4LjlaIi8+PC9nPjwvZz48L3N2Zz4=)](https://discuss.huggingface.co/t/chapter-2-questions)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/course/en/chapter12/grpo_finetune.ipynb)

# [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#practical-exercise-fine-tune-a-model-with-grpo)Practical Exercise: Fine-tune a model with GRPO

Now that you’ve seen the theory, let’s put it into practice! In this exercise, you’ll fine-tune a model with GRPO.

This exercise was written by LLM fine-tuning expert [@mlabonne](https://huggingface.co/mlabonne).

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#install-dependencies)Install dependencies

First, let’s install the dependencies for this exercise.

Copied

!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off
!pip install -qqq flash-attn --no-build-isolation --progress-bar off

Now we’ll import the necessary libraries.

Copied

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#import-and-log-in-to-weights--biases)Import and log in to Weights & Biases

Weights & Biases is a tool for logging and monitoring your experiments. We’ll use it to log our fine-tuning process.

Copied

import wandb

wandb.login()

You can do this exercise without logging in to Weights & Biases, but it’s recommended to do so to track your experiments and interpret the results.

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-the-dataset)Load the dataset

Now, let’s load the dataset. In this case, we’ll use the [`mlabonne/smoltldr`](https://huggingface.co/datasets/mlabonne/smoltldr) dataset, which contains a list of short stories.

Copied

dataset = load_dataset("mlabonne/smoltldr")
print(dataset)

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-model)Load model

Now, let’s load the model.

For this exercise, we’ll use the [`SmolLM2-135M`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) model.

This is a small 135M parameter model that runs on limited hardware. This makes the model ideal for learning, but it’s not the most powerful model out there. If you have access to more powerful hardware, you can try to fine-tune a larger model like [`SmolLM2-1.7B`](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B).

Copied

model_id = "HuggingFaceTB/SmolLM-135M-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-lora)Load LoRA

Now, let’s load the LoRA configuration. We’ll take advantage of LoRA to reduce the number of trainable parameters, and in turn the memory footprint we need to fine-tune the model.

If you’re not familiar with LoRA, you can read more about it in [Chapter 11](https://huggingface.co/learn/course/en/chapter11/3).

Copied

# Load LoRA
lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=16,
    lora_alpha=32,
    target_modules="all-linear",
)
model = get_peft_model(model, lora_config)
print(model.print_trainable_parameters())

Copied

Total trainable parameters: 135M

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#define-the-reward-function)Define the reward function

As mentioned in the previous section, GRPO can use any reward function to improve the model. In this case, we’ll use a simple reward function that encourages the model to generate text that is 50 tokens long.

Copied

# Reward function
ideal_length = 50

def reward_len(completions, **kwargs):
    return [-abs(ideal_length - len(completion)) for completion in completions]

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#define-the-training-arguments)Define the training arguments

Now, let’s define the training arguments. We’ll use the `GRPOConfig` class to define the training arguments in a typical `transformers` style.

If this is the first time you’re defining training arguments, you can check the [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainingarguments) class for more information, or [Chapter 2](https://huggingface.co/learn/course/en/chapter2/1) for a detailed introduction.

Copied

# Training arguments
training_args = GRPOConfig(
    output_dir="GRPO",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_prompt_length=512,
    max_completion_length=96,
    num_generations=8,
    optim="adamw_8bit",
    num_train_epochs=1,
    bf16=True,
    report_to=["wandb"],
    remove_unused_columns=False,
    logging_steps=1,
)

Now, we can initialize the trainer with model, dataset, and training arguments and start training.

Copied

# Trainer
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[reward_len],
    args=training_args,
    train_dataset=dataset["train"],
)

# Train model
wandb.init(project="GRPO")
trainer.train()

Training takes around 1 hour on a single A10G GPU which is available on Google Colab or via Hugging Face Spaces.

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#push-the-model-to-the-hub-during-training)Push the model to the Hub during training

If we set the `push_to_hub` argument to `True` and the `model_id` argument to a valid model name, the model will be pushed to the Hugging Face Hub whilst we’re training. This is useful if you want to start vibe testing the model straight away!

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#interpret-training-results)Interpret training results

`GRPOTrainer` logs the reward from your reward function, the loss, and a range of other metrics.

We will focus on the reward from the reward function and the loss.

As you can see, the reward from the reward function moves closer to 0 as the model learns. This is a good sign that the model is learning to generate text of the correct length.

![Reward from reward function](https://huggingface.co/reasoning-course/images/resolve/main/grpo/13.png)

You might notice that the loss starts at zero and then increases during training, which may seem counterintuitive. This behavior is expected in GRPO and is directly related to the mathematical formulation of the algorithm. The loss in GRPO is proportional to the KL divergence (the cap relative to original policy) . As training progresses, the model learns to generate text that better matches the reward function, causing it to diverge more from its initial policy. This increasing divergence is reflected in the rising loss value, which actually indicates that the model is successfully adapting to optimize for the reward function.

![Loss](https://huggingface.co/reasoning-course/images/resolve/main/grpo/14.png)

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#save-and-publish-the-model)Save and publish the model

Let’s share the model with the community!

Copied

merged_model = trainer.model.merge_and_unload()
merged_model.push_to_hub(
    "SmolGRPO-135M", private=False, tags=["GRPO", "Reasoning-Course"]
)

## [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#generate-text)Generate text

🎉 You’ve successfully fine-tuned a model with GRPO! Now, let’s generate some text with the model.

First, we’ll define a really long document!

Copied

prompt = """
# A long document about the Cat

The cat (Felis catus), also referred to as the domestic cat or house cat, is a small 
domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.
Advances in archaeology and genetics have shown that the domestication of the cat occurred
in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges
freely as a feral cat avoiding human contact. It is valued by humans for companionship and
its ability to kill vermin. Its retractable claws are adapted to killing small prey species
such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,
and its night vision and sense of smell are well developed. It is a social species,
but a solitary hunter and a crepuscular predator. Cat communication includes
vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as
well as body language. It can hear sounds too faint or too high in frequency for human ears,
such as those made by small mammals. It secretes and perceives pheromones.
"""

messages = [
    {"role": "user", "content": prompt},
]

Now, we can generate text with the model.

Copied

# Generate text
from transformers import pipeline

generator = pipeline("text-generation", model="SmolGRPO-135M")

## Or use the model and tokenizer we defined earlier
# generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

generate_kwargs = {
    "max_new_tokens": 256,
    "do_sample": True,
    "temperature": 0.5,
    "min_p": 0.1,
}

generated_text = generator(messages, generate_kwargs=generate_kwargs)

print(generated_text)

# [](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#conclusion)Conclusion

In this chapter, we’ve seen how to fine-tune a model with GRPO. We’ve also seen how to interpret the training results and generate text with the model.

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/5.mdx)

[←Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise with Unsloth→](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)

[Practical Exercise: Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#practical-exercise-fine-tune-a-model-with-grpo)[Install dependencies](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#install-dependencies)[Import and log in to Weights & Biases](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#import-and-log-in-to-weights--biases)[Load the dataset](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-the-dataset)[Load model](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-model)[Load LoRA](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#load-lora)[Define the reward function](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#define-the-reward-function)[Define the training arguments](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#define-the-training-arguments)[Push the model to the Hub during training](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#push-the-model-to-the-hub-during-training)[Interpret training results](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#interpret-training-results)[Save and publish the model](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#save-and-publish-the-model)[Generate text](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt#generate-text)


-----------


[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

[![Ask a Question](https://img.shields.io/badge/Ask%20a%20question-ffcb4c.svg?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgLTEgMTA0IDEwNiI+PGRlZnM+PHN0eWxlPi5jbHMtMXtmaWxsOiMyMzFmMjA7fS5jbHMtMntmaWxsOiNmZmY5YWU7fS5jbHMtM3tmaWxsOiMwMGFlZWY7fS5jbHMtNHtmaWxsOiMwMGE5NGY7fS5jbHMtNXtmaWxsOiNmMTVkMjI7fS5jbHMtNntmaWxsOiNlMzFiMjM7fTwvc3R5bGU+PC9kZWZzPjx0aXRsZT5EaXNjb3Vyc2VfbG9nbzwvdGl0bGU+PGcgaWQ9IkxheWVyXzIiPjxnIGlkPSJMYXllcl8zIj48cGF0aCBjbGFzcz0iY2xzLTEiIGQ9Ik01MS44NywwQzIzLjcxLDAsMCwyMi44MywwLDUxYzAsLjkxLDAsNTIuODEsMCw1Mi44MWw1MS44Ni0uMDVjMjguMTYsMCw1MS0yMy43MSw1MS01MS44N1M4MCwwLDUxLjg3LDBaIi8+PHBhdGggY2xhc3M9ImNscy0yIiBkPSJNNTIuMzcsMTkuNzRBMzEuNjIsMzEuNjIsMCwwLDAsMjQuNTgsNjYuNDFsLTUuNzIsMTguNEwzOS40LDgwLjE3YTMxLjYxLDMxLjYxLDAsMSwwLDEzLTYwLjQzWiIvPjxwYXRoIGNsYXNzPSJjbHMtMyIgZD0iTTc3LjQ1LDMyLjEyYTMxLjYsMzEuNiwwLDAsMS0zOC4wNSw0OEwxOC44Niw4NC44MmwyMC45MS0yLjQ3QTMxLjYsMzEuNiwwLDAsMCw3Ny40NSwzMi4xMloiLz48cGF0aCBjbGFzcz0iY2xzLTQiIGQ9Ik03MS42MywyNi4yOUEzMS42LDMxLjYsMCwwLDEsMzguOCw3OEwxOC44Niw4NC44MiwzOS40LDgwLjE3QTMxLjYsMzEuNiwwLDAsMCw3MS42MywyNi4yOVoiLz48cGF0aCBjbGFzcz0iY2xzLTUiIGQ9Ik0yNi40Nyw2Ny4xMWEzMS42MSwzMS42MSwwLDAsMSw1MS0zNUEzMS42MSwzMS42MSwwLDAsMCwyNC41OCw2Ni40MWwtNS43MiwxOC40WiIvPjxwYXRoIGNsYXNzPSJjbHMtNiIgZD0iTTI0LjU4LDY2LjQxQTMxLjYxLDMxLjYxLDAsMCwxLDcxLjYzLDI2LjI5YTMxLjYxLDMxLjYxLDAsMCwwLTQ5LDM5LjYzbC0zLjc2LDE4LjlaIi8+PC9nPjwvZz48L3N2Zz4=)](https://discuss.huggingface.co/t/chapter-2-questions)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_\(1B\)-GRPO.ipynb)

# [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#practical-exercise-grpo-with-unsloth)Practical Exercise: GRPO with Unsloth

In this exercise, you’ll fine-tune a model with GRPO (Group Relative Policy Optimization) using Unsloth, to improve a model’s reasoning capabilities. We covered GRPO in [Chapter 3](https://huggingface.co/course/chapter3/3).

Unsloth is a library that accelerates LLM fine-tuning, making it possible to train models faster and with less computational resources. Unsloth is plugs into TRL, so we’ll build on what we learned in the previous sections, and adapt it for Unsloth specifics.

This exercise can be run on a free Google Colab T4 GPU. For the best experience, follow along with the notebook linked above and try it out yourself.

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#install-dependencies)Install dependencies

First, let’s install the necessary libraries. We’ll need Unsloth for the accelerated fine-tuning and vLLM for fast inference.

Copied

pip install unsloth vllm
pip install --upgrade pillow

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#setting-up-unsloth)Setting up Unsloth

Unsloth provides a class (`FastLanguageModel`) that integrates transformers with Unsloth optimizations. Let’s import it:

Copied

from unsloth import FastLanguageModel

Now, let’s load Google’s Gemma 3 1B Instruct model and configure it for fine-tuning:

Copied

from unsloth import FastLanguageModel
import torch

max_seq_length = 1024  # Can increase for longer reasoning traces
lora_rank = 32  # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="google/gemma-3-1b-it",
    max_seq_length=max_seq_length,
    load_in_4bit=True,  # False for LoRA 16bit
    fast_inference=True,  # Enable vLLM fast inference
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.6,  # Reduce if out of memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],  # Remove QKVO if out of memory
    lora_alpha=lora_rank,
    use_gradient_checkpointing="unsloth",  # Enable long context finetuning
    random_state=3407,
)

This code loads the model in 4-bit quantization to save memory and applies LoRA (Low-Rank Adaptation) for efficient fine-tuning. The `target_modules` parameter specifies which layers of the model to fine-tune, and `use_gradient_checkpointing` enables training with longer contexts.

We won’t cover the details of LoRA in this chapter, but you can learn more in [Chapter 11](https://huggingface.co/course/chapter11/3).

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#data-preparation)Data Preparation

For this exercise, we’ll use the GSM8K dataset, which contains grade school math problems. We’ll format the data to encourage the model to show its reasoning before providing an answer.

First, we will define the format of the prompts and answers:

Copied

# Define the system prompt that instructs the model to use a specific format
SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
"""

XML_COT_FORMAT = """\
<reasoning>
{reasoning}
</reasoning>
<answer>
{answer}
</answer>
"""

Now, let’s prepare the dataset:

Copied

import re
from datasets import load_dataset, Dataset

# Helper functions to extract answers from different formats
def extract_xml_answer(text: str) -> str:
    answer = text.split("<answer>")[-1]
    answer = answer.split("</answer>")[0]
    return answer.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

# Function to prepare the GSM8K dataset
def get_gsm8k_questions(split="train") -> Dataset:
    data = load_dataset("openai/gsm8k", "main")[split]
    data = data.map(
        lambda x: {
            "prompt": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": x["question"]},
            ],
            "answer": extract_hash_answer(x["answer"]),
        }
    )
    return data


dataset = get_gsm8k_questions()

The dataset is prepared by extracting the answer from the dataset and formatting it as a string.

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#defining-reward-functions)Defining Reward Functions

As we discussed in [an earlier page](https://huggingface.co/course/chapter13/4), GRPO can use reward functions to guide the model’s learning based on verifiable criteria like length and formatting.

In this exercise, we’ll define several reward functions that encourage different aspects of good reasoning. For example, we’ll reward the model for providing an integer answer, and for following the strict format.

Copied

# Reward function that checks if the answer is correct
def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]["content"] for completion in completions]
    q = prompts[0][-1]["content"]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    print(
        "-" * 20,
        f"Question:\n{q}",
        f"\nAnswer:\n{answer[0]}",
        f"\nResponse:\n{responses[0]}",
        f"\nExtracted:\n{extracted_responses[0]}",
    )
    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]

# Reward function that checks if the answer is an integer
def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]["content"] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]

# Reward function that checks if the completion follows the strict format
def strict_format_reward_func(completions, **kwargs) -> list[float]:
    pattern = r"^<reasoning>\n.*?\n</reasoning>\n<answer>\n.*?\n</answer>\n$"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

# Reward function that checks if the completion follows a more relaxed format
def soft_format_reward_func(completions, **kwargs) -> list[float]:
    pattern = r"<reasoning>.*?</reasoning>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

# Reward function that counts XML tags and penalizes extra content
def count_xml(text) -> float:
    count = 0.0
    if text.count("<reasoning>\n") == 1:
        count += 0.125
    if text.count("\n</reasoning>\n") == 1:
        count += 0.125
    if text.count("\n<answer>\n") == 1:
        count += 0.125
        count -= len(text.split("\n</answer>\n")[-1]) * 0.001
    if text.count("\n</answer>") == 1:
        count += 0.125
        count -= (len(text.split("\n</answer>")[-1]) - 1) * 0.001
    return count

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_xml(c) for c in contents]

These reward functions serve different purposes:

|Reward Function|Purpose|
|---|---|
|`correctness_reward_func`|Rewards the model when its answer matches the correct answer|
|`int_reward_func`|Rewards the model for providing a numeric answer|
|`strict_format_reward_func` and `soft_format_reward_func`|Reward the model for following the specified format|
|`xmlcount_reward_func`|Rewards proper XML tag usage and penalizes extra content after the closing tags|

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#training-with-grpo)Training with GRPO

Now we’ll set up the GRPO trainer with our model, tokenizer, and reward functions. This part follows the same approach as the [previous exercise](https://huggingface.co/course/chapter12/5).

Copied

from trl import GRPOConfig, GRPOTrainer

max_prompt_length = 256

training_args = GRPOConfig(
    learning_rate=5e-6,
    adam_beta1=0.9,
    adam_beta2=0.99,
    weight_decay=0.1,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    optim="paged_adamw_8bit",
    logging_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,  # Increase to 4 for smoother training
    num_generations=6,  # Decrease if out of memory
    max_prompt_length=max_prompt_length,
    max_completion_length=max_seq_length - max_prompt_length,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps=250,
    save_steps=250,
    max_grad_norm=0.1,
    report_to="none",  # Can use Weights & Biases
    output_dir="outputs",
)

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[
        xmlcount_reward_func,
        soft_format_reward_func,
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func,
    ],
    args=training_args,
    train_dataset=dataset,
)

The `GRPOConfig` sets various hyperparameters for training:

- `use_vllm`: Enables fast inference with vLLM
- `learning_rate`: Controls how quickly the model learns
- `num_generations`: Number of completions to generate for each prompt
- `max_steps`: Total number of training steps to perform

Now let’s start the training:

Copied

trainer.train()

Training may take some time. You might not see rewards increase immediately - it can take 150-200 steps before you start seeing improvements. Be patient!

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#testing-the-model)Testing the Model

After training, let’s test our model to see how it performs. First, we’ll save the LoRA weights:

Copied

model.save_lora("grpo_saved_lora")

Now, let’s test the model with a new question:

Copied

from vllm import SamplingParams

text = tokenizer.apply_chat_template(
    [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": "Calculate pi."},
    ],
    tokenize=False,
    add_generation_prompt=True,
)

sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=1024,
)
output = (
    model.fast_generate(
        text,
        sampling_params=sampling_params,
        lora_request=model.load_lora("grpo_saved_lora"),
    )[0]
    .outputs[0]
    .text
)

print(output)

You should see that the model now follows the specified format, showing its reasoning before providing an answer.

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#saving-the-model)Saving the Model

Unsloth provides several options for saving your fine-tuned model, but we’ll focus on the most common.

Copied

# Save to 16-bit precision
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#pushing-to-hugging-face-hub)Pushing to Hugging Face Hub

We’ll push the model to the Hugging Face Hub using the `push_to_hub_merged` method. This method allows us to push the model in multiple quantization formats.

Copied

# Push to Hugging Face Hub (requires a token)
model.push_to_hub_merged(
    "your-username/model-name", tokenizer, save_method="merged_16bit", token="your-token"
)

Unsloth also supports saving to GGUF format for use with llama.cpp:

Copied

model.push_to_hub_gguf(
    "your-username/model-name",
    tokenizer,
    quantization_method=["q4_k_m", "q8_0", "q5_k_m"],
    token="your-token",
)

The GGUF files can be used with llama.cpp or UI-based systems like Jan or Open WebUI.

## [](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#conclusion)Conclusion

In this exercise, you’ve learned how to:

1. Set up Unsloth for accelerated fine-tuning
2. Prepare data for GRPO training
3. Define custom reward functions to guide the model’s learning
4. Train a model using GRPO
5. Test the fine-tuned model
6. Save the model in various formats

GRPO is a powerful technique for aligning language models with specific behaviors, and Unsloth makes it accessible even on limited hardware. By combining multiple reward functions, you can guide the model to follow a specific format while also improving its reasoning capabilities.

For more information and resources, check out:

- [Unsloth Documentation](https://docs.unsloth.ai/)
- [Unsloth Discord](https://discord.gg/unsloth)
- [Unsloth GitHub](https://github.com/unslothai/unsloth)

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/6.mdx)

[←Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Coming soon...→](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

[Practical Exercise: GRPO with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#practical-exercise-grpo-with-unsloth)[Install dependencies](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#install-dependencies)[Setting up Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#setting-up-unsloth)[Data Preparation](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#data-preparation)[Defining Reward Functions](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#defining-reward-functions)[Training with GRPO](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#training-with-grpo)[Testing the Model](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#testing-the-model)[Saving the Model](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#saving-the-model)[Pushing to Hugging Face Hub](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#pushing-to-hugging-face-hub)[Conclusion](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt#conclusion)


------------

[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)

- [Models](https://huggingface.co/models)
- [Datasets](https://huggingface.co/datasets)
- [Spaces](https://huggingface.co/spaces)
- Community
    
- [Docs](https://huggingface.co/docs)
- [Enterprise](https://huggingface.co/enterprise)
- [Pricing](https://huggingface.co/pricing)

- ---
    
- ![](https://huggingface.co/avatars/5718fc9db9d5ef597ef85560419fd2ea.svg)
    

# LLM Course

🏡 View all resourcesAgents CourseAudio CourseCommunity Computer Vision CourseDeep RL CourseDiffusion CourseLLM CourseMCP CourseML for 3D CourseML for Games CourseOpen-Source AI Cookbook

Search documentation

⌘K

ARBNDEENESFAFRGJHEHIIDITJAKONEPLPTRURUMTHTRVIZH-CNZH-TW

 [3,022](https://github.com/huggingface/course)

0. Setup

1. Transformer models

2. Using 🤗 Transformers

3. Fine-tuning a pretrained model

4. Sharing models and tokenizers

5. The 🤗 Datasets library

6. The 🤗 Tokenizers library

7. Classical NLP tasks

8. How to ask for help

9. Building and sharing demos

10. Curate high-quality datasets

11. Fine-tune Large Language Models

12. Build Reasoning Models new

[Introduction](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)[Reinforcement Learning on LLMs](https://huggingface.co/learn/llm-course/chapter12/2?fw=pt)[The Aha Moment in the DeepSeek R1 Paper](https://huggingface.co/learn/llm-course/chapter12/3?fw=pt)[Advanced Understanding of GRPO in DeepSeekMath](https://huggingface.co/learn/llm-course/chapter12/3a?fw=pt)[Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/chapter12/4?fw=pt)[Practical Exercise to Fine-tune a model with GRPO](https://huggingface.co/learn/llm-course/chapter12/5?fw=pt)[Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Coming soon...](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt)

Course Events

# [](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#coming-soon)Coming soon…

This chapter is being run as a live cohort now! If you’ve finished the material so far, here’s what to expect:

## [](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#course-schedule)Course Schedule

|Date|Unit|
|---|---|
|~March 7th, 2025~|~No-Code Exam and Certification~|
|~March 14th, 2025~|~Next Practical Exercise~|
|March 21st, 2025|Interactive code review|
|April 2025|More written material on building reasoning models|
|April 2025|Live sessions on building Open R1|
|April 2025|Code Exam and Certification|

## [](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#staying-up-to-date)Staying Up to Date

If you want to follow the course, follow the [The Reasoning Course](https://huggingface.co/reasoning-course) and join the [Discord community](https://discord.gg/F3vZujJH)!

[<>Update on GitHub](https://github.com/huggingface/course/blob/main/chapters/en/chapter12/7.mdx)

[←Practical Exercise with Unsloth](https://huggingface.co/learn/llm-course/chapter12/6?fw=pt)[Live sessions and workshops→](https://huggingface.co/learn/llm-course/events/1?fw=pt)

[Coming soon…](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#coming-soon)[Course Schedule](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#course-schedule)[Staying Up to Date](https://huggingface.co/learn/llm-course/chapter12/7?fw=pt#staying-up-to-date)


---------


