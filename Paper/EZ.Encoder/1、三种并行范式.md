
三种并行的范式： 

* data parallelism 
* tensor parallelism 
* pipeline parallelism 


![[Pasted image 20250330220840.png]]

单 GPU 情况下，如果模型不大，模型首先会被加载到 GPU 当中，数据以 Batch 的形式进到 GPU 进行计算，最大的制约因素是 GPU 显存的大小，那在计算时候有哪些因素会占据这个 GPU 的显存呢？ 主要有四个因素：

* 模型权重，即模型的参数
* 前向传播中的激活值
* 反向传播中计算得到的梯度（推理时不需要）
* 优化器状态，在训练过程中用于加速收敛（推理时不需要）

### 0、通信问题

![[Pasted image 20250330222259.png|500]]

GPU 之间的通讯，通常涉及大量的数据传输，特别是在模型的权重同步和梯度传递的过程中，会显著的影响并行计算的速度和效率，优化 GPU 之间的通讯效率，就成为 LLM 高效训练的一个非常重要的挑战。

现在 Nvidia 提供了一些比较好的解决方案，同一个节点内的 GPU 通讯可以使用 NVLink，其带宽非常高，能显著的减少通讯延迟；在不同节点之间，可以使用 InfiniBand，速度可以达到数百 Gbps，极大的提高了跨节点的通信效率。









### 1、数据并行

![[Pasted image 20250330222051.png|500]]

当有多个 GPU 时候， 最简单的并行方式就是将数据拆开放到每个 GPU 上面，每个 GPU 其实是一个模型副本。

**优缺点：**

* 优点：
	1. 简单容易实现，对数据的拆分，不依赖具体的模型实现；
	2. 通讯量较少，每个 GPU 只需要计算自己那部分数据，最终再进行结果的整合
* 局限性：无法处理大模型，模型本身就无法放入单个 GPU 显存中 


### 2、模型并行

![[Pasted image 20250330225351.png|300]]

针对 DP 的局限性，最直接的想法就是对模型进行拆分，将计算的任务分发到不同的 GPU 上，模型并行主要分为下面两种方式：

* 流水行并行：将模型不同层给拆开分给不同的 GPU 进行处理，inter-layer，层间并行
* 张量并行：在每一层的内部进行拆分，将同一层 tensor 的运算分配到不同的 GPU 上，即每个 GPU 只负责某个 tensor 的部分计算，实现更加细腻度的并行化。适用于模型单层计算量特别大的情况。因为这种方式是在层的内部进行切割，所以也称为 intra-layer 并行


可以看到由于在 pipeline parallelism 和 tensor parallelism 中需要将模型拆分到不同的 GPU 上面
              
                  06:38
                  所以 GPU 之间的通行量相对来说会更大一些 那具体来说 因为 tanser parallelism 里需要在每一层的内部进行计算结构的共享 这就导致了大量的通信需求 通信的代价是最高的 而  pipeline parallelism 由于每个 GPU 它负责不同的 模型的层 通信相对来说是比较低一点 但仍然需要在流水线阶段进行信息的传递 所以相比之下 data parallelism 仅需要在每个批次结束的时候进行结果的整合, 通讯需求量是最低的 这里就总结到这个地方了 回到  data parallelism 的缺点, 近几年 data parallelism 也引入了两个新的框架 分别是这 PyTorch 的 FSDP 和 Microsoft DeepSpeed Zero 系列 这两个方法通过对权重和梯度进行shard 和 offload 它其实优化了显存的使用 从而显著降低了显存的消耗 其中加 FSDP 这个方法可以将模型的参数 shard 到不同的 GPU 上面
              
                  07:44
                  并在需要时动态的加载 显著的减少了显存的使用啊 deepspeed 的 Zero 的方法都通过将优化器的状态和梯度进行拆分 使得更大的模型能够在有限的 GPU 资源上进行训练 那我会在后续的视频当中给大家进行讲解 FSDP 和 Zero 所以如果你感兴趣 请订阅我的频道 这里就给大家 具体的介绍一下 pipeline  parallelism 思想 那简单的说就是将模型按照 layer 进行拆分 放到不同的 GPU 上面进行计算 对于同一个数据来说 前一个 GPU 计算完当前的 layer 的结果之后 会将这个结果传递到下一个 GPU 上面 进行下一层结果的计算 可以看到这样的方式 存在一个很大的问题 那就是大部分的 GPU 都在等待前一个 GPU 的计算结果 所以很多的GPU都处于一个空闲的状态 导致的这种资源的利用率不高 这种串型的依赖方式会带来 比较显著的计算瓶颈
              
                  08:48
                  特别是在层数较多的情况下 整体的计算效率会严重的受到影响 下面这个图 就来自于 Google 的非常著名的一篇paper 叫GPipe  就进一步展示了这个问题 图中的 F 表示这个前向传播 B 表示反向传播 Backward pass  每一行代表一个 GPU 当然Google使用的是 TPU 那我这里为了简单理解 我都把它统一成 GPU 那可以看到 不论是 forward pass 还是 backward pass 大部分的 GPU 时间都其实在等待其他 GPU 的计算结果 导致显著的资源浪费和效率的降低 为了更好的利用资源 那我们需要更加高效的并行策略 GPipe这里面文章就引入了一个叫 Micro-Batch 的概念， 简单的说 将原来大的批次数据进行一步 更细分的划分 paper 里面称为 Micro-Batch 把 Micro-Batch 分到不同的 GPU 上面 这样一来 多个 GPU 就可以像流水线上的工人一样 同时处理不同的小批次的数据
              
                  09:52
                  实现真正的并型处理 我这里就画了一个图 比如说有三个数据 那你可以看到这三个数据可以 依次的在不同的 layer 上面 针对不同的GPU进行计算 就像一个流水线一样 所以在同一个时刻 大部分的 GPU 其实都有数据在进行计算的 除了数据进入和离开流水线的过程中可能会有一些GPU  没有计算 下面这张图就来自于 GPipe 这个图就总结了  pipe parallelism 整体的过程， 虽然这种方法提高了 GPU 的整体的利用率 但仍然有一些时间段内 这个 GPU 是处于一种等待的状态 这种现象就被称为 Bubble Bubble 的引入是由于数据进入和离开流水线的过程中 有一些 GPU 是产生了空闲 我们其实希望 GPU 处于满载的时间段就尽可能的长 就是说我们要尽量减少 Bubble 的存在， 使GPU的资源得到最大化的利用 为了减少 Bubble 的时间所占的比例 有一个可行的方案就是增加 Micro-Batch 的数量
              
                  10:58
                  那为什么增加 Micro-Batch 的数量可以减少 Bubble? 因为通过增加 Micro-Batch 的数量 流水线他可以更加连续的工作 简单的说就让流水线一直处于 图上圈出来的的状态 所以主要是减少 GPU 在等待的时间 相当于 Bubble 的时间给压缩了 所以就能提升整体 GPU 计算效率 那另外一种减少 Bubble 的思路来自于 Nvidia 的一篇论文 那这个作者就提出了一种 schedule 的方法 叫做 1F1B 那简单的说就是 one fordpass followed by one backward pass  大家可以看到上图 GPipe 里面 pipeline parallelism naive schedule 的方式是 等所有 GPU 上的所有层的 forward pass 全部完成之后才开始所有层的backword pass 这就导致了一些GPU在等待过程中是处于一种空闲的状态的 那1F1 schedule 思想 就是说 我每完成一个某一个 layer 的前向传播 就立即对该部分进行反向传播，
              
                  12:02
                  从而减少 GPU 在流水线中的等待时间 大家可以看到下图如果使用1F1B之后 Bubble 时间就变得更少一些 所以这种方法可以有效的减少个空闲时间 使得GPU能够更加高效的工作 提高整体的效率 我前面已经讲完了 pipeline parallelism 的一些主要的思想 那这里再来讲一下  tensor parallelism 的主要思想 简单的回顾一下 pipeline parallelism 是将模型的不同的 layer 进行切分 分配到不同的 GPU 上进行计算 tensor parallelism 就是将一个层的weights tensor 进行切分， 分布到不同的GPU上进行计算 所以他的这种切分方式是不太一样的 之所以可以这样进行切分 我们先回顾一下linear algebra的基础知识 在神经网络的计算过程当中 大部分的操作都是矩阵乘法 像我图中表示一样 很多时候 网络里面其实就是一个inputs 比如说 x 是一个矩阵 和一个大的参数矩阵
              
                  13:07
                  weights 进行相乘， 最后得到这样的输出 这样的乘法在 linear algebra 里面其实 可以将它拆开的 比如说我们可以将为此按照列进行拆分 比如说这个里面就是右图所表示的 A1, A2, A3 那我们可以把不同的列放到不同的GPU上面， 比如说 GPU0 上面我们就计算 x 乘以 A1 GPU1 上面我们就计算 x 乘以 A2 GPU2 上面就计算 x 乘以 A3  那么我们在每个 GPU 上就分别得到 Y1, Y2, Y3 这三个结果 最后我们就把它拼起来 左边这种方法和右边这种方法其实是等价的 所以这个就是 tensor parallelism 的理论基础 所以我们可以将模型的 weights tenser 分割到多个GPU上面 所以 tensor parallelism 是可以 减小每个 GPU 的显存需求 将计算任务分到多个 GPU 上面 这种方法特别适用于那些模型 每一层的计算量都非常的大 所以单层都没法放到一个GPU上面去的情况
              
                  14:12
                  用这种方法就能够很好的解决 这个图是来自于Megatron-LM paper  这篇 paper 里面有一个关于  tensor parallelism 的例子 这个例子是 transformer 的 MLP 层 简单的说就是在 MLP 层这个地方进行计算的时候 我们可以把它的两个 weights tensor 图中的 B 进行拆开 那我们可以对 A 按照列也把它拆成 A1, A2  在计算的时候 我们就 把这两个weights tensor 放到两个GPU上面去分别进行计算 再跟拆开B进行计算, 最后完了之后把它给合起来 我们这样实现了  tensor parallelism 如果transformer特别大的时候 这个MLP层 有可能会特别的大 那么单一的这一层都无法放进单个 GPU 的时候 那么就需要用 tensor parallelism 好了 到这里我已经把三种常用的并行范式介绍完了 这里我想最后强调一下这三种范式
              
                  15:16
                  其实是 彼此独立 它并不是互斥的 在实际当中你可以将这三种  parallelism 的方法结合起来， 那比如这个图就来自于 DeepSpeed 的这篇博客 DeepSpeed 其实就将  data parallelism  tensor  parallelism 和 pipeline  parallelism 三种方法结合起来了 这样可以充分的结合这三种方法优势来更高效的进行模型的训练 论文会经常用一个词叫 3D parallelism 那 3D parallelism 其实就是指的DP 加 PP 加 TP  比如说最近的Meta 405B 模型其实用了这种3D parallelism的方式训练出来 好 这里就进入到了这个视频的彩蛋时间 那我这个彩蛋想给大家展示一下 logo 是如何设计的 简单的说就是我用ChatGPT, 给了他一些 Prompt 想让他帮我设计一个Deep learning的Encoder 我最开始的时候其实想让他帮我设计一个比较卡通 比较拟人化的Encoder
              
                  16:21
                  经过多次尝试之后 最终失败了 所以放弃了 用拟人化Encoder的想法， 这里我保留了几个 早期的时候ChatGPT给我产生的LOGO 我就放在这里供大家娱乐一下 第一个ChatGPT产生的logo 是这样的 长得像个面包一样， 其实并不像一个Encoder， 而且有四个眼睛， 我觉得是 挺吓人的 第二个 ChatGPT给我产生的是这个样子的 长得也是 不伦不类 长得像一个外星人一样 也是挺吓人的 我最后都没有采用 这就是本期的彩蛋了 供大家娱乐一下
              
            