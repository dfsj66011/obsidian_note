
> [# RL — Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)

-)

Looking to hide highlights? You can now hide them from the “•••” menu.

Okay, got it

![](https://miro.medium.com/v2/resize:fit:1000/0*ep1_h3aZgY5MKFsm)

Photo by [Alex Read](https://unsplash.com/@alexread?utm_source=medium&utm_medium=referral)

Policy Gradient Methods (**PG**) are frequently used algorithms in reinforcement learning (RL). The principle is very simple.

> We observe and act.

A human takes actions based on observations. As a quote from Stephen Curry:

You have to rely on the fact that you put the work in to create the muscle memory and then trust that it will kick in. The reason you practice and work on it so much is so that during the game your instincts take over to a point where it feels weird if you don’t do it the right way.

![](https://miro.medium.com/v2/resize:fit:700/1*VDCraEnXZtx-Q1BBEvSVRQ.jpeg)

[Source](http://www.espn.com/nba/story/_/id/10703246/golden-state-warriors-stephen-curry-reinventing-shooting-espn-magazine)

Constant practice is the key to build muscle memory for athletes. For PG, we train a policy to act based on observations. The training in PG makes actions with high rewards more likely, or vice versa.

> We keep what is working and throw away what is not.

In policy gradients, Curry is our agent.

![](https://miro.medium.com/v2/resize:fit:700/1*94EI9DpoXnWa6oLHvh14pw.jpeg)

1. He observes the state of the environment (_s_).
2. He takes action (_u_) based on his instinct (a policy π) on the state _s._
3. He moves and the opponents react. A new state is formed.
4. He takes further actions based on the observed state.
5. After a trajectory _τ_ of motions, he adjusts his instinct based on the total rewards **_R(τ)_** received.

Curry visualizes the situation and instantly knows what to do. Years of training perfects the instinct to maximize the rewards. In RL, the instinct may be mathematically described as:

![](https://miro.medium.com/v2/resize:fit:700/1*nFBKqcUyE5Qzji3AQWl2Xg.png)

the probability of taking the action _u_ given a state _s_. π is the **policy** in RL. For example, what is the chance of turning or stopping when you see a car in front:

![](https://miro.medium.com/v2/resize:fit:700/1*HDGxQ2SgZng8DZ6Mzp0meQ.jpeg)

# Objective

How can we formulate our objective mathematically? The expected rewards equal the sum of the probability of a trajectory × corresponding rewards:

![](https://miro.medium.com/v2/resize:fit:700/1*QCCr4lzSXBSi-YGZKGZHQg.jpeg)

And our objective is to find a policy _θ_ that create a trajectory **_τ_**

![](https://miro.medium.com/v2/resize:fit:700/1*M9SeXz21wvhsGS9o5rBQ8w.png)

that maximizes the expected rewards.

![](https://miro.medium.com/v2/resize:fit:700/1*sFyXY3IVUFBGNCdmIA-11A.png)

# Input features & rewards

![](https://miro.medium.com/v2/resize:fit:700/1*2r_qbs-dhkKuqv8UW9Ih_A.png)

**_s_** can be handcrafted features for the state (like the joint angles/velocity of a robotic arm) but in some problem domains, RL is mature enough to handle raw images directly. **_π_** can be a deterministic policy which output the exact action to be taken (move the joystick left or right). **_π_** can be **a stochastic policy** also which outputs the possibility of an action that it may take.

We record the reward **_r_** given at each time step. In a basketball game, all are 0 except the terminate state which equals 0, 1, 2 or 3.

![](https://miro.medium.com/v2/resize:fit:700/1*CoigUy974OVb4AGk2PwMrQ.png)

Let’s introduce one more term **_H_** called the horizon. We can run the course of simulation indefinitely (_h→∞_) until it reaches the terminate state, or we set a limit to **_H_** steps.

# Optimization

First, let’s identify a common and important trick in Deep Learning and RL. The partial derivative of a function _f(x)_ (R.H.S.) is equal to _f(x)_ times the partial derivative of the _log(f(x))_.

![](https://miro.medium.com/v2/resize:fit:700/1*4y8n5BohMTpqR_xhLjyizQ.jpeg)

Replace f(x) with **_π._**

![](https://miro.medium.com/v2/resize:fit:700/1*IP0dEvYryFrLqww5Oxay1g.jpeg)

Also, for a continuous space, expectation can be expressed as:

![](https://miro.medium.com/v2/resize:fit:700/1*0aDOHM18NWdeRx3JmVUxlw.jpeg)

Now, let’s formalize our optimization problem mathematically. We want to model a policy that creates trajectories that maximize the total rewards.

![](https://miro.medium.com/v2/resize:fit:700/1*3Rw6OuCLrjpk28rr2GH2-A.png)

However, to use gradient descent to optimize our problem, do we need to take the derivative of the reward function **_r_** which may not be differentiable or formalized?

Let’s rewrite our objective function **_J_** as:

![](https://miro.medium.com/v2/resize:fit:700/1*FyHy0PM_Tj3UKvlpbw_GWQ.png)

The gradient (**policy gradient**) becomes:

![](https://miro.medium.com/v2/resize:fit:700/1*Xsb11fW8GluAt0tiYi2uuQ.png)

Great news! The policy gradient can be represented as an expectation. It means we can use sampling to approximate it. Also, we sample the value of **_r_** but not differentiate it. It makes sense because the rewards do not directly depend on how we parameterize the model. But the trajectories _τ_ are. So what is the partial derivative of the log π(_τ_).

π(_τ_) is defined as:

![](https://miro.medium.com/v2/resize:fit:700/1*qF_agjFzPMwYfafsVJMo9A.png)

==Take the log:==

![](https://miro.medium.com/v2/resize:fit:700/1*dSO_ST85b_VdaghwBrX9TA.png)

The first and the last term does not depend on _θ_ and can be removed.

![](https://miro.medium.com/v2/resize:fit:700/1*cb00sUNRtKP2ieBV1pVnkQ.png)

So the policy gradient

![](https://miro.medium.com/v2/resize:fit:700/1*5xSqPc0ekeL9XLpPKq1BIw.png)

==becomes:==

![](https://miro.medium.com/v2/resize:fit:700/1*0A4M301ZwWzn1JyGAyHapQ.png)

And we use this policy gradient to update the policy _θ_.

# Intuition

![](https://miro.medium.com/v2/resize:fit:700/1*HjG9oKTtPtULEim4t2p5bQ.png)

How can we make sense of these equations? ==The underlined term is the maximum log likelihood==. In deep learning, it measures the likelihood of the observed data. In our context, it measures how likely the trajectory is under the current policy. By multiplying it with the rewards, we want to increase the likelihood of a policy if the trajectory results in a high positive reward. On the contrary, we want to decrease the likelihood of a policy if it results in a high negative reward. In short, keep what is working and throw out what is not.

If going up the hill below means higher rewards, we will change the model parameters (policy) to increase the likelihood of trajectories that move higher.

![](https://miro.medium.com/v2/resize:fit:700/1*zkOBQ9Izq28yXCANTmdKtA.png)

[Source](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf)

There is one thing significant about the policy gradient. The probability of a trajectory is defined as:

![](https://miro.medium.com/v2/resize:fit:700/1*m5--PCQa5gvU0Z7e8d-E2A.png)

States in a trajectory are strongly related. In Deep Learning, a long sequence of multiplication with factors that are strongly correlated can trigger vanishing or exploding gradient easily. However, the policy gradient only sums up the gradient which breaks the curse of multiplying a long sequence of numbers.

![](https://miro.medium.com/v2/resize:fit:700/1*DkFVqjX4CVnSNjjcfIOKXw.png)

The trick

![](https://miro.medium.com/v2/resize:fit:700/1*IP0dEvYryFrLqww5Oxay1g.jpeg)

==creates a maximum log likelihood and the log breaks the curse of multiplying a long chain of policy==.

# Policy Gradient with Monte Carlo rollouts

Here is the REINFORCE algorithm which uses **Monte Carlo** rollout to compute the rewards. i.e. play out the whole episode to compute the total rewards.

![](https://miro.medium.com/v2/resize:fit:700/1*2Zs3yqlYflf4uqssMNDTcg.png)

[Source](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf)

**Policy gradient with automatic differentiation**

==The policy gradient can be computed easily with many Deep Learning software packages. For example, this is the partial code for TensorFlow:==

![](https://miro.medium.com/v2/resize:fit:700/1*VszBEU86sub-vmO58sJeRg.png)

[Source](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)

Yes, as often, coding looks simpler than the explanations.

# Continuous control with Gaussian policies

How can we model a continuous control?

![](https://miro.medium.com/v2/resize:fit:700/1*4Dl4KD6Hk3dqrAPcv-1zTw.png)

Let’s assume the values for actions are Gaussian distributed

![](https://miro.medium.com/v2/resize:fit:700/1*nLiB_BSY7uCIGbJIZULeRg.png)

and the policy is defined using a Gaussian distribution with means computed from a deep network:

![](https://miro.medium.com/v2/resize:fit:700/1*7EMqIf41obEaRAywBEMWpQ.png)

With

![](https://miro.medium.com/v2/resize:fit:700/1*1z0DeYDd_uhkdNw3jE2aWw.png)

We can compute the partial derivative of the log π as:

![](https://miro.medium.com/v2/resize:fit:700/1*JmVoAoQ6opm_q669_-CV9A.png)

So we can backpropagate

![](https://miro.medium.com/v2/resize:fit:700/1*HO4qzrqEuCb4wZ1-NW9n4Q.png)

through the policy network π to update the policy _θ_. The algorithm will look exactly the same as before. Just start with a slightly different way in calculating the log of the policy.

![](https://miro.medium.com/v2/resize:fit:700/1*PRYEapEDt0A4Uh8iFN_fxw.png)

[Source](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf)

# Policy Gradients improvements

> ==Policy Gradients suffer from high variance and low convergence.==

Monte Carlo plays out the whole trajectory and records the exact rewards of a trajectory. However, the stochastic policy may take different actions in different episodes. One small turn can completely alter the result. So Monte Carlo has no bias but high variance. Variance hurts deep learning optimization. The variance provides conflicting descent direction for the model to learn. One sampled rewards may want to increase the log likelihood and another may want to decrease it. This hurts the convergence. To reduce the variance caused by actions, we want to reduce the variance for the sampled rewards.

![](https://miro.medium.com/v2/resize:fit:700/1*YFddfJV2mieJ2IQTw4srmQ.png)

> Increasing the batch size in PG reduces variance.

However, increasing the batch size significantly reduces sample efficiency. So we cannot increase it too far, we need additional mechanisms to reduce the variance.

**Baseline**

![](https://miro.medium.com/v2/resize:fit:700/1*0VAsJ3HDSi4mvDnBJFmyeQ.png)

==We can always subtract a term to the optimization problem as long as the term is not related to== ==_θ_====. So instead of using the total reward, we subtract it with== ==_V_====(====_s_====).==

![](https://miro.medium.com/v2/resize:fit:700/1*pzW16xJ4_U_Qsjlw1t8F0Q.png)

We define the advantage function **_A_** and rewrite the policy gradient in terms of _A_.

![](https://miro.medium.com/v2/resize:fit:700/1*EWuvBxscWd7cJlYC7oXLWA.png)

In deep learning, we want input features to be zero-centered. Intuitively, RL is interested in knowing whether an action is performed better than the average. If rewards are always positive (R>0), PG always try to increase a trajectory probability even if it receives much smaller rewards than others. Consider two different situations:

- Situation 1: Trajectory A receives+10 rewards and Trajectory B receives -10 rewards.
- Situation 2: Trajectory A receives +10 rewards and Trajectory B receives +1 rewards.

In the first situation, PG will increase the probability of Trajectory A while decreasing B. In the second situation, it will increase both. As a human, we will likely decrease the likelihood of trajectory B in both situations.

By introducing a baseline, like **_V_**, we can recalibrate the rewards relative to the average action.

**Vanilla Policy Gradient Algorithm**

Here is the generic algorithm for the Policy Gradient Algorithm using a baseline **_b_**.

![](https://miro.medium.com/v2/resize:fit:700/1*ZPYbuDkPodvY3sNHVL4Cxw.png)

![](https://miro.medium.com/v2/resize:fit:700/1*VJefxhGskx_wu_BiqaNe2g.png)

[Source](https://drive.google.com/file/d/0BxXI_RttTZAhY216RTMtanBpUnc/view)

**Causality**

Future actions should not change past decisions. Present actions only impact the future. Therefore, we can change our objective function to reflect this also.

![](https://miro.medium.com/v2/resize:fit:700/1*8mzz-Q1C0YFJRFPo7Rj0sw.jpeg)

**Reward discount**

Reward discount reduces variance which reduces the impact of distant actions. Here, a different formula is used to compute the total rewards.

![](https://miro.medium.com/v2/resize:fit:700/1*wZU7sKpHjqCGZYNw9RNSsQ.jpeg)

And the corresponding objective function becomes:

![](https://miro.medium.com/v2/resize:fit:700/1*cglnX3JI1XcBxik_IPjzqw.png)

# Part 2

This ends part 1 of the policy gradient methods. In the second part, we continue on the Temporal Difference, Hyperparameter tuning, and importance sampling. Temporal Difference will further reduce the variance and the importance sampling will lay down the theoretical foundation for more advanced policy gradient methods like TRPO and PPO.

[

## RL — Policy Gradients Explained (Advanced topic)

### In the first part of the Policy Gradients article, we cover the basic. In the second part, we continue on the Temporal…

medium.com



](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b?source=post_page-----9b13b688b146---------------------------------------)

# Credit and references

[UCL RL course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)

[UC Berkeley RL course](http://rail.eecs.berkeley.edu/deeprlcourse/)

[UC Berkeley RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)

[A3C paper](https://arxiv.org/pdf/1602.01783.pdf)

[GAE paper](https://arxiv.org/pdf/1506.02438.pdf)

[

Machine Learning

](https://medium.com/tag/machine-learning?source=post_page-----9b13b688b146---------------------------------------)

[

Reinforcement Learning

](https://medium.com/tag/reinforcement-learning?source=post_page-----9b13b688b146---------------------------------------)

[

Artificial Intelligence

](https://medium.com/tag/artificial-intelligence?source=post_page-----9b13b688b146---------------------------------------)

[

Data Science

](https://medium.com/tag/data-science?source=post_page-----9b13b688b146---------------------------------------)

[

Deep Learning

](https://medium.com/tag/deep-learning?source=post_page-----9b13b688b146---------------------------------------)

2.9K

28

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:96:96/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---post_author_info--9b13b688b146---------------------------------------)

[

## Written by Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---post_author_info--9b13b688b146---------------------------------------)

[41K followers](https://jonathan-hui.medium.com/followers?source=post_page---post_author_info--9b13b688b146---------------------------------------)

·[39 following](https://jonathan-hui.medium.com/following?source=post_page---post_author_info--9b13b688b146---------------------------------------)

Deep Learning

Follow

## Responses (28)

[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--9b13b688b146---------------------------------------)

![Dfsj](https://miro.medium.com/v2/resize:fill:32:32/0*PLjgQ4YC02_2Jtni)

Dfsj

What are your thoughts?﻿

Cancel

Respond

[

![JJ Zeng](https://miro.medium.com/v2/resize:fill:32:32/0*lJhUnLEvO1mMZuAd)





](https://medium.com/@zengjie617789?source=post_page---post_responses--9b13b688b146----0-----------------------------------)

[

JJ Zeng



](https://medium.com/@zengjie617789?source=post_page---post_responses--9b13b688b146----0-----------------------------------)

[

Sep 25, 2018

](https://medium.com/@zengjie617789/i-am-confused-you-said-the-policy-gradient-update-only-at-the-end-of-every-episode-but-your-86a52feb3508?source=post_page---post_responses--9b13b688b146----0-----------------------------------)

i am confused you said the policy gradient update only at the end of every episode,but your REINFORCE algorithm which uses Monte Carlo rollout update gradient at every step of the episode~~~

3

1 reply

Reply

[

![pren1 pren1](https://miro.medium.com/v2/resize:fill:32:32/0*9E2xTd4PRzxjW4An)





](https://medium.com/@pren1?source=post_page---post_responses--9b13b688b146----1-----------------------------------)

[

pren1 pren1



](https://medium.com/@pren1?source=post_page---post_responses--9b13b688b146----1-----------------------------------)

[

Sep 14, 2018

](https://medium.com/@pren1/thank-you-for-this-awesome-article-2a0de2108a50?source=post_page---post_responses--9b13b688b146----1-----------------------------------)

Thank you for this awesome article! Actually, I was just looking for some articles about policy gradients, then Medium recommended this brilliant blog to me. Thank you! :D

4

1 reply

Reply

[

![Allohvk](https://miro.medium.com/v2/resize:fill:32:32/1*_cbNu8dxSQBV1X-oIYhNJg.png)





](https://medium.com/@allohvk?source=post_page---post_responses--9b13b688b146----2-----------------------------------)

[

Allohvk



](https://medium.com/@allohvk?source=post_page---post_responses--9b13b688b146----2-----------------------------------)

[

Oct 18, 2022

](https://medium.com/@allohvk/you-have-mastered-the-art-of-explaining-complicated-math-in-simple-english-f39acb4474d7?source=post_page---post_responses--9b13b688b146----2-----------------------------------)

You have mastered the art of explaining complicated math in simple English....

1

1 reply

Reply

See all responses

## More from Jonathan Hui

![mAP (mean Average Precision) for Object Detection](https://miro.medium.com/v2/resize:fit:679/1*FrmKLxCtkokDC3Yr1wc70w.png)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----0---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----0---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

## mAP (mean Average Precision) for Object Detection

### AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision…



](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173?source=post_page---author_recirc--9b13b688b146----0---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

Mar 7, 2018

[

7.6K

54





](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173?source=post_page---author_recirc--9b13b688b146----0---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

![Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)](https://miro.medium.com/v2/resize:fit:679/0*Kik_29u0aNSWCsux)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----1---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----1---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

## Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)

### In machine learning (ML), one of the most important linear algebra concepts is the singular value decomposition (SVD). With all the raw…



](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491?source=post_page---author_recirc--9b13b688b146----1---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

Mar 7, 2019

[

4K

35





](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491?source=post_page---author_recirc--9b13b688b146----1---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

![Understanding Feature Pyramid Networks for object detection (FPN)](https://miro.medium.com/v2/resize:fit:679/0*RCxiFYR3z6NTsi2p.)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----2---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----2---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

## Understanding Feature Pyramid Networks for object detection (FPN)

### Detecting objects in different scales is challenging in particular for small objects. We can use a pyramid of the same image at different…



](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c?source=post_page---author_recirc--9b13b688b146----2---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

Mar 27, 2018

[

4.2K

30





](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c?source=post_page---author_recirc--9b13b688b146----2---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

![SSD object detection: Single Shot MultiBox Detector for real-time processing](https://miro.medium.com/v2/resize:fit:679/1*N-ZCvRQL9iwnlYTX5XM2Iw.jpeg)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----3---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146----3---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

## SSD object detection: Single Shot MultiBox Detector for real-time processing

### SSD is designed for object detection in real-time. Faster R-CNN uses a region proposal network to create boundary boxes and utilizes those…



](https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06?source=post_page---author_recirc--9b13b688b146----3---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

Mar 14, 2018

[

3.4K

37





](https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06?source=post_page---author_recirc--9b13b688b146----3---------------------829373fa_3984_4faf_90e9_79c4000a7c77--------------)

[

See all from Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--9b13b688b146---------------------------------------)

## Recommended from Medium

![This new IDE from Google is an absolute game changer](https://miro.medium.com/v2/resize:fit:679/1*f-1HQQng85tbA7kwgECqoQ.png)

[

![Coding Beauty](https://miro.medium.com/v2/resize:fill:20:20/1*ViyWUoh4zqx294no1eENxw.png)



](https://medium.com/coding-beauty?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

In

[

Coding Beauty

](https://medium.com/coding-beauty?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

by

[

Tari Ibaba

](https://medium.com/@tariibaba?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## This new IDE from Google is an absolute game changer

### This new IDE from Google is seriously revolutionary.



](https://medium.com/@tariibaba/new-google-project-idx-fae1fdd079c7?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Mar 12

[

5.4K

315





](https://medium.com/@tariibaba/new-google-project-idx-fae1fdd079c7?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

![Value-Based vs Policy-Based Reinforcement Learning](https://miro.medium.com/v2/resize:fit:679/1*PkeEQpSJ-kViy7KmbwzSTA.png)

[

![Papers in 100 Lines of Code](https://miro.medium.com/v2/resize:fill:20:20/1*HFmaIJiZ7BehRryMgLDvxQ.jpeg)



](https://papers-100-lines.medium.com/?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

Papers in 100 Lines of Code

](https://papers-100-lines.medium.com/?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## Value-Based vs Policy-Based Reinforcement Learning

### Two primary approaches in Reinforcement Learning (RL) are value-based methods and policy-based methods. In this article, we are going to…



](https://papers-100-lines.medium.com/value-based-vs-policy-based-reinforcement-learning-92da766696fd?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Nov 21, 2024

[

4

1





](https://papers-100-lines.medium.com/value-based-vs-policy-based-reinforcement-learning-92da766696fd?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

![OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python](https://miro.medium.com/v2/resize:fit:679/0*zf05Z-2A_5rXq-VP.png)

[

![Neural pAi](https://miro.medium.com/v2/resize:fill:20:20/1*WMzhCjQjIT1OxUdTZs9TPQ.png)



](https://neuralpai.medium.com/?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

Neural pAi

](https://neuralpai.medium.com/?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python



](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Mar 3

[

53





](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--9b13b688b146----0---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

![Easy Explained → Mixture of Experts](https://miro.medium.com/v2/resize:fit:679/0*qSkYpXGAeg_XGBnY)

[

![Marco Pi](https://miro.medium.com/v2/resize:fill:20:20/1*hfxmDm7LXECW3lki7j0BaQ@2x.jpeg)



](https://medium.com/@marcopia.remote?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

Marco Pi

](https://medium.com/@marcopia.remote?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## Easy Explained → Mixture of Experts

### Think about how you’d fix your car. You don’t ask the baker. You don’t ask the teacher. Heck, you’d surely don’t ask the dentist to look at…



](https://medium.com/@marcopia.remote/easy-explained-mixture-of-experts-b8347d8686ed?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Mar 17

[

435





](https://medium.com/@marcopia.remote/easy-explained-mixture-of-experts-b8347d8686ed?source=post_page---read_next_recirc--9b13b688b146----1---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

![Mastering Reinforcement Learning: Chapter By Chapter Guide to Sutton & Barto](https://miro.medium.com/v2/resize:fit:679/1*yuVzMhCJyDENbyhwAsrkwA.png)

[

![Ajay Kumar](https://miro.medium.com/v2/resize:fill:20:20/0*YftkGL-ygoKiogyl)



](https://medium.com/@trivajay259?source=post_page---read_next_recirc--9b13b688b146----2---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

Ajay Kumar

](https://medium.com/@trivajay259?source=post_page---read_next_recirc--9b13b688b146----2---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## Mastering Reinforcement Learning: Chapter By Chapter Guide to Sutton & Barto

### Chapter 1: Introduction



](https://medium.com/@trivajay259/mastering-reinforcement-learning-chapter-by-chapter-guide-to-sutton-barto-f60ccc84ed89?source=post_page---read_next_recirc--9b13b688b146----2---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Apr 19

[](https://medium.com/@trivajay259/mastering-reinforcement-learning-chapter-by-chapter-guide-to-sutton-barto-f60ccc84ed89?source=post_page---read_next_recirc--9b13b688b146----2---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

![Google just Punished GeekforGeeks](https://miro.medium.com/v2/resize:fit:679/1*DYureqcXI2qWrENhi5XiGA.png)

[

![Write A Catalyst](https://miro.medium.com/v2/resize:fill:20:20/1*KCHN5TM3Ga2PqZHA4hNbaw.png)



](https://medium.com/write-a-catalyst?source=post_page---read_next_recirc--9b13b688b146----3---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

In

[

Write A Catalyst

](https://medium.com/write-a-catalyst?source=post_page---read_next_recirc--9b13b688b146----3---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

by

[

Adarsh Gupta

](https://adarsh-gupta.medium.com/?source=post_page---read_next_recirc--9b13b688b146----3---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

## Google just Punished GeekforGeeks

### And that’s for a reason.



](https://adarsh-gupta.medium.com/google-just-punished-geekforgeeks-528ff2d3edad?source=post_page---read_next_recirc--9b13b688b146----3---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

Apr 11

[

3.2K

143





](https://adarsh-gupta.medium.com/google-just-punished-geekforgeeks-528ff2d3edad?source=post_page---read_next_recirc--9b13b688b146----3---------------------8c772389_655a_45b3_9846_ae32ef8e6012--------------)

[

See more recommendations

](https://medium.com/?source=post_page---read_next_recirc--9b13b688b146---------------------------------------)

[

Help

](https://help.medium.com/hc/en-us?source=post_page-----9b13b688b146---------------------------------------)

[

Status

](https://medium.statuspage.io/?source=post_page-----9b13b688b146---------------------------------------)

[

About

](https://medium.com/about?autoplay=1&source=post_page-----9b13b688b146---------------------------------------)

[

Careers

](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9b13b688b146---------------------------------------)

[

Press

](mailto:pressinquiries@medium.com)

[

Blog

](https://blog.medium.com/?source=post_page-----9b13b688b146---------------------------------------)

[

Privacy

](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9b13b688b146---------------------------------------)

[

Rules

](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----9b13b688b146---------------------------------------)

[

Terms

](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9b13b688b146---------------------------------------)

[

Text to speech

](https://speechify.com/medium?source=post_page-----9b13b688b146---------------------------------------)

------
[

Write



](https://medium.com/new-story?source=post_page---top_nav_layout_nav-----------------------------------------)

[

](https://medium.com/me/notifications?source=post_page---top_nav_layout_nav-----------------------------------------)

![Dfsj](https://miro.medium.com/v2/resize:fill:64:64/0*PLjgQ4YC02_2Jtni)

Get unlimited access to the best of Medium for less than $1/week.

[

Become a member

](https://medium.com/plans?source=upgrade_membership---post_top_nav_upsell-----------------------------------------)

# RL — Policy Gradients Explained (Part 2)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:64:64/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)





](https://jonathan-hui.medium.com/?source=post_page---byline--20c2b81a9a8b---------------------------------------)

[Jonathan Hui](https://jonathan-hui.medium.com/?source=post_page---byline--20c2b81a9a8b---------------------------------------)

Follow

8 min read

·

Sep 15, 2018

684

2

[

](https://medium.com/plans?dimension=post_audio_button&postId=20c2b81a9a8b&source=upgrade_membership---post_audio_button-----------------------------------------)

Looking to hide highlights? You can now hide them from the “•••” menu.

Okay, got it

![](https://miro.medium.com/v2/resize:fit:700/0*Io4ny2TMRtRxTxj7)

Photo by [Lauren Fleischmann](https://unsplash.com/@theburbgirl?utm_source=medium&utm_medium=referral)

In the first part of the [Policy Gradients article](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146), we cover the basic. In the second part, we continue on the Temporal Difference, Hyperparameter tuning, and Importance Sampling. Temporal Difference will further reduce the variance and the importance sampling lays down the theoretical foundation for more advanced policy gradient methods like TRPO and PPO.

# **Temporal difference TD**

Next, consider you are a driver who charges your service by hours. So the value function V(s) measures how many hours to get to your final destination. We can bootstrap the value function from traveling from San Francisco to San Diego as:

![](https://miro.medium.com/v2/resize:fit:700/1*JwCtxViFKM4qtLIZYgoNLg.png)

which it assumes it takes 1, 6 and 1 hours of travel for each segment respectively.

Let’s sample our experience with a Monte Carlo rollout. In the example below, the trip takes 2, 6 and 2 hours for each segment respectively. The calculated V(SF) becomes 10 now.

![](https://miro.medium.com/v2/resize:fit:700/1*dxd9ylNLXRW27PjNNRrbsA.png)

Instead of Monte Carlo, we can use the temporal difference TD to compute **_V_**. In a 1-step lookahead, the V(S) of SF is the time taken (rewards) from SF to SJ plus V(SJ). So here is the result of the same sampled trajectory.

![](https://miro.medium.com/v2/resize:fit:700/1*RfunnpS2k9pqh7AtcEQcVw.png)

This scheme has a lower variance because we are looking into fewer actions. However, at least in the early training, it is highly biased as the _V_ values are not accurate. As we progress, the bias in _V_ will lower.

Here is the result with 2-step lookahead.

![](https://miro.medium.com/v2/resize:fit:700/1*-wfUQGfTOxWFfvN9VFTu4g.png)

Let’s write down the math. For a one-step lookahead, V is

![](https://miro.medium.com/v2/resize:fit:700/1*dkMQXO0PeI50I3rLPQgn6A.png)

The temporal difference becomes

![](https://miro.medium.com/v2/resize:fit:700/1*6kcdIYRYuv8cFE8CYtgSIw.png)

We can compute the _Q_ function with a similar concept.

![](https://miro.medium.com/v2/resize:fit:700/1*prSSZQ08M4lovU6RhT5y0w.png)

So which one do we use in calculating the policy gradient?

# **Generalized advantage estimation** (**GAE)**

An n-step look ahead advantage function is defined as:

![](https://miro.medium.com/v2/resize:fit:700/1*pg7HeWGA7Ci5nEGcdbY81A.jpeg)

In GAE, we blend the temporal difference results together. Here are different advantage function with 1 to k-step lookahead.

![](https://miro.medium.com/v2/resize:fit:700/1*EDJU3mUUU5YKDb7XaPxwkg.png)

[Source](https://arxiv.org/pdf/1506.02438.pdf)

The final advantage function for GAE is

![](https://miro.medium.com/v2/resize:fit:700/1*CNLtrsHS8uAwBFl5aXqvnA.png)

[Source](https://arxiv.org/pdf/1506.02438.pdf)

where **_λ_** is a hyperparameter from 0 to 1. When λ is 1, it is Monte Carlo. When λ is 0, it is TD with one step look ahead.

![](https://miro.medium.com/v2/resize:fit:700/1*8Av2vm0WCtrtdheZOLr1IA.png)

Now, we have the formula to blend Monte Carlo and TD together. But we still have not answered what λ to use. This will be answered next in hyperparameter tuning.

# Hyperparameter tuning

What are the hyperparameter values for the reward discount rate **_γ_** and **_λ_**?

_λ_ blends TD results with Monte Carlo to reduce variance. As _λ_ decreases from one to zero, we weight heavier towards the TD than Monte Carlo. TD reduces variance but it increases bias. We are not using the exact result from playing the whole trajectory in computing the rewards and therefore the bias increases. In practice, we want mainly Monte Carlo result with some minor help from TD. As shown below in the figure on the left for one of the toy experiment, the cost drops as we have _λ_ closer to one but not exactly one. So we should focus on Monte Carlo with little TD learning.

![](https://miro.medium.com/v2/resize:fit:700/1*chchgVsdnTq3LGAa341utQ.png)

[Source](https://arxiv.org/pdf/1506.02438.pdf)

In many RL problems, the rewards obtained in the future is as good as the rewards now. The discount rate _γ_ can set to one for no discount. However, in many algorithms, like Actor-critic algorithm, the variance in the _Q_ estimation (the critic value) decreases if γ is smaller than one. This helps the models to learn much better even the problem itself does not ask for it.

In the toy experiment demonstrated above, it achieves the best performance (the brighter the better in the figure above) when _γ_ is 0.98 and _λ_ is 0.96. These parameters need to be tuned and hopefully, this section gives you some pointers.

# On-policy learning v.s. off-policy learning

PG that we described is an on-policy learning. We refine the current policy and use the same policy to explore and to collect samples to compute the policy gradient.

![](https://miro.medium.com/v2/resize:fit:700/1*0OlxZe1CCZvwq8MnGpjLTQ.png)

On-policy learning has poor sample efficiency. Next update needs to recollect new samples again using the new policy to compute the policy gradient. Old samples collected are not reusable. For a trajectory with hundreds of moves, this is awfully inefficient for just a single policy update.

Alternatively, the rewards of a trajectory can be computed using [importance sampling](https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6). In importance sampling, the expected reward can be computed with a different policy and later recalibrated by the distribution ratio of τ (underline in red below).

![](https://miro.medium.com/v2/resize:fit:700/1*vszYDIllvVsg0xgtlwBGrQ.png)

So the expected rewards can be estimated from samples collected from previous policy. This opens the door of not refreshing the collected samples whenever a policy is changed. But it has its own limitations.

**Importance sampling**

But let’s look into how importance sampling is related to policy gradient first. Can we derive the Policy Gradient using importance sampling?

![](https://miro.medium.com/v2/resize:fit:700/1*Sh3ImcteMAEXwoiab2Ro0g.png)

[Modified from source](https://drive.google.com/file/d/0BxXI_RttTZAhY216RTMtanBpUnc/view)

This is the same result that we derive the policy gradient before.

Based on that, we can express our objective using importance sampling. The policy gradient becomes:

![](https://miro.medium.com/v2/resize:fit:700/1*I77bAuHqrdY4BN7MK2fdrg.jpeg)

Again, we will deal with a chain of multiplication that may explode or shrink to zero.

Let’s rework on the objective function again:

![](https://miro.medium.com/v2/resize:fit:700/1*VGHKwQKNrsqLBuoktxF1xw.png)

Applying importance sampling, it becomes:

![](https://miro.medium.com/v2/resize:fit:700/1*2IbYWrzSslTKRD3loGEoag.png)

If we can constraint how far we change the policy, we can ignore the term cross out above. i.e. the probability distribution of states between two similar policies should be close to one. Hence, the objective can be formulated as

![](https://miro.medium.com/v2/resize:fit:700/1*8fq8BMEYDJQY_8XuSpHPdQ.png)

with an added constraint that the new policy cannot be different from the old policy by _δ_ (measured with divergence). And we can improve and refresh the objective iteratively to find the optimal policy.

Why do we formulate the problem in yet another way using the importance sampling? In RL, the input training samples change alone the course of training. As we know better, we search a different part of the space. The tradition optimization in deep learning like gradient descent assumes the data distribution for input is relatively constant. However, RL breaks this assumption and makes learning rate tuning very hard.

==The importance sampling concept provides a foundation for more advanced Policy Gradient methods including TRPO and PPO.== The added constraint provides us with a guideline of how far we can change the policy before our calculation will be too far away from the current policy and we can not trust the calculation anymore. This trust region helps us not to take over-optimistic actions that hurt the training progress. To detail the concept, it needs more explanation and therefore we will reserve the discussion in separate articles.

# More thoughts

In _Q_-learning, we try out different actions from a state and figure out how good (the value of _Q_) in taking those actions. Does Curry try our different poses and measure how good they are? Or he starts with some pose and evolves them gradually (Just like the Policy Gradient). We will not have the answer for you. There is no theoretical reasoning to claim whether _Q_-learning or Policy Gradient is better. In practice, we work with limit resources and different approaches may favor different RL tasks. But Policy Gradient is obviously one intuitive and popular way to solve RL problems. This is how a human may make decisions and the RL training is more interpretable.

However, Policy Gradient has high variance and bad sample efficiency. To combat the variance problem, we need a larger batch of samples to compute each policy gradient. We can compute a baseline to reduce the variance. To balance between bias and variance, GAE mixes Monte Carlo and TD learning which provides us a mechanism to tune the training using different tradeoffs. Tweaking learning rates for PG is very hard. Consider using more advanced optimization like ADAM or RMSProp.

# Advanced Policy Gradient Methods

Nevertheless, Natural Policy Gradient becomes a more popular approach in optimizing the policy. It discourages making too aggressive moves that turn out to be wrong and destroy the training progress. Conceptually, it was done by taking moves only within a trust-region distance. In short, don’t make policy change so big that the calculation becomes not reliable enough to be trusted. But the new approach is a second-order optimization that suffers badly in complexity and does not scale well for large models. TRPO is introduced to remove the necessity of computing the expensive inverse of the Fisher Information Matrix FIM. But it is still not enough since computing the FIM still requires a lot of sampling. Can we relax the requirement from the Natural Policy Gradient so we don’t need to perform a second-order optimization? PPO responds by only imposing a soft constraint on the policy change so it can use the regular gradient ascent method in optimizing the object. Here is a quick summary of the trends in Policy Gradient. Stay tuned in [the deep reinforcement learning series](https://medium.com/@jonathan_hui/rl-deep-reinforcement-learning-series-833319a95530) articles for each advanced method.

# Credit and references

[UCL RL course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)

[UC Berkeley RL course](http://rail.eecs.berkeley.edu/deeprlcourse/)

[UC Berkeley RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)

[A3C paper](https://arxiv.org/pdf/1602.01783.pdf)

[GAE paper](https://arxiv.org/pdf/1506.02438.pdf)

[

Machine Learning

](https://medium.com/tag/machine-learning?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Reinforcement Learning

](https://medium.com/tag/reinforcement-learning?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Deep Learning

](https://medium.com/tag/deep-learning?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Data Science

](https://medium.com/tag/data-science?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Artificial Intelligence

](https://medium.com/tag/artificial-intelligence?source=post_page-----20c2b81a9a8b---------------------------------------)

684

2

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:96:96/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---post_author_info--20c2b81a9a8b---------------------------------------)

[

## Written by Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---post_author_info--20c2b81a9a8b---------------------------------------)

[41K followers](https://jonathan-hui.medium.com/followers?source=post_page---post_author_info--20c2b81a9a8b---------------------------------------)

·[39 following](https://jonathan-hui.medium.com/following?source=post_page---post_author_info--20c2b81a9a8b---------------------------------------)

Deep Learning

Follow

## Responses (2)

[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--20c2b81a9a8b---------------------------------------)

![Dfsj](https://miro.medium.com/v2/resize:fill:32:32/0*PLjgQ4YC02_2Jtni)

Dfsj

What are your thoughts?﻿

Cancel

Respond

[

![Lucas Baião Pires](https://miro.medium.com/v2/resize:fill:32:32/0*w-fSKfKGdVmvWQAg.)





](https://medium.com/@lucasbaiao?source=post_page---post_responses--20c2b81a9a8b----0-----------------------------------)

[

Lucas Baião Pires



](https://medium.com/@lucasbaiao?source=post_page---post_responses--20c2b81a9a8b----0-----------------------------------)

[

Apr 21, 2020

](https://medium.com/@lucasbaiao/i-cant-see-how-gae-is-not-0-when-lambda-1-could-you-explain-it-67f4062d446f?source=post_page---post_responses--20c2b81a9a8b----0-----------------------------------)

I can’t see how GAE is not 0 when \lambda=1. Could you explain it?

1

1 reply

Reply

[

![Nico buton](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)





](https://medium.com/@butonnico?source=post_page---post_responses--20c2b81a9a8b----1-----------------------------------)

[

Nico buton



](https://medium.com/@butonnico?source=post_page---post_responses--20c2b81a9a8b----1-----------------------------------)

[

Dec 24, 2019

](https://medium.com/@butonnico/in-the-part-generalized-advantage-estimation-gae-in-the-first-formula-2b2cdd7bd90?source=post_page---post_responses--20c2b81a9a8b----1-----------------------------------)

In the part Generalized advantage estimation (GAE), in the first formula. I think there is an error on the indice of the sum, it’s between t and t+n-1.

1 reply

Reply

## More from Jonathan Hui

![mAP (mean Average Precision) for Object Detection](https://miro.medium.com/v2/resize:fit:679/1*FrmKLxCtkokDC3Yr1wc70w.png)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----0---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----0---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

## mAP (mean Average Precision) for Object Detection

### AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision…



](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173?source=post_page---author_recirc--20c2b81a9a8b----0---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

Mar 7, 2018

[

7.6K

54





](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173?source=post_page---author_recirc--20c2b81a9a8b----0---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

![Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)](https://miro.medium.com/v2/resize:fit:679/0*Kik_29u0aNSWCsux)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----1---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----1---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

## Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)

### In machine learning (ML), one of the most important linear algebra concepts is the singular value decomposition (SVD). With all the raw…



](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491?source=post_page---author_recirc--20c2b81a9a8b----1---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

Mar 7, 2019

[

4K

35





](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491?source=post_page---author_recirc--20c2b81a9a8b----1---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

![Understanding Feature Pyramid Networks for object detection (FPN)](https://miro.medium.com/v2/resize:fit:679/0*RCxiFYR3z6NTsi2p.)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----2---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----2---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

## Understanding Feature Pyramid Networks for object detection (FPN)

### Detecting objects in different scales is challenging in particular for small objects. We can use a pyramid of the same image at different…



](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c?source=post_page---author_recirc--20c2b81a9a8b----2---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

Mar 27, 2018

[

4.2K

30





](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c?source=post_page---author_recirc--20c2b81a9a8b----2---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

![SSD object detection: Single Shot MultiBox Detector for real-time processing](https://miro.medium.com/v2/resize:fit:679/1*N-ZCvRQL9iwnlYTX5XM2Iw.jpeg)

[

![Jonathan Hui](https://miro.medium.com/v2/resize:fill:20:20/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg)



](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----3---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b----3---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

## SSD object detection: Single Shot MultiBox Detector for real-time processing

### SSD is designed for object detection in real-time. Faster R-CNN uses a region proposal network to create boundary boxes and utilizes those…



](https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06?source=post_page---author_recirc--20c2b81a9a8b----3---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

Mar 14, 2018

[

3.4K

37





](https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06?source=post_page---author_recirc--20c2b81a9a8b----3---------------------cb09e378_5651_47ac_b031_1770612f7c27--------------)

[

See all from Jonathan Hui

](https://jonathan-hui.medium.com/?source=post_page---author_recirc--20c2b81a9a8b---------------------------------------)

## Recommended from Medium

![OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python](https://miro.medium.com/v2/resize:fit:679/0*zf05Z-2A_5rXq-VP.png)

[

![Neural pAi](https://miro.medium.com/v2/resize:fill:20:20/1*WMzhCjQjIT1OxUdTZs9TPQ.png)



](https://neuralpai.medium.com/?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

Neural pAi

](https://neuralpai.medium.com/?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## OpenAI Gym and Gymnasium: Reinforcement Learning Environments for Python



](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

Mar 3

[

53





](https://neuralpai.medium.com/openai-gym-and-gymnasium-reinforcement-learning-environments-for-python-c889aed0e784?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

![Value-Based vs Policy-Based Reinforcement Learning](https://miro.medium.com/v2/resize:fit:679/1*PkeEQpSJ-kViy7KmbwzSTA.png)

[

![Papers in 100 Lines of Code](https://miro.medium.com/v2/resize:fill:20:20/1*HFmaIJiZ7BehRryMgLDvxQ.jpeg)



](https://papers-100-lines.medium.com/?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

Papers in 100 Lines of Code

](https://papers-100-lines.medium.com/?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## Value-Based vs Policy-Based Reinforcement Learning

### Two primary approaches in Reinforcement Learning (RL) are value-based methods and policy-based methods. In this article, we are going to…



](https://papers-100-lines.medium.com/value-based-vs-policy-based-reinforcement-learning-92da766696fd?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

Nov 21, 2024

[

4

1





](https://papers-100-lines.medium.com/value-based-vs-policy-based-reinforcement-learning-92da766696fd?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

![Mastering Reinforcement Learning: Chapter By Chapter Guide to Sutton & Barto](https://miro.medium.com/v2/resize:fit:679/1*yuVzMhCJyDENbyhwAsrkwA.png)

[

![Ajay Kumar](https://miro.medium.com/v2/resize:fill:20:20/0*YftkGL-ygoKiogyl)



](https://medium.com/@trivajay259?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

Ajay Kumar

](https://medium.com/@trivajay259?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## Mastering Reinforcement Learning: Chapter By Chapter Guide to Sutton & Barto

### Chapter 1: Introduction



](https://medium.com/@trivajay259/mastering-reinforcement-learning-chapter-by-chapter-guide-to-sutton-barto-f60ccc84ed89?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

Apr 19

[](https://medium.com/@trivajay259/mastering-reinforcement-learning-chapter-by-chapter-guide-to-sutton-barto-f60ccc84ed89?source=post_page---read_next_recirc--20c2b81a9a8b----0---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

![Understanding the GRPO Algorithm in Reinforcement Learning for LLMs](https://miro.medium.com/v2/resize:fit:679/1*GXJSX3S5hZvXqZgWsdY_tA.png)

[

![LM Po](https://miro.medium.com/v2/resize:fill:20:20/1*8biNIOdTZO6v4MDdtPmm2Q.png)



](https://medium.com/@lmpo?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

LM Po

](https://medium.com/@lmpo?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## Understanding the GRPO Algorithm in Reinforcement Learning for LLMs

### Reinforcement Learning (RL) has become a cornerstone in training advanced AI systems, particularly large language models (LLMs) like…



](https://medium.com/@lmpo/understanding-the-grpo-algorithm-in-reinforcement-learning-for-llms-09914cd21750?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

May 5

[

8





](https://medium.com/@lmpo/understanding-the-grpo-algorithm-in-reinforcement-learning-for-llms-09914cd21750?source=post_page---read_next_recirc--20c2b81a9a8b----1---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

![This new IDE from Google is an absolute game changer](https://miro.medium.com/v2/resize:fit:679/1*f-1HQQng85tbA7kwgECqoQ.png)

[

![Coding Beauty](https://miro.medium.com/v2/resize:fill:20:20/1*ViyWUoh4zqx294no1eENxw.png)



](https://medium.com/coding-beauty?source=post_page---read_next_recirc--20c2b81a9a8b----2---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

In

[

Coding Beauty

](https://medium.com/coding-beauty?source=post_page---read_next_recirc--20c2b81a9a8b----2---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

by

[

Tari Ibaba

](https://medium.com/@tariibaba?source=post_page---read_next_recirc--20c2b81a9a8b----2---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## This new IDE from Google is an absolute game changer

### This new IDE from Google is seriously revolutionary.



](https://medium.com/@tariibaba/new-google-project-idx-fae1fdd079c7?source=post_page---read_next_recirc--20c2b81a9a8b----2---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

Mar 12

[

5.4K

315





](https://medium.com/@tariibaba/new-google-project-idx-fae1fdd079c7?source=post_page---read_next_recirc--20c2b81a9a8b----2---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

![Easy Explained → Mixture of Experts](https://miro.medium.com/v2/resize:fit:679/0*qSkYpXGAeg_XGBnY)

[

![Marco Pi](https://miro.medium.com/v2/resize:fill:20:20/1*hfxmDm7LXECW3lki7j0BaQ@2x.jpeg)



](https://medium.com/@marcopia.remote?source=post_page---read_next_recirc--20c2b81a9a8b----3---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

Marco Pi

](https://medium.com/@marcopia.remote?source=post_page---read_next_recirc--20c2b81a9a8b----3---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

## Easy Explained → Mixture of Experts

### Think about how you’d fix your car. You don’t ask the baker. You don’t ask the teacher. Heck, you’d surely don’t ask the dentist to look at…



](https://medium.com/@marcopia.remote/easy-explained-mixture-of-experts-b8347d8686ed?source=post_page---read_next_recirc--20c2b81a9a8b----3---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

Mar 17

[

435





](https://medium.com/@marcopia.remote/easy-explained-mixture-of-experts-b8347d8686ed?source=post_page---read_next_recirc--20c2b81a9a8b----3---------------------7c7b0ed0_ebc0_4b00_84fa_22e2b4fef45f--------------)

[

See more recommendations

](https://medium.com/?source=post_page---read_next_recirc--20c2b81a9a8b---------------------------------------)

[

Help

](https://help.medium.com/hc/en-us?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Status

](https://medium.statuspage.io/?source=post_page-----20c2b81a9a8b---------------------------------------)

[

About

](https://medium.com/about?autoplay=1&source=post_page-----20c2b81a9a8b---------------------------------------)

[

Careers

](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Press

](mailto:pressinquiries@medium.com)

[

Blog

](https://blog.medium.com/?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Privacy

](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Rules

](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Terms

](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----20c2b81a9a8b---------------------------------------)

[

Text to speech

](https://speechify.com/medium?source=post_page-----20c2b81a9a8b---------------------------------------)