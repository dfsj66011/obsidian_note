[

Master Generative AI with 10+ Real-world Projects in 2025!

Download Projects](https://www.analyticsvidhya.com/pinnacleplus/pinnacleplus-projects?utm_source=blog_india&utm_medium=desktop_flashstrip&utm_campaign=15-Feb-2025||&utm_content=projects)

[![Analytics Vidhya](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/icon/av-logo-svg.svg)](https://www.analyticsvidhya.com/blog/)

- [Free Courses](https://www.analyticsvidhya.com/courses/?ref=Navbar)
- [Learning Paths](https://www.analyticsvidhya.com/learning-path/chat/?utm_source=blog&utm_medium=navbar)
- [GenAI Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=blognavbar)New
- [Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=blognavbar)

- [](https://www.analyticsvidhya.com/?s=)
- [](https://datahack.analyticsvidhya.com/blogathon/)
- Login

[Interview Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)

[Career](https://www.analyticsvidhya.com/blog/category/career/?ref=category)

[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=category)

[Prompt Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)

[ChatGPT](https://www.analyticsvidhya.com/blog/category/chatgpt/?ref=category)

[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=category)

[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)

[RAG](https://www.analyticsvidhya.com/blog/category/rag/?ref=category)

[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)

[Machine Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)

[Deep Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)

[GenAI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)

[LLMOps](https://www.analyticsvidhya.com/blog/category/llmops/?ref=category)

[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)

[NLP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)

[SQL](https://www.analyticsvidhya.com/blog/category/sql/?ref=category)

[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category)

1. [Home](https://www.analyticsvidhya.com/blog/)
2. [LLMs](https://www.analyticsvidhya.com/blog/category/llms/)
3. LLM Optimization: Optimizing AI with GRPO, PPO, and DPO

# LLM Optimization: Optimizing AI with GRPO, PPO, and DPO

[![Neil D](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_sT5MuqV.webp)](https://www.analyticsvidhya.com/blog/author/akashdas/)

[Neil D](https://www.analyticsvidhya.com/blog/author/akashdas/)Last Updated : 20 Mar, 2025

 21 min read

23

Reinforcement Learning (RL) has driven breakthroughs in robotics, game-playing AI, and control systems by optimizing decision-making through long-term rewards. Initially, large language models (LLMs) relied on supervised learning, limiting adaptability and nuanced human preference alignment. The introduction of RLHF (Reinforcement Learning with Human Feedback) revolutionized LLMs, enabling models like ChatGPT and DeepSeek to optimize responses based on user feedback. However, standard PPO-based RLHF faced inefficiencies, requiring costly reward modeling. DeepSeek’s Group Relative Policy Optimization (GRPO) eliminated this by directly optimizing preference rankings, marking a significant advancement in LLM optimization and modern reinforcement learning techniques, enhancing adaptability and performance.

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/image-7-2.webp)

Source: [Link](https://pub.towardsai.net/group-relative-policy-optimization-grpo-illustrated-breakdown-explanation-684e71b8a3f2)

### Learning Objectives 

- Understand why RL-based techniques are crucial for optimizing LLMs like ChatGPT, DeepSeek, Claude, and Gemini.
- Learn the fundamentals of policy optimization, including PG, TRPO, and PPO.Explore DPO and GRPO for preference-based LLM training without explicit reward models.
- Compare PG, TRPO, PPO, DPO, and GRPO to determine the best approach for RL and LLM fine-tuning.
- Gain hands-on experience with Python implementations of policy optimization algorithms.
- Evaluate fine-tuning impact using training loss curves and probability distributions.
- Apply DPO and GRPO to enhance LLM safety, alignment, and reliability.

_**This article was published as a part of the**_ [_**Data Science Blogathon.**_](https://www.analyticsvidhya.com/datahack/blogathon)

## Table of contents

1. [Primer on Policy Optimization Techniques](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-primer-on-policy-optimization-techniques)
2. [Mathematical Foundations (Required for All Methods)](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-mathematical-foundations-required-for-all-methods)
3. [Policy Gradient (PG) – The Foundation](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-policy-gradient-pg-the-foundation)
4. [The Policy Gradient Theorem](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-the-policy-gradient-theorem)
5. [Code Example: REINFORCE Algorithm](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-code-example-reinforce-algorithm)
6. [Trust Region Policy Optimization (TRPO)](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-trust-region-policy-optimization-trpo-nbsp) 
7. [TRPO Algorithm & Key Mathematical Concepts](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-trpo-algorithm-amp-key-mathematical-concepts)
8. [Code Example: TRPO Training Loop](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-code-example-trpo-training-loop)
9. [Proximal Policy Optimization (PPO)](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-proximal-policy-optimization-ppo)
10. [PPO Algorithm & Key Mathematical Concept](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-ppo-algorithm-amp-key-mathematical-concept)
11. [Code Example: PPO Training Loop](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-code-example-ppo-training-loop)
12. [Direct Preference Optimization (DPO) – Preference Learning for LLMs](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-direct-preference-optimization-dpo-preference-learning-for-llms)
13. [Code Example: Direct Preference Optimization (DPO)](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-code-example-direct-preference-optimization-dpo)
14. [GRPO – Group Relative Policy Optimization (DeepSeek’s Approach)](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-grpo-group-relative-policy-optimization-deepseek-s-approach)
15. [Mathematical Foundation of GRPO](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-mathematical-foundation-of-grpo)
16. [Data for GRPO Fine-Tuning](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-data-for-grpo-fine-tuning)
17. [Code Implementation: Group-Based Preference Optimization](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-code-implementation-group-based-preference-optimization)
18. [Training Loop for GRPO](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-training-loop-for-grpo)
19. [Expected Outcome and Results](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-expected-outcome-and-results)
20. [Final Model Insights: Why GRPO Excels in LLM Fine-Tuning](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-final-model-insights-why-grpo-excels-in-llm-fine-tuning)
21. [Conclusion](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-conclusion)
22. [Frequently Asked Questions](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/#h-frequently-asked-questions)

## Primer on Policy Optimization Techniques

Before diving into DeepSeek’s GRPO, it’s crucial to understand the policy optimization techniques that form the foundation of reinforcement learning (RL) in both traditional control tasks and [LLM fine-tuning](https://www.analyticsvidhya.com/blog/2024/12/fine-tuning-llama-3-2-3b-for-rag/). Policy optimization refers to the process of improving an AI agent’s decision-making strategy (policy) to maximize expected rewards. While early methods like vanilla [policy gradient (PG)](https://www.analyticsvidhya.com/blog/2020/11/baseline-for-policy-gradients/) laid the groundwork, more sophisticated techniques such as TRPO, PPO, DPO, and GRPO evolved to address issues like stability, efficiency, and preference alignment.

[

New Feature

##### Get Personalized Learning Path!Set your goal and timeline. Get a path—under 2 mins.

Create My Path

](https://www.analyticsvidhya.com/learning-path/chat/?article_id=221804&utm_source=blog_banner)

### What is Policy Optimization?

  At its core, policy optimization is about learning the optimal policy **π_θ(a∣s)**, which maps a state _s_ to an action _a_ while maximizing long-term rewards. The objective function in RL is typically formulated as:  

![Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/Eq1.webp)

Where **R(τ)** is the total reward collected in a trajectory τ, and the expectation is taken over all possible trajectories following policy **π_θ**.

There are three major approaches to policy optimization:

#### 1. Gradient-Based Optimization (Policy Gradient Methods)

- These methods directly compute gradients of expected reward and update policy parameters using gradient ascent.
- Example: REINFORCE algorithm (Vanilla Policy Gradient).
- Pros: Simple, works with continuous and discrete actions.
- Cons: High variance, requires tricks like baseline subtraction.

#### 2. Trust-Region Optimization (TRPO, PPO)

- Introduces constraints (KL divergence) to ensure policy updates are stable and not too drastic.
- Example: TRPO ensures updates stay within a “trust region”; PPO simplifies this with clipping.
- Pros: More stable than raw policy gradients.
- Cons: Computationally expensive (TRPO), hyperparameter-sensitive (PPO).

#### 3. Preference-Based Optimization (DPO, GRPO)

- Optimizes directly from ranked human preferences instead of rewards.
- Example: DPO learns from preferred vs. rejected responses; GRPO generalizes to groups.
- Pros: Eliminates the need for reward models and better aligns LLMs with human intent.
- Cons: Requires high-quality preference data.

## Mathematical Foundations (Required for All Methods)

### A. Markov Decision Process (MDP)

RL is typically formulated as a **Markov Decision Process** (MDP), represented as:

![Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/MDP.webp)

where:

- **S** is the state space,
- **A** is the action space,
- **P(s′∣s,a)** is the transition probability to state s′,
- **R(s,a)** is the reward function,
- **γ** is the discount factor (how much future rewards are valued).

###   B. Expected Return J(θ)

-   The **Expected Return** (ER) measures how much cumulative reward we expect from following policy **π_θ**:  

![Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/ER.webp)

  where **γ** (0 ≤ **γ** ≤ 1) determines how much future rewards contribute.  

### C. Policy Gradient Theorem

Policy gradient (PG) methods update the policy using gradients of expected rewards. The key equation:

![Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PG.webp)

where:

- **A(s,a)** is the advantage function (how good action **a** is compared to average actions in state **s**).
- **logπ_θ​** ensures we increase the probabilities of better actions.

### D. Advantage Function A(s,a)

To reduce variance in gradient estimates, we use the advantage function:

![Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/AdvFunc.webp)

where:

- **Q(s,a)** is the expected return for taking action **a** at state **s**.
- **V(s)** is the expected return following policy **π** from **s**.

Using **A(s,a)** helps make updates more stable and efficient.

## Policy Gradient (PG) – The Foundation

The Policy Gradient (PG) method is the most fundamental approach to [reinforcement learning](https://www.analyticsvidhya.com/blog/2021/02/introduction-to-reinforcement-learning-for-beginners/). Instead of learning a value function, PG directly parameterizes the policy **π_θ(a∣s)** and updates it using gradient ascent. This allows learning in continuous action spaces, making it effective for tasks like robotics, game AI, and LLM fine-tuning.

However, PG methods suffer from high variance due to their reliance on sampling full trajectories. More advanced methods like TRPO, PPO, and GRPO build upon PG to improve stability.

## The Policy Gradient Theorem

-   The goal of policy optimization is to find policy parameters **θ** that maximize expected return:  

![The Policy Gradient Theorem](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PG_1.webp)

- Using the log-derivative trick, we obtain the Policy Gradient Theorem:

![The Policy Gradient Theorem](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PG_2.webp)

where:

- **∇θ​logπθ​(a∣s)** is the gradient of the log-probability of taking action aaa.
- **A(s,a)** (Advantage function) determines how much better action aaa is compared to others.
- We perform gradient ascent to increase the probability of good actions.

## Code Example: REINFORCE Algorithm

The REINFORCE algorithm is the simplest form of PG. It samples trajectories, computes rewards, and updates the policy parameters. Below is the main training loop (only the key function is shown to limit the scope; the full notebook is [linked](https://colab.research.google.com/drive/1oqRPR_mg8tpw6JxJZlFBdHfyJ6Sp7_t7?usp=sharing)).

```scss
def train_policy_gradient(env, policy, optimizer, num_episodes=500, gamma=0.99):
    """Train a policy using the REINFORCE algorithm"""
    reward_history = []

    for episode in range(num_episodes):
        state, _ = env.reset()
        log_probs = []
        rewards = []
        done = False

        while not done:
            state = torch.FloatTensor(state).unsqueeze(0)
            action_probs = policy(state)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample()

            log_probs.append(action_dist.log_prob(action))
            next_state, reward, done, _, _ = env.step(action.item())
            rewards.append(reward)
            state = next_state

        # Compute discounted rewards
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)

        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)  # Normalize for stability

        # Compute policy gradient loss
        loss = []
        for log_prob, G in zip(log_probs, returns):
            loss.append(-log_prob * G)  # Gradient ascent on expected return
        loss = torch.stack(loss).sum()

        # Optimize policy
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        reward_history.append(sum(rewards))

    return reward_history
```

  🔗 [Full implementation available here](https://colab.research.google.com/drive/1oqRPR_mg8tpw6JxJZlFBdHfyJ6Sp7_t7?usp=sharing) 

### Code Explanation

The **_train_policy_gradient_** function implements the **_REINFORCE_** algorithm, which optimizes policy parameters using Monte Carlo updates. The training begins by initializing the environment and iterating over multiple episodes, collecting state-action-reward trajectories. For each step in an episode, an action is sampled from the policy, executed in the environment, and its corresponding reward is stored. After completing an episode, the discounted rewards are computed using the **_compute_discounted_rewards_** function, ensuring that future rewards contribute appropriately to policy updates. These rewards are then normalized to reduce variance, making training more stable. The policy loss is calculated by multiplying the log probabilities of actions by their respective discounted rewards. Finally, the policy is updated using gradient descent, which maximizes the expected return by reinforcing actions that led to higher rewards.

#### Expected Outcomes & Justification

The training plot demonstrates how the total episode rewards evolve over 500 episodes. Initially, the agent performs poorly, as seen in the low reward values in early episodes (e.g., Episode 50: 20.0). However, as training progresses, the agent learns more effective strategies, leading to higher rewards (Episode 100: 134.0, Episode 150: 229.0). The performance peaks when the agent successfully balances the pole for the maximum time, reaching 500 rewards per episode (Episode 200, 350, and 450). However, instability is evident, as seen in the sharp reward drop in Episode 250 (26.0) and Episode 500 (9.0). This behaviour arises due to the high variance of PG methods, where updates can occasionally lead to suboptimal policies before stabilizing.

![Policy Gradient (REINFORCE)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PG_OP1.webp)

![Policy Gradient (REINFORCE)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PG_OpP2.webp)

The overall trend shows increasing average rewards, indicating that the policy is improving. However, fluctuations in rewards highlight the limitation of vanilla PG methods, which motivates the need for more stable techniques like TRPO and PPO.

## Trust Region Policy Optimization (TRPO) 

While Policy Gradient (PG) methods like REINFORCE are effective, they suffer from high variance and instability in updates. One bad update can drastically collapse the learned policy. TRPO (Trust Region Policy Optimization) improves upon PG by ensuring updates are constrained within a trust region, preventing abrupt changes that could harm performance.

- Instead of using vanilla gradient descent, TRPO solves a constrained optimization problem:

![Trust Region Policy Optimization (TRPO)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_Eq1.webp)

- This KL-divergence constraint ensures that the new policy is not too far from the previous policy, leading to more stable updates.

## TRPO Algorithm & Key Mathematical Concepts

TRPO optimizes the policy using Generalized Advantage Estimation (GAE) and Conjugate Gradient Descent.

**1. Generalized Advantage Estimation (GAE):** Computes an advantage function to estimate how much better an action is compared to the expected return.

![Generalized Advantage Estimation (GAE)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_Eq2.webp)

  where **_δ_t_** is the TD error:  

![Generalized Advantage Estimation (GAE)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_Eq3.webp)

**2. Trust Region Constraint:** Ensures updates stay within a safe region using KL-divergence.

  where **_δ_**  is the maximum step size.  

![Trust Region Constraint](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_Eq4.webp)

**3.  Conjugate Gradient Optimization:** Instead of directly computing the inverse Hessian, TRPO uses a conjugate gradient to find the optimal update direction efficiently.

## Code Example: TRPO Training Loop

Below is the main TRPO training function, where we apply trust region updates and compute the discounted rewards and advantages. (Only the key function is shown; the full notebook [link](https://colab.research.google.com/drive/15g81iXXCtVvAkSXuVcd9zoqobyrl6kCW?usp=sharing).)

```scss
def train_trpo(env, policy, num_episodes=500, gamma=0.99):
    reward_history = []

    for episode in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]  # Handle Gym versions that return (state, info)

        log_probs = []
        states = []
        actions = []
        rewards = []

        done = False
        while not done:
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
            probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(probs)
            action = action_dist.sample()

            step_result = env.step(action.item())

            if len(step_result) == 5:
                next_state, reward, terminated, truncated, _ = step_result
                done = terminated or truncated  # New Gym API
            else:
                next_state, reward, done, _ = step_result  # Old Gym API

            log_probs.append(action_dist.log_prob(action))
            states.append(state_tensor)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        # Compute discounted rewards and advantages
        discounted_rewards = compute_discounted_rewards(rewards, gamma)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) 
        / (discounted_rewards.std() + 1e-9)

        # Convert lists to tensors
        states = torch.cat(states)
        actions = torch.tensor(actions)
        advantages = discounted_rewards

        # Copy old policy before updating
        old_policy = PolicyNetwork(env.observation_space.shape[0], 
        env.action_space.n)
        old_policy.load_state_dict(policy.state_dict())

        # Apply TRPO update
        trpo_step(policy, old_policy, states, actions, advantages)

        total_episode_reward = sum(rewards)
        reward_history.append(total_episode_reward)

        if (episode + 1) % 50 == 0:
            print(f"Episode {episode+1}, Total Reward: {total_episode_reward}")

    return reward_history
```

  🔗 [Full implementation available here](https://colab.research.google.com/drive/15g81iXXCtVvAkSXuVcd9zoqobyrl6kCW?usp=sharing) 

### Code Explanation

The **_train_trpo_** function implements the Trust Region Policy Optimization update. The training loop initializes the environment and runs 500 episodes, collecting states, actions, and rewards for each step. The key difference from Policy Gradient (PG) is that TRPO maintains an old policy copy and updates the new policy while ensuring the update remains within a KL-divergence bound.

The advantages are computed using discounted rewards and normalized to reduce variance. Finally, conjugate gradient descent is used to determine the optimal policy step direction. Unlike standard gradient updates, TRPO restricts step size to prevent drastic policy changes, leading to more stable performance.

### Expected Outcomes & Justification

The training curve for TRPO exhibits significant reward fluctuations, and the numerical results indicate that the policy does not consistently improve over time as shown below.

![Expected Outcomes & Justification](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_OP1.webp)

![Expected Outcomes & Justification](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/TRPO_OpP2.webp)

Unlike Policy Gradient (PG), which showed steady learning progress, TRPO struggles to maintain consistent improvements. Despite its theoretical advantages (trust region constraint preventing catastrophic updates), the actual results show high instability. The total rewards oscillate between low values (9-20), indicating that the agent fails to learn an optimal strategy efficiently.

This is a known issue with TRPO—it requires careful tuning of KL divergence constraints, and in many cases, the update process is computationally expensive and prone to suboptimal convergence. The reward fluctuations suggest that the agent isn’t exploiting learned knowledge effectively, reinforcing the need for a more practical and robust policy optimization method. PPO simplifies TRPO by approximating the trust region constraint using a clipped objective function, leading to faster and more efficient training. 

## Proximal Policy Optimization (PPO)

TRPO ensures stable policy updates but is computationally expensive due to solving a constrained optimization problem at each step. PPO (Proximal Policy Optimization) simplifies this process by using a clipped objective function to restrict updates without requiring second-order optimization.

Instead of solving:

![Proximal Policy Optimization (PPO)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_Eq1.webp)

PPO modifies the objective function by introducing a clipped surrogate loss:

![Proximal Policy Optimization (PPO)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_Eq2.webp)

where:

- **_r_t​(θ_****_)_** is the probability ratio between new and old policies.
- **_A__****_t​_** is the advantage estimate.
- **_ϵ_** is a small constant (e.g., 0.2) that limits excessive policy updates.

This prevents overshooting updates, making PPO more computationally efficient while retaining TRPO’s stability.

## PPO Algorithm & Key Mathematical Concept

**1. Advantage Estimation using GAE:** PPO improves TRPO by using Generalized Advantage Estimation (GAE) to compute stable gradients:  

![PPO Algorithm & Key Mathematical Concept](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_Eq3.webp)

  where **_δ_t_** = **_r_t_** + **_γV(s_(t+1))_** −**_V(s_t)_**.  

**2.** **Clipped Objective Function:** Unlike TRPO, which enforces a strict KL constraint, PPO approximates the constraint using clipping:

![PPO Algorithm & Key Mathematical Concept](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_Eq4.webp)

This ensures that the update does not move too far, preventing policy collapse.

**3.** **Mini-Batch Training:** Instead of updating the policy after each episode, PPO trains using mini-batches over multiple epochs, improving sample efficiency.

## Code Example: PPO Training Loop

Below is the main PPO training function, where we compute advantages, apply clipped policy updates, and use mini-batches for stable learning. (Only the key function is shown; full notebook [link](https://colab.research.google.com/drive/1Qphha6EKPR12s2Yt71qd1UPFXR2KFiw3?usp=sharing).)

```scss
def train_ppo(env, policy, optimizer, num_episodes=500, gamma=0.99, lambda_=0.95, epsilon=0.2, batch_size=32, epochs=5):
    reward_history = []

    for episode in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]  # Handle Gym versions returning (state, info)

        log_probs = []
        values = []
        states = []
        actions = []
        rewards = []

        done = False
        while not done:
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
            probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(probs)
            action = action_dist.sample()

            step_result = env.step(action.item())

            # Handle different Gym API versions
            if len(step_result) == 5:
                next_state, reward, terminated, truncated, _ = step_result
                done = terminated or truncated  # New API
            else:
                next_state, reward, done, _ = step_result  # Old API

            log_probs.append(action_dist.log_prob(action))
            states.append(state_tensor)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        # Compute advantages
        values = [0] * len(rewards)  # Placeholder for value estimates (since we use policy-only PPO)
        advantages = compute_advantages(rewards, values, gamma, lambda_)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)  # Normalize advantages

        # Convert lists to tensors
        states = torch.cat(states)
        actions = torch.tensor(actions)
        old_log_probs = torch.tensor(log_probs)

        # PPO Training Loop
        for _ in range(epochs):
            for i in range(0, len(states), batch_size):
                batch_indices = slice(i, i + batch_size)

                new_probs = policy(states[batch_indices])
                new_action_dist = torch.distributions.Categorical(new_probs)
                new_log_probs = new_action_dist.log_prob(actions[batch_indices])

                loss = ppo_loss(old_log_probs[batch_indices], new_log_probs, advantages[batch_indices], epsilon)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        total_episode_reward = sum(rewards)
        reward_history.append(total_episode_reward)

        if (episode + 1) % 50 == 0:
            print(f"Episode {episode+1}, Total Reward: {total_episode_reward}")

    return reward_history
```

🔗 [Full implementation available here](https://colab.research.google.com/drive/1Qphha6EKPR12s2Yt71qd1UPFXR2KFiw3?usp=sharing) 

### Code Explanation

The **_train_ppo_** function implements Proximal Policy Optimization (PPO) using a clipped surrogate loss and mini-batch updates. Unlike TRPO, which computes trust region constraints, PPO approximates them by clipping policy updates, making it much more efficient.

- The function begins by collecting episode trajectories (states, actions, log probabilities, and rewards).
- Advantage estimation is computed using Generalized Advantage Estimation (GAE).
- Mini-batches are used to update the policy over multiple epochs, improving sample efficiency.
- Instead of a strict KL divergence constraint, PPO applies a clipped loss function to prevent destructive updates.

### Expected Outcomes for PPO

The PPO training curve and numerical results show a clear improvement in policy learning over time:

![PPO training curve](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_OP1.webp)

![PPO training curve](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/PPO_OP2.webp)

#### Key Observations:

- **Stable Improvement:** The early rewards (Ep 50-100) are low, indicating the agent is still exploring.
- **Steady Progress:** By Episode 200, the total reward surpasses 200, showing the agent is learning a structured policy.
- **Fluctuations Exist, But Recovery is Fast:** Between Ep 300-400, rewards drop, but PPO stabilizes and quickly rebounds to peak performance (500).
-  **Final Convergence:** The model reaches 500 rewards (max score) by Ep 500, confirming PPO effectively learns an optimal strategy.

**Compared to TRPO, PPO exhibits:**

- Less noisy training
- Faster convergence
- More efficient sample utilization: These improvements validate PPO’s clipped updates and mini-batch training as a superior approach to policy learning.

PPO is excellent for reward-based learning, but it struggles with preference-based fine-tuning in applications like LLMs (e.g., ChatGPT, DeepSeek, Claude, Gemini). DPO (Direct Preference Optimization) improves upon PPO by directly learning from human preference data instead of optimizing pure rewards.

## Direct Preference Optimization (DPO) – Preference Learning for LLMs

Traditional reinforcement learning (RL) techniques are designed to optimize numerical reward-based objectives. However, Large Language Models (LLMs) like ChatGPT, DeepSeek, Claude, and Gemini require fine-tuning that aligns with human preferences rather than just maximizing a reward function. This is where Direct Preference Optimization (DPO) plays a crucial role. Unlike RL-based methods like PPO, which rely on an explicitly trained reward model, DPO optimizes models directly using human feedback. By leveraging preference pairs (where one response is preferred over another), DPO enables models to learn human-like responses efficiently. Here are the given for DPO:

- **Simpler and Data-Driven Approach**: DPO eliminates the need for a separate reward model, making it simpler and more data-driven compared to Reinforcement Learning from Human Feedback (RLHF).
- **Direct Parameter Updates**: Instead of reward-based fine-tuning, DPO updates model parameters to increase the probability of preferred responses and decrease the probability of rejected responses.
- **Training Stability**: DPO’s approach makes the training process more stable by avoiding the complexities of RL algorithms like PPO, which involve constrained policy updates and KL penalties.
- **Improved Response Alignment**: DPO fine-tunes LLMs to better align responses with human expectations.
- **Elimination of Reward Models**: By removing explicit reward models, DPO prevents instability often associated with RL-based fine-tuning.
- **Reduced Risk of Harmful Outputs**: DPO reduces the risk of harmful, misleading, or biased outputs, making LLMs safer and more reliable.
- **Streamlined Optimization**: DPO offers a practical alternative to RL-based fine-tuning, especially when large-scale human preference data is available.

### The DPO Training Dataset

For DPO, we use human preference data, where each prompt has a preferred response and a rejected response.

Example Preference Dataset (Used for Fine-Tuning)

```ini
preference_data = [
    {"prompt": "What is the capital of France?",
     "preferred": "The capital of France is Paris.",
     "rejected": "France is a country in Europe."},

    {"prompt": "Who wrote Hamlet?",
     "preferred": "Hamlet was written by William Shakespeare.",
     "rejected": "Hamlet is an old book."},

    {"prompt": "Tell me a joke.",
     "preferred": "Why did the scarecrow win an award? Because he was outstanding in his field!",
     "rejected": "I don’t know any jokes."},

    {"prompt": "What is artificial intelligence?",
     "preferred": "Artificial intelligence is the simulation of human intelligence in machines.",
     "rejected": "AI is just robots."},

    {"prompt": "How to stay motivated?",
     "preferred": "Set clear goals, track progress, and reward yourself for achievements.",
     "rejected": "Just be motivated."},
]
```

The preferred responses are accurate, informative, and well-structured, while the rejected responses are vague, incorrect, or unhelpful.

### The DPO Loss Function

DPO is formulated as a pairwise ranking problem between a preferred response and a rejected response for the same prompt. The goal is to increase the log probability of preferred responses while decreasing the probability of rejected ones.

Mathematically, the DPO objective is:

![The DPO Loss Function](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_Eq1.webp)

Where:

- **y^+** is the preferred response
- **_y^-_** is the rejected response
- _**β**_ is a scaling hyperparameter controlling preference strength
- **_P_θ(y∣x)_** is the log probability of generating a response given input **_x_**

This is similar to logistic regression, where the model maximizes separation between preferred and rejected responses.

## Code Example: Direct Preference Optimization (DPO)

DPO fine-tunes LLMs by training on human-labeled preference pairs. The core logic of DPO training involves optimizing model weights based on preferred vs. rejected responses. The function below trains a transformer-based model to increase the likelihood of preferred responses while decreasing the likelihood of rejected ones. Below is the key function for computing the DPO loss and updating the model (only the main function is shown for scope; full notebook is [linked](https://colab.research.google.com/drive/1MPEYbpJRjocNOCTPouIKOXlPWPVXv753?usp=sharing)).

```python
def dpo_loss(preferred_log_probs, rejected_log_probs, beta=0.1):
    """Computes the DPO loss function to optimize based on preferences"""
    return -torch.mean(torch.sigmoid(beta * (preferred_log_probs - 
    rejected_log_probs)))

def encode_text(prompt, response):
    """Encodes the prompt + response into tokenized format with proper padding"""
    tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue
    input_text = f"User: {prompt}\nAssistant: {response}"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,         # Enable padding
        truncation=True,      # Truncate if too long
        max_length=512        # Set max length for safety
    )

    return inputs["input_ids"], inputs["attention_mask"]

loss_history = []  # Store loss values

optimizer = optim.AdamW(model.parameters(), lr=5e-5)

for epoch in range(10):  # Train for 10 epochs
    total_loss = 0

    for data in preference_data:
        prompt, preferred, rejected = data["prompt"], data["preferred"], 
        data["rejected"]

        # Encode preferred and rejected responses
        pref_input_ids, pref_attention_mask = encode_text(prompt, preferred)
        rej_input_ids, rej_attention_mask = encode_text(prompt, rejected)

        # Get log probabilities from the model
        preferred_logits = model(pref_input_ids, attention_mask=
        pref_attention_mask).logits[:, -1, :]
        rejected_logits = model(rej_input_ids, attention_mask=rej_attention_mask)
        .logits[:, -1, :]

        preferred_log_probs = preferred_logits.log_softmax(dim=-1)
        rejected_log_probs = rejected_logits.log_softmax(dim=-1)

        # Compute DPO loss
        loss = dpo_loss(preferred_log_probs, rejected_log_probs, beta=0.5)

        # Optimize the model
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    loss_history.append(total_loss)  # Store loss for visualization
    print(f"Epoch {epoch + 1}, Loss: {total_loss:.4f}")
```

🔗 [Full implementation available here](https://colab.research.google.com/drive/1MPEYbpJRjocNOCTPouIKOXlPWPVXv753?usp=sharing) 

### Expected Output & Analysis

The outcomes of Direct Preference Optimization (DPO) can be analyzed from multiple angles: loss convergence, probability shifts, and qualitative response improvements. The training loss curve shows a sharp drop in the initial epochs, followed by stabilization, indicating that the model quickly learns to align with human preferences. The plateau in loss suggests that further optimization yields diminishing improvements, confirming effective preference-based fine-tuning.

![Direct Preference Optimization (DPO)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP1.webp)

![Direct Preference Optimization (DPO)](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP3.webp)

The probability shift visualization reveals that preferred responses consistently achieve higher log probabilities than rejected ones. This confirms that DPO successfully adjusts the model’s behaviour, reinforcing the correct responses while suppressing undesired ones. Some variance in probability shifts suggests that certain prompts may still require fine-tuning for optimal alignment.

![DPO probability shift visualization](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP2.webp)

A direct comparison of model responses before and after DPO fine-tuning highlights clear improvements. Initially, the model fails to generate a joke, instead providing an irrelevant response. After fine-tuning, it attempts humor but still lacks coherence. This demonstrates that while DPO enhances preference alignment, additional refinements or complementary techniques may be required to generate high-quality, structured responses.

![DPO](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP4.webp)

Although DPO effectively tunes LLMs without an explicit reward function, it lacks the structured policy learning of reinforcement learning-based methods. This is where General Reinforcement Pretraining Optimization (GRPO) by DeepSeek comes in, combining the strengths of DPO and PPO to enhance LLM fine-tuning further. The next section will explore how GRPO refines policy optimization for large-scale models.

## GRPO – Group Relative Policy Optimization (DeepSeek’s Approach)

DeepSeek’s Group Relative Policy Optimization (GRPO) is an advanced preference optimization technique that extends Direct Preference Optimization (DPO) while incorporating elements from Proximal Policy Optimization (PPO). Unlike traditional policy optimization methods that operate on single preference pairs, GRPO leverages group-wise preference ranking, enabling better alignment with human feedback in large-scale LLM fine-tuning.

Traditional preference-based optimization methods, such as DPO (Direct Preference Optimization), operate on pairwise comparisons—one preferred and one rejected response. However, this approach fails to scale efficiently when optimizing on large datasets where multiple responses per prompt are ranked in order of preference. To address this limitation, DeepSeek introduced Group Relative Policy Optimization (GRPO), which allows group-based preference ranking rather than just single-pair preference updates. Instead of comparing two responses at a time, GRPO compares all ranked responses within a batch and optimizes the policy accordingly.

Mathematically, GRPO extends DPO’s reward-free optimization by defining an ordered preference ranking among multiple completions and optimizing their relative likelihoods accordingly.

## Mathematical Foundation of GRPO

Since this is the main intent behind the blog, we will dive deep into the mathematics of this.

### 1. Expected Return in Preference Optimization

In standard reinforcement learning, the expected return of a policy **_π_θ_** is:

![Expected Return in Preference Optimization](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq1.webp)

where **_R(s_t,a_t)_** is the reward at timestep **_t_**.

However, LLM fine-tuning does not operate in traditional reward-based RL. Instead, we optimize over human preferences, meaning that reward models are unnecessary.

Instead of learning a reward function, GRPO directly optimizes the model parameters to increase the likelihood of higher-ranked responses over lower-ranked ones.

### 2. Ranking-Based Probability Optimization

Given a set of responses **_r_1,r_2, …, r_n_** ranked in order of preference, we define a likelihood ratio:

![Ranking-Based Probability Optimization](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq2.webp)

where **_x_** is the input prompt, and **_π_θ_** represents the policy (LLM) parameterized by _**θ**_. The key objective is to maximize the probability of higher-ranked responses while suppressing the probability of lower-ranked ones.

To enforce relative preference constraints, GRPO optimizes the following pairwise ranking loss across all response pairs:

![Ranking-Based Probability Optimization](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq3.webp)

where:

- **_σ(x)_** is the sigmoid function ensuring probability normalization
- **_β_** is a temperature scaling parameter controlling gradient magnitude.
- **_π__****_θ​_** is the policy (LLM).
- The sum iterates over all pairs (i, j) where **_r_i_**, is ranked higher than **_r_j_**.

The KL-regularized version of GRPO adds a penalty term to prevent drastic shifts in model behaviour:

![KL-regularized version of GRPO](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq4.webp)

where **_D_KL_**​ ensures conservative updates to prevent overfitting.

## Data for GRPO Fine-Tuning

Below is an example dataset used to fine-tune an LLM using ranked preferences:

```ini
grpo_preference_data = [
    {"prompt": "What is the capital of France?",
     "responses": [
         {"text": "The capital of France is Paris.", "rank": 1},
         {"text": "Paris is the largest city in France.", "rank": 2},
         {"text": "Paris is in France.", "rank": 3},
         {"text": "France is a country in Europe.", "rank": 4}
     ]},

    {"prompt": "Tell me a joke.",
     "responses": [
         {"text": "Why did the scarecrow win an award? Because he was outstanding 
         in his field!", "rank": 1},
         {"text": "Why did the chicken cross the road? To get to the other side.",
          "rank": 2},
         {"text": "Jokes are funny.", "rank": 3},
         {"text": "I don’t know any jokes.", "rank": 4}
     ]}
]
```

Each prompt has multiple responses with assigned ranks. The model learns to increase the probability of higher-ranked responses while reducing the probability of lower-ranked ones.

## Code Implementation: Group-Based Preference Optimization

Below is the key function for computing the [DPO](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/) loss and updating the model (only the main function is shown for scope; the full notebook is [linked](https://colab.research.google.com/drive/1zamZF7qm4qx-tjysJPn1STZbtJFcQaqe?usp=sharing)). The GRPO training function processes multiple ranked responses per prompt, optimizing log-likelihood differences while enforcing KL constraints.

```python
def deepseek_grpo_loss(log_probs, rankings, input_ids, beta=1.0, kl_penalty=0.02, epsilon=1e-6):
    """Computes DeepSeek GRPO loss with pairwise ranking and KL regularization."""
    loss_terms = []
    num_pairs = 0

    log_probs = torch.clamp(log_probs, min=-10, max=10)  # Prevent extreme values

    for i in range(len(rankings)):
        for j in range(i + 1, len(rankings)):
            if rankings[i] < rankings[j]:  # Higher-ranked response should be preferred
                prob_diff = log_probs[i] - log_probs[j]
                pairwise_loss = -torch.log(torch.sigmoid(beta * prob_diff) + epsilon)  # Avoid log(0)
                loss_terms.append(pairwise_loss)
                num_pairs += 1

    loss = torch.stack(loss_terms).mean() if num_pairs > 0 else torch.tensor(0.0, device=log_probs.device)

    # KL regularization to prevent policy divergence
    old_logits = base_model(input_ids).logits[:, -1, :]
    old_log_probs = old_logits.log_softmax(dim=-1)

    kl_div = torch.nn.functional.kl_div(log_probs, old_log_probs.clamp(min=epsilon), reduction="batchmean")

    return loss + (kl_penalty * kl_div.mean())  # Ensure single scalar
```

## Training Loop for GRPO

The training loop processes ranked responses, computes loss, and updates the model while enforcing stability constraints.

```python
loss_history = []
num_epochs = 15

for epoch in range(num_epochs):
    total_loss = 0

    for data in grpo_preference_data:
        prompt, responses = data["prompt"], data["responses"]

        input_ids, rankings = encode_text(prompt, responses)

        logits = model(input_ids).logits[:, -1, :]
        log_probs = logits.log_softmax(dim=-1)

        loss = deepseek_grpo_loss(log_probs, rankings, input_ids)

        if torch.isnan(loss):
            print(f"Skipping update at epoch {epoch} due to NaN loss.")
            continue

        optimizer.zero_grad()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        total_loss += loss.item()

    loss_history.append(total_loss)
    scheduler.step()
    print(f"Epoch {epoch + 1}, Loss: {total_loss:.4f}")
```

🔗 [Full implementation available here](https://colab.research.google.com/drive/1zamZF7qm4qx-tjysJPn1STZbtJFcQaqe?usp=sharing) 

## Expected Outcome and Results

The expected outcomes of GRPO fine-tuning on the LLM, based on the provided outputs, highlight improvements in model optimization and preference-based ranking.

The training loss curve shows a gradual and stable decline over 15 epochs, indicating that the model is learning effectively. Unlike conventional policy optimization methods, GRPO ensures that ranked responses improve without drastic fluctuations, suggesting smooth convergence.

![DeepSeek GRPO Training Loss Curve](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_OP1.webp)

![DeepSeek GRPO Training Loss Curve](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_OP4.webp)

The loss value distribution over epochs presents a histogram where most values concentrate around a decreasing trend, showing that GRPO efficiently optimizes the model while maintaining stable loss updates. This distribution further indicates that loss values do not exhibit large variations, preventing instability in preference ranking.

![distribution of Loss Values over Epochs](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_OP2.webp)

The log probability distribution before vs. after fine-tuning provides crucial insights into the model’s response generation. The shift in probability distribution suggests that after fine-tuning, the model assigns higher confidence to preferred responses. This shift results in responses that align better with human expectations and rankings.

![Log Probability Distribution before vs. after fine-tuning](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_OP3.webp)

Overall, the expected outcome of GRPO fine-tuning is a well-optimized model capable of generating high-quality responses ranked effectively based on preference learning. This demonstrates why GRPO is an effective alternative to traditional RL methods like PPO or DPO, offering a structured approach to optimizing LLMs without explicit reward models.

## Final Model Insights: Why GRPO Excels in LLM Fine-Tuning

Unlike pairwise DPO and trust-region PPO, GRPO allows LLMs to learn from multiple ranked completions per prompt, significantly improving response quality, stability, and human alignment.

- More scalable than pairwise methods → Learns from multiple ranked completions rather than just binary comparisons.
- No explicit reward modeling → Unlike RLHF, GRPO fine-tunes without requiring a trained reward model.
- KL regularization stabilizes updates → Prevents catastrophic shifts in response distribution.
- Better generalization across prompts → Ensures the LLM produces high-quality, human-aligned responses.

With reinforcement learning playing an increasingly central role in fine-tuning LLMs, GRPO stands out as the next step in AI preference learning, setting a new standard for human-aligned language modeling.

## Conclusion

Policy optimization techniques play a critical role in reinforcement learning and LLM fine-tuning. Each method—Policy Gradient (PG), Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO)—offers unique advantages and trade-offs. PG serves as the foundation but suffers from high variance, while TRPO provides stability at the cost of computational complexity. PPO, being a refined version of TRPO, balances efficiency and robustness, making it widely used in RL applications. DPO, on the other hand, optimizes LLMs directly using preference data, eliminating the need for a reward model. Finally, GRPO, as introduced by DeepSeek, enhances preference-based fine-tuning by leveraging relative ranking in a structured manner.

Below is a comparison of these LLM Optimization methods based on key aspects such as variance, stability, sample efficiency, and suitability for reinforcement learning versus LLM fine-tuning:

|Method|Variance|Stability|Sample Efficiency|Best for|Limitations|
|---|---|---|---|---|---|
|PG (REINFORCE)|High|Low|Inefficient|Simple RL problems|High variance, slow convergence|
|TRPO|Low|High|Moderate|High-stability RL tasks|Complex second-order updates, expensive|
|PPO|Medium|High|Efficient|General RL tasks, Robotics, Games|May require careful hyperparameter tuning|
|DPO|Low|High|High|LLM fine-tuning with human preferences|Lacks explicit reinforcement learning framework|
|GRPO|Low|High|High|Preference-based LLM fine-tuning|Newer method, requires further empirical validation|

For practitioners, the choice depends on the task at hand. If optimizing reinforcement learning agents in games or robotics, PPO is the best choice due to its balance of efficiency and performance. If high-stability optimization is required, TRPO is preferred despite its computational cost. DPO and GRPO, however, are better suited for LLM fine-tuning, with GRPO providing an even stronger optimization framework based on relative preference ranking rather than just binary preference signals.

### Key Takeaways

Reinforcement learning (RL) plays a crucial role in both game-playing agents and LLM fine-tuning, but the optimization techniques vary significantly.

- PG, TRPO, and PPO are fundamental in RL, with PPO being the most practical choice for its efficiency and performance balance.
- DPO introduced a major shift in LLM fine-tuning by eliminating explicit reward models, making human preference alignment easier and more efficient.
- GRPO, pioneered by DeepSeek, further refines LLM fine-tuning by optimizing for relative ranking rather than just binary comparisons, improving preference-based alignment.
- For RL tasks, PPO remains the dominant method, while for LLM fine-tuning, DPO and GRPO are superior choices due to their ability to fine-tune models using direct preference data without RL instability.

This blog highlights how reinforcement learning and preference-based fine-tuning are converging, with new techniques like GRPO bridging the gap between structured optimization and real-world deployment of large-scale AI systems.

[![Neil D](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_sT5MuqV.webp)](https://www.analyticsvidhya.com/blog/author/akashdas/)

[Neil D](https://www.analyticsvidhya.com/blog/author/akashdas/)

Advancing language model research by day and writing about my work online by night. I explore AI breakthroughs and transform complex studies into clear, engaging insights that empower professionals and enthusiasts alike.

Thanks for stopping by my profile!

[Advanced](https://www.analyticsvidhya.com/blog/category/advanced/)[Best of Tech](https://www.analyticsvidhya.com/blog/category/best-of-tech/)[Guide](https://www.analyticsvidhya.com/blog/category/guide/)[LLMs](https://www.analyticsvidhya.com/blog/category/llms/)[Reinforcement Learning](https://www.analyticsvidhya.com/blog/category/reinforcement-learning-2/)

## Free Courses

 [![Generative AI](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Generative-AI---A-Way-of-Life---Free-Course.webp) 4.7

#### Generative AI - A Way of Life

Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.](https://courses.analyticsvidhya.com/courses/genai-a-way-of-life/?ref=blog_freecourse)

 [![Large Language Models](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Getting-Started-with-Large-Language-Models.webp) 4.5

#### Getting Started with Large Language Models

Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple.](https://courses.analyticsvidhya.com/courses/getting-started-with-llms?ref=blog_freecourse)

 [![Prompt Engineering](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Building-LLM-Applications-using-Prompt-Engineering---Free-Course.webp) 4.6

#### Building LLM Applications using Prompt Engineering

This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.](https://courses.analyticsvidhya.com/courses/building-llm-applications-using-prompt-engineering-free?ref=blog_freecourse)

 [![LlamaIndex](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Real-World-RAG-Systems.webp) 4.8

#### Improving Real World RAG Systems: Key Challenges & Practical Solutions

Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.](https://courses.analyticsvidhya.com/courses/improving-real-world-rag-systems-key-challenges/?ref=blog_freecourse)

 [![RAG systems using LlamaIndex](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/excel.webp) 4.7

#### Microsoft Excel: Formulas & Functions

Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course.](https://courses.analyticsvidhya.com/courses/microsoft-excel-formulas-functions?ref=blog_freecourse)

#### Recommended Articles

- [
    
    Optimizers in Deep Learning: A Detailed Guide
    
    ](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)
- [
    
    GPT-4o vs OpenAI o1: Is the New OpenAI Model Wo...
    
    ](https://www.analyticsvidhya.com/blog/2024/09/gpt-4o-vs-openai-o1/)
- [
    
    What is Direct Preference Optimization (DPO)?
    
    ](https://www.analyticsvidhya.com/blog/2024/01/dpo-andrew-ngs-perspective-on-the-next-big-thing-in-ai/)
- [
    
    Fine-tune Llama 3 using Direct Preference Optim...
    
    ](https://www.analyticsvidhya.com/blog/2024/05/fine-tune-llama-3-using-direct-preference-optimization/)
- [
    
    Finetuning Llama 3 with Odds Ratio Preference O...
    
    ](https://www.analyticsvidhya.com/blog/2024/05/finetuning-llama-3-with-odds-ratio-preference-optimization/)
- [
    
    GRPO Fine-Tuning on DeepSeek-7B with Unsloth
    
    ](https://www.analyticsvidhya.com/blog/2025/02/grpo-fine-tuning-on-deepseek-7b/)
- [
    
    Top 9 Fine-tuning Interview Questions and Answers
    
    ](https://www.analyticsvidhya.com/blog/2024/04/fine-tuning-interview-questions-and-answers/)
- [
    
    What is RLHF?
    
    ](https://www.analyticsvidhya.com/blog/2023/05/reinforcement-learning-from-human-feedback/)
- [
    
    Dynamic Programming For Beginners
    
    ](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/)
- [
    
    REINFORCE Algorithm: Taking baby steps in reinf...
    
    ](https://www.analyticsvidhya.com/blog/2020/11/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning/)

### Responses From Readers

Submit reply

[

## Write for us 

Write, captivate, and earn accolades and rewards for your work

](https://datahack.analyticsvidhya.com/blogathon/)[

- Reach a Global Audience
- Get Expert Feedback
- Build Your Brand & Audience

- Cash In on Your Knowledge
- Join a Thriving Community
- Level Up Your Data Science Game

](https://datahack.analyticsvidhya.com/blogathon/)

[![imag](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Write-for-us.webp)](https://datahack.analyticsvidhya.com/blogathon/)

We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our [Privacy Policy](https://www.analyticsvidhya.com/privacy-policy) & [Cookies Policy](https://www.analyticsvidhya.com/cookies-policy).

Show details

Accept all cookies

Use necessary cookies

## Flagship Courses

[GenAI Pinnacle Program](https://www.analyticsvidhya.com/genaipinnacle/?ref=footer)| [GenAI Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=blogflashstripfooter)| [AI/ML BlackBelt Courses](https://www.analyticsvidhya.com/bbplus?ref=footer)| [Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer?ref=footer)

## Free Courses

[Generative AI](https://www.analyticsvidhya.com/courses/genai-a-way-of-life/?ref=footer)| [DeepSeek](https://www.analyticsvidhya.com/courses/getting-started-with-deepSeek/?ref=footer)| [OpenAI Agent SDK](https://www.analyticsvidhya.com/courses/demystifying-openai-agents-sdk/?ref=footer)| [LLM Applications using Prompt Engineering](https://www.analyticsvidhya.com/courses/building-llm-applications-using-prompt-engineering-free/?ref=footer)| [DeepSeek from Scratch](https://www.analyticsvidhya.com/courses/deepseek-from-scratch/?ref=footer)| [Stability.AI](https://www.analyticsvidhya.com/courses/exploring-stability-ai/?ref=footer)| [SSM & MAMBA](https://www.analyticsvidhya.com/courses/building-smarter-llms-with-mamba-and-state-space-model/?ref=footer)| [RAG Systems using LlamaIndex](https://www.analyticsvidhya.com/courses/building-first-rag-systems-using-llamaindex/?ref=footer)| [Getting Started with LLMs](https://www.analyticsvidhya.com/courses/getting-started-with-llms/?ref=footer)| [Python](https://www.analyticsvidhya.com/courses/introduction-to-python/?ref=footer)| [Microsoft Excel](https://www.analyticsvidhya.com/courses/microsoft-excel-formulas-functions/?ref=footer)| [Machine Learning](https://www.analyticsvidhya.com/courses/machine-learning-certification-course-for-beginners/?ref=footer)| [Deep Learning](https://www.analyticsvidhya.com/courses/getting-started-with-deep-learning/?ref=footer)| [Mastering Multimodal RAG](https://www.analyticsvidhya.com/courses/mastering-multimodal-rag-and-embeddings-with-amazon-nova-and-bedrock/?ref=footer)| [Introduction to Transformer Model](https://www.analyticsvidhya.com/courses/introduction-to-transformers-and-attention-mechanisms/?ref=footer)| [Bagging & Boosting](https://www.analyticsvidhya.com/courses/bagging-boosting-ML-Algorithms/?ref=footer)| [Loan Prediction](https://www.analyticsvidhya.com/courses/loan-prediction-practice-problem-using-python/?ref=footer)| [Time Series Forecastingn](https://www.analyticsvidhya.com/courses/time-series-forecasting-using-python/?ref=footer)| [Tableau](https://www.analyticsvidhya.com/courses/tableau-for-beginners/?ref=footer)| [Business Analytics](https://www.analyticsvidhya.com/courses/introduction-to-analytics/?ref=footer)| [Vibe Coding in Windsurf](https://www.analyticsvidhya.com/courses/guide-to-vibe-coding-in-windsurf/?ref=footer)

## Popular Categories

[Generative AI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=footer)| [Prompt Engineering](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=footer)| [Generative AI Application](https://www.analyticsvidhya.com/blog/category/generative-ai-application/?ref=footer)| [News](https://news.google.com/publications/CAAqBwgKMJiWzAswyLHjAw?hl=en-IN&gl=IN&ceid=IN%3Aen)| [Technical Guides](https://www.analyticsvidhya.com/blog/category/guide/?ref=footer)| [AI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=footer)| [Interview Preparation](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=footer)| [Research Papers](https://www.analyticsvidhya.com/blog/category/research-paper/?ref=footer)| [Success Stories](https://www.analyticsvidhya.com/blog/category/success-story/?ref=footer)| [Quiz](https://www.analyticsvidhya.com/blog/category/quiz/?ref=footer)| [Use Cases](https://www.analyticsvidhya.com/blog/category/use-cases/?ref=footer)| [Listicles](https://www.analyticsvidhya.com/blog/category/listicle/?ref=footer)

## Generative AI Tools and Techniques

[GANs](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/?ref=footer)| [VAEs](https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/?ref=footer)| [Transformers](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models?ref=footer)| [StyleGAN](https://www.analyticsvidhya.com/blog/2021/05/stylegan-explained-in-less-than-five-minutes/?ref=footer)| [Pix2Pix](https://www.analyticsvidhya.com/blog/2023/10/pix2pix-unleashed-transforming-images-with-creative-superpower?ref=footer)| [Autoencoders](https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction?ref=footer)| [GPT](https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/?ref=footer)| [BERT](https://www.analyticsvidhya.com/blog/2022/11/comprehensive-guide-to-bert/?ref=footer)| [Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?ref=footer)| [LSTM](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm?ref=footer)| [Attention Mechanisms](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)| [Diffusion Models](https://www.analyticsvidhya.com/blog/2024/09/what-are-diffusion-models/?ref=footer)| [LLMs](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/?ref=footer)| [SLMs](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models?ref=footer)| [StyleGAN](https://www.analyticsvidhya.com/blog/2024/05/what-are-small-language-models-slms/?ref=footer)| [Encoder Decoder Models](https://www.analyticsvidhya.com/blog/2023/10/advanced-encoders-and-decoders-in-generative-ai/?ref=footer)| [Prompt Engineering](https://www.analyticsvidhya.com/blog/2023/06/what-is-prompt-engineering/?ref=footer)| [LangChain](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/?ref=footer)| [LlamaIndex](https://www.analyticsvidhya.com/blog/2023/10/rag-pipeline-with-the-llama-index/?ref=footer)| [RAG](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/?ref=footer)| [Fine-tuning](https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/?ref=footer)| [LangChain AI Agent](https://www.analyticsvidhya.com/blog/2024/07/langchains-agent-framework/?ref=footer)| [Multimodal Models](https://www.analyticsvidhya.com/blog/2023/12/what-are-multimodal-models/?ref=footer)| [RNNs](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/?ref=footer)| [DCGAN](https://www.analyticsvidhya.com/blog/2021/07/deep-convolutional-generative-adversarial-network-dcgan-for-beginners/?ref=footer)| [ProGAN](https://www.analyticsvidhya.com/blog/2021/05/progressive-growing-gan-progan/?ref=footer)| [Text-to-Image Models](https://www.analyticsvidhya.com/blog/2024/02/llm-driven-text-to-image-with-diffusiongpt/?ref=footer)| [DDPM](https://www.analyticsvidhya.com/blog/2024/08/different-components-of-diffusion-models/?ref=footer)| [Document Question Answering](https://www.analyticsvidhya.com/blog/2024/04/a-hands-on-guide-to-creating-a-pdf-based-qa-assistant-with-llama-and-llamaindex/?ref=footer)| [Imagen](https://www.analyticsvidhya.com/blog/2024/09/google-imagen-3/?ref=footer)| [T5 (Text-to-Text Transfer Transformer)](https://www.analyticsvidhya.com/blog/2024/05/text-summarization-using-googles-t5-base/?ref=footer)| [Seq2seq Models](https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/?ref=footer)| [WaveNet](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/?ref=footer)| [Attention Is All You Need (Transformer Architecture)](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)

## Popular GenAI Models

[Llama 3.1](https://www.analyticsvidhya.com/blog/2024/07/meta-llama-3-1/?ref=footer)| [Llama 3](https://www.analyticsvidhya.com/blog/2024/04/meta-llama-3-redefining-large-language-model-standards/?ref=footer)| [Llama 2](https://www.analyticsvidhya.com/blog/2023/08/getting-started-with-llama-2/?ref=footer)| [GPT 4o Mini](https://www.analyticsvidhya.com/blog/2024/07/gpt-4o-mini/?ref=footer)| [GPT 4o](https://www.analyticsvidhya.com/blog/2024/05/openai-flagship-model-gpt-omni/?ref=footer)| [GPT 3](https://www.analyticsvidhya.com/blog/2021/01/gpt-3-the-next-big-thing-foundation-of-future/?ref=footer)| [Claude 3 Haiku](https://www.analyticsvidhya.com/blog/2024/03/the-fastest-ai-model-by-anthropic-claude-haiku/?ref=footer)| [Claude 3.5 Sonnet](https://www.analyticsvidhya.com/blog/2024/06/claude-3-5-sonnet/?ref=footer)| [Phi 3.5](https://www.analyticsvidhya.com/blog/2024/09/phi-3-5-slms/?ref=footer)| [Phi 3](https://www.analyticsvidhya.com/blog/2024/05/microsoft-phi3/?ref=footer)| [Mistral Large 2](https://www.analyticsvidhya.com/blog/2024/07/mistral-large-2-powerful-enough-to-challenge-llama-3-1-405b/?ref=footer)| [Mistral NeMo](https://www.analyticsvidhya.com/blog/2024/08/mistral-nemo/?ref=footer)| [Mistral-7b](https://www.analyticsvidhya.com/blog/2024/01/making-the-most-of-mistral-7b-with-finetuning/?ref=footer)| [Gemini 1.5 Pro](https://www.analyticsvidhya.com/blog/2024/04/gemini-pro-goes-global-with-powerful-new-features/?ref=footer)| [Gemini Flash 1.5](https://www.analyticsvidhya.com/blog/2024/08/building-a-food-vision-webapp-with-the-gemini-flash-1-5-model/?ref=footer)| [Bedrock](https://www.analyticsvidhya.com/blog/2024/02/building-end-to-end-generative-ai-models-with-aws-bedrock/?ref=footer)| [Vertex AI](https://www.analyticsvidhya.com/blog/2024/02/build-deploy-and-manage-ml-models-with-google-vertex-ai/?ref=footer)| [DALL.E](https://www.analyticsvidhya.com/blog/2021/01/openais-future-of-vision-with-dall-e-creating-images-from-text/?ref=footer)| [Midjourney](https://www.analyticsvidhya.com/courses/midjourney_from_inspiration_to_implementation/?ref=footer)| [Stable Diffusion](https://www.analyticsvidhya.com/blog/2023/12/what-is-stable-diffusion/?ref=footer)

## Data Science Tools and Techniques

[Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?ref=footer)| [R](https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/?ref=footer)| [SQL](https://www.analyticsvidhya.com/blog/2022/01/learning-sql-from-basics-to-advance/?ref=footer)| [Jupyter Notebooks](https://www.analyticsvidhya.com/blog/2018/05/starters-guide-jupyter-notebook/?ref=footer)| [TensorFlow](https://www.analyticsvidhya.com/blog/2021/11/tensorflow-for-beginners-with-examples-and-python-implementation/?ref=footer)| [Scikit-learn](https://www.analyticsvidhya.com/blog/2021/08/complete-guide-on-how-to-learn-scikit-learn-for-data-science/?ref=footer)| [PyTorch](https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/?ref=footer)| [Tableau](https://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-tableau-for-beginners-in-data-visualization/?ref=footer)| [Apache Spark](https://www.analyticsvidhya.com/blog/2022/08/introduction-to-on-apache-spark-and-its-datasets/?ref=footer)| [Matplotlib](https://www.analyticsvidhya.com/blog/2021/10/introduction-to-matplotlib-using-python-for-beginners/?ref=footer)| [Seaborn](https://www.analyticsvidhya.com/blog/2021/02/a-beginners-guide-to-seaborn-the-simplest-way-to-learn/?ref=footer)| [Pandas](https://www.analyticsvidhya.com/blog/2021/03/pandas-functions-for-data-analysis-and-manipulation/?ref=footer)| [Hadoop](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-hadoop-ecosystem-for-big-data/?ref=footer)| [Docker](https://www.analyticsvidhya.com/blog/2021/10/end-to-end-guide-to-docker-for-aspiring-data-engineers/?ref=footer)| [Git](https://www.analyticsvidhya.com/blog/2021/09/git-and-github-tutorial-for-beginners/?ref=footer)| [Keras](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/?ref=footer)| [Apache Kafka](https://www.analyticsvidhya.com/blog/2022/12/introduction-to-apache-kafka-fundamentals-and-working/?ref=footer)| [AWS](https://www.analyticsvidhya.com/blog/2020/09/what-is-aws-amazon-web-services-data-science/?ref=footer)| [NLP](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/?ref=footer)| [Random Forest](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/?ref=footer)| [Computer Vision](https://www.analyticsvidhya.com/blog/2020/01/computer-vision-learning-path/?ref=footer)| [Data Visualization](https://www.analyticsvidhya.com/blog/2021/04/a-complete-beginners-guide-to-data-visualization/?ref=footer)| [Data Exploration](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/?ref=footer)| [Big Data](https://www.analyticsvidhya.com/blog/2021/05/what-is-big-data-introduction-uses-and-applications/?ref=footer)| [Common Machine Learning Algorithms](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?ref=footer)| [Machine Learning](https://www.analyticsvidhya.com/blog/category/Machine-Learning/?ref=footer)

## Company

- [About Us](https://www.analyticsvidhya.com/about/?ref=global_footer)
- [Contact Us](https://www.analyticsvidhya.com/contact/?ref=global_footer)
- [Careers](https://www.analyticsvidhya.com/careers/?ref=global_footer)

## Discover

- [Blogs](https://www.analyticsvidhya.com/blog/?ref=global_footer)
- [Expert Sessions](https://www.analyticsvidhya.com/events/datahour/?ref=global_footer)
- [Learning Paths](https://www.analyticsvidhya.com/blog/category/learning-path/?ref=global_footer)
- [Comprehensive Guides](https://www.analyticsvidhya.com/category/guide/?ref=global_footer)

## Learn

- [Free Courses](https://www.analyticsvidhya.com/courses?ref=global_footer)
- [AI&ML Program](https://www.analyticsvidhya.com/bbplus?ref=global_footer)
- [Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=global_footer)
- [Agentic AI Program](https://www.analyticsvidhya.com/agenticaipioneer/?ref=global_footer)

## Engage

- [Community](https://community.analyticsvidhya.com/?ref=global_footer)
- [Hackathons](https://www.analyticsvidhya.com/datahack/?ref=global_footer)
- [Events](https://www.analyticsvidhya.com/events/?ref=global_footer)
- [Podcasts](https://www.analyticsvidhya.com/events/leading-with-data/?ref=global_footer)

## Contribute

- [Become an Author](https://www.analyticsvidhya.com/datahack/blogathon/?ref=global_footer)
- [Become a Speaker](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)
- [Become a Mentor](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)
- [Become an Instructor](https://docs.google.com/forms/d/e/1FAIpQLSdTDIsIUzmliuTkXIlTX6qI65RCiksQ3nCbTJ7twNx2rgEsXw/viewform?ref=global_footer)

## Enterprise

- [Our Offerings](https://enterprise.analyticsvidhya.com/?ref=global_footer)
- [Trainings](https://www.analyticsvidhya.com/enterprise/training?ref=global_footer)
- [Data Culture](https://www.analyticsvidhya.com/enterprise/data-culture?ref=global_footer)
- [AI Newsletter](https://newsletter.ai/?ref=global_footer)

[](https://www.instagram.com/analytics_vidhya/?utm_source=newhomepage)[](https://twitter.com/analyticsvidhya)[](https://www.linkedin.com/company/analytics-vidhya/)[](https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA)

[Terms & conditions](https://www.analyticsvidhya.com/terms/)  [Refund Policy](https://www.analyticsvidhya.com/refund-policy/)  [Privacy Policy](https://www.analyticsvidhya.com/privacy-policy/)  [Cookies Policy](https://www.analyticsvidhya.com/cookies-policy) © Analytics Vidhya 2025.All rights reserved.