
但我们也会发现，计算这种分块运算的最大问题在于softmax函数。因为softmax需要访问s矩阵的所有行来实施运算——它必须获取一个归一化因子，这个因子就是应用于整行的所有数值的指数之和。稍后我们将探讨如何解决这个问题。

那么我们继续。好的，各位——呃，当我用"guys"这个词时，其实也包括女生们，因为平时我习惯性会说"guys"...但女生们千万别觉得被排除在外啊。我们刚才看到闪存注意力机制只专注于优化这个运算：查询向量经过转置的softmax再乘以3，除以dk平方根与v的乘积。这里需要先引入一些符号说明。

为了避免在接下来的幻灯片中迷失方向，首先需要明确的是：这些公式是我从Flash Attention论文中摘取的，但此刻我们暂且假设Flash Attention从未存在过。我们要一步步解决这个问题。现在，我们需要将q视为已经通过wq处理的输入序列输出，k视为已经通过wk处理的结果，v则是已经通过wv处理的结果。

因为我们不想优化矩阵乘法，因为它已经足够快了。另外，让我们谈谈这些矩阵的维度，这样我们就能理解这些操作的输出维度了。我们将把q视为一个由n个标记组成的序列。每个标记有d个维度。所以是小写的d维度。为什么呢？因为我们通常会把查询拆分到多个头上。我们假设已经完成了这个拆分过程。也就是说，我们假装已经处理了输入序列，已经通过wq运行，并且已经将其分割到多个头上。每个头都会执行以下操作。

所以我们之前已经看到，通常的公式是查询乘以键的转置。每个头都会在这些维度上处理查询、键和值序列。现在，我们来看一下输出的维度。我们要做的第一个操作是查询乘以键的转置，其中键的转置矩阵原本是n×d，但转置后会变成d×n。

所以，当d除以n时，结果将是一个n×n的矩阵，因为在矩阵乘法中，外维度会成为输出矩阵的维度。接下来我们要进行的操作是什么呢？我们取这个操作的输出，也就是查询乘以键的转置，然后通过softmax运算进行处理。我们将看到softmax运算是什么，它会保持输入的形状不变。因此它不会改变输入矩阵的形状，只是改变其数值。然后我们取这个softmax的输出，并将其与v相乘，这将改变——当然这会改变形状——因为p矩阵是n×n的。所以这个矩阵是n×n，而v是n×d的。因此这个矩阵乘法的输出将是n×d，即外部维度。现在，让我们详细看看这些操作的每一个细节。

因此，当我们用查询乘以键的转置时，我们会得到一个n×n的矩阵，其中矩阵中的每个值都是q的一行和k的一列的点积。具体来说，这个矩阵的第一个元素将是第一个查询向量与第一个键向量的点积，第二个元素将是第一个查询向量与第二个键向量的点积，第三个元素将是第一个查询向量与第三个键向量的点积，以此类推。

假设这个矩阵的最后一行将是最后一个查询与第一个键的点积，然后最后一个查询与第二个键的点积，最后一个查询与第三个键的点积，依此类推，直到最后一个查询与最后一个键的点积。你可能还注意到这里我写了查询转置键，因为当我们问q1是什么时，首先q1是查询矩阵的第一行。

关于矩阵乘法的一些背景知识，我们知道在进行矩阵乘法时，每个输出元素都是第一个矩阵的一行与第二个矩阵的一列的乘积。但我们现在是将第一个矩阵与第二个矩阵的转置相乘。因此，这将是查询矩阵的一行与键矩阵的一行的点积，因为我们是用转置后的键矩阵k进行乘法运算。嗯。

当你从矩阵中取一个向量时，通常的表示方法是这样的。在数学中的线性代数里，我们总是假设向量是一个列向量。所以我们不能直接写q乘以k，因为那意味着我们在做点积。呃，我们实际上是在做一个列矩阵与另一个列矩阵的矩阵乘法，这是不可能的，因为它们的形状不匹配。

因此，作为一种表示方法，我们这样写：我们对第一个矩阵的转置（即一个列向量）进行点积运算，但转置后它就变成了行向量，然后与第二个向量相乘。这只是因为表示方法的需要，大家明白吗？嗯，你只需要假装这是第一个查询与第一个键的点积，然后是第一个查询与第二个键的点积，接着是第一个查询与第三个键的点积，以此类推。

因此我们正在计算向量的点积。然后我们应用这个softmax操作。softmax操作的作用是将这些点积（都是标量）进行转换——因为点积的输出是一个标量——并以某种方式转换这些数字，使它们变成一种按行排列的概率分布。这意味着每个数字都在0到1之间，并且当我们把这些数字相加时...

它们的总和为一。这个条件，这个性质对每一行都成立。所以这一行的总和也将为一，这一行的总和为一，这一行的总和为一，等等，等等。让我们看看，现在什么是softmax操作。给定一个向量，我们称之为x，它由n个维度组成。softmax的定义如下。

因此，softmax 本质上将这个向量转换为另一个具有相同维度的向量，其中输出向量的每个元素按如下方式计算：输出向量的高度元素是输入元素的指数，除以向量所有维度指数的总和。基本上，这被称为归一化因子，目的是让所有这些数字介于零和一之间，我们通常会进行归一化处理。这就是为什么它被称为归一化因子。而我们使用softmax是因为我们希望这些数字都是正数。我们不希望这个操作的输出结果为负数。这就是为什么我们使用指数函数。但这里有一个问题。

---------

问题在于，想象一下我们的输入向量由许多可能很大的数字组成。例如，假设x1等于100，x2等于200，x3等于300，这种情况是有可能发生的。如果我们对这些数字进行指数运算，比如100的指数运算结果将是一个巨大的数字，几乎接近无穷大，至少相对于计算机能存储的范围来说是这样。

因此，100的指数输出可能无法放入32位浮点数或16位浮点数中，甚至无法放入32位整数中。所以我们无法计算它，因为它会溢出我们的变量——存储这个值、这个输出的整数。因此，在这种情况下我们讨论的是数值不稳定性。所以每当你在计算机科学中听到“数值不稳定性”这个术语时……

这意味着该数字无法用我们现有的位数（通常是32位或16位）在固定表示法中表示。我们也有64位，但使用起来成本太高。因此，让我们尝试找到一个解决方案，使这里的计算在数值上稳定。为了使这个softmax操作在数值上稳定，这意味着我们希望这些数字不会爆炸或变得太小以至于无法表示。

我们需要找到一个解决方案，幸运的是，这其实很简单。正如我们之前所见，softmax的公式如下：每个数字先取指数，然后除以归一化因子，这个因子就是输入向量每个维度的指数之和。如果我们用一个常数（即一个数字）同时乘以分数的分子和分母，分数的值不会改变。

所以这就是我们要做的。我们正在将分子和分母乘以这个因子c，当然前提是c不等于零。然后我们可以利用乘积对加法的分配性质，将这个c带入求和符号内部，正如你在这里所看到的。接着，我们还可以将每个数字写成其自身对数的指数形式。因为指数和对数会相互抵消。

然后我们可以利用指数的性质。我们知道两个指数的乘积等于其各自指数参数之和的指数。我们对分子和分母都进行这样的操作。接着，我们将这个量减去log c等于k，或者说k等于负的log c，因此我们可以用k来替换这个量。之所以可以这样做，是因为这是我们选择的一个常数，现在只是将它赋给另一个常数。

通过这个推导过程，我们可以发现可以在指数函数中巧妙地引入一个值。如果精心选择这个值，就能减小指数函数的参数值。我们会将这个k值设定为输入向量的最大元素（即应用softmax函数的向量），这样每个参数在xi等于向量当前处理的最大元素时为零，否则为负值。我们知道当指数函数的参数为零时，其输出值为1；当参数为负数时（即处于负值区间），输出值会小于1。

其值将介于0和1之间，因此可以轻松用32位浮点数表示。如此一来，这个指数运算就不会再出现数值爆炸的问题。本质上，为了以数值安全的方式对向量应用softmax函数，我们需要找到一个常数k——即该向量的最大值。在应用时，我们需要从每个元素中减去这个选定的常数。下面我们来看计算softmax的具体算法。

首先，给定一个向量或一个n×n矩阵，因为我们想对这个n×n矩阵应用softmax函数。我们需要遍历这个矩阵的每一行，对于每一行，我们需要找到元素中的最大值，这需要的时间复杂度与向量的长度（即我们应用softmax的行的大小）成线性关系。然后，我们需要计算归一化因子，也就是这里的这个部分。而我们无法在第一步之前计算它，因为我们需要有最大元素来计算这里的求和。在我们计算出归一化因子之后，我们就可以将每个元素的指数除以归一化因子。我们无法在计算归一化因子之前进行第三步，因为我们需要将每个数字除以归一化因子。

因此，如果你喜欢算法的伪代码，那么这就是一个我们刚刚看到的计算softmax的算法。首先，我们找到应用softmax的行中的最大值。然后，我们计算归一化因子，接着对每个元素应用softmax。这意味着我们计算每个元素减去向量最大值后的指数，再除以归一化因子。现在，这个伪代码算法相当慢，因为来看一个实际的例子。想象我们这里有这个向量。

首先，我们需要执行第一步：找到这个向量中的最大值，也就是数字5，这一步需要线性时间的计算。然后，我们需要计算归一化常数，即每个元素减去最大值后的指数之和。也就是e的3减5次方加上e的2减5次方，以此类推。

我们将其称为l，然后我们需要再次遍历这个向量，并对每个元素取指数，减去最大值后除以归一化因子。因此，要对一个n×n的矩阵应用softmax函数，我们需要三次遍历该矩阵的每个元素，并且这些操作必须按顺序执行。因此，我们必须在完成操作一之后才能开始操作二，并且在完成操作一和操作二之后才能开始操作三。这样效率相当低，仅仅是为了应用一个甚至不改变矩阵形状的操作。这只是对数值进行归一化处理，所以肯定有更好的方法，不需要涉及三个顺序操作，也不需要遍历这个矩阵三次。让我们看看。好的，伙计们，我们来演练一下。

我们试图解决的问题是什么？问题陈述如下：能否找到一种更好的方法来计算softmax，而不需要遍历向量三次？让我们来看看目前找到的计算softmax的算法伪代码。假设我们有一个由四个元素组成的向量，首先需要做的是计算这个向量中的最大元素，这意味着要遍历这里的for循环。

这使我们能够计算这个向量中的最大元素 也就是说，我们从向量的左侧开始，逐步向右移动 所以我们从第一个元素开始，一直到最后一个元素，并将之前找到的最大值与当前元素进行比较，以找到全局最大值，基本上，这意味着我知道这非常简单，我几乎可以肯定你不需要这个例子，但通过这个例子将帮助我们理解接下来要做的事情

所以请耐心听我说，即使我做的超级简单。好的，我们一开始m0等于负无穷。m1基本上是第一次迭代时的for循环，这意味着m1将等于之前对m的估计值（即负无穷）与当前元素（即3）的最大值。因此，它将等于3，然后m2将等于之前计算的最大值，即m1（也就是3）与当前元素2的最大值，所以m2将等于3。m3将等于之前计算的最大值（即3）与当前元素5的最大值。

所以这将等于五，m4将等于之前计算的最大值和当前元素中的较大者，因此它将等于五。这样我们就可以计算出最大元素。在第四次迭代时，我们将得到全局最大值，无论输入数组是什么。删除，好的，在我们计算出已知为五的最大值后，我们可以计算归一化因子。

那么我们从 l0 开始，l0 等于零，l1 将等于 L0 的指数，实际上，抱歉，应该是 l0 加上当前元素的指数。所以三减去我们在前四个五的循环中找到的最大元素。然后 l2 将等于 l1 加上当前元素的指数，也就是二减去最大值。接着 l3 将等于 l2 加上当前元素的指数，五减去五。然后 l4 将等于 l3 加上一减去五的指数。如果你展开这个 l，基本上就是...

e的3次方减5，加上e的2次方减5，加上e的5次方减5，再加上e的1次方减1减5。计算完这个归一化因子后，我们可以用它来对输入向量中的每个元素进行归一化处理。也就是说，新的x1（记作x1'）将等于e的（第一个元素3减5）次方除以之前for循环中计算得到的l值。

所以在第四次迭代时，新的x2（即x2'）将等于e的(2减5)次方除以l4，而x3'将等于e的(5减5)次方除以l4，以此类推所有元素。我知道这超级简单，但这对我们后续会有帮助。因此在这个for循环中，我们需要遍历向量三次——因为首先需要计算这个for循环，然后还需要计算另一个for循环。

然后我们需要计算另一个for循环，我们不能不按这个顺序执行，因为为了这个for循环，我们需要有最大元素，因为我们在这里需要它。而且我们不能计算这个for循环，直到我们计算完前一个，因为我们需要有归一化因子。然而我们很固执，让我们尝试将这两个操作融合到一个for循环中，这意味着我们遍历数组。

同时计算mi，在同一迭代中，我们也尝试计算lj。当然，我们无法计算lj，因为我们还没有全局最大值，因为我们还没有遍历整个旧数组。不过，让我们尝试使用目前为止我们拥有的局部最大值估计值。因此，让我们尝试使用mi，而不是mn，即我们目前为止计算出的局部最大值。

因此，如果我们以这种融合的方式对这个向量应用softmax，我们将得到以下迭代步骤。这是我们的数组或向量，第一步是mi。m1将等于之前的最大值（即负无穷大）与当前元素的比较结果，所以最大值在负无穷大和当前元素3之间取3。而l1将等于之前的l（即从0开始的l0）加上e的（当前元素减去全局最大值的幂），但此时我们还没有全局最大值。

因此，让我们使用目前为止的最大值，所以现在我们可以使用3。在第二次迭代时，我们位于向量的这个元素，并计算目前为止的最大值。因此，目前为止的最大值是前一个最大值和当前元素中的较大者，即3和2之间的最大值，也就是3。归一化因子是前一个归一化因子加上2减去3的指数，即当前元素减去目前为止的最大值。

现在，如果我们的数组仅由这两个元素组成，即3和2，那么我们计算的结果实际上是正确的，因为我们找到的最大值是3，它确实是全局最大值，而我们计算的归一化因子也是正确的，因为每个指数都是基于全局最大值计算的，因为第一个元素是以3作为参数减去3来计算的。

此外，第二个元素的计算使用了参数，该参数中包含负3，这是向量的全局最大值。然而，当我们进行到第三次迭代时——让我先删除这个向量——到达第三次迭代时，最大值会发生变化，这也会导致我们的归一化因子出错，因为我们处理的是第三个元素。

所以这里的数字是5，我们计算最大值，最大值是前一个最大值和当前元素的比较，所以新的最大值变为5，归一化因子是前一个归一化因子，即l2加上当前元素减去当前最大值估计值（即5）的指数。然而，如果你看这个l3，这是错误的，为什么呢

因为L3等于，如果你展开这个求和式，它将等于e的3次方减3，加上e的2次方减3，加上e的5次方减5。这里的指数是以5作为全局最大值，这里的指数是以3作为全局最大值，而这个是以3作为全局最大值。所以前两个元素在计算时认为全局最大值是3，但实际上我们后来发现了一个更好的全局最大值5，这使得这个归一化因子是错误的。

然而，我们能否在第三次迭代时修正目前为止在第二次迭代中计算出的归一化值？实际上是可以的，因为如果我们像这里展示的那样展开计算，就会发现此处需要的是-5，因为这才是我们目前找到的全局最大值，而非之前迭代中的-3。因此，这里我们也需要将-3修正替换为-5。

如果我们在这里乘以一个校正因子，它会在这个指数函数中悄悄引入一个新的最大值，那么我们就能解决这个问题。实际上，这个校正因子非常容易计算，因为在第三次迭代时，如果我们把之前计算出的归一化因子 l2 乘以这个因子（即前一次估计的最大值减去当前估计的最大值的指数），就能得出结果。

所以，通过指数的性质，我们可以看到这里的这个会变成e的3减3加3减5次方，这个减3会和这个3抵消，第二个因子中的这个3也会和这个减3抵消，它们会变成e的3减5次方和e的2减5次方，这实际上是正确的，因为在第三次迭代时，我们实际上应该高兴，应该使用减5作为数组的最大值。

到目前为止，我们发现的方法是在遍历数组时修正已计算的归一化因子。当我们找到一个比当前更好的最大值时，如果不需要修正任何内容，公式仍然成立。因为我们在这里所做的修正因子实际上就是之前的最大值，即之前对最大值的估计减去当前迭代中对最大值的当前估计。

所以当前的最大值，这基本上是m[i-1]，而这是m[i]，也就是当前迭代中的当前最大值。让我删除它，否则它会永远留在我的幻灯片里。基本上，当我们到达最后一个元素时，会发现最大值没有变化，因为我们将之前的最大值与当前元素进行比较，而当前元素小于之前的最大值，所以最大值没有变化，我们也不需要修正任何东西，因为之前的l3。

因此，之前计算出的归一化因子是正确的，因为它们都使用了减5的处理方式。当我们不需要修正任何数值时，只需乘以e的（先前最大值减去当前最大值）次方——在本例中即为e的零次方，因此无需进行任何修正。这样一来，我们就找到了一种方法，在遍历数组的过程中修正之前计算出的归一化因子，即便在当前迭代时尚未获得全局最大值。

这样每当最大值变化时我们可以修正，而当它不变化时我们只需乘以e的零次方，这就像乘以1一样。因此，我们为softmax找到的新算法如下：我们从m0等于负无穷开始，l0等于零开始，遍历数组，计算局部的最大值。

从第零个元素到第i个元素，也就是我们正在迭代的元素，之前计算出的li可以通过使用这个修正因子来修正。这个修正因子是e的（前一个最大值减去当前最大值）次方，加上当前元素的指数减去当前最大值的估计值。通过这种方式，我们只需遍历数组一次。

我们得到了两个值：全局最大值和归一化因子，同时还能在最后一步计算softmax。这样，我们就把原本需要遍历数组三次的操作优化为只需两次遍历，这一点非常重要。稍后我们将看到如何利用这种方法推导出Flash Attention。不过，目前我举的这个例子并不能证明我们的算法在所有情况下都适用。

因为我们举了一个非常简单的例子，使用了由四个元素组成的向量，但我们的新算法是否在所有情况下都能适用，无论数字是什么？我们需要证明这一点，因此我们将通过归纳法来证明。首先，我们试图证明什么？我们已经将前两个for循环合并为一个for循环，正如你在这里看到的。我们期望的是，在这个for循环结束时，这个mn（即最后一次迭代时的m）实际上就是向量中的全局最大值。

因此，在最后一次迭代中，l将等于向量中所有元素的指数之和减去该向量的全局最大值（即向量中的最大元素）。我们需要证明这一点，因为之前我所做的只是一个示例，并非严格的证明。我们将通过归纳法来证明这一点，这是证明此类定理的典型方法。归纳法的证明过程大致如下：

我们需要证明我们的算法适用于一个基础情况，例如当n等于1时；然后我们假设该算法对n成立，并需要证明它对n+1也成立。如果这一点成立，那么我们就证明了该算法对所有可能的n都有效，因为它首先适用于基础情况（如n=1），然后通过归纳步骤，我们可以说：如果它对n成立，那么它对n+1也成立。这意味着它同样适用于2，而既然对2成立，由于我们将证明的归纳步骤，它必然也对3成立；以此类推，若对3成立则对4也成立，依此类推直至无穷。

那么让我们从基础情况开始证明，即当n等于1时。这非常简单。当n等于1时，这个for循环只会进行一次迭代。所以m1和l1中的m1将是前一个m（初始化为负无穷大，因为我们设m0等于负无穷大）与当前元素x1的最大值。因此，m1将等于x1，无论x1是什么值。通常x1不可能等于负无穷大，因为它是一个固定表示的数字。

因此它不能是负无穷，所以x，最后的m1，它将因为我们只有一个元素n等于1，这个m1也是这个for循环的最后一个m，它将等于由仅一个元素组成的向量的全局最大值，而l1将等于我们从零开始的之前的l，即l0乘以一个修正因子，在这种情况下，修正因子将是e的负无穷次方，因为修正因子是之前对最大值的估计减去当前对最大值的估计，但之前对最大值的估计是负无穷减去x1，等于负无穷。

所以这一个会是好的，这个会被抵消掉，然后加上e的x1减去当前最大值（即x1）的幂，也就是m1。如果这个等于由仅一个元素组成的向量所有元素之和减去数组中的最大元素（即x1），那么我们就证明了当n等于1时它是成立的。现在我们假设它对n成立，那么它是否也适用于大小为n加1的向量或数组呢？让我们看看在第n加1次迭代时会发生什么。在第n加1次迭代时，我们将取之前对m的估计值（即第n次迭代时的m）的最大值。

根据max函数的性质，当前元素xₙ₊₁实际上等于全局向量到n+1的最大值，因为max函数会选择前一个估计值和当前估计值之间的最大值。而lₙ₊₁作为第n+1次迭代的归一化因子，将等于lₙ（即前一个估计值）。

但在第n次迭代时，前一个归一化因子需乘以校正因子——该校正因子由前一个最大值减去当前最大值，再加上当前元素x的指数值减去当前最大值的估计值（取自然对数）。我们假设该性质成立，因此该算法可运行至第n次迭代。可以确定的是，ln等于向量前n个元素指数值的总和减去向量前n个元素的局部最大值mn，最后再乘以校正因子。

如果要修正某些内容，那将是前一个最大值减去当前最大值，加上当前元素的指数值，再减去当前对最大值的估计。根据指数的性质，我们可以将这一项带入求和式中，可以看到这个mn和这个mn会相互抵消，因为它会是xj减去mn加上mn减去mn加1，所以这个mn和这个mn会抵消掉，我们得到这一项加上这里保持不变的这个因子。

不过你可以看到，这里的这部分内容正是这个求和式在第n+1次迭代时的参数。因此，这部分等于e的xj次方，其中j从1到n减去mn加1，再加上e的xn加1减去mn加1次方。所以j只出现在这里，最大等于n，这与j等于n加1类似。因此，我们可以将这个求和式的索引增加1，结果将是相同的，最终会得到相同的求和式。

因此我们已经证明，在第n+1次迭代中，l将等于数组中所有元素的和，即数组中所有元素直到第n+1个元素的指数，减去直到第n+1个元素的最大值。因此我们已经证明，如果它对n成立，那么它对n+1也成立。这足以证明它对所有大小的数组都成立。

如果你还没弄懂归纳法证明，别担心。如果是第一次接触这类证明，可能需要一点时间才能理解。想进一步学习归纳法证明的话，我建议多看几个相关证明案例——其实非常简单，关键是要调整好思维方式。好了我们继续前进。接下来讲讲分块矩阵乘法，我知道你们都想直接看代码实现部分，我们马上就会讲到那里，但现在还需要补充一点理论基础。

想象一下，我们正在进行矩阵乘法运算。我们有一个矩阵A，想要将其与矩阵B相乘，从而生成一个输出矩阵C。假设第一个矩阵的维度是m×k，第二个矩阵的维度是k×n，那么输出的矩阵维度将是m×n。现在设想我们想要并行化这个输出矩阵的计算过程。我知道我还没有讨论过GPU。

所以我们不会讨论GPU，而是要讨论多核CPU的并行处理。你们很可能对多核CPU很熟悉，因为现在买电脑时，都会有一个CPU，通常你可以买单核CPU或多核CPU，比如双核、四核、八核等等。这些核心实际上就像是CPU内部的小CPU，可以并行执行操作。

如何并行化矩阵乘法。假设你需要并行化这个矩阵乘法运算，输出矩阵C中的每个元素都是矩阵A的一行与矩阵B的一列的乘积。例如，左上角这个元素是矩阵A第一行与矩阵B第一列的乘积，右上角这个元素是矩阵A第一行与矩阵B最后一列的乘积，左下角这个元素是矩阵A最后一行与矩阵B第一列的乘积，以此类推。现在要并行化这个计算，我们需要与矩阵C元素数量相同的计算核心来实现完全并行化。

因此，如果m和n非常小，那么我们可能有足够的核心来处理。但想象一下，如果m和n非常大，比如100乘以100，我们目前并没有10000个CPU核心。那么，我们如何用比矩阵元素数量更少的核心来并行化矩阵运算呢？这就是我们讨论分块矩阵乘法的时候了。基本上，分块矩阵乘法意味着你可以将原始矩阵划分为更小的元素块，然后在这些块之间进行矩阵乘法的运算。

例如，假设我们有一个8行4列的矩阵，这意味着它有32个元素。然后我们将其与另一个4行8列的矩阵相乘，这个矩阵同样有32个元素。输出矩阵应该有64个元素，但我们没有64个核心。那么如何并行化呢？假设我们只有8个核心，现在可以将原始矩阵A分成四个块，其中第一个块是左上角的4乘2元素块。

假设这个矩阵的左上角有8个元素，右上角也有8个元素，左下角和右下角同样各有8个元素，这样就形成了四个区块。接着，我们将矩阵B也划分为八个区块，每个区块由四个元素组成。其中，B11代表原矩阵左上角的四个元素，B4代表原矩阵右上角的四个元素，B21则对应原矩阵左下角的四个元素，以此类推。




how do we do this block matrix multiplication we can watch these matrices as made only by their blocks so we can view this matrix here as made up only by its blocks we can view this matrix here as made up only by its blocks and the output of this multiplication will be a matrices that is computed in the same way as the original matrix but where the output of each dot product will not be a single element of the output matrix but it will be a block of elements of the output matrix for example the top left block here is the dot product of the first row of this matrix with the first column of this matrix and it will be computed as follows so it will be a11 multiplied by b11 plus a12 multiplied by b21 and this output will not be a single scalar but it will be well let me count it should be eight elements so it should be four made up it should be a block of four elements or eight elements let me let me count actually so because we have eight blocks and it should be made up of eight elements let's we can see that here how to find the dimensions of this output block well we can check what is a11 a11 is four by two so it's eight elements in is a smaller matrix made up of eight elements where the elements are distributed in four rows and two columns we are multiplying it by b11 which is a smaller matrix compared to the original made up of two by two elements so four elements so when we multiply four by two multiplied by two it will produce a four by two output block matrix so block so if we do this computation here block by block it will produce a block of output elements of the original matrix so not not a single scalar but a block of outputs which makes it very easy to parallelize because if we have only eight cores we can assign each output block to one core and each core will not produce one output element of the original matrix but it will produce eight elements of the original matrix as a four by two matrix so basically block matrix allow us to to do the matrix multiplication either by element by element so like in the original matrix so each row with each column or blocks by blocks in the same way like we do normal matrix multiplication because the the matrix multiplication that we are doing between blocks is the same way as we do matrix multiplication with the original matrix and it will produce not a scalar but a block and now let's see why this is very important for us so why should we care about block matrix multiplication because we are trying to compute the following operation so the query multiplied by the transpose of the keys and then we will should apply the softmax of this operation and then we should multiply the output of the softmax with v for now let's ignore the softmax let's pretend that we are not going to apply any softmax so we take the output of the query multiplied by the transpose of the keys and we just multiply it by v to obtain the output of the attention which is wrong of course but it simplifies our tractation of what we are going to do next so for for this moment let's pretend that we are not going to apply any softmax so we just do the query multiplied by transpose of and directly we multiply the result of this operation with v this will result in a matrix that is n by d so n tokens each made up of an embedding of d dimensions so lowercase d dimensions and we know that query key and values are themselves matrices of n by d dimensions so the n tokens which made up of an embedding of d dimensions so imagine we have a query matrix and the key and the value matrix that are 8 by 128 so we have 8 tokens each token is made up of 128 dimensions we can divide as we have seen each when we compute a matrix multiplication we can divide our matrix into blocks how we choose the blocks is up to us as long as the shapes of the blocks match when doing the matrix multiplication so for example in the previous case we divided our matrix a into blocks such that the the shape of the block matrix so the matrix that is made up only of the blocks is compatible with the block matrix b so that this operation is possible so this is the only requirement that we need to be aware when doing the block matrix multiplication the shapes of the blocked matrix so the matrix that is made only of the blocks should match in the matrix multiplication for the rest it doesn't matter how we divide it so imagine that we choose to divide this query matrix into blocks of rows and we can do that we don't have to necessarily divide also the columns we can just divide the rows so that each q is not a single row but it's a group of two rows so q1 is a group of the first two rows of the q matrix of the q sequence q2 is the group of the second two rows of the q sequence etc etc and we do the same also for v for k we don't do it because we are actually going to multiply with the k transposed so we do this subdivision directly on k transposed so we so we have the q which has been divided into groups of rows and then we have a k transposed which is a matrix that is 108 by 8 because it's the transpose of the keys which is 8 by 108 and we decide to divide each of the group of columns of k into a single block so the k1 is the first two columns of k transposed k2 is the second group of two columns in k transposed etc etc until k4 which is the last two columns in k transposed the first operation that we do is the multiplication query multiplied by the transpose of the keys which basically means that we need to multiply each query with all the keys then the second query with all the keys etc etc now each query is not a single row of the q sequence it's a group of two rows of this q sequence and each k is not a single column of k transposed it's a group of two columns of k transposed but doesn't matter because we have seen that the matrix multiplication if we write the matrices as made up of blocks we just compute it in the same way when we do normal matrix multiplication so we are multiplying this matrix by this matrix and for what we know this matrix here is made up of four rows with some dimensions which is 128 dimensions and this one here is made of how many rows 128 rows and four columns i didn't draw the columns because it's too many to draw here but you need to pretend it's a lot of dimensions one for each 128 for each vector and here you need to prevent that this is 128 rows when we do the matrix multiplication we apply the normal matrix multiplication procedure which is each output element so this first of all the shape of this matrix of this matrix multiplication will be four by four because it's the outer dimensions of the two metrics that you are multiplying the first element of the output will be the dot product of this vector here with this vector here the second element so this one here will be the dot product of this vector here with this vector here however this is not vector and this is not a vector so it's actually a matrix multiplication in this case this element here is not a scalar it is a group of elements of the output matrix because we are doing block matrix multiplication and how many elements it will be well we know that the original q1 is a 2 by 128 the k1 is 108 by 2 so it will be a group of 2 by 2 elements of the output matrix so we are

multiplication of the q1 with k1 then q1 with k2 then q1 with k3 q1 with k4 etc etc for the first row and then the second row will be q2 with all the k's and the q3 with all the k's and q4 with all the k's so as you can see when we do matrix multiplication we don't even care if what is underlying is a block or a vector or a scalar we just apply the same procedure first row of block matrix multiplication with the first column of the matrix of the second matrix and then the first row with the second column the first row with the third column etc etc let's then multiply because the formula says that we need to multiply query with the transpose of the keys and then multiply by v all of these are block matrices now as you can see from my using of colors every time i refer to the original matrix i use the blue color and every time i refer to the block matrix i use the pink color so we need to multiply the output of the query multiplied by the transpose of the key then by v because we are skipping for now the softmax and later we will see why so if we want to do this multiplication we need to do the following so it will be this matrix is made up of blocks and block matrix multiplication just ignores this fact and just does the matrix multiplication like it is a normal matrix multiplication so we do the first row with the first column then the first row with the second column then the third row the first row with the third column etc etc so the first block of row how is going to be calculated this output in the output matrix of this matrix multiplication well it will be the the first row so the dot product of the first row the dot product because it's not really a dot product it's the actually the matrix multiplication of the first row but in a dot product way let's say with the first column which is made up of v1 v2 v3 and v4 so it will be this element with v1 plus this element with v2 plus this element with v3 plus this element with v4 and this will be the first output element the second output block will be this row with this column which will be this element with v1 this element plus this element with v2 plus this element with v3 plus this element with v4 and this will produce the second output block etc etc also for the third and the fourth block output let's look at what is each block made up of so each block is made up of the um the first element so query one multiplied by key one because um it's the result of the query multiplied by the keys with the v1 of the second matrix plus the this element with this one plus this element with this one plus this element with this one so the pseudo code for generating this output of this attention mechanism which is not really attention mechanism because we skip the softmax but i just want you to get into the habit of thinking in terms of blocks is the following so we take each query block um we go through each query and as you can see let's look at actually what this output is made up of it is made up of the query one multiplied by key one and the result multiplied by v1 then the query one with k2 then the result multiplied by v2 then the query one with the k3 and the result multiplied by v3 plus the query one with the k4 and the result multiplied by v4 this is basically what we are doing is the dot product of this row with this column made up of blocks so the the pseudo code for generating this first row is the query is then query number one and then we iterate through the keys and the values from one to four and we sum iteratively so for each block basically to generate this output matrix and if you for each row we will see that it's a different query with all the keys and values and then this will be the the query number three with all the keys and values and this will be the query four with all the keys and values so to generate this output matrix we need to do we iterate through the queries and this will be one row of this output matrix and then we need to do this iterative sum of the query i that we are iterating through multiplied by the jth k and v and we keep summing them iteratively and that would that will produce the output matrix or you can see here i know that what i have done so far is not useless not useful for flash attention but it's useful for us to get into the mindset of computing this product by blocks because later we will use it also with the softmax all right guys i i know that we have computed what we have computed so far is not really the softmax operation is not sorry they're really the attention mechanism because we have skipped the softmax so somehow we need to restore it and the following few i think 10-20 minutes we are going to be really really challenging because i am going to do a lot of operations that will involve a lot of different blocks and a lot of different matrix multiplication and the variants of the softmax so it may be difficult to follow however don't give up you can watch this part twice three times and every time you it will have a better understanding i also recommend watch it until we reach the flash attention algorithm before we start restarting from to go back to to re-watch it because you watch it we reach the flash attention algorithm and it will give you a better understanding of what has happened so far and then you can re-watch it to deepen your understanding another thing that i recommend is take pen and paper and write exactly the operations that you are seeing and write the shapes of each of these blocks of these elements that are made in that are part in this matrix multiplications so that you better understand what is happening and you better remember what when i refer to a particular element or a particular block okay after giving this small motivational speech let's start so what we have done so far was query multiplied by the transpose of the keys however each query is not a single row of the query sequence but it's a block of queries it's a block of rows in our particular case this q1 is not one row of the query sequence it's two rows of the query sequence because we have chosen as a block size a group of two rows and this k transposed one is not one column of the k transposed matrix is two columns of the k transposed matrix because we have chosen it like this and if you don't remember let's go back to see it here we have chosen k1 is two columns and q1 is two rows of the query original matrix and every time i use the blue color i am referring to the original shape and every time i'm using the pink or violet whatever it is i am referring to the block matrix so it's a block of elements of the original matrix okay now the first thing that we have done was a query multiplied by the transpose of the keys and this produces a block matrix as output that we will call s where each element sij so the s11 element of this matrix will be the query1 with the k transposed 1 this s12 will be query1 with k transposed 2 s13 will be query1 with k transposed 3 etc etc for all the rows and for all the columns then we should be applying the softmax because if you remember the formula is softmax of the query multiplied by the transpose of the keys however i want to restore the softmax operation but with a twist which means that we will apply the simplified version of the softmax and we will call it softmax star which is just the softmax without the normalization so let me write it for you what it means let's do it with the same color that i chose for the softmax which is orange so the softmax if you remember correctly if we remember it's the softmax of a vector we apply it element wise so each element is modified according to the following formula so the i-th element of the output vector to which we are applying the softmax is equal to the exponential of the i-th element of the input vector minus the maximum element in the input vector divided by a normalization factor that is calculated according to this summation that is going from j equal to 1 up to n of the exponential of xi minus x max so basically we are doing the exponential of each element minus this x max and why are if you remember correctly why are we subtracting this x max to make this exponential numerically stable computable because otherwise it will explode and because we are applying it to the numerator we also need to apply it to the denominator okay the softmax star operation is exactly like the softmax but without the normalization part which means that it's just the numerator of the softmax so we will modify each element of the vector to which we apply the softmax star according to this formula let me move it more aligned like this so we just do element wise operation that is the exponential of each element minus the maximum of the vector to which we are applying softmax star okay now why did i introduce this softmax star operation because we will be applying it to the matrix that we have computed so far which is this s matrix so we apply the softmax star to each element of this s matrix but each element of this s matrix is itself a matrix because it's a block matrix and each element of this s matrix so for example the element s11 is a two by two matrix because it is coming from the product of two matrices which are a group of rows and a group of columns from the q and the k so for example this s11 is what is let's draw it actually this s11 will be for example made up of four elements let's call it i don't know a of s11 let's let's choose better naming let's call it i don't know a b c and d just the generic elements when we apply the softmax star to this s11 it will result so let's apply the softmax star softmax star it will result in a matrix that is each element the exponential of each element minus the maximum for each row now we don't know which is the maximum so let's choose one suppose that the maximum for this row is a and the maximum for this row is d the first element of the output of this softmax star applied to this block s11 will be the exponential of a minus a because that's what we chose as the maximum for this row the second element will be the exponential of b minus a because it's the maximum for that row then in the bottom row it will be the exponential of c minus d because that's the maximum for the bottom row and this will be the exponential of d minus d and that's the exponential that's how the softmax star will modify each block in this block matrix let me delete this stuff otherwise it will remain in my slides forever and later i want to share the slides with you guys so you can use my same slides so delete delete delete okay after we have applied the softmax to each of the elements in this s matrix we will call it the p matrix and each element p11 will again be a block of two by two elements so p11 will be the softmax so p11 will be the softmax star applied to s11 where s11 is what is a query one k transposed one and the p12 will be the softmax star applied to s12 where s12 is what is a query one multiplied by k transposed two etc etc etc for all the elements of s okay now that we have applied this softmax star operation the next operation that we should be doing according to the formula of the attention is the softmax of the query multiplied by the transpose of the keys then the result of the softmax multiplied by v i know that we didn't apply the real softmax we apply the softmax star which is softmax without the normalization later we will see how to compensate this lack of normalization because we will do it at the end and it's something that we can do okay so we take this p matrix which is the result of the softmax star applied to this s matrix and we multiply it by v what how do we do it well it's a block or it's a matrix made up of blocks of matrices so p11 is actually not a scalar but it's a matrix of two by two elements and we need to multiply it by v but we don't multiply with the original sequence v but with the blocked sequence v just like before where each v is not one row of v but it's a group of rows of v and how many rows is it is it is two rows of v for now please ignore completely whatever i have written here because we will use it later so we need to do this product of this matrix here which is made up of blocks remember with this matrix here which is made up of blocks it is made up of four rows where each row is not really a row it is a block of rows and this one it is made up of four by four elements where each element is not really a scalar but it's a matrix so as you remember in the block matrix multiplication when the algorithm for computing the matrix multiplication is the same as the normal matrix multiplication except that we use blocks so what i am doing is guys the following operation so let's write it somewhere let's say o is equal to p multiplied by v okay so um the first output row a row because it's not really a row but it's a block row will be computed as follows the first row of this block matrix with the first with the first column of this v matrix and we are treating it like a block matrix so it will be p11 multiplied by v1 plus p12 multiplied by v2 plus p13 multiplied by v3 plus p14 multiplied by v4 this will produce the first output row of o but it's not really a row because it's a made up of two rows so this stuff here is not one row it is two row and we can prove that because what is p11? p11 is let's write it somewhere so p11 is a two by two matrix yeah two by two and we are multiplying it with v1 which is a block of two rows of v so it is a two rows by 128 dimensions so it is equal to 2 by 128 so this stuff here is 2 by 128 so this block here the output block that we're computing is a block of two rows of the output matrix that we are computing i know this is really difficult to follow because we are involving blocks so we need to visualize at the same time matrix as blocks and as the original matrix that's why i highly recommend you to pause the video think it's true write down whatever you need to write down because it's not easy to follow it just by memorizing the shapes you you actually need to write down things anyway we are computing the first output block of the output o matrix now if we if you remember the output the output this output here should be the output of the output of the softmax multiplied by v now this softmax has not been applied to the entire row of this matrix here s matrix here basically to compute this softmax star what we did was to compute the softmax star at each block independently from the other blocks which means that the maximum that we are using to compute each softmax star is not the global maximum for the row of this s matrix but the local maximum of each block and this is wrong actually because when we compute the softmax we apply the softmax we should be using the global row i want to give you an example without using blocks because otherwise i think it's not easy to follow so when we do the normal attention so we have a query multiplied by the transpose of the keys this produces a matrix that is n by n so sequence by sequence where each element of this matrix so let's say three four five i don't know how many is one two three four five six yeah six two three four and five six should be one two three four five six okay this one here should be the dot product of the first query with the first let me use because query one transpose the key one this is because as i said before when we do the product of two vectors we always treat them as column vectors so when you want to write the dot product you cannot multiply two column vectors you need to multiply one row vector with one column vector that's why we transpose this one if it confuses you you can also write q1 k1 that's totally fine it's just wrong from a notation point of view anyway the first one will be the dot product of the query one with the k1 the second element will be the dot product of the query one with the k2 the third will be the query one with the k3 etc etc etc um so this is a q1 with the k1 q1 with the k2 k2 and q1 with the k3 q1 with the k4 um anyway when we do this softmax we actually calculate the maximum on this entire row however what we are doing is we are actually doing a block matrix multiplication and as you remember um when we do by blocks we are grouping together rows of queries and rows of keys and in this particular case we are grouping two queries together to create one one group of queries and two keys together to create one block of keys so we need another row of this one so it's the let me choose a query one okay well query two k1 this should be query 2k1 query 2k2 query 2k3 query 2k4 query 2k5 and query 2k6 um when we each of this each of this block here is computing this block here is computing two by two elements of the original matrix if we had never applied the blocks so it is computing these two four elements here and if we apply the softmax star to each of these blocks we are not using the maximum element in this we are only using the maximum element in each block which means that when we will use it in the downstream product with vmatrix we will be summing values that are wrong because each of these values here will be based on a maximum that is not the global maximum for this row it is the local maximum of this block here and um and this block here will have the global the it will use the local maximum of this block here and this block here will use the local maximum of this block here etc etc etc so what i'm trying to say is that when you sum p11 with v1 p11 may have some maximum local maximum that is different than from the local maximum of p12 and p13 may have a maximum local maximum that of p1 p11 and p12 so we need to find a way to fix the maximum that was used to compute the exponential here with the maximum found here in case the maximum here is higher than the one local to p11 so if we have found for example here a maximum that is higher than the maximum used here here then we need to fix this one and this one because that maximum in the softmax should be the maximum for all the row not the one belonging to the each block and this leads to our next step how to fix this first of all let me introduce a little pseudo code for computing this output matrix here which is an output block matrix and later we will use this pseudo code to adjust the error that we have made in some blocks in case the future blocks so the p13 has a better maximum than p11 or p12 so to compute this output matrix o we go through so for example to compute the first row we choose well p11 is what is is let's go back p11 is let me of q1 k1 p12 is the softmax star of q1 k2 p13 is the softmax star of q1 k3 p14 is the softmax star of q1 k4 which means that to compute this block here here we first need to compute the p11 what is p11 well p11 is the softmax star of a block of q and another block of k which in the case of the first row of the output matrix means that it is the query one with the softmax star of the query one with q1 the softmax star of the query one with k2 the softmax star of the query one with k3 the softmax star of the query one with k4 which means that we need to go we need to make a for loop through all the keys while keeping the query fixed so to compute the first output row we need to do the softmax star to produce p11 we need to do the softmax star of query one k1 and we sum it initially to zeros because we don't we need to initialize our output somehow and we initialize it zeros then we sum the next p12 which is the query one with the k2 and then we sum the next p13 which is a query one with the k3 etc etc that's why we have this inner loop here all right so however this output that we are computing is wrong because i told you we have computed the softmax star using a statistics the maximum value that is belonging to each block and not the one that is the overall row of the original matrix how to fix that we have a tool actually we have computed before an algorithm called the online softmax i don't know if i referred to it before as the online softmax but it's called the online softmax that allows to fix previous iterations when we are computing the current iteration based how well let's review the online softmax we start imagine we are working with one single vector so we are a vector made up of n elements what we do is we do a for loop where we compute iteratively the maximum up to the height element and we fix the normalization factor computed in previous iteration in case we found a better maximum at the current element if this is not clear guys go back and watch the online softmax because this is very important because this is what we are going to use to fix this p11 p12 blocks in case we found better maximum in p13 or p14 etc so let's see how to apply this online softmax to this case here so that we can compute so you may be wondering why are we going through all these troubles i mean why the real reason is when first of all why did we introduce block matrix multiplication because we want to compute matrix multiplication in parallel so you can think that each of these p11 because they are independent from each other and because each of them are using the maximum belonging to each block they can be computed independently from each other then however we need to somehow aggregate their value and to aggregate the value we need to fix the values that have been calculated independently because we didn't when computing values independently we don't have a global view we have a local view so we compute local blocks p11 p12 p13 etc etc and then when we aggregate these values we need to fix them so that's why we are trying to come up with this system of fixing values that have been calculated independently so how to fix this let's look at the following algorithm first of all this o block here as i said before it is a block of two rows where each row is made up of 128 dimensions and we have seen that before by checking the dimensions of p11 and v1 the result of p11 v1 which means that for each output block we need to take care of two maximums and two normalization factors so up to now i didn't use the normalization factor we said that we are applying softmax star which is the softmax without the normalization but eventually we will need to compute this normalization so we want to create an algorithm that fixes the maximum used to compute each of this p11 and also computes simultaneously the normalization factor and at the end we will apply this normalization factor and the way we will do it is as follows we start with initializing the maximum to minus infinity one for each row that we are computing so is our output block is made up of two rows so we need one maximum for the top row and one maximum for the bottom row and also the normalization factor which we initialize with zero because we didn't sum anything for now and the output we initialize it with all zeros because we didn't sum anything to this output for now we compute to compute the output row so this output block here so this output block here we need to go through all the keys to produce this p11 p12 p13 p14 while the query is the query number one the query block number one so the first step that we do is we compute the maximum of the first block p11 which is the row max so the maximum for each row of the block q1 k1 this is not p11 it's s1 sorry guys this is s11 so we compute the maximum of this one and we call it actually s1 as you can see here then we can calculate p11 which is the softmax star which is the exponential