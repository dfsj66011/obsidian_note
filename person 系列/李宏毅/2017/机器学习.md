
## 一、误差来自什么地方？

两个地方：bisa 和 variance。如果可以诊断 error 的来源，就可以选择适当的方法来改进模型。

**偏差和方差估计**：

* 估计变量 $x$ 的均值，假设均值为 $\mu$；方差为 $\sigma^2$
* 采样 $N$ 个点，求均值为 $m$，通常 $m \neq \mu$，然而如果采样多次，每次都计算一个 $m$，那么$$E[m]=E\left[\frac{1}{N}\sum_{n}x^n\right]=\frac{1}{N}\sum_{n}E[x^n]=\mu$$所以用 $m$ 来估计 $\mu$ 是无偏的（和的期望=期望的和）。
* 就像打靶，瞄准靶心，但弹着点会散开，散的有多开，取决于方差，$\text{Var}[m]=\frac{\sigma^{2}}{N}$，方差大小取决于采样的数量
* 同样采样 $N$ 个点，先计算均值 $m$，然后计算方差 $s^{2}=\frac{1}{N}\sum_{n}(x^n-m)^{2}$，用 $s^{2}$ 估测 $\sigma^{2}$，但$$E[s^{2}]=\frac{N-1}{N}\sigma^{2}\neq \sigma^{2}$$有偏的，普遍而言，$s^{2}$ 比 $\sigma^{2}$ 要小

<img src="https://www.codespeedy.com/wp-content/uploads/2020/05/Bias-vs-variance.png" width="400">
* 简单的 model，方差小，散布的小；复杂的 model，方差大，散布的大，为什么？简单的 model 不太容易受 data 的影响
* 把所有的 $f^\star$ 平均起来与真实 $\bar{f}$ 比较，简单的 model，偏差大，复杂的 model，偏差小


**改进方向**：

* 偏差大，欠拟合（Underfitting），无法很好的拟合训练数据；
	* 增加更多的特征
	* 使用更复杂的模型
* 方差大，过拟合（Overfitting），过于拟合训练数据，但无法很好拟合测试数据
	* 增加更多的数据，非常有效
	* 正则化（Regularization）

可以做 N-fold Cross Validation，去挑模型

---------

## 二、梯度下降


最好的状况应该是，每一个不同的参数都给它不同的 learning rate，

**Adagrad**：每一个参数的学习率，都把它除上之前算出来的微分值的均方根

原始梯度下降：$$w^{t+1} \leftarrow w^t-\eta^tg^t$$比如一般情况下，$$\eta^{t} = \frac{\eta}{\sqrt{ t+1 }} \quad\quad g^t=\frac{\partial L(\theta^t)}{\partial w}$$而在 adagrad 中：$$w^{t+1} \leftarrow w^{t} - \frac{\eta^{t}}{\sigma^t}   g^t$$这个 $\sigma^t$ 是过去所有微分的值的 root mean square，对每一个参数而言都是不一样的。例如：$$\sigma^{0} = \sqrt{ (g^{0})^{2} }  \quad \sigma^{1} = \sqrt{ \frac{1}{2} \left [(g^{0})^{2} +(g^1)^2\right]}
\quad \sigma^{t} = \sqrt{ \frac{1}{t+1} \sum^t_{i=0} (g^i)^2}
$$
因为分子 $\eta^t$ 和 $\sigma^t$ 都含有 $\sqrt{ t+1 }$ ，所以可简化为：$$w^{t+1} \leftarrow w^{t} - \frac{\eta}{\sqrt{ \sum^t_{i=0}(g^i)^2 }}   g^t$$
其实这是一系列的方法，Adagrad 是里面最简单的，都是用 Ada- 开头的样子，其实，如果你没有什么特别偏好的话，现在可以用 Adam，它应该是现在我觉得最稳定的。

*疑问点：* 在做一般的梯度下降的时候，参数的更新取决于两件事情：学习率和梯度值大小。一般来说梯度越大，参数值更新越大。但是在 Adagrad 里面，分子中的 $g^t$ 是说梯度越大步伐越大，而分母部分则是梯度累计值越大，步伐越小。

就越大
你底下算出来的这项越大， 你的参数 update 的步伐就越小
这不就跟我们原来要做的事情是有所冲突的吗？ 在分母的地方告诉我们说
Gradient 越大，踏的步伐越大 ，参数就 update 的越大
但是分母的地方却说 如果 Gradient 越大，参数 update 的越小这样
好，怎么解释这件事情呢？ 有一些 paper 这样解释的
这个 Adagrad 它想要考虑的是： 今天这个 Gradient 有多 surprise
也就是所谓的"反差"这样 反差，大家知道吗？
就是比如说，反差萌的意思就是说 如果本来一个很凶恶的角色，突然对你很温柔
你就会觉得它特别温柔这样，所以呢，对 Gradient 来说 也是一样的道理
假设有某一个参数 ，它在第一次 update 参数的时候 它算出来的 Gradient 是 0.001
再来又算 0.001, 0.003, 等等...等等 到某一次呢，它 Gradient 算出来是 0.1
你就会觉得特别大，因为它比之前算出来的 Gradient 都大了 100 倍，特别大
但是，如果是有另外一个参数 它一开始算出来是 10.8, 再来算 20.9, 再来算 31.7
它的 Gradient 平常都很大 但是它在某一次算出来的 Gradient 是 0.1
这时候，你就会觉得它特别小这样子 所以为了强调这种反差的效果
所以在 Adagrad 里面呢，我们就把它除以这项
这项就是把过去这些 Gradient 的平方
把它算出来，我们就想要知道说过去 Gradient 有多大 然后再把它们相除，看这个反差有多大这样
这个是直观的解释 那更正式的解释呢，我有这样的解释
我们来考虑一个二次函数，来考虑一个二次函数
这个二次函数呢，我们就写成这样子 他只有一个参数，就是 x
如果我们把这个二次函数，对 x 做微分的话
把 y 对 x 做微分，这个国中生就知道，这是 2ax + b
如果它绝对值算出来的话，长这样子 好，那这个二次函数的最低点在哪里呢？
是 -(b/2a)，我国中就被过这个式子了 如果你今天呢，在这个二次函数上，
你随机的选一个点开始 ，你要做 Gradient Descent
那你的步伐多大，踏出去是最好的？ 假设这个起始的点是 x0
最低点是 -(b/2a) 那踏出去一步，最好的步伐，
其实就是这两个点之间的距离 因为如果你踏出去的步伐，是这两个点之间的距离的话
你就一步到位了 这两个点之间的距离是甚么呢？ 这两个点之间的距离，你整理一下，
它是 |2a x0 + b| / 2a |2a x0 + b| 这一项，就是这一项
2a x0 + b 就是 x0 这一点的微分
x0 这一点的一次微分 所以 Gradient Descent 你不觉得说听起来很有道理
就是说，如果我今天算出来的微分越大 我就离原点越远
如果踏出去的(我最好的)步伐，是跟微分的大小成正比
如果踏出去的步伐跟微分的大小成正比 它可能是最好的步伐
但是，这件事情只有在，只考虑一个参数的时候才成立
如果我们今天呢，同时有好几个参数 我们要同时考虑好几个参数的时候
这个时候呢，刚才的论述就不见得成立了 也就是说，Gradient 的值越大就跟最低点的距离越远
这件事情，在有好多个参数的时候 ，是不一定成立的 比如说，你想看看，我们现在考虑 w1 跟 w2 两个参数
这个图上面的颜色，是它的 loss
那如果我们考虑 w1 的变化 我们就在蓝色这条线这边切一刀
我们把蓝色这条线切一刀， 我们看到的 error surface 长得是这个样子
如果你比较图上的两个点，a 点跟 b 点 那确实 a 点的微分值比较大，那它就距离最低点比较远
但是，如果我们同时考虑几个参数 我们同时考虑 w2 这个参数
我们在绿色的这条线上切一刀 如果我们在绿色这条线上切一刀的话
我们得到的值是这样子 我们得到的 error surface 是这样子 它是比较尖的，这个谷呢，是比较深的
因为你会发现说，w2 在这个方向的变化是比较猛烈的 如果我们只比较在 w2 这条线上的两个点 , c 跟 d 的话
确实 c 的微分比较大 所以，它距离最低点是比较远的
但是，如果我们今天的比较是跨参数的话 如果我们比较 a 这的点对 w1 的微分
c 这个点对 w2 的微分 这个结论呢，就不成立了 虽然说，c 这个点对 w2 的微分值是比较大的
这个微分值是比较小的 但 c 呢，是离最低点比较近的，而 a 是比较远的
所以，当我们 update 参数 当我们 update 参数选择跟微分值成正比
这样的论述是在，没有考虑跨参数的条件下
这件事情才成立的 当我们要同时考虑好几个参数的时候呢
我们这样想呢，就不足够了 所以，如果我们今天要同时考虑好几个参数的话
我们应该要怎么想呢？ 如果你看看，我们说的最好的 step 的话
我们看最好的这个 step 它其实还有分母这一项 ，它的分母这一项呢，是 2a
这个 2a 哪来的呢？这个 2a 是甚么呢？ 这个 2a 呢，如果我们今天把这个 y 做2次微分
我们做一次微分得到这个式子 那如果我们做二次微分的话，就得到 2a
那它是一个 constant 这个 2a 呢，就出现在最好的 step 的分母的地方
所以，今天最好的 step，它不只是要正比于一次微分
它同时要和二次微分的大小成反比 如果你二次微分比较大
这个时候你参数 update 量应该要小 如果二次微分小的话，你参数 update 量应该要比较大
所以，最好的 step 应该要把二次微分考虑进来 所以，如果我们今天把二次微分考虑进来的话
你会发现说，在 w1 这个方向上
你的二次微分是比较小的 因为这个是一个比较平滑的弧
所以这个二次微分是比较小的 在 w2 的方向上
这个是一个比较尖的弧、比较深的弧 它是一个比较尖的弧，所以它的二次微分是比较大的
所以你光比较 a 跟 c 的微分值呢 ，是不够的
你要比较 a 的微分值除掉它的二次 跟 c 的微分值除掉它的二次，再去比
如果你做这件事，你才能够真正显示 这些点跟最低点的距离这样
虽然 a 这个点，它的微分是比较小的 但它的二次也同时是比较小的
c 比较大、二次是比较大的 所以，如果你把二次微分的值呢，考虑进去
做这个评检、做调整的话 那你这个时候，才能真正反映， 你现在所在位置跟最低点的距离
好，那这件事情跟 Adagrad 的关系是甚么呢？ 如果你把 Adagrad 的式子列出来的话
你把 Adagrad 的式子列出来的话 它参数的 update 量是这个样子的
η 是一个 constant，所以我们就不理它 这个 g^t 阿，它就是一次微分，对不对
下面这个，过去所有微分值的平方和开根号
神奇的是，它想要代表的是二次微分 那你可能会问说，怎么不直接算二次微分呢？
你可以直接算二次微分 确实可以这么做，也有这样的方法， 而且你确实可以这么做
但是，有时候你会遇到的状况是 你在作业一里面是比较简单的 case
相信你都秒算，秒给你结果 但是，有时候你参数量大、data 多的时候
你可能算一次微分就花一天这样子 然后你再算二次微分，你要再多花一天
有时候，这样子的结果是你不能承受的 而且你多花一天 performance 还不见得会比较好
其实这个结果，是你不能承受的 所以，Adagrad 它提供的做法就是 我们在没有增加任何额外运算的前提之下
想办法能不能够做一件事情 去估一下，二次的微分应该是多少
在 Adagrad 里面，你只需要一次微分的值 那这个东西我们本来就要算它了
所以并没有，多做任何多余的运算 好，怎么做呢？
如果我们考虑一个二次微分比较小的峡谷
跟一个二次微分比较大的峡谷 然后我们把它的一次微分的值，考虑进来的话
这个是长这样 这个是长这样 如果你只是在，这个区间和这个区间里面
随机 sample 一个点，算它的一次微分的话 你看不出来它的二次微分值是多少
但是如果你 sample 够多点 你在某一个 range 之内，sample 够多点的话
那你就会发现说，在这个比较平滑的峡谷里面 它的一次微分通常就是比较小的
在比较尖的峡谷里面，它的一次微分通常是比较大的 而 Adagrad 这边，这一件事情
summation over 过去所有的微分的平方，这件事情 你就可以想成，在这个地方呢，做 sampling
就在这个地方呢，做 sampling 那你再把它的平方和呢，再开根号算出来
那这个东西，就反映了二次微分的大小
这个 Adagrad 怎么做，其实我们上次已经有示范过了 那所以我们就不再示范
接下来我们要奖的另外一件事情呢， 是 Stochastic 的 Gradient Descent 那它可以让你的 training 呢，更快一点
好，这个怎么说呢？ 我们之前讲说，我们的 loss function
它的 loss function，它的样子呢 如果我们今天做的是这个 Regression 的话
这个是 Regression 的式子 Regression 得到的 estimation 的结果
那你把 Regression 得到 estimation 的结果 减掉 y\head，再去平方
再 summation over 所有的 training data 这是我们的 loss function 所以，这个式子非常合理
我们的 loss 本来就应该考虑所有的 example 它本来就应该 summation over 所有的 example
有这些以后，你就可以去算 Gradient 然后你就可以做 Gradient Descent
但 Stochastic Gradient Descent，他的想法不一样 Stochastic Gradient Descent 它做的事情是
每次就拿一个 x^n 出来 这边你可以随机取，也可以按照顺序取
那其实随机取的时候 如果你今天是在做 deep learning 的 case 也就是说你的 error surface 不是 convex
是非常崎岖的，随机取呢，是有帮助的 总之，你就取一个 example 出来
假设取出来的 example 是 x^n 这个时候呢，你要计算你的 loss
你的 loss 呢，只考虑一个 example 你只考虑你现在的参数，对这个 example 的 y 的估测值
再减掉它的正确答案，再做平方 然后就不 summation over 所有的 example
因为你现在只取一个 example 出来 你只算某一个 example 的 loss
那接下来呢，你在 update 参数的时候 你只考虑那一个 example
我们只考虑一个 example 的 loss function，我们就写成 L^n，代表它是考虑第 n 个 example 的 loss function
那你在算 Gradient 的时候呢？ 你不是算对 total 所有的 data，它的 Gradient 的和
你只算对某一个 example，它的 loss 的 Gradient
然后呢，你就很急躁的 update 参数了 所以在原来的 Gradient Descent 里面， 你计算所有 data 的 loss
然后才 update 参数 但是在 Stochastic Gradient Descent 里面 你看一个 example，就 update 一个参数这样
你可能想说，这有啥好呢？ 听起来好像没有甚么好的
那我们实际来操作一下好了 刚才看到图呢，它可能是这个样子的
我们刚才看到的图呢，它可能是这个样子 原来的 Gradient Descent，你看完所有的 example 以后
你就 update 一次参数 那它其实是比较稳定
你会发现说，它走的方向 就是按照 Gradient 建议我们的方向呢，来走
但是如果你是用 Stochastic Gradient Descent 的话 你每看到一个 example ，你就 update 一次参数
如果你有 20 个 example 的时候 那你就 update 20 次参数
那这边他是看完 20 个 example 才 update 一次参数 这边是，每一个 example 都 update 一次参数
所以在它看 20 个 example 的时候 你这边也已经看了 20 个 example， 而且 update 20 次参数了
所以 update 20 次参数的结果呢，看起来就像是这样 从一样的起始点开始，但它已经 update 了 20 次参数
所以，这个如果只看一个 example 的话 它的步伐是小的
而且可能是散乱的 因为你每次只考虑一个 example
所以它参数 update 的方向，跟这个 Gradient Descent total loss 的 error surface 界定我们走的方向
不见得是一致的，但是因为我们可以看很多个 example
所以天下武功，为快不破。在它走一步的时候 你已经出 20 拳了，所以它走的反而是比较快的
然后呢，接下来我们要讲的是第三个 tip 就是你可以做 Feature 的 Scaling
所谓的 Feature Scaling 意思呢是这样子 假设我们现在要做 Regression
那我们这个 Regression 的 function 里面 input 的 feature 有两个，x1 跟 x2
比如说，如果是要 predict 宝可梦进化以后 CP 值的话 那 x1 是进化前的 CP值，x2 是它的生命值...等等这样
你有两个 input feature, x1 跟 x2 那如果你看你今天的 x1 跟 x2
它们分布的 range 是很不一样的话 那就建议你呢，把它们做 scaling
把它们的 range 分布变成是一样 比如，这边的 x2 它的分布是远比 x1 大
那就建议你把 x2 这个值呢，做一下 rescaling 把它的值缩小，让 x2 的分布跟 x1 的分布是比较像的
你希望不同的 feature，他们的 scale 是一样的 为甚么要这么做呢？
我们举个例子 假设这个是我们的 Regression 的 function
那我们写成这样，这边这个意思跟这个是一样的啦 y = b + w1*x1 + w2*x2
y = b + w1*x1 再加 w2*x2
那假设 x1 平常的值，都是比较小的，假设说 1, 2 之类的
假设 x2 它平常的值都很大 它 input 的值都很大，100, 200 之类的
那这个时候，如果你把 loss 的 surface 画出来 会遇到甚么样的状况呢？
你会发现说，如果你更动 w1 跟 w2 的值
假设你把 w1 跟 w2 的值都做一样的更动
都加个 △w ，你会发现说 w1 的变化，对 y 的变化而言是比较小的
w2 的变化，对 y 的变化而言是比较大的 对不对，这件事情是很合理的
因为你要把 w2 乘上它 input 的这些值 你要把 w1 乘上它 input 的这些值
如果 w2 它乘的这些 input 的值是比较大的 那只要把 w2 小小的变化，那 y 就会有很大的变化
那同样的变化，w1 它 input 的值是比较小的 它对 y 的影响呢，就变成是比较小的
所以如果你把他们的 error surface 画出来的话呢 你看到的可能像是这个样子
所以如果你把他们的 error surface 画出来的话呢， 你看到的可能像是这样 这个图，是甚么意思呢？
因为 w1 对 y 的影响比较小
所以 w1 就对 loss 的影响比较小 所以 w1 对 loss 是有比较小的微分的
所以 w1 这个方向上，它是比较平滑 w2 对 y 的影响比较大，所以它对 loss 的影响比较大
所以改变 w2 的时候，它对 loss 的影响比较大 所以，它在这个方向上，是比较 sharp 的
所以这个方向上，有一个比较尖的峡谷 那如果今天，x1 跟 x2 的值，它们的 scale 是接近的
那如果你把 loss 画出来的话呢 它就会比较接近圆形 w1 跟 w2 呢，对你的 loss 是有差不多的影响力
这个对做 Gradient Descent 会有甚么样的影响呢？ 是会有影响的 比如说，如果你从这个地方开始
其实我们上次已经有看到了，就是这样 这种长椭圆的 error surface 阿
如果你不出些 Adagrad 甚么的，你是很难搞定它的 因为就在这个方向上，和这个方向上
你会需要非常不同的 learning rate 你同一组 learning rate 会搞不定它 你要 adaptive learning 才能够搞定它
所以这样子的状况，没有 scaling 的时候， 它 update 参数是比较难的
但是，如果你有 scale 的话，它就变成一个正圆形 如果是在正圆形的时候 ，update 参数就会变得比较容易
而且，你知道说 Gradient Descent 它并不是向著最低点走
在这个蓝色圈圈，它的最低点是在这边 绿色圈圈最低点是在这边 但是你今天在 update 参数的时候，走的方向是顺著
等高线的方向，是顺著 Gradient 箭头的方向 所以，虽然最低点在这边 你从边开始走，你还是会走这个方向，再走进去
你不会只向那个最低点去走 那如果是绿色的呢，绿色的又不一样
因为，它如果真的是一个正圆的话 你不管在这个区域的哪一个点
它都会向著圆心走 所以，如果你有做 feature scaling 的时候
你在做参数的 update 的时候呢 它是会比较有效率的
那你可能会问说，怎么做 scaling 这个方法有千百种啦
你就选一个你喜欢的就是了 那常见的作法是这样 假设我有 r 个 example, x^1, x^2 到 x^R
每一笔 example，里面都有一组 feature
x^1 它第一个 component 就是 x(1,1) x^2 它第一个 component 就是 x(2,1)
x^1 它第二个 component 就是 x(1,2) x^2 它第二个 component 就是 x(2,2) 那怎么做 feature scaling？
你就对每一个 dimension i 都去算它的 mean，这边写成 m_i
都去算它的 deviation，这边写成 σ_i
然后呢，对第 r 个 example 的第 i 个 component
对第 r 个 example 的第 i 个 component 你就把它减掉，所有的 data 的 第 i 个 component 的 mean，也就是 m_i
你就把它减掉所有的 data 的 第 i 个 component 的 mean 再除掉所有的 data 的第 i 个 component 的 standard deviation
然后呢，你就会得到说 你做完这件事以后
你所有 dimension 的 mean 就会是 0 你的 variance 就会是 1
这是其中一个常见地做 localization 的方法 最后，在下课前呢，我们来讲一下
为甚么 Gradient Descent 它会 work Gradient Descent 背后的理论基础是什么
那在真正深入数学部分的基础之前呢 我们来问大家一个问题
大家都已经知道 Gradient Descent 是怎么做的 假设，我问你一个这样的是非题
每一次，我们在 update 参数的时候 我们都得到一个新的 θ
这个新的 θ，总是会让我们的 loss 比较小
这个陈述，是对的吗？
好，也就是意思就是说 θ_0 你把它代到 L 里面 它会大于 θ_1 代到 L 里面，它会大于 θ_2 代到 L 里面
每次 update 参数的时候， 这个 loss 的值，它都是越来越小的
这陈述，是正确的吗？ 你觉得它是正确的同学举手
那你觉得这个陈述，它是不对的同学举手 好，手放下
大家的观念都很正确，没错 就是 update 参数以后，loss 不见得会下降的
所以如果你今天自己 implement Gradient Descent 做出来，update 参数以后的 loss 没有下降
那不见得是你的程式有 bug 因为，本来就有可能发生这种事情 我们刚已经看过说，如果你 learning rate 调太大的话
会发生这种事情 或许，我们可以在下课前，做一个 demo
好，那在解释 Gradient Descent 的 Theory 之前 这边有一个 Warning of Math ，意思就是说
这个部分，就算是你没有听懂，也没有关系 太阳明天依旧会升起
好，那我们先不要管 Gradient Descent 我们先来想想看， 假如你要解一个 Optimization 的 problem
你要在这一个 figure 上面，找他的最低点
你到底应该要怎么做？ 那有一个这样子的作法
如果今天给我一个起始的点 ，也就是 θ_0
我们有方法，在这个起始点的附近 画一个圆圏、画一个范围、画一个红色圈圈
然后，在这个红色圈圈里面 找出它的最低点 比如说，红色圈圈里面的最低点，就是在这个边上
这个意思就是说，如果你给我一整个 error function 我没有办法，马上一秒钟就告诉你说
我没有办法马上告诉你说，它的最低点在哪里 但是如果你给我一个 error function，加上一个初始的点
我可以告诉你说，在这个初始点附近，画一个范围之内 哦，有问题是吗？
谢谢，谢谢，没有问题 我们可以在那个附近，找出一个最小的值
那你假设找到最小的值以后 我们就更新我们中间的位置
中间的位置挪到 θ_1 接下来呢，再画一个圆圈 我们可以在这个圆圈范围之内
再找一个最小的点 假设呢，它是落在这个地方
然后，你就再更新中心点的参数 到 θ_2 这个地方 然后，你就可以再找小小范围内的最小值
然后，再更新你的参数，就一直这样下去 好，那现在的问题就是
怎么很快的在红色圈圈里面 找一个可以让 loss 最小的参数呢？
怎么做这件事呢？ 这个地方要从 Taylor series 说起
假设你是知道 Taylor series 的，那个微积分有教过 Taylor series 告诉我们什么呢？
它告诉我们说，任何一个 function h(x) 如果它在 x = x_0 这点呢
是 infinitely differentiable 那你可以把这个 h(x) 写成以下这个样子
你可以把 h(x) 写成 Σ(k=0, ∞)，这里 k 代表微分的次数
(h 在 x_0 微分 k 次以后的值) / k!
然后 (x-x_0)^k 不过，把它展开的话，你可以把 h(x) 写成 h(x_0)
+ h''(x_0) * (x - x_0) + h''''(x_0) * (x - x_0)^2
那当 x 很接近 x_0 的时候 当 x 很接近 x_0 的时候
(x - x_0) 就会远大于 (x - x_0)^2，就会远大于 后面的 3次,、4次，到无穷多次
所以，这个时候，你可以把后面的高次项删掉 所以，当 x 很接近 x_0 的时候
这个只有在 x 很接近 x_0 的时候才成立 h(x) 就可以写成
h(x_0) + h''(x_0) * (x - x_0) 那这个是只有考虑一个 variable 的 case
那其实，我这边有个例子 假设 h(x) = sin(x)
那在 x_0 约等于 (π/4) 的地方 sin(x) 你可以写成什么样子呢？
你用计算机算一下，它算出来是这样子 这个 sin(x)，可以写成这么多这么多这么多项的相加
那如果我们把这些项，画出来的话 你得到这样子，一个结果
如果是 1/sqrt(2)，只有考虑 0 次的话 是这条水平线
考虑 1/sqrt(2) + (x-π/4)/sqrt(2) 考虑一次的话，是这条斜线
如果你有再把 2 次考虑进去，考虑 0 次, 1 次, 2 次的话 我猜你得到的，可能是这条线
如果你再把 3 次考虑进去的话，你得到这条线 如果你再把 4 次考虑进去的话，你可能得到橙色这条线
但是，虽然说，比如说如果你看 成色这条线，应该是 sin(x)，不好意思
好，你发现说，如果你只有考虑一次的时候
它其实跟这个 sin(x)，橙色这条线差很多啊，根本不像
但是，它在 (π/4) 2的附近 在这个地方附近，它是像的
因为，如果 x 很接近 (π/4) 的话 那后面这些项，平方项、三次方项这些都很小
所以就可以忽略它们，只考虑一次的部分 那这个 Taylor series 也可以是有好几个参数的
如果今天有好几个参数的话 那你就可以这样子做 这个 h(x, y)，假设这个 function 有两个参数
它在 x_0 和 y_0 附近 你可以把它写成呢
这个 h(x, y)，你可以用 Taylor series 把它展开成这样 就有 0 次的，有考虑 (x - x_0) 的
有考虑 (y - y_0) 的 还有考虑 (x - x_0)^2 跟 (y - y_0)^2 的
如果今天 x, y 很接近 x_0, y_0 的话 那平方项呢，就可以被消掉
就只剩这个部份而已 所以，今天 x, y 如果很接近 x_0, y_0 的话
那 h(x, y) 就可以写成呢 约等于 h(x_0, y_0) 加上
(x - x_0) * (x_0, y_0) 对 x 做偏微分
(y - y_0) * (x_0, y_0) 对 y 做偏微分 这个偏微分的值，你不要看他这么复杂
微分的值，它其实就是一个 constant 而已 就是一个常数项而已
这个是一个常数项，这个也是一个常数项 好，那如果我们今天考虑 Gradient Descent 的话
如果我们今天考虑我们刚才讲的问题 如果，今天给我一个中心点，这是 a 跟 b
那我画了一个很小很小的圆圈 红色的圆圈，假设它是很小的
再这个红色圆圈的范围之内 我其实可以把 loss function 用 Taylor series 做简化
我可以把 loss function, L(θ) 写成 L(a, b) + θ_1 对 loss function 的偏微分， 在 (a, b) 这个位置的偏微分 ，乘上 (θ_1 - a)
加上 θ_2 对 loss function 在 (a, b) 这个位置 的偏微分，再乘上 (θ_2 - b)
所以在红色的圈圈内，loss function 可以写成这样子
那我们把 L(a,b)，L 用 (a, b) 代进去， 它就是一个 constant，用一个 s 来表示
那 θ_1 对 L 的偏微分 在 (a, b) 这个位置，这也是一个 constant， 所以我们用 u 来表示
这也是一个 constant，所以我们用 v 来表示 这样这个式子呢，看起来就非常简单了 所以在这个范围之内
L 对 θ 跟 θ_1
所以呢，在红色圈圈范围内呢 这个式子是非常简单的
就写成左下角这个样子 再来，如果告诉你说，红色圈圈内的式子都长这个样子
你能不能秒算， 哪一个 θ_1 跟 θ_2 可以让它的 loss 最小呢？
我相信你可以秒算这个结果 不过，我们还是很快地稍微看一下 好，L 写成这样
s, u, v 都是常数，我们就把它放在蓝色的框框那里面 不用管它值是多少
我们现在的问题，就是找 在红色的圈圈内呢，找 θ_1 跟 θ_2 让 loss 最小
那所谓的在红色圈圈内的意思就是说 红色圈圈的中心就是 a 跟 b
所以你这个 (θ_1 - a)^2 + (θ_2 - b)^2 ≦ d^2
他们要在这个红色圈圈的范围内 这件事情，其实就是秒算对不对
太简单了，你一眼就可以看出来 如果你今天把 (θ_1 - a) 都用 △θ_1 表示
(θ_2 - b) 都用 △θ_2 来表示 s 你可以不用理它，因为它跟 θ 没关系啊
所以你要找不同 θ 让它值最小，不用管 s 好，如果我们看一下 L
你会发现说它是 u * △θ_1 + v * △θ_2 也就是说，它就好像是
它的值就是，有一个 vector ，叫做 (△θ_1, △θ_2)
有另外一个 vector，叫做 (u, v) 你把这个 vector 跟这个 vector 做 inner product
你就把 △θ_1 * u + θ_2 * v 你就得到这个值，如果我们忽略 s 的话， 你就得到这个值
接下来的问题就是，如果我们要让 L(θ) 最小 我们应该选择什么样的 (△θ_1, △θ_2) 呢？
我们要选择什么样的 (△θ_1, △θ_2) 我们才能够让 L(θ) 最小呢？
这个，太容易了，就是选正对面的，对不对？
如果我们今天把 (△θ_1, △θ_2) 转成跟 (u, v) 这条反方向
然后，再把 (△θ_1, △θ_2) 的长度增长 我们把它转到反方向，再把它伸长
长到极限，也就是长到这个红色圈圈的边缘 那这个 (△θ_1, △θ_2) 跟 (u, v)
它们做 inner product 的时候，它的值是最大的 所以，这告诉我们说
什么样的 (△θ_1, △θ_2) 可以让 loss 的值最小呢？ 就是它是 (u, v) 乘上负号
再乘上一个 scale 再乘上一个 constant，也就是说你要把 (△θ_1, △θ_2)
调整它的长度，长到正好顶到这个红色圈圈的边边
这个时候呢，它算出来的 loss 是最小的 这一项应该跟长度是成正比的
所以呢，我们再整理一下式子 △θ_1 就是 (θ_1 - a)，△θ_2 就是 (θ_2 - b)
所以，如果我们今天要再红色圈圈里面 找一个 θ_1 跟 θ_2 让 loss 最小的话
那怎么做呢？那个最小的值，就是中心点 (a, b) 减掉某一个 constant 乘上 (u, v)
中心点 (a, b) 减掉某一个 constant 乘上 (u, v)
所以，我们就知道了这件事 那你接下来要做的事，就是把 (u, v) 带进去
把它带进去，就得到这样子的式子 那这个式子，你就发现它其实 exactly 就是 Gradient Descent
对不对？我们做 Gradient Descent 的时候，就是找一个初始值 算每一个参数在初始值的地方的偏微分
把它排成一个 vector，就是 Gradient 然后再乘上某一个东西，叫做 learning rate，再把它减掉 所以这个式子，exactly 就是 Gradient Descent 的式子
但你要想想看，我们今天可以做这件事情 我们可以用这个方法，找一个最小值
它的前提是什么？ 它的前提是 你的上面这个式子，要成立
Maclaurin series给你的这个 approximation 是够精确的 什么样 Taylor series 给我们的 approximation 才够精确呢？
当你今天画出来的红色圈圈够小的时候 Taylor series 给我们的 approximation 才会够精确
好，才会够精确 所以，这个就告诉我们什么？
这个告诉我们说，你这个红色圈圈的半径是小的 那这个 η，这个 learning rate 它跟红色圈圈的半径是成正比的
所以这个 learning rate 不能太大 你 learning rate 要很小， 你这个 learning rate 无穷小的时候呢
这个式子才会成立 所以 Gradient Descent，如果你要让你每次 update 参数的时候 你的 loss 都越来越小的话
其实，理论上你的 learning rate 要无穷小， 你才能够保证这件事情
虽然实作上，只要够小就行了 所以，你会发现说，如果你的 learning rate 没有设好
是有可能说，你每次 update 参数的时候 这个式子是不成立的 所以导致你做 Gradient Descent 的时候， 你没有办法让 loss 越来越小
那你会发现说 这个 L，它只考虑了 Taylor series 里面的一次式
可不可以考虑二次式呢？ Taylor series 不是有二次、三次，还有很多吗？
如果你把二次式考虑进来 你把二次式考虑进来 理论上，你的 learning rate 就可以设大一点
对不对，如果我们把二次式考虑进来 可不可以呢？是可以的
那有一些方法，我们今天没有要讲， 是有考虑到二次式的 比如说，牛顿法这样子
那在实作上，尤其是假设你在做 deep learning 的时候 这样的方法，不见得太普及，不见得太 practical
为甚么呢？因为你现在要算二次微分 甚至它还会包含一个 Hessian 的 matrix
和 Hessian matrix 的 inverse，总之，你会多很多运算 而这些运算，在做 deep learning 的时候呢
你是无法承受的 你用这个运算，来换你 update 的时候比较有效率
会觉得是不划算的 所以，今天如果在做，比如说，deep learning 的时候 通常，还是 Gradient Descent 是比较普及、主流的作法
上面如果你没有听懂的话，也没关系 在最后一页，我们要讲的是 Gradient Descent 的限制
Gradient Descent 有什么样的限制呢？ 有一个大家都知道的是， 它会卡在这个 local minimum 的地方
它会卡在 local minimum 的地方 所以，如果这是你的 error surface
那你从这个地方，当作你的初始值，去更新你的参数 最后走到一个微分值是 0，也就是 local minimum 的地方
你参数的更新，就停止了 但是，一般人就只知道这个问题而已
那其实还有别的问题 事实上，这个微分值是 0 的地方， 并不是只有 local minimum 阿
对不对，settle point 也是微分值是 0
所以，你今天在参数 update 的时候 你也是有可能卡在一个不是 local minimum， 但是微分值是 0 的地方
这件事情，也是有可能发生的 但是，这什么卡在 local minimum 或微分值是 0 的地方啊
这都只是幻想啦 其实，真正的问题是这样
你今天其实只要 你想想看，你 implement 作业一了
你几时是真的算出来，那个微分值 exactly 等于 0 的时候
就把它停下来了 也就是，你最多就做微分值小于 10^(-6)
小于一个很小的值，你就把它停下来了，对不对？ 但是，你怎么知道，那个微分值算出来很小的时候
它就很接近 local minimum 呢？ 不见得很接近 local minimum 阿 有可能，微分值算出来很小， 但它其实是在一个高原的地方
而那高原的地方，微分值算出来很小，你就觉得说 哦，那这个一定就是很接近 local minimum
在 local minimum 附近，所以你就停下来了 因为你们真的很少有机会 exactly 微分值算出来是 0 嘛
对不对，你可能觉得说， 微分算出来很小，就很接近 local minimum 你就把它停下来，那其实搞不好，它是一个高原的地方
它离那个 local minimum 还很远啊 这也是有可能的
讲到这边，有人都会问我一个问题 这个问题，我很难回答，他说
你怎么会不知道你是不是接近 local minimum 了呢？ 我一眼就知道说 local minimum 在这边阿
呵呵，你怎么会不知道呢？ 所以我觉得这些图都没有办法 表示 Gradient Descent 的精神 台湾大学人工智慧中心 科技部人工智慧技术暨全幅健康照护联合研究中心