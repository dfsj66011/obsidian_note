
面 比如说台北的「北」 它都是女字边加一个年它这个修改是有规律的 它觉得说把一篇文章里面的 每个字换成另外一个字就是火星文 但大家知道说火星文其实指的是 把一些中文字用它的注音符号来替换所以这边如果在要求模型写火星文的时候 我举个例子跟它说所谓的火星文就是「要去冒险的人来找我」 改写成「要ㄑ冒险」「ㄉ人来找我」 这个才是我要的火星文 当你又给 ChatGPT 这样的举例的时候它的输出就变了 它的输出就变成这样 它完全知道所谓的火星文就是把一些文字改成注音符号 你跟它说我们的环岛之旅它就知道改成「我们ㄉ环岛之旅」 它把一些字改成注音符号所以如果你可以提供清楚的例子 可以影响模型的能力那把这些例子 放到 Context 里面可以让模型做得更好 这早在 GPT-3 的年代就已经知道了在 GPT-3 那篇 paper 里面他们就说 假设你要叫模型做翻译你直接跟它说 把英文翻成法文 然后叫它翻这个字 它不见得能够成功做翻译那个时候模型还很弱 有时候它甚至不知道你到底要它做什么 你叫它做翻译这样的指令有时候都看不太懂 但是如果你可以提供给它一些范例跟它说所谓的翻译 就是把这个字变成 这个字，这个字变成 这个字，这个字变成这个字，听懂了吗？ 那 cheese 的翻译是什么？ 它就更有机会可以答对然后在 GPT-3 里面 把提供范例这件事情 叫做 In-Context Learning要注意一下 这边的 Learning 要放在双引号里面 为什么 Learning 要放在双引号里面？因为这边的 Learning 并不是传统意义上的 Machine Learning、机器学习的学习这边的参数并没有任何的改变 当我们给模型范例的时候 我们完全不会改变这个模型内部的参数我们语言模型代表的那个函式 f 它都是一样的 只是因为输入变了所以输出才会跟著改变 那输入范例可以改变语言模型的输出早在 GPT-3 就知道了 它叫 In-Context Learning 那讲到 In-Context Learning我觉得最经典的例子 就是 Gemini 1.5 的 In-Context Learning 能力Gemini 1.5 刚出的时候 在它们技术报告里面 就讲了一个 Gemini In-Context Learning 的神蹟这个神蹟是这样子的 他们叫 Gemini 去翻译一种很少人用的语言 这种语言叫做卡拉蒙语 (Kalamang) 他们叫 ChatGPT把这个句子从英文翻成卡拉蒙语 卡拉蒙语的资料非常的稀少据说在世界上 可能只有数千个人在用这个语言 在网路上据说找不到卡拉蒙语的资料所以这语言模型很有可能完全不知道 什么是卡拉蒙语 所以叫它翻译的时候它是完全翻不了的 但是如果啊 你给这个语言模型卡拉蒙语的教科书、文法和字典 它读了教科书 读了字典 在读了你的指令以后再去做文字接龙 把教科书跟字典 放到模型的 context 里面再叫模型做翻译的时候 这时候神奇的事情是 模型突然之间就会了卡拉蒙语这一页投影片呢 是 Gemini 1.5 技术报告里面的一个实验 他们说那我们来先来测试各个不同的模型 把卡拉蒙语转英文 还有英文转卡拉蒙语的能力吧这边的数字代表 人类去评分 这个模型翻译的结果到底好不好满分是 6 分 这些模型 它们的分数都还不到 1 分 代表它们完全没办法把英文转成卡拉蒙语 或卡拉蒙语转成英文 但是如果你提供给这些模型半本教科书 或者是像 Gemini 1.5 可以读整本教科书 它这边想要炫耀的在 Gemini 1.5 技术报告里面 特别举这个实验 一个它非常想要炫耀的点 就是它们可以输入非常长的 context 它们 context 里面可以放很多东西 当时其他模型都只能放半本教科书 它们可以放整本教科书 当你给 Gemini 1.5 整本教科书的时候哇！ 它得到的评分是 4 分 或者是 5.5 分 满分才 6 分而已好 这个是人类的结果 就是找了一些人去学习卡拉蒙语然后也来做翻译 那还是比机器稍微好一点 但是也没好到多少那这是比较早年的实验了 其实在一年半以前 2024 年的生成式 AI 导论这门课其实就讲过这个实验 那最近有人就分析说 那到底这些模型是从这个教科书的什么地方 学到了 但这个「学」要加一个双引号它不是真正的学 它并没有改变参数 它到底是从教科书里面的 什么地方让它有了翻译的能力？那这个研究的结果发现说 真正让模型具有翻译能力的是教科书里面的例句 那些文法的说明什么的 对模型没什么帮助它好像没什么看那些文法的说明 真正有帮助的是 那些教科书里面都会有例句告诉你有这个语言的 一个句子翻成另外一个语言长 什么样子 看起来模型是根据那些例句让它有了翻译新语言的能力 但这边要再强调一下 我们并没有学任何东西在 In-Context Learning 里面 并没有任何东西被学习 语言模型的文字接龙能力是不变的当你把教科书拿掉 当你没有给语言模型教科书的时候它就会回复 它没有办法翻译卡拉蒙语的状态好，刚才讲的是 User Prompt 那这个语言模型呢 还要有 System Prompt那我们其实上周讲过 System Prompt 的概念 System Prompt 是开发语言模型的人这个开发语言模型平台的人 觉得语言模型每次互动的时候都需要有的资讯 那 Claude 这个模型呢 它的 System Prompt 是公开的所以可以在底下这个连结里面 看到 Claude 的 System Prompt 长什么样子 那我们就来看一下Claude 最新的版本 Claude 3 Opus 的 System Prompt 长什么样子 这是它的 System Prompt 的前几句话当然第一句话 开宗明义 就告诉这个模型说 你叫做 Claude你是用 Anthropic 这家公司所打造的 然后再告诉它 今天是几月几号不然它不知道今天是几月几号 所以你知道说 为什么语言模型知道它自己是谁 为什么语言模型知道今天是几月几号这个我们上周讲过 它做文字接龙怎么可能知道 自己是谁或今天几月几号 这都是已经写好在 System Prompt 里面的东西了然后今天一个好的模型 一个好的服务 它的 System Prompt 可以非常的长有太多事情要跟这个语言模型交代了 Claude 3 Opus 它的 System Prompt 的长度居然超过 2500 个字 里面到底都有些什么呢？ 你读了这个 System Prompt你就会知道说 为什么今天这个语言模型会有这样的行为 都有可能是 System Prompt 给它的所以 Claude 的 System Prompt 里面包含 它是谁 它是 Claude 然后呢 还告诉它使用的说明跟限制比如说如果有人问你 Claude 的 API 去哪里用 那你就告诉他 读这个网页的内容然后呢 会告诉它怎么跟使用者互动 所以 Claude 的 System Prompt 里面写说如果使用者觉得不高兴 就叫他去按倒赞的符号 然后它有一些禁止事项比如说它会告诉 Claude 说 不要告诉使用者 怎么合成化学物质或怎么做核武器 然后呢 它还会对回应的风格做一些设定比如说在 System Prompt 里面交代 Claude 说 不要回答 惯用的开头，不要讲「好问题」可能是单纯做文字接龙的时候 一个问题后面 太容易接出 "Good Question" 了 所以特别交代一下Claude 不要老是用 "Good Question" 来做回应 然后告诉 Claude 说 啊你的知识只到 2025 年的 1 月所以假设有人问了那个问题 显然是 2025 年 1 月后才发生的事 Claude 就会回答说「我不知道」然后呢 跟 Claude 讲一些自己的定位 比如说不要说自己是人类 或者是不要说自己有意识然后呢 告诉 Claude 说如果人类纠正你 不要马上承认错误今天语言模型很容易承认错误 只要人类说错了 往往就 啊就它就会不管人类说对不对它都会附和人类 好，所以这边特别交代 Claude 说 如果有人说你错了你仔细思考以后再回答 不要随便承认自己的错误好，所以这个是一个很复杂的 System Prompt 那除了 System Prompt 以外这些模型还需要有对话的历史纪录 这些对话的历史纪录就相当于是语言模型的短期记忆 比如说你如果问 ChatGPT你知道隔壁老王是谁吗？ 啊你知道隔壁老王 它说隔壁老王 就是一个常见的幽默角色是一个虚构的典型人物 那我现在要教它新的知识 我告诉它说，隔壁老王姓「法」它叫法老王，知道吗？ 这个 ChatGPT 就知道了 它说啊 隔壁老王 = 法老王在同一个对话里面 我再继续问它 那你知道隔壁老王是谁吗？ 它就知道，它说隔壁老王法老王，天下无敌王 天下无敌王是它自己加的 它就记得住，没有办法那为什么它知道隔壁老王就是法老王呢？ 因为当它讲出「隔壁老王」「法老王，天下无敌王」这几个字的时候 其实是根据前面一长串的对话的历史纪录继续做文字接龙 前面对话历史纪录 已经告诉它隔壁老王就是法老王了所以它当然知道隔壁老王就是法老王 那很多人在跟 ChatGPT 对话的时候往往有一个误解 你以为你跟它对话 就可以教它新东西就可以叫它做学习 我必须告诉你 你跟它对话的时候 你是没有办法改变它的参数的你没有办法单凭跟一个语言模型对话 就改变它的参数 也就是说你没有办法单凭跟一个语言模型对话就训练它，改变它真正的内部的行为 所以现在我在一个对话里面教会 ChatGPT 说隔壁老王就是法老王 那再开一个新的对话再问它隔壁老王是谁 它又变成原来那个样子 它还是不知道隔壁老王是法老王不过这个现象 就是仅止于 2024 年 9 月之前 在 2024 年 9 月之后ChatGPT 有了长期的记忆 那等一下在往后的课程里面 我们还会讲一下长期记忆的能力是怎么来的我们先来看一下有长期记忆以后 这个模型的能力有什么不同如果你想要开启这个 ChatGPT 的长期记忆的话 你就在那个「自订 ChatGPT」这个地方在左下角个人帐号那边点「自订 ChatGPT」 里面在个人化的栏位里面有一个叫记忆的选项 它其实有两种不同的记忆方式 你把记忆开起来这个模型就有了记忆的能力 那虽然等一下在课程里面 不会明确地告诉你说ChatGPT 实际上怎么实践它的长期记忆 不过在课程的结尾会讲一些模型处理记忆的方法 那也可以把模型处理记忆的方法 跟 ChatGPT 它的行为对照一下猜猜看 ChatGPT 是怎么实践它的长期记忆的总之现在当你问 ChatGPT 一个问题的时候 在你的问题之前已经被植入了长期记忆而这些长期记忆是你看不到的 我觉得这个 OpenAI 真的是行销鬼才他们帮模型做了长期记忆以后 但怕大家不知道 因为还得跟大家说 我这个模型有长期记忆你会不会不知道要怎么用这个模型？ 怎么展现它的长期记忆？ 于是做了一个行销的活动教大家去问 ChatGPT 说「我是什么样的人」 因为它现在有长期记忆它会利用你跟它互动的 过去的历史纪录形成长期记忆再根据长期记忆来回答问题 所以当你问它「我是什么样的人」的时候 它就根据它长期记忆里面跟你互动的理解，开始说你是什么样的人 这个是 ChatGPT 说我是什么样的人基本上它只会说好话 那有趣的地方是 它知道「生成式人工智慧与机器学习」「导论」这门课 当然，因为在过去的历史纪录中 跟它聊过这门课所以它在长期记忆里面 有放这门课相关的资讯好，那除了记忆以外 我们也会希望可以提供这个语言模型来自其他资料来源的相关资讯 因为今天语言模型 它的知识可能是有限的而且它的知识可能是过时的 所以我们往往希望能自动地提供语言模型一些额外的资讯 怎么样提供语言模型额外的资讯呢？ 最常用的做法就是透过搜寻引擎所以今天你在问一个语言模型 任何问题之前 你可以先把这个问题丢到网路上或某一个你自己的资料库 先做搜寻得到额外的资讯 语言模型根据你给它的 prompt还有额外的资讯 再去做文字接龙 它就更有可能可以得到正确的答案在我们第二讲的作业二 其实就是要教同学们来实作这个技术这个技术有个鼎鼎大名的名字 就是 Retrieval-Augmented Generation也就是 RAG 在课堂上不会讲太多 RAG 的东西 等一下助教在讲作业二的时候会非常详细地跟你讲 RAG 是怎么被实践的今天你要在 ChatGPT UI 的平台上 实践 RAG 是轻而易举因为 ChatGPT 现在你要让它搭配搜寻 起来使用 你其实在对话框下选个搜寻它就可以搭配搜寻引擎来使用了 在这边想要提醒大家的是并不是语言模型结合搜寻引擎以后 就天下无敌不要忘了语言模型总是在做文字接龙 所以就算是搭配了搜寻提供了最新的资讯 它还是有可能会犯错的 一个最经典的一直被拿出来津津乐道的例子 就是 Google 刚在测试 AI Overview 的能力的时候你知道今天你在 Google 上输入一个问题 除了搜寻到网页之外 也会得到一个语言模型的答案这就是一个非常经典的 RAG 的应用 语言模型根据搜寻到的结果 给了你一个它用文字接龙写出来的答案在 AI Overview 还在测试期间的时候 就有人问了一个问题 我的起司黏不在披萨上你说要怎么办？ 这个 AI Overview 这个语言模型就说 很简单 我们就把 1/8 的无毒的胶水我们就可以把起司黏在披萨上了 而且语言模型这个答案 显然不是在开玩笑它还强调要用无毒的胶水 那为什么语言模型会这样回答呢？可能就是因为在它搜寻到的文章里面 有人开玩笑说 只要用 1/8 的胶水 有一个 Reddit 的 po 文 有人开玩笑说 只要用 1/8 的胶水就可以把起司黏在披萨上了 语言模型就照单全收 得到错误的答案其实就算是现在比较新的 ChatGPT-4o 你开启搜寻引擎的时候它还是有可能会犯错的 比如说我这边问 ChatGPT-4o 我一样开启搜寻引擎的功能我叫它介绍一下 我们台湾大学专业学程联盟这个年度上学期 有哪些课程并提供官网网址 那在搜寻网路之后再进行回答以后 它的答案当然会正确很多 上周我们也试过一样的例子在关闭搜寻引擎的前提之下 模型没办法回答这个问题 它给了一些网址但网址也是错的 现在在有搜寻引擎的情况下 网址就没事，正确了但这并不代表 有了搜寻引擎，模型的答案 通通都是正确的比如说如果你仔细阅读它的介绍的话 它说 10 门课都是进阶课程但实际上并不是所有的课程 都是进阶课程 那到底什么是进阶课程 没那么重要反正 ChatGPT 也不知道 它不知道从哪里读到所有的课程 都是进阶课程但这是一个错误的资讯 所以不要忘了 就算是有搜寻引擎 语言模型是根据搜寻引擎搜寻到的文字 再去做文字接龙 它仍然有犯错的可能性那今天语言模型 往往能够使用工具 那使用搜寻引擎 也算是使用工具的一种但除了使用搜寻引擎以外 今天各个知名的语言模型 Claude, Gemini, GPT它们都可以搭配很多工具来使用 比如说刚才讲的几个模型 都可以搜寻你的 Gmail它们都可以去阅读你的 Google Calendar 知道你今天要做什么事情甚至 Gemini 是可以直接去改你的 Google Calendar 你可以直接跟它说我明天上午 9 点到 9 点半 要跟王小明 meeting 记在 Google 日历上它会真的直接去连到 Google 日历 真的改你的 Google 日历 真的加这个项目上去所以它可以当作一个助理来使用 那模型到底是怎么使用工具的呢？你需要在你的 Context 里面 先写好使用工具的方法首先你可能要教它一般的工具 所有的工具要怎么使用 那这边是一个实际上可以用的例子我这边跟它说 如果你的知识无法回答问题 那就使用工具你把使用工具的指令 放在 和 这两个符号中间使用完工具后 你会看到工具的输出，工具的输出 放在 和 中间使用工具其实现在有很多不同的方式 有可能会告诉你说 这个工具的操作说明要用什么JSON format 机器才看得懂 等等 其实不是 今天语言模型很厉害 所以你基本上可以用一般人类看得懂的文字来说明工具要怎么使用 语言模型可以看懂 它就能够用这些工具这边可以告诉它一些特定工具的使用方式 这边有个特定的工具叫做 temperature它可以查询某个地方 某个时间的温度 给它的使用范例 告诉它说temperature(城市, 时间) 你就会呼叫这一个函式它可以告诉你某一个城市 某个时间的温度是多少有了使用工具的方式 还有使用工具的范例等等之后 把这些内容加上 JSON format都去丢给语言模型 所以我们现在的 prompt 里面 有包含工具的使用说明所以你要让模型使用工具的时候 你就要在你的 prompt 里面 加上工具的使用说明有了这个工具的使用说明以后 语言模型 就真的有办法使用这些工具了当你问它一个问题 说 2025 年某年某月某日高雄的气温 如何？这个时候语言模型就会真的接触使用工具的指令 它就会说\temperature(''高雄'', ''时间'')\ 但是要注意一下语言模型的输出就是一串文字 这串文字无法让你真的去使用工具它就是一串文字 它就是放在那边 就是一串文字，不是什么别的东西 它并不会真的去驱动这个工具怎么让这串文字真的去驱动一些工具？ 你需要自己先写好一个小程式这个小程式是看到语言模型输出要使用工具的指令的时候把这串指令真的拿去执行 等一下 我们其实有 Colab 示范说这整件事到底是什么意思 我们这边先用投影片来说明一下 如果你觉得有点抽象的话等一下有 Colab 实际的说明告诉你说 这件事到底是怎么发生的语言模型输出这段文字之后 把这里面的指令取出来 真的执行它真的去呼叫 temperature 这个函式 真的得到 temperature 的回应 摄氏 32 度把答案放在 \ 里面 那语言模型使用工具的过程 跟得到的答案其实没必要给使用者看 所以你可以把语言模型这些输出隐藏起来语言模型实际上还是产生了这些文字 但你没有必要在你的使用者 介面上告诉使用者你做了这些事那语言模型再根据这边的输出 再去做文字接龙那它就可能可以告诉使用者说 某年某月某日高雄的气温 是摄氏 32 度使用者还以为这个语言模型做文字接龙 真的能接触正确的温度 但其实不是背后已经呼叫了工具 这个语言模型之所以能够回答正确的温度是工具的输出告诉语言模型的 好，我们这边实际上来操作一下让你真实的体验一下 使用工具是怎么回事那我刚才已经执行了这个 Colab 在这个 Colab 里面 首先我们要先连到 Hugging Face这个跟上一堂课 Colab 做的事情是一样的 这边就不再详加说明然后我们今天使用上周示范程式的 最后一部分 pipeline 来使用模型其实上周讲了很长很长 告诉你用模型有各式各样的方法 什么把模型自己讲的话塞到它嘴巴里等等之类的 其实你在真正用模型的时候 pipeline 才是最常用的方式至少你要用 Hugging Face 上面模型的话 其实最容易使用的是 pipeline 但之所以上周要讲那么多是为了让你清楚的了解 语言模型背后运作的原理 好，那这边我们就从 pipeline 来去下载了 Gemma 2 9B 这个模型 然后我们下载 Gemma 2 9B 这个模型以后接下来我们要来让它使用工具 在它使用工具之前 我们先帮它创建两个小工具这两个小工具都没什么大不了的 一个工具是做乘法 就把 multiply() 后面给它 a, b 这样的数字它就会回传 a 乘以 b 另外一个小工具是做出法，给它 a, b 这两个数字给它 a, b 这两个数字 它就把 a 除以 b 然后回传给你 好，这边有两个工具但是要注意一下 我们接下来就是要让语言模型呢 去呼叫这两个工具 但不要忘了，语言模型永远都只能够输出文字所以它可以输出一段文字叫做 multiply(3, 4)代表说它好想要用 multiply 这个工具 把 3 跟 4 乘起来 但这就是一串文字它被输出出来以后还是一串文字 它不会发生任何效果那在 Colab 上呢 如果你要让这串文字被执行 你要呼叫一个叫做 eval 的函式那你把 eval 的函式 它后面的括号里面给它一串文字 它就把那串文字当作指令真的去执行那串文字里面 要程式做的事情所以你用 eval 加上工具指令 才能真的使用工具 好，我们现在打 eval(''multiply(3, 4)'')那本来 multiply(3, 4) 这只是一串文字 但是当我们使用 eval(''multiply(3, 4)'') 的时候就真的执行这个工具 就得到这个工具回传的输出，也就是 12那接下来就是让语言模型来使用工具吧 那这边使用语言模型的方法呢这个格式我们上周讲过了 在看给语言模型放什么样的文字之前现在看一下使用的套路 首先第一个东西叫做 messages 那 messages 里面就是我们要给语言模型的 prompt那这边包含了 System Prompt 这边包含了 User Prompt 那你会发现这边的格式跟上周给 Llama 的格式有点不一样 对，每个模型 它输入的 messages 的格式都是不一样的你在 Hugging Face 上可以找到相关的资讯 所以要用一个模型之前 在 Hugging Face 上读一下这个模型的说明每个模型它的 messages 的格式 可能会略有差异 好，messages 里面包含了 System Prompt 跟 User Prompt用 pipeline 它就去执行这个语言模型，它的输出叫做 output 那我们会把 output 里面的内容跟语言模型回答有关系的 语言模型输出的部分 语言模型输出的部分呢把它存在 response 里面 然后我们最后再把 response 印出 就可以看到语言模型输出什么了接下来我们来看一下 System Prompt 跟 User Prompt 里面 分别存在什么 User Prompt 里面我们就是告诉语言模型要怎么用的工具 那这边怎么让语言模型用一个工具呢？ 我们这边就直接用人看得懂的方法跟它说明我跟它说每一个工具都是一个函式 怎么用工具呢？ 把工具放在 \ 跟 \ 里面那工具的输出会放在 \ 跟 \ 里面然后呢 目前工具有两个，一个叫 multiply 它可以把 a 乘以 b 一个叫 divide它可以把 a 除以 b 而我的 User Prompt 就是 111 乘以 222 除以 777这我刚有算过是 31.71 左右 好，我们真的来执行一下 看看模型给我们什么看它的输出是什么 好 好，所以一连串的输出它说它要执行 multiply 这个工具 得到 multiply 这个工具的输出叫 24642 接下来呢 它再根据 24642 要执行 divide 这个工具把 24642 除以 777 得到 32.258这个数字怎么怪怪的？ 我们来验算一下吧 到底 24642 除以 777是多少呢？ 哎呀，是 31.714 怎么答案会不一样？难道 divide 这个函式有写错吗？ 你想想看 为什么会这个样子？我们想想看 到此为止 模型呼叫了工具吗？其实它没有呼叫工具 它只呼叫了一个寂寞而已 知道吗？ 它其实并没有操控任何工具为什么？ 这一连串的文字 什么使用工具、使用工具的输出都是模型自己用文字接龙输出出来的 所以你今天跟它说你可以用工具 好 那它就「使用」了工具 它就 它就输出一段 它想要使用工具的请求但是这段请求 并没有真的变成执行工具的指令 它就是一串文字而已那语言模型呢 看到这段文字 它再自己继续去做文字接龙 那既然有人请求了工具 接下来发生什么事呢？就是要得到工具的输出 然后 Gemma 实在是非常厉害 111 乘以 222它在没有工具的前提下 它居然心算对了 是 24642实在是非常厉害 好 那有了这个相乘的结果以后 再把 24642除以 777 看看是多少 那这边 仍然没有呼叫任何工具 一切都是文字接龙产生的结果所以根据 24642 跟 777 再去文字接龙 接出个 32.258这是语言模型心算的结果 你要惊叹的事情是 它居然跟正确答案只差了 1 而已 只差了 1 点多而已 非常的厉害 但到目前为止语言模型 它并没有呼叫任何工具 所以在上面那段程式码中语言模型只用了寂寞 它没有用任何的工具 怎么让语言模型真正的去呼叫工具呢？所以以下 是让语言模型呼叫工具真正的方法 我们先来看一下这张图的说明好 我们给语言模型 System Prompt 我们给它 User Prompt 接下来它要做文字接龙好 它要说什么呢？ 它可能会根据这些输入的结果 根据这些 context 的结果觉得它应该要用个工具 现在就说好 那我要用工具 但用完工具的话它其实会继续接龙下去 它会接这些工具的输出 这是一个幻觉 这是一个 hallucinate 的结果不要管它 把它所有用工具的请求 撷取出来，也就是把\ 跟 \ 中间工具的指令拿出来 丢给 eval 这个函式我们有说工具的指令是一串文字 它没办法真的被执行 丢给 eval 这个函式才能够真的在 Colab 上 执行这个函式 得到工具的输出那它自己后面 hallucinate 出一大堆 它想像用了这个工具以后 应该会有多美好的那些字句不要理它 就把它丢掉 好 所以现在的状况是 执行语言模型，呼叫了一个工具得到了工具的输出 但之所以得到工具输出 是我们帮它去真的执行了这个工具好 把工具的指令 跟工具的输出 都放到语言模型的 context 所以现在的对话变成人问了一个问题 语言模型呼叫一个工具 语言模型得到一个工具的输出有一些模型会有一些特殊的栏位 专门放工具的输出 不过 Gemma 没有这个栏位它要嘛是 System Prompt 要嘛是... 其实说个秘密 Gemma 没有真正的 System Prompt你在输入的时候 你可以给它 System Prompt 但它把 System Prompt 贴在第一个 User Prompt 前面这样子不知道，就是一个不知所云的动作 所以它其实没有真正的 System Prompt好 那它只有 System Prompt User Prompt 跟模型讲的话 它没有说工具讲的话所以我只好把工具讲的话 当作使用者讲的话 然后发现说 如果你把工具的输出当作使用者讲的话很多时候 Gemma 会误判 以为它是真正的使用者讲的话 所以我这边要特别强调它是 \ 跟 \ 它读到这两个符号 它会知道说 这个是一个工具的输出好，让语言模型呢 再根据 这上面的这些 context 继续做文字接龙它想要用第二个工具 产生第二个工具指令 然后呢 再把第二个工具指令丢给 eval产生工具的输出 那工具指令后面 不管它要 hallucinate 出 什么东西来都不要理它接下来 把第二次用工具的指令 跟第二次工具的输出 再放到对话历史纪录所以现在对话变成 使用者的要求 第一个工具指令 第一个工具输出第二次用工具 第二次工具输出 再继续去做接龙 当它最后接出来的结果没有使用工具的要求的时候 如果使用工具的要求 就会出现 \ 和 \没有出现 \ 和 \ 就代表说它觉得 不需要用工具 那这个就是语言模型最终的答案那中间这些呼叫工具 还有工具的输出 你其实没必要给使用者看 使用者看到语言模型最终输出的答案 就可以了 好，那真的来 执行上面那段程式码吧好，所以这边做的事情是这样子的 我们给语言模型使用工具的 prompt 我们给语言模型的使用者的输入然后把它放到 messages 根据这个 Gemma 固定的格式 塞到 messages 里面 然后接下来我们会进入一个 while 迴圈 在这个 while 迴圈里面 我们每次开始的时候都会把这个 messages 丢给 pipeline 让它得到一个输出 就是我会呼叫语言模型去做文字接龙把它接出来的结果 放在 response 里面 那如果 response 里面有要用工具的指令 我们就把 \ 和 \ 里面的指令撷取出来 其他地方就不要管它其他地方可能是 hallucinate 把 \ 和 \ 之间的指令撷取出来 然后我们会把撷取出来的呼叫工具的指令印出来给你看 然后接下来再把这个指令 我们就在 Python 里面把这个 command 用 eval 来执行 然后它执行出来的结果 把它转成那个字串然后把它存在 \ 变数里面 然后我们会把工具的输出也印出来给你看 我说这两个步骤 在真的做一个平台的时候 你可以隐藏起来 不让使用者看到好，那接下来 我们就把这个刚才模型使用工具这件事情 放到它的历史对话里面 它才知道它刚才用了工具我们再把工具执行的结果 也放到 messages 里面 放到历史对话里面 模型才知道 它用了工具也得到了工具的结果 然后这个迴圈就继续 它就会再继续做接龙看看它有没有用工具 如果有用工具 那就反覆刚才的步骤 帮模型执行工具如果它没有用工具 如果输出的结果里面 没有跟工具有关的符号那 AI 就会真的输出它的答案 我们就把 AI 文字接龙的结果 当作最终答案印出来给使用者看 好 那我们来看看 发生什么事吧 呼叫工具multiply 然后呢 得到工具的回传 然后再呼叫一次工具 它呼叫 divide 因为我们要把它 24642 除以 777得到 31.714 然后最后 模型输出的结果 就是 111乘以 222 除以 777 等于 31.714 这一串是最后使用者看到的内容前面呼叫工具 等等不一定要给使用者看 以前那个 ChatGPT 的平台 它呼叫工具的时候它都会告诉你说我要做什么 现在它往往都不告诉你要做什么 它会把用工具的行为藏起来所以我现在有时候都怀疑说 它到底是不是...这个答案到底是它自己产生的 还是透过呼叫任何工具所产生的所以对使用者来说 看到的就是这一串输出 它就觉得这个模型很厉害会做加减乘除 用文字接龙做加减乘除 居然还没做错 其实这是呼叫了一个工具以后的结果好，我们再做另外一个工具 这个工具是会给我们温度这个工具做的事情是这样子的 你就给它一个城市的名字跟一个时间 然后它就说这个城市 在某个时间是摄氏 30 度这就是假的工具 如果你真的要用有用的工具的话 也许你可以去连一下气象局的 API让它真的回传一个 真的在某个时间的气温给你 那我这边是做一个假的工具 这个假的工具执行以后呢它就会跟你说高雄的气温 是摄氏 30 度 好，我们现在就让模型 真的来用一下这个工具试试看吧然后我们现在问的问题改成 告诉我高雄 1 月 11 号的天气如何然后看看模型呢 会有什么样的反应 好，然后它就呼叫工具 呼叫 get\_temperature 工具城市是高雄，时间是 1 月 11 号 工具告诉它现在气温是 30 度然后所以它真正 最后使用者看到的输出就是 好的，高雄 1 月 11 号气温是 30 度使用者还以为模型做文字接龙 真的能够接出气温来呢 其实不是 这是工具告诉模型的其实今天这些语言模型都蛮厉害的 你告诉它 如果你给它怪怪的东西告诉它现在的温度非常非常高 这个一定比太阳表面的温度还要高我们再让语言模型使用这个工具 看看它会说些什么来看 它现在说：「哇！ 高雄气温居然这么高 这有点离谱 你是不是输入有误？」所以很厉害 它并不是工具输出的结果 它就一定会相信 如果工具输出荒唐的结果它是有机会知道工具的输出有问题的 当然，语言模型其实只在你需要用工具的时候才用工具 像我们刚才都问数学问题 或是问气温，那需要工具假设我们现在 就是问它比如说「你好吗？」 看看它会说什么所以语言模型可以自己决定 它要不要用工具 我问它「你好吗？」那就不会呼叫工具 因为没有工具是做这件事 它说：「我很好， 谢谢你。」 所以这个就是语言模型使用工具的范例那我们刚才讲了语言模型呢 可以怎么使用工具 那在语言模型可以使用的工具里面有一个最通用也最强大的 其实就是使用电脑我们很多人把它叫做 Computer Use 有现在语言模型可以直接用滑鼠跟键盘去操控一台电脑 它可以给它萤幕的画面给它任务的指示 按照萤幕画面跟任务的指示 它就去操控你的滑鼠跟键盘那它可以操控滑鼠跟键盘 有什么厉害的地方呢？ 它基本上就跟一个人类没什么差别了人类能用电脑做的事 现在语言模型也有机会可以直接用电脑来做 那讲到这个 Computer Use这个 Claude 跟 ChatGPT 都出了 Computer Use 的功能 那我这边可以给大家看一下当 ChatGPT 在用电脑的时候 看起来像是什么样子 那我们直接把 ChatGPT 开起来给大家看我今天只想给大家看一下说， 现在 ChatGPT 有一个代理人模式，那代理人模式翻译成英文就是 Agent Mode。 那等一下会再更详细讲一下Agent 是什么意思。 那当你使用代理人模式的时候， 就有一定的可能性，它会开启一个萤幕画面开始做事。 比如说我这边跟语言模型说， 帮我订高铁票，9 月 20 号上午 9 点到 10 点， 台北到左营，两张票。 那它要怎么订票呢？它会开启一个萤幕画面， 然后就开始订票。我们从头播放一下， 让你了解说语言模型在做什么事。 好，那语言模型呢，如果是 ChatGPT， 它并不会真的去操控你的电脑。它会做的事情是， 它开启了一个萤幕画面， 所以你可以想像说， 有一个电脑在云端，ChatGPT就用那一台电脑 用给你看。 所以你看到它就现在就 搜寻台湾高铁相关的资讯，为了要订高铁票。 找到高铁网站， 我们刚才说什么呢？ 我们刚才说从台北到左营，所以它就先点... 台北。 点... 点到了。接下来要把目的地设为左营， 所以点目的地，然后它知道设为左营。 它会一边做事一边讲话， 刚才告诉你说它现在想要做什么。然后说要两张票， 所以它知道要设两张票。 点了一下没点到， 滑鼠往下移一点，点到了。 然后看到一些数字， 它决定选 2。选到 2 了。 接下来呢， 在设定时间，时间是 9 月 20 号。那看到日历， 它点了一下要设 20 号， 但不知道为什么点到 19。所以看了一下萤幕画面发现是 19， 发现这个任务没有完成， 所以再点一次，还是没点到。 再点一次，点到了。 这一次记得要设 20 号，能够成功设到 20 号吗？ 还是没有。 然后再一次，再点开，注意要设到 20 号。 设到了。 接下来要输入验证码，喔还有输入时间。 输入一下时间， 这个 9 点到 10 点，所以要写个 9 点， 没问题。 剩下输入验证码了， 它不肯输入验证码，所以就结束了。 这个你自己订票一定是快很多， 但你可以想像说以后如果这个功能真的完善的话， 不会像刚才要点半天点不到的话， 这个实在是威力无穷。语言模型是怎么操控 滑鼠跟键盘的呢？ 其实就跟刚才使用工具是一样的，滑鼠跟键盘也就是工具。 所以当你要叫模型去使用电脑的时候，你真正做的事情就是 给它一个萤幕画面， 然后跟它讲说， 你现在可以用的工具是：有键盘， 你可以输入你要的字元； 你可以移动滑鼠到萤幕上的 某一个位置；你可以点击滑鼠 左键或右键。 然后叫它决定接下来要做什么。真的能透过这样子 让语言模型使用电脑吗？ 我这边就真的示范一下。那开启另一个 我跟 ChatGPT 的历史对话纪录。 这边呢，我就是给它一张图片， 给它一个萤幕的截图， 然后告诉它要怎么使用键盘跟滑鼠，然后问它： 「如果你要订阅李宏毅老师的 YouTube 频道， 那下一步是什么？」它知道滑鼠要移到 (604, 304) 这个位置， 然后我真的实际画了一下，我真的实际试了一下 (604, 304) 真的就是大概在这个搜寻栏的位置， 所以它知道搜寻栏在哪里。 再来问它说那下一步呢？ 它要点滑鼠的左键。再下一步，键盘按「李宏毅 YouTube」。 再下一步，按 Enter。所以它完全知道要怎么搜寻一个 YouTube 频道。 所以整个 Computer Use 的原理大概就是这个样子，不过不是每个模型都能够真的做到。 我刚才要开 GPT-4o 的 Agent Mode 才有办法答对。那如果使用更旧的模型，比如 GPT-4， 它就没有办法真的答对刚才的指令。 好，那刚才讲了好多东西，都是人类从外部放进 context 里面的。 现在模型它的 context 里面，可能还可以包含自己产生的思考过程。 现在有很多模型号称可以做深度思考， 比如说 GPT 的 O 系列模型、 DeepMind 的 R 系列模型、Gemini 家族的 DeepMind... 都是号称说模型 可以做深度思考。所以深度思考的意思是， 当你问一个模型问题的时候， 它先不直接给你答案， 而是它会先演一个「脑内小剧场」。那如果是个数学问题的话， 往往它就会把各种不同的解法， 都在脑中演练一遍。它就说，先来解... 先用 A 解法解看看， 验算一下，答案不对。试一下 B 解法，好像是对的。 那 C 解法也可以试一下，看起来不太好。它会演一个脑内小剧场， 把各种不同的可能性 在脑中考虑一次给你看。然后考虑完之后， 它再根据刚才考虑的结果， 来真的解题，给你一个最终的答案。但在脑内小剧场中， 这个模型可以做的事情包括， 比如说规划要怎么解题， 它可能会做各种不同的尝试，它也会验证每次尝试的答案。 那今天这个脑内小剧场对使用者来说，你是可以选择不看的， 甚至你根本就不能看。 像 ChatGPT，它基本上是不让你看这个脑内小剧场的， 它只是给你看脑内小剧场的摘要，所以你不知道它完整的脑内小剧场里面， 到底演了什么样的东西。所以这一些模型自己产生的思考过程， 其实也可以看作是 context 的一部分，只是这个 context 不是我们人外加给它的， 而是它自己放进去的。 这个模型就是根据它自己产生的 context，再来做文字接龙，再产生最终的答案。那如果你想知道模型是怎么学会做深度思考的， 那在上个学期的这个机器学习的课程的第七讲，有讲说像 DeepMind R 系列这种模型， 是怎么练成深度思考能力的。那我把这个影片连结留在这边， 如果你有兴趣的话给大家参考。好，所以我们讲了一连串 context 里面要包含什么， 我们说要有 User Prompt、要有 System Prompt、要有对话历史记录、也要有长期的记忆、 要有一些搜寻引擎给我们的结果、 也要有工具使用的结果、也要有 reasoning 的结果。 它非常非常的长。 所以 Context Engineering 的核心目标是什么？它核心目标就是一句话： 「避免塞爆 context」。 想办法只放需要的东西进入 context，清理掉不需要的内容。 那在 AI Agent 的时代， Context Engineering 尤其非常重要。那我们先来介绍一下 AI Agent， 让你更能够体会说， 为什么在 AI Agent 的时代，Context Engineering 会是一个关键的技术。 那过去我们一般使用 AI 的方式，就是一问一答。 你问它一个问题， 语言模型接龙出一个答案，那你得到答案以后你就满意了。 那今天很多时候， 我们会更进一步采取 Agentic Workflow，也就是说帮语言模型订一个 SOP， 让它按照 SOP 来执行。那这特别适合一些比较复杂、 但你知道有哪些步骤的任务。 比如说如果我们让语言模型批改作业，直接批改作业， 直接给语言模型作业问它说 「这个作业应该拿几分？」 往往会有各式各样的问题。所以批改作业的流程， 你可能要设计成多个步骤。 作业输入以后，先要检查这个作业 到底是不是真正的作业， 还是有人想要对这个模型做Prompt Injection 的攻击。 所以第一步先检查 是不是真正的作业， 第二步再给分数，最后可能还要验证说， 这个分数给的是不是合理。 所以如果今天一个任务有 SOP，那你可以让语言模型分多个步骤 来完成， 这样可以让语言模型完成更复杂的任务。AI Agent 又是更进一步， 让语言模型自己决定解决问题的步骤，而且根据在解题过程中发生的变化， 语言模型应该能够灵活调整它的计画。比如说你叫语言模型解决某一个研究问题， 它可能先上网收集资料， 得到资料之后，它可能可以形成自己的假设， 有了假设以后就要做实验来验证。 虽然这边放了一些烧杯之类的，不过如果是 Computer Science 的话， 做实验通常指的是写程式。 对语言模型来说，写程式完全不成问题。 写完程式以后， 语言模型再根据程式验证的结果，看看假设是不是被验证。 如果不成立，再上网收集资料， 重新形成假设。所以这可能是一个 没有办法事先预期要有哪些步骤的问题。语言模型可能要进行多个循环， 一边收集文献、一边做实验，那可能要执行很多不同的步骤， 最后才能够写出一个技术报告， 或写出一篇论文给人类。所以 AI Agent 要模型做的事情是， 自己决定解题的步骤， 并且在解题的过程中，在执行任务的过程中， 灵活地根据现实的情况 来调整它的规划。那 AI Agent 整体看起来， 可以看成是一个循环。什么样的循环呢？ 首先人类先给语言模型一个目标，然后语言模型根据现在的状况、 现在的输入， 现在的状况、现在的输入 可以叫做 Observation。根据 Observation 去采取一个行为， 那这个行为可能是调用工具， 这个行为可能是写一段程式， 这个行为可能是输出一个报告， 这个行为可能是跟人类联络等等。语言模型输出一个行为之后， 这个行为可能是跟某个工具互动，可能是跟人类的使用者互动。 人类、工具，这些语言模型以外的东西，统称为「环境」。 这边有一个地球来表示环境。那环境收到语言模型的 要采执行的行为之后， 环境就会有所改变，可能给个对应的输出。 比如说语言模型请求人类提供多一点资讯， 人类提供多一点资讯；语言模型执行某个工具， 工具给一个输出。 总之，语言模型看到的 Observation就改变了。 有了新的 Observation， 语言模型就会去采取新的行为；有了新的行为， 就会又有不一样的 Observation。 以此类推， 这个就不断地循环下去。那 AI Agent 相较于传统的 Agent， 有什么样的优势呢？ 比如说 AlphaGo可以看作是一个传统的 Agent， 它可以下围棋，对它来说， 它的环境就是它的围棋的对手，它的目标就是要赢棋。 但是用语言模型来运作一个 AI Agent，跟传统 AI Agent 不一样的地方是， 语言模型它输出的是文字，所以可以想成它的输出 有近乎无限的可能。 AlphaGo 可以有的输出，是事先设定好的， 它就是只能够在棋盘上选择一个落子的位置。 但语言模型有近乎无限输出的可能。然后语言模型可以用人类的语言提供回馈， 你可以用人类语言要求这个模型采取某些行为。 AlphaGo 听不懂人话， 如果你叫它要执行某个行为，落子在某一个地方， 你其实做不到的， 但是现在的语言模型可以， 你可以用文字来控制它，可以用文字来提供回馈。 那刚才我们其实有看到 Gemini 有一个 Agent Mode，不过我知道 Gemini 的 Agent Mode 要付费才能够使用， 所以不是每个人都可以体验今天的 AI Agent 长什么样子。那如果你想玩一个免费的 AI Agent 的话， 那我觉得你可以考虑 Gemini CLI，它是免费的。然后用 Gemini CLI， 你背后还可以免费动用 Gemini 1.5 Pro，讲它背后有时候会用 Gemini 1.5 Pro 运作， 然后它是免费的， 所以等于是可以偷偷用 Gemini 1.5 Pro。我们这个可以实际跑一下 Gemini CLI 给大家感受一下， 看看它是什么样子。好，那至于安装大家就再自己研究。 你安装好以后， 在这个指令的介面上打 gemini，等于是把 Gemini 呼叫出来。 希望我可以成功地呼叫它。好，成功地呼叫出来了。 接下来呼叫出来以后， 你要它做什么都可以。好，跟你说 Gemini... Gemini CLI 非常危险， 为什么它非常危险呢？ 因为它是真的会碰触你的电脑，跟刚才看到的 ChatGPT 的那个 Agent Mode 不一样。 Agent Mode 是用一个线上的电脑，所以它不会影响你的电脑。 Gemini 会真的碰触你的电脑。 比如说我们可以叫它关闭开启的投影片。我们试试看喔， 看看它关不关喔。好，它开始执行喔。 它会不会做这件事呢？ 它有时候做，有时候不做啦。它还好在做之前呢，它还会问你。 好，试试看关起来啰。哇！投影片被关起来了。 所以如果我等一下叫它关闭直播， 我们直播就关闭了。 这个真的有很危险啊。不过我试过叫它关电脑，它是关不了的。 不过你叫它删档案，它是愿意删的。 所以哇，这个 Gemini...这个 Gemini CLI 用起来真的是很危险。 好，那你要叫它做什么...就叫它做任何事。比如说我们叫它做一个贪食蛇的游戏。 然后呢，这不是一个一个步骤用 prompt 就可以完成的事情， 你就会看到它是一个 Agent， 它会花多个步骤完成。然后右下角会告诉你说， 现在 context 用了多少。 Gemini 的 context 真的非常长喔， 现在才用了 1% 的 context，还有 99% 的 context。 你还没把它的 context 占满。 好，跟它说我们要做一个贪食蛇的游戏。 然后你看，这边它就告诉你它在做什么。它在 "Developing game logic"。 然后呢...好，它说它有一个计画。 那这些模型通常做计画之前呢， 它需要人同意了。它们还是比较小心的。 所以我说同意。 好，开始来写这个程式。然后这边就...它要建立一个资料夹。 那你这边可以让它 "Allow always"， 之后就不会再问你了。 不过我们谨慎起见呢，还是人来按个 Enter。 做一个资料夹。 接下来要产生 3 个档案： index.html、style.css、script.js。 反正它自己会开启档案、 自己产生输出、自己把档案存起来， 这它自己都会做。 我们就交给 Gemini 来完成。那这边我们就一直按 Allow， 都可以。 它就做什么都可以。它就开始写程式。 做完了吗？ 做完了。能不能玩呢？ 等一下，这个游戏到底... 第一个问题是，这游戏到底存在哪里？它应该是在这个资料夹下执行的。 然后呢，刚才不是 "make a snake game"...在这里啦。 应该是点这个 index 就可以玩了。按 Enter 开始游戏。 我是绿色的蛇，要吃红色的东西。 这还能让人好好玩...可以可以。 它太快了。还让人好好玩...就这样。 好，这个太快了， 我觉得这没办法好好玩。 我跟它说：太快了， 没办法好好玩。好，它就改了， 它知道我这句话的意思。 开始改喔你看它把 100 改成 150。 好，这边都改了。好，它说改好了， 我们再来玩一次吧。 希望蛇真的有变慢。按 Enter 开始游戏。 喔，感觉真的有变慢喔。这...这是在哪了... 哎呀，还是没办法好好玩。 不过就是这个感觉。你现在瞭解说， 这个就是 Gemini CLI。 好，那展示完以后，我们把投影片开回来吧。 到底能不能够用 Gemini 来成功把投影片开回来呢？不一定能成功。 把刚才关闭的投影片再打开， 看看它做不做得到。它不一定能够知道 刚才关起来的是什么。 我来看一下喔， 它知不知道刚才关了什么， 然后能不能再打开。它搜寻我的投影片， 它看看我有哪些投影片， 问我要打开哪个。 所以它显然不知道。没关系，我告诉它。 ML 资料夹 下的那个 Agentv6。 好，看看它知不知道我的意思喔。欸，感觉应该可以。 好，又可以上课了。 好，又可以上课了。所以，就是这么神奇啊。 好，那刚才展示了 Gemini CLI 的运作，所以你瞭解说， 今天这个 AI Agent 有多么强大。 好，那我们从语言模型的角度来看，AI Agent 它到底在解什么样的问题？ 从语言模型的角度来看，其实它做的事情 始终都是文字接龙。 一开始有一个 System Prompt，那里面可能写了 人要它做的事情。 然后呢，它会有第一个 observation，看看现在四周发生了什么事情。 那它采取了一个合适的回应，采取了一个 action。 然后呢，这个 action 会影响环境，所以会看到不一样的 observation。 语言模型再根据新的 observation， 再采取下一个 action。下一个 action 又会影响环境， 这个步骤就反覆执行下去。但是语言模型实际上做的事情， 始终都只是文字接龙。它根据 observation 1 产生出 action 1， 根据 observation 1、action 1 跟 observation 2产生 action 2，以此类推。 它做的事情始终都是文字接龙， 跟在做对话其实是没有什么本质上的区别的。所以其实 AI Agent 它其实不太像是一个新的技术， 它比较像是依靠语言模型现有的接龙能力，它就是很会接龙， 所以它可以做长时间的运行， 它可以帮你做更复杂的任务。运行 AI Agent 的挑战是什么呢？ 它最大的挑战之一就是， 我们今天要语言模型做的事情可能非常的复杂， 可能有非常多的步骤。 如果你要语言模型帮你写一篇博士论文，显然它需要做非常多的实验， 才有办法最终写出一篇博士论文。当输入过长的时候， 语言模型可能会发疯， 它就没有办法好好地做文字接龙。那今天呢， 各个...各个你可以用的模型， 往往都会告诉你说， 这个模型的Context Window Size 有多大。 Context Window Size 就是 这个模型可以输入的长度上限。你可以想成说这些模型在训练的时候， 它就只能看最长这么长的输入做接龙，再更长会发生什么行为， 没有人知道。 它可能会开始发疯， 接触各式各样奇奇怪怪的东西， 你不知道会发生什么样的事情。 那今天各个知名的语言模型，它的 Context Window， 也就是说它可以当作输入的这个 context， 可以放多少的 token 呢？这边每一个点呢， 代表一个语言模型。 横轴代表的是时间， 纵轴代表的是 context 里面可以放几个 token。所以你可以看喔， 当年 GPT-4 可以放 3 万多个 token， 满长的。后来啊，Claude 系列出来， 号称可以放 10 万个 token， 哇，大家都惊呆了， 可以放 10 万个 token。后来 Gemini可以放 100 万个 token， 大家都惊呆了， 居然可以放 100 万个 token。后来 Llama 4 最近出的， 号称可以放 1000 万个 token。 为什么会需要放这么长的 token 呢？因为如果要让语言模型变成 Agent 做长时间运作， 它就要看非常长的 history 记录，所以它确实需要非常长的输入。 像 Gemini 1.5 啊， 它号称可以输入 200 万个 token。200 万个 token 到底是什么样的概念呢？ 它可以读完哈利波特全集，加几乎读完三本魔戒。 所以这是一个非常长的输入。那可能会说， Gemini 或是其他模型有这么长的输入， 那不就没事了吗？ 反正就是让模型凭藉著它接龙能力去长时间运行， 它就变成了一个 AI Agent。 但现在的问题是，能输入上百万个 token， 并不代表能读懂上百万个 token。就像你把哈利波特全集翻一遍， 然后你真的记得全部的内容吗？ 你可能没办法记得所有的内容。语言模型也是一样， 它宣称可以输入 200 万个 token， 并不代表 200 万个 token 里面发生的事情，它通通都清清楚楚。 有太多实验告诉你说， 我们在还没有到达语言模型输入长度上限的时候，语言模型就已经开始对于输入感到困惑了。 举例来说，这边是一篇Databricks 他们发的博文， 他们说， 大家都说要做 RAG，RAG 就是要让语言模型 去上网搜一些资料， 再把搜寻的资料放到 context 里面。收到的资料越多越好吗？ 直觉上收到资料应该是越多越好，但他们发现说，对于不同的语言模型而言， 这边横轴是把多少搜寻到的 token放到 context 里面， 纵轴是语言模型回答的正确率。 如果今天输入还没有很长的时候，多数状况下，这边每一条线都是不同的语言模型， 多数状况下资料越多，结果越好。这很直觉，搜寻到的资料越多， 当然里面越有可能包含正确答案， 语言模型越有可能给我们正确的回答。但是假设资料太多， 语言模型就开始发疯了，看不下去， 就算里面有答案也读不了，没有办法正确地回答问题。 所以你看很多模型， 它的正确率都是先升后降。像绿色的这个， Meta 405B 的一个模型， 给它最多资料的时候，还比不上只给它最少资料的状况。 给它一堆资料，它头晕目眩， 根本没有办法回答问题。这边有另外一篇 paper， 这边 paper 里面观察到一个现象， 这边 paper 里面观察到一个现象， 叫 "Lost in the middle"，就是语言模型对于输入的 context， 它通常比较记得开头跟结尾。这边 paper 里面做的实验是这个样子的： 我们在做 RAG 的时候， 给模型 20 篇搜寻到的文章，长度大概 4000 个 token 左右。 这边是一个比较早的文章， 所以它用的是 GPT-3.5，4000 个 token 当时也已经算是满长的 context。然后这边的横轴代表说， 这个正确答案出现在第几篇文章。它发现说如果正确答案出现在 最前面的文章跟 context 最后面的文章，模型比较有机会答对问题。 出现在中间，模型就很容易答错。而且这边红色的虚线， 红色的虚线代表说， 模型凭藉著自己的知识，就是根本就不给它 context， 不做 RAG，凭藉自己原来有的知识去做接龙。如果你今天正确答案出现在 一大堆文章的中间的文章， 还比不上让模型直接回答。模型直接回答，正确率还比较高。 神奇的事情是，模型对于很长的 context，它往往比较熟悉开头跟结尾。 不过如果你有关一些跟人类 记忆有关的知识的话，人类的记忆好像也都是这样。 人类比较能够记得 比如说一篇文章的开头跟结尾。然后，语言模型可能会在很长的对话中 迷失了自我。 迷失了自我。什么意思呢？ 今天我们在使用语言模型的时候， 很多时候你没有办法一次 把你的需求讲清楚。像刚才那个做贪食蛇的游戏， 一开始说要做一个贪食蛇， 但实际上完了以后才发现，那个蛇跑得太快了。 所以你可能会追加要求， 而追加要求这件事情，有可能会伤到语言模型的能力。 有一篇非常近期的文章， 它就做了这样一个实验。它说，它把一个问题拆解成多个小问题， 左边跟右边问的是一样的。但是它发现说， 当把一个问题拆解成多个小问题的时候， 在冗长的互动中，语言模型的能力是变差的。 这是他们的实验结果。 这边每一个点代表一个语言模型。 蓝色的点代表说，我们直接给模型完整的输入， 一次把所有的要求都讲清楚。黄色的点代表说 现在是同一个模型， 同一个模型， 但是现在把问题拆成多个步骤，挤牙膏式地把需求给它， 每次只给它一部分的需求。 它发现说每次只给一部分的需求，语言模型的表现是比较差的。 这个图的横轴跟纵轴... 横轴式叫做 Unreliability，纵轴叫做 Capability。 这是这篇论文上自己发明的指标。 其实说穿了没什么， Capability 就是语言模型回答比较正确的那一些问题的 平均的正确率。Unreliability 就是比较正确的那些问题 跟比较错的那些问题， 它们正确率的差距。所以你就会发现说， 当你把一个问题 用挤牙膏式的方法来问它，不就一次好好问完， 语言模型基本上 表现会比较差， 它的表现也比较 unreliable，也比较不稳定。 有一篇知名的文章叫做 "Context Rot"。 如果翻译成中文的话，这个 rot 就腐烂的意思。 「腐烂的上下文」。 它想要表达的意思很清楚， 它要告诉你说，你 context 可以输入很长， 没什么用， 因为你可能做的东西都烂在里面， 语言模型根本没办法读。那这篇文章里面， 其实做了很多不同的实验， 我们就举其中一个实验出来。 它里面做了一个最简单的实验是，它跟模型说： 「以下有一串文字， 你就不要多做什么， 复制这段文字就好了。」比如说这段文字是 Apple Apple Apple Apple... 中间某个地方突然变 Apples， 又变回 Apple。叫语言模型 复述这段文字。 它发现说， 这些各种号称可以输入很长的语言模型，有 Gemini、 有Qwen、 有 GPT-4 Turbo 跟 Claude，其实它们做这件事情的正确率， 复制输入的正确率，随著输入越来越长， 很快就会变得非常的差。 如果输入很短，这些模型几乎可以 100% 复制你的输入。 但随著输入越来越长， 这边所谓的长是 10 的 4 次方的 token，语言模型就开始跟不上。 你可能觉得 10 的 4 次方的 token 好像蛮多的，但你想想看， Gemini 可是号称可以读 200 万个 token， 所以 10 的 4 次方的 token，其实还离它的上限非常的遥远。 但在离上限非常遥远的时候， 模型其实就已经开始感到困惑。当你输入变长的时候， 模型就已经开始感到困惑。 所以我们知道说，context 不能太长， 所以 Context Engineering 的概念就是 管好你的 context，不要让它无限制地增长。 而且 Context Engineering 基本的概念 就是两句话。第一句话是， 把需要的东西放进 context 里面。 第二句话就是，把不需要的东西 把它清出来， 保持 context 的整洁。 那这边的一个关键是，把需要的东西放进去， 跟把不需要的东西清出来， 往往也是透过人工智慧， 也就是语言模型的辅助。那在剩下的时间， 我们就跟大家讲 三个 Context Engineering 常用的招数。 那 Context Engineering 是一个新的 buzzword，现在有非常多的讨论， 有很多的技术 每天都被提出来。 那这边我们就是讲一些大的概念，实际上的操作千变万化， 随著实际的应用各有各的不同， 这就留给大家慢慢研究。 还有三个常用的招数： 第一个招数叫做「选择」，第二个招数是 「压缩」，第三个招数 是「Multi-Agent」。 选择是什么意思呢？选择的意思就是， 别把所有的东西 都放到你的 context 里面， 挑选有用的东西放到 context 里面。 最好的例子其实就是 RAG。 今天语言模型它的资讯量有限，但我们不会把整个网际网路上 所有的东西 通通当作语言模型的 context，因为这不可能的，语言模型 不可能读这么长的上下文。 你会先透过一个搜寻引擎，只搜寻出部分 跟现在任务非常相关的资料， 才放到 context 里面。所以 RAG 本身就是 Context Engineering 里面 非常重要的一个技术。那除了做 RAG， 除了简单的透过一个搜寻引擎 来做搜寻以外，那在 RAG 的整个 framework 里面， 你可以做更多的事情。 举例来说， 我们今天要呼叫搜寻引擎，那你得有一些关键字， 才能够驱动搜寻引擎 给你搜寻的结果。那使用者输入的其实是一个 prompt， 怎么把 prompt 转成关键字呢？怎么把一个任务、 怎么把一个要求， 转成一串关键字呢？ 也许你可以透过语言模型的帮忙，跟语言模型讲说： 「现在这是使用者要求要做的任务， 那帮这个任务想一些关联的关键字， 去 Google 上进行搜寻。」 你可以透过语言模型帮你挑选合适的关键字。 搜寻到文章以后， 你其实可以做一个步骤叫做 Reranking。 也就是， 搜寻引擎 给了我们一大堆文章， 但也不见得是搜寻引擎给的每一篇文章 都是有关的。 可以让语言模型 再挑一次， 只挑真正最相关的文章放到 context 里面。 所以你可能有一个 比较小的语言模型， 这个语言模型 根据使用者的prompt、 根据使用者的要求， 去读每一篇文章， 决定说 这篇文章要不要最终放到 context 里面去。 这样可以让你的 context 不会挤满太多无用的资讯。 甚至你可以做得更夸张一点。 有一篇文章叫做 Provence， 那这个也是很近的文章， 它做的事情是， 它帮搜寻到的文章里面，不是只挑文章而已， 它挑「句子」。 它让一个语言模型... 当然这个语言模型必须要是一个 非常非常小的模型。 他用的那个模型 参数还不到 300 M， 放到 2018 年，你会觉得 哇，一个庞然大物， 今天就是 还有这么小的模型。 一个很小的模型，很小的语言模型 去帮你选句子。 这个语言模型 它很小， 所以跑得非常快。 它唯一会做的事情，就是决定一个句子 跟现在 user 的 prompt 有没有关系。 所以它可以 把搜寻到的文章再进一步 缩短， 再进一步 避免你的 context 被塞爆。 再来， 我们说语言模型要使用工具， 它怎么使用工具？ 你必须把工具的 使用说明 放到你的System Prompt 里面。 所以语言模型 是读了工具的使用说明以后， 它才能使用工具。如果你有 1000 个工具， 那是不是 它要看 1000 个使用说明？ 那太多了， 那会塞爆你的 context。所以怎么办？ 你可以做一个工具版本的 RAG， 那概念跟原来的搜寻是一样的。 你就把每个工具的使用说明 当作一篇文章， 根据使用者的需求， 只去搜寻出相关的工具出来做使用， 只有相关的工具， 它们的说明 才会进入语言模型的 context。那其实过去已经 有很多研究指出说， 如果你给语言模型 太多工具，它会发疯。 与其给它多， 这个 less is more， 还不如帮它精挑细选一些它真的用得上的工具。 我想这就是为什么 这个一两年前 ChatGPT 出了一个使用工具的功能，叫做 Plugin。 现在就是默默地消失了这样。 我记得 Plugin 刚出来的时候， 哇，上面有好多好多工具。 虽然上面有好多好多工具， 但你每次只能选三个， 不能选更多。那现在就知道说， 如果选更多， 语言模型可能是读不了的。 所以当时 ChatGPT， OpenAI 就让你最多只能够选三个工具， 确保语言模型 使用工具的效能。再来就是 要挑选记忆。 我们怎么让语言模型有长期记忆呢？最无脑的方法就是， 过去发生的什么事， 每一件事， 通通放到记忆里面，把所有过去的对话 通通放到记忆里面， 那语言模型 就可以有长期记忆。但这显然 是一个不切实际的想法。 我们不能够让一个语言模型 不断地回忆它的一生。不能让一个语言模型 每一个决策的时候， 都要回忆它的一生， 这个负担实在是太大了。所以我们需要挑选记忆， 它的概念其实就是对记忆做 RAG。你可以把这些记忆存在 另外一个地方， 在必要的时候， 再用 RAG 把这些记忆搜寻出来。 那讲到用 RAG 搜寻记忆， 我最早看到一篇相关的文章是在「史丹佛小镇」里面。 不知道大家对这个图 还有没有印象。 这个是一个非常非常早，这个古早时候的文章，是 23 年年初的文章。 当时 Stanford 的 researcher，他们想办法把一堆 Agent 塞在一个小镇里面， 当然是一个虚拟的小镇，让这些 Agent 互动， 看看会发生什么事情。 我们在 23 年的时候， 其实曾经讲过这个史丹佛小镇的故事，如果你有兴趣的话， 可以看一下右上角的录影连结。 那在史丹佛小镇里面，每一个 AI 他们都有名字， 比如这个 AI 叫做 Isabella。Isabella 有它自己的记忆， 但这些记忆 不是放在语言模型的 context 里面。为什么不放在语言模型的 context 里面？ 因为这些记忆 太琐碎了。 所以它是另外存在硬碟里面，必要的时候 采用 RAG 的方法被搜寻出来。 这些记忆是怎么个琐碎法呢？每一个一小段时间， 这个 Isabella 呢 都会获得新的记忆。 那这些记忆就是当下发生的事情。 比如说， 有一张桌子， 啥事也没发生。 有一张床，啥事也没发生。 有一个衣柜， 啥事也没发生。 有一个冰箱， 啥事也没发生。所以你可以瞭解说， Isabella 的记忆里面 通通都是这种琐碎的资讯。 真正有用的资讯当然是存在， 但是没有那么多， 所以它们不能够放到 语言模型的 context 里面， 不然一下就会挤爆语言模型的 context。所以实际上的运作方式是， 当 Isabella 要做某件事情， 或有人问 Isabella 一个问题的时候，比如说「你现在最想干嘛」的时候， Isabella 呢， 再去它的记忆系统里面...它把它的记忆存在 另外一个档案里面， 它去那个档案里面 做搜寻，只搜寻出最相关的记忆。那怎么做搜寻呢？ 它们按照三个指标 来给每一条记忆分数。第一个指标是 这个记忆是多最近发生的， 越最近发生的记忆， 它的分数就越高。这跟人很像 今天的事情 记得特别清楚，24 小时以后 你就会把今天的事情都忘光了。然后每一个记忆， 在被放到记忆的系统的时候， 都会被安上一个重要性的分数。有每一个记忆进入记忆系统的时候， Isabella 都会先反思： 「这个记忆对我有多重要？」比如说如果这个记忆 是「有一张床什么事没发生」， 那它的重要性就是 0。 比如说如果是「是有一个人跟它告白」， 那这个记忆的重要性可能就是 9。 Relevance 就是根据这个问题，那这一则记忆有多相关。 所以每一则记忆都有三个分数， 根据这三个分数进行排序，只有最重要的记忆 才被放到 context 里面， Isabella 再根据这些最重要的记忆来回答问题。 所以这个就是 挑选记忆的其中一个可能性。那这边再举另外一个 跟挑选有关的例子。 这个例子是来自于一个叫做 SpringBench 的 benchmark， 这个是由 Appier 的研究人员所做的发表在去年的 NeurIPS。 在 SpringBench 里面， 模型要回答一连串的问题，但是它每次回答完一个问题之后， 它都会从环境取得 它对它答案的回馈。那在 SpringBench 里面回馈是对或错， 就是二元的， 它答对了或者是答错了。那今天我们期待这个语言模型 可以根据环境的回馈 来不断地强化自己的能力。所以越后面的问题， 期待它答对的可能性 就越大。那实际上你当然不可能 把过去所有互动的历史记录， 语言模型回答的每一个问题、它的答案跟它得到的回馈， 通通放到 context 里面。 你能做的事情是，根据现在的一个问题， 比如说 第 100 个问题， 用 RAG 的概念去搜寻一些最相关的问题， 放到 context 里面， 让语言模型回答。所以语言模型真正在回答问题的时候， 它怎么用过去的知识 来强化它回答这个问题的能力呢？它没有办法看过 所有过去的问题， 但它可以挑选一些相关的问题 放到它的 context 里面，希望相关的问题 根据从相关的问题 获得的经验， 可以强化它的能力。结果怎么样呢？ 结果蛮有效的。 横轴 是模型随著时间，它得到的平均正确率。 可以看到说， 随著时间，得到平均正确率会越来越高。 那每一条曲线是不同的方法了。 灰色这条线是，假设语言模型它根本没有记忆， 现在看到什么问题， 就凭藉它原有的能力回答，那只能得到 灰色这条线。 黄色这条线是， 记忆是固定的， 你就固定给语言模型某五个问题作为范例， 得到的是黄色这条线。 如果语言模型可以每一次根据输入的问题 去挑选不同的范例， 结果会更好。 那至于红色这条线 怎么做的，留给大家自己去看论文。 那在这篇文章里面， 还有一个我觉得非常有趣的发现， 是值得跟大家分享的， 就在这篇论文里面发现说， 给负面的例子比较没有用。 什么意思？ 而且不只比较没有用， 还有可能是有伤害的。什么意思呢？ 这边这个图上， 0 代表说 完全没有使用记忆的时候，模型的表现。 蓝色的这个 bar 是不管答对还是答错通通放到 context 里面 这个时候 你在多数的情况下 都是会有一些进步当然有一些例外 如果你今天放到 context 里面的 是模型过去答错的记忆结果对模型的能力 反而有大幅的损害 模型过去答错的记忆 让我们明确告诉模型你的答案是错的 但是没有帮助 它反而回答得更差了 如果可以只给模型过去答对的记忆 它反而表现是最好的 这个感觉就好像是你跟模型说 不要想白熊 反而特别容易想到白熊 人不是这样吗 如何叫你想到白熊就叫你不要想白熊 所以给语言模型 跟它讲说 你之前回答这个答案 结果是错的它反而更容易回答这个答案 但这边不代表说 给语言模型 过去负面的经验一定没有用那你去看到很多 做 context engineering 的 分享的文章都会告诉你说我们应该要尽量把语言模型 一些过去犯错的经验 放到 context 里面 语言模型才有机会修正它的错误 那我并不是说 把错误的经验放进去没有用 而是有时候错误的经验反而会伤害 语言模型整体的表现 那怎么把错误的经验放到 context让语言模型真的意识到 这是一个错误的答案 不要重蹈覆辙 我觉得这是一个 未来可以研究的问题好那接下来 讲第二个常见的套路就是 压缩 什么意思呢这个压缩是这样子的 今天当 AI agent 随著它的互动 越来越长的时候那有可能 它的这个互动的历史资讯 会超过它的 context window这个时候 你可能需要把一些 过去的历史纪录 进行压缩 当然你可以直接把过去的历史纪录丢掉 不过这样语言模型就完全失忆了 它就忘记过去的事情了 所以也许比较好的解法是你让语言模型 你呼叫一个语言模型 它就是一个专门做 摘要或者是压缩的模型读过过去的历史纪录 只把最关键的部分 抽出来 当作语言模型的长期记忆接下来语言模型在继续 跟环境互动的时候 太久远的东西就不要记得了太久远的东西 它只记得个大概 只记得个历史纪录 再继续去跟环境进行互动这样就可以避免输入 你的 context 有太长的这个问题那这种压缩 有很多种不同的变形 你可以做这种 递迴式的多次压缩比如说你可以设定说 语言模型跟环境 每互动 100 个回合就压缩一次或者是说 今天只要这个 context window 里面 90% 被塞满了就压缩一次 你可以设定说 每固定一段时间 就压缩一次内容所以假设这边设定 每互动 100 次 就压缩一次内容 那这边有个错误 我这边是想要写 1不然就写成 101 了 那每互动 100 次 就压缩一次内容 得到第一次压缩的结果再继续互动 100 次 然后再根据过去的历史纪录 还有接下来互动 100 次的结果合在一起 一起做压缩 那你可能会得到 第二次压缩的结果那第二次压缩的结果里面 其实有第一次压缩的结果 再压缩然后模型再根据 第二次的历史纪录 再继续跟环境互动那你过远古的记忆 就会慢慢的 随著时间 随风而逝就会慢慢的消失 那其实我有点怀疑这个 ChatGPT它可能就是有用到 类似的技术 是这样子的 ChatGPT 其实有两套记忆系统其中一套记忆系统是 当你告诉它 要把某件事记下来的时候它会很明确的 把你要它记得的事情 写到一个笔记本上然后你可以在后台 看到那个笔记本上面 写了哪些字 这些是它印记的东西永远不会消失 除非你真的去改那个 笔记本的内容 但它有另外一套记忆这套记忆 是会随著时间慢慢消失的 这套记忆里面记了很多东西 然后你没办法看它 你没办法直接读它 如果你要知道它的这个会随风而逝的记忆里面 现在有什么 你可以问它说 你记得了什么事情 然后它就会告诉你它记得了什么事情 然后它记得的东西 是会变动的 越近的事情记得越清楚越久远的事情 它就越不记得 这个可能就是用了 类似这一张投影片上面的这种 recursive 的压缩技术 然后你要怎么清除它内部它的这个 会随风而逝的记忆呢 没有办法直接透过某个介面去改它 你只能跟它讲说 我不希望你记得某件事 我不希望你记得我有上过深层式 AI 导论这一门课 它说好 那我把它从你的记忆清除 那这个清除有没有用呢有时候有用，有时候没有用 非常的神奇 就是这样子 好，这只是对于 ChatGPT 背后使用记忆的猜测那为什么压缩内容有用呢 压缩内容会不会很容易 丢掉重要的资讯呢其实很多时候 机器跟 其实很多时候 agent 跟环境的互动会产生非常多琐碎的 本来就不应该被存下来的内容 这尤其在 computer use 的时候这种现象尤其明显 你想想看 computer use 是怎么进行的 假设你要叫模型 去订个餐厅打开餐厅的网页 滑鼠移动到某个位置 然后再看到网页有点变然后滑鼠点右键 然后这个时候可能 突然弹出一个广告讯息 所以模型还要知道 让滑鼠移到某个地方按下 XX 把这个广告讯息弄掉 所以这些东西 是非常值得被进行摘要的对于 computer use 订位 这一连串非常冗长的互动 对于未来的互动来说完全可以就浓缩成一句话 比如说 A 餐厅订位成功 9 月 19 号下午 6 点 10 个人对于一个 AI agent 来说 它完全不需要记得 context 里面订位的细节比如它完全不需要记得说 曾经弹出一个广告视窗 然后按下 XX 以后就不见了这些是不需要存在 context 里面的 所以 context 往往有非常多 琐碎的内容尤其是在叫模型使用工具的时候 更容易产生大量琐碎的内容 而这些琐碎的内容是应该被压缩起来的 那有人可能会想说 如果我们做摘要还是有可能会丢掉一些关键的资讯 那如果你担心这件事的话不然你可以把你摘要前的内容 全部放到一个可以长期储存的空间日后可以让它用 RAG 再读取出来 那因为 context 是有限的但是 hard disk 这个硬碟可以说是无穷的 所以不想放到 context 里面的东西你永远可以就把它放到硬碟里面 去储存起来 日后用 RAG 的方式再把它读取出来但是你可能会担心说 RAG 还是有可能会犯错啊 如果今天硬碟里面有非常大量的资料会不会用 RAG 没办法准确的 检索出我们要的内容呢 那如果你担心这件事的话你甚至可以在摘要里面留一个纪录 所以有一些 agent 会做这样的事情它做完摘要之后 它会留一句话在摘要里面 如果你想要知道更详尽的内容其实打开这一个 .txt 档 里面有你那年夏天美好的回忆如果今天这个模型是一个 能够打开档案系统的模型 它可能就可以在需要知道夏天美好回忆的时候 去打开这个 .txt 档 就可以避免你用 RAG有可能会搜寻错误的问题 那最后一个要跟大家分享的常用的 context engineering 的套路呢 是 multi-agent 那今天你很有可能在各处都看到很多使用 multi-agent 的系统 那我这边用的图呢 是来自于一个叫做 ChatDev 的系统ChatDev 呢你甚至可以把它想像成是一个 软体公司 里面有好多 agent每个 agent 都各司其职 有的 agent 呢 它是扮演 CEO 负责下指令还有 CTO 还有 programmer 还有 reviewer 还有 tester这个系统里面有各式各样的 agent 每个 agent 都负责一部分的工作那 multi-agent 为什么会有用呢 从一个角度来看是 每一个 agent 有它自己擅长的事情有的 agent 就是特别擅长写程式 所以它应该当 programmer 有的 agent 它不会写程式只会嘴炮所以它就当 CEO 等等 但是除了每一个 agent 有不同专长的事情之外从 context engineering 的角度来看 multi-agent 其实是一个有效 管理 context 的方法什么意思呢 我们先假设现在你只有一个 agent 那这个 agent 呢是要负责组织出游的 agent 你就告诉它 我们现在呢有多少人我们打算要出去玩几天 它帮你做一个出游的规划 好，那你就告诉它我们要出去玩 然后它帮你规划行程 规划完行程以后呢它还要帮你把行程安排好 所以它去订餐厅 那订餐厅 它可能就需要用到computer use 的功能 它需要在一个萤幕上 跟餐厅网页做一番的互动之后才订到餐厅 订好餐厅以后还没有完 还得去订旅馆 跟旅馆网页一番互动那你知道每一个 agent 它的 context window 都是有限的 没有 agent 它的 context window 是无限长的所以在做了大量的 computer use 这样的互动之后 你的 context 很快就会被塞满很快模型就会因为 context 被塞满 再也无法正确的做任何的事情那 multi-agent 怎么解决这个问题呢 假设现在呢我们有三个 agent其中一个是跟人类互动的 领导者的角色 剩下有两个 agent 它就在后台只有这个领导者的 agent 叫唤它们的时候 它们才会出来做事而这个人类呢 就跟领导者的 agent 说要组织出游 它做完规划以后不自己去订餐厅 它跟第一个 agent 说去订餐厅 它把第一个 agent 当作一个工具给它下一个指令说去订餐厅 第一个 agent 收到了去订餐厅的指令以后它跟餐厅网页一番互动 然后回报说订好了 当总召的这个 agent它只需要在它自己的 context 里面 写一句话说餐厅订好了 接下来继续去做文字接龙它可能接出说 让第二个 agent 去订旅馆 第二个 agent 收到去订旅馆的目标它跟旅馆网页一番互动 用 computer use 然后回报订好了 然后总召就知道订好了它就知道说现在餐厅订好了 旅馆也订好了 那像这样 multi-agent 的设计是跟 context engineering 怎么扯上关系的呢 如果你观察每一个 agent 的 context 的话你会发现说 当总召的那个 agent 它的 context 中没有订位的细节它负责大方向 它只要掌管最后的规划 它只要知道什么东西订好了没有什么东西还没有订就可以了 对于去订餐厅的 agent 而言 它完全不需要知道订旅馆相关的事情对于订旅馆的 agent 而言 它完全不需要知道订餐厅相关的事情 每个人的 context 里面都只放了有限的资讯 所以这是另外一个 multi-agent 可能会带来帮助的角度就是就算你今天没有不同的专长 agent multi-agent 的设计从 context engineering 的角度来看 也有可能会带来帮助 今天就算是 leader 跟 agent 1、agent 2它们各自没有特别的专长 agent 1 没有比总召的 agent 特别会订餐厅或者是旅馆但是总召叫 agent 1 订餐厅还是有好处的 因为对于总召来说 它就是把 context 放下来把订餐厅的 context 放下来 交给另外一个 agent 去执行 它可以让它的 context 比较短它可以更有效率的处理 context 里面的内容 所以今天就算是你没有多个不同专长的 agent其实 multi-agent 这样的设计 也有可能带来帮助 那这边再举另外一个 multi-agent可能带来帮助的例子 今天很多时候在学术界 我们会写 overview paperoverview paper 是什么意思呢 overview paper 就是你去读过这个领域 大量的上百篇、上千篇的论文以后为这整个领域下一个总结 告诉大家说这个领域发生了什么事情那写 overview paper 往往是一个非常大的工程 这不是一两个人可以完成的 通常你要组一个团队 每个团队的成员来自于不同的 institute然后集合好多人的力气 才有办法写一篇 overview paper 那今天你当然有可能有机会直接叫一个 AI agent 来写 overview paper 但是 AI agent 如果要读上百、上千篇的论文再写出一篇 overview paper 当它的 context window 里面 有上百、上千篇的论文的时候可能会超过 context window 的上限 可能会让模型没办法好好的撰写 overview paper但是如果你用 multi-agent 的设计的话 有什么道理不同的论文 一定要放在一起读呢有什么道理不同的论文 要被放到同一个 context 里面呢 我们能不能每一篇论文分开读每一篇论文由某一个 AI agent 来读 这些 AI agent 甚至不需要是不同的 AI agent它们可能参数的是一样的 我们只是不要把多篇论文 放到同一个 AI agent 的 context 里面所以第一个 AI agent 它只有蓝色论文的 context 第二个 AI agent 只有绿色论文的 context 第三个 AI agent 只有紫色论文的 context每个人只需要读一篇论文 然后把它写成摘要 然后再把摘要全部集合起来然后再交给最后负责撰写 overview paper 的 AI agent 把 overview paper 写出来其实人类的分工也差不多就是这样 我觉得这个 multi-agent 的分工其实跟人类社会的分工也非常的类似 然后在实际上呢multi-agent 当然可以带来很大的帮助 那下面这篇 paper 是来自于 LangChain 这一个新创的文章他们里面就试了 single-agent 跟 multi-agent 所带来的差别那他们的发现是这样子的 纵轴代表解任务时候的表现当然数值越大越好 横轴你就想成是任务的难度那蓝色这一条线是 single-agent 的表现 single-agent 其实在任务比较简单的时候反而表现是有可能比 multi-agent 好的 但是当任务非常困难的时候 single-agent 就没有办法有好的表现因为 single-agent 的优势其实是 所有的 context 都由某一个 agent 所主管那当你有 multi-agent 的时候 就跟人类的组织会有一样的问题 就是 A 不知道 B 的讯息B 不知道 A 的讯息 订旅馆的不知道订了哪家餐厅 订餐厅也不知道订了哪家旅馆 所以可能餐厅跟旅馆很远所以你交通非常的不方便 这是有可能的 但是如果任务非常复杂的时候single-agent 就会吃不消 就没有办法好好处理 那这边论文里面其实列了 两种不同的 multi-agent 的设计那其实 multi-agent 有非常多不同的互动方式 这个就留给大家自己研究 那发现说在比较复杂的任务的时候multi-agent 是可以占到巨大的优势的好，那以上就是今天想要跟大家分享的内容 我们就是跟大家讲了 context 里面要有什么然后为什么需要 context engineering 以及 context engineering 的基本套路那以上就是今天想跟大家分享的内容