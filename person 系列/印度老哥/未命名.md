(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

Hello everyone and welcome to this playlist which is titled Build DeepSeq from Scratch. My name is Dr. Raj Dhandekar, I graduated with a B.Tech in Mechanical Engineering from IIT Madras in 2017 and then I completed my PhD from the Massachusetts Institute of Technology in Machine Learning and I graduated in 2022. After that I returned back to India and I am one of the three co-founders of Bijuwara whose main mission is to make Artificial Intelligence accessible to all. 

The main reason why I am starting this playlist is of course because DeepSeq is what everyone is talking about right now. There are blog posts written about it, there are flashy news headlines about the topic, everyone seems to have an opinion about DeepSeq. Researchers, engineers, even companies are migrating from OpenAI API calls to DeepSeq API calls. 

People are just going crazy over how China could have built an open source model which is at par or even better than GPT. However, what I strongly strongly believe is that all engineers, students, researchers and working professionals are making a very very big mistake in this DeepSeq wave. Of course DeepSeq is a revolutionary movement but instead of building flashy applications using DeepSeq, what we should instead be focusing on is the foundations, the fundamentals. 

What we should really try to understand is how did they build DeepSeq from scratch. We should understand the nuts and bolts of how DeepSeq is really built and we should try to build it ourselves. That is what we should be doing. 

Behind all the flashy headlines, behind the competition between companies, what we should be doing is understanding the basics of how DeepSeq was built from scratch. That is where the true focus should be and that's the main purpose of this playlist. Through this playlist, I am planning to record a massive series of 25, 30 or even more videos in which I will teach you every single building block of the DeepSeq architecture and by teach I don't mean just showing you blogs and showing high-level things but we will be coding and deriving the mathematical details of every single aspect.

By the end of these video lectures, you will be able to build every single DeepSeq module yourself. You will be able to build the architecture and the modeling aspect also. In this playlist or in this video rather, I plan to go over three major things.

The first thing is I'll take you through the journey of how I tried to discover learning material with respect to DeepSeq and I just couldn't find learning material out there. Either I could find flashy articles or blog posts which are not very very deep and then I was just lost and then I'll talk to you about my process of understanding the entire journey of building DeepSeq. My second objective with this video is to show you how I have divided the different aspects of how I will make this playlist and in the final aspect, I will show you what you can achieve after you go through this entire playlist. 

I am pretty sure there are not too many engineers in the world right now who can build every single aspect of DeepSeq themselves and understand the mathematics of what's going on beneath and by mathematics, I mean showing and deriving every single matrix multiplication. So you will be that engineer once you complete this playlist. You will be a very strong foundationally oriented engineer who can build a DeepSeq model or a DeepSeq architecture. 

Why wouldn't anyone want to have such an engineer in their organization? I will equip you with this knowledge through this playlist. On the Vijwara channel, we already have a number of playlists on machine learning, deep learning and building large language models from scratch. I hope you thoroughly enjoy this series as well. 

So let's dive into this video where I'll cover the three aspects which I mentioned. So the first thing I want to get started with is my journey into how I reached this point of wanting to build every single aspect of DeepSeq from scratch. It all started with this DeepSeq R1 release and at that time I checked this out and this looked awesome but very honestly at that time so many new models were coming out every single week that I thought this was one such wave which will die out pretty soon. 

So I saw this release, I checked out their website but I did not look into it further after that but then things just started to get pretty crazy after that. So here was a LinkedIn post which was released which said that DeepSeq had erased 2 trillion dollar worth of market cap in US stocks. Companies in the US lost a huge amount of money and the reason was because China's DeepSeq built a reasoning AI model that rivals OpenAI's best model and that too it did it for one thousandth of the cost and more than that it was open source. 

DeepSeq was not just cheaper to build, it was also cheaper to run. So this was more than technical, it was also a political thing right. So had Silicon Valley lost its AI advantage, DeepSeq raised all of these questions. 

It was around 27 times cheaper than OpenAI. It was open source and permissively licensed. It's just amazing. 

It led many countries to act also after that. So for example India released a proposal which was a call for building the nation's first foundational model. So people were suddenly like if China as a country can do it with few resources compared to the US why not other countries also. 

So this started this debate also and then I got really curious about this and I started searching a lot about DeepSeq. I always like to build things from scratch. There is this build LLM from scratch playlist which I released which has now around 40-45 videos and it has received a huge amount of interest for people and I saw this one post where someone had tried to replicate DeepSeq R1. 

It was just a small tutorial but this ignited the idea in my mind that why don't I try to build DeepSeq from scratch or why don't I at least understand how every single module of DeepSeq was built from scratch and then I started doing a bit of research right and I realized that DeepSeq R1 was not the first thing which came out of this company. In January 2024 they had this DeepSeq LLM paper then in Jan 2024 again they had DeepSeq coder then in March of 2024 they had a vision language model. In April of 2024 they had DeepSeq math which was related to mathematical reasoning in open language models then came DeepSeq version 2 in June 2024 then DeepSeq coder version 2 in June 2024 then finally came along DeepSeq version 3 and this really blew everyone's mind because this was the foundational model which ultimately led to this paper called DeepSeq R1 which only came out in January of 2025. 

So there was one year of research or maybe more than that to reach to this stage where it finally came into the public limelight and all of these innovations which happened in these papers right all of these papers I really wanted to dig deeper and I wanted to understand. I knew exactly how LLMs were built from scratch but DeepSeq was a revolution on top of the foundational LLM models and the architecture which we have traditionally seen so I wanted to learn everything about it and naturally I started searching on youtube first related to DeepSeq from scratch then I could see here's a 20 minute video which is around 1.5 million views here is a video which is again 21 minutes and it has huge number of views but you can see that all of these videos are either 10 minutes or 15 minutes this is another video which is just 8 minutes this is not what I was looking for at all I wanted to know the nuts and bolts of every single piece of how DeepSeq was built and assembled from scratch it's as if I were to build a sports car all by myself and I'm just being shown a five minute version of how a sports car looks like that's not what I was looking for I wanted a video which explains the mathematical details which explains the code from scratch and which takes me through the building blocks right similar to the way I did for build LLM from scratch but I just could not find anything on youtube then I started searching on google I could see that several people are building apps using DeepSeq that is fine building apps with something already existing is cool but I really don't want to do that I want to be able to build DeepSeq on my own that's where the real power lies in my opinion not in building applications on top of something which already exists then I searched on internet again build DeepSeq from scratch and I could see several forums several articles like the ultimate guide to DeepSeq R1 but here again it already uses DeepSeq R1 builds apps on top of that so this was not what I was looking for there were reddit posts but this was again not very useful or impactful at all so then I decided that this time I need to go my own route and I need to do my own research I need to do my own study and I need to build that first video playlist which teaches DeepSeq fully from scratch and the last aspect of this playlist will be recreating something like this mini R1 but to get to this stage we need to explain every single thing in detail and then I started diving into understanding these papers I made a huge amount of detailed notes on these papers and all the previous papers which I showed you I already had the knowledge of standard LLM so that really helped me in my research I made notes in all I spent about I would say the last three weeks I have spent about 10 to 12 hours every day understanding everything about DeepSeq making notes and ultimately I came up with this plan of how to divide the DeepSeq architecture and how to teach it to all of you so as you can see I've divided it into chapters and I'm also planning to write a book on it the first is of course DeepSeq overview and you will see that the chapters or the videos will be divided into two parts architecture and modeling DeepSeq has a huge number of revolutions in their architecture and these small things added up to contribute to why DeepSeq is so good right they had something called mixture of experts they had something called multi-head latent attention they implemented quantization they had something called multi-token prediction all of these things are not easy to understand if someone just writes a one-page report you cannot build a mixture of experts model from scratch multi-head latent attention is a topic on which I have made notes of 50 pages and only then I have been able to understand it in fact I can show you some of these notes which I've made in Canva if you go to the multi-head latent attention part you'll see that these are the kind of notes which I have made for every single aspect of this so this is what understanding from scratch means right going down to the matrix multiplication details and I'll be teaching all of this to you in this course so that's the architecture aspect of it the second aspect is the modeling where we have to learn about reinforcement learning dpo ppo and grpo group relative policy optimization which is one of their major things which they implemented to create the reinforcement learning pipeline we have to learn about the modeling aspects step by step and only after we learn about all of this then we will truly know how every single element of deepseek is assembled from scratch and towards the end of this playlist we'll build something like this as I mentioned we'll recreate deepseek r1 and I'll show you how to run it on gpus as well as I mentioned I'm parallelly working on a book where I mean I'm mentioning or taking down all of these notes and I'll also plan to make this book public along with the videos but that will come one or two months later and simultaneously here are the notes with design notes which I've made I plan to make this course as illustrative as possible because from blog articles it's very difficult to understand text but I think if all of you understand visually I'll be able to explain concepts in a much better manner so I've spent a lot of time organizing this it takes a huge amount of time and effort on part of our entire team but the end goal is to help you build every single thing here from scratch so when I show you MLA I'll multi-head latent attention I won't just explain the theory after theory we'll go into code and build the multi-head latent attention module from scratch and we'll compare it with the traditional multi-head attention without the part similarly we'll do for mixture of experts and all other things here we'll understand the reinforcement learning methods fully from scratch so overall this course you can expect to be around 20 to 25 hours or it can even be more than that and I'll try to release lectures as frequently as possible but it takes time for every single video I have to curate notes like this and then put it out there so that all of you can understand but the reason I made this introductory video is to share the plan of what this lecture series is going to be all about what you are going to be learning this is what you are going to be learning so what all other people are doing is that they are only focusing on these three parts right which are running deep seek models with API calls building rack based chatbots with deep C or building LLM applications with deep C that's also fine we'll learn that but that's probably five percent of the effort the 95 percent of the effort will be to teach you these the architecture and the modeling because once you know the architecture and modeling the applications will seem straightforward and easy to you that way you'll truly become a strong LLM or a machine learning engineer so again thank you so much everyone and I look forward to starting this journey of building deep seek from scratch with you see you hello everyone and welcome to this next lecture in the build deep seek from scratch series today my main agenda is to cover three things first we are going to look at what exactly is deep seek second we are going to look at what makes deep seek so special and why is everyone talking about it what are the technical details which make deep seek so special and the third thing which we are going to look at is what is the plan in this lecture series what we are going to learn and what is the sequence in which we are going to learn about different topics so let's get started first what is deep seek essentially deep seek is a Chinese company which builds large language models and before we dive any deeper first let me give you a quick overview of what large language models essentially are all of you would have interacted with chat gpt right i can go to chat gpt right now and let's say i can ask that make make a travel plan for me to visit italy and then chat gpt will come up with this travel plan this is a large language model but one thing to keep in mind is that at the heart of it what a large language model is is that it's an engine which takes in a sequence of words and then it gives a probability of what's the most likely next token and when such tokens are aggregated together it forms sentences such as what we see here right now so essentially large language models are probabilistic engines for predicting the next token this is the first thing to keep in mind here's a simple code which i wrote to demonstrate that the models and their prediction are probabilistic in nature so for example if the sentence is after years of hard work your effort will take you and we have to complete this sentence i use the openai api key and i ran this code which predicts the top 10 tokens and their probabilities so although the output will be 2 which is essentially or which essentially implies that the next token is 2 after years of hard work your effort will take you 2 but you will see that there is also some amount of probability which is assigned to tokens like far places where etc so although you see these answers over here and they look deterministic it seems that chat gpt is very confident in giving these answers just keep in mind that underneath for every single token there is a probability and we choose the next token with the highest probability usually the distribution of the next token probabilities look something like this on a log log scale it looks something like this the first token generally has very high probability in the case of confident predictions and then the predict and then the probability just keeps falling down all right so that's the key idea behind what large language models are and then what exactly is this large behind large language models to put it simply no one really has an exact definition of what large is but essentially there is a scaling law with size the first paper which kind of figured this out was the gpt3 paper let me quickly show that paper to you so here's the gpt3 paper which was titled language models are few short learners where the authors essentially proved that if you increase the model size to something as high as 175 billion parameters for both one shot and few short learning the model performance essentially dramatically improves i believe that this was when we truly crossed the size barrier from 1.3 billion parameters to 13 billion parameters to ultimately 175 million parameters and once this size barrier is crossed we start seeing wonderful properties of large language models so this was gpt2 gpt2 the largest model had around 1.5 billion parameters gpt3 the largest model had around 175 billion parameters and in general here is a paper which was published in about neural networks and size so from the 1950s to 2020 you can see that there is an exponential increase in the number of parameters which we are using and recently you see the orange dots are more this is because language models have really dominated the size space so essentially language models and their size has been drastically increasing and we have reached about 1 trillion right now so look at this straight line here which i have shown by this red arrow so the y-axis is in log scale so this is an exponential scaling log which means that the size of the language models is increasing exponentially why do we really care about size so much and why do we increase the size well because people have seen that as you keep on increasing the size of language models you observe something called emergent behavior or emergent properties these are essentially properties which are not present in smaller models but they are present in larger models so for example if you see all of these tasks here such as performing arithmetic word unscrambling tasks etc all of these tasks you see there is a pickup point in all of these and on the x-axis you can think of this as computational power or roughly equivalent to the model size so if the model size increases beyond a certain point we have this pickup point so the model suddenly starts learning about new things the model starts developing these magical awesome properties although the model is trained simply on the next token prediction task as the size of the llm goes on increasing after a specific size after a specific size the model can do these wonderful properties or rather the model shows these wonderful properties like could be translation summarization grammar checking etc and that's why we are in a race to build larger and larger and larger models companies like open ai anthropic have even publicly said that they are just chasing size at this moment because there is still some hope that maybe after 10 trillion or after 100 trillion number of parameters the llm shows some properties which we are not aware of at all right now so these are the emergent properties of large language models one thing to note is that llms differ from earlier nlp models because earlier nlp models were essentially designed for specific tasks such as language translation whereas due to such emerging properties which we just saw large language models can do a wide range of tasks such as translation summarization fact checking grammar checking etc as all of you might have explored with jpg right just a key point to note earlier language models could not even write an email from custom instructions a task which is trivial to modern large language models so until now we have seen that large language models really become better and better with size they develop emergent properties and one more thing to note is that at the heart of this language revolution rather is this architecture which is known as transformers if you don't know what transformer architecture is don't worry we'll cover that in this series but essentially there was this paper called attention is all you need which introduced a transformer architecture it looks a bit complicated as shown in this diagram to truly unpack the transformer architecture it takes a couple of lectures but essentially this is the secret sauce which powers language models and we are going to learn about this so don't worry um finally one thing which i want to mention is that when we say creating a large language model it involves two stages the first is a pre-training stage where we don't have a label data set but essentially the model on its own creates training data and the labels this is called as an auto-regressive stage so models which are pre-trained they are also called as models so to do this pre-training we typically assemble huge amounts of data from internet textbooks media research articles etc for example gpt2 was trained on data from reddit books wikipedia articles open web corpus etc and then this whole this giant large language model is trained on this huge amount of data this training costs upward of million dollars might even cost tens of hundreds of millions of dollars as the size of the llm increases keep in mind that after pre-training the model develops basic capabilities and after that we typically need to fine-tune the model so that's the second stage we fine-tune the model with labeled data set so for example you could teach the model to translate by giving it some labels of how translation usually proceeds you can teach the model to follow instructions by giving some instructions such as hey convert 45 kilometers to meters and then the answer is 45 000 meters that's a labeled data which you give to the model so it learns to follow instructions gpt 3.5 which became the product chat gpt it was trained with reinforcement or rlhf which is reinforcement learning human feedback so essentially there were human annotators which graded the output and that was passed back as a feedback to the element this this is very important for us because this fine tuning is a stage where deep seek really changed the game completely and we are going to come to that when we see what makes deep seek so special but to understand what is to follow after or after this section it is very important for all of us to be on the same page with respect to what are llms what are emergent properties what's the secret sauce which powers llms and most importantly the two stages involved in creating an llm and that is pre-training plus fine tuning with that let's go to the next section which is essentially what are the llms built by deep seek and how did it get so popular so let's take a look at the different llms which are built by deep seek so if you go to their website which is deepseek.com and if you scroll down to the bottom in the research section you will see the different versions of the llms which they built they first started with a simple deep seek llm then the significant milestones was building of this deep seek version 2 then they have deep seek version 3 and then after that came deep seek r1 they released papers for each of these so for example here you will see the paper for deep seek origin 2 it's a 52 page paper here you will see the report for deep seek version 3 again 53 page paper and here you will see the paper for deep seek r1 my goal in this series is that there are several amazing things which they have mentioned in these papers related to the architecture related to the training etc i'm going to unpack all of that and i'm going to break it down into modular lectures so that you don't have to read this report but rather if you just go through these lectures you will understand the nuts and bolts of what is happening the main model which caught everyone's attention though is the deep seek r1 because as we'll soon see deep seek r1 was a reasoning model which achieved comparable performance to openai's top model that too at a fraction of the cost and plus it was open source this were like two amazing things at the same time right deep seek r1 which was released in january of 2025 it was a reasoning model but it had a comparable performance to openai remember openai is closed source this model was fully open source which means you could literally download the model and run it locally if you have a big setup and secondly the api cost to this model is a tiny fraction of what the latest openai model costs and that's pretty amazing if you put these two things together open source plus low cost that's pretty awesome right taking a look at the number of parameters as we saw the size scaling law language models get better with size and deep seek was no exception deep seek version 3 has 671 billion parameters and then deep seek r1 was a reasoning model which was essentially constructed after deep seek version 3 foundational model was built so version 2 and version 3 are different foundational models which they have and r1 is the reasoning model which came from deep seek v3 so keep in mind this progression they first had the version 1 which they called deep seek llm and then they made deep seek llm math deep seek llm coder then they had deep seek version 2 and then deep seek version 2 coder then we had deep seek version 3 and finally

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

which really broke the internet and because of which we are having this lecture series at this moment. All right, so until now we have seen about LLMs, we have seen about the different LLMs which were built by DeepSeek. Now let's start getting into the core content a bit which is first of all let's compare DeepSeek with other AI models and let's see is it better, why is it better, how low is its pricing etc. 

So what I actually did was I am very fascinated with this mathematical question because it's an amazing problem. Here is an integral from 0 to 1, x raised to 4 multiplied by 1 minus x raised to 4 divided by 1 plus x square. The cool thing about this problem which completely blew my mind when I solved this for the first time is that the answer to this problem is 22 by 7 minus pi. 

Pretty awesome right. First of all many of us are already confused that oh I thought pi is equal to 22 by 7 but that's not the case. 22 by 7 is actually greater than pi by a tiny amount and this integral captures that amount. 

It's a beautiful mathematical puzzle which was by the way also introduced in the 1968 Putnam competition which is known to have very hard problems. The proof was first devised in somewhere in 1944 and again this is a beautiful relationship right. So what I did is I went to DeepSeek and chat GPT and I asked them to solve this. 

I went to GPT 4.0, I gave it this integral and I asked to choose the correct answer. Remember option A is the correct answer right. Let's see what GPT did. 

As you scroll down below it says that the correct answer is 2 by 105 which is option B. So it spectacularly failed. Now I went to DeepSeek and I asked the same thing solve this. DeepSeek did this step by step and I got to the correct answer which is 22 by 7 minus pi. 

Although this is just one example and by no means from one example we can compare and contrast the two but this just shows that DeepSeek is pretty awesome at difficult problems. It got to the correct answer and by the way they did it like 10 to 15 seconds. All right so now let us do a bit of formal comparison of DeepSeek with other AI models. 

First of all let's compare DeepSeek with GPT 4.0 right. In terms of performance this is still debatable and it's hotly debated on several reddit forums but generally the consensus is that DeepSeek is similar or superior to GPT 4.0 on several tasks. But still performance wise let's say you consider them to be at the same level but the huge difference is with respect to cost right. 

So let's say if I'm invoking a DeepSeek versus a GPT model. If you look at the pricing per million tokens GPT 4.0 is around 30 dollars and DeepSeek R1 is like 0.55 dollars. It's literally a fraction of the cost. 

In fact you can even see the pricing here on the x-axis its evaluation score or an equivalent of evaluation score rather and on the y-axis I have pricing. What you see clearly is that if you compare GPT 4.0 and if you compare DeepSeek version 3, DeepSeek version 3 has much better performance and it's reasonably priced with respect to GPT 4.0 mini but now if you compare DeepSeek version 3 with GPT 4.0 you will see that DeepSeek version 3 has much higher performance on the y-axis it's higher and it has significantly lower cost if you compare DeepSeek version 3 and GPT 4.0. In fact DeepSeek version 3 seems to be vertically at the top right which means its performance is really very good and it's also at the leftmost side compared to all of these other models which indicates that it's also relatively cheap and that's what the community was extremely excited about. Here we have a highly performant model which is very cheap and you know what's the awesome thing this is fully open source. 

There is always a tension between open source and closed source models but here is an example of a model which has finally reached the gap. We have an open source model which is equally performant or even better than GPT 4.0 which you can invoke or you can make an API call at a fraction of the cost. In fact if you go to DeepSeek website and if you look at the different parameters you'll see that DeepSeek version 3 outperforms GPT 4.0 on almost all of these parameters which have been considered earlier. 

So the main thing is that in terms of pricing DeepSeek completely outweighs GPT 4.0 because it's literally a fraction of the pricing which is there and most importantly Deep Seek is open source whereas GPT 4.0 is closed source which means you can literally download DeepSeek, host it if you have a GPU or if your machine is big you can host it and run it but remember it's like 671 billion parameters so it's not quite easy to do this. Secondly if you compare DeepSeek with Lama for example remember Lama is a host of open source model which have been released by Meta. Amazing models but they are not quite as performant as DeepSeek. 

DeepSeek has great scale and performance its foundational model has hundreds of billions of parameters and it has strong results which really exceed what Lama 70 billion parameter can So if you look at this plot again there is also Lama here right and if you see in terms of performance DeepSeek really outweighs if you see on the y-axis DeepSeek outweighs all of the Lama models it outweighs the 70 billion instruct the 405 billion instruct because it has 671 billion parameters not just that it has a lot of other innovations in the architecture which we are going to see in a moment mixture of experts reinforcement learning training multi-agent attention multi-token prediction quantization in the input etc all of this really give DeepSeek an edge over Lama. All right so this is with respect to comparison of DeepSeek with GPT-4 and with respect to Lama so in terms of strength and weaknesses DeepSeek is pretty awesome with respect to cost efficiency its performance is also quite good and it's open source these are the three biggest strength but the biggest weakness is that it might not be as polished or as safe as GPT-4. In my opinion with further origins this might get tweaked so DeepSeek might actually become safer or more polished but right now that is a bit of a concern for big corporations to really implement this. 

Secondly it's 671 billion parameters right so let's say if you are an organization who does not want to use open ai because you have to make an API call so you're not comfortable with your data going somewhere else so you decide to download and host this model locally that might take computational resources because it's a pretty large model 671 billion parameter is not tiny not easy to deal with so you need to figure out the computing infrastructure for this but it will give you data privacy if you host it on your own server let's say as an organization you must weigh these factors right if you really care about safety guardrails maybe stick with GPT-4 for now and soon change to DeepSeek if you're a lean fast-growing startup go with DeepSeek because it will tremendously cut down costs it is highly performant and of course it's open source if you want privacy which means if you don't want your data to be going to closed source companies again then that could give the open source nature of DeepSeek an advantage for you now let's come to the main point which is the next section of this lecture what makes so special or what is so special about DeepSeek how is it able to literally charge people so less how does it achieve so much cost efficiency and still be competitive in performance with GPT-4 so i believe there are four major things which we need to talk about here the first is that DeepSeek has an innovative architecture second is that the training methodology is very creative and innovative third is that they have implemented several GPU optimization tricks and fourth is that they have a model ecosystem which favors distillation etc we'll see about all four of these first of all in the architecture itself i believe that DeepSeek has done the following five things right which make it truly an innovative architecture first they have something called multi-head latent attention they have a mixture of experts model then they have multi-token prediction then they have quantization and finally they have rotary positional encodings we are going to learn about all of these in detail in fact going into each one of them itself will take two lectures there is very limited information about all of this on the internet and it's just you need to spend a lot of time on each of these to truly understand the architecture innovation here let me give you a quick flavor of what each of these actually mean right so if you go to the original attention mechanism which i showed you in this paper which is attention is all you need there was the multi-head attention mechanism which looks something like this which looks something like what you're seeing in the figure right now this is the multi-head attention mechanism which was there now DeepSeek what they did was they introduced something completely different in order to make sure that the attention mechanism is implemented effectively they had a key value cache but instead of a normal key value cache they had a key value cache in a latent space don't worry if you don't know what these terms mean right now we will we will cover all of this in subsequent lectures but just make sure that or for now you should understand that they did this special type of caching special type of key value caching in the latent space so that the attention mechanism computation becomes much more efficient it takes up less space and it's computationally also fast that's one change which they made so this was the multi-head latent attention which was the first thing which we discussed the second thing was with respect to mixture of experts so if you see the normal attention mechanism it looks something like this right there is a feed forward there is the attention layer and followed by a feed forward neural network after the attention layer and the feed forward neural network looks something like this in a mixture of experts model what you essentially do is that you have four experts the entire neural network is not activated at once only parts of the neural network are activated so there is a special routing mechanism which which actually decides which part gets activated and which part does not get activated this is the router which essentially decides which expert is going to get activated which expert is not going to get activated we are going to learn about all of this also in a lot of detail this is another key innovation which they implemented third is multi-token prediction as we saw at the beginning of this class that usually we just predict a single token right they implemented a new thing which was discovered in a paper released just last month that why don't you predict multiple tokens instead of one token what if that speeds up the process makes it more efficient then fourth thing is the implemented quantization so instead of representing every parameter as a large floating point number you just represent it in a slightly compressed manner the best way to think about quantization is like here right in the original image on the left hand side there are huge number of pixels whereas on the right hand side the image is just constructed out of eight colors so it's kind of pixelized so if you zoom in it's completely pixelized see here it's pixelized over here compared to the left hand side which is very sharp but if you zoom out you will see that it almost looks the same right so this is called as quantization and the implemented quantization in the parameters of the transformer block and then finally they have something called rotary positional encodings so let's let me show you the rotary positional encodings right now essentially in the attention mechanism which was published in 2017 they just added positional encodings to token encodings which polluted the embedding vector but soon after that people realized that why don't we just rotate the original vector the query and the key vectors to capture the effect of positional encodings so that will not change the magnitude and that's a highly efficient way to encode positions so instead of encoding positions in the token embedding itself we encode it a bit later in the query and in the keys so yeah this is with respect to the rotary positional encoding which which is the fifth key innovation which the deepseak architecture implemented we are going to look at all of this in a lot of detail now this was the first aspect which was innovative architecture but they did not stop there i believe the deepseak paper truly makes the field of reinforcement learning reborn because instead of just relying on human-labeled data like we saw for gpt 3.5 where humans created the quality of the data and that was fed back to the training process what they did is they use large-scale reinforcement learning to teach complex reasoning to the model and instead of human-labeled data they had a rule-based reward system so it was not relying on human judgment but purely it was a rule-based system through this they implemented a framework which is called group-relative policy optimization which is at the heart of the reinforcement learning training mechanism which they implemented we are going to learn about all of this which this is the main reason why deepseak r1 is such a good reasoning model so reinforcement learning is a major part of what we will be covering the third aspect is gpu optimization tricks now this is a bit tough to understand so i will not be spending too much time here even in the course but essentially what they did was instead of using cuda they used something called nvidia's parallel thread execution the simplest way to think about this is that in fact i asked at gpt what's the simplest way to think about this so if you think of cuda is writing python or java code ptx is like one lower below so it's an intermediate step before machine code execution and that just speeds up things a lot more so in high-level programming you are not at the machine code level right if you are at the machine code level it's the fastest like c or c plus plus whereas python or java takes you to a higher level that slows down things you can think of ptx as somewhere in the middle whereas cuda is at the higher level but ptx is at the middle a bit closer to the machine code execution so that speeds up things a lot more there was also a nice article published about this yeah deep six ai breakthrough bypasses industry standard cuda for some functions using nvidia's assembly line assembly like ptx programming instead so i believe that also played a major role in uh in speeding up um their or making their architecture a lot more efficient and which ultimately reduce the costs and finally they have a strong model ecosystem where although the main model is 671 billion they have distilled it down to smaller models even as low as 1.5 billion that makes it a pretty awesome model ecosystem we are going to look at model destination also so just to recap these four aspects which make deep six so special first is the innovative architecture second is the training methodology which is centered around reinforcement learning third is the bag of gpu optimization tricks which they have and fourth is model ecosystem especially distilling a from a larger model into smaller models as small as about 1.5 billion parameters now let's come to the last two sections of today's class first is why is deep six such a turning point in ai history and i believe it is a turning point because of the following reasons it is the first time that a small scrappy startup has reached parity with the best ai models using novel techniques and far less resources they have slashed down the development cost although it's not as low as six million dollars i believe it's still quite low compared to what big corporations like gpt or metas lama etc the cost requirements which they use to train the models so it's like it was the first proof that even small startups even companies which are not maybe as funded as open ai can build a large language model which is which is awesome right which performs at par which with gpt4 it and it does so with far less resources what also happened with this is that people got scared right investors got scared there was a huge dip in the u.s tech u.s tech stocks in january 2025 because of deep six advancement the main reason was the idea of a low cost open source chinese ai model threatened the profit models of open ai microsoft and even google and raised concerns about the ai supply chain and gpo markets one model or one company deep seek brought about so many changes and that's what makes deep seeker turning point in history one major thing which i believe has also happened because of deep seek is that developing countries such as for example india have started have heavily started investing into building their own large scale foundational models if china's deep seek can do it then why not other countries why only us and why only companies coming out of us rather right why not other companies with resources which deep seek used can build their own foundational models so all of this discussion has been started in fact the indian government has also released a call for building foundational models that's pretty awesome i think and it's one of the main motivating factors for me to create this series now let's come to the last section of today's lecture which is our plan for this lecture series we have developed or i have rather divided this lecture series into four phases based on what is the specialty about deep seek the first phase for us is going to be going into the architecture so first i'll start with the attention mechanism then i'll go into multi head latent attention mixture of experts multi-token prediction quantization and rotary positional recordings i am going to assume that you have some amount of knowledge of attention if not you can check the build llm from scratch series so this series is going to be a bit more advanced and it assumes that you you go through that previous series before i am going to start at a slightly higher level here then we are going to go in the phase two in the training methodology phase three gpu optimization text this will be a small phase i won't be having too many lectures here and then i'll conclude with lectures on distillation a bulk of the lectures will be on phase number one and phase number two and smaller number of lectures on phase three and phase four so this is the main plan which will be following for the series let me quickly summarize what we learned today first we looked at large language models and the fact that they are engines of probabilistic next token prediction we saw that size is a very important factor in large language models there is a size scaling law as the size increases the models get better and better they start developing emerging emergent properties which are not present in smaller models then we saw that the llm secret sauce is essentially transformer the transformer architecture and then finally we saw that creating an llm means building a foundational model which is essentially the pre-training stage and then we have a fine-tuning stage there are two parts then we saw that although deepseek r1 has become popular the deepseek company started long back they started with the deepseek llm first which is version 1 then they had version 2 and ultimately version 3 which was a huge model 671 billion parameter and then ultimately they made deepseek r1 which is a reasoning model and that broke the internet why did it break the internet because deepseek r1 has comparable performance to openai stock model and at a fraction of the cost plus it's open source so deepseek is equally performant as gpt4 their pricing is literally i think 100 to 500 times less as i showed you in this lecture and finally it's fully open source strength and weaknesses the biggest strengths of deepseek are that it's open source it's cost efficient and it has competitive performance so three big strength the biggest weakness might be that it's not maybe as polished or safe as let's say gpt4 another weakness is that if you're planning to deploy it locally or planning to use it securely you need to have infrastructure for downloading and using a 671 billion 671 billion parameter then we saw what makes deepseek so special and there are four key ingredients here the first is the innovative architecture training methodology gpu optimization and model ecosystem within the training or within the innovative architecture we have five key things multi-head latent attention mixture of experts multi-token prediction quantization and rotary positional encodings then in the training methodology we have the fact that they use large-scale reinforcement learning to teach complex reasoning to the model and they used a rule-based reward system which is also known as group relative policy optimization rather than relying on human label data in the gpu optimization tricks they used parallel thread execution pta instead of cuda only in some places i believe and then finally they have a strong model ecosystem where they distill their main model into smaller models as low as 1.5 billion parameters in this lecture series we are going to follow the same workflow we'll go with the first phase which is innovative architecture then we'll go to the second phase which is training methodology then we'll go to the third phase which is gpu optimization tricks then we'll go to the fourth phase which is model ecosystem i am going to assume a good amount of knowledge about llms and i will explain the attention mechanism again but i'll essentially start from the attention mechanism and then dive into the details if you're a complete beginner i recommend the build llm from scratch series first and then finally i believe deep seek is a turning point in history because they literally showed that even developing countries can build their own foundational model if we are smart about the innovative architecture if we are creative we can build a foundational model which is as good as let's say open as models and that too at a low cost and fully open source so they are truly democratizing ai that way so thanks a lot everyone and i look forward to seeing you during the next lecture thank you hello everyone my name is dr raj dandekar i graduated with a phd in machine learning from mit in 2022 and i am the creator of the build deep seek from scratch series before we get started i want to introduce all of you to our sponsor and our partner for this series in video ai all of you know how much we value foundational content building ai models from the nuts and bolts in video ai follows a very similar principle and philosophy to that of us let me show you how so here's the website of in video ai with a small engineering team they have built an incredible product in which you can create high quality ai videos from just text prompts so as you can see here i've mentioned a text prompt create a hyper realistic video commercial of a premium luxury watch and make it cinematic with that i click on generate a video within some time i am presented with this incredible video which is highly realistic what fascinates me about this video is its attention to detail look at this the quality and the texture is just incredible and all of this has been created from a single text prompt that's the power of in videos product the backbone behind the awesome video which you just saw is in video ai's video creation pipeline in which they are rethinking video generation and editing from the first principles to experiment and tinker with foundational models they have one of the largest clusters of h100s and h200s in india and are also experimenting with b200s in video ai is the fastest growing ai startup in india building for the world and that's why i resonate with them so much the good news is that they have multiple job openings at the moment you can join their amazing team i am posting more details in the description below hello everyone and welcome to this lecture in the build deep seek from scratch series in the previous lecture we learned about the four phases in which we are going to divide the lecture series the first phase is going to be the innovative architecture behind deep seek so that's phase number one right over here phase number two is the training methodology itself the rise of reinforcement learning and how they relied on rl to teach complex reasoning to the model using rule-based reward systems that's phase number two phase number three is gpu optimization tricks so how they used nvidia's parallel thread execution ptx or cuda let's say and phase number four is their model ecosystem itself so they did not stop at just building a huge 671 billion parameter model but that large model was essentially distilled down into a much smaller model whose size was around 1.5 billion parameters so that's essentially phase number four we are going to go through phase number one to phase number two phase number three and phase number four and in this lecture today we are going to start with phase number one which is essentially the innovative architecture behind deep seek and what makes it so efficient the two major aspects of their architecture which contribute to the deep seek efficiency is essentially something called multi-head latent attention mla which makes the attention mechanism itself more efficient and second is the mixture of experts which means that although the number of parameters is 671 billion all of those parameters are not active at the same time only about 37 billion parameters are so essentially parts of the parameters are turned off and parts are turned on like light bulbs going on and off and that makes the model extremely efficient in its computations parameters which are not needed are turned off when they are needed they are suddenly turned on and then they start working right then we have multi-token prediction quantization and rotary positional encodings as well which we are going to learn about so the first thing which i want to teach you is

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

Attention and then I was thinking about how to exactly teach this concept right because the concept itself is pretty advanced so if you search about multi-head, if you search about multi-head latent attention you'll get certain blog posts which talk about this so they do talk about essentially what multi-head attention is and if you scroll down here below you'll see that it's just one page of this blog article and they start out with multi-query attention, group query attention and they go to rotary positional encoding and then there is a very small section on multi-head latent attention which is impossible for a person to understand if if they are just starting out to explore what deep sick is and so I don't want to follow that approach of just going through a simple lecture and explaining MLA by assuming that you know the prerequisite knowledge instead as I mentioned to you at the start of the series itself I want to make this lecture series very deep so the way I'm going to explain multi-head latent attention is through a four-part process first we are going to understand the architecture of LLMs itself and that's going to be the main purpose of today's lecture I believe that without having an intuition of the LLM architecture it's impossible to understand latent attention then we are going to understand why there was a need for self-attention and what is the self-attention mechanism itself once we understand self-attention we are going to understand how self-attention was transformed into multi-head attention and what does it mean to have multiple attention heads that's the third aspect over here and then the fourth aspect is essentially key value cache so then we are going to understand okay multi-head attention works and it really works very well but then what can we start doing to improve the efficiency of multi-head attention to make it computationally faster to make sure that the number of parameters which you are storing in the memory is reduced and that's when key value cache comes into the picture only when you truly understand key value cache then you will slowly start to understand multi-head latent attention so after kv cache then we are going to understand MLA but I'm going to devote a lot of time to develop your foundations in these first four concepts before we go ahead to MLA itself many of these blog posts which I mentioned right now they assume that you are already familiar with this so in the build a deep seek from scratch series we had 43 lectures explaining you everything about these different aspects the in this lecture series I am not going to go as deep so for example today I am going to have just one lecture on the LLM architecture but in the build LLM from scratch there were three to four lectures right so my aim is that I want to explain this knowledge to you but also I want to make sure that beginners who have started watching this series they also feel connected with the series so the challenge for me was to come up with a new set of lecture notes specifically for this series because I'm trying to explain a concept in a in one hour but also I want to make sure that I don't lose out beginners in the process so I've made a whole series of new notes for explaining these different concepts all right so I hope everyone has understood the flow of how we are going to understand multi-head latent attention today our main aim is to understand the architecture of LLMs so after today's lecture all of you should have a mental map or a visual roadmap of what happens with the world or a token when it goes into an LLM first of all let me explain what does it mean architecture of an LLM right so if you pass in some certain a sentence or a sequence of words we have seen that when a sequence of words is passed into an LLM what the LLM essentially does is that it predicts the next word or it predicts the next token rather so an LLM or a large language model can be thought of as the next token prediction engine right so it can be thought of as the next token prediction engine and just like an engine so let's say if I go and search engine car and if I copy this right now copy image or let me copy a good image right now if I copy this image right now and if I paste it over here right this is an engine so if we are calling it the next token prediction engine we need to know how does this engine actually work how does this engine actually work in the previous lecture we have learned some things about the engine what have we learned the first thing which we have learned about this engine is that it has a huge number of parameters so to give you an example gpt3 has around 175 billion parameters whereas gpt4 although it's not been released yet they probably have around a trillion parameters gpt 4.5 which was just released two to three days back maybe have 5 trillion or 10 trillion parameters that's not yet released but we know that this engine has a huge number of parameters which are acting together and if you search engine car working right you'll see that there is this piston cylinder mechanism here right and the piston cylinder mechanism essentially works and then that's how the engine operates similarly we need to make sure that we understand how do these parameters actually work where are these parameters can we open the engine of the large language model and try to see what happens inside that engine essentially given a sequence of words so similarly to a car right when a fuel is injected into this engine what essentially happens with that fuel and how is that converted into the motion of the car similarly when a sequence of words is passed to this engine what's underneath the llm which causes us to predict the next word so think of the sequence of words as a fuel and think of the llm of course as the engine and the next word as the motion of the car let's say so here we want to open the engine right and to truly understand how the engine actually works we need to understand the architecture of the engine which means that we need to understand what are the different components inside that engine how these components are actually connected to each other then what exactly happens with the fuel or the sequence of words when it is being passed through this architecture that's what we essentially want to understand in today's lecture so you can think of today's lecture as going to your car opening the engine and trying to peek inside the engine and trying to really understand how do you work one way to also think about this is that all of us have worked with right and if we ask something like give me an essay give me a short essay on friendship right and we see that gpt essentially predicts one token at a time how does this actually work what is the architecture which is giving me this one token at a time prediction that's what we are going to find out today okay i'm going to try to keep this lecture as beginner friendly as possible so you'll see that there's a whole story which i have constructed to explain how the llm architecture works i don't think this has ever been done before so it's also a bit of an experiment for me to explain this to you in the story kind of a format but i hope through this explanation you will understand really how the llm architecture is working okay so here's the schematic of what happens when you take a look at the engine what happens when you open the black box what lies within the black box itself and when you open the black box you will see that there are a huge number of things which pop out it's not simple at all uh if you think of the 175 billion parameters the 175 billion parameters are scattered across multiple places of this black box in other words the llm architecture is quite complex um and why do you think it might be complex well because in making the next token prediction the llm is actually learning language itself right i strongly think that language learning is a byproduct of the next token prediction task and to learn the language you cannot have an engine which is small or you cannot have an engine which is not complex enough so that's why our engine is quite complex we have a huge number of blocks or layers which are linked together today we are going to try to understand this architecture so if you just take a look at this schematic you will see that broadly this schematic is divided into three parts there is this the there is this part number one over here part number one then there is a part number two which i marked by saying that it's something which is called as the transformer block which let me mark like this and then there is a part number three which is basically the part which is the output part so here is where the next token is actually predicted so the architecture of the llm can be thought of in three parts the first part can be thought of as the input the second part can be thought of as the processor and the third part essentially can be thought of as the output right in the input part we have the sentence let's say if you have the sentence any sentence let's say the next day is bright there are a number of things which happen with that sentence and with every token or every word in that sentence before we pass it to the processor and the things which happen to the sentence are first of all we do something which is called as tokenization secondly we do token embedding and third we do something called as positional embedding after these three steps are done the input is essentially passed to the processor which is also called as transformer block within the transformer block there are six different things the normalization layer multi-head attention layer dropout second normalization layer feed forward neural network another layer of dropout these two plus signs here are what are called as skip connections or shortcut connections finally when we come out of the processor or the transformer block we have the output and here we have another layer normalization layer and the final layer for the next token prediction okay so all of what i said right now if you are learning this for the first time you might be thinking what is going on here and all of this seems too complex let's break it down further and that's exactly what i'm going to do right now but keep a keep close eye or attention on this part which i've marked as purple because the first innovation which i'm going to explain later which is the multi-head latent attention or mla here is to do with this aspect which is titled as multi-head attention so out of the entire architecture there are two major places where deep seek has contributed in making their innovations the first is this called as multi-head attention that block and the second is the feed forward neural network so let me actually rub this a bit right now so the mla or the multi-head attention is an innovation which happened in this part of the architecture so i'm going to call this mla and the mixture of experts innovation that actually happened in this part of the architecture moe so again if you don't understand the architecture itself you will not be able to appreciate where the innovation has happened but it's like imagine opening an engine right and you have opened the deep seek engine right now and the deep seek engine is that of a car which has performed very well and you want to understand why has it performed very well you open the engine you see all these parts and now i'm telling you there are two parts in this engine which were augmented to improve the performance but all right coming back to the engine itself how do the input processor and the output actually work and now my challenge for this lecture was that i wanted to create one lecture in which i explain all of this to you i explain the input i explain the processor and i explain the output and i wanted to explain it in an easy to understand manner so that you get an feel for the architecture itself so the way i thought of doing this was that i thought from the perspective of the fuel so if you're a fuel let's say and if you go to the car engine what happens with you do you go to the engine first then are you rotated because of the pistons and then some kind of a power or energy is produced if i understand the life of the fuel i'll effectively understand how the engine works right similarly today i want to show you the life cycle of a single word and what happens to a single word when it essentially goes through the llm architecture think about it this way right when we when we put this sentence give me a short essay on friendship or let's say if we are going to complete the next sentence and the sentence which i am going to take is let's say the next day is bright the next day is bright it goes into the llm engine and then the next token is predicted let's say the next token is and the next day is bright and what i want to show you is we are going to focus on just one token and we are going to see what happens to this token as it goes through every single step of this llm architecture and by looking at from the perspective of the token so i want you to now imagine that you are that token you are that token and i will now take you through what happens to the token as it goes through several of these layers in the llm architecture itself and ultimately we predict the next token so that's how i'm going to explain this whole lecture to you in a story format i am going to explain this lecture to you as if you are the token now imagine you are the token you are surrounded by a bunch of words and suddenly you are thrown to the llm architecture let's understand the life cycle of a single token so that's how the next part of this lecture is going to be all right so let's embark on this journey together in which we will understand how the life of a token essentially looks like so the title which i have given to this section is the journey of a token through the llm architecture so what i first did is i went to chat gpt and i asked it to write a short paragraph on friends right so i took some random sentence from this which is a true friend accepts you let's say we are looking at this sentence a true friend accepts you which is a sequence of five words let's say that's my input sequence at the moment and we have to predict the next token given this input sequence and i am going to specifically focus on the word friend and i am going to think from the perspective of friend now put yourself in the shoes of this word or this token first of all this token so i am going to interchangeably use token and word although they are not the same for the sake of simplicity i am just going to say one token is equal to one word so put yourself in the shoes of this token now what do you see you see that there are these other tokens around me right there is a true accepts you there are these four other tokens which i am used to hang out with just like friends hang out together i am used to having these as my neighbors and my friends a true accepts and you these are the four neighbors of this token which we have chosen that is friend now suddenly what happens in the first step is that the first step in the llm architecture is that that's the step of isolation so currently we are looking at this phase right which is the input phase right the first step which happens in the input phase is the isolation phase so what happens is that the word is detached from its neighbors the word is isolated from its neighbors so imagine like a group of friends and every person is isolated from their neighbors so this word is isolated and we look at it in isolation that's phase number one phase number two is essentially called as token id assignment which means that imagine now every word is isolated and we want to put a badge or a stamp on every word or every token similar to how let's say if you are getting enrolled in a camp or enrolled in military or any other group activity you are given id that's your roll number or in school all of us have roll numbers right it's similar to that so every token is isolated and then it's assigned a separate token id the way the token id is assigned it's i'm calling it getting your badge the way the token id is assigned is a very interesting process we have a book of token id so you can think of this like a encyclopedia or a book of token ids in this book basically all the possible tokens are listed all the possible tokens are all the possible words and then there is a number which is associated with every word in this book there are not just words there can be characters or there can be even sub words so this book consists of characters like a b up to z it even consists of sub words such as maybe cr can be a word in this vocabulary then it may consist of sub words like isation that can be a sub word and then it also consists of words like let's say token can be a full word in this vocabulary enter can be a full word in this vocabulary begin can be a full word in this vocabulary so you can think of this book of token ids as consisting of characters words and sub words so let me write this down here that's going to be very important this book of token ids essentially consists of it consists of characters it consists of words and it also consists of sub words so as a result essentially we make sure that every token or every word which is isolated it finds certain badge there is no token or no word which is isolated which won't find any badge readers who are familiar with the concept of byte pair encoding remember that to create this book of token id itself there is a certain scheme which is called as a byte pair encoding scheme this is a sub word tokenization scheme and to create this book of token id we use this scheme so gpt2 for example relied on the byte pair encoding mechanism to create its vocabulary this vocabulary is this book of token ids is also called as a vocabulary and then it changes from one large language model to another so let's say gpt2 has a vocabulary of 50 000 gpt4 might have a vocabulary which is higher maybe 100 000 right so based on the llm which we are using the token id which is assigned to let's say this friend might change so i am using a large language model here right now which has a vocabulary size of 50 000 which means that there are 50 000 tokens which might be a combination of characters words and sub words all right then what i am going to do is that i am going to essentially look at this vocabulary and i am going to find where the friend comes into the picture and i am going to find the token id associated with it right so for the word friend the token id which is associated now is 2012 so i am going to note that down so that's the badge or that's the roll number which is now assigned to this token so the roll number assigned to the token friend is now 2012 similarly all the other tokens or all the other words will get a similar badge that's the first step or rather that's phase number two which is token id assignment so now imagine that this token friend which was isolated from its neighbors it has now been given a badge or a stamp which is 2012 that's phase number two i did not go into the details of how this book of token id was created because if you want more details on this there's a separate lecture on creating this vocabulary itself for every large language model and it's called byte pair encoding from scratch it's present in the lecture series build llm from scratch but for now stay with me imagine you are this token friend you have been isolated and now you have been given a badge or you have been given a roll number then you essentially come to phase number three in phase number three something interesting essentially happens in phase number three the you until now you just had one number associated with you right but now you are going to have a huge vector of numbers which are going to be associated with you and this is called as token embedding assignment one way to think of this is that let's say we have an entrance examination which has 768 questions and each question essentially tests a certain feature of you so now we are looking at the word friend right each question will test are you a noun are you a gender are you a verb are you a sport are you an emotion etc we actually don't know what these features or what these questions are but i'm just trying to explain you so that explain to you so that you get an intuition of what token embedding is so imagine there are 768 questions like this which are asked to every token which we have isolated and then based on the answers we get to understand something about that token whether it's a noun whether it's a sport whether it's a adjective whether it's something which appears always at the end of a sentence whether it's something related to gender whether it's something related to monarchy like kings princess queens etc so here we are actually getting to know about the meaning of the token itself in the token id assignment we did not get to know anything about the meaning but in phase 3 in token embedding because we ask a big list of questions we get to know something about the meaning and based on the answers which are given there is a result so every token so now this friend right it will have some values for each of these questions maybe the values are 0.1 0.2 0.1 0.3 etc and if we are assuming that there are 768 questions this will now be a vector of 768 values one thing which i want to point out here is that this number of questions here 768 that vary from one large language model to another large language model so now if you go and and see let's see for gpt2 gpt2 token embedding dimension so if we search gpt2 token embedding dimension we'll see that it's 768 right but here also gpt2 small had 768 but the largest gpt2 had 1600 dimensions so this number of questions 768 actually varies from one large language model to another so here what we are going to do is that we are going to assume that the number of questions is 768 for gpt2 but remember that if the token goes to different llms it might be asked different questions so now imagine you are a token you have been given a badge or a roll number and suddenly you are asked this huge set of 768 questions you respond then your answers are collected in one 768 dimensional vector that is called as the token embedding vector so now along with the badge you also carry your result with you so you have a badge with you and you have this result of 768 values with you that's what has happened to you until this stage right that's the stage of token embedding the difference again between token id is that token id does not carry any notion about semantics whereas token embedding in token embedding assignment will care a lot about the meaning of the word itself the reason token embedding is done is that to create llms you ultimately need to extract meaning right you're teaching something about the language to the model so this is a very crucial step these set of questions or these set of features which are collected about every single token so until now every token has a badge and every token has 768 value result sheet which they take along with them then one more thing which also matters is your position among your neighbors so here if you see a true friend accepts you so friend comes in the middle of the sentence right it comes at position number three over here so a comes at position number one true comes at position two friend comes at position number three accepts comes at position number four and you comes at position number five so the friend is coming at position number three and that position also matters why does the position matter because if you say the dog the dog chased another dog okay if you take a look at this sentence you need to somehow be aware that this dog is basically different than this second dog so if you just take the meanings of the words right as in phase number three we just took the meanings so the token embedding for this dog and this dog will be the same but actually there are two separate dogs and we need to teach the model related to that so the only way to distinguish between this dog and this dog is to know that this comes at position number two and this comes at position number five so as a result it's important to also have some knowledge about the position so similar to the 768 questions which we asked 768 questions will again be asked with respect to the position so remember that although this number varies across different models if you fix a particular language model the number of questions which are asked in token embedding and the number of questions which are asked in the positional embeddings are the same so if we are looking at gpt2 small right now as the model there were 768 questions asked in token embedding similarly 768 questions will be asked in positional embedding and what might these positions or what might these questions be they might be something like are you at the beginning or are you around the middle of the sequence or do

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

