
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


（24.56）

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
* 第三，他们实现了多项 GPU 优化技巧；
* 第四，构建了一个有利于蒸馏等技术的模型生态系统。


现在，这就是目前存在的多头注意力机制。而DeepSeek团队所做的创新在于，他们引入了一种完全不同的方法，以确保注意力机制能够高效实现。他们采用了键值缓存技术，但不同于常规的键值缓存，他们创新性地将其置于潜在空间中（若您暂时不理解这些术语不必担心，后续课程我们会详细讲解）。现阶段您只需记住：他们通过在潜在空间实施这种特殊的键值缓存技术，使得注意力机制的计算效率显著提升——不仅占用更少存储空间，计算速度也更快。这正是他们做出的重要变革之一。

这就是多头潜在注意力机制，这是我们讨论的第一个要点。第二个要点是关于专家混合模型（MoE）。如果你观察常规的注意力机制，它的结构大致是这样的：首先是前馈层，接着是注意力层，之后又接一个前馈神经网络。而在专家混合模型中，核心操作是设置了四个专家模块——整个神经网络并非同时激活，每次只激活其中的部分子网络。

因此，他们设计了一种特殊的路由机制，由它来决定哪些部分会被激活，哪些部分不会。这个路由器本质上就是决定哪些专家模块会被激活，哪些不会。我们后续会非常详细地学习这部分内容。这是他们实现的第二个关键创新点。

第三项创新是多token预测技术。就像我们这节课开头看到的，传统模型通常只预测单个token。而他们采用了一项上个月刚发表论文提出的新技术——为什么不尝试同时预测多个token呢？这可能会加速推理过程，提高模型效率。

第四项创新是他们实施了量化技术。因此，量化不是将每个参数都表示为大型浮点数，而是以一种略微压缩的方式来表示。理解量化的最佳方式就像这里的例子：左边是原始图像，包含大量像素点，而右边的图像仅由八种颜色构成，呈现出一种像素化的效果。如果你放大看，会发现它完全是由像素块组成的——看这里，相比左边非常锐利的图像，这边明显像素化了。但当你缩小视图时，会发现两者看起来几乎一模一样对吧？这就是所谓的量化。他们将这种量化技术应用到了Transformer模块的参数中。

最后，他们还引入了一种称为"旋转位置编码"的技术。让我现在就来展示旋转位置编码的原理——它本质上应用于2017年提出的注意力机制中。

他们只是将位置编码添加到标记编码中，这污染了嵌入向量。但不久之后，人们意识到，为什么不直接旋转原始向量、查询向量和键向量来捕捉位置编码的效果呢？这样就不会改变向量的幅度，而且这是一种非常高效的位置编码方式。因此，我们不是在标记嵌入本身中编码位置，而是在查询和键中稍后编码位置。是的，这就是旋转位置编码，这是DeepSeek架构实现的第五个关键创新。我们现在将详细研究所有这些内容。这是第一个创新方面——架构创新，但他们并没有止步于此。我认为DeepSeek论文真正让强化学习领域重获新生，因为它不仅仅依赖于人类标记的数据，就像我们在GPT-3.5中看到的那样，人类创造了数据的质量，然后将这些数据反馈到训练过程中。

他们采用大规模强化学习来教授模型复杂推理能力，并非依赖人工标注数据，而是构建了一套基于规则的奖励系统——这意味着整个训练过程不依赖人类判断，完全由规则驱动。通过这种方式，他们实现了名为"群体相对策略优化"的框架，这正是其强化学习训练机制的核心所在。我们将深入解析这套体系，而这正是DeepSeek R1成为卓越推理模型的关键所在。

强化学习是我们将要涵盖的主要内容之一。第三个方面是GPU优化技巧，这部分理解起来有点难度，所以即使在课程中我也不会花太多时间讲解。简单来说，他们所做的不是使用CUDA，而是采用了NVIDIA的并行线程执行技术（PTX）。最直观的理解方式——其实我问过GPT如何用最简单的方式来解释——如果把CUDA比作编写Python或Java代码，那么PTX就像是更底层的一步，它是机器代码执行前的一个中间步骤，这能大幅提升运行速度。

因此，在高级编程中，你并不处于机器码层面。如果你处于机器码层面，那是最快的，比如C或C++，而Python或Java则让你处于更高的层次，这会减慢速度。你可以把PTX看作是介于两者之间的东西，而CUDA处于更高的层次，但PTX处于中间，更接近机器码执行，因此能大大加快速度。关于这一点，还有一篇不错的文章发表过，是的，Deep Six AI在某些功能上突破了行业标准的CUDA，转而使用NVIDIA的汇编式编程，比如PTX编程。

因此我认为，这也对加速他们的架构优化起到了重要作用，最终降低了成本。最后，他们拥有一个强大的模型生态系统——尽管主模型规模高达6710亿参数，但他们已将其蒸馏压缩至小至15亿参数的模型，这构成了一个非常出色的模型生态系统。我们稍后也会探讨模型蒸馏技术。现在简要回顾一下让Deep Six如此突出的四个关键点：首先是创新架构；第二是以强化学习为核心的训练方法；第三是他们掌握的一系列GPU优化技巧；第四是模型生态系统，特别是将大模型蒸馏压缩至约15亿参数的小型模型的能力。

现在，我们进入今天课程的最后两个部分。首先，为什么Deep Six是AI历史上的一个转折点？我认为它是一个转折点，原因如下：这是第一次有一个小而精悍的初创公司，通过新颖的技术和少得多的资源，达到了与最佳AI模型相当的水平。他们大幅降低了开发成本，虽然不像600万美元那么低，但我相信与像GPT或Meta的Llama等大公司训练模型所需的成本相比，仍然相当低。因此，这就像是第一次证明，即使是小型初创公司，即使是资金不如OpenAI雄厚的公司，也能构建出大型语言模型，这真是太棒了。

DeepSeek的表现与GPT4不相上下，但所需资源却少得多。这一进展也引发了恐慌，投资者们感到不安。2025年1月，由于DeepSeek的突破性进展，美国科技股出现了大幅下跌。主要原因是，低成本、开源的中国AI模型威胁到了OpenAI、微软甚至谷歌的盈利模式，并引发了人们对AI供应链和GPU市场的担忧。一个模型、一家公司——DeepSeek，带来了如此多的变化，这也正是DeepSeek成为历史转折点的关键所在。我认为，还有一件重要的事情也已经发生了。

因为DeepSeek的出现，发展中国家如印度已经开始大力投资建设自己的大规模基础模型。如果中国的DeepSeek能做到，为什么其他国家不行？为什么只有美国？为什么只有美国公司能做到？其他拥有DeepSeek所用资源的公司为什么不能建立自己的基础模型？事实上，所有这些讨论已经开始。印度政府甚至发布了建设基础模型的号召，这非常棒，我认为这也是我制作这个系列的主要动力之一。

现在进入今天讲座的最后一个部分，也就是我们这个系列讲座的计划。我已经根据DeepSeek的特点，将这个系列讲座分成了四个阶段。第一阶段我们将深入探讨架构，首先我会从注意力机制开始讲解，接着是多头潜在注意力、专家混合、多令牌预测、量化以及旋转位置编码。我会假设大家对注意力机制有一定的了解，如果没有的话，可以参考"从零开始构建LLM"系列讲座。这个系列讲座会稍微高级一些，建议大家先学习之前的那个系列。

在开始之前，我会先从一个稍高的层次切入，然后我们会进入训练方法论的第二阶段，第三阶段是GPU优化文本——这部分内容会比较简短，我不会安排太多课程。最后我将以知识蒸馏相关课程作为收尾。整个系列课程的主体将集中在第一阶段和第二阶段，第三阶段和第四阶段的课程会相对较少。这就是本系列课程的主要规划框架。

让我们快速回顾今天的学习要点：首先我们探讨了大语言模型，认识到它们本质上是基于概率预测下一个标记的引擎。同时我们发现，模型规模是大语言模型非常关键的决定性因素。

随着模型规模的增大，存在一个规模扩展定律，模型性能会不断提升，并开始展现出小规模模型所不具备的涌现特性。我们发现大语言模型（LLM）的核心要素本质上就是Transformer架构。最后我们认识到，构建LLM意味着首先要建立基础模型（即预训练阶段），随后进入微调阶段（包含两个环节）。虽然DeepSeek R1近期声名鹊起，但DeepSeek公司早在多年前就已起步——他们最初推出的是DeepSeek LLM第一代（V1版本），随后迭代出V2版本，最终研发出拥有6710亿参数的巨型V3版本，并在此基础上最终打造出推理专用模型DeepSeek R1。

这简直让互联网炸开了锅！为什么？因为DeepSeek R1的性能竟然能媲美OpenAI的原版模型，而成本却只是零头，更关键的是它还是开源的！DeepSeek的表现与GPT-4旗鼓相当，但价格却便宜了100到500倍——就像我在这次讲座中展示的数据那样。最后，它完全开源。

优势与不足：  
DeepSeek最大的优势在于：开源、高性价比、性能强劲——三大核心优势。而它最主要的不足可能是，相比GPT-4，它的打磨程度和安全性稍逊一筹。另一个弱点是：如果你想在本地部署或安全使用，你需要有能下载和运行6710亿参数模型的基础设施。

然后我们看到了DeepSeek如此特别的原因，这里有四个关键要素：首先是创新的架构、训练方法、GPU优化和模型生态系统。在创新的架构中，我们有五个关键点：多头潜在注意力、专家混合、多令牌预测、量化和旋转位置编码。在训练方法方面，他们使用大规模强化学习来教授模型复杂推理，并采用了基于规则的奖励系统，也称为群体相对策略优化，而不是依赖人工标注数据。在GPU优化技巧方面……

我认为他们只在某些地方使用了并行线程执行（PTA）而非仅依赖CUDA，最终他们建立了一个强大的模型生态系统，将主模型蒸馏为参数低至15亿的小型模型。在本系列讲座中，我们将遵循相同的工作流程：第一阶段是创新架构，第二阶段是训练方法，第三阶段是GPU优化技巧，第四阶段是模型生态系统。我会假设大家对大型语言模型（LLMs）有相当的了解。

我会再次讲解注意力机制，但基本上会从注意力机制开始，然后深入细节。如果你是零基础，我建议你先看《从零构建LLM》系列。最后，我认为深度求索是历史上的一个转折点，因为他们用实际行动证明，即使是发展中国家，只要我们在创新架构上足够聪明、足够有创意，也能构建出与OpenAI模型相媲美的基础模型，而且成本低廉、完全开源。他们正在以这种方式真正实现AI的民主化。非常感谢大家，期待下次讲座再见！

大家好，我是Raj Dandekar博士。我于2022年从麻省理工学院获得机器学习博士学位，同时也是"从零构建Deep Seek"系列教程的创作者。在开始之前，我想向大家介绍本系列视频的赞助商兼合作伙伴——InVideo AI。大家都知道我们多么重视基础性内容，即从最底层构建AI模型。InVideo AI的理念与我们的原则非常相似。让我来为大家展示一下。这就是InVideo AI的网站，他们凭借一个小型工程团队就打造出了令人惊叹的产品，只需输入文字提示就能生成高质量的AI视频。

如你所见，这里我输入了一段文字提示："创作一段超写实的高端奢华腕表商业视频，并赋予电影级质感"。点击生成视频后，很快我就获得了这段令人惊叹的逼真视频。最让我震撼的是它对细节的极致把控——看看这个质感与纹理，简直不可思议！而这一切仅源于一段文字指令，这正是InVideo产品的魔力所在。支撑这段惊艳视频的核心，是InVideo AI基于第一性原理重构的视频创作流程，他们通过对基础模型的反复实验与调试，从根本上革新了视频生成与剪辑技术。

他们在印度拥有最大的H100和H200集群之一，并且正在试验B200。在视频AI领域，他们是印度发展最快的AI初创公司，面向全球市场，这也是我如此认同他们的原因。好消息是，他们目前有多个职位空缺，你可以加入他们优秀的团队。我会在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建DeepSeek”系列的这一讲。在上一讲中，我们了解了我们将这个系列讲座划分的四个阶段。

第一阶段是DeepSeek背后的创新架构，这就是这里的第一阶段。第二阶段是训练方法本身，即强化学习的兴起，以及他们如何依赖RL，通过基于规则的奖励系统来教授模型进行复杂推理，这就是第二阶段。第三阶段是GPU优化技巧，即他们如何使用NVIDIA的并行线程执行（PTX或CUDA）。第四阶段是他们的模型生态系统本身，他们不仅仅停留在构建一个庞大的6710亿参数模型，而是将这个大型模型蒸馏成一个更小的模型，其大小约为15亿参数。

这就是我们即将经历的第四阶段，我们将从第一阶段开始，经过第二阶段、第三阶段，最终到达第四阶段。在今天的讲座中，我们将首先介绍第一阶段，这本质上是Deep Seek背后的创新架构，以及使其如此高效的原因。其架构的两个主要方面极大地提升了Deep Seek的效率：一个是多头潜在注意力机制（MLA），它使注意力机制本身更加高效；另一个是专家混合机制，这意味着尽管参数数量达到了6710亿。

所有这些参数并非同时激活，实际上只有约370亿个参数处于活跃状态。本质上，部分参数会被关闭，部分则被开启——就像灯泡明灭闪烁那样。这种机制让模型的计算效率极高：不需要的参数会被关闭，需要时则瞬间激活并立即投入工作。此外，我们还会学习多令牌预测量化技术和旋转位置编码技术。不过首先要讲解的是注意力机制——我一直在思考如何精准地向你们阐释这个概念。

因为概念本身相当高级，所以如果你搜索多头注意力，或者搜索多头潜在注意力，你会找到一些相关的博客文章来讨论这个问题。这些文章确实从本质上讲解了多头注意力是什么。如果你往下滚动页面，你会看到这只是博客文章的一页内容，他们从多查询注意力、组查询注意力开始讲起，然后讲到旋转位置编码，最后才有一小部分是关于多头潜在注意力的。这部分内容对于刚开始探索深度学习的人来说几乎不可能理解。因此，我不想采用那种仅仅通过简单讲解的方式来进行。

在讲解多头潜在注意力机制（Multi-Head Latent Attention）时，我并未预设听众已掌握先验知识——正如本系列课程开篇所言，我希望将这个讲座系列打造得非常深入。因此我将通过四个阶段来阐释：首先我们将剖析大语言模型（LLMs）的架构本身，这将是今天课程的核心目标。我坚信，若缺乏对大语言模型架构的直觉认知，就根本无法理解潜在注意力机制。随后我们将探讨自注意力机制诞生的必要性及其本质，在掌握自注意力之后，最终揭示自注意力如何演变为多头注意力机制。

拥有多个注意力头意味着什么？这是第三个方面。第四个方面本质上是键值缓存。接下来我们将理解多头注意力机制的工作原理，它确实表现非常出色。但为了提高多头注意力的效率，使其计算速度更快，并减少存储在内存中的参数数量，我们可以采取哪些措施？这时键值缓存就派上用场了。只有真正理解了键值缓存，你才能逐步理解多头潜在注意力机制。

因此，在了解KV缓存之后，我们将开始学习MLA。但在深入探讨MLA本身之前，我会花大量时间来夯实你们在这前四个概念上的基础。现在提到的许多博客文章都默认你们已经熟悉这些内容。在“从零构建DeepSeek”系列中，我们用了43节课详细讲解这些不同方面。而在这个系列中，我不会讲得那么深入。例如，今天关于LLM架构我只安排了一节课，但在“从零构建LLM”系列中，这部分内容用了三到四节课来讲解。

好的，我的目标是既要向你们解释这些知识，也要确保刚开始观看这个系列的初学者能跟上进度。因此，我的挑战在于为这个系列专门准备一套新的讲义。因为我要在一个小时内解释一个概念，同时还要确保初学者不会在这个过程中掉队。为此，我专门制作了一系列新的笔记来讲解这些不同的概念。好了，希望大家都能理解我们今天要学习多头潜在注意力的流程。我们的主要目标是理解LLMs的架构。

所以在今天的讲座之后，你们都应该在脑海中形成一幅思维导图或视觉路线图，了解当一个词或标记进入大型语言模型（LLM）时会发生什么。首先，让我解释一下LLM的架构是什么意思。如果你输入一个句子或一系列单词，我们已经看到，当一系列单词输入到LLM时，LLM本质上做的是预测下一个单词，或者更准确地说，预测下一个标记。因此，LLM或大型语言模型可以被视为下一个标记预测引擎，就像一台引擎一样。

假设我现在去搜索引擎搜索“汽车”，然后复制一张图片——或者让我现在复制一张合适的图片——如果我此刻复制这张图片并粘贴到这里，这就是一个引擎。如果我们称其为“下一个词预测引擎”，我们需要了解这个引擎实际上是如何运作的。在之前的课程中，我们已经学习了一些关于这个引擎的知识。我们学到了什么呢？首先，我们了解到这个引擎拥有海量的参数。举个例子，GPT-3约有1750亿个参数，而GPT-4虽然尚未正式发布，其参数规模可能已达到万亿级别。至于两三天前刚发布的GPT-4.5，其参数可能高达5万亿甚至10万亿（尽管尚未完全公开）。但我们可以确定的是，这个引擎通过天文数字般的参数协同工作。

如果你搜索汽车引擎的工作原理，就会发现这里有一个活塞气缸机构，对吧？这个活塞气缸机构本质上就是引擎运转的核心。同样地，我们需要确保自己理解这些参数究竟如何运作——这些参数藏在哪里？我们能否像打开汽车引擎盖那样，打开大语言模型的"引擎"看看内部运作？具体来说，当输入一串词语时（就像汽车注入燃油），这个"引擎"内部究竟发生了什么？燃油如何转化为汽车的动力，同理词语序列如何在这个语言模型里被处理，最终促使模型预测出下一个词——这才是隐藏在LLM之下的奥秘。

所以，把单词序列想象成燃料，把大语言模型（LLM）比作引擎，而下一个单词就是汽车的行驶动作。这么说吧，我们现在要打开引擎盖，真正理解引擎的工作原理，就需要了解引擎的结构。这意味着我们需要知道引擎内部有哪些不同的部件，这些部件之间是如何相互连接的，以及当燃料（即单词序列）通过这个结构时，到底发生了什么变化。这就是我们今天这节课要深入探讨的核心内容。

所以你可以把今天的讲座想象成打开汽车引擎盖，试图窥探引擎内部，真正理解它是如何工作的。另一种理解方式是，我们所有人都用过GPT这样的工具，比如当我们提出“给我写一篇关于友谊的短文”这样的要求时，我们会发现GPT实际上是一个标记一个标记地预测输出的。那么，这种预测是如何实现的？是什么样的架构在支持这种逐个标记的预测？这就是我们今天要探究的内容。

好的，我会尽量让这个讲座对初学者友好一些。你会看到我构建了一个完整的故事来解释LLM架构的工作原理。我认为以前从未有人这样做过，所以对我来说，用这种故事的形式来解释也是一种实验。但希望通过这个解释，你能真正理解LLM架构是如何运作的。那么，这是当你查看引擎时的示意图，当你打开黑盒子时会发生什么，黑盒子里面有什么。当你打开黑盒子时，你会发现有很多东西跳出来，一点也不简单。如果你想到1750亿个参数，这些1750亿个参数分散在这个黑盒子的多个地方。换句话说，LLM架构相当复杂。

你认为它可能很复杂的原因是什么呢？因为在进行下一个标记预测时，大语言模型实际上是在学习语言本身。我强烈认为语言学习是下一个标记预测任务的副产品，而要学习语言，你不能有一个太小的引擎，也不能有一个不够复杂的引擎。这就是为什么我们的引擎相当复杂——我们有大量相互连接的块或层。今天我们将尝试理解这种架构。

所以如果你看一下这个示意图，你会发现它大致分为三个部分。首先是这里的第一个部分，我称之为第一部分；然后是第二部分，我标记为变压器模块，让我这样标注一下；最后是第三部分，基本上是输出部分，也就是预测下一个标记的地方。因此，LLM的架构可以分为三个部分来理解：第一部分可以看作是输入部分，第二部分可以看作是处理器，第三部分本质上就是输出部分。在输入部分，我们有句子。

比如说，你有一个句子，任何句子都可以，比如“明天会更好”。在将这个句子传递给处理器之前，这个句子以及其中的每个标记或每个单词都会经历一系列处理步骤。首先，我们会进行所谓的“分词”（tokenization）；其次，我们会进行“标记嵌入”（token embedding）；最后，我们会进行“位置嵌入”（positional embedding）。这三个步骤完成后，输入内容才会被传递给处理器，也就是所谓的“Transformer块”。

在Transformer块内部，有六个不同的组成部分：归一化层、多头注意力层、Dropout层、第二个归一化层、前馈神经网络层以及另一层Dropout层。这里的两个加号代表所谓的跳跃连接或快捷连接。最后，当我们从处理器或Transformer块出来时，会得到输出结果，这里还有一个归一化层以及用于预测下一个token的最终层。好了，我刚才所说的内容，如果你是第一次学习这些，可能会觉得有点摸不着头脑。

而这一切看起来太复杂了，让我们进一步分解，这正是我现在要做的。但请密切关注我用紫色标记的这一部分，因为我要解释的第一个创新——多头潜在注意力（MLA）——与这个标为“多头注意力”的方面有关。在整个架构中，深度探索（DeepSeek）在两个主要地方做出了创新贡献：第一个就是这个称为“多头注意力”的模块，第二个是前馈神经网络。让我现在稍微擦除一下，MLA或多头注意力是架构这一部分的创新。

所以我将这种创新称为MLA（混合专家）架构中的创新。再次强调，如果你不了解架构本身，就无法真正领会创新点在哪里。这就像拆开发动机一样——你现在拆开的是DeepSeek的"引擎"，这个表现优异的"汽车引擎"。要理解它为何表现卓越，就得打开引擎盖，观察所有这些零部件的运作。

现在我告诉你，这台引擎中有两个部件经过增强以提高性能。不过言归正传，回到引擎本身——输入处理器和输出系统究竟如何运作？这次课程我给自己设定的挑战，就是要用一堂课的时间把所有这些原理讲透：既要解析输入系统，又要阐释处理器，还要说明输出机制。我希望用通俗易懂的方式讲解，让你们能真正感知到整个架构的精妙。我的讲解思路是从燃料的视角出发——假设你是一滴燃油，进入汽车引擎后会经历什么？是先进入引擎内部，然后被活塞带动旋转，最终产生某种动力或能量吗？只要理解了燃料的生命历程，实际上就掌握了引擎的工作原理，对吧？

今天我想向大家展示一个单词的生命周期，以及当它经历LLM架构时会发生什么。我们可以这样思考：当我们输入这个句子"给我一篇关于友谊的短文"，或者假设我们要完成下一句话，比如我选择的句子是"明天会更好"——"明天会更好"这句话进入LLM引擎后，系统就会预测出下一个标记。

假设下一个词是“and”，而明天是光明的。我想向你们展示的是，我们将只关注一个词，看看这个词在LLM架构的每一步中会发生什么。通过这个词的视角，我希望你们现在想象自己就是这个词。你就是这个词，我将带你经历这个词在LLM架构的多个层中所经历的一切，最终我们预测下一个词。这就是我如何以故事的形式向你们解释这整个讲座。我会把这次讲座讲得好像你就是这个词一样。现在想象你就是这个词，你被一堆词包围着，突然你被扔进了LLM架构中。

让我们来了解单个token的生命周期，这就是接下来这部分讲座的主要内容。好的，让我们一起踏上这段旅程，了解token的生命周期究竟是什么样的。我给这一部分起的标题是“token在LLM架构中的旅程”。我首先做的是去ChatGPT那里，让它写一段关于朋友的短文。然后我从里面随机选了一句话：“一个真正的朋友会接纳你”。假设我们正在看这句话——“一个真正的朋友会接纳你”，它由五个单词组成。

假设这是我的当前输入序列，我们需要根据这个输入序列预测下一个标记。我将特别关注“friend”这个词，并从“friend”的角度来思考。现在，请站在这个词或这个标记的立场上——首先，这个标记（为了方便起见，我会交替使用“标记”和“词”，尽管它们并不完全相同，但为了简单起见，我就说一个标记等于一个词）。所以，请设身处地为这个标记着想。

现在你看到了什么？你看到我周围还有这些其他标记，对吧？那里有一个真正的接受者（true accepts），还有这四个其他标记，我习惯和它们待在一起，就像朋友们聚在一起一样。我习惯把它们当作我的邻居和朋友——一个真正的接受者（true accepts）和你。这四个邻居就是我们为这个标记选择的“朋友”。现在突然发生的第一步是——在LLM架构中，第一步就是隔离的步骤。所以目前我们正在观察这个阶段，也就是输入阶段。

在输入阶段的第一步是隔离阶段，即单词与其相邻词分离。想象一群朋友，每个人都与同伴隔开——这个词被单独隔离出来，我们孤立地审视它，这就是第一阶段。第二阶段本质上称为"令牌ID分配"，这意味着现在每个单词都被隔离，我们要为每个词或令牌贴上类似徽章或编号的标识。就像参加营地、入伍或任何团体活动时会被分配ID（比如学号），学校里每个人都有学号对吧？同理，每个令牌被隔离后都会获得专属令牌ID。我将其比喻为"领取徽章"的过程，而令牌ID的分配机制其实非常有趣——我们拥有一本令牌ID字典。

所以你可以把这想象成一本百科全书或一本标记ID的书。这本书里基本上列出了所有可能的标记，所有可能的标记就是所有可能的单词，然后每个单词都对应一个数字。这本书里不仅有单词，还有字符，甚至还有子词。所以这本书包含像a、b一直到z这样的字符，它甚至包含像“cr”这样的子词，这个词可能就在这个词汇表中。然后它还可能包含像“isation”这样的子词，这可以是一个子词。此外，它还包含像“token”这样的完整单词，这个词可以在这个词汇表中作为一个完整单词存在，“enter”也可以是一个完整单词，“begin”同样可以是一个完整单词。

所以你可以把这本标记ID的书想象成由字符、单词和子词组成。让我在这里写下来，这一点非常重要：这本标记ID的书本质上由字符、单词和子词组成。因此，我们确保每个被分离的标记或单词都能找到对应的编码，不会有任何被分离的标记或单词找不到对应的编码。熟悉字节对编码概念的读者会记得，为了创建这本标记ID的书，我们使用了一种称为字节对编码的方案。这是一种子词标记化方案，而GPT-2就依赖于字节对编码机制来创建其词汇表。这本标记ID的书也被称为词汇表。

然后它会从一个大型语言模型转变为另一个，比如说GPT-2的词汇量是5万，而GPT-4的词汇量可能更高，也许是10万。所以根据我们使用的大型语言模型，分配给某个词的标记ID可能会变化。我现在使用的这个大型语言模型词汇量是5万，这意味着有5万个标记，可能是字符、单词和子词的组合。接下来我要做的就是查看这个词汇表。

我要找到“friend”这个词在序列中的位置，并确定与之关联的token ID。现在“friend”这个词对应的token ID是2012，我会把这个数字记录下来——这就像是给这个词分配的徽章或学号。所以现在“friend”这个词的token学号就是2012。同理，其他所有token（即所有其他单词）都会获得类似的编号标识。这是整个流程的第一步，或者更准确地说，是第二阶段的工作：token ID分配。

那么现在想象一下，这个被孤立出来的标记朋友，它现在被赋予了一个徽章或编号——2012，这就是第二阶段。我没有详细解释这个标记ID簿是如何创建的，因为如果你想了解更多细节，有一个单独的讲座专门讲解如何为每个大型语言模型构建这种词汇表，它被称为"从零开始实现字节对编码"。这个讲座包含在"从零开始构建LLM"系列课程中。但现在请跟着我的思路——想象你就是这个标记朋友，你被孤立出来后获得了一个编号，然后你就进入了第三阶段。在第三阶段会发生一件有趣的事情：在此之前，你只关联着一个数字。

没错，但现在你将拥有一个庞大的数字向量，这些数字将与您相关联，这被称为标记嵌入分配。可以这样理解：假设我们有一个入学考试，共有768道题目，每道题本质上都在测试你的某个特征。现在我们来看“朋友”这个词，每道题会测试：你是名词吗？你有性别吗？你是动词吗？你是运动吗？你是情感吗？等等。

实际上，我们并不清楚这些特征或问题的具体内容，但我正试图向你解释，以便你能对词嵌入有一个直观的理解。想象一下，有768个类似这样的问题，每一个被我们单独提取出来的词都会被问到这些问题。然后，根据得到的答案，我们可以理解这个词的某些特性——比如它是一个名词、一项运动、一个形容词，还是总是出现在句子末尾的某个词；或者它是否与性别相关，是否与君主制相关，比如国王、公主、女王等等。

现在我们终于开始了解标记本身的意义了。在标记ID分配阶段，我们并未涉及任何意义层面的内容。但到了第三阶段的标记嵌入（token embedding），由于我们提出了一系列问题，就能获取到一些语义信息。根据这些问题的回答结果，每个标记都会生成对应的数值。比如这个"friend"标记，它会对每个问题产生一个数值（可能是0.1、0.2、0.1、0.3等）。假设我们设置了768个问题，那么就会生成一个768维的向量。需要特别说明的是，这个问题的数量（768）会因不同的大语言模型而有所差异。比如我们来看GPT-2模型，它的标记嵌入维度...

所以如果我们搜索GPT2的词元嵌入维度，会发现它是768对吧？但这里GPT2小型版也是768，而最大的GPT2模型则有1600维。因此这个768的提问数量实际上会因不同大语言模型而异。接下来我们要做的是：假设GPT2的提问数量是768，但请记住——如果词元进入不同的大语言模型，它可能会被问不同的问题。现在想象你是一个词元，刚领到编号徽章，突然就要回答这768个问题。当你作答后，所有答案会被收集成一个768维的向量——这就是所谓的词元嵌入向量。

所以现在，除了徽章之外，你还随身携带着你的结果，也就是说你有一个徽章，还有这768个数值的结果，这就是到目前为止发生在你身上的事情，对吧？这就是词元嵌入的阶段。再次强调，词元ID和词元嵌入的区别在于，词元ID不携带任何关于语义的概念，而词元嵌入在分配时会非常关注单词本身的含义。进行词元嵌入的原因是为了创建LLMs（大语言模型），你最终需要提取意义，对吧？你是在向模型教授一些关于语言的知识。

因此，这是一个非常关键的步骤。这些关于每个标记的问题或特征集合都会被收集起来。到目前为止，每个标记都有一个标识，并且每个标记都附带一个768维度的值表。此外，还有一个重要的因素是你在邻居中的位置。在这里，如果你看到“一个真正的朋友接受你”，那么“朋友”这个词位于句子的中间，也就是第三个位置。具体来说，“一个”在第一位，“真正的”在第二位，“朋友”在第三位，“接受”在第四位，“你”在第五位。所以，“朋友”出现在第三个位置，而这个位置也很重要。为什么位置很重要呢？

因为如果你说“狗追另一只狗”，看看这句话，你需要意识到这只狗和另一只狗是不同的。如果只按字面意思理解，如第三阶段所示，我们只取了词义，那么这两只狗的标记嵌入会是相同的。但实际上这是两只不同的狗，我们需要教会模型识别这一点。唯一能区分这两只狗的方法就是知道第一只出现在第二个位置，第二只出现在第五个位置。

因此，了解位置信息同样重要。类似于我们提出的768个问题，关于位置信息也会再次提出768个问题。需要注意的是，虽然不同模型这个数字会有所变化，但如果你固定一个特定的语言模型，那么在词嵌入中提出的问题数量与位置嵌入中提出的问题数量是相同的。以我们现在研究的GPT-2小模型为例，词嵌入中提出了768个问题，同样地，位置嵌入中也会提出768个问题。

那么这些位置或问题可能是什么呢？它们可能是类似这样的问题：你是处于序列的开头、中间部分，还是能够处理长距离依赖关系等等。实际上，没人确切知道这些问题具体是什么，但我觉得这是解释位置嵌入和标记嵌入最简单的方式。现在进入第三阶段，每个标记都有一个与之关联的标记嵌入——这是一个768维的向量。而到了第四阶段，根据所处位置，你需要回答这768个问题，对吧？

所以你还会有一个768维的位置嵌入与你相关联。现在想象一下，一个标记首先会受到标记ID的印记或徽章的影响，然后它会得到标记嵌入的结果。这些都是它需要回答的问题，也就是第一个测试。接着，这个标记会进入另一个测试，即位置嵌入，然后它再次拥有这768个值。每个标记都需要进行大量的处理。它基本上必须通过大量的测试。然后在第五步，我们所做的是将你的标记嵌入结果与位置嵌入相加。这样你就不必再分别携带这两个结果了。

你将它们合并在一起。所以现在768维向量的词元嵌入和768维向量的位置嵌入相加在一起，这就是所谓的输入嵌入。因此，这就是词元“friend”的输入嵌入。它是一个768维的向量。对于所有其他词元或所有其他单词，我们也会有类似的输入嵌入。但在这里，我向你展示的是词元“friend”的输入嵌入。这是词元嵌入加上位置嵌入的结果。所以现在你不必分别携带这两个结果。你只需要携带一个结果。

现在，那768维向量就是与你作为标记相关联的标识。这是你在后续旅程中最重要的区分特征。想象一下这段旅程：你最初是孤立的，被授予徽章后，先接受第一次标记嵌入测试，再经历第二次标记嵌入测试。最终，经过所有这些步骤，你获得了唯一能标识自己的东西——输入嵌入。你可以将其视为你的专属制服。这套制服是为你量身定制的，而你作为标记就要穿着它。与你同行的每个朋友（每个标记）、每个相邻词汇都会穿着不同的制服。为什么？因为它们的含义各不相同，对这些问题的回答也会不同，它们所处的位置自然也不一样。

所以他们需要分别回答这些问题。因此，每个标记的统一处理方式都会有所不同。到目前为止，我们所做的这五个步骤，本质上就是输入块或输入层中发生的事情。这是我们目前学习的第一部分。现在，我认为这里提到的这三个步骤对你们来说会很容易理解。首先是标记化。

其次是标记嵌入。第三是位置嵌入，这正是我们刚才看到的，然后标记嵌入和位置嵌入相加，得到所谓的输入嵌入。整个过程就是这样，记住这里的标记化文本。我们通过词汇表或标记ID手册看到了标记ID的分配。这就是每个标记经过输入块（这里是第一阶段的第一部分）后的输入嵌入。它们有一个统一的特征，使它们与其他标记区分开来。

好的，这就是第一部分——输入部分。只有当你拥有了统一的“制服”，才能进入下一个环节，也就是处理器部分。现在，每个标记（token）都穿上了统一的“制服”，就像《哈利·波特》里只有穿上特定学院的制服（比如格兰芬多、拉文克劳或斯莱特林）才能进入霍格沃茨一样。此时，每个词或标记都拥有了自己的“制服”，终于可以登上通往Transformer模块的“列车”了。

你看，这五个标记现在会一起进入Transformer模块。真正的朋友会接纳你的一切。现在每当我提到“制服”，你应该想到它代表的是一个768维的向量。Transformer模块并不理解单词，甚至目前还完全不懂单词的含义——它只知道每个标记都是一个768维的向量。而在Transformer模块中，神奇的事情将会发生：不同标记之间的含义会变得清晰可辨，模型自身也将逐步理解语言。

因此，变压器块本质上就是所有魔法发生的地方。第二个处理器部分才是真正一切发生的核心所在——就是这里的这个部分。这个处理器部分确实是所有奇迹诞生的地方。你可能会想：大语言模型究竟是如何运作的？虽然它们只是预测下一个标记，但它们似乎已经掌握了语言的某些本质。奇妙的是，它们与人类互动的方式，几乎就像你在和我这个真人交流一样。

它们能总结任务，擅长语法检查，帮我起草邮件，还能完成复杂的编码工作。这一切都归功于Transformer模块的运行机制。现在，每个标记实际上都会经历一段旅程，穿过卡车（比喻性表述）进入Transformer模块本身。好了，现在让我们来思考这个Transformer模块。

我们需要明白，Transformer模块就像一列由大量不同部件组成的火车，对吧？首先我们要看看Transformer模块这列火车本身的组成部分。我不会详细讲解所有部件，只是简单介绍一下Transformer模块中每个部件的作用。现在想象一下，这五位乘客被分配到了一个车厢里，而且他们现在都是768维的输入嵌入。

他们必须在一个Transformer块中经历一、二、三、四、五、六这六个步骤。那么这六个步骤是什么呢？你可以这样想象：如果把Transformer块比作一列火车，这六个步骤就像是连接在一起的六个车厢。一旦你上了这列火车，就必须和你的“邻居”一起经历所有这六个步骤。

第一步是层归一化（layer normalization），这意味着对768维的向量进行处理。现在让我们聚焦在“friend”这个词上。这个768维的“friend”向量会被归一化，即调整其均值和标准差，使得均值变为0，标准差变为1。这个步骤相对简单。然后我们来到多头注意力机制（multi-head attention）。这里我用不同的颜色标记了这个部分，因为这才是真正赋予Transformer块强大能力的创新之处。

我们本质上是在学习：当我们关注一个标记时，应该给予其他标记多少注意力？例如，当你看到“朋友”这个词时，应该对“真实的”和“你”给予多少关注？多头注意力机制实际上编码了关于上下文的信息。当你观察一个标记时，你会突然绘制出一张地图，显示所有其他标记的重要性。仔细想想，这对理解语言、理解句子本身的上下文或段落的上下文非常有帮助。比如我说：“我来自印度浦那，我讲……”在这里，如果你要完成下一句话，你需要知道应该更多地关注“浦那”和“印度”，因为那是我来自的地方，对吧？

所以你不需要太关注前三个标记，也许“So”这个词说明了注意力机制的重要性，它帮助我们理解句子的上下文并预测下一个可能的词。我们将在下一节课中更详细地学习注意力机制，但请记住，这是Transformer块的第二个组成部分。Transformer块的第三个组成部分是Dropout层。如果你已经学过神经网络，Dropout本质上是指如果有100个参数，且Dropout因子为0.5，那么你随机将其中50个参数设为0。为什么呢？

因为如果某些参数很懒，根本不学习任何东西呢？突然，如果其他参数现在被丢弃，意味着它们被设为0，这些参数别无选择，只能自己学习一些东西。因此，dropout是一种让懒惰参数重新活跃起来的机制。它提高了泛化性能，并防止过拟合。所以，如果你看到这里有一层dropout，然后还有一层dropout。在dropout层之后，

我们有一个跳跃连接或捷径连接。捷径连接的作用是帮助梯度通过另一条路径流动，确保不会出现梯度消失问题。然后经过归一化处理，接着是多头注意力机制和丢弃层（dropout），之后还有另一个归一化层，其功能与第一个相同。随后是一个前馈神经网络，这也是Transformer模块中非常重要的组成部分。具体来说，当前我的词元（token）维度是768，前馈神经网络实际上会将其映射到更高维的空间中。

这是768的四倍，然后它又将其压缩回768维空间。这种扩展-收缩机制确保我们探索更丰富的空间。我们探索的是具有更多维度和更多参数的空间，从而确保我们的语言模型拥有足够的参数来捕捉额外的复杂性。前馈神经网络正是深度探索中专家混合创新实际发生的地方。

最后我们还有一个dropout层，然后还有一个跳跃连接或快捷连接。记住这些加号，无论它们出现在哪里，都代表跳跃或快捷连接。它们确保梯度有替代路径流动，因为如果梯度以链式方式流动，一旦某个梯度很小，相乘后梯度就会变为零，或者如果梯度很大，相乘后就会爆炸。这可能导致梯度消失问题，使学习停止，或者导致梯度爆炸问题，使学习变得非常不稳定。因此，这五个标记必须经历这些不同的步骤。

每个标记都必须经历以下流程：先经过归一化处理，再通过多头注意力机制，接着是随机失活层和跳跃连接，然后再次归一化，随后通过前馈神经网络，再经过一次随机失活层，最后再进行一次跳跃连接。这就是Transformer模块的结构。如果你回顾我们在课程开始时看到的示意图，你会发现这里描述的完全一致——每个标记都必须依次通过层归一化、注意力机制、随机失活、跳跃连接、前馈神经网络的层归一化、随机失活以及跳跃连接。这就是每个标记必须遵循的完整路径，看起来确实是个相当繁琐的过程，对吧？

首先，我必须完成这五个步骤才能拿到我的制服，除此之外，之后我还得逐一检查每个变压器模块，并完成这些步骤。但还有一层额外的复杂性——就像这个变压器模块一样，一个大型语言模型包含多个变压器模块，对吧？比如GPT-2，它有多少个变压器模块呢？如果你看一下GPT-2本身，GPT-2小型有12个变压器模块，GPT-2中型有24个，GPT-2大型有36个，而GPT-2 XL则有48个。所以，即使我们现在只看小型模型，每个变压器模块都包含所有这些步骤。

所以现在每个token基本上都要重复这12个步骤 嗯哼。因此这就是为什么——如果你把一个transformer块比作火车的一节车厢——总共有12个这样的transformer块相互连接，而每个token都必须完整通过这12个transformer块。整个旅程变得极其繁琐。你看我已经把流程画出来了对吧？这12个transformer块就是每个token必须穿越的关卡。就像这位token朋友，它得先通过第一个transformer块...

它必须经过第二个。同样地，它必须经过第三个，以此类推，必须经过这全部的12个变压器模块。所以这里的第12个模块就像一段非常繁琐的火车旅程，每个标记都必须严格遵循这一路径。获得统一是一项艰巨的任务，我们需要经历五个步骤。而通过处理器的过程更是艰难，因为你必须再次经历这全部的12个步骤。这就是处理器中实际发生的事情，也就是我们刚才在处理器中看到的第二部分。

情况是这样的：我这里展示了一个Transformer模块，对吧？如果我们使用GPT小模型，你可以想象这个模块被复制12次；如果使用最大的GPT-2模型，则需要48个这样的Transformer模块。而现代GPT模型可能包含96个甚至更多的Transformer模块。因此，每个token现在都必须通过这些模块进行处理。请记住，token的维度通常在通过Transformer处理后仍保持不变。比如说输入——我们之前看到的统一输入是768维的，如果你还记得的话，那是一个768维的向量，对吧？

在经过所有这些Transformer块后，经过12个Transformer块后，它从这12个Transformer块中出来时保留了其维度。因此，它仍然有768个维度。所以现在，一个真正的朋友会接受你已经从Transformer块中出来，并且它们自然仍然都有768个维度，当然，值已经发生了变化，对吧？然后它们现在进入输出层。这里有一个归一化步骤。所以如果你在这里看到，这里有一个归一化步骤，被称为最终层归一化。

所以归一化这一步在这里被提到了。每一个768维的向量都会再次经过这个归一化阶段，然后我们还有最后一层，这层非常重要。现在记住我们已经到达了最后一层，我们有一个真实的，所以这是一个真正的朋友需要你，每一个都是768维的向量，对吧？现在我们需要以某种方式将这768维转换成我们的词汇量大小，也就是50,000，因为现在我们需要预测下一个标记。

那么每个标记本质上都会通过一个神经网络，其大小为50,000，或者说大小为768乘以50,000。因此，当这些向量与之相乘时，每个标记会生成50,000维的向量。所以现在每个标记的大小是——一个真正的朋友需要你——对吗？每个标记在经过输出层后的大小将是50,000。这一层也被称为输出投影层，每个标记经过输出投影层后，其维度等于词汇表的大小。

记住我们的词汇量是50,000。这个50,000来源于词汇量，我来解释为什么需要维度相等。词汇量和最后一步是选择下一个标记，对吧？现在我们到达最后一步时得到了什么？我们有五个标记或"真正的朋友接受你"，每个标记都有一个50,000维的向量。接下来我们要做的是查看这50,000个维度，找到具有最高值或最高概率的索引。

然后我们会在这里找到那个索引，接着寻找它对应的标记。就是这样。所以如果这里的索引是第一个“The”，那么“a true friend accepts you”在这里有多个输入输出任务。当输入是“O”时，输出应该是“true”；当输入是“O true”时，输出应该是“friend”；当输入是“O true friend”时，输出应该是“accept”；当输入是“O true friend accepts”时，输出应该是“you”；当输入是“O true friend accepts you”时，输出会是其他东西，也就是“for”。所以“a true friend accepts you”，如果你看这个句子并且要预测下一个标记，这不仅仅是一个下一个标记的预测任务。

同一句话中包含多个输入-输出任务。当O作为输入时，真实值应为输出等，这些输入-输出预测任务是什么？而这里唯一与我们相关的是最初的下一词预测。当然，一开始我们不会得到好的词元，对吧？但我们会有一个基于实际值的损失函数。


So this is the actual next token, which I want but the predicted ones initially will be completely far off and That's when backpropagation comes into the picture when all the parameters which are there. They are actually optimized We'll come to that in a moment. But for now, just let me explain the final step again We have every token and now every token is associated with a different uniform Whose dimensions are 50,000 Why do we have dimensions 50,000 because we have to predict the next token for every word, which is over here So for O we have to predict the next token. 

So we look at that index, which has the highest or that token ID Which has the highest probability here We go to the book of the words or book of token IDs and then we reverse map The word which is corresponding to that token ID. So if this token ID here is let's say 555 Or this token ID is 5000. I Go to the word here, which is 5000 and I ideally want true over here But initially when things are not trained, maybe I have for So the actual prediction might be for Similarly, I'll get the actual predictions for true. 

Maybe the highest token ID is your friend here and then I'll get the highest token ID here and that's how I'll predict the next token for each of these and Then I'll find the last term between the actual value and the predicted value That's how the entire architecture of the LLM is structured. So now if you go to the output layer, which is my final layer Here you see we have two things which are chained to each other right we have the final layer normalization layer Which is connected to the output layer and then we have the matrix for next token prediction. This logits matrix is this This logits matrix is this this one which I showed to you right now and we use that to make the next token prediction So now you might be thinking that what are all the parameters which are optimized here So right from the start it's in these token embedding values which are there. 

We do not know them a priori So let me mark the parameters which are trained by a star these we do not know a priori So these are trained positional embedding assignment assignment. We do not know a priori. So these are trained then Every single aspect of the transformer block has some parameters multi-ed attention as parameters that is trained the feed-forward neural network has parameters that is trained and There are 12 or 24 such blocks that even increases the parameter size further So there are a huge number of parameters throughout this entire process Which are trained even this final neural network. 

It has these many parameters which need to be trained all these parameters add up together to lead to the total number of parameters, which are 175 billion or Maybe a trillion so think about the engine which we started out, right? We started out with knowing that we started out with thinking. Okay. This is the LLM engine But how does the LLM engine actually work? What are the parameters beneath the LLM engine and where are these 175 parameters actually going 175 billion? So now we have taken a look at the detailed architecture which is the input which is the processor and which is the output so and we have seen the journey of a single token, right a Token essentially first goes through the input phase which it is isolated It is assigned a token ID or a badge then it's given one quiz or one set of 768 questions That's the token embedding it encodes meaning Then it's given a second set of questions, which is positional embedding that encodes its positional value We add the token and the positional embedding that gives the input embedding or the uniform for every token with this uniform different tokens sit on the train to the transformer block and Each transformer block essentially has the normalization layer multi-head attention dropout normalization again feed forward and dropout interspersed with two skip connections and There are 12 such blocks like this in GPT 2 in GPT 2 excel there are I think 48 such blocks but in the advanced GPT's there might be 96 or even more number of blocks like this So every token needs to go through all of these blocks when it comes out of all these blocks its size still remains 768 Then it goes through one more normalization layer size is still 768 and then finally we have an output layer where for every token which we have It's converted into a vector now with size of 50,000 which is equal to the vocabulary size And then we look at every will we look at every token basically It's 50,000 dimensional vector and then we look at that token ID, which is the highest probability and we use that to predict the next token So in one sequence, which we have we have multiple input output prediction tasks So if we have a sequence with five tokens there are five input output prediction tasks, which essentially give our loss function and Then our loss function is basically backpropagated and all the parameters are optimized all these 175 billion parameters which come through in several stages in token embedding there are parameters in positional embedding there are parameters then in Several aspects of the transformer block there are parameters in the output layer. 

There are parameters all of these Parameters are essentially optimized through back propagation and Then ultimately what we have is a model which has intuition about language itself and it can also predict the next token So next token prediction is the task as you see here We are predicting the next token and we are comparing with the actual value That's the task but in this task since we have so many parameters the byproduct is learning the language itself So in today's lecture, my main purpose was to take you through the journey of a token Think from the point of view of what happens with one token Try to open this engine Try to open this engine of the LLM and really try to see how the engine is actually working and I hope I have conveyed that to you the reason I Constructed this analogy or a story of a journey of a token is for you to really understand what goes on inside the LLM architecture because without understanding that We cannot move ahead to the next part which is attention Now The plan is that in the next lecture I'm going to motivate why we need attention in the first place Then we are going to look at self attention. Then we are going to look at multi head attention Then we look at key value cash So if you see the next plan is the need for an attention mechanism Then we have self attention and then ultimately we have the multi head attention mechanism So all the future lectures are planned in a lot of detail as I mentioned This won't be a small series with 5 or 10 minute videos Every single video of this series will be pretty long around 40 to 45 minutes and I will plan to go through the entire steps So multi head latent attention is a very important concept But I want all of us to be at the same page when we actually understand that concept Thanks a lot everyone and I really look forward to seeing you in the next lecture Please make notes along with me. This series can get a bit difficult I'm trying to distill the concepts in as easy to understand a manner as possible But still there might be some challenges across the way. 

So please make notes so that you strengthen your concepts Thank you everyone, and I look forward to seeing you in the next lecture Hello everyone. My name is dr. Raj Dhandekar I graduated with a PhD in machine learning from MIT in 2022 and I am the creator of the build deep seek from scratch series Before we get started I want to introduce all of you to our sponsor and our partner for this series in video AI All of you know how much we value foundational content building AI models from the nuts and bolts InVideo AI follows a very similar principle and philosophy to that of us. 

Let me show you how So here's the website of InVideo AI With a small engineering team They have built an incredible product in which you can create high quality AI videos from just text prompts So as you can see here I've mentioned a text prompt create a hyper realistic video commercial of a premium luxury watch and make it cinematic With that I click on generate a video Within some time I am presented with this incredible video, which is highly realistic What fascinates me about this video is its attention to detail look at this the quality and the texture is just incredible And all of this has been created from a single text prompt That's the power of InVideo's product. The backbone behind the awesome video Which you just saw is InVideo AI's video creation pipeline in which they are rethinking video generation and editing from the first principles To experiment and tinker with foundational models They have one of the largest clusters of H100s and H200s in India and are also experimenting with B200s InVideo AI is the fastest growing AI startup in India building for the world and that's why I resonate with them so much The good news is that they have multiple job openings at the moment. You can join their amazing team I am posting more details in the description below Hello everyone and welcome to the next lecture in the build deep seek from scratch series Today we are going to learn about a very important concept and that is the need for an attention mechanism First let me quickly summarize what all we have covered so far in this lecture series And then let's come to the topic of discussion today So two lectures back in the lecture called deep-seek basics, we looked at the four phases of the deep-seek Architecture or rather the four phases through which we are going to cover this lecture series The first phase is the innovations in the deep-seek architecture The second phase is the training methodology The third phase is the GPU optimization tricks and the fourth phase is the model ecosystem In the previous lecture, which was titled LLM architecture We started looking at phase one in particular Our main goal is that in the next two lectures or two to three lectures We have to start understanding about the multi-head latent attention Which is the first major innovation in the deep-seek architecture However to understand MLA or multi-head latent attention We cannot directly start looking at that concept we need to slowly work our way towards it So first we looked at the architecture of the LLM itself We saw that the architecture of the LLMs looked something like this. 

We had

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

First we had the input section, then we had the processor and then we had the output section. Each section essentially had different building blocks. So we saw that to really understand how the if you think of the LLM as the engine right and to truly understand how the engine works, we really need to open the engine and try to see what lies underneath.

And how does essentially LLMs learn to predict the next token or the next world? If you think of this analogy as similar to the motion of a car, we see that if you put fuel in the car, the car essentially moves, right? So something happens in the engine and the car moves. Similarly, for LLM, the fuel is the sequence of words which you feed in as an input to the engine which is the LLM and the output is the next world prediction or the next token prediction. And that's why we also called LLMs as next token prediction engine.

Now this engine has huge number of parameters. GPT-3 has 175 million, GPT-4 probably has around 1 trillion and GPT-4.5 which was released recently probably has around 5 to 10 trillion. So to truly understand how this engine works, we opened the black box and we looked at these three aspects. 

But I did not directly explain these three aspects to you. I told you that what if you put yourself in the shoes of a token or a world and you imagine yourself going through the different phases. So if you are a token, first you get assigned a token id, that's your badge, then you get assigned a token embedding vector, then you get assigned a position embedding vector, the token and the position embedding are added together and that's the input embedding which is your uniform. 

Now similarly whoever are your neighbors they also have their uniform or the input embedding and all of you together on board the train to the transformer. Each transformer block has multiple components such as the normalization, multi-head attention, dropouts, skip connection, again a normalization layer, a feed forward neural network and a dropout layer followed by another skip connection. This is just one transformer block. 

There are multiple transformer blocks like this, it can be 12, 24, 96 etc. So as a token with your uniform which is your input embedding you have to go through all of these transformer blocks and then when you come out you have a layer of normalization and finally you have an output layer in which if you are a 768 dimensional vector you are mapped or projected into a 50,000 dimensional vector. Why 50,000 because that's the vocabulary size and that essentially helps us to choose the next token.

That is the whole life cycle of a token as it goes through the LLM architecture. In today's lecture we are going to focus on one single aspect of this entire architecture and that's this multi-head attention. I want you to appreciate where this multi-head attention comes into the picture. 

So out of all of these steps which the token goes through the multi-head attention comes in the transformer block and within the transformer block also it comes after the normalization layer in this particular case. So today we are going to understand multi-head attention but to do that first we will understand what is attention itself and what is the need for the attention mechanism. Why do we start talking about this term attention? Why has it got so much popularity in recent times? So our today's goal is to motivate the concept of self-attention. 

We are not going to see the mathematics of self-attention mechanism today. Today the whole goal is to try to understand why we need attention in the first place and why it has been such a game changer for large language models. Just think about this right out of all of these steps which a token goes through the most important step lies within the transformer block lies within one step of the transformer block and that's the attention. 

That's why I marked it with a different color here. This block of the train essentially gives all the properties which have made LLM so good at understanding language. So to truly understand how attention works I have created this separate lecture for you in which we'll try to motivate the need for an attention mechanism.

Before understanding why we needed attention and how it has changed the field first let us deep dive a bit into the field of generative AI itself. So let's rewind and go back to history a bit. I believe it is essential to appreciate the attention mechanism. 

In 1960s there was this ELISA chatbot. You can think of this. I would not call it a large language model.

I would call it a chatbot. It was the first natural language processing chatbot which was meant to be a therapist. So if you ask please tell me what's been bothering you I can say that I am having a hard time learning AI. 

Can you help me? Do you believe it is normal to be having a hard time learning AI? So see it's like not very helpful but remember that it's 1960s. So at that time this was considered to be a revolution. So here you can see the original program was described by Joseph Weizenbaum in 1966. 

Compare this to chat GPT right now where we say that I want to learn about AI. Can you help me? And let me switch the model to GPT 4.0 so that I get a quicker response and then here you will see that I am immediately presented with a list of actionable items which I can start implementing to learn about AI. So within a span of 64 years we have come a long way in the field of natural language processing and understanding language itself. 

So let's see what happened between 1960s to 2020s right. So 1960s is where the ELISA chatbot came out. Then in 1980s and 1997 there were these two major fundamental building blocks of the field of generative AI or language modeling and you can think of those as recurrent neural networks which came out in 1980s and long short-term memory networks which essentially came out in 1997.

Both of these built on top of the neural network boom which happened in the 1970s. So people discovered that neural networks can do a bunch of awesome things but the real issue which neural networks have is that neural networks essentially neural networks can't deal with memory and that's the biggest issue with neural networks. Why do you have to deal with memory? Because imagine that if you're predicting the next word right or if you're generating some new text you would want to know what has come at the start of a paragraph. 

Just having a small memory does not help. Context is very important and this word will come up a lot in today's lecture context. Context essentially means that if there is a huge paragraph being given to the model and we want the model to do some task or just answer some question just looking at the sentence the word of the sentences which come immediately does not help. 

We need to have a context in mind of what was being told at the beginning of the paragraph itself. So if I say that let's say I am from Pune, India and then after that let's say I write a huge bunch of different things. I write a huge bunch of sentences and then finally I ask what language do I speak. 

Now to generate a response based on this question which has been asked all these things in the middle are not very important but I need to have a context of what came at the beginning of this paragraph because that's what will help me really answer this question. That's why you need to have memory and neural networks had no provision to encode memory really but recurrent neural networks and long short-term networks solved that problem. Let me quickly show you how they solve that problem.

So people typically used RNNs or LSTMs initially for sequence to sequence modeling or what is called as sequence to sequence translation tasks. So simple sequence to sequence translation tasks can be just language translation. If you want to take a bunch of words and if you want to translate those from one language to another language you can use a recurrent neural network.

So recurrent neural network has a bunch of hidden states like this h0, h1, h2, h3. Let's say these are the input words I will eat and I want to translate it to French which translates to Je vais manger which has been written over here. The way I can use a recurrent neural network to do this translation is that I have an encoder block and I have a decoder block. 

Both these encoder and decoder blocks have something called as hidden states which you can think of as vectors or matrices. So here is the hidden state h1, hidden state h2, hidden state h3 of the encoder and then we have the hidden state s1, hidden state s2 and the hidden state s3 of the decoder. Let's visualize what happens in the encoder block. 

In the encoder block you first give one the first word of the input sentence which is I that gets converted to the first hidden state h1 then you give the second word. Now keep in mind to get the second hidden state h2 you use the second input but you also use the previous hidden state. That's how memory is captured in recurrent neural networks. 

The previous hidden state is used to compute the current hidden state. Similarly when you come to eat that's the input x3 but to compute h3 which is the final hidden state you use the previous hidden state which is h2 and h2 encoded h1 or h2 had the knowledge of h1. So h3 now has the knowledge of h1 as well as h2. 

h3 has the knowledge of the past it retains memory. So h3 can be thought of as that one vector maybe it's like a 500 or a 1000 dimensional vector which essentially contains all the information of what the input essentially means and this is the only higher dimensional vector which is passed to the decoder. The decoder then takes a look at this vector and then every hidden state of the decoder starts decoding it. 

So the decoder imagine you have the decoder right and it's the relay race. So let's think of it as a relay race. The baton is with h1 it passes it to h2 the h2 passes it to h3. 

Now h3 has the baton which passes it to the decoder. Now I'm the first hidden state of the decoder. I have the baton. 

I decode the first token which is JOR. Then I pass it to the second then I decode the second token and then I decode the third token. This is how language to language translation works in recurrent neural networks. 

LSTMs are a bit different to predict the current hidden state based on the previous hidden state. They also maintain something called as a cell state. So LSTMs have a provision for taking into account long memories as well as short memories.

So that's essentially to increase the context window they deal with. But the architecture fundamentally remains the same. It's just that the mathematics beneath is a lot more complicated for LSTMs. 

Now do you see a problem with this? Take a look at this example which we had for discussing about context right. What is the problem with recurrent neural networks when let's say you have big paragraphs to deal with. Let's say you have a big paragraph and you want to translate that entire paragraph.

What is the problem with recurrent neural networks in doing that? So let's take a simple example so that I can explain that problem to you a bit better. Imagine that you have this paragraph okay. I have constructed this randomly from chat gpt.

I want you to do the following. I want you to first read this paragraph once maybe take 30 seconds or one minute or more to read this paragraph just once. Then close your eyes and then translate it into any language you want. 

You can translate it into Hindi, Spanish, French, German whatever language you speak other than English. So close your eyes and translate it into another language. Were you able to do this? It's impossible right because when you close your eyes you essentially probably remember some summary of what you have read. 

You don't remember exactly each and every word but to translate we have to look at each and every word and translate it individually right. That's the exact same problem which is happening with recurrent neural networks. Why? Because we are relying on only one hidden state or one vector to capture all the context of what has happened in the past. 

So it's like closing your eyes and then trying to remember the entire paragraph. There is so much pressure on this one hidden state or this vector to recollect all the memory that if it's a huge paragraph how can you compress all of that information in a 500 dimensional vector or a 1000 dimensional vector. Naturally some information is going to get lost and remember the decoder does not take input from the first hidden state or the second hidden state.

It only takes the final hidden state as the input. That's where the main issue arises. The context which goes from the encoder to the decoder is only from the final hidden state.

That is why this is called as the context bottleneck problem. The final hidden state is like you when you closed your eyes and when you try to memorize the entire paragraph it's impossible to do that. You cannot put so much pressure on one vector.

So context bottleneck in recurrent neural networks is really not good for retaining a long range context and this serves as a great pathway for understanding why attention comes into the picture. So now if you look at the recurrent neural network let me rub some things over here or erase some things. What do you think is the ideal solution to this problem? If let's say this is the problem which we are encountering right all the context cannot be captured in one single vector.

What is the solution to this problem? Well the solution seems to be that let's say when we are decoding when I'm decoding this let's say when I'm decoding from the first hidden state of the decoder instead of having access to just the final hidden state what if I have access to all the hidden states of the encoder and all the inputs also. So if during decoding I have access to all of the inputs and then I can make predictions such that okay if I'm decoding the first token I should give more importance to the first hidden state and I should not give importance to the second hidden state and I should not give importance to the third hidden state. What if I could create a mechanism which can understand the relative importance which needs to be given to different tokens. 

This is the most important part of this lecture so please try to focus over here. The main thing which can solve the context bottleneck problem is that when I'm decoding what if I develop a mechanism which can try to quantify how much importance I should give to every hidden state. So while the first decoding is taking place let's say I give 80 percent importance to the first hidden state, 10 percent importance to the second hidden state and 10 percent importance to the third hidden state. 

If I have this information along with me then the context bottleneck problem is solved right because even if the paragraph is long if I can attend to all the tokens in that paragraph I can essentially also give importance to something which has come in the past. So let's say this is the sentence and I'm predicting something here. In the attention mechanism I can just try to see how much attention to give to the first token, the second token, third and the entire paragraph and then I can say that I need to give maximum attention to these tokens over here. 

So you see I'm already starting to use the word attention over here. You can think of it as relative importance to individual tokens in the paragraph. So the ideal solution is that we need to selectively access parts of the input sequence during decoding. 

So when you are decoding let's say I want to make certain predictions so that if I'm decoding from the if I'm decoding the first token right, this alpha 1 1 is the relative importance given to the first hidden state, alpha 2 1 is the relative importance given to the second hidden state, alpha 3 1 is the relative importance which is given to the third hidden state. So I want to be able to make predictions such that alpha 1 1 is 100 percent, alpha 2 1 is 0 and alpha 3 1 is 0 something like that. But you see here instead of just relying on h3 I'm now relying on all of my previous hidden states and not just that I'm quantifying how much I should rely on each of the previous hidden states. 

So again this is important we need to selectively access parts of the input sequence during decoding and I want to explain this to you with this example again let's say I take a screenshot here okay and then I put it over here let's try to understand what this statement actually means. So if I ask you to translate this paragraph how would you actually do it you would probably start you would actually probably start with let's say this right let's say you start with this that how much words you can look at at one point let's say that's your context window you cannot look at more words than this at one point then what your mind will do is that for your mind the rest of what comes in the paragraph is irrelevant right so you will mask out all of that in your mind you will mask out all of that it's not relevant so what you will do is that all of these other things are not important to you you just want to look at the current context and you want to selectively hide out every other thing right so what your mind does is that your mind pays attention this is very important your mind is paying attention to this part of the paragraph and then it's starting to translate only this part of the paragraph and after this is translated then your mind will move to the next maybe next five words after this is translated your mind will move to the next five words and then you will go to the next five words etc so you see what you are doing here at a particular time your mind is making a decision that i want to pay the maximum attention to these tokens when you are looking at the first first context window and then your mind is also making certain quantitative judgments that don't pay any attention to the rest of the tokens at all don't pay any attention to these so that's what is actually meant by selectively access parts of the input sequence during decoding right selectively accessing parts of the input sequence that's the most important sentence to understand the attention mechanism it means that only pay attention to that part of the input sequence which is important at that moment and this seems like an intuitive thing to do right but it's actually the most important thing which helped language models to become better so imagine that you want to find a spelling mistake in a piece of paragraph right what do you do you want to find out where the spelling mistake is you go through the paragraph step by step so you pay attention at one then you pay attention to the next etc this is what llms also need to do they need to selectively pay attention to the part of the sequence and then find that part let's say which has the spelling mistake for example right so this is the main purpose of why we need attention while decoding we can quantify how much attain importance or attention needs to be given to each input token that is what i am getting at at the moment now the first paper which really implemented this was not the attention is all you need so if you see this paper this paper has around 1.170,000 citations right huge number of citations so many people actually believe that the many people believe that the attention mechanism was first introduced in this paper but the first paper which actually introduced the attention mechanism is this bahadanav attention mechanism so if you search bahadanav attention and you click on this you will see that this is the first paper which actually implemented the attention mechanism for translation tasks this paper i think came out in 2014 and this was published at iclr in 2015 their main purpose was they essentially implemented this architecture in which we can selectively decide how much attention to pay to each hidden so that's what's called a1, a2, a3 that's the attention score and based on that you can do the translation tasks and they essentially proved that we can do sequence to sequence translation in a much better manner if we use the attention mechanism so just keep this particular paper in mind so here you can see that on the on this here we have the english sentences and on the y-axis we have the french translation and the diagonals are essentially showing the english word paying the most attention to the translated french word and what's cool thing about this is that the order is not the same so the agreement on the european economic area right european economic area that's english but the way it translates to french is economic european right so it translates to area economic european so it's not direct word to word translation and still the attention mechanism essentially figures out that see this has the maximum uh this is brightest right which means the european year in french corresponds to the european year in english although they both don't occur at the same position so the european in english occurs at position number five and the european in french occurs at position number seven so all the maxi all the brightest areas are not on the diagonal there are some which are off the diagonal itself and even that is captured by the attention mechanism so the coolest thing about this plot is that this off diagonal elements because translation does not always happen word by word right there are some words which come before in some languages which are which come after in some other languages based on our nouns words etc are arranged so that's the cool thing about this paper this was the first one where the attention mechanism was used for translation task and then attention is all you need paper is where the transformer block was introduced and the attention mechanism was integrated within the transformer block that's the main advantage or that's the main unique point of this paper attention is all you need right so what happened is that rnn's and lsdm's are quite good for language translation but they have this context bottleneck problem uh so researchers figured out that rnn architectures are not really needed for building deep neural networks so 2014 is where the badana attention mechanism was introduced where they had the rnn so they retained the rnn plus they had the attention mechanism so essentially in badana attention you can think of the encoder decoder block in a similar manner but just they added this attention mechanism uh three years later which means in 2017 researchers essentially figured out that rnn architectures are not even needed for building deep neural networks for nlp and they proposed the transformer architecture so think about the field which evolved in this way right 1980s was rnn 1997 was lsdm actually 1966 was elisa let me write this also then 2014 was where the attention mechanism was introduced but that was still attached to rnn's the encoder decoder architecture of rnn's 2017 is when researchers figured out that rnn's are not needed for natural language processing tasks so rnn's were scrapped out and then essentially the attention mechanism remained but it was coupled with the transformer block that's the main difference between the 2014 paper and the 2017 paper in the 2014 paper the rnn block was still there but that was then removed and then that was replaced with the transformer block or the transformer architecture in 2017 and then it led to this full architecture right we now have the transformer block and within that we have the attention right so this is the gpt architecture this is not the architecture in the original transformer paper the original transformer paper had an encoder as well as the decoder whereas this architecture which i'm showing you right now only has a decoder don't get confused by this at the moment the main purpose of this today's lecture is for you to understand why attention needed to be introduced in the historical perspective of natural language processing and the main reason is that the one sentence you need to keep in mind is that we need to selectively access parts of the input sequence during decoding and first attention was introduced in rnn's itself then researchers figured out that okay let me get rid of the rnn's still let me try to merge the attention mechanism somewhere and that's when they merged it with the transformer block over here that was in 2017 then in 2018 is when the gpt architecture came out so then it's the attention plus gpt so gpt is based on this original transformer architecture but instead of having the encoder and the decoder block it only has the decoder block as you see over here and it retains the attention mechanism over here so the thing which we talked about why attention mechanism is needed it manifests itself over here in this part of the llm architecture now we are going to start looking at if you take a look at next token prediction tasks what is self-attention really and what are context vectors and what is the main purpose of the attention mechanism so let's what is the main purpose of the self-attention mechanism for next token prediction so let's start learning about that now now that we have understood about the attention mechanism and the history of the attention mechanism let's try to look at this term self-attention what does self attention actually mean so self-attention means that it's a mechanism which allows every position in the input sequence to attend to all the positions in the same sequence what this means is that

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

If I have a sentence such as the next day is bright, self-attention essentially means that until now if you looked at the RNN, we saw the attention which needs to be given from the decoder to the encoder. So if the first decoded word is a French one, we are basically looking between two different sequences. So the English sequence is this, let us say the English sequence is this, I will eat and the French sequence is this, this is the French sequence and we are looking at if you are doing the first decoding, how much attention should you give to the English sequence.

So here the attention is between sequences, it is not within the same sequence. Self-attention is on the other hand when you are predicting the next token which is typically done for the LLM, since LLMs predict the next token, they are not specifically trained for translation tasks. To predict the next token, you essentially do not have different languages, you just have a bunch of data.

So instead of having attention between two sequences, what you do is that you just take the same sentence and let us say if you take a look at next, you try to find out if you look at one token or one word, how much attention should I pay to all the other tokens in the sentence. That is the most important thing to understand over here. If you look at one token essentially or one word, then how important are the neighboring words to that particular token, why is this knowledge, can you try to think why this knowledge is important for us, why do we need to encode the knowledge of the tokens which essentially surround a given token.

The reason this knowledge is important to us is because when you are predicting the next token, you essentially need information about the context of a sequence, you need information about how different words relate to each other. So again taking the same example, let us say if I say that I am from Pune, India, I speak, let us say if this is the sentence right, if you look at speak, I need to know that when I look at speak, the maximum attention needs to be paid to Pune and India, maybe all the other words are not as relevant because my dialect what I speak is very heavily influenced with the region which I come from. So, when you look at a cloud of words and if your transformer architecture or if your LLM engine has information about how one word relates to other words which are surrounding it and how much importance needs to be paid to the different words surrounding it, then it just becomes very very easy to predict the next token, if you look at one token.

So that is the reason why the self-attention mechanism becomes very important. If self-attention mechanism was not there, you would have lost this contextual information about how other tokens relate to a given token which we have chosen and I hope now you understand why it is called self-attention. In this case, when we are looking at sequence-to-sequence language translation, the attention is between sequences but self-attention is when we look at one sentence itself and we look at tokens within the sentence and we essentially see how these tokens are relating to each other.

So let us take the same example again, the next day is bright. So the next day is bright and remember that when these tokens go to the transformer architecture, they are now vectors as you have seen before, they have this uniform now remember, every token has a uniform which was a 768 dimensional vector here, that is the input embedding right. So whenever I am showing these blocks here, it essentially means a vector.

So the next day is bright, these are all vectors now, for a transformer it does not understand words, it does not understand sentences also, all it knows is that every token is a vector. So the is a vector which I am calling x1, next is a vector which I am calling x2, day is a vector which I am calling x3, is is a vector which I am calling x4, bright is a vector which I am calling x5 right and now if I am looking at a specific word let's say next as I showed over here, I want to see if I look at this word next, how much attention should I give to all the other tokens and this attention is given by alpha to 1 or I am calling it a symbol alpha to 1, why to 1 because next is the second token and I am wanting to find the attention score between the second token and the first token, that's alpha to 1, this will be alpha to 2, this will be alpha to 3, here will be alpha to 4 and here will be alpha to 5. I essentially want to find out all the attention scores if I am looking at a particular token, so this is called as a query, the token which I am focusing on right now that's called as the query, query token and I want to find out if I am looking at the query, how much attention should I give to all the other tokens, these are also called as keys sometimes in the common nomenclature right, so ultimately what I want to do, I want to take this information let's say I get these attention scores, I want to somehow take all of this information and transform this vector from an input embedding vector to a context vector, now here is very important distinction which I want to make, currently next is a input embedding vector right, so it contains token embedding plus positional embedding but context vector is something very different, so if this is the input embedding vector for next and if I plot the context vector in the same space here, so this is the context embedding vector for next, the context vector is actually much richer than the token embedding vector, why? Because the token embedding vector or the input embedding vector contains no information of the neighboring words but now my context vector consists of information of my neighbors also, that information is now baked into my input embedding, so if you have an input embedding, if you have the input embedding vector which is the uniform which I talked about right and if you augment this input embedding vectors with context about the neighbors, we will see how this augmentation is done but essentially this leads to something which is called as the context vector, so the whole goal of the attention mechanism or the self-attention mechanism is to convert all the input embedding vectors to context vectors, so all of these uniforms, so we saw these uniforms right, all tokens have a 768 dimensional uniform, when they come out of the normalization layer and when we go to the multi-head attention layer, what comes in the attention layer is an input embedding vector, what comes out of the attention layer is a context vector, so something much richer comes out after we exit the attention block and that's why I marked it with a different color, the reason it's richer is because now it encodes information of other tokens also, so it retains context, so context vector is an enriched embedding vector, it combines information from all the other input elements, so in self-attention context vectors play a very crucial role, their purpose is to create enriched representations of each element in an input sequence by incorporating information from all the other elements in that sequence, so this is again keep this thing in mind that input embedding vectors only contain information about that word or that token, it might encode information about the meaning of that word and its position, but it has no clue of the neighbors, context vector has clue about the neighbors because neighbors are so important right, when you look at a sentence, when you look at a paragraph, individual tokens don't mean anything, it's only how they relate with the neighbors that essentially produces the context of that paragraph and why is this needed in LLMs, it's needed to understand the relationship and relevance of words in a sentence to each other, actually this is that fundamental thing which has made LLMs so so much better, so if you look at this advancements in history right, ELISA, RNN, LSTM, until here attention was not there, 2014 I believe is a very critical point that was 10 years back, when the attention mechanism was introduced and then people started thinking that oh instead of looking at words in isolation, what if I take a step back and try to see how different words essentially relate to each other, so then we are exploiting the maximum richness from text because just like images are made up of patterns of pixels, text or paragraphs only make sense if you take a look at all the words together and how they relate to each other, so now the question is that okay you have an input embedding vector, let's say for next, how do you convert it into a context vector, so you have an input embedding vector, how do you go from the input embedding vector to the context vector and I want you to think about this from the first principles, pause this video for a moment and think about this right, you have the input embedding vector and let's say you have these attention scores, how will you modify the input embedding vector so that somehow these attention scores are taken into account and you have a context vector, so you can pause here for a moment, first you can try to also think about how these attention scores themselves are computed, okay, so the simplest thing to do is that let's say if you have this vector right and if you have all the other vectors, why don't we take a simple dot product, so you have the input embedding vector for next, you have the input embedding vector for though, just take a dot product between these two that will give you alpha 2 1, just take a dot product between next and next that will give you alpha 2 2, then take a dot product between next and day that will give you alpha 2 3, then take a dot product between next and is that will give you alpha 2 4 and next take a dot product between next and bright that will give you alpha 2 5 and once you have all these alphas, you can simply do alpha 2 1 times x 1 plus alpha 2 2 times x 2 plus alpha 2 3 times x 3 plus alpha 2 4 times x 4 plus alpha 2 5 times x 5 and why do we take a dot product here essentially, the reason you might think of a dot product is that a dot product essentially encapsulates information about whether vectors are similar or closer to each other or not right, if you have one vector here v1 and if you have another vector here v2, the dot product between them will be higher than let's say v1 and v3, so if two vectors are similar then dot products will be higher and that's exactly what you wanted to quantify with the attention mechanism you might think that I want to quantify whether two vectors are similar right, so if next and the are more similar of course they should have a higher attention score, so this calculation seems to make sense to me, I just calculate the dot products and then I just scale the different vectors with their dot products and add it up, so whatever this sum would be that would be the context vector now for next and similarly I can find context vectors for all the other tokens as well, what's wrong with this approach, why will this approach not work or why can't we just take simple dot products to find the attention scores, again you can pause here for a moment and try to think of the context which we are trying to encode, I want you to think from first principles here, I'm going to reveal the answer very soon, all right so the main answer is that let's say you consider this sentence the dog chased the ball but it couldn't catch it right, the dog chased the ball but it couldn't catch it and let's say I have the input embedding vector for dog as this, input embedding vector for ball as this and input embedding vector for it as this, okay and if it is my query vector right now, how did you decide to compute the attention score between a query vector and other vectors, you decided to take a dot product right, this is exactly what I'm going to do, if it is my query vector to get the attention score between it and dog let me simply take a dot product between it and dog and if I take a dot product it's 0.51, if I take a simple dot product between it and ball that is also 0.51, you see the problem here both the attention scores are completely identical but that's not what I wanted, when you say but it couldn't catch it, it actually means the ball right, the dog chased the ball but it couldn't catch it, so the second it means the ball not the dog, so when I'm looking at it I need to pay more attention to ball not the dog, let me write this with a different ink so that this is clear, I need to pay a lot more attention to ball when I'm looking at it and not really the dog but that's what's that's not what is happening here, if you take a simple dot product there is no provision for me to encode the information that ball should be given more priority than dog, both dog and ball should not be having the same attention score when we say it, this is actually this example is a brilliant demonstration of why we should selectively attend to different tokens, the dog chased the ball but it couldn't catch it, the first it is the first dog so if this was the query token it would have attended more to dog but now this is the query token so it should attend more to ball, I don't want both to have the same attention score, so simple dot product cannot distinguish between subtle contextual relationships here, it doesn't consider the context of chase couldn't catch or linguistic nuance such as the fact that catch is more likely to refer to a moving object which is the ball, so the main issue is that simple dot product only measures semantic similarity but it cannot deal with contextual issues and many sentences might have contextual complexity like this right and I need to encode a mechanism so that I can capture these complexities and I don't know what that mechanism would be so then we use the trick which researchers have used for a long period of time now, if you don't know what the underlying relationship between things is you just replaced it with a neural network or a bunch of trainable weight matrices and let back propagation figure it out and that's exactly what happened in the field of attention also, researchers essentially could not figure out what that mechanism can be so and that's where the field of machine learning deviates or deep learning deviates from physics right, in physics if you were stuck with this problem you would have spent six months one year trying to develop a law for the underlying mechanism to capture complexities or underlying mechanism to capture the context but in the field of deep learning you don't do that you say that I'll replace it with a bunch of matrices and I'll train these matrices through back propagation so that's what researchers did right so they invented new matrices which are let's say called as the query matrix and the key matrix what it means is that instead of just looking at the input embedding representations what if I multiply every input embedding with a matrix so if my query here is it right my query is it I'll multiply it with something which is called as the query matrix this can be a high dimensional matrix for dog so dog and ball are the keys right because keys are essentially if you have the query keys are essentially all the other tokens which you are looking for so that's dog and ball so you multiply both of them with a keys matrix now see the advantage here is that if a dot product cannot get the contextual relationship you are hoping that these WQ and WK you are not assuming these metrics as anything you are just you will initialize them randomly and then you will train them through back propagation it is the same deep learning trick which researchers now have used for a very long time if you cannot figure out the relationship yourself you take a step back and you let neural network do its job instead of restraining the neural network by imposing some laws let it figure it out itself so you see the advantages now we have multiple trainable factors in our control so if WQ is let's say 3 by 3 and WK is 3 by 3 right so dog ball and it these are my keys and if these are the embeddings for these which we saw here also the input embeddings which we saw and now I multiply these input embeddings with the query I multiply this these I multiply these two with the keys so I will multiply so it will be 3 by 3 multiplied by 3 by 1 so this will be a 3 by 1 and this will also be a 3 by 1 so then the keys become 0.9 0.2 and 0.1 and 0.1 1.8 and 0.1 you see these values change because I multiplied them with the matrix and the query is it so it will be multiplied with the queries matrix so that the query for it will become 0.5 0.1 and 0.5 1.0 and 0.1 and so now if you plot this in vector space this is the query vector this is the keys for ball and this is the keys for dog so now we have we are going from the input embedding space to a different space which we get after multiplying with the queries and the key matrix and now I will compute the attention scores between these vectors not the original vectors so now if you compute the attention score between it and the ball you will see that it's 0.56 it and the it and the ball is 0.96 and if you compute the attention score between it and the dog that is 0.56 so here you see the attention score between it and ball is 0.96 which is higher than the attention score between it and dog which is lower so these are clearly distinct attention scores so adding these trainable matrices has actually helped us why has it helped us because it has given a number of parameters to tune so that we can encode some complex relationships between tokens so if you take a simple dot product the attention scores will be identical but if you take if you have the query key matrix we have not yet seen the value matrix we will see that in the next class but essentially if you just have trainable matrices then you can have attention scores which are different because now you suddenly have more parameters to work with so if you got confused in this part let me repeat it once more we started this section by thinking that if you have an input embedding vector right what can you do to the input embedding vector to get the context vector so to get the context vector we essentially need alphas after you get the alphas then you just have to multiply them with the input embedding vectors and then you will get the context vector but then the question is that how do you get the alphas between one input embedding vector and another input embedding vector how do you get the attention scores the simplest way is probably taking a dot product but we saw that let's say if this is the sentence right and if it is my query and if I want to find the attention score between it this it dog and ball I will take a dot product between it and the ball first which comes out to be 0.51 and I will take a dot product between it and the dog which comes out to be again 0.51 so the attention scores comes out to be similar but this is not what I wanted because when I say it I want it to be the ball so I want the attention score between it and ball to be much higher than the attention score between it and dog so how to do it now dot product clearly does not have the complexity to capture these contextual relationships I need more parameters to work with I need some knobs which I don't know currently but let neural networks or let back propagation figure out what those knobs can be at least let me initialize it randomly for now and that's where this new terminologies come into the picture right I want to have new trainable matrices let me call this query matrix and let me transform the input embeddings into another space by multiplying it with the query matrix and the input embeddings for the keys which are the dog and the ball they will be multiplied with the keys matrix and will transform it into another space and then I will find the attention scores in that transformed vectors between those transformed vectors and now if my model learns these parameters of these matrices correctly I can get the model to learn that the attention score between it and the ball is 0.96 which is higher than the attention score between it and the dog which is 0.56 don't worry about these multiplications or mathematics right now I'll do the mathematics in detail in the next lecture for now just remember that we don't know how to physically capture the contextual relationship so it's like an easy way out it's a trick you introduce the queries you introduce the keys randomly these are random trainable matrices you initialize them randomly and then you train them so you might have heard of this word query keys there is actually no proper physical reason why they are introduced the only reason they are introduced is because humans could not figure out how to capture these attention scores themselves the only way we know is that okay if we cannot figure it out let me project my input embeddings into higher dimensions or different dimensions or let me have few trainable parameters to work with and then hopefully the training itself will figure it out on its own and this trick humans have done in the field of computer vision also if you train a CNN to distinguish between dogs and cats you cannot write down all the features yourself you rely on a convolutional neural network to do that it's kind of a similar thing over here in the next lecture we are actually going to see how do we exactly compute the queries matrix the keys matrix and there is also one more matrix which is called the values matrix how that is used in the next token prediction task that we are going to see in the next lecture so the next lecture is all about the next lecture is about the mathematics of self-attention mechanism what do we do with the queries matrix key matrix and the values matrix exactly how do we calculate the context vectors mathematically and from those context vectors ultimately what do we do in all these steps to get the next open prediction so next class is pretty much going to be a deep dive into this section which we just saw and expanding it into a full lecture of mathematics but now in today's lecture I just want to motivate these concepts of queries keys and values values where we have not seen yet we will see that in the next lecture all right everyone so this brings us to the end of today's lecture which you can think of as a mixture of the history of the attention mechanism plus an introduction to self-attention for the next open prediction task as a summary remember how the attention mechanism has evolved first we had Elisa Elisa was a revolution at that time and it's pretty awesome considering it got invented in 1966 then came recurrent neural networks and LSTMs they had the context bottleneck issue which means that all the context was compressed into just one hidden state to solve that we understood that we need to selectively pay attention to different parts of the input sequence and that is what is called as attention so to encode that we introduce something called as the attention mechanism which computes the attention scores between the decoded output or the decoder hidden states and the input hidden states that paper was the Badanov attention mechanism published in 2014 that paper essentially still had RNN so that was attention plus RNN in 2017 there came a paper in which researchers realized that we don't even need RNNs so they scrapped out RNNs and they came up with a new architecture called the transformer architecture which had the attention mechanism at the heart of it 2018 researchers modified the transformer architecture they scrapped the encoder kept the decoder and had this architecture in which the attention mechanism was again at the heart of it so this until now the attention mechanism was from one sequence to another sequence then when we talk about self attention we essentially look at just one sequence because that will be used for next token prediction tasks so in next token prediction tasks like GPT we use self attention where we look at one token and how it attends to its surrounding or neighboring tokens so the token which we are looking at is called as query and the other tokens are called as keys and we want to find the attention score between the query vector and the keys we realize that the main purpose of the attention mechanism is to get these attention scores and to convert it into context vector context vector is a more enriched origin of the input embedding vector because it also contains information about how one token relates to its neighbors to get these attention scores the naive way or the simplest way to think about it is just to take a dot product between vectors but we realize that that's not the best way to go about it because just taking the simple dot product can't capture subtle contextual relationships like we saw in this example the dog chased the ball but it couldn't catch it the first it is the dog the second it is the ball to capture such contextual complexities we need to add trainable weight matrices so we need to increase the parameters so that we have different knobs to play around with these trainable matrices are called as the query weight matrix and the key weight matrix there is also value weight matrix which we will see in the next class the input embedding for the all input embeddings are multiplied with the query weight matrix to get the query matrix and also we have the keys matrix like that so then the attention scores are not found between the input embeddings of the vector they are found between the queries and the keys and since we have a flexibility of so many parameters to play with we hope that when we train the parameters they will learn that the attention score between the it the second it second it and the ball is higher than the attention score between the second it and the dog so it captures more contextual complexities so addition of this trainable weight matrices captures more contextual complexities and that's why we humans added these weight matrices and then we call them queries keys and values because it it sounds cool and also it relates to the field of information

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)