
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
	* 强化学习的兴起
	* 不再仅仅依赖人工标注的数据，而是利用大规模强化学习来教授模型进行复杂推理
	* 基于规则的奖励系统
* 第三，他们实现了多项 GPU 优化技巧；
	* 采用了 NVIDIA 的并行线程执行技术（PTX）
* 第四，构建了一个有利于蒸馏等技术的模型生态系统
	* 蒸馏至更小的模型（1.5B）


### 1、MLA


**1、当我们有了每个 token 的嵌入向量，为什么不能直接简单地用点积来计算注意力分数？**

直接使用 token 嵌入向量做内积确实在技术上是可行的，但通过 QKV 变换有几个重要的优势：

1. 表征空间的分离和专门化

```python
# 直接内积方式
attention_score = embedding_i @ embedding_j  # 单一表征空间

# QKV方式
Q = embedding @ W_q  # 查询空间：我在寻找什么？
K = embedding @ W_k  # 键空间：我能提供什么信息？
V = embedding @ W_v  # 值空间：我实际包含什么内容？
```

QKV 将不同的语义角色分离到不同的子空间中，使模型能够学习更精细的注意力模式。

2. 增强表达能力和灵活性：直接内积只能捕获原始嵌入空间中的相似性，而QKV变换允许模型学习：
	* 非对称关系：Q 和 K 可以学习不同的变换，使得 attention(A,B) ≠ attention(B,A)
	* 任务特定的相似性：不同的注意力头可以关注不同类型的关系（语法、语义、位置等）

3. 多头注意力的实现，使得不同的注意力头能够捕获不同类型的依赖关系。

4. 梯度流和训练稳定性：QKV变换提供了额外的可训练参数，有助于更好的梯度传播，避免嵌入向量直接被注意力机制"绑架"，并提供更多的学习自由度

5. 维度控制：可以将高维嵌入投影到更适合的注意力计算维度

总的来说，虽然直接内积在某些简单场景下可能有效，但 QKV 变换提供了更强的表达能力、更好的可解释性和更灵活的学习机制，这是现代 Transformer 架构成功的关键因素之一。

-----------

**2、注意力机制中，需要对结果进行缩放，即除以 $\sqrt{ k }$ 的目的**

在注意力机制中，如果 softmax 分布过于尖锐，模型就会对某一个特定的键变得非常自信，而对其他键的置信度会非常低，这会导致训练过程非常不稳定。在 attention 计算公式中除以 $\sqrt{d_k}$ 的目的是 *防止点积结果过大，避免 softmax 函数进入饱和区域*。

1. 点积幅度问题：
	1. 当维度 $d_k$ 较大时，两个向量的点积 $Q \cdot K^T$ 的幅度会随维度增长
	2. 假设 $Q$ 和 $K$ 的元素是独立的随机变量，均值为 0，方差为 1
	3. 那么点积的方差约为 $d_k$，标准差约为 $\sqrt{d_k}$

2. Softmax 饱和问题：例如：`softmax([10, 1, 2]) ≈ [0.9999, 0.0000, 0.0001]`

3. 梯度消失：当 softmax 输出接近 0 或 1 时，其梯度接近 0，影响训练效果

----

3、**为什么是平方根，而不是平方或者直接 $d_{k}$ 或其他形式？**

选择 $\sqrt{d_k}$ 而不是其他形式有深刻的数学和统计学原理。

1. 方差分析：假设 $Q$ 和 $K$ 的每个元素都是独立同分布的随机变量，均值为 0，方差为 1：对于点积 $QK^T = \sum_{i=1}^{d_k} q_i k_i$：$$
\text{Var}(QK^T) = \text{Var}\left(\sum q_i k_i\right) = \sum \text{Var}(q_i k_i)$$由于 $q_i$ 和 $k_i$ 独立，且均值为 0：$$
\text{Var}(q_i k_i) = \mathbb{E}[q_i^2 k_i^2] - (\mathbb{E}[q_i k_i])^2 
= \mathbb{E}[q_i^2] \mathbb{E}[k_i^2] - 0
= \text{Var}(q_i) \times \text{Var}(k_i) = 1 \times 1 = 1$$因此：$$
\begin{align}
\text{Var}(QK^T) &= d_k \times 1 = d_{k}\\ \\
\text{Std}(QK^T) &= \sqrt{d_k}
\end{align}$$
2. 标准化目标：我们希望缩放后的点积方差保持为 1：$$
\text{Var}\left(\frac{QK^T}{\text{scale}}\right) = \frac{\text{Var}(QK^T)}{\text{scale}^2} = \frac{d_k}{\text{scale}^2} = 1$$
解得：$\text{scale} = \sqrt{d_k}$

3. 实验验证不同缩放方式的效果：

```python
import torch
import torch.nn.functional as F


def compare_scaling_methods(d_k=64, num_samples=1000):
    results = {}
    
    for _ in range(num_samples):
        Q = torch.randn(10, d_k)
        K = torch.randn(10, d_k)
        scores = Q @ K.T
        
        # 不同的缩放方式
        scales = {
            'no_scale': 1,
            'sqrt_dk': torch.sqrt(torch.tensor(d_k, dtype=torch.float)),
            'dk': d_k,
            'dk_squared': d_k**2
        }
        
        for name, scale in scales.items():
            scaled_scores = scores / scale
            if name not in results:
                results[name] = []
            results[name].append(scaled_scores.var().item())
    
    # 打印方差统计
    for name, variances in results.items():
        mean_var = torch.tensor(variances).mean()
        print(f"{name:12}: 平均方差 = {mean_var:.4f}")

compare_scaling_methods()
```

```
no_scale    : 平均方差 = 63.5230 
sqrt_dk     : 平均方差 = 0.9925 
dk          : 平均方差 = 0.0155 
dk_squared  : 平均方差 = 0.0000
```

4. 为什么其他缩放方式不好：

	1. 直接除以 $d_k$：$\text{Var}(QK^T / d_k) = d_k / d_k^2 = 1/d_k$；当 $d_k$ 很大时，方差过小，导致所有 attention 权重趋于均匀分布，失去选择性注意的能力
	2. 除以 $d_k^2$：$\text{Var}(QK^T / d_k^2) = d_k / d_k^4 = 1/d_k^3$；方差极小，几乎没有区分度
	3. 不缩放：高维时 softmax 饱和严重

--------

**掩码注意力机制：** 抹掉上三角部分的注意力分数，同时保证剩下的部分，每行归一化后总值仍为 1.

策略 1：先计算完整归一化后的注意力权重，然后实施掩码，对剩余部分的值做归一化处理，例如第 2 行抹掉后，只剩余 0.1, 0.4 两个值，归一化后值为 0.1/0.5,   0.4/0.5；这个策略的问题是在计算注意力得分时，实际上已经计算过一次归一化了，相当于做了两遍归一化，是否可以采用更智能的方法？

策略 2：基于未进行归一化后的注意力分数（$QK^T$）直接实施掩码，即将上三角部分赋值 $-\infty$，这么做的目的是，在 softmax 中，存在指数计算，而 $e^{-\infty}=0$

**Dropout**：随机将部分值抹掉为 0，例如抹掉 5% 的比例，则剩下的这部分需要适当放大，放大系数为 $1/0.95$

---------

**KV 缓存**：

首先要理解的是，键值缓存仅在推理阶段发挥作用，这是最需要注意的关键点。在推理过程中，实际上只有最后一个 token 的上下文向量才是关键

首先只有最后一个 token 的上下文向量，最后用于预测 next token。举个例子，原输入 token 分别为 A, B, C, D 4 个 token。每个 token 输入维度为 8，则原始输入维度（词嵌入+位置嵌入）大小为 (4, 8)，$W_{Q}, W_{K}, W_{V}$ 都是 (8, 4)，则显然 Q、K、V 矩阵大小都是 (4, 4)，注意力 score 大小也是 (4, 4)

现在假设 D 对应的输出预测 token 为 E，需要把 E 添加到输入中，继续预测下一个 token F 等，也就是说 F 的预测仅与 E 最后的上下文向量相关。而为了得到 E 的上下文向量，需要（逆序考虑）：

* 注意力权重 x V 矩阵 = (5, 5) x (5, 4)，其中 V 矩阵的前四行 (4, 4) 来自缓存，最后一行为 E 的值向量，通过 (1, 8) x (W_v) 计算得到
* 然后是注意力权重 (5, 5) 由 E 的 Q（1,4）和 K(5,4) 的转置 (4,5) 相乘，K 的前 4 行来自缓存，最后一行 E 的键向量通过 (1, 8) x (W_k) 计算得到；注意 Q 不需要缓存
* 这样得到注意力权重 (1, 5) ，即 E 的注意力权重项 -> (1, 5) x V(5, 4) 得到 (1, 4) 进一步预测 F

随着标记数量的增加，如果我们不进行缓存，所需的计算量将以二次方式增长。但一旦我们实施了缓存，它最终实现了线性计算时间。

存储量，想想存的 K 和 V 都是什么维度，首先是序列长度 s、隐藏层维度 d（实际上是多头 head 乘 每个头的小维度）、批次 b、多少层 l，然后 KV 两份，乘 2，然后再考虑数据精度，比如 16bit，可计算出总共需要多大内存占用。

以 deepseek 的体量计算，大约在 400GB

-------

**Multi-Query Attention（MQA）**

所有的注意力头之间共享会怎么样，这样一来，$W_{K}, W_{V}$ 在所有 head 上一样，计算出来的 $K$ 和 $V$ 自然也就一样，只缓存一份即可。像 DeepSeek 有 128 个头，那么多查询注意力机制就能将KV缓存大小减少 128 倍——这是个巨大的缩减

这里需要注意的一点是，如果你查看不同注意力头（Q1、Q2、Q3 和 Q4）的查询向量，它们其实是不同的。就是要在 MQA 机制中，虽然键和值在不同注意力头之间是共享的，但 $Q$ 向量在不同 head 中是不同的。

MQA 的好处是大幅降低内存的使用量，但随之的劣势则是无法从多个视角对输入内容进行解读，这意味着语言模型的性能将无法达到多头注意力机制的水平。

总结：以 GPT-3 为例，如果 KV 缓存原本占用 4.5 GB，由于模型有 96 个注意力头，采用多查询注意力机制后，内存占用量几乎能降低 100 倍。而 DeepSeq 模型拥有 128 个注意力头，因此多查询注意力机制实际上将 KV 缓存大小缩减了 128 倍——从 400GB 降至 3GB。这就是多查询注意力机制的优点。但其弊端在于，它违背了多头注意力机制的初衷。MHA（多头注意力）的核心设计目标，正是为了让不同注意力头捕捉不同的特征视角。在多查询注意力机制中，这一目的无法实现。因为尽管不同注意力头的值会因查询向量的不同而有所差异，但键和值向量完全相同，因此我们无法捕捉足够的多样性，从而导致模型性能下降。

------

**分组查询注意力机制（GQA）**

在 MHA 中，所有的键和值都有不同的键矩阵和值矩阵，它们的值各不相同。在 MQA 中，所有注意力头的键矩阵和值矩阵都具有相同的值，而 GQA 则介于两者之间。本质上，分组查询注意力的理念是：与其让所有注意力头共享相同的键值矩阵，不如将注意力头分成若干组。然后让每组内的注意力头共享相同的值。

----

**多头潜注意力机制（MLA）**

成功在保持性能的同时减小了 KV 缓存的大小，这是 DeepSeek 变得如此受欢迎且推理成本极低的主要原因之一。DeepSeek V2 paper (2024.06)，提出的最简单的变体称之为低秩 KV 联合压缩。后面还将了解什么是解耦旋转位置嵌入。

在键值缓存公式中，有两个项——n x h（即注意力头的数量乘头的维度）——看起来是在我们控制范围内的。其他因素，比如 Transformer 块的数量、批处理大小、上下文大小等不做调整。deepseek 使用的是 128 个头，每个头 128 维，这是一个 128×128 的量级。他们思考的核心是：如果我们不必分别缓存键和值会怎样？（没有了 2 的系数（KV））如果这个矩阵的维度比 $n \times h$ 还要小呢？他们实现这一点的方式是利用了潜空间的概念。

假设输入嵌入矩阵的大小是 4×8，而 $W_{{dkv}}$ 大小是 8×4，得到的潜空间矩阵 $C_{kv}$，大小为 4×4，DeepSeek 展示的方案表明，我们不需要缓存键和值，而是只需要缓存这个矩阵就能实现两全其美的效果。这一步可以理解为 encoder 过程；然后分别乘 $W_{uk}$ 和 $W_{uv}$ ，均为 4×4，得到键矩阵和值矩阵，这一步可以理解为 decoder 过程。

我们将 $C_{kv}$ 与 $W_{uk}$ 和 $W_{uv}$ 相乘，从而得到键矩阵和值矩阵，此后一切保持不变。这里到底改变了什么呢？在之前的流程中，我们也是用 $W_k$ 和 $W_v$ 进行乘法运算，而非现在的 $W_{uk}$ 和 $W_{uv}$。这里看似引入了额外的步骤。因此，感觉非但没有减少键值缓存，反而似乎增加了其大小，添加这个潜在矩阵究竟如何发挥作用？DeepSeek 团队将其称为 *"吸收技巧"*。

* 第一步：$Q=X W_{q}$；
* 第二步：$C_{kv}=XW_{dkv}$；
* 第三步：$K=C_{kv} \times W_{uk} = X \times W_{dkv} \times W_{uk}$
* 第四步：$V=C_{kv} \times W_{uv} = X \times W_{dkv} \times W_{uv}$

注意力分数 $QK^T=XW_{q}W^T_{uk}W^T_{dkv}X^T=X\textcolor{blue}{(W_{q}W_{uk}^T)}\textcolor{green}{(XW_{dkv})^T}$

其中，蓝色部分 $\textcolor{blue}{(W_{q}W_{uk}^T)}$ 在推理阶段是固定的，不需要计算它们。后面绿色这一项 $\textcolor{green}{(XW_{dkv})^T}$ 实际上就是 $C_{kv}$，是需要缓存的。而 $X\textcolor{blue}{(W_{q}W_{uk}^T)}$ 就是被吸收的查询。

再将注意力权重与值矩阵相乘，以及再乘输出投影得到 logits 矩阵：$(QK^T)(XW_{dkv}W_{uv})W_o=\textcolor{red}{(\text{Attention Scores})}\textcolor{blue}{(XW_{dkv})}\textcolor{green}{(W_{uv}W_{o})}$ ，

红色这一项上面已经就算过，绿色这一项也是在推理阶段固定，蓝色这一项又是 $C_{kv}$ 

所以，如果我们只缓存 $C_{kv}$，我们就可以计算注意力分数，也可以计算上下文向量矩阵，这将帮助我们计算下一个 token。

原来 KV 缓存的内存占用大小是 $l\times b \times n \times h \times s\times 2 \times 2$。分别是 Transformer 块数量，batch，头数量，头维度，序列长度，KV，2byte，现在 KV 2 系数没了，nh 缩为 $d_{l}$，它是 KV 缓存的维度。

$$\frac{2nh}{d_{l}}=\frac{2 \times 128 \times 128}{576}=57$$

这意味着原本需要存储 400 GB的数据，现在仅需约 6 GB——这个优化效果堪称惊人。

*那么它是否解决了第二个问题，即在不同注意力头之间共享内容的问题呢* ？潜在注意力的妙处在于 $W_{uk}$ 和 $W_{uv}$ 在每个注意力头中具有不同的内容。因此，这个键矩阵和值矩阵在每个注意力头中都有不同的内容。

---------

**余弦位置编码：**$$\begin{align*}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{align*}$$
取决于位置 pos 和索引 $i$。

![[Pasted image 20250820113114.png]]

分别取 i=1, i=50, i=150 时，各位置上编码值大小构成的图像，可以看到无论使用二进制还是这个公式，我们都得到了相同的结果：较低的索引振荡得更快，较高的索引振荡得更慢。其次，正弦编码现在是连续的，它们介于 -1 和 1 之间，是平滑、连续且可微分的曲线。

因此，正弦编码的主要优势在于：位置关系图是连续的，而非跳跃式或间断的——这使得大语言模型的优化过程更加稳定。它由此解决了整数位置编码存在的缺陷。

理解这个公式的另一种方式是：振荡频率由以下公式给出：$\sin(\omega P)$，$P$ 代表位置，$\omega=\frac{1}{10000^{2i/d_{\text{model}}}}$，在这里可以清楚地看到 $i$ 出现在分母中。因此，对于较低的指数，振荡频率会更高；而对于较高的指数，振荡频率会更低。这里使用的 10000，是个实验性的选择。

理解使用正弦和余弦的关键在于掌握我们在构建位置编码时所需的一个关键特性。当我们构建位置编码时，我们希望两个编码位置之间存在线性关系。这意味着如果我们知道位置 $p$ 的编码，那么计算位置 $p+k$ 的编码应该很简单。

举例说明，例如在 $p$ 位置下；当 $i=1$ 时，$2i=2, 2i+1=3$，则 $PE(p, 2) = \sin \theta=y, PE(p, 3)=\cos \theta=x$，这好比一个向量 $v_{1}=(\cos \theta, \sin \theta)$，其中 $\theta=\frac{p}{10000^{2d}}$；而当我们处于 $p+k$ 位置呢？$\theta'=\theta+\theta_{1}=\frac{p+k}{10000^{2d}}$，这相当于 $v_{1}$ 旋转了一个角度，$v_{2}=(\cos (\theta+\theta_{1}), \sin (\theta+\theta_{1}))$，其中 $\theta_{1}=\omega k$

我们之所以有 $\sin$ 和 $\cos$，是因为它们使得旋转成为可能。它满足这样一个特性：编码位置之间现在存在一种关系，这意味着如果我们有一个位置的编码向量，要找到另一个位置的编码向量，只需旋转初始向量即可，这就是旋转概念的由来，也是旋转位置编码的核心思想。

正弦位置编码的一个主要问题是，位置嵌入直接与 token 嵌入相加。尽管位置嵌入的幅度较小，但将位置嵌入添加到 token 嵌入中这一行为本身就会污染 token 嵌入所携带的语义信息。理想情况下，我们希望 token 嵌入所携带的语义信息在进入 Transformer 模块时能够保持不变。

两个想法，第一个想法是直接在查询和键的层面上进行位置操作。第二个想法是，与其添加一个向量，直接旋转这些向量，这两个想法促成了旋转位置编码的诞生。在旋转嵌入中，我们将对查询向量和键向量进行增强处理，而完全不会触及 token 嵌入向量本身；第二件事是保持查询向量和键向量的模长不变。我们只是通过旋转它们来表示或影响标记的位置，而不会改变向量的模长。

--------

**旋转位置编码（RoPE）：**

多头注意力机制实际上由计算注意力分数组成。查询与键的转置相乘得到注意力分数。在这种机制中，我们考虑了不同 token 的位置并计算出注意力分数，实际上我们可以直接将位置信息注入到查询向量和键向量中；此外，将 token 嵌入与位置嵌入相加会改变 token 嵌入本身的幅度，这是不利的，而旋转操作不影响幅度。

旋转位置编码的主要思想是对查询向量和键向量应用正弦位置编码。使用同样的公式，但这次将其应用于查询向量和键向量，不是将其加到向量上，而是进行旋转。

* 步骤 1：取某个 token 的 query/key，按维度两两分组，构成一个向量；
* 步骤 2：注入位置编码值，即旋转角度 $\theta=\omega p$，其中 $\omega = \frac{1}{10000^{2i/d}}$，$p$ 是绝对位置，$i$ 是维度索引（除 2），$d$ 是维度大小。如 768 

因此，对于给定的位置 $p$，随着索引的增加，旋转量会进一步减小。

$$\begin{bmatrix}
x_1' \\
x_2'
\end{bmatrix}
=
\mathbf{M}_i \cdot
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\quad \text{where} \quad
\mathbf{M}_i =
\begin{bmatrix}
\cos(\omega_i p) & \sin(\omega_i p) \\
-\sin(\omega_i p) & \cos(\omega_i p)
\end{bmatrix}
$$

一对具有较长相对距离的查询向量会拥有非常不同的位置编码，因为它们具有不同的 $p$ 值；而位置相近的一对查询向量则会有相似的位置编码，这对我们来说是直观合理的。此外，低索引位置值会随位置快速变化，而高索引位置值则随位置缓慢变化。

--------

**解耦旋转位置编码（MLA+RoPE）**

回顾一下 *"吸收技巧"*。

* 第一步：$Q=X W_{q}$；
* 第二步：$C_{kv}=XW_{dkv}$；
* 第三步：$K=C_{kv} \times W_{uk} = X \times W_{dkv} \times W_{uk}$
* 第四步：$V=C_{kv} \times W_{uv} = X \times W_{dkv} \times W_{uv}$

注意力分数 $QK^T=XW_{q}W^T_{uk}W^T_{dkv}X^T=X\textcolor{blue}{(W_{q}W_{uk}^T)}\textcolor{green}{(XW_{dkv})^T}$

其中，蓝色部分 $\textcolor{blue}{(W_{q}W_{uk}^T)}$ 在推理阶段是固定的，不需要计算它们。后面绿色这一项 $\textcolor{green}{(XW_{dkv})^T}$ 实际上就是 $C_{kv}$，是需要缓存的。而 $X\textcolor{blue}{(W_{q}W_{uk}^T)}$ 就是被吸收的查询。

在此基础上我们现在需要再 $Q$ 和 $K$ 中引入 RoPE，即 $\text{RoPE}(XW_{q}) * \text{RoPE}(W^T_{uk}W^T_{dkv}X^T)$，而这里的 $\text{RoPE}(\text{pos}, \text{index})$ 依赖于绝对位置和维度索引信息，需要按维度信息两两组合进行旋转。

然而，由于 RoPE 的存在，$\textcolor{blue}{(W_{q}W_{uk}^T)}$ 没办法融合吸收了，因此，我们必须在推理过程中重新计算所有前缀标记的 keys，这将显著降低推理效率。正如 DeepSeek V2 Paper 2.1.3 章节中所写的那样。

解决方案：

把 $Q$ 和 $K$ 都分成两部分，$[Q_{C}; Q_{R}] * [K_{C}; K_{R}]^T$，其中 $Q_{C}$ 是指不应用 RoPE 操作的，而 $Q_{R}$ 是应用 RoPE 操作的，同理对 $K$ 也是如此，则 $$QK^{T}=Q_{C}K_{C}^{T}+Q_{R}K_{R}^T$$因此，第一项仍可以应用吸收技巧。因此现在的计算可以分为左右两侧，独立计算。

注：下标中的 d 表示向下投射，压缩的意思，u 是向上投射，还原的意思

左侧：

* 对于 $K,V$ 的计算，假设现在数据有 4 个 token，每个 token 维度为 8，即 $X=(4,8), W_{dkv}=(8, 4)$，则 $C_{kv}=(4,4)$，相应的可分别计算出 $K_{C}=C_{kv}W_{uk}=(4,4)\times(4,8)=(4,8)$ 和 $V_{C}=C_{kv}W_{uv}=(4,4)\times(4,8)=(4,8)$。

* 此外我们可以对 $Q$ 做类似的处理，先通过压缩矩阵 $W_{dq}=(8,4)$，得到 $C_{q}=XW_{dq}=(4,4)$，然后在通过向上投射矩阵 $W_{uq}$ 还原 $Q_{C}=C_{q}Wuq=(4,4)\times(4, 8)=(4, 8)$

右侧：

* 我们有矩阵 $W_{KR}=(8, 4)$，这所有的 head 中是共享的，将其应用在 $X$ 上，可以得到 $(4,4)$ 矩阵，在此之上应用 RoPE，考虑到多头，将其复制连接，组成 $K_{R} = (4, 8)$，即所有头使用相同的 $W_{KR}$

* 同样有 $W_{dq}=(8,4)$，得到 $C_{q}=(4,4)$，然后向上经过 $W_{qR}=(4,8)$ 投射得到 $(4, 8)$，再经过 RoPE 操作得到 $Q_{R}=(4, 8)$；*注意，$K_{R}$ 是多头共享拼接出来的，$Q_{R}$ 在多头之间不存在共享*


**论文对照讲解：**

2.1.2 低秩键值联合压缩

MLA 的核心是对键和值进行低秩联合压缩以减少 KV 缓存：

$$\begin{align}
    \textcolor{blue}{\mathbf{c}_{t}^{KV}} &= W^{DKV} \mathbf{h}_{t}, \tag{9}\\
    \mathbf{k}_{t}^{C} &= W^{UK} \mathbf{c}_{t}^{KV}, \tag{10}\\
    \mathbf{v}_{t}^{C} &= W^{UV} \mathbf{c}_{t}^{KV},\tag{11}
\end{align}$$
符号解释：

* 维度解释：
	* $d$ 是嵌入层维度，或者可以理解为隐藏层的维度
	* $n_{h}$ 是注意力机制的 head 数量，即有多少个头
	* $d_{h}$ 是每个头的维度，即 $d = n_{h} \times d_{h}$
	* $d_{c}$ 与 $d_c^{\prime}$ 可以是不同的大小，即 KV 和 Q 的压缩维度可以不一样，压缩比可以不同

* 变量解释：
	* $\mathbf{h}_{t} \in \mathbb{R}^{d}$：为注意力层中第 $t$ 个token的注意力输入，可以理解为 $X$；
	* $W^{DKV} \in \mathbb{R}^{d_c \times d}$：是个向下投影的矩阵，而 $d_c (\ll d_h n_h)$ 表示 KV 压缩维度；
	* $\mathbf{c}_{t}^{KV} \in \mathbb{R}^{d_c}$ 是键和值的压缩潜在向量；
	* $W^{UK},W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 则分别是 K、V 的向上投射矩阵，用于从 $d_{c}$ 恢复原始维度
	* $\mathbf{c}_{t}^{Q} \in \mathbb{R}^{d_c^{\prime}}$ 是查询的压缩潜在向量，而 $d_c^{\prime} (\ll d_h n_h)$ 表示查询压缩维度，通常 $d_c^{\prime} < d_c$；
	* $W^{DQ} \in \mathbb{R}^{d_c^{\prime} \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c^{\prime}}$ 分别是针对 Q 的向下和向上投影矩阵。
	

在推理过程中，MLA 仅需缓存 $\mathbf{c}_{t}^{KV}$，因此其 KV 缓存仅包含 $d_{c}l$ 个元素，其中 $l$ 表示层数。此外，在推理过程中，由于 $W^{UK}$ 可被吸收进 $W^{Q}$，且 $W^{UV}$ 可被吸收进 $W^{O}$，我们甚至无需为注意力机制计算键和值。此外，为了减少训练过程中的激活内存，我们还对查询进行了低秩压缩，尽管这无法减少 KV 缓存。
$$\begin{align}
    \mathbf{c}_{t}^{Q} &= W^{DQ} \mathbf{h}_{t}, \tag{12}\\
    \mathbf{q}_{t}^{C} &= W^{UQ} \mathbf{c}_{t}^{Q},\tag{13}
\end{align}$$

2.1.3 解耦旋转位置嵌入

继 DeepSeek 67B 之后，我们计划在 DeepSeek-V2 中采用旋转位置编码（RoPE）。然而，*RoPE 与 低秩 KV 压缩存在兼容性问题。具体而言，RoPE 对键向量和查询向量都具有位置敏感性*。若对键向量 $\mathbf{k}_{t}^{C}$ 应用 RoPE，公式 (10) 中的 $W^{UK}$ 将与具有位置敏感性的 RoPE 矩阵耦合。如此一来，在推理过程中 $W^{UK}$ 无法再被吸收到 $W^{Q}$ 中——因为在 $W^{Q}$ 与 $W^{UK}$ 之间会插入与当前生成 token 相关的 RoPE 矩阵，而矩阵乘法不满足交换律。这将导致我们必须为所有前缀 token 重新计算键向量，从而严重拖累推理效率。

作为一种解决方案，我们提出了解耦的 RoPE 策略，该策略使用额外的多头查询 $\mathbf{q}_{t, i}^{R} \in \mathbb{R}^{d_h^R}$ 和 *共享键* $\mathbf{k}_{t}^{R} \in \mathbb{R}^{d_h^R}$ 来承载 RoPE，其中 $d_h^R$ 表示解耦查询和键的每个头的维度。配备解耦 RoPE 策略后，MLA 执行以下计算：

$$\begin{align}
    [\mathbf{q}_{t, 1}^{R};\mathbf{q}_{t, 2}^{R};...;\mathbf{q}_{t, n_{h}}^{R}] = \mathbf{q}_{t}^{R} &= \operatorname{RoPE}({W^{QR}} \mathbf{c}_{t}^{Q}),\tag{14} \\
    \textcolor{blue}{\mathbf{k}_{t}^{R}} &= \operatorname{RoPE}({W^{KR}} \mathbf{h}_{t}), \tag{15}\\
    \mathbf{q}_{t, i} &= [\mathbf{q}_{t, i}^{C}; \mathbf{q}_{t, i}^{R}], \tag{16}\\
    \mathbf{k}_{t, i} &= [\mathbf{k}_{t, i}^{C}; \mathbf{k}_{t}^{R}], \tag{17}\\
    \mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_j(\frac{\mathbf{q}_{t, i}^T \mathbf{k}_{j, i}}{\sqrt{d_{h} + d_{h}^{R}}}) \mathbf{v}_{j, i}^{C}, \tag{18}\\ 
    \mathbf{u}_{t} &= W^{O} [\mathbf{o}_{t, 1};\mathbf{o}_{t, 2};...;\mathbf{o}_{t, n_{h}}],\tag{19}
\end{align}$$

* $W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c^{\prime}}$ 和 $W^{KR} \in \mathbb{R}^{d_h^R \times d}$ 矩阵分别用于生成解耦的查询和键；
* 这里需要注意的是，$W^{QR}$ 在多头之间不共享，维度是 $d_h^R n_h \times d_c^{\prime}$，而 $W^{KR}$ 是共享的，维度只有 $d_h^R \times d$
* 此外，$Q$ 是直接将 $W^{QR}$ 作用在潜在向量上的，然后应用 RoPE，而 $K$ 是将 $W^{KR}$ 作用在 $\mathbf{h}_{t}$ 而不是 $\mathbf{c}_{t}^{KV}$ 上。
* 公式 (16, 17) 是将未应用 RoPE 的以及应用 RoPE 各部分组合起来。
* 公式 (18) 中缩放因子，注意到现在的维度大小是 $d_{h}+d_{h}^R$

在推理过程中，解耦后的键也应被缓存。因此，DeepSeek-V2 总共需要一个包含 $(d_{c} + d_h^R)l$ 元素的 KV缓存。现在再结合图 2 查看整个计算过程就很清楚了。潜在注意力结合旋转位置编码技术实际能节省 *57 倍* 的内存容量

标注蓝色的两项 $\textcolor{blue}{\mathbf{c}_{t}^{KV}}$ 和 $\textcolor{blue}{\mathbf{k}_{t}^{R}}$ 是需要缓存的，首先关于 $K^C$ 和 $V^C$ 以及 $Q^C$ 的计算与朴素版本的 MLA 完全一样，不涉及 RoPE，仅需要缓存 $\textcolor{blue}{\mathbf{c}_{t}^{KV}}$ 即可，而右侧，缓存 $\textcolor{blue}{\mathbf{k}_{t}^{R}}$ 即可，每次只计算新增 token 的单个向量即可。

实际整体上，我们可以看到，$Q,K$ 被分为两部分，一部分 NoPE，即不参与旋转，一部分 RoPE，近期有专门分析 “RoPE vs NoPE vs 混合” 的工作，结论是**混合式注意力**在长上下文上可超越纯 RoPE；另外，把一部分维度的 RoPE 去掉（有选择地“partial-RoPE”）也能维持甚至提升效果，并为 MLA 这类结构提供了更好的迁移路径。这些结果支持“不是 RoPE 越多越准”，而是要给模型**选择权**。

-------

**原始 MOE**

*稀疏性* 和 *路由机制*

* 从一个输入矩阵大小为 (4, 8) 的矩阵开始，4 个 token，每个 token 的维度为 8；将这个输入矩阵传递给一个由 3 个专家组成的 MOE 模型。
* 分别经过三个专家，分别得到了不动的 (4, 8) 矩阵
* 如何合并？方法是负载均衡，确定每个 token 有几个专家被激活，比如 2 个。
* 应该选出哪两个专家？各自分配多少权重？通过路由矩阵决定（8, 3）--> (4, 3)，每个 token 根据分值大小选择最大的前两个，重新正则化，softmax，未被选中的为 $- \infty$，使其剩下的两项和为 1，从而得到权重分布
* 一旦我们有了专家选择器权重矩阵，就可以通过分配的权重因子来合并在步骤 2 中获得的三个专家输出矩阵。
* 根据相应的权重组合步骤二中各个 token 在各个专家下的 vector 的加权和

*辅助损失 version 1*

在专家混合模型中，路由机制本质上会选择一部分专家。对于每个 token，我们都会选择该 token 被路由到的一部分专家。到目前为止，我们还没有实现一种机制来确保这种路由是平衡的，这意味着有时可能会出现某些专家被多次选择，而其他专家则完全没有被利用的情况，这会导致学习效率低下，性能也不如预期那样理想。辅助损失项，会惩罚不平衡的专家选择，并推动路由函数趋向更均匀的分布。理想情况下，我们希望看到每位专家处理相似数量的 tokens。

路由机制产生的矩阵，比如 (4, 3)，4 个 token，3 个专家，则每一列是某个专家在各个 token 上分配的概率值，专家的重要性就是各个 token 上的概率和。

所以我们本质上要做的是构建一个损失函数。我们将构建一个损失项，如果 e1、e2 和 e3 之间存在很大差异，这个损失项就会很高，反之就会很低，这可以通过变异系数来衡量。变异系数是一个统计学概念，只需要取标准差除以均值。

$$\text{CV}=\frac{\sigma}{\mu}$$
例如三个专家重要性分布为 1.4, 1.0, 1.6，CV=0.187，这个值越小越好，辅助损失函数可以定义为：

$$\text{辅助损失}=\lambda(\text{CV})^2$$

*负载均衡损失*

事实证明，仅仅拥有或最小化辅助损失并不足以确保不同专家之间的 *负载均衡*。如果发送给每个专家的 tokens 分布不均匀，可能会导致内存使用量过高，并降低专家混合的性能。

我们基本上需要计算两个关键因素：

* 首先，我们需要计算路由选择某个特定专家的概率，这个概率被称为 $P_{i}=\frac{1}{T}\sum p_{i(x)}$，这其实很简单，因为我们只需要使用专家重要性分数，例如此处的 1.4、1.0 和 1.6，算下占比分别为 1.4/(1.4+1+1.6)=0.35， 0.25，0.4。

* 另一个量本质上就是分配给每个专家的 token 比例，即 $f_{i}=\frac{N_{r}}{K_{r}T}\sum^{1}_{t=1}\mathbb{1}(\text{Token } t \text{ selects Expert } i)$,$$\sum f_{i}P_{i}$$
整体上，负载均衡损失为：<img src="https://substackcdn.com/image/fetch/$s_!HmXE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F644cec45-8dff-491e-9d41-e53ee4b0c7df_1574x764.png" width="500">
单纯的 $\sum f_{i}p_{i}$ 实际上当给定了其中一个比如 $p_{i}$ 的概率分布并顺序排序，则 $f_{i}$ 概率值能对应逆序排序时，总体值最小。例如 $p_{i}=[0.2, 0.3, 0.5]$ 时，$f_{i}=[0.6, 0.3, 0.1]$，通俗点说，就是 $p_{i}$ 大的对应的 $f_{i}$ 小的时候，整体结果会更小，然而这是不可能的。因为 $f_{i}$ 的选择已经是通过 Top-K 选择的前 K 个专家了，这说明它对应的 $p_{i}$ 至少也是 top-k 中的，不可能太小，所以事实情况是，如果不加以约束，会导致 $p_{i}$ 大的 $f_{i}$ 更大，因此这个损失的目的在于避免这种情况，理想情况是迫使 $f_{i}$ 和 $p_{i}$ 的分布更加均匀。

*注意*，这里的公式是参考 Switch Transformers 的，$f_{i}$ 被定义为 argmax，这一点，这位印度老哥的视频中也是错乱的，视频中整体是按照 argmax 计算并讲解的，但是视频 30:34 前后的说明图中又是按照 Deepseek 中 $f_{i}$ 公式计算的。

在辅助损失中，我们只需关注专家重要性，并确保不同专家之间的重要性均衡。而在负载均衡的情况下，我们虽然也会通过 $p_i$ 来考察专家重要性，但还会将其与 $f_i$ 相乘—— $f_i$ 是分配给每个专家的 tokens 比例，这个乘积正是我们试图最小化的量，从而确保 $p_i$ 的均匀分布。

*容量因子*

为避免所有 tokens 都被路由到个别的几个专家那里，引入了专家容量或容量因子的概念，其计算公式如下：$$
\text{Expert Capacity} = \frac{\text{Total Tokens in the Batch}}{N} \cdot \text{Capacity Factor}$$
Tokens per batch = batch size * sequence length * top_k

容量因子范围在 1.125-2 之间，如果设定为 1，则每个专家处理的上限就是平均分配了，例如 1000 个 tokens，4 个专家，则平均每个专家只允许处理 250 个。


**DeepSeek-MoE**


DeepSeek 实施的三大主要创新：首先是所谓的无辅助损失负载均衡，其次是共享专家，第三个创新是细粒度专家分割。2024.01 发布 DeepSeek-MOE，2024.06 发布 V2，提出共享专家机制和细粒度专家划分。随后在 V3 中又引入无辅助损失的自由负载均衡。

*无辅助损失的自由负载均衡：*

问题在于原来的负载均衡损失被加到了训练损失中，这种损失实际上干扰了训练损失，两种损失，它们本质上表示的是完全不同的事情。训练损失表示在下一个标记预测上的表现如何，而负载均衡损失则包含了不同专家之间的平衡信息，以及 tokens 是否均匀地路由到专家等等。

因此，这两种损失从根本上代表着不同的含义，将它们相加后再进行梯度计算和反向传播是非常低效的。使用负载均衡损失有助于保持专家模型的负载平衡，这是它的优势，但这种损失也起到了干扰语言建模的正则化项的作用。例如，如果缩放因子非常小，那么后一项几乎可以忽略不计，我们就无法充分利用专家的能力，那么混合专家模型就几乎没什么用处了。而如果 $λ$ 值非常高，就意味着这会降低模型的性能。*所以主要问题在于，要在不损害训练质量的前提下有效平衡专家模型是相当困难的，这才是关键所在。*

DeepSeek 如何做到在没有损失项的情况下实现了负载均衡？

首先要做的是无损失负载均衡。需要找到每个专家的平均负载，即计算路由到每个专家的平均 token 数量 (total tokens / num of expert)，然后根据路由到某个专家的 tokens 数量可以判断该专家是过载还是欠载，可以计算出差值（均值-当前负载值，即 load violation error）。

引入一个 bias 项，初始时对于每个专家均为 0，更新公式为：$$b_{i}=b_{i}+u \times \text{sign}(\text{load violation error})$$实际上，这等价于$$
b_i = 
\begin{cases} 
b_i + u, & \text{如果负载违反误差} > 0 \\
b_i - u, & \text{如果负载违反误差} < 0 
\end{cases}$$
我们将输入矩阵与路由矩阵相乘，就得到了这个专家选择矩阵，这里的增减就是基于这个专家选择矩阵进行的。相当于要增加欠载部分专家的选择概率并适当减小过载专家的选择概率值。

通过这种偏置的动态调整，DeepSeek 实现了良好的专家负载平衡，而实际上并未向模型引入任何噪声梯度。现在负载均衡的损失项现在完全不存在了，因此它被称为无辅助损耗负载均衡。实际上，DeepSeek 证明了无辅助损失的负载均衡在性能和负载均衡方面都比传统的负载均衡方法表现更优。


*共享专家机制：*

DeepSeek 指出了传统专家混合架构的两大主要问题：

* 知识混杂性：现有专家混合实践通常只采用有限数量的专家，无法实现专业化（大量细分专家）
* 知识冗余：分配给不同专家的 tokens 可能需要共同的知识，结果导致多个专家在各自的参数中获取共享知识，从而造成专家参数的冗余（共享专家）


共享专家总是被激活，这意味着所有 token 都会通过这些专家。接下来将共享专家的输出和路由专家的输出相加，这样我们就得到了最终的结果输出。因此，对这两个专家的输出进行加权求和，从而得到最终的专家混合输出。


*细粒度专家分割：*

将每个专家拆分成多个较小的专家，同时保持整体模型容量和计算成本不变。在细粒度专家分割中，每个大型专家前馈网络通过将隐藏维度缩小至原来的 $1/m$ 倍，被拆分为 $m$ 个较小的专家。

之所以这样做，是因为如果专家数量较少，每位专家就不得不学习广泛的知识类型，这会降低其在细粒度专家细分领域的专业性。通过增加专家数量，可以拥有更专业的专家，因为现在每位专家都能学习新的内容，从而解决了我们最初面临的知识混杂问题。
<img src="https://k.sinaimg.cn/n/spider20240111/767/w1080h487/20240111/94dd-d49904cbdf22bf82dd7a5a6cb3db9d83.png/w700d1q75cms.jpg?by=cms_fixed_width" width="600">
DeepSeek 通过实证表明，共享专家机制确实能提升模型在多项任务中的表现——这是我们从图表中得出的第一个结论。第二个关键发现是细粒度专家分区的效果对比：因此你会持续看到橙线在所有指标中表现最佳。记住：共享专家能解决知识冗余问题，而细粒度的专家分工则能解决知识混杂问题。

--------

**Multi-Token Prediction (MTP)**

如摘要所述，作者们指出大语言模型通常采用下一个词预测的方式进行训练。而本研究提出，同时预测多个未来 tokens 的训练方式能显著提升样本效率。在传统训练过程中，我们有一个序列的 tokens，每个 token 都会预测一个新的 token，现在是预测三个，训练阶段实际上也有对应的三个实际标签，同样损失也是在这三个 tokens 上的聚集。

多标记预测的真正优势是什么？它为什么真的有用？以下内容引用论文 [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)（Meta， 2024.04）

之所以有用，主要有四个原因：

1. 训练信号的密集化
2. 数据效率的提升
3. 更好的规划
4. 更高的推理速度

*训练信号的密集化*：这意味着多 tokens 预测比单 token 预测能提供更丰富、更密集的训练信号。传统的单 token 预测只引导模型预测下一个即时 token，而在多 tokens 预测中，我们会指导模型同时预测多个未来 token，从而为每个训练样本生成信息量更大的梯度信号。在多 tokens 预测中，模型实际上从每个训练样本中直接学习到了更长范围的结构、语法和连贯性。可以让让模型能同时观察并掌握多个未来步骤间的关联性，这促使模型内部表征朝着更优的序列规划与预测方向发展——这点至关重要。随着模型不断学习，它在规划能力和前瞻性方面都获得提升，这两个特质对语言模型实现类人行为及提升性能都极为关键。

*提高数据效率*：论文表明，多标记预测训练的模型在标准基准测试（如 HumanEval 和 MBPP）上取得了更好的结果。在相同的训练数据量下，平均能解决约 15% 更多的代码问题。

*擅长规划*：最重要的原因之一，它隐式地赋予选择点更大的重要性，这些选择点是显著影响未来结果的关键 token。所谓选择点，指的是对未来结果有重大影响的关键 token。因此，模型学会了优先考虑关键的决策要素，从而在规划方面表现得更好。

<img src="https://cdn.linkresearcher.com/bxa60w7l-c9dv-cdeo-uld5-oxzkcfdm" width="400">
图例中，"5->A" 被认为是选择点的关键 token，因为在这一点上发生了完全的转变。我们最初是按顺序预测数字，然后突然从预测数字转变为预测字母。然而我们现在让它从看见 3 的时候就开始尝试预测 "A"，可以看到 "A" 同时出现在 3、4、5 这三个输入的预测结果中。

> [!NOTE]
> 在论文 5.1 章节中，他们提到并非所有标记决策对语言模型生成有用文本都同等重要。有些标记允许不影响文本后续内容的风格变化，而另一些则代表与文本更高层次语义属性相关的选择点，这些选择可能决定一个回答是被视为有用还是偏离主题。
> 
> 多标记预测会根据训练标记与其后续标记的相关性程度，隐式地为这些训练标记分配权重。举例说明，假设图 9 中展示的序列存在一个难以预测的选择点，而其他过渡被视为"无关紧要"。选择点之后的无关紧要过渡同样难以提前预测。通过标记和统计损失项，我们发现 $n$-标记预测通过其关联项为选择点分配了 $n(n+1)/2$ 的权重，而为无关紧要点分配了较小的 $n$ 权重。更多细节请参阅附录 L.3。总体而言，我们认为文本生成的质量取决于在关键选择点上做出正确决策，而 $n$ 标记预测损失正是促进这一点的。


*更高的推理速度*：（论文 3.2 章节）它可以将推理速度提高多达 3 倍。平均每 3 个建议中有 2.5 个 tokens 被接受。这意味着多 token 预测确实让推理变得快很多。（与此相关的术语，自推测解码）
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*krIJR3xtwG1v1u2-" width="500">
自推测解码实际的操作是这样的：在推理过程中，如果观察左侧，每次只预测一个标记。但在自推测解码中，我们一次预测多个标记，然后使用另一个更大的语言模型来验证这些响应。如果它们是正确的，就会被保留；如果是错误的，则不会被保留。这就是所谓的并行验证，这个想法被称为推测解码，它可以加速大语言模型的推理。这就是为什么多 tokens 预测经常与加速大语言模型推理以及自推测或推测解码联系在一起。

注：*DeepSeek 仅在预训练阶段使用了多 tokens 预测带来的增益*。在推理阶段，他们并未采用推测性解码技术。这背后有多重考量：既能获得更密集的训练信号，又能提升数据使用效率，还能优化规划能力等。但值得注意的是，他们并未利用其可带来的更高推理速度优势。

----------

**DeepSeek MTP：**

参考资料：[# How Multi-Token Prediction (MTP) works in DeepSeek-V3](https://blog.gopenai.com/how-multi-token-prediction-mtp-works-in-deepseek-v3-94bb9301989c)

假设深度为 $k=3$，即预测未来的 3 个 tokens，这需要 3 个不同的输出头 head ($k_{1}, k_{2}, k_{3}$)，每个预测深度都需要 *两个输入*：该预测深度的隐藏状态，该深度下的输入嵌入。
<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPqmPvkxSjpn5shJxQTc6Q.png" width="600">
该深度下的输入嵌入就是普通的 pos 位置的嵌入，例如图中的 $t_{1}, t_{2}, t_{3}$ 等。而每个深度下的隐藏状态：$k_{1}$ 对应的隐藏状态输入就是 transformer 块的输出 $h_{i}^0$；而 $k_{2}$ 对应的是有经历了一些额外操作层处理后的状态 $h_{i}^1$；$k_{3}$ 对应的是 $h_{i}^2$。

需要注意的是，由于序列边界限制，例如序列长度为 $7$，则预测仅针对 $i$ 等于 0、1、2、3 的位置进行，因为在 $i$ 等于 4 的位置时，我们只能预测 5 和 6，位置 7 没有标记存在。 

举例说明：假如输入序列有  7  个 tokens，表示为 $t_{1}$ 到 $t_{7}$。MTP 机制以多轮方式运行，每轮通过复用隐藏状态并将其传递至额外的 MTP 模块，预测不同跨度的未来 tokens。每个 MTP 模块包含一个 Transformer 块，用于优化下一次预测的隐藏表示。
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*TFYHeeAH7A9jC11v0bQDdw.png" width="600">
在第一轮中：输入 token 为 $t_{1}$，

* 在 *Main Model* 中，处理 $t_{1}$，并输出隐藏层 $h_{1}^0$，同时预测出 $P_{2}^0$；
* $h_{1}^0$ 被输送到 *MTP Module 1* 中，与 $t_{2}$ 嵌入合并，并经过 Transformer 块产生隐藏状态 $h_{1}^1$，同时预测出 $P_{3}^1$；
* 同上，$h_{1}^1$ 被输送到 *MTP Module 2* 中，与 $t_{3}$ 嵌入合并，并经过 Transformer 块产生 $h_{1}^2$，同时预测出 $P_{4}^2$；

<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*hlT74kMaLER_NdSSL3YDhQ.png" width="600">
在第二轮中：输入为 $t_{1}$ 和 $t_{2}$

* 在 *Main Model* 中，处理 $t_{1}$ 和 $t_{2}$，并输出隐藏层 $h_{2}^0$，同时预测出 $P_{3}^0$；
* $h_{2}^0$ 被输送到 *MTP Module 1* 中，与 $t_{3}$ 嵌入合并，并经过 Transformer 块产生隐藏状态 $h_{2}^1$，同时预测出 $P_{4}^1$；
* 同上，$h_{2}^1$ 被输送到 *MTP Module 2* 中，与 $t_{4}$ 嵌入合并，并经过 Transformer 块产生 $h_{2}^2$，同时预测出 $P_{5}^2$；

第三轮，略
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*9gs7wf805CnEssXZr0lcWA.png" width="600">
最后一轮，第 4 轮中，输入为 $t_{1}, t_{2}, t_{3}, t_{4}$

* 在 *Main Model* 中，处理 $t_{1}, t_{2}, t_{3}, t_{4}$，并输出隐藏层 $h_{4}^0$，同时预测出 $P_{5}^0$；
* $h_{4}^0$ 被输送到 *MTP Module 1* 中，与 $t_{5}$ 嵌入合并，并经过 Transformer 块产生隐藏状态 $h_{4}^1$，同时预测出 $P_{6}^1$；
* 同上，$h_{4}^1$ 被输送到 *MTP Module 2* 中，与 $t_{6}$ 嵌入合并，并经过 Transformer 块产生 $h_{4}^2$，同时预测出 $P_{7}^2$；


可以看到，这里在多个 token 的预测之间存在一个因果链，这与 Meta 2024.04 的论文不同，Meta 那边是独立预测的。直觉上应该是认为过去预测的信息会影响到未来的预测。

*每个头内部发生的操作：* 假设隐藏层维度为 8

* 在 Main Model 中，取最后一个位置的隐藏层内容，维度为 (1, 8)，嵌入也是 (1, 8)
	* 首先经过 RMSNorm 后合并（concatenation）两个输入内容，(1, 16)，均方根归一化，先计算平方的均值，然后取其平方根，除以它即可。
	* 线性投影层 (16, 8)，然后输出 (1, 8)，在公式 (21) 中表示为 $M_{k} \in \mathbb{R}^{d \times 2d}$
	* Transformer 块，得到新的 (1, 8) 隐藏层输出，可用于下一个模块的预测
	* 经过共享输出头，预测一个 token。
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*iJiqj2LTvbhQMWMc9BX8IQ.png" width="550">
损失函数的预测就是在每一步中，各个未来位置的预测结果与真实标记之间进行交叉熵，并累计。


---

**量化：**


量化流程中实现的五项创新技术：

1. 混合精度框架，
2. 细粒度量化，
3. 提高累积精度，
4. 尾数超过指数，
5. 在线量化。

量化可视化参考资料：[# A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)，[中文版](https://mp.weixin.qq.com/s/dgS-yRVpGe_w1uzbcVctXg)

*混合精度框架：*
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*DDhx_PcOT5rYMpwT2FEjLA.png" width="600">
其中：

* 前向传播（Fprop）是指权重乘以输入的操作 $y=wx$，如图所示，首先将 $x$ 从 BF16 -> FP8，另一方面，*权重保持高精度存储*，这意味着它们以 BF16 或 FP32 形式保存，但*权重会在运算时动态转换* 为 FP8 格式。为确保数值稳定性输出结果类型为 FP32，随后会被转换回 BF16 优化内存占用。
* 数据的反向传播（Dgrad）：计算损失对于输入的偏导数，用于继续向前传播梯度。$\frac{\partial L}{\partial x}=\frac{\partial L}{\partial Z}W^T$，输出梯度被转为 FP8，权重也被转为 FP8，计算输出为 FP32 被被转为 BF16 存储。
* 权重的反向传播（Wgrad）：计算损失对于权重的偏导数，用于更新参数。$\frac{\partial L}{\partial W}=X^T\frac{\partial L}{\partial Z}$，还是均转为 FP8 进行乘法计算，结果为 FP32 格式*并存储*（权重梯度的存储精度高），转为 BF16 更新优化器的状态，最后权重的更新操作（Master Weight）是采用 FP32 计算的（高精度计算）

为什么要进行这种混合精度呢？在运行过程中将几个变量转换为 FP8，显著降低了计算和内存成本，并为我们带来了巨大的计算速度提升，同时保留了一些更高精度的状态，以确保训练的稳定性。DeepSeek 论文中指出，嵌入模块、标记和位置嵌入、输出头（最终输出头将嵌入维度转换为词汇表大小的维度）、专家混合、门控模块、归一化操作符和注意力算子对精度敏感，因此这些组件会保留更高精度的计算格式。

*细粒度量化*：
<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*HczXZ-Xm1xynrKsJJqjaow.png" width="600">

从 32 位浮点数转换为 8 位整数，一般做法是：取数字序列，用其中的最大绝对值进行除法运算，然后乘以127（因为我们要转换为 8 位整数）。这种做法显然对异常值敏感，比如一堆 0~10 区间的数混入了一个 100，则会导致 0~10 区间的数量化后分布非常密集，区分不开从而失去精度。

DeepSeek 提出的细粒度量化，做法是，不是用一个单一的数字来缩放所有的数值，而是将整个激活输出分解成多个块，每个块用自己的最大值进行缩放，因此每个块都有自己的缩放因子。怎么分组？例如 $256\times 256$ 的矩阵分为 4 个 $128 \times 128$ 的矩阵，即便其中一个出现异常值，其它三块不受影响。

所以 (a) 图中，上面是 Input，右侧是 Weight，分别分组缩放，Tensor Core 中用低精度乘法计算，并在输出中根据各自的缩放因子，反量化为高精度表示，这是在 CUDA Core 上实现的。


*提高累积（求和）精度(图 7b)：*

GEMM：通用矩阵乘法。当执行矩阵乘法运算时，有类似 $y=wx+b$ 这样的式子，如果 $w,x$ 都是 FP8，由于中间结果太小导致下溢问题，我们会很快失去精度。这意味着，如果两个浮点数相乘，当降低这两个数的精度时，再对它们进行乘法运算，接着又执行一系列类似的操作，由于中间结果过小，我们会迅速失去精度，这种现象被称为*受限累积精度*。

因此，如果我们在 TensorCore 上进行 FP8 运算，它的累积精度是有限的。所以，TensorCore 在内部以远低于 FP32 累积精度的受限精度累积 GEMM 结果。因此，如果将这些结果存储在 TensorCore 上，它们的精度总是较低，大约为 14 位，但这并不好，会导致数值错误。理想情况下，我们希望将这些参数以尽可能高的精度保存在 CUDACore 上，而不是 TensorCore 上。因此，在执行 GEMM 操作时，比如处理内部维度 $k$ 较大的矩阵时，低累积误差和低累积精度可能会导致显著的数值错误。例如，如果用内部维度 $k$ 为 4096 的矩阵相乘，并且我们使用低累积精度，这可能导致高达 2% 的误差，这会严重影响模型的准确性，这是相当不利的。

*如何提高累积精度呢*？作者提出的建议是，假设我们正确完成了这些计算，在 TensorCore 中得到了计算结果，我们可以定期将它们转移到 CUDACore。图 7b 中绿色乘以黄色，GEMM操作，结果为浅粉色圆点代表 TensorCore，它会定期移动到深粉色圆点所示的 CUDACore。这意味着我们会将中间累加结果从低精度的 TensorCore 临时转移到高精度的 CUDACore（即 FP32），并在计算过程中定期执行这一操作。这一过程被称为"提升至 CUDACore"。

WGMMA（warp group level matrix multiply accumulate（warp组级别矩阵乘积累加））。这是一个NVIDIA GPU 的术语，其中 warp 是指一组线程的集合。本质上就是 warp 组级别的矩阵乘积累加，它利用一组 warps 来执行矩阵乘积累加操作。可以简单理解为它可以在 NVIDIA GPU 中高效执行 MMA运算。在经过一定间隔（通常为 128 个元素，此处用 $N_C$ 表示）后，部分低精度累加结果会被提升——这里的“提升”指的是它们被复制到 CUDACore 中的更高精度寄存器中。

从低精度到高精度，这是反量化操作，因此需要蓝色的缩放因子。这就是整个提升累加精度背后的原理。

*尾数超过指数：*

<img src="https://substackcdn.com/image/fetch/$s_!Bn0Y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4783fd02-a138-40c7-82c7-79dd05a179e4_1472x772.png" width="500">
每个浮点表示法都包含符号位、指数位和尾数位，如图所示。尾数决定了精度，而指数则决定了动态范围。对于 FP8 浮点格式主要分为两种：e4m3 和 e5m2。e4m3 有 4 个指数位和 3 个尾数位，而 e5m2 有 5 个指数位和 2 个尾数位。

通常人们会使用精度更高的 e4m3 进行前向传递，而使用精度较低的 e5m2 进行反向传递。因此，前向传递具有更高的精度但动态范围较低，而反向传递则精度较低但动态范围较高。*DeepSeek 则始终使用 e4m3，并认为由于细粒度量化的优势，这种做法对他们有利*。

*举例而言：* 假设我们有一组数字 `[0.25, 0.5, 0.75, 32.0]`，如果不进行细粒度的量化处理，这些数字会被最大的那个异常值除，导致它们变得非常小 `[0.0078, 0.0156, 0.0234, 1.0]`，并且在 e4m3 格式下由于只有三个尾数位可用，这些数字会显著丧失精度（例如 0.0078 就会失去精度）。但在这种情况下，使用细粒度量化时，数值并不会变得这么小，例如分组 `[0.25, 0.5]` 和 `[0.75, 32.0]`，每组都有独立的缩放因子 0.5 和 32.0，缩放后为 `[0.5, 1.0]` 和 `[0.0234,  1.0]`，数值依然能得到非常精确的表示。因此，即使我们按比例因子进行缩放，也不会有数值变得非常小，并且数值仍然可以在 e4m3 格式中准确表示而不会丢失精度。

这里的主要思想是，如果没有细粒度的量化，一些数值会变得非常小，如果我们全程使用 e4m3，这些数值就会失去精度。但通过细粒度量化，数值不会变得极小。因此，即使我们只有三个尾数位，通常也足够应对各种情况。正如 DeepSeek 所提到的，每个元素实际上共享该组的指数位，这是由于细粒度量化的实现方式，使得每个组内都能获得更高的精度。这显著扩展了有效精度，而无需额外的指数位。

*在线量化：*

首先需要理解什么是延迟量化。量化需要比例因子，将每个值除以最大的绝对值，然后乘以比如 127。延迟量化的意思是，用于量化当前张量的比例因子来自过去的迭代，也就是说它是从历史信息而非当前值中得出的。这意味着我们使用过去的最大值来缩放当前张量，这可能会导致不准确。因此，如果当前张量的范围差异很大，可能会导致溢出或下溢。

而在线量化的做法是 DeepSeek 的解决方案，即实时根据当前张量的数据计算比例因子。因此，最大绝对值是实时计算的。这不会导致溢出或下溢的问题。






