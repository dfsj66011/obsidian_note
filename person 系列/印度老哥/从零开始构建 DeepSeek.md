
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


（24.56）

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
	* 强化学习的兴起
	* 不再仅仅依赖人工标注的数据，而是利用大规模强化学习来教授模型进行复杂推理
	* 基于规则的奖励系统
* 第三，他们实现了多项 GPU 优化技巧；
	* 采用了 NVIDIA 的并行线程执行技术（PTX）
* 第四，构建了一个有利于蒸馏等技术的模型生态系统
	* 蒸馏至更小的模型（1.5B）


### 1、MLA

分为四个部分：

* LLMs 架构本身
* 自注意力
* 多头注意力
* KV 缓存

我们需要了解这个引擎实际上是如何运作的。在之前的课程中，我们已经学习了一些关于这个引擎的知识。我们学到了什么呢？首先，我们了解到这个引擎拥有海量的参数。举个例子，GPT-3约有1750亿个参数，而GPT-4虽然尚未正式发布，其参数规模可能已达到万亿级别。至于两三天前刚发布的GPT-4.5，其参数可能高达5万亿甚至10万亿（尽管尚未完全公开）。但我们可以确定的是，这个引擎通过天文数字般的参数协同工作。

如果你搜索汽车引擎的工作原理，就会发现这里有一个活塞气缸机构，对吧？这个活塞气缸机构本质上就是引擎运转的核心。同样地，我们需要确保自己理解这些参数究竟如何运作——这些参数藏在哪里？我们能否像打开汽车引擎盖那样，打开大语言模型的"引擎"看看内部运作？具体来说，当输入一串词语时（就像汽车注入燃油），这个"引擎"内部究竟发生了什么？燃油如何转化为汽车的动力，同理词语序列如何在这个语言模型里被处理，最终促使模型预测出下一个词——这才是隐藏在LLM之下的奥秘。

所以，把单词序列想象成燃料，把大语言模型（LLM）比作引擎，而下一个单词就是汽车的行驶动作。这么说吧，我们现在要打开引擎盖，真正理解引擎的工作原理，就需要了解引擎的结构。这意味着我们需要知道引擎内部有哪些不同的部件，这些部件之间是如何相互连接的，以及当燃料（即单词序列）通过这个结构时，到底发生了什么变化。这就是我们今天这节课要深入探讨的核心内容。

所以你可以把今天的讲座想象成打开汽车引擎盖，试图窥探引擎内部，真正理解它是如何工作的。另一种理解方式是，我们所有人都用过GPT这样的工具，比如当我们提出“给我写一篇关于友谊的短文”这样的要求时，我们会发现GPT实际上是一个标记一个标记地预测输出的。那么，这种预测是如何实现的？是什么样的架构在支持这种逐个标记的预测？这就是我们今天要探究的内容。

好的，我会尽量让这个讲座对初学者友好一些。你会看到我构建了一个完整的故事来解释LLM架构的工作原理。我认为以前从未有人这样做过，所以对我来说，用这种故事的形式来解释也是一种实验。但希望通过这个解释，你能真正理解LLM架构是如何运作的。那么，这是当你查看引擎时的示意图，当你打开黑盒子时会发生什么，黑盒子里面有什么。当你打开黑盒子时，你会发现有很多东西跳出来，一点也不简单。如果你想到1750亿个参数，这些1750亿个参数分散在这个黑盒子的多个地方。换句话说，LLM架构相当复杂。

你认为它可能很复杂的原因是什么呢？因为在进行下一个标记预测时，大语言模型实际上是在学习语言本身。我强烈认为语言学习是下一个标记预测任务的副产品，而要学习语言，你不能有一个太小的引擎，也不能有一个不够复杂的引擎。这就是为什么我们的引擎相当复杂——我们有大量相互连接的块或层。今天我们将尝试理解这种架构。

所以如果你看一下这个示意图，你会发现它大致分为三个部分。首先是这里的第一个部分，我称之为第一部分；然后是第二部分，我标记为变压器模块，让我这样标注一下；最后是第三部分，基本上是输出部分，也就是预测下一个标记的地方。因此，LLM的架构可以分为三个部分来理解：第一部分可以看作是输入部分，第二部分可以看作是处理器，第三部分本质上就是输出部分。在输入部分，我们有句子。

比如说，你有一个句子，任何句子都可以，比如“明天会更好”。在将这个句子传递给处理器之前，这个句子以及其中的每个标记或每个单词都会经历一系列处理步骤。首先，我们会进行所谓的“分词”（tokenization）；其次，我们会进行“标记嵌入”（token embedding）；最后，我们会进行“位置嵌入”（positional embedding）。这三个步骤完成后，输入内容才会被传递给处理器，也就是所谓的“Transformer块”。

在Transformer块内部，有六个不同的组成部分：归一化层、多头注意力层、Dropout层、第二个归一化层、前馈神经网络层以及另一层Dropout层。这里的两个加号代表所谓的跳跃连接或快捷连接。最后，当我们从处理器或Transformer块出来时，会得到输出结果，这里还有一个归一化层以及用于预测下一个token的最终层。好了，我刚才所说的内容，如果你是第一次学习这些，可能会觉得有点摸不着头脑。

而这一切看起来太复杂了，让我们进一步分解，这正是我现在要做的。但请密切关注我用紫色标记的这一部分，因为我要解释的第一个创新——多头潜在注意力（MLA）——与这个标为“多头注意力”的方面有关。在整个架构中，深度探索（DeepSeek）在两个主要地方做出了创新贡献：第一个就是这个称为“多头注意力”的模块，第二个是前馈神经网络。让我现在稍微擦除一下，MLA或多头注意力是架构这一部分的创新。

所以我将这种创新称为MLA（混合专家）架构中的创新。再次强调，如果你不了解架构本身，就无法真正领会创新点在哪里。这就像拆开发动机一样——你现在拆开的是DeepSeek的"引擎"，这个表现优异的"汽车引擎"。要理解它为何表现卓越，就得打开引擎盖，观察所有这些零部件的运作。

现在我告诉你，这台引擎中有两个部件经过增强以提高性能。不过言归正传，回到引擎本身——输入处理器和输出系统究竟如何运作？这次课程我给自己设定的挑战，就是要用一堂课的时间把所有这些原理讲透：既要解析输入系统，又要阐释处理器，还要说明输出机制。我希望用通俗易懂的方式讲解，让你们能真正感知到整个架构的精妙。我的讲解思路是从燃料的视角出发——假设你是一滴燃油，进入汽车引擎后会经历什么？是先进入引擎内部，然后被活塞带动旋转，最终产生某种动力或能量吗？只要理解了燃料的生命历程，实际上就掌握了引擎的工作原理，对吧？

今天我想向大家展示一个单词的生命周期，以及当它经历LLM架构时会发生什么。我们可以这样思考：当我们输入这个句子"给我一篇关于友谊的短文"，或者假设我们要完成下一句话，比如我选择的句子是"明天会更好"——"明天会更好"这句话进入LLM引擎后，系统就会预测出下一个标记。

假设下一个词是“and”，而明天是光明的。我想向你们展示的是，我们将只关注一个词，看看这个词在LLM架构的每一步中会发生什么。通过这个词的视角，我希望你们现在想象自己就是这个词。你就是这个词，我将带你经历这个词在LLM架构的多个层中所经历的一切，最终我们预测下一个词。这就是我如何以故事的形式向你们解释这整个讲座。我会把这次讲座讲得好像你就是这个词一样。现在想象你就是这个词，你被一堆词包围着，突然你被扔进了LLM架构中。

让我们来了解单个token的生命周期，这就是接下来这部分讲座的主要内容。好的，让我们一起踏上这段旅程，了解token的生命周期究竟是什么样的。我给这一部分起的标题是“token在LLM架构中的旅程”。我首先做的是去ChatGPT那里，让它写一段关于朋友的短文。然后我从里面随机选了一句话：“一个真正的朋友会接纳你”。假设我们正在看这句话——“一个真正的朋友会接纳你”，它由五个单词组成。

假设这是我的当前输入序列，我们需要根据这个输入序列预测下一个标记。我将特别关注“friend”这个词，并从“friend”的角度来思考。现在，请站在这个词或这个标记的立场上——首先，这个标记（为了方便起见，我会交替使用“标记”和“词”，尽管它们并不完全相同，但为了简单起见，我就说一个标记等于一个词）。所以，请设身处地为这个标记着想。

现在你看到了什么？你看到我周围还有这些其他标记，对吧？那里有一个真正的接受者（true accepts），还有这四个其他标记，我习惯和它们待在一起，就像朋友们聚在一起一样。我习惯把它们当作我的邻居和朋友——一个真正的接受者（true accepts）和你。这四个邻居就是我们为这个标记选择的“朋友”。现在突然发生的第一步是——在LLM架构中，第一步就是隔离的步骤。所以目前我们正在观察这个阶段，也就是输入阶段。

在输入阶段的第一步是隔离阶段，即单词与其相邻词分离。想象一群朋友，每个人都与同伴隔开——这个词被单独隔离出来，我们孤立地审视它，这就是第一阶段。第二阶段本质上称为"令牌ID分配"，这意味着现在每个单词都被隔离，我们要为每个词或令牌贴上类似徽章或编号的标识。就像参加营地、入伍或任何团体活动时会被分配ID（比如学号），学校里每个人都有学号对吧？同理，每个令牌被隔离后都会获得专属令牌ID。我将其比喻为"领取徽章"的过程，而令牌ID的分配机制其实非常有趣——我们拥有一本令牌ID字典。

所以你可以把这想象成一本百科全书或一本标记ID的书。这本书里基本上列出了所有可能的标记，所有可能的标记就是所有可能的单词，然后每个单词都对应一个数字。这本书里不仅有单词，还有字符，甚至还有子词。所以这本书包含像a、b一直到z这样的字符，它甚至包含像“cr”这样的子词，这个词可能就在这个词汇表中。然后它还可能包含像“isation”这样的子词，这可以是一个子词。此外，它还包含像“token”这样的完整单词，这个词可以在这个词汇表中作为一个完整单词存在，“enter”也可以是一个完整单词，“begin”同样可以是一个完整单词。

所以你可以把这本标记ID的书想象成由字符、单词和子词组成。让我在这里写下来，这一点非常重要：这本标记ID的书本质上由字符、单词和子词组成。因此，我们确保每个被分离的标记或单词都能找到对应的编码，不会有任何被分离的标记或单词找不到对应的编码。熟悉字节对编码概念的读者会记得，为了创建这本标记ID的书，我们使用了一种称为字节对编码的方案。这是一种子词标记化方案，而GPT-2就依赖于字节对编码机制来创建其词汇表。这本标记ID的书也被称为词汇表。

然后它会从一个大型语言模型转变为另一个，比如说GPT-2的词汇量是5万，而GPT-4的词汇量可能更高，也许是10万。所以根据我们使用的大型语言模型，分配给某个词的标记ID可能会变化。我现在使用的这个大型语言模型词汇量是5万，这意味着有5万个标记，可能是字符、单词和子词的组合。接下来我要做的就是查看这个词汇表。

我要找到“friend”这个词在序列中的位置，并确定与之关联的token ID。现在“friend”这个词对应的token ID是2012，我会把这个数字记录下来——这就像是给这个词分配的徽章或学号。所以现在“friend”这个词的token学号就是2012。同理，其他所有token（即所有其他单词）都会获得类似的编号标识。这是整个流程的第一步，或者更准确地说，是第二阶段的工作：token ID分配。

那么现在想象一下，这个被孤立出来的标记朋友，它现在被赋予了一个徽章或编号——2012，这就是第二阶段。我没有详细解释这个标记ID簿是如何创建的，因为如果你想了解更多细节，有一个单独的讲座专门讲解如何为每个大型语言模型构建这种词汇表，它被称为"从零开始实现字节对编码"。这个讲座包含在"从零开始构建LLM"系列课程中。但现在请跟着我的思路——想象你就是这个标记朋友，你被孤立出来后获得了一个编号，然后你就进入了第三阶段。在第三阶段会发生一件有趣的事情：在此之前，你只关联着一个数字。

没错，但现在你将拥有一个庞大的数字向量，这些数字将与您相关联，这被称为标记嵌入分配。可以这样理解：假设我们有一个入学考试，共有768道题目，每道题本质上都在测试你的某个特征。现在我们来看“朋友”这个词，每道题会测试：你是名词吗？你有性别吗？你是动词吗？你是运动吗？你是情感吗？等等。

实际上，我们并不清楚这些特征或问题的具体内容，但我正试图向你解释，以便你能对词嵌入有一个直观的理解。想象一下，有768个类似这样的问题，每一个被我们单独提取出来的词都会被问到这些问题。然后，根据得到的答案，我们可以理解这个词的某些特性——比如它是一个名词、一项运动、一个形容词，还是总是出现在句子末尾的某个词；或者它是否与性别相关，是否与君主制相关，比如国王、公主、女王等等。

现在我们终于开始了解标记本身的意义了。在标记ID分配阶段，我们并未涉及任何意义层面的内容。但到了第三阶段的标记嵌入（token embedding），由于我们提出了一系列问题，就能获取到一些语义信息。根据这些问题的回答结果，每个标记都会生成对应的数值。比如这个"friend"标记，它会对每个问题产生一个数值（可能是0.1、0.2、0.1、0.3等）。假设我们设置了768个问题，那么就会生成一个768维的向量。需要特别说明的是，这个问题的数量（768）会因不同的大语言模型而有所差异。比如我们来看GPT-2模型，它的标记嵌入维度...

所以如果我们搜索GPT2的词元嵌入维度，会发现它是768对吧？但这里GPT2小型版也是768，而最大的GPT2模型则有1600维。因此这个768的提问数量实际上会因不同大语言模型而异。接下来我们要做的是：假设GPT2的提问数量是768，但请记住——如果词元进入不同的大语言模型，它可能会被问不同的问题。现在想象你是一个词元，刚领到编号徽章，突然就要回答这768个问题。当你作答后，所有答案会被收集成一个768维的向量——这就是所谓的词元嵌入向量。

所以现在，除了徽章之外，你还随身携带着你的结果，也就是说你有一个徽章，还有这768个数值的结果，这就是到目前为止发生在你身上的事情，对吧？这就是词元嵌入的阶段。再次强调，词元ID和词元嵌入的区别在于，词元ID不携带任何关于语义的概念，而词元嵌入在分配时会非常关注单词本身的含义。进行词元嵌入的原因是为了创建LLMs（大语言模型），你最终需要提取意义，对吧？你是在向模型教授一些关于语言的知识。

因此，这是一个非常关键的步骤。这些关于每个标记的问题或特征集合都会被收集起来。到目前为止，每个标记都有一个标识，并且每个标记都附带一个768维度的值表。此外，还有一个重要的因素是你在邻居中的位置。在这里，如果你看到“一个真正的朋友接受你”，那么“朋友”这个词位于句子的中间，也就是第三个位置。具体来说，“一个”在第一位，“真正的”在第二位，“朋友”在第三位，“接受”在第四位，“你”在第五位。所以，“朋友”出现在第三个位置，而这个位置也很重要。为什么位置很重要呢？

因为如果你说“狗追另一只狗”，看看这句话，你需要意识到这只狗和另一只狗是不同的。如果只按字面意思理解，如第三阶段所示，我们只取了词义，那么这两只狗的标记嵌入会是相同的。但实际上这是两只不同的狗，我们需要教会模型识别这一点。唯一能区分这两只狗的方法就是知道第一只出现在第二个位置，第二只出现在第五个位置。

因此，了解位置信息同样重要。类似于我们提出的768个问题，关于位置信息也会再次提出768个问题。需要注意的是，虽然不同模型这个数字会有所变化，但如果你固定一个特定的语言模型，那么在词嵌入中提出的问题数量与位置嵌入中提出的问题数量是相同的。以我们现在研究的GPT-2小模型为例，词嵌入中提出了768个问题，同样地，位置嵌入中也会提出768个问题。

那么这些位置或问题可能是什么呢？它们可能是类似这样的问题：你是处于序列的开头、中间部分，还是能够处理长距离依赖关系等等。实际上，没人确切知道这些问题具体是什么，但我觉得这是解释位置嵌入和标记嵌入最简单的方式。现在进入第三阶段，每个标记都有一个与之关联的标记嵌入——这是一个768维的向量。而到了第四阶段，根据所处位置，你需要回答这768个问题，对吧？

所以你还会有一个768维的位置嵌入与你相关联。现在想象一下，一个标记首先会受到标记ID的印记或徽章的影响，然后它会得到标记嵌入的结果。这些都是它需要回答的问题，也就是第一个测试。接着，这个标记会进入另一个测试，即位置嵌入，然后它再次拥有这768个值。每个标记都需要进行大量的处理。它基本上必须通过大量的测试。然后在第五步，我们所做的是将你的标记嵌入结果与位置嵌入相加。这样你就不必再分别携带这两个结果了。

你将它们合并在一起。所以现在768维向量的词元嵌入和768维向量的位置嵌入相加在一起，这就是所谓的输入嵌入。因此，这就是词元“friend”的输入嵌入。它是一个768维的向量。对于所有其他词元或所有其他单词，我们也会有类似的输入嵌入。但在这里，我向你展示的是词元“friend”的输入嵌入。这是词元嵌入加上位置嵌入的结果。所以现在你不必分别携带这两个结果。你只需要携带一个结果。

现在，那768维向量就是与你作为标记相关联的标识。这是你在后续旅程中最重要的区分特征。想象一下这段旅程：你最初是孤立的，被授予徽章后，先接受第一次标记嵌入测试，再经历第二次标记嵌入测试。最终，经过所有这些步骤，你获得了唯一能标识自己的东西——输入嵌入。你可以将其视为你的专属制服。这套制服是为你量身定制的，而你作为标记就要穿着它。与你同行的每个朋友（每个标记）、每个相邻词汇都会穿着不同的制服。为什么？因为它们的含义各不相同，对这些问题的回答也会不同，它们所处的位置自然也不一样。

所以他们需要分别回答这些问题。因此，每个标记的统一处理方式都会有所不同。到目前为止，我们所做的这五个步骤，本质上就是输入块或输入层中发生的事情。这是我们目前学习的第一部分。现在，我认为这里提到的这三个步骤对你们来说会很容易理解。首先是标记化。

其次是标记嵌入。第三是位置嵌入，这正是我们刚才看到的，然后标记嵌入和位置嵌入相加，得到所谓的输入嵌入。整个过程就是这样，记住这里的标记化文本。我们通过词汇表或标记ID手册看到了标记ID的分配。这就是每个标记经过输入块（这里是第一阶段的第一部分）后的输入嵌入。它们有一个统一的特征，使它们与其他标记区分开来。

好的，这就是第一部分——输入部分。只有当你拥有了统一的“制服”，才能进入下一个环节，也就是处理器部分。现在，每个标记（token）都穿上了统一的“制服”，就像《哈利·波特》里只有穿上特定学院的制服（比如格兰芬多、拉文克劳或斯莱特林）才能进入霍格沃茨一样。此时，每个词或标记都拥有了自己的“制服”，终于可以登上通往Transformer模块的“列车”了。

你看，这五个标记现在会一起进入Transformer模块。真正的朋友会接纳你的一切。现在每当我提到“制服”，你应该想到它代表的是一个768维的向量。Transformer模块并不理解单词，甚至目前还完全不懂单词的含义——它只知道每个标记都是一个768维的向量。而在Transformer模块中，神奇的事情将会发生：不同标记之间的含义会变得清晰可辨，模型自身也将逐步理解语言。

因此，变压器块本质上就是所有魔法发生的地方。第二个处理器部分才是真正一切发生的核心所在——就是这里的这个部分。这个处理器部分确实是所有奇迹诞生的地方。你可能会想：大语言模型究竟是如何运作的？虽然它们只是预测下一个标记，但它们似乎已经掌握了语言的某些本质。奇妙的是，它们与人类互动的方式，几乎就像你在和我这个真人交流一样。

它们能总结任务，擅长语法检查，帮我起草邮件，还能完成复杂的编码工作。这一切都归功于Transformer模块的运行机制。现在，每个标记实际上都会经历一段旅程，穿过卡车（比喻性表述）进入Transformer模块本身。好了，现在让我们来思考这个Transformer模块。

我们需要明白，Transformer模块就像一列由大量不同部件组成的火车，对吧？首先我们要看看Transformer模块这列火车本身的组成部分。我不会详细讲解所有部件，只是简单介绍一下Transformer模块中每个部件的作用。现在想象一下，这五位乘客被分配到了一个车厢里，而且他们现在都是768维的输入嵌入。

他们必须在一个Transformer块中经历一、二、三、四、五、六这六个步骤。那么这六个步骤是什么呢？你可以这样想象：如果把Transformer块比作一列火车，这六个步骤就像是连接在一起的六个车厢。一旦你上了这列火车，就必须和你的“邻居”一起经历所有这六个步骤。

第一步是层归一化（layer normalization），这意味着对768维的向量进行处理。现在让我们聚焦在“friend”这个词上。这个768维的“friend”向量会被归一化，即调整其均值和标准差，使得均值变为0，标准差变为1。这个步骤相对简单。然后我们来到多头注意力机制（multi-head attention）。这里我用不同的颜色标记了这个部分，因为这才是真正赋予Transformer块强大能力的创新之处。

我们本质上是在学习：当我们关注一个标记时，应该给予其他标记多少注意力？例如，当你看到“朋友”这个词时，应该对“真实的”和“你”给予多少关注？多头注意力机制实际上编码了关于上下文的信息。当你观察一个标记时，你会突然绘制出一张地图，显示所有其他标记的重要性。仔细想想，这对理解语言、理解句子本身的上下文或段落的上下文非常有帮助。比如我说：“我来自印度浦那，我讲……”在这里，如果你要完成下一句话，你需要知道应该更多地关注“浦那”和“印度”，因为那是我来自的地方，对吧？

所以你不需要太关注前三个标记，也许“So”这个词说明了注意力机制的重要性，它帮助我们理解句子的上下文并预测下一个可能的词。我们将在下一节课中更详细地学习注意力机制，但请记住，这是Transformer块的第二个组成部分。Transformer块的第三个组成部分是Dropout层。如果你已经学过神经网络，Dropout本质上是指如果有100个参数，且Dropout因子为0.5，那么你随机将其中50个参数设为0。为什么呢？

因为如果某些参数很懒，根本不学习任何东西呢？突然，如果其他参数现在被丢弃，意味着它们被设为0，这些参数别无选择，只能自己学习一些东西。因此，dropout是一种让懒惰参数重新活跃起来的机制。它提高了泛化性能，并防止过拟合。所以，如果你看到这里有一层dropout，然后还有一层dropout。在dropout层之后，

我们有一个跳跃连接或捷径连接。捷径连接的作用是帮助梯度通过另一条路径流动，确保不会出现梯度消失问题。然后经过归一化处理，接着是多头注意力机制和丢弃层（dropout），之后还有另一个归一化层，其功能与第一个相同。随后是一个前馈神经网络，这也是Transformer模块中非常重要的组成部分。具体来说，当前我的词元（token）维度是768，前馈神经网络实际上会将其映射到更高维的空间中。

这是768的四倍，然后它又将其压缩回768维空间。这种扩展-收缩机制确保我们探索更丰富的空间。我们探索的是具有更多维度和更多参数的空间，从而确保我们的语言模型拥有足够的参数来捕捉额外的复杂性。前馈神经网络正是深度探索中专家混合创新实际发生的地方。

最后我们还有一个dropout层，然后还有一个跳跃连接或快捷连接。记住这些加号，无论它们出现在哪里，都代表跳跃或快捷连接。它们确保梯度有替代路径流动，因为如果梯度以链式方式流动，一旦某个梯度很小，相乘后梯度就会变为零，或者如果梯度很大，相乘后就会爆炸。这可能导致梯度消失问题，使学习停止，或者导致梯度爆炸问题，使学习变得非常不稳定。因此，这五个标记必须经历这些不同的步骤。

每个标记都必须经历以下流程：先经过归一化处理，再通过多头注意力机制，接着是随机失活层和跳跃连接，然后再次归一化，随后通过前馈神经网络，再经过一次随机失活层，最后再进行一次跳跃连接。这就是Transformer模块的结构。如果你回顾我们在课程开始时看到的示意图，你会发现这里描述的完全一致——每个标记都必须依次通过层归一化、注意力机制、随机失活、跳跃连接、前馈神经网络的层归一化、随机失活以及跳跃连接。这就是每个标记必须遵循的完整路径，看起来确实是个相当繁琐的过程，对吧？

首先，我必须完成这五个步骤才能拿到我的制服，除此之外，之后我还得逐一检查每个变压器模块，并完成这些步骤。但还有一层额外的复杂性——就像这个变压器模块一样，一个大型语言模型包含多个变压器模块，对吧？比如GPT-2，它有多少个变压器模块呢？如果你看一下GPT-2本身，GPT-2小型有12个变压器模块，GPT-2中型有24个，GPT-2大型有36个，而GPT-2 XL则有48个。所以，即使我们现在只看小型模型，每个变压器模块都包含所有这些步骤。

所以现在每个token基本上都要重复这12个步骤 嗯哼。因此这就是为什么——如果你把一个transformer块比作火车的一节车厢——总共有12个这样的transformer块相互连接，而每个token都必须完整通过这12个transformer块。整个旅程变得极其繁琐。你看我已经把流程画出来了对吧？这12个transformer块就是每个token必须穿越的关卡。就像这位token朋友，它得先通过第一个transformer块...

它必须经过第二个。同样地，它必须经过第三个，以此类推，必须经过这全部的12个变压器模块。所以这里的第12个模块就像一段非常繁琐的火车旅程，每个标记都必须严格遵循这一路径。获得统一是一项艰巨的任务，我们需要经历五个步骤。而通过处理器的过程更是艰难，因为你必须再次经历这全部的12个步骤。这就是处理器中实际发生的事情，也就是我们刚才在处理器中看到的第二部分。

情况是这样的：我这里展示了一个Transformer模块，对吧？如果我们使用GPT小模型，你可以想象这个模块被复制12次；如果使用最大的GPT-2模型，则需要48个这样的Transformer模块。而现代GPT模型可能包含96个甚至更多的Transformer模块。因此，每个token现在都必须通过这些模块进行处理。请记住，token的维度通常在通过Transformer处理后仍保持不变。比如说输入——我们之前看到的统一输入是768维的，如果你还记得的话，那是一个768维的向量，对吧？

在经过所有这些Transformer块后，经过12个Transformer块后，它从这12个Transformer块中出来时保留了其维度。因此，它仍然有768个维度。所以现在，一个真正的朋友会接受你已经从Transformer块中出来，并且它们自然仍然都有768个维度，当然，值已经发生了变化，对吧？然后它们现在进入输出层。这里有一个归一化步骤。所以如果你在这里看到，这里有一个归一化步骤，被称为最终层归一化。

所以归一化这一步在这里被提到了。每一个768维的向量都会再次经过这个归一化阶段，然后我们还有最后一层，这层非常重要。现在记住我们已经到达了最后一层，我们有一个真实的，所以这是一个真正的朋友需要你，每一个都是768维的向量，对吧？现在我们需要以某种方式将这768维转换成我们的词汇量大小，也就是50,000，因为现在我们需要预测下一个标记。

那么每个标记本质上都会通过一个神经网络，其大小为50,000，或者说大小为768乘以50,000。因此，当这些向量与之相乘时，每个标记会生成50,000维的向量。所以现在每个标记的大小是——一个真正的朋友需要你——对吗？每个标记在经过输出层后的大小将是50,000。这一层也被称为输出投影层，每个标记经过输出投影层后，其维度等于词汇表的大小。

记住我们的词汇量是50,000。这个50,000来源于词汇量，我来解释为什么需要维度相等。词汇量和最后一步是选择下一个标记，对吧？现在我们到达最后一步时得到了什么？我们有五个标记或"真正的朋友接受你"，每个标记都有一个50,000维的向量。接下来我们要做的是查看这50,000个维度，找到具有最高值或最高概率的索引。

然后我们会在这里找到那个索引，接着寻找它对应的标记。就是这样。所以如果这里的索引是第一个“The”，那么“a true friend accepts you”在这里有多个输入输出任务。当输入是“O”时，输出应该是“true”；当输入是“O true”时，输出应该是“friend”；当输入是“O true friend”时，输出应该是“accept”；当输入是“O true friend accepts”时，输出应该是“you”；当输入是“O true friend accepts you”时，输出会是其他东西，也就是“for”。所以“a true friend accepts you”，如果你看这个句子并且要预测下一个标记，这不仅仅是一个下一个标记的预测任务。

同一句话中包含多个输入-输出任务。当O作为输入时，真实值应为输出等，这些输入-输出预测任务是什么？而这里唯一与我们相关的是最初的下一词预测。当然，一开始我们不会得到好的词元，对吧？但我们会有一个基于实际值的损失函数。

所以这就是实际的下一个标记，也是我想要的，但最初预测的标记会完全偏离。这时就需要反向传播发挥作用，当所有参数都存在时，它们实际上会被优化。我们稍后会讲到这一点。但现在，让我再解释一下最后一步。我们有了每个标记，现在每个标记都与一个不同的统一体相关联，其维度为50,000。为什么维度是50,000？因为我们需要为每个单词预测下一个标记，而这里就是如此。比如对于“O”，我们需要预测下一个标记。

所以我们查看那个索引，也就是概率最高的那个标记ID。我们转到词汇表或标记ID表，然后反向映射出对应那个标记ID的单词。比如这个标记ID是555，或者这个标记ID是5000。我在这里找到5000对应的单词，理想情况下我希望这里输出的是"true"。但在训练初期模型未收敛时，可能输出的是"for"。同样地，我也会得到"true"的实际预测结果。

也许最高的token ID在这里是你的朋友，然后我会在这里获取最高的token ID，这就是我如何预测每个token的下一个token。然后我会找出实际值和预测值之间的最后一个项。这就是整个LLM架构的结构。所以现在如果你去看输出层，也就是我的最后一层，你会看到我们有两个相互连接的东西，对吧？我们有最终的层归一化层，它连接到输出层，然后我们有用于下一个token预测的矩阵。这个logits矩阵就是这个，就是我刚才展示给你的那个，我们用这个来预测下一个token。所以现在你可能会想，这里优化的所有参数是什么？从最开始就在这些token嵌入值中。

我们无法先验地知道这些参数。因此，让我用星号标记那些需要训练的参数——这些是我们无法预先知晓的。这些需要训练的参数包括位置嵌入的分配、分配方式等。我们无法先验地确定它们，所以它们都需要通过训练获得。Transformer模块的每一个环节都包含若干需要训练的参数：多头注意力机制有需要训练的参数，前馈神经网络也有需要训练的参数。而且这样的模块有12个或24个，这进一步增加了参数规模。因此在整个过程中存在大量需要训练的参数，甚至最后的神经网络层也不例外。

它有这么多需要训练的参数，所有这些参数加在一起构成了参数的总数，达到1750亿甚至可能一万亿。想想我们最初构建的引擎，对吧？我们一开始就知道，我们是从思考开始的。好的，这就是LLM引擎。但LLM引擎实际上是如何工作的呢？LLM引擎背后的参数是什么？这1750亿个参数究竟在哪里？

现在我们来看一下详细的架构，即输入部分、处理器部分和输出部分。我们已经了解了单个标记的处理过程，对吧？本质上，一个标记首先经过输入阶段，在那里它被分离出来，并被分配一个标记ID或标识。然后，它会接受一个测试或一组768个问题，这就是标记嵌入，它编码了含义。

然后，它会接收第二组问题，即位置嵌入，用于编码其位置值。我们将标记和位置嵌入相加，得到每个标记的输入嵌入或统一表示。通过这些统一表示，不同的标记进入Transformer块进行处理。每个Transformer块基本上包含以下部分：归一化层、多头注意力、Dropout、再次归一化、前馈网络以及Dropout，中间还穿插着两个跳跃连接。

在GPT-2中有12个这样的模块，而在GPT-2 excel中我认为有48个这样的模块，但在更高级的GPT模型中，可能会有96个甚至更多的类似模块。因此，每个标记都需要经过所有这些模块的处理，当它通过这些模块后，其大小仍然保持768。然后，它还会经过一个归一化层，大小依然是768。最后，我们有一个输出层，对于每一个标记，它会被转换成一个大小为50,000的向量，这个大小等于词汇表的大小。

然后我们逐个查看每个标记，基本上它是一个50,000维的向量。接着我们查看那个概率最高的标记ID，并用它来预测下一个标记。因此，在一个序列中，我们会有多个输入输出预测任务。比如，如果我们有一个包含五个标记的序列，那么就有五个输入输出预测任务，这些任务最终构成了我们的损失函数。

那么我们的损失函数基本上是通过反向传播来优化的，所有这些1750亿个参数都经过多个阶段的调整：词嵌入中有参数，位置嵌入中有参数，在Transformer块的多个环节中有参数，输出层中也有参数。

所有这些参数本质上都是通过反向传播优化的，最终我们得到的是一个对语言本身有直觉的模型，它还能预测下一个标记。所以下一个标记预测就是这里的任务。我们预测下一个标记，并与实际值进行比较。这就是任务，但在这个任务中，由于我们有这么多参数，副产品就是学习语言本身。所以在今天的讲座中，我的主要目的是带你经历一个标记的旅程。从一个标记的角度思考发生了什么。

试着打开这个引擎，试着打开LLM的这个引擎，真正去看看这个引擎实际上是如何工作的。我希望我已经向你们传达了我构造这个类比或一个token旅程故事的原因，是为了让你们真正理解LLM架构内部发生了什么。因为如果不理解这一点，我们就无法继续前进到下一个部分，即注意力机制。现在，计划是在下一讲中，我将首先阐述为什么我们需要注意力机制，然后我们将看看自注意力机制。

然后我们将探讨多头注意力机制。接着我们会研究键值缓存。如果你看接下来的计划，就会发现注意力机制的必要性。之后我们会学习自注意力机制，最终引出多头注意力机制。正如我所说，所有后续课程都经过非常详细的规划。这不是一个只有5到10分钟视频的简短系列，本系列的每一集都会相当长，大约40到45分钟，我计划逐步讲解所有内容。

多头潜在注意力是一个非常重要的概念。但希望大家在真正理解这个概念时能保持同步。非常感谢大家，我真的很期待在下一次讲座中见到你们。请和我一起做笔记。这个系列可能会有点难度，我正试图以尽可能容易理解的方式来提炼这些概念，但过程中可能还是会遇到一些挑战。

所以请大家做好笔记，以巩固你们的概念。谢谢大家，期待在下一讲中见到你们。大家好，我是Raj Dhandekar博士，2022年从麻省理工学院获得机器学习博士学位，我是“从零开始构建深度探索”系列的创始人。在我们开始之前，我想向大家介绍本系列的赞助商和合作伙伴——InVideo AI。大家都知道我们多么重视从基础构建AI模型的内容。InVideo AI的理念和原则与我们非常相似。

让我来为你展示一下 这里是InVideo AI的网站 凭借一支小型工程团队 他们打造了一款令人惊叹的产品 只需输入文字提示就能生成高质量AI视频 正如你在这里看到的 我输入了一段文字提示："制作一个超写实的高端奢华手表广告视频 要具有电影质感" 点击生成视频后 很快我就得到了这个令人难以置信的超写实视频 最让我着迷的是它对细节的把控 看看这个 画质和纹理简直不可思议

而这一切仅通过一条文本指令就实现了，这就是InVideo产品的强大之处。刚才你所看到的精彩视频背后，是InVideo AI的视频创作流程，他们正在从基本原理重新思考视频生成和编辑。为了试验和优化基础模型，他们在印度拥有最大的H100和H200集群之一，同时也在测试B200。InVideo AI是印度发展最快的人工智能初创公司，面向全球市场，这也是我如此认同他们的原因。好消息是，他们目前有多个职位空缺。

你可以加入他们出色的团队，我将在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建深度探索”系列的下一个讲座。今天我们将学习一个非常重要的概念，那就是注意力机制的必要性。首先，让我快速总结一下我们在这个系列讲座中已经涵盖的内容，然后再来讨论今天的主题。在两个讲座之前，在名为“深度探索基础”的讲座中，我们研究了深度探索架构的四个阶段，或者更准确地说，是我们将要涵盖这个系列讲座的四个阶段。第一阶段是深度探索架构中的创新。

第二阶段是训练方法  
第三阶段是GPU优化技巧  
第四阶段是模型生态系统

在上一讲《LLM架构》中  
我们重点探讨了第一阶段  
我们的主要目标是  
在接下来的两到三讲课程中  
必须开始理解多头潜在注意力机制  
这是深度求索架构中的首个重大创新

但要理解MLA（多头潜在注意力）  
我们不能直接切入这个概念  
需要循序渐进地推进  
因此我们首先剖析了LLM的架构本身  
发现其架构呈现这样的形态

一开始我们有输入部分，然后是处理器，接着是输出部分。每个部分基本上都有不同的构建模块。所以我们会发现，如果把大语言模型想象成引擎的话，要真正理解这个引擎的工作原理，我们确实需要打开引擎，看看里面到底有什么。

那么，大型语言模型（LLM）究竟是如何学会预测下一个标记或下一个词的呢？如果把这个过程类比为汽车的运动，我们会发现，当你给汽车加油时，汽车就会移动，对吧？引擎中发生了一些变化，汽车就开始行驶。同样地，对于LLM来说，燃料就是你输入到引擎（即LLM）中的一系列词语，而输出则是对下一个词或下一个标记的预测。正因为如此，我们也把LLM称为“下一个标记预测引擎”。

现在这个引擎拥有海量参数。GPT-3有1.75亿个参数，GPT-4可能有约1万亿个，而最近发布的GPT-4.5大概有5到10万亿个参数。为了真正理解这个引擎的工作原理，我们打开了黑箱，从这三个方面进行了研究。

但我并没有直接向你解释这三个方面。我告诉你，如果你把自己想象成一个令牌或一个世界，想象自己经历不同的阶段会怎样。所以如果你是一个令牌，首先你会被分配一个令牌ID，那是你的徽章，然后你会被分配一个令牌嵌入向量，接着是一个位置嵌入向量，令牌嵌入和位置嵌入相加，就形成了输入嵌入，也就是你的制服。

同样地，无论你的邻居是谁，他们也都有自己的统一表示或输入嵌入，大家一起登上通往Transformer的列车。每个Transformer块包含多个组件，如归一化层、多头注意力机制、丢弃层、跳跃连接，接着又是一个归一化层、前馈神经网络和丢弃层，最后再跟一个跳跃连接。而这仅仅是一个Transformer块的构成。

有多个这样的Transformer块，可能是12个、24个、96个等等。因此，作为携带你统一特征（即输入嵌入）的token，你需要经过所有这些Transformer块的处理，然后当你出来时，会有一层归一化处理，最后是一个输出层。在这个输出层中，如果你是一个768维的向量，你将被映射或投影到一个50,000维的向量。为什么是50,000？因为这是词汇表的大小，这本质上帮助我们选择下一个token。

这就是一个标记在整个LLM架构中的完整生命周期。在今天的讲座中，我们将重点讨论整个架构中的一个方面，那就是多头注意力机制。我希望你们能理解多头注意力机制在其中的作用。在所有这些步骤中，令牌会经过多头注意力机制，而多头注意力机制出现在Transformer块中。在这个特定的情况下，它还在归一化层之后出现。今天，我们将理解多头注意力机制，但在此之前，我们首先要理解什么是注意力本身，以及为什么需要注意力机制。我们为什么要开始讨论“注意力”这个术语？为什么最近它变得如此流行？因此，我们今天的目标是激发自注意力这个概念。

我们今天不会讨论自注意力机制的数学原理。今天的全部目标在于尝试理解为什么我们首先需要注意力机制，以及为什么它会对大型语言模型产生如此革命性的影响。请思考这一点：在所有标记（token）经历的步骤中，最重要的步骤就存在于Transformer模块内部——具体就存在于Transformer模块的某个环节，那就是注意力机制。

这就是为什么我在这里用不同的颜色标记出来。这部分内容本质上展示了让大语言模型（LLM）如此擅长理解语言的所有特性。为了真正理解注意力机制的工作原理，我专门为你们准备了这节单独的课程，我们将尝试阐述引入注意力机制的必要性。

在理解我们为何需要注意力机制以及它如何改变这一领域之前，让我们先深入探讨一下生成式人工智能本身。因此，让我们稍微回顾一下历史。我认为理解注意力机制的本质至关重要。20世纪60年代有一个叫ELISA的聊天机器人。你可以想象一下。我不会称它为大语言模型。

我会称它为聊天机器人。这是第一个旨在充当治疗师的自然语言处理聊天机器人。所以如果你问“请告诉我是什么困扰着你”，我可以说我在学习人工智能方面遇到了困难。你能帮我吗？你觉得学习人工智能很困难是正常的吗？你看它其实并不是很有帮助，但别忘了那是20世纪60年代。在当时，这已经被认为是一场革命了。所以在这里你可以看到，这个原始程序是由约瑟夫·维森鲍姆在1966年描述的。

与此相比，看看现在的ChatGPT，我们说我想学习人工智能。你能帮我吗？然后让我把模型切换到GPT 4.0，这样我能得到更快的回复。接着你会看到，我立刻获得了一系列可操作的项目清单，可以立即开始实施来学习人工智能。因此，在短短64年的时间里，我们在自然语言处理和理解语言本身这一领域已经取得了长足的进步。

让我们来看看从20世纪60年代到2020年代发生了什么。20世纪60年代是ELISA聊天机器人问世的时期。然后在20世纪80年代和1997年，出现了生成式人工智能或语言建模领域的两个重要基础构件，你可以把它们看作是20世纪80年代出现的循环神经网络和1997年问世的长短期记忆网络。

这两者都建立在20世纪70年代神经网络热潮的基础上。人们发现神经网络可以做很多了不起的事情，但神经网络真正的问题在于，它们本质上无法处理记忆，这是神经网络最大的缺陷。为什么需要处理记忆呢？因为想象一下，如果你要预测下一个单词是否正确，或者生成一些新文本，你会想知道段落的开头写了什么。

仅仅记住一点信息是不够的。上下文非常重要，这个词在今天的讲座中会频繁出现。上下文本质上意味着，如果给模型提供了一大段文字，而我们希望模型执行某项任务或仅通过查看句子来回答某个问题，那么仅靠句子中紧邻的词语是无济于事的。

我们需要在脑海中记住段落开头所讲述的内容。比如说，我来自印度浦那，然后接着写了一大堆不同的事情。我写了一大堆句子，最后问我说的是什么语言。现在要根据这个问题生成一个回答，中间所有这些内容并不太重要，但我需要了解这段开头的内容，因为这才能真正帮助我回答这个问题。这就是为什么需要具备记忆能力，而神经网络原本并没有真正编码记忆的机制，但循环神经网络和长短时记忆网络解决了这个问题。让我快速展示一下它们是如何解决这个问题的。

因此，人们最初通常使用RNN或LSTM进行序列到序列建模，也就是所谓的序列到序列翻译任务。简单的序列到序列翻译任务可以只是语言翻译。如果你想处理一组单词，并将它们从一种语言翻译成另一种语言，你可以使用循环神经网络。

所以循环神经网络有一堆隐藏状态，比如h0、h1、h2、h3。假设这些是输入单词"I will eat"，我想把它翻译成法语，即"Je vais manger"，已经写在这里了。我可以用循环神经网络进行这种翻译的方式是，我有一个编码器块和一个解码器块。

编码器和解码器块都有所谓的隐藏状态，你可以将其视为向量或矩阵。这里是编码器的隐藏状态h1、隐藏状态h2、隐藏状态h3，然后是解码器的隐藏状态s1、隐藏状态s2和隐藏状态s3。让我们可视化编码器块中发生的情况。

在编码器模块中，首先输入句子的第一个单词"I"，它会转换为第一个隐藏状态h1；接着输入第二个单词。需要注意的是，为了得到第二个隐藏状态h2，你不仅要使用第二个输入，还要利用前一个隐藏状态。这就是循环神经网络捕捉记忆的方式。

前一个隐藏状态用于计算当前隐藏状态。同理，当你来吃饭时，那是输入x3，但要计算最终的隐藏状态h3，你需要使用前一个隐藏状态h2，而h2编码了h1或者说h2包含了h1的信息。因此，h3现在既包含了h1的信息，也包含了h2的信息。

h3拥有对过去的认知，它保留着记忆。因此，h3可以被视为一个向量，可能是500维或1000维的向量，它本质上包含了输入的所有信息，代表着输入的含义。这是唯一一个传递给解码器的高维向量。解码器随后会查看这个向量，解码器的每一个隐藏状态都会开始对其进行解码。

所以解码器就像接力赛跑。想象一下，接力棒在h1手中，它传给h2，h2再传给h3。现在h3拿到了接力棒，并将其传递给解码器。现在我是解码器的第一个隐藏状态。我拿到了接力棒。我先解码第一个标记JOR，然后传递给第二个解码器，接着解码第二个标记，最后解码第三个标记。这就是循环神经网络中语言到语言翻译的工作原理。

LSTM（长短期记忆网络）在预测当前隐藏状态时与前代隐藏状态的处理方式有所不同。它们还维护一种称为细胞状态的结构。因此，LSTM既能考虑长期记忆，也能兼顾短期记忆。所以这本质上是为了扩大它们处理的上下文窗口。但架构从根本上保持不变。只是对于LSTMs来说，底层的数学运算要复杂得多。

现在你看出问题所在了吗？让我们看看这个例子，我们之前用它来讨论上下文的问题。那么，当处理大段文本时，循环神经网络存在什么问题呢？比如说，你有一段很长的文本，你想翻译整段内容。循环神经网络在处理这个问题时存在什么缺陷？让我们通过一个简单的例子来更清楚地说明这个问题。想象一下你手上有这样一段文字（这是我随机从ChatGPT生成的）。

我希望你按照以下步骤操作：首先，花大约30秒到一分钟或更长的时间，完整地阅读这一段文字。然后闭上眼睛，将它翻译成你选择的任何语言。你可以把它翻译成印地语、西班牙语、法语、德语，或者除英语以外的任何你会的语言。所以，闭上眼睛，把它翻译成另一种语言。你能做到吗？这不可能，对吧？因为当你闭上眼睛时，你基本上只能记住你读过的一些概要。

你并不需要记住每一个确切的单词，但为了翻译，我们不得不逐字逐句地翻译，对吧？这正是循环神经网络所面临的问题。为什么呢？因为我们仅依赖一个隐藏状态或一个向量来捕捉过去发生的所有上下文信息。

这就好比闭上眼睛后试图回忆整段文字。所有的记忆压力都集中在这个隐藏状态或向量上，如果是一大段文字，如何能将所有信息压缩到一个500维或1000维的向量中？自然会有一些信息丢失，而且解码器并不会从第一个或第二个隐藏状态获取输入。

它仅将最终的隐藏状态作为输入。这就是主要问题所在。从编码器传递到解码器的上下文仅来自最终的隐藏状态。这就是为什么这被称为上下文瓶颈问题。最终的隐藏状态就像你闭上眼睛试图记住整个段落时一样，这是不可能做到的。你不能对一个向量施加这么大的压力。

循环神经网络中的上下文瓶颈确实不利于保留长距离上下文，这为我们理解注意力机制为何出现提供了一个很好的途径。现在，如果你观察循环神经网络，让我在这里擦除一些内容。你认为这个问题的理想解决方案是什么？假设这就是我们目前遇到的问题——所有上下文信息无法被压缩到单个向量中。

这个问题的解决方案是什么？嗯，解决方案似乎是这样的：比如说，当我们在解码时，当我在解码这个时，假设我在从解码器的第一个隐藏状态进行解码时，与其只能访问最终的隐藏状态，不如让我能够访问编码器的所有隐藏状态以及所有输入。因此，如果在解码过程中我可以访问所有输入，那么我就可以进行预测，比如如果我在解码第一个标记，我应该更重视第一个隐藏状态，而不应该重视第二个隐藏状态，也不应该重视第三个隐藏状态。如果我能创建一个机制，能够理解需要给予不同标记的相对重要性，那会怎样呢？

这是本讲座最重要的部分，请大家集中注意力。解决上下文瓶颈问题的关键在于：当我在解码时，能否开发一种机制来量化每个隐藏状态的重要性。例如，在进行首次解码时，假设我给第一个隐藏状态分配80%的重要性，第二个和第三个隐藏状态各分配10%。

如果我掌握了这些信息，上下文瓶颈问题就迎刃而解了。因为即便段落很长，只要我能关注到段落中的所有标记，我实际上也能重视之前出现的内容。比如说这是句子，而我要在这里预测某个内容。在注意力机制中，我可以尝试判断应该给予第一个标记、第二个标记、第三个标记乃至整个段落多少注意力，然后我就能确定需要对这些标记给予最大的关注。

所以你看，我已经开始在这里使用“注意力”这个词了。你可以把它看作是段落中各个标记的相对重要性。因此，理想的解决方案是我们在解码过程中需要选择性地访问输入序列的某些部分。因此，当你在解码时，假设我想做出某些预测，比如如果我在解码第一个标记，那么alpha 1 1就是赋予第一个隐藏状态的相对重要性，alpha 2 1是赋予第二个隐藏状态的相对重要性，alpha 3 1则是赋予第三个隐藏状态的相对重要性。我希望能够做出这样的预测：alpha 1 1为100%，alpha 2 1为0，alpha 3 1也为0，类似这样。但你可以看到，现在我不再仅仅依赖于h3，而是依赖于之前所有的隐藏状态，不仅如此，我还在量化应该对每一个之前的隐藏状态依赖多少。

因此，这一点很重要：我们需要在解码过程中有选择地访问输入序列的部分内容。我想通过这个例子再次向你们解释这一点。假设我在这里截了一张图，然后把它放在这里。让我们试着理解这句话的实际含义。如果我让你翻译这一段，你会怎么做？你可能会从——比如说这里开始。假设你从这里开始，你能一次看到多少个词？假设这是你的上下文窗口，你一次无法看到比这更多的词。那么你的大脑会怎么做呢？对于你的大脑来说，段落中其余的内容就无关紧要了。

所以你会屏蔽掉脑海中所有无关的信息，屏蔽掉那些不相关的内容。你要做的就是忽略其他所有不重要的事物，只关注当前的语境，并选择性地隐藏其他一切。你的大脑会集中注意力——这一点非常重要——它会专注于段落的这一部分，然后开始仅翻译这一部分。翻译完这部分后，你的大脑会移动到接下来的内容，可能是接下来的五个单词。翻译完这五个单词后，你的大脑会继续移动到下五个单词，依此类推。

所以你看，在特定时刻你在这里所做的，是你的大脑正在做出一个决定：当你在看第一个上下文窗口时，我想要对这些标记给予最大的关注。然后你的大脑也在做出一些定量的判断，完全不去关注其余的标记，完全不关注这些。这就是在解码过程中选择性访问输入序列部分的真正含义，对吧？选择性访问输入序列的部分，这是理解注意力机制最重要的句子。它意味着只关注在那一刻重要的输入序列的那部分，这看起来是一个很直观的做法。

没错，但这恰恰是帮助语言模型变得更强大的最关键因素。想象一下，你要在一段文字中找出拼写错误，你会怎么做？你需要逐字逐句检查，先关注第一个词，然后是下一个词，依此类推。这正是语言模型也需要做的——它们需要选择性地关注序列中的特定部分，比如找出含有拼写错误的那个词。这就是解码过程中需要注意力机制的核心原因：我们可以量化每个输入词元应该获得多少重要性或注意力，这就是我现在要阐述的重点。

现在，真正实现这一点的第一篇论文并不是《Attention Is All You Need》。如果你看这篇论文，你会发现它有大约117万次引用，引用量非常大。所以很多人实际上认为注意力机制是在这篇论文中首次提出的。但真正首次引入注意力机制的论文是这篇《Bahdanau Attention Mechanism》。如果你搜索“Bahdanau Attention”并点击这篇论文，你会发现这是第一篇真正将注意力机制应用于翻译任务的论文。这篇论文我认为是在2014年发表的，并在2015年的ICLR会议上发表。

他们的主要目的是实现了一种架构，让我们可以有选择性地决定对每个隐藏状态投入多少注意力——这就是所谓的a1、a2、a3注意力分数。基于这个机制，就能完成翻译任务。他们通过实验证明：使用注意力机制后，序列到序列的翻译效果能得到显著提升。请务必记住这篇论文的结论——现在大家可以看到，这张图的x轴是英语句子，y轴是法语翻译，对角线上的色块正显示出英语单词与对应法语译词之间的注意力权重分布。

而最酷的地方在于，顺序并不相同。比如《欧洲经济区协定》中的"欧洲经济区"——英文是"European Economic Area"，但翻译成法语就变成了"Economic European Area"。你看，它并不是逐字直译的。然而，注意力机制却能精准识别出：法语里的"European"对应英语里的"European"，尽管它们出现的位置不同——英语版出现在第五个词，而法语版出现在第七个词。

所以所有这些最亮的区域并不都在对角线上，有些偏离了对角线本身，甚至这些也被注意力机制捕捉到了。这张图最酷的地方就在于这些非对角线元素，因为翻译并不总是逐字进行的，对吧？有些语言中某些词会先出现，而在其他语言中则后出现，这取决于名词、词汇等的排列方式。这就是这篇论文的亮点所在，这是第一篇将注意力机制用于翻译任务的论文。

然后，《Attention Is All You Need》这篇论文提出了Transformer模块，并将注意力机制整合到其中——这正是该论文的核心优势或独特之处。关键在于，虽然RNN和LSTM在语言翻译领域表现不错，但它们存在上下文瓶颈问题。于是研究人员发现，构建深度神经网络其实并不需要RNN架构。2014年Bahdanau注意力机制的诞生就是个转折点，当时他们还在使用RNN结构。

所以他们保留了RNN（循环神经网络），同时还引入了注意力机制。实际上，在Bahdanau注意力模型中，你可以用类似的方式看待编码器-解码器模块，只不过他们在三年后（即2017年）又加入了这种注意力机制。这意味着研究人员最终发现，在构建用于自然语言处理的深度神经网络时，甚至根本不需要RNN架构，于是他们提出了Transformer架构。让我们梳理一下这个领域的发展脉络吧：1980年代是RNN的时代，1997年出现了LSTM（其实1966年就有ELIZA了，我把这个也写下来），2014年引入了注意力机制——但当时仍依附于RNN的编码器-解码器架构。直到2017年，研究人员才彻底认识到自然语言处理任务根本不需要RNN。

因此，RNN被淘汰了，基本上只保留了注意力机制，但它与Transformer模块相结合，这就是2014年论文和2017年论文之间的主要区别。在2014年的论文中，RNN模块仍然存在，但后来被移除，并在2017年被Transformer模块或Transformer架构所取代。这最终形成了完整的架构——现在我们有了Transformer模块，其中包含了注意力机制。这就是GPT架构，并不是原始Transformer论文中的架构。原始Transformer论文中既有编码器也有解码器，而我展示的这个架构则不同。

目前仅有一个解码器，请不要对此感到困惑。今天讲座的主要目的是让大家从自然语言处理的历史视角理解为何需要引入注意力机制。关键原因可以用一句话概括：我们需要在解码过程中有选择性地访问输入序列的各个部分。最初注意力机制是在RNN中提出的，后来研究人员发现可以去掉RNN结构，尝试将注意力机制与其他模块结合，最终在2017年将其整合到了Transformer模块中。

然后在2018年，GPT架构问世了，所以它是注意力机制加上GPT。GPT基于最初的Transformer架构，但它不像原版那样同时包含编码器和解码器模块，而是只保留了解码器模块，就像你在这里看到的。它还保留了注意力机制，我们之前讨论过为什么需要注意力机制，这一点在LLM架构的这一部分得到了体现。

现在我们将开始探讨，如果你观察下一个标记预测任务，自注意力究竟是什么，上下文向量又是什么，以及注意力机制的主要目的是什么。那么，让我们来看看自注意力机制在下一个标记预测中的主要目的是什么。既然我们已经理解了注意力机制及其历史，现在让我们尝试理解“自注意力”这个术语。自注意力实际上是什么意思呢？自注意力指的是一种机制，它允许输入序列中的每个位置关注同一序列中的所有其他位置。

这意味着，如果我有一个句子，比如“明天是晴天”，自注意力机制本质上指的是，到目前为止，如果你看RNN，我们看到的是解码器需要给予编码器的注意力。所以如果第一个解码的词是法语词，我们基本上是在看两个不同的序列之间的关联。假设英语序列是这样的，比如“I will eat”，法语序列是这样的，这是法语序列，我们在看如果你在做第一次解码时，你应该给予英语序列多少注意力。

因此，这里的注意力机制是作用于序列之间的，而非同一序列内部。而自注意力机制则用于预测下一个标记（token），这正是大语言模型（LLM）的典型运作方式——由于LLM的核心任务是预测后续标记，它们并非专门针对翻译任务进行训练。在预测下一个标记的场景下，本质上并不存在不同语言的区分，你处理的只是一堆数据。

因此，我们不再关注两个序列之间的注意力，而是采用同一个句子。比如说，当你观察下一个词时，你会试图找出：如果关注某一个标记或单词，我应该对句子中所有其他标记给予多少注意力。这才是这里最关键的理解点。如果你主要关注某一个标记或单词，那么周围的单词对这个特定标记有多重要？为什么这种认知对我们至关重要？你能试着思考一下，为什么我们需要编码那些本质上围绕某个给定标记的其他标记的信息吗？

这些知识对我们之所以重要，是因为当你预测下一个词时，本质上需要关于序列上下文的信息，需要了解不同单词之间如何相互关联。再以同样的例子来说，假设我说我来自印度浦那，我说，假设这是句子，如果你看“说”这个词，我需要知道当我看到“说”时，最大注意力应该放在“浦那”和“印度”上，也许其他所有词都没那么相关，因为我所说的方言深受我来自的地区影响。

因此，当你观察一个词云时，如果你的Transformer架构或LLM引擎掌握了某个词与周围其他词的关系信息，并且知道需要给予周围不同词多少关注度，那么仅凭一个词元就能非常轻松地预测下一个词元。

这就是为什么自注意力机制变得如此重要。如果没有自注意力机制，你就会丢失关于其他标记与我们选择的特定标记之间关联的上下文信息。我希望现在你能明白为什么它被称为"自注意力"。在这个例子中，当我们处理序列到序列的语言翻译时，注意力机制作用于不同序列之间；而自注意力则是当我们观察单个句子本身时，关注句子内部的各个标记，并从根本上理解这些标记之间是如何相互关联的。

让我们再次以同一个例子来说明，第二天是晴朗的。记住，当这些标记进入Transformer架构时，它们现在是以向量的形式存在，正如你之前所见，它们现在具有统一的维度。记住，每个标记都有一个统一的768维向量，这就是输入嵌入。因此，每当我在这里展示这些块时，本质上它们代表的是一个向量。

因此，第二天是明亮的，现在这些都是向量了。对于Transformer来说，它并不理解单词，也不理解句子，它只知道每个标记都是一个向量。所以，"the"是一个向量，我称之为x1；"next"是一个向量，我称之为x2；"day"是一个向量，我称之为x3；"is"是一个向量，我称之为x4；"bright"是一个向量，我称之为x5。

现在，如果我正在看一个特定的词，比如我之前在这里展示的“next”，我想看看当我注视这个词“next”时，我应该给予其他所有标记多少注意力。这个注意力由α2,1给出，或者我称之为符号α2,1。为什么是2,1呢？因为“next”是第二个标记，而我想找出第二个标记和第一个标记之间的注意力分数，这就是α2,1。这个会是α2,2，这个会是α2,3，这里会是α2,4，这里会是α2,5。

我本质上想找出当我关注某个特定标记时所有的注意力分数，因此这被称为查询（query），也就是我当前关注的标记，即查询标记（query token）。我想知道，当我关注这个查询时，应该给其他所有标记分配多少注意力，这些标记有时在通用术语中也被称为键（keys）。最终，我想做的是利用这些注意力分数，以某种方式整合所有这些信息，将这个向量从输入嵌入向量转换为上下文向量。

现在我要强调一个非常重要的区别：目前"next"是一个输入嵌入向量，对吧？所以它包含词元嵌入加上位置嵌入。但上下文向量是完全不同的概念。如果这是"next"的输入嵌入向量，而我在同一空间中绘制上下文向量——这就是"next"的上下文嵌入向量——你会发现上下文向量实际上比词元嵌入向量丰富得多。为什么呢？因为词元嵌入向量或输入嵌入向量不包含相邻单词的信息，而现在我的上下文向量包含了邻居单词的信息，这些信息现在已经被"烘焙"进了我的输入嵌入中。

所以如果你有一个输入嵌入，如果你有输入嵌入向量，也就是我之前提到的统一向量，如果你用关于邻居的上下文来增强这些输入嵌入向量，我们会看到这种增强是如何完成的，但本质上这会导致一种被称为上下文向量的东西，所以注意力机制或自注意力机制的整个目标是将所有输入嵌入向量转换为上下文向量，所有这些统一向量，我们之前看到的这些统一向量，所有标记都有一个768维的统一向量，

当它们从归一化层出来并进入多头注意力层时，进入注意力层的是一个输入嵌入向量，而从注意力层出来的是一个上下文向量。因此，在离开注意力块后，我们得到的输出会更加丰富，这就是为什么我用不同的颜色标记它。之所以更丰富，是因为现在它还编码了其他标记的信息，从而保留了上下文。因此，上下文向量是一个经过丰富的嵌入向量，它结合了所有其他输入元素的信息。因此，在自注意力机制中，上下文向量起着非常关键的作用。

它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住这一点：输入嵌入向量仅包含有关该单词或标记的信息，它可能编码了该单词的含义及其位置的信息，但对邻近元素一无所知。而上下文向量则能感知邻近元素，因为邻近元素至关重要。当你观察一个句子或一个段落时，单个标记本身毫无意义，正是它们与邻近元素的关联才构成了该段落的上下文。这就是为什么在大型语言模型（LLMs）中需要这种机制。

需要理解句子中词语之间的关系和相关性，实际上这正是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个至关重要的转折点——距今正好十年——当注意力机制被提出后，人们开始意识到：与其孤立地看待词语，不如退后一步观察词语之间究竟如何相互关联。这样一来，我们就能充分挖掘文本的最大价值。因为就像图像由像素图案构成一样，语言的意义也源自词语之间的有机联系。

只有当你把所有单词放在一起看，并理解它们之间的关系时，文本或段落才有意义。所以现在的问题是，你有一个输入嵌入向量，比如说“next”，如何将其转换为上下文向量？你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频一会儿，思考一下。你有输入嵌入向量，假设你还有这些注意力分数，你将如何修改输入嵌入向量，以便以某种方式考虑这些注意力分数，从而得到一个上下文向量？

所以你可以在这里稍作停顿，首先你也可以试着思考一下这些注意力分数本身是如何计算的。好的，最简单的做法是，假设你有这个向量，还有所有其他向量，我们为什么不简单地做一个点积呢？比如你有“next”的输入嵌入向量，也有“though”的输入嵌入向量，只需在这两个向量之间做点积，就能得到α21；再对“next”和“next”做点积，就能得到α22；然后对“next”和“day”做点积，就能得到α23。

然后计算next和is的点积，这将得到alpha2_4；接着计算next和bright的点积，这将得到alpha2_5。一旦你获得了所有这些alpha值，就可以简单地计算alpha2_1乘以x1加上alpha2_2乘以x2加上alpha2_3乘以x3加上alpha2_4乘以x4加上alpha2_5乘以x5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你在这里有一个向量v1，而这里有另一个向量v2，它们之间的点积会比比如说v1和v3之间的点积更高。

因此，如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的内容。你可能会想，我想要量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，它们当然应该有一个更高的注意力分数。因此，这个计算对我来说是有道理的：我只需计算点积，然后用点积来缩放不同的向量并将它们相加。无论这个和是多少，它现在就是“next”的上下文向量。同样，我也可以为所有其他标记找到上下文向量。

这种方法有什么问题，为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们想要编码的上下文。我希望你从基本原理出发来思考，我很快就会揭晓答案。好了，主要答案是，假设我们考虑这个句子：“狗追球但没抓住”，狗追球但没抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。如果“它”现在是我的查询向量，

你是如何决定计算查询向量与其他向量之间的注意力分数的，你决定采用点积对吧，这正是我要做的。如果这是我的查询向量，要计算它与“狗”之间的注意力分数，我只需对它和“狗”进行点积运算，结果是0.51；如果我对它和“球”进行简单的点积运算，结果也是0.51。你看这里的问题：两个注意力分数完全相同，但这并不是我想要的。当你说“但它没能抓住”时，实际上指的是“球”对吧——狗追球但没抓住，所以这里的“它”指的是球而不是狗。因此当我观察时，需要更关注“球”而非“狗”。让我用不同颜色的笔写下来以便更清晰——

我在观察时需要更加关注球本身而非狗，但实际情况并非如此。如果采用简单的点积运算，就无法体现"球应比狗获得更高优先级"这一信息——当我们提及二者时，狗和球的注意力分数不应相同。这个例子绝妙地证明了为何需要选择性关注不同标记：狗追逐了球但没能接住它。第一个"它"指代的是狗（若作为查询标记会更多关注狗），而现在这个"它"作为查询标记则应更多关注球。

我不希望两者具有相同的注意力分数，因此简单的点积无法区分此处微妙的上下文关系——它没有考虑"追逐未能抓住"的语境，也无法处理语言上的细微差别（例如"抓住"更可能指代移动的物体即球）。核心问题在于：简单点积仅能衡量语义相似度，却无法应对上下文问题。很多句子都可能存在这类复杂的上下文关系，对吧？我需要设计一种机制来捕捉这些复杂性，但又不确定具体该采用什么机制。于是我们采用了研究人员沿用多年的技巧：

如果你不知道事物之间的本质关系是什么，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。这正是注意力领域所发生的事情。研究人员基本上无法弄清楚这个机制可能是什么，这就是机器学习领域或深度学习领域与物理学不同的地方。在物理学中，如果你遇到这个问题，你会花六个月到一年的时间试图为底层机制制定一个定律，以捕捉复杂性或捕捉上下文的底层机制。

但在深度学习领域，你并不这样做。你会说，我要用一堆矩阵来替代它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？于是他们发明了新的矩阵，比如所谓的查询矩阵和键矩阵。这意味着，与其仅仅看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。所以，如果我的查询在这里是正确的，我的查询是“它”，我就会将其与所谓的查询矩阵相乘。这个矩阵可以是一个高维矩阵，用于“狗”。

所以“狗”和“球”就是关键（keys）对吧？因为本质上，关键（keys）就是查询时你要寻找的所有其他标记——在这里就是“狗”和“球”。你把它们都乘以一个关键矩阵（keys matrix）。现在看这里的优势：如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵有什么特定含义，你只需随机初始化它们，然后通过反向传播来训练它们。这是研究人员长期使用的深度学习技巧——如果你自己无法理清关系，就退一步让神经网络来完成这项工作。与其用某些规则限制神经网络，不如让它自己找到规律。现在你看到了优势：我们掌握了多个可训练因素的控制权。

假设WQ是3×3矩阵，WK也是3×3矩阵对吧？比如"狗"、"球"这些就是我的键值。这些就是我们之前看到的输入嵌入向量。现在我要用查询向量乘以这些输入嵌入，再用这两个结果乘以键值矩阵。具体来说就是3×3矩阵乘以3×1向量，结果会得到3×1向量。这样键值就会变成0.9、0.2、0.1和0.1、1.8、0.1——你看这些数值都发生了变化。

因为我用矩阵与它们相乘，而查询也是如此，所以它将与查询矩阵相乘，使得查询结果变为0.5、0.1和0.5、1.0以及0.1等等。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正从输入嵌入空间转移到另一个空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的分数。

所以现在如果你计算它与球之间的注意力分数，你会发现它是0.56，而它与球之间的注意力分数是0.96。如果你计算它与狗之间的注意力分数，结果是0.56。这里你可以看到，它与球之间的注意力分数是0.96，高于它与狗之间的注意力分数0.56，因此这些注意力分数明显不同。添加这些可训练的矩阵实际上对我们有帮助，为什么有帮助呢？因为它提供了许多可调整的参数。

这样我们就可以编码标记之间的一些复杂关系。如果你进行简单的点积运算，注意力分数会是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，这会在下一节课讲到），本质上，如果你有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多的参数可以使用。如果你在这部分感到困惑，让我再重复一遍：我们开始这一节时认为，如果你有一个输入嵌入向量

那么，如何从输入嵌入向量得到上下文向量呢？要得到上下文向量，本质上需要先计算出注意力权重α。得到α之后，只需将它们与输入嵌入向量相乘，就能得到上下文向量。但问题在于，如何计算一个输入嵌入向量与另一个输入嵌入向量之间的α值，也就是如何得到注意力分数。最简单的方法可能是计算点积。比如，假设这是句子中的查询词"it"，要计算"it"与"ball"之间的注意力分数，我先计算"it"和"ball"的点积，结果是0.51；再计算"it"和"dog"的点积，结果也是0.51。

因此注意力分数看起来相似，但这并不是我想要的。因为当我说"it"时，我希望它指向的是"ball"，所以我希望"it"和"ball"之间的注意力分数要远高于"it"和"dog"之间的分数。那么现在该怎么办呢？显然点积运算的复杂度不足以捕捉这些上下文关系，我需要更多可调参数来处理——虽然目前我还不知道具体需要什么样的调节旋钮，但可以让神经网络或反向传播算法来找出这些旋钮应该是什么。至少现在可以先随机初始化这些参数，而这就是这些新术语的用武之地——我想要引入新的可训练矩阵。

让我把这个称为查询矩阵，并通过将其与查询矩阵相乘，将输入嵌入转换到另一个空间。而对于键（即狗和球）的输入嵌入，它们将与键矩阵相乘，同样转换到另一个空间。然后，我将在这些转换后的向量之间计算注意力分数。如果我的模型能正确学习这些矩阵的参数，就能让模型学会它与球之间的注意力分数是0.96，高于它与狗之间的分数0.56。不用担心这些乘法或数学运算。

现在我将在下一节课中详细讲解数学部分，目前只需记住：我们尚不清楚如何从物理层面捕捉上下文关系，因此这更像是一种简便方法或技巧。你引入了查询（queries）和键（keys）——这些随机初始化的可训练矩阵，通过训练来优化它们。你可能听说过"查询"和"键"这些术语，实际上它们被引入并没有确切的物理依据，唯一原因在于人类尚未找到直接计算注意力分数的方法，当前我们掌握的途径只有——

好的，如果我们无法解决这个问题，让我尝试将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数来处理，然后希望训练过程本身能够自行解决。这个技巧在计算机视觉领域也被人类使用过，例如当你训练一个卷积神经网络（CNN）来区分狗和猫时，你无法自己写下所有特征，而是依赖卷积神经网络来完成。这里的情况有点类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，还有另一个称为值矩阵的矩阵，以及它在我们将在下一讲中看到的下一个令牌预测任务中是如何使用的。

所以下一讲将全面探讨自注意力机制的数学原理：查询矩阵、键矩阵和值矩阵的具体运算，如何通过数学方法计算上下文向量，以及如何通过这些向量最终实现开放预测。下节课我们将深入剖析刚刚演示的这个模块，将其扩展成完整的数学推导课程。不过在今天的课堂上，我主要想先帮大家建立对查询-键-值概念的基本认知——具体数学实现我们留待下节课详解。

好的，今天的讲座就到这里结束了。今天的讲座内容可以看作是注意力机制的发展历程与自注意力机制的入门介绍。作为总结，请大家记住注意力机制的演变过程：首先是Elisa，在当时是一项革命性的突破，考虑到它诞生于1966年，这已经非常了不起了；随后出现了循环神经网络和LSTM。

他们遇到了上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了一种称为注意力机制的机制，它计算解码器输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力加RNN的结合。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

于是他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心就是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并构建了这种以注意力机制为核心的新架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们谈论自注意力时，实际上我们只关注一个序列，因为它将被用于下一个令牌预测任务。因此，在像GPT这样的下一个令牌预测任务中...

我们使用自注意力机制来观察一个标记（token）及其与周围或邻近标记的关联方式。这里，被观察的标记称为查询（query），其他标记称为键（keys）。我们的目标是计算查询向量与键之间的注意力分数。我们认识到，注意力机制的核心目的就是获取这些注意力分数，并将其转化为上下文向量。上下文向量是对输入嵌入向量的一种更丰富、更具信息量的表示。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但第一次没抓住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵，因此需要增加参数数量。

这样我们就有不同的参数可以调整，这些可训练的矩阵被称为查询权重矩阵和键权重矩阵。还有一个值权重矩阵，我们将在下一节课中看到。所有输入嵌入的输入嵌入都与查询权重矩阵相乘，得到查询矩阵，同样我们也有键矩阵。因此，注意力分数不是在向量的输入嵌入之间找到的，而是在查询和键之间找到的。

由于我们拥有众多可灵活调整的参数，我们希望在进行参数训练时，模型能够学会让第二个"it"与"ball"之间的注意力分数高于第二个"it"与"dog"之间的注意力分数。这样一来，模型就能捕捉到更多上下文复杂性。通过添加这些可训练的权重矩阵，模型能够更好地把握上下文中的复杂关系。

这就是为什么我们人类添加了这些权重矩阵，然后称它们为查询、键和值，因为这听起来很酷，而且与信息领域相关。但如果你深入观察，我们自己无法找出这种注意力机制的规则，点积方法失效了。所以我们无法自己弄清楚如何得到这些注意力分数，如何计算它们。于是我们转向神经网络来替我们完成这项工作。

好的。非常感谢大家。在下一讲中，我们将深入探讨自注意力机制背后的数学原理。在接下来的讲座中，我们将探讨多头注意力机制，只有到那时——也就是介绍完多头注意力节点之后——我们才能真正理解键值缓存的概念。让我看看这部分内容在哪里...对，只有到那时我们才能真正准备好理解键值缓存，这个概念正好引出多头潜在注意力机制（MLA）的讲解。

This series is going to be a bit deep, but I'm trying to make the lectures as long as possible so that I don't miss out anything. This is for serious learners. So please make notes as you are watching this series, and it will be incredibly useful for you.

Thanks a lot, everyone. And I look forward to seeing you in the next lecture. Hello, everyone. 

My name is Dr. Raj Dhandekar. I graduated with a PhD in machine learning from MIT in 2022, and I'm the creator of the Build Deep Seek From Scratch series. Before we get started, I want to introduce all of you to our sponsor and our partner for this series, NVIDIA AI. 

All of you know how much we value foundational content, building AI models from the nuts and bolts. NVIDIA AI follows a very similar principle and philosophy to that of us. Let me show you how. 

So here's the website of NVIDIA AI. With a small engineering team, they have built an incredible product in which you can create high quality AI videos from just text prompts. So as you can see here, I've mentioned a text prompt, create a hyper realistic video commercial of a premium luxury watch and make it cinematic. 

With that, I click on generate a video. Within some time, I'm presented with this incredible video, which is highly realistic. What fascinates me about this video is its attention to detail. 

Look at this, the quality and the texture is just incredible. And all of this has been created from a single text prompt. That's the power of NVIDIA's product. 

The backbone behind the awesome video which you just saw is NVIDIA AI's video creation pipeline in which they are rethinking video generation and editing from the first principles. To experiment and tinker with foundational models, they have one of the largest clusters of H100s and H200s in India and are also experimenting with B200s. NVIDIA AI is the fastest growing AI startup in India building for the world and that's why I resonate with them so much. 

The good news is that they have multiple job openings at the moment. You can join their amazing team. I am posting more details in the description below. 

Hello everyone and welcome to this lecture in the Build Deep Seek from Scratch series. Today, we have a very important topic to cover and that is understanding self-attention with trainable weights. So first, let's do a quick recap of what our plan is and what we have been doing so far. 

So the main aim of this lecture series is to explain how Deep Seek was built and the innovations which power Deep Seek. We have divided this innovation into four phases. The phase 1 is with respect to the architecture, phase 2 is with respect to training methodology, phase 3 is with respect to the GPU optimization tricks which Deep Seek has implemented and phase 4 is with respect to their model ecosystem itself.

We have been looking at phase 1 for the past two lectures and we have been building up towards understanding the multi-head latent attention mechanism. So our plan for understanding the multi-head latent attention is that we are going to do this sequentially. We looked at the LLMs and in the previous lecture, we looked at the need for why attention needed to be introduced and in today's lecture, our main goal is to see how the attention weights, the attention scores are calculated. 

So let me just take you quickly through the previous lecture. In the previous lecture, we saw that the main aim of the self-attention mechanism is to take in input embedding vectors and convert them into something known as context vectors. Remember that context vectors are much richer than input embeddings. 

Input embedding vectors encode information about the meaning of a word and the position in the sequence but it contains no information about how that word relates to other words in the sequence. Context vector on the other hand contains that information. Context vector contains information of the meaning of the word, its position and also how it relates to the other tokens in the sequence.

So to get the context vector from the input embedding vector, we saw that intuitively the first thing we can do is take a dot product but that does not work very well because the dot product is inherently limited and if there is a sentence such as the dog chased the ball but it could not catch it and if this it is my query, I need my attention mechanism to differentiate that this it actually relates to the ball and not the dog. So if we just use a simple dot product, it does not, it cannot capture this distinction. So what do we do? We introduce some trainable weight matrices. 

The role of this trainable weight matrices is that okay we cannot encode the complex relationship in the attention mechanism so why not we leave it to weight matrices which can be trained. So this is exactly what is done instead of directly having the input embeddings, what we essentially do is that instead of taking the dot product between the input embeddings, we project the input embeddings into different spaces, into different vector spaces. So for example if you are looking at a particular query, we will multiply it by the query weight matrix and that will be the query vector. 

So for example in this example which we just saw, the dog chased the ball but it could not catch it. If the second it is a query, instead of directly taking the input embedding vector for this it, we multiply it with the trainable query weight matrix so that it becomes a query vector and the other words with whom we want to see how this it relates to or how much attention we need to pay to other tokens, they are called as keys and even these vectors we do not take directly as input embedding vectors, we multiply them with something which is called as the keys weight matrix right and then we get the key vectors for dog and the ball and then what we do is that instead of earlier what we did is we had the input embedding for it, we had the input embedding for dog and we had the input embedding for let's say ball but now what we do is we project this into the query vector. So now we have a query vector for it which looks like this, we have a key vector for ball and we have a key vector for dog and now we take the dot product between the queries and the keys, that's how we get how much attention needs to be paid to each key for a particular query. 

So for this query it, how much attention should we pay to ball, you just take the dot product between the query vector and the key vector for ball. If the query is it, how much attention needs to be paid to the word dog, we just take the dot product between it vector between the query vector for it and the key vector for dog right. So in the previous lecture I just wanted to introduce to you that instead of just taking a simple dot product we have to work with trainable weight matrices and today we learn more about these three trainable weight matrices which is the queries, keys and values. 

If I introduce these queries, keys and values to you directly you would have been confused regarding why do we need these trainable weight matrices in the first place, it seems a bit odd but remember that as humans we have the limitation that we cannot come up with the attention mechanism formula, there is no formula like physics. So we offload the difficult thing to a neural network or to trainable weight matrices and we realize that when you offload it to matrices or we project the input embeddings to higher or different dimensions through weight matrices we are able to get good answers right. So we just call these matrices as query, key and value because they just make more sense to use in common literature but actually we are kind of doing a lazy trick here right. 

Since we cannot figure out the physics ourselves we think that let me have some weight matrices which I'll initialize randomly at the start and then I hope that after training is completed I will learn something regarding the attention which needs to be paid to different tokens and that's exactly what happens. So today we are actually going to see the step-by-step procedure of how we go from input embedding vector to the context vectors. So we are going to see a step-by-step procedure for let's say if you have certain input embedding vectors how exactly we do we take these input embedding vectors and how do we convert these input embedding vectors into context vectors.

So let's begin our journey right now my meaning today is to show you some visuals and intersperse it with bit of a code so that you can visualize the calculation process entirely yourself right. So let's get started I am going to look at this sequence the next day is bright okay and remember now before going to the transformer architecture or before going to the attention mechanism these are just input embeddings which are token embeddings plus position embeddings. So remember our previous lecture where we saw that essentially before going to the transformer architecture every token gets a uniform and that uniform is the summation of the token embedding plus position embedding which is called as the input embedding. 

So this input embedding is actually what I have shown over here right so for the input embedding is an eight-dimensional vector for next the input embedding is an eight-dimensional vector for day the input embedding is an eight-dimensional vector for is the input embedding is an eight-dimensional vector and for bright the input embedding is an eight-dimensional vector. Now I am going to show you how these input embedding vectors are transformed into context vectors but I hope you understand why we need to transform them into context vectors right because as such right now if I look at any of these embedding vectors for example day it carries no information about how much importance needs to be given to next or the or is or bright that information is completely lost I want to integrate that information and that's why I want to convert the input embedding vectors into context vectors alright so there are three trainable weight matrices which we are going to introduce the first trainable weight matrix is called as the query weight matrix which is denoted by WQ this is the query weight matrix the second trainable weight matrix is the keys weight matrix which is denoted by WK and the third trainable weight matrix is the values weight matrix which is denoted by WV right now so now it shouldn't come as a surprise to you that where are these matrices coming from remember these matrices are coming because we want to transform the input embeddings into a different space so that our expressivity increases and we can capture underlying complexities which cannot be done through a simple dot product remember all of these trainable weight matrices which I have mentioned here the values here I do not know these values at the start they are initialized randomly the hope is that when we backpropagate through the entire LLM architecture these values update themselves so whenever I show these matrices I do not fix them at the start I just initialize them randomly now let's pay careful attention to dimensions a bit the input so this matrix right here is called as the input embedding matrix this is called as the input embedding matrix and take a look at the dimensions of this matrix this matrix has essentially five rows why does it have five rows because I have five tokens the next day is bright so it has five rows and it has eight columns why does it have eight columns because the input embedding dimension for every word I have chosen it to be eight over here you can choose it to be any dimension but I will just chosen it to be eight for the sake of simplicity now this is the dimensions of the input embedding matrix now let's take a look at the dimensions of the query the key and the value weight matrix right if you take a closer look you will see that the number of rows of these matrices have to be same as the number of columns of the input embedding matrix so the number of rows so WQ WK and WV then if you think of them as rows and columns the dimensions the number of rows of WQ WK and WV have to be equal to the input embedding dimension that's fixed because we are going to be taking the multiplication so that's going to be eight in this case but the number of columns can essentially be anything right so now when GPT 2 GPT 3 etc their architecture is coded out the number of columns here this is also called as the output dimension so this is called as the output dimension whereas here we also call it the input dimension or D in now the output dimension can be different as well as same with respect to the input dimension and for GPT 2 GPT 3 etc it's usually the same so if it's the same I'll have D out also equal to 8 but here for the sake of simplicity I have just chosen D out to be equal to 4 but remember that this value is generally taken to be similar to the D in which is the input dimension but here I want to show you that it can really be anything because the product can still happen even if the column value is different right so then my query weight matrix my keys weight matrix and my values weight matrix all are matrices whose dimensions are 8 by 4 the first step which I will do is I will multiply my input embedding matrix with the query matrix and this will give me query vectors so take a look at the dimensions of 5 by 8 matrix multiplied by an 8 by 4 matrix 5 by 8 multiplied by 8 by 4 this will result in a 5 by 4 matrix right so these are my query vectors that's a 5 by 4 matrix these are my key vectors these are that's also a 5 by 4 matrix and these are my value vectors that's also 5 by 4 matrix the way to interpret this is that every row here corresponds to the token so the first row is the first token the second is the second token next third is day fourth is is and the fifth is bright but now remember that from 8 dimensions we have gone to a 4 dimensional output space so the space in which we are operating is now different after this point we no longer look at the input embedding vectors all we look at is the query vectors the key vectors and the values vectors this is what I was trying to motivate you at the start of the lecture that if we directly deal with input embedding vectors and take the dot product it's limited right it does not work that's why we project them into different spaces and this trick has been done in deep learning in many different fields right if if a linear classifier is not working on a data you augment it with features you project it into a higher dimensional space if handwritten features for computer vision are not working you use a convolutional neural network which discovers features on its own in a higher dimensional space so taking stuff from the input dimension and putting it into a different dimension is what we have been doing in deep learning for a long time and this is exactly what we are doing here it's just that the names are a bit fancier the query key and the value and at the end of this lecture I'll tell you why these names come into picture but for now just remember that the query vectors is 5 by 4 matrix the keys is 5 by 4 and the value is 5 by 4 the way to interpret it is that every row belongs to one token the next day is bright for the query key and even the values and the number of columns is equal to the output dimension which is 4 in this case so I hope all of you are with me until this point where we have the queries vectors the keys and the values right so here I have just taken some values some actual numerical values where the inputs is 5 by 8 input embedding matrix I multiplied it with the query matrix the key and the value which are 8 by 4 and here I get the queries matrix which is 5 by 4 the keys the keys the query vectors here which is 5 by 4 the key vectors 5 by 4 and the value vectors which are 5 by 4 so here the left hand side is the visual without any numerical values and the right hand side is I've just plugged in some numerical values here so that you have a visual representation as well as the mathematical representation side by side next so this was step number one where we get the query vectors the keys vectors and the value vectors and now we'll operate in the Q K V space instead of the input embedding space Q K V is query key value so let's go next the next step is basically we have to find the attention scores and this is basically taking a dot product right so I have my query now my query vectors which is 5 by 4 and I have the keys where keys matrix now which is also 5 by 4 so if I have to take a dot product between the query vectors and the key vectors I cannot directly multiply these matrices because the number of columns in the first and the number of rows in the second won't align so I'll have to take a transpose so I'll have to take a transpose of the keys matrix so the query vectors stay 5 by 4 which was what was here but now here I have the keys transpose which is a 4 by 5 right so now the way this works is that you multiply a 5 by 4 matrix with a 4 by 5 and then you get an attention scores matrix which is a 5 by 5 matrix now this is a very important step because here we actually find out how much one query relates to the other tokens basically so now let me say the next day is bright right and let me focus on the second row here which is the next and let's say I want to find out the attention scores between next and the other tokens right so let's say I want to find out how much next relates to the next day is bright I want to find out the attention scores here so I want to find out what is that tension score between next and the what's the attention score between next and next what's the attention score between next and day what's the attention score between next and is and what's the attention score between next and bright so to get the attention score between let's say next and the we'll take the query for this query vector for next which is this row and multiply it with the keys transpose of the which is the first column over here so the first column over here is the so when i multiply the road the row of the next with the column of the i'll get the attention score between next and the so let me call it alpha 2 1 so this value over here so let's look at the second row over here which corresponds to the attention scores for next this first value over here is alpha 2 1 now which is the dot product between this next row vector and the column vector for the now if i want to find the attention score between next and next i take a dot product between this row again and the second column which is next and this dot product will give me alpha 2 2 which is the second value over here if i want to find that so that's alpha 2 2 if i want to find the attention score between next and day i take a dot product between this row again this remains fixed and the third row here which is now corresponding to day and then that will give me the third value here which is alpha 2 3 if i want to find the attention score between next and is so here this is alpha 2 3 next and is will be alpha 2 4 so that's the dot product between this next row and the fourth column over here so that will give me alpha 2 4 which is over here and then if i want to find the attention score between next and bright that's essentially alpha 2 5 and that's the dot product between the second row next and the last column which is bright and this dot product will essentially give me alpha 2 5 so now this entire second row represents the attention score between next and all the other keys which is the next day is bright similarly now if you see the fifth row the fifth row represents the attention between bright and all the other tokens the next day is bright that's the way to interpret this attention score matrix every row of the attention score matrix denotes the attention between that query which corresponds to dot that row and all the other keys so if you look at the fourth row for example it denotes the attention score between the fourth query which is is and all the other tokens which is the next day is bright so to get this row you fix the query row and then you multiply it with all the columns in the keys transpose and that's how you get each value in the row so every time you think of the attention scores matrix try to visualize how to interpret every single row of that matrix and that's when you will truly understand or you will never forget how the attention scores matrix is calculated so if the queries looks like this which is 5 by 4 and the keys transpose is 4 by 5 so here again i am showing the mathematical calculations when you multiply the queries with the keys transpose you will get attention scores matrix which looks something like this so now if you look at the second row the second row is essentially if next is the query 0.1 corresponds to the attention between next and the 1.8 is the attention between next and next 0.6 is the attention between next and day 0.1 is the attention between next and is and 0.1 is the attention between next and right so that's the way you want to interpret the attention scores matrix right okay now what there is one problem with this attention scores matrix and that problem is that when i'm looking at let's say next i want to make statements such as when i'm looking at next as the query give 50 attention to the give 20 attention to next give 10 attention to day give 10 attention to is and give 0 attention to bright etc so i want to make interpretable statements like this so let me repeat what i mean when i look at next i want to make statements like give 10 attention to let's say the give 20 attention to next give 20 attention to day give 30 give 30 attention to is and give 20 attention to bright so i want to make statements like this so that now when i look at these values i can immediately say that the maximum attention between all of this is given to next and is next and is so essentially what i want to do is that i want all of these values to sum up to 100 in terms of probability i want all of these probabilities to sum up to one so that i can just look at this pie chart right i can just look at this pie chart the next day is bright and based on this pie chart i can see how much attention needs to be paid to each token whereas if you see these values right now let's see yeah if you see these values right now for the second row that is the attention score for next you will see that these values don't really sum up to one which means that i cannot make statements like give 10 attention to the first token 18 attention to second token it will not work like that the rows the values in the rows here do not sum up to one and that is the main problem so the next step which is step number three is to make sure that the attention scores are converted into something which is called as attention weights and to go from attention scores to attention weights we are going to apply the softmax operation softmax essentially means that let's say we take a look at this row and let me write it down in column format right now so then in column format this will become 0.1 1.8 0.6 0.1 and 0.1 what i want to do is i want to somehow convert all of these values so that they lie between 0 to 1 and they also sum up to 1 so the softmax operation essentially what it does is that if this is x1 x2 x3 x4 and x5 softmax what it does it replaces x1 with e raised to x1 divided by summation it replaces x2 with e raised to x2 divided by summation it replaces x3 with e raised to x3 divided by summation e raised to x4 divided by summation and e raised to x5 divided by summation. Now what exactly is summation? Summation is just e raised to x1 plus e raised to x2 plus e raised to x3 plus e raised to x4 plus e raised to x5.

Now if you take a look at all of these five values, you will see that if you add them the numerator will be e raised to x1 plus e raised to x2 plus e raised to x3 plus e raised to x4 plus e raised to x5 that is equal to summation which is the denominator. So all of these will definitely add up to 1 and they will lie between 0 to 1. Softmax also has the additional important property that it gives a lot of weightage to very high values and very low weightage to very low values. This makes the classification very easy.

So essentially that's the softmax operation which we are going to implement. But the main problem here is that I told you right softmax gives very high attention to values which are very high and it does not pay that much attention to values which are very low and that's a big problem for us. So let's say if the attention, let's say if this if the attention scores are something like this okay.

Now you see this value is very high right. If you apply softmax to this the way softmax will work is that it will put 0.95 or something to this value and make sure that all the other values are actually very low and if these are our attention weights that's not very good for us because then we will pay a lot of attention to one key and will not pay attention to all the other keys at all. So that's why there is a scaling which needs to be performed before we apply the softmax.

Before we apply the softmax we need to make sure that all of these values are divided by some value and only then we will apply the softmax. So now let me talk about that part.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

Let's go to this part, yeah so here I want to first of all explain the issues with softmax right so what I am doing is that let's say these are my attention score values and then I apply softmax to these values right so we can see that softmax for all of these values is almost equally similar in terms of range that's good for us but now what I want to do is that I want to multiply all of these values with 8 and then let us apply softmax so if all of these values are multiplied by 8 which means that some values will be very large and some values will not be that will not be that high right so if I multiply all of these values by 8 and then I apply softmax you will see that softmax places a lot of importance on this value which is 0.8 and it gives negligible importance to some other values see that is what happens when the values are very large before applying softmax and that is what I have mentioned over here the softmax function is sensitive to the magnitude of its inputs when the inputs are very large the differences between exponential values of each input become much more pronounced and this causes softmax output to become peaky what peaky output means is that it gives a lot of importance to some values and very low importance to others so in attention mechanism this is not very good because it if it's a sharp softmax distribution then the model becomes very confident in one particular key so now you see the model becomes very confident in this key and it will give very low confidence to the other keys that leads to very unstable training when we look at the transformer architecture later and we do not want that so that is why we need to scale that is why we need to scale this this this vector before we apply softmax so the value with which this vector is actually scaled is square root of the keys dimension right so the keys dimension here is in this case it's 5 by 4 right so the output dimension is equal to 5 so it's scaled by square the output dimension is equal to 4 sorry so it's scaled by a square root of 4 and now you must be thinking that why is it scaled by square root of 4 why just why don't we just scale it with the keys dimension or dimension raised to 2 or something like that why only square root of the keys dimension what's so special about the square root so the main idea here is that the reason we scale with the square root of the keys dimension is because of a concept of variance so if you if you take if let's say this is my queries this is my queries vector which is a six dimensional vector and this is my keys transpose which is a six dimensional vector this is my keys transpose okay and if I multiply these vectors I will get some values right but remember in the multiplication what is happening is that each value is getting multiplied and then summed up now what usually happens is that if there are two random vectors right if there are two random vectors whose dimensions are 6 and if I sample hundred such random vectors so I'm going to do something like this I I take hundred vectors of the queries and I take hundred vectors of the keys transpose and then what I do is that I collect the values of queries multiplied by keep keys transpose for all of these hundred what happens is that the variance of this product so now I can take these hundred values and I can compute their variance right how the distribution is the variance of this product actually scales with the keys dimension so as has been mentioned here the variance of this product scales by the square root of the keys dimension actually scales with square root of the keys dimension which means that as the keys dimensions goes on increasing a lot or rather as the dimensions of keys transpose here let's say if that increases the variance of this product increases a lot which means that since the queries and the keys transpose are initialized randomly these are random vectors right their product can be usually varying from some very high values to some very low values and we want to avoid that as much as possible we want to make sure that this product variance is equal to one so that the product does not wildly oscillate the queries and the keys are defined randomly at the start right and the dimensions can be very high so if the dimension is very high and if the variance actually scales with the square root of this dimension then that's not very good for us because it will also make the learning unstable and to illustrate this concept further I want to explain it to you with the case of dice right so if you are rolling one dice let's say and it just has one to six numbers right the average of this is let's say 3.5 and the variance is relatively small that is let's say 2.9 there are predictable outcomes but now what I do is that I roll I roll the dice and then I sum the output of hundred I roll and some dice basically so if you roll a dice hundred times and you will sum the output what happens here is that if summation is involved the mean is around 350 but the variance grows significantly to around 290 so the output becomes unpredictable and the dot product without normalization so if we if we just take the queries multiplied by the keys transpose and if we do not divide it by the square root of the keys dimension we will see that increasing the number of dimensions is like rolling more dice and summing the results why because dot product essentially is where we sum up the product right where we sum up if the dimension is 100 we sum up 100 entries of the queries multiplied by keys transpose so increasing the number of dimensions is like rolling more dice and summing the number of results each dimension contributes some variance and as the dimension grows variance accumulates and so here I am trying to say that as the query and the keys dimension grows the variance increases a lot and the dot products before softmax become either very large or it becomes very small because the variance is large and that makes the attention weights unstable and so the training procedure also becomes unstable when we divide by square root of D it scales down the variance of this dot product and brings it equal to 1 and that stabilizes the expected outcomes the attention weights become more stable and they become more predictable I have actually explained this using the code below so here what we will do is that we will take thousand trials okay and in each trial we will generate so first we will do a five-dimensional query and the key vector and then we will do a hundred dimensional query and the key vector right so let's say that I mentioned is equal to 5 we will generate queries and key vectors of dimension 5 and we will do that thousand times we will compute the dot product between the query and the key for each trial and then in one case we will divide by the square root of the dimension and in the other case we will just take the dot product like that without without doing the scaling and we will collect all the results of the thousand trials and then we will find the variance we will find the variance before scaling and we will find the variance after scaling so remember we are taking thousand query vectors thousand key vectors we are taking the dot product and in one case we are scaling it in one case we are not scaling it and then we are finding the variance before scaling and after scaling so if you run this you will see that for the dimension of 5 variance before scaling is equal to 5 for the dimension of 100 variance before scaling is actually hundred so the variance actually directly grows with dimension the variance is directly proportional to the dimension if you see but after scaling the good thing is that which happens is after you scale with the square root of the dimension the variance almost becomes equal to one that's very cool right so now here it's clearly proved that if you divide the product if you divide the dot product of the queries and the keys with the square root of the keys dimension then what you ultimately get is that after scaling whether the dimension is 5 or whether the dimension is 100 the variance of the product is constrained this means that the queries multiplies multiplied by keys transpose the values won't blow up to very high values or very low values the variance will remain equal to 1 that will lead to very stable training procedure so that's why what we do is that in step number 3 the attention scores are scaled by square root of the keys dimension which means that we take the attention scores and we divide by square root of keys dimension keys dimension essentially is every key vector what's the dimension of it in this case it's equal to 4 right so we divide by square root of 4 and then we apply softmax so after this division is done after this division is done by square root of the keys dimension we apply softmax essentially we apply this function and when we apply the softmax attention scores are converted into what is called as attention weights so that's the key difference here between attention scores and attention weights attention scores are not normalized and attention weights are normalized so if you look at every row over here you will see that every row essentially sums up to 1 and now we can make the quantitative statements or the qualitative statements which I was mentioning right this second row corresponds to next so now I can say that we can pay 10% attention between next and though we can pay 50% attention between next and next we can pay 20% attention to next and day we can pay 20% attention to next and is and we can pay 20 10 so 10% attention to next studies and we can pay 10% attention to next and bright so that is how I can make qualitative statements now because all the elements of each row essentially sum up to one that's the difference between attention weights and attention scores attention weights are normalized they sum up to one whereas attention scores are not normalized and now what we'll do is that we'll come to step number four in this step we actually compute the context vectors from the attention weights let's see how that is done so until now we have the attention weights that's again a 5 by 5 matrix and when you see this matrix again try to visualize what every row represents so the second row here represents if next is the query how much attention should I give to the next day is bright and all of these values will now sum up to 1 because they are normalized in the last step what we do is that remember until now we have not used the values vectors at all in the last step these values vectors come into the picture where what we do is that the attention weight matrix which is a 5 by 5 matrix is simply multiplied with the values vector so the values vector is 5 by 4 right and that gives us the context vector the context vector has 5 rows and it has 4 columns the first row corresponds to the second row corresponds to next third row corresponds to day is and bright the next day is bright and now you can see that initially we started with input embedding vectors for the next day is bright and after all these steps step number one step number two step number three and step number four we have got the context vectors for each of these tokens now now remember these context vectors are much more richer than the input embedding vector predominantly because their calculation involves the attention weights so I've just shown this calculation here mathematically the attention weights it's a 5 by 5 matrix which has been shown over here the values is a 5 by 4 matrix and so the context vector matrix is a 5 by 4 matrix so for example the second row corresponds to the context vector of next the last row over here corresponds to the context vector for a bright so every context vector is a four dimensional vector so here I just want to have a small section on the intuition behind the context vector calculation so the way the context vector is actually calculated is as follows right let's say if you want to find the context vector for next we have found the attention weights right between next and all the other tokens so we have found that when you have next we should pay 10% attention to the we should pay 50% attention to next we should pay 20% attention to be we should pay 10% attention to is and we should pay 10% attention to bright we have understood this based on these attention weight values sorry these attention weights and we have the value vector so you can think of the value vector essentially as an input embedding vector itself but it's transformed to a higher dimension or a different dimensional space so now if we want to find the context vector for next what we essentially do is that we know that next gives 10% attention to the so the so this is the vector values vector for the so we'll scale this vector with 10% or we'll multiply this with 0.1 then we know that 0.5 which is 50% attention to next so I have written these attention weights here 0.1 0.5 0.2 0.1 0.1 so we give 0.1 importance to the first row which is the we give 0.5 importance to the second row which is next we give 20% importance to the third row which is day so we multiply the third row with 0.2 we multiply the fourth row with 0.1 and we multiply the fifth row with 0.1 and all of these multiplications are then added together so we scale the first vector with 0.1 we scale the second vector with 0.5 we scale the third vector with 0.2 we scale the fourth vector with 0.1 and you scale the fifth vector with 0.1 and then add all these scaled vectors together this is how we get the ultimate context vector essentially after all this addition that's how we get the context vector for next and to represent it visually it looks something like this the next day is bright if you see the blue vectors they are the input embedding vectors to get the context vector we scale each vector by the amount of importance it needs to be paid so the right that's 10% importance so we scale it with 0.1 next is 50% importance so we scale it with 0.5 day is 20% importance so we scale it with 0.2 is is 10% importance so we scale it with 0.1 and bright is 10% importance to scale it with 0.1 all of these scaled vectors are shown by this green and then all these green vectors are added together and that gives me the context vector for next so in this one figure you can see how the context vector differs from the input embedding vector right the input embedding vector for next only contains information about the meaning of that token but the context vector for next now contains information of all these attention weights also and all this weight weighted sum ultimately gives the context vector so this is how the attention scores are added together to get the context vector itself and this is how the context vector differs from the input embedding vector now through this same visual you can try to see how to get the context vector for all the other tokens also if you have to get the context vector for bright you just take a look at these attention scores you multiply the first row with 0.1 you multiply the second row with 0.05 you multiply the third row with 0.1 you multiply the fourth row with 0.25 and you multiply the fifth row with 0.5 and then you add all these together so that will give you the context vector for bright which is essentially this this last row over here that's how you get the context vector from the attention weight matrix and the value matrix and to understand the visualization behind the context vector calculation look at this diagram in this one diagram you can see how the attention weights are used as scaling factors and then the weighted sum is essentially taken to convert us or to take us from the input embedding space to the context vector space so this is the step number four which is essentially getting the context vector matrix and then that's it in step number five I will just summarize this below we have the input embedding matrix we have the input embedding matrix and then we have the self attention layer so whenever you see this self attention layer right now it means all of these steps which have been implemented over here step number one is multiplication with the WQ WK and WV to get the query vectors the keys vectors and value vectors step number two is multiplying the queries with the keys transpose to get the attention scores step number three is scaling the attention scores with square root of the keys dimension and applying the softmax to get the attention weights step number four is multiplying the attention weights with the values matrix to get the context vector matrix that's it all these four steps essentially are involved in the self attention module which ultimately takes the input embedding matrix and converts it into a context vector matrix that's it that's the whole process which is going on in the attention mechanism or in the self attention mechanism and this process powers the transformer block which is at the core of why language models work so well so now if you take a look at our lecture where we had seen the different components of the transformer block this multi head attention is where all the magic happens right we have the input embedding vectors as an input and the context vectors as output and now you know how the context vectors are calculated from the input embedding vectors why this is why is this called multi head attention we'll see that in the next lecture but for now I hope you have understood the mechanism behind the self attention behind self attention and I have just written a small code over here to demonstrate this to you so let's call this a self attention class so we initialize the query weight matrix the key weight matrix and the value weight matrix initially we initialize them randomly and we set bias equal to false because we just have to multiply the input embedding matrix with these and then in the forward pass what we do is we get the keys vectors the query vectors and the value vectors by multiplying the input embedding matrix with the trainable key matrix trainable query matrix and trainable value matrix that's essentially step number one which we looked at then we come to step number two where we get the attention scores by multiplying the queries with the keys transpose right then we come to step number three in step number three we essentially first divide by the square root of keys dimension and then we apply softmax and then finally we come to step number four which is multiplying the attention weights with the value matrix to get the context vector matrix that's it so I could have directly explained to you this code because it just 10 to 11 lines of code but to understand these multiplications it's very important for us to write it down on a whiteboard so that we can visualize the dimensions so this is how the context vector weight matrix is calculated you can run this block I'll share this code also and then you can take in any input so here I am taking an input your journey starts with one step and input D in is equal to 3 over here the input embedding dimension and I am assuming the output embedding dimension equal to 2 and let me just run this so here we can see that the input embedding time input embedding matrix is ultimately converted into the context vector matrix when we pass it through this self attention class that's it is it's as simple as that to retain the dimensions of what we had in the current code what we can actually do is that we can make sure the input is 8 dimensions so let's let's take it let's make the code similar to what we had here so the next day is bright right let's say this is the next day is the next day is bright and let's say now this is not needed okay and now we in the code or in the whiteboard we have seen that there is 8 dimensions for every every vector so let me just copy paste it to make 8 dimensions and then add some random values here and let me just add this the same thing here for the sake of simplicity now so I'm making everything 8 dimensions here so that it matches what we had on the whiteboard okay and D in is now equal to 8 but my D out which I taken over here was equal to 4 so I'll just take D out equal to 4 and then let me run this block so here you see it runs almost immediately and then I have the context vector matrix let's check the dimensions the dimensions here of the context vector matrix is that we have five rows essentially and four columns this is exactly what we had seen over here the context vector or the context weight context vector matrix is five rows and four columns that's exactly what we have implemented in the code right now in the next class what we will see is that we will implement something which is called as causal attention in the causal attention we will hide out the future attention weights which are not which are not needed or which are not available to the current token so that we only look at the past tokens before predicting the next well so that's called causal attention we'll see that in the next lecture and then we'll move to multi head attention I know that these lectures are a bit long and I'm repeating some elements with respect to attention or transformer blocks causal attention context vectors etc but I believe it's extremely crucial for us to understand this if we don't understand this the learning of multi head latent attention will not be very strong I want all of us to be on the same page when we start the multi head latent attention part so all of you really need to understand the nuts and bolts of self attention and multi head attention and that's why I'm making these lectures in so much more detail I'll share the code files with you and I hope that as you're going through these lectures you also make notes so write this down on a piece of paper make sure that you familiarize yourself with the calculations later we'll go to multi head attention and later we go to multi head latent attention so remember that it's a multi-step journey it's not easy this is not going to be a 30 minute course it's going to be a course of 35 to 40 videos and it will be a very very detailed course I intend to make it like a university lecture right so I could have directly started with latent attention but that would be very difficult I think to directly understand and I want to make these lectures as useful for an audience which is seeing this series for the first time also so I hope you are enjoying this series and I look forward to seeing you in the next lecture thank you hello everyone my name is dr. Raj Dhandekar I graduated with a PhD in machine learning from MIT in 2022 and I am the creator of the build deepseek from scratch series before we get started I want to introduce all of you to our sponsor and our partner for this series in video AI all of you know how much we value foundational content building AI models from the nuts and bolts in video AI follows a very similar principle and philosophy to that of us let me show you how so here's the website of in video AI with a small engineering team they have built an incredible product in which you can create high quality AI videos from just text prompts so as you can see here I have mentioned a text prompt create a hyper realistic video commercial of a premium luxury watch and make it cinematic with that I click on generate a video within some time I am presented with this incredible video which is highly realistic what fascinates me about this video is its attention to detail look at this the quality and the texture is just incredible and all of this has been created from a single text prompt that's the power of in videos product the backbone behind the awesome video which you just saw is in video as video creation pipeline in which they are rethinking video generation and editing from the first principles to experiment and tinker with foundational models they have one of the largest clusters of h100s and h200s in India and are also experimenting with b200s in video AI is the fastest growing AI startup in India building for the world and that's why I resonate with them so much the good news is that they have multiple job openings at the moment you can join their amazing team I am posting more details in the description below hello everyone and welcome to this lecture in the build deepseep from scratch series today we are going to proceed ahead in our journey of understanding the attention mechanism and briefly speaking years are planned eventually we know that we want to understand multi head latent attention which is one of the key components of the deep seek architecture but to do that we need to proceed in a step-by-step manner so initially we started out with understanding self-attention we finished this in the previous lecture so if you remember in the previous lecture we saw exactly the step-by-step procedure of how the self-attention mechanism operated we started with the query key and value trainable weight matrices we multiplied the input embedding matrix with these and got the query key and the value vectors then we multiplied the query matrix with the keys transpose to get the attention scores we then scaled the attention scores with the square root of dimension applied softmax and we got the attention weights and finally we multiplied the attention weights with the values matrix and we got the context vector matrix the way to interpret the context vector matrix is that essentially every row of the context vector matrix corresponds to the context vector for that particular token so if you see the input embedding matrix every row corresponds to the input embedding vector for the next day is bright in this example similarly in the context vector matrix the first row corresponds to the context vector for the the second row corresponds to the context vector for next etc so the whole aim of self-attention is to take the input embedding vector and convert it into a context vector and why does it do that because the context vector is much more richer than the input embedding vector the input embedding vector just contains information about the semantics of the word but it does not contain any information about how that word is related to all the other words around it on the other hand the context vector contains information of not just the semantics but it contains information of how much importance needs to be paid to all the other words or tokens surrounding it and that's what makes the context vector much more richer in the representation than the input embedding vector so today what we are going to do is we are going to look at something which is called as causal attention we have already looked at self-attention and the next step along our journey to ultimately understand multihit latent attention is

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

As the name suggests causal implies that so one thing causes another right. So we only need to look at those things which are responsible for producing the next token and I'll clarify what this means but causal attention is a central building block of understanding multi head attention and then eventually understanding key value cache and then ultimately understanding multi head latent attention. So let's begin our journey of understanding causal attention.

Before we get started with the lecture I want to just take a quick, I want to explain a quick summary of how exactly the next token prediction is done so that I can explain or set the context for causal attention in a much better manner. So let's say we have the Harry Potter's first book right and that's a part of the data set which we are using to pre-train the large language model and so for the sake of simplicity let me just take a screenshot of the first page over here and let me bring the screenshot onto my mirror board. So I have brought the screenshot of this first page over here and now let me show you how the input and the target pairs are actually constructed to be fed to the transformer architecture.

So here's how that is done. First we determine something called as the context size. So if the context size is equal to 4 we create a sequence of 4 tokens.

So this is my first input and then I also determine something called as a stride which means how much I have to skip before I create the next input. So this is my first input, this is my second input, this is my third input. So similarly what I'll do is that I will divide the entire data set into chunks of 4 words or 4 tokens right and then based on my batch size I will have a matrix which is created so that's let's see the input batch 1. So if the batch size is equal to 6 or if the batch size is equal to 8 then the first batch will have let's say the token IDs corresponding to Mr. and Mrs. Dudley that's the first row.

So let's say this is 1, 15, 18, 22. The second row will be the token IDs corresponding to these 4 tokens. So that will be something like 3, 4, 5, 6. Similarly I will have 8 rows like this because the batch size is equal to 8 right and why do I have 4 columns? The reason I have 4 columns is because the context size is equal to 4. So that's how the input batch is created.

So this is my first input batch. The way the output is created is that every input is essentially shifted to the right hand side by 1. So this is my first output, this is my second output, this is my third output etc. So once you get the input batch, getting the output batch is very easy you just shift the input to the right hand side by 1. So this will be something like 15, 18, 22, 3 etc.

So that's my output batch 1. Now if you look at the first, if you look at the first row here and the first row here that's my first input output prediction pair. So for example let's see. So the input will be Mr. and Mrs. Dursley that's the input.

So this corresponds to 1, 15, 18 and 22 and then the output is the input shifted to the right by 1. So then this will be and Mrs. Dursley off that's the output. Now if you see within this one pair of inputs and outputs there are actually four prediction tasks. The first prediction task is when Mr. is the input and is the output.

The second prediction task is when Mr. and and is the input, Mrs. is the output. The third prediction task is when Mr. and Mrs. is the input then Dursley is the output and the fourth prediction task is when Mr. and Mrs. Dursley is the input then off is the output. So if you see every row here although it looks like we have four tokens right so there might be just one prediction task which is predicting the next token but that's not the case if the input is Mr. and Mrs. Dursley and the output is and Mrs. Dursley off if the context size is 4 there are four input and target prediction tasks or input output prediction tasks which I have written over here right now.

The thing which I want you to focus is that whenever you are predicting certain output so for example when Mrs. is the output which is to be predicted the input is Mr. and and when Dursley is the output which is to be predicted the input is Mr. and Mrs. So let's take a look at the data set now and let me clean this further to explain this point to you. So what I am trying to say is that when Dursley is the output let's say Dursley is the output the input is always the tokens which only come before Dursley. Similarly you take any output you take Mrs. as the output if Mrs. is the output if Mrs. is the output the input is Mr. and and if you see the input is Mr. and and so for every output so wherever you go in this so let's say if you go in this you let's say if this is the input and then the output is shifted to the right by one now within this there are four input output prediction tasks right if you see was was the output you will see that Mrs. and Dursley was the input if thin is the output then was Dursley Mrs. is the input etc.

So based on the context size wherever whenever you have an output the first conclusion from today's lecture is that the input is only the tokens which come before the output right so essentially what I mean to say is that if you have if you have the input as oops if you have the input as let's say Mr. and if you have the input as Mr. and and to predict the next token which is Mrs. you do not have access to all the future tokens which come after Mrs. and this seems like a trivial point but that is the whole premise of causal attention the whole premise of causal attention is that to predict the output we only have access to the tokens before the output so we cannot cheat and we cannot look into the future so the simplest way to explain causal attention is that you cannot cheat we cannot cheat and we cannot we cannot look into the future the reason I explained the beginning of this lecture with this Harry Potter example is so that you feel interested in this topic and then you don't forget what causal attention is essentially what causal attention is simply is that for every output the input is only the tokens which come before it so you check any input and output pair in the in these four input output pairs which I have the for every output the input on input is only those words or tokens which come before the output okay and now let's start understanding causal attention in a more formal manner right so causal attention is also sometimes known as masked attention and it's a special form of self attention right it restricts the model to only consider previous and the current inputs in a sequence when processing in any given token this is what we saw to predict any output it restricts the model to only consider previous and the current inputs in the sequence this is in contrast to the self attention mechanism which allows access to the entire input sequence at once and let us see this in action actually so if you remember what we did with the queries keys and the value vectors we had an input and the input had tokens such as let's say your journey starts with one step and here I have shown that every token is a three-dimensional vector we will multiply this with the trainable query matrix trainable keys matrix and trainable values matrix and ultimately we will have the queries the keys and the values now let's see how we compute the attention score right the attention score is the multiplication of queries multiplied by the keys transpose so let's look at this scores in more detail so if I have my queries vector if I have my queries vector to be 6 by 2 and if I have my keys vector to be let's say 6 by 2 to get the attention score what what I actually do is I do queries multiplied by keys transpose right so it will be a 6 6 by 2 it will be a 6 by 2 multiplied by it will be a 2 by 6 so this will ultimately result in a 6 by 6 vector or a 6 by 6 matrix and the tokens which I am looking at for now are your journey starts with one step right so let's see your journey starts with one step and then your my keys are your journey starts with one step right and basically the 6 by 6 matrix indicates that I have values for every single entry here so so if I am looking at journey and that's my query I am basically finding the attention scores between journey and all the other words even before and after journey right so this is the attention score between journey and your this is the attention score between journey and journey this is the attention score between journey and starts this is the attention score between journey and with this is the attention score between journey and one and this is the attention score between journey and step now this can be represented in a visual manner like this your journey starts with one step and if journey is my query vector I am essentially finding the attention of journey with all the other tokens no matter whether the token comes before journey or after journey now pay very careful attention here if journey is my query right so if your and journey are essentially my inputs to predict the output journey will not have access to the tokens which come after it at all so when journey is my query vector if we are looking at causal attention ideally we should find the attention scores only between your and journey all the other tokens which come after this point do not matter too much to us at all because they are anyway not going to influence the calculation of the next token let me explain this point once more essentially what we do in self attention mechanism is that we get these attention scores right so every token for every token we find its attention with the tokens which come before it and which come after it but the key point to realize is that for every token we do not have access to the tokens which come after it at any point in the prediction process so take a look at the Harry Potter example right every token for every token we don't have access to the words which come after it we cannot look into the future and since we cannot look into the future there is no use finding the attention scores between a token and the tokens which come after that point so ideally if you have a six by six matrix such as this so let's say this is one two three four five six one two three four five six two three so these are three rows and let me actually copy this and paste it let me just do it again here so these are three rows this is my fourth row over here this is my fifth row over here and this is my sixth row these are the attention scores right this is a six by six so if we cannot look into the future for the first token we only need to find the attention score between the first token and itself so that's this for the second token journey we only need to find the attention score between your and journey for the third token we only need to find the attention score between your journey starts for the fourth token we only need to find the attention scores between your journey starts with for the fifth token we need to find the attention between one and your journey starts with one so all the tokens which come before one and for the sixth we can essentially find all the attention scores between step and all the tokens which come before it so now if you see all these attention scores which are above the diagonal which are marked over here all of those are not needed at all all of these attention scores are not needed because we anyway won't look into the future at any time point remember why did we get the attention scores we got the attention scores between a token and the other token so that that will help us get from the input embedding vector to the context vector but in self attention we kind of looked into the future right because we for one token we even found the attention score between that token and tokens which come after that point and so the value vector which we ultimately so the context vector is a multiplication of the values vector and the attention scores matrix right so the context vector for every token essentially had information about the future and that's not that's not what actually happens when we predict the next token when we predict the next token we are not going to have information about the future at all so ideally when we get the context vectors we should prevent access to this future information completely so all the elements above this diagonal should be should be essentially set to zero because we don't have access for one token we only have access to the tokens before or equal to that current token this is the main intuition behind causal attention so this is in contrast to the self attention mechanism which allows access to the entire input sequence at once now while computing the attention scores the causal attention mechanism ensures that the model only factors tokens that occur at or before the current token in the sequence this is what we saw so for every token we should only consider the tokens which occur at that point or the ones which occur before that current token to get the attention scores to achieve this for each token process we mask out the future tokens which come after the current token in the input text this is the most important sentence that's why causal attention is also sometimes called as mask retention so if you look at bi-directional attention which is self attention every token essentially attends to all the other tokens right so we get information of the attention scores which come before and after a current token but in uni unidirectional attention this is also called auto regressive attention or causal attention or mask retention there are many names we essentially mask out all the tokens which come above the diagonal so essentially all of these tokens which which come above the diagonal we will set these tokens equal to zero and that's what this rest of the lecture is going to be about that how do we exactly set these tokens to be equal to zero so the rest of the lecture is going to be a bit mathematical but until now i hope you have understood the intuition of what we are trying to do right so we mask out all the elements above the diagonal and remember that in attention weights matrix in the attention weight matrix which we had seen before so here was the attention weight matrix the main difference between attention scores and attention weights is that each row essentially summed up to one so that is a property which we ideally want to retain so the problem is that if you randomly put all the elements to zero the rows will no longer sum up to one so we'll need to do a normalization step once more so that the attention weight in each row actually sum up to one so now let's see how we actually um so the task is now we have the attention scores right we have to put all the elements above the diagonal in the attention scores to be equal to zero let's see how to do that now so there are actually two strategies to do this right the and i want you to pause here and i want to and i want you to think right the the problem at hand is that imagine or take into account this previous lecture in the previous lecture we had calculated these attention scores but now i am telling you that all of these values need to be set to zero this this all of these these values which are essentially above the diagonal these values need to be set to zero all these values need to be set to zero so how will you go about essentially calculating the attentions weights or another way to think about it is that you have the attention weights right what if you set all of these values to zero let's say you set all of these values to zero now what will happen if you if you just put all of these values to zero if that is the case you will see that the rows don't sum up to be equal to one so then what you can simply do is that whatever is remaining so if you set all of these to zero this matrix will be something like this matrix will be something like 0.1 0 0 0 it will be 0.1 0.5 0 0 0.2 so actually the actual values which you need to set to zero are these ones this this needs to be set to zero these three these two and this these values you need to set to zero so if you set this to zero directly what will happen is that this will be 0.1 0 0 0 0 this will be 0.1 0.5 0 0 0 this will be 0.05 0.2 0.2 0 0 etc so the remaining two rows also so now you see the problem here is that each row does not sum up to one now the first row sums up to 0.1 the second row sums up to 0.6 etc so what can you do in this case so that each row sums up to one the simplest thing you can do is that the first row the entire summation is now 0.1 right so you divide this by 0.1 the second row entire summation is now 0.6 so you divide each element by 0.6 the third row the entire summation is now 0.2 plus 0.2 0.4 plus 0.05 which is 0.45 so you divide each of these by 0.45 you divide this also by 0.45 and you divide this also by 0.45 what that will ensure is that that will ensure that each that will ensure that each row still sums up to one so this is actually the first strategy which is implemented to to get the causal attention scores the first strategy is like you have the you already have the attention scores to which you have applied softmax and you've got the attention weights you did that in self-attention itself then what you do is that you just add zeros above the diagonals and you get the masked attention scores but then you normalize the rows again so that they sum up to one and then ultimately you get the masked attention weights that's the strategy which we saw right now but do you see the problem with this strategy the problem with this strategy is that let's see here again you see to go from attention scores to attention weights we are already doing softmax so we are doing one step of normalization here and then again we are doing one more step of normalization when we divide by the sum of the rows so we are unnecessarily doing two normalization steps over here so then the question is can we do a smarter way of normalization so that we can we need to only apply softmax once and it turns out it is possible to do that and the way to do that is we directly target the attention scores so let me tell you how we do this so the way to do that is we directly target the attention scores here is where we make our intervention instead of making our intervention in the attention weights so the way we do this now is that essentially what we do is that all of these values all of these values which are marking right now in circles which are essentially above the diagonal and which need to be replaced eventually with zero and what we do with these values is that we will replace these values with negative infinity we will replace these values with negative infinity so let's see what the attention scores matrix now becomes so the attention scores matrix now becomes 0.6 then negative infinity negative infinity negative infinity and negative infinity the second row becomes 0.1 0.1 then 1.8 and then again i will have these three negative infinity values so i just copy pasted them over here then in the third row i'll have two negative infinity values so i'll just show this for the three rows and the third row values will be 0.2 the third row values will be 0.2 1.1 1.2 now why are we doing this because remember that when you apply softmax what softmax does is that softmax actually takes the exponent so softmax will for the first row each element the softmax will replace with e raised to so the first element it will replace with e raised to x1 divided by the sum the second element the softmax will replace with the e raised to x2 divided by the sum so this we are doing for the first row so let's say let me mark the first row over here so if this is my first row each element will be replaced with this again so if this is my first row each element will be replaced with this e raised to x1 by sum e raised to x2 by sum etc now i want you to check what is e raised to minus infinity so you will see that e raised to minus infinity is actually equal to 0 right so the second element will be replaced with e raised to minus infinity divided by some which is anyway zero because e raised to minus infinity is zero this third element will be replaced with 4th element will be replaced with 0 and 5th element will be replaced with 0. Essentially wherever we have minus infinity all of those will be replaced by 0 because exponent raised to minus infinity is equal to 0 anyways. And the first element will be replaced with e raised to 0.6 divided by e raised to 0.6 so it will be replaced with 1 so it will sum up to 1. Here the first element will be replaced with e raised to 0.1 divided by e raised to 0.1 plus e raised to 0.8. The second element will be replaced with e raised to 1.8 divided by e raised to 0.1 plus e raised to 1.8. So the in each row will sum up to 1 and we will also make sure that all the elements above the diagonal are essentially 0. So this will make sure that we are not doing two stages of normalization. Here we did two stages of normalization right we did softmax followed by every row normalization but here we are doing only one softmax normalization that's it.

That's this trick of introducing negative infinity above the diagonal and it's a very powerful trick and it saves the computations for us. So the more efficient way is essentially you have the attention scores. The more efficient way is that you have these attention scores and then you apply something which is called as an upper triangular infinity mask.

What this mask essentially means is that you replace the upper triangular which is all the elements above the diagonal will be replaced with negative infinity and then you directly apply softmax only once. So see here you have only one softmax whereas here you had one softmax to get the attention weights and then you had another layer of normalization. So that way there are two normalizations.

So this on the other hand is a much more efficient way. We are now just going to see this in code so that you can understand what we are trying to do over here. Remember in the previous lecture we started out with this inputs embedding matrix where we had your journey starts with one and step.

So these are the six inputs over here and there is a vector embedding for each input here which is a three-dimensional vector embedding and at the end of the previous lecture we had defined this self-attention class. Essentially what this does is that it takes in the inputs it finds the keys queries and the values it finds the attention scores then it gets the attention weights and then it finds the context vector in causal attention we are going to just make changes in this part so that all the elements above the diagonal are essentially masked out. So let's see what is done in a causal attention.

So this section is titled hiding future words with causal attention. We start with the same inputs and I'm just printing out the attention weights over here and I'm first going to show you this first approach. In the first approach what we do is that we start with the attention weights which have been already obtained previously.

So remember when we are showing these attention weights we have already applied softmax before to the attention scores and in the first in the first method what we do is that we just take elements above the diagonal and put them to be equal to zero. So now here is the mask which is essentially all the elements above the diagonal are zero and then we apply this mask to the attention weights. So when we apply this mask to the attention weights you will see that we have the attention weights matrix but all the elements above the diagonal are now put to zero.

But this presents the problem that every row now does not sum up to one right and that's an issue. So to solve this what we do is that we simply divide by the summation of the rows and this is what we saw. This is exactly what we saw actually on the whiteboard over here if you remember the first step what we saw.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)