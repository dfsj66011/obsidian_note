
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


（24.56）

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
* 第三，他们实现了多项 GPU 优化技巧；
* 第四，构建了一个有利于蒸馏等技术的模型生态系统。


现在，这就是目前存在的多头注意力机制。而DeepSeek团队所做的创新在于，他们引入了一种完全不同的方法，以确保注意力机制能够高效实现。他们采用了键值缓存技术，但不同于常规的键值缓存，他们创新性地将其置于潜在空间中（若您暂时不理解这些术语不必担心，后续课程我们会详细讲解）。现阶段您只需记住：他们通过在潜在空间实施这种特殊的键值缓存技术，使得注意力机制的计算效率显著提升——不仅占用更少存储空间，计算速度也更快。这正是他们做出的重要变革之一。

这就是多头潜在注意力机制，这是我们讨论的第一个要点。第二个要点是关于专家混合模型（MoE）。如果你观察常规的注意力机制，它的结构大致是这样的：首先是前馈层，接着是注意力层，之后又接一个前馈神经网络。而在专家混合模型中，核心操作是设置了四个专家模块——整个神经网络并非同时激活，每次只激活其中的部分子网络。

因此，他们设计了一种特殊的路由机制，由它来决定哪些部分会被激活，哪些部分不会。这个路由器本质上就是决定哪些专家模块会被激活，哪些不会。我们后续会非常详细地学习这部分内容。这是他们实现的第二个关键创新点。

第三项创新是多token预测技术。就像我们这节课开头看到的，传统模型通常只预测单个token。而他们采用了一项上个月刚发表论文提出的新技术——为什么不尝试同时预测多个token呢？这可能会加速推理过程，提高模型效率。

第四项创新是他们实施了量化技术。因此，量化不是将每个参数都表示为大型浮点数，而是以一种略微压缩的方式来表示。理解量化的最佳方式就像这里的例子：左边是原始图像，包含大量像素点，而右边的图像仅由八种颜色构成，呈现出一种像素化的效果。如果你放大看，会发现它完全是由像素块组成的——看这里，相比左边非常锐利的图像，这边明显像素化了。但当你缩小视图时，会发现两者看起来几乎一模一样对吧？这就是所谓的量化。他们将这种量化技术应用到了Transformer模块的参数中。

最后，他们还引入了一种称为"旋转位置编码"的技术。让我现在就来展示旋转位置编码的原理——它本质上应用于2017年提出的注意力机制中。

他们只是将位置编码添加到标记编码中，这污染了嵌入向量。但不久之后，人们意识到，为什么不直接旋转原始向量、查询向量和键向量来捕捉位置编码的效果呢？这样就不会改变向量的幅度，而且这是一种非常高效的位置编码方式。因此，我们不是在标记嵌入本身中编码位置，而是在查询和键中稍后编码位置。是的，这就是旋转位置编码，这是DeepSeek架构实现的第五个关键创新。我们现在将详细研究所有这些内容。这是第一个创新方面——架构创新，但他们并没有止步于此。我认为DeepSeek论文真正让强化学习领域重获新生，因为它不仅仅依赖于人类标记的数据，就像我们在GPT-3.5中看到的那样，人类创造了数据的质量，然后将这些数据反馈到训练过程中。

他们采用大规模强化学习来教授模型复杂推理能力，并非依赖人工标注数据，而是构建了一套基于规则的奖励系统——这意味着整个训练过程不依赖人类判断，完全由规则驱动。通过这种方式，他们实现了名为"群体相对策略优化"的框架，这正是其强化学习训练机制的核心所在。我们将深入解析这套体系，而这正是DeepSeek R1成为卓越推理模型的关键所在。

强化学习是我们将要涵盖的主要内容之一。第三个方面是GPU优化技巧，这部分理解起来有点难度，所以即使在课程中我也不会花太多时间讲解。简单来说，他们所做的不是使用CUDA，而是采用了NVIDIA的并行线程执行技术（PTX）。最直观的理解方式——其实我问过GPT如何用最简单的方式来解释——如果把CUDA比作编写Python或Java代码，那么PTX就像是更底层的一步，它是机器代码执行前的一个中间步骤，这能大幅提升运行速度。

因此，在高级编程中，你并不处于机器码层面。如果你处于机器码层面，那是最快的，比如C或C++，而Python或Java则让你处于更高的层次，这会减慢速度。你可以把PTX看作是介于两者之间的东西，而CUDA处于更高的层次，但PTX处于中间，更接近机器码执行，因此能大大加快速度。关于这一点，还有一篇不错的文章发表过，是的，Deep Six AI在某些功能上突破了行业标准的CUDA，转而使用NVIDIA的汇编式编程，比如PTX编程。

因此我认为，这也对加速他们的架构优化起到了重要作用，最终降低了成本。最后，他们拥有一个强大的模型生态系统——尽管主模型规模高达6710亿参数，但他们已将其蒸馏压缩至小至15亿参数的模型，这构成了一个非常出色的模型生态系统。我们稍后也会探讨模型蒸馏技术。现在简要回顾一下让Deep Six如此突出的四个关键点：首先是创新架构；第二是以强化学习为核心的训练方法；第三是他们掌握的一系列GPU优化技巧；第四是模型生态系统，特别是将大模型蒸馏压缩至约15亿参数的小型模型的能力。

现在，我们进入今天课程的最后两个部分。首先，为什么Deep Six是AI历史上的一个转折点？我认为它是一个转折点，原因如下：这是第一次有一个小而精悍的初创公司，通过新颖的技术和少得多的资源，达到了与最佳AI模型相当的水平。他们大幅降低了开发成本，虽然不像600万美元那么低，但我相信与像GPT或Meta的Llama等大公司训练模型所需的成本相比，仍然相当低。因此，这就像是第一次证明，即使是小型初创公司，即使是资金不如OpenAI雄厚的公司，也能构建出大型语言模型，这真是太棒了。

DeepSeek的表现与GPT4不相上下，但所需资源却少得多。这一进展也引发了恐慌，投资者们感到不安。2025年1月，由于DeepSeek的突破性进展，美国科技股出现了大幅下跌。主要原因是，低成本、开源的中国AI模型威胁到了OpenAI、微软甚至谷歌的盈利模式，并引发了人们对AI供应链和GPU市场的担忧。一个模型、一家公司——DeepSeek，带来了如此多的变化，这也正是DeepSeek成为历史转折点的关键所在。我认为，还有一件重要的事情也已经发生了。

因为DeepSeek的出现，发展中国家如印度已经开始大力投资建设自己的大规模基础模型。如果中国的DeepSeek能做到，为什么其他国家不行？为什么只有美国？为什么只有美国公司能做到？其他拥有DeepSeek所用资源的公司为什么不能建立自己的基础模型？事实上，所有这些讨论已经开始。印度政府甚至发布了建设基础模型的号召，这非常棒，我认为这也是我制作这个系列的主要动力之一。

现在进入今天讲座的最后一个部分，也就是我们这个系列讲座的计划。我已经根据DeepSeek的特点，将这个系列讲座分成了四个阶段。第一阶段我们将深入探讨架构，首先我会从注意力机制开始讲解，接着是多头潜在注意力、专家混合、多令牌预测、量化以及旋转位置编码。我会假设大家对注意力机制有一定的了解，如果没有的话，可以参考"从零开始构建LLM"系列讲座。这个系列讲座会稍微高级一些，建议大家先学习之前的那个系列。

在开始之前，我会先从一个稍高的层次切入，然后我们会进入训练方法论的第二阶段，第三阶段是GPU优化文本——这部分内容会比较简短，我不会安排太多课程。最后我将以知识蒸馏相关课程作为收尾。整个系列课程的主体将集中在第一阶段和第二阶段，第三阶段和第四阶段的课程会相对较少。这就是本系列课程的主要规划框架。

让我们快速回顾今天的学习要点：首先我们探讨了大语言模型，认识到它们本质上是基于概率预测下一个标记的引擎。同时我们发现，模型规模是大语言模型非常关键的决定性因素。

随着模型规模的增大，存在一个规模扩展定律，模型性能会不断提升，并开始展现出小规模模型所不具备的涌现特性。我们发现大语言模型（LLM）的核心要素本质上就是Transformer架构。最后我们认识到，构建LLM意味着首先要建立基础模型（即预训练阶段），随后进入微调阶段（包含两个环节）。虽然DeepSeek R1近期声名鹊起，但DeepSeek公司早在多年前就已起步——他们最初推出的是DeepSeek LLM第一代（V1版本），随后迭代出V2版本，最终研发出拥有6710亿参数的巨型V3版本，并在此基础上最终打造出推理专用模型DeepSeek R1。

这简直让互联网炸开了锅！为什么？因为DeepSeek R1的性能竟然能媲美OpenAI的原版模型，而成本却只是零头，更关键的是它还是开源的！DeepSeek的表现与GPT-4旗鼓相当，但价格却便宜了100到500倍——就像我在这次讲座中展示的数据那样。最后，它完全开源。

优势与不足：  
DeepSeek最大的优势在于：开源、高性价比、性能强劲——三大核心优势。而它最主要的不足可能是，相比GPT-4，它的打磨程度和安全性稍逊一筹。另一个弱点是：如果你想在本地部署或安全使用，你需要有能下载和运行6710亿参数模型的基础设施。

然后我们看到了DeepSeek如此特别的原因，这里有四个关键要素：首先是创新的架构、训练方法、GPU优化和模型生态系统。在创新的架构中，我们有五个关键点：多头潜在注意力、专家混合、多令牌预测、量化和旋转位置编码。在训练方法方面，他们使用大规模强化学习来教授模型复杂推理，并采用了基于规则的奖励系统，也称为群体相对策略优化，而不是依赖人工标注数据。在GPU优化技巧方面……

我认为他们只在某些地方使用了并行线程执行（PTA）而非仅依赖CUDA，最终他们建立了一个强大的模型生态系统，将主模型蒸馏为参数低至15亿的小型模型。在本系列讲座中，我们将遵循相同的工作流程：第一阶段是创新架构，第二阶段是训练方法，第三阶段是GPU优化技巧，第四阶段是模型生态系统。我会假设大家对大型语言模型（LLMs）有相当的了解。

我会再次讲解注意力机制，但基本上会从注意力机制开始，然后深入细节。如果你是零基础，我建议你先看《从零构建LLM》系列。最后，我认为深度求索是历史上的一个转折点，因为他们用实际行动证明，即使是发展中国家，只要我们在创新架构上足够聪明、足够有创意，也能构建出与OpenAI模型相媲美的基础模型，而且成本低廉、完全开源。他们正在以这种方式真正实现AI的民主化。非常感谢大家，期待下次讲座再见！

大家好，我是Raj Dandekar博士。我于2022年从麻省理工学院获得机器学习博士学位，同时也是"从零构建Deep Seek"系列教程的创作者。在开始之前，我想向大家介绍本系列视频的赞助商兼合作伙伴——InVideo AI。大家都知道我们多么重视基础性内容，即从最底层构建AI模型。InVideo AI的理念与我们的原则非常相似。让我来为大家展示一下。这就是InVideo AI的网站，他们凭借一个小型工程团队就打造出了令人惊叹的产品，只需输入文字提示就能生成高质量的AI视频。

如你所见，这里我输入了一段文字提示："创作一段超写实的高端奢华腕表商业视频，并赋予电影级质感"。点击生成视频后，很快我就获得了这段令人惊叹的逼真视频。最让我震撼的是它对细节的极致把控——看看这个质感与纹理，简直不可思议！而这一切仅源于一段文字指令，这正是InVideo产品的魔力所在。支撑这段惊艳视频的核心，是InVideo AI基于第一性原理重构的视频创作流程，他们通过对基础模型的反复实验与调试，从根本上革新了视频生成与剪辑技术。

他们在印度拥有最大的H100和H200集群之一，并且正在试验B200。在视频AI领域，他们是印度发展最快的AI初创公司，面向全球市场，这也是我如此认同他们的原因。好消息是，他们目前有多个职位空缺，你可以加入他们优秀的团队。我会在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建DeepSeek”系列的这一讲。在上一讲中，我们了解了我们将这个系列讲座划分的四个阶段。

第一阶段是DeepSeek背后的创新架构，这就是这里的第一阶段。第二阶段是训练方法本身，即强化学习的兴起，以及他们如何依赖RL，通过基于规则的奖励系统来教授模型进行复杂推理，这就是第二阶段。第三阶段是GPU优化技巧，即他们如何使用NVIDIA的并行线程执行（PTX或CUDA）。第四阶段是他们的模型生态系统本身，他们不仅仅停留在构建一个庞大的6710亿参数模型，而是将这个大型模型蒸馏成一个更小的模型，其大小约为15亿参数。

这就是我们即将经历的第四阶段，我们将从第一阶段开始，经过第二阶段、第三阶段，最终到达第四阶段。在今天的讲座中，我们将首先介绍第一阶段，这本质上是Deep Seek背后的创新架构，以及使其如此高效的原因。其架构的两个主要方面极大地提升了Deep Seek的效率：一个是多头潜在注意力机制（MLA），它使注意力机制本身更加高效；另一个是专家混合机制，这意味着尽管参数数量达到了6710亿。

所有这些参数并非同时激活，实际上只有约370亿个参数处于活跃状态。本质上，部分参数会被关闭，部分则被开启——就像灯泡明灭闪烁那样。这种机制让模型的计算效率极高：不需要的参数会被关闭，需要时则瞬间激活并立即投入工作。此外，我们还会学习多令牌预测量化技术和旋转位置编码技术。不过首先要讲解的是注意力机制——我一直在思考如何精准地向你们阐释这个概念。

因为概念本身相当高级，所以如果你搜索多头注意力，或者搜索多头潜在注意力，你会找到一些相关的博客文章来讨论这个问题。这些文章确实从本质上讲解了多头注意力是什么。如果你往下滚动页面，你会看到这只是博客文章的一页内容，他们从多查询注意力、组查询注意力开始讲起，然后讲到旋转位置编码，最后才有一小部分是关于多头潜在注意力的。这部分内容对于刚开始探索深度学习的人来说几乎不可能理解。因此，我不想采用那种仅仅通过简单讲解的方式来进行。

在讲解多头潜在注意力机制（Multi-Head Latent Attention）时，我并未预设听众已掌握先验知识——正如本系列课程开篇所言，我希望将这个讲座系列打造得非常深入。因此我将通过四个阶段来阐释：首先我们将剖析大语言模型（LLMs）的架构本身，这将是今天课程的核心目标。我坚信，若缺乏对大语言模型架构的直觉认知，就根本无法理解潜在注意力机制。随后我们将探讨自注意力机制诞生的必要性及其本质，在掌握自注意力之后，最终揭示自注意力如何演变为多头注意力机制。

拥有多个注意力头意味着什么？这是第三个方面。第四个方面本质上是键值缓存。接下来我们将理解多头注意力机制的工作原理，它确实表现非常出色。但为了提高多头注意力的效率，使其计算速度更快，并减少存储在内存中的参数数量，我们可以采取哪些措施？这时键值缓存就派上用场了。只有真正理解了键值缓存，你才能逐步理解多头潜在注意力机制。

因此，在了解KV缓存之后，我们将开始学习MLA。但在深入探讨MLA本身之前，我会花大量时间来夯实你们在这前四个概念上的基础。现在提到的许多博客文章都默认你们已经熟悉这些内容。在“从零构建DeepSeek”系列中，我们用了43节课详细讲解这些不同方面。而在这个系列中，我不会讲得那么深入。例如，今天关于LLM架构我只安排了一节课，但在“从零构建LLM”系列中，这部分内容用了三到四节课来讲解。

好的，我的目标是既要向你们解释这些知识，也要确保刚开始观看这个系列的初学者能跟上进度。因此，我的挑战在于为这个系列专门准备一套新的讲义。因为我要在一个小时内解释一个概念，同时还要确保初学者不会在这个过程中掉队。为此，我专门制作了一系列新的笔记来讲解这些不同的概念。好了，希望大家都能理解我们今天要学习多头潜在注意力的流程。我们的主要目标是理解LLMs的架构。

所以在今天的讲座之后，你们都应该在脑海中形成一幅思维导图或视觉路线图，了解当一个词或标记进入大型语言模型（LLM）时会发生什么。首先，让我解释一下LLM的架构是什么意思。如果你输入一个句子或一系列单词，我们已经看到，当一系列单词输入到LLM时，LLM本质上做的是预测下一个单词，或者更准确地说，预测下一个标记。因此，LLM或大型语言模型可以被视为下一个标记预测引擎，就像一台引擎一样。

假设我现在去搜索引擎搜索“汽车”，然后复制一张图片——或者让我现在复制一张合适的图片——如果我此刻复制这张图片并粘贴到这里，这就是一个引擎。如果我们称其为“下一个词预测引擎”，我们需要了解这个引擎实际上是如何运作的。在之前的课程中，我们已经学习了一些关于这个引擎的知识。我们学到了什么呢？首先，我们了解到这个引擎拥有海量的参数。举个例子，GPT-3约有1750亿个参数，而GPT-4虽然尚未正式发布，其参数规模可能已达到万亿级别。至于两三天前刚发布的GPT-4.5，其参数可能高达5万亿甚至10万亿（尽管尚未完全公开）。但我们可以确定的是，这个引擎通过天文数字般的参数协同工作。

如果你搜索汽车引擎的工作原理，就会发现这里有一个活塞气缸机构，对吧？这个活塞气缸机构本质上就是引擎运转的核心。同样地，我们需要确保自己理解这些参数究竟如何运作——这些参数藏在哪里？我们能否像打开汽车引擎盖那样，打开大语言模型的"引擎"看看内部运作？具体来说，当输入一串词语时（就像汽车注入燃油），这个"引擎"内部究竟发生了什么？燃油如何转化为汽车的动力，同理词语序列如何在这个语言模型里被处理，最终促使模型预测出下一个词——这才是隐藏在LLM之下的奥秘。

所以，把单词序列想象成燃料，把大语言模型（LLM）比作引擎，而下一个单词就是汽车的行驶动作。这么说吧，我们现在要打开引擎盖，真正理解引擎的工作原理，就需要了解引擎的结构。这意味着我们需要知道引擎内部有哪些不同的部件，这些部件之间是如何相互连接的，以及当燃料（即单词序列）通过这个结构时，到底发生了什么变化。这就是我们今天这节课要深入探讨的核心内容。

所以你可以把今天的讲座想象成打开汽车引擎盖，试图窥探引擎内部，真正理解它是如何工作的。另一种理解方式是，我们所有人都用过GPT这样的工具，比如当我们提出“给我写一篇关于友谊的短文”这样的要求时，我们会发现GPT实际上是一个标记一个标记地预测输出的。那么，这种预测是如何实现的？是什么样的架构在支持这种逐个标记的预测？这就是我们今天要探究的内容。

好的，我会尽量让这个讲座对初学者友好一些。你会看到我构建了一个完整的故事来解释LLM架构的工作原理。我认为以前从未有人这样做过，所以对我来说，用这种故事的形式来解释也是一种实验。但希望通过这个解释，你能真正理解LLM架构是如何运作的。那么，这是当你查看引擎时的示意图，当你打开黑盒子时会发生什么，黑盒子里面有什么。当你打开黑盒子时，你会发现有很多东西跳出来，一点也不简单。如果你想到1750亿个参数，这些1750亿个参数分散在这个黑盒子的多个地方。换句话说，LLM架构相当复杂。

你认为它可能很复杂的原因是什么呢？因为在进行下一个标记预测时，大语言模型实际上是在学习语言本身。我强烈认为语言学习是下一个标记预测任务的副产品，而要学习语言，你不能有一个太小的引擎，也不能有一个不够复杂的引擎。这就是为什么我们的引擎相当复杂——我们有大量相互连接的块或层。今天我们将尝试理解这种架构。

所以如果你看一下这个示意图，你会发现它大致分为三个部分。首先是这里的第一个部分，我称之为第一部分；然后是第二部分，我标记为变压器模块，让我这样标注一下；最后是第三部分，基本上是输出部分，也就是预测下一个标记的地方。因此，LLM的架构可以分为三个部分来理解：第一部分可以看作是输入部分，第二部分可以看作是处理器，第三部分本质上就是输出部分。在输入部分，我们有句子。

比如说，你有一个句子，任何句子都可以，比如“明天会更好”。在将这个句子传递给处理器之前，这个句子以及其中的每个标记或每个单词都会经历一系列处理步骤。首先，我们会进行所谓的“分词”（tokenization）；其次，我们会进行“标记嵌入”（token embedding）；最后，我们会进行“位置嵌入”（positional embedding）。这三个步骤完成后，输入内容才会被传递给处理器，也就是所谓的“Transformer块”。

在Transformer块内部，有六个不同的组成部分：归一化层、多头注意力层、Dropout层、第二个归一化层、前馈神经网络层以及另一层Dropout层。这里的两个加号代表所谓的跳跃连接或快捷连接。最后，当我们从处理器或Transformer块出来时，会得到输出结果，这里还有一个归一化层以及用于预测下一个token的最终层。好了，我刚才所说的内容，如果你是第一次学习这些，可能会觉得有点摸不着头脑。

而这一切看起来太复杂了，让我们进一步分解，这正是我现在要做的。但请密切关注我用紫色标记的这一部分，因为我要解释的第一个创新——多头潜在注意力（MLA）——与这个标为“多头注意力”的方面有关。在整个架构中，深度探索（DeepSeek）在两个主要地方做出了创新贡献：第一个就是这个称为“多头注意力”的模块，第二个是前馈神经网络。让我现在稍微擦除一下，MLA或多头注意力是架构这一部分的创新。

所以我将这种创新称为MLA（混合专家）架构中的创新。再次强调，如果你不了解架构本身，就无法真正领会创新点在哪里。这就像拆开发动机一样——你现在拆开的是DeepSeek的"引擎"，这个表现优异的"汽车引擎"。要理解它为何表现卓越，就得打开引擎盖，观察所有这些零部件的运作。

现在我告诉你，这台引擎中有两个部件经过增强以提高性能。不过言归正传，回到引擎本身——输入处理器和输出系统究竟如何运作？这次课程我给自己设定的挑战，就是要用一堂课的时间把所有这些原理讲透：既要解析输入系统，又要阐释处理器，还要说明输出机制。我希望用通俗易懂的方式讲解，让你们能真正感知到整个架构的精妙。我的讲解思路是从燃料的视角出发——假设你是一滴燃油，进入汽车引擎后会经历什么？是先进入引擎内部，然后被活塞带动旋转，最终产生某种动力或能量吗？只要理解了燃料的生命历程，实际上就掌握了引擎的工作原理，对吧？

今天我想向大家展示一个单词的生命周期，以及当它经历LLM架构时会发生什么。我们可以这样思考：当我们输入这个句子"给我一篇关于友谊的短文"，或者假设我们要完成下一句话，比如我选择的句子是"明天会更好"——"明天会更好"这句话进入LLM引擎后，系统就会预测出下一个标记。

假设下一个词是“and”，而明天是光明的。我想向你们展示的是，我们将只关注一个词，看看这个词在LLM架构的每一步中会发生什么。通过这个词的视角，我希望你们现在想象自己就是这个词。你就是这个词，我将带你经历这个词在LLM架构的多个层中所经历的一切，最终我们预测下一个词。这就是我如何以故事的形式向你们解释这整个讲座。我会把这次讲座讲得好像你就是这个词一样。现在想象你就是这个词，你被一堆词包围着，突然你被扔进了LLM架构中。

让我们来了解单个token的生命周期，这就是接下来这部分讲座的主要内容。好的，让我们一起踏上这段旅程，了解token的生命周期究竟是什么样的。我给这一部分起的标题是“token在LLM架构中的旅程”。我首先做的是去ChatGPT那里，让它写一段关于朋友的短文。然后我从里面随机选了一句话：“一个真正的朋友会接纳你”。假设我们正在看这句话——“一个真正的朋友会接纳你”，它由五个单词组成。

假设这是我的当前输入序列，我们需要根据这个输入序列预测下一个标记。我将特别关注“friend”这个词，并从“friend”的角度来思考。现在，请站在这个词或这个标记的立场上——首先，这个标记（为了方便起见，我会交替使用“标记”和“词”，尽管它们并不完全相同，但为了简单起见，我就说一个标记等于一个词）。所以，请设身处地为这个标记着想。

现在你看到了什么？你看到我周围还有这些其他标记，对吧？那里有一个真正的接受者（true accepts），还有这四个其他标记，我习惯和它们待在一起，就像朋友们聚在一起一样。我习惯把它们当作我的邻居和朋友——一个真正的接受者（true accepts）和你。这四个邻居就是我们为这个标记选择的“朋友”。现在突然发生的第一步是——在LLM架构中，第一步就是隔离的步骤。所以目前我们正在观察这个阶段，也就是输入阶段。

在输入阶段的第一步是隔离阶段，即单词与其相邻词分离。想象一群朋友，每个人都与同伴隔开——这个词被单独隔离出来，我们孤立地审视它，这就是第一阶段。第二阶段本质上称为"令牌ID分配"，这意味着现在每个单词都被隔离，我们要为每个词或令牌贴上类似徽章或编号的标识。就像参加营地、入伍或任何团体活动时会被分配ID（比如学号），学校里每个人都有学号对吧？同理，每个令牌被隔离后都会获得专属令牌ID。我将其比喻为"领取徽章"的过程，而令牌ID的分配机制其实非常有趣——我们拥有一本令牌ID字典。

所以你可以把这想象成一本百科全书或一本标记ID的书。这本书里基本上列出了所有可能的标记，所有可能的标记就是所有可能的单词，然后每个单词都对应一个数字。这本书里不仅有单词，还有字符，甚至还有子词。所以这本书包含像a、b一直到z这样的字符，它甚至包含像“cr”这样的子词，这个词可能就在这个词汇表中。然后它还可能包含像“isation”这样的子词，这可以是一个子词。此外，它还包含像“token”这样的完整单词，这个词可以在这个词汇表中作为一个完整单词存在，“enter”也可以是一个完整单词，“begin”同样可以是一个完整单词。

所以你可以把这本标记ID的书想象成由字符、单词和子词组成。让我在这里写下来，这一点非常重要：这本标记ID的书本质上由字符、单词和子词组成。因此，我们确保每个被分离的标记或单词都能找到对应的编码，不会有任何被分离的标记或单词找不到对应的编码。熟悉字节对编码概念的读者会记得，为了创建这本标记ID的书，我们使用了一种称为字节对编码的方案。这是一种子词标记化方案，而GPT-2就依赖于字节对编码机制来创建其词汇表。这本标记ID的书也被称为词汇表。

然后它会从一个大型语言模型转变为另一个，比如说GPT-2的词汇量是5万，而GPT-4的词汇量可能更高，也许是10万。所以根据我们使用的大型语言模型，分配给某个词的标记ID可能会变化。我现在使用的这个大型语言模型词汇量是5万，这意味着有5万个标记，可能是字符、单词和子词的组合。接下来我要做的就是查看这个词汇表。

我要找到“friend”这个词在序列中的位置，并确定与之关联的token ID。现在“friend”这个词对应的token ID是2012，我会把这个数字记录下来——这就像是给这个词分配的徽章或学号。所以现在“friend”这个词的token学号就是2012。同理，其他所有token（即所有其他单词）都会获得类似的编号标识。这是整个流程的第一步，或者更准确地说，是第二阶段的工作：token ID分配。

那么现在想象一下，这个被孤立出来的标记朋友，它现在被赋予了一个徽章或编号——2012，这就是第二阶段。我没有详细解释这个标记ID簿是如何创建的，因为如果你想了解更多细节，有一个单独的讲座专门讲解如何为每个大型语言模型构建这种词汇表，它被称为"从零开始实现字节对编码"。这个讲座包含在"从零开始构建LLM"系列课程中。但现在请跟着我的思路——想象你就是这个标记朋友，你被孤立出来后获得了一个编号，然后你就进入了第三阶段。在第三阶段会发生一件有趣的事情：在此之前，你只关联着一个数字。

没错，但现在你将拥有一个庞大的数字向量，这些数字将与您相关联，这被称为标记嵌入分配。可以这样理解：假设我们有一个入学考试，共有768道题目，每道题本质上都在测试你的某个特征。现在我们来看“朋友”这个词，每道题会测试：你是名词吗？你有性别吗？你是动词吗？你是运动吗？你是情感吗？等等。

实际上，我们并不清楚这些特征或问题的具体内容，但我正试图向你解释，以便你能对词嵌入有一个直观的理解。想象一下，有768个类似这样的问题，每一个被我们单独提取出来的词都会被问到这些问题。然后，根据得到的答案，我们可以理解这个词的某些特性——比如它是一个名词、一项运动、一个形容词，还是总是出现在句子末尾的某个词；或者它是否与性别相关，是否与君主制相关，比如国王、公主、女王等等。

现在我们终于开始了解标记本身的意义了。在标记ID分配阶段，我们并未涉及任何意义层面的内容。但到了第三阶段的标记嵌入（token embedding），由于我们提出了一系列问题，就能获取到一些语义信息。根据这些问题的回答结果，每个标记都会生成对应的数值。比如这个"friend"标记，它会对每个问题产生一个数值（可能是0.1、0.2、0.1、0.3等）。假设我们设置了768个问题，那么就会生成一个768维的向量。需要特别说明的是，这个问题的数量（768）会因不同的大语言模型而有所差异。比如我们来看GPT-2模型，它的标记嵌入维度...

所以如果我们搜索GPT2的词元嵌入维度，会发现它是768对吧？但这里GPT2小型版也是768，而最大的GPT2模型则有1600维。因此这个768的提问数量实际上会因不同大语言模型而异。接下来我们要做的是：假设GPT2的提问数量是768，但请记住——如果词元进入不同的大语言模型，它可能会被问不同的问题。现在想象你是一个词元，刚领到编号徽章，突然就要回答这768个问题。当你作答后，所有答案会被收集成一个768维的向量——这就是所谓的词元嵌入向量。

所以现在，除了徽章之外，你还随身携带着你的结果，也就是说你有一个徽章，还有这768个数值的结果，这就是到目前为止发生在你身上的事情，对吧？这就是词元嵌入的阶段。再次强调，词元ID和词元嵌入的区别在于，词元ID不携带任何关于语义的概念，而词元嵌入在分配时会非常关注单词本身的含义。进行词元嵌入的原因是为了创建LLMs（大语言模型），你最终需要提取意义，对吧？你是在向模型教授一些关于语言的知识。

因此，这是一个非常关键的步骤。这些关于每个标记的问题或特征集合都会被收集起来。到目前为止，每个标记都有一个标识，并且每个标记都附带一个768维度的值表。此外，还有一个重要的因素是你在邻居中的位置。在这里，如果你看到“一个真正的朋友接受你”，那么“朋友”这个词位于句子的中间，也就是第三个位置。具体来说，“一个”在第一位，“真正的”在第二位，“朋友”在第三位，“接受”在第四位，“你”在第五位。所以，“朋友”出现在第三个位置，而这个位置也很重要。为什么位置很重要呢？

因为如果你说“狗追另一只狗”，看看这句话，你需要意识到这只狗和另一只狗是不同的。如果只按字面意思理解，如第三阶段所示，我们只取了词义，那么这两只狗的标记嵌入会是相同的。但实际上这是两只不同的狗，我们需要教会模型识别这一点。唯一能区分这两只狗的方法就是知道第一只出现在第二个位置，第二只出现在第五个位置。

因此，了解位置信息同样重要。类似于我们提出的768个问题，关于位置信息也会再次提出768个问题。需要注意的是，虽然不同模型这个数字会有所变化，但如果你固定一个特定的语言模型，那么在词嵌入中提出的问题数量与位置嵌入中提出的问题数量是相同的。以我们现在研究的GPT-2小模型为例，词嵌入中提出了768个问题，同样地，位置嵌入中也会提出768个问题。

那么这些位置或问题可能是什么呢？它们可能是类似这样的问题：你是处于序列的开头、中间部分，还是能够处理长距离依赖关系等等。实际上，没人确切知道这些问题具体是什么，但我觉得这是解释位置嵌入和标记嵌入最简单的方式。现在进入第三阶段，每个标记都有一个与之关联的标记嵌入——这是一个768维的向量。而到了第四阶段，根据所处位置，你需要回答这768个问题，对吧？

所以你还会有一个768维的位置嵌入与你相关联。现在想象一下，一个标记首先会受到标记ID的印记或徽章的影响，然后它会得到标记嵌入的结果。这些都是它需要回答的问题，也就是第一个测试。接着，这个标记会进入另一个测试，即位置嵌入，然后它再次拥有这768个值。每个标记都需要进行大量的处理。它基本上必须通过大量的测试。然后在第五步，我们所做的是将你的标记嵌入结果与位置嵌入相加。这样你就不必再分别携带这两个结果了。

你将它们合并在一起。所以现在768维向量的词元嵌入和768维向量的位置嵌入相加在一起，这就是所谓的输入嵌入。因此，这就是词元“friend”的输入嵌入。它是一个768维的向量。对于所有其他词元或所有其他单词，我们也会有类似的输入嵌入。但在这里，我向你展示的是词元“friend”的输入嵌入。这是词元嵌入加上位置嵌入的结果。所以现在你不必分别携带这两个结果。你只需要携带一个结果。

现在，那768维向量就是与你作为标记相关联的标识。这是你在后续旅程中最重要的区分特征。想象一下这段旅程：你最初是孤立的，被授予徽章后，先接受第一次标记嵌入测试，再经历第二次标记嵌入测试。最终，经过所有这些步骤，你获得了唯一能标识自己的东西——输入嵌入。你可以将其视为你的专属制服。这套制服是为你量身定制的，而你作为标记就要穿着它。与你同行的每个朋友（每个标记）、每个相邻词汇都会穿着不同的制服。为什么？因为它们的含义各不相同，对这些问题的回答也会不同，它们所处的位置自然也不一样。

所以他们需要分别回答这些问题。因此，每个标记的统一处理方式都会有所不同。到目前为止，我们所做的这五个步骤，本质上就是输入块或输入层中发生的事情。这是我们目前学习的第一部分。现在，我认为这里提到的这三个步骤对你们来说会很容易理解。首先是标记化。

其次是标记嵌入。第三是位置嵌入，这正是我们刚才看到的，然后标记嵌入和位置嵌入相加，得到所谓的输入嵌入。整个过程就是这样，记住这里的标记化文本。我们通过词汇表或标记ID手册看到了标记ID的分配。这就是每个标记经过输入块（这里是第一阶段的第一部分）后的输入嵌入。它们有一个统一的特征，使它们与其他标记区分开来。

好的，这就是第一部分——输入部分。只有当你拥有了统一的“制服”，才能进入下一个环节，也就是处理器部分。现在，每个标记（token）都穿上了统一的“制服”，就像《哈利·波特》里只有穿上特定学院的制服（比如格兰芬多、拉文克劳或斯莱特林）才能进入霍格沃茨一样。此时，每个词或标记都拥有了自己的“制服”，终于可以登上通往Transformer模块的“列车”了。

你看，这五个标记现在会一起进入Transformer模块。真正的朋友会接纳你的一切。现在每当我提到“制服”，你应该想到它代表的是一个768维的向量。Transformer模块并不理解单词，甚至目前还完全不懂单词的含义——它只知道每个标记都是一个768维的向量。而在Transformer模块中，神奇的事情将会发生：不同标记之间的含义会变得清晰可辨，模型自身也将逐步理解语言。

因此，变压器块本质上就是所有魔法发生的地方。第二个处理器部分才是真正一切发生的核心所在——就是这里的这个部分。这个处理器部分确实是所有奇迹诞生的地方。你可能会想：大语言模型究竟是如何运作的？虽然它们只是预测下一个标记，但它们似乎已经掌握了语言的某些本质。奇妙的是，它们与人类互动的方式，几乎就像你在和我这个真人交流一样。

它们能总结任务，擅长语法检查，帮我起草邮件，还能完成复杂的编码工作。这一切都归功于Transformer模块的运行机制。现在，每个标记实际上都会经历一段旅程，穿过卡车（比喻性表述）进入Transformer模块本身。好了，现在让我们来思考这个Transformer模块。

我们需要明白，Transformer模块就像一列由大量不同部件组成的火车，对吧？首先我们要看看Transformer模块这列火车本身的组成部分。我不会详细讲解所有部件，只是简单介绍一下Transformer模块中每个部件的作用。现在想象一下，这五位乘客被分配到了一个车厢里，而且他们现在都是768维的输入嵌入。

他们必须在一个Transformer块中经历一、二、三、四、五、六这六个步骤。那么这六个步骤是什么呢？你可以这样想象：如果把Transformer块比作一列火车，这六个步骤就像是连接在一起的六个车厢。一旦你上了这列火车，就必须和你的“邻居”一起经历所有这六个步骤。

第一步是层归一化（layer normalization），这意味着对768维的向量进行处理。现在让我们聚焦在“friend”这个词上。这个768维的“friend”向量会被归一化，即调整其均值和标准差，使得均值变为0，标准差变为1。这个步骤相对简单。然后我们来到多头注意力机制（multi-head attention）。这里我用不同的颜色标记了这个部分，因为这才是真正赋予Transformer块强大能力的创新之处。

我们本质上是在学习：当我们关注一个标记时，应该给予其他标记多少注意力？例如，当你看到“朋友”这个词时，应该对“真实的”和“你”给予多少关注？多头注意力机制实际上编码了关于上下文的信息。当你观察一个标记时，你会突然绘制出一张地图，显示所有其他标记的重要性。仔细想想，这对理解语言、理解句子本身的上下文或段落的上下文非常有帮助。比如我说：“我来自印度浦那，我讲……”在这里，如果你要完成下一句话，你需要知道应该更多地关注“浦那”和“印度”，因为那是我来自的地方，对吧？

所以你不需要太关注前三个标记，也许“So”这个词说明了注意力机制的重要性，它帮助我们理解句子的上下文并预测下一个可能的词。我们将在下一节课中更详细地学习注意力机制，但请记住，这是Transformer块的第二个组成部分。Transformer块的第三个组成部分是Dropout层。如果你已经学过神经网络，Dropout本质上是指如果有100个参数，且Dropout因子为0.5，那么你随机将其中50个参数设为0。为什么呢？

因为如果某些参数很懒，根本不学习任何东西呢？突然，如果其他参数现在被丢弃，意味着它们被设为0，这些参数别无选择，只能自己学习一些东西。因此，dropout是一种让懒惰参数重新活跃起来的机制。它提高了泛化性能，并防止过拟合。所以，如果你看到这里有一层dropout，然后还有一层dropout。在dropout层之后，

我们有一个跳跃连接或捷径连接。捷径连接的作用是帮助梯度通过另一条路径流动，确保不会出现梯度消失问题。然后经过归一化处理，接着是多头注意力机制和丢弃层（dropout），之后还有另一个归一化层，其功能与第一个相同。随后是一个前馈神经网络，这也是Transformer模块中非常重要的组成部分。具体来说，当前我的词元（token）维度是768，前馈神经网络实际上会将其映射到更高维的空间中。

这是768的四倍，然后它又将其压缩回768维空间。这种扩展-收缩机制确保我们探索更丰富的空间。我们探索的是具有更多维度和更多参数的空间，从而确保我们的语言模型拥有足够的参数来捕捉额外的复杂性。前馈神经网络正是深度探索中专家混合创新实际发生的地方。

最后我们还有一个dropout层，然后还有一个跳跃连接或快捷连接。记住这些加号，无论它们出现在哪里，都代表跳跃或快捷连接。它们确保梯度有替代路径流动，因为如果梯度以链式方式流动，一旦某个梯度很小，相乘后梯度就会变为零，或者如果梯度很大，相乘后就会爆炸。这可能导致梯度消失问题，使学习停止，或者导致梯度爆炸问题，使学习变得非常不稳定。因此，这五个标记必须经历这些不同的步骤。

每个标记都必须经历以下流程：先经过归一化处理，再通过多头注意力机制，接着是随机失活层和跳跃连接，然后再次归一化，随后通过前馈神经网络，再经过一次随机失活层，最后再进行一次跳跃连接。这就是Transformer模块的结构。如果你回顾我们在课程开始时看到的示意图，你会发现这里描述的完全一致——每个标记都必须依次通过层归一化、注意力机制、随机失活、跳跃连接、前馈神经网络的层归一化、随机失活以及跳跃连接。这就是每个标记必须遵循的完整路径，看起来确实是个相当繁琐的过程，对吧？

首先，我必须完成这五个步骤才能拿到我的制服，除此之外，之后我还得逐一检查每个变压器模块，并完成这些步骤。但还有一层额外的复杂性——就像这个变压器模块一样，一个大型语言模型包含多个变压器模块，对吧？比如GPT-2，它有多少个变压器模块呢？如果你看一下GPT-2本身，GPT-2小型有12个变压器模块，GPT-2中型有24个，GPT-2大型有36个，而GPT-2 XL则有48个。所以，即使我们现在只看小型模型，每个变压器模块都包含所有这些步骤。

所以现在每个token基本上都要重复这12个步骤 嗯哼。因此这就是为什么——如果你把一个transformer块比作火车的一节车厢——总共有12个这样的transformer块相互连接，而每个token都必须完整通过这12个transformer块。整个旅程变得极其繁琐。你看我已经把流程画出来了对吧？这12个transformer块就是每个token必须穿越的关卡。就像这位token朋友，它得先通过第一个transformer块...

它必须经过第二个。同样地，它必须经过第三个，以此类推，必须经过这全部的12个变压器模块。所以这里的第12个模块就像一段非常繁琐的火车旅程，每个标记都必须严格遵循这一路径。获得统一是一项艰巨的任务，我们需要经历五个步骤。而通过处理器的过程更是艰难，因为你必须再次经历这全部的12个步骤。这就是处理器中实际发生的事情，也就是我们刚才在处理器中看到的第二部分。

情况是这样的：我这里展示了一个Transformer模块，对吧？如果我们使用GPT小模型，你可以想象这个模块被复制12次；如果使用最大的GPT-2模型，则需要48个这样的Transformer模块。而现代GPT模型可能包含96个甚至更多的Transformer模块。因此，每个token现在都必须通过这些模块进行处理。请记住，token的维度通常在通过Transformer处理后仍保持不变。比如说输入——我们之前看到的统一输入是768维的，如果你还记得的话，那是一个768维的向量，对吧？

在经过所有这些Transformer块后，经过12个Transformer块后，它从这12个Transformer块中出来时保留了其维度。因此，它仍然有768个维度。所以现在，一个真正的朋友会接受你已经从Transformer块中出来，并且它们自然仍然都有768个维度，当然，值已经发生了变化，对吧？然后它们现在进入输出层。这里有一个归一化步骤。所以如果你在这里看到，这里有一个归一化步骤，被称为最终层归一化。

所以归一化这一步在这里被提到了。每一个768维的向量都会再次经过这个归一化阶段，然后我们还有最后一层，这层非常重要。现在记住我们已经到达了最后一层，我们有一个真实的，所以这是一个真正的朋友需要你，每一个都是768维的向量，对吧？现在我们需要以某种方式将这768维转换成我们的词汇量大小，也就是50,000，因为现在我们需要预测下一个标记。

那么每个标记本质上都会通过一个神经网络，其大小为50,000，或者说大小为768乘以50,000。因此，当这些向量与之相乘时，每个标记会生成50,000维的向量。所以现在每个标记的大小是——一个真正的朋友需要你——对吗？每个标记在经过输出层后的大小将是50,000。这一层也被称为输出投影层，每个标记经过输出投影层后，其维度等于词汇表的大小。

记住我们的词汇量是50,000。这个50,000来源于词汇量，我来解释为什么需要维度相等。词汇量和最后一步是选择下一个标记，对吧？现在我们到达最后一步时得到了什么？我们有五个标记或"真正的朋友接受你"，每个标记都有一个50,000维的向量。接下来我们要做的是查看这50,000个维度，找到具有最高值或最高概率的索引。

然后我们会在这里找到那个索引，接着寻找它对应的标记。就是这样。所以如果这里的索引是第一个“The”，那么“a true friend accepts you”在这里有多个输入输出任务。当输入是“O”时，输出应该是“true”；当输入是“O true”时，输出应该是“friend”；当输入是“O true friend”时，输出应该是“accept”；当输入是“O true friend accepts”时，输出应该是“you”；当输入是“O true friend accepts you”时，输出会是其他东西，也就是“for”。所以“a true friend accepts you”，如果你看这个句子并且要预测下一个标记，这不仅仅是一个下一个标记的预测任务。

同一句话中包含多个输入-输出任务。当O作为输入时，真实值应为输出等，这些输入-输出预测任务是什么？而这里唯一与我们相关的是最初的下一词预测。当然，一开始我们不会得到好的词元，对吧？但我们会有一个基于实际值的损失函数。

所以这就是实际的下一标记，也就是我想要的，但最初预测的标记会完全偏离。这时反向传播就派上用场了，当所有参数都存在时，它们实际上会被优化。我们稍后会讲到这一点。但现在，让我再解释一下最后一步。我们有了每一个标记，现在每一个标记都与一个不同的均匀分布相关联，其维度为50,000。为什么维度是50,000呢？因为我们必须为这里的每一个单词预测下一个标记。所以对于O，我们必须预测下一个标记。

所以我们查看那个索引，也就是具有最高概率的标记ID。我们查阅词汇表或标记ID列表，然后进行反向映射，找到对应那个标记ID的单词。比如，如果这里的标记ID是555，或者这个标记ID是5000。我在这里找到5000对应的单词，理想情况下我希望这里对应的是"true"。但在初始阶段模型未经训练时，可能对应的是"for"。因此实际预测结果可能是"for"。同样地，我也会得到"true"的实际预测结果。

也许这里的最高令牌ID是你的朋友，然后我会在这里获取最高的令牌ID，这就是我如何预测每个令牌的下一个令牌。然后我会找到实际值和预测值之间的最后一项。这就是整个LLM架构的结构。所以现在如果你去输出层，也就是我的最后一层，你会看到我们有两个相互链接的东西。

好的，我们有了最终的层归一化层，它连接到输出层，然后我们有了用于下一个令牌预测的矩阵。这个逻辑矩阵就是这个，这个就是我刚才展示给你的那个，我们用它来进行下一个令牌的预测。现在你可能会想，这里优化的所有参数是什么。从一开始，就是这些存在的令牌嵌入值。

我们无法先验地知道这些参数。因此，让我用星号标记那些需要训练的参数——这些是我们无法预先知晓的。所以这些是需要训练的位置嵌入分配参数，我们无法预先知晓。因此这些都需要经过训练。Transformer模块的每一个组成部分都包含需要训练的参数：多头注意力机制有需要训练的参数，前馈神经网络也有需要训练的参数。而且这样的模块有12或24个，这进一步增加了参数量。因此在整个流程中存在着海量需要训练的参数，甚至最后的神经网络层也是如此。

它有这么多需要训练的参数，所有这些参数加在一起构成了参数总数，达到1750亿甚至可能上万亿。回想一下我们最初讨论的引擎，对吧？我们一开始只是知道并思考：好吧，这是个LLM引擎。但LLM引擎究竟如何运作？LLM引擎底层的参数是什么？这1750亿个参数究竟分布在哪里？现在我们已经深入了解了其详细架构——包括输入部分、处理器部分和输出部分——并追踪了一个单标记的完整旅程。

在令牌处理过程中，首先会经历输入阶段，此时它被隔离。它会被分配一个令牌ID或徽章，然后接受一组768个问题的测试（即令牌嵌入，用于编码含义）。接着，它会接受第二组问题，即位置嵌入，用于编码其位置值。我们将令牌嵌入和位置嵌入相加，得到输入嵌入，也就是每个令牌的统一表示。凭借这种统一表示，不同的令牌会被送入Transformer模块进行处理。

每个Transformer块基本上包含归一化层、多头注意力、dropout、再次归一化、前馈网络和dropout，其间穿插着两个跳跃连接。在GPT-2中，有12个这样的块；在GPT-2 XL中，我认为有48个这样的块；而在更高级的GPT模型中，可能会有96个甚至更多这样的块。因此，每个token都需要经过所有这些块，当它从所有这些块中出来时，其大小仍然保持768。

然后，它还会经过一个归一化层，其尺寸仍为768。最后，我们有一个输出层，其中每个标记都会被转换为一个大小为50,000的向量，这个大小等于词汇表的大小。接着，我们基本上会查看每个标记的50,000维向量，然后找出概率最高的那个标记ID，并用它来预测下一个标记。因此，在一个序列中，我们会有多个输入输出的预测任务。

因此，如果我们有一个包含五个标记的序列，就会有五个输入输出预测任务，这些任务实质上构成了我们的损失函数。然后我们的损失函数基本上会进行反向传播，所有的参数都会被优化——这1750亿个参数分布在多个环节：标记嵌入层中有参数，位置嵌入层中有参数，Transformer块的多个组成部分中有参数，输出层中也有参数。

这些参数都是通过反向传播进行优化的，最终我们得到的是一个对语言本身有直觉的模型，它还能预测下一个标记。正如你在这里看到的，下一个标记预测就是我们的任务。我们预测下一个标记，并与实际值进行比较。这就是任务，但由于我们有这么多参数，这个任务的副产品就是学习语言本身。所以在今天的讲座中，

我的主要目的是带你了解一个标记（token）的旅程。试着从单个标记的角度出发，思考会发生什么。试着打开这个引擎，打开大型语言模型（LLM）的引擎，真正去观察这个引擎是如何运作的。我希望我已经向你传达了这一点。我之所以构建这个类比或标记旅程的故事，是为了让你真正理解LLM架构内部发生的事情。因为如果不理解这一点，我们就无法继续前进到下一部分——注意力机制。

现在的计划是，在下一讲中，我将首先阐述为什么我们需要注意力机制。然后我们会探讨自注意力机制，接着是多头注意力机制，最后是键值缓存。所以，如果你看到接下来的计划，首先是注意力机制的必要性，然后是自注意力机制，最终是多头注意力机制。正如我提到的，所有未来的课程都已经详细规划好了，这将不是一个只有5到10分钟视频的简短系列。

本系列的每一期视频都会比较长，大约40到45分钟，我计划详细讲解所有步骤。"多头潜在注意力"是一个非常重要的概念，但我希望大家在真正理解这个概念时能保持同步。非常感谢大家，我真的很期待在下一讲中见到你们。请和我一起做笔记。这个系列可能会有点难度，我正试图以尽可能容易理解的方式来提炼这些概念，但过程中可能还是会遇到一些挑战。

所以请大家做好笔记，以巩固所学概念。谢谢大家，期待在下一次讲座中见到你们。大家好，我是Raj Dhandekar博士，2022年从麻省理工学院获得机器学习博士学位，也是“从零开始构建深度探索”系列的创始人。在开始之前，我想向大家介绍本系列的赞助商和合作伙伴——InVideo AI。大家都知道我们非常重视从基础构建AI模型的内容，InVideo AI的理念和原则与我们非常相似。

让我来为你展示一下。这里是InVideo AI的网站，凭借一支精干的工程团队，他们打造出了一款令人惊叹的产品——仅需输入文字指令就能生成高质量AI视频。正如你所见，我输入了一段文字指令："制作一款超写实的高端奢侈腕表广告视频，呈现电影级质感"。点击生成视频后，很快我就获得了这段令人惊艳的成片，其逼真程度令人叹服。最让我震撼的是视频对细节的把握，看看这个画面，材质纹理的表现简直不可思议。而这一切都源自于短短的文字指令，这就是InVideo产品的强大之处。

你刚刚看到的精彩视频背后，是InVideo AI的视频创作流程在支撑。他们正从底层原理重新构想视频生成与剪辑方式，致力于基础模型的实验与优化。该公司拥有印度规模最大的H100和H200计算集群之一，同时也在测试B200芯片。作为印度发展最迅猛的AI初创企业，InVideo AI正为全球市场打造产品，这正是我如此认同他们的原因。好消息是，他们目前有多个职位正在招聘。

你可以加入他们出色的团队，我会在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建深度搜索”系列的下一个讲座。今天我们将学习一个非常重要的概念，那就是注意力机制的必要性。首先，让我快速总结一下我们在这个系列讲座中已经涵盖的内容，然后再来讨论今天的主题。在之前的两节课中，我们学习了名为“深度搜索基础”的内容，

我们探讨了深度探索架构的四个阶段，或者说我们将在这个系列讲座中涵盖的四个阶段。第一阶段是深度探索架构的创新。第二阶段是训练方法。第三阶段是GPU优化技巧，第四阶段是模型生态系统。在上一讲中，标题为LLM架构。

我们开始特别关注第一阶段。我们的主要目标是，在接下来的两到三节课中，必须开始理解多头潜在注意力机制，这是深度探索架构中的第一个重大创新。然而，要理解MLA（多头潜在注意力），我们不能直接开始研究这个概念，而是需要循序渐进地接近它。因此，我们首先研究了LLM本身的架构，发现LLM的架构大致是这样的。

首先我们有了输入部分，然后有了处理器，接着是输出部分。每个部分基本上都有不同的构建模块。所以我们发现，如果把LLM看作引擎的话，要真正理解这个引擎如何运作，我们确实需要打开引擎，看看里面到底有什么。

那么，大型语言模型（LLM）究竟是如何学会预测下一个标记或下一个词的呢？如果把这个过程类比为汽车的运动，我们会发现，当你给汽车加油时，汽车就会动起来，对吧？引擎里发生了一些变化，汽车就移动了。同样地，对于LLM来说，"燃料"就是你输入到引擎（即LLM）中的单词序列，而输出则是下一个词的预测或下一个标记的预测。这就是为什么我们也把LLM称为"下一个标记预测引擎"。

如今这个引擎拥有海量参数。GPT-3的参数量达1.75亿个，GPT-4可能达到约1万亿个，而最新发布的GPT-4.5参数量可能在5到10万亿之间。为了真正理解这个引擎的运作原理，我们打开了这个黑匣子，从以下三个维度进行了剖析。

但我并没有直接向你解释这三个方面。我告诉你，如果你把自己想象成一个令牌或一个世界，并想象自己经历不同的阶段会怎样。所以如果你是一个令牌，首先你会被分配一个令牌ID，那是你的徽章，然后你会被分配一个令牌嵌入向量，接着是一个位置嵌入向量，令牌嵌入和位置嵌入相加，就形成了你的制服——输入嵌入。

同样地，无论你的邻居是谁，他们也都有自己的统一输入嵌入表示，你们一起搭乘通往Transformer的列车。每个Transformer块包含多个组件，如归一化层、多头注意力机制、随机失活层、跳跃连接、又一个归一化层、前馈神经网络和另一个随机失活层，最后再接一个跳跃连接。而这仅仅是一个Transformer模块的结构。

像这样的Transformer模块有很多，可以是12个、24个、96个等等。因此，作为一个标记（token），连同你的统一表示（即输入嵌入），你必须经过所有这些Transformer模块的处理。当你通过所有模块后，会有一层归一化处理，最后是一个输出层。如果你是一个768维的向量，你将被映射或投影到一个50,000维的向量中。为什么是50,000？因为这是词汇表的大小，这实际上帮助我们选择下一个标记。

这就是令牌在LLM架构中的完整生命周期。在今天的讲座中，我们将重点讨论整个架构中的一个单一环节——多头注意力机制。我希望你们能理解多头注意力机制在其中的作用位置。在所有这些步骤中，令牌会经过多头注意力机制，它出现在Transformer块中，而且在这个特定的情况下，它位于归一化层之后。今天，我们将要理解多头注意力机制，但在此之前，我们首先要理解什么是注意力本身，以及为什么需要注意力机制。我们为什么要开始讨论“注意力”这个术语？为什么它在最近变得如此流行？因此，我们今天的目标是激发自注意力这个概念。

我们今天不会深入探讨自注意力机制的数学原理。今天的核心目标是理解为什么我们首先需要注意力机制，以及它为何能成为大型语言模型的关键突破点。试想一下，在所有处理词元的步骤中，最重要的环节就藏在Transformer模块里——确切地说，就藏在Transformer模块的某个步骤中，那就是注意力机制。

这就是为什么我在这里用不同的颜色标记出来。这个训练模块本质上提供了所有使LLM在理解语言方面表现优异的特性。因此，为了真正理解注意力机制的工作原理，我专门为你们准备了这堂单独的课程，我们将尝试阐述引入注意力机制的必要性。

在理解我们为何需要注意力机制以及它如何改变了这一领域之前，让我们先深入探讨一下生成式人工智能本身。让我们先回顾一下历史。我认为理解注意力机制的本质至关重要。20世纪60年代有一个叫ELISA的聊天机器人。你可以想象一下。我不会称它为大型语言模型。

我会称它为聊天机器人。这是第一个旨在充当治疗师的自然语言处理聊天机器人。所以如果你问“请告诉我你有什么困扰”，我可以说我在学习人工智能方面遇到了困难。你能帮我吗？你觉得学习人工智能很困难是正常的吗？你看它其实没那么有用，但要记住那是在1960年代。那时候这被认为是一场革命。所以你可以看到这个原始程序是由约瑟夫·维森鲍姆在1966年描述的。

与此相比，看看现在的ChatGPT，我们说我想学习人工智能。你能帮我吗？然后让我把模型切换到GPT 4.0，这样我能得到更快的回复。接着你会看到，我立刻获得了一份可操作的项目清单，可以立即开始实施来学习人工智能。因此，在短短64年的时间里，我们在自然语言处理和语言理解领域已经取得了长足的进步。

让我们来看看从20世纪60年代到2020年代发生了什么。20世纪60年代是ELISA聊天机器人问世的时期。然后在20世纪80年代和1997年，出现了生成式AI或语言建模领域的两个重要基础构建模块，你可以把它们看作是20世纪80年代出现的循环神经网络和1997年问世的长短期记忆网络。

这两者都建立在20世纪70年代神经网络蓬勃发展的基础上。人们发现神经网络可以做很多了不起的事情，但神经网络真正的问题在于它们本质上无法处理记忆，这是神经网络最大的缺陷。为什么必须处理记忆呢？因为想象一下，如果你要预测下一个单词是否正确，或者生成一些新文本，你会想知道段落的开头是什么。

仅仅记住一点信息是不够的。上下文非常重要，这个词在今天的讲座中会频繁出现。上下文本质上意味着，如果给模型提供了一大段文字，而我们希望模型执行某项任务或仅通过查看句子来回答问题，仅靠句子中紧邻出现的单词是没有帮助的。

我们需要先了解这段话开头所讲述的背景。比如说，我先提到我来自印度浦那，然后接着写了一大堆不同的内容。我写了一大堆句子，最后才问我说的是什么语言。现在要根据这个问题生成一个回答。中间提到的所有这些内容并不是很重要，但我需要了解这段开头的内容，因为这才能真正帮助我回答这个问题。这就是为什么需要记忆功能。而神经网络原本并没有真正编码记忆的机制，但循环神经网络和长短时记忆网络解决了这个问题。让我快速展示一下它们是如何解决这个问题的。

因此，人们最初通常使用RNN或LSTM进行序列到序列建模，也就是所谓的序列到序列翻译任务。简单的序列到序列翻译任务可以仅仅是语言翻译。如果你想处理一串单词，并希望将它们从一种语言翻译成另一种语言，你可以使用循环神经网络。

所以循环神经网络有一堆隐藏状态，比如h0、h1、h2、h3。假设这些是输入单词"I will eat"，我想把它翻译成法语，即"Je vais manger"，已经写在这里了。我可以用循环神经网络进行这种翻译的方式是，我有一个编码器块和一个解码器块。

这些编码器和解码器块都有一个称为隐藏状态的东西，你可以将其视为向量或矩阵。这里是编码器的隐藏状态h1、隐藏状态h2、隐藏状态h3，然后我们有解码器的隐藏状态s1、隐藏状态s2和隐藏状态s3。让我们可视化编码器块中发生的情况。

在编码器模块中，首先输入句子的第一个单词"I"，它会转换为第一个隐藏状态h1；接着输入第二个单词。需要注意的是，要获得第二个隐藏状态h2，不仅要使用第二个输入，还要利用前一个隐藏状态。这就是循环神经网络捕捉记忆的方式。

前一个隐藏状态用于计算当前的隐藏状态。同理，当你来吃饭时，输入的是x3，但要计算最终的隐藏状态h3，你需要使用前一个隐藏状态h2，而h2编码了h1或者说h2包含了h1的信息。因此，h3现在既包含了h1的信息，也包含了h2的信息。

h3拥有关于过去的知识，它保留了记忆。因此，h3可以被视为一个向量，可能是500维或1000维的向量，本质上包含了输入的所有信息含义。这是唯一一个传递给解码器的高维向量。解码器随后会查看这个向量，解码器的每一个隐藏状态都开始对其进行解码。

所以解码器想象一下你有一个解码器，就像接力赛一样。我们可以把它想象成一场接力赛。接力棒在h1手中，它传给h2，h2再传给h3。现在h3拿到了接力棒，并将其传递给解码器。现在我是解码器的第一个隐藏状态。我拿到了接力棒。

我先解码第一个标记JOR，然后将其传递给第二个解码器，接着解码第二个标记，最后解码第三个标记。这就是循环神经网络中语言间翻译的工作原理。LSTM（长短期记忆网络）在预测当前隐藏状态时与前一个隐藏状态有所不同。它们还维护一种称为细胞状态的结构。因此，LSTM能够兼顾长期记忆和短期记忆。

所以这本质上是为了扩大它们处理的上下文窗口。但架构从根本上保持不变。只是对于LSTMs来说，底层的数学要复杂得多。现在你看出问题了吗？看看这个例子，我们之前用它来讨论上下文问题。那么，当处理大段文字时，循环神经网络存在什么问题呢？比如说你有一大段文字，你想翻译整段内容。

循环神经网络在处理这个问题时存在什么缺陷呢？让我们举一个简单的例子，以便更清楚地解释这个问题。假设你眼前有这样一段文字——这是我随机从ChatGPT生成的文本中截取的。我希望你这样做：首先花30秒到1分钟或更长时间完整阅读这段文字一次。然后闭上眼睛，将它翻译成你想要的任何语言。

你可以把它翻译成印地语、西班牙语、法语、德语，或者你会的任何其他语言。所以，闭上眼睛，把它翻译成另一种语言。你能做到吗？这不可能，对吧？因为当你闭上眼睛时，你基本上只能记住你读过的一些概要。

你并不需要记住每一个确切的单词，但为了翻译，我们不得不逐字逐句地看并单独翻译，对吧？这正是循环神经网络所面临的问题。为什么呢？因为我们只依赖一个隐藏状态或一个向量来捕捉过去发生的所有上下文信息。

这就像你闭上眼睛，试图回忆整段内容。这个隐藏状态或向量承受着巨大的压力，要重新唤起所有记忆——如果是一大段文字，你怎么能把所有信息压缩进一个500维或1000维的向量里呢？自然会有信息丢失。而且别忘了，解码器并不会从第一个或第二个隐藏状态获取输入。

它仅将最终的隐藏状态作为输入。这就是主要问题所在。从编码器传递到解码器的上下文仅来自最终的隐藏状态。这就是为什么这被称为上下文瓶颈问题。最终的隐藏状态就像你闭上眼睛试图记住整段文字时的情况——这是不可能做到的。你不能给一个向量施加这么大的压力。

循环神经网络中的上下文瓶颈确实不利于保留长距离上下文，这为理解注意力机制的出现提供了很好的途径。现在，如果你看一下循环神经网络，让我在这里擦掉一些东西。你认为这个问题的理想解决方案是什么？假设这就是我们目前遇到的问题——所有上下文信息无法被压缩到单个向量中。

这个问题的解决方案是什么？嗯，解决方案似乎是这样的：比如说，当我们在解码时，当我解码这个的时候，假设我在解码器的第一个隐藏状态下进行解码，与其只能访问最终的隐藏状态，不如让我也能访问编码器的所有隐藏状态以及所有的输入。

因此，如果在解码过程中我能获取所有输入，那么我就可以做出预测，比如在解码第一个标记时，我应该更重视第一个隐藏状态，而不应该重视第二个隐藏状态，也不应该重视第三个隐藏状态。如果我能够创建一种机制，能够理解对不同标记需要赋予的相对重要性，那会怎样呢？

这是本次讲座最重要的部分，请大家务必集中注意力。解决上下文瓶颈问题的关键在于：在解码过程中，我需要建立一种机制来量化每个隐藏状态应获得的注意力权重。比如在首次解码时，我可以给第一个隐藏状态分配80%的权重，第二个和第三个隐藏状态各分配10%的权重。

如果我掌握了这些信息，那么上下文瓶颈问题就迎刃而解了。因为即使段落很长，只要我能关注到该段落中的所有标记，我实际上也能重视之前出现的内容。比如说这是句子，我在这里预测某个内容。在注意力机制中，我可以尝试确定应该给予第一个标记、第二个标记、第三个标记以及整个段落多少注意力，然后我可以说我需要对这些标记给予最大的关注。

所以你看，我已经开始在这里使用“注意力”这个词了。你可以把它理解为对段落中各个标记的相对重要性。因此，理想的解决方案是我们在解码过程中需要有选择地访问输入序列的部分内容。

因此，当你在解码时，比如说我想做出某些预测，这样如果我在解码第一个标记，对吧，这个alpha 1 1就是赋予第一个隐藏状态的相对重要性，alpha 2 1是赋予第二个隐藏状态的相对重要性，alpha 3 1则是赋予第三个隐藏状态的相对重要性。

所以我希望能做出这样的预测：α₁₁为100%，α₂₁为0，α₃₁为0，诸如此类。但你看这里，我不再仅仅依赖h₃，而是依赖之前所有的隐藏状态，不仅如此，我还在量化应该对每个之前的隐藏状态依赖多少。因此，这一点很重要，我们需要在解码过程中有选择性地访问输入序列的某些部分。我想通过这个例子再次向你们解释这一点。假设我在这里截了一张图，然后把它放在这里。让我们试着理解这句话的真正含义。如果我让你翻译这一段，你会怎么做？你可能会从——比如说这个部分开始，假设你从这里开始，看看你一次能看多少个单词。

假设这就是你的上下文窗口，你无法一次看到更多的单词，那么你的大脑会怎么做呢？对于你的大脑来说，段落中剩下的内容是不相关的，对吧？所以你会在大脑中屏蔽掉所有这些内容，你会屏蔽掉所有不相关的东西。因此，你会认为所有这些其他内容对你来说都不重要，你只想关注当前的上下文，并选择性地忽略其他所有内容，对吧？所以你的大脑会集中注意力——这一点非常重要——你的大脑会集中注意力于段落的这一部分，然后开始只翻译这一部分的内容。翻译完这一部分后，你的大脑才会移动到下一部分。

也许接下来的五个词翻译完后，你的思绪会转移到下五个词，然后再到下五个词，依此类推。所以你会看到自己在某个特定时刻的行为：你的大脑正在做出决定，当你在看第一个上下文窗口时，我想要对这些标记给予最大的关注。同时，你的大脑也在做出一些定量的判断，完全不去关注其余的标记，完全不去关注这些。这就是在解码过程中选择性访问输入序列部分的真正含义，对吧？选择性访问输入序列的部分——这是理解注意力机制最重要的一句话。

这意味着只关注输入序列中在当下重要的部分，这看起来像是一种直觉行为，对吧？但实际上，这是帮助语言模型变得更好的最重要因素。想象一下，你想在一段文字中找出拼写错误，你会怎么做？你会逐步浏览这段文字，先关注一个部分，然后再关注下一个部分，以此类推。这正是大型语言模型也需要做的——它们需要选择性地关注序列中的特定部分，然后找出例如含有拼写错误的那部分。

没错，这就是为什么我们在解码时需要注意力机制的核心原因——我们可以量化每个输入标记应该获得多少重要性或关注度，这就是我现在要表达的重点。实际上，第一个真正实现这一点的论文并不是《Attention Is All You Need》。你看这篇论文（《Attention Is All You Need》）目前约有117万次引用，引用量惊人。很多人误以为注意力机制最早是在这篇论文中提出的，但实际上首次提出注意力机制的论文是这篇《Bahdanau Attention Mechanism》。

所以如果你搜索“Bahdanau Attention”并点击这个，你会发现这是第一篇真正将注意力机制应用于翻译任务的论文。我认为这篇论文发表于2014年，并在2015年的ICLR会议上发表。他们的主要目的是实现了这种架构，使我们能够选择性地决定对每个隐藏状态分配多少注意力，这就是所谓的a1、a2、a3——注意力分数。基于此，你可以执行翻译任务，他们基本上证明了如果我们使用注意力机制，可以以更好的方式进行序列到序列的翻译。所以请记住这篇特别的论文。

所以在这里你可以看到，在这个图表上，x轴是英语句子，y轴是法语翻译。对角线基本上显示了英语单词最关注翻译的法语单词。有趣的是，顺序并不相同。比如"European Economic Area"（欧洲经济区）在英语中是这么说的，但翻译成法语时变成了"zone économique européenne"（经济欧洲区），所以它被翻译为"zone économique européenne"。

所以这并不是逐字逐句的直接翻译，但注意力机制本质上能识别出——看，这里是最亮的对吧？这意味着法语中的"european"对应英语中的"european"，尽管它们出现的位置并不相同。英语的"european"出现在第五个位置，而法语的"european"出现在第七个位置。因此，所有最亮的区域并不都在对角线上，有些偏离了对角线本身——这些细节也都被注意力机制捕捉到了。

这个图表最酷的地方在于这些非对角线元素，因为翻译并不总是逐字进行的。在某些语言中，有些词会先出现，而在其他语言中则后出现，这取决于名词、词汇等的排列方式。这就是这篇论文的亮点所在，这是第一篇将注意力机制应用于翻译任务的论文。随后，《Attention Is All You Need》这篇论文引入了Transformer模块，并将注意力机制整合到了Transformer模块中，这是该论文的主要优势或独特之处。

是这样的，RNN（循环神经网络）和LSTM（长短期记忆网络）在语言翻译方面表现不错，但它们存在上下文瓶颈问题。于是研究人员发现，构建深度神经网络其实并不需要RNN架构。2014年，Bahdanau注意力机制被提出，他们在保留RNN的同时引入了注意力机制。你可以把Bahdanau注意力机制中的编码器-解码器模块想象成类似结构，只是额外添加了这个注意力机制。三年后（也就是2017年），研究人员进一步发现，即便是RNN架构，在构建自然语言处理的深度神经网络时也并非必需，于是他们提出了Transformer架构。

那么让我们回顾一下这个领域的发展历程：20世纪80年代是RNN的时代，1997年诞生了LSTM（其实1966年就出现了ELISA——让我把这个也写上）。到了2014年，注意力机制被提出，但当时仍依附于RNN的编码器-解码器架构。真正的转折点在2017年，研究人员发现自然语言处理任务其实不需要RNN结构。于是RNN被彻底摒弃，只保留了注意力机制的核心思想，但这次它与Transformer模块相结合——这正是2014年论文与2017年论文的本质区别：2014年的架构仍保留RNN模块，而2017年则用Transformer架构完全取代了它。

于是这就形成了完整的架构——现在我们有了Transformer模块，其核心就是注意力机制。这就是GPT的架构，它与原始Transformer论文中的架构不同。原版Transformer同时包含编码器和解码器，而我现在展示的这个架构只有解码器部分。目前不必对此感到困惑，今天课程的主要目的是让大家从自然语言处理的发展历程中理解为何需要引入注意力机制。关键要记住的一句话是：在解码过程中，我们需要选择性地访问输入序列的特定部分。最初，注意力机制是在RNN中率先提出的。

后来研究人员发现，好吧，让我去掉RNN，但仍然尝试将注意力机制融入其中。于是他们在2017年将其与这里的Transformer模块合并。到了2018年，GPT架构问世，于是就有了注意力机制加GPT。GPT基于原始的Transformer架构，但与同时包含编码器和解码器模块不同，它只保留了这里的解码器模块，并保留了注意力机制。我们之前讨论过为什么需要注意力机制，这一点在LLM架构的这个部分得到了体现。

现在我们将开始探讨：如果你观察下一个标记预测任务，自注意力机制究竟是什么？上下文向量又是什么？注意力机制的主要目的是什么？那么，让我们来看看自注意力机制在下一个标记预测中的主要目的是什么。既然我们已经理解了注意力机制及其发展历程，现在就来学习这个概念吧。让我们试着理解“自注意力”这个术语，它实际上意味着什么？

自注意力机制意味着它是一种让输入序列中的每个位置都能关注到同一序列中所有位置的机制。也就是说，如果我有一个句子比如“明天是晴天”，自注意力本质上指的是，到目前为止如果你看RNN，我们看到的是解码器需要给予编码器的注意力。所以如果第一个解码词是法语词，我们基本上是在看两个不同的序列之间的关系。假设英语序列是这样的，比如“我会吃”，法语序列是这样的，这是法语序列，我们在看如果你在做第一次解码，你应该给予英语序列多少注意力。

因此，这里的注意力机制是在序列之间进行的，而不是在同一个序列内部。而自注意力机制则是在预测下一个标记时使用的，这通常是大型语言模型（LLM）的做法。由于LLM的任务是预测下一个标记，它们并不是专门为翻译任务而训练的。在预测下一个标记时，本质上你并不需要区分不同的语言，你只需要处理一堆数据即可。

因此，与其在两个序列之间建立注意力机制，你只需取同一个句子，比如说，当你观察下一个词时，你试图找出：如果关注一个标记或一个词，我应该对句子中所有其他标记给予多少注意力。这是这里最重要的理解点。如果你关注一个标记或一个词，那么邻近的词对这个特定标记有多重要？为什么这个知识如此重要？你能试着思考为什么这个知识对我们至关重要吗？为什么我们需要编码一个给定标记周围其他标记的信息？

这一知识对我们之所以重要，是因为在预测下一个词时，本质上需要了解序列的上下文信息，需要掌握不同词语之间的关联。再以同一个例子说明，假设我说"我来自印度浦那，我会说"，就以这句话为例，当你看到"说"这个词时，我需要知道在"说"之后

必须高度关注浦那和印度，也许其他所有词汇都不那么相关，因为我所说的方言深受我所在地区的影响。因此，当你观察一个词汇云时，如果你的Transformer架构或LLM引擎掌握了某个词与周围其他词的关系信息，以及需要赋予这些周边词汇多少权重，那么仅凭一个标记就能非常非常容易地预测下一个标记。

这就是自注意力机制变得非常重要的原因。如果没有自注意力机制，你就会丢失关于其他标记与我们选择的特定标记之间关联的上下文信息。我希望现在你能理解为什么它被称为"自注意力"。在这个案例中，当我们处理序列到序列的语言翻译时，注意力机制作用于不同序列之间；而自注意力则是当我们观察单个句子本身时，关注句子内部的标记，并从根本上理解这些标记之间是如何相互关联的。

让我们再次以同样的例子来说明，第二天是光明的。记住，当这些标记进入转换器架构时，它们现在是向量，正如你之前所见，它们现在具有统一的维度。记住，每个标记都有一个统一的768维向量，这就是输入嵌入。所以，每当我在这里展示这些块时，本质上就是指一个向量。

因此，第二天是明亮的，现在这些都是向量，对于一个转换器来说，它不理解单词，也不理解句子，它只知道每个标记都是一个向量。所以“the”是一个向量，我称之为x1，“next”是一个向量，我称之为x2，“day”是一个向量，我称之为x3，“is”是一个向量，我称之为x4，“bright”是一个向量，我称之为x5。现在，如果我在看一个特定的词，比如我在这里展示的“next”，我想看看如果我关注这个词“next”，我应该给予其他所有标记多少注意力，而这个注意力是由alpha to 1给出的，或者我称之为符号alpha to 1。

为什么要关注第一个标记，因为下一个是第二个标记，我想找出第二个标记与第一个标记之间的注意力分数，即α1，这将是α2，这将是α3，这里将是α4，这里将是α5。本质上，如果我在看一个特定的标记，我想找出所有的注意力分数，这被称为查询，我现在关注的标记被称为查询标记，我想找出如果我在看查询，我应该给所有其他标记多少注意力，这些有时在通用术语中也被称为键，所以最终我想做的是，假设我得到了这些注意力分数，我想利用这些信息。

我想以某种方式获取所有这些信息，并将这个向量从输入嵌入向量转换为上下文向量。这里有一个非常重要的区别需要说明：目前next是一个输入嵌入向量，对吧？所以它包含了词元嵌入加上位置嵌入。但上下文向量是完全不同的东西。如果这是next的输入嵌入向量，而我在同一空间中绘制上下文向量，那么这就是next的上下文嵌入向量。实际上，上下文向量比词元嵌入向量丰富得多，为什么呢？

因为词嵌入向量或输入嵌入向量不包含相邻单词的信息，但现在我的上下文向量包含了相邻单词的信息，这些信息现在被融入到了我的输入嵌入中。所以，如果你有一个输入嵌入向量，也就是我之前提到的那个统一的向量，并且你用关于相邻单词的上下文信息来增强这个输入嵌入向量，我们将会看到这种增强是如何实现的，但本质上这会导致所谓的上下文向量。因此，注意力机制或自注意力机制的整个目标就是将所有的输入嵌入向量转换为上下文向量。

因此，所有这些统一的向量——我们刚才看到的这些统一向量，所有词元在经过归一化层后都会输出一个768维的统一向量。当它们进入多头注意力层时，输入的是嵌入向量，而输出的则是上下文向量。经过注意力模块处理后，输出的内容会丰富得多，这就是为什么我用不同颜色标记它们。之所以更丰富，是因为现在它还编码了其他词元的信息，保留了上下文。因此，上下文向量是一种经过丰富的嵌入向量，它融合了所有其他输入元素的信息。在自注意力机制中，上下文向量扮演着至关重要的角色。

它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住：输入嵌入向量仅包含该单词或标记本身的信息——它可能编码了该单词的含义及其位置信息，但对相邻元素一无所知。而上下文向量则能感知相邻元素，因为邻居关系至关重要。试想，当你阅读一个句子或段落时，单个标记本身毫无意义，正是它们与相邻元素的关联关系才最终构成了段落的上下文语境。对于大语言模型而言，这种机制之所以必要...

需要理解句子中词语之间的关系和关联性，这实际上是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个关键转折点——距今十年前——当注意力机制被引入后，人们开始意识到：与其孤立地看待词语，不如退后一步，尝试理解不同词语之间本质上是如何相互关联的。这样我们就能从文本中挖掘出最丰富的内涵，因为就像图像由像素模式构成那样，只有当您将所有词语作为一个整体并观察它们之间的关联时，文本或段落才具有真正的意义。

那么现在的问题是，你已经有了一个输入嵌入向量，比如说对于下一个词，如何将其转换为上下文向量？也就是说，你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频片刻，好好想一想。你有了输入嵌入向量，假设还有这些注意力分数，你会如何修改输入嵌入向量，使得这些注意力分数能够被纳入考虑，从而得到上下文向量？你可以在这里暂停一下，首先也可以思考一下这些注意力分数本身是如何计算的。

好的，最简单的做法是这样的：假设你有一个向量，还有其他所有向量，我们为什么不简单地做个点积呢？比如你有“next”这个词的输入嵌入向量，还有“though”的输入嵌入向量，就在这两个向量之间做个点积，这样就能得到α2,1；然后在“next”和“next”之间做个点积，得到α2,2；接着在“next”和“day”之间做个点积，得到α2,3；再在“next”和“is”之间做个点积，得到α2,4；最后在“next”和“bright”之间做个点积，得到α2,5。一旦你得到了所有这些α值，

你可以简单地计算 alpha 2 1 乘以 x 1 加上 alpha 2 2 乘以 x 2 加上 alpha 2 3 乘以 x 3 加上 alpha 2 4 乘以 x 4 加上 alpha 2 5 乘以 x 5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你这里有一个向量 v1，另一个向量 v2，它们之间的点积会比比如说 v1 和 v3 之间的点积更高。所以如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的东西。你可能会想，我希望量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，当然它们应该有一个更高的注意力分数，所以这个计算对我来说似乎是有道理的。

我只是计算点积，然后用各自的点积缩放不同的向量并将它们相加，所以无论这个和是多少，它就会成为下一个词的上下文向量。同样，我也可以为所有其他标记找到上下文向量。这种方法有什么问题？为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们试图编码的上下文。我希望你从基本原理出发思考，我很快就会揭晓答案。好的，主要的答案是，假设你考虑这个句子“狗追球但没抓住”，对吧，

狗追着球跑，但没能抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。好的，如果“它”现在是我的查询向量，你是如何决定计算查询向量与其他向量之间的注意力分数的呢？你决定使用点积对吧，这正是我要做的。如果“它”是我的查询向量，为了得到它与“狗”之间的注意力分数，我只需计算“它”与“狗”的点积。如果我计算点积，结果是0.51；如果我再计算“它”与“球”的简单点积，结果也是0.51。你看出来问题了吗？这两个注意力分数完全相同，但这并不是我想要的。当你说“但没能抓住它”时，

实际上这里指的是球，对吧？狗追着球跑但没抓住，所以第二个"它"指的是球而非狗。因此我在看这句话时需要更关注球而不是狗，让我用不同颜色的笔标注一下以便更清晰——我必须把注意力重点放在"球"上而非"狗"，但现实情况并非如此。如果简单做点积运算，算法机制里并没有设置让"球"比"狗"获得更高权重的编码规则，当我们处理这句话时，"狗"和"球"不应该获得同等的注意力分数。这个例子精彩地论证了为什么需要对不同词元进行选择性关注——"狗追着球跑但没抓住"。

第一个词是“the”，所以如果这是查询标记，它会更多地关注“dog”；但现在这是查询标记，它应该更多地关注“ball”。我不希望两者具有相同的注意力分数，因此简单的点积无法区分这里微妙的上下文关系，它没有考虑“chase couldn't catch”的上下文或语言上的细微差别，比如“catch”更可能指的是移动的物体，也就是“ball”。所以主要问题是，简单的点积只能衡量语义相似性，但无法处理上下文问题，而且许多句子可能都有这样的上下文复杂性，对吧？我需要编码一种机制，以便能够捕捉这些复杂性，但我不知道这种机制会是什么。于是我们采用了研究人员长期以来使用的技巧。

如果你不明白事物之间的内在联系，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。注意力机制领域的情况正是如此，研究人员基本上无法确定那个机制可能是什么。这就是机器学习领域或深度学习领域与物理学不同的地方，对吧？

在物理学中，如果你遇到这个问题，可能会花上六个月到一年的时间去研究一个定律，以揭示其背后的机制来捕捉复杂性，或是建立一个能捕捉背景的机制。但在深度学习领域，你不需要这么做。你会说，我要用一堆矩阵来代替它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？

所以他们发明了新的矩阵，我们暂且称之为查询矩阵和键矩阵。这意味着，与其仅仅查看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。比如，如果这里的查询是“it”，我就会用所谓的查询矩阵与之相乘。这个矩阵可以是一个高维矩阵。对于“dog”来说，“dog”和“ball”就是键，因为键本质上就是除了查询之外的所有其他标记，也就是“dog”和“ball”。

所以你用键矩阵将它们相乘。现在看这里的优势在于，如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵具有任何特定含义——你只需随机初始化它们，然后通过反向传播来训练它们。这正是研究人员长期使用的深度学习技巧：当你无法自行推导出关系时，就退后一步，让神经网络来完成它的工作。与其通过强加某些规则来限制神经网络，不如让它自己找出答案。

现在你看到了优势所在，我们掌握了多个可训练的因素。假设WQ是3×3的矩阵，WK也是3×3的矩阵，对吧？比如“狗”和“球”这些就是我的键值。而这些是它们的嵌入向量，也就是我们之前看到的输入嵌入。现在，我将这些输入嵌入与查询相乘，将这两个与键相乘。具体来说，就是3×3的矩阵乘以3×1的向量，结果会是一个3×1的向量。这样一来，键值就变成了0.9、0.2、0.1和0.1、1.8、0.1。你会发现这些值发生了变化，因为它们与矩阵相乘了。而查询部分也会与查询矩阵相乘。

因此，针对它的查询将变为0.5 0.1和0.5 1.0以及0.1。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正在从输入嵌入空间转换到一个不同的空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的。因此，如果你计算“它”和“球”之间的注意力分数，你会发现是0.56，“它”和“球”之间的分数是0.96。而如果你计算“它”和“狗”之间的注意力分数，结果是0.56。所以在这里，你可以看到“它”和“球”之间的注意力分数是0.96，比“它”和“狗”之间的分数要高，后者较低。

所以这些显然是不同的注意力分数，因此添加这些可训练的矩阵实际上对我们有帮助。为什么会有帮助呢？因为它提供了一些可调整的参数，使我们能够编码标记之间的一些复杂关系。如果你采用简单的点积，注意力分数将是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，将在下一节课中介绍），本质上，如果你只有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多可以调整的参数。

所以如果你在这一部分感到困惑，让我再重复一遍：我们开始这一部分时思考的是，如果你有一个输入嵌入向量，那么你可以对这个输入嵌入向量做什么来得到上下文向量？为了得到上下文向量，我们本质上需要阿尔法值。得到阿尔法值后，你只需要将它们与输入嵌入向量相乘，就能得到上下文向量。但接下来的问题是，你如何在一个输入嵌入向量和另一个输入嵌入向量之间得到阿尔法值？你如何得到注意力分数？最简单的方法可能是取点积，但我们看到，假设这是句子，如果它是我的查询...

如果我想计算“it”（它）与“ball”（球）以及“it”与“dog”（狗）之间的注意力分数，我会先计算“it”与“ball”的点积，结果为0.51；再计算“it”与“dog”的点积，结果同样是0.51。这样一来，两者的注意力分数相近，但这并非我想要的——因为当我说“it”时，我希望它指向“ball”，所以“it”与“ball”的注意力分数应当显著高于“it”与“dog”的分数。那么如何实现呢？显然，点积运算缺乏捕捉这种上下文关系的复杂度，我需要更多可调节的参数，但目前尚不清楚具体需要哪些控制机制。

但让神经网络或反向传播算法来找出这些参数的最佳值，至少让我先随机初始化它们。这时新术语就派上用场了——我需要可训练的矩阵，姑且称之为查询矩阵。通过将输入嵌入与查询矩阵相乘，我可以将其转换到另一个空间。而对于表示"狗"和"球"的关键词输入嵌入，它们将与键矩阵相乘并转换到另一个空间。然后我就能在这些转换后的向量之间计算注意力分数。如果模型能正确学习这些矩阵参数，就能学会计算"它"和"球"之间的注意力分数是0.96（高于"它"和"狗"之间的0.56）。不必担心这些乘法运算或数学细节。

现在我将详细讲解数学部分，这将在下一节课中进行。目前只需记住，我们不知道如何从物理上捕捉上下文关系，所以这就像是一种简便方法，一种技巧。你引入了查询（queries）和键（keys），这些都是随机初始化的可训练矩阵，然后对它们进行训练。你可能听说过“查询”和“键”这两个词，但实际上引入它们并没有确切的物理原因。唯一的原因是人类无法自行找到计算这些注意力分数的方法，我们唯一知道的就是这样。

如果我们无法理解它，就让我将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数，然后希望训练过程本身能自行解决这个问题。人类在计算机视觉领域也采用了类似的技巧：如果你训练一个卷积神经网络（CNN）来区分狗和猫，你无法自己写下所有特征，而是依赖CNN来完成。这里的情况也类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，以及还有一个称为值矩阵的矩阵，它们将如何用于我们将在下一讲中看到的下一个令牌预测任务。

所以下一讲将全面探讨自注意力机制的数学原理——我们将如何处理查询矩阵、键矩阵和值矩阵？如何精确计算上下文向量？从这些上下文向量出发，我们最终要通过哪些步骤来获得下一个开放预测？因此下节课本质上是对刚才所见内容的深度剖析，并将其扩展为一整节数学原理课程。

但在今天的讲座中，我只是想激发大家对查询、键和值这些概念的兴趣，这些我们尚未深入探讨的内容将在下一讲中详细展开。好了各位，今天的讲座就到这里，你可以将其视为注意力机制发展历程与自注意力机制入门的一次融合。作为下个开放式预测任务的总结，请记住注意力机制的演变过程：最初我们有Elisa，它在当时堪称革命性的突破，考虑到它诞生于1966年，至今仍令人惊叹。

接着出现了循环神经网络和长短期记忆网络（LSTM），它们存在上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了注意力机制，它计算解码输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力机制加RNN。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

所以他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并再次以注意力机制为核心构建了这一架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们讨论自注意力时，我们实际上只关注一个序列，因为它将用于下一个标记预测任务。

因此，在像GPT这样的下一个标记预测任务中，我们使用自注意力机制，即观察一个标记及其如何关注周围或邻近的标记。我们正在观察的标记被称为查询（query），其他标记被称为键（keys）。我们的目标是找到查询向量与键之间的注意力分数。我们认识到，注意力机制的主要目的是获取这些注意力分数，并将其转换为上下文向量。上下文向量是输入嵌入向量的一种更为丰富的表现形式。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但没接住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵。

因此我们需要增加参数，以便有更多可调节的"旋钮"来操作这些可训练的矩阵。这些矩阵被称为查询权重矩阵和键权重矩阵。此外还有值权重矩阵，我们将在下节课中介绍。所有输入嵌入都会与查询权重矩阵相乘，得到查询矩阵；同样地，我们也有键矩阵。这样，注意力分数就不再是在向量的输入嵌入之间计算，而是在查询和键之间计算。


and since we have a flexibility of so many parameters to play with we hope that when we train the parameters they will learn that the attention score between the it the second it second it and the ball is higher than the attention score between the second it and the dog so it captures more contextual complexities so addition of this trainable weight matrices captures more contextual complexities and that's why we humans added these weight matrices and then we call them queries keys and values because it it sounds cool and also it relates to the field of information

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)