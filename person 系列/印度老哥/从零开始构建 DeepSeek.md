
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


（24.56）

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
	* 强化学习的兴起
	* 不再仅仅依赖人工标注的数据，而是利用大规模强化学习来教授模型进行复杂推理
	* 基于规则的奖励系统
* 第三，他们实现了多项 GPU 优化技巧；
	* 采用了 NVIDIA 的并行线程执行技术（PTX）
* 第四，构建了一个有利于蒸馏等技术的模型生态系统
	* 蒸馏至更小的模型（1.5B）


### 1、MLA

分为四个部分：

* LLMs 架构本身
* 自注意力
* 多头注意力
* KV 缓存


2022年从麻省理工学院获得机器学习博士学位，我是“从零开始构建深度探索”系列的创始人。在我们开始之前，我想向大家介绍本系列的赞助商和合作伙伴——InVideo AI。大家都知道我们多么重视从基础构建AI模型的内容。InVideo AI的理念和原则与我们非常相似。

让我来为你展示一下 这里是InVideo AI的网站 凭借一支小型工程团队 他们打造了一款令人惊叹的产品 只需输入文字提示就能生成高质量AI视频 正如你在这里看到的 我输入了一段文字提示："制作一个超写实的高端奢华手表广告视频 要具有电影质感" 点击生成视频后 很快我就得到了这个令人难以置信的超写实视频 最让我着迷的是它对细节的把控 看看这个 画质和纹理简直不可思议

而这一切仅通过一条文本指令就实现了，这就是InVideo产品的强大之处。刚才你所看到的精彩视频背后，是InVideo AI的视频创作流程，他们正在从基本原理重新思考视频生成和编辑。为了试验和优化基础模型，他们在印度拥有最大的H100和H200集群之一，同时也在测试B200。InVideo AI是印度发展最快的人工智能初创公司，面向全球市场，这也是我如此认同他们的原因。好消息是，他们目前有多个职位空缺。

你可以加入他们出色的团队，我将在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建深度探索”系列的下一个讲座。今天我们将学习一个非常重要的概念，那就是注意力机制的必要性。首先，让我快速总结一下我们在这个系列讲座中已经涵盖的内容，然后再来讨论今天的主题。在两个讲座之前，在名为“深度探索基础”的讲座中，我们研究了深度探索架构的四个阶段，或者更准确地说，是我们将要涵盖这个系列讲座的四个阶段。第一阶段是深度探索架构中的创新。

第二阶段是训练方法  
第三阶段是GPU优化技巧  
第四阶段是模型生态系统

在上一讲《LLM架构》中  
我们重点探讨了第一阶段  
我们的主要目标是  
在接下来的两到三讲课程中  
必须开始理解多头潜在注意力机制  
这是深度求索架构中的首个重大创新

但要理解MLA（多头潜在注意力）  
我们不能直接切入这个概念  
需要循序渐进地推进  
因此我们首先剖析了LLM的架构本身  
发现其架构呈现这样的形态

一开始我们有输入部分，然后是处理器，接着是输出部分。每个部分基本上都有不同的构建模块。所以我们会发现，如果把大语言模型想象成引擎的话，要真正理解这个引擎的工作原理，我们确实需要打开引擎，看看里面到底有什么。

那么，大型语言模型（LLM）究竟是如何学会预测下一个标记或下一个词的呢？如果把这个过程类比为汽车的运动，我们会发现，当你给汽车加油时，汽车就会移动，对吧？引擎中发生了一些变化，汽车就开始行驶。同样地，对于LLM来说，燃料就是你输入到引擎（即LLM）中的一系列词语，而输出则是对下一个词或下一个标记的预测。正因为如此，我们也把LLM称为“下一个标记预测引擎”。

现在这个引擎拥有海量参数。GPT-3有1.75亿个参数，GPT-4可能有约1万亿个，而最近发布的GPT-4.5大概有5到10万亿个参数。为了真正理解这个引擎的工作原理，我们打开了黑箱，从这三个方面进行了研究。

但我并没有直接向你解释这三个方面。我告诉你，如果你把自己想象成一个令牌或一个世界，想象自己经历不同的阶段会怎样。所以如果你是一个令牌，首先你会被分配一个令牌ID，那是你的徽章，然后你会被分配一个令牌嵌入向量，接着是一个位置嵌入向量，令牌嵌入和位置嵌入相加，就形成了输入嵌入，也就是你的制服。

同样地，无论你的邻居是谁，他们也都有自己的统一表示或输入嵌入，大家一起登上通往Transformer的列车。每个Transformer块包含多个组件，如归一化层、多头注意力机制、丢弃层、跳跃连接，接着又是一个归一化层、前馈神经网络和丢弃层，最后再跟一个跳跃连接。而这仅仅是一个Transformer块的构成。

有多个这样的Transformer块，可能是12个、24个、96个等等。因此，作为携带你统一特征（即输入嵌入）的token，你需要经过所有这些Transformer块的处理，然后当你出来时，会有一层归一化处理，最后是一个输出层。在这个输出层中，如果你是一个768维的向量，你将被映射或投影到一个50,000维的向量。为什么是50,000？因为这是词汇表的大小，这本质上帮助我们选择下一个token。

这就是一个标记在整个LLM架构中的完整生命周期。在今天的讲座中，我们将重点讨论整个架构中的一个方面，那就是多头注意力机制。我希望你们能理解多头注意力机制在其中的作用。在所有这些步骤中，令牌会经过多头注意力机制，而多头注意力机制出现在Transformer块中。在这个特定的情况下，它还在归一化层之后出现。今天，我们将理解多头注意力机制，但在此之前，我们首先要理解什么是注意力本身，以及为什么需要注意力机制。我们为什么要开始讨论“注意力”这个术语？为什么最近它变得如此流行？因此，我们今天的目标是激发自注意力这个概念。

我们今天不会讨论自注意力机制的数学原理。今天的全部目标在于尝试理解为什么我们首先需要注意力机制，以及为什么它会对大型语言模型产生如此革命性的影响。请思考这一点：在所有标记（token）经历的步骤中，最重要的步骤就存在于Transformer模块内部——具体就存在于Transformer模块的某个环节，那就是注意力机制。

这就是为什么我在这里用不同的颜色标记出来。这部分内容本质上展示了让大语言模型（LLM）如此擅长理解语言的所有特性。为了真正理解注意力机制的工作原理，我专门为你们准备了这节单独的课程，我们将尝试阐述引入注意力机制的必要性。

在理解我们为何需要注意力机制以及它如何改变这一领域之前，让我们先深入探讨一下生成式人工智能本身。因此，让我们稍微回顾一下历史。我认为理解注意力机制的本质至关重要。20世纪60年代有一个叫ELISA的聊天机器人。你可以想象一下。我不会称它为大语言模型。

我会称它为聊天机器人。这是第一个旨在充当治疗师的自然语言处理聊天机器人。所以如果你问“请告诉我是什么困扰着你”，我可以说我在学习人工智能方面遇到了困难。你能帮我吗？你觉得学习人工智能很困难是正常的吗？你看它其实并不是很有帮助，但别忘了那是20世纪60年代。在当时，这已经被认为是一场革命了。所以在这里你可以看到，这个原始程序是由约瑟夫·维森鲍姆在1966年描述的。

与此相比，看看现在的ChatGPT，我们说我想学习人工智能。你能帮我吗？然后让我把模型切换到GPT 4.0，这样我能得到更快的回复。接着你会看到，我立刻获得了一系列可操作的项目清单，可以立即开始实施来学习人工智能。因此，在短短64年的时间里，我们在自然语言处理和理解语言本身这一领域已经取得了长足的进步。

让我们来看看从20世纪60年代到2020年代发生了什么。20世纪60年代是ELISA聊天机器人问世的时期。然后在20世纪80年代和1997年，出现了生成式人工智能或语言建模领域的两个重要基础构件，你可以把它们看作是20世纪80年代出现的循环神经网络和1997年问世的长短期记忆网络。

这两者都建立在20世纪70年代神经网络热潮的基础上。人们发现神经网络可以做很多了不起的事情，但神经网络真正的问题在于，它们本质上无法处理记忆，这是神经网络最大的缺陷。为什么需要处理记忆呢？因为想象一下，如果你要预测下一个单词是否正确，或者生成一些新文本，你会想知道段落的开头写了什么。

仅仅记住一点信息是不够的。上下文非常重要，这个词在今天的讲座中会频繁出现。上下文本质上意味着，如果给模型提供了一大段文字，而我们希望模型执行某项任务或仅通过查看句子来回答某个问题，那么仅靠句子中紧邻的词语是无济于事的。

我们需要在脑海中记住段落开头所讲述的内容。比如说，我来自印度浦那，然后接着写了一大堆不同的事情。我写了一大堆句子，最后问我说的是什么语言。现在要根据这个问题生成一个回答，中间所有这些内容并不太重要，但我需要了解这段开头的内容，因为这才能真正帮助我回答这个问题。这就是为什么需要具备记忆能力，而神经网络原本并没有真正编码记忆的机制，但循环神经网络和长短时记忆网络解决了这个问题。让我快速展示一下它们是如何解决这个问题的。

因此，人们最初通常使用RNN或LSTM进行序列到序列建模，也就是所谓的序列到序列翻译任务。简单的序列到序列翻译任务可以只是语言翻译。如果你想处理一组单词，并将它们从一种语言翻译成另一种语言，你可以使用循环神经网络。

所以循环神经网络有一堆隐藏状态，比如h0、h1、h2、h3。假设这些是输入单词"I will eat"，我想把它翻译成法语，即"Je vais manger"，已经写在这里了。我可以用循环神经网络进行这种翻译的方式是，我有一个编码器块和一个解码器块。

编码器和解码器块都有所谓的隐藏状态，你可以将其视为向量或矩阵。这里是编码器的隐藏状态h1、隐藏状态h2、隐藏状态h3，然后是解码器的隐藏状态s1、隐藏状态s2和隐藏状态s3。让我们可视化编码器块中发生的情况。

在编码器模块中，首先输入句子的第一个单词"I"，它会转换为第一个隐藏状态h1；接着输入第二个单词。需要注意的是，为了得到第二个隐藏状态h2，你不仅要使用第二个输入，还要利用前一个隐藏状态。这就是循环神经网络捕捉记忆的方式。

前一个隐藏状态用于计算当前隐藏状态。同理，当你来吃饭时，那是输入x3，但要计算最终的隐藏状态h3，你需要使用前一个隐藏状态h2，而h2编码了h1或者说h2包含了h1的信息。因此，h3现在既包含了h1的信息，也包含了h2的信息。

h3拥有对过去的认知，它保留着记忆。因此，h3可以被视为一个向量，可能是500维或1000维的向量，它本质上包含了输入的所有信息，代表着输入的含义。这是唯一一个传递给解码器的高维向量。解码器随后会查看这个向量，解码器的每一个隐藏状态都会开始对其进行解码。

所以解码器就像接力赛跑。想象一下，接力棒在h1手中，它传给h2，h2再传给h3。现在h3拿到了接力棒，并将其传递给解码器。现在我是解码器的第一个隐藏状态。我拿到了接力棒。我先解码第一个标记JOR，然后传递给第二个解码器，接着解码第二个标记，最后解码第三个标记。这就是循环神经网络中语言到语言翻译的工作原理。

LSTM（长短期记忆网络）在预测当前隐藏状态时与前代隐藏状态的处理方式有所不同。它们还维护一种称为细胞状态的结构。因此，LSTM既能考虑长期记忆，也能兼顾短期记忆。所以这本质上是为了扩大它们处理的上下文窗口。但架构从根本上保持不变。只是对于LSTMs来说，底层的数学运算要复杂得多。

现在你看出问题所在了吗？让我们看看这个例子，我们之前用它来讨论上下文的问题。那么，当处理大段文本时，循环神经网络存在什么问题呢？比如说，你有一段很长的文本，你想翻译整段内容。循环神经网络在处理这个问题时存在什么缺陷？让我们通过一个简单的例子来更清楚地说明这个问题。想象一下你手上有这样一段文字（这是我随机从ChatGPT生成的）。

我希望你按照以下步骤操作：首先，花大约30秒到一分钟或更长的时间，完整地阅读这一段文字。然后闭上眼睛，将它翻译成你选择的任何语言。你可以把它翻译成印地语、西班牙语、法语、德语，或者除英语以外的任何你会的语言。所以，闭上眼睛，把它翻译成另一种语言。你能做到吗？这不可能，对吧？因为当你闭上眼睛时，你基本上只能记住你读过的一些概要。

你并不需要记住每一个确切的单词，但为了翻译，我们不得不逐字逐句地翻译，对吧？这正是循环神经网络所面临的问题。为什么呢？因为我们仅依赖一个隐藏状态或一个向量来捕捉过去发生的所有上下文信息。

这就好比闭上眼睛后试图回忆整段文字。所有的记忆压力都集中在这个隐藏状态或向量上，如果是一大段文字，如何能将所有信息压缩到一个500维或1000维的向量中？自然会有一些信息丢失，而且解码器并不会从第一个或第二个隐藏状态获取输入。

它仅将最终的隐藏状态作为输入。这就是主要问题所在。从编码器传递到解码器的上下文仅来自最终的隐藏状态。这就是为什么这被称为上下文瓶颈问题。最终的隐藏状态就像你闭上眼睛试图记住整个段落时一样，这是不可能做到的。你不能对一个向量施加这么大的压力。

循环神经网络中的上下文瓶颈确实不利于保留长距离上下文，这为我们理解注意力机制为何出现提供了一个很好的途径。现在，如果你观察循环神经网络，让我在这里擦除一些内容。你认为这个问题的理想解决方案是什么？假设这就是我们目前遇到的问题——所有上下文信息无法被压缩到单个向量中。

这个问题的解决方案是什么？嗯，解决方案似乎是这样的：比如说，当我们在解码时，当我在解码这个时，假设我在从解码器的第一个隐藏状态进行解码时，与其只能访问最终的隐藏状态，不如让我能够访问编码器的所有隐藏状态以及所有输入。因此，如果在解码过程中我可以访问所有输入，那么我就可以进行预测，比如如果我在解码第一个标记，我应该更重视第一个隐藏状态，而不应该重视第二个隐藏状态，也不应该重视第三个隐藏状态。如果我能创建一个机制，能够理解需要给予不同标记的相对重要性，那会怎样呢？

这是本讲座最重要的部分，请大家集中注意力。解决上下文瓶颈问题的关键在于：当我在解码时，能否开发一种机制来量化每个隐藏状态的重要性。例如，在进行首次解码时，假设我给第一个隐藏状态分配80%的重要性，第二个和第三个隐藏状态各分配10%。

如果我掌握了这些信息，上下文瓶颈问题就迎刃而解了。因为即便段落很长，只要我能关注到段落中的所有标记，我实际上也能重视之前出现的内容。比如说这是句子，而我要在这里预测某个内容。在注意力机制中，我可以尝试判断应该给予第一个标记、第二个标记、第三个标记乃至整个段落多少注意力，然后我就能确定需要对这些标记给予最大的关注。

所以你看，我已经开始在这里使用“注意力”这个词了。你可以把它看作是段落中各个标记的相对重要性。因此，理想的解决方案是我们在解码过程中需要选择性地访问输入序列的某些部分。因此，当你在解码时，假设我想做出某些预测，比如如果我在解码第一个标记，那么alpha 1 1就是赋予第一个隐藏状态的相对重要性，alpha 2 1是赋予第二个隐藏状态的相对重要性，alpha 3 1则是赋予第三个隐藏状态的相对重要性。我希望能够做出这样的预测：alpha 1 1为100%，alpha 2 1为0，alpha 3 1也为0，类似这样。但你可以看到，现在我不再仅仅依赖于h3，而是依赖于之前所有的隐藏状态，不仅如此，我还在量化应该对每一个之前的隐藏状态依赖多少。

因此，这一点很重要：我们需要在解码过程中有选择地访问输入序列的部分内容。我想通过这个例子再次向你们解释这一点。假设我在这里截了一张图，然后把它放在这里。让我们试着理解这句话的实际含义。如果我让你翻译这一段，你会怎么做？你可能会从——比如说这里开始。假设你从这里开始，你能一次看到多少个词？假设这是你的上下文窗口，你一次无法看到比这更多的词。那么你的大脑会怎么做呢？对于你的大脑来说，段落中其余的内容就无关紧要了。

所以你会屏蔽掉脑海中所有无关的信息，屏蔽掉那些不相关的内容。你要做的就是忽略其他所有不重要的事物，只关注当前的语境，并选择性地隐藏其他一切。你的大脑会集中注意力——这一点非常重要——它会专注于段落的这一部分，然后开始仅翻译这一部分。翻译完这部分后，你的大脑会移动到接下来的内容，可能是接下来的五个单词。翻译完这五个单词后，你的大脑会继续移动到下五个单词，依此类推。

所以你看，在特定时刻你在这里所做的，是你的大脑正在做出一个决定：当你在看第一个上下文窗口时，我想要对这些标记给予最大的关注。然后你的大脑也在做出一些定量的判断，完全不去关注其余的标记，完全不关注这些。这就是在解码过程中选择性访问输入序列部分的真正含义，对吧？选择性访问输入序列的部分，这是理解注意力机制最重要的句子。它意味着只关注在那一刻重要的输入序列的那部分，这看起来是一个很直观的做法。

没错，但这恰恰是帮助语言模型变得更强大的最关键因素。想象一下，你要在一段文字中找出拼写错误，你会怎么做？你需要逐字逐句检查，先关注第一个词，然后是下一个词，依此类推。这正是语言模型也需要做的——它们需要选择性地关注序列中的特定部分，比如找出含有拼写错误的那个词。这就是解码过程中需要注意力机制的核心原因：我们可以量化每个输入词元应该获得多少重要性或注意力，这就是我现在要阐述的重点。

现在，真正实现这一点的第一篇论文并不是《Attention Is All You Need》。如果你看这篇论文，你会发现它有大约117万次引用，引用量非常大。所以很多人实际上认为注意力机制是在这篇论文中首次提出的。但真正首次引入注意力机制的论文是这篇《Bahdanau Attention Mechanism》。如果你搜索“Bahdanau Attention”并点击这篇论文，你会发现这是第一篇真正将注意力机制应用于翻译任务的论文。这篇论文我认为是在2014年发表的，并在2015年的ICLR会议上发表。

他们的主要目的是实现了一种架构，让我们可以有选择性地决定对每个隐藏状态投入多少注意力——这就是所谓的a1、a2、a3注意力分数。基于这个机制，就能完成翻译任务。他们通过实验证明：使用注意力机制后，序列到序列的翻译效果能得到显著提升。请务必记住这篇论文的结论——现在大家可以看到，这张图的x轴是英语句子，y轴是法语翻译，对角线上的色块正显示出英语单词与对应法语译词之间的注意力权重分布。

而最酷的地方在于，顺序并不相同。比如《欧洲经济区协定》中的"欧洲经济区"——英文是"European Economic Area"，但翻译成法语就变成了"Economic European Area"。你看，它并不是逐字直译的。然而，注意力机制却能精准识别出：法语里的"European"对应英语里的"European"，尽管它们出现的位置不同——英语版出现在第五个词，而法语版出现在第七个词。

所以所有这些最亮的区域并不都在对角线上，有些偏离了对角线本身，甚至这些也被注意力机制捕捉到了。这张图最酷的地方就在于这些非对角线元素，因为翻译并不总是逐字进行的，对吧？有些语言中某些词会先出现，而在其他语言中则后出现，这取决于名词、词汇等的排列方式。这就是这篇论文的亮点所在，这是第一篇将注意力机制用于翻译任务的论文。

然后，《Attention Is All You Need》这篇论文提出了Transformer模块，并将注意力机制整合到其中——这正是该论文的核心优势或独特之处。关键在于，虽然RNN和LSTM在语言翻译领域表现不错，但它们存在上下文瓶颈问题。于是研究人员发现，构建深度神经网络其实并不需要RNN架构。2014年Bahdanau注意力机制的诞生就是个转折点，当时他们还在使用RNN结构。

所以他们保留了RNN（循环神经网络），同时还引入了注意力机制。实际上，在Bahdanau注意力模型中，你可以用类似的方式看待编码器-解码器模块，只不过他们在三年后（即2017年）又加入了这种注意力机制。这意味着研究人员最终发现，在构建用于自然语言处理的深度神经网络时，甚至根本不需要RNN架构，于是他们提出了Transformer架构。让我们梳理一下这个领域的发展脉络吧：1980年代是RNN的时代，1997年出现了LSTM（其实1966年就有ELIZA了，我把这个也写下来），2014年引入了注意力机制——但当时仍依附于RNN的编码器-解码器架构。直到2017年，研究人员才彻底认识到自然语言处理任务根本不需要RNN。

因此，RNN被淘汰了，基本上只保留了注意力机制，但它与Transformer模块相结合，这就是2014年论文和2017年论文之间的主要区别。在2014年的论文中，RNN模块仍然存在，但后来被移除，并在2017年被Transformer模块或Transformer架构所取代。这最终形成了完整的架构——现在我们有了Transformer模块，其中包含了注意力机制。这就是GPT架构，并不是原始Transformer论文中的架构。原始Transformer论文中既有编码器也有解码器，而我展示的这个架构则不同。

目前仅有一个解码器，请不要对此感到困惑。今天讲座的主要目的是让大家从自然语言处理的历史视角理解为何需要引入注意力机制。关键原因可以用一句话概括：我们需要在解码过程中有选择性地访问输入序列的各个部分。最初注意力机制是在RNN中提出的，后来研究人员发现可以去掉RNN结构，尝试将注意力机制与其他模块结合，最终在2017年将其整合到了Transformer模块中。

然后在2018年，GPT架构问世了，所以它是注意力机制加上GPT。GPT基于最初的Transformer架构，但它不像原版那样同时包含编码器和解码器模块，而是只保留了解码器模块，就像你在这里看到的。它还保留了注意力机制，我们之前讨论过为什么需要注意力机制，这一点在LLM架构的这一部分得到了体现。

现在我们将开始探讨，如果你观察下一个标记预测任务，自注意力究竟是什么，上下文向量又是什么，以及注意力机制的主要目的是什么。那么，让我们来看看自注意力机制在下一个标记预测中的主要目的是什么。既然我们已经理解了注意力机制及其历史，现在让我们尝试理解“自注意力”这个术语。自注意力实际上是什么意思呢？自注意力指的是一种机制，它允许输入序列中的每个位置关注同一序列中的所有其他位置。

这意味着，如果我有一个句子，比如“明天是晴天”，自注意力机制本质上指的是，到目前为止，如果你看RNN，我们看到的是解码器需要给予编码器的注意力。所以如果第一个解码的词是法语词，我们基本上是在看两个不同的序列之间的关联。假设英语序列是这样的，比如“I will eat”，法语序列是这样的，这是法语序列，我们在看如果你在做第一次解码时，你应该给予英语序列多少注意力。

因此，这里的注意力机制是作用于序列之间的，而非同一序列内部。而自注意力机制则用于预测下一个标记（token），这正是大语言模型（LLM）的典型运作方式——由于LLM的核心任务是预测后续标记，它们并非专门针对翻译任务进行训练。在预测下一个标记的场景下，本质上并不存在不同语言的区分，你处理的只是一堆数据。

因此，我们不再关注两个序列之间的注意力，而是采用同一个句子。比如说，当你观察下一个词时，你会试图找出：如果关注某一个标记或单词，我应该对句子中所有其他标记给予多少注意力。这才是这里最关键的理解点。如果你主要关注某一个标记或单词，那么周围的单词对这个特定标记有多重要？为什么这种认知对我们至关重要？你能试着思考一下，为什么我们需要编码那些本质上围绕某个给定标记的其他标记的信息吗？

这些知识对我们之所以重要，是因为当你预测下一个词时，本质上需要关于序列上下文的信息，需要了解不同单词之间如何相互关联。再以同样的例子来说，假设我说我来自印度浦那，我说，假设这是句子，如果你看“说”这个词，我需要知道当我看到“说”时，最大注意力应该放在“浦那”和“印度”上，也许其他所有词都没那么相关，因为我所说的方言深受我来自的地区影响。

因此，当你观察一个词云时，如果你的Transformer架构或LLM引擎掌握了某个词与周围其他词的关系信息，并且知道需要给予周围不同词多少关注度，那么仅凭一个词元就能非常轻松地预测下一个词元。

这就是为什么自注意力机制变得如此重要。如果没有自注意力机制，你就会丢失关于其他标记与我们选择的特定标记之间关联的上下文信息。我希望现在你能明白为什么它被称为"自注意力"。在这个例子中，当我们处理序列到序列的语言翻译时，注意力机制作用于不同序列之间；而自注意力则是当我们观察单个句子本身时，关注句子内部的各个标记，并从根本上理解这些标记之间是如何相互关联的。

让我们再次以同一个例子来说明，第二天是晴朗的。记住，当这些标记进入Transformer架构时，它们现在是以向量的形式存在，正如你之前所见，它们现在具有统一的维度。记住，每个标记都有一个统一的768维向量，这就是输入嵌入。因此，每当我在这里展示这些块时，本质上它们代表的是一个向量。

因此，第二天是明亮的，现在这些都是向量了。对于Transformer来说，它并不理解单词，也不理解句子，它只知道每个标记都是一个向量。所以，"the"是一个向量，我称之为x1；"next"是一个向量，我称之为x2；"day"是一个向量，我称之为x3；"is"是一个向量，我称之为x4；"bright"是一个向量，我称之为x5。

现在，如果我正在看一个特定的词，比如我之前在这里展示的“next”，我想看看当我注视这个词“next”时，我应该给予其他所有标记多少注意力。这个注意力由α2,1给出，或者我称之为符号α2,1。为什么是2,1呢？因为“next”是第二个标记，而我想找出第二个标记和第一个标记之间的注意力分数，这就是α2,1。这个会是α2,2，这个会是α2,3，这里会是α2,4，这里会是α2,5。

我本质上想找出当我关注某个特定标记时所有的注意力分数，因此这被称为查询（query），也就是我当前关注的标记，即查询标记（query token）。我想知道，当我关注这个查询时，应该给其他所有标记分配多少注意力，这些标记有时在通用术语中也被称为键（keys）。最终，我想做的是利用这些注意力分数，以某种方式整合所有这些信息，将这个向量从输入嵌入向量转换为上下文向量。

现在我要强调一个非常重要的区别：目前"next"是一个输入嵌入向量，对吧？所以它包含词元嵌入加上位置嵌入。但上下文向量是完全不同的概念。如果这是"next"的输入嵌入向量，而我在同一空间中绘制上下文向量——这就是"next"的上下文嵌入向量——你会发现上下文向量实际上比词元嵌入向量丰富得多。为什么呢？因为词元嵌入向量或输入嵌入向量不包含相邻单词的信息，而现在我的上下文向量包含了邻居单词的信息，这些信息现在已经被"烘焙"进了我的输入嵌入中。

所以如果你有一个输入嵌入，如果你有输入嵌入向量，也就是我之前提到的统一向量，如果你用关于邻居的上下文来增强这些输入嵌入向量，我们会看到这种增强是如何完成的，但本质上这会导致一种被称为上下文向量的东西，所以注意力机制或自注意力机制的整个目标是将所有输入嵌入向量转换为上下文向量，所有这些统一向量，我们之前看到的这些统一向量，所有标记都有一个768维的统一向量，

当它们从归一化层出来并进入多头注意力层时，进入注意力层的是一个输入嵌入向量，而从注意力层出来的是一个上下文向量。因此，在离开注意力块后，我们得到的输出会更加丰富，这就是为什么我用不同的颜色标记它。之所以更丰富，是因为现在它还编码了其他标记的信息，从而保留了上下文。因此，上下文向量是一个经过丰富的嵌入向量，它结合了所有其他输入元素的信息。因此，在自注意力机制中，上下文向量起着非常关键的作用。

它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住这一点：输入嵌入向量仅包含有关该单词或标记的信息，它可能编码了该单词的含义及其位置的信息，但对邻近元素一无所知。而上下文向量则能感知邻近元素，因为邻近元素至关重要。当你观察一个句子或一个段落时，单个标记本身毫无意义，正是它们与邻近元素的关联才构成了该段落的上下文。这就是为什么在大型语言模型（LLMs）中需要这种机制。

需要理解句子中词语之间的关系和相关性，实际上这正是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个至关重要的转折点——距今正好十年——当注意力机制被提出后，人们开始意识到：与其孤立地看待词语，不如退后一步观察词语之间究竟如何相互关联。这样一来，我们就能充分挖掘文本的最大价值。因为就像图像由像素图案构成一样，语言的意义也源自词语之间的有机联系。

只有当你把所有单词放在一起看，并理解它们之间的关系时，文本或段落才有意义。所以现在的问题是，你有一个输入嵌入向量，比如说“next”，如何将其转换为上下文向量？你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频一会儿，思考一下。你有输入嵌入向量，假设你还有这些注意力分数，你将如何修改输入嵌入向量，以便以某种方式考虑这些注意力分数，从而得到一个上下文向量？

所以你可以在这里稍作停顿，首先你也可以试着思考一下这些注意力分数本身是如何计算的。好的，最简单的做法是，假设你有这个向量，还有所有其他向量，我们为什么不简单地做一个点积呢？比如你有“next”的输入嵌入向量，也有“though”的输入嵌入向量，只需在这两个向量之间做点积，就能得到α21；再对“next”和“next”做点积，就能得到α22；然后对“next”和“day”做点积，就能得到α23。

然后计算next和is的点积，这将得到alpha2_4；接着计算next和bright的点积，这将得到alpha2_5。一旦你获得了所有这些alpha值，就可以简单地计算alpha2_1乘以x1加上alpha2_2乘以x2加上alpha2_3乘以x3加上alpha2_4乘以x4加上alpha2_5乘以x5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你在这里有一个向量v1，而这里有另一个向量v2，它们之间的点积会比比如说v1和v3之间的点积更高。

因此，如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的内容。你可能会想，我想要量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，它们当然应该有一个更高的注意力分数。因此，这个计算对我来说是有道理的：我只需计算点积，然后用点积来缩放不同的向量并将它们相加。无论这个和是多少，它现在就是“next”的上下文向量。同样，我也可以为所有其他标记找到上下文向量。

这种方法有什么问题，为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们想要编码的上下文。我希望你从基本原理出发来思考，我很快就会揭晓答案。好了，主要答案是，假设我们考虑这个句子：“狗追球但没抓住”，狗追球但没抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。如果“它”现在是我的查询向量，

你是如何决定计算查询向量与其他向量之间的注意力分数的，你决定采用点积对吧，这正是我要做的。如果这是我的查询向量，要计算它与“狗”之间的注意力分数，我只需对它和“狗”进行点积运算，结果是0.51；如果我对它和“球”进行简单的点积运算，结果也是0.51。你看这里的问题：两个注意力分数完全相同，但这并不是我想要的。当你说“但它没能抓住”时，实际上指的是“球”对吧——狗追球但没抓住，所以这里的“它”指的是球而不是狗。因此当我观察时，需要更关注“球”而非“狗”。让我用不同颜色的笔写下来以便更清晰——

我在观察时需要更加关注球本身而非狗，但实际情况并非如此。如果采用简单的点积运算，就无法体现"球应比狗获得更高优先级"这一信息——当我们提及二者时，狗和球的注意力分数不应相同。这个例子绝妙地证明了为何需要选择性关注不同标记：狗追逐了球但没能接住它。第一个"它"指代的是狗（若作为查询标记会更多关注狗），而现在这个"它"作为查询标记则应更多关注球。

我不希望两者具有相同的注意力分数，因此简单的点积无法区分此处微妙的上下文关系——它没有考虑"追逐未能抓住"的语境，也无法处理语言上的细微差别（例如"抓住"更可能指代移动的物体即球）。核心问题在于：简单点积仅能衡量语义相似度，却无法应对上下文问题。很多句子都可能存在这类复杂的上下文关系，对吧？我需要设计一种机制来捕捉这些复杂性，但又不确定具体该采用什么机制。于是我们采用了研究人员沿用多年的技巧：

如果你不知道事物之间的本质关系是什么，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。这正是注意力领域所发生的事情。研究人员基本上无法弄清楚这个机制可能是什么，这就是机器学习领域或深度学习领域与物理学不同的地方。在物理学中，如果你遇到这个问题，你会花六个月到一年的时间试图为底层机制制定一个定律，以捕捉复杂性或捕捉上下文的底层机制。

但在深度学习领域，你并不这样做。你会说，我要用一堆矩阵来替代它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？于是他们发明了新的矩阵，比如所谓的查询矩阵和键矩阵。这意味着，与其仅仅看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。所以，如果我的查询在这里是正确的，我的查询是“它”，我就会将其与所谓的查询矩阵相乘。这个矩阵可以是一个高维矩阵，用于“狗”。

所以“狗”和“球”就是关键（keys）对吧？因为本质上，关键（keys）就是查询时你要寻找的所有其他标记——在这里就是“狗”和“球”。你把它们都乘以一个关键矩阵（keys matrix）。现在看这里的优势：如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵有什么特定含义，你只需随机初始化它们，然后通过反向传播来训练它们。这是研究人员长期使用的深度学习技巧——如果你自己无法理清关系，就退一步让神经网络来完成这项工作。与其用某些规则限制神经网络，不如让它自己找到规律。现在你看到了优势：我们掌握了多个可训练因素的控制权。

假设WQ是3×3矩阵，WK也是3×3矩阵对吧？比如"狗"、"球"这些就是我的键值。这些就是我们之前看到的输入嵌入向量。现在我要用查询向量乘以这些输入嵌入，再用这两个结果乘以键值矩阵。具体来说就是3×3矩阵乘以3×1向量，结果会得到3×1向量。这样键值就会变成0.9、0.2、0.1和0.1、1.8、0.1——你看这些数值都发生了变化。

因为我用矩阵与它们相乘，而查询也是如此，所以它将与查询矩阵相乘，使得查询结果变为0.5、0.1和0.5、1.0以及0.1等等。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正从输入嵌入空间转移到另一个空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的分数。

所以现在如果你计算它与球之间的注意力分数，你会发现它是0.56，而它与球之间的注意力分数是0.96。如果你计算它与狗之间的注意力分数，结果是0.56。这里你可以看到，它与球之间的注意力分数是0.96，高于它与狗之间的注意力分数0.56，因此这些注意力分数明显不同。添加这些可训练的矩阵实际上对我们有帮助，为什么有帮助呢？因为它提供了许多可调整的参数。

这样我们就可以编码标记之间的一些复杂关系。如果你进行简单的点积运算，注意力分数会是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，这会在下一节课讲到），本质上，如果你有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多的参数可以使用。如果你在这部分感到困惑，让我再重复一遍：我们开始这一节时认为，如果你有一个输入嵌入向量

那么，如何从输入嵌入向量得到上下文向量呢？要得到上下文向量，本质上需要先计算出注意力权重α。得到α之后，只需将它们与输入嵌入向量相乘，就能得到上下文向量。但问题在于，如何计算一个输入嵌入向量与另一个输入嵌入向量之间的α值，也就是如何得到注意力分数。最简单的方法可能是计算点积。比如，假设这是句子中的查询词"it"，要计算"it"与"ball"之间的注意力分数，我先计算"it"和"ball"的点积，结果是0.51；再计算"it"和"dog"的点积，结果也是0.51。

因此注意力分数看起来相似，但这并不是我想要的。因为当我说"it"时，我希望它指向的是"ball"，所以我希望"it"和"ball"之间的注意力分数要远高于"it"和"dog"之间的分数。那么现在该怎么办呢？显然点积运算的复杂度不足以捕捉这些上下文关系，我需要更多可调参数来处理——虽然目前我还不知道具体需要什么样的调节旋钮，但可以让神经网络或反向传播算法来找出这些旋钮应该是什么。至少现在可以先随机初始化这些参数，而这就是这些新术语的用武之地——我想要引入新的可训练矩阵。

让我把这个称为查询矩阵，并通过将其与查询矩阵相乘，将输入嵌入转换到另一个空间。而对于键（即狗和球）的输入嵌入，它们将与键矩阵相乘，同样转换到另一个空间。然后，我将在这些转换后的向量之间计算注意力分数。如果我的模型能正确学习这些矩阵的参数，就能让模型学会它与球之间的注意力分数是0.96，高于它与狗之间的分数0.56。不用担心这些乘法或数学运算。

现在我将在下一节课中详细讲解数学部分，目前只需记住：我们尚不清楚如何从物理层面捕捉上下文关系，因此这更像是一种简便方法或技巧。你引入了查询（queries）和键（keys）——这些随机初始化的可训练矩阵，通过训练来优化它们。你可能听说过"查询"和"键"这些术语，实际上它们被引入并没有确切的物理依据，唯一原因在于人类尚未找到直接计算注意力分数的方法，当前我们掌握的途径只有——

好的，如果我们无法解决这个问题，让我尝试将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数来处理，然后希望训练过程本身能够自行解决。这个技巧在计算机视觉领域也被人类使用过，例如当你训练一个卷积神经网络（CNN）来区分狗和猫时，你无法自己写下所有特征，而是依赖卷积神经网络来完成。这里的情况有点类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，还有另一个称为值矩阵的矩阵，以及它在我们将在下一讲中看到的下一个令牌预测任务中是如何使用的。

所以下一讲将全面探讨自注意力机制的数学原理：查询矩阵、键矩阵和值矩阵的具体运算，如何通过数学方法计算上下文向量，以及如何通过这些向量最终实现开放预测。下节课我们将深入剖析刚刚演示的这个模块，将其扩展成完整的数学推导课程。不过在今天的课堂上，我主要想先帮大家建立对查询-键-值概念的基本认知——具体数学实现我们留待下节课详解。

好的，今天的讲座就到这里结束了。今天的讲座内容可以看作是注意力机制的发展历程与自注意力机制的入门介绍。作为总结，请大家记住注意力机制的演变过程：首先是Elisa，在当时是一项革命性的突破，考虑到它诞生于1966年，这已经非常了不起了；随后出现了循环神经网络和LSTM。

<<<<<<< HEAD
所以这就是实际的下一标记，也就是我想要的，但最初预测的标记会完全偏离。这时反向传播就派上用场了，当所有参数都存在时，它们实际上会被优化。我们稍后会讲到这一点。但现在，让我再解释一下最后一步。我们有了每一个标记，现在每一个标记都与一个不同的均匀分布相关联，其维度为50,000。为什么维度是50,000呢？因为我们必须为这里的每一个单词预测下一个标记。所以对于O，我们必须预测下一个标记。

所以我们查看那个索引，也就是具有最高概率的标记ID。我们查阅词汇表或标记ID列表，然后进行反向映射，找到对应那个标记ID的单词。比如，如果这里的标记ID是555，或者这个标记ID是5000。我在这里找到5000对应的单词，理想情况下我希望这里对应的是"true"。但在初始阶段模型未经训练时，可能对应的是"for"。因此实际预测结果可能是"for"。同样地，我也会得到"true"的实际预测结果。

也许这里的最高令牌ID是你的朋友，然后我会在这里获取最高的令牌ID，这就是我如何预测每个令牌的下一个令牌。然后我会找到实际值和预测值之间的最后一项。这就是整个LLM架构的结构。所以现在如果你去输出层，也就是我的最后一层，你会看到我们有两个相互链接的东西。

好的，我们有了最终的层归一化层，它连接到输出层，然后我们有了用于下一个令牌预测的矩阵。这个逻辑矩阵就是这个，这个就是我刚才展示给你的那个，我们用它来进行下一个令牌的预测。现在你可能会想，这里优化的所有参数是什么。从一开始，就是这些存在的令牌嵌入值。

我们无法先验地知道这些参数。因此，让我用星号标记那些需要训练的参数——这些是我们无法预先知晓的。所以这些是需要训练的位置嵌入分配参数，我们无法预先知晓。因此这些都需要经过训练。Transformer模块的每一个组成部分都包含需要训练的参数：多头注意力机制有需要训练的参数，前馈神经网络也有需要训练的参数。而且这样的模块有12或24个，这进一步增加了参数量。因此在整个流程中存在着海量需要训练的参数，甚至最后的神经网络层也是如此。

它有这么多需要训练的参数，所有这些参数加在一起构成了参数总数，达到1750亿甚至可能上万亿。回想一下我们最初讨论的引擎，对吧？我们一开始只是知道并思考：好吧，这是个LLM引擎。但LLM引擎究竟如何运作？LLM引擎底层的参数是什么？这1750亿个参数究竟分布在哪里？现在我们已经深入了解了其详细架构——包括输入部分、处理器部分和输出部分——并追踪了一个单标记的完整旅程。

在令牌处理过程中，首先会经历输入阶段，此时它被隔离。它会被分配一个令牌ID或徽章，然后接受一组768个问题的测试（即令牌嵌入，用于编码含义）。接着，它会接受第二组问题，即位置嵌入，用于编码其位置值。我们将令牌嵌入和位置嵌入相加，得到输入嵌入，也就是每个令牌的统一表示。凭借这种统一表示，不同的令牌会被送入Transformer模块进行处理。

每个Transformer块基本上包含归一化层、多头注意力、dropout、再次归一化、前馈网络和dropout，其间穿插着两个跳跃连接。在GPT-2中，有12个这样的块；在GPT-2 XL中，我认为有48个这样的块；而在更高级的GPT模型中，可能会有96个甚至更多这样的块。因此，每个token都需要经过所有这些块，当它从所有这些块中出来时，其大小仍然保持768。

然后，它还会经过一个归一化层，其尺寸仍为768。最后，我们有一个输出层，其中每个标记都会被转换为一个大小为50,000的向量，这个大小等于词汇表的大小。接着，我们基本上会查看每个标记的50,000维向量，然后找出概率最高的那个标记ID，并用它来预测下一个标记。因此，在一个序列中，我们会有多个输入输出的预测任务。

因此，如果我们有一个包含五个标记的序列，就会有五个输入输出预测任务，这些任务实质上构成了我们的损失函数。然后我们的损失函数基本上会进行反向传播，所有的参数都会被优化——这1750亿个参数分布在多个环节：标记嵌入层中有参数，位置嵌入层中有参数，Transformer块的多个组成部分中有参数，输出层中也有参数。

这些参数都是通过反向传播进行优化的，最终我们得到的是一个对语言本身有直觉的模型，它还能预测下一个标记。正如你在这里看到的，下一个标记预测就是我们的任务。我们预测下一个标记，并与实际值进行比较。这就是任务，但由于我们有这么多参数，这个任务的副产品就是学习语言本身。所以在今天的讲座中，

我的主要目的是带你了解一个标记（token）的旅程。试着从单个标记的角度出发，思考会发生什么。试着打开这个引擎，打开大型语言模型（LLM）的引擎，真正去观察这个引擎是如何运作的。我希望我已经向你传达了这一点。我之所以构建这个类比或标记旅程的故事，是为了让你真正理解LLM架构内部发生的事情。因为如果不理解这一点，我们就无法继续前进到下一部分——注意力机制。

现在的计划是，在下一讲中，我将首先阐述为什么我们需要注意力机制。然后我们会探讨自注意力机制，接着是多头注意力机制，最后是键值缓存。所以，如果你看到接下来的计划，首先是注意力机制的必要性，然后是自注意力机制，最终是多头注意力机制。正如我提到的，所有未来的课程都已经详细规划好了，这将不是一个只有5到10分钟视频的简短系列。

本系列的每一期视频都会比较长，大约40到45分钟，我计划详细讲解所有步骤。"多头潜在注意力"是一个非常重要的概念，但我希望大家在真正理解这个概念时能保持同步。非常感谢大家，我真的很期待在下一讲中见到你们。请和我一起做笔记。这个系列可能会有点难度，我正试图以尽可能容易理解的方式来提炼这些概念，但过程中可能还是会遇到一些挑战。

所以请大家做好笔记，以巩固所学概念。谢谢大家，期待在下一次讲座中见到你们。大家好，我是Raj Dhandekar博士，2022年从麻省理工学院获得机器学习博士学位，也是“从零开始构建深度探索”系列的创始人。在开始之前，我想向大家介绍本系列的赞助商和合作伙伴——InVideo AI。大家都知道我们非常重视从基础构建AI模型的内容，InVideo AI的理念和原则与我们非常相似。

让我来为你展示一下。这里是InVideo AI的网站，凭借一支精干的工程团队，他们打造出了一款令人惊叹的产品——仅需输入文字指令就能生成高质量AI视频。正如你所见，我输入了一段文字指令："制作一款超写实的高端奢侈腕表广告视频，呈现电影级质感"。点击生成视频后，很快我就获得了这段令人惊艳的成片，其逼真程度令人叹服。最让我震撼的是视频对细节的把握，看看这个画面，材质纹理的表现简直不可思议。而这一切都源自于短短的文字指令，这就是InVideo产品的强大之处。

你刚刚看到的精彩视频背后，是InVideo AI的视频创作流程在支撑。他们正从底层原理重新构想视频生成与剪辑方式，致力于基础模型的实验与优化。该公司拥有印度规模最大的H100和H200计算集群之一，同时也在测试B200芯片。作为印度发展最迅猛的AI初创企业，InVideo AI正为全球市场打造产品，这正是我如此认同他们的原因。好消息是，他们目前有多个职位正在招聘。

你可以加入他们出色的团队，我会在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建深度搜索”系列的下一个讲座。今天我们将学习一个非常重要的概念，那就是注意力机制的必要性。首先，让我快速总结一下我们在这个系列讲座中已经涵盖的内容，然后再来讨论今天的主题。在之前的两节课中，我们学习了名为“深度搜索基础”的内容，

我们探讨了深度探索架构的四个阶段，或者说我们将在这个系列讲座中涵盖的四个阶段。第一阶段是深度探索架构的创新。第二阶段是训练方法。第三阶段是GPU优化技巧，第四阶段是模型生态系统。在上一讲中，标题为LLM架构。

我们开始特别关注第一阶段。我们的主要目标是，在接下来的两到三节课中，必须开始理解多头潜在注意力机制，这是深度探索架构中的第一个重大创新。然而，要理解MLA（多头潜在注意力），我们不能直接开始研究这个概念，而是需要循序渐进地接近它。因此，我们首先研究了LLM本身的架构，发现LLM的架构大致是这样的。

首先我们有了输入部分，然后有了处理器，接着是输出部分。每个部分基本上都有不同的构建模块。所以我们发现，如果把LLM看作引擎的话，要真正理解这个引擎如何运作，我们确实需要打开引擎，看看里面到底有什么。

那么，大型语言模型（LLM）究竟是如何学会预测下一个标记或下一个词的呢？如果把这个过程类比为汽车的运动，我们会发现，当你给汽车加油时，汽车就会动起来，对吧？引擎里发生了一些变化，汽车就移动了。同样地，对于LLM来说，"燃料"就是你输入到引擎（即LLM）中的单词序列，而输出则是下一个词的预测或下一个标记的预测。这就是为什么我们也把LLM称为"下一个标记预测引擎"。

如今这个引擎拥有海量参数。GPT-3的参数量达1.75亿个，GPT-4可能达到约1万亿个，而最新发布的GPT-4.5参数量可能在5到10万亿之间。为了真正理解这个引擎的运作原理，我们打开了这个黑匣子，从以下三个维度进行了剖析。

但我并没有直接向你解释这三个方面。我告诉你，如果你把自己想象成一个令牌或一个世界，并想象自己经历不同的阶段会怎样。所以如果你是一个令牌，首先你会被分配一个令牌ID，那是你的徽章，然后你会被分配一个令牌嵌入向量，接着是一个位置嵌入向量，令牌嵌入和位置嵌入相加，就形成了你的制服——输入嵌入。

同样地，无论你的邻居是谁，他们也都有自己的统一输入嵌入表示，你们一起搭乘通往Transformer的列车。每个Transformer块包含多个组件，如归一化层、多头注意力机制、随机失活层、跳跃连接、又一个归一化层、前馈神经网络和另一个随机失活层，最后再接一个跳跃连接。而这仅仅是一个Transformer模块的结构。

像这样的Transformer模块有很多，可以是12个、24个、96个等等。因此，作为一个标记（token），连同你的统一表示（即输入嵌入），你必须经过所有这些Transformer模块的处理。当你通过所有模块后，会有一层归一化处理，最后是一个输出层。如果你是一个768维的向量，你将被映射或投影到一个50,000维的向量中。为什么是50,000？因为这是词汇表的大小，这实际上帮助我们选择下一个标记。

这就是令牌在LLM架构中的完整生命周期。在今天的讲座中，我们将重点讨论整个架构中的一个单一环节——多头注意力机制。我希望你们能理解多头注意力机制在其中的作用位置。在所有这些步骤中，令牌会经过多头注意力机制，它出现在Transformer块中，而且在这个特定的情况下，它位于归一化层之后。今天，我们将要理解多头注意力机制，但在此之前，我们首先要理解什么是注意力本身，以及为什么需要注意力机制。我们为什么要开始讨论“注意力”这个术语？为什么它在最近变得如此流行？因此，我们今天的目标是激发自注意力这个概念。

我们今天不会深入探讨自注意力机制的数学原理。今天的核心目标是理解为什么我们首先需要注意力机制，以及它为何能成为大型语言模型的关键突破点。试想一下，在所有处理词元的步骤中，最重要的环节就藏在Transformer模块里——确切地说，就藏在Transformer模块的某个步骤中，那就是注意力机制。

这就是为什么我在这里用不同的颜色标记出来。这个训练模块本质上提供了所有使LLM在理解语言方面表现优异的特性。因此，为了真正理解注意力机制的工作原理，我专门为你们准备了这堂单独的课程，我们将尝试阐述引入注意力机制的必要性。

在理解我们为何需要注意力机制以及它如何改变了这一领域之前，让我们先深入探讨一下生成式人工智能本身。让我们先回顾一下历史。我认为理解注意力机制的本质至关重要。20世纪60年代有一个叫ELISA的聊天机器人。你可以想象一下。我不会称它为大型语言模型。

我会称它为聊天机器人。这是第一个旨在充当治疗师的自然语言处理聊天机器人。所以如果你问“请告诉我你有什么困扰”，我可以说我在学习人工智能方面遇到了困难。你能帮我吗？你觉得学习人工智能很困难是正常的吗？你看它其实没那么有用，但要记住那是在1960年代。那时候这被认为是一场革命。所以你可以看到这个原始程序是由约瑟夫·维森鲍姆在1966年描述的。

与此相比，看看现在的ChatGPT，我们说我想学习人工智能。你能帮我吗？然后让我把模型切换到GPT 4.0，这样我能得到更快的回复。接着你会看到，我立刻获得了一份可操作的项目清单，可以立即开始实施来学习人工智能。因此，在短短64年的时间里，我们在自然语言处理和语言理解领域已经取得了长足的进步。

让我们来看看从20世纪60年代到2020年代发生了什么。20世纪60年代是ELISA聊天机器人问世的时期。然后在20世纪80年代和1997年，出现了生成式AI或语言建模领域的两个重要基础构建模块，你可以把它们看作是20世纪80年代出现的循环神经网络和1997年问世的长短期记忆网络。

这两者都建立在20世纪70年代神经网络蓬勃发展的基础上。人们发现神经网络可以做很多了不起的事情，但神经网络真正的问题在于它们本质上无法处理记忆，这是神经网络最大的缺陷。为什么必须处理记忆呢？因为想象一下，如果你要预测下一个单词是否正确，或者生成一些新文本，你会想知道段落的开头是什么。

仅仅记住一点信息是不够的。上下文非常重要，这个词在今天的讲座中会频繁出现。上下文本质上意味着，如果给模型提供了一大段文字，而我们希望模型执行某项任务或仅通过查看句子来回答问题，仅靠句子中紧邻出现的单词是没有帮助的。

我们需要先了解这段话开头所讲述的背景。比如说，我先提到我来自印度浦那，然后接着写了一大堆不同的内容。我写了一大堆句子，最后才问我说的是什么语言。现在要根据这个问题生成一个回答。中间提到的所有这些内容并不是很重要，但我需要了解这段开头的内容，因为这才能真正帮助我回答这个问题。这就是为什么需要记忆功能。而神经网络原本并没有真正编码记忆的机制，但循环神经网络和长短时记忆网络解决了这个问题。让我快速展示一下它们是如何解决这个问题的。

因此，人们最初通常使用RNN或LSTM进行序列到序列建模，也就是所谓的序列到序列翻译任务。简单的序列到序列翻译任务可以仅仅是语言翻译。如果你想处理一串单词，并希望将它们从一种语言翻译成另一种语言，你可以使用循环神经网络。

所以循环神经网络有一堆隐藏状态，比如h0、h1、h2、h3。假设这些是输入单词"I will eat"，我想把它翻译成法语，即"Je vais manger"，已经写在这里了。我可以用循环神经网络进行这种翻译的方式是，我有一个编码器块和一个解码器块。

这些编码器和解码器块都有一个称为隐藏状态的东西，你可以将其视为向量或矩阵。这里是编码器的隐藏状态h1、隐藏状态h2、隐藏状态h3，然后我们有解码器的隐藏状态s1、隐藏状态s2和隐藏状态s3。让我们可视化编码器块中发生的情况。

在编码器模块中，首先输入句子的第一个单词"I"，它会转换为第一个隐藏状态h1；接着输入第二个单词。需要注意的是，要获得第二个隐藏状态h2，不仅要使用第二个输入，还要利用前一个隐藏状态。这就是循环神经网络捕捉记忆的方式。

前一个隐藏状态用于计算当前的隐藏状态。同理，当你来吃饭时，输入的是x3，但要计算最终的隐藏状态h3，你需要使用前一个隐藏状态h2，而h2编码了h1或者说h2包含了h1的信息。因此，h3现在既包含了h1的信息，也包含了h2的信息。

h3拥有关于过去的知识，它保留了记忆。因此，h3可以被视为一个向量，可能是500维或1000维的向量，本质上包含了输入的所有信息含义。这是唯一一个传递给解码器的高维向量。解码器随后会查看这个向量，解码器的每一个隐藏状态都开始对其进行解码。

所以解码器想象一下你有一个解码器，就像接力赛一样。我们可以把它想象成一场接力赛。接力棒在h1手中，它传给h2，h2再传给h3。现在h3拿到了接力棒，并将其传递给解码器。现在我是解码器的第一个隐藏状态。我拿到了接力棒。

我先解码第一个标记JOR，然后将其传递给第二个解码器，接着解码第二个标记，最后解码第三个标记。这就是循环神经网络中语言间翻译的工作原理。LSTM（长短期记忆网络）在预测当前隐藏状态时与前一个隐藏状态有所不同。它们还维护一种称为细胞状态的结构。因此，LSTM能够兼顾长期记忆和短期记忆。

所以这本质上是为了扩大它们处理的上下文窗口。但架构从根本上保持不变。只是对于LSTMs来说，底层的数学要复杂得多。现在你看出问题了吗？看看这个例子，我们之前用它来讨论上下文问题。那么，当处理大段文字时，循环神经网络存在什么问题呢？比如说你有一大段文字，你想翻译整段内容。

循环神经网络在处理这个问题时存在什么缺陷呢？让我们举一个简单的例子，以便更清楚地解释这个问题。假设你眼前有这样一段文字——这是我随机从ChatGPT生成的文本中截取的。我希望你这样做：首先花30秒到1分钟或更长时间完整阅读这段文字一次。然后闭上眼睛，将它翻译成你想要的任何语言。

你可以把它翻译成印地语、西班牙语、法语、德语，或者你会的任何其他语言。所以，闭上眼睛，把它翻译成另一种语言。你能做到吗？这不可能，对吧？因为当你闭上眼睛时，你基本上只能记住你读过的一些概要。

你并不需要记住每一个确切的单词，但为了翻译，我们不得不逐字逐句地看并单独翻译，对吧？这正是循环神经网络所面临的问题。为什么呢？因为我们只依赖一个隐藏状态或一个向量来捕捉过去发生的所有上下文信息。

这就像你闭上眼睛，试图回忆整段内容。这个隐藏状态或向量承受着巨大的压力，要重新唤起所有记忆——如果是一大段文字，你怎么能把所有信息压缩进一个500维或1000维的向量里呢？自然会有信息丢失。而且别忘了，解码器并不会从第一个或第二个隐藏状态获取输入。

它仅将最终的隐藏状态作为输入。这就是主要问题所在。从编码器传递到解码器的上下文仅来自最终的隐藏状态。这就是为什么这被称为上下文瓶颈问题。最终的隐藏状态就像你闭上眼睛试图记住整段文字时的情况——这是不可能做到的。你不能给一个向量施加这么大的压力。

循环神经网络中的上下文瓶颈确实不利于保留长距离上下文，这为理解注意力机制的出现提供了很好的途径。现在，如果你看一下循环神经网络，让我在这里擦掉一些东西。你认为这个问题的理想解决方案是什么？假设这就是我们目前遇到的问题——所有上下文信息无法被压缩到单个向量中。

这个问题的解决方案是什么？嗯，解决方案似乎是这样的：比如说，当我们在解码时，当我解码这个的时候，假设我在解码器的第一个隐藏状态下进行解码，与其只能访问最终的隐藏状态，不如让我也能访问编码器的所有隐藏状态以及所有的输入。

因此，如果在解码过程中我能获取所有输入，那么我就可以做出预测，比如在解码第一个标记时，我应该更重视第一个隐藏状态，而不应该重视第二个隐藏状态，也不应该重视第三个隐藏状态。如果我能够创建一种机制，能够理解对不同标记需要赋予的相对重要性，那会怎样呢？

这是本次讲座最重要的部分，请大家务必集中注意力。解决上下文瓶颈问题的关键在于：在解码过程中，我需要建立一种机制来量化每个隐藏状态应获得的注意力权重。比如在首次解码时，我可以给第一个隐藏状态分配80%的权重，第二个和第三个隐藏状态各分配10%的权重。

如果我掌握了这些信息，那么上下文瓶颈问题就迎刃而解了。因为即使段落很长，只要我能关注到该段落中的所有标记，我实际上也能重视之前出现的内容。比如说这是句子，我在这里预测某个内容。在注意力机制中，我可以尝试确定应该给予第一个标记、第二个标记、第三个标记以及整个段落多少注意力，然后我可以说我需要对这些标记给予最大的关注。

所以你看，我已经开始在这里使用“注意力”这个词了。你可以把它理解为对段落中各个标记的相对重要性。因此，理想的解决方案是我们在解码过程中需要有选择地访问输入序列的部分内容。

因此，当你在解码时，比如说我想做出某些预测，这样如果我在解码第一个标记，对吧，这个alpha 1 1就是赋予第一个隐藏状态的相对重要性，alpha 2 1是赋予第二个隐藏状态的相对重要性，alpha 3 1则是赋予第三个隐藏状态的相对重要性。

所以我希望能做出这样的预测：α₁₁为100%，α₂₁为0，α₃₁为0，诸如此类。但你看这里，我不再仅仅依赖h₃，而是依赖之前所有的隐藏状态，不仅如此，我还在量化应该对每个之前的隐藏状态依赖多少。因此，这一点很重要，我们需要在解码过程中有选择性地访问输入序列的某些部分。我想通过这个例子再次向你们解释这一点。假设我在这里截了一张图，然后把它放在这里。让我们试着理解这句话的真正含义。如果我让你翻译这一段，你会怎么做？你可能会从——比如说这个部分开始，假设你从这里开始，看看你一次能看多少个单词。

假设这就是你的上下文窗口，你无法一次看到更多的单词，那么你的大脑会怎么做呢？对于你的大脑来说，段落中剩下的内容是不相关的，对吧？所以你会在大脑中屏蔽掉所有这些内容，你会屏蔽掉所有不相关的东西。因此，你会认为所有这些其他内容对你来说都不重要，你只想关注当前的上下文，并选择性地忽略其他所有内容，对吧？所以你的大脑会集中注意力——这一点非常重要——你的大脑会集中注意力于段落的这一部分，然后开始只翻译这一部分的内容。翻译完这一部分后，你的大脑才会移动到下一部分。

也许接下来的五个词翻译完后，你的思绪会转移到下五个词，然后再到下五个词，依此类推。所以你会看到自己在某个特定时刻的行为：你的大脑正在做出决定，当你在看第一个上下文窗口时，我想要对这些标记给予最大的关注。同时，你的大脑也在做出一些定量的判断，完全不去关注其余的标记，完全不去关注这些。这就是在解码过程中选择性访问输入序列部分的真正含义，对吧？选择性访问输入序列的部分——这是理解注意力机制最重要的一句话。

这意味着只关注输入序列中在当下重要的部分，这看起来像是一种直觉行为，对吧？但实际上，这是帮助语言模型变得更好的最重要因素。想象一下，你想在一段文字中找出拼写错误，你会怎么做？你会逐步浏览这段文字，先关注一个部分，然后再关注下一个部分，以此类推。这正是大型语言模型也需要做的——它们需要选择性地关注序列中的特定部分，然后找出例如含有拼写错误的那部分。

没错，这就是为什么我们在解码时需要注意力机制的核心原因——我们可以量化每个输入标记应该获得多少重要性或关注度，这就是我现在要表达的重点。实际上，第一个真正实现这一点的论文并不是《Attention Is All You Need》。你看这篇论文（《Attention Is All You Need》）目前约有117万次引用，引用量惊人。很多人误以为注意力机制最早是在这篇论文中提出的，但实际上首次提出注意力机制的论文是这篇《Bahdanau Attention Mechanism》。

所以如果你搜索“Bahdanau Attention”并点击这个，你会发现这是第一篇真正将注意力机制应用于翻译任务的论文。我认为这篇论文发表于2014年，并在2015年的ICLR会议上发表。他们的主要目的是实现了这种架构，使我们能够选择性地决定对每个隐藏状态分配多少注意力，这就是所谓的a1、a2、a3——注意力分数。基于此，你可以执行翻译任务，他们基本上证明了如果我们使用注意力机制，可以以更好的方式进行序列到序列的翻译。所以请记住这篇特别的论文。

所以在这里你可以看到，在这个图表上，x轴是英语句子，y轴是法语翻译。对角线基本上显示了英语单词最关注翻译的法语单词。有趣的是，顺序并不相同。比如"European Economic Area"（欧洲经济区）在英语中是这么说的，但翻译成法语时变成了"zone économique européenne"（经济欧洲区），所以它被翻译为"zone économique européenne"。

所以这并不是逐字逐句的直接翻译，但注意力机制本质上能识别出——看，这里是最亮的对吧？这意味着法语中的"european"对应英语中的"european"，尽管它们出现的位置并不相同。英语的"european"出现在第五个位置，而法语的"european"出现在第七个位置。因此，所有最亮的区域并不都在对角线上，有些偏离了对角线本身——这些细节也都被注意力机制捕捉到了。

这个图表最酷的地方在于这些非对角线元素，因为翻译并不总是逐字进行的。在某些语言中，有些词会先出现，而在其他语言中则后出现，这取决于名词、词汇等的排列方式。这就是这篇论文的亮点所在，这是第一篇将注意力机制应用于翻译任务的论文。随后，《Attention Is All You Need》这篇论文引入了Transformer模块，并将注意力机制整合到了Transformer模块中，这是该论文的主要优势或独特之处。

是这样的，RNN（循环神经网络）和LSTM（长短期记忆网络）在语言翻译方面表现不错，但它们存在上下文瓶颈问题。于是研究人员发现，构建深度神经网络其实并不需要RNN架构。2014年，Bahdanau注意力机制被提出，他们在保留RNN的同时引入了注意力机制。你可以把Bahdanau注意力机制中的编码器-解码器模块想象成类似结构，只是额外添加了这个注意力机制。三年后（也就是2017年），研究人员进一步发现，即便是RNN架构，在构建自然语言处理的深度神经网络时也并非必需，于是他们提出了Transformer架构。

那么让我们回顾一下这个领域的发展历程：20世纪80年代是RNN的时代，1997年诞生了LSTM（其实1966年就出现了ELISA——让我把这个也写上）。到了2014年，注意力机制被提出，但当时仍依附于RNN的编码器-解码器架构。真正的转折点在2017年，研究人员发现自然语言处理任务其实不需要RNN结构。于是RNN被彻底摒弃，只保留了注意力机制的核心思想，但这次它与Transformer模块相结合——这正是2014年论文与2017年论文的本质区别：2014年的架构仍保留RNN模块，而2017年则用Transformer架构完全取代了它。

于是这就形成了完整的架构——现在我们有了Transformer模块，其核心就是注意力机制。这就是GPT的架构，它与原始Transformer论文中的架构不同。原版Transformer同时包含编码器和解码器，而我现在展示的这个架构只有解码器部分。目前不必对此感到困惑，今天课程的主要目的是让大家从自然语言处理的发展历程中理解为何需要引入注意力机制。关键要记住的一句话是：在解码过程中，我们需要选择性地访问输入序列的特定部分。最初，注意力机制是在RNN中率先提出的。

后来研究人员发现，好吧，让我去掉RNN，但仍然尝试将注意力机制融入其中。于是他们在2017年将其与这里的Transformer模块合并。到了2018年，GPT架构问世，于是就有了注意力机制加GPT。GPT基于原始的Transformer架构，但与同时包含编码器和解码器模块不同，它只保留了这里的解码器模块，并保留了注意力机制。我们之前讨论过为什么需要注意力机制，这一点在LLM架构的这个部分得到了体现。

现在我们将开始探讨：如果你观察下一个标记预测任务，自注意力机制究竟是什么？上下文向量又是什么？注意力机制的主要目的是什么？那么，让我们来看看自注意力机制在下一个标记预测中的主要目的是什么。既然我们已经理解了注意力机制及其发展历程，现在就来学习这个概念吧。让我们试着理解“自注意力”这个术语，它实际上意味着什么？

自注意力机制意味着它是一种让输入序列中的每个位置都能关注到同一序列中所有位置的机制。也就是说，如果我有一个句子比如“明天是晴天”，自注意力本质上指的是，到目前为止如果你看RNN，我们看到的是解码器需要给予编码器的注意力。所以如果第一个解码词是法语词，我们基本上是在看两个不同的序列之间的关系。假设英语序列是这样的，比如“我会吃”，法语序列是这样的，这是法语序列，我们在看如果你在做第一次解码，你应该给予英语序列多少注意力。

因此，这里的注意力机制是在序列之间进行的，而不是在同一个序列内部。而自注意力机制则是在预测下一个标记时使用的，这通常是大型语言模型（LLM）的做法。由于LLM的任务是预测下一个标记，它们并不是专门为翻译任务而训练的。在预测下一个标记时，本质上你并不需要区分不同的语言，你只需要处理一堆数据即可。

因此，与其在两个序列之间建立注意力机制，你只需取同一个句子，比如说，当你观察下一个词时，你试图找出：如果关注一个标记或一个词，我应该对句子中所有其他标记给予多少注意力。这是这里最重要的理解点。如果你关注一个标记或一个词，那么邻近的词对这个特定标记有多重要？为什么这个知识如此重要？你能试着思考为什么这个知识对我们至关重要吗？为什么我们需要编码一个给定标记周围其他标记的信息？

这一知识对我们之所以重要，是因为在预测下一个词时，本质上需要了解序列的上下文信息，需要掌握不同词语之间的关联。再以同一个例子说明，假设我说"我来自印度浦那，我会说"，就以这句话为例，当你看到"说"这个词时，我需要知道在"说"之后

必须高度关注浦那和印度，也许其他所有词汇都不那么相关，因为我所说的方言深受我所在地区的影响。因此，当你观察一个词汇云时，如果你的Transformer架构或LLM引擎掌握了某个词与周围其他词的关系信息，以及需要赋予这些周边词汇多少权重，那么仅凭一个标记就能非常非常容易地预测下一个标记。

这就是自注意力机制变得非常重要的原因。如果没有自注意力机制，你就会丢失关于其他标记与我们选择的特定标记之间关联的上下文信息。我希望现在你能理解为什么它被称为"自注意力"。在这个案例中，当我们处理序列到序列的语言翻译时，注意力机制作用于不同序列之间；而自注意力则是当我们观察单个句子本身时，关注句子内部的标记，并从根本上理解这些标记之间是如何相互关联的。

让我们再次以同样的例子来说明，第二天是光明的。记住，当这些标记进入转换器架构时，它们现在是向量，正如你之前所见，它们现在具有统一的维度。记住，每个标记都有一个统一的768维向量，这就是输入嵌入。所以，每当我在这里展示这些块时，本质上就是指一个向量。

因此，第二天是明亮的，现在这些都是向量，对于一个转换器来说，它不理解单词，也不理解句子，它只知道每个标记都是一个向量。所以“the”是一个向量，我称之为x1，“next”是一个向量，我称之为x2，“day”是一个向量，我称之为x3，“is”是一个向量，我称之为x4，“bright”是一个向量，我称之为x5。现在，如果我在看一个特定的词，比如我在这里展示的“next”，我想看看如果我关注这个词“next”，我应该给予其他所有标记多少注意力，而这个注意力是由alpha to 1给出的，或者我称之为符号alpha to 1。

为什么要关注第一个标记，因为下一个是第二个标记，我想找出第二个标记与第一个标记之间的注意力分数，即α1，这将是α2，这将是α3，这里将是α4，这里将是α5。本质上，如果我在看一个特定的标记，我想找出所有的注意力分数，这被称为查询，我现在关注的标记被称为查询标记，我想找出如果我在看查询，我应该给所有其他标记多少注意力，这些有时在通用术语中也被称为键，所以最终我想做的是，假设我得到了这些注意力分数，我想利用这些信息。

我想以某种方式获取所有这些信息，并将这个向量从输入嵌入向量转换为上下文向量。这里有一个非常重要的区别需要说明：目前next是一个输入嵌入向量，对吧？所以它包含了词元嵌入加上位置嵌入。但上下文向量是完全不同的东西。如果这是next的输入嵌入向量，而我在同一空间中绘制上下文向量，那么这就是next的上下文嵌入向量。实际上，上下文向量比词元嵌入向量丰富得多，为什么呢？

因为词嵌入向量或输入嵌入向量不包含相邻单词的信息，但现在我的上下文向量包含了相邻单词的信息，这些信息现在被融入到了我的输入嵌入中。所以，如果你有一个输入嵌入向量，也就是我之前提到的那个统一的向量，并且你用关于相邻单词的上下文信息来增强这个输入嵌入向量，我们将会看到这种增强是如何实现的，但本质上这会导致所谓的上下文向量。因此，注意力机制或自注意力机制的整个目标就是将所有的输入嵌入向量转换为上下文向量。

因此，所有这些统一的向量——我们刚才看到的这些统一向量，所有词元在经过归一化层后都会输出一个768维的统一向量。当它们进入多头注意力层时，输入的是嵌入向量，而输出的则是上下文向量。经过注意力模块处理后，输出的内容会丰富得多，这就是为什么我用不同颜色标记它们。之所以更丰富，是因为现在它还编码了其他词元的信息，保留了上下文。因此，上下文向量是一种经过丰富的嵌入向量，它融合了所有其他输入元素的信息。在自注意力机制中，上下文向量扮演着至关重要的角色。

它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住：输入嵌入向量仅包含该单词或标记本身的信息——它可能编码了该单词的含义及其位置信息，但对相邻元素一无所知。而上下文向量则能感知相邻元素，因为邻居关系至关重要。试想，当你阅读一个句子或段落时，单个标记本身毫无意义，正是它们与相邻元素的关联关系才最终构成了段落的上下文语境。对于大语言模型而言，这种机制之所以必要...

需要理解句子中词语之间的关系和关联性，这实际上是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个关键转折点——距今十年前——当注意力机制被引入后，人们开始意识到：与其孤立地看待词语，不如退后一步，尝试理解不同词语之间本质上是如何相互关联的。这样我们就能从文本中挖掘出最丰富的内涵，因为就像图像由像素模式构成那样，只有当您将所有词语作为一个整体并观察它们之间的关联时，文本或段落才具有真正的意义。

那么现在的问题是，你已经有了一个输入嵌入向量，比如说对于下一个词，如何将其转换为上下文向量？也就是说，你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频片刻，好好想一想。你有了输入嵌入向量，假设还有这些注意力分数，你会如何修改输入嵌入向量，使得这些注意力分数能够被纳入考虑，从而得到上下文向量？你可以在这里暂停一下，首先也可以思考一下这些注意力分数本身是如何计算的。

好的，最简单的做法是这样的：假设你有一个向量，还有其他所有向量，我们为什么不简单地做个点积呢？比如你有“next”这个词的输入嵌入向量，还有“though”的输入嵌入向量，就在这两个向量之间做个点积，这样就能得到α2,1；然后在“next”和“next”之间做个点积，得到α2,2；接着在“next”和“day”之间做个点积，得到α2,3；再在“next”和“is”之间做个点积，得到α2,4；最后在“next”和“bright”之间做个点积，得到α2,5。一旦你得到了所有这些α值，

你可以简单地计算 alpha 2 1 乘以 x 1 加上 alpha 2 2 乘以 x 2 加上 alpha 2 3 乘以 x 3 加上 alpha 2 4 乘以 x 4 加上 alpha 2 5 乘以 x 5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你这里有一个向量 v1，另一个向量 v2，它们之间的点积会比比如说 v1 和 v3 之间的点积更高。所以如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的东西。你可能会想，我希望量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，当然它们应该有一个更高的注意力分数，所以这个计算对我来说似乎是有道理的。

我只是计算点积，然后用各自的点积缩放不同的向量并将它们相加，所以无论这个和是多少，它就会成为下一个词的上下文向量。同样，我也可以为所有其他标记找到上下文向量。这种方法有什么问题？为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们试图编码的上下文。我希望你从基本原理出发思考，我很快就会揭晓答案。好的，主要的答案是，假设你考虑这个句子“狗追球但没抓住”，对吧，

狗追着球跑，但没能抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。好的，如果“它”现在是我的查询向量，你是如何决定计算查询向量与其他向量之间的注意力分数的呢？你决定使用点积对吧，这正是我要做的。如果“它”是我的查询向量，为了得到它与“狗”之间的注意力分数，我只需计算“它”与“狗”的点积。如果我计算点积，结果是0.51；如果我再计算“它”与“球”的简单点积，结果也是0.51。你看出来问题了吗？这两个注意力分数完全相同，但这并不是我想要的。当你说“但没能抓住它”时，

实际上这里指的是球，对吧？狗追着球跑但没抓住，所以第二个"它"指的是球而非狗。因此我在看这句话时需要更关注球而不是狗，让我用不同颜色的笔标注一下以便更清晰——我必须把注意力重点放在"球"上而非"狗"，但现实情况并非如此。如果简单做点积运算，算法机制里并没有设置让"球"比"狗"获得更高权重的编码规则，当我们处理这句话时，"狗"和"球"不应该获得同等的注意力分数。这个例子精彩地论证了为什么需要对不同词元进行选择性关注——"狗追着球跑但没抓住"。

第一个词是“the”，所以如果这是查询标记，它会更多地关注“dog”；但现在这是查询标记，它应该更多地关注“ball”。我不希望两者具有相同的注意力分数，因此简单的点积无法区分这里微妙的上下文关系，它没有考虑“chase couldn't catch”的上下文或语言上的细微差别，比如“catch”更可能指的是移动的物体，也就是“ball”。所以主要问题是，简单的点积只能衡量语义相似性，但无法处理上下文问题，而且许多句子可能都有这样的上下文复杂性，对吧？我需要编码一种机制，以便能够捕捉这些复杂性，但我不知道这种机制会是什么。于是我们采用了研究人员长期以来使用的技巧。

如果你不明白事物之间的内在联系，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。注意力机制领域的情况正是如此，研究人员基本上无法确定那个机制可能是什么。这就是机器学习领域或深度学习领域与物理学不同的地方，对吧？

在物理学中，如果你遇到这个问题，可能会花上六个月到一年的时间去研究一个定律，以揭示其背后的机制来捕捉复杂性，或是建立一个能捕捉背景的机制。但在深度学习领域，你不需要这么做。你会说，我要用一堆矩阵来代替它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？

所以他们发明了新的矩阵，我们暂且称之为查询矩阵和键矩阵。这意味着，与其仅仅查看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。比如，如果这里的查询是“it”，我就会用所谓的查询矩阵与之相乘。这个矩阵可以是一个高维矩阵。对于“dog”来说，“dog”和“ball”就是键，因为键本质上就是除了查询之外的所有其他标记，也就是“dog”和“ball”。

所以你用键矩阵将它们相乘。现在看这里的优势在于，如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵具有任何特定含义——你只需随机初始化它们，然后通过反向传播来训练它们。这正是研究人员长期使用的深度学习技巧：当你无法自行推导出关系时，就退后一步，让神经网络来完成它的工作。与其通过强加某些规则来限制神经网络，不如让它自己找出答案。

现在你看到了优势所在，我们掌握了多个可训练的因素。假设WQ是3×3的矩阵，WK也是3×3的矩阵，对吧？比如“狗”和“球”这些就是我的键值。而这些是它们的嵌入向量，也就是我们之前看到的输入嵌入。现在，我将这些输入嵌入与查询相乘，将这两个与键相乘。具体来说，就是3×3的矩阵乘以3×1的向量，结果会是一个3×1的向量。这样一来，键值就变成了0.9、0.2、0.1和0.1、1.8、0.1。你会发现这些值发生了变化，因为它们与矩阵相乘了。而查询部分也会与查询矩阵相乘。

因此，针对它的查询将变为0.5 0.1和0.5 1.0以及0.1。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正在从输入嵌入空间转换到一个不同的空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的。因此，如果你计算“它”和“球”之间的注意力分数，你会发现是0.56，“它”和“球”之间的分数是0.96。而如果你计算“它”和“狗”之间的注意力分数，结果是0.56。所以在这里，你可以看到“它”和“球”之间的注意力分数是0.96，比“它”和“狗”之间的分数要高，后者较低。

所以这些显然是不同的注意力分数，因此添加这些可训练的矩阵实际上对我们有帮助。为什么会有帮助呢？因为它提供了一些可调整的参数，使我们能够编码标记之间的一些复杂关系。如果你采用简单的点积，注意力分数将是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，将在下一节课中介绍），本质上，如果你只有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多可以调整的参数。

所以如果你在这一部分感到困惑，让我再重复一遍：我们开始这一部分时思考的是，如果你有一个输入嵌入向量，那么你可以对这个输入嵌入向量做什么来得到上下文向量？为了得到上下文向量，我们本质上需要阿尔法值。得到阿尔法值后，你只需要将它们与输入嵌入向量相乘，就能得到上下文向量。但接下来的问题是，你如何在一个输入嵌入向量和另一个输入嵌入向量之间得到阿尔法值？你如何得到注意力分数？最简单的方法可能是取点积，但我们看到，假设这是句子，如果它是我的查询...

如果我想计算“it”（它）与“ball”（球）以及“it”与“dog”（狗）之间的注意力分数，我会先计算“it”与“ball”的点积，结果为0.51；再计算“it”与“dog”的点积，结果同样是0.51。这样一来，两者的注意力分数相近，但这并非我想要的——因为当我说“it”时，我希望它指向“ball”，所以“it”与“ball”的注意力分数应当显著高于“it”与“dog”的分数。那么如何实现呢？显然，点积运算缺乏捕捉这种上下文关系的复杂度，我需要更多可调节的参数，但目前尚不清楚具体需要哪些控制机制。

但让神经网络或反向传播算法来找出这些参数的最佳值，至少让我先随机初始化它们。这时新术语就派上用场了——我需要可训练的矩阵，姑且称之为查询矩阵。通过将输入嵌入与查询矩阵相乘，我可以将其转换到另一个空间。而对于表示"狗"和"球"的关键词输入嵌入，它们将与键矩阵相乘并转换到另一个空间。然后我就能在这些转换后的向量之间计算注意力分数。如果模型能正确学习这些矩阵参数，就能学会计算"它"和"球"之间的注意力分数是0.96（高于"它"和"狗"之间的0.56）。不必担心这些乘法运算或数学细节。

现在我将详细讲解数学部分，这将在下一节课中进行。目前只需记住，我们不知道如何从物理上捕捉上下文关系，所以这就像是一种简便方法，一种技巧。你引入了查询（queries）和键（keys），这些都是随机初始化的可训练矩阵，然后对它们进行训练。你可能听说过“查询”和“键”这两个词，但实际上引入它们并没有确切的物理原因。唯一的原因是人类无法自行找到计算这些注意力分数的方法，我们唯一知道的就是这样。

如果我们无法理解它，就让我将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数，然后希望训练过程本身能自行解决这个问题。人类在计算机视觉领域也采用了类似的技巧：如果你训练一个卷积神经网络（CNN）来区分狗和猫，你无法自己写下所有特征，而是依赖CNN来完成。这里的情况也类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，以及还有一个称为值矩阵的矩阵，它们将如何用于我们将在下一讲中看到的下一个令牌预测任务。

所以下一讲将全面探讨自注意力机制的数学原理——我们将如何处理查询矩阵、键矩阵和值矩阵？如何精确计算上下文向量？从这些上下文向量出发，我们最终要通过哪些步骤来获得下一个开放预测？因此下节课本质上是对刚才所见内容的深度剖析，并将其扩展为一整节数学原理课程。

但在今天的讲座中，我只是想激发大家对查询、键和值这些概念的兴趣，这些我们尚未深入探讨的内容将在下一讲中详细展开。好了各位，今天的讲座就到这里，你可以将其视为注意力机制发展历程与自注意力机制入门的一次融合。作为下个开放式预测任务的总结，请记住注意力机制的演变过程：最初我们有Elisa，它在当时堪称革命性的突破，考虑到它诞生于1966年，至今仍令人惊叹。

接着出现了循环神经网络和长短期记忆网络（LSTM），它们存在上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了注意力机制，它计算解码输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力机制加RNN。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

所以他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并再次以注意力机制为核心构建了这一架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们讨论自注意力时，我们实际上只关注一个序列，因为它将用于下一个标记预测任务。

因此，在像GPT这样的下一个标记预测任务中，我们使用自注意力机制，即观察一个标记及其如何关注周围或邻近的标记。我们正在观察的标记被称为查询（query），其他标记被称为键（keys）。我们的目标是找到查询向量与键之间的注意力分数。我们认识到，注意力机制的主要目的是获取这些注意力分数，并将其转换为上下文向量。上下文向量是输入嵌入向量的一种更为丰富的表现形式。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但没接住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵。

因此我们需要增加参数，以便有更多可调节的"旋钮"来操作这些可训练的矩阵。这些矩阵被称为查询权重矩阵和键权重矩阵。此外还有值权重矩阵，我们将在下节课中介绍。所有输入嵌入都会与查询权重矩阵相乘，得到查询矩阵；同样地，我们也有键矩阵。这样，注意力分数就不再是在向量的输入嵌入之间计算，而是在查询和键之间计算。


and since we have a flexibility of so many parameters to play with we hope that when we train the parameters they will learn that the attention score between the it the second it second it and the ball is higher than the attention score between the second it and the dog so it captures more contextual complexities so addition of this trainable weight matrices captures more contextual complexities and that's why we humans added these weight matrices and then we call them queries keys and values because it it sounds cool and also it relates to the field of information
=======
他们遇到了上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了一种称为注意力机制的机制，它计算解码器输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力加RNN的结合。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

于是他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心就是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并构建了这种以注意力机制为核心的新架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们谈论自注意力时，实际上我们只关注一个序列，因为它将被用于下一个令牌预测任务。因此，在像GPT这样的下一个令牌预测任务中...

我们使用自注意力机制来观察一个标记（token）及其与周围或邻近标记的关联方式。这里，被观察的标记称为查询（query），其他标记称为键（keys）。我们的目标是计算查询向量与键之间的注意力分数。我们认识到，注意力机制的核心目的就是获取这些注意力分数，并将其转化为上下文向量。上下文向量是对输入嵌入向量的一种更丰富、更具信息量的表示。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但第一次没抓住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵，因此需要增加参数数量。

这样我们就有不同的参数可以调整，这些可训练的矩阵被称为查询权重矩阵和键权重矩阵。还有一个值权重矩阵，我们将在下一节课中看到。所有输入嵌入的输入嵌入都与查询权重矩阵相乘，得到查询矩阵，同样我们也有键矩阵。因此，注意力分数不是在向量的输入嵌入之间找到的，而是在查询和键之间找到的。

由于我们拥有众多可灵活调整的参数，我们希望在进行参数训练时，模型能够学会让第二个"it"与"ball"之间的注意力分数高于第二个"it"与"dog"之间的注意力分数。这样一来，模型就能捕捉到更多上下文复杂性。通过添加这些可训练的权重矩阵，模型能够更好地把握上下文中的复杂关系。

这就是为什么我们人类添加了这些权重矩阵，然后称它们为查询、键和值，因为这听起来很酷，而且与信息领域相关。但如果你深入观察，我们自己无法找出这种注意力机制的规则，点积方法失效了。所以我们无法自己弄清楚如何得到这些注意力分数，如何计算它们。于是我们转向神经网络来替我们完成这项工作。

好的。非常感谢大家。在下一讲中，我们将深入探讨自注意力机制背后的数学原理。在接下来的讲座中，我们将探讨多头注意力机制，只有到那时——也就是介绍完多头注意力节点之后——我们才能真正理解键值缓存的概念。让我看看这部分内容在哪里...对，只有到那时我们才能真正准备好理解键值缓存，这个概念正好引出多头潜在注意力机制（MLA）的讲解。

这个系列会有点深度，但我尽量把课程讲得详细些，以免遗漏任何内容。这是为认真的学习者准备的。所以观看这个系列时请做好笔记，这对你会非常有帮助。非常感谢大家。我期待在下一堂课上见到你们。大家好。

我叫Raj Dhandekar博士。2022年获得麻省理工学院机器学习博士学位，是《从零构建深度探索》系列的创作者。在开始之前，我想向大家介绍本系列的赞助商兼合作伙伴——NVIDIA人工智能。众所周知，我们非常重视基础内容，从最基本的构件开始构建人工智能模型。NVIDIA AI 的理念和原则与我们非常相似。让我来展示一下。

这就是NVIDIA AI的官方网站。仅凭一支小型工程师团队，他们就打造出了这款惊艳的产品——只需输入文字指令，就能生成高质量的AI视频。正如各位所见，我刚才输入了一段文字指令："制作一段超写实的高端奢华腕表广告视频，并赋予其电影质感"。

就这样，我点击了生成视频。没过多久，眼前就呈现出了这段令人惊叹的视频，画面极其逼真。最让我着迷的是它对细节的极致把控。看看这个，质量和质感简直令人难以置信。而这一切都仅仅通过一个文本提示就创造出来了。这就是NVIDIA产品的强大之处。

你刚刚看到的精彩视频背后，是NVIDIA AI的视频创作流程在支撑。他们正从基本原理出发，重新思考视频生成与剪辑的方式。为了对基础模型进行实验和优化，他们在印度拥有规模最大的H100和H200计算集群之一，同时也在测试B200芯片。NVIDIA AI是印度发展最快的人工智能初创公司，致力于为全球打造创新产品——这正是我如此认同他们的原因。

好消息是他们目前有多个职位空缺。你可以加入他们出色的团队。我会在下面的描述中发布更多详细信息。大家好，欢迎来到"从零构建Deep Seek"系列讲座。今天我们要探讨一个非常重要的主题——理解带可训练权重的自注意力机制。首先，让我们快速回顾一下课程规划和我们目前的学习进度。

因此，本系列讲座的主要目的是解释Deep Seek是如何构建的，以及推动Deep Seek的创新之处。我们将这些创新分为四个阶段：第一阶段涉及架构设计，第二阶段涉及训练方法，第三阶段涉及Deep Seek所实施的GPU优化技巧，第四阶段则涉及其模型生态系统本身。

在过去的两次讲座中，我们一直在研究第一阶段，并逐步构建对多头潜在注意力机制的理解。因此，我们理解多头潜在注意力的计划是按顺序进行的。我们研究了大型语言模型（LLMs），在上一次讲座中，我们探讨了为什么需要引入注意力机制，而在今天的讲座中，我们的主要目标是了解注意力权重和注意力分数是如何计算的。

那么，让我快速带大家回顾一下上一讲的内容。在上一讲中，我们了解到自注意力机制的主要目的是接收输入嵌入向量，并将其转换为所谓的上下文向量。请记住，上下文向量比输入嵌入向量包含的信息要丰富得多。

输入嵌入向量编码了单词的含义及其在序列中的位置信息，但不包含该单词与序列中其他单词的关系信息。而上下文向量则包含了这些关系信息。上下文向量不仅包含单词的含义和位置信息，还包含了它与序列中其他标记之间的关系信息。

因此，要从输入嵌入向量中获取上下文向量，我们直观想到的第一种方法是计算点积。但这种方法效果并不理想，因为点积本身存在局限性。例如，在"狗追逐球但没接住它"这个句子中，如果"它"是我的查询对象，我需要注意力机制能够区分出"它"实际指向的是"球"而非"狗"。若仅使用简单的点积运算，就无法捕捉这种指代关系。那么我们该怎么办？答案是引入可训练的权重矩阵。

这些可训练的权重矩阵的作用在于：既然我们无法在注意力机制中直接编码复杂的关联性，何不将其交给可以训练的权重矩阵来处理？这正是我们所采取的做法——并非直接使用输入嵌入向量，而是先将输入嵌入向量投影到不同的空间、不同的向量空间中去。例如，当处理某个查询时，我们会将其与查询权重矩阵相乘，从而得到查询向量。

例如，在我们刚刚看到的这个例子中，狗追着球跑，但没能抓住它。如果第二个“it”是一个查询，我们不会直接使用它的输入嵌入向量，而是将其与可训练的查询权重矩阵相乘，使其成为一个查询向量。而我们想看看这个“it”与其他词之间的关系，或者我们需要对其他标记给予多少关注的其他词，它们被称为键。同样，我们也不会直接将这些向量作为输入嵌入向量使用。

我们将它们与所谓的键权重矩阵相乘，然后得到“狗”和“球”的键向量。接下来，我们不再像之前那样使用输入嵌入（即“狗”的输入嵌入和“球”的输入嵌入），而是将其投影为查询向量。因此，现在我们有一个查询向量（看起来像这样），一个“球”的键向量和一个“狗”的键向量。然后，我们计算查询向量和键向量之间的点积，从而确定对于特定查询需要给予每个键多少注意力。

因此，对于这个查询“it”，我们应该对“ball”给予多少关注，你只需要计算查询向量和“ball”的关键向量之间的点积。如果查询是“it”，我们需要对“dog”这个词给予多少关注，我们只需要计算“it”的查询向量和“dog”的关键向量之间的点积，对吧。所以在上节课中，我只是想向大家介绍，我们不仅要进行简单的点积运算，还需要使用可训练的权重矩阵。今天，我们将进一步学习这三个可训练的权重矩阵，即查询、键和值。

如果我直接向你介绍这些查询、键和值，你可能会对我们为什么一开始就需要这些可训练的权重矩阵感到困惑，这看起来有点奇怪。但请记住，作为人类，我们有一个局限，那就是我们无法凭空想出注意力机制的公式，这不像物理学那样有现成的公式。因此，我们将这个难题交给了神经网络或可训练的权重矩阵。我们发现，当你通过权重矩阵将输入嵌入投影到更高或不同的维度时，我们能够得到不错的答案。所以，我们只是把这些矩阵称为查询、键和值，因为在文献中这样称呼更常见，但实际上我们在这里耍了个“偷懒”的小把戏。

如果我直接向你介绍这些查询（query）、键（key）和值（value），你可能会困惑为什么我们一开始就需要这些可训练的权重矩阵。这看起来有点奇怪，但请记住，作为人类，我们有局限性，无法凭空想出注意力机制的公式——它不像物理学那样存在现成的公式。因此，我们将这个难题交给神经网络或可训练的权重矩阵来处理。我们发现，当通过权重矩阵将输入嵌入投影到更高或不同的维度时，就能得到不错的答案。所以我们把这些矩阵称为查询、键和值，只是因为这在文献中更常见且便于理解，但实际上我们在这里耍了个"偷懒"的小花招。

既然我们自己无法理解其中的物理原理，我们就想，让我用一些权重矩阵吧，一开始我会随机初始化它们，然后希望训练完成后，我能学到一些关于不同标记需要注意的地方，而这正是实际发生的情况。所以今天，我们实际上将一步步地了解如何从输入嵌入向量到上下文向量的过程。也就是说，我们将一步步地了解，如果你有一些输入嵌入向量，我们究竟如何获取这些输入嵌入向量，以及如何将这些输入嵌入向量转换为上下文向量。

那么让我们现在就开始我们的旅程吧。今天我的目的是向大家展示一些可视化内容，并穿插一些代码片段，这样你们就能完全自己想象出整个计算过程了，对吧？那么我们就开始吧。我要看看这个序列，明天是光明的，好的。记住，在进入Transformer架构或注意力机制之前，这些都只是输入嵌入，也就是词元嵌入加上位置嵌入。还记得我们之前的讲座吗？我们看到，在进入Transformer架构之前，每个词元都会得到一个统一的表示，这个统一表示就是词元嵌入加上位置嵌入的总和，也就是所谓的输入嵌入。

所以这个输入嵌入实际上就是我在这里展示的内容，对吧？对于"next"，输入嵌入是一个八维向量；对于"day"，输入嵌入是一个八维向量；对于"is"，输入嵌入是一个八维向量；对于"bright"，输入嵌入也是一个八维向量。现在我要向你们展示如何将这些输入嵌入向量转换为上下文向量。但我希望你们能理解为什么我们需要将它们转换为上下文向量，因为就目前而言，如果我查看这些嵌入向量中的任何一个，比如"day"，它并不包含关于需要给予"next"、"the"、"is"或"bright"多少重要性的信息，这些信息完全丢失了。我想要整合这些信息，这就是为什么我要将输入嵌入向量转换为上下文向量。

因此，我们将介绍三个可训练的权重矩阵。第一个可训练的权重矩阵称为查询权重矩阵，用WQ表示，这就是查询权重矩阵。第二个可训练的权重矩阵是键权重矩阵，用WK表示。第三个可训练的权重矩阵是值权重矩阵，用WV表示。现在，你应该不会对这些矩阵的来源感到惊讶了。记住，这些矩阵的出现是因为我们希望将输入嵌入转换到不同的空间，从而增加表达能力，捕捉那些无法通过简单点积实现的潜在复杂性。

记住所有这些可训练的权重矩阵，我刚才提到的这些值在开始时是未知的，它们是随机初始化的。我们的期望是，当我们通过整个LLM架构进行反向传播时，这些值会自行更新。因此，每当我展示这些矩阵时，我并不会在一开始就固定它们，而是随机初始化它们。现在让我们仔细关注一下维度。输入部分，这个矩阵被称为输入嵌入矩阵，这就是所谓的输入嵌入矩阵。看看这个矩阵的维度，它实际上有五行。为什么是五行呢？因为我有五个标记（tokens）：“the next day is bright”，所以它有五行。它有八列，为什么是八列呢？因为我在这里选择每个词的输入嵌入维度为八。你可以选择任何维度。

但为了简化起见，我暂且将其设定为8。这就是输入嵌入矩阵的维度。现在让我们看看查询、键和值权重矩阵的维度。仔细观察你会发现，这些矩阵的行数必须与输入嵌入矩阵的列数相同。因此，WQ、WK和WV的行数——如果你把它们看作行和列的话——其维度中的行数必须等于输入嵌入维度，这是固定的，因为我们要进行矩阵乘法运算。在本例中这个值就是8，但它们的列数理论上可以是任意值。

好的，现在当我们谈到GPT-2、GPT-3等模型时，它们的架构中这个列数（这里也称为输出维度）——我们称之为输出维度，而在这里我们也称其为输入维度或D_in。输出维度可以与输入维度不同，也可以相同。对于GPT-2、GPT-3等模型来说，它们通常是相同的。因此，如果相同的话，D_out也会等于8。但为了简单起见，我这里选择了D_out等于4。不过要记住，这个值通常与输入维度D_in相似。但在这里，我想展示的是，它实际上可以是任意值，因为乘积运算仍然可以进行。

即使列值不同，我的查询权重矩阵、键权重矩阵和值权重矩阵都是8×4维的矩阵。第一步，我会将输入嵌入矩阵与查询矩阵相乘，从而得到查询向量。来看一下维度：5×8的矩阵乘以8×4的矩阵，结果是5×4的矩阵。这就是我的查询向量，一个5×4的矩阵。同样，键向量也是一个5×4的矩阵，值向量同样如此。可以这样理解：这里的每一行都对应一个标记。

所以第一行是第一个标记，第二行是第二个标记，接下来第三行是“day”，第四行是“is”，第五行是“bright”。但现在要记住，我们从8维降到了4维的输出空间，因此我们操作的空间现在已经不同了。从这一点开始，我们不再关注输入的嵌入向量，我们只关注查询向量、键向量和值向量。这就是我在讲座一开始试图启发你们的——如果我们直接处理输入嵌入向量并取点积，它的局限性很大，效果不佳。这就是为什么我们要将它们投影到不同的空间中。这一技巧在深度学习的许多不同领域都有应用。

如果线性分类器在数据上不起作用，你可以通过增加特征来提升它，将其投影到更高维的空间。如果手工设计的计算机视觉特征不起作用，你可以使用卷积神经网络，它能在更高维的空间中自行发现特征。因此，将输入维度的内容转换到另一个维度，是我们长期以来在深度学习中一直在做的事情，而这正是我们在这里所做的。只是这里的名称稍微花哨一些——查询、键和值。在本讲座的最后，我会告诉你为什么会出现这些名称，但现在你只需要记住查询向量是一个5×4的矩阵。

键矩阵的维度是5×4，值矩阵的维度也是5×4。其解读方式是每一行对应一个词元。查询键的未来是光明的，值矩阵也是如此，而列数等于输出维度（在本例中为4）。希望大家能跟上我的思路，到现在为止我们已经有了查询向量、键向量和值向量。这里我采用了一些具体数值：输入是5×8的词嵌入矩阵，我将其与8×4的查询矩阵、键矩阵和值矩阵相乘，最终得到5×4的查询矩阵、5×4的键向量矩阵以及5×4的值向量矩阵。

左边是没有任何数值的可视化展示，右边则是已经填入具体数值的版本，这样你们就能同时看到直观的图形表达和数学表达并列呈现。刚才第一步我们得到了查询向量(Q)、键向量(K)和值向量(V)，现在我们要在QKV空间进行操作，而不是在输入嵌入空间（注：QKV即查询Query/键Key/值Value的缩写）。接下来进入第二步——我们需要计算注意力分数，这本质上就是进行点积运算。

好的，现在我有我的查询向量，它是一个5×4的矩阵，还有键矩阵，也是5×4的。如果要计算查询向量和键向量的点积，我不能直接相乘这两个矩阵，因为第一个矩阵的列数和第二个矩阵的行数不匹配。所以我需要对键矩阵进行转置。这样查询向量保持5×4不变，而键矩阵转置后变成4×5。这样操作后，用一个5×4的矩阵乘以一个4×5的矩阵，就能得到一个5×5的注意力分数矩阵。

这是一个非常重要的步骤，因为在这里我们实际上可以找出一个查询与其他标记之间的关联程度。现在，假设我说“明天很晴朗”，然后让我聚焦在第二行的“明天”这个词上。假设我想找出“明天”与其他标记之间的注意力分数。也就是说，我想知道“明天”与“明天很晴朗”之间的关联程度。具体来说，我想找出“明天”与“明天”之间的注意力分数是多少，“明天”与“很”之间的注意力分数是多少，“明天”与“晴朗”之间的注意力分数是多少。

因此，为了计算“next”和“the”之间的注意力分数，我们会取“next”对应的查询向量（即这一行），并将其与“the”的键转置（即这里的第1列）相乘。这里的第1列对应“the”，所以当我们将“next”的行与“the”的列相乘时，就得到了“next”和“the”之间的注意力分数。我将其称为α2,1，也就是这里的这个值。现在来看这里的第二行，它对应“next”的注意力分数，这里的第一个值就是α2,1。

现在，这是下一个行向量与当前列向量的点积。如果我想找到“next”与“next”之间的注意力分数，我会再次取这个行向量与第二列（即“next”）的点积，这个点积将给出alpha 2 2，也就是这里的第二个值。所以这就是alpha 2 2。如果我想找到“next”与“day”之间的注意力分数，我会再次取这个行向量（保持不变）与这里的第三列（对应“day”）的点积，这将给出这里的第三个值，即alpha 2 3。如果我想找到“next”与“is”之间的注意力分数，那么这里就是alpha 2 3，“next”与“is”将是alpha 2 4。所以这就是“next”行向量与这里第四列的点积，这将给出alpha 2 4，也就是这里的值。

那么，如果我想计算"next"和"bright"之间的注意力分数，本质上就是α₂₅，也就是第二行"next"与最后一列"bright"的点积。这个点积运算最终会给出α₂₅的值。现在，这整个第二行就代表了"next"与所有其他键（即"the"、"next"、"day"、"is"、"bright"）之间的注意力分数。同理，第五行表示的是"bright"与所有其他标记之间的注意力关系。通过这种方式，我们可以解读这个注意力分数矩阵——矩阵的每一行都代表该行对应的查询词（即该行的词）与所有其他键之间的注意力关系。

例如，如果你看第四行，它表示第四个查询词“is”与所有其他标记“the next day is bright”之间的注意力分数。要得到这一行，你需要固定查询行，然后将其与键转置矩阵中的所有列相乘，这样就能得到该行中的每一个值。所以，每当你想到注意力分数矩阵时，试着想象如何解释矩阵的每一行，这时你才能真正理解，或者永远不会忘记注意力分数矩阵是如何计算的。如果查询矩阵是5×4，键转置矩阵是4×5的话。

所以这里我再次展示数学计算过程：当你将查询向量与键向量的转置相乘时，会得到注意力分数矩阵，其形式大致如下。现在观察第二行数据——这行本质上表示当"next"作为查询词时：0.1对应"next"与"the"的注意力关联度，1.8是"next"与"next"自身的注意力值，0.6代表"next"与"day"的注意力强度，0.1是"next"与"is"的关联度，而0.1则是"next"与"right"的注意力权重。这就是解读注意力分数矩阵的正确方式。

不过这个注意力分数矩阵存在一个问题：比如当处理"next"这个词时，我们希望能明确表述——当"next"作为查询词时，应当给予"the"50%的注意力权重，给"next"自身20%，给"day"分配10%，给"is"10%，而"bright"则完全不分配注意力权重等等。

所以我想做出可解释的陈述，让我重申一下我的意思。当我看到下一个词时，我想做出这样的陈述：比如给"let's say"分配10%的注意力，给"next"分配20%，给"day"分配20%，给"is"分配30%，给"bright"分配20%。这样当我看到这些数值时，就能立即看出其中"next"和"is"获得了最多的注意力。本质上，我希望所有这些数值加起来等于100的概率，也就是总和为1。这样我就能直接看这个饼图——"the next day is bright"——根据这个饼图，我可以看到每个token需要分配多少注意力。而如果你现在看这些数值...让我们看看...是的，如果你看这些数值...

现在来看第二行，也就是下一个的注意力分数。你会发现这些值加起来并不等于一，这意味着我不能说给第一个词元分配10%的注意力，给第二个词元分配18%的注意力，这样是行不通的。这里的行中的数值加起来不等于一，这就是主要问题所在。因此，下一步，也就是第三步，是确保将注意力分数转换为所谓的注意力权重。为了从注意力分数得到注意力权重，我们将应用softmax操作。softmax本质上意味着，比如说我们看一下这一行，让我以列的形式写下来。

现在，以列格式表示，这些数值将变为0.1、1.8、0.6、0.1和0.1。我想要做的是以某种方式转换所有这些值，使它们介于0到1之间，并且总和为1。这就是softmax操作的作用。具体来说，如果这些值是x1、x2、x3、x4和x5，softmax操作会将x1替换为e的x1次方除以总和，x2替换为e的x2次方除以总和，x3替换为e的x3次方除以总和，x4替换为e的x4次方除以总和，x5替换为e的x5次方除以总和。那么，总和到底是什么呢？总和就是e的x1次方加上e的x2次方加上e的x3次方加上e的x4次方加上e的x5次方。

现在来看看这五个值，你会发现如果将它们相加，分子就是e的x1次方加e的x2次方加e的x3次方加e的x4次方加e的x5次方，这等于分母的总和。因此，所有这些值加起来肯定会等于1，并且它们会介于0到1之间。Softmax还有一个额外的重要特性，就是它对非常大的值赋予很高的权重，而对非常小的值赋予很低的权重。这使得分类变得非常容易。

所以本质上这就是我们要实现的softmax操作。但这里的主要问题在于，正如我刚才所说，softmax对数值较高的值会给予极高的关注度，而对数值较低的值则关注不足，这对我们来说是个大问题。举个例子，如果注意力分数是这样的话。

现在你看到这个值非常高对吧。如果我们直接对它应用softmax，softmax的作用会让这个值变成0.95左右，同时确保其他所有值都非常低。如果这些是我们的注意力权重，那对我们来说就不太理想了，因为这样我们会过度关注某一个键，而完全忽略其他所有键。所以这就是为什么在应用softmax之前需要进行缩放处理。

在我们应用softmax之前，需要确保所有这些值都除以某个数，然后才能应用softmax。现在让我来谈谈这一部分。我们来看这部分，首先我想解释一下softmax的问题。假设这些是我的注意力分数值，然后我对这些值应用softmax函数。可以看到，所有这些值的softmax结果在范围上几乎都差不多，这对我们来说是好的。但现在我想做的是，把这些值都乘以8，然后再应用softmax。如果所有这些值都乘以8，意味着有些值会变得非常大，而有些值则不会那么高。

好的，那么如果我将所有这些值乘以8，然后应用softmax函数，你会发现softmax会非常重视这个0.8的值，而对其他一些值几乎不予理会。这就是在应用softmax之前数值非常大的情况下会发生的事情，正如我在这里提到的，softmax函数对其输入的大小非常敏感。当输入值非常大时，每个输入的指数值之间的差异会变得更加明显，这导致softmax的输出变得"尖峰化"。所谓"尖峰化"输出，意味着它对某些值赋予很高的权重，而对其他值赋予非常低的权重。

因此在注意力机制中，这并不理想，因为如果softmax分布过于尖锐，模型就会对某一个特定的键变得非常自信。现在你可以看到，模型对这个键变得非常自信，而对其他键的置信度会非常低，这会导致训练过程非常不稳定。稍后我们研究transformer架构时会看到这一点，而我们不希望这种情况发生。这就是为什么我们需要进行缩放，这就是为什么在应用softmax之前需要对这个向量进行缩放。实际用来缩放这个向量的值是键的维度的平方根。

好的，这里的键维度在这个例子中是5×4对吧？所以输出维度等于5时，缩放因子是平方根；当输出维度等于4时，抱歉，缩放因子就变成了4的平方根。这时候你可能会想：为什么非要按键维度的平方根来缩放呢？为什么不直接用键维度本身？或者用维度的平方？为什么偏偏选中平方根？平方根到底有什么特别之处？这背后的核心原理其实与方差的概念有关。

假设这是我的查询向量，一个六维向量，而这是我的键转置向量，也是一个六维向量。如果我相乘这两个向量，会得到一些值，对吧？但记住，在相乘的过程中，每个值都会先相乘再求和。通常的情况是，如果有两个随机向量，维度都是6，如果我采样一百个这样的随机向量——也就是说，我取一百个查询向量和一百个键转置向量，然后计算所有这些查询向量与键转置向量相乘的结果——那么这些乘积的方差会……

所以现在我可以取这100个值并计算它们的方差，对吧？这个分布的方差实际上会随着键的维度而变化。正如这里提到的，这个乘积的方差会随着键维度的平方根变化，实际上是与键维度的平方根成比例。这意味着，随着键维度的不断增加，或者说随着键转置的维度增加，这个乘积的方差会大幅增加。也就是说，由于查询和键转置是随机初始化的，这些都是随机向量，它们的乘积通常会从一些非常高的值变化到一些非常低的值。

我们想尽可能避免这种情况，我们要确保这个产品方差等于一，这样产品就不会剧烈波动。查询和键在开始时是随机定义的，维度可能非常高。所以如果维度非常高，而方差实际上与这个维度的平方根成比例，那对我们来说就不太好，因为这也会使学习不稳定。为了进一步说明这个概念，我想用骰子的例子来解释。

那么，如果你掷一个骰子，假设它只有1到6的数字，平均值大约是3.5，方差相对较小，比如2.9，结果是可预测的。但现在我掷骰子，然后对掷100次的结果求和。也就是说，如果你掷骰子100次并求和，这里的均值会在350左右，但方差会显著增加到约290，结果就变得不可预测了。而没有归一化的点积也是如此。

因此，如果我们仅将查询向量与转置后的键向量相乘，且不除以其维度大小的平方根，我们会发现：增加维度数量就像掷更多骰子并累加结果。原因在于点积运算本质上是对乘积项的求和——当维度为100时，我们就要累加查询向量与转置键向量对应100个元素的乘积。所以维度增加就如同增加骰子数量并合计点数，每个维度都会引入方差，随着维度增长，这些方差会不断累积。

所以我在这里想说的是，随着查询和键的维度增加，方差会大幅上升，导致softmax之前的点积结果要么变得非常大，要么变得非常小。由于方差过大，注意力权重会变得不稳定，进而导致训练过程也不稳定。当我们除以D的平方根时，这个操作会降低点积的方差，使其等于1，从而稳定了预期结果。这样，注意力权重变得更加稳定和可预测。实际上，我已经用下面的代码解释了这一点。接下来我们要做的是进行一千次试验，在每次试验中我们会生成...

首先，我们将进行一个五维的查询和关键向量操作，然后进行一个百维的查询和关键向量操作。假设我提到的维度等于5，我们将生成维度为5的查询和关键向量，并将这个过程重复一千次。对于每一次试验，我们将计算查询和关键向量之间的点积。在一种情况下，我们会将点积结果除以维度的平方根进行缩放，而在另一种情况下，我们则直接取点积结果而不进行缩放。然后，我们将收集这一千次试验的所有结果，并计算缩放前的方差和缩放后的方差。

所以记住，我们取一千个查询向量和一千个键向量，计算它们的点积。在一种情况下我们进行了缩放，另一种情况下没有，然后我们比较缩放前后的方差。如果你运行这段代码，会发现对于维度为5的情况，缩放前的方差等于5；对于维度为100的情况，缩放前的方差实际上是100。这说明方差实际上随着维度的增加而直接增长——方差与维度成正比。但缩放后的美妙之处在于：当你用维度的平方根进行缩放后，方差几乎会趋近于1，这非常棒。

好的，现在这里已经明确证明：如果你将查询向量与键向量的点积除以键向量维度的平方根，最终得到的结果是——经过缩放后，无论维度是5还是100，点积的方差都会被限制在一定范围内。这意味着查询向量乘以键向量转置后，其值不会暴涨到极高或暴跌到极低，方差将保持为1，从而确保训练过程非常稳定。因此，我们在第三步所做的就是将注意力分数除以键向量维度的平方根进行缩放。也就是说，我们取注意力分数，然后除以键向量维度的平方根。这里的键向量维度本质上是指每个键向量的维度大小，在本例中等于4。

好的，所以我们先除以4的平方根，然后应用softmax函数。在这个除法完成之后——具体来说是在除以键的维度的平方根之后——我们就会应用softmax。本质上，我们应用这个函数后，注意力分数就被转换成了所谓的注意力权重。这就是注意力分数和注意力权重之间的关键区别：注意力分数是没有归一化的，而注意力权重是归一化后的结果。如果你看这里的每一行，你会发现每一行的总和基本上都等于1。现在，我们就可以做出定量或定性的陈述了，就像我刚才提到的——第二行对应的是下一个...

所以现在我可以这样说：我们可以在"next"和"though"之间分配10%的注意力，在"next"和"next"之间分配50%的注意力，在"next"和"day"之间分配20%的注意力，在"next"和"is"之间分配20%的注意力，在"next"和"studies"之间分配10%的注意力，在"next"和"bright"之间分配10%的注意力。这就是我现在能够做出定性陈述的方式，因为每一行的所有元素加起来本质上等于一，这就是注意力权重和注意力分数之间的区别。注意力权重是经过归一化的，它们加起来等于一，而注意力分数则没有经过归一化。

现在，我们将进入第四步，在这一步中，我们实际上是从注意力权重计算上下文向量。让我们看看这是如何完成的。到目前为止，我们已经有了注意力权重，这又是一个5x5的矩阵。当你再次看到这个矩阵时，试着想象每一行代表什么。例如，这里的第二行表示，如果“next”是查询词，我应该给予“the next day is bright”多少注意力。所有这些值现在加起来等于1，因为它们在最后一步进行了归一化处理。我们接下来要做的是，记住到目前为止我们还没有使用值向量。在最后一步中，这些值向量开始发挥作用，我们所做的就是将5x5的注意力权重矩阵简单地与值向量相乘。

因此，值向量是5行4列的，对吧？这给了我们上下文向量。上下文向量有5行和4列。第一行对应"the"，第二行对应"next"，第三行对应"day"，第四行对应"is"，第五行对应"bright"。现在你可以看到，最初我们是从"the next day is bright"的输入嵌入向量开始的，经过所有这些步骤——第一步、第二步、第三步和第四步——我们现在得到了每个标记的上下文向量。请记住，这些上下文向量比输入嵌入向量丰富得多，主要是因为它们的计算涉及注意力权重。

因此，我刚才通过数学计算展示了注意力权重——它是一个5×5的矩阵（如这里所示），而数值矩阵是5×4的，因此上下文向量矩阵也是5×4的。例如，第二行对应着"next"的上下文向量，而这里的最后一行则对应着"bright"的上下文向量。每个上下文向量都是一个四维向量。在此，我想用一小节来阐释上下文向量计算背后的直观逻辑——其实际计算方式如下：

假设你想为“next”这个词找到上下文向量，我们已经计算出了“next”与所有其他词之间的注意力权重。我们发现，当处理“next”这个词时，我们应该给予“we”10%的注意力，给予“next”50%的注意力，给予“be”20%的注意力，给予“is”10%的注意力，给予“bright”10%的注意力。我们是通过这些注意力权重值（抱歉，是这些注意力权重）来理解这一点的。此外，我们还有值向量，你可以将值向量本质上视为输入嵌入向量本身。

但它被转换到了一个更高维度或不同维度的空间。因此，现在如果我们想找到下一个词的上下文向量，我们本质上做的是：我们知道“next”对“the”的关注度是10%（即0.1），这是“the”对应的向量值。我们会将这个向量按10%缩放，或者说乘以0.1。然后我们知道“next”对“next”本身的关注度是50%（即0.5）——我已在此标注了注意力权重：0.1（the）、0.5（next）、0.2（day）、0.1（...）、0.1（...）。具体来说，我们给第一行（the）赋予0.1的重要性权重，给第二行（next）赋予0.5的重要性权重，给第三行（day）赋予20%（0.2）的重要性权重。

所以我们用0.2乘以第三行，用0.1乘以第四行，用0.1乘以第五行，然后将所有这些乘积相加。接着我们用0.1缩放第一个向量，用0.5缩放第二个向量，用0.2缩放第三个向量，用0.1缩放第四个向量，用0.1缩放第五个向量，最后将所有缩放后的向量相加。这就是我们最终得到上下文向量的方式——本质上经过所有这些加法运算后，我们就得到了下一个词的上下文向量。若用视觉化表示，它看起来是这样的：如果你看到蓝色向量，它们就是输入嵌入向量，用于生成上下文向量。"明天会更好"

我们根据每个向量需要被关注的程度对其进行缩放。例如，"right"的重要性是10%，所以我们用0.1对其进行缩放；接下来是"50%重要性的"，我们用0.5进行缩放；"day"的重要性是20%，我们用0.2进行缩放；"is"的重要性是10%，我们用0.1进行缩放；"bright"的重要性也是10%，我们用0.1进行缩放。所有这些缩放后的向量都用绿色表示，然后将所有这些绿色向量相加，最终得到下一个的上下文向量。

因此，在这张图中，你可以看到上下文向量与输入嵌入向量的区别。输入嵌入向量仅包含该标记的含义信息，而上下文向量则包含了所有这些注意力权重信息，这些加权求和最终形成了上下文向量。这就是注意力分数如何相加得到上下文向量本身，以及上下文向量与输入嵌入向量的不同之处。

现在，通过同样的可视化方式，你也可以尝试理解如何获取其他所有标记的上下文向量。比如，要获取"bright"的上下文向量，你只需查看这些注意力分数：将第一行乘以0.1，第二行乘以0.05，第三行乘以0.1，第四行乘以0.25，第五行乘以0.5，然后将所有这些结果相加，这样就能得到"bright"的上下文向量——本质上就是这里的最后一行。这就是你如何从注意力权重矩阵中获取上下文向量的方法。

以及价值矩阵，要理解上下文向量计算背后的可视化过程，请看这张图。在这张图中，你可以看到注意力权重是如何被用作缩放因子的，然后通过加权求和，基本上将我们从输入嵌入空间转换到上下文向量空间。这就是第四步，本质上是获取上下文向量矩阵。然后在第五步中，我将简单地总结一下：我们有输入嵌入矩阵，我们有输入嵌入矩阵，然后我们有自注意力层。

所以当你现在看到这个自注意力层时，它意味着所有这些步骤已经在这里实现：第一步是与WQ、WK和WV相乘，得到查询向量、键向量和值向量；第二步是将查询与键的转置相乘，得到注意力分数；第三步是用键维度的平方根缩放注意力分数，并应用softmax得到注意力权重；第四步是将注意力权重与值矩阵相乘，得到上下文向量矩阵。就是这样，所有这四个步骤本质上都包含在自注意力模块中，最终将输入嵌入矩阵转换为上下文向量矩阵。

就是这样，这就是注意力机制或自注意力机制中发生的整个过程。这一过程为Transformer模块提供了动力，而Transformer模块正是语言模型表现如此出色的核心原因。现在，如果你回顾一下我们的课程，我们曾看过Transformer模块的不同组成部分，这个多头注意力机制就是所有魔法发生的地方。我们将输入嵌入向量作为输入，上下文向量作为输出，现在你知道了上下文向量是如何从输入嵌入向量计算出来的。为什么这被称为多头注意力？我们将在下一讲中看到。

但目前我希望你已经理解了自注意力机制背后的原理，我刚刚在这里写了一个小代码来向你演示这一点。我们称之为自注意力类。首先，我们初始化查询权重矩阵、键权重矩阵和值权重矩阵，最初是随机初始化的，并且我们将偏置设为false，因为我们只需要将输入嵌入矩阵与这些矩阵相乘即可。在前向传播过程中，我们通过将输入嵌入矩阵与可训练的键矩阵、可训练的查询矩阵和可训练的值矩阵相乘，得到键向量、查询向量和值向量。这基本上是我们看的第一步。然后我们进入第二步，通过将查询与键的转置相乘得到注意力分数。接着我们进入第三步，在第三步中，我们首先除以键维度的平方根。

然后我们应用softmax函数，最终来到第四步，即将注意力权重与值矩阵相乘，得到上下文向量矩阵。就是这样。其实我本可以直接向你解释这段代码，因为它只有10到11行代码，但为了理解这些乘法运算，我们有必要在白板上写下来，以便可视化维度。这就是上下文向量权重矩阵的计算方法。你可以运行这段代码，我也会分享这段代码，然后你可以输入任何内容。这里我输入的是一句话“你的旅程始于一步”，输入维度D_in在这里等于3，即输入嵌入维度。

我假设输出嵌入维度等于2，现在让我运行一下。可以看到，当我们通过这个自注意力类传递输入嵌入矩阵时，它最终被转换为上下文向量矩阵。就是这么简单。为了保持当前代码中的维度，我们实际上可以确保输入是8维的。让我们调整代码使其与之前保持一致。接下来的一天是光明的，对吧？假设这就是"接下来的一天是光明的"，现在这部分不需要了。在代码或白板中，我们看到每个向量都是8维的，让我直接复制粘贴调整为8维。

然后在这里添加一些随机值，为了简单起见，让我在这里也添加同样的内容。现在我把所有东西都设为8维，这样就能和我们白板上的内容匹配了。好的，现在D_in等于8，但我在这里取的D_out是4，所以我就让D_out等于4。然后让我运行这个代码块，你看它几乎立刻就运行完了。现在我有了上下文向量矩阵，让我们检查一下维度。这个上下文向量矩阵的维度是5行4列，这正是我们之前在这里看到的。上下文向量或者说上下文权重矩阵是5行4列，这正是我们在代码中实现的。

接下来在下一节课中，我们将要学习的是实现一种称为因果注意力的机制。在因果注意力中，我们会屏蔽掉那些不需要或当前标记无法获取的未来注意力权重，这样在预测下一个标记时，我们只会关注过去的标记。这就是所谓的因果注意力，我们将在下一讲中详细探讨。之后，我们会转向多头注意力。我知道这些课程有点长，而且我在重复一些关于注意力或Transformer模块、因果注意力、上下文向量等内容。

但我认为这对我们理解这一点至关重要。如果我们不理解这一点，多头潜在注意力的学习就不会很扎实。我希望在我们开始多头潜在注意力部分时，大家都能达成共识。因此，你们真的需要理解自注意力和多头注意力的基本原理，这也是为什么我要如此详细地讲解这些内容。我会与你们分享代码文件，并希望你们在学习这些课程时也做笔记。把这些内容写在纸上，确保你们熟悉这些计算。之后我们会讲到多头注意力，再往后我们会讲到多头潜在注意力。

请记住，这是一个需要分步进行的旅程，并不容易。这不是一个30分钟的速成课程，而是包含35到40个视频的系列课程，内容会非常非常详细。我打算把它做成类似大学讲座的形式。其实我本可以直接从潜在注意力机制开始讲起，但那样理解起来会非常困难。我希望这些课程对首次接触本系列的观众也同样实用。希望大家喜欢这个系列，期待在下节课中与你们相见。谢谢！

大家好，我是Raj Dhandekar博士，2022年获得麻省理工学院机器学习博士学位，也是"从零构建Deepseek"系列课程的创建者。

在我们开始之前，我想向大家介绍一下我们本系列的赞助商兼合作伙伴——InVideo AI。大家都知道我们多么重视基础内容建设，从零开始构建AI模型。InVideo AI遵循与我们非常相似的原则和理念，让我来展示一下。这是InVideo AI的网站，凭借一个小型工程团队，他们打造了一款令人惊叹的产品，只需文本提示就能创建高质量的AI视频。正如你们在这里看到的，我输入了一个文本提示："制作一个超现实的高端豪华手表视频广告，并使其具有电影感"。点击生成视频后，很快我就得到了这个高度逼真的精彩视频。这个视频最让我着迷的是...

它最令人惊叹之处在于对细节的极致把控——看看这质感与纹理，简直不可思议！而这一切仅通过一条文本指令生成，这正是InVideo产品的魔力所在。您刚才看到的震撼视频，其核心引擎正是InVideo的视频创作流水线。他们从底层原理重构视频生成与剪辑技术，通过海量实验不断优化基础模型。这家公司拥有印度规模最大的H100和H200计算集群，并已开始测试B200芯片。作为印度增长最快、面向全球市场的AI初创企业，InVideo的理念与我深度共鸣。好消息是：他们目前正在广纳贤才，您也有机会加入这支非凡团队！

我正在下方描述中发布更多详细信息。大家好，欢迎来到“从零构建DeepSeek”系列讲座的这节课。今天，我们将继续深入理解注意力机制的旅程。简单来说，我们最终的目标是理解多头潜在注意力机制，这是DeepSeek架构的关键组成部分之一。但要做到这一点，我们需要循序渐进地进行。最初，我们从理解自注意力机制开始，并在上一节课中完成了这部分内容。如果你还记得的话，在上一节课中...

我们详细了解了自注意力机制逐步运作的过程：首先从可训练的查询、键和值权重矩阵开始，将输入嵌入矩阵与这些权重相乘，得到查询向量、键向量和值向量；接着将查询矩阵与转置后的键矩阵相乘，获得注意力分数；然后对这些分数进行维度平方根的缩放处理，应用softmax函数后，最终得到注意力权重。

最后，我们将注意力权重与值矩阵相乘，得到了上下文向量矩阵。理解上下文向量矩阵的方式是：本质上，上下文向量矩阵的每一行都对应着特定标记的上下文向量。例如，在输入嵌入矩阵中，每一行对应一个输入嵌入向量（以本例中的"the next day is bright"为例）。同样地，在上下文向量矩阵中，第一行对应"the"的上下文向量，第二行对应"next"的上下文向量，以此类推。因此，自注意力机制的整个目标就是将输入嵌入向量转化为上下文向量。

为什么会这样呢？因为上下文向量比输入嵌入向量丰富得多。输入嵌入向量仅包含单词的语义信息，但不包含该单词与周围其他单词之间关系的任何信息。而上下文向量不仅包含语义信息，还包含需要关注周围其他单词或标记的重要程度信息。正是这一点使得上下文向量在表征上比输入嵌入向量丰富得多。

那么今天我们要做的就是研究一种叫做因果注意力的机制。我们已经学习过自注意力机制，而在我们最终理解多头潜在注意力的探索之旅中，下一步就是研究因果注意力。顾名思义，"因果"意味着一个事物导致另一个事物的发生。因此我们只需要关注那些对生成下一个标记起决定性作用的因素——稍后我会详细解释这个概念。因果注意力是理解多头注意力机制的核心基础模块，进而帮助我们理解键值缓存机制，最终掌握多头潜在注意力的原理。现在，让我们开始探索因果注意力的奥秘吧。

在我们开始讲座之前，我想先快速解释一下下一个词预测的具体实现方式，这样我能更好地为因果注意力机制铺垫背景。假设我们手头有《哈利·波特》系列的第一本书，它正是我们用于预训练大语言模型的数据集组成部分。为了方便演示，我直接截取了这本书第一页的截图，现在把它展示在我的电子白板上。大家可以看到，我已经把首页截图放在这里了，接下来我将演示如何构建输入与目标配对数据，这些数据最终会被输入到Transformer架构中进行训练。

具体操作步骤如下：首先我们需要确定一个称为上下文长度的参数。例如当上下文长度设为4时，就会生成一个包含4个标记的序列。这是我的第一个输入，然后我还确定了一个称为步长的参数，意思是在创建下一个输入之前需要跳过多少。这是我的第一个输入，这是第二个，这是第三个。类似地，我会将整个数据集分成每块4个单词或4个标记，然后根据我的批次大小，会生成一个矩阵。让我们看看第一个输入批次。如果批次大小是6或8，那么第一批次的第一行将包含对应于Dudley先生和夫人的标记ID。

假设这是1、15、18、22。第二行将是与这4个标记对应的标记ID。那么它会是类似3、4、5、6这样的数字。同样地，我将有8行这样的数据，因为批量大小等于8，对吧？为什么我有4列呢？原因在于上下文大小等于4。这就是输入批量数据的创建方式。

这是我的第一批输入数据。生成输出的方式是，每个输入数据都会向右移动一位。因此，这是我的第一个输出，这是我的第二个输出，这是我的第三个输出，以此类推。一旦你获得了输入批次，生成输出批次就非常简单，只需将输入数据向右移动一位即可。结果会类似于15、18、22、3等等。


So that's my output batch 1. Now if you look at the first, if you look at the first row here and the first row here that's my first input output prediction pair. So for example let's see. So the input will be Mr. and Mrs. Dursley that's the input.

So this corresponds to 1, 15, 18 and 22 and then the output is the input shifted to the right by 1. So then this will be and Mrs. Dursley off that's the output. Now if you see within this one pair of inputs and outputs there are actually four prediction tasks. The first prediction task is when Mr. is the input and is the output.

The second prediction task is when Mr. and and is the input, Mrs. is the output. The third prediction task is when Mr. and Mrs. is the input then Dursley is the output and the fourth prediction task is when Mr. and Mrs. Dursley is the input then off is the output. So if you see every row here although it looks like we have four tokens right so there might be just one prediction task which is predicting the next token but that's not the case if the input is Mr. and Mrs. Dursley and the output is and Mrs. Dursley off if the context size is 4 there are four input and target prediction tasks or input output prediction tasks which I have written over here right now.

The thing which I want you to focus is that whenever you are predicting certain output so for example when Mrs. is the output which is to be predicted the input is Mr. and and when Dursley is the output which is to be predicted the input is Mr. and Mrs. So let's take a look at the data set now and let me clean this further to explain this point to you. So what I am trying to say is that when Dursley is the output let's say Dursley is the output the input is always the tokens which only come before Dursley. Similarly you take any output you take Mrs. as the output if Mrs. is the output if Mrs. is the output the input is Mr. and and if you see the input is Mr. and and so for every output so wherever you go in this so let's say if you go in this you let's say if this is the input and then the output is shifted to the right by one now within this there are four input output prediction tasks right if you see was was the output you will see that Mrs. and Dursley was the input if thin is the output then was Dursley Mrs. is the input etc.

So based on the context size wherever whenever you have an output the first conclusion from today's lecture is that the input is only the tokens which come before the output right so essentially what I mean to say is that if you have if you have the input as oops if you have the input as let's say Mr. and if you have the input as Mr. and and to predict the next token which is Mrs. you do not have access to all the future tokens which come after Mrs. and this seems like a trivial point but that is the whole premise of causal attention the whole premise of causal attention is that to predict the output we only have access to the tokens before the output so we cannot cheat and we cannot look into the future so the simplest way to explain causal attention is that you cannot cheat we cannot cheat and we cannot we cannot look into the future the reason I explained the beginning of this lecture with this Harry Potter example is so that you feel interested in this topic and then you don't forget what causal attention is essentially what causal attention is simply is that for every output the input is only the tokens which come before it so you check any input and output pair in the in these four input output pairs which I have the for every output the input on input is only those words or tokens which come before the output okay and now let's start understanding causal attention in a more formal manner right so causal attention is also sometimes known as masked attention and it's a special form of self attention right it restricts the model to only consider previous and the current inputs in a sequence when processing in any given token this is what we saw to predict any output it restricts the model to only consider previous and the current inputs in the sequence this is in contrast to the self attention mechanism which allows access to the entire input sequence at once and let us see this in action actually so if you remember what we did with the queries keys and the value vectors we had an input and the input had tokens such as let's say your journey starts with one step and here I have shown that every token is a three-dimensional vector we will multiply this with the trainable query matrix trainable keys matrix and trainable values matrix and ultimately we will have the queries the keys and the values now let's see how we compute the attention score right the attention score is the multiplication of queries multiplied by the keys transpose so let's look at this scores in more detail so if I have my queries vector if I have my queries vector to be 6 by 2 and if I have my keys vector to be let's say 6 by 2 to get the attention score what what I actually do is I do queries multiplied by keys transpose right so it will be a 6 6 by 2 it will be a 6 by 2 multiplied by it will be a 2 by 6 so this will ultimately result in a 6 by 6 vector or a 6 by 6 matrix and the tokens which I am looking at for now are your journey starts with one step right so let's see your journey starts with one step and then your my keys are your journey starts with one step right and basically the 6 by 6 matrix indicates that I have values for every single entry here so so if I am looking at journey and that's my query I am basically finding the attention scores between journey and all the other words even before and after journey right so this is the attention score between journey and your this is the attention score between journey and journey this is the attention score between journey and starts this is the attention score between journey and with this is the attention score between journey and one and this is the attention score between journey and step now this can be represented in a visual manner like this your journey starts with one step and if journey is my query vector I am essentially finding the attention of journey with all the other tokens no matter whether the token comes before journey or after journey now pay very careful attention here if journey is my query right so if your and journey are essentially my inputs to predict the output journey will not have access to the tokens which come after it at all so when journey is my query vector if we are looking at causal attention ideally we should find the attention scores only between your and journey all the other tokens which come after this point do not matter too much to us at all because they are anyway not going to influence the calculation of the next token let me explain this point once more essentially what we do in self attention mechanism is that we get these attention scores right so every token for every token we find its attention with the tokens which come before it and which come after it but the key point to realize is that for every token we do not have access to the tokens which come after it at any point in the prediction process so take a look at the Harry Potter example right every token for every token we don't have access to the words which come after it we cannot look into the future and since we cannot look into the future there is no use finding the attention scores between a token and the tokens which come after that point so ideally if you have a six by six matrix such as this so let's say this is one two three four five six one two three four five six two three so these are three rows and let me actually copy this and paste it let me just do it again here so these are three rows this is my fourth row over here this is my fifth row over here and this is my sixth row these are the attention scores right this is a six by six so if we cannot look into the future for the first token we only need to find the attention score between the first token and itself so that's this for the second token journey we only need to find the attention score between your and journey for the third token we only need to find the attention score between your journey starts for the fourth token we only need to find the attention scores between your journey starts with for the fifth token we need to find the attention between one and your journey starts with one so all the tokens which come before one and for the sixth we can essentially find all the attention scores between step and all the tokens which come before it so now if you see all these attention scores which are above the diagonal which are marked over here all of those are not needed at all all of these attention scores are not needed because we anyway won't look into the future at any time point remember why did we get the attention scores we got the attention scores between a token and the other token so that that will help us get from the input embedding vector to the context vector but in self attention we kind of looked into the future right because we for one token we even found the attention score between that token and tokens which come after that point and so the value vector which we ultimately so the context vector is a multiplication of the values vector and the attention scores matrix right so the context vector for every token essentially had information about the future and that's not that's not what actually happens when we predict the next token when we predict the next token we are not going to have information about the future at all so ideally when we get the context vectors we should prevent access to this future information completely so all the elements above this diagonal should be should be essentially set to zero because we don't have access for one token we only have access to the tokens before or equal to that current token this is the main intuition behind causal attention so this is in contrast to the self attention mechanism which allows access to the entire input sequence at once now while computing the attention scores the causal attention mechanism ensures that the model only factors tokens that occur at or before the current token in the sequence this is what we saw so for every token we should only consider the tokens which occur at that point or the ones which occur before that current token to get the attention scores to achieve this for each token process we mask out the future tokens which come after the current token in the input text this is the most important sentence that's why causal attention is also sometimes called as mask retention so if you look at bi-directional attention which is self attention every token essentially attends to all the other tokens right so we get information of the attention scores which come before and after a current token but in uni unidirectional attention this is also called auto regressive attention or causal attention or mask retention there are many names we essentially mask out all the tokens which come above the diagonal so essentially all of these tokens which which come above the diagonal we will set these tokens equal to zero and that's what this rest of the lecture is going to be about that how do we exactly set these tokens to be equal to zero so the rest of the lecture is going to be a bit mathematical but until now i hope you have understood the intuition of what we are trying to do right so we mask out all the elements above the diagonal and remember that in attention weights matrix in the attention weight matrix which we had seen before so here was the attention weight matrix the main difference between attention scores and attention weights is that each row essentially summed up to one so that is a property which we ideally want to retain so the problem is that if you randomly put all the elements to zero the rows will no longer sum up to one so we'll need to do a normalization step once more so that the attention weight in each row actually sum up to one so now let's see how we actually um so the task is now we have the attention scores right we have to put all the elements above the diagonal in the attention scores to be equal to zero let's see how to do that now so there are actually two strategies to do this right the and i want you to pause here and i want to and i want you to think right the the problem at hand is that imagine or take into account this previous lecture in the previous lecture we had calculated these attention scores but now i am telling you that all of these values need to be set to zero this this all of these these values which are essentially above the diagonal these values need to be set to zero all these values need to be set to zero so how will you go about essentially calculating the attentions weights or another way to think about it is that you have the attention weights right what if you set all of these values to zero let's say you set all of these values to zero now what will happen if you if you just put all of these values to zero if that is the case you will see that the rows don't sum up to be equal to one so then what you can simply do is that whatever is remaining so if you set all of these to zero this matrix will be something like this matrix will be something like 0.1 0 0 0 it will be 0.1 0.5 0 0 0.2 so actually the actual values which you need to set to zero are these ones this this needs to be set to zero these three these two and this these values you need to set to zero so if you set this to zero directly what will happen is that this will be 0.1 0 0 0 0 this will be 0.1 0.5 0 0 0 this will be 0.05 0.2 0.2 0 0 etc so the remaining two rows also so now you see the problem here is that each row does not sum up to one now the first row sums up to 0.1 the second row sums up to 0.6 etc so what can you do in this case so that each row sums up to one the simplest thing you can do is that the first row the entire summation is now 0.1 right so you divide this by 0.1 the second row entire summation is now 0.6 so you divide each element by 0.6 the third row the entire summation is now 0.2 plus 0.2 0.4 plus 0.05 which is 0.45 so you divide each of these by 0.45 you divide this also by 0.45 and you divide this also by 0.45 what that will ensure is that that will ensure that each that will ensure that each row still sums up to one so this is actually the first strategy which is implemented to to get the causal attention scores the first strategy is like you have the you already have the attention scores to which you have applied softmax and you've got the attention weights you did that in self-attention itself then what you do is that you just add zeros above the diagonals and you get the masked attention scores but then you normalize the rows again so that they sum up to one and then ultimately you get the masked attention weights that's the strategy which we saw right now but do you see the problem with this strategy the problem with this strategy is that let's see here again you see to go from attention scores to attention weights we are already doing softmax so we are doing one step of normalization here and then again we are doing one more step of normalization when we divide by the sum of the rows so we are unnecessarily doing two normalization steps over here so then the question is can we do a smarter way of normalization so that we can we need to only apply softmax once and it turns out it is possible to do that and the way to do that is we directly target the attention scores so let me tell you how we do this so the way to do that is we directly target the attention scores here is where we make our intervention instead of making our intervention in the attention weights so the way we do this now is that essentially what we do is that all of these values all of these values which are marking right now in circles which are essentially above the diagonal and which need to be replaced eventually with zero and what we do with these values is that we will replace these values with negative infinity we will replace these values with negative infinity so let's see what the attention scores matrix now becomes so the attention scores matrix now becomes 0.6 then negative infinity negative infinity negative infinity and negative infinity the second row becomes 0.1 0.1 then 1.8 and then again i will have these three negative infinity values so i just copy pasted them over here then in the third row i'll have two negative infinity values so i'll just show this for the three rows and the third row values will be 0.2 the third row values will be 0.2 1.1 1.2 now why are we doing this because remember that when you apply softmax what softmax does is that softmax actually takes the exponent so softmax will for the first row each element the softmax will replace with e raised to so the first element it will replace with e raised to x1 divided by the sum the second element the softmax will replace with the e raised to x2 divided by the sum so this we are doing for the first row so let's say let me mark the first row over here so if this is my first row each element will be replaced with this again so if this is my first row each element will be replaced with this e raised to x1 by sum e raised to x2 by sum etc now i want you to check what is e raised to minus infinity so you will see that e raised to minus infinity is actually equal to 0 right so the second element will be replaced with e raised to minus infinity divided by some which is anyway zero because e raised to minus infinity is zero this third element will be replaced with 4th element will be replaced with 0 and 5th element will be replaced with 0. Essentially wherever we have minus infinity all of those will be replaced by 0 because exponent raised to minus infinity is equal to 0 anyways. And the first element will be replaced with e raised to 0.6 divided by e raised to 0.6 so it will be replaced with 1 so it will sum up to 1. Here the first element will be replaced with e raised to 0.1 divided by e raised to 0.1 plus e raised to 0.8. The second element will be replaced with e raised to 1.8 divided by e raised to 0.1 plus e raised to 1.8. So the in each row will sum up to 1 and we will also make sure that all the elements above the diagonal are essentially 0. So this will make sure that we are not doing two stages of normalization. Here we did two stages of normalization right we did softmax followed by every row normalization but here we are doing only one softmax normalization that's it.

That's this trick of introducing negative infinity above the diagonal and it's a very powerful trick and it saves the computations for us. So the more efficient way is essentially you have the attention scores. The more efficient way is that you have these attention scores and then you apply something which is called as an upper triangular infinity mask.

What this mask essentially means is that you replace the upper triangular which is all the elements above the diagonal will be replaced with negative infinity and then you directly apply softmax only once. So see here you have only one softmax whereas here you had one softmax to get the attention weights and then you had another layer of normalization. So that way there are two normalizations.

So this on the other hand is a much more efficient way. We are now just going to see this in code so that you can understand what we are trying to do over here. Remember in the previous lecture we started out with this inputs embedding matrix where we had your journey starts with one and step.

So these are the six inputs over here and there is a vector embedding for each input here which is a three-dimensional vector embedding and at the end of the previous lecture we had defined this self-attention class. Essentially what this does is that it takes in the inputs it finds the keys queries and the values it finds the attention scores then it gets the attention weights and then it finds the context vector in causal attention we are going to just make changes in this part so that all the elements above the diagonal are essentially masked out. So let's see what is done in a causal attention.

So this section is titled hiding future words with causal attention. We start with the same inputs and I'm just printing out the attention weights over here and I'm first going to show you this first approach. In the first approach what we do is that we start with the attention weights which have been already obtained previously.

So remember when we are showing these attention weights we have already applied softmax before to the attention scores and in the first in the first method what we do is that we just take elements above the diagonal and put them to be equal to zero. So now here is the mask which is essentially all the elements above the diagonal are zero and then we apply this mask to the attention weights. So when we apply this mask to the attention weights you will see that we have the attention weights matrix but all the elements above the diagonal are now put to zero.

But this presents the problem that every row now does not sum up to one right and that's an issue. So to solve this what we do is that we simply divide by the summation of the rows and this is what we saw. This is exactly what we saw actually on the whiteboard over here if you remember the first step what we saw.
>>>>>>> 2b76e227f38df40d3378d0614a3f3b8915545b7d

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)