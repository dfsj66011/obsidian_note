
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


（24.56）

DeepSeek 究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与 GPT-4 竞争的性能的同时，实现如此高的成本效益的？这里有四个主要方面需要讨论:

* 首先，DeepSeek 拥有创新的架构；
	* 采用了多头潜注意力机制（MLA）
	* 混合专家架构（MoE）
	* 多 token 预测（MTP）
	* 引入量化技术
	* RoPE
* 其次，其训练方法极具创造性和创新性；
	* 强化学习的兴起
	* 不再仅仅依赖人工标注的数据，而是利用大规模强化学习来教授模型进行复杂推理
	* 基于规则的奖励系统
* 第三，他们实现了多项 GPU 优化技巧；
	* 采用了 NVIDIA 的并行线程执行技术（PTX）
* 第四，构建了一个有利于蒸馏等技术的模型生态系统
	* 蒸馏至更小的模型（1.5B）


### 1、MLA

分为四个部分：

* LLMs 架构本身
* 自注意力
* 多头注意力
* KV 缓存


它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住这一点：输入嵌入向量仅包含有关该单词或标记的信息，它可能编码了该单词的含义及其位置的信息，但对邻近元素一无所知。而上下文向量则能感知邻近元素，因为邻近元素至关重要。当你观察一个句子或一个段落时，单个标记本身毫无意义，正是它们与邻近元素的关联才构成了该段落的上下文。这就是为什么在大型语言模型（LLMs）中需要这种机制。

需要理解句子中词语之间的关系和相关性，实际上这正是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个至关重要的转折点——距今正好十年——当注意力机制被提出后，人们开始意识到：与其孤立地看待词语，不如退后一步观察词语之间究竟如何相互关联。这样一来，我们就能充分挖掘文本的最大价值。因为就像图像由像素图案构成一样，语言的意义也源自词语之间的有机联系。

只有当你把所有单词放在一起看，并理解它们之间的关系时，文本或段落才有意义。所以现在的问题是，你有一个输入嵌入向量，比如说“next”，如何将其转换为上下文向量？你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频一会儿，思考一下。你有输入嵌入向量，假设你还有这些注意力分数，你将如何修改输入嵌入向量，以便以某种方式考虑这些注意力分数，从而得到一个上下文向量？

所以你可以在这里稍作停顿，首先你也可以试着思考一下这些注意力分数本身是如何计算的。好的，最简单的做法是，假设你有这个向量，还有所有其他向量，我们为什么不简单地做一个点积呢？比如你有“next”的输入嵌入向量，也有“though”的输入嵌入向量，只需在这两个向量之间做点积，就能得到α21；再对“next”和“next”做点积，就能得到α22；然后对“next”和“day”做点积，就能得到α23。

然后计算next和is的点积，这将得到alpha2_4；接着计算next和bright的点积，这将得到alpha2_5。一旦你获得了所有这些alpha值，就可以简单地计算alpha2_1乘以x1加上alpha2_2乘以x2加上alpha2_3乘以x3加上alpha2_4乘以x4加上alpha2_5乘以x5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你在这里有一个向量v1，而这里有另一个向量v2，它们之间的点积会比比如说v1和v3之间的点积更高。

因此，如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的内容。你可能会想，我想要量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，它们当然应该有一个更高的注意力分数。因此，这个计算对我来说是有道理的：我只需计算点积，然后用点积来缩放不同的向量并将它们相加。无论这个和是多少，它现在就是“next”的上下文向量。同样，我也可以为所有其他标记找到上下文向量。

这种方法有什么问题，为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们想要编码的上下文。我希望你从基本原理出发来思考，我很快就会揭晓答案。好了，主要答案是，假设我们考虑这个句子：“狗追球但没抓住”，狗追球但没抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。如果“它”现在是我的查询向量，

你是如何决定计算查询向量与其他向量之间的注意力分数的，你决定采用点积对吧，这正是我要做的。如果这是我的查询向量，要计算它与“狗”之间的注意力分数，我只需对它和“狗”进行点积运算，结果是0.51；如果我对它和“球”进行简单的点积运算，结果也是0.51。你看这里的问题：两个注意力分数完全相同，但这并不是我想要的。当你说“但它没能抓住”时，实际上指的是“球”对吧——狗追球但没抓住，所以这里的“它”指的是球而不是狗。因此当我观察时，需要更关注“球”而非“狗”。让我用不同颜色的笔写下来以便更清晰——

我在观察时需要更加关注球本身而非狗，但实际情况并非如此。如果采用简单的点积运算，就无法体现"球应比狗获得更高优先级"这一信息——当我们提及二者时，狗和球的注意力分数不应相同。这个例子绝妙地证明了为何需要选择性关注不同标记：狗追逐了球但没能接住它。第一个"它"指代的是狗（若作为查询标记会更多关注狗），而现在这个"它"作为查询标记则应更多关注球。

我不希望两者具有相同的注意力分数，因此简单的点积无法区分此处微妙的上下文关系——它没有考虑"追逐未能抓住"的语境，也无法处理语言上的细微差别（例如"抓住"更可能指代移动的物体即球）。核心问题在于：简单点积仅能衡量语义相似度，却无法应对上下文问题。很多句子都可能存在这类复杂的上下文关系，对吧？我需要设计一种机制来捕捉这些复杂性，但又不确定具体该采用什么机制。于是我们采用了研究人员沿用多年的技巧：

如果你不知道事物之间的本质关系是什么，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。这正是注意力领域所发生的事情。研究人员基本上无法弄清楚这个机制可能是什么，这就是机器学习领域或深度学习领域与物理学不同的地方。在物理学中，如果你遇到这个问题，你会花六个月到一年的时间试图为底层机制制定一个定律，以捕捉复杂性或捕捉上下文的底层机制。

但在深度学习领域，你并不这样做。你会说，我要用一堆矩阵来替代它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？于是他们发明了新的矩阵，比如所谓的查询矩阵和键矩阵。这意味着，与其仅仅看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。所以，如果我的查询在这里是正确的，我的查询是“它”，我就会将其与所谓的查询矩阵相乘。这个矩阵可以是一个高维矩阵，用于“狗”。

所以“狗”和“球”就是关键（keys）对吧？因为本质上，关键（keys）就是查询时你要寻找的所有其他标记——在这里就是“狗”和“球”。你把它们都乘以一个关键矩阵（keys matrix）。现在看这里的优势：如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵有什么特定含义，你只需随机初始化它们，然后通过反向传播来训练它们。这是研究人员长期使用的深度学习技巧——如果你自己无法理清关系，就退一步让神经网络来完成这项工作。与其用某些规则限制神经网络，不如让它自己找到规律。现在你看到了优势：我们掌握了多个可训练因素的控制权。

假设WQ是3×3矩阵，WK也是3×3矩阵对吧？比如"狗"、"球"这些就是我的键值。这些就是我们之前看到的输入嵌入向量。现在我要用查询向量乘以这些输入嵌入，再用这两个结果乘以键值矩阵。具体来说就是3×3矩阵乘以3×1向量，结果会得到3×1向量。这样键值就会变成0.9、0.2、0.1和0.1、1.8、0.1——你看这些数值都发生了变化。

因为我用矩阵与它们相乘，而查询也是如此，所以它将与查询矩阵相乘，使得查询结果变为0.5、0.1和0.5、1.0以及0.1等等。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正从输入嵌入空间转移到另一个空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的分数。

所以现在如果你计算它与球之间的注意力分数，你会发现它是0.56，而它与球之间的注意力分数是0.96。如果你计算它与狗之间的注意力分数，结果是0.56。这里你可以看到，它与球之间的注意力分数是0.96，高于它与狗之间的注意力分数0.56，因此这些注意力分数明显不同。添加这些可训练的矩阵实际上对我们有帮助，为什么有帮助呢？因为它提供了许多可调整的参数。

这样我们就可以编码标记之间的一些复杂关系。如果你进行简单的点积运算，注意力分数会是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，这会在下一节课讲到），本质上，如果你有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多的参数可以使用。如果你在这部分感到困惑，让我再重复一遍：我们开始这一节时认为，如果你有一个输入嵌入向量

那么，如何从输入嵌入向量得到上下文向量呢？要得到上下文向量，本质上需要先计算出注意力权重α。得到α之后，只需将它们与输入嵌入向量相乘，就能得到上下文向量。但问题在于，如何计算一个输入嵌入向量与另一个输入嵌入向量之间的α值，也就是如何得到注意力分数。最简单的方法可能是计算点积。比如，假设这是句子中的查询词"it"，要计算"it"与"ball"之间的注意力分数，我先计算"it"和"ball"的点积，结果是0.51；再计算"it"和"dog"的点积，结果也是0.51。

因此注意力分数看起来相似，但这并不是我想要的。因为当我说"it"时，我希望它指向的是"ball"，所以我希望"it"和"ball"之间的注意力分数要远高于"it"和"dog"之间的分数。那么现在该怎么办呢？显然点积运算的复杂度不足以捕捉这些上下文关系，我需要更多可调参数来处理——虽然目前我还不知道具体需要什么样的调节旋钮，但可以让神经网络或反向传播算法来找出这些旋钮应该是什么。至少现在可以先随机初始化这些参数，而这就是这些新术语的用武之地——我想要引入新的可训练矩阵。

让我把这个称为查询矩阵，并通过将其与查询矩阵相乘，将输入嵌入转换到另一个空间。而对于键（即狗和球）的输入嵌入，它们将与键矩阵相乘，同样转换到另一个空间。然后，我将在这些转换后的向量之间计算注意力分数。如果我的模型能正确学习这些矩阵的参数，就能让模型学会它与球之间的注意力分数是0.96，高于它与狗之间的分数0.56。不用担心这些乘法或数学运算。

现在我将在下一节课中详细讲解数学部分，目前只需记住：我们尚不清楚如何从物理层面捕捉上下文关系，因此这更像是一种简便方法或技巧。你引入了查询（queries）和键（keys）——这些随机初始化的可训练矩阵，通过训练来优化它们。你可能听说过"查询"和"键"这些术语，实际上它们被引入并没有确切的物理依据，唯一原因在于人类尚未找到直接计算注意力分数的方法，当前我们掌握的途径只有——

好的，如果我们无法解决这个问题，让我尝试将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数来处理，然后希望训练过程本身能够自行解决。这个技巧在计算机视觉领域也被人类使用过，例如当你训练一个卷积神经网络（CNN）来区分狗和猫时，你无法自己写下所有特征，而是依赖卷积神经网络来完成。这里的情况有点类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，还有另一个称为值矩阵的矩阵，以及它在我们将在下一讲中看到的下一个令牌预测任务中是如何使用的。

所以下一讲将全面探讨自注意力机制的数学原理：查询矩阵、键矩阵和值矩阵的具体运算，如何通过数学方法计算上下文向量，以及如何通过这些向量最终实现开放预测。下节课我们将深入剖析刚刚演示的这个模块，将其扩展成完整的数学推导课程。不过在今天的课堂上，我主要想先帮大家建立对查询-键-值概念的基本认知——具体数学实现我们留待下节课详解。

好的，今天的讲座就到这里结束了。今天的讲座内容可以看作是注意力机制的发展历程与自注意力机制的入门介绍。作为总结，请大家记住注意力机制的演变过程：首先是Elisa，在当时是一项革命性的突破，考虑到它诞生于1966年，这已经非常了不起了；随后出现了循环神经网络和LSTM。

<<<<<<< HEAD
所以这就是实际的下一标记，也就是我想要的，但最初预测的标记会完全偏离。这时反向传播就派上用场了，当所有参数都存在时，它们实际上会被优化。我们稍后会讲到这一点。但现在，让我再解释一下最后一步。我们有了每一个标记，现在每一个标记都与一个不同的均匀分布相关联，其维度为50,000。为什么维度是50,000呢？因为我们必须为这里的每一个单词预测下一个标记。所以对于O，我们必须预测下一个标记。

所以我们查看那个索引，也就是具有最高概率的标记ID。我们查阅词汇表或标记ID列表，然后进行反向映射，找到对应那个标记ID的单词。比如，如果这里的标记ID是555，或者这个标记ID是5000。我在这里找到5000对应的单词，理想情况下我希望这里对应的是"true"。但在初始阶段模型未经训练时，可能对应的是"for"。因此实际预测结果可能是"for"。同样地，我也会得到"true"的实际预测结果。

也许这里的最高令牌ID是你的朋友，然后我会在这里获取最高的令牌ID，这就是我如何预测每个令牌的下一个令牌。然后我会找到实际值和预测值之间的最后一项。这就是整个LLM架构的结构。所以现在如果你去输出层，也就是我的最后一层，你会看到我们有两个相互链接的东西。

好的，我们有了最终的层归一化层，它连接到输出层，然后我们有了用于下一个令牌预测的矩阵。这个逻辑矩阵就是这个，这个就是我刚才展示给你的那个，我们用它来进行下一个令牌的预测。现在你可能会想，这里优化的所有参数是什么。从一开始，就是这些存在的令牌嵌入值。

我们无法先验地知道这些参数。因此，让我用星号标记那些需要训练的参数——这些是我们无法预先知晓的。所以这些是需要训练的位置嵌入分配参数，我们无法预先知晓。因此这些都需要经过训练。Transformer模块的每一个组成部分都包含需要训练的参数：多头注意力机制有需要训练的参数，前馈神经网络也有需要训练的参数。而且这样的模块有12或24个，这进一步增加了参数量。因此在整个流程中存在着海量需要训练的参数，甚至最后的神经网络层也是如此。

它有这么多需要训练的参数，所有这些参数加在一起构成了参数总数，达到1750亿甚至可能上万亿。回想一下我们最初讨论的引擎，对吧？我们一开始只是知道并思考：好吧，这是个LLM引擎。但LLM引擎究竟如何运作？LLM引擎底层的参数是什么？这1750亿个参数究竟分布在哪里？现在我们已经深入了解了其详细架构——包括输入部分、处理器部分和输出部分——并追踪了一个单标记的完整旅程。

在令牌处理过程中，首先会经历输入阶段，此时它被隔离。它会被分配一个令牌ID或徽章，然后接受一组768个问题的测试（即令牌嵌入，用于编码含义）。接着，它会接受第二组问题，即位置嵌入，用于编码其位置值。我们将令牌嵌入和位置嵌入相加，得到输入嵌入，也就是每个令牌的统一表示。凭借这种统一表示，不同的令牌会被送入Transformer模块进行处理。

每个Transformer块基本上包含归一化层、多头注意力、dropout、再次归一化、前馈网络和dropout，其间穿插着两个跳跃连接。在GPT-2中，有12个这样的块；在GPT-2 XL中，我认为有48个这样的块；而在更高级的GPT模型中，可能会有96个甚至更多这样的块。因此，每个token都需要经过所有这些块，当它从所有这些块中出来时，其大小仍然保持768。

然后，它还会经过一个归一化层，其尺寸仍为768。最后，我们有一个输出层，其中每个标记都会被转换为一个大小为50,000的向量，这个大小等于词汇表的大小。接着，我们基本上会查看每个标记的50,000维向量，然后找出概率最高的那个标记ID，并用它来预测下一个标记。因此，在一个序列中，我们会有多个输入输出的预测任务。

因此，如果我们有一个包含五个标记的序列，就会有五个输入输出预测任务，这些任务实质上构成了我们的损失函数。然后我们的损失函数基本上会进行反向传播，所有的参数都会被优化——这1750亿个参数分布在多个环节：标记嵌入层中有参数，位置嵌入层中有参数，Transformer块的多个组成部分中有参数，输出层中也有参数。

这些参数都是通过反向传播进行优化的，最终我们得到的是一个对语言本身有直觉的模型，它还能预测下一个标记。正如你在这里看到的，下一个标记预测就是我们的任务。我们预测下一个标记，并与实际值进行比较。这就是任务，但由于我们有这么多参数，这个任务的副产品就是学习语言本身。所以在今天的讲座中，

我的主要目的是带你了解一个标记（token）的旅程。试着从单个标记的角度出发，思考会发生什么。试着打开这个引擎，打开大型语言模型（LLM）的引擎，真正去观察这个引擎是如何运作的。我希望我已经向你传达了这一点。我之所以构建这个类比或标记旅程的故事，是为了让你真正理解LLM架构内部发生的事情。因为如果不理解这一点，我们就无法继续前进到下一部分——注意力机制。

现在的计划是，在下一讲中，我将首先阐述为什么我们需要注意力机制。然后我们会探讨自注意力机制，接着是多头注意力机制，最后是键值缓存。所以，如果你看到接下来的计划，首先是注意力机制的必要性，然后是自注意力机制，最终是多头注意力机制。正如我提到的，所有未来的课程都已经详细规划好了，这将不是一个只有5到10分钟视频的简短系列。

本系列的每一期视频都会比较长，大约40到45分钟，我计划详细讲解所有步骤。"多头潜在注意力"是一个非常重要的概念，但我希望大家在真正理解这个概念时能保持同步。非常感谢大家，我真的很期待在下一讲中见到你们。请和我一起做笔记。这个系列可能会有点难度，我正试图以尽可能容易理解的方式来提炼这些概念，但过程中可能还是会遇到一些挑战。

所以请大家做好笔记，以巩固所学概念。谢谢大家，期待在下一次讲座中见到你们。大家好，我是Raj Dhandekar博士，2022年从麻省理工学院获得机器学习博士学位，也是“从零开始构建深度探索”系列的创始人。在开始之前，我想向大家介绍本系列的赞助商和合作伙伴——InVideo AI。大家都知道我们非常重视从基础构建AI模型的内容，InVideo AI的理念和原则与我们非常相似。

让我来为你展示一下。这里是InVideo AI的网站，凭借一支精干的工程团队，他们打造出了一款令人惊叹的产品——仅需输入文字指令就能生成高质量AI视频。正如你所见，我输入了一段文字指令："制作一款超写实的高端奢侈腕表广告视频，呈现电影级质感"。点击生成视频后，很快我就获得了这段令人惊艳的成片，其逼真程度令人叹服。最让我震撼的是视频对细节的把握，看看这个画面，材质纹理的表现简直不可思议。而这一切都源自于短短的文字指令，这就是InVideo产品的强大之处。

你刚刚看到的精彩视频背后，是InVideo AI的视频创作流程在支撑。他们正从底层原理重新构想视频生成与剪辑方式，致力于基础模型的实验与优化。该公司拥有印度规模最大的H100和H200计算集群之一，同时也在测试B200芯片。作为印度发展最迅猛的AI初创企业，InVideo AI正为全球市场打造产品，这正是我如此认同他们的原因。好消息是，他们目前有多个职位正在招聘。

你可以加入他们出色的团队，我会在下面的描述中发布更多详细信息。大家好，欢迎来到“从零开始构建深度搜索”系列的下一个讲座。今天我们将学习一个非常重要的概念，那就是注意力机制的必要性。首先，让我快速总结一下我们在这个系列讲座中已经涵盖的内容，然后再来讨论今天的主题。在之前的两节课中，我们学习了名为“深度搜索基础”的内容，

我们探讨了深度探索架构的四个阶段，或者说我们将在这个系列讲座中涵盖的四个阶段。第一阶段是深度探索架构的创新。第二阶段是训练方法。第三阶段是GPU优化技巧，第四阶段是模型生态系统。在上一讲中，标题为LLM架构。

我们开始特别关注第一阶段。我们的主要目标是，在接下来的两到三节课中，必须开始理解多头潜在注意力机制，这是深度探索架构中的第一个重大创新。然而，要理解MLA（多头潜在注意力），我们不能直接开始研究这个概念，而是需要循序渐进地接近它。因此，我们首先研究了LLM本身的架构，发现LLM的架构大致是这样的。

首先我们有了输入部分，然后有了处理器，接着是输出部分。每个部分基本上都有不同的构建模块。所以我们发现，如果把LLM看作引擎的话，要真正理解这个引擎如何运作，我们确实需要打开引擎，看看里面到底有什么。

那么，大型语言模型（LLM）究竟是如何学会预测下一个标记或下一个词的呢？如果把这个过程类比为汽车的运动，我们会发现，当你给汽车加油时，汽车就会动起来，对吧？引擎里发生了一些变化，汽车就移动了。同样地，对于LLM来说，"燃料"就是你输入到引擎（即LLM）中的单词序列，而输出则是下一个词的预测或下一个标记的预测。这就是为什么我们也把LLM称为"下一个标记预测引擎"。

如今这个引擎拥有海量参数。GPT-3的参数量达1.75亿个，GPT-4可能达到约1万亿个，而最新发布的GPT-4.5参数量可能在5到10万亿之间。为了真正理解这个引擎的运作原理，我们打开了这个黑匣子，从以下三个维度进行了剖析。

但我并没有直接向你解释这三个方面。我告诉你，如果你把自己想象成一个令牌或一个世界，并想象自己经历不同的阶段会怎样。所以如果你是一个令牌，首先你会被分配一个令牌ID，那是你的徽章，然后你会被分配一个令牌嵌入向量，接着是一个位置嵌入向量，令牌嵌入和位置嵌入相加，就形成了你的制服——输入嵌入。

同样地，无论你的邻居是谁，他们也都有自己的统一输入嵌入表示，你们一起搭乘通往Transformer的列车。每个Transformer块包含多个组件，如归一化层、多头注意力机制、随机失活层、跳跃连接、又一个归一化层、前馈神经网络和另一个随机失活层，最后再接一个跳跃连接。而这仅仅是一个Transformer模块的结构。

像这样的Transformer模块有很多，可以是12个、24个、96个等等。因此，作为一个标记（token），连同你的统一表示（即输入嵌入），你必须经过所有这些Transformer模块的处理。当你通过所有模块后，会有一层归一化处理，最后是一个输出层。如果你是一个768维的向量，你将被映射或投影到一个50,000维的向量中。为什么是50,000？因为这是词汇表的大小，这实际上帮助我们选择下一个标记。

这就是令牌在LLM架构中的完整生命周期。在今天的讲座中，我们将重点讨论整个架构中的一个单一环节——多头注意力机制。我希望你们能理解多头注意力机制在其中的作用位置。在所有这些步骤中，令牌会经过多头注意力机制，它出现在Transformer块中，而且在这个特定的情况下，它位于归一化层之后。今天，我们将要理解多头注意力机制，但在此之前，我们首先要理解什么是注意力本身，以及为什么需要注意力机制。我们为什么要开始讨论“注意力”这个术语？为什么它在最近变得如此流行？因此，我们今天的目标是激发自注意力这个概念。

我们今天不会深入探讨自注意力机制的数学原理。今天的核心目标是理解为什么我们首先需要注意力机制，以及它为何能成为大型语言模型的关键突破点。试想一下，在所有处理词元的步骤中，最重要的环节就藏在Transformer模块里——确切地说，就藏在Transformer模块的某个步骤中，那就是注意力机制。

这就是为什么我在这里用不同的颜色标记出来。这个训练模块本质上提供了所有使LLM在理解语言方面表现优异的特性。因此，为了真正理解注意力机制的工作原理，我专门为你们准备了这堂单独的课程，我们将尝试阐述引入注意力机制的必要性。

在理解我们为何需要注意力机制以及它如何改变了这一领域之前，让我们先深入探讨一下生成式人工智能本身。让我们先回顾一下历史。我认为理解注意力机制的本质至关重要。20世纪60年代有一个叫ELISA的聊天机器人。你可以想象一下。我不会称它为大型语言模型。

我会称它为聊天机器人。这是第一个旨在充当治疗师的自然语言处理聊天机器人。所以如果你问“请告诉我你有什么困扰”，我可以说我在学习人工智能方面遇到了困难。你能帮我吗？你觉得学习人工智能很困难是正常的吗？你看它其实没那么有用，但要记住那是在1960年代。那时候这被认为是一场革命。所以你可以看到这个原始程序是由约瑟夫·维森鲍姆在1966年描述的。

与此相比，看看现在的ChatGPT，我们说我想学习人工智能。你能帮我吗？然后让我把模型切换到GPT 4.0，这样我能得到更快的回复。接着你会看到，我立刻获得了一份可操作的项目清单，可以立即开始实施来学习人工智能。因此，在短短64年的时间里，我们在自然语言处理和语言理解领域已经取得了长足的进步。

让我们来看看从20世纪60年代到2020年代发生了什么。20世纪60年代是ELISA聊天机器人问世的时期。然后在20世纪80年代和1997年，出现了生成式AI或语言建模领域的两个重要基础构建模块，你可以把它们看作是20世纪80年代出现的循环神经网络和1997年问世的长短期记忆网络。

这两者都建立在20世纪70年代神经网络蓬勃发展的基础上。人们发现神经网络可以做很多了不起的事情，但神经网络真正的问题在于它们本质上无法处理记忆，这是神经网络最大的缺陷。为什么必须处理记忆呢？因为想象一下，如果你要预测下一个单词是否正确，或者生成一些新文本，你会想知道段落的开头是什么。

仅仅记住一点信息是不够的。上下文非常重要，这个词在今天的讲座中会频繁出现。上下文本质上意味着，如果给模型提供了一大段文字，而我们希望模型执行某项任务或仅通过查看句子来回答问题，仅靠句子中紧邻出现的单词是没有帮助的。

我们需要先了解这段话开头所讲述的背景。比如说，我先提到我来自印度浦那，然后接着写了一大堆不同的内容。我写了一大堆句子，最后才问我说的是什么语言。现在要根据这个问题生成一个回答。中间提到的所有这些内容并不是很重要，但我需要了解这段开头的内容，因为这才能真正帮助我回答这个问题。这就是为什么需要记忆功能。而神经网络原本并没有真正编码记忆的机制，但循环神经网络和长短时记忆网络解决了这个问题。让我快速展示一下它们是如何解决这个问题的。

因此，人们最初通常使用RNN或LSTM进行序列到序列建模，也就是所谓的序列到序列翻译任务。简单的序列到序列翻译任务可以仅仅是语言翻译。如果你想处理一串单词，并希望将它们从一种语言翻译成另一种语言，你可以使用循环神经网络。

所以循环神经网络有一堆隐藏状态，比如h0、h1、h2、h3。假设这些是输入单词"I will eat"，我想把它翻译成法语，即"Je vais manger"，已经写在这里了。我可以用循环神经网络进行这种翻译的方式是，我有一个编码器块和一个解码器块。

这些编码器和解码器块都有一个称为隐藏状态的东西，你可以将其视为向量或矩阵。这里是编码器的隐藏状态h1、隐藏状态h2、隐藏状态h3，然后我们有解码器的隐藏状态s1、隐藏状态s2和隐藏状态s3。让我们可视化编码器块中发生的情况。

在编码器模块中，首先输入句子的第一个单词"I"，它会转换为第一个隐藏状态h1；接着输入第二个单词。需要注意的是，要获得第二个隐藏状态h2，不仅要使用第二个输入，还要利用前一个隐藏状态。这就是循环神经网络捕捉记忆的方式。

前一个隐藏状态用于计算当前的隐藏状态。同理，当你来吃饭时，输入的是x3，但要计算最终的隐藏状态h3，你需要使用前一个隐藏状态h2，而h2编码了h1或者说h2包含了h1的信息。因此，h3现在既包含了h1的信息，也包含了h2的信息。

h3拥有关于过去的知识，它保留了记忆。因此，h3可以被视为一个向量，可能是500维或1000维的向量，本质上包含了输入的所有信息含义。这是唯一一个传递给解码器的高维向量。解码器随后会查看这个向量，解码器的每一个隐藏状态都开始对其进行解码。

所以解码器想象一下你有一个解码器，就像接力赛一样。我们可以把它想象成一场接力赛。接力棒在h1手中，它传给h2，h2再传给h3。现在h3拿到了接力棒，并将其传递给解码器。现在我是解码器的第一个隐藏状态。我拿到了接力棒。

我先解码第一个标记JOR，然后将其传递给第二个解码器，接着解码第二个标记，最后解码第三个标记。这就是循环神经网络中语言间翻译的工作原理。LSTM（长短期记忆网络）在预测当前隐藏状态时与前一个隐藏状态有所不同。它们还维护一种称为细胞状态的结构。因此，LSTM能够兼顾长期记忆和短期记忆。

所以这本质上是为了扩大它们处理的上下文窗口。但架构从根本上保持不变。只是对于LSTMs来说，底层的数学要复杂得多。现在你看出问题了吗？看看这个例子，我们之前用它来讨论上下文问题。那么，当处理大段文字时，循环神经网络存在什么问题呢？比如说你有一大段文字，你想翻译整段内容。

循环神经网络在处理这个问题时存在什么缺陷呢？让我们举一个简单的例子，以便更清楚地解释这个问题。假设你眼前有这样一段文字——这是我随机从ChatGPT生成的文本中截取的。我希望你这样做：首先花30秒到1分钟或更长时间完整阅读这段文字一次。然后闭上眼睛，将它翻译成你想要的任何语言。

你可以把它翻译成印地语、西班牙语、法语、德语，或者你会的任何其他语言。所以，闭上眼睛，把它翻译成另一种语言。你能做到吗？这不可能，对吧？因为当你闭上眼睛时，你基本上只能记住你读过的一些概要。

你并不需要记住每一个确切的单词，但为了翻译，我们不得不逐字逐句地看并单独翻译，对吧？这正是循环神经网络所面临的问题。为什么呢？因为我们只依赖一个隐藏状态或一个向量来捕捉过去发生的所有上下文信息。

这就像你闭上眼睛，试图回忆整段内容。这个隐藏状态或向量承受着巨大的压力，要重新唤起所有记忆——如果是一大段文字，你怎么能把所有信息压缩进一个500维或1000维的向量里呢？自然会有信息丢失。而且别忘了，解码器并不会从第一个或第二个隐藏状态获取输入。

它仅将最终的隐藏状态作为输入。这就是主要问题所在。从编码器传递到解码器的上下文仅来自最终的隐藏状态。这就是为什么这被称为上下文瓶颈问题。最终的隐藏状态就像你闭上眼睛试图记住整段文字时的情况——这是不可能做到的。你不能给一个向量施加这么大的压力。

循环神经网络中的上下文瓶颈确实不利于保留长距离上下文，这为理解注意力机制的出现提供了很好的途径。现在，如果你看一下循环神经网络，让我在这里擦掉一些东西。你认为这个问题的理想解决方案是什么？假设这就是我们目前遇到的问题——所有上下文信息无法被压缩到单个向量中。

这个问题的解决方案是什么？嗯，解决方案似乎是这样的：比如说，当我们在解码时，当我解码这个的时候，假设我在解码器的第一个隐藏状态下进行解码，与其只能访问最终的隐藏状态，不如让我也能访问编码器的所有隐藏状态以及所有的输入。

因此，如果在解码过程中我能获取所有输入，那么我就可以做出预测，比如在解码第一个标记时，我应该更重视第一个隐藏状态，而不应该重视第二个隐藏状态，也不应该重视第三个隐藏状态。如果我能够创建一种机制，能够理解对不同标记需要赋予的相对重要性，那会怎样呢？

这是本次讲座最重要的部分，请大家务必集中注意力。解决上下文瓶颈问题的关键在于：在解码过程中，我需要建立一种机制来量化每个隐藏状态应获得的注意力权重。比如在首次解码时，我可以给第一个隐藏状态分配80%的权重，第二个和第三个隐藏状态各分配10%的权重。

如果我掌握了这些信息，那么上下文瓶颈问题就迎刃而解了。因为即使段落很长，只要我能关注到该段落中的所有标记，我实际上也能重视之前出现的内容。比如说这是句子，我在这里预测某个内容。在注意力机制中，我可以尝试确定应该给予第一个标记、第二个标记、第三个标记以及整个段落多少注意力，然后我可以说我需要对这些标记给予最大的关注。

所以你看，我已经开始在这里使用“注意力”这个词了。你可以把它理解为对段落中各个标记的相对重要性。因此，理想的解决方案是我们在解码过程中需要有选择地访问输入序列的部分内容。

因此，当你在解码时，比如说我想做出某些预测，这样如果我在解码第一个标记，对吧，这个alpha 1 1就是赋予第一个隐藏状态的相对重要性，alpha 2 1是赋予第二个隐藏状态的相对重要性，alpha 3 1则是赋予第三个隐藏状态的相对重要性。

所以我希望能做出这样的预测：α₁₁为100%，α₂₁为0，α₃₁为0，诸如此类。但你看这里，我不再仅仅依赖h₃，而是依赖之前所有的隐藏状态，不仅如此，我还在量化应该对每个之前的隐藏状态依赖多少。因此，这一点很重要，我们需要在解码过程中有选择性地访问输入序列的某些部分。我想通过这个例子再次向你们解释这一点。假设我在这里截了一张图，然后把它放在这里。让我们试着理解这句话的真正含义。如果我让你翻译这一段，你会怎么做？你可能会从——比如说这个部分开始，假设你从这里开始，看看你一次能看多少个单词。

假设这就是你的上下文窗口，你无法一次看到更多的单词，那么你的大脑会怎么做呢？对于你的大脑来说，段落中剩下的内容是不相关的，对吧？所以你会在大脑中屏蔽掉所有这些内容，你会屏蔽掉所有不相关的东西。因此，你会认为所有这些其他内容对你来说都不重要，你只想关注当前的上下文，并选择性地忽略其他所有内容，对吧？所以你的大脑会集中注意力——这一点非常重要——你的大脑会集中注意力于段落的这一部分，然后开始只翻译这一部分的内容。翻译完这一部分后，你的大脑才会移动到下一部分。

也许接下来的五个词翻译完后，你的思绪会转移到下五个词，然后再到下五个词，依此类推。所以你会看到自己在某个特定时刻的行为：你的大脑正在做出决定，当你在看第一个上下文窗口时，我想要对这些标记给予最大的关注。同时，你的大脑也在做出一些定量的判断，完全不去关注其余的标记，完全不去关注这些。这就是在解码过程中选择性访问输入序列部分的真正含义，对吧？选择性访问输入序列的部分——这是理解注意力机制最重要的一句话。

这意味着只关注输入序列中在当下重要的部分，这看起来像是一种直觉行为，对吧？但实际上，这是帮助语言模型变得更好的最重要因素。想象一下，你想在一段文字中找出拼写错误，你会怎么做？你会逐步浏览这段文字，先关注一个部分，然后再关注下一个部分，以此类推。这正是大型语言模型也需要做的——它们需要选择性地关注序列中的特定部分，然后找出例如含有拼写错误的那部分。

没错，这就是为什么我们在解码时需要注意力机制的核心原因——我们可以量化每个输入标记应该获得多少重要性或关注度，这就是我现在要表达的重点。实际上，第一个真正实现这一点的论文并不是《Attention Is All You Need》。你看这篇论文（《Attention Is All You Need》）目前约有117万次引用，引用量惊人。很多人误以为注意力机制最早是在这篇论文中提出的，但实际上首次提出注意力机制的论文是这篇《Bahdanau Attention Mechanism》。

所以如果你搜索“Bahdanau Attention”并点击这个，你会发现这是第一篇真正将注意力机制应用于翻译任务的论文。我认为这篇论文发表于2014年，并在2015年的ICLR会议上发表。他们的主要目的是实现了这种架构，使我们能够选择性地决定对每个隐藏状态分配多少注意力，这就是所谓的a1、a2、a3——注意力分数。基于此，你可以执行翻译任务，他们基本上证明了如果我们使用注意力机制，可以以更好的方式进行序列到序列的翻译。所以请记住这篇特别的论文。

所以在这里你可以看到，在这个图表上，x轴是英语句子，y轴是法语翻译。对角线基本上显示了英语单词最关注翻译的法语单词。有趣的是，顺序并不相同。比如"European Economic Area"（欧洲经济区）在英语中是这么说的，但翻译成法语时变成了"zone économique européenne"（经济欧洲区），所以它被翻译为"zone économique européenne"。

所以这并不是逐字逐句的直接翻译，但注意力机制本质上能识别出——看，这里是最亮的对吧？这意味着法语中的"european"对应英语中的"european"，尽管它们出现的位置并不相同。英语的"european"出现在第五个位置，而法语的"european"出现在第七个位置。因此，所有最亮的区域并不都在对角线上，有些偏离了对角线本身——这些细节也都被注意力机制捕捉到了。

这个图表最酷的地方在于这些非对角线元素，因为翻译并不总是逐字进行的。在某些语言中，有些词会先出现，而在其他语言中则后出现，这取决于名词、词汇等的排列方式。这就是这篇论文的亮点所在，这是第一篇将注意力机制应用于翻译任务的论文。随后，《Attention Is All You Need》这篇论文引入了Transformer模块，并将注意力机制整合到了Transformer模块中，这是该论文的主要优势或独特之处。

是这样的，RNN（循环神经网络）和LSTM（长短期记忆网络）在语言翻译方面表现不错，但它们存在上下文瓶颈问题。于是研究人员发现，构建深度神经网络其实并不需要RNN架构。2014年，Bahdanau注意力机制被提出，他们在保留RNN的同时引入了注意力机制。你可以把Bahdanau注意力机制中的编码器-解码器模块想象成类似结构，只是额外添加了这个注意力机制。三年后（也就是2017年），研究人员进一步发现，即便是RNN架构，在构建自然语言处理的深度神经网络时也并非必需，于是他们提出了Transformer架构。

那么让我们回顾一下这个领域的发展历程：20世纪80年代是RNN的时代，1997年诞生了LSTM（其实1966年就出现了ELISA——让我把这个也写上）。到了2014年，注意力机制被提出，但当时仍依附于RNN的编码器-解码器架构。真正的转折点在2017年，研究人员发现自然语言处理任务其实不需要RNN结构。于是RNN被彻底摒弃，只保留了注意力机制的核心思想，但这次它与Transformer模块相结合——这正是2014年论文与2017年论文的本质区别：2014年的架构仍保留RNN模块，而2017年则用Transformer架构完全取代了它。

于是这就形成了完整的架构——现在我们有了Transformer模块，其核心就是注意力机制。这就是GPT的架构，它与原始Transformer论文中的架构不同。原版Transformer同时包含编码器和解码器，而我现在展示的这个架构只有解码器部分。目前不必对此感到困惑，今天课程的主要目的是让大家从自然语言处理的发展历程中理解为何需要引入注意力机制。关键要记住的一句话是：在解码过程中，我们需要选择性地访问输入序列的特定部分。最初，注意力机制是在RNN中率先提出的。

后来研究人员发现，好吧，让我去掉RNN，但仍然尝试将注意力机制融入其中。于是他们在2017年将其与这里的Transformer模块合并。到了2018年，GPT架构问世，于是就有了注意力机制加GPT。GPT基于原始的Transformer架构，但与同时包含编码器和解码器模块不同，它只保留了这里的解码器模块，并保留了注意力机制。我们之前讨论过为什么需要注意力机制，这一点在LLM架构的这个部分得到了体现。

现在我们将开始探讨：如果你观察下一个标记预测任务，自注意力机制究竟是什么？上下文向量又是什么？注意力机制的主要目的是什么？那么，让我们来看看自注意力机制在下一个标记预测中的主要目的是什么。既然我们已经理解了注意力机制及其发展历程，现在就来学习这个概念吧。让我们试着理解“自注意力”这个术语，它实际上意味着什么？

自注意力机制意味着它是一种让输入序列中的每个位置都能关注到同一序列中所有位置的机制。也就是说，如果我有一个句子比如“明天是晴天”，自注意力本质上指的是，到目前为止如果你看RNN，我们看到的是解码器需要给予编码器的注意力。所以如果第一个解码词是法语词，我们基本上是在看两个不同的序列之间的关系。假设英语序列是这样的，比如“我会吃”，法语序列是这样的，这是法语序列，我们在看如果你在做第一次解码，你应该给予英语序列多少注意力。

因此，这里的注意力机制是在序列之间进行的，而不是在同一个序列内部。而自注意力机制则是在预测下一个标记时使用的，这通常是大型语言模型（LLM）的做法。由于LLM的任务是预测下一个标记，它们并不是专门为翻译任务而训练的。在预测下一个标记时，本质上你并不需要区分不同的语言，你只需要处理一堆数据即可。

因此，与其在两个序列之间建立注意力机制，你只需取同一个句子，比如说，当你观察下一个词时，你试图找出：如果关注一个标记或一个词，我应该对句子中所有其他标记给予多少注意力。这是这里最重要的理解点。如果你关注一个标记或一个词，那么邻近的词对这个特定标记有多重要？为什么这个知识如此重要？你能试着思考为什么这个知识对我们至关重要吗？为什么我们需要编码一个给定标记周围其他标记的信息？

这一知识对我们之所以重要，是因为在预测下一个词时，本质上需要了解序列的上下文信息，需要掌握不同词语之间的关联。再以同一个例子说明，假设我说"我来自印度浦那，我会说"，就以这句话为例，当你看到"说"这个词时，我需要知道在"说"之后

必须高度关注浦那和印度，也许其他所有词汇都不那么相关，因为我所说的方言深受我所在地区的影响。因此，当你观察一个词汇云时，如果你的Transformer架构或LLM引擎掌握了某个词与周围其他词的关系信息，以及需要赋予这些周边词汇多少权重，那么仅凭一个标记就能非常非常容易地预测下一个标记。

这就是自注意力机制变得非常重要的原因。如果没有自注意力机制，你就会丢失关于其他标记与我们选择的特定标记之间关联的上下文信息。我希望现在你能理解为什么它被称为"自注意力"。在这个案例中，当我们处理序列到序列的语言翻译时，注意力机制作用于不同序列之间；而自注意力则是当我们观察单个句子本身时，关注句子内部的标记，并从根本上理解这些标记之间是如何相互关联的。

让我们再次以同样的例子来说明，第二天是光明的。记住，当这些标记进入转换器架构时，它们现在是向量，正如你之前所见，它们现在具有统一的维度。记住，每个标记都有一个统一的768维向量，这就是输入嵌入。所以，每当我在这里展示这些块时，本质上就是指一个向量。

因此，第二天是明亮的，现在这些都是向量，对于一个转换器来说，它不理解单词，也不理解句子，它只知道每个标记都是一个向量。所以“the”是一个向量，我称之为x1，“next”是一个向量，我称之为x2，“day”是一个向量，我称之为x3，“is”是一个向量，我称之为x4，“bright”是一个向量，我称之为x5。现在，如果我在看一个特定的词，比如我在这里展示的“next”，我想看看如果我关注这个词“next”，我应该给予其他所有标记多少注意力，而这个注意力是由alpha to 1给出的，或者我称之为符号alpha to 1。

为什么要关注第一个标记，因为下一个是第二个标记，我想找出第二个标记与第一个标记之间的注意力分数，即α1，这将是α2，这将是α3，这里将是α4，这里将是α5。本质上，如果我在看一个特定的标记，我想找出所有的注意力分数，这被称为查询，我现在关注的标记被称为查询标记，我想找出如果我在看查询，我应该给所有其他标记多少注意力，这些有时在通用术语中也被称为键，所以最终我想做的是，假设我得到了这些注意力分数，我想利用这些信息。

我想以某种方式获取所有这些信息，并将这个向量从输入嵌入向量转换为上下文向量。这里有一个非常重要的区别需要说明：目前next是一个输入嵌入向量，对吧？所以它包含了词元嵌入加上位置嵌入。但上下文向量是完全不同的东西。如果这是next的输入嵌入向量，而我在同一空间中绘制上下文向量，那么这就是next的上下文嵌入向量。实际上，上下文向量比词元嵌入向量丰富得多，为什么呢？

因为词嵌入向量或输入嵌入向量不包含相邻单词的信息，但现在我的上下文向量包含了相邻单词的信息，这些信息现在被融入到了我的输入嵌入中。所以，如果你有一个输入嵌入向量，也就是我之前提到的那个统一的向量，并且你用关于相邻单词的上下文信息来增强这个输入嵌入向量，我们将会看到这种增强是如何实现的，但本质上这会导致所谓的上下文向量。因此，注意力机制或自注意力机制的整个目标就是将所有的输入嵌入向量转换为上下文向量。

因此，所有这些统一的向量——我们刚才看到的这些统一向量，所有词元在经过归一化层后都会输出一个768维的统一向量。当它们进入多头注意力层时，输入的是嵌入向量，而输出的则是上下文向量。经过注意力模块处理后，输出的内容会丰富得多，这就是为什么我用不同颜色标记它们。之所以更丰富，是因为现在它还编码了其他词元的信息，保留了上下文。因此，上下文向量是一种经过丰富的嵌入向量，它融合了所有其他输入元素的信息。在自注意力机制中，上下文向量扮演着至关重要的角色。

它们的目的是通过整合输入序列中所有其他元素的信息，为序列中的每个元素创建丰富的表征。因此，请再次记住：输入嵌入向量仅包含该单词或标记本身的信息——它可能编码了该单词的含义及其位置信息，但对相邻元素一无所知。而上下文向量则能感知相邻元素，因为邻居关系至关重要。试想，当你阅读一个句子或段落时，单个标记本身毫无意义，正是它们与相邻元素的关联关系才最终构成了段落的上下文语境。对于大语言模型而言，这种机制之所以必要...

需要理解句子中词语之间的关系和关联性，这实际上是让大语言模型（LLMs）表现如此出色的根本原因。回顾历史上的技术进步，从ELISA、RNN到LSTM，注意力机制在那时尚未出现。我认为2014年是一个关键转折点——距今十年前——当注意力机制被引入后，人们开始意识到：与其孤立地看待词语，不如退后一步，尝试理解不同词语之间本质上是如何相互关联的。这样我们就能从文本中挖掘出最丰富的内涵，因为就像图像由像素模式构成那样，只有当您将所有词语作为一个整体并观察它们之间的关联时，文本或段落才具有真正的意义。

那么现在的问题是，你已经有了一个输入嵌入向量，比如说对于下一个词，如何将其转换为上下文向量？也就是说，你有一个输入嵌入向量，如何从输入嵌入向量过渡到上下文向量？我希望你从基本原理出发思考这个问题，暂停视频片刻，好好想一想。你有了输入嵌入向量，假设还有这些注意力分数，你会如何修改输入嵌入向量，使得这些注意力分数能够被纳入考虑，从而得到上下文向量？你可以在这里暂停一下，首先也可以思考一下这些注意力分数本身是如何计算的。

好的，最简单的做法是这样的：假设你有一个向量，还有其他所有向量，我们为什么不简单地做个点积呢？比如你有“next”这个词的输入嵌入向量，还有“though”的输入嵌入向量，就在这两个向量之间做个点积，这样就能得到α2,1；然后在“next”和“next”之间做个点积，得到α2,2；接着在“next”和“day”之间做个点积，得到α2,3；再在“next”和“is”之间做个点积，得到α2,4；最后在“next”和“bright”之间做个点积，得到α2,5。一旦你得到了所有这些α值，

你可以简单地计算 alpha 2 1 乘以 x 1 加上 alpha 2 2 乘以 x 2 加上 alpha 2 3 乘以 x 3 加上 alpha 2 4 乘以 x 4 加上 alpha 2 5 乘以 x 5。那么为什么我们要在这里使用点积呢？本质上，你可能会想到点积的原因是，点积实际上包含了关于向量是否相似或彼此接近的信息，对吧？如果你这里有一个向量 v1，另一个向量 v2，它们之间的点积会比比如说 v1 和 v3 之间的点积更高。所以如果两个向量相似，点积就会更高，而这正是你希望通过注意力机制来量化的东西。你可能会想，我希望量化两个向量是否相似，对吧？所以如果“next”和“the”更相似，当然它们应该有一个更高的注意力分数，所以这个计算对我来说似乎是有道理的。

我只是计算点积，然后用各自的点积缩放不同的向量并将它们相加，所以无论这个和是多少，它就会成为下一个词的上下文向量。同样，我也可以为所有其他标记找到上下文向量。这种方法有什么问题？为什么这种方法行不通，或者为什么我们不能简单地用点积来计算注意力分数？你可以在这里暂停一下，试着思考一下我们试图编码的上下文。我希望你从基本原理出发思考，我很快就会揭晓答案。好的，主要的答案是，假设你考虑这个句子“狗追球但没抓住”，对吧，

狗追着球跑，但没能抓住它。假设“狗”的输入嵌入向量是这个，“球”的输入嵌入向量是这个，“它”的输入嵌入向量是这个。好的，如果“它”现在是我的查询向量，你是如何决定计算查询向量与其他向量之间的注意力分数的呢？你决定使用点积对吧，这正是我要做的。如果“它”是我的查询向量，为了得到它与“狗”之间的注意力分数，我只需计算“它”与“狗”的点积。如果我计算点积，结果是0.51；如果我再计算“它”与“球”的简单点积，结果也是0.51。你看出来问题了吗？这两个注意力分数完全相同，但这并不是我想要的。当你说“但没能抓住它”时，

实际上这里指的是球，对吧？狗追着球跑但没抓住，所以第二个"它"指的是球而非狗。因此我在看这句话时需要更关注球而不是狗，让我用不同颜色的笔标注一下以便更清晰——我必须把注意力重点放在"球"上而非"狗"，但现实情况并非如此。如果简单做点积运算，算法机制里并没有设置让"球"比"狗"获得更高权重的编码规则，当我们处理这句话时，"狗"和"球"不应该获得同等的注意力分数。这个例子精彩地论证了为什么需要对不同词元进行选择性关注——"狗追着球跑但没抓住"。

第一个词是“the”，所以如果这是查询标记，它会更多地关注“dog”；但现在这是查询标记，它应该更多地关注“ball”。我不希望两者具有相同的注意力分数，因此简单的点积无法区分这里微妙的上下文关系，它没有考虑“chase couldn't catch”的上下文或语言上的细微差别，比如“catch”更可能指的是移动的物体，也就是“ball”。所以主要问题是，简单的点积只能衡量语义相似性，但无法处理上下文问题，而且许多句子可能都有这样的上下文复杂性，对吧？我需要编码一种机制，以便能够捕捉这些复杂性，但我不知道这种机制会是什么。于是我们采用了研究人员长期以来使用的技巧。

如果你不明白事物之间的内在联系，你只需用一个神经网络或一堆可训练的权重矩阵来代替它，然后让反向传播算法去解决这个问题。注意力机制领域的情况正是如此，研究人员基本上无法确定那个机制可能是什么。这就是机器学习领域或深度学习领域与物理学不同的地方，对吧？

在物理学中，如果你遇到这个问题，可能会花上六个月到一年的时间去研究一个定律，以揭示其背后的机制来捕捉复杂性，或是建立一个能捕捉背景的机制。但在深度学习领域，你不需要这么做。你会说，我要用一堆矩阵来代替它，然后通过反向传播来训练这些矩阵。这就是研究人员所做的，对吧？

所以他们发明了新的矩阵，我们暂且称之为查询矩阵和键矩阵。这意味着，与其仅仅查看输入的嵌入表示，不如将每个输入嵌入与一个矩阵相乘。比如，如果这里的查询是“it”，我就会用所谓的查询矩阵与之相乘。这个矩阵可以是一个高维矩阵。对于“dog”来说，“dog”和“ball”就是键，因为键本质上就是除了查询之外的所有其他标记，也就是“dog”和“ball”。

所以你用键矩阵将它们相乘。现在看这里的优势在于，如果点积无法捕捉到你期望的上下文关系，你并不需要假设这些WQ和WK矩阵具有任何特定含义——你只需随机初始化它们，然后通过反向传播来训练它们。这正是研究人员长期使用的深度学习技巧：当你无法自行推导出关系时，就退后一步，让神经网络来完成它的工作。与其通过强加某些规则来限制神经网络，不如让它自己找出答案。

现在你看到了优势所在，我们掌握了多个可训练的因素。假设WQ是3×3的矩阵，WK也是3×3的矩阵，对吧？比如“狗”和“球”这些就是我的键值。而这些是它们的嵌入向量，也就是我们之前看到的输入嵌入。现在，我将这些输入嵌入与查询相乘，将这两个与键相乘。具体来说，就是3×3的矩阵乘以3×1的向量，结果会是一个3×1的向量。这样一来，键值就变成了0.9、0.2、0.1和0.1、1.8、0.1。你会发现这些值发生了变化，因为它们与矩阵相乘了。而查询部分也会与查询矩阵相乘。

因此，针对它的查询将变为0.5 0.1和0.5 1.0以及0.1。现在，如果你在向量空间中绘制这些，这是查询向量，这是“球”的键向量，这是“狗”的键向量。现在我们正在从输入嵌入空间转换到一个不同的空间，这个空间是通过与查询和键矩阵相乘后得到的。现在我将计算这些向量之间的注意力分数，而不是原始向量之间的。因此，如果你计算“它”和“球”之间的注意力分数，你会发现是0.56，“它”和“球”之间的分数是0.96。而如果你计算“它”和“狗”之间的注意力分数，结果是0.56。所以在这里，你可以看到“它”和“球”之间的注意力分数是0.96，比“它”和“狗”之间的分数要高，后者较低。

所以这些显然是不同的注意力分数，因此添加这些可训练的矩阵实际上对我们有帮助。为什么会有帮助呢？因为它提供了一些可调整的参数，使我们能够编码标记之间的一些复杂关系。如果你采用简单的点积，注意力分数将是相同的。但如果你使用查询键矩阵（我们还没有看到值矩阵，将在下一节课中介绍），本质上，如果你只有可训练的矩阵，那么你就可以得到不同的注意力分数，因为现在你突然有了更多可以调整的参数。

所以如果你在这一部分感到困惑，让我再重复一遍：我们开始这一部分时思考的是，如果你有一个输入嵌入向量，那么你可以对这个输入嵌入向量做什么来得到上下文向量？为了得到上下文向量，我们本质上需要阿尔法值。得到阿尔法值后，你只需要将它们与输入嵌入向量相乘，就能得到上下文向量。但接下来的问题是，你如何在一个输入嵌入向量和另一个输入嵌入向量之间得到阿尔法值？你如何得到注意力分数？最简单的方法可能是取点积，但我们看到，假设这是句子，如果它是我的查询...

如果我想计算“it”（它）与“ball”（球）以及“it”与“dog”（狗）之间的注意力分数，我会先计算“it”与“ball”的点积，结果为0.51；再计算“it”与“dog”的点积，结果同样是0.51。这样一来，两者的注意力分数相近，但这并非我想要的——因为当我说“it”时，我希望它指向“ball”，所以“it”与“ball”的注意力分数应当显著高于“it”与“dog”的分数。那么如何实现呢？显然，点积运算缺乏捕捉这种上下文关系的复杂度，我需要更多可调节的参数，但目前尚不清楚具体需要哪些控制机制。

但让神经网络或反向传播算法来找出这些参数的最佳值，至少让我先随机初始化它们。这时新术语就派上用场了——我需要可训练的矩阵，姑且称之为查询矩阵。通过将输入嵌入与查询矩阵相乘，我可以将其转换到另一个空间。而对于表示"狗"和"球"的关键词输入嵌入，它们将与键矩阵相乘并转换到另一个空间。然后我就能在这些转换后的向量之间计算注意力分数。如果模型能正确学习这些矩阵参数，就能学会计算"它"和"球"之间的注意力分数是0.96（高于"它"和"狗"之间的0.56）。不必担心这些乘法运算或数学细节。

现在我将详细讲解数学部分，这将在下一节课中进行。目前只需记住，我们不知道如何从物理上捕捉上下文关系，所以这就像是一种简便方法，一种技巧。你引入了查询（queries）和键（keys），这些都是随机初始化的可训练矩阵，然后对它们进行训练。你可能听说过“查询”和“键”这两个词，但实际上引入它们并没有确切的物理原因。唯一的原因是人类无法自行找到计算这些注意力分数的方法，我们唯一知道的就是这样。

如果我们无法理解它，就让我将输入嵌入投影到更高维度或不同维度，或者让我使用少量可训练参数，然后希望训练过程本身能自行解决这个问题。人类在计算机视觉领域也采用了类似的技巧：如果你训练一个卷积神经网络（CNN）来区分狗和猫，你无法自己写下所有特征，而是依赖CNN来完成。这里的情况也类似。在下一讲中，我们实际上将看到如何精确计算查询矩阵、键矩阵，以及还有一个称为值矩阵的矩阵，它们将如何用于我们将在下一讲中看到的下一个令牌预测任务。

所以下一讲将全面探讨自注意力机制的数学原理——我们将如何处理查询矩阵、键矩阵和值矩阵？如何精确计算上下文向量？从这些上下文向量出发，我们最终要通过哪些步骤来获得下一个开放预测？因此下节课本质上是对刚才所见内容的深度剖析，并将其扩展为一整节数学原理课程。

但在今天的讲座中，我只是想激发大家对查询、键和值这些概念的兴趣，这些我们尚未深入探讨的内容将在下一讲中详细展开。好了各位，今天的讲座就到这里，你可以将其视为注意力机制发展历程与自注意力机制入门的一次融合。作为下个开放式预测任务的总结，请记住注意力机制的演变过程：最初我们有Elisa，它在当时堪称革命性的突破，考虑到它诞生于1966年，至今仍令人惊叹。

接着出现了循环神经网络和长短期记忆网络（LSTM），它们存在上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了注意力机制，它计算解码输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力机制加RNN。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

所以他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并再次以注意力机制为核心构建了这一架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们讨论自注意力时，我们实际上只关注一个序列，因为它将用于下一个标记预测任务。

因此，在像GPT这样的下一个标记预测任务中，我们使用自注意力机制，即观察一个标记及其如何关注周围或邻近的标记。我们正在观察的标记被称为查询（query），其他标记被称为键（keys）。我们的目标是找到查询向量与键之间的注意力分数。我们认识到，注意力机制的主要目的是获取这些注意力分数，并将其转换为上下文向量。上下文向量是输入嵌入向量的一种更为丰富的表现形式。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但没接住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵。

因此我们需要增加参数，以便有更多可调节的"旋钮"来操作这些可训练的矩阵。这些矩阵被称为查询权重矩阵和键权重矩阵。此外还有值权重矩阵，我们将在下节课中介绍。所有输入嵌入都会与查询权重矩阵相乘，得到查询矩阵；同样地，我们也有键矩阵。这样，注意力分数就不再是在向量的输入嵌入之间计算，而是在查询和键之间计算。

他们遇到了上下文瓶颈问题，这意味着所有上下文都被压缩到一个隐藏状态中。为了解决这个问题，我们意识到需要选择性地关注输入序列的不同部分，这就是所谓的注意力机制。为了编码这一点，我们引入了一种称为注意力机制的机制，它计算解码器输出或解码器隐藏状态与输入隐藏状态之间的注意力分数。这篇论文就是2014年发表的Badanov注意力机制。那篇论文本质上仍然使用了RNN，所以那是注意力加RNN的结合。到了2017年，有一篇论文中研究人员意识到我们甚至不需要RNN。

于是他们放弃了RNN，提出了一种名为Transformer架构的新结构，其核心就是注意力机制。2018年，研究人员对Transformer架构进行了修改，去掉了编码器，保留了解码器，并构建了这种以注意力机制为核心的新架构。在此之前，注意力机制是从一个序列到另一个序列的。而当我们谈论自注意力时，实际上我们只关注一个序列，因为它将被用于下一个令牌预测任务。因此，在像GPT这样的下一个令牌预测任务中...

我们使用自注意力机制来观察一个标记（token）及其与周围或邻近标记的关联方式。这里，被观察的标记称为查询（query），其他标记称为键（keys）。我们的目标是计算查询向量与键之间的注意力分数。我们认识到，注意力机制的核心目的就是获取这些注意力分数，并将其转化为上下文向量。上下文向量是对输入嵌入向量的一种更丰富、更具信息量的表示。

因为它还包含了关于一个标记如何与其相邻标记相关联的信息，以获取这些注意力分数。最直观或最简单的方法就是计算向量之间的点积。但我们意识到这并不是最佳方法，因为单纯的点积无法捕捉微妙的上下文关系。就像在这个例子中看到的：“狗追球，但第一次没抓住。”第一个“它”指的是狗，第二个“它”指的是球。为了捕捉这种复杂的上下文关系，我们需要添加可训练的权重矩阵，因此需要增加参数数量。

这样我们就有不同的参数可以调整，这些可训练的矩阵被称为查询权重矩阵和键权重矩阵。还有一个值权重矩阵，我们将在下一节课中看到。所有输入嵌入的输入嵌入都与查询权重矩阵相乘，得到查询矩阵，同样我们也有键矩阵。因此，注意力分数不是在向量的输入嵌入之间找到的，而是在查询和键之间找到的。

由于我们拥有众多可灵活调整的参数，我们希望在进行参数训练时，模型能够学会让第二个"it"与"ball"之间的注意力分数高于第二个"it"与"dog"之间的注意力分数。这样一来，模型就能捕捉到更多上下文复杂性。通过添加这些可训练的权重矩阵，模型能够更好地把握上下文中的复杂关系。

这就是为什么我们人类添加了这些权重矩阵，然后称它们为查询、键和值，因为这听起来很酷，而且与信息领域相关。但如果你深入观察，我们自己无法找出这种注意力机制的规则，点积方法失效了。所以我们无法自己弄清楚如何得到这些注意力分数，如何计算它们。于是我们转向神经网络来替我们完成这项工作。

好的。非常感谢大家。在下一讲中，我们将深入探讨自注意力机制背后的数学原理。在接下来的讲座中，我们将探讨多头注意力机制，只有到那时——也就是介绍完多头注意力节点之后——我们才能真正理解键值缓存的概念。让我看看这部分内容在哪里...对，只有到那时我们才能真正准备好理解键值缓存，这个概念正好引出多头潜在注意力机制（MLA）的讲解。

这个系列会有点深度，但我尽量把课程讲得详细些，以免遗漏任何内容。这是为认真的学习者准备的。所以观看这个系列时请做好笔记，这对你会非常有帮助。非常感谢大家。我期待在下一堂课上见到你们。大家好。

我叫Raj Dhandekar博士。2022年获得麻省理工学院机器学习博士学位，是《从零构建深度探索》系列的创作者。在开始之前，我想向大家介绍本系列的赞助商兼合作伙伴——NVIDIA人工智能。众所周知，我们非常重视基础内容，从最基本的构件开始构建人工智能模型。NVIDIA AI 的理念和原则与我们非常相似。让我来展示一下。

这就是NVIDIA AI的官方网站。仅凭一支小型工程师团队，他们就打造出了这款惊艳的产品——只需输入文字指令，就能生成高质量的AI视频。正如各位所见，我刚才输入了一段文字指令："制作一段超写实的高端奢华腕表广告视频，并赋予其电影质感"。

就这样，我点击了生成视频。没过多久，眼前就呈现出了这段令人惊叹的视频，画面极其逼真。最让我着迷的是它对细节的极致把控。看看这个，质量和质感简直令人难以置信。而这一切都仅仅通过一个文本提示就创造出来了。这就是NVIDIA产品的强大之处。

你刚刚看到的精彩视频背后，是NVIDIA AI的视频创作流程在支撑。他们正从基本原理出发，重新思考视频生成与剪辑的方式。为了对基础模型进行实验和优化，他们在印度拥有规模最大的H100和H200计算集群之一，同时也在测试B200芯片。NVIDIA AI是印度发展最快的人工智能初创公司，致力于为全球打造创新产品——这正是我如此认同他们的原因。

好消息是他们目前有多个职位空缺。你可以加入他们出色的团队。我会在下面的描述中发布更多详细信息。大家好，欢迎来到"从零构建Deep Seek"系列讲座。今天我们要探讨一个非常重要的主题——理解带可训练权重的自注意力机制。首先，让我们快速回顾一下课程规划和我们目前的学习进度。

因此，本系列讲座的主要目的是解释Deep Seek是如何构建的，以及推动Deep Seek的创新之处。我们将这些创新分为四个阶段：第一阶段涉及架构设计，第二阶段涉及训练方法，第三阶段涉及Deep Seek所实施的GPU优化技巧，第四阶段则涉及其模型生态系统本身。

在过去的两次讲座中，我们一直在研究第一阶段，并逐步构建对多头潜在注意力机制的理解。因此，我们理解多头潜在注意力的计划是按顺序进行的。我们研究了大型语言模型（LLMs），在上一次讲座中，我们探讨了为什么需要引入注意力机制，而在今天的讲座中，我们的主要目标是了解注意力权重和注意力分数是如何计算的。

那么，让我快速带大家回顾一下上一讲的内容。在上一讲中，我们了解到自注意力机制的主要目的是接收输入嵌入向量，并将其转换为所谓的上下文向量。请记住，上下文向量比输入嵌入向量包含的信息要丰富得多。

输入嵌入向量编码了单词的含义及其在序列中的位置信息，但不包含该单词与序列中其他单词的关系信息。而上下文向量则包含了这些关系信息。上下文向量不仅包含单词的含义和位置信息，还包含了它与序列中其他标记之间的关系信息。

因此，要从输入嵌入向量中获取上下文向量，我们直观想到的第一种方法是计算点积。但这种方法效果并不理想，因为点积本身存在局限性。例如，在"狗追逐球但没接住它"这个句子中，如果"它"是我的查询对象，我需要注意力机制能够区分出"它"实际指向的是"球"而非"狗"。若仅使用简单的点积运算，就无法捕捉这种指代关系。那么我们该怎么办？答案是引入可训练的权重矩阵。

这些可训练的权重矩阵的作用在于：既然我们无法在注意力机制中直接编码复杂的关联性，何不将其交给可以训练的权重矩阵来处理？这正是我们所采取的做法——并非直接使用输入嵌入向量，而是先将输入嵌入向量投影到不同的空间、不同的向量空间中去。例如，当处理某个查询时，我们会将其与查询权重矩阵相乘，从而得到查询向量。

例如，在我们刚刚看到的这个例子中，狗追着球跑，但没能抓住它。如果第二个“it”是一个查询，我们不会直接使用它的输入嵌入向量，而是将其与可训练的查询权重矩阵相乘，使其成为一个查询向量。而我们想看看这个“it”与其他词之间的关系，或者我们需要对其他标记给予多少关注的其他词，它们被称为键。同样，我们也不会直接将这些向量作为输入嵌入向量使用。

我们将它们与所谓的键权重矩阵相乘，然后得到“狗”和“球”的键向量。接下来，我们不再像之前那样使用输入嵌入（即“狗”的输入嵌入和“球”的输入嵌入），而是将其投影为查询向量。因此，现在我们有一个查询向量（看起来像这样），一个“球”的键向量和一个“狗”的键向量。然后，我们计算查询向量和键向量之间的点积，从而确定对于特定查询需要给予每个键多少注意力。

因此，对于这个查询“it”，我们应该对“ball”给予多少关注，你只需要计算查询向量和“ball”的关键向量之间的点积。如果查询是“it”，我们需要对“dog”这个词给予多少关注，我们只需要计算“it”的查询向量和“dog”的关键向量之间的点积，对吧。所以在上节课中，我只是想向大家介绍，我们不仅要进行简单的点积运算，还需要使用可训练的权重矩阵。今天，我们将进一步学习这三个可训练的权重矩阵，即查询、键和值。

如果我直接向你介绍这些查询、键和值，你可能会对我们为什么一开始就需要这些可训练的权重矩阵感到困惑，这看起来有点奇怪。但请记住，作为人类，我们有一个局限，那就是我们无法凭空想出注意力机制的公式，这不像物理学那样有现成的公式。因此，我们将这个难题交给了神经网络或可训练的权重矩阵。我们发现，当你通过权重矩阵将输入嵌入投影到更高或不同的维度时，我们能够得到不错的答案。所以，我们只是把这些矩阵称为查询、键和值，因为在文献中这样称呼更常见，但实际上我们在这里耍了个“偷懒”的小把戏。

如果我直接向你介绍这些查询（query）、键（key）和值（value），你可能会困惑为什么我们一开始就需要这些可训练的权重矩阵。这看起来有点奇怪，但请记住，作为人类，我们有局限性，无法凭空想出注意力机制的公式——它不像物理学那样存在现成的公式。因此，我们将这个难题交给神经网络或可训练的权重矩阵来处理。我们发现，当通过权重矩阵将输入嵌入投影到更高或不同的维度时，就能得到不错的答案。所以我们把这些矩阵称为查询、键和值，只是因为这在文献中更常见且便于理解，但实际上我们在这里耍了个"偷懒"的小花招。

既然我们自己无法理解其中的物理原理，我们就想，让我用一些权重矩阵吧，一开始我会随机初始化它们，然后希望训练完成后，我能学到一些关于不同标记需要注意的地方，而这正是实际发生的情况。所以今天，我们实际上将一步步地了解如何从输入嵌入向量到上下文向量的过程。也就是说，我们将一步步地了解，如果你有一些输入嵌入向量，我们究竟如何获取这些输入嵌入向量，以及如何将这些输入嵌入向量转换为上下文向量。

那么让我们现在就开始我们的旅程吧。今天我的目的是向大家展示一些可视化内容，并穿插一些代码片段，这样你们就能完全自己想象出整个计算过程了，对吧？那么我们就开始吧。我要看看这个序列，明天是光明的，好的。记住，在进入Transformer架构或注意力机制之前，这些都只是输入嵌入，也就是词元嵌入加上位置嵌入。还记得我们之前的讲座吗？我们看到，在进入Transformer架构之前，每个词元都会得到一个统一的表示，这个统一表示就是词元嵌入加上位置嵌入的总和，也就是所谓的输入嵌入。

所以这个输入嵌入实际上就是我在这里展示的内容，对吧？对于"next"，输入嵌入是一个八维向量；对于"day"，输入嵌入是一个八维向量；对于"is"，输入嵌入是一个八维向量；对于"bright"，输入嵌入也是一个八维向量。现在我要向你们展示如何将这些输入嵌入向量转换为上下文向量。但我希望你们能理解为什么我们需要将它们转换为上下文向量，因为就目前而言，如果我查看这些嵌入向量中的任何一个，比如"day"，它并不包含关于需要给予"next"、"the"、"is"或"bright"多少重要性的信息，这些信息完全丢失了。我想要整合这些信息，这就是为什么我要将输入嵌入向量转换为上下文向量。

因此，我们将介绍三个可训练的权重矩阵。第一个可训练的权重矩阵称为查询权重矩阵，用WQ表示，这就是查询权重矩阵。第二个可训练的权重矩阵是键权重矩阵，用WK表示。第三个可训练的权重矩阵是值权重矩阵，用WV表示。现在，你应该不会对这些矩阵的来源感到惊讶了。记住，这些矩阵的出现是因为我们希望将输入嵌入转换到不同的空间，从而增加表达能力，捕捉那些无法通过简单点积实现的潜在复杂性。

记住所有这些可训练的权重矩阵，我刚才提到的这些值在开始时是未知的，它们是随机初始化的。我们的期望是，当我们通过整个LLM架构进行反向传播时，这些值会自行更新。因此，每当我展示这些矩阵时，我并不会在一开始就固定它们，而是随机初始化它们。现在让我们仔细关注一下维度。输入部分，这个矩阵被称为输入嵌入矩阵，这就是所谓的输入嵌入矩阵。看看这个矩阵的维度，它实际上有五行。为什么是五行呢？因为我有五个标记（tokens）：“the next day is bright”，所以它有五行。它有八列，为什么是八列呢？因为我在这里选择每个词的输入嵌入维度为八。你可以选择任何维度。

但为了简化起见，我暂且将其设定为8。这就是输入嵌入矩阵的维度。现在让我们看看查询、键和值权重矩阵的维度。仔细观察你会发现，这些矩阵的行数必须与输入嵌入矩阵的列数相同。因此，WQ、WK和WV的行数——如果你把它们看作行和列的话——其维度中的行数必须等于输入嵌入维度，这是固定的，因为我们要进行矩阵乘法运算。在本例中这个值就是8，但它们的列数理论上可以是任意值。

好的，现在当我们谈到GPT-2、GPT-3等模型时，它们的架构中这个列数（这里也称为输出维度）——我们称之为输出维度，而在这里我们也称其为输入维度或D_in。输出维度可以与输入维度不同，也可以相同。对于GPT-2、GPT-3等模型来说，它们通常是相同的。因此，如果相同的话，D_out也会等于8。但为了简单起见，我这里选择了D_out等于4。不过要记住，这个值通常与输入维度D_in相似。但在这里，我想展示的是，它实际上可以是任意值，因为乘积运算仍然可以进行。

即使列值不同，我的查询权重矩阵、键权重矩阵和值权重矩阵都是8×4维的矩阵。第一步，我会将输入嵌入矩阵与查询矩阵相乘，从而得到查询向量。来看一下维度：5×8的矩阵乘以8×4的矩阵，结果是5×4的矩阵。这就是我的查询向量，一个5×4的矩阵。同样，键向量也是一个5×4的矩阵，值向量同样如此。可以这样理解：这里的每一行都对应一个标记。

所以第一行是第一个标记，第二行是第二个标记，接下来第三行是“day”，第四行是“is”，第五行是“bright”。但现在要记住，我们从8维降到了4维的输出空间，因此我们操作的空间现在已经不同了。从这一点开始，我们不再关注输入的嵌入向量，我们只关注查询向量、键向量和值向量。这就是我在讲座一开始试图启发你们的——如果我们直接处理输入嵌入向量并取点积，它的局限性很大，效果不佳。这就是为什么我们要将它们投影到不同的空间中。这一技巧在深度学习的许多不同领域都有应用。

如果线性分类器在数据上不起作用，你可以通过增加特征来提升它，将其投影到更高维的空间。如果手工设计的计算机视觉特征不起作用，你可以使用卷积神经网络，它能在更高维的空间中自行发现特征。因此，将输入维度的内容转换到另一个维度，是我们长期以来在深度学习中一直在做的事情，而这正是我们在这里所做的。只是这里的名称稍微花哨一些——查询、键和值。在本讲座的最后，我会告诉你为什么会出现这些名称，但现在你只需要记住查询向量是一个5×4的矩阵。

键矩阵的维度是5×4，值矩阵的维度也是5×4。其解读方式是每一行对应一个词元。查询键的未来是光明的，值矩阵也是如此，而列数等于输出维度（在本例中为4）。希望大家能跟上我的思路，到现在为止我们已经有了查询向量、键向量和值向量。这里我采用了一些具体数值：输入是5×8的词嵌入矩阵，我将其与8×4的查询矩阵、键矩阵和值矩阵相乘，最终得到5×4的查询矩阵、5×4的键向量矩阵以及5×4的值向量矩阵。

左边是没有任何数值的可视化展示，右边则是已经填入具体数值的版本，这样你们就能同时看到直观的图形表达和数学表达并列呈现。刚才第一步我们得到了查询向量(Q)、键向量(K)和值向量(V)，现在我们要在QKV空间进行操作，而不是在输入嵌入空间（注：QKV即查询Query/键Key/值Value的缩写）。接下来进入第二步——我们需要计算注意力分数，这本质上就是进行点积运算。

好的，现在我有我的查询向量，它是一个5×4的矩阵，还有键矩阵，也是5×4的。如果要计算查询向量和键向量的点积，我不能直接相乘这两个矩阵，因为第一个矩阵的列数和第二个矩阵的行数不匹配。所以我需要对键矩阵进行转置。这样查询向量保持5×4不变，而键矩阵转置后变成4×5。这样操作后，用一个5×4的矩阵乘以一个4×5的矩阵，就能得到一个5×5的注意力分数矩阵。

这是一个非常重要的步骤，因为在这里我们实际上可以找出一个查询与其他标记之间的关联程度。现在，假设我说“明天很晴朗”，然后让我聚焦在第二行的“明天”这个词上。假设我想找出“明天”与其他标记之间的注意力分数。也就是说，我想知道“明天”与“明天很晴朗”之间的关联程度。具体来说，我想找出“明天”与“明天”之间的注意力分数是多少，“明天”与“很”之间的注意力分数是多少，“明天”与“晴朗”之间的注意力分数是多少。

因此，为了计算“next”和“the”之间的注意力分数，我们会取“next”对应的查询向量（即这一行），并将其与“the”的键转置（即这里的第1列）相乘。这里的第1列对应“the”，所以当我们将“next”的行与“the”的列相乘时，就得到了“next”和“the”之间的注意力分数。我将其称为α2,1，也就是这里的这个值。现在来看这里的第二行，它对应“next”的注意力分数，这里的第一个值就是α2,1。

现在，这是下一个行向量与当前列向量的点积。如果我想找到“next”与“next”之间的注意力分数，我会再次取这个行向量与第二列（即“next”）的点积，这个点积将给出alpha 2 2，也就是这里的第二个值。所以这就是alpha 2 2。如果我想找到“next”与“day”之间的注意力分数，我会再次取这个行向量（保持不变）与这里的第三列（对应“day”）的点积，这将给出这里的第三个值，即alpha 2 3。如果我想找到“next”与“is”之间的注意力分数，那么这里就是alpha 2 3，“next”与“is”将是alpha 2 4。所以这就是“next”行向量与这里第四列的点积，这将给出alpha 2 4，也就是这里的值。

那么，如果我想计算"next"和"bright"之间的注意力分数，本质上就是α₂₅，也就是第二行"next"与最后一列"bright"的点积。这个点积运算最终会给出α₂₅的值。现在，这整个第二行就代表了"next"与所有其他键（即"the"、"next"、"day"、"is"、"bright"）之间的注意力分数。同理，第五行表示的是"bright"与所有其他标记之间的注意力关系。通过这种方式，我们可以解读这个注意力分数矩阵——矩阵的每一行都代表该行对应的查询词（即该行的词）与所有其他键之间的注意力关系。

例如，如果你看第四行，它表示第四个查询词“is”与所有其他标记“the next day is bright”之间的注意力分数。要得到这一行，你需要固定查询行，然后将其与键转置矩阵中的所有列相乘，这样就能得到该行中的每一个值。所以，每当你想到注意力分数矩阵时，试着想象如何解释矩阵的每一行，这时你才能真正理解，或者永远不会忘记注意力分数矩阵是如何计算的。如果查询矩阵是5×4，键转置矩阵是4×5的话。

所以这里我再次展示数学计算过程：当你将查询向量与键向量的转置相乘时，会得到注意力分数矩阵，其形式大致如下。现在观察第二行数据——这行本质上表示当"next"作为查询词时：0.1对应"next"与"the"的注意力关联度，1.8是"next"与"next"自身的注意力值，0.6代表"next"与"day"的注意力强度，0.1是"next"与"is"的关联度，而0.1则是"next"与"right"的注意力权重。这就是解读注意力分数矩阵的正确方式。

不过这个注意力分数矩阵存在一个问题：比如当处理"next"这个词时，我们希望能明确表述——当"next"作为查询词时，应当给予"the"50%的注意力权重，给"next"自身20%，给"day"分配10%，给"is"10%，而"bright"则完全不分配注意力权重等等。

所以我想做出可解释的陈述，让我重申一下我的意思。当我看到下一个词时，我想做出这样的陈述：比如给"let's say"分配10%的注意力，给"next"分配20%，给"day"分配20%，给"is"分配30%，给"bright"分配20%。这样当我看到这些数值时，就能立即看出其中"next"和"is"获得了最多的注意力。本质上，我希望所有这些数值加起来等于100的概率，也就是总和为1。这样我就能直接看这个饼图——"the next day is bright"——根据这个饼图，我可以看到每个token需要分配多少注意力。而如果你现在看这些数值...让我们看看...是的，如果你看这些数值...

现在来看第二行，也就是下一个的注意力分数。你会发现这些值加起来并不等于一，这意味着我不能说给第一个词元分配10%的注意力，给第二个词元分配18%的注意力，这样是行不通的。这里的行中的数值加起来不等于一，这就是主要问题所在。因此，下一步，也就是第三步，是确保将注意力分数转换为所谓的注意力权重。为了从注意力分数得到注意力权重，我们将应用softmax操作。softmax本质上意味着，比如说我们看一下这一行，让我以列的形式写下来。

现在，以列格式表示，这些数值将变为0.1、1.8、0.6、0.1和0.1。我想要做的是以某种方式转换所有这些值，使它们介于0到1之间，并且总和为1。这就是softmax操作的作用。具体来说，如果这些值是x1、x2、x3、x4和x5，softmax操作会将x1替换为e的x1次方除以总和，x2替换为e的x2次方除以总和，x3替换为e的x3次方除以总和，x4替换为e的x4次方除以总和，x5替换为e的x5次方除以总和。那么，总和到底是什么呢？总和就是e的x1次方加上e的x2次方加上e的x3次方加上e的x4次方加上e的x5次方。

现在来看看这五个值，你会发现如果将它们相加，分子就是e的x1次方加e的x2次方加e的x3次方加e的x4次方加e的x5次方，这等于分母的总和。因此，所有这些值加起来肯定会等于1，并且它们会介于0到1之间。Softmax还有一个额外的重要特性，就是它对非常大的值赋予很高的权重，而对非常小的值赋予很低的权重。这使得分类变得非常容易。

所以本质上这就是我们要实现的softmax操作。但这里的主要问题在于，正如我刚才所说，softmax对数值较高的值会给予极高的关注度，而对数值较低的值则关注不足，这对我们来说是个大问题。举个例子，如果注意力分数是这样的话。

现在你看到这个值非常高对吧。如果我们直接对它应用softmax，softmax的作用会让这个值变成0.95左右，同时确保其他所有值都非常低。如果这些是我们的注意力权重，那对我们来说就不太理想了，因为这样我们会过度关注某一个键，而完全忽略其他所有键。所以这就是为什么在应用softmax之前需要进行缩放处理。

在我们应用softmax之前，需要确保所有这些值都除以某个数，然后才能应用softmax。现在让我来谈谈这一部分。我们来看这部分，首先我想解释一下softmax的问题。假设这些是我的注意力分数值，然后我对这些值应用softmax函数。可以看到，所有这些值的softmax结果在范围上几乎都差不多，这对我们来说是好的。但现在我想做的是，把这些值都乘以8，然后再应用softmax。如果所有这些值都乘以8，意味着有些值会变得非常大，而有些值则不会那么高。

好的，那么如果我将所有这些值乘以8，然后应用softmax函数，你会发现softmax会非常重视这个0.8的值，而对其他一些值几乎不予理会。这就是在应用softmax之前数值非常大的情况下会发生的事情，正如我在这里提到的，softmax函数对其输入的大小非常敏感。当输入值非常大时，每个输入的指数值之间的差异会变得更加明显，这导致softmax的输出变得"尖峰化"。所谓"尖峰化"输出，意味着它对某些值赋予很高的权重，而对其他值赋予非常低的权重。

因此在注意力机制中，这并不理想，因为如果softmax分布过于尖锐，模型就会对某一个特定的键变得非常自信。现在你可以看到，模型对这个键变得非常自信，而对其他键的置信度会非常低，这会导致训练过程非常不稳定。稍后我们研究transformer架构时会看到这一点，而我们不希望这种情况发生。这就是为什么我们需要进行缩放，这就是为什么在应用softmax之前需要对这个向量进行缩放。实际用来缩放这个向量的值是键的维度的平方根。

好的，这里的键维度在这个例子中是5×4对吧？所以输出维度等于5时，缩放因子是平方根；当输出维度等于4时，抱歉，缩放因子就变成了4的平方根。这时候你可能会想：为什么非要按键维度的平方根来缩放呢？为什么不直接用键维度本身？或者用维度的平方？为什么偏偏选中平方根？平方根到底有什么特别之处？这背后的核心原理其实与方差的概念有关。

假设这是我的查询向量，一个六维向量，而这是我的键转置向量，也是一个六维向量。如果我相乘这两个向量，会得到一些值，对吧？但记住，在相乘的过程中，每个值都会先相乘再求和。通常的情况是，如果有两个随机向量，维度都是6，如果我采样一百个这样的随机向量——也就是说，我取一百个查询向量和一百个键转置向量，然后计算所有这些查询向量与键转置向量相乘的结果——那么这些乘积的方差会……

所以现在我可以取这100个值并计算它们的方差，对吧？这个分布的方差实际上会随着键的维度而变化。正如这里提到的，这个乘积的方差会随着键维度的平方根变化，实际上是与键维度的平方根成比例。这意味着，随着键维度的不断增加，或者说随着键转置的维度增加，这个乘积的方差会大幅增加。也就是说，由于查询和键转置是随机初始化的，这些都是随机向量，它们的乘积通常会从一些非常高的值变化到一些非常低的值。

我们想尽可能避免这种情况，我们要确保这个产品方差等于一，这样产品就不会剧烈波动。查询和键在开始时是随机定义的，维度可能非常高。所以如果维度非常高，而方差实际上与这个维度的平方根成比例，那对我们来说就不太好，因为这也会使学习不稳定。为了进一步说明这个概念，我想用骰子的例子来解释。

那么，如果你掷一个骰子，假设它只有1到6的数字，平均值大约是3.5，方差相对较小，比如2.9，结果是可预测的。但现在我掷骰子，然后对掷100次的结果求和。也就是说，如果你掷骰子100次并求和，这里的均值会在350左右，但方差会显著增加到约290，结果就变得不可预测了。而没有归一化的点积也是如此。

因此，如果我们仅将查询向量与转置后的键向量相乘，且不除以其维度大小的平方根，我们会发现：增加维度数量就像掷更多骰子并累加结果。原因在于点积运算本质上是对乘积项的求和——当维度为100时，我们就要累加查询向量与转置键向量对应100个元素的乘积。所以维度增加就如同增加骰子数量并合计点数，每个维度都会引入方差，随着维度增长，这些方差会不断累积。

所以我在这里想说的是，随着查询和键的维度增加，方差会大幅上升，导致softmax之前的点积结果要么变得非常大，要么变得非常小。由于方差过大，注意力权重会变得不稳定，进而导致训练过程也不稳定。当我们除以D的平方根时，这个操作会降低点积的方差，使其等于1，从而稳定了预期结果。这样，注意力权重变得更加稳定和可预测。实际上，我已经用下面的代码解释了这一点。接下来我们要做的是进行一千次试验，在每次试验中我们会生成...

首先，我们将进行一个五维的查询和关键向量操作，然后进行一个百维的查询和关键向量操作。假设我提到的维度等于5，我们将生成维度为5的查询和关键向量，并将这个过程重复一千次。对于每一次试验，我们将计算查询和关键向量之间的点积。在一种情况下，我们会将点积结果除以维度的平方根进行缩放，而在另一种情况下，我们则直接取点积结果而不进行缩放。然后，我们将收集这一千次试验的所有结果，并计算缩放前的方差和缩放后的方差。

所以记住，我们取一千个查询向量和一千个键向量，计算它们的点积。在一种情况下我们进行了缩放，另一种情况下没有，然后我们比较缩放前后的方差。如果你运行这段代码，会发现对于维度为5的情况，缩放前的方差等于5；对于维度为100的情况，缩放前的方差实际上是100。这说明方差实际上随着维度的增加而直接增长——方差与维度成正比。但缩放后的美妙之处在于：当你用维度的平方根进行缩放后，方差几乎会趋近于1，这非常棒。

好的，现在这里已经明确证明：如果你将查询向量与键向量的点积除以键向量维度的平方根，最终得到的结果是——经过缩放后，无论维度是5还是100，点积的方差都会被限制在一定范围内。这意味着查询向量乘以键向量转置后，其值不会暴涨到极高或暴跌到极低，方差将保持为1，从而确保训练过程非常稳定。因此，我们在第三步所做的就是将注意力分数除以键向量维度的平方根进行缩放。也就是说，我们取注意力分数，然后除以键向量维度的平方根。这里的键向量维度本质上是指每个键向量的维度大小，在本例中等于4。

好的，所以我们先除以4的平方根，然后应用softmax函数。在这个除法完成之后——具体来说是在除以键的维度的平方根之后——我们就会应用softmax。本质上，我们应用这个函数后，注意力分数就被转换成了所谓的注意力权重。这就是注意力分数和注意力权重之间的关键区别：注意力分数是没有归一化的，而注意力权重是归一化后的结果。如果你看这里的每一行，你会发现每一行的总和基本上都等于1。现在，我们就可以做出定量或定性的陈述了，就像我刚才提到的——第二行对应的是下一个...

所以现在我可以这样说：我们可以在"next"和"though"之间分配10%的注意力，在"next"和"next"之间分配50%的注意力，在"next"和"day"之间分配20%的注意力，在"next"和"is"之间分配20%的注意力，在"next"和"studies"之间分配10%的注意力，在"next"和"bright"之间分配10%的注意力。这就是我现在能够做出定性陈述的方式，因为每一行的所有元素加起来本质上等于一，这就是注意力权重和注意力分数之间的区别。注意力权重是经过归一化的，它们加起来等于一，而注意力分数则没有经过归一化。

现在，我们将进入第四步，在这一步中，我们实际上是从注意力权重计算上下文向量。让我们看看这是如何完成的。到目前为止，我们已经有了注意力权重，这又是一个5x5的矩阵。当你再次看到这个矩阵时，试着想象每一行代表什么。例如，这里的第二行表示，如果“next”是查询词，我应该给予“the next day is bright”多少注意力。所有这些值现在加起来等于1，因为它们在最后一步进行了归一化处理。我们接下来要做的是，记住到目前为止我们还没有使用值向量。在最后一步中，这些值向量开始发挥作用，我们所做的就是将5x5的注意力权重矩阵简单地与值向量相乘。

因此，值向量是5行4列的，对吧？这给了我们上下文向量。上下文向量有5行和4列。第一行对应"the"，第二行对应"next"，第三行对应"day"，第四行对应"is"，第五行对应"bright"。现在你可以看到，最初我们是从"the next day is bright"的输入嵌入向量开始的，经过所有这些步骤——第一步、第二步、第三步和第四步——我们现在得到了每个标记的上下文向量。请记住，这些上下文向量比输入嵌入向量丰富得多，主要是因为它们的计算涉及注意力权重。

因此，我刚才通过数学计算展示了注意力权重——它是一个5×5的矩阵（如这里所示），而数值矩阵是5×4的，因此上下文向量矩阵也是5×4的。例如，第二行对应着"next"的上下文向量，而这里的最后一行则对应着"bright"的上下文向量。每个上下文向量都是一个四维向量。在此，我想用一小节来阐释上下文向量计算背后的直观逻辑——其实际计算方式如下：

假设你想为“next”这个词找到上下文向量，我们已经计算出了“next”与所有其他词之间的注意力权重。我们发现，当处理“next”这个词时，我们应该给予“we”10%的注意力，给予“next”50%的注意力，给予“be”20%的注意力，给予“is”10%的注意力，给予“bright”10%的注意力。我们是通过这些注意力权重值（抱歉，是这些注意力权重）来理解这一点的。此外，我们还有值向量，你可以将值向量本质上视为输入嵌入向量本身。

但它被转换到了一个更高维度或不同维度的空间。因此，现在如果我们想找到下一个词的上下文向量，我们本质上做的是：我们知道“next”对“the”的关注度是10%（即0.1），这是“the”对应的向量值。我们会将这个向量按10%缩放，或者说乘以0.1。然后我们知道“next”对“next”本身的关注度是50%（即0.5）——我已在此标注了注意力权重：0.1（the）、0.5（next）、0.2（day）、0.1（...）、0.1（...）。具体来说，我们给第一行（the）赋予0.1的重要性权重，给第二行（next）赋予0.5的重要性权重，给第三行（day）赋予20%（0.2）的重要性权重。

所以我们用0.2乘以第三行，用0.1乘以第四行，用0.1乘以第五行，然后将所有这些乘积相加。接着我们用0.1缩放第一个向量，用0.5缩放第二个向量，用0.2缩放第三个向量，用0.1缩放第四个向量，用0.1缩放第五个向量，最后将所有缩放后的向量相加。这就是我们最终得到上下文向量的方式——本质上经过所有这些加法运算后，我们就得到了下一个词的上下文向量。若用视觉化表示，它看起来是这样的：如果你看到蓝色向量，它们就是输入嵌入向量，用于生成上下文向量。"明天会更好"

我们根据每个向量需要被关注的程度对其进行缩放。例如，"right"的重要性是10%，所以我们用0.1对其进行缩放；接下来是"50%重要性的"，我们用0.5进行缩放；"day"的重要性是20%，我们用0.2进行缩放；"is"的重要性是10%，我们用0.1进行缩放；"bright"的重要性也是10%，我们用0.1进行缩放。所有这些缩放后的向量都用绿色表示，然后将所有这些绿色向量相加，最终得到下一个的上下文向量。

因此，在这张图中，你可以看到上下文向量与输入嵌入向量的区别。输入嵌入向量仅包含该标记的含义信息，而上下文向量则包含了所有这些注意力权重信息，这些加权求和最终形成了上下文向量。这就是注意力分数如何相加得到上下文向量本身，以及上下文向量与输入嵌入向量的不同之处。

现在，通过同样的可视化方式，你也可以尝试理解如何获取其他所有标记的上下文向量。比如，要获取"bright"的上下文向量，你只需查看这些注意力分数：将第一行乘以0.1，第二行乘以0.05，第三行乘以0.1，第四行乘以0.25，第五行乘以0.5，然后将所有这些结果相加，这样就能得到"bright"的上下文向量——本质上就是这里的最后一行。这就是你如何从注意力权重矩阵中获取上下文向量的方法。

以及价值矩阵，要理解上下文向量计算背后的可视化过程，请看这张图。在这张图中，你可以看到注意力权重是如何被用作缩放因子的，然后通过加权求和，基本上将我们从输入嵌入空间转换到上下文向量空间。这就是第四步，本质上是获取上下文向量矩阵。然后在第五步中，我将简单地总结一下：我们有输入嵌入矩阵，我们有输入嵌入矩阵，然后我们有自注意力层。

所以当你现在看到这个自注意力层时，它意味着所有这些步骤已经在这里实现：第一步是与WQ、WK和WV相乘，得到查询向量、键向量和值向量；第二步是将查询与键的转置相乘，得到注意力分数；第三步是用键维度的平方根缩放注意力分数，并应用softmax得到注意力权重；第四步是将注意力权重与值矩阵相乘，得到上下文向量矩阵。就是这样，所有这四个步骤本质上都包含在自注意力模块中，最终将输入嵌入矩阵转换为上下文向量矩阵。

就是这样，这就是注意力机制或自注意力机制中发生的整个过程。这一过程为Transformer模块提供了动力，而Transformer模块正是语言模型表现如此出色的核心原因。现在，如果你回顾一下我们的课程，我们曾看过Transformer模块的不同组成部分，这个多头注意力机制就是所有魔法发生的地方。我们将输入嵌入向量作为输入，上下文向量作为输出，现在你知道了上下文向量是如何从输入嵌入向量计算出来的。为什么这被称为多头注意力？我们将在下一讲中看到。

但目前我希望你已经理解了自注意力机制背后的原理，我刚刚在这里写了一个小代码来向你演示这一点。我们称之为自注意力类。首先，我们初始化查询权重矩阵、键权重矩阵和值权重矩阵，最初是随机初始化的，并且我们将偏置设为false，因为我们只需要将输入嵌入矩阵与这些矩阵相乘即可。在前向传播过程中，我们通过将输入嵌入矩阵与可训练的键矩阵、可训练的查询矩阵和可训练的值矩阵相乘，得到键向量、查询向量和值向量。这基本上是我们看的第一步。然后我们进入第二步，通过将查询与键的转置相乘得到注意力分数。接着我们进入第三步，在第三步中，我们首先除以键维度的平方根。

然后我们应用softmax函数，最终来到第四步，即将注意力权重与值矩阵相乘，得到上下文向量矩阵。就是这样。其实我本可以直接向你解释这段代码，因为它只有10到11行代码，但为了理解这些乘法运算，我们有必要在白板上写下来，以便可视化维度。这就是上下文向量权重矩阵的计算方法。你可以运行这段代码，我也会分享这段代码，然后你可以输入任何内容。这里我输入的是一句话“你的旅程始于一步”，输入维度D_in在这里等于3，即输入嵌入维度。

我假设输出嵌入维度等于2，现在让我运行一下。可以看到，当我们通过这个自注意力类传递输入嵌入矩阵时，它最终被转换为上下文向量矩阵。就是这么简单。为了保持当前代码中的维度，我们实际上可以确保输入是8维的。让我们调整代码使其与之前保持一致。接下来的一天是光明的，对吧？假设这就是"接下来的一天是光明的"，现在这部分不需要了。在代码或白板中，我们看到每个向量都是8维的，让我直接复制粘贴调整为8维。

然后在这里添加一些随机值，为了简单起见，让我在这里也添加同样的内容。现在我把所有东西都设为8维，这样就能和我们白板上的内容匹配了。好的，现在D_in等于8，但我在这里取的D_out是4，所以我就让D_out等于4。然后让我运行这个代码块，你看它几乎立刻就运行完了。现在我有了上下文向量矩阵，让我们检查一下维度。这个上下文向量矩阵的维度是5行4列，这正是我们之前在这里看到的。上下文向量或者说上下文权重矩阵是5行4列，这正是我们在代码中实现的。

接下来在下一节课中，我们将要学习的是实现一种称为因果注意力的机制。在因果注意力中，我们会屏蔽掉那些不需要或当前标记无法获取的未来注意力权重，这样在预测下一个标记时，我们只会关注过去的标记。这就是所谓的因果注意力，我们将在下一讲中详细探讨。之后，我们会转向多头注意力。我知道这些课程有点长，而且我在重复一些关于注意力或Transformer模块、因果注意力、上下文向量等内容。

但我认为这对我们理解这一点至关重要。如果我们不理解这一点，多头潜在注意力的学习就不会很扎实。我希望在我们开始多头潜在注意力部分时，大家都能达成共识。因此，你们真的需要理解自注意力和多头注意力的基本原理，这也是为什么我要如此详细地讲解这些内容。我会与你们分享代码文件，并希望你们在学习这些课程时也做笔记。把这些内容写在纸上，确保你们熟悉这些计算。之后我们会讲到多头注意力，再往后我们会讲到多头潜在注意力。

请记住，这是一个需要分步进行的旅程，并不容易。这不是一个30分钟的速成课程，而是包含35到40个视频的系列课程，内容会非常非常详细。我打算把它做成类似大学讲座的形式。其实我本可以直接从潜在注意力机制开始讲起，但那样理解起来会非常困难。我希望这些课程对首次接触本系列的观众也同样实用。希望大家喜欢这个系列，期待在下节课中与你们相见。谢谢！

大家好，我是Raj Dhandekar博士，2022年获得麻省理工学院机器学习博士学位，也是"从零构建Deepseek"系列课程的创建者。

在我们开始之前，我想向大家介绍一下我们本系列的赞助商兼合作伙伴——InVideo AI。大家都知道我们多么重视基础内容建设，从零开始构建AI模型。InVideo AI遵循与我们非常相似的原则和理念，让我来展示一下。这是InVideo AI的网站，凭借一个小型工程团队，他们打造了一款令人惊叹的产品，只需文本提示就能创建高质量的AI视频。正如你们在这里看到的，我输入了一个文本提示："制作一个超现实的高端豪华手表视频广告，并使其具有电影感"。点击生成视频后，很快我就得到了这个高度逼真的精彩视频。这个视频最让我着迷的是...

它最令人惊叹之处在于对细节的极致把控——看看这质感与纹理，简直不可思议！而这一切仅通过一条文本指令生成，这正是InVideo产品的魔力所在。您刚才看到的震撼视频，其核心引擎正是InVideo的视频创作流水线。他们从底层原理重构视频生成与剪辑技术，通过海量实验不断优化基础模型。这家公司拥有印度规模最大的H100和H200计算集群，并已开始测试B200芯片。作为印度增长最快、面向全球市场的AI初创企业，InVideo的理念与我深度共鸣。好消息是：他们目前正在广纳贤才，您也有机会加入这支非凡团队！

我正在下方描述中发布更多详细信息。大家好，欢迎来到“从零构建DeepSeek”系列讲座的这节课。今天，我们将继续深入理解注意力机制的旅程。简单来说，我们最终的目标是理解多头潜在注意力机制，这是DeepSeek架构的关键组成部分之一。但要做到这一点，我们需要循序渐进地进行。最初，我们从理解自注意力机制开始，并在上一节课中完成了这部分内容。如果你还记得的话，在上一节课中...

我们详细了解了自注意力机制逐步运作的过程：首先从可训练的查询、键和值权重矩阵开始，将输入嵌入矩阵与这些权重相乘，得到查询向量、键向量和值向量；接着将查询矩阵与转置后的键矩阵相乘，获得注意力分数；然后对这些分数进行维度平方根的缩放处理，应用softmax函数后，最终得到注意力权重。

最后，我们将注意力权重与值矩阵相乘，得到了上下文向量矩阵。理解上下文向量矩阵的方式是：本质上，上下文向量矩阵的每一行都对应着特定标记的上下文向量。例如，在输入嵌入矩阵中，每一行对应一个输入嵌入向量（以本例中的"the next day is bright"为例）。同样地，在上下文向量矩阵中，第一行对应"the"的上下文向量，第二行对应"next"的上下文向量，以此类推。因此，自注意力机制的整个目标就是将输入嵌入向量转化为上下文向量。

为什么会这样呢？因为上下文向量比输入嵌入向量丰富得多。输入嵌入向量仅包含单词的语义信息，但不包含该单词与周围其他单词之间关系的任何信息。而上下文向量不仅包含语义信息，还包含需要关注周围其他单词或标记的重要程度信息。正是这一点使得上下文向量在表征上比输入嵌入向量丰富得多。

那么今天我们要做的就是研究一种叫做因果注意力的机制。我们已经学习过自注意力机制，而在我们最终理解多头潜在注意力的探索之旅中，下一步就是研究因果注意力。顾名思义，"因果"意味着一个事物导致另一个事物的发生。因此我们只需要关注那些对生成下一个标记起决定性作用的因素——稍后我会详细解释这个概念。因果注意力是理解多头注意力机制的核心基础模块，进而帮助我们理解键值缓存机制，最终掌握多头潜在注意力的原理。现在，让我们开始探索因果注意力的奥秘吧。

在我们开始讲座之前，我想先快速解释一下下一个词预测的具体实现方式，这样我能更好地为因果注意力机制铺垫背景。假设我们手头有《哈利·波特》系列的第一本书，它正是我们用于预训练大语言模型的数据集组成部分。为了方便演示，我直接截取了这本书第一页的截图，现在把它展示在我的电子白板上。大家可以看到，我已经把首页截图放在这里了，接下来我将演示如何构建输入与目标配对数据，这些数据最终会被输入到Transformer架构中进行训练。

具体操作步骤如下：首先我们需要确定一个称为上下文长度的参数。例如当上下文长度设为4时，就会生成一个包含4个标记的序列。这是我的第一个输入，然后我还确定了一个称为步长的参数，意思是在创建下一个输入之前需要跳过多少。这是我的第一个输入，这是第二个，这是第三个。类似地，我会将整个数据集分成每块4个单词或4个标记，然后根据我的批次大小，会生成一个矩阵。让我们看看第一个输入批次。如果批次大小是6或8，那么第一批次的第一行将包含对应于Dudley先生和夫人的标记ID。

假设这是1、15、18、22。第二行将是与这4个标记对应的标记ID。那么它会是类似3、4、5、6这样的数字。同样地，我将有8行这样的数据，因为批量大小等于8，对吧？为什么我有4列呢？原因在于上下文大小等于4。这就是输入批量数据的创建方式。

这是我的第一批输入数据。生成输出的方式是，每个输入数据都会向右移动一位。因此，这是我的第一个输出，这是我的第二个输出，这是我的第三个输出，以此类推。一旦你获得了输入批次，生成输出批次就非常简单，只需将输入数据向右移动一位即可。结果会类似于15、18、22、3等等。

这是我的第一批输出。现在，如果你看第一行，这里的第一行和这里的第一行就是我的第一个输入输出预测对。举个例子，我们来看看。输入是“德思礼先生和夫人”，这就是输入。所以这对应着1、15、18和22，然后输出就是将输入向右移动1位。那么这将是Dursley先生和夫人的输出。现在如果你看这一对输入和输出，实际上有四个预测任务。第一个预测任务是当输入是Mr.时，输出是Mrs.。

第二个预测任务是当输入为“Mr. and”时，输出为“Mrs.”。第三个预测任务是当输入为“Mr. and Mrs.”时，输出为“Dursley”。第四个预测任务是当输入为“Mr. and Mrs. Dursley”时，输出为“off”。因此，虽然看起来每一行有四个标记，似乎只有一个预测任务即预测下一个标记，但实际情况并非如此。如果输入是“Mr. and Mrs. Dursley”，输出是“and Mrs. Dursley off”，当上下文大小为4时，就有四个输入和目标预测任务，或者说输入输出预测任务，正如我在这里所写的。

我希望你关注的重点是：当你预测某个输出时，例如当预测输出为"Mrs."时，输入是"Mr."；而当预测输出为"Dursley"时，输入则是"Mr. and Mrs."。现在让我们看看数据集，我再进一步清理数据来向你解释这一点。我想表达的是，当"Dursley"作为输出时，输入始终是仅出现在"Dursley"之前的标记。

同样地，你以任何输出为例，如果输出是“Mrs.”，那么输入就是“Mr.”；如果输出是“Mrs.”，那么输入就是“Mr.”，以此类推。所以无论你在这个序列中走到哪一步，比如说，如果你走到这一步，假设这是输入，那么输出就会向右移动一位。现在，在这个序列中，有四个输入-输出预测任务。如果你看到“was”是输出，你会发现输入是“Mrs. and Dursley was”；如果“thin”是输出，那么输入就是“was Dursley Mrs.”，依此类推。

因此，根据上下文的大小，无论何时何地，只要你有输出，今天讲座的第一个结论就是：输入仅仅是输出之前的那些标记，对吧？所以本质上我想说的是，如果你的输入是“哎呀”，或者假设你的输入是“先生”，并且要预测下一个标记“夫人”，你是无法访问“夫人”之后的所有未来标记的。

这一点看似微不足道，但却是因果注意力的核心前提。因果注意力的整个前提在于：为了预测输出，我们只能访问输出之前的标记。因此我们无法作弊，也无法窥探未来。解释因果注意力的最简单方式就是——你无法作弊，我们无法窥探未来。我在课程开头用《哈利波特》的例子来解释，就是为了让你们对这个主题产生兴趣，这样你们就不会忘记因果注意力的本质。因果注意力的本质很简单：对于每个输出，其输入仅是位于它之前的标记。

所以你要检查这四个输入输出对中的任意一个输入输出对，对于每一个输出，其对应的输入仅限于那些出现在输出之前的单词或标记，明白吗？现在让我们开始更正式地理解因果注意力机制。因果注意力有时也被称为掩码注意力，它是自注意力的一种特殊形式。它限制模型在处理任何给定标记时，只能考虑序列中当前及之前的输入。正如我们所见，为了预测任何输出，它限制模型只能考虑之前的输入。

这与自注意力机制形成对比，自注意力机制允许一次性访问整个输入序列。让我们实际看看这是如何运作的。如果你还记得我们对查询、键和值向量的处理方式：我们有一个输入，输入中包含诸如"your journey starts with one step"这样的词元。这里我展示了每个词元都是一个三维向量。我们将这些向量分别与可训练的查询矩阵、可训练的键矩阵以及可训练的值矩阵相乘，最终得到查询向量、键向量和值向量。

现在让我们来看看如何计算注意力分数。注意力分数实际上是查询向量与转置后的键向量的乘积。让我们更详细地看看这些分数。假设我的查询向量是6×2的，键向量也是6×2的，那么为了得到注意力分数，我实际上做的是查询向量乘以转置后的键向量，对吧？所以这会是一个6×2的矩阵乘以一个2×6的矩阵，最终得到一个6×6的向量或矩阵。目前我关注的标记是“你的旅程始于一步”。

好的，让我们来看看你的旅程始于一步，然后你的我的钥匙是你的旅程始于一步，对吧？基本上，6x6的矩阵表示我在这里的每一个条目都有值。所以，如果我在看“journey”这个词，那是我的查询，我基本上是在寻找“journey”与所有其他词之间的注意力分数，包括“journey”之前和之后的词。所以这是“journey”和“your”之间的注意力分数，这是“journey”和“journey”之间的注意力分数，这是“journey”和“starts”之间的注意力分数，这是“journey”和“with”之间的注意力分数，这是“journey”和“one”之间的注意力分数，这是“journey”和“step”之间的注意力分数。

现在，这可以用视觉方式表示如下：你的旅程始于一步，如果旅程是我的查询向量，我实际上是在寻找旅程与所有其他标记的注意力，无论这些标记是在旅程之前还是之后。现在请特别注意这里，如果旅程是我的查询，那么如果“你的”和“旅程”本质上是我用来预测输出的输入，旅程将完全无法访问它之后的标记。因此，当旅程是我的查询向量时。

如果我们关注的是因果注意力，理想情况下我们应该只发现“your”和“journey”之间的注意力分数。在此之后的所有其他标记对我们来说都不太重要，因为它们无论如何都不会影响下一个标记的计算。让我再解释一下这一点。本质上，在自注意力机制中，我们得到这些注意力分数，因此对于每个标记，我们都会找到它与之前和之后标记的注意力关系。但关键是要认识到，对于每个标记来说，

在预测过程中的任何时刻，我们都无法获取后续的标记。以《哈利·波特》的例子来看，对于每一个标记，我们都无法访问它后面的单词——我们无法预知未来。既然无法预知未来，那么计算一个标记与后续标记之间的注意力分数也就毫无意义了。因此，假设你有一个6×6的矩阵，比如这样：1、2、3、4、5、6；1、2、3、4、5、6；2、3...（这里有三行数据）。让我直接复制粘贴这部分内容。

让我在这里再演示一遍，这是三行，这是我的第四行，这是我的第五行，这是我的第六行。这些是注意力分数，对吧？这是一个6x6的矩阵。对于第一个标记，我们不能看到未来，所以只需要找到第一个标记和它自己之间的注意力分数，就是这个。对于第二个标记"journey"，只需要找到"your"和"journey"之间的注意力分数。对于第三个标记，只需要找到"your journey"和"starts"之间的注意力分数。对于第四个标记，只需要找到"your journey starts"和"with"之间的注意力分数。对于第五个标记，需要找到"one"和"your journey starts with one"之间的注意力分数。

因此，对于第六步之前的所有标记，我们基本上可以找到该步骤与之前所有标记之间的注意力分数。现在，如果你看到所有这些在对角线上方的注意力分数（这里标记出来的），它们都是完全不需要的。这些注意力分数之所以不需要，是因为我们无论如何都不会在任何时间点去关注未来的信息。还记得我们为什么要计算这些注意力分数吗？我们计算一个标记与其他标记之间的注意力分数，是为了帮助我们从输入嵌入向量得到上下文向量。但在自注意力机制中，我们某种程度上是在“窥探”未来的信息，对吧？

因为我们发现，对于每一个标记，我们甚至计算了该标记与之后出现的标记之间的注意力分数，而最终的上下文向量实际上是值向量与注意力分数矩阵的乘积。因此，每个标记的上下文向量本质上包含了关于未来的信息。但实际上，当我们预测下一个标记时，我们根本不会拥有关于未来的任何信息。

因此，理想情况下，当我们获取上下文向量时，应当完全阻止对未来信息的访问——对角线以上的所有元素本质上都应设为零。因为对于某个标记，我们只能访问位于它之前或等于当前标记的内容。这就是因果注意力机制的核心思想。这与自注意力机制形成鲜明对比，后者允许一次性访问整个输入序列。在计算注意力分数时，因果注意力机制确保模型仅考虑序列中当前标记及其之前的标记，这正是我们之前所见的实现方式。

因此，对于每个标记，我们只应考虑在该点出现的标记或在该当前标记之前出现的标记，以获得注意力分数。为了实现这一点，对于每个标记的处理过程，我们会屏蔽掉输入文本中在当前标记之后出现的未来标记。这是最重要的句子，这也是为什么因果注意力有时也被称为掩码保留。所以，如果你看一下双向注意力（即自注意力），每个标记基本上都会关注所有其他标记，对吧。

所以我们得到了当前标记前后注意力分数的信息，但在单向注意力机制中，这被称为自回归注意力、因果注意力或掩码保留——它有很多名称。我们本质上会屏蔽掉对角线上方的所有标记，也就是说，我们会将这些对角线以上的标记设为零。这正是接下来课程要讲的内容：我们如何具体将这些标记设为零。接下来的课程会有点数学化，但到目前为止，我希望你已经理解了我们要做的事情的直觉，对吧？

因此，我们将对角线上方的所有元素屏蔽掉，并记住之前见过的注意力权重矩阵中的这一特性。这里的注意力权重矩阵与注意力分数的主要区别在于，每一行的总和都等于一，这是我们希望保留的理想属性。问题是，如果随机将所有元素设为零，各行之和将不再等于一。因此，我们需要再次进行归一化处理，确保每一行的注意力权重总和恢复为一。

那么现在让我们来看看如何实际操作。任务是这样的：我们已经有了注意力分数，对吧？我们需要将注意力分数中对角线以上的所有元素都设为零。现在来看看怎么做。实际上有两种策略可以实现这一点。我希望你们在这里暂停一下，思考一下当前的问题。想象一下或者回顾一下之前的课程，在之前的课程中我们计算了这些注意力分数，但现在我告诉你们所有这些值都需要设为零。具体来说，所有这些对角线以上的值都需要设为零，所有这些值都需要设为零。

那么，你将如何从根本上计算注意力权重呢？或者换一种思路，你已经有了注意力权重，对吧？假设你把所有这些值都设为零，会发生什么呢？比如说，你现在把这些值都设为零，那么如果你把所有值都设为零，会发生什么？如果是这种情况，你会发现这些行的总和并不等于一。那么，你可以简单地做的是，剩下的部分。如果你把所有值都设为零，这个矩阵会变成类似这样：矩阵会是0.1 0 0 0，然后是0.1 0.5 0 0，接着是0.2。

所以实际上你需要设为零的真实数值是这几个：这个需要设为零，这三个、这两个以及这个——这些数值你都需要设为零。如果你直接把这个设为零，结果会是：这个变成0.1 0 0 0 0，这个变成0.1 0.5 0 0 0，这个变成0.05 0.2 0.2 0 0等等，剩下的两行也是同理。现在你发现问题了：每一行的数值加起来不等于一。

现在第一行的总和是0.1，第二行的总和是0.6，以此类推。那么在这种情况下，你能做些什么让每一行的总和都等于1呢？最简单的做法是：第一行的总和现在是0.1，对吧？所以你把这一行每个元素都除以0.1；第二行的总和现在是0.6，所以你把这一行每个元素都除以0.6；第三行的总和现在是0.2加0.2等于0.4，再加上0.05等于0.45，所以你把这一行每个元素都除以0.45。这样就能确保每一行的总和仍然等于1。

所以这实际上是实现因果注意力分数的第一种策略。第一种策略是这样的：你已经有了经过softmax处理后的注意力分数，也就是在自注意力机制中计算得到的注意力权重。然后你要做的就是在对角线以上添加零值，从而得到掩码注意力分数。但接下来你需要再次对每一行进行归一化，使其总和为一，最终得到掩码注意力权重。这就是我们刚才看到的策略。但你发现这个策略的问题了吗？让我们再看一下这里——这个策略的问题是...

从注意力分数到注意力权重，我们已经使用了softmax进行归一化，这里已经进行了一步归一化。然后，当我们除以行的总和时，又进行了一步归一化。因此，我们在这里不必要地进行了两步归一化。于是问题来了：我们能否采用更智能的归一化方式，从而只需应用一次softmax？事实证明这是可行的，具体做法是直接针对注意力分数进行操作。

让我来告诉你我们是怎么做的。我们的方法是直接针对注意力分数进行干预，而不是在注意力权重上进行干预。具体来说，我们现在是这样操作的：所有这些目前用圆圈标记的值，它们实际上位于对角线上方，最终需要被替换为零。而我们对这些值的处理方式是用负无穷来替换它们，我们会用负无穷来替换这些值。

那么我们来看看注意力分数矩阵现在变成了什么样子。现在，注意力分数矩阵的第一行变为0.6，然后是负无穷、负无穷、负无穷和负无穷。第二行变为0.1、0.1，接着是1.8，然后又是这三个负无穷值，所以我直接把它们复制粘贴到这里。第三行会有两个负无穷值，所以我只展示这三行的情况。第三行的值将是0.2、1.1和1.2。

我们为什么要这样做呢？因为请记住，当你应用softmax时，softmax的作用是对每个元素取指数。具体来说，对于第一行的每个元素，softmax会将其替换为e的x1次方除以总和，第二个元素则替换为e的x2次方除以总和，以此类推。我们正在对第一行进行这样的操作。让我在这里标记一下第一行。假设这是我的第一行，每个元素都将被替换为e的x1次方除以总和、e的x2次方除以总和等等。

现在我要你验证一下e的负无穷次方是多少，你会发现e的负无穷次方实际上等于0对吧？所以第二个元素会被替换为e的负无穷次方除以某个值——反正结果都是0，因为e的负无穷次方就是0。第三个元素会被替换为0，第四个元素会被替换为0，第五个元素也会被替换为0。本质上，所有出现负无穷的地方都会被替换为0，因为任何数的负无穷次方终究都等于0。

第一个元素将被替换为e的0.6次方除以e的0.6次方，因此它将被替换为1，所以总和为1。这里的第一个元素将被替换为e的0.1次方除以e的0.1次方加上e的0.8次方。第二个元素将被替换为e的1.8次方除以e的0.1次方加上e的1.8次方。因此，每一行的总和将为1，我们还将确保对角线上方的所有元素基本上为0。这将确保我们不会进行两个阶段的归一化。在这里，我们进行了两个阶段的归一化，即先进行softmax，然后对每一行进行归一化，但在这里我们只进行一次softmax归一化，就是这样。

这就是在对角线上方引入负无穷的技巧，这是一个非常强大的技巧，它为我们节省了计算量。因此，更高效的方式本质上是你有了注意力分数。更高效的方法是，你有了这些注意力分数，然后应用一种叫做上三角无穷掩码的东西。

这个面具本质上意味着你将上三角部分（即对角线以上的所有元素）替换为负无穷，然后你只需直接应用一次softmax。所以你看这里你只有一个softmax，而之前你需要一个softmax来获取注意力权重，然后还有另一层归一化。这样一来就有两次归一化。

另一方面，这是一种更高效的方式。接下来我们将通过代码来演示，以便你能理解我们在此要实现的目标。还记得在上节课中，我们从输入嵌入矩阵开始，那时你的旅程始于第一步。这里有六个输入，每个输入都有一个三维向量嵌入。在上一节课的最后，我们定义了这个自注意力类。本质上，它接收输入，找到键、查询和值，计算注意力分数，然后得到注意力权重，最后找到上下文向量。在因果注意力中，我们只需要对这一部分进行修改，使得对角线以上的所有元素都被屏蔽掉。那么，让我们来看看因果注意力中具体做了什么。

所以这一节的标题是“用因果注意力机制隐藏未来词汇”。我们从相同的输入开始，我这里只是打印出注意力权重，首先我要向你展示第一种方法。在第一种方法中，我们做的是从之前已经获得的注意力权重开始。

请记住，当我们展示这些注意力权重时，我们已经对注意力分数应用了softmax。在第一种方法中，我们所做的是仅取对角线以上的元素并将它们设为零。因此，这里的掩码实质上是将对角线以上的所有元素设为零，然后我们将这个掩码应用于注意力权重。当我们对这个注意力权重应用掩码时，你会看到我们得到了注意力权重矩阵，但对角线以上的所有元素现在都被设为零。

但这带来了一个问题，即每一行的总和不再等于一，这是一个问题。为了解决这个问题，我们所做的就是简单地将每一行除以其总和，这就是我们所看到的。实际上，这正是我们之前在黑板这里看到的，如果你还记得我们看到的第一个步骤的话。

只需将对角线以上的元素设为零，然后除以行的总和即可。当你运行这段代码时，它会给出掩码注意力权重，这正是因果注意力的主要目的。现在，我将向你展示第二种方法，这种方法实际上更有效。这就是注意力分数矩阵。还记得在第二种方法中，我们不像第一种方法那样从注意力权重开始，而是从注意力分数入手。我们首先计算注意力分数，然后这里是我们所使用的掩码。

所以我们有一个这样的掩码，对角线以上的部分都是1，然后我们拿这个掩码并用它来将对角线以上的所有元素替换为负无穷。一旦我们完成了这一步，接下来要做的就是只需要进行一次softmax运算。softmax会确保这里所有的负无穷实际上都被置为零。

这就是我现在屏幕上展示的内容，softmax函数还能确保每一行的总和等于1。现在如果你比较这些数值，比如1.5517.4483，你会发现它们实际上是完全相同的值。第三行是0.38.3097.3103，0.38.3097.3103。所以本质上两种方法得出的答案完全相同，但第二种方法实际上更高效，因为我们直接从注意力分数开始计算。我们将对角线上方的元素替换为负无穷，然后只需进行一次softmax运算。

我们不必像之前的方法那样先进行softmax再归一化。在实际编写因果注意力机制的代码之前，还有一个最终步骤叫做dropout。dropout实际上是一种深度学习技术，在这种技术中，我们有时会在训练神经网络时观察到有些神经元没有学到任何东西，这些神经元实际上变得懒惰，在学习过程中没有任何贡献。

假设这是我的第一个神经元，这是我的第二个神经元，这是我的第三个神经元，这是我的第四个神经元，这是我的第五个神经元，这是我的第六个神经元。假设这是我的第六个神经元。现在假设这六个神经元在学习过程中，我们观察到其中有实际上两个神经元无所作为，这意味着它们是懒惰神经元，而所有工作都由这四个神经元完成。

解决这个问题的一种方法是在训练过程中随机关闭一些神经元。例如，如果在训练过程中这个神经元和那个神经元被随机关闭，而这两个神经元原本承担了大部分工作，那么现在它们不存在了，剩下的两个神经元别无选择，只能自己学习一些东西。因此，dropout实际上解决了懒惰神经元的问题，即某些神经元根本不工作。

事实上，我们随机丢弃或使某些神经元失活时，其他神经元就必须加快步伐。这就像在做一个小组项目，我们总会遇到这样的情况：只有两个人在干活，其他人什么都不做。但如果这两个人突然生病了、无法参与，其他人就不得不接手工作。Dropout机制也是同样的道理。

所以我们实际的做法是，在因果注意力机制中应用了一种类似的dropout机制。具体来说，在计算完注意力权重后，假设我们得到了注意力权重矩阵——当然这个矩阵会呈现对角线以上元素全为0的特征。我们会随机将某些注意力权重置零，这里的"随机"指的是在每次迭代过程中，被置零的权重位置都会发生变化。

因此唯一固定的是丢弃率。如果丢弃率为50%，就意味着在大型语言模型的每次前向传播过程中，每一行中50%的注意力权重会被随机置为0，而且每次被选中的权重不会相同，因为这些权重是以随机方式选择的，但平均来说会有一半的注意力权重被置为0，这就是通过丢弃机制实现的。这张图实际上展示了这些灰色的神经元，或者更准确地说，这些灰色的注意力权重在这里被屏蔽掉了，或者说它们被随机关闭了。实施丢弃机制的原因在于它能提高泛化性能。

惰性神经元的问题在于，如果我们将这个神经网络应用于新问题时，惰性神经元仍然不会激活，因此泛化能力会不佳。而Dropout机制能确保所有神经元都能有效学习，从而避免对噪声的过拟合，或防止对数据的死记硬背——这些情况通常会导致过拟合和泛化问题。这就是Dropout的原理，接下来我将通过代码展示Dropout的具体实现方式。

假设你有一个这样的矩阵，为了简单起见，可以把它想象成注意力权重矩阵。当你对这样的矩阵应用dropout操作，比如torch.nn.dropout(0.5)，意味着平均而言每一行有50%的元素会被置零——注意这是平均概率，并不保证每行都恰好有一半元素归零。你看这里有些行完全没有被置零的元素，但有些行有五个元素归零，有些行四个，还有些行三个。

所以这是一个随机过程，但关键在于：当使用丢弃率为0.5的dropout时，未被置零的数值实际上会被放大2倍。如果采用0.4的丢弃率（即40%的节点被丢弃），那么剩余数值需要乘以1/0.4的系数进行补偿。这种缩放机制通过将矩阵中保留元素的值乘以1/0.5（即2倍）来抵消激活元素减少的影响。这种调整对于维持注意力权重的整体平衡至关重要，它能确保注意力机制在训练和推理阶段始终保持一致的平均影响力。

Dropout是一种非常简单的机制，你可以把注意力权重想象成灯泡。也就是说，如果这些是注意力权重，就把它们都看作灯泡，而dropout就是在每次迭代过程中灯泡随机熄灭或点亮，你可以指定dropout率。例如，如果dropout率设为50%，就意味着每一行中会随机将一半的灯泡熄灭（置为0）。理解这一点非常重要，它能防止过拟合并提升模型的泛化性能。现在我们已经掌握了这些知识，实际上可以开始编写因果注意力类了。

我们将从相同的输入开始，所以这里有六个标记。你的旅程始于一步，你会看到每个标记都是一个三维的输入嵌入向量。在这种情况下，我要做的是创建两个批次。这是第一个批次，记住每个批次有六个标记，每个标记有三个维度。所以当我把两个批次堆叠在一起时，我们会得到二，这是批次大小；六，这是这里的行数；三，这是列数，这本质上是输入嵌入的维度。

因此，这是我的输入嵌入向量在因果注意力机制中的应用。目标依然相同，我们最终会接收每个输入嵌入向量并将其转换为上下文向量。但现在唯一改变的是，假设我们得到了这些注意力分数，即查询向量与转置后的键向量相乘的结果。

唯一的变化是，我们将对角线上方的所有元素替换为负无穷大，然后进行softmax运算，实际上就是这样。注意力权重的计算方式如下：这将确保对角线上方的所有元素都等于零。接下来，我们应用dropout机制，这意味着在注意力权重的每一行中，权重会被随机置零。正如我们之前讨论的，这实际上提高了泛化能力。而上下文向量的计算方式保持不变。

我们只需获取注意力权重矩阵，并将其与值矩阵相乘，从而得到上下文向量矩阵。因此，如果您思考自注意力与因果注意力之间的区别，唯一的差异发生在注意力分数计算之后。在计算完注意力分数后，我们将对角线以上的元素替换为负无穷大，然后进行softmax运算，接着应用dropout。实际上这里有两处改动：第一处是负无穷大及随后的softmax处理，第二处则是引入dropout机制，它会随机将部分注意力权重置零。

你可能会注意到这里有一些我没有解释的内容，比如这个等于false的bias是什么意思。这基本上意味着我们只是将输入与键、查询和值相乘，而不添加任何偏置项，这就是为什么这个bias等于false。其次，这里的register buffer是什么？为什么要用self.register_buffer来创建这个mask。

所以主要观点是，虽然这不是绝对必要的，但缓冲区会随着我们的模型自动移动到适当的设备上，这在后续训练LLM时会很重要。这意味着我们不需要手动确保这些张量与模型参数位于同一设备上，从而可能避免设备不匹配的错误。目前你本身并不需要这个功能，但如果它存在的话，这只是一个更好的实践。但请记住，这三个维度将贯穿始终，即批大小、令牌数量和输入维度，也就是我们在这里看到的每个输入实际上都有三个维度，这一点非常重要。

好的，现在这个因果注意力类已经实现了，我们来测试一下。我假设输入维度d_in为3，输出维度d_out为2——这和之前示例中看到的输入输出维度一致。你们看这里（滚动屏幕），对，输入维度确实是3，但输出维度是2。这意味着可训练的查询、键、值权重矩阵会将每个输入向量投影到二维输出空间。正如我上节课提到的，在GPT和现代大语言模型中，输入输出维度通常保持一致，但为了演示方便，我们这里暂时采用不同维度。

那么我们接下来要做的就是定义批次，这里记住批次是2,6,3。如果你看的话，嗯，对的，批次就是2,6,3，这是我的输入。然后我只需要定义因果注意力。因果注意力类实际上需要四个输入，嗯，需要四个输入。这里我有我的d_in，我有我的d_out，我有我的上下文长度，最后一个是丢弃率。记住上下文长度是batch.shape[1]，也就是第一个索引。如果你看batch.shape，它是2,6,3，所以batch.shape索引为1的值就是6，因为我在序列中看的是6个元素，所以在这种情况下上下文长度是6。

所以上下文长度等于六，因果注意力类还需要什么？它需要d_in等于三，d_out等于二，上下文长度等于六，还需要一个dropout率。这里我刚刚提到dropout率等于零，所以你可以运行这个，最终看到这个，嗯是的，所以这里的dropout率等于零。在dropout之前你有这些注意力权重，嗯，实际上这部分我想我应该再运行一次。这是我的因果注意力类d_in，d_in，嗯是的，现在我打印了上下文向量，这些就是得到的上下文向量，对吧？所以这个大小是六乘二，这正是我们在这里看到的大小。

因此，上下文向量的尺寸将是6行2列（6x2）。但请记住，我们有两个批次的数据。在第一个批次中，我们有6个标记需要处理，这将生成一个6x2的上下文向量矩阵。当处理第二个批次时，同样会生成另一个6x2的上下文向量矩阵。这就是第二个批次的情况。现在请注意，我们这里传递的是两个批次的数据——如果你观察输入数据，会发现我们是将两个批次堆叠在一起的。

所以这里有两批数据，因此输出将是“2,6,2”。这里是“2,6”，第一批输出的是第一批的上下文向量矩阵，而这是第二批的上下文向量矩阵。因此输出尺寸是“2,6,2”。不要困惑为什么输出尺寸或输出维度不是“6×2”，原因是“2,6,2”表示我们有两批数据，每批的尺寸是“6×2”，对吧？这就是我的上下文向量矩阵，看起来是正确的。呃，这里我写了另一个函数，用于打印出应用 dropout 前后的注意力权重。这里我用 dropout 率为 0.5 运行了因果注意力类，并展示了应用 dropout 前后的打印结果。

所以你会看到，在dropout之前，比如说这里有四个权重是活跃的，对吧？但在dropout之后，只有其中一个保持活跃，但它的值被乘以了2，因为现在的dropout率是0.5。同样地，如果你看第三行，dropout前有三个活跃权重，但dropout后只有两个保持活跃。再看第五行，dropout前有五个活跃权重，但dropout后一个都不剩了——记住这是个随机过程。

平均而言，50%的权重会被关闭，但有时也可能所有权重都被关闭。你可以运行这部分代码亲自验证dropout的实际效果。至此，关于因果注意力的课程就结束了。请记住，因果注意力其实非常简单——只要理解了自注意力机制，其核心逻辑在于：对于给定token，我们无法获取未来信息，只能访问该token及其之前的上下文。

所以我们实际要做的是，需要获取注意力权重并将对角线以上的所有元素设为零。具体有两种实现方式：第一种是直接从先前通过注意力分数和softmax计算得到的注意力权重入手，将主对角线上方的所有元素置零，然后再次对行进行归一化——但这里你需要先做一次softmax，接着还要再做一次行归一化，相当于进行了两次归一化操作。而更高效的做法是：直接从应用softmax之前的注意力分数矩阵出发进行处理。

然后将对角线上方的所有元素设为负无穷大，这样在计算softmax时，系统会自动将这些负无穷大的元素归零，确保每一行的总和为一。这就是注意力权重的生成方式，也是如何使对角线上方的元素归零的原理。但在因果注意力机制中，我们并不止步于此——通常还会加入dropout操作，即随机屏蔽部分权重并将其设为零，以提升模型的泛化性能。每次前向传播时，被屏蔽的权重都会随机变化。

因此，这确保了没有“懒惰”的权重，每个权重在训练过程中实际上都在学习一些东西。然后我们实现了一个因果注意力类，如果你看的话，它实际上看起来像这样，大约15到20行代码。我希望你已经理解了这段代码的每一个方面。记住输入形状的这三个维度：批大小、标记数量和输入维度，正如你在每节课中一定已经注意到的那样。

我非常关注维度问题，因为归根结底，我认为理解注意力机制实际上就是理解矩阵运算。但人们往往对矩阵感到不自在，因为他们无法直观想象高维空间——这就是为什么我如此强调维度的重要性。下节课我们将继续深入：目前我们已经学完了自注意力机制和因果注意力机制，接下来就要开始学习多头注意力机制了。

那么我们将完全准备好理解e值缓存，最终我们将完全准备好理解多头潜在注意力机制。正如我一直强调的，在我讲解时一定要记笔记，这样你才能真正理解课程内容。如果你只是听课，可能会觉得自己理解了，但概念并不会得到强化。课程内容会越来越深入。

随着我们进入后续模块的学习，我衷心希望大家能坚持完成这门课程，听完所有讲座内容。请保持学习动力，做好课堂笔记，随时提出疑问，我们会为大家逐一解答。非常感谢各位的参与，期待下节课与大家再见！大家好，我是Raj Dandekar博士，2022年获得麻省理工学院机器学习专业博士学位，同时也是"从零构建Deep Seek"系列课程的创始人。

在我们开始之前，我想向大家介绍一下本系列的赞助商兼合作伙伴——InVideo AI。大家都知道我们有多重视基础内容建设，从零开始构建AI模型。InVideo AI遵循与我们非常相似的原则和理念。让我来展示一下。这是InVideo AI的网站，他们凭借一支精干的工程团队，打造了一款令人惊叹的产品，仅通过文本提示就能生成高质量的AI视频。

正如你所见，我输入了一段文字提示："创作一则超写实的高端奢华腕表广告视频，呈现电影级质感"。点击生成视频后，很快我就获得了这段令人惊叹的逼真视频。最让我震撼的是它对细节的极致把控——看这个材质纹理和光影效果简直不可思议，而这一切仅通过文字指令就实现了，这就是InVideo产品的魔力。

你刚才看到的惊艳视频背后，是InVideo AI基于第一性原理重构的视频创作流程。他们从底层模型开始实验与优化，彻底革新了视频生成与剪辑的技术范式。

他们在印度拥有最大的H100和H200集群之一，并且正在试验B200。在视频AI领域，他们是印度发展最快的AI初创公司，面向全球市场。这就是为什么我对他们如此有共鸣。好消息是，他们目前有多个职位空缺，你可以加入他们出色的团队。我会在下面的描述中发布更多详细信息。大家好，欢迎来到下一堂课。

在《从零开始构建深度学习》系列中，为了快速回顾，以下是目前为止我们已涵盖的内容：我们正在探索多头潜在注意力机制的旅程中。迄今我们已经理解了自注意力机制——我们从基础概念到代码实现全面讲解了这个模块。在上一讲中，我们掌握了因果注意力机制，而今天我们将要学习多头注意力机制的基础知识，这部分内容将在下一讲展开。


we are going to code multi-head attention but today the main thing which i want to convey to you is what what is the real need for having this multi-head attention and why can't we just stick with causal attention let's say the more i read in literature and the more resources i have seen very few people have explained what is the real need for having multiple heads in attention so without going into the code or without directly jumping into the code first it's very important for us to develop a very strong intuition of why we have multiple heads so today's lecture is titled going from self-attention to multi-head attention and i'm going to try to explain this to you in a very intuitive manner directly from the first principles so let's get started with today's lecture as we have seen before the main goal of the self-attention mechanism is essentially to take input embeddings and to transform them into something which is called as context embeddings remember that input embeddings on their own do not contain any information about neighbors so for example if i look at a single token in a sentence input embedding captures the semantic meaning of that token and its position but it does not capture how that token relates to the other tokens in the sequence and that's the additional mechanism which self-attention brings into the picture the context vector which is the main outcome of the self-attention mechanism contains information of how a given token attends to all its neighboring tokens and to go from the input embedding matrix to the context embedding matrix we have essentially four parts the first part is transformation to query key value matrices second part is calculation of the attention scores third part is calculation of the attention weights and fourth part is the context vector matrix calculation so let's quickly see these four parts in action right now so let me take you to that let me take you to that part of the notebook where we did those calculations here it is so here's the input embedding matrix which we have there are one two three four five tokens and each token is essentially an eight dimensional vector in the first step what we do is that we multiply this input embedding matrix with trainable query key and value weight matrices and then we get this query vector matrix the key vector matrix and the value vector matrix after this point we forget about the input embedding matrix completely and only operate on these queries keys and the values that's step number one step number two is we take a product of queries multiplied by the keys transpose and that essentially gives us the attention scores so every row of this corresponds to the attention score of that particular token so for example the second row of this matrix corresponds to the attention scores for next and how next actually relates to the next day is bright so these five attention scores are mentioned in this or are calculated in this second row similarly all other rows have attention scores of that particular token the main drawback of attention scores is that the rows don't sum up to one so we cannot make intuitive claims that when next is the query give 10 importance to the give 50 importance to next etc so to do that we have to normalize the attention scores and we have two steps in the normalization first what we do is that we take the attention scores and we divide them by the square root of the keys dimension this is done so that the variance of the queries multiplied by the keys transpose that stays equal to one and the variance does not become too large and then we apply the softmax function the softmax essentially ensures that when you look at a particular row all the entries of that row will sum up to one so this is normal self-attention where we look for a given token we look forward as well as backward in the causal attention mechanism what we do is that all the elements essentially about this diagonal all of these elements are essentially put to zero because for a given token we should not have access to all the tokens which come after that particular token so all of these elements which are marked in the green color right now they are effectively masked to zero that's what happens in the causal attention mechanism and then we can apply dropout to the attention weight so that we randomly mask out some elements to be zero this is to improve the generalization performance and to prevent overfitting the last step which is done here is that we take the attention weights matrix and we multiply with the value matrix to give us the context vector matrix and here you can see that every row of the context vector corresponds to my token so the first row is the context vector for the the second row is the context vector for next etc and if you look at the visualization of the context vector you will see that the context vector is much richer because it takes into account how much attention is paid to all the nearby or the neighboring tokens of next for example whereas the input embedding does not have that richness in information about the neighboring tokens so this was in a nutshell the self-attention mechanism and the causal attention mechanism also remember that when we saw the causal attention mechanism what we essentially did was we put the all the elements of the attention weights above this diagonal to be equal to zero and then after that we can also add dropout so we can further randomly turn off certain attention weights to zero that was causal attention okay so with that let's get started with today's lecture in which we will motivate why do we need to go from self-attention to multi-head attention so first remember that self-attention is awesome and self-attention has a lot of advantages because we finally start including or encoding information about the context which is present in the given sentence without self-attention mechanism language models would not be as good as understanding the meaning of sentences as they are today but for all the advantages of self-attention self-attention mechanisms come with one very major problem and multi-head attention solves that problem so let's see what this problem with the self-attention mechanism is and for that i want to start out with a simple illustration so what i want to start out with is that i want to start out with a sentence right and the sentence is the artist the artist painted the portrait of a woman with a brush let me read this sentence aloud once more the artist painted the portrait of a woman with a brush now try to think about what do you think this sentence means you can pause the video for a while and think about the interpretation of this sentence okay so if you have thought for some time you will have realized that the reason i have chosen this sentence is that it has two interpretations the first interpretation is that the artist so let's say if this is the artist the artist has painted the portrait of this woman

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

Using a brush. So, The Artist painted the portrait of a woman with a brush, which means that The Artist had a brush in their hand, and with that The Artist painted the portrait of a woman. That's the first interpretation, The second interpretation is that the artist has painted the portrait of a woman with a brush.

So, the painting is of a woman who has a brush in their hand and The Artist has painted this. So you see the difference in the first case, the painting is that of a woman who does not have a brush in her hand. The artist is painting the portrait with a brush.

But in the second case, it's the painting or the portrait of a woman with a brush in their hand. So this is the painting. So this is a woman with a brush in her hand and the artist has painted this.

So the first is a painting, painting a woman with a brush and the second is painting of a woman with a brush, right? So these are the two interpretations of this sentence. The problem of the self-attention mechanism comes into the picture over here. So what I want to now do is that I want you to focus on two attention scores matrices and which I'm going to put them together side by side over here now.

So take a look at the attention score matrix number one and so take a look at attention score number matrix number one and matrix number two. And what I'm also going to do is that I'm going to bring these corresponding images below it. Okay, so this is the first image.

And then I'm also going to bring the second image now. Here I have brought the second image now and let me just rub this arrow. Okay, now I want you to focus on these two attention score matrices.

Okay, the first attention score matrix, which is which I marked here as number one, it corresponds to this this image, right? And why is that? Because focus on the woman, if the woman is my query, the circles which I've marked in the red here correspond to the highest or the strongest attention scores. So you can focus on those red circles for now. So here we have a red circle between woman and a portrait.

Since it's a portrait of a woman, the attention score between woman and portrait is quite high. That is understandable. Whereas if you see over here in the second in the second attention scores matrix, the attention score between woman and the portrait is still high.

That is understandable because here also we have a portrait of a woman. But notice this red color over here, the attention score between woman and the brush is also very high. And why is the attention score between woman and the brush also high? Because in the second interpretation, the woman is holding a brush in her hand.

That is the difference between this row in the first attention score matrix and this row in the second attention score matrix. In the second attention scores, the attention score between woman and the brush is quite high. Whereas that's not the case in the first attention score because in the first attention score matrix, the woman is not holding a brush in her hand.

The second distinction which we see is when we look at, when we look at essentially artist. So when we look at artist in the first matrix, the artist has a high attention score with painted and the artist has a high attention score with brush. Why? Because the artist is holding a brush in her hand in the first interpretation.

Whereas if you look at the second interpretation, if you look at the second interpretation, the artist is actually not holding a brush in their hand and that's why the attention score between artist and the brush is actually not very high. That's the second difference between these two attention score matrices. And the third difference between these two is when you look at brush.

So when you look at brush, you'll see that the attention, if brush is the query, it has high attention scores with artist because the artist is holding the brush and it has high attention scores with, what is this, yeah, this should not be there. So the brush just has a high attention score with the artist because the artist seems to be holding the brush, right? Whereas here, if you see, brush has a high attention score with portrait because the portrait is that of a woman with a brush and brush also has a high attention score with woman. Why? Because the woman has a brush in her hand.

So brush has a high attention score with portrait and woman in this case, whereas in this case brush has a high attention score with artist. So, I hope I have convinced you now that in these two interpretations of the sentence, the attention scores matrix are completely different from each other, right? There can be two attention score matrices based on the interpretation which you have for the sentence. Now the main problem with the self-attention mechanism is that the self-attention mechanism can only capture a single perspective in a given input sequence.

It cannot capture multiple perspectives. So if you are looking at this sentence, the self-attention mechanism will either have this attention scores matrix or it will have this attention scores matrix and consequently the context vector will either be based on the first attention score matrix or the second attention score matrix. So it cannot take into account both of these perspectives at the same time and that's one major limitation of the self-attention mechanism that if you are given a piece of paragraph or a piece of sentences, self-attention can only capture one perspective.

So if you think about it more deeply, why would you want to capture multiple perspectives in a sentence? The reason you would want to capture multiple perspectives in a sentence or paragraph is that sometimes it is difficult to understand what the paragraph represents in just one form or meaning. One paragraph might have different angles, right? One paragraph might be expressed in multiple different ways and when we design our language model, we want our language model to be knowledgeable of multiple different perspectives present within a paragraph. An example here is that let's say we have a huge paragraph, right? And within that paragraph, we have a sentence which reads like this.

The government should regulate free speech. Now if you look at this sentence, the government should regulate free speech. The first perspective should be the government should impose restrictions on free speech.

So free speech should be curtailed, that's the first perspective. The second perspective should be the government should protect and preserve free speech. It's not really clear from this sentence which perspective is this paragraph referring to.

So it just says regulate free speech. So regulating free speech can either mean that free speech should be curtailed or free speech should be protected and preserved. What does this line really mean? So if we use the self-attention mechanism, again it will assign only one perspective to this.

But we actually want the attention mechanism to capture both of the perspectives which this sentence can include. And the advantage of capturing both perspectives is that our model just becomes more richer in its understanding. So when a user asks to summarize this document, we can summarize both the perspectives, whereas if we just use a self-attention mechanism and a user asks to summarize this document, we will only summarize one perspective and that is not good.

So we want to overcome this limitation of self-attention mechanism. So essentially what we want to do is that we want to have provision in our model so that we can have or we can capture different perspectives in my paragraph. So if one self-attention mechanism can only capture one perspective, can we somehow extend the self-attention mechanism so that it can capture multiple perspectives? So if one self-attention mechanism can essentially just capture one perspective, within the same architecture, what if I have multiple self-attention mechanisms? What if I have multiple self-attention mechanisms? So if one self-attention mechanism can capture one perspective, multiple self-attention mechanism can capture multiple perspectives.

So if let us say one self-attention mechanism gives me one context vector matrix, another self-attention mechanism gives me another context vector matrix, a third self-attention mechanism gives me a third context vector matrix, I will have these multiple context vector matrices, each of which capture different perspectives and then I will just merge them together so that the final context matrix which I have will be much richer in that it will capture multiple perspectives of the given sentence or the given paragraph. So the main idea is this right, you have instead of just having one self-attention mechanism, what you can do is that let us say you have the input, let us say you have the input embedding matrix right, you pass it through self-attention mechanism number 1, you pass it through self-attention mechanism number 2, let us say we look at 2 right now, so this will give me a context vector matrix 1, context vector matrix number 1 and this will give me a context vector matrix number 2 and then what I will do is that I will just merge these two context vector matrices together so that I will have one resultant context vector matrix which now consists of multiple perspectives, this is perspective number 1 and this is essentially perspective number 2 and when I say perspective, it does not necessarily have to do with meaning always, it can be something else like one context vector matrix essentially looks at or pays more attention to verbs in a given sentence, the second context vector matrix maybe pays more attention to the hidden meaning of a given sentence etc. So multiple representations might come out from different context vector matrix, so here we are just trying to expand our range of what we capture from a given sentence and this thing of converting the self-attention mechanism to multiple self-attention mechanism is essentially called as multi-head attention, multi-head attention, the reason this term head comes into the picture is that it is essentially like having multiple self-attention heads, so if you think of one self-attention as one person with one head, it is like one head is giving one attention, but now you have multiple heads, each of these heads is capturing a different perspective.

So think of these as multiple people, let us say this is the first person with the first perspective, second person with the second perspective, so this is just a nomenclature but the head comes from the fact that we are aggregating multiple self-attention blocks together and that is where the name multi-head attention actually has its origins. So I hope until now I have motivated the concept of why do we need multi-head attention and what are the limitations of the self-attention mechanism and now we are going to start looking at how does multi-head attention actually operate in the context of the matrices which we have seen before, the queries, keys, values etc., how exactly can we change that whole procedure so that we have multiple heads. So the main question here right now is that what if we have two self-attention mechanisms instead of one, so let us take this same sentence, the artist painted the portrait of a woman with a brush, now I am going to show you the step-by-step procedure of how we can have two self-attention mechanisms within this sentence and remember that the main purpose of this exercise is that if we have two self-attention mechanisms, we should have two attention scores matrices and we should have two context vector matrices because each has to capture a different perspective. 

So I will need to show you how this is exactly done in the query key value representation which we have. So what if we have two self-attention mechanisms instead of one, now by the way this two self-attention mechanism is also called having two heads and that is the origin of the term multi-head. So now what we are going to do is that we are going to take this sentence and we are going to see a step-by-step procedure of how we can implement a two-head attention on this input sequence and when you understand this step-by-step procedure, you will have a complete visual roadmap of how the multi-head attention mechanism works.

If you have understood self-attention multi-head attention is actually quite straightforward but I think in literature and in other videos it just explained in a complicated and reverse manner instead it's much easier to explain multi-head attention if you motivate it like this and then if you show the step-by-step visual matrix or visual matrices calculation. So let's see this step-by-step procedure right now. Okay, the first thing which we do is as always we start with an input embedding matrix and the input embedding matrix looks like this. 

We have these tokens the artist painted the portrait of a woman with a brush right. We have these 11 tokens and then every token is essentially an input embedding of 8 dimensions which we have considered over here this is also called as the input embedding dimension or DIN. So the dimensions of this entire matrix are we have 11 rows and we have 8 columns so the dimensions are 11 by 8 that's my input embedding matrix.

Remember the goal of this two-head attention now is to take this input embedding matrix and to convert it into two context vector matrices not just one we have to convert it into two so that each context vector matrix captures a different perspective. So this is the input embedding matrix which we have started with and then the first thing which I want to do is show you what we would have done if we just had a single attention head. So if we have a single attention head what would we have done we would have multiplied this input embedding matrix with the trainable query matrix which is an 8 by 4 dimension matrix a trainable key matrix which is an 8 by 4 dimension matrix and a trainable value matrix which is 8 by 4 and this multiplication would have resulted in a query vector matrix that's 11 by 4 a key vector matrix that's 11 by 4 and a value vector matrix that's 11 by 4. Now to extend this into a multi-head attention or a two-head attention in this case what we have to do is that we have to first decide on the output dimension which we want and here I am deciding the output dimension is equal to 4 that is something which is fixed at the start the input dimension is fixed the output dimension is fixed then what we do is that then we decide the number of heads which we want and here we are having two attention heads right so this output dimension is then split among these two attention heads so each attention head will have dimension equal to 2 and the way this visually looks right now is something like this so now my trainable query key and the value weight matrices earlier they looked like this right but now I am just going to divide them into two parts so my trainable query matrix is now for the first attention head it is this so see the size of this has been now it's 8 rows and 2 columns instead of 8 rows and 4 columns and for my second head the trainable query matrix is this which is again 8 rows and 2 columns so in terms of a nomenclature now instead of wq I have wq1 which corresponds to the first head and I have wq2 which corresponds to the second head so if dimensions understanding dimensions is a bit difficult just remember that to split this into two attention heads I have just divided the query into two parts the keys weight matrix into 2 and the value weight matrix into 2 that's it so this dimension over here which the trainable query weight matrix has the trainable query second for the second head this is called as the head dimension and the head dimension is just the output dimension divided by the number of heads so the output dimension in our case is equal to 4 and the number of heads is 2 so the head dimension is just 4 by 2 which is equal to 2 so what is the head dimension it's essentially the number of columns in each attention head and that's equal to 2 in our case and this same split happens for the trainable key matrix and the trainable value matrix as well so similar to what we did for the trainable query matrix we now have wk1 wv1 wk2 and wv2 so these are the trainable key matrices for the both heads and these are the trainable value matrices for the both heads so see what we are doing in this step is that we are creating multiple copies of the trainable query weight matrix the trainable key weight matrix and the trainable value weight matrix this is the main idea in the multi-head attention and if you think about it it's quite simple right in a single head we just had one matrix and remember that now that the d out is fixed we cannot change this so if we want two attention heads we just split the d out into two parts so this is my these are my trainable query weight matrices trainable key weight matrices and trainable value weight matrices that's step number three we essentially split or create multiple copies of wq wk and wv now now that we have multiple copies of wk wq wk and wv it will naturally create multiple copies of the query vectors the key vectors and the value vectors right because let's look at the query vectors first i will first take my input embedding matrix x and i will multiply it with wq1 so that's 11 by 8 multiplied by 8 by 2 and that will give me my first query vector matrix q1 which is 11 by 2 then i will take my input embedding matrix x and multiply it with wq2 that will give me my second query vector matrix that's q2 which is again 11 by 2 similarly i take my input embedding matrix multiply it with wk1 and wk2 and that gives me the two key vector matrices and i take my input embedding matrix and i multiply it with wv1 and multiply it with wv2 and then i get my two value vector matrices v1 and v2 now remember here what we have done simply is that instead of having one one query vector matrix one key vector matrix and one value vector matrix for single head since we have multiple heads now we have two query vector matrices q1 and q2 we have two key vector matrices k1 and k2 and we have two value vector matrices v1 and v2 and what are the dimensions of this these matrices the number of rows essentially remain the same so if you see for all of them the number of rows remains 11 why do the number of rows remain 11 because the number of tokens which i have the artist painted the portrait of a woman with a brush those are 11 tokens but the key thing to note here is that the number of columns which we have the number of columns now becomes equal to 2 because that's the head dimension remember the head dimension is just the d out divided by the number of heads which is equal to 4 divided by 2 which is equal to 2 so the number of columns in all of these matrices are equal to 2 again if you are getting confused just look at the head number 1 all of these matrices are the query key and the value matrices for head number 1 and all of these matrices are the query the key and the value matrices for head number 2 remember this head number 1 we have these matrices and head number 2 we have these matrices it's just that we have now created multiple copies so what happens is that we have the same vectors for a single head but now we have two copies and now that we have two copies we still have only four dimensions right so each copy has to have only two dimensions remember the d out is fixed at the start so in step number four we create multiple copies of the query the key and the value vector matrices which i have denoted over here right now q1 q2 k1 k2 v1 v2 now think about this right what is usually done in the next step usually we take the dot product of queries and the keys to get the attention score matrix but here we have two query matrices we have two key matrices so what will happen naturally we will have two attention score matrices right so that's what happened next we compute the attention scores for each attention head so this is q1 q2 k1 and k2 so for computing the head one attention scores what we simply do is we multiply q1 with the we multiply q1 with k1 transpose we multiply q1 with k1 transpose so that's 11 by 2 multiplied by 2 comma 11 and that gives us the attention score of the first head that's 11 by 11 then what we do is that to find the attention scores matrix of the second head we multiply q2 with k2 transpose so that's 11 by 2 multiplied by 2 by 11 that's 11 by 11 so now take a look here what is exactly happening when we looked at single head if we look at a single head attention we'll have an attention score matrix of 11 by 11 right if we just look at one head because there are 11 tokens here the cool thing which has happened or the amazing thing which has happened with multiple heads is that although the output dimension is getting split into two parts so the head dimension is equal to 2 the attention scores dimension remains the same for both the heads it's 11 by 11 for the first head and it's 11 by 11 for the second head and it would have been 11 by 11 if we just did a single head so essentially now what we have done is that we have two copies of the attention scores we have one 11 by 11 attention score and one 11 by 11 attention score why is it 11 by 11 because remember there are 11 tokens right the artist painted the woman painted the portrait of a woman with a brush etc and if you think about where we started with this is exactly what we wanted right instead of just getting one attention scores matrix we wanted to extend the self-attention mechanism so that we can get multiple attention scores matrices and that is exactly what has happened here since we had two copies of the queries since we had two copies of the queries and we had two copies of the keys we can essentially multiply these two copies and get two attention scores matrices so each attention score matrix essentially can capture a different perspective and that is the main advantage of multi-head attention this step here that although we have multiple heads and although the dimension of each head is now split into two so the head dimension is now equal to 2 which was 4 before so in single head attention it was 11 by 4 multiplied by 4 by 11 and that gave us the 11 by 11 attention score matrix but now it's 11 by 2 multiplied by 2 by 11 so although this dimension is reduced by half so although this dimension is essentially reduced by half the final attention score matrix still is 11 by 11 so this dimension is same in both of these cases that's the beauty of multi-head attention although the head each head has a reduced dimension when we take the dot product of the queries and keys transpose for both the heads it's we get two attention score matrices of dimensions 11 by 11 and each of these can now capture a different perspective essentially we have two copies of the attention score matrices now then what happens in the next step is the same since we have two copies of the attention scores now what we'll do is that we'll scale we'll scale by square root of the keys dimension we'll apply softmax and then we'll apply causal attention which means that we'll just make sure that all the elements above the diagonal are set to zero remember we cannot peek into the future and if needed we can also apply dropout so in this schematic i have assumed the dropout rate to be zero but after you get the attention weight matrix you can even have a dropout rate and randomly turn off different elements in the attention weight matrix so this is the head 1 attention weight matrix that is 11 by 11 and this is the head 2 attention weight matrix that is also 11 by 11 matrix now and what is the difference between attention weights and attention scores attention weights every row will just be normalized so if you look at every row it will be summed up to 1 and also remember that we are implementing causality here so we make sure that for both of these attention weight matrices the elements above the diagonal will essentially be equal to zero so just keep that in mind and then what we do in the last step is that now we have two we have an attention head matrix for both these heads right and remember earlier we had calculated the value matrices v1 and v2 v1 was 11 by 2 v2 was 11 by 2 so v1 is the value matrix for head 1 v2 is the value matrix for head 2 so what we will do in this last step is essentially we take the attention weight matrix of the first head we multiply it with the value matrix of the first head so that gives us the context vector matrix for head 1 which is 11 by 2 and for the second head we similarly take the attention weight matrices of the second head and we multiply it with the value vector for the second head so that is 11 by 11 multiplied by 11 by 2 and that gives us the head 2 context matrix so head 1 context matrix is 11 by 2 and head 2 context matrix is also 11 by 2 and now remember what we do after this point is that we have the context vector matrices from both the heads and remember what we had discussed at the start once we have the context once we have the context vector matrix for the head number one and once we have the context vector matrix for head number two we just merge these context vector matrices and that's exactly what we do in the last step in the last step what we do is that we have the first head one context matrix which is which you you can say as giving us the first perspective that is perspective one and we have the head sorry this should be head two so we have the head two context matrix and that essentially gives us the perspective two and when you merge these context vector matrices you will have the final context vector matrix which is of the size of 11 by 4 so to the left side of this is my first head to the right side of this is my second head so ultimately when i merge the results from both the heads together i'll have the context vector of vector of size 11 by 4 and remember now if you had just done a single head attention if you are just a single head attention without splitting into two heads the output dimension is four right so you would have also got the same context vector matrix size 11 by 4 but it would not have consisted of two perspectives in a single head if you had just used a single head there also you would have gotten the same context vector matrix of 11 by 4 but there the whole thing would have been just one perspective but now the advantage here is that

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

The size remains the same, but it consists of two perspectives. The first perspective given by my first head, which I am calling P1 and the second perspective given by my second head, which is called as P2. So we have just extracted more information from my text.

Of course, the disadvantage of this is that for extracting each perspective, we have only two dimensions to play with now, that is the drawback, whereas here we had essentially four dimensions for each perspective, right? But now we have a reduced number of dimensions to play with for each perspective. That is the drawback for multi-head attention. The main drawback is that the dimension size for each head reduces, right? As you see over here, the dimension for each head is effectively reduced because we have to split the whole query weight matrix, key weight matrix and value weight matrix into two.

So the dimension size for each head is reduced. So the amount of information we can capture is a bit reduced, but the number of perspectives we can capture is increased. So each head captures more perspective.

So the way I think about it is like divide and conquer. Instead of conquering the whole sentence at once, you divide into different parts and then each part conquers some different perspective. That's the simplest way I like to think about multi-head attention.

So this whole step-by-step procedure which we saw, let's recap it quickly. We start with the input embedding matrix, the artist painted the portrait of a woman with a brush and I have deliberately started with the sentence here which can be looked at from different perspectives, correct? So what we do here is that we start with the input embedding matrix and when we multiply it with the trainable query key and the value matrix, we split these trainable weight matrices into two parts. So we fix the output dimension equal to 4 and we decide the number of heads.

So since we have two heads here, each head will essentially get two dimensions. That's called as the head dimension, which is the d out 4 divided by the number of heads which is equal to 2. So wq1 is 8 by 2, wq2 is 8 by 2 etc. So these are the query key and the value trainable weight matrices for head number 1 and these are the trainable query key and the value weight matrices for head number 2. Alright, so once we have these multiple copies of w, q, w, k and w, v, naturally it leads to multiple copies of query key and value.

So head 1 has one copy of q, k, v which is q1, k1 and v1 and head 2 has another copy of q, k, v that's q2, k2 and v2. Then what we do is that for q1 and k1 we have the first attention scores matrix, for q2 and k2 we have the second attention scores matrix. The first attention scores matrix is from head 1, second attention scores matrix is from head 2. Why do we have two attention score matrices? Well, each head might be capturing a different perspective such as maybe the first head might be capturing this perspective, maybe the second head might be capturing this perspective etc.

So each head might be capturing different perspective and that's why we have two attention scores matrix here. In my view, this part is the most important step because here we see that each attention scores matrix captures a different perspective and that's the whole advantage of the multi-head attention mechanism. And then after that we follow similar steps which we have seen for self-attention.

We then take the attention score matrix, scale it by square root of keys dimension, apply softmax, apply causal attention which means we mask out all elements above the diagonal in the attention weights to be 0 and then if needed we can apply dropout to improve the generalization performance or to prevent overfitting. So until this point we have the attention weights which have been calculated and then what we do is we multiply the attention weights for every head into the value vector for that head v1 and v2 and then we get the context vector matrix for head number 1, context vector matrix for head number 2. What the context vector matrix for each head represents is that now we have 11 rows here right, the artist, painted etc. So we go from input embedding to a context vector.

So now for artist, instead of just looking at the semantic notion of artist, the context vector for artist now captures information about how this artist relates to the other tokens. That's why this matrix is much more richer than the input embedding matrix. So this is the head 1 context vector matrix and this is the head 2 context vector matrix and in the last step what we do is that we merge the context vector matrices for both the heads and that leads to the final context vector matrix 11 by 4. The size of this is the same as what it would have been if we just used self-attention with a single head but the main advantage is that we have now two perspectives within this context vector matrix p1 and p2.

So hopefully we will capture richer representations in the text itself. The disadvantage is of course in each perspective we now get reduced number of dimensions to play with. So the expressivity in each perspective might be reduced but this is a trade-off which seems to work well in our favor because all the modern LLMs are based on the multi-head attention mechanism.

We know LLM just has a single head, we have multiple heads so that each head can capture a different perspective. So this is the whole step-by-step procedure of how we go from the self-attention mechanism to the multi-head attention mechanism which was the main purpose of today's lecture. Now what I want to show you is that I want to show you a very quick demonstration of how of visualization of these attention heads.

So what we are going to do is that we are going to take a pre-trained large language model. We are going to take a pre-trained LLM. So this is going to be a BERT model and it will have a bi-directional attention.

So causality is not implemented. So every token will look at previous tokens and also the tokens after that. So this is pre-trained which means it has already been optimized on a huge amount of data.

What we will do is that we will pass our input sentence to this pre-trained LLM and what's the input sentence, the artist painted the portrait of a woman with a brush. We will pass our input sentence to this pre-trained LLM and then what we will do is that we will peek into the different attention heads and we will see what every attention head essentially gives us. So remember that when you see the code there will be two parameters, there will be layer and the head.

So what layer essentially means is that an LLM architecture has multiple transformer blocks and each transformer block has multiple attention heads. So when we look at different layers it means different transformer block and when we look at head it means that which head we are in a particular layer. So for the purposes of demonstration we are only going to look at layer number 3 which is essentially the third transformer block and in this layer number 3 we are going to look at attention head number 3 and attention head number 8. What does it mean attention head number 3 and attention head number 8? The pre-trained LLM which we are looking at will have 11 attention heads.

So the output dimension is split into 11 different parts and then every attention head will essentially get D out divided by 11 and then we are going to look at the attention weights matrix such as this for each head and that is going to tell us that when we look at woman for example what is prioritized by different attention heads. So let's quickly jump into the demonstration right now. So the package which I have downloaded over here is birthwiz and then what I am simply doing is that I have loaded the pre-trained model over here and I am just showing a visualization for this sentence the artist painted the portrait of a woman with a brush and first I want to show you for layer number 3 and we will see for layer number 3 and essentially head number 3. So if you go into layer number 3 and head number 3 and if you hover on to woman let's see so if you hover on to woman you will see that the maximum attention is given to brush.

If you see on the right hand side the maximum attention if you trace this line you will see that the maximum attention is given to brush and we can also confirm this. So in this in this code I have essentially plotted the different attention scores which are given for woman and I have taken a screenshot over here. So if you look at layer 3 and head number 3 and if you take the query as woman here you can plot the tokens for which the maximum attention weight is given.

So the maximum attention weight of score is given to brush for layer 3 and head 3 but now let's go to layer 3 and head number 8. So if I go to layer 3 and head number 8 right now you will see that when you see woman the maximum attention is now given to portrait and that is again confirmed over here if you see layer number 3 and head number 8 maximum attention is given to portrait. So this might indicate that here the attention of the woman is given to brush right so that might mean that here we have this second visualization. So let me take a after this loads let me take a screenshot of that second visualization here.

So it seems that the attention between woman and brush is the maximum right. So it seems that head number 3 which we saw over here thinks of this interpretation or this perspective because it seems in the second perspective the woman holds a brush in her hand and the attention between woman and brush is the maximum in this head whereas if you look at head number 8 the attention between woman and brush is very low. So it seems that this head has recognized that maybe the woman is not holding the brush but the woman is just present in the portrait.

So it might mean that this second head thinks that this perspective is more strong. So it may be decodes this perspective. So here you can see this is the direct proof that a pre-trained transformer or a pre-trained LLM rather has different attention heads and each attention head can essentially uncover a different meaning the head number 3 which we saw over here uncovers this meaning that maybe the woman holds a brush in her hand whereas head number 8 uncovers this meaning that maybe it's just a portrait of a woman and the woman might not be holding a brush but the artist might just be painting the portrait of a woman.

So in this hands-on demonstration we just saw that different attention heads can capture different perspectives and that's the whole aim or the whole purpose of the multi-head attention mechanism. I hope I have been able to explain why what is the intuitive need that we need to go from the self-attention mechanism to the multi-head attention mechanism. This lecture was specifically dedicated to introducing the intuition behind multi-head attention mechanism.

In the next lecture what we are going to do is we are going to do actual calculation using mathematical numbers. So we are going to start with a given sentence we are going to assume some numbers and we are going to apply multi-head attention in practice. But I did not want to directly jump to this lecture without giving you an intuition for why we move from self-attention mechanism to multi-head attention mechanism.

This is the core building block of why LLMs work and it's also the core building block which DeepSeek figured out that we need to modify this block itself to make it a bit better. So although multi-head attention has a lot of advantages it does have some disadvantages in terms of storage space and computational efficiency which are mitigated or reduced by key value cache and further reduced by multi-head latent attention. So that's why we have this important milestone in our way.

We cannot understand KV cache or multi-head latent attention without understanding multi-head attention itself. So after the next lecture we will directly move to key value cache and then multi-head latent attention which is the first fundamental innovation in DeepSeek. So stay tuned till that time and I hope you are making notes alongside.

These lectures are a bit dense and I am deliberately making them a bit longer so that everything is explained to you. This won't be a lecture series of 2 to 3 lectures. I am planning to make it 35 to 40 videos of lecture series so that ultimately you really understand the nuts and bolts of how DeepSeek is constructed.

But to go to that stage it's important for us to be on the same page with the building blocks. Thanks a lot everyone and I look forward to seeing you in the next lecture. Hello everyone, my name is Dr. Raj Dhandekar.

I graduated with a PhD in Machine Learning from MIT in 2022 and I am the creator of the Build DeepSeek from Scratch series. Before we get started, I want to introduce all of you to our sponsor and our partner for this series, Invidio AI. All of you know how much we value foundational content, building AI models from the nuts and bolts.

Invidio AI follows a very similar principle and philosophy to that of us. Let me show you how. So here's the website of Invidio AI.

With a small engineering team, they have built an incredible product in which you can create high quality AI videos from just text prompts. So as you can see here, I have mentioned a text prompt, create a hyper-realistic video commercial of a premium luxury watch and make it cinematic. With that, I click on generate a video.

Within some time, I am presented with this incredible video, which is highly realistic. What fascinates me about this video is its attention to detail. Look at this, the quality and the texture is just incredible.

And all of this has been created from a single text prompt. That's the power of Invidio's product. The backbone behind the awesome video which you just saw is Invidio AI's video creation pipeline in which they are rethinking video generation and editing from the first principles.

To experiment and tinker with foundational models, they have one of the largest clusters of H100s and H200s in India and are also experimenting with B200s. Invidio AI is the fastest growing AI startup in India, building for the world. And that's why I resonate with them so much.

The good news is that they have multiple job openings at the moment. You can join their amazing team. I am posting more details in the description below.

Hello everyone, and welcome to this lecture in the build deep seek from scratch series. Today, we are going to have our second lecture on the multi head attention. If you remember in the previous lecture, we went through the conceptual overview of multi head attention and how the mechanism actually works, how we split the query key and value vectors into multiple different heads and how each of the heads eventually leads to a different attention score.

And that actually helps us in capturing two different perspectives with just the self attention mechanism, we can capture only one perspective of a given sentence or a given paragraph. But in some cases, we would like to essentially compute the concepts in a lot more detail or from a lot more perspectives, which is not possible through just a self attention mechanism. And that's why we have the multi head attention mechanism where each head allows us to compute a different perspective.

So we saw the whole process in the previous lecture. Today, what we are going to do is that today we are going to take a matrix, we are going to take an actual input embedding matrix, and we are going to see step by step about how the multi head attention works in practice. So this is going to be a mathematics based lecture.

The previous lecture was more about intuition. But here we are going to go step by step with respect to mathematics. And parallelly, I'm also going to show you the code for how the multi head attention function is written.

And by the way, what we are going to cover in today's class is exactly how the first multi head attention implementation works in practice. So whatever we have done until now in this lecture series, starting from self attention to causal attention to the first lecture of multi head attention, it all comes to a culmination point in this lecture, where we'll code out the entire multi head attention class in multi head attention class in Python. And I'll also show you the step by step mathematical derivation and every matrix multiplication in a lot of detail.

First, let's do a quick overview of what we learned in the previous lecture. In the previous lecture, we saw that if you have sentences such as this, the artist painted the portrait of a woman with a brush. This can be viewed in terms of two perspectives, either the artist painted the portrait of a woman using a brush or the artist painted the portrait of a woman with a brush.

So the woman had a brush in her hand, it can be two different perspectives, right? But a self attention mechanism can only capture one such perspective because there is only one attention scores matrix either we can have this or we can have this. So then the question is, can we somehow extend the self attention mechanism so that it can capture multiple perspectives? So what we did was a single head can get us one attention score. So what if we have multiple heads, right? So what if we have two self attention mechanisms instead of one, and then we saw how to implement this multi head attention step by step.

We started with an input embedding matrix, the artist painted the portrait of a woman with a brush. We saw that the first step which is done is that we decide the output dimension. And this is going to stay the same in today's lecture as well.

We have to decide the output dimension and we have to decide the number of heads which we want, the number of attention heads. So in our case, in the previous lecture, we had output dimension equal to 4 and number of attention heads equal to 2. So the head dimension is the output dimension divided by the number of attention heads that's equal to 2. This is the dimension of each head. So what actually happens when we start doing the calculation is that we split the trainable query matrix, the key matrix and value matrix into two parts.

And that eventually splits the query vectors, the key vectors and the value vectors into two parts. Why do we have two parts because there are two attention heads. So there is one copy of Q, K and V, which is the query key and value matrix for each head.

And now that we have two copies of the query key and value, it essentially leads to two attention scores. So this is the attention scores matrix calculated from the first head. And this is the attention score matrix calculated from the second head.

This right here is the most important point in the multi-head attention mechanism workflow because each attention scores matrix here essentially represents a different perspective. This was not possible with just the self-attention mechanism. Now we have two attention scores matrices, right? So each can capture a different perspective.

And that's the main advantage of multi-head attention. Then what we do is after we have the attention scores, we do the scaling with square root of the keys dimension, we apply softmax, causal attention. And if we want, we can do dropout also.

That leads to the attention weights, the head 1 attention weights and the head 2 attention weights. And then what we do is that we multiply the attention weights of the head 1 and head 2 with their corresponding value vectors. And then we get the context matrix for head 1 and head 2. We merge these context matrices and then we get the final context matrix, which now is a mixture of two perspectives.

So remember, the more the attention heads we have, the more the perspectives we can capture into our final context matrix. That's exactly how the multi-head attention actually works in practice. We also saw a cool visualization towards the end where we took a pre-trained large language model and we actually explored inside the attention heads.

So today, as I mentioned, we are going to do a mathematical calculation of exactly how the multi-head attention works. So I'm going to show you every single matrix multiplication from scratch. And then I'm also going to show you how it relates to the code for the multi-head attention.

So here if you see, this is the multi-head attention class. And if you just see the class without understanding the mathematical details, this all might seem a bit confusing to you, but I will take you today through the entire mathematical derivation and then I'll show you that it's actually directly mapped to the code and then understanding the code becomes that much easier. So let's get started.

Today, the example which we are going to consider is this example where we'll start with an input embedding matrix. So the input embedding matrix is X and there are three tokens essentially. Let me change my color here so that maybe I'll change it to this new color over here, which is orange.

So this is my input embedding matrix here. The way to visualize this matrix is that there are three tokens. This is token one that corresponds to the first row.

This is token two, which corresponds to the second row. And this is token three, which corresponds to the third row. And every token essentially has a certain dimension.

That's my input dimension. And this is going to be equal to six in this case. So there are three tokens and each token has six dimensions.

But as you see this, this is a tensor with three dimensions over here. So what's these three? The second is essentially just the number of tokens. The third is essentially the input dimension din.

So this is the tokens. This is the din. But what's the first dimension? The first dimension is essentially the batch, the batch size.

So we are going to just pass in one batch for the sake of simplicity. But if there are two batches which are passed, then this will be of a size two by three by six. If there are three batches which are passed together, then this will be of a size three by three by six, etc.

Right. But for the sake of simplicity, I am going to assume that just one batch is passed at one time. But remember that whenever you write the code for the multihead attention block, it always starts with the input embedding matrix with three dimensions.

So when you go to the code, you will see that we start the forward method of the multihead attention class with three dimensions, the batch size, the number of tokens and the input embedding dimension. That's the X dot shape, right? So this is my input embedding. So just to give you a quick recap of what this is, is if you remember the journey of the token through the.

So we had a lecture regarding the journey of a token through the LLM architecture, right? There is a whole data pre-processing step where every token essentially gets converted into something which is called as the input embedding, which is the summation of token embedding plus position embedding. And we call the input embeddings as uniform, essentially every token gets its own uniform. So the input embedding for every token, which I'm showing over here is actually that uniform.

So every token has its own row, right, which is a vector of six dimensions. That's its input embedding, which is the summation of token embedding plus position embedding. If you have not seen that lecture before, I highly recommend you to go through that lecture, which is titled journey of a token through the LLM architecture.

So now we have done the first step, which is essentially defining our input, which consists of three dimensions. All right, now next, let's go ahead to apply the multihead attention mechanism. As we saw in the previous lecture, we have to decide two things, we have to decide the output dimension and we also have to define the number of attention heads which we want.

Why do we need to decide these two dimensions? Because it eventually decides the dimension of each attention head, which I have, which is called as head dimension. Remember, the head dimension is going to be D out divided by n heads. So in this case, what I'm going to do is that I'm going to decide my output dimension equal to six, and I'm going to decide the number of heads to be equal to two.

So you can do a calculation of what each head dimension will be, it's going to be six divided by two. So each attention head is going to have three dimensions now. Okay, great.

The next step which I'm going to do is I'm going to multiply my input embedding matrix with my trainable query key and the value matrix. So first I have to initialize these matrices, right, I have to initialize. So remember my x now, which is this input has is one by three comma six.

So if you forget this first batch for now, it's three by six. So if you want to multiply it with trainable query matrix, trainable key matrix and trainable value matrix, the first the dimensions of this, the dimensions of this are D in comma D out. The dimensions of these trainable matrices are always D in comma D out.

And remember now we have finalized D in and D out, my D in is equal to my dimension in is equal to six and my D out is also equal to six. So my trainable weight matrices for the query key and value will all be six by six matrices which are initialized randomly. So that's what I'm going to do over here.

I'm initializing the trainable weight matrices for the key query and value. And these will be six by six matrices. This is for WQ, this is for WK and this is WV.

Remember, all the values inside these matrices are completely random right now. Our goal later through back propagation is to optimize these values so that the next token is predicted correctly. Once I have this trainable weight matrices, the next step what I'll do is that I'll take my input, I'll take my input and I'll multiply with these matrices.

So I'll multiply X with WK, X with WQ and X with WV. So remember my X is now one by three comma six. So if I multiply it with six by six that will lead to again a one by three comma six matrix.

So after this multiplication, I get the keys matrix, which is one by three by six. I get the queries matrix, which is one by three by six and I get the values matrix, which is one by three by six. I want you to pay very close attention to these three values over here.

Remember we started out with an X input whose dimensions are batch size, number of tokens and din. Now check the dimensions of these queries, keys and the values. The first is of course the batch size, the second is the number of tokens.

So these two remain the same, but the third is now dout. So instead of the input dimension, we are now in the output dimension space. So the keys, queries and values are all of the dimensions of batch size comma number of tokens comma output dimension.

And until now, we have not applied the multihead attention at all, but bear with me, we will apply it later. So let's go to the code right now. We have the X dot shape, which is B comma number of tokens comma the input dimension.

And then what we are going to do is that in the init method. So if you see, we have to initialize the trainable query key and value matrices, right? That's essentially done over here. The query, the key and the value matrices are initialized through a linear layer of a neural network and the bias term is equal to zero.

What this essentially means is that when you do self dot or when you pass X, which is the input through the trainable key weight matrix, it essentially takes a multiplication of the trainable key matrix multiplied by X. This is exactly what we wanted, right to get the keys, queries and the values. We just have to take the multiplication of the we have to take the multiplication of the input embedding matrix and the trainable query key and the value matrices. That's what's done in this step.

So here, although you cannot see a direct multiplication operation, your X is being passed X, which is my input is being passed as an input to this W key, which is the linear layer of a neural network. This essentially is a multiplication because the.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)