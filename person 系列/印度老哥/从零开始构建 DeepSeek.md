
Author：Raj Dandekar

google docs: docs.google.com/spreadsheets/d/1GLAndnI1-PbFDXSa0qdbRaBLJiTQHdcZpmmfMbeRAqc/edit?gid=867380576#gid=867380576


快速展示给你看。这就是GPT-3的论文，标题为《语言模型是少样本学习者》，作者们基本上证明了，如果将模型规模增加到高达1750亿个参数，无论是单样本学习还是少样本学习，模型性能都会显著提升。我认为，正是在这个时候，我们真正跨越了规模障碍，从13亿参数到130亿参数，最终达到了1750亿参数。

一旦突破这个规模障碍，我们就能开始看到大型语言模型的惊人特性。这是GPT-2，其最大模型约有15亿参数；而GPT-3的最大模型则达到了约1750亿参数。这里有一篇关于神经网络与规模关系的论文，从1950年代到2020年，你可以看到我们所使用的参数数量呈指数级增长。最近，这些橙色点更加密集，这是因为语言模型确实主导了规模领域。本质上，语言模型及其规模正在急剧扩大，目前我们已经达到了约1万亿参数。看看这条直线，我用红色箭头标出的部分——注意Y轴是对数刻度。

因此，这是一个指数级增长的日志，意味着语言模型的规模正在呈指数级扩大。为什么我们如此关注模型的规模？又为何要不断扩大它？原因在于人们发现，随着语言模型规模的持续增加，会出现所谓的“涌现行为”或“涌现特性”——这些特性本质上不存在于小模型中，却会在大型模型中显现。例如，观察这里列出的所有任务（如算术运算、单词重组等），你会发现每项任务都存在一个性能突增的临界点。而横轴（X轴）可以理解为计算能力，大致等同于模型规模。

因此，当模型规模超过某个临界点时，就会出现一个转折点——模型突然开始学习新事物，展现出这些神奇而强大的特性。虽然模型只是基于下一个词预测任务进行训练，但随着大语言模型（LLM）规模的持续扩大，超过特定规模后，模型就能（或者说开始展现）实现这些惊人功能，比如翻译、摘要、语法检查等。正因如此，我们正竞相构建越来越庞大的模型。OpenAI、Anthropic等公司甚至公开表示，他们现阶段就是在追求规模扩张，因为人们仍寄希望于模型参数突破10万亿甚至100万亿大关后可能出现的突破。

大型语言模型展现出一些我们目前完全不了解的特性，这些就是大语言模型的涌现特性。需要注意的是，大语言模型与早期的自然语言处理模型不同，因为早期的自然语言处理模型基本上是针对特定任务（如语言翻译）而设计的，而由于我们刚刚看到的这些涌现特性，大语言模型可以执行广泛的任务，如翻译、摘要、事实核查、语法检查等，正如大家可能已经通过JPG探索过的那样。关键的一点是，早期的语言模型甚至无法根据自定义指令写一封电子邮件，而这对于现代大语言模型来说是一个微不足道的任务。因此，到目前为止，我们已经看到大语言模型确实随着规模的增大而变得越来越好。

它们发展出了涌现特性，还有一点需要注意的是，这场语言革命的核心其实是这种被称为Transformer的架构。如果你不知道Transformer架构是什么，别担心，我们会在本系列中讲到。但本质上，有一篇名为《Attention Is All You Need》的论文介绍了Transformer架构。正如这张图所示，它看起来有点复杂。要真正解析Transformer架构，需要几节课的时间，但本质上，这就是驱动语言模型的秘密武器，我们将学习它，所以不用担心。最后我想提到的一点是，当我们说创建一个大语言模型时，它涉及两个阶段。

首先是预训练阶段，此时我们并没有标注数据集，但模型本质上会自行生成训练数据和标签，这被称为自回归阶段。因此，经过预训练的模型也被称为基础模型。为了进行这种预训练，我们通常会从互联网、教科书、媒体、研究文章等来源收集大量数据。例如，GPT-2 的训练数据来自 Reddit、书籍、维基百科文章、开放网络语料库等。然后，这个庞大的大型语言模型就在这些海量数据上进行训练。这种训练的成本高达数百万美元，随着模型规模的增大，甚至可能花费数千万乃至数亿美元。需要注意的是，预训练后模型仅具备基本能力，之后我们通常还需要对模型进行微调。

这就是第二阶段，我们用标注数据集对模型进行微调。例如，你可以通过提供一些翻译通常如何进行标注来教模型翻译，也可以通过给出一些指令来教模型遵循指令，比如“将45公里转换为米”，然后答案是45,000米，这就是你提供给模型的标注数据，这样它就能学会遵循指令。GPT-3.5（后来成为产品ChatGPT）是通过强化学习人类反馈（RLHF）训练的，实际上是由人类标注员对输出进行评分。

这一反馈被传递回该元素，这对我们来说非常重要，因为微调阶段是Deep Seek真正彻底改变游戏规则的时刻。我们稍后将探讨是什么让Deep Seek如此特别。但要理解后续内容，我们所有人都必须对以下概念达成共识：什么是大语言模型（LLMs）、涌现特性是什么、推动大语言模型的秘密配方是什么，以及最重要的是创建大语言模型所涉及的两个阶段——预训练加微调。带着这些认识，让我们进入下一节：Deep Seek构建的大语言模型究竟是什么。

它是如何变得如此受欢迎的呢？让我们来看看深度求索（DeepSeek）构建的不同大语言模型（LLMs）。如果你访问他们的网站deepseek.com，并向下滚动到研究部分，你会看到他们构建的不同版本的大语言模型。他们最初从一个简单的DeepSeek LLM开始，之后的重要里程碑是构建了DeepSeek Version 2，接着是DeepSeek Version 3，随后又推出了DeepSeek R1。他们为每个版本都发布了论文，例如这里你可以看到DeepSeek Origin 2的论文，这是一份52页的文档；这里还有DeepSeek Version 3的报告，同样是53页；此外，你还能看到DeepSeek R1的论文。

我在这个系列中的目标是，他们在这几篇论文中提到的关于架构和训练等方面的一些惊人之处，我将逐一解析，并将其分解成模块化的讲座。这样你就不必阅读这份报告，只需听完这些讲座，就能理解其中的核心内容。不过，最引人注目的主要模型是DeepSeek R1，因为正如我们即将看到的，DeepSeek R1是一个推理模型，其性能可与OpenAI的顶级模型相媲美，而且成本仅为后者的一小部分，更重要的是它是开源的。这两点同时出现简直太棒了，对吧？DeepSeek R1于2025年1月发布。

这是一个推理模型，但其性能与OpenAI相当。记住，OpenAI是闭源的，而这个模型是完全开源的，这意味着如果你有足够大的设备配置，你实际上可以下载该模型并在本地运行。其次，这个模型的API成本只是最新OpenAI模型成本的一小部分，如果把这两点结合起来——开源加低成本，那就相当棒了，对吧？看看参数数量，正如我们所看到的规模扩展法则，语言模型随着规模的增大而变得更好。

深度求索也不例外。深度求索V3拥有6710亿参数，随后推出的深度求索R1是基于V3基础模型构建的推理专用模型。需要注意的是，V2和V3是两个不同的基础模型，而R1则是源自V3的推理模型。要理清这个发展脉络：他们最初发布了V1版本（称为深度求索LLM），随后开发了深度求索LLM数学版、深度求索LLM编程版；接着推出深度求索V2及其编程版V2 Coder；最后才演进到我们现在看到的深度求索V3版本。

最后，这件事真正引爆了互联网，也正因如此，我们此刻才能举办这个系列讲座。好了，到目前为止，我们已经了解了大型语言模型（LLMs），也看到了深度求索公司开发的各种大型语言模型。现在，让我们开始深入一些核心内容：首先，让我们将深度求索与其他人工智能模型进行比较，看看它是否更优秀，为什么更优秀，以及它的定价有多低等等。

实际上，我之所以对这个数学问题如此着迷，是因为它确实令人惊叹。这里有一个从0到1的积分，x的四次方乘以1减去x的四次方，再除以1加上x的平方。这个问题的奇妙之处在于，当我第一次解出它时，答案竟然是22/7减去π，这完全颠覆了我的认知。

太棒了，对吧？首先，我们很多人都已经搞混了，以为π等于22除以7，但事实并非如此。22除以7实际上比π大一点点，而这个积分正好捕捉到了这个微小的差异。这是一道优美的数学谜题，顺便一提，它曾在1968年的普特南数学竞赛中出现过，众所周知该竞赛题目难度极高。其证明最初于1944年某个时候被构想出来，这同样展现了一种美妙的数学关联对吧。于是我去找了DeepSeek和ChatGPT，请它们来解答这个问题。

我去了GPT 4.0，给了它这个积分，并让它选择正确答案。记住选项A是正确答案对吧。让我们看看GPT做了什么。当你往下滚动时，它会显示正确答案是2乘以105，也就是选项B。所以它彻底失败了。然后我去了DeepSeek，问了同样的问题：解决这个。DeepSeek一步步地进行了计算，我得出了正确答案，即22除以7减去π。

虽然这只是一个例子，我们当然不能仅凭一个例子就对两者进行比较，但这确实表明DeepSeek在处理难题方面非常出色。它不仅得出了正确答案，而且只用了大约10到15秒的时间。好了，现在让我们对DeepSeek和其他AI模型进行一些正式的比较。

首先，我们来比较一下DeepSeek和GPT 4.0。在性能方面，这仍然存在争议，Reddit的几个论坛上对此进行了激烈的讨论，但普遍的共识是，DeepSeek在某些任务上与GPT 4.0相似甚至更优。不过，假设在性能方面你认为它们处于同一水平，那么巨大的差异就在于成本了。

比如说，我调用DeepSeek和GPT模型做个对比。GPT 4.0每百万token的价格大约是30美元，而DeepSeek R1只要0.55美元。成本简直天差地别。事实上，你甚至可以在x轴上看到价格，这是评估分数或相当于评估分数，而在y轴上则是价格。你可以清楚地看到，如果将GPT 4.0和DeepSeek版本3进行比较，DeepSeek版本3的性能要好得多，而且相对于GPT 4.0 mini来说价格合理。但如果你将DeepSeek版本3与GPT 4.0进行比较，你会发现DeepSeek版本3在y轴上的性能更高，而且如果你比较DeepSeek版本3和GPT 4.0，它的成本显著更低。

事实上，DeepSeek版本3在垂直方向上位于右上角，这意味着它的性能确实非常出色。同时，与其他所有模型相比，它位于最左侧，这表明它的成本也相对较低。这正是社区对此感到极为兴奋的原因。我们在这里看到的是一款高性能且价格低廉的模型，更棒的是，它还是完全开源的。

开源模型与闭源模型之间总是存在一种紧张关系，但这里有一个最终弥合差距的例子。我们拥有一个开源模型，其性能与GPT 4.0相当甚至更优，您可以调用它或以极低的成本进行API调用。事实上，如果您访问DeepSeek网站并查看不同的参数，您会发现DeepSeek版本3在几乎所有先前考虑的参数上都优于GPT 4.0。

所以关键在于，从定价来看，DeepSeek完全胜过GPT 4.0，因为它的价格确实只是后者的一小部分。更重要的是，DeepSeek是开源的，而GPT 4.0是闭源的，这意味着你可以直接下载DeepSeek，如果你有GPU或者你的机器足够强大，你可以托管并运行它。但请记住，它有6710亿个参数，所以这并不是一件容易的事。其次，如果你将DeepSeek与Lama进行比较，比如Lama是Meta发布的一系列开源模型。这些模型很棒，但它们的性能不如DeepSeek。

DeepSeek在规模和性能上表现卓越，其基础模型拥有数千亿参数，实际效果远超700亿参数的Llama模型。若重新审视这张对比图，Llama系列也列于其中，但从性能坐标轴（Y轴）来看，DeepSeek全面超越了所有Llama模型——无论是700亿参数的指导模型还是4050亿参数的指导模型。这不仅因其具备6710亿参数的体量优势，更源于多项架构创新：混合专家系统、强化学习训练、多智能体注意力机制、多令牌预测、输入量化等技术，这些创新共同铸就了DeepSeek对Llama的显著优势。

好的，那么这是关于DeepSeek与GPT-4以及Lama的比较。就优缺点而言，DeepSeek在成本效益方面非常出色，性能也相当不错，而且是开源的，这是它最大的三个优势。但最大的缺点是它可能不如GPT-4那样完善或安全。在我看来，随着进一步的发展，这一点可能会有所调整，DeepSeek实际上可能会变得更安全或更完善，但目前对于大公司来说，真正实施它还是有点顾虑的。

其次，它有6710亿个参数。假设你是一家不愿使用OpenAI的组织，因为你需要通过API调用，而你不希望自己的数据传到其他地方。于是你决定下载这个模型并在本地托管。这可能需要大量的计算资源，因为这是一个相当大的模型——6710亿参数可不小，处理起来并不容易。所以你需要为此搭建相应的计算基础设施。但如果你在自己的服务器上托管它，就能确保数据隐私。作为一个组织，你必须权衡这些因素。如果你真的非常在意安全防护措施，也许现在还是继续使用GPT-4，等以后再转向DeepSeek。

如果你是一家精干且快速发展的初创公司，选择DeepSeek将大幅降低成本，它性能卓越，而且还是开源的。如果你注重隐私，也就是说你不希望自己的数据再次流入闭源公司手中，那么DeepSeek的开源特性对你来说就是一个优势。现在，让我们进入本讲座的重点部分——DeepSeek究竟有何特别之处？它是如何做到收费如此低廉的？又是如何在保持与GPT-4竞争的性能的同时，实现如此高的成本效益的？

因此，我认为这里有四个主要方面需要讨论。首先，DeepSeek拥有创新的架构；其次，其训练方法极具创造性和创新性；第三，他们实现了多项GPU优化技巧；第四，他们构建了一个有利于蒸馏等技术的模型生态系统。我们将逐一探讨这四点。首先，在架构本身方面，我认为DeepSeek做对了以下五件事，使其真正成为一个创新架构：第一，他们采用了多头部潜在注意力机制；第二，他们采用了混合专家模型；第三，他们实现了多令牌预测。

然后他们引入了量化技术，最后还采用了旋转位置编码。我们将详细学习所有这些内容——事实上，其中每一项都需要用两节课来专门讲解。互联网上关于这些内容的资料非常有限，你必须投入大量时间才能真正理解这种架构的创新之处。现在让我快速带你们了解这些概念的实际含义：如果我们回溯到原始注意力机制——就是我在论文《Attention Is All You Need》中展示的那种——当时采用的是多头注意力机制，其结构大致如图所示。

现在，这就是目前存在的多头注意力机制。而DeepSeek团队所做的创新在于，他们引入了一种完全不同的方法，以确保注意力机制能够高效实现。他们采用了键值缓存技术，但不同于常规的键值缓存，他们创新性地将其置于潜在空间中（若您暂时不理解这些术语不必担心，后续课程我们会详细讲解）。现阶段您只需记住：他们通过在潜在空间实施这种特殊的键值缓存技术，使得注意力机制的计算效率显著提升——不仅占用更少存储空间，计算速度也更快。这正是他们做出的重要变革之一。

这就是多头潜在注意力机制，这是我们讨论的第一个要点。第二个要点是关于专家混合模型（MoE）。如果你观察常规的注意力机制，它的结构大致是这样的：首先是前馈层，接着是注意力层，之后又接一个前馈神经网络。而在专家混合模型中，核心操作是设置了四个专家模块——整个神经网络并非同时激活，每次只激活其中的部分子网络。

因此，他们设计了一种特殊的路由机制，由它来决定哪些部分会被激活，哪些部分不会。这个路由器本质上就是决定哪些专家模块会被激活，哪些不会。我们后续会非常详细地学习这部分内容。这是他们实现的第二个关键创新点。

第三项创新是多token预测技术。就像我们这节课开头看到的，传统模型通常只预测单个token。而他们采用了一项上个月刚发表论文提出的新技术——为什么不尝试同时预测多个token呢？这可能会加速推理过程，提高模型效率。

第四项创新是他们实施了量化技术。因此，量化不是将每个参数都表示为大型浮点数，而是以一种略微压缩的方式来表示。理解量化的最佳方式就像这里的例子：左边是原始图像，包含大量像素点，而右边的图像仅由八种颜色构成，呈现出一种像素化的效果。如果你放大看，会发现它完全是由像素块组成的——看这里，相比左边非常锐利的图像，这边明显像素化了。但当你缩小视图时，会发现两者看起来几乎一模一样对吧？这就是所谓的量化。他们将这种量化技术应用到了Transformer模块的参数中。

最后，他们还引入了一种称为"旋转位置编码"的技术。让我现在就来展示旋转位置编码的原理——它本质上应用于2017年提出的注意力机制中。

他们只是将位置编码添加到标记编码中，这污染了嵌入向量。但不久之后，人们意识到，为什么不直接旋转原始向量、查询向量和键向量来捕捉位置编码的效果呢？这样就不会改变向量的幅度，而且这是一种非常高效的位置编码方式。因此，我们不是在标记嵌入本身中编码位置，而是在查询和键中稍后编码位置。是的，这就是旋转位置编码，这是DeepSeek架构实现的第五个关键创新。我们现在将详细研究所有这些内容。这是第一个创新方面——架构创新，但他们并没有止步于此。我认为DeepSeek论文真正让强化学习领域重获新生，因为它不仅仅依赖于人类标记的数据，就像我们在GPT-3.5中看到的那样，人类创造了数据的质量，然后将这些数据反馈到训练过程中。

他们采用大规模强化学习来教授模型复杂推理能力，并非依赖人工标注数据，而是构建了一套基于规则的奖励系统——这意味着整个训练过程不依赖人类判断，完全由规则驱动。通过这种方式，他们实现了名为"群体相对策略优化"的框架，这正是其强化学习训练机制的核心所在。我们将深入解析这套体系，而这正是DeepSeek R1成为卓越推理模型的关键所在。

强化学习是我们将要涵盖的主要内容之一。第三个方面是GPU优化技巧，这部分理解起来有点难度，所以即使在课程中我也不会花太多时间讲解。简单来说，他们所做的不是使用CUDA，而是采用了NVIDIA的并行线程执行技术（PTX）。最直观的理解方式——其实我问过GPT如何用最简单的方式来解释——如果把CUDA比作编写Python或Java代码，那么PTX就像是更底层的一步，它是机器代码执行前的一个中间步骤，这能大幅提升运行速度。

因此，在高级编程中，你并不处于机器码层面。如果你处于机器码层面，那是最快的，比如C或C++，而Python或Java则让你处于更高的层次，这会减慢速度。你可以把PTX看作是介于两者之间的东西，而CUDA处于更高的层次，但PTX处于中间，更接近机器码执行，因此能大大加快速度。关于这一点，还有一篇不错的文章发表过，是的，Deep Six AI在某些功能上突破了行业标准的CUDA，转而使用NVIDIA的汇编式编程，比如PTX编程。

因此我认为，这也对加速他们的架构优化起到了重要作用，最终降低了成本。最后，他们拥有一个强大的模型生态系统——尽管主模型规模高达6710亿参数，但他们已将其蒸馏压缩至小至15亿参数的模型，这构成了一个非常出色的模型生态系统。我们稍后也会探讨模型蒸馏技术。现在简要回顾一下让Deep Six如此突出的四个关键点：首先是创新架构；第二是以强化学习为核心的训练方法；第三是他们掌握的一系列GPU优化技巧；第四是模型生态系统，特别是将大模型蒸馏压缩至约15亿参数的小型模型的能力。

现在，我们进入今天课程的最后两个部分。首先，为什么Deep Six是AI历史上的一个转折点？我认为它是一个转折点，原因如下：这是第一次有一个小而精悍的初创公司，通过新颖的技术和少得多的资源，达到了与最佳AI模型相当的水平。他们大幅降低了开发成本，虽然不像600万美元那么低，但我相信与像GPT或Meta的Llama等大公司训练模型所需的成本相比，仍然相当低。因此，这就像是第一次证明，即使是小型初创公司，即使是资金不如OpenAI雄厚的公司，也能构建出大型语言模型，这真是太棒了。


right which performs at par which with gpt4 it and it does so with far less resources what also happened with this is that people got scared right investors got scared there was a huge dip in the u.s tech u.s tech stocks in january 2025 because of deep six advancement the main reason was the idea of a low cost open source chinese ai model threatened the profit models of open ai microsoft and even google and raised concerns about the ai supply chain and gpo markets one model or one company deep seek brought about so many changes and that's what makes deep seeker turning point in history one major thing which i believe has also happened because of deep seek is that developing countries such as for example india have started have heavily started investing into building their own large scale foundational models if china's deep seek can do it then why not other countries why only us and why only companies coming out of us rather right why not other companies with resources which deep seek used can build their own foundational models so all of this discussion has been started in fact the indian government has also released a call for building foundational models that's pretty awesome i think and it's one of the main motivating factors for me to create this series now let's come to the last section of today's lecture which is our plan for this lecture series we have developed or i have rather divided this lecture series into four phases based on what is the specialty about deep seek the first phase for us is going to be going into the architecture so first i'll start with the attention mechanism then i'll go into multi head latent attention mixture of experts multi-token prediction quantization and rotary positional recordings i am going to assume that you have some amount of knowledge of attention if not you can check the build llm from scratch series so this series is going to be a bit more advanced and it assumes that you you go through that previous series before i am going to start at a slightly higher level here then we are going to go in the phase two in the training methodology phase three gpu optimization text this will be a small phase i won't be having too many lectures here and then i'll conclude with lectures on distillation a bulk of the lectures will be on phase number one and phase number two and smaller number of lectures on phase three and phase four so this is the main plan which will be following for the series let me quickly summarize what we learned today first we looked at large language models and the fact that they are engines of probabilistic next token prediction we saw that size is a very important factor in large language models there is a size scaling law as the size increases the models get better and better they start developing emerging emergent properties which are not present in smaller models then we saw that the llm secret sauce is essentially transformer the transformer architecture and then finally we saw that creating an llm means building a foundational model which is essentially the pre-training stage and then we have a fine-tuning stage there are two parts then we saw that although deepseek r1 has become popular the deepseek company started long back they started with the deepseek llm first which is version 1 then they had version 2 and ultimately version 3 which was a huge model 671 billion parameter and then ultimately they made deepseek r1 which is a reasoning model and that broke the internet why did it break the internet because deepseek r1 has comparable performance to openai stock model and at a fraction of the cost plus it's open source so deepseek is equally performant as gpt4 their pricing is literally i think 100 to 500 times less as i showed you in this lecture and finally it's fully open source strength and weaknesses the biggest strengths of deepseek are that it's open source it's cost efficient and it has competitive performance so three big strength the biggest weakness might be that it's not maybe as polished or safe as let's say gpt4 another weakness is that if you're planning to deploy it locally or planning to use it securely you need to have infrastructure for downloading and using a 671 billion 671 billion parameter then we saw what makes deepseek so special and there are four key ingredients here the first is the innovative architecture training methodology gpu optimization and model ecosystem within the training or within the innovative architecture we have five key things multi-head latent attention mixture of experts multi-token prediction quantization and rotary positional encodings then in the training methodology we have the fact that they use large-scale reinforcement learning to teach complex reasoning to the model and they used a rule-based reward system which is also known as group relative policy optimization rather than relying on human label data in the gpu optimization tricks they used parallel thread execution pta instead of cuda only in some places i believe and then finally they have a strong model ecosystem where they distill their main model into smaller models as low as 1.5 billion parameters in this lecture series we are going to follow the same workflow we'll go with the first phase which is innovative architecture then we'll go to the second phase which is training methodology then we'll go to the third phase which is gpu optimization tricks then we'll go to the fourth phase which is model ecosystem i am going to assume a good amount of knowledge about llms and i will explain the attention mechanism again but i'll essentially start from the attention mechanism and then dive into the details if you're a complete beginner i recommend the build llm from scratch series first and then finally i believe deep seek is a turning point in history because they literally showed that even developing countries can build their own foundational model if we are smart about the innovative architecture if we are creative we can build a foundational model which is as good as let's say open as models and that too at a low cost and fully open source so they are truly democratizing ai that way so thanks a lot everyone and i look forward to seeing you during the next lecture thank you hello everyone my name is dr raj dandekar i graduated with a phd in machine learning from mit in 2022 and i am the creator of the build deep seek from scratch series before we get started i want to introduce all of you to our sponsor and our partner for this series in video ai all of you know how much we value foundational content building ai models from the nuts and bolts in video ai follows a very similar principle and philosophy to that of us let me show you how so here's the website of in video ai with a small engineering team they have built an incredible product in which you can create high quality ai videos from just text prompts so as you can see here i've mentioned a text prompt create a hyper realistic video commercial of a premium luxury watch and make it cinematic with that i click on generate a video within some time i am presented with this incredible video which is highly realistic what fascinates me about this video is its attention to detail look at this the quality and the texture is just incredible and all of this has been created from a single text prompt that's the power of in videos product the backbone behind the awesome video which you just saw is in video ai's video creation pipeline in which they are rethinking video generation and editing from the first principles to experiment and tinker with foundational models they have one of the largest clusters of h100s and h200s in india and are also experimenting with b200s in video ai is the fastest growing ai startup in india building for the world and that's why i resonate with them so much the good news is that they have multiple job openings at the moment you can join their amazing team i am posting more details in the description below hello everyone and welcome to this lecture in the build deep seek from scratch series in the previous lecture we learned about the four phases in which we are going to divide the lecture series the first phase is going to be the innovative architecture behind deep seek so that's phase number one right over here phase number two is the training methodology itself the rise of reinforcement learning and how they relied on rl to teach complex reasoning to the model using rule-based reward systems that's phase number two phase number three is gpu optimization tricks so how they used nvidia's parallel thread execution ptx or cuda let's say and phase number four is their model ecosystem itself so they did not stop at just building a huge 671 billion parameter model but that large model was essentially distilled down into a much smaller model whose size was around 1.5 billion parameters so that's essentially phase number four we are going to go through phase number one to phase number two phase number three and phase number four and in this lecture today we are going to start with phase number one which is essentially the innovative architecture behind deep seek and what makes it so efficient the two major aspects of their architecture which contribute to the deep seek efficiency is essentially something called multi-head latent attention mla which makes the attention mechanism itself more efficient and second is the mixture of experts which means that although the number of parameters is 671 billion all of those parameters are not active at the same time only about 37 billion parameters are so essentially parts of the parameters are turned off and parts are turned on like light bulbs going on and off and that makes the model extremely efficient in its computations parameters which are not needed are turned off when they are needed they are suddenly turned on and then they start working right then we have multi-token prediction quantization and rotary positional encodings as well which we are going to learn about so the first thing which i want to teach you is

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

Attention and then I was thinking about how to exactly teach this concept right because the concept itself is pretty advanced so if you search about multi-head, if you search about multi-head latent attention you'll get certain blog posts which talk about this so they do talk about essentially what multi-head attention is and if you scroll down here below you'll see that it's just one page of this blog article and they start out with multi-query attention, group query attention and they go to rotary positional encoding and then there is a very small section on multi-head latent attention which is impossible for a person to understand if if they are just starting out to explore what deep sick is and so I don't want to follow that approach of just going through a simple lecture and explaining MLA by assuming that you know the prerequisite knowledge instead as I mentioned to you at the start of the series itself I want to make this lecture series very deep so the way I'm going to explain multi-head latent attention is through a four-part process first we are going to understand the architecture of LLMs itself and that's going to be the main purpose of today's lecture I believe that without having an intuition of the LLM architecture it's impossible to understand latent attention then we are going to understand why there was a need for self-attention and what is the self-attention mechanism itself once we understand self-attention we are going to understand how self-attention was transformed into multi-head attention and what does it mean to have multiple attention heads that's the third aspect over here and then the fourth aspect is essentially key value cache so then we are going to understand okay multi-head attention works and it really works very well but then what can we start doing to improve the efficiency of multi-head attention to make it computationally faster to make sure that the number of parameters which you are storing in the memory is reduced and that's when key value cache comes into the picture only when you truly understand key value cache then you will slowly start to understand multi-head latent attention so after kv cache then we are going to understand MLA but I'm going to devote a lot of time to develop your foundations in these first four concepts before we go ahead to MLA itself many of these blog posts which I mentioned right now they assume that you are already familiar with this so in the build a deep seek from scratch series we had 43 lectures explaining you everything about these different aspects the in this lecture series I am not going to go as deep so for example today I am going to have just one lecture on the LLM architecture but in the build LLM from scratch there were three to four lectures right so my aim is that I want to explain this knowledge to you but also I want to make sure that beginners who have started watching this series they also feel connected with the series so the challenge for me was to come up with a new set of lecture notes specifically for this series because I'm trying to explain a concept in a in one hour but also I want to make sure that I don't lose out beginners in the process so I've made a whole series of new notes for explaining these different concepts all right so I hope everyone has understood the flow of how we are going to understand multi-head latent attention today our main aim is to understand the architecture of LLMs so after today's lecture all of you should have a mental map or a visual roadmap of what happens with the world or a token when it goes into an LLM first of all let me explain what does it mean architecture of an LLM right so if you pass in some certain a sentence or a sequence of words we have seen that when a sequence of words is passed into an LLM what the LLM essentially does is that it predicts the next word or it predicts the next token rather so an LLM or a large language model can be thought of as the next token prediction engine right so it can be thought of as the next token prediction engine and just like an engine so let's say if I go and search engine car and if I copy this right now copy image or let me copy a good image right now if I copy this image right now and if I paste it over here right this is an engine so if we are calling it the next token prediction engine we need to know how does this engine actually work how does this engine actually work in the previous lecture we have learned some things about the engine what have we learned the first thing which we have learned about this engine is that it has a huge number of parameters so to give you an example gpt3 has around 175 billion parameters whereas gpt4 although it's not been released yet they probably have around a trillion parameters gpt 4.5 which was just released two to three days back maybe have 5 trillion or 10 trillion parameters that's not yet released but we know that this engine has a huge number of parameters which are acting together and if you search engine car working right you'll see that there is this piston cylinder mechanism here right and the piston cylinder mechanism essentially works and then that's how the engine operates similarly we need to make sure that we understand how do these parameters actually work where are these parameters can we open the engine of the large language model and try to see what happens inside that engine essentially given a sequence of words so similarly to a car right when a fuel is injected into this engine what essentially happens with that fuel and how is that converted into the motion of the car similarly when a sequence of words is passed to this engine what's underneath the llm which causes us to predict the next word so think of the sequence of words as a fuel and think of the llm of course as the engine and the next word as the motion of the car let's say so here we want to open the engine right and to truly understand how the engine actually works we need to understand the architecture of the engine which means that we need to understand what are the different components inside that engine how these components are actually connected to each other then what exactly happens with the fuel or the sequence of words when it is being passed through this architecture that's what we essentially want to understand in today's lecture so you can think of today's lecture as going to your car opening the engine and trying to peek inside the engine and trying to really understand how do you work one way to also think about this is that all of us have worked with right and if we ask something like give me an essay give me a short essay on friendship right and we see that gpt essentially predicts one token at a time how does this actually work what is the architecture which is giving me this one token at a time prediction that's what we are going to find out today okay i'm going to try to keep this lecture as beginner friendly as possible so you'll see that there's a whole story which i have constructed to explain how the llm architecture works i don't think this has ever been done before so it's also a bit of an experiment for me to explain this to you in the story kind of a format but i hope through this explanation you will understand really how the llm architecture is working okay so here's the schematic of what happens when you take a look at the engine what happens when you open the black box what lies within the black box itself and when you open the black box you will see that there are a huge number of things which pop out it's not simple at all uh if you think of the 175 billion parameters the 175 billion parameters are scattered across multiple places of this black box in other words the llm architecture is quite complex um and why do you think it might be complex well because in making the next token prediction the llm is actually learning language itself right i strongly think that language learning is a byproduct of the next token prediction task and to learn the language you cannot have an engine which is small or you cannot have an engine which is not complex enough so that's why our engine is quite complex we have a huge number of blocks or layers which are linked together today we are going to try to understand this architecture so if you just take a look at this schematic you will see that broadly this schematic is divided into three parts there is this the there is this part number one over here part number one then there is a part number two which i marked by saying that it's something which is called as the transformer block which let me mark like this and then there is a part number three which is basically the part which is the output part so here is where the next token is actually predicted so the architecture of the llm can be thought of in three parts the first part can be thought of as the input the second part can be thought of as the processor and the third part essentially can be thought of as the output right in the input part we have the sentence let's say if you have the sentence any sentence let's say the next day is bright there are a number of things which happen with that sentence and with every token or every word in that sentence before we pass it to the processor and the things which happen to the sentence are first of all we do something which is called as tokenization secondly we do token embedding and third we do something called as positional embedding after these three steps are done the input is essentially passed to the processor which is also called as transformer block within the transformer block there are six different things the normalization layer multi-head attention layer dropout second normalization layer feed forward neural network another layer of dropout these two plus signs here are what are called as skip connections or shortcut connections finally when we come out of the processor or the transformer block we have the output and here we have another layer normalization layer and the final layer for the next token prediction okay so all of what i said right now if you are learning this for the first time you might be thinking what is going on here and all of this seems too complex let's break it down further and that's exactly what i'm going to do right now but keep a keep close eye or attention on this part which i've marked as purple because the first innovation which i'm going to explain later which is the multi-head latent attention or mla here is to do with this aspect which is titled as multi-head attention so out of the entire architecture there are two major places where deep seek has contributed in making their innovations the first is this called as multi-head attention that block and the second is the feed forward neural network so let me actually rub this a bit right now so the mla or the multi-head attention is an innovation which happened in this part of the architecture so i'm going to call this mla and the mixture of experts innovation that actually happened in this part of the architecture moe so again if you don't understand the architecture itself you will not be able to appreciate where the innovation has happened but it's like imagine opening an engine right and you have opened the deep seek engine right now and the deep seek engine is that of a car which has performed very well and you want to understand why has it performed very well you open the engine you see all these parts and now i'm telling you there are two parts in this engine which were augmented to improve the performance but all right coming back to the engine itself how do the input processor and the output actually work and now my challenge for this lecture was that i wanted to create one lecture in which i explain all of this to you i explain the input i explain the processor and i explain the output and i wanted to explain it in an easy to understand manner so that you get an feel for the architecture itself so the way i thought of doing this was that i thought from the perspective of the fuel so if you're a fuel let's say and if you go to the car engine what happens with you do you go to the engine first then are you rotated because of the pistons and then some kind of a power or energy is produced if i understand the life of the fuel i'll effectively understand how the engine works right similarly today i want to show you the life cycle of a single word and what happens to a single word when it essentially goes through the llm architecture think about it this way right when we when we put this sentence give me a short essay on friendship or let's say if we are going to complete the next sentence and the sentence which i am going to take is let's say the next day is bright the next day is bright it goes into the llm engine and then the next token is predicted let's say the next token is and the next day is bright and what i want to show you is we are going to focus on just one token and we are going to see what happens to this token as it goes through every single step of this llm architecture and by looking at from the perspective of the token so i want you to now imagine that you are that token you are that token and i will now take you through what happens to the token as it goes through several of these layers in the llm architecture itself and ultimately we predict the next token so that's how i'm going to explain this whole lecture to you in a story format i am going to explain this lecture to you as if you are the token now imagine you are the token you are surrounded by a bunch of words and suddenly you are thrown to the llm architecture let's understand the life cycle of a single token so that's how the next part of this lecture is going to be all right so let's embark on this journey together in which we will understand how the life of a token essentially looks like so the title which i have given to this section is the journey of a token through the llm architecture so what i first did is i went to chat gpt and i asked it to write a short paragraph on friends right so i took some random sentence from this which is a true friend accepts you let's say we are looking at this sentence a true friend accepts you which is a sequence of five words let's say that's my input sequence at the moment and we have to predict the next token given this input sequence and i am going to specifically focus on the word friend and i am going to think from the perspective of friend now put yourself in the shoes of this word or this token first of all this token so i am going to interchangeably use token and word although they are not the same for the sake of simplicity i am just going to say one token is equal to one word so put yourself in the shoes of this token now what do you see you see that there are these other tokens around me right there is a true accepts you there are these four other tokens which i am used to hang out with just like friends hang out together i am used to having these as my neighbors and my friends a true accepts and you these are the four neighbors of this token which we have chosen that is friend now suddenly what happens in the first step is that the first step in the llm architecture is that that's the step of isolation so currently we are looking at this phase right which is the input phase right the first step which happens in the input phase is the isolation phase so what happens is that the word is detached from its neighbors the word is isolated from its neighbors so imagine like a group of friends and every person is isolated from their neighbors so this word is isolated and we look at it in isolation that's phase number one phase number two is essentially called as token id assignment which means that imagine now every word is isolated and we want to put a badge or a stamp on every word or every token similar to how let's say if you are getting enrolled in a camp or enrolled in military or any other group activity you are given id that's your roll number or in school all of us have roll numbers right it's similar to that so every token is isolated and then it's assigned a separate token id the way the token id is assigned it's i'm calling it getting your badge the way the token id is assigned is a very interesting process we have a book of token id so you can think of this like a encyclopedia or a book of token ids in this book basically all the possible tokens are listed all the possible tokens are all the possible words and then there is a number which is associated with every word in this book there are not just words there can be characters or there can be even sub words so this book consists of characters like a b up to z it even consists of sub words such as maybe cr can be a word in this vocabulary then it may consist of sub words like isation that can be a sub word and then it also consists of words like let's say token can be a full word in this vocabulary enter can be a full word in this vocabulary begin can be a full word in this vocabulary so you can think of this book of token ids as consisting of characters words and sub words so let me write this down here that's going to be very important this book of token ids essentially consists of it consists of characters it consists of words and it also consists of sub words so as a result essentially we make sure that every token or every word which is isolated it finds certain badge there is no token or no word which is isolated which won't find any badge readers who are familiar with the concept of byte pair encoding remember that to create this book of token id itself there is a certain scheme which is called as a byte pair encoding scheme this is a sub word tokenization scheme and to create this book of token id we use this scheme so gpt2 for example relied on the byte pair encoding mechanism to create its vocabulary this vocabulary is this book of token ids is also called as a vocabulary and then it changes from one large language model to another so let's say gpt2 has a vocabulary of 50 000 gpt4 might have a vocabulary which is higher maybe 100 000 right so based on the llm which we are using the token id which is assigned to let's say this friend might change so i am using a large language model here right now which has a vocabulary size of 50 000 which means that there are 50 000 tokens which might be a combination of characters words and sub words all right then what i am going to do is that i am going to essentially look at this vocabulary and i am going to find where the friend comes into the picture and i am going to find the token id associated with it right so for the word friend the token id which is associated now is 2012 so i am going to note that down so that's the badge or that's the roll number which is now assigned to this token so the roll number assigned to the token friend is now 2012 similarly all the other tokens or all the other words will get a similar badge that's the first step or rather that's phase number two which is token id assignment so now imagine that this token friend which was isolated from its neighbors it has now been given a badge or a stamp which is 2012 that's phase number two i did not go into the details of how this book of token id was created because if you want more details on this there's a separate lecture on creating this vocabulary itself for every large language model and it's called byte pair encoding from scratch it's present in the lecture series build llm from scratch but for now stay with me imagine you are this token friend you have been isolated and now you have been given a badge or you have been given a roll number then you essentially come to phase number three in phase number three something interesting essentially happens in phase number three the you until now you just had one number associated with you right but now you are going to have a huge vector of numbers which are going to be associated with you and this is called as token embedding assignment one way to think of this is that let's say we have an entrance examination which has 768 questions and each question essentially tests a certain feature of you so now we are looking at the word friend right each question will test are you a noun are you a gender are you a verb are you a sport are you an emotion etc we actually don't know what these features or what these questions are but i'm just trying to explain you so that explain to you so that you get an intuition of what token embedding is so imagine there are 768 questions like this which are asked to every token which we have isolated and then based on the answers we get to understand something about that token whether it's a noun whether it's a sport whether it's a adjective whether it's something which appears always at the end of a sentence whether it's something related to gender whether it's something related to monarchy like kings princess queens etc so here we are actually getting to know about the meaning of the token itself in the token id assignment we did not get to know anything about the meaning but in phase 3 in token embedding because we ask a big list of questions we get to know something about the meaning and based on the answers which are given there is a result so every token so now this friend right it will have some values for each of these questions maybe the values are 0.1 0.2 0.1 0.3 etc and if we are assuming that there are 768 questions this will now be a vector of 768 values one thing which i want to point out here is that this number of questions here 768 that vary from one large language model to another large language model so now if you go and and see let's see for gpt2 gpt2 token embedding dimension so if we search gpt2 token embedding dimension we'll see that it's 768 right but here also gpt2 small had 768 but the largest gpt2 had 1600 dimensions so this number of questions 768 actually varies from one large language model to another so here what we are going to do is that we are going to assume that the number of questions is 768 for gpt2 but remember that if the token goes to different llms it might be asked different questions so now imagine you are a token you have been given a badge or a roll number and suddenly you are asked this huge set of 768 questions you respond then your answers are collected in one 768 dimensional vector that is called as the token embedding vector so now along with the badge you also carry your result with you so you have a badge with you and you have this result of 768 values with you that's what has happened to you until this stage right that's the stage of token embedding the difference again between token id is that token id does not carry any notion about semantics whereas token embedding in token embedding assignment will care a lot about the meaning of the word itself the reason token embedding is done is that to create llms you ultimately need to extract meaning right you're teaching something about the language to the model so this is a very crucial step these set of questions or these set of features which are collected about every single token so until now every token has a badge and every token has 768 value result sheet which they take along with them then one more thing which also matters is your position among your neighbors so here if you see a true friend accepts you so friend comes in the middle of the sentence right it comes at position number three over here so a comes at position number one true comes at position two friend comes at position number three accepts comes at position number four and you comes at position number five so the friend is coming at position number three and that position also matters why does the position matter because if you say the dog the dog chased another dog okay if you take a look at this sentence you need to somehow be aware that this dog is basically different than this second dog so if you just take the meanings of the words right as in phase number three we just took the meanings so the token embedding for this dog and this dog will be the same but actually there are two separate dogs and we need to teach the model related to that so the only way to distinguish between this dog and this dog is to know that this comes at position number two and this comes at position number five so as a result it's important to also have some knowledge about the position so similar to the 768 questions which we asked 768 questions will again be asked with respect to the position so remember that although this number varies across different models if you fix a particular language model the number of questions which are asked in token embedding and the number of questions which are asked in the positional embeddings are the same so if we are looking at gpt2 small right now as the model there were 768 questions asked in token embedding similarly 768 questions will be asked in positional embedding and what might these positions or what might these questions be they might be something like are you at the beginning or are you around the middle of the sequence or do

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

