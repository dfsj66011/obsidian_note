
2022.12，GPT 3.5 发布，2024.09.12 发布了首个推理模型 o1。2025 年初，DeepSeek 推出 R1。


### 方法一：推理时计算扩展

**思维链方法（CoT）：**

人类在花更多时间思考时能给出更好的答案，那如果我们让 LLMs 在给出答案前多思考一下，会发生什么？例如：
<img src="https://learnprompting.org/_next/image?url=%2Fdocs%2Fassets%2Fbasics%2Fchain_of_thought_example.webp&w=1920&q=75&dpl=dpl_HJ8sim5PnfxkCQcmtEoWaqMNexSF" width="500">
模型在给出答案前所使用的计算资源量被称为测试时间计算量，基于推理的大语言模型与常规大语言模型相比，测试时间计算量显著更高。但事实证明，如果在测试阶段投入更多的计算资源，并让模型进行更深入的思考，模型对其他问题的推理能力也会随之提升，其准确性将显著增强。

具体来说，这里展示的是通过一种名为 "*思维链提示*" 的简单方法，让 *足够大的语言模型* 自然涌现出推理能力。这篇论文发表于 2022 年，由谷歌研究院的 Brain 团队发布。这篇论文教会我们的是：在输入输出提示中引入思维链有助于模型在复杂推理任务中更好地进行推理。(19000+ 引用)
<img src="https://towardsdatascience.com/wp-content/uploads/2023/03/1OUAjEbFXiQeiNxQlZPAcTQ.png" width="300">
在论文图 4 中，我们观察到三个不同的现象：

1. 思维链提示法在标准提示法的基础上表现显著更优；
2. 模型的准确率会随着模型规模的增大而提高；
3. 对于非常大的 PaLM 模型，其准确率几乎可以与监督微调相媲美。

-------

**zero-shot：**

分两步处理的，首先在问题后面添加：`Let's think step by step.`；将模型的输出回传给 LLMs，追加另一个提示语：`Therefore, the answer (arabic numerals) is`

### 方法二：验证器搜索

验证指的是生成不同答案，然后最终从所有生成的答案中选择最佳答案的过程。比如提出一个问题，然后问题被传递给推理模型。推理模型提供四个不同的答案，分别标记为 A1、A2、A3 和 A4。中间有验证层，用于验证这四个答案中哪一个是最好的，最后给出最佳答案。

当引入验证层时，LLMs 在推理过程中消耗的计算资源将会增加。这正是它被归类为推理时计算扩展的原因——因为推理时间将会延长。

这个验证层，既可以通过人工完成，也可以借助模型（奖励模型）实现。奖励模型经过训练，能够预测在给定问题下，人类评判者会认为哪个生成答案更优质。更具体地说，奖励模型有两种类型：有结果奖励模型，也有过程奖励模型。

结果奖励模型（ORM）的作用是，它们不评估思考过程，基本上只对最终结果进行评分。更受欢迎的是过程奖励模型（PRM），思考过程非常重要。与 ORM 的主要区别在于，它对各个推理步骤进行评分，而不对最终答案进行评分。

paper: [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)

三种不同的验证器：

1. 多数投票，主要缺点是答案可能是错误的，因为未经验证
2. Best of N，从 N 个中选择最佳的，可以使用 ORM，只对最后结果评分，也可以通过 PRM 对过程评分，对路径评分求均值，取均值最高的，论文中主要关注 PRM
3. beam search（1976 年提出的）：beam_num=4， 每层的答案总数量，beam_width=2，每次选 2 个

----------
### 强化学习


这就是奖励信号的含义。第三个概念有点反直觉，被称为价值函数。到这节课结束时，我们将理解价值函数的真正含义，但主要的区别在于...


no.4 30:00  后面视频中音频有问题，无法转录