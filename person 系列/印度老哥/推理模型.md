
2022.12，GPT 3.5 发布，2024.09.12 发布了首个推理模型 o1。2025 年初，DeepSeek 推出 R1。


### 方法一：推理时计算扩展

**思维链方法（CoT）：**

人类在花更多时间思考时能给出更好的答案，那如果我们让 LLMs 在给出答案前多思考一下，会发生什么？例如：
<img src="https://learnprompting.org/_next/image?url=%2Fdocs%2Fassets%2Fbasics%2Fchain_of_thought_example.webp&w=1920&q=75&dpl=dpl_HJ8sim5PnfxkCQcmtEoWaqMNexSF" width="500">
模型在给出答案前所使用的计算资源量被称为测试时间计算量，基于推理的大语言模型与常规大语言模型相比，测试时间计算量显著更高。但事实证明，如果在测试阶段投入更多的计算资源，并让模型进行更深入的思考，模型对其他问题的推理能力也会随之提升，其准确性将显著增强。

具体来说，这里展示的是通过一种名为 "*思维链提示*" 的简单方法，让 *足够大的语言模型* 自然涌现出推理能力。这篇论文发表于 2022 年，由谷歌研究院的 Brain 团队发布。这篇论文教会我们的是：在输入输出提示中引入思维链有助于模型在复杂推理任务中更好地进行推理。(19000+ 引用)
<img src="https://towardsdatascience.com/wp-content/uploads/2023/03/1OUAjEbFXiQeiNxQlZPAcTQ.png" width="300">
在论文图 4 中，我们观察到三个不同的现象：

1. 思维链提示法在标准提示法的基础上表现显著更优；
2. 模型的准确率会随着模型规模的增大而提高；
3. 对于非常大的 PaLM 模型，其准确率几乎可以与监督微调相媲美。

-------

**zero-shot：**

分两步处理的，首先在问题后面添加：`Let's think step by step.`；将模型的输出回传给 LLMs，追加另一个提示语：`Therefore, the answer (arabic numerals) is`



bastian Raska的帖子。

他在那里从头开始完全实现了 Lama 3.2。这里的“从头开始”是什么意思呢？在这些其他例子中，你可以看到我正在从某个地方导入模型或其他东西。所以在这里你可以看到，好吧，我是从某个地方加载模型的，比如从 Hugging Face。但在这里，整个脚本都在我的控制之下，我没有从任何地方加载模型，尤其是那些完全托管在 Hugging Face 上的模型。所以这就像是一个 LLM 是从头开始构建的，Lama 3.2 是从头开始构建的。

我只是想稍微试验一下，所以你可以看到这里，如果我输入“好的”，那么输入就在这里：羊驼吃什么？输出在底部这里：羊驼是草食动物，这意味着……而且这只适用于30亿参数模型，我注意到如果你用10亿参数模型，效果就没那么好。所以当你实现这个笔记本时，当你感觉自己从头实现了LLM时，会有一种满足感。

虽然这段代码并非由你编写，但我们的目标是在此应用零样本提示（zero-shot prompting）和零样本思维链推理（zero-shot chain-of-thought reasoning），尝试观察模型在常识推理和简单算术任务上的表现。因此，这是给你们所有人的另一项有趣作业，它将帮助你们理解如何在并非托管于某处的模型上实现思维链提示，而是在从头构建的模型上实现，并观察它们的表现如何。这是一个非常肥沃的实验土壤，有助于加深理解和培养直觉。

好的，让我们就此结束本次讲座。希望这次讲座对大家来说都非常有趣。在下一讲中，我们将继续讨论模块1，即推理时间计算扩展。非常感谢大家，祝大家度过愉快的一天。大家好，欢迎来到下一讲，关于基于推理的大型语言模型。

今天是我们课程的第三讲，我们将继续讨论推理时间计算扩展的问题。记得在上一讲中，我们讨论了思维链推理和零样本推理，这两种都是提示技术，用于在大型语言模型给出答案之前引导其进行推理。还记得在思维链推理中我们发现，对于更大的模型，思维链推理的表现要比小模型好得多，这一点我们也在Google Colab的实际演示中看到了。

在零短推理中我们看到了什么？在零短思维链推理中，我们不在提示中提供任何输入输出示例，这就是为什么它被称为零短推理。相反，我们只是通过告诉大语言模型在回答之前先思考来推动它进行推理。事实证明，这种巧妙的推动显著优于普通的零短推理，后者没有给出这个要求大语言模型逐步思考的指令。

这两种方法都属于提示技术，这意味着当用户输入提示时，该提示会与输入输出示例对结合，要么通过思维链的方式，要么通过简单的"让我们一步步思考"的陈述。今天我们将了解第二类测试时计算，它被称为验证器搜索，这与普通提示技术有很大不同。

那么让我们开始，并理解什么是针对验证器的搜索的含义。验证指的是生成不同答案，然后最终从所有生成的答案中选择最佳答案的过程。现在我们为什么要讨论验证呢？让我们借助下面的图表来试着理解这一点。

在这张图中我们可以看到，我们在顶部提出一个问题，然后问题被传递给推理模型。接下来发生的是，推理模型不会在输出中提供一个答案，而是提供四个不同的答案，分别标记为A1、A2、A3和A4。现在中间有一个验证层，用于验证这四个答案中哪一个是最好的，最后给出最佳答案。

这被称为验证层，我们将在今天的课程中详细学习这一层。让我们来看一个非常简单的类比。想象一下，你被赋予了一项任务，要在田地里挑选出质量最好的作物。假设田地非常大，你开始逐一调查田地。你发现了一株作物，把它摘下来，然后发现这确实是一株非常好的作物。你会就此停下来吗？不会，对吧？


Generally what we do is that we pluck more and more crops and then we select the best crop out of maybe five or six that we have sampled. That way the probability of getting the best crop increases.That is exactly the case with large language models also. Instead of generating a direct answer, what is usually done is that there are a sample of answers which are generated and then with the help of the verification layer, the best answer out of the samples is chosen and then that answer is shown back to the user. Now let's go back to the title of this lecture, Inference Time Compute Scaling.How does this fit under this title? Well, remember that if we have a verification layer, we talked about test time compute in the first and second lecture. When you have a verification layer, it's going to increase the computational resources which the LLM uses during inference. That is why it comes under inference time compute scaling because the inference time is going to increase.Let's look at it in the form of a diagram. Whenever you are using an LLM, there is a pre-training, then there is fine-tuning and then there is inference. So without verification inference is like this, but when you increase verification, you are basically increasing the time which the LLM takes to give you the answer because you are sampling from different answers and you are selecting the best answer.That's why it increases the inference time compute. Now this verification layer that we looked at, it can either be done by humans or it can be done by a model. The models who do this verification are called as reward models.Why don't you go to Hugging Face and type this reward model. It's called reward model Deberta V3 large V2. What it says is that this reward model is trained to predict which generated answer is better judged by a human given a question.So it's used for model evaluation, reward score in RLHF and detect potential toxic responses via ranking. So this model is trained such that if you give an input to this model, it will give you a score based on how good the generated answer is. So the training data that this model has been trained on was generated by humans itself and then that data is being used to train the model.So we are essentially mimicking the ability of humans to verify answers by replacing humans with a model. So these are called as reward models because the best answer is rewarded with the maximum score. If the answer is not good, then the score will be less and the reward will be less.In the next section, we are going to look at reward models in detail. So more specifically, there are two types of reward models. There are outcome reward models and there are process reward models and the purpose of both these type of models are pretty much explained in their names.So let's look at the first category which is the outcome reward model. So imagine that you have a question and you pass this question to a reasoning model. So the reasoning model is going to give you a chain of thought, it's going to give you the thought process through which it thinks before it gives you the answer.Now what the outcome reward models do is that they don't evaluate these thoughts, they don't care about the thoughts basically, they only care about the final answer and only the final outcome is scored. So here ORM represents the outcome reward model and 0.45 is the score which the ORM gives to the final answer. The next category which are more popular are process reward models and these make more sense because when you are using a reasoning model, the thought process is very important for you.So there needs to be a mechanism which judges not just the final answer but also the individual thoughts which are leading to the final answer. So here what you see is that thought 1 is sent to the process reward model and then the model is scoring the thought, thought 2 is again sent to the process reward model and the model is again scoring the thought. So here the main difference compared to the ORM is that individual reasoning steps are scored and the final answer is not scored.So you are assuming that if the reasoning steps are correct, obviously the final answer will also be correct. So you are giving a lot of emphasis on the chain of thoughts which are leading to the final answer. That is why it is called as a process reward model.So now let us look at a practical example of a process reward model and how does a process reward model work in practice. So let us take a simple example, let us say the example which we are considering is Roger has 5 tennis balls and he buys 2 more cans of tennis balls. Each can has 3 tennis balls, how many tennis balls does he have now? So like we saw in the last lecture, we are going to use a chain of thought reasoning to come up with the final answer.So we have 3 steps. Step 1 is the reasoning LLM says Roger started with 5 balls. Step 2 is 2 cans of 3 tennis balls, each is 5 tennis balls and step 3 is so the total number of tennis balls is 5 plus 6 which is 11.Now read these and just try to understand which one is correct and which one is not correct. The first one is correct Roger starts with 5 tennis balls so you get a higher score. The second one 2 cans of 3 tennis balls should be totally 6 tennis balls not 5. So this one gets a lower score see and then so the total number of tennis balls the final answer is correct.So this one gets a higher score. So wrong reasoning gets a lower reward. The main advantage of using verifiers is basically there is no need to fine tune or retrain the LLM which you are using to answer the question.So whenever we are working with reasoning models whichever reasoning models you see in practice whether it's O1, deep seek etc. it is likely that these reasoning models before they give you an answer a verification layer has been added where there is a process reward model that is evaluating the answers and it is giving you the answer which has the best possible reasoning path. So now let's move ahead and we will look at the different types of verifiers.Broadly speaking we will look at 3 different types of verifiers we will look at majority voting as the first type of verifier then we will look at best of n and then finally we will look at beam search. All of these are very intuitive and simple to understand but we are going to have a look at these 3 types in detail so that you understand the intuition behind them. So the first one let's look at it the first type of verifier is called as majority voting and before we go ahead I want all of you to download this paper which I have enjoyed thoroughly there might be some aspects of this paper which you do not understand but I hope this paper will if you have gone through the first 3 lectures I hope you will understand some aspects of this paper and really appreciate this paper because this paper specifically discusses about test time compute.So you will be able to understand the theory which has been outlined in this paper. Okay so after you have downloaded this paper we will start to now discuss about the 3 main types of verifiers the first verifier is called as majority voting and this is clearly the most intuitive in majority voting what happens is that there is not even a verifier used actually what happens is let's say you probe an LLM to generate samples of answers let's say there are 10 answers generated by the LLM and you pick the answer which appears maximum number of times that's all. So something similar to this let's say you ask the question are the number of prime numbers finite and then you give it to the reasoning model and the reasoning model generates 6 chains of thoughts which you are not concerned about at all you just look at the final answer okay so you can see that there are 2 yeses but there are 4 noes so what's the majority the majority is no.So we will choose the majority in this case and the final answer we will pick as no. So this method is also called as self consistency but this is probably the first thing which came to the mind whenever people when people thought of a verifier is that let's sample a lot of answers and pick the answer which comes the maximum number of times. Okay so this is the first answer and now here mind you the main drawback of this method is that the answer can be wrong because you are not using any

any verifier to evaluate the answer. So there is no way to know whether the answer is correct or not, but we are simply taking the majority and that concern is actually addressed in the next method which is called as best of n samples. So here again what happens is that there are n samples which are generated by the LLM, but the difference from the previous method is that you apply a verification layer here.

You don't directly choose the majority but you apply a verification layer and then you choose the answer which the verifier is giving the maximum score to and this can either be done by a output reward model or a process reward model. So let us look at both these examples now at the bottom. So the question that we have asked is how many moons does Jupiter have? It is passed through a reasoning model and then we have generated six chains of thoughts t1, t2, t3, t4, t5, t6.

So as we are looking at ORM here we are not concerned with the chains of thoughts at all. We are only concerned with the final answer. These final answers which are 90, 1, 95, 82, 76 and 14 they are passed through the output reward models and then this verifier scores all these answers 0.6, 0.2, 0.9, 0.7, 0.3, 0.5 and then you pick the answer with the best score.

So the number of moons in Jupiter which Jupiter has is 95 by using the ORM this is the answer we get. Now when you look at PRM in PRM what we do is we do not score the final answer like we do in the previous step but let us imagine that this is a question and we pass this question through to the reasoning model and here we are evaluating three chains of thoughts which are t1, t2 and t3. So t1 goes down to t11, t2 goes down to t21 and t3 goes down to t31.

Both these reasoning steps are passed through the PRM and we are getting scores 0.1 and 0.2 and we are doing a weighted average. So it is 0.15 here again we are doing an average which is 0.6 and here we are doing an average which is 0.55 and now we are not even bothered about the final answer which the reasoning model is giving because the verification layer only verifies the reasoning steps which are appearing before the final answer. So from these three we can see that this answer has the maximum score so that is why we will pick this answer.

So the answer with the best average is finally selected as the final answer. So this is using PRM for best of n sampling and then you can also imagine an example for this which looks very similar to the above example but the primary motive behind showing you both these methods is to understand the differences between the two and you can see in this paper also which I have shared before in this paper they have predominantly focused on process reward models and these models are more effective and they are more frequently used in practice by researchers. Ok now we will go to the next algorithm or next verifier in this case which is using the technique of or we can call it verification using the algorithm of beam search.So the beam search algorithm was originally proposed in the year 1976 and it was proposed in this paper which is shown below. The title of the paper is the Harpy speech recognition system and this paper actually came out in the year 1976. So this algorithm was first used in the speech recognition domain and here you can see that the search strategy the name of the search strategy is a few best paths in parallel.

So this is quite an ancient algorithm we can say it is not an algorithm which is developed by LLM researchers or AI researchers. It was developed way back around 50 years back we can say where the name was not even given as beam search but the algorithm was very similar to the modern algorithm and it was used for speech recognition tasks. So beam search continued to be used for speech recognition and translation tasks in subsequent years.

It was a popular algorithm you can even go to wikipedia and search about it you can see that the references date back to earlier years. So why are we discussing beam search in this course of reasoning models and particularly how can we construct an effective verifier using the algorithm of beam search. So let's look at that in detail.

So first we will understand how the traditional beam search algorithm works and so that your intuition is developed and then we will apply that algorithm to construct a verifier which is very effective and then finally we will run a hands-on a google colab notebook which will give you a practical understanding of how beam search is used in actual LLMs for verification layer. Okay so we will take a very simple example which is a sentence in Hindi. So the sentence says that I am going to Delhi.

This is a sentence in Hindi which translates to English as I am going to Delhi and this is the task that I want to do and now I am going to explain to you about beam search algorithm specifically applied to this task. So firstly we have this text corpus which is let's say around 50,000 words and we can only use these words in the translation tasks. We cannot use any other word when we are translating this sentence from Hindi to English.

Okay so the approach that we are going to take is we are going to translate this sentence word by word. So now let's look at it. Step 0 is we are given a text corpus a list of 10,000 words or we can even call it say 50,000 words and we have to use only the words in the text corpus for the translation task.Now the next step is out of these 50,000 words we will select the most probable three words for the first word in the translated sentence. So if we go through all these words we have selected I, we have selected Delhi and we have selected Zoo. We have eliminated all other words.You might ask the question how are these selections made? These selections are made via an encoder decoder model but we are not concerned about how these selections are made right now. Imagine that there is a model which is making this selection. We are going to understand about the beam search strategy.

This model can be anything it can be a black box for all we know. So we pass this text corpus to the model and we select the three words which have the maximum probability that is step number one. So these words are I, Delhi and Zoo.Now the interesting thing comes in step number two. In step number two what we do is we pass these three words again through the entire corpus and now we find the next three combinations of two words which have the maximum probability. So now how many combinations we have to see let's say 10,000 plus 10,000 plus 10,000.So now we have to evaluate sorry this is 50,000 50,000 plus 50,000 plus 50,000 out of 1,50,000 combinations we have to choose the best three combinations of the first two words. So you can see here that we have eliminated I was, we have eliminated Delhi is, we have eliminated all others and we have selected only three again. We have selected I am, we have selected I love and we have selected Zoo is.

Remember that in the first step we passed 50,000 words in the second step we are searching across 1,50,000 entries and then we are again choosing the three most probable entries by passing it through the model which is a black box for all we know right now. So step number two is for each of these words we will select the most probable combination of first word and second word and select the best three. So these are I am, I love and Zoo is.Okay so now one thing you can immediately notice from this is that there is no Delhi here so you can actually completely eliminate this Delhi. So that reduces the chances of first word from three to only two that is the first word can either be I or Zoo it cannot be Delhi. Okay now in the next step what we do is we again pass these combinations of three to 50,000, 50,000 and 50,000.

So again we evaluate across 1,50,000 combinations and now we choose the next best possible three which can be let's say I am, I am going. So these after this here we again have to pick three. So let's say I am going, I love Delhi and Zoo is green.

We select these or let's not or third one is I love Zoo. So now you can see here that Zoo is not coming anywhere first. So this means that we can eliminate Zoo and now we fix our first word which is I and then this process will continue till we have translated the entire sentence.

So beam search is essentially a technique. So now here you can see that why did we select three at every step we were saying let's select three, let's select three. Three is called as the beam width.

So instead of selecting just one, we are selecting three in each layer then we are proceeding ahead then again we are selecting three in the next layer etc. So three in this case is called as the beam width and I think the name beam search came because it's like imagine there is a huge library and you are shining a torch in that library and there is a beam which is created through that torch shining and that beam falls on three books or three letters or three words. So here I think it's like that right.

It's like it's branches into three. So from a huge corpus it branches into three then again it branches into three. So probably it widens your search scope and that's why it's called as beam search.

But this is the exact algorithm which was used in the original paper which came out in 1976 and subsequently it has been a popular model in speech recognition and tasks. Now for us and for this course we are going to look at how beam search can be combined with a reasoning large language model and a reward model to search many different reasoning paths and come up with an optimum answer. So in this case we searched across many different words right.

But here when we are looking at reasoning models we are going to use beam search to explore multiple reasoning paths before we come to the final answer. I hope the intuition is clear for all of you and with this intuition in mind now we are going to move towards understanding the beam search algorithm and how can we develop a verifier which uses a beam search algorithm to effectively search across multiple reasoning paths. Ok so now there are basically two parameters here that we are going to look at.

The first parameter is the number of beams and the second parameter is the beam width. Ok so there are two parameters which we are going to look at in this example. The number of beams we are going to fix as 4 and the beam width we are going to fix as 2. So let's understand now how we construct our reasoning paths using the beam search algorithm.

Now because the total number of beams we have set to 4 in the first step what the model will do is that the model will generate 4 different thoughts which are given here 1, 2, 3 and 4. Ok so in the first step the model has generated 4 different thoughts. Now what we will do is out of these 4 thoughts only 2 will be selected. Why 2 will be selected? Because the beam width is 2. Remember how in the previous example also we selected 3 in each layer exactly with the same intuition we are going to select 2 in each layer.So these two are you know marked in green here. So we are going to eliminate thought number 1 and thought number 3 and we are going to only choose thought number 2 and thought number 4. The purple background here is the verifier. So it is because it has gone through a process a reward model that it has scored each of these steps maybe this step this thought got 0.15, this thought got 0.8, this one got 0.3 and this one got 0.7. So that is why we choose the best 2. This is the first layer.

Now what we will do is from both of these we will select 2 each. So in the next layer we will have 2 into 2 that is 4. In each layer we need to have 4 different thoughts because the number of beams are 4. So now what we will do from each of these we will select 2 more. So in the first layer we had 4 and now in the second layer also we have 4, 1, 2, 3, 4. From this we have branched 1 and 2, from this again we have 1 and 2. So this part is clear and then again the process repeats out of these 4 because the beam width is 2 we will select the best 2. So probably this got a score of 0.9, this one 0.1, 0.2 and 0.7. So after the verifier has scored them we can see that 1 and 4 have the maximum score.

So we will select 1 and 4 and the process continues. So finally you get a diagram which looks like this. So here the question is Roger has 5 tennis balls, he buys 2 more cans of tennis balls and each can has 3 tennis balls.

How many tennis balls does he have now and this is the first layer, this is the second layer, this is the third layer and this is the fourth layer. You can see that in the first layer we have selected 2, in the second layer out of 4 we have selected 2, in the third layer again we have selected 2 and in the last layer we have selected 1 which is the final solution, which is the full solution it's marked in the complete. So the color is complete inside, it's not an empty color which indicates intermediate solution.

So this is the diagram where number of beams are 4 and the beam width is equal to 2. Because the beam width is equal to 2 we only selected 2 at each step. So this is how beam search allows us to effectively search all the reasoning paths. We are not just scoring each reasoning path and then taking the average now, this is a bit advanced.

What we are doing is that we are selecting the best at each and then we are completely neglecting others. Then from there we are again branching out and this kind of beam search technique when applied in the reasoning models it is shown to increase the accuracy of the answers. So this is how beam search can be used to explore multiple reasoning paths and then finally come up with the best answer.

And now in the next step we are going to look at the hands-on implementation of beam search with process reward models. Again all of these are process reward models because we are scoring each step, we are not just scoring the final answer. So that's why we have employed process reward models in this methodology.Okay now what we are going to do is that we are going to write a code which gives us a beam search tree like this and we are going to give an exactly same prompt which we have given over here and then you can see that in this layer there are 4 again in this layer there are 4, 4 and 4 because the number of beams are 4 and the beam width are 2. So we are choosing 2 in each this 2, this 2 and this 2 and the 2 that we are choosing in each layer let's look at this layer. So we are choosing the thoughts which have the maximum PRM score. So here you can see that green and yellow form lie somewhere here whereas purple lies somewhere at the bottom.

So the model does not choose that, the model only chooses thoughts which has the maximum color. And I am going to show you a Google Colab notebook which I have written which can give you a beam search tree like this. I will share this link in the chat.

Again I have used a A100 GPU here to run this code since I have a Google Colab pro subscription and these are the different steps which I am going to explain to you right now. The first step is to install the dependencies and here we are also installing this pygraph wiz because we want to plot this final graph at the end and because of that we are installing network x and matplotlib.pyplot as well. Then after that what we do is we load our reasoning model.

We are using this Zephyr 7 billion parameter reasoning model for our case and you can refer back to the code which we saw in the last lecture where we had this exact same block of code. So now this block of code repeats. Whenever you are loading your reasoning model the block of code remains the same.

And now we are loading our process reward model. I am using this process reward model here which if you copy paste you will see it's posted on hugging face. So this is the process reward model I am using for for this Google Colab notebook which is called a reward model Deberta v3 large v2.

So this is the reward model which has been trained from human feedback. It can also be used for toxic response detection and some other tasks as well. After the process model has been reward has been loaded.We are now using the ORM. This is called ORM here but actually it's PRM. So we are converting this ORM to calculate scores for individual steps.

So we need to write a piece of code which performs stepwise coding. So that's why we call this function as stepwise PRM score. And then this particular block of code is used for this particular block of code is used to actually write the beam search algorithm.

So here I have the same notation I have n for the total number of beams and I have m for the beam width and the maximum steps which are the different layers are three here and this is what is shown here because it's three we have only one two and three. We don't go below that. You can play around with these parameters and see how the graph changes as well.

Okay so this is how the graph is plotted and then finally for visualizing the beam search tree we define a function called plotTraceGraphTreeClean and then this is where we enter our PROM. Roger has five tennis balls. He buys two cans of three tennis balls each.How many tennis balls does he have now? So here you know you can see the scores which the reward model gives for different beams. So you can see that here we do not have much variation in the answers actually. So all of these beams are giving the same answer which is 11 which is the correct answer.

One of the techniques which they have mentioned in the paper is that if you change the temperature of the model you get a different you get different variations in the answers and that allows you to explore the answer space much better. So here we are not doing any kind of exploration as such because we are not varying the temperature variable and also because probably our model size is also smaller that we are not able to see that effect clearly. So the temperature we have set to 0.9 which is which is pretty high.

Initially I had set it to 0.7 but then I increased it because I wanted to see the variation in the answers which are given by these different beams. So you can see that we have a final beam 1 answer, we have beam 2 answer, we have beam 3 answer and we have beam 4 answer and then all of these are actually same. So here we are not able to see that difference essentially but you can see exactly how the beam search code can be written along with the reward model to score you know the reasoning steps and then select the best to move to the next layer, select the best to again move to the next layer and then select the final answer.So I encourage you to try out different plots, different prompts, different number of beams, beam width and then play around with this code a little bit and also try to see what kind of variations we can do to get different answers in whenever you are looking at these final answers. Now another thing I want to highlight is that the paper which I was mentioning before. Okay so they mention so the way that paper is written is that you will so this is the paper I am talking about.

In this paper they divide the paper into two categories. One category is of course verifiers which you will be able to understand after going through this lecture properly. The second category which they mention is modifying proposal distribution.

So verifiers is where you have a verification layer which grades your answers but proposal distribution is modifying the answers which are generated in the first place. So what comes under proposal distribution is chain of thought reasoning which is few short as well as zero short chain of thought reasoning. These are examples of strategies, prompting strategies which can modify the proposal distribution.

So you can essentially look at these two methods as this is the sampling method which you know samples different answers or generates different answers and this is the method which grades different answers. So whenever you are using this technique of you know building a reasoning model by leveraging inference time compute scaling then you can either modify proposal distribution or you can implement a verification layer inside. Both of these are viable techniques.

In the previous lecture we looked at how you can modify the proposal distribution by prompt strategies which is a chain of thought prompting and zero short chain of thought prompting and then the second method is implementing a verifier along with the reasoning model. So the verifier can be using beam search, majority voting or best of n. Any of the three options can be used. Okay so this brings us to the end of the lecture.

I expect all of you to implement this Google Colab notebook and I expect all of you to download and start reading this paper also. This paper will strengthen your knowledge of inference time compute scaling and it will act as an effective culmination of the first three lectures that we have seen so far. The next lecture we will start with some items such as a tree of thought reasoning and Monte Carlo search.

We will look at both these techniques very briefly and then we will start with the next method of building reasoning models which is learning about reinforcement learning and we will start learning RL from the complete basics. Please let me know in the comments if you have any doubts if you are following this course so far because that really gives us a push ahead and to keep going with these lectures. Thank you very much everyone.

Bye. Hello everyone and welcome to the fourth lecture in the series on reasoning based large language models. In today's lecture we will start a very very interesting topic which is reinforcement learning.

So far in the course we have covered the techniques which come under the category of inference time compute scaling which means that we are not touching the trained model at all.