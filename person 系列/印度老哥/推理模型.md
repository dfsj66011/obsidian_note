
<<<<<<< HEAD
(1) *Inference-Time Compute Scaling：* 如果在推理过程中给予更多时间，模型就有更多时间进行思考，从而生成更好的答案。目前人们已经观察到，如果增加测试时的计算量，模型的准确性通常也会随着测试时计算量的增加而提高。因此我们也可以说，模型的表现会随之提升。一般来说，测试时间的计算可以分为两大类：第一类称为提示；第二类则是验证器（verifiers）。

(2) *Pure Reinforcement Learning* Wait, wait. Wait. That's an aha moment I can flag here. 我们将在此介绍的概念包括：什么是强化学习（RL）、强化学习的要素、理解智能体、环境、奖励、回报、马尔可夫决策过程、马尔科夫性质、价值函数、最优价值函数、策略评估、策略迭代、策略与价值迭代。随后我们将探讨蒙特卡洛方法、时序差分学习以及近似求解方法。第一部分是经典强化学习。第二部分是现代强化学习，我们将涵盖信任区域策略优化，然后我们会讲到近端策略优化，最后我们会讲到GRPO，即群体相对策略优化。

(3) *Supervised Fine-Tuning and Reinforcement Learning* 人们认为 OpenAI 的 o1 模型很可能是用这种方法制造的。他们没有也不会公布 o1 的架构，但它很可能是用这种方法制造的，而这种方法也是 DeepSeek R1 最终采用的。所以在文献中，这被称为构建推理模型的蓝图。

(4) *Pure Supervised Fine-Tuning and Distillation*

-----------

**推理时间计算扩展 —— 思维链（2022, Google）：**

事实证明，如果要求大语言模型在给出最终答案之前生成一系列思考步骤，它在复杂推理任务上的准确性也会提高。因此，接下来介绍的第一个技术就叫做"思维链推理"。

思维链提示的准确性实际上随着模型规模的增大而提高，这意味着越大的模型越擅长推理，并且具有很高的准确性。如果你观察非常大的 PaLM 模型，它们几乎可以与最好的监督微调模型相媲美。这意味着，无需经历计算成本高昂的监督微调任务，我们只需使用思维链提示技术，就能在算术推理数据集（GSM8k）上达到相同的准确率。*思维链推理能力是随着模型规模扩大而涌现的新能力，思维链推理实际上只对非常大的模型才有效。*


**推理时间计算扩展 —— 零样本推理**：

这种方法甚至比思维链推理更为简单。零样本意味着我们在提示阶段完全不提供任何示例。取而代之的是我们只写上“让我们一步步思考（Let‘s think step by step.）”，然后将 LLM 的输出附加在之前的 prompt 中，并加入 “因此答案是（Therefore, the answer (arabic numerals) is）”，这是要给 *两阶段* 的零样本思维链过程。

有意思的是，通过实测比较，这种方法在较小的模型上是相对而言最好的方式，即使在 7B 左右的模型上也是，而且相对而言 few-shot CoT 示例数量为 10 也不太行。（不过模型测试的较少，或许也与 few-shot 质量相关？不管怎么说，这是一种性价比很高的方式，无需准备示例，效果还不错。）

> 注：老哥案例中使用的是 `microsoft/phi-2` 和 `openchat/openchat-3.5-1210` 模型，非常弱，如果细看其输出结果，根本就没有在回答我们的问题，反而是在模仿我们的输入或非常随机的内容，尤其在 few-shot 中就是在仿写案例，这应该是模型没有做很多的监督训练，反而 zero-shot 等方式干扰更小，因此这里的结论不一定是真实情况。

----------

**推理时间计算扩展 —— 验证器 Verifiers（Beam Search）**：

验证指的是生成不同答案，然后从所有生成的答案中最终选出最佳答案的过程。中间有一个验证层，它会验证这 N 个答案中哪一个是最好的，最后给出最佳答案。当增加验证环节时，本质上是在延长大语言模型给出答案所需的时间，因为需要从不同的答案中进行采样，并选择最佳答案。

这个验证层，既可以通过人工完成，也可以通过模型完成。执行这种验证的模型被称为奖励模型。这个模型的用途是：在给定一个问题的情况下，经过训练后能预测出人类认为哪个生成的答案更好。具体来说，奖励模型分为两种类型：结果奖励模型（ORM）和过程奖励模型（PRM）。结果奖励模型的作用是不评估这些思考过程，它们基本上不关心这些思考，只关心最终答案，并且只对最终结果进行评分。

更受欢迎的是过程奖励模型，这些模型更有意义，因为当你使用推理模型时，思考过程对你来说非常重要。因此，需要建立一种机制，不仅要评判最终答案，还要评判导致最终答案的各个思考步骤，与结果奖励模型的主要区别在于，这里对各个推理步骤进行评分，而不对最终答案进行评分。

总的来说，我们将探讨三种不同类型的验证器：

* 多数表决，实际上甚至没有使用验证
* n 选优验证器，应用一个验证层，然后选择验证器（ORM or PRM）给出最高分的答案
* 最后是 beam search 验证器，例如 beam 为 4，即首先生成 4 个不同的链，width 为 2，即利用验证器选择其中 2 个，然后继续每个链产生 2 个新的推理步骤，最后一步选择最高的即可。

beam search 的优势在于能高效探索所有推理路径——我们并非简单地对每条路径评分后取平均值，而是采用更高级的策略：在每一步仅保留最优路径，完全舍弃其他可能性，再从保留路径继续分支出新的可能性。研究表明，当这种束搜索技术应用于推理模型时，能显著提升答案的准确率。


### 强化学习


而它做出这些决策的参数是当前的电池电量，以及过去为我充电所花费的时间，或是过去找到充电器所花费的时间。因此，Roomba机器人在你的房子里移动，收集垃圾，并根据过去的经验以及当前的电池电量实时做出决策。第四个例子是菲尔准备他的早餐，你也可以想象在这种情况下准备你的早餐。

每当你在准备早餐时，你都在进行一系列动作：走向橱柜、打开橱柜、挑选麦片盒、伸手去拿麦片盒、抓住并取出盒子、拿盘子、拿勺子——这个过程中的每一个步骤都涉及一系列获取信息的眼球运动。然后每一个行动都由一个目标引导，而这个目标又服务于其他目标，最终的目标是获取营养。例如，你为什么要拿勺子？因为勺子能帮你吃饭。所以拿勺子的行为服务于帮助你吃饭的目标，而吃饭的目标又反过来服务于获取营养的目标。

因此，当你准备早餐时，你正在采取一系列行动，而你甚至没有意识到这些行动，所有这些行动都指向一个共同的回报，即获得营养和满足你的饥饿感。这四个例子激发了强化学习领域的发展，现在我们将通过这些例子，尝试从一个共同的结构角度来理解它们，这将帮助我们以正确的方式思考这些问题。

目前我已经向你介绍了强化学习与监督学习的几点不同之处，以及推动强化学习领域发展的各种实例。现在，让我们尝试为我们的对话构建一个框架。这些例子有什么共同点？所有这些例子的共同点是都有一个积极的决策主体。这个主体与环境互动以实现目标。例如，在第一个例子中，决策主体是棋手；第二个例子中是瞪羚；然后是移动机器人；最后是菲尔。

现在所有这些智能体都在与周围的环境互动，并且它们都在努力实现一个目标。那么这些案例中的目标分别是什么呢？对于棋手来说，目标是战胜对手；对于瞪羚来说，目标是站起来并快速奔跑；对于移动机器人来说，目标是收集最多的垃圾；而对于菲尔来说，目标是获取营养并满足饥饿感。现在有一个智能体和一个目标，但为了实现这个目标，智能体必须穿越环境，而每种情况下的环境都是不同的。

对于棋手来说，环境就是棋盘；对于瞪羚来说，环境是周围的自然环境，还包括帮助瞪羚做出决策的内部环境。那么，这个内部环境是什么呢？内部环境也是代理的记忆和偏好。因此，在大多数情况下，代理和环境之间并没有明确的界限。

例如，对于移动机器人，你可能会认为机器人是智能体，而机器人之外的一切都是环境。但事实并非如此，因为机器人内部的电池也是环境的一部分，因为智能体做出的决策取决于电池剩余的电量水平，尽管电池位于机器人内部。

同样，当菲尔准备早餐时，周围的环境当然是厨房，但还有一个内部环境，那就是他过去的记忆和经历。例如，过去他拿勺子时是否在某个地方伤到自己，所有这些经历都会在菲尔做决定时被考虑进去。通常情况下，环境与智能体之间并非泾渭分明，而是部分存在于智能体内部，但大多数情况下仍属于外部范畴。这些案例的共同点在于：都存在一个主动决策的智能体，它不断与环境交互，并朝着最终目标推进。这就是我们理解所有强化学习问题的通用框架。

现在想象一下这与监督学习有多么不同。假设你想让一只瞪羚或菲尔学会准备早餐，如果将其视为监督学习问题，你需要提供多少标签？甚至很难用这种结构来思考这个问题，因为你需要做出某种决定，比如伸手去拿橱柜里的东西，那么你该如何为这类动作打标签呢？

因此，这类问题很难用监督学习问题的结构来定义，因为不同的类别会根据其具体内容被赋予不同的标签。因此，即使是在无监督学习的框架下思考这些问题也很困难，这就是为什么需要一个完全不同的独立类别来理解这类问题——即存在一个与环境互动的智能体。也许这个智能体正在发现某些隐藏的结构，但这只是整个过程中的一部分。

例如，羚羊可能找到了某种跑得更快的方法。所以在某种程度上，它试图发现一些隐藏的结构，但最终目标是跑得更快——最终目标不仅仅是识别某种结构，而是跑得更快。这就是它与无监督学习问题类别的不同之处。

因此，在所有这些例子中，智能体通过与环境互动，利用其经验逐步提升自身表现。强化学习问题的一个独特之处在于：智能体与环境互动的次数越多，随着时间的推移，它的表现就会越好。你能想到这是如何实现的吗？让我们以瞪羚本身为例来说明。

起初，羚羊试图奔跑却摔倒了，它甚至挣扎着站起来。但随着羚羊不断奔跑、不断练习，最终它开始慢慢跑起来，最后终于能够快速奔跑。除非它与周围环境有更多次的互动，否则它不可能跑得快。所以每次你与环境互动时，你会变得越来越好，这是我们所有人都有过的体验。当我们试图掌握一项技能时，突然有一天我们会意识到，我在这项技能上比两个月前进步了很多，这都是因为我进行了大量的练习。

这正是我们在讨论强化学习问题结构时所看到的情况。现在每当人们谈论强化学习方法时，强化学习系统都有四个主要元素。第一个元素是策略。该策略定义了智能体在特定时间的行为方式，从形式上看，策略是一种从感知到的环境状态到在这些状态下所采取行动的映射。简单来说，策略就是当处于某种情境中时，决定如何行动的规则。因此，策略将决定我在该情境中的行为方式。

接下来是奖励信号。奖励信号定义了强化学习问题中的目标。在每个时间步，环境会向强化学习智能体发送一个数字，即奖励，而智能体的唯一目标就是最大化其随时间累积获得的总奖励。因此，奖励类似于生物系统中的愉悦或痛苦。如果你用手触摸沸腾的容器，你的身体会立即给你一个负面的奖励信号，由于你收到的信号是负面的，你将来就不会再执行这个动作。同样地，假设一个国际象棋选手做了一系列的移动然后输了，他会立即得到一个负面的信号。

因此，他将来不会再执行那一系列步骤。这就是奖励信号的含义。第三个概念有点反直觉，被称为价值函数。在这节课结束时，我们将理解价值函数的真正含义，但主要的区别在于。


(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)
=======
2022.12，GPT 3.5 发布，2024.09.12 发布了首个推理模型 o1。2025 年初，DeepSeek 推出 R1。


### 方法一：推理时计算扩展

**思维链方法（CoT）：**

人类在花更多时间思考时能给出更好的答案，那如果我们让 LLMs 在给出答案前多思考一下，会发生什么？例如：
<img src="https://learnprompting.org/_next/image?url=%2Fdocs%2Fassets%2Fbasics%2Fchain_of_thought_example.webp&w=1920&q=75&dpl=dpl_HJ8sim5PnfxkCQcmtEoWaqMNexSF" width="500">
模型在给出答案前所使用的计算资源量被称为测试时间计算量，基于推理的大语言模型与常规大语言模型相比，测试时间计算量显著更高。但事实证明，如果在测试阶段投入更多的计算资源，并让模型进行更深入的思考，模型对其他问题的推理能力也会随之提升，其准确性将显著增强。

具体来说，这里展示的是通过一种名为 "*思维链提示*" 的简单方法，让 *足够大的语言模型* 自然涌现出推理能力。这篇论文发表于 2022 年，由谷歌研究院的 Brain 团队发布。这篇论文教会我们的是：在输入输出提示中引入思维链有助于模型在复杂推理任务中更好地进行推理。(19000+ 引用)
<img src="https://towardsdatascience.com/wp-content/uploads/2023/03/1OUAjEbFXiQeiNxQlZPAcTQ.png" width="300">
在论文图 4 中，我们观察到三个不同的现象：

1. 思维链提示法在标准提示法的基础上表现显著更优；
2. 模型的准确率会随着模型规模的增大而提高；
3. 对于非常大的 PaLM 模型，其准确率几乎可以与监督微调相媲美。

-------

**zero-shot：**

分两步处理的，首先在问题后面添加：`Let's think step by step.`；将模型的输出回传给 LLMs，追加另一个提示语：`Therefore, the answer (arabic numerals) is`



bastian Raska的帖子。

他在那里从头开始完全实现了 Lama 3.2。这里的“从头开始”是什么意思呢？在这些其他例子中，你可以看到我正在从某个地方导入模型或其他东西。所以在这里你可以看到，好吧，我是从某个地方加载模型的，比如从 Hugging Face。但在这里，整个脚本都在我的控制之下，我没有从任何地方加载模型，尤其是那些完全托管在 Hugging Face 上的模型。所以这就像是一个 LLM 是从头开始构建的，Lama 3.2 是从头开始构建的。

我只是想稍微试验一下，所以你可以看到这里，如果我输入“好的”，那么输入就在这里：羊驼吃什么？输出在底部这里：羊驼是草食动物，这意味着……而且这只适用于30亿参数模型，我注意到如果你用10亿参数模型，效果就没那么好。所以当你实现这个笔记本时，当你感觉自己从头实现了LLM时，会有一种满足感。

虽然这段代码并非由你编写，但我们的目标是在此应用零样本提示（zero-shot prompting）和零样本思维链推理（zero-shot chain-of-thought reasoning），尝试观察模型在常识推理和简单算术任务上的表现。因此，这是给你们所有人的另一项有趣作业，它将帮助你们理解如何在并非托管于某处的模型上实现思维链提示，而是在从头构建的模型上实现，并观察它们的表现如何。这是一个非常肥沃的实验土壤，有助于加深理解和培养直觉。

好的，让我们就此结束本次讲座。希望这次讲座对大家来说都非常有趣。在下一讲中，我们将继续讨论模块1，即推理时间计算扩展。非常感谢大家，祝大家度过愉快的一天。大家好，欢迎来到下一讲，关于基于推理的大型语言模型。

今天是我们课程的第三讲，我们将继续讨论推理时间计算扩展的问题。记得在上一讲中，我们讨论了思维链推理和零样本推理，这两种都是提示技术，用于在大型语言模型给出答案之前引导其进行推理。还记得在思维链推理中我们发现，对于更大的模型，思维链推理的表现要比小模型好得多，这一点我们也在Google Colab的实际演示中看到了。

在零短推理中我们看到了什么？在零短思维链推理中，我们不在提示中提供任何输入输出示例，这就是为什么它被称为零短推理。相反，我们只是通过告诉大语言模型在回答之前先思考来推动它进行推理。事实证明，这种巧妙的推动显著优于普通的零短推理，后者没有给出这个要求大语言模型逐步思考的指令。

这两种方法都属于提示技术，这意味着当用户输入提示时，该提示会与输入输出示例对结合，要么通过思维链的方式，要么通过简单的"让我们一步步思考"的陈述。今天我们将了解第二类测试时计算，它被称为验证器搜索，这与普通提示技术有很大不同。

那么让我们开始，并理解什么是针对验证器的搜索的含义。验证指的是生成不同答案，然后最终从所有生成的答案中选择最佳答案的过程。现在我们为什么要讨论验证呢？让我们借助下面的图表来试着理解这一点。

在这张图中我们可以看到，我们在顶部提出一个问题，然后问题被传递给推理模型。接下来发生的是，推理模型不会在输出中提供一个答案，而是提供四个不同的答案，分别标记为A1、A2、A3和A4。现在中间有一个验证层，用于验证这四个答案中哪一个是最好的，最后给出最佳答案。

这被称为验证层，我们将在今天的课程中详细学习这一层。让我们来看一个非常简单的类比。想象一下，你被赋予了一项任务，要在田地里挑选出质量最好的作物。假设田地非常大，你开始逐一调查田地。你发现了一株作物，把它摘下来，然后发现这确实是一株非常好的作物。你会就此停下来吗？不会，对吧？


Generally what we do is that we pluck more and more crops and then we select the best crop out of maybe five or six that we have sampled. That way the probability of getting the best crop increases.That is exactly the case with large language models also. Instead of generating a direct answer, what is usually done is that there are a sample of answers which are generated and then with the help of the verification layer, the best answer out of the samples is chosen and then that answer is shown back to the user. Now let's go back to the title of this lecture, Inference Time Compute Scaling.How does this fit under this title? Well, remember that if we have a verification layer, we talked about test time compute in the first and second lecture. When you have a verification layer, it's going to increase the computational resources which the LLM uses during inference. That is why it comes under inference time compute scaling because the inference time is going to increase.Let's look at it in the form of a diagram. Whenever you are using an LLM, there is a pre-training, then there is fine-tuning and then there is inference. So without verification inference is like this, but when you increase verification, you are basically increasing the time which the LLM takes to give you the answer because you are sampling from different answers and you are selecting the best answer.That's why it increases the inference time compute. Now this verification layer that we looked at, it can either be done by humans or it can be done by a model. The models who do this verification are called as reward models.Why don't you go to Hugging Face and type this reward model. It's called reward model Deberta V3 large V2. What it says is that this reward model is trained to predict which generated answer is better judged by a human given a question.So it's used for model evaluation, reward score in RLHF and detect potential toxic responses via ranking. So this model is trained such that if you give an input to this model, it will give you a score based on how good the generated answer is. So the training data that this model has been trained on was generated by humans itself and then that data is being used to train the model.So we are essentially mimicking the ability of humans to verify answers by replacing humans with a model. So these are called as reward models because the best answer is rewarded with the maximum score. If the answer is not good, then the score will be less and the reward will be less.In the next section, we are going to look at reward models in detail. So more specifically, there are two types of reward models. There are outcome reward models and there are process reward models and the purpose of both these type of models are pretty much explained in their names.So let's look at the first category which is the outcome reward model. So imagine that you have a question and you pass this question to a reasoning model. So the reasoning model is going to give you a chain of thought, it's going to give you the thought process through which it thinks before it gives you the answer.Now what the outcome reward models do is that they don't evaluate these thoughts, they don't care about the thoughts basically, they only care about the final answer and only the final outcome is scored. So here ORM represents the outcome reward model and 0.45 is the score which the ORM gives to the final answer. The next category which are more popular are process reward models and these make more sense because when you are using a reasoning model, the thought process is very important for you.So there needs to be a mechanism which judges not just the final answer but also the individual thoughts which are leading to the final answer. So here what you see is that thought 1 is sent to the process reward model and then the model is scoring the thought, thought 2 is again sent to the process reward model and the model is again scoring the thought. So here the main difference compared to the ORM is that individual reasoning steps are scored and the final answer is not scored.So you are assuming that if the reasoning steps are correct, obviously the final answer will also be correct. So you are giving a lot of emphasis on the chain of thoughts which are leading to the final answer. That is why it is called as a process reward model.So now let us look at a practical example of a process reward model and how does a process reward model work in practice. So let us take a simple example, let us say the example which we are considering is Roger has 5 tennis balls and he buys 2 more cans of tennis balls. Each can has 3 tennis balls, how many tennis balls does he have now? So like we saw in the last lecture, we are going to use a chain of thought reasoning to come up with the final answer.So we have 3 steps. Step 1 is the reasoning LLM says Roger started with 5 balls. Step 2 is 2 cans of 3 tennis balls, each is 5 tennis balls and step 3 is so the total number of tennis balls is 5 plus 6 which is 11.Now read these and just try to understand which one is correct and which one is not correct. The first one is correct Roger starts with 5 tennis balls so you get a higher score. The second one 2 cans of 3 tennis balls should be totally 6 tennis balls not 5. So this one gets a lower score see and then so the total number of tennis balls the final answer is correct.So this one gets a higher score. So wrong reasoning gets a lower reward. The main advantage of using verifiers is basically there is no need to fine tune or retrain the LLM which you are using to answer the question.So whenever we are working with reasoning models whichever reasoning models you see in practice whether it's O1, deep seek etc. it is likely that these reasoning models before they give you an answer a verification layer has been added where there is a process reward model that is evaluating the answers and it is giving you the answer which has the best possible reasoning path. So now let's move ahead and we will look at the different types of verifiers.Broadly speaking we will look at 3 different types of verifiers we will look at majority voting as the first type of verifier then we will look at best of n and then finally we will look at beam search. All of these are very intuitive and simple to understand but we are going to have a look at these 3 types in detail so that you understand the intuition behind them. So the first one let's look at it the first type of verifier is called as majority voting and before we go ahead I want all of you to download this paper which I have enjoyed thoroughly there might be some aspects of this paper which you do not understand but I hope this paper will if you have gone through the first 3 lectures I hope you will understand some aspects of this paper and really appreciate this paper because this paper specifically discusses about test time compute.So you will be able to understand the theory which has been outlined in this paper. Okay so after you have downloaded this paper we will start to now discuss about the 3 main types of verifiers the first verifier is called as majority voting and this is clearly the most intuitive in majority voting what happens is that there is not even a verifier used actually what happens is let's say you probe an LLM to generate samples of answers let's say there are 10 answers generated by the LLM and you pick the answer which appears maximum number of times that's all. So something similar to this let's say you ask the question are the number of prime numbers finite and then you give it to the reasoning model and the reasoning model generates 6 chains of thoughts which you are not concerned about at all you just look at the final answer okay so you can see that there are 2 yeses but there are 4 noes so what's the majority the majority is no.So we will choose the majority in this case and the final answer we will pick as no. So this method is also called as self consistency but this is probably the first thing which came to the mind whenever people when people thought of a verifier is that let's sample a lot of answers and pick the answer which comes the maximum number of times. Okay so this is the first answer and now here mind you the main drawback of this method is that the answer can be wrong because you are not using any

any verifier to evaluate the answer. So there is no way to know whether the answer is correct or not, but we are simply taking the majority and that concern is actually addressed in the next method which is called as best of n samples. So here again what happens is that there are n samples which are generated by the LLM, but the difference from the previous method is that you apply a verification layer here.

You don't directly choose the majority but you apply a verification layer and then you choose the answer which the verifier is giving the maximum score to and this can either be done by a output reward model or a process reward model. So let us look at both these examples now at the bottom. So the question that we have asked is how many moons does Jupiter have? It is passed through a reasoning model and then we have generated six chains of thoughts t1, t2, t3, t4, t5, t6.

So as we are looking at ORM here we are not concerned with the chains of thoughts at all. We are only concerned with the final answer. These final answers which are 90, 1, 95, 82, 76 and 14 they are passed through the output reward models and then this verifier scores all these answers 0.6, 0.2, 0.9, 0.7, 0.3, 0.5 and then you pick the answer with the best score.

So the number of moons in Jupiter which Jupiter has is 95 by using the ORM this is the answer we get. Now when you look at PRM in PRM what we do is we do not score the final answer like we do in the previous step but let us imagine that this is a question and we pass this question through to the reasoning model and here we are evaluating three chains of thoughts which are t1, t2 and t3. So t1 goes down to t11, t2 goes down to t21 and t3 goes down to t31.

Both these reasoning steps are passed through the PRM and we are getting scores 0.1 and 0.2 and we are doing a weighted average. So it is 0.15 here again we are doing an average which is 0.6 and here we are doing an average which is 0.55 and now we are not even bothered about the final answer which the reasoning model is giving because the verification layer only verifies the reasoning steps which are appearing before the final answer. So from these three we can see that this answer has the maximum score so that is why we will pick this answer.

So the answer with the best average is finally selected as the final answer. So this is using PRM for best of n sampling and then you can also imagine an example for this which looks very similar to the above example but the primary motive behind showing you both these methods is to understand the differences between the two and you can see in this paper also which I have shared before in this paper they have predominantly focused on process reward models and these models are more effective and they are more frequently used in practice by researchers. Ok now we will go to the next algorithm or next verifier in this case which is using the technique of or we can call it verification using the algorithm of beam search.So the beam search algorithm was originally proposed in the year 1976 and it was proposed in this paper which is shown below. The title of the paper is the Harpy speech recognition system and this paper actually came out in the year 1976. So this algorithm was first used in the speech recognition domain and here you can see that the search strategy the name of the search strategy is a few best paths in parallel.

So this is quite an ancient algorithm we can say it is not an algorithm which is developed by LLM researchers or AI researchers. It was developed way back around 50 years back we can say where the name was not even given as beam search but the algorithm was very similar to the modern algorithm and it was used for speech recognition tasks. So beam search continued to be used for speech recognition and translation tasks in subsequent years.

It was a popular algorithm you can even go to wikipedia and search about it you can see that the references date back to earlier years. So why are we discussing beam search in this course of reasoning models and particularly how can we construct an effective verifier using the algorithm of beam search. So let's look at that in detail.

So first we will understand how the traditional beam search algorithm works and so that your intuition is developed and then we will apply that algorithm to construct a verifier which is very effective and then finally we will run a hands-on a google colab notebook which will give you a practical understanding of how beam search is used in actual LLMs for verification layer. Okay so we will take a very simple example which is a sentence in Hindi. So the sentence says that I am going to Delhi.

This is a sentence in Hindi which translates to English as I am going to Delhi and this is the task that I want to do and now I am going to explain to you about beam search algorithm specifically applied to this task. So firstly we have this text corpus which is let's say around 50,000 words and we can only use these words in the translation tasks. We cannot use any other word when we are translating this sentence from Hindi to English.

Okay so the approach that we are going to take is we are going to translate this sentence word by word. So now let's look at it. Step 0 is we are given a text corpus a list of 10,000 words or we can even call it say 50,000 words and we have to use only the words in the text corpus for the translation task.Now the next step is out of these 50,000 words we will select the most probable three words for the first word in the translated sentence. So if we go through all these words we have selected I, we have selected Delhi and we have selected Zoo. We have eliminated all other words.You might ask the question how are these selections made? These selections are made via an encoder decoder model but we are not concerned about how these selections are made right now. Imagine that there is a model which is making this selection. We are going to understand about the beam search strategy.

This model can be anything it can be a black box for all we know. So we pass this text corpus to the model and we select the three words which have the maximum probability that is step number one. So these words are I, Delhi and Zoo.Now the interesting thing comes in step number two. In step number two what we do is we pass these three words again through the entire corpus and now we find the next three combinations of two words which have the maximum probability. So now how many combinations we have to see let's say 10,000 plus 10,000 plus 10,000.So now we have to evaluate sorry this is 50,000 50,000 plus 50,000 plus 50,000 out of 1,50,000 combinations we have to choose the best three combinations of the first two words. So you can see here that we have eliminated I was, we have eliminated Delhi is, we have eliminated all others and we have selected only three again. We have selected I am, we have selected I love and we have selected Zoo is.

Remember that in the first step we passed 50,000 words in the second step we are searching across 1,50,000 entries and then we are again choosing the three most probable entries by passing it through the model which is a black box for all we know right now. So step number two is for each of these words we will select the most probable combination of first word and second word and select the best three. So these are I am, I love and Zoo is.Okay so now one thing you can immediately notice from this is that there is no Delhi here so you can actually completely eliminate this Delhi. So that reduces the chances of first word from three to only two that is the first word can either be I or Zoo it cannot be Delhi. Okay now in the next step what we do is we again pass these combinations of three to 50,000, 50,000 and 50,000.

So again we evaluate across 1,50,000 combinations and now we choose the next best possible three which can be let's say I am, I am going. So these after this here we again have to pick three. So let's say I am going, I love Delhi and Zoo is green.

We select these or let's not or third one is I love Zoo. So now you can see here that Zoo is not coming anywhere first. So this means that we can eliminate Zoo and now we fix our first word which is I and then this process will continue till we have translated the entire sentence.

So beam search is essentially a technique. So now here you can see that why did we select three at every step we were saying let's select three, let's select three. Three is called as the beam width.

So instead of selecting just one, we are selecting three in each layer then we are proceeding ahead then again we are selecting three in the next layer etc. So three in this case is called as the beam width and I think the name beam search came because it's like imagine there is a huge library and you are shining a torch in that library and there is a beam which is created through that torch shining and that beam falls on three books or three letters or three words. So here I think it's like that right.

It's like it's branches into three. So from a huge corpus it branches into three then again it branches into three. So probably it widens your search scope and that's why it's called as beam search.

But this is the exact algorithm which was used in the original paper which came out in 1976 and subsequently it has been a popular model in speech recognition and tasks. Now for us and for this course we are going to look at how beam search can be combined with a reasoning large language model and a reward model to search many different reasoning paths and come up with an optimum answer. So in this case we searched across many different words right.

But here when we are looking at reasoning models we are going to use beam search to explore multiple reasoning paths before we come to the final answer. I hope the intuition is clear for all of you and with this intuition in mind now we are going to move towards understanding the beam search algorithm and how can we develop a verifier which uses a beam search algorithm to effectively search across multiple reasoning paths. Ok so now there are basically two parameters here that we are going to look at.

The first parameter is the number of beams and the second parameter is the beam width. Ok so there are two parameters which we are going to look at in this example. The number of beams we are going to fix as 4 and the beam width we are going to fix as 2. So let's understand now how we construct our reasoning paths using the beam search algorithm.

Now because the total number of beams we have set to 4 in the first step what the model will do is that the model will generate 4 different thoughts which are given here 1, 2, 3 and 4. Ok so in the first step the model has generated 4 different thoughts. Now what we will do is out of these 4 thoughts only 2 will be selected. Why 2 will be selected? Because the beam width is 2. Remember how in the previous example also we selected 3 in each layer exactly with the same intuition we are going to select 2 in each layer.So these two are you know marked in green here. So we are going to eliminate thought number 1 and thought number 3 and we are going to only choose thought number 2 and thought number 4. The purple background here is the verifier. So it is because it has gone through a process a reward model that it has scored each of these steps maybe this step this thought got 0.15, this thought got 0.8, this one got 0.3 and this one got 0.7. So that is why we choose the best 2. This is the first layer.

Now what we will do is from both of these we will select 2 each. So in the next layer we will have 2 into 2 that is 4. In each layer we need to have 4 different thoughts because the number of beams are 4. So now what we will do from each of these we will select 2 more. So in the first layer we had 4 and now in the second layer also we have 4, 1, 2, 3, 4. From this we have branched 1 and 2, from this again we have 1 and 2. So this part is clear and then again the process repeats out of these 4 because the beam width is 2 we will select the best 2. So probably this got a score of 0.9, this one 0.1, 0.2 and 0.7. So after the verifier has scored them we can see that 1 and 4 have the maximum score.

So we will select 1 and 4 and the process continues. So finally you get a diagram which looks like this. So here the question is Roger has 5 tennis balls, he buys 2 more cans of tennis balls and each can has 3 tennis balls.

How many tennis balls does he have now and this is the first layer, this is the second layer, this is the third layer and this is the fourth layer. You can see that in the first layer we have selected 2, in the second layer out of 4 we have selected 2, in the third layer again we have selected 2 and in the last layer we have selected 1 which is the final solution, which is the full solution it's marked in the complete. So the color is complete inside, it's not an empty color which indicates intermediate solution.

So this is the diagram where number of beams are 4 and the beam width is equal to 2. Because the beam width is equal to 2 we only selected 2 at each step. So this is how beam search allows us to effectively search all the reasoning paths. We are not just scoring each reasoning path and then taking the average now, this is a bit advanced.

What we are doing is that we are selecting the best at each and then we are completely neglecting others. Then from there we are again branching out and this kind of beam search technique when applied in the reasoning models it is shown to increase the accuracy of the answers. So this is how beam search can be used to explore multiple reasoning paths and then finally come up with the best answer.

And now in the next step we are going to look at the hands-on implementation of beam search with process reward models. Again all of these are process reward models because we are scoring each step, we are not just scoring the final answer. So that's why we have employed process reward models in this methodology.Okay now what we are going to do is that we are going to write a code which gives us a beam search tree like this and we are going to give an exactly same prompt which we have given over here and then you can see that in this layer there are 4 again in this layer there are 4, 4 and 4 because the number of beams are 4 and the beam width are 2. So we are choosing 2 in each this 2, this 2 and this 2 and the 2 that we are choosing in each layer let's look at this layer. So we are choosing the thoughts which have the maximum PRM score. So here you can see that green and yellow form lie somewhere here whereas purple lies somewhere at the bottom.

So the model does not choose that, the model only chooses thoughts which has the maximum color. And I am going to show you a Google Colab notebook which I have written which can give you a beam search tree like this. I will share this link in the chat.

Again I have used a A100 GPU here to run this code since I have a Google Colab pro subscription and these are the different steps which I am going to explain to you right now. The first step is to install the dependencies and here we are also installing this pygraph wiz because we want to plot this final graph at the end and because of that we are installing network x and matplotlib.pyplot as well. Then after that what we do is we load our reasoning model.

We are using this Zephyr 7 billion parameter reasoning model for our case and you can refer back to the code which we saw in the last lecture where we had this exact same block of code. So now this block of code repeats. Whenever you are loading your reasoning model the block of code remains the same.

And now we are loading our process reward model. I am using this process reward model here which if you copy paste you will see it's posted on hugging face. So this is the process reward model I am using for for this Google Colab notebook which is called a reward model Deberta v3 large v2.

So this is the reward model which has been trained from human feedback. It can also be used for toxic response detection and some other tasks as well. After the process model has been reward has been loaded.We are now using the ORM. This is called ORM here but actually it's PRM. So we are converting this ORM to calculate scores for individual steps.

So we need to write a piece of code which performs stepwise coding. So that's why we call this function as stepwise PRM score. And then this particular block of code is used for this particular block of code is used to actually write the beam search algorithm.

So here I have the same notation I have n for the total number of beams and I have m for the beam width and the maximum steps which are the different layers are three here and this is what is shown here because it's three we have only one two and three. We don't go below that. You can play around with these parameters and see how the graph changes as well.

Okay so this is how the graph is plotted and then finally for visualizing the beam search tree we define a function called plotTraceGraphTreeClean and then this is where we enter our PROM. Roger has five tennis balls. He buys two cans of three tennis balls each.How many tennis balls does he have now? So here you know you can see the scores which the reward model gives for different beams. So you can see that here we do not have much variation in the answers actually. So all of these beams are giving the same answer which is 11 which is the correct answer.

One of the techniques which they have mentioned in the paper is that if you change the temperature of the model you get a different you get different variations in the answers and that allows you to explore the answer space much better. So here we are not doing any kind of exploration as such because we are not varying the temperature variable and also because probably our model size is also smaller that we are not able to see that effect clearly. So the temperature we have set to 0.9 which is which is pretty high.

Initially I had set it to 0.7 but then I increased it because I wanted to see the variation in the answers which are given by these different beams. So you can see that we have a final beam 1 answer, we have beam 2 answer, we have beam 3 answer and we have beam 4 answer and then all of these are actually same. So here we are not able to see that difference essentially but you can see exactly how the beam search code can be written along with the reward model to score you know the reasoning steps and then select the best to move to the next layer, select the best to again move to the next layer and then select the final answer.So I encourage you to try out different plots, different prompts, different number of beams, beam width and then play around with this code a little bit and also try to see what kind of variations we can do to get different answers in whenever you are looking at these final answers. Now another thing I want to highlight is that the paper which I was mentioning before. Okay so they mention so the way that paper is written is that you will so this is the paper I am talking about.

In this paper they divide the paper into two categories. One category is of course verifiers which you will be able to understand after going through this lecture properly. The second category which they mention is modifying proposal distribution.

So verifiers is where you have a verification layer which grades your answers but proposal distribution is modifying the answers which are generated in the first place. So what comes under proposal distribution is chain of thought reasoning which is few short as well as zero short chain of thought reasoning. These are examples of strategies, prompting strategies which can modify the proposal distribution.

So you can essentially look at these two methods as this is the sampling method which you know samples different answers or generates different answers and this is the method which grades different answers. So whenever you are using this technique of you know building a reasoning model by leveraging inference time compute scaling then you can either modify proposal distribution or you can implement a verification layer inside. Both of these are viable techniques.

In the previous lecture we looked at how you can modify the proposal distribution by prompt strategies which is a chain of thought prompting and zero short chain of thought prompting and then the second method is implementing a verifier along with the reasoning model. So the verifier can be using beam search, majority voting or best of n. Any of the three options can be used. Okay so this brings us to the end of the lecture.

I expect all of you to implement this Google Colab notebook and I expect all of you to download and start reading this paper also. This paper will strengthen your knowledge of inference time compute scaling and it will act as an effective culmination of the first three lectures that we have seen so far. The next lecture we will start with some items such as a tree of thought reasoning and Monte Carlo search.

We will look at both these techniques very briefly and then we will start with the next method of building reasoning models which is learning about reinforcement learning and we will start learning RL from the complete basics. Please let me know in the comments if you have any doubts if you are following this course so far because that really gives us a push ahead and to keep going with these lectures. Thank you very much everyone.

Bye. Hello everyone and welcome to the fourth lecture in the series on reasoning based large language models. In today's lecture we will start a very very interesting topic which is reinforcement learning.

So far in the course we have covered the techniques which come under the category of inference time compute scaling which means that we are not touching the trained model at all.
>>>>>>> 6c679990d9b440578bcb5315aca72a45bb88980d
