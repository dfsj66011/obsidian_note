
(1) *Inference-Time Compute Scaling：* 如果在推理过程中给予更多时间，模型就有更多时间进行思考，从而生成更好的答案。目前人们已经观察到，如果增加测试时的计算量，模型的准确性通常也会随着测试时计算量的增加而提高。因此我们也可以说，模型的表现会随之提升。一般来说，测试时间的计算可以分为两大类：第一类称为提示；第二类则是验证器（verifiers）。

(2) *Pure Reinforcement Learning* Wait, wait. Wait. That's an aha moment I can flag here. 我们将在此介绍的概念包括：什么是强化学习（RL）、强化学习的要素、理解智能体、环境、奖励、回报、马尔可夫决策过程、马尔科夫性质、价值函数、最优价值函数、策略评估、策略迭代、策略与价值迭代。随后我们将探讨蒙特卡洛方法、时序差分学习以及近似求解方法。第一部分是经典强化学习。第二部分是现代强化学习，我们将涵盖信任区域策略优化，然后我们会讲到近端策略优化，最后我们会讲到GRPO，即群体相对策略优化。

(3) *Supervised Fine-Tuning and Reinforcement Learning* 人们认为 OpenAI 的 o1 模型很可能是用这种方法制造的。他们没有也不会公布 o1 的架构，但它很可能是用这种方法制造的，而这种方法也是 DeepSeek R1 最终采用的。所以在文献中，这被称为构建推理模型的蓝图。

(4) *Pure Supervised Fine-Tuning and Distillation*

-----------

**推理时间计算扩展 —— 思维链（2022, Google）：**

事实证明，如果要求大语言模型在给出最终答案之前生成一系列思考步骤，它在复杂推理任务上的准确性也会提高。因此，接下来介绍的第一个技术就叫做"思维链推理"。

思维链提示的准确性实际上随着模型规模的增大而提高，这意味着越大的模型越擅长推理，并且具有很高的准确性。如果你观察非常大的 PaLM 模型，它们几乎可以与最好的监督微调模型相媲美。这意味着，无需经历计算成本高昂的监督微调任务，我们只需使用思维链提示技术，就能在算术推理数据集（GSM8k）上达到相同的准确率。*思维链推理能力是随着模型规模扩大而涌现的新能力，思维链推理实际上只对非常大的模型才有效。*


**推理时间计算扩展 —— 零样本推理**：

这种方法甚至比思维链推理更为简单。零样本意味着我们在提示阶段完全不提供任何示例。取而代之的是我们只写上“让我们一步步思考（Let‘s think step by step.）”，然后将 LLM 的输出附加在之前的 prompt 中，并加入 “因此答案是（Therefore, the answer (arabic numerals) is）”，这是要给 *两阶段* 的零样本思维链过程。

有意思的是，通过实测比较，这种方法在较小的模型上是相对而言最好的方式，即使在 7B 左右的模型上也是，而且相对而言 few-shot CoT 示例数量为 10 也不太行。（不过模型测试的较少，或许也与 few-shot 质量相关？不管怎么说，这是一种性价比很高的方式，无需准备示例，效果还不错。）

> 注：老哥案例中使用的是 `microsoft/phi-2` 和 `openchat/openchat-3.5-1210` 模型，非常弱，如果细看其输出结果，根本就没有在回答我们的问题，反而是在模仿我们的输入或非常随机的内容，尤其在 few-shot 中就是在仿写案例，这应该是模型没有做很多的监督训练，反而 zero-shot 等方式干扰更小，因此这里的结论不一定是真实情况。

----------

**推理时间计算扩展 —— 验证器 Verifiers（Beam Search）**：

验证指的是生成不同答案，然后从所有生成的答案中最终选出最佳答案的过程。中间有一个验证层，它会验证这 N 个答案中哪一个是最好的，最后给出最佳答案。当增加验证环节时，本质上是在延长大语言模型给出答案所需的时间，因为需要从不同的答案中进行采样，并选择最佳答案。

这个验证层，既可以通过人工完成，也可以通过模型完成。执行这种验证的模型被称为奖励模型。这个模型的用途是：在给定一个问题的情况下，经过训练后能预测出人类认为哪个生成的答案更好。具体来说，奖励模型分为两种类型：结果奖励模型（ORM）和过程奖励模型（PRM）。结果奖励模型的作用是不评估这些思考过程，它们基本上不关心这些思考，只关心最终答案，并且只对最终结果进行评分。

更受欢迎的是过程奖励模型，这些模型更有意义，因为当你使用推理模型时，思考过程对你来说非常重要。因此，需要建立一种机制，不仅要评判最终答案，还要评判导致最终答案的各个思考步骤，与结果奖励模型的主要区别在于，这里对各个推理步骤进行评分，而不对最终答案进行评分。

总的来说，我们将探讨三种不同类型的验证器：

* 多数表决，实际上甚至没有使用验证
* n 选优验证器，应用一个验证层，然后选择验证器（ORM or PRM）给出最高分的答案
* 最后是 beam search 验证器，例如 beam 为 4，即首先生成 4 个不同的链，width 为 2，即利用验证器选择其中 2 个，然后继续每个链产生 2 个新的推理步骤，最后一步选择最高的即可。

beam search 的优势在于能高效探索所有推理路径——我们并非简单地对每条路径评分后取平均值，而是采用更高级的策略：在每一步仅保留最优路径，完全舍弃其他可能性，再从保留路径继续分支出新的可能性。研究表明，当这种束搜索技术应用于推理模型时，能显著提升答案的准确率。


### 强化学习


而它做出这些决策的参数是当前的电池电量，以及过去为我充电所花费的时间，或是过去找到充电器所花费的时间。因此，Roomba机器人在你的房子里移动，收集垃圾，并根据过去的经验以及当前的电池电量实时做出决策。第四个例子是菲尔准备他的早餐，你也可以想象在这种情况下准备你的早餐。

每当你在准备早餐时，你都在进行一系列动作：走向橱柜、打开橱柜、挑选麦片盒、伸手去拿麦片盒、抓住并取出盒子、拿盘子、拿勺子——这个过程中的每一个步骤都涉及一系列获取信息的眼球运动。然后每一个行动都由一个目标引导，而这个目标又服务于其他目标，最终的目标是获取营养。例如，你为什么要拿勺子？因为勺子能帮你吃饭。所以拿勺子的行为服务于帮助你吃饭的目标，而吃饭的目标又反过来服务于获取营养的目标。

因此，当你准备早餐时，你正在采取一系列行动，而你甚至没有意识到这些行动，所有这些行动都指向一个共同的回报，即获得营养和满足你的饥饿感。这四个例子激发了强化学习领域的发展，现在我们将通过这些例子，尝试从一个共同的结构角度来理解它们，这将帮助我们以正确的方式思考这些问题。

目前我已经向你介绍了强化学习与监督学习的几点不同之处，以及推动强化学习领域发展的各种实例。现在，让我们尝试为我们的对话构建一个框架。这些例子有什么共同点？所有这些例子的共同点是都有一个积极的决策主体。这个主体与环境互动以实现目标。例如，在第一个例子中，决策主体是棋手；第二个例子中是瞪羚；然后是移动机器人；最后是菲尔。

现在所有这些智能体都在与周围的环境互动，并且它们都在努力实现一个目标。那么这些案例中的目标分别是什么呢？对于棋手来说，目标是战胜对手；对于瞪羚来说，目标是站起来并快速奔跑；对于移动机器人来说，目标是收集最多的垃圾；而对于菲尔来说，目标是获取营养并满足饥饿感。现在有一个智能体和一个目标，但为了实现这个目标，智能体必须穿越环境，而每种情况下的环境都是不同的。

对于棋手来说，环境就是棋盘；对于瞪羚来说，环境是周围的自然环境，还包括帮助瞪羚做出决策的内部环境。那么，这个内部环境是什么呢？内部环境也是代理的记忆和偏好。因此，在大多数情况下，代理和环境之间并没有明确的界限。

例如，对于移动机器人，你可能会认为机器人是智能体，而机器人之外的一切都是环境。但事实并非如此，因为机器人内部的电池也是环境的一部分，因为智能体做出的决策取决于电池剩余的电量水平，尽管电池位于机器人内部。

同样，当菲尔准备早餐时，周围的环境当然是厨房，但还有一个内部环境，那就是他过去的记忆和经历。例如，过去他拿勺子时是否在某个地方伤到自己，所有这些经历都会在菲尔做决定时被考虑进去。通常情况下，环境与智能体之间并非泾渭分明，而是部分存在于智能体内部，但大多数情况下仍属于外部范畴。这些案例的共同点在于：都存在一个主动决策的智能体，它不断与环境交互，并朝着最终目标推进。这就是我们理解所有强化学习问题的通用框架。

现在想象一下这与监督学习有多么不同。假设你想让一只瞪羚或菲尔学会准备早餐，如果将其视为监督学习问题，你需要提供多少标签？甚至很难用这种结构来思考这个问题，因为你需要做出某种决定，比如伸手去拿橱柜里的东西，那么你该如何为这类动作打标签呢？

因此，这类问题很难用监督学习问题的结构来定义，因为不同的类别会根据其具体内容被赋予不同的标签。因此，即使是在无监督学习的框架下思考这些问题也很困难，这就是为什么需要一个完全不同的独立类别来理解这类问题——即存在一个与环境互动的智能体。也许这个智能体正在发现某些隐藏的结构，但这只是整个过程中的一部分。

例如，羚羊可能找到了某种跑得更快的方法。所以在某种程度上，它试图发现一些隐藏的结构，但最终目标是跑得更快——最终目标不仅仅是识别某种结构，而是跑得更快。这就是它与无监督学习问题类别的不同之处。

因此，在所有这些例子中，智能体通过与环境互动，利用其经验逐步提升自身表现。强化学习问题的一个独特之处在于：智能体与环境互动的次数越多，随着时间的推移，它的表现就会越好。你能想到这是如何实现的吗？让我们以瞪羚本身为例来说明。

起初，羚羊试图奔跑却摔倒了，它甚至挣扎着站起来。但随着羚羊不断奔跑、不断练习，最终它开始慢慢跑起来，最后终于能够快速奔跑。除非它与周围环境有更多次的互动，否则它不可能跑得快。所以每次你与环境互动时，你会变得越来越好，这是我们所有人都有过的体验。当我们试图掌握一项技能时，突然有一天我们会意识到，我在这项技能上比两个月前进步了很多，这都是因为我进行了大量的练习。

这正是我们在讨论强化学习问题结构时所看到的情况。现在每当人们谈论强化学习方法时，强化学习系统都有四个主要元素。第一个元素是策略。该策略定义了智能体在特定时间的行为方式，从形式上看，策略是一种从感知到的环境状态到在这些状态下所采取行动的映射。简单来说，策略就是当处于某种情境中时，决定如何行动的规则。因此，策略将决定我在该情境中的行为方式。

接下来是奖励信号。奖励信号定义了强化学习问题中的目标。在每个时间步，环境会向强化学习智能体发送一个数字，即奖励，而智能体的唯一目标就是最大化其随时间累积获得的总奖励。因此，奖励类似于生物系统中的愉悦或痛苦。如果你用手触摸沸腾的容器，你的身体会立即给你一个负面的奖励信号，由于你收到的信号是负面的，你将来就不会再执行这个动作。同样地，假设一个国际象棋选手做了一系列的移动然后输了，他会立即得到一个负面的信号。

因此，他将来不会再执行那一系列步骤。这就是奖励信号的含义。第三个概念有点反直觉，被称为价值函数。在这节课结束时，我们将理解价值函数的真正含义，但主要的区别在于。


(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)