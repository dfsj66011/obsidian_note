
2024.09 OpenAI 发布了他们的首个推理模型 OpenAI-o1。当 DeepSeek-R1 问世时，其准确度与 OpenAI-o1 相当，aha moment。

1950-1990：经典强化学习时代。
90年代之后：开始转向策略梯度方法。
2013 年开始：在 RL 问题中使用神经网络，DRL 诞生了。
2018 年之后，在 LLMs 中应用 RL。



这是第一种人工智能模型。第二种类型或第二种分类被称为无监督学习。无监督学习与监督学习有很大不同，因为在这里我们没有任何标签。想象一下，你被聘为一组科学家，参观一个花园，并接到一项任务：这里有一个天平。

去测量这些花瓣和萼片的长度和宽度，然后告诉我有多少种花。这是一个你不知道花园里有多少种花的问题，但你收集了所有的数据，并试图将数据分类到不同的簇中。现在记住，我们事先没有得到任何标签，我们只是看着我们收集的数据，并试图将数据分组到簇中。

聚类是一种常用于解决无监督学习问题的方法。其目标是在未标记的数据集中发现隐藏的结构。现在让我们来看看强化学习，并比较它与这两类学习的不同之处。监督学习的数据是带有标签的，无监督学习虽然有数据，但这些数据没有标签。而在强化学习中，模型一开始不会被输入任何数据，而是通过与环境交互来获取数据。

因此，这是监督学习、无监督学习和强化学习之间的关键区别之一。它之所以通过交互获取数据，是因为在强化学习问题中，通常无法提供所有输入输出数据对的集合，因为那将是一个庞大的数据集。想象一下，你正在下国际象棋，并试图开发一个强化学习代理来下棋，以便最终赢得比赛。在国际象棋游戏中，你能提供多少数据呢？

棋盘上棋子的可能排列组合高达10的44次方。提供如此庞大的数据集是不可能的。因此，最佳方案是帮助模型通过互动学习。顺便说一句，这与人类的学习方式非常相似。回想一下你第一次学骑自行车的时候：有人教你，你犯了很多错误，然后通过不断迭代慢慢进步。这正是我们使用强化学习时发生的过程。

监督学习的目标是对训练数据中未出现的情况进行泛化。无监督学习是在未标记的数据集中发现隐藏结构，而强化学习的目标则是最大化某个目标或奖励信号。这是强化学习与其他类型机器学习算法的根本区别之一。在这里，我们试图最大化奖励信号。这是你在其他算法中不会遇到的情况。

因此，我们可以将其总结为：强化学习问题涉及学习如何行动，如何将情境映射到动作，从而最大化奖励信号。事实上，许多核心强化学习算法实际上是从生物学习系统中获得灵感的，因为它与人类思考情境的方式非常相似。然而，如果你是在60年代或70年代工作的科学家，人们不会认真对待你，因为当时人们普遍认为这些谈论从经验中学习的方法都是弱方法。

他们并不真正相信这些方法，因为任何需要大量规划的任务都被认为非常复杂，他们不认为机器学习可以用来解决这样的任务。这就是为什么这个领域没有取得非常快速的进展，因为它总是受到这些来自科学家的想法的阻碍，他们说不要过多关注这些问题，这不会有什么结果。相反，可以尝试使用其他算法，比如当时相当流行的遗传算法。

那么让我们思考一些现实生活中的强化学习例子，以便更好地理解这一点。第一个例子是一位国际象棋大师在思考如何走棋。这一步棋并非基于过去获得的数据，而是基于“如果我走这一步，接下来会发生什么”。对手可能会采取哪十种应对走法？而眼下哪一步棋才是最有意义的？你需要对所有可能的走法做出直观判断，评估它们的可取性。

当你看到这个问题时，立刻就会发现它无法归类到我们讨论过的其他任何类型的机器学习问题中。这个问题的复杂性不同，因为我们没有获得标注数据。事实上，我们根本没有获得任何数据。我们是在生成越来越多经验的同时收集数据。强化学习问题的第二个例子是，假设我们以一只瞪羚为例。想象一只刚出生的动物，一只正在学习的小羚羊。

现在，人们注意到这只瞪羚幼崽在出生两到三分钟后，就挣扎着站起来。然后它通过与环境的互动学习。它犯了很多错误。它不断调整自己的运动模式。半小时后，它就能以每小时36公里的速度奔跑，这真是令人惊叹。这是一个例子。另一个例子是想象我们有一个移动机器人。想象一个吸尘器自动前进，在房间里移动，然后收集垃圾并将其放入垃圾桶。

现在，当你设计这样一个机器人时，这个移动机器人必须做出多个决策。其中一个问题是：我应该去新房间寻找垃圾，还是返回充电站？这些都是机器人必须采取的行动。而这些行动必须基于某些因素。我所说的这些因素包括机器人当前的电池电量，以及过去它找到充电器所需的时间。

然后期末考试对我们所有人来说都很容易理解。就是我们准备早餐或吃东西的场景。每当我们准备早餐时，我们会快速执行一系列动作。我们走向橱柜，打开橱柜，选择麦片盒。我们伸手去拿麦片盒，抓住它，取出盒子。然后我们拿一个盘子、勺子，坐下来吃饭。所以这是一系列连贯的动作。

事实上，在这四个例子中，你都能看到一系列采取的行动。所有这些行动都指向一个共同的目标。我们稍后会谈到这个共同目标。但在我继续之前，试着思考一下，在这四种情况下，目标会是什么？记住，我说过，强化学习与其他算法的主要区别之一在于，智能体必须实现一个目标。

那么，在这四种情境中，目标分别是什么？在某些情况下，目标相当明确直接。但在另一些情况下，你可能需要稍加思考。好，那么这些例子中有什么共同点呢？所有这些例子中，都有一个非常活跃的决策主体。然后，这个决策主体与环境互动，以实现某个特定目标的最大化。那么，所有这些情况下的目标是什么呢？

在第一个例子中，当我们下棋时，目标显然是为了赢得比赛，对吧？在第二个例子中，当小牛学会快速奔跑时，目标就是跑得快。移动机器人的目标是收集最多的垃圾。而准备早餐的目标则是获得最高的营养。这些就是强化学习问题的目标。然后还有一个决策代理。

因此在第一种情况下，智能体是棋手。在第二种情况下，智能体是瞪羚，然后是移动机器人，最后智能体是菲尔。在所有这些情况下，都有一个智能体与之交互的环境。这个环境因问题而异。在第一种情况下，环境是棋盘。在第二种情况下，环境是大自然。在第三种情况下，环境是房间加上电池。

在第四种情况下，环境是厨房。你可以看到我在这里标记了一个星号，因为这里有一点复杂。有时候环境实际上也存在于智能体内部。比如，当我决定早餐吃什么或如何准备早餐时，环境不仅仅是厨房，我脑海中的记忆也构成了环境的一部分。不过这有点复杂。

所以，我们不要纠结这个。我建议大家不要在板上做任何标记，因为现在它会一直留在这里，而我继续往下讲。好了，现在所有这些例子中，智能体都在利用自己的经验，通过与周围环境的互动，随着时间的推移来提高自己的表现。这就是它们的共同点。事实上，在这个训练营中，你们会听到我提到“智能体”这个词超过500次。然后你们还会听到我谈论“环境”。


你会听到我经常提到目标这个概念。因此我希望你们能真正理解这些术语的内涵，因为无论我们研究什么问题——比如大语言模型或电子游戏——我们首先要问：我们的智能体是什么？我们的环境是什么？我们的目标又是什么？这三大要素很可能是强化学习领域中最关键的核心概念。

现在我们将探讨构成强化学习系统的要素。强化学习系统包含四个要素。第一个要素称为策略。策略定义了智能体在特定时刻的行为方式。以国际象棋为例，假设对手已经走了一步棋，而我想决定下一步棋该怎么走，这就由你的策略来决定。

你的策略基本上会查看棋子的当前配置，并告诉你从该状态出发的最佳可能行动。因此，从非常正式的角度来看，策略是从状态到行动的映射。例如，假设有一个状态S1，你有四个可能的行动A1、A2、A3和A4。策略会告诉你，在这些行动中，哪一个是你应该执行的最佳行动。可能是A1、A2、A3或A4。因此，策略就是告诉你如何在特定情况下行动的东西。

其次是奖励信号。奖励信号基本上定义了强化学习问题的目标。再以国际象棋为例，当我们下棋时，奖励不会在每一步之间即时出现，而是在游戏结束时直接给出。例如，赢得比赛可能获得+1的奖励，而输掉比赛则可能获得-1的奖励。

如果比赛是平局，那么奖励为零。所以我认为，强化学习的复杂性之一在于，在像国际象棋这样的情况下，我们获得的奖励是延迟的，这些奖励要到比赛结束时才能得到。我无法立即获得奖励。这就是奖励信号的含义。它与你在生物系统中感受到的快乐或痛苦非常相似。

假设有一锅水，我正在锅里烧水。如果我碰到那口锅，身体会立即做出反应，意识到这非常烫。当你第一次经历这种情况时，你得到了一个奖励信号：每当你碰到热的容器，就会引起很大的疼痛。这个奖励信号很重要，因为没有它，你永远不会知道不应该碰热的容器，因为它会引起很大的疼痛。

这就是为什么奖励信号很重要。当你学习骑自行车时，你的父母可能会告诉你，不要这样做。如果你这样做，你会摔倒。在某些情况下，你可能真的摔倒了。这就是来自环境的奖励信号，它告诉你不要这样做。否则，它会导致疼痛或让你摔倒。然后我们来到最重要的概念之一，即价值函数。

最近我们公司发生了一件非常有趣的事。当时我正在做报告，讲的是强化学习如何应用于电子游戏。很多听众都是强化学习的新手。整个讨论最终归结为对价值函数的一个非常直观的理解。所以我想尽力在这里解释一下价值函数的含义，这样当我们遇到类似问题时，你们就会意识到，对这种概念的直观理解也是我们理解这些问题的核心所在。

奖励信号表示的是即时意义上的好处，对吧？当你碰到那个滚烫的容器时，会立即收到信号。而价值函数则指明了长期来看的好处。让我们举个例子来理解这一点。假设我们是锦标赛的教练。我们需要为锦标赛中的每场比赛挑选11名球员。整个锦标赛共有14场比赛要进行。

现在，假设你选择了球员X。好，我们就称他为XP。这是该球员的名字。在第一场比赛中，这个人表现非常糟糕。所以我会说表现差。这就是奖励，对吧？这是这个人打了一场非常糟糕的比赛后得到的即时奖励。那么，你会立即决定在第一场比赛后就换掉这名球员吗？很可能不会。如果你是一个明智的教练，你不会只看即时的奖励，而是会看这名球员能带来什么价值。以及这名球员的表现会如何。

不是在当前的比赛中，而是在所有比赛的综合表现中。因此，最终，这位球员的表现将取决于，这是第一场，第二场，以及所有14场比赛。所以你的决定不仅仅取决于R1，即眼前的回报，而是取决于R1加R2加点点点加R14，也就是未来所有可能从这位球员身上获得的回报的总和。

这正是该玩家的价值所在。或者用强化学习的术语来说，我们称之为状态价值。这也是奖励与价值之间的主要区别。奖励关注的是短期收益，而价值则着眼于长期收益。

下棋时，每走一步棋，你不能只看眼前的得失——比如吃掉对方的皇后是否能直接赢得比赛？很可能不会，因为你需要长远考虑。即使我现在不立即吃掉对方的皇后，也许这步棋会在对手走完10步后对我有利。因此，我们在这里优先考虑的是这步棋的价值，而非它带来的即时回报。这是其一。

