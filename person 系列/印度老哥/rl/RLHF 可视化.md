
而大语言模型的不同之处在于，奖励只有在句子完成时才会获得，过程中不会得到。例如，在这句话中，“浦那在哪里？浦那在印度。”如果我只孤立地看“浦那在哪里？浦那”，那么获得奖励是没有意义的，因为句子还没有完成。所以，只有在所有状态和动作都完成且这一回合结束后，才能获得奖励。

这就是为什么我们只会在完成一个章节后给予一次奖励，并将其附加在末尾。因此，你可以认为这个奖励是针对文本结束标记的。那么对于所有这些标记来说，浦那在哪里？浦那在印度。我们没有奖励。我们唯一给予的奖励是针对文本结束标记的，如这里所写的是2.5。所有其他值仅仅是KL散度。

好的，那么现在我们所做的是：在这个综合得分向量中，首先将KL散度乘以一个负号，然后再乘以一个超参数beta。好了，我们已经构建了我们的得分向量，其中包含了所有标记的KL散度，并在完成时附加了一个单独的奖励。

现在，我们的下一步是计算优势。为了计算优势，我们首先需要预测所有这些标记的值，而这我们已经知道该如何操作。我们使用之前介绍过的价值模型的预测结果，利用该模型来预测我们在序列中遇到的所有标记的值。例如，在这里我们预测“Pune在哪里？Pune在印度。”这些句子的值。

我们还会在文本标记的末尾分配一个问号或赋予一个值，通常为零。首先，我们计算一个得分向量，然后预测这些值。请注意，这些值的预测未来可能会发生变化，因为值模型也在由我们进行训练。

现在，在下一步中，我们计算优势值，并通过广义优势估计公式迭代计算这些优势值。我不会详细介绍所有步骤，但本质上是从后向前推进的——从第八步开始，然后如动画所示，逐步向上移动到第七步、第六步、第五步、第四步、第三步，直到第一步。最终，你会得到所有这些标记的优势值。

你也可以计算所有这些代币的回报。这些回报无非就是优势加上奖励。好了，在整个模块的最后，我们所做的就是学会了如何计算序列中所有代币的优势，以及如何计算序列中所有代币的回报。

现在，看起来我们已经准备好了所有的部分。我们只需要在PPO训练循环中将它们组合起来。但在我们这样做之前，还有最后一步，那就是填充和掩码步骤。那么，为什么这一步很重要呢？假设模型正在生成大量的轨迹。

每个轨迹的提示长度和回答长度都会有所不同。因此，完成部分的长度也会各不相同。通常的做法是对数据进行填充，使填充后的序列长度与块大小相匹配。在这个例子中，我将块大小设为16。例如："浦那在哪里？浦那在印度。"

你可以看到总共有9个标记。所以我们有9个优势，9个回报，还有18个对数概率。现在我们需要把所有这些增加到16个。因此，我们在其后添加了一个填充层，您可以看到我们在末尾添加了值为0的部分。这些值本身没有意义，但我们这样做是为了能够对输入进行批处理。而要创建批次，我们不能让输入的长度各不相同。

我们确保所有输入具有相同的长度，即块大小（在本例中为16）的长度。然后，我们对优势、回报进行填充，接着也对对数概率进行填充。随后，我们对完成标记和目标标记也进行填充。最后一步，我们创建一个动作掩码。为什么要创建动作掩码呢？

因此，当你生成一个轨迹时，你有提示词、响应和填充部分。现在，当你修改LLM的权重时，提示词和填充部分对你来说并不重要，只有完成部分才是重要的。那么，如何告诉模型只有完成部分是重要的，而忽略提示词和填充部分的标记呢？

这是通过创建一个掩码来实现的。掩码的作用是告诉LLM（大型语言模型）：这是序列，忽略这些标记，只关注这些标记。在动作掩码中，我们希望忽略的标记被赋值为0，而希望选中的标记则赋值为1。"mask"这个词可能有点令人困惑，但实际上它只是一个由0和1组成的数值数组。

0表示忽略该标记，1表示不忽略该标记。这就是整个过程。在这一步中，我们对优势、回报、对数概率、完成标记和目标标记进行填充，然后在接近尾声时创建一个动作掩码。现在我们已经完全准备好可视化PPO训练循环了。

那么PPO训练中会发生什么（这也是实践中使用该方法时最大的问题），我们同时在并行训练四个不同的模型。我们正在训练LLM（大语言模型），目的是使其与人类偏好对齐——这个模型正处于训练阶段。同时我们还有价值模型，其架构与LLM完全相同，只是输出层被替换成了线性层。这两个模型都在进行主动训练。

有两个模型被冻结了，即奖励模型和参考模型，我们想用它们来与大语言模型进行比较。虽然这些模型被冻结了，但它们仍然存储在内存的某个地方。所有这些权重都存储在某个位置。在实际训练基于PPO的RLHF框架时，会消耗大量内存，该框架包含四个不同的模型。实际操作中，我们会创建或展开批次处理——比如可以用大语言模型批量生成大量文本序列，但不会一次性生成太多。因为推理过程本身也需要较长时间。

所以你推出这些小批次，以我举的例子来说，每个小批次包含四个序列。总体而言，可能有16个或12个序列。但在一个批次中，你一次只处理四个序列。总的来说，你可能需要在一个epoch中处理多个批次。那么现在这种情况下，我接收了第一个小批次，里面有四个序列。浦那在哪里？浦那在印度。

孟买在哪里？孟买在印度。德里在哪里？德里在印度。那么浦那在哪里？浦那。什么是浦那？浦那是一座城市。因此，在一个小批次中有四个序列。在为每个序列收集完这些数据后，我们计算对数概率。例如，问“浦那在哪里？”浦那在印度。我们使用之前解释过的方法计算所有这些标记的对数概率。然后我们计算这四个序列中生成的所有这些标记的优势和回报。

然后我们的做法是，通过随机排列生成这些小批次，从不同的序列中创建排列。例如，如果你有12个序列，我们想要整体处理，而在第一批次中，我只想选择四个序列。那么这四个序列是随机选择的。例如，序列0、序列3、序列7和序列9可以作为第一批。然后，在这些小批次被选择之后，我们执行第一次前向传递。在前向传递中，发生的情况是，对于你采样的序列，模型会根据正在训练的策略生成对数概率。

然后你用所有这些值来计算PPO损失。现在，正如我在这一步提到的，PPO损失是通过将三个损失相加来计算的。那么让我们来理解一下这三个损失是如何计算的。首先，我们来看序列0的第4个位置，对应的标记是"booming"。这里我们已经计算了RT，即新策略与旧策略的比率。这个值是在前向传播过程中计算得出的。优势值已经被采样，并且我们在前向传播之前就已经计算好了这些优势值。因此我们不需要重新计算，而是直接使用这个已有的值。我们将这个值乘以R，然后计算这个经过裁剪的值。

我们将R的这一片段乘以优势函数，然后计算PPO损失。我们对所有序列中的每个标记都执行这一操作，并取其平均值。这样就得到了平均PPO损失。接下来计算的是价值损失，通过取价值模型预测与回报之间的均方误差来计算，这些回报是在计算优势函数时得出的。同样，我们对所有标记执行此操作并取其平均值。

最后一步是熵损失。熵损失通常通过查看概率分布或策略本身来计算。我们计算所有这些的平均值并将其相加，得到总体损失。让我们播放动画以便您理解我们的操作。我们遍历所有这些标记，然后为每个标记计算PPO损失。之后是反向传播步骤，实际上这一步在此之前已经发生。

我会在反向传播步骤暂停，在我们处理第二个小批次的时候。但基本上在反向传播步骤中，我们所做的就是简单地反向传播损失，然后我们计算，根据梯度下降法更新权重，这与我们在机器学习模型中所做的非常相似。让我们来看看，这就是反向传播过程。整个目标是最小化这个损失。然后，一旦所有三个小批次处理完毕，我们会根据训练周期中设定的轮数（epoch）再进行一轮。同样地，我们会重复相同的过程，计算所有未被掩码标记的新对数概率。

然后我们利用样本轨迹已经计算好的优势和返回值。接着计算PPO损失、价值损失和熵。我们将所有这些组合起来构建一个聚合损失函数，在整个过程中需要最小化这个函数。好的，这就是PPO训练循环的工作原理。总结一下，我们首先讨论了奖励模型的构建。接下来我们讨论了PPO训练。PPO训练基于一组策略采样方法，其中对数概率、优势函数和回报值都已计算并存储在内存中。

但我们必须根据正在训练的策略计算对数概率，这在前向传播过程中完成。然后所有这些值一起用于计算PPO损失，接着通过反向传播，我们根据梯度下降规则更新权重。非常感谢大家。


Okay, so today I want to try an experimental lecture. So, it is something which I have been reading up for the past 2-3 weeks and the topic which we are going to cover today is called agentic reinforcement learning. So far, we have seen how RL has progressed through the last 50-60 years.

We started with classical reinforcement learning where we looked at the algorithms of Monte Carlo temporary difference and then dynamic programming methods and then towards 2014-2015, we entered this era of deep reinforcement learning and that mainly happened when the Atari paper came out. So, people started applying reinforcement learning to tasks in robotics and video games. The main change which happened during that phase was that we could now represent our policy as a neural network which previously could be represented in the form of a table.

But it is impossible to represent all the states and their values in one single table and people were always trying to experiment that can we represent the policy as a neural network but until actually before the deep QM paper in 2014, there was a TD Gammon paper which came out in I think around 2000s which also was a proof of concept where deep neural networks can be used to represent policy. And then after 2015, when people really started applying reinforcement learning to LLMs happened because the algorithm of PPO was invented which was proximal policy optimization. This happened in 2017 and then people found that PPO algorithm is very well suited for using RL in LLMs.

If you remember that's the same time when transformers, the paper attention is all you need happened and the parallel research was going on in the LLM space and people also applied RL in LLMs. But the best application was through reinforcement learning through human feedback which made sure that okay you are pre-training a base model that is fine but how do you make the base model aligned with human preferences to behave like a chatbot like humans interact with each other. So that is called as post-training and in post-training the crucial component was that of alignment and reinforcement learning through human feedback played a big role in that.

So the first chat GPT which came out in 22 December it would not have been possible without RLHF which at the heart of it had PPO which had come out five years earlier in 2017. Then people understood that okay you are using PPO and in RLHF one of the crucial step is you have a reward model. So reward model is sort of like you pass in any kind of prompt and the completion and the model gives you a score which rates how good or bad the completion is and people train this reward model based on human preferences and why is this required because if you have questions which are very subjective in nature for example if you ask me do you like this program or not my answer will differ from your answer.

So what people did was people gave humans a preference between two options where they had to select their own preference. So for example if I am given two poems I can compare two poems that is better than asking me to score for each individual poem and then based on these preferences which human labelers gave a reward model was constructed. Now this process was very expensive like monetary expensive it incurred millions or millions of dollars because it involved a lot of human labor cost and it was not so only high budget AI labs could do the RLHF process.Startups people who do not have access to that kind of money had no idea how the RLHF pipeline works. The first paper which made this accessible to everyone was the paper which introduced this concept of using LLMs as a judge. So it basically said that you do not need human labelers to label the preferences but you can pass it to an LLM and the LLM can be used as the judge if you prompt it saying that you have this role and you have been given two answers and you have to rate one answer.So then people started using that and the first LLM which performed well in that was GPT 4.0 and then that completely revolutionized the RLHF workflow because it suddenly made that accessible for everyone. So even smaller AI labs could now align their models to human preferences because they could use LLMs as judges. Okay so this is also called as RLAIF which is reinforcement learning through AI feedback because there is no human in the loop anymore.Now okay fine so we have gone from PPO to RLHF to RLAIF but in 2025 one more thing came out which was GRPO. The main problem with PPO is that you need a critic model which is very heavy it takes a lot of memory. GRPO came and said that you remove the critic model from your pipeline completely and instead of having a critic you sample multiple responses and calculate the advantage from the relative rewards which these responses get and that made sure that you don't need a very heavy critic model in the RLHF pipeline anymore.

But they added one more interesting innovation was they introduced this reinforcement learning through verifiable rewards which means that you use let's say completions which can be verified which do not even require a reward model for example mathematics coding all of these tasks are there are objective answers either something is right or something is wrong you don't need a human labeler or even an AI to give feedback on that. So essentially their pipeline reduced the number of models from four to two. Now you only needed the base model which is the main policy that you are trying to train and you need the reference model so that you make sure that your model doesn't diverge too much from the reference model.

So it reduced the number of models from four to two. So these were the stages which have happened when applying RL in LLMs and in the last lecture we actually looked at the entire GRPO pipeline and we looked at some nice examples where we can transform a model into a reasoning LLM using the process of GRPO. Now the next frontier which is very new it has only started in the last three four months is reinforcement learning agents or agentic RL.The objective of agentic RL is basically to use reinforcement learning to operate a set of tools and perform some tasks using LLMs. So if you look at chat gpt gemina all of these now have a deep research mode where you ask any question and it takes some time sometimes it takes 10-15 minutes for the LLM to go through a bunch of documents and come up with the answer which is usually very comprehensive. Now behind the scenes if you look at it they have implemented agentic reinforcement learning and the paper which I have referred for today's lecture I am just going to share it with all of you.

It is a very interesting paper it is somewhat long but many of the pages are taken up by the references so actually the paper is 50 pages long it appears to be 7500 pages long so it outlines the landscape of reinforcement learning in for LLMs. So I will just start sharing my screen now so this is the paper which I have shared with all of you. So this paper came out early this month and all of the references you will see are mostly from this year only because as I said this is not a very old field basically it goes through all the details and what exactly is agentic RL etc but today's lecture we have dedicated to this and the reason I wanted to cover this as a last lecture of our bootcamp is because I really think agentic RL is the future and my thinking is heavily influenced from this paper which was written by David silver and Reid Sutton where they say that they have this interesting graph which says that up until now we were in this era of human data.

ChatGPT for example is trained on massive amounts of human data but you can't rely on data anymore you only have so much data at your disposal so moving forward we need agents who can rely on experience and not on data. Experience means that you have to try out different things understand whether your trial has worked or not and then iteratively improve that is exactly what reinforcement learning does. If you remember the agent environment interface which we discussed in the second lecture the agent gets observations performs some actions gets rewards and then updates its policy based on the reward which it gets that is exactly what we are moving into which is the era of experience and I think that this era of experience can also be represented in more technical words as agentic reinforcement learning.

So today we are also we are going to look at we are going to understand this terminology what does it exactly mean and we are also going to take one practical example where I have implemented the agentic RL application for one specific use case so that we will get some clarity with respect to not just the theory of this but the practical application side also. Okay so let's start with today's lecture so the main idea of the field of agentic reinforcement learning is this ability to train an autonomous agent which is composed of the following set of abilities the first is planning the second is tool use third is memory fourth is self-improvement reasoning and perception. So we are going to look at some of these individual components in detail there are some components which are not really relevant to us so we are going to ignore that for example memory is one component which we are going to ignore we are going to focus heavily on tool use and we are going to focus on reasoning these are the two main areas that we are going to focus on.

Now before we dive deep let us understand what really are AI agents okay so AI agents in simple words let's say you are at point A and you want to go to point B so to go from point A to point B to for us to define that task to be an agentic AI task we need two things the first thing that we need is we need an LLM which helps us to carry that task to go from point A to point B and the second thing we need is a set of tools for example if you are asking the deep research chat gpt tool let's say let me show that okay so this is the this is the deep research okay so I ask it that plan me a detailed itinerary for a trip to Spain so it'll ask me a bunch of follow-up questions what are your travel dates with cities so this is I want to go from a point A to point B I'm using chat gpt5 as the LLM for this and the this LLM is going to use a set of tools for example one tool it might use is looking up some websites like web searcher there might be a web searching tool then there might be a payment gateway tool etc so there might be a lot of different tools which this agent internally has access to currently I'm not aware which all different tools does gpt5 have access to but then this is an example of an agentic task okay I'm going from point A to point B and I'm using LLM and some set of tools to go from point A to point B now we are in the same framework but we somehow want to understand where does RL come into the picture here how can we use reinforcement learning to be able to do this task in an efficient way so let's look at the first thing which is tool use so one thing which you will see in traditional AI agents example is that the tool which the agent is supposed to use is already defined the LLM does not decide the pattern in which the tool has to be used so let's say there are three tools A B and C the LLM first goes to A then B then C and then performs the action or calls the relevant tools in that specific order but what agentic RL does is that it enables the agent to autonomously discover when how and which tools to deploy and this the agent will learn through a trial and error process initially it might start with the order A B C then it learns that starting with A is not giving me higher reward so I'll switch the order maybe I'll start with B and then later it tries out C A B so it tries out different variations and finally chooses the right order to optimize the reward this is exactly the flexibility which reinforcement learning allows us we can now just define the final reward and the agent will optimize the set of actions to maximize that reward so we are going to look at some examples some github repos which have actually done this so this concept is called as tool integrated reasoning or TIR so let's look at the first repo which is deep RL deep eyes okay so you can see that this already has more than 800 stars and the title is that deep eyes incentivizing thinking with images via reinforcement learning so let's see what they are exactly trying to do okay so if you look at the question the question says is the cell phone to the left or right of the backpack and if you look at this image closely the backpack is the one which they have marked in green it's over here and the cell phone is something which the person is holding in their hand which you can see in the yeah which you can see the the person is standing right next to the door so the answer should be that the mobile phone is towards the left of the backpack now how this process is done is initially the the question is processed through the large language model so you take the question and you pass it through an llm and then the llm understands this question and breaks it down into chain of thoughts so chain of thought one chain of thought two and chain of thought three and then you can see that the llm has some tools at its disposal you can see this here there is a zooming tool which the llm has used after this chain of thought and after the second chain of thought the llm has used another zooming tool which you can see through this microscopic symbol and then finally it comes to the final answer so the model decides whether to perform a second perception via zoom in or to answer directly so now you can see here how the tool use is up to the hands of llms we are not defining the tool use beforehand the llm decides do i need to use the zoom in tool for the second time also or the first time i am using the zoom in tool that is enough so here i only have one tool but the llm is not deciding the order of tools here but the llm is deciding when to use that tool so remember i had said the llm here decides when how and which tools to deploy so in this application we are not deciding which tools to deploy but we are deciding when the tool has to be deployed so this is an amazing application where you are mixing large language models with some agentic capabilities and you are making that process much more reasoning oriented by using reinforcement learning so this is an interesting example let's look at another example which is called retool reinforcement learning for strategic tool use okay so yeah so what they have done is they have also done something similar they have used tool augmented reinforcement learning to guide llms to optimal strategies for leveraging external computational tools during reasoning so from their accuracy graph you can see that if you augment the agent with this tool integrated reasoning suddenly you start to get much greater accuracy and they have also made their repo open source there are a few other examples here there is 2rl which is also tool integrated reinforcement learning maybe i can zoom in and show it to you what's happening okay so here the question we are asking is that two parabolas are the graphs of the equation y equal to 2x squared please give the value of k plus m so first what it does is it invokes this code tool where it executes a python code and it generate it generates some answer but it realizes that the output that it gets from this answer is probably not correct so it goes back and it rechecks the steps if you remember the deep seek r10 we had discussed an aha moment where the model learns to re-evaluate its initial steps this is exactly like that so you don't get this behavior of re or evaluating itself without reinforcement learning so whenever you see something like this happening it's probably a sign that the pipeline has some reinforcement learning step in in between then there is another tool which i could not find an open source repo for this it's called as artist which is agentic reasoning and tool integration in self-improving transformers so this research has been carried out by microsoft and this i think came out two or three months back so the reason i wanted to show you all all of this is because all of these are repos which have been made open source very recently and this is a very ripe area for research and for replicating these repos understanding whether you are getting similar results to what the repo says and maybe going one step ahead and doing better than what the repo has done so this is the state of the art right now with respect to tool integrated reasoning so i have just marked here a bunch of tools where we have already seen artist we have seen dpies we have seen tool rl but there are some other agentic tool integrated reasoning solutions also okay so now if you look at it these are all open source for closed source all of these research labs have come up with their own variants this one which i just showed you right now this is open as deep research tool kimi k2 qwq then this one i have not personally used even gemina has a research agent deep research agent that one is pretty good so all of these are tool integrated rl which means the llm is actually deciding which tool to use when to use and how to use those tools so in today's class what we are going to do is that we are going to replicate a very powerful example where we are going to just use one tool but we are going to use that tool with reinforcement learning and we are going to do one agentic rag application with that whole pipeline line now we have seen the first application of agentic rl which is in in tool use the second modality is reasoning now reasoning is something we have already covered in the class broadly speaking reasoning can be divided into fast reasoning and slow reasoning and previously before 2025 there was little connection between reinforcement learning and reasoning but only when the deep seek r10 model came out in the deep seek technical report people realized that you could train llms to autonomously develop reasoning skills uh using the algorithm of grpo and then this is an example of modifying the weights of the llms using reinforcement learning and imbibing reasoning in the llm but there are some other techniques also which people use to induce reasoning one of them is is chain of thought reasoning uh another is a zero shot or few thought prompt a few shot prompting for example just adding a simple prompt like think about this step by step or let's think about this in a step by step manner just using this forces the model to output its completion by giving the chain of thoughts along with the final answer so i have a complete lecture on this in the reasoning llm from scratch series which is there on youtube so you can refer to that so broadly speaking these methods are called test time compute methods the reason they are called test type compute or inference time compute methods is you're not doing anything with the weights of the llm the weights are frozen but you are trying to induce reasoning capabilities inside the llm by doing some post-training techniques such as chain of thought reasoning or few short or zero uh zero short prompting so uh this also works very well so there are there are multiple techniques used to imbibe reasoning in llms for us we are currently going to focus on this first section which is a reasoning imbibed by changing the weights of the model by fine-tuning the model using reinforcement learning so if you looked at this example closely in fact all of the github repos which i showed you many of them use grpo and they they get this pattern where the llm learns to reason for example let me show you this repo which we are going to replicate today uh this one also uses grpo in the agentic rack pipeline and you can see that in the answer the llm there is a reasoning step here there like within the reasoning open and closing tags you can see that the llm is trying to think of something so this happens because of the grpo step within the fine-tuning process so so far we have seen uh two modalities of agentic reinforcement learning the first is tool usage and the second is inducing reasoning capabilities to the model and we are going to look at an example which integrates both of this very nicely today uh okay so the application of agentic rl there are four main applications the first is called as search and information retrieval this is exactly the example which i showed you over here and we are going to take a practical example in this same category today which is search and information retrieval the second is code generation and software engineering the third is mathematical reasoning and the fourth is vision understanding tasks so let's try to understand uh the first one first which is the search and research agents so this search and research agents there are two types one are closed source agent and second are open source agents so within the closed source research agents you have all these famous ones open as deep research perplexities deep research google gemini's deep research kimi researcher grok ai deep research etc but no one knows the recipe behind these agents exactly how they work people know that they are using reinforcement learning somewhere but no one really knows what is happening under the hood and uh i wanted to show you this uh two weeks back there was this github repo which was released called uh i don't know how to pronounce this so i'm calling it tongi deep research so if you look at it uh this is developed by the nlp team at alibaba and they have got