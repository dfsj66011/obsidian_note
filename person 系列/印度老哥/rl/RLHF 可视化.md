

首先，你可以看到步骤数量在增加，这意味着正在进行某种训练。你可以看到这些权重也在变化，这个delta值在变化，随着训练的进行，损失值随时间递减。你还可以看到这是与正样本一起传递的掩码，这是与负样本一起传递的掩码。接下来，让我们试着理解这个训练过程中到底发生了什么。

因此，我们计算损失的方式是：我们希望了解奖励模型是否为所选回答给予更多奖励，而对被拒绝的回答给予较少奖励。理想情况下，我们希望最大化差值，其中差值为所选回答的奖励减去被拒绝回答的奖励。所以，你可以看到delta值随时间递增，从0.71、0.72等逐步上升。Delta值增加意味着我们的模型正在扩大所选答案与被拒答案之间的差距——这正是我们想要的效果。损失值是根据delta计算的：delta越高，损失越低；delta越低，损失越高。

所以，正如我所说，我们不会深入探讨如何计算这个损失的数学细节，但对我们来说，目前最重要的是直观理解。比如说，如果你将这些选中的样本和被拒绝的样本通过奖励模型，得到一个负的差值，这意味着你需要更多的训练，以便差值首先变为正值，然后你再增加这个差值的数值。

现在，这里发生的情况是，我已经提到了“head weights”，那么在LLM架构中，“head weights”是什么意思呢？我们有一个transformer块，在此之前有一个输入块，然后是transformer块，接着是输出块。

现在，奖励模型完全相同，唯一的区别在于输出块被替换为一个单一的线性层，该层输出单个奖励值。因此，这个单一的线性层关联了大量权重参数，而我们正在训练的就是这些权重——你可以在训练过程中观察到它们的调整变化。

因此，我们冻结了整个Transformer和输入块，完全不进行修改。唯一需要调整的是紧接在Transformer块之后的线性层。这就是整个训练过程，奖励模型实际上就是这样训练的。我这里只举了一个被选中样本和被拒绝样本的例子，但通常会有大量样本，以确保奖励模型非常准确和稳健。

好的，现在我们已经了解了奖励模型的数据处理方式以及奖励模型的训练过程。接下来，让我们理解整个PPO训练流程。我将这个流程分为六个步骤，并将从头开始向你解释这些步骤。第一步是，我们如何为语言模型创建输入输出对。

那么，让我们举一个例子，这个例子我们将在整个流程的六个步骤中反复使用。提示词是“浦那在哪里？”，回答是“浦那在印度”，完整序列就是“浦那在哪里？浦那在印度”。好的，要生成这个完整回答，你需要使用model.generator命令。这个命令会将提示词和回答结合起来，输出完整的序列。

所以，这也是我可能会用“轨迹”这个词来描述的情况。一条单一的轨迹意味着由我们的大型语言模型提供的一个完整输出。这是第一步。在下一步中，我们要做的是对这个序列进行标记化处理。也就是说，我们不再用英语书写这个序列，而是以数字的形式来表示它，其中每个数字对应一个标记。

下一步是创建输入和输出对。在代码中可以看到，有一行是将最后一个标记移除后的完整内容，而第二行则是移除第一个标记后的完整内容。最初看到这段代码时，我很惊讶，仅凭一行代码如何能创建输入输出对？但通过这个可视化过程，现在可以清楚地看到：在输入部分，我们考虑了除最后一个标记（即"Pune在哪里？"）之外的所有标记。

浦那位于印度。我们不考虑句号，然后在目标中我们做的是考虑除第一个标记外的所有标记。因此，“where”被忽略了。所以它从“is”开始，以句号结束。那么现在我们得到的是，我们为每一个标记都有一个输入输出映射，例如对于“where”，输出是“is”；对于“is”，输出是“浦那”；对于“浦那”，输出或目标是问号等等。

所以，我们用这一行代码创建了七个输入输出对，这就是输入输出对的创建方式。问题是，我们为什么要创建这些输入输出对？这对PPO训练有什么意义？尽管我们在强化学习中称之为输入和输出，但这些实际上就是我提到的状态和动作。输入就是状态，如S0、S1、S2、S7，而输出则是我们的智能体（即语言模型）针对特定状态所采取的动作。因此，对于任何强化学习任务，第一步都是确定状态和动作是什么。

如果你在下国际象棋，那么棋子的布局就是状态，而动作则是玩家在棋盘上走的棋步。类似地，这里的状态是LLM预测的当前标记，动作是LLM预测的下一个标记，这些状态-动作对就是通过这行代码创建的。那么，我们该如何处理这些状态-动作对呢？

那么，首先让我们直观地了解这些训练对是什么样子的，这样你就不会忘记这个过程。我们创建状态-动作对的原因在于，如果你还记得我们将用于性能度量梯度的公式，我们需要给定策略下动作的对数概率。

因此，为了计算对数概率，我们首先需要理解什么是状态和动作，这正是我们在这一步要做的。现在让我们来了解对数概率是如何计算的。首先，输入目标标记对已经创建好了，我们在下一步已经理解了这一点。接下来我们要做的是理解Transformer架构的流程，看看这些对数概率是如何计算的。

所以，如果你看一下Transformer架构，它由三个模块组成：第一个是输入模块，包括词嵌入和位置编码；第二个是Transformer模块，其中有多头注意力、归一化、前馈神经网络、Dropout层等。

我希望你重点关注最后一层，也就是线性投影层。在线性投影层中，隐藏表示中的每个神经元都会被投影到一个词汇表大小的维度上，假设词汇表中有50,000个标记。

因此，每个隐藏状态都会被映射到词汇表的大小，并且我们会为每个映射计算原始逻辑值。例如，如果我们将一个神经元映射到50,000个值，我们会分别计算所有这些不同的值，然后通过对这些原始逻辑值应用softmax函数，将这些值转换为概率。

所以，我目前所说的所有内容，请将注意力集中在我最后提到的这一点上：在Transformer架构中，我们得到的是——当你输入一个单一的token时，最终你会得到一个针对该输入token在整个词汇空间所有token上的概率分布。现在你可能就能理解如何计算这些动作的对数概率了。

我们只是将其映射到由LLM执行的操作上，这一点在下一步我们将可视化模型前向传播时会变得清晰。因此，在这里，如果你看到输入标记的情况，其中可能有大量的输出标记，这就是动作空间，与词汇空间相同。

因此，智能体可以采取50,000种动作，但我只选择了其中一个动作“is”，并计算了其概率。最初我们得到的logits值为8.5，接着我们计算下一个标记“is”的logits值。同样有50,000种可能的动作，但我只选择了动作“UNE”，得到的logits值为9.1。类似地，我为序列中所有输入标记的对应动作计算了logits值。

接下来这一步很简单，我们只需将这些逻辑值转换为概率分数。例如，当某个状态的概率分数为-0.008，另一个状态的概率分数为-0.005时，我们实际上已经将每个标记都转换为了一个输出概率。这个概率仅针对为该特定标记所选择的动作进行计算。

好的，这一步是计算对数概率的步骤，而对数概率是整个PPO训练循环的核心。因为如果你看一下梯度公式，对数概率就位于该公式的中心位置，所以对我们来说计算它非常重要。

接下来我们要做的是理解价值模型。我们究竟如何计算这些价值？在某些文献中，这也被称为批判模型。实际上，价值模型与大型语言模型本身非常相似，但有一个关键的区别出现在最后阶段。

正如你在可视化中所见，让我们看看当标记通过LLM时会发生什么。第一步是标记被嵌入，这就是标记嵌入过程。接下来，标记会通过转换器块，经过注意力层，再通过前馈网络，你可以看到维度保持不变，仍然是768，就像它从转换器块出来时一样。

现在，当它从transformer模块输出时，在实际的大型语言模型中，它会经过一个最终的投影层。但在一个价值模型中有一个小区别：你会完全移除投影层，并用一个单一的线性层来替代它。因此，变压器块的输出被映射到一个单一的值，即价值头。所以，基本上输入块和变压器块保持不变，但投影或输出块现在被移除，并被替换为一个单一的线性层，该层输出价值。

因此，你可以从这个可视化中看到，权重矩阵的尺寸是768乘以1，这里的1是一个单一的标量值，我们将其作为输出，而不是将其投影到词汇空间。我们对序列中的所有标记都这样做。因此，序列中的每一个标记都会有一个与之相关的单一值。所以，在我们继续并查看PPO训练循环之前，你可以把这个可视化记在心里。

但本质上，当人们使用价值模型时，他们会冻结输入层权重和转换器权重，唯一被修改的是线性层中的权重，这样训练速度会快得多。好的，现在我们已经了解了如何计算对数概率和价值模型，下一步非常重要，就是计算优势。那么，优势究竟是如何计算的呢？让我们再次以同样的例子为例：浦那在哪里？浦那在印度。

首先，为了计算优势，我们将再次计算所有标记的对数概率。但这次我们不仅会计算正在训练的模型的对数概率，还会计算参考模型的对数概率。这个参考模型是在监督微调阶段之后得到的模型。问题是，我们为什么要为两个不同的模型计算这个？原因是我们不希望当前训练的模型偏离参考模型太远。

因此，如果模型偏离得太远，我们就必须施加惩罚，这个惩罚是通过测量我们正在训练的模型与参考模型之间的KL散度来计算的。KL散度的计算公式非常简单，我们只需用正在训练的模型的对数概率减去参考模型的对数概率即可。所以，你可以看到，对于所有这些标记，我分别计算了它们的KL散度，而高KL散度对我们来说是不利的，因为它意味着模型正在偏离参考策略。

因此，在构建奖励函数时，KL散度前应添加负号以满足这一条件。计算完KL散度后，下一步就是计算奖励值。在典型的强化学习问题中，环境会针对每个状态和动作给予奖励——让我们举个例子来说明。假设你正在学习骑自行车，每次都在改变踏板的位置。因此，每次对自行车转向进行微调后，你都会从环境中得到反馈。

因此，在整个学习周期中，你会不断获得奖励。而大语言模型的不同之处在于，奖励只有在句子完成时才会获得，过程中不会得到。例如，在这句话中，“浦那在哪里？浦那在印度。”如果我只孤立地看“浦那在哪里？浦那”，那么获得奖励是没有意义的，因为句子还没有完成。所以，只有在所有状态和动作都完成且这一回合结束后，才能获得奖励。

这就是为什么我们只会在完成一个章节后给予一次奖励，并将其附加在末尾。因此，你可以认为这个奖励是针对文本结束标记的。那么对于所有这些标记来说，浦那在哪里？浦那在印度。我们没有奖励。我们唯一给予的奖励是针对文本结束标记的，如这里所写的是2.5。所有其他值仅仅是KL散度。

好的，那么现在我们所做的是：在这个综合得分向量中，首先将KL散度乘以一个负号，然后再乘以一个超参数beta。好了，我们已经构建了我们的得分向量，其中包含了所有标记的KL散度，并在完成时附加了一个单独的奖励。

现在，我们的下一步是计算优势。为了计算优势，我们首先需要预测所有这些标记的值，而这我们已经知道该如何操作。我们使用之前介绍过的价值模型的预测结果，利用该模型来预测我们在序列中遇到的所有标记的值。例如，在这里我们预测“Pune在哪里？Pune在印度。”这些句子的值。

我们还会在文本标记的末尾分配一个问号或赋予一个值，通常为零。首先，我们计算一个得分向量，然后预测这些值。请注意，这些值的预测未来可能会发生变化，因为值模型也在由我们进行训练。

现在，在下一步中，我们计算优势值，并通过广义优势估计公式迭代计算这些优势值。我不会详细介绍所有步骤，但本质上是从后向前推进的——从第八步开始，然后如动画所示，逐步向上移动到第七步、第六步、第五步、第四步、第三步，直到第一步。最终，你会得到所有这些标记的优势值。

你也可以计算所有这些代币的回报。这些回报无非就是优势加上奖励。好了，在整个模块的最后，我们所做的就是学会了如何计算序列中所有代币的优势，以及如何计算序列中所有代币的回报。

现在，看起来我们已经准备好了所有的部分。我们只需要在PPO训练循环中将它们组合起来。但在我们这样做之前，还有最后一步，那就是填充和掩码步骤。那么，为什么这一步很重要呢？假设模型正在生成大量的轨迹。

每个轨迹的提示长度和回答长度都会有所不同。因此，完成部分的长度也会各不相同。通常的做法是对数据进行填充，使填充后的序列长度与块大小相匹配。在这个例子中，我将块大小设为16。例如："浦那在哪里？浦那在印度。"

你可以看到总共有9个标记。所以我们有9个优势，9个回报，还有18个对数概率。现在我们需要把所有这些增加到16个。因此，我们在其后添加了一个填充层，您可以看到我们在末尾添加了值为0的部分。这些值本身没有意义，但我们这样做是为了能够对输入进行批处理。而要创建批次，我们不能让输入的长度各不相同。

我们确保所有输入具有相同的长度，即块大小（在本例中为16）的长度。然后，我们对优势、回报进行填充，接着也对对数概率进行填充。随后，我们对完成标记和目标标记也进行填充。最后一步，我们创建一个动作掩码。为什么要创建动作掩码呢？

因此，当你生成一个轨迹时，你有提示词、响应和填充部分。现在，当你修改LLM的权重时，提示词和填充部分对你来说并不重要，只有完成部分才是重要的。那么，如何告诉模型只有完成部分是重要的，而忽略提示词和填充部分的标记呢？

这是通过创建一个掩码来实现的。掩码的作用是告诉LLM（大型语言模型）：这是序列，忽略这些标记，只关注这些标记。在动作掩码中，我们希望忽略的标记被赋值为0，而希望选中的标记则赋值为1。"mask"这个词可能有点令人困惑，但实际上它只是一个由0和1组成的数值数组。

0表示忽略该标记，1表示不忽略该标记。这就是整个过程。在这一步中，我们对优势、回报、对数概率、完成标记和目标标记进行填充，然后在接近尾声时创建一个动作掩码。现在我们已经完全准备好可视化PPO训练循环了。

那么PPO训练中会发生什么（这也是实践中使用该方法时最大的问题），我们同时在并行训练四个不同的模型。我们正在训练LLM（大语言模型），目的是使其与人类偏好对齐——这个模型正处于训练阶段。同时我们还有价值模型，其架构与LLM完全相同，只是输出层被替换成了线性层。这两个模型都在进行主动训练。

有两个模型被冻结了，即奖励模型和参考模型，我们想用它们来与大语言模型进行比较。虽然这些模型被冻结了，但它们仍然存储在内存的某个地方。所有这些权重都存储在某个位置。在实际训练基于PPO的RLHF框架时，会消耗大量内存，该框架包含四个不同的模型。实际操作中，我们会创建或展开批次处理——比如可以用大语言模型批量生成大量文本序列，但不会一次性生成太多。因为推理过程本身也需要较长时间。

所以你推出这些小批次，以我举的例子来说，每个小批次包含四个序列。总体而言，可能有16个或12个序列。但在一个批次中，你一次只处理四个序列。总的来说，你可能需要在一个epoch中处理多个批次。那么现在这种情况下，我接收了第一个小批次，里面有四个序列。浦那在哪里？浦那在印度。

孟买在哪里？孟买在印度。德里在哪里？德里在印度。那么浦那在哪里？浦那。什么是浦那？浦那是一座城市。因此，在一个小批次中有四个序列。在为每个序列收集完这些数据后，我们计算对数概率。例如，问“浦那在哪里？”浦那在印度。我们使用之前解释过的方法计算所有这些标记的对数概率。然后我们计算这四个序列中生成的所有这些标记的优势和回报。

然后我们的做法是，通过随机排列生成这些小批次，从不同的序列中创建排列。例如，如果你有12个序列，我们想要整体处理，而在第一批次中，我只想选择四个序列。那么这四个序列是随机选择的。例如，序列0、序列3、序列7和序列9可以作为第一批。然后，在这些小批次被选择之后，我们执行第一次前向传递。在前向传递中，发生的情况是，对于你采样的序列，模型会根据正在训练的策略生成对数概率。

然后你用所有这些值来计算PPO损失。现在，正如我在这一步提到的，PPO损失是通过将三个损失相加来计算的。那么让我们来理解一下这三个损失是如何计算的。首先，我们来看序列0的第4个位置，对应的标记是"booming"。这里我们已经计算了RT，即新策略与旧策略的比率。这个值是在前向传播过程中计算得出的。优势值已经被采样，并且我们在前向传播之前就已经计算好了这些优势值。因此我们不需要重新计算，而是直接使用这个已有的值。我们将这个值乘以R，然后计算这个经过裁剪的值。

我们将R的这一片段乘以优势函数，然后计算PPO损失。我们对所有序列中的每个标记都执行这一操作，并取其平均值。这样就得到了平均PPO损失。接下来计算的是价值损失，通过取价值模型预测与回报之间的均方误差来计算，这些回报是在计算优势函数时得出的。同样，我们对所有标记执行此操作并取其平均值。

最后一步是熵损失。熵损失通常通过查看概率分布或策略本身来计算。我们计算所有这些的平均值并将其相加，得到总体损失。让我们播放动画以便您理解我们的操作。我们遍历所有这些标记，然后为每个标记计算PPO损失。之后是反向传播步骤，实际上这一步在此之前已经发生。

我会在反向传播步骤暂停，在我们处理第二个小批次的时候。但基本上在反向传播步骤中，我们所做的就是简单地反向传播损失，然后我们计算，根据梯度下降法更新权重，这与我们在机器学习模型中所做的非常相似。让我们来看看，这就是反向传播过程。整个目标是最小化这个损失。然后，一旦所有三个小批次处理完毕，我们会根据训练周期中设定的轮数（epoch）再进行一轮。同样地，我们会重复相同的过程，计算所有未被掩码标记的新对数概率。

然后我们利用样本轨迹已经计算好的优势和返回值。接着计算PPO损失、价值损失和熵。我们将所有这些组合起来构建一个聚合损失函数，在整个过程中需要最小化这个函数。好的，这就是PPO训练循环的工作原理。总结一下，我们首先讨论了奖励模型的构建。接下来我们讨论了PPO训练。PPO训练基于一组策略采样方法，其中对数概率、优势函数和回报值都已计算并存储在内存中。

但我们必须根据正在训练的策略计算对数概率，这在前向传播过程中完成。然后所有这些值一起用于计算PPO损失，接着通过反向传播，我们根据梯度下降规则更新权重。非常感谢大家。
