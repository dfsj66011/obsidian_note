
在这节课结束时，我们将定义并训练一个神经网络，你将有机会深入了解其内部运作机制，直观感受它是如何运作的。具体来说，我想带你们一步步构建 micrograd。

## micrograd 概览

micrograd 本质上是一个自动微分引擎。它实现了反向传播算法，反向传播是一种能够高效计算神经网络权重相对于某种损失函数梯度的算法。这样一来，我们就能通过迭代调整神经网络各层权值，使损失函数最小化，从而提升网络预测精度。反向传播算法正是现代深度学习框架（如 PyTorch）的数学核心所在。

micrograd 是一个标量值自动求导引擎，它工作在单个标量的层面上，比如 -4 和 2。这只是出于教学目的，因为它能让我们不必处理那些在现代深度神经网络库中会用到的 $n$ 维张量，所以这样做是为了让你理解并重构反向传播和链式法则，以及理解神经网络的训练过程。这就是编写 micrograd 的根本原因，因为你可以从基础层面理解事物是如何运作的。之后你可以再对它进行加速。

我的观点是，micrograd 就是训练神经网络所需的一切，其他都只是效率问题。所以你可能以为 micrograd 会是一段非常复杂的代码。但事实证明并非如此。实际实现反向传播自动微分的引擎，赋予神经网络能力的核心代码，仅用 100 行极其简洁的 Python 就完成了——这堂课结束前我们就能完全掌握它。

## 简单函数的导数

```python
import math
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

首先定义一个标量值函数 $$f(x)=3x^2-4x+5$$

```python
def f(x):
    return 3*x**2 - 4*x + 5
```

这是一个二次函数，我们可以创建一个标量值集合并作图，会得到一个漂亮的抛物线

```python
xs = np.arange(-5, 5, 0.25)
ys = f(xs)
plt.plot(xs, ys)
```

![|350](data:image/png;base64,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)

现在我们思考一下这个函数在任一 $x$ 处的导数是多少？如果你还记得微积分课，你可能已经推导过导数。不过我们实际上不会这么做，因为在神经网络领域，根本没人会去写出神经网络的数学表达式。这将是一个庞大的表达式，它会有成千上万个项。

因此，我们不会采用这种符号化的方法。相反，回忆一下导数的定义：

$$L = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}
$$

这是关于可微性的定义。如果你还记得微积分中的内容，它就是当 $h$ 趋近于 $0$ 时，$[f(x+h) - f(x)]/h$ 的极限。基本上它表达的意思是：如果你在某一点 $x$（或 $a$）处稍微增加一个很小的数 $h$，函数会如何响应？它的响应灵敏度是多少？该点的斜率是多少？函数是上升还是下降？变化幅度有多大？这就是该函数在该点的斜率，即响应的斜率。

因此，我们可以通过取一个非常小的 $h$ 来数值计算这里的导数。当然，定义要求我们让 $h$ 趋近于 $0$。我们只需要选取一个非常小的 $h$，比如 $0.001$。假设我们关注的点是 $3.0$。那么我们可以把 $f(x)$ 看作 20。现在来看 $f(x + h)$，如果我们稍微向正方向推动 $x$，函数会如何响应？

```python
```python
h = 0.001
x = 3.0
f(x)
# 20.0
f(x+h)
# 20.014003000000002
f(x+h) - f(x)
# 0.01400300000000243
(f(x+h) - f(x)) / h
# 14.00300000000243
```

当然，这只是斜率的数值近似，因为我们必须让 $h$ 非常非常小才能收敛到精确值。但如果用了太多零，在某些情况下会得到错误答案，因为我们使用的是浮点运算，而所有这些数字在计算机内存中的表示都是有限的，到某个临界点就会出现问题。

----
## Value 对象及可视化

由于神经网络会是非常庞大的数学表达式。因此我们需要一些数据结构来维护这些表达式。这就是我们现在要开始构建的内容。

```python
class Value:
    
    def __init__(self, data, _children=(), _op=""):
        self.data = data
        self._prev = set(_children)
        self._op = _op
        
    def __repr__(self):
        return f"Value(data={self.data})"
    
    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), "+")
        return out
    
    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), "*")
        return out
```

我们先构建一个非常简单的 Value 对象的框架。这个类 Value 接受一个单一的标量值，将其包装并跟踪。并能够进行 `a+b` 这样的操作。基本上，在 Python 中你需要使用这些特殊的双下划线方法来为这些对象定义运算符。使用加号运算符，Python 内部会调用 `a.__add__(b)`，这就是内部实际发生的过程。同理乘法运算也是如此：

正如前面提到的，我们希望保留这些表达式图。因此我们需要了解并保存关于哪些值生成其他值的指针。例如在这里，我们将引入一个新变量，我们称之为 `_children`，默认情况下，它将是一个空元组。然后我们实际上会在类中保留一个稍有不同的变量，我们称之为 `_prev`，它将是子节点的集合。

在最初的 micrograd 项目中就是这么做的，具体原因记不太清了，应该是出于效率考虑，不过为了方便起见，这个 `_children` 会是个元组。但在实际在类中维护时，为了效率考虑，我认为它应该就是这个集合。因此，当我们像这样通过构造函数创建值时，`_children` 会是空的，而 `_prev` 会是空集。

然而，当我们通过加法或乘法创建值时，我们会传入该值的 `_children`，在这里就是 `self` 和 `other`。这些就是这里的子节点。

现在我们还有最后一条信息未知。我们已经知道了每个值的子节点，但还不知道是哪个运算生成了这个值。因此，我们还需要一个元素，我们称之为 `_op`。

现在，由于这些表达式即将变得相当庞大，我们需要将其可视化出来。

```python
from graphviz import Digraph

def trace(root):
    # builds a set of all nodes and edges in a graph
    nodes, edges = set(), set()
    def build(v):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child, v))
                build(child)
    build(root)
    return nodes, edges

def draw_dot(root):
    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right
  
    nodes, edges = trace(root)
    for n in nodes:
        uid = str(id(n))
        # for any value in the graph, create a rectangular ('record') node for it
        dot.node(name = uid, label = "{ data %.4f }" % (n.data, ), shape='record')
        if n._op:
            # if this value is a result of some operation, create an op node for it
            dot.node(name = uid + n._op, label = n._op)
            # and connect this node to it
            dot.edge(uid + n._op, uid)

    for n1, n2 in edges:
        # connect n1 to the op node of n2
        dot.edge(str(id(n1)), str(id(n2)) + n2._op)

    return dot
```

我们还可以给这些图表加上标签，这样就能知道变量都在哪里了。那就让我们创建一个标签。

```python
def __init__(self, data, _children=(), _op="", label=""):
	self.data = data
	self._prev = set(_children)
	self._op = _op
	self.label = label

a = Value(2.0, label="a")
b = Value(-3.0, label="b")
c = Value(10.0, label="c")
e = a * b; e.label = "e"
d = e + c; d.label = "d"
```

```python
# 代码更新
dot.node(name = uid, label = "{ %s | data %.4f }" % (n.label, n.data), shape='record')
```

最后，让我们把这个表达式再加深一层，在 `d` 之后，创建 `f` 的 Value 对象，值为 $-2.0$。`L` 将是我们图的输出。`L=d*f`。因此，输出结果  `L` 将为 $-8$。

```python
f = Value(-2， label="f")
L = d * f; L.label ="L"

draw_dot(L)
```

![|600](data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C!DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%0A%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3C!--%20Generated%20by%20graphviz%20version%202.44.0%20(0)%0A%20--%3E%0A%3C!--%20Pages%3A%201%20--%3E%0A%3Csvg%20width%3D%22913pt%22%20height%3D%22128pt%22%0A%20viewBox%3D%220.00%200.00%20913.00%20128.00%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%3E%0A%3Cg%20id%3D%22graph0%22%20class%3D%22graph%22%20transform%3D%22scale(1%201)%20rotate(0)%20translate(4%20124)%22%3E%0A%3Cpolygon%20fill%3D%22white%22%20stroke%3D%22transparent%22%20points%3D%22-4%2C4%20-4%2C-124%20909%2C-124%20909%2C4%20-4%2C4%22%2F%3E%0A%3C!--%20140014807801376%20--%3E%0A%3Cg%20id%3D%22node1%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140014807801376%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22519%2C-82.5%20519%2C-118.5%20648%2C-118.5%20648%2C-82.5%20519%2C-82.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22530%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ef%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22541%2C-82.5%20541%2C-118.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22594.5%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526748720*%20--%3E%0A%3Cg%20id%3D%22node3%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526748720*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22711%22%20cy%3D%22-72.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22711%22%20y%3D%22-68.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140014807801376%26%2345%3B%26gt%3B140013526748720*%20--%3E%0A%3Cg%20id%3D%22edge9%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140014807801376%26%2345%3B%26gt%3B140013526748720*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M648.35%2C-86.25C657.65%2C-84.18%20666.92%2C-82.11%20675.33%2C-80.23%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22676.13%2C-83.64%20685.13%2C-78.05%20674.6%2C-76.81%20676.13%2C-83.64%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526748720%20--%3E%0A%3Cg%20id%3D%22node2%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526748720%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22774%2C-54.5%20774%2C-90.5%20905%2C-90.5%20905%2C-54.5%20774%2C-54.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22786%22%20y%3D%22-68.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3EL%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22798%2C-54.5%20798%2C-90.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22851.5%22%20y%3D%22-68.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B8.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526748720*%26%2345%3B%26gt%3B140013526748720%20--%3E%0A%3Cg%20id%3D%22edge1%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526748720*%26%2345%3B%26gt%3B140013526748720%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M738.21%2C-72.5C745.92%2C-72.5%20754.76%2C-72.5%20763.92%2C-72.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22763.96%2C-76%20773.96%2C-72.5%20763.96%2C-69%20763.96%2C-76%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751264%20--%3E%0A%3Cg%20id%3D%22node4%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751264%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22260%2C-55.5%20260%2C-91.5%20392%2C-91.5%20392%2C-55.5%20260%2C-55.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22272.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ee%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22285%2C-55.5%20285%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22338.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B6.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751216%2B%20--%3E%0A%3Cg%20id%3D%22node10%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751216%2B%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22456%22%20cy%3D%22-45.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22456%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E%2B%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751264%26%2345%3B%26gt%3B140013526751216%2B%20--%3E%0A%3Cg%20id%3D%22edge8%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751264%26%2345%3B%26gt%3B140013526751216%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M392.12%2C-59.25C401.89%2C-57.12%20411.62%2C-54.99%20420.41%2C-53.07%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22421.21%2C-56.47%20430.23%2C-50.92%20419.72%2C-49.64%20421.21%2C-56.47%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751264*%20--%3E%0A%3Cg%20id%3D%22node5%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751264*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22196%22%20cy%3D%22-73.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22196%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751264*%26%2345%3B%26gt%3B140013526751264%20--%3E%0A%3Cg%20id%3D%22edge2%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751264*%26%2345%3B%26gt%3B140013526751264%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M223.21%2C-73.5C231.19%2C-73.5%20240.39%2C-73.5%20249.93%2C-73.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22249.96%2C-77%20259.96%2C-73.5%20249.96%2C-70%20249.96%2C-77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013528028736%20--%3E%0A%3Cg%20id%3D%22node6%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013528028736%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%223.5%2C-83.5%203.5%2C-119.5%20129.5%2C-119.5%20129.5%2C-83.5%203.5%2C-83.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2216%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ea%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2228.5%2C-83.5%2028.5%2C-119.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2279%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%202.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013528028736%26%2345%3B%26gt%3B140013526751264*%20--%3E%0A%3Cg%20id%3D%22edge7%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013528028736%26%2345%3B%26gt%3B140013526751264*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M129.76%2C-87.83C140.25%2C-85.52%20150.79%2C-83.21%20160.25%2C-81.13%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22161.03%2C-84.54%20170.04%2C-78.98%20159.52%2C-77.71%20161.03%2C-84.54%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526750064%20--%3E%0A%3Cg%20id%3D%22node7%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526750064%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%220%2C-28.5%200%2C-64.5%20133%2C-64.5%20133%2C-28.5%200%2C-28.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2213%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Eb%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2226%2C-28.5%2026%2C-64.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2279.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B3.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526750064%26%2345%3B%26gt%3B140013526751264*%20--%3E%0A%3Cg%20id%3D%22edge5%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526750064%26%2345%3B%26gt%3B140013526751264*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M133.12%2C-60.4C142.49%2C-62.38%20151.8%2C-64.35%20160.25%2C-66.14%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22159.57%2C-69.58%20170.08%2C-68.22%20161.02%2C-62.73%20159.57%2C-69.58%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526750592%20--%3E%0A%3Cg%20id%3D%22node8%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526750592%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22259%2C-0.5%20259%2C-36.5%20393%2C-36.5%20393%2C-0.5%20259%2C-0.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22271%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ec%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22283%2C-0.5%20283%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22338%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%2010.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526750592%26%2345%3B%26gt%3B140013526751216%2B%20--%3E%0A%3Cg%20id%3D%22edge6%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526750592%26%2345%3B%26gt%3B140013526751216%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M393.25%2C-32.47C402.62%2C-34.45%20411.92%2C-36.41%20420.36%2C-38.19%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22419.65%2C-41.62%20430.16%2C-40.26%20421.1%2C-34.77%20419.65%2C-41.62%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751216%20--%3E%0A%3Cg%20id%3D%22node9%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751216%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22520%2C-27.5%20520%2C-63.5%20647%2C-63.5%20647%2C-27.5%20520%2C-27.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22533%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ed%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22546%2C-27.5%20546%2C-63.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22596.5%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%204.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751216%26%2345%3B%26gt%3B140013526748720*%20--%3E%0A%3Cg%20id%3D%22edge4%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751216%26%2345%3B%26gt%3B140013526748720*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M647.25%2C-59C656.94%2C-61.08%20666.63%2C-63.17%20675.4%2C-65.06%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22674.69%2C-68.48%20685.21%2C-67.17%20676.17%2C-61.64%20674.69%2C-68.48%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751216%2B%26%2345%3B%26gt%3B140013526751216%20--%3E%0A%3Cg%20id%3D%22edge3%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751216%2B%26%2345%3B%26gt%3B140013526751216%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M483%2C-45.5C491%2C-45.5%20500.22%2C-45.5%20509.76%2C-45.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22509.77%2C-49%20519.77%2C-45.5%20509.77%2C-42%20509.77%2C-49%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A)


快速回顾一下到目前为止的成果。我们已经能够仅用加法和乘法构建数学表达式，且这些表达式在计算过程中都是标量值。我们可以进行前向传递并构建出一个数学表达式。这里有多个输入 `a`、`b`、`c` 和 `f`，它们进入一个数学表达式后产生单个输出 `L`。前向传递的输出结果是 $-8$，这就是最终数值。

## backpropagation

在反向传播过程中，我们将从最终结果开始，逆向计算所有这些中间值的梯度。实际上，我们会为这里的每个值计算该节点相对于 `L` 的导数—— `L` 对 `L` 的导数自然是 $1$。然后我们将依次推导出 `L` 对 `f`、`D`、`C`、`E`、`B` 以及 对 `A` 的导数。在神经网络环境中，我们最关注的是这个损失函数 `L` 相对于神经网络权重的导数。虽然这里我们只有变量 `a`、`b`、`c` 和 `f`，但其中某些变量最终会代表神经网络的权重。因此我们需要了解这些权重是如何影响损失函数的。

接下来，我们将在 Value 类中创建一个变量，用于保存 `L` 相对于该值的导数。我们将这个变量称为`grad`，初始时它为零。

```python
def __init__(self, data, _children=(), _op="", label=""):
	self.data = data
	self.grad = 0
	self._prev = set(_children)
	self._op = _op
	self.label = label
```

```python
# 代码更新
dot.node(name = uid, label = "{ %s | data %.4f | grad %.4f}" % (n.label, n.data, n.grad), shape='record')
```

![|700](data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C!DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%0A%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3C!--%20Generated%20by%20graphviz%20version%202.44.0%20(0)%0A%20--%3E%0A%3C!--%20Pages%3A%201%20--%3E%0A%3Csvg%20width%3D%221325pt%22%20height%3D%22128pt%22%0A%20viewBox%3D%220.00%200.00%201325.00%20128.00%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%3E%0A%3Cg%20id%3D%22graph0%22%20class%3D%22graph%22%20transform%3D%22scale(1%201)%20rotate(0)%20translate(4%20124)%22%3E%0A%3Cpolygon%20fill%3D%22white%22%20stroke%3D%22transparent%22%20points%3D%22-4%2C4%20-4%2C-124%201321%2C-124%201321%2C4%20-4%2C4%22%2F%3E%0A%3C!--%20140013524541968%20--%3E%0A%3Cg%20id%3D%22node1%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524541968%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22362%2C-83.5%20362%2C-119.5%20599%2C-119.5%20599%2C-83.5%20362%2C-83.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22374%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ec%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22386%2C-83.5%20386%2C-119.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22441%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%2010.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22496%2C-83.5%20496%2C-119.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22547.5%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22node3%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542496%2B%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22662%22%20cy%3D%22-73.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22662%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E%2B%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541968%26%2345%3B%26gt%3B140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22edge5%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524541968%26%2345%3B%26gt%3B140013524542496%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M596.96%2C-83.49C607.12%2C-81.91%20616.8%2C-80.4%20625.41%2C-79.05%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22626.02%2C-82.5%20635.36%2C-77.5%20624.94%2C-75.58%20626.02%2C-82.5%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%20--%3E%0A%3Cg%20id%3D%22node2%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542496%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22726%2C-55.5%20726%2C-91.5%20956%2C-91.5%20956%2C-55.5%20726%2C-55.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22739%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ed%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22752%2C-55.5%20752%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22802.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%204.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22853%2C-55.5%20853%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22904.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024*%20--%3E%0A%3Cg%20id%3D%22node7%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524537024*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%221020%22%20cy%3D%22-45.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221020%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%26%2345%3B%26gt%3B140013524537024*%20--%3E%0A%3Cg%20id%3D%22edge7%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542496%26%2345%3B%26gt%3B140013524537024*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M956.1%2C-55.45C965.88%2C-53.9%20975.21%2C-52.43%20983.54%2C-51.11%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22984.19%2C-54.55%20993.52%2C-49.53%20983.1%2C-47.64%20984.19%2C-54.55%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%2B%26%2345%3B%26gt%3B140013524542496%20--%3E%0A%3Cg%20id%3D%22edge1%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542496%2B%26%2345%3B%26gt%3B140013524542496%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M689.12%2C-73.5C696.91%2C-73.5%20706.01%2C-73.5%20715.81%2C-73.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22715.82%2C-77%20725.82%2C-73.5%20715.82%2C-70%20715.82%2C-77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751360%20--%3E%0A%3Cg%20id%3D%22node4%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751360%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%220%2C-56.5%200%2C-92.5%20236%2C-92.5%20236%2C-56.5%200%2C-56.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2213%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Eb%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2226%2C-56.5%2026%2C-92.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2279.5%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B3.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22133%2C-56.5%20133%2C-92.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22184.5%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304*%20--%3E%0A%3Cg%20id%3D%22node9%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542304*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22299%22%20cy%3D%22-46.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22299%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751360%26%2345%3B%26gt%3B140013524542304*%20--%3E%0A%3Cg%20id%3D%22edge8%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751360%26%2345%3B%26gt%3B140013524542304*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M234.13%2C-56.49C244.27%2C-54.91%20253.92%2C-53.4%20262.51%2C-52.05%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22263.09%2C-55.5%20272.43%2C-50.5%20262.01%2C-48.59%20263.09%2C-55.5%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524534960%20--%3E%0A%3Cg%20id%3D%22node5%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524534960%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22725%2C-0.5%20725%2C-36.5%20957%2C-36.5%20957%2C-0.5%20725%2C-0.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22736%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ef%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22747%2C-0.5%20747%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22800.5%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22854%2C-0.5%20854%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22905.5%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524534960%26%2345%3B%26gt%3B140013524537024*%20--%3E%0A%3Cg%20id%3D%22edge6%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524534960%26%2345%3B%26gt%3B140013524537024*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M957.09%2C-36.06C966.46%2C-37.49%20975.39%2C-38.85%20983.4%2C-40.07%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22982.96%2C-43.54%20993.37%2C-41.59%20984.01%2C-36.62%20982.96%2C-43.54%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024%20--%3E%0A%3Cg%20id%3D%22node6%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524537024%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221083%2C-27.5%201083%2C-63.5%201317%2C-63.5%201317%2C-27.5%201083%2C-27.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221095%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3EL%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221107%2C-27.5%201107%2C-63.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221160.5%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B8.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221214%2C-27.5%201214%2C-63.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221265.5%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024*%26%2345%3B%26gt%3B140013524537024%20--%3E%0A%3Cg%20id%3D%22edge2%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524537024*%26%2345%3B%26gt%3B140013524537024%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1047.26%2C-45.5C1054.65%2C-45.5%201063.21%2C-45.5%201072.43%2C-45.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221072.66%2C-49%201082.66%2C-45.5%201072.66%2C-42%201072.66%2C-49%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304%20--%3E%0A%3Cg%20id%3D%22node8%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542304%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22363%2C-28.5%20363%2C-64.5%20598%2C-64.5%20598%2C-28.5%20363%2C-28.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22375.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ee%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22388%2C-28.5%20388%2C-64.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22441.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B6.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22495%2C-28.5%20495%2C-64.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22546.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304%26%2345%3B%26gt%3B140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22edge4%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542304%26%2345%3B%26gt%3B140013524542496%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M598.22%2C-64.06C607.92%2C-65.52%20617.15%2C-66.9%20625.41%2C-68.15%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22624.9%2C-71.61%20635.31%2C-69.64%20625.94%2C-64.69%20624.9%2C-71.61%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304*%26%2345%3B%26gt%3B140013524542304%20--%3E%0A%3Cg%20id%3D%22edge3%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542304*%26%2345%3B%26gt%3B140013524542304%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M326.1%2C-46.5C333.9%2C-46.5%20343.03%2C-46.5%20352.87%2C-46.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22352.93%2C-50%20362.93%2C-46.5%20352.93%2C-43%20352.93%2C-50%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541344%20--%3E%0A%3Cg%20id%3D%22node10%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524541344%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%223.5%2C-1.5%203.5%2C-37.5%20232.5%2C-37.5%20232.5%2C-1.5%203.5%2C-1.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2216%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ea%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2228.5%2C-1.5%2028.5%2C-37.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2279%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%202.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22129.5%2C-1.5%20129.5%2C-37.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22181%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541344%26%2345%3B%26gt%3B140013524542304*%20--%3E%0A%3Cg%20id%3D%22edge9%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524541344%26%2345%3B%26gt%3B140013524542304*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M232.62%2C-36.64C243.26%2C-38.24%20253.4%2C-39.77%20262.39%2C-41.13%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22261.97%2C-44.6%20272.38%2C-42.64%20263.01%2C-37.68%20261.97%2C-44.6%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A)

我们即将开始计算反向传播。当然，正如我提到的，这个梯度（grad）都是代表输出（在这里是 `L`）相对于其他值的导数。

## 手动反向传播示例 1:简单表达式

现在手动进行反向传播，从最末端开始填充这些梯度。`L` 对 `L` 的导数是 $1$。

```python
def lol():
    h = 0.001
  
    a = Value(2.0, label='a')
    b = Value(-3.0, label='b')
    c = Value(10.0, label='c')
    e = a*b; e.label = 'e'
    d = e + c; d.label = 'd'
    f = Value(-2.0, label='f')
    L = d * f; L.label = 'L'
    L1 = L.data
  
    a = Value(2.0 + h, label='a')     # 计算 L 对 a 的导数
    b = Value(-3.0, label='b')
    c = Value(10.0, label='c')
    e = a*b; e.label = 'e'
    d = e + c; d.label = 'd'
    f = Value(-2.0, label='f')
    L = d * f; L.label = 'L'
    L2 = L.data
  
    print((L2 - L1)/h)
  
lol()
# 6.000000000000227
```

我们可以像之前一样测量或估计这些数值梯度。（这里创建一个 `lol` 函数，原因是我不想污染或搞乱 Jupyter 中前文的一些全局作用域变量）$$L=d \times f$$
接下来我们计算 $\frac{\mathrm{d}L}{\mathrm{d}f}$ 和 $\frac{\mathrm{d}L}{\mathrm{d}d}$，显然这里的值分别为 $d$ 和 $f$，即 $\frac{\mathrm{d}L}{\mathrm{d}f}=d=(a\times b+c)=4$，$\frac{\mathrm{d}L}{\mathrm{d}d}=f=-2$

$$L=(c+e) \times f$$

接下来我们计算 $\frac{\mathrm{d}L}{\mathrm{d}c}$ 和 $\frac{\mathrm{d}L}{\mathrm{d}e}$，这里就需要用到求导 *链式法则*，例如 $\frac{\mathrm{d}L}{\mathrm{d}e} = \frac{\mathrm{d}L}{\mathrm{d}d} \times \frac{\mathrm{d}d}{\mathrm{d}e} = -2 \times 1 =-2$，同理 $\frac{\mathrm{d}L}{\mathrm{d}c} = \frac{\mathrm{d}L}{\mathrm{d}d} \times \frac{\mathrm{d}d}{\mathrm{d}c} = -2 \times 1 =-2$

这个链式法则就是反向传播的核心问题，这将是需要理解的最重要的内容，理解了这些，基本上就理解了整个反向传播和神经网络训练的全部内容。链式法则本质上是在告诉我们如何正确地将这些导数串联起来。
$$L=(a \times b+c) \times f$$

再传播一层，我们计算 $\frac{\mathrm{d}L}{\mathrm{d}a}$ 和 $\frac{\mathrm{d}L}{\mathrm{d}b}$，$\frac{\mathrm{d}L}{\mathrm{d}a} = \frac{\mathrm{d}L}{\mathrm{d}e} \times \frac{\mathrm{d}e}{\mathrm{d}a} = -2\times b=-2\times -3=6$，同理可计算 $\frac{\mathrm{d}L}{\mathrm{d}b} = \frac{\mathrm{d}L}{\mathrm{d}e} \times \frac{\mathrm{d}e}{\mathrm{d}b} = -2\times a=-2\times 2=-4$

就是这样，我们手动一步步地完成了反向传播过程，从 `L` 计算到所有的叶节点。

整个过程中，我们可以每计算一层，就更新图中的 `grad` 属性值，最后就会变为：

![|700](data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C!DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%0A%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3C!--%20Generated%20by%20graphviz%20version%202.44.0%20(0)%0A%20--%3E%0A%3C!--%20Pages%3A%201%20--%3E%0A%3Csvg%20width%3D%221338pt%22%20height%3D%22128pt%22%0A%20viewBox%3D%220.00%200.00%201338.00%20128.00%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%3E%0A%3Cg%20id%3D%22graph0%22%20class%3D%22graph%22%20transform%3D%22scale(1%201)%20rotate(0)%20translate(4%20124)%22%3E%0A%3Cpolygon%20fill%3D%22white%22%20stroke%3D%22transparent%22%20points%3D%22-4%2C4%20-4%2C-124%201334%2C-124%201334%2C4%20-4%2C4%22%2F%3E%0A%3C!--%20140013524541968%20--%3E%0A%3Cg%20id%3D%22node1%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524541968%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22367%2C-83.5%20367%2C-119.5%20609%2C-119.5%20609%2C-83.5%20367%2C-83.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22379%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ec%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22391%2C-83.5%20391%2C-119.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22446%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%2010.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22501%2C-83.5%20501%2C-119.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22555%22%20y%3D%22-97.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22node3%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542496%2B%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22672%22%20cy%3D%22-73.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22672%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E%2B%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541968%26%2345%3B%26gt%3B140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22edge5%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524541968%26%2345%3B%26gt%3B140013524542496%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M606.06%2C-83.49C616.68%2C-81.86%20626.78%2C-80.3%20635.71%2C-78.93%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22636.27%2C-82.38%20645.62%2C-77.4%20635.21%2C-75.47%20636.27%2C-82.38%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%20--%3E%0A%3Cg%20id%3D%22node2%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542496%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22735%2C-55.5%20735%2C-91.5%20970%2C-91.5%20970%2C-55.5%20735%2C-55.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22748%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ed%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22761%2C-55.5%20761%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22811.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%204.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22862%2C-55.5%20862%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22916%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024*%20--%3E%0A%3Cg%20id%3D%22node7%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524537024*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%221033%22%20cy%3D%22-45.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221033%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%26%2345%3B%26gt%3B140013524537024*%20--%3E%0A%3Cg%20id%3D%22edge7%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542496%26%2345%3B%26gt%3B140013524537024*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M968.31%2C-55.49C978.42%2C-53.91%20988.04%2C-52.4%20996.61%2C-51.05%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22997.16%2C-54.51%201006.5%2C-49.5%20996.08%2C-47.59%20997.16%2C-54.51%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542496%2B%26%2345%3B%26gt%3B140013524542496%20--%3E%0A%3Cg%20id%3D%22edge1%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542496%2B%26%2345%3B%26gt%3B140013524542496%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M699.34%2C-73.5C706.74%2C-73.5%20715.33%2C-73.5%20724.57%2C-73.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22724.84%2C-77%20734.84%2C-73.5%20724.84%2C-70%20724.84%2C-77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751360%20--%3E%0A%3Cg%20id%3D%22node4%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013526751360%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%220%2C-56.5%200%2C-92.5%20241%2C-92.5%20241%2C-56.5%200%2C-56.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2213%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Eb%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2226%2C-56.5%2026%2C-92.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2279.5%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B3.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22133%2C-56.5%20133%2C-92.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22187%22%20y%3D%22-70.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%20%26%2345%3B4.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304*%20--%3E%0A%3Cg%20id%3D%22node9%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542304*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22304%22%20cy%3D%22-46.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22304%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013526751360%26%2345%3B%26gt%3B140013524542304*%20--%3E%0A%3Cg%20id%3D%22edge8%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013526751360%26%2345%3B%26gt%3B140013524542304*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M238.24%2C-56.49C248.61%2C-54.89%20258.49%2C-53.37%20267.27%2C-52.01%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22268.03%2C-55.44%20277.38%2C-50.45%20266.97%2C-48.52%20268.03%2C-55.44%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524534960%20--%3E%0A%3Cg%20id%3D%22node5%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524534960%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22736.5%2C-0.5%20736.5%2C-36.5%20968.5%2C-36.5%20968.5%2C-0.5%20736.5%2C-0.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22747.5%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ef%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22758.5%2C-0.5%20758.5%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22812%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22865.5%2C-0.5%20865.5%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22917%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%204.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524534960%26%2345%3B%26gt%3B140013524537024*%20--%3E%0A%3Cg%20id%3D%22edge6%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524534960%26%2345%3B%26gt%3B140013524537024*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M968.56%2C-35.9C978.53%2C-37.41%20988.02%2C-38.85%20996.48%2C-40.13%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22996.2%2C-43.62%201006.61%2C-41.66%20997.25%2C-36.7%20996.2%2C-43.62%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024%20--%3E%0A%3Cg%20id%3D%22node6%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524537024%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221096%2C-27.5%201096%2C-63.5%201330%2C-63.5%201330%2C-27.5%201096%2C-27.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221108%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3EL%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221120%2C-27.5%201120%2C-63.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221173.5%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B8.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221227%2C-27.5%201227%2C-63.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221278.5%22%20y%3D%22-41.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%201.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524537024*%26%2345%3B%26gt%3B140013524537024%20--%3E%0A%3Cg%20id%3D%22edge2%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524537024*%26%2345%3B%26gt%3B140013524537024%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1060.26%2C-45.5C1067.65%2C-45.5%201076.21%2C-45.5%201085.43%2C-45.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221085.66%2C-49%201095.66%2C-45.5%201085.66%2C-42%201085.66%2C-49%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304%20--%3E%0A%3Cg%20id%3D%22node8%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524542304%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22368%2C-28.5%20368%2C-64.5%20608%2C-64.5%20608%2C-28.5%20368%2C-28.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22380.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ee%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22393%2C-28.5%20393%2C-64.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22446.5%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B6.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22500%2C-28.5%20500%2C-64.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22554%22%20y%3D%22-42.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%20%26%2345%3B2.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304%26%2345%3B%26gt%3B140013524542496%2B%20--%3E%0A%3Cg%20id%3D%22edge4%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542304%26%2345%3B%26gt%3B140013524542496%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M608.11%2C-64.17C617.85%2C-65.61%20627.11%2C-66.99%20635.38%2C-68.22%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22634.89%2C-71.68%20645.3%2C-69.69%20635.92%2C-64.76%20634.89%2C-71.68%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524542304*%26%2345%3B%26gt%3B140013524542304%20--%3E%0A%3Cg%20id%3D%22edge3%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524542304*%26%2345%3B%26gt%3B140013524542304%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M331.08%2C-46.5C338.74%2C-46.5%20347.68%2C-46.5%20357.33%2C-46.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22357.62%2C-50%20367.62%2C-46.5%20357.62%2C-43%20357.62%2C-50%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541344%20--%3E%0A%3Cg%20id%3D%22node10%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013524541344%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%226%2C-1.5%206%2C-37.5%20235%2C-37.5%20235%2C-1.5%206%2C-1.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2218.5%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ea%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2231%2C-1.5%2031%2C-37.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2281.5%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%202.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22132%2C-1.5%20132%2C-37.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22183.5%22%20y%3D%22-15.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%206.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013524541344%26%2345%3B%26gt%3B140013524542304*%20--%3E%0A%3Cg%20id%3D%22edge9%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013524541344%26%2345%3B%26gt%3B140013524542304*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M235.16%2C-36.41C246.7%2C-38.12%20257.71%2C-39.76%20267.38%2C-41.2%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22266.9%2C-44.67%20277.31%2C-42.68%20267.93%2C-37.74%20266.9%2C-44.67%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A)

正如你所看到的，我们实际上所做的就是逐个遍历所有节点，并局部应用链式法则。我们始终知道损失函数 `L` 相对于每一个输入节点的导数是多少。然后我们再看看这个节点是如何产生的。中间的一些节点可能是通过某种操作产生的，我们拥有指向该子节点的指针。因此在这个小操作中，我们知道局部导数是什么，我们总是将它们乘到导数上。所以我们只需遍历并递归地将局部导数相乘。

这就是反向传播的本质。它不过是通过计算图反向递归应用链式法则。

## 单次优化步骤的预览

我们要做的是微调输入，以试图让 `L` 上升。具体来说，我们想要改变 `a.data`。如果我们想让 `L` 上升，就意味着我们只需要沿着梯度的方向前进。因此，`a` 应该沿着梯度的方向以某个小的步长增加。这个步长就是学习率。我们也对 `B` 这样做，对 `C` 和 `F` 也是如此。这些都是叶节点，通常是我们能够控制的。

如果我们沿着梯度的方向微调，我们预计会对 `L` 产生积极影响。因此，我们预计 `L` 会正向上升。

```python
a.data += 0.01 * a.grad
b.data += 0.01 * b.grad
c.data += 0.01 * c.grad
f.data += 0.01 * f.grad

e = a * b
d = e + c
L = d * f

print(L.data)
# -7.286496
```

## 手动反向传播示例 2:神经元

**激活函数**：tanh


```python
plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid();
```

![|500](data:image/png;base64,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)

可以看到，随着输入值的增加，它们在 y 坐标上被压缩。在 0 点处，输出正好是 0。当输入值向正方向增加时，函数值会逐渐趋近于 1 并最终趋于平稳。因此，当输入非常大的正数时，输出会被平滑地限制在 1。而在负方向，输出会被平滑地限制在 -1。这就是 tanh 函数的特点。这就是所谓的压缩函数或激活函数。

这个神经元输出的结果就是权重与输入的点积经过激活函数处理后的值。让我们来写一个例子。我打算直接复制粘贴，因为不想打太多字。不过好吧，现在我们有了输入 x1 和 x2。这是一个二维神经元。所以会有两个输入进来。这些被视为这个神经元的权重，权重 w1 和 w2。这些权重，再次强调，是每个输入的突触强度。

```python
# inputs x1,x2
x1 = Value(2.0, label='x1')
x2 = Value(0.0, label='x2')
# weights w1,w2
w1 = Value(-3.0, label='w1')
w2 = Value(1.0, label='w2')
# bias of the neuron
b = Value(6.7, label='b')
# x1*w1 + x2*w2 + b
x1w1 = x1*w1; x1w1.label = 'x1*w1'
x2w2 = x2*w2; x2w2.label = 'x2*w2'
x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'
n = x1w1x2w2 + b; n.label = 'n'
o = n.tanh(); o.label = 'o'
```

**实现 tanh**
  
$$\tanh x={\frac {\sinh x}{\cosh x}}={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}={\frac {e^{2x}-1}{e^{2x}+1}}$$

```python
def tanh(self):
    x = self.data
    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)
    out = Value(t, (self, ), 'tanh')
          
    return out
```

![](data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C!DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%0A%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3C!--%20Generated%20by%20graphviz%20version%202.44.0%20(0)%0A%20--%3E%0A%3C!--%20Pages%3A%201%20--%3E%0A%3Csvg%20width%3D%221833pt%22%20height%3D%22210pt%22%0A%20viewBox%3D%220.00%200.00%201833.39%20210.00%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%3E%0A%3Cg%20id%3D%22graph0%22%20class%3D%22graph%22%20transform%3D%22scale(1%201)%20rotate(0)%20translate(4%20206)%22%3E%0A%3Cpolygon%20fill%3D%22white%22%20stroke%3D%22transparent%22%20points%3D%22-4%2C4%20-4%2C-206%201829.39%2C-206%201829.39%2C4%20-4%2C4%22%2F%3E%0A%3C!--%20140013488977440%20--%3E%0A%3Cg%20id%3D%22node1%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488977440%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22770%2C-82.5%20770%2C-118.5%201106%2C-118.5%201106%2C-82.5%20770%2C-82.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22833%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ex1*w1%20%2B%20x2*w2%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22896%2C-82.5%20896%2C-118.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22949.5%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B6.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221003%2C-82.5%201003%2C-118.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221054.5%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488966736%2B%20--%3E%0A%3Cg%20id%3D%22node4%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488966736%2B%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%221169%22%20cy%3D%22-127.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221169%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E%2B%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488977440%26%2345%3B%26gt%3B140013488966736%2B%20--%3E%0A%3Cg%20id%3D%22edge13%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488977440%26%2345%3B%26gt%3B140013488966736%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1091.75%2C-118.51C1106.57%2C-120.26%201120.43%2C-121.89%201132.16%2C-123.27%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221131.95%2C-126.77%201142.29%2C-124.47%201132.77%2C-119.82%201131.95%2C-126.77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488977440%2B%20--%3E%0A%3Cg%20id%3D%22node2%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488977440%2B%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22707%22%20cy%3D%22-100.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22707%22%20y%3D%22-96.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E%2B%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488977440%2B%26%2345%3B%26gt%3B140013488977440%20--%3E%0A%3Cg%20id%3D%22edge1%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488977440%2B%26%2345%3B%26gt%3B140013488977440%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M734.27%2C-100.5C741.59%2C-100.5%20750.14%2C-100.5%20759.5%2C-100.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22759.54%2C-104%20769.54%2C-100.5%20759.54%2C-97%20759.54%2C-104%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488966736%20--%3E%0A%3Cg%20id%3D%22node3%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488966736%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221232%2C-109.5%201232%2C-145.5%201462%2C-145.5%201462%2C-109.5%201232%2C-109.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221245%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3En%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221258%2C-109.5%201258%2C-145.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221308.5%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%200.8814%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221359%2C-109.5%201359%2C-145.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221410.5%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488967648tanh%20--%3E%0A%3Cg%20id%3D%22node15%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488967648tanh%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%221529.2%22%20cy%3D%22-127.5%22%20rx%3D%2231.4%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221529.2%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Etanh%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488966736%26%2345%3B%26gt%3B140013488967648tanh%20--%3E%0A%3Cg%20id%3D%22edge7%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488966736%26%2345%3B%26gt%3B140013488967648tanh%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1462.12%2C-127.5C1471.05%2C-127.5%201479.65%2C-127.5%201487.51%2C-127.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221487.75%2C-131%201497.75%2C-127.5%201487.75%2C-124%201487.75%2C-131%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488966736%2B%26%2345%3B%26gt%3B140013488966736%20--%3E%0A%3Cg%20id%3D%22edge2%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488966736%2B%26%2345%3B%26gt%3B140013488966736%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1196.35%2C-127.5C1203.74%2C-127.5%201212.3%2C-127.5%201221.51%2C-127.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221221.74%2C-131%201231.74%2C-127.5%201221.74%2C-124%201221.74%2C-131%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488965728%20--%3E%0A%3Cg%20id%3D%22node5%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488965728%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%224.5%2C-165.5%204.5%2C-201.5%20242.5%2C-201.5%20242.5%2C-165.5%204.5%2C-165.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2221.5%22%20y%3D%22-179.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ex1%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2238.5%2C-165.5%2038.5%2C-201.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2289%22%20y%3D%22-179.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%202.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22139.5%2C-165.5%20139.5%2C-201.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22191%22%20y%3D%22-179.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488973456*%20--%3E%0A%3Cg%20id%3D%22node8%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488973456*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22310%22%20cy%3D%22-128.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22310%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488965728%26%2345%3B%26gt%3B140013488973456*%20--%3E%0A%3Cg%20id%3D%22edge9%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488965728%26%2345%3B%26gt%3B140013488973456*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M216.38%2C-165.43C226.79%2C-162.74%20237.18%2C-159.76%20247%2C-156.5%20257.7%2C-152.95%20269.05%2C-148.15%20279.07%2C-143.53%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22280.66%2C-146.65%20288.2%2C-139.2%20277.66%2C-140.32%20280.66%2C-146.65%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488965200%20--%3E%0A%3Cg%20id%3D%22node6%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488965200%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%223%2C-55.5%203%2C-91.5%20244%2C-91.5%20244%2C-55.5%203%2C-55.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2221.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ew2%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2240%2C-55.5%2040%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2290.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%201.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22141%2C-55.5%20141%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22192.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488968656*%20--%3E%0A%3Cg%20id%3D%22node13%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488968656*%3C%2Ftitle%3E%0A%3Cellipse%20fill%3D%22none%22%20stroke%3D%22black%22%20cx%3D%22310%22%20cy%3D%22-73.5%22%20rx%3D%2227%22%20ry%3D%2218%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22310%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3E*%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488965200%26%2345%3B%26gt%3B140013488968656*%20--%3E%0A%3Cg%20id%3D%22edge11%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488965200%26%2345%3B%26gt%3B140013488968656*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M244.21%2C-73.5C254.31%2C-73.5%20263.93%2C-73.5%20272.51%2C-73.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22272.79%2C-77%20282.79%2C-73.5%20272.79%2C-70%20272.79%2C-77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488973456%20--%3E%0A%3Cg%20id%3D%22node7%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488973456%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22373%2C-110.5%20373%2C-146.5%20644%2C-146.5%20644%2C-110.5%20373%2C-110.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22403.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ex1*w1%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22434%2C-110.5%20434%2C-146.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22487.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B6.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22541%2C-110.5%20541%2C-146.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22592.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488973456%26%2345%3B%26gt%3B140013488977440%2B%20--%3E%0A%3Cg%20id%3D%22edge14%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488973456%26%2345%3B%26gt%3B140013488977440%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M635.89%2C-110.49C648.28%2C-108.73%20660.01%2C-107.05%20670.21%2C-105.6%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22670.84%2C-109.05%20680.25%2C-104.17%20669.85%2C-102.12%20670.84%2C-109.05%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488973456*%26%2345%3B%26gt%3B140013488973456%20--%3E%0A%3Cg%20id%3D%22edge3%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488973456*%26%2345%3B%26gt%3B140013488973456%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M337.09%2C-128.5C344.53%2C-128.5%20353.2%2C-128.5%20362.61%2C-128.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22362.67%2C-132%20372.67%2C-128.5%20362.67%2C-125%20362.67%2C-132%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970384%20--%3E%0A%3Cg%20id%3D%22node9%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488970384%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%224.5%2C-0.5%204.5%2C-36.5%20242.5%2C-36.5%20242.5%2C-0.5%204.5%2C-0.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2221.5%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ex2%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2238.5%2C-0.5%2038.5%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2289%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%200.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22139.5%2C-0.5%20139.5%2C-36.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22191%22%20y%3D%22-14.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970384%26%2345%3B%26gt%3B140013488968656*%20--%3E%0A%3Cg%20id%3D%22edge12%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488970384%26%2345%3B%26gt%3B140013488968656*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M212.7%2C-36.58C224.33%2C-39.57%20236.02%2C-42.89%20247%2C-46.5%20257.46%2C-49.94%20268.57%2C-54.48%20278.45%2C-58.85%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22277.31%2C-62.18%20287.86%2C-63.13%20280.2%2C-55.8%20277.31%2C-62.18%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970528%20--%3E%0A%3Cg%20id%3D%22node10%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488970528%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%220%2C-110.5%200%2C-146.5%20247%2C-146.5%20247%2C-110.5%200%2C-110.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2218.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ew1%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%2237%2C-110.5%2037%2C-146.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%2290.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%20%26%2345%3B3.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22144%2C-110.5%20144%2C-146.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22195.5%22%20y%3D%22-124.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970528%26%2345%3B%26gt%3B140013488973456*%20--%3E%0A%3Cg%20id%3D%22edge8%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488970528%26%2345%3B%26gt%3B140013488973456*%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M247.05%2C-128.5C256.2%2C-128.5%20264.91%2C-128.5%20272.74%2C-128.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22272.85%2C-132%20282.85%2C-128.5%20272.85%2C-125%20272.85%2C-132%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970144%20--%3E%0A%3Cg%20id%3D%22node11%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488970144%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22823%2C-137.5%20823%2C-173.5%201053%2C-173.5%201053%2C-137.5%20823%2C-137.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22836%22%20y%3D%22-151.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Eb%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22849%2C-137.5%20849%2C-173.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22899.5%22%20y%3D%22-151.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%206.8814%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22950%2C-137.5%20950%2C-173.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221001.5%22%20y%3D%22-151.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488970144%26%2345%3B%26gt%3B140013488966736%2B%20--%3E%0A%3Cg%20id%3D%22edge10%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488970144%26%2345%3B%26gt%3B140013488966736%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1053.16%2C-141.54C1081.86%2C-138.03%201110.58%2C-134.52%201132.12%2C-131.89%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221132.58%2C-135.36%201142.08%2C-130.67%201131.73%2C-128.41%201132.58%2C-135.36%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488968656%20--%3E%0A%3Cg%20id%3D%22node12%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488968656%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22376%2C-55.5%20376%2C-91.5%20641%2C-91.5%20641%2C-55.5%20376%2C-55.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22406.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Ex2*w2%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22437%2C-55.5%20437%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22487.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%200.0000%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%22538%2C-55.5%20538%2C-91.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%22589.5%22%20y%3D%22-69.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488968656%26%2345%3B%26gt%3B140013488977440%2B%20--%3E%0A%3Cg%20id%3D%22edge6%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488968656%26%2345%3B%26gt%3B140013488977440%2B%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M640.57%2C-91.51C651.3%2C-92.98%20661.43%2C-94.38%20670.38%2C-95.61%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22669.92%2C-99.08%20680.3%2C-96.97%20670.87%2C-92.14%20669.92%2C-99.08%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488968656*%26%2345%3B%26gt%3B140013488968656%20--%3E%0A%3Cg%20id%3D%22edge4%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488968656*%26%2345%3B%26gt%3B140013488968656%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M337.09%2C-73.5C345.24%2C-73.5%20354.87%2C-73.5%20365.33%2C-73.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%22365.59%2C-77%20375.59%2C-73.5%20365.59%2C-70%20365.59%2C-77%22%2F%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488967648%20--%3E%0A%3Cg%20id%3D%22node14%22%20class%3D%22node%22%3E%0A%3Ctitle%3E140013488967648%3C%2Ftitle%3E%0A%3Cpolygon%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221596.39%2C-109.5%201596.39%2C-145.5%201825.39%2C-145.5%201825.39%2C-109.5%201596.39%2C-109.5%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221608.89%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Eo%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221621.39%2C-109.5%201621.39%2C-145.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221671.89%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Edata%200.7071%3C%2Ftext%3E%0A%3Cpolyline%20fill%3D%22none%22%20stroke%3D%22black%22%20points%3D%221722.39%2C-109.5%201722.39%2C-145.5%20%22%2F%3E%0A%3Ctext%20text-anchor%3D%22middle%22%20x%3D%221773.89%22%20y%3D%22-123.8%22%20font-family%3D%22Times-Roman%22%20font-size%3D%2214.00%22%3Egrad%200.0000%3C%2Ftext%3E%0A%3C%2Fg%3E%0A%3C!--%20140013488967648tanh%26%2345%3B%26gt%3B140013488967648%20--%3E%0A%3Cg%20id%3D%22edge5%22%20class%3D%22edge%22%3E%0A%3Ctitle%3E140013488967648tanh%26%2345%3B%26gt%3B140013488967648%3C%2Ftitle%3E%0A%3Cpath%20fill%3D%22none%22%20stroke%3D%22black%22%20d%3D%22M1560.68%2C-127.5C1568.25%2C-127.5%201576.85%2C-127.5%201585.99%2C-127.5%22%2F%3E%0A%3Cpolygon%20fill%3D%22black%22%20stroke%3D%22black%22%20points%3D%221586.11%2C-131%201596.11%2C-127.5%201586.11%2C-124%201586.11%2C-131%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A)

就这个例子而言，我特别想说明的是，我们并不一定要在这个 Value 对象中包含最基本的原子部分。实际上，我们可以在任意的抽象层次上创建函数。这些函数可以是复杂的，但也可能是非常、非常简单的，比如加法运算。这完全取决于我们。唯一重要的是我们知道如何通过函数进行区分。因此，我们接收一些输入并产生输出。

唯一重要的是，它可以是一个任意复杂的函数，只要你懂得如何创建局部导数。如果你知道输入如何影响输出的局部导数，那么这就是你所需要的全部，因此我们需要 tanh 的导数。

$$\frac{\mathrm{d}}{\mathrm{d}x}\tanh x = 1-\tanh^2x$$

`o` 对自身求导是 1.0，然后 $\frac{\mathrm{d}o}{\mathrm{d}n}=1-n ^2 =0.5$；

接下来求上一层 $o = \tanh(b+(x_{1}w_{1}+x_{2}w_{2}))$，所以 $\frac{\mathrm{d}o}{\mathrm{d}b}=\frac{\mathrm{d}o}{\mathrm{d}n}\frac{\mathrm{d}n}{\mathrm{d}b}=0.5*1=0.5$ ，同理另一项也是 0.5，...一级一级推导下去即可。

-----------

好了，手动进行反向传播显然很荒谬。所以我们现在要结束这种痛苦，看看如何更自动地实现反向传播。我们不会在这里手动完成所有操作。通过示例我们已经很清楚这些加法和乘法是如何反向传播梯度的了。

现在让我们回到 Value 对象，开始正式定义下面示例中所展示的内容。我们将通过存储一个特殊的 `self.backward` 和 `_backward` 来实现这一点，这个函数将负责执行那一小段链式法则的计算。在每个接收输入并产生输出的小节点上，我们将存储如何将输出的梯度链式传递到输入的梯度中。默认情况下，这将是一个不执行任何操作的函数，因此，这个反向传播函数默认情况下不会执行任何操作，它是一个空函数。例如，对于叶子节点来说，通常就是这种情况，对于叶子节点，无需任何操作。

但现在，当我们创建这些输出值时，这些输出值是自身与其他值的相加结果。因此，我们希望将输出的反向传播函数设置为梯度传播的函数。那么让我们来定义应该发生的事情。我们将把它存储在一个闭包中。让我们定义当我们调用 out 的 `grad` 时应该发生的事情。

对于加法运算，我们的任务是将输出梯度分别传播到自身梯度（self's grad）和其他梯度（other grad）。简而言之，我们需要为 `self.grad` 赋值，同时也需要为 `other.grad` 赋值。而我们之前已经了解了链式法则的运作方式，我们需要将局部导数乘以所谓的全局导数——也就是表达式最终输出相对于 out 的导数，相对于 out 本身。在加法运算中，`self` 的局部导数是 1.0。因此就是 1.0 乘以 out 的梯度值。这就是链式法则。而 `other.grad` 将是 1.0 乘以 out 的 grad。基本上，你在这里看到的是，out 的 grad 会简单地复制到 self 的 grad 和 other 的 grad 上，就像我们在加法操作中看到的那样。因此，我们稍后将调用此函数来传播梯度，完成加法运算。


现在让我们进行乘法运算。我们还将定义一个反向点积。我们将把它的反向传播设置为 backward。我们希望将 out 的梯度链式传递到 `self.grad` 和 `others.grad` 中。这将是乘法链式法则的一小部分应用。那么，这里应该是什么？你能想明白吗？这里的局部导数是什么？局部导数是 `others.data` 乘以 `out` 的梯度。这就是链式法则。这里我们有 `self.data` 乘以 out 的梯度。这就是我们一直在做的。

最后是 tanh 的反向传播部分。我们想要将 out 的反向传播设置为 backward。这里我们需要进行反向传播。我们已经得到了 out 的梯度，现在要将其链式传递到 self.grad。这里的 self.grad 将代表我们当前 tanh 运算的局部导数。正如我们所见，tanh 的局部梯度是 1 减去 tanh(x) 的平方，而这里的 x 对应着 t。因为 t 是这个 tanh 运算的输出值，所以 1 减去 t 的平方就是该运算的局部导数。然后由于链式法则，梯度必须相乘。因此，out的梯度通过局部梯度链式传递到self.grad。基本上就是这样。

因此，我们将重新定义我们的 Value 节点。我们将一路向下摆动到这里。然后重新定义我们的表达式。确保所有梯度都为零。好的。但现在我们不再需要手动操作了。我们将按照正确的顺序反向调用点。首先，我们要调用out的dot backward方法。这里的o是tanh的输出结果，对吧？因此，调用out的backward方法将会执行这个函数。这就是它的作用。现在我们必须小心，因为有一个 times out dot grad。记住，out dot grad 初始化为零。所以这里我们看到 grad 为零。因此，作为基础情况，我们需要将o的grad属性设为1.0，以1进行初始化。一旦这个值为1，我们就可以调用o.backward()方法。这个方法的作用是将这个grad值通过tanh函数反向传播——即局部导数乘以初始化为1的全局导数。这样操作即可实现反向传播。

所以我考虑过重做，但我觉得还是把这个错误保留下来比较好，因为还挺搞笑的。为什么“not an object”不可调用？因为我搞砸了。我们正试图保存这些函数。所以这是正确的。这里，我们不想调用这个函数，因为它返回的是None。这些函数返回的都是None。我们只是想存储这个函数。所以让我重新定义这个值对象。然后我们会回来，重新定义表达式，draw dot。一切都很顺利。O点的梯度是1。O点的梯度是1。现在这应该能正常工作，当然。好的，所以o点向后，如果我们重新绘制，这个梯度现在应该是0.5。如果一切顺利的话，0.5。太好了。好的，现在我们需要调用ns.grad。抱歉，是ns.backward。ns向后。看来这已经起作用了。所以ns点向后将梯度路由到了这两个地方。看起来很不错。当然，我们现在可以调用 b.grad。抱歉，是 b.backward。会发生什么？好吧，b没有向后。b是向后的，因为b是一个叶节点。b是向后的是通过初始化为空函数。所以什么都不会发生。但我们可以调用它。但是当我们调用这个，这个反向的，那么我们期望这个0.5会被进一步路由，对吧？所以我们就得到了0.5，0.5。最后，我们想在这里调用x2w2和x1w1。让我们把这两个都做一遍。就这样，我们得到了0、0.5、-1.5和1，和之前的结果完全一致。

但现在我们已经通过手动调用 `_backward` 方法实现了。现在只剩下最后一个需要解决的问题，那就是我们还在手动调用 `_backward` 方法。让我们仔细思考一下我们实际上在做什么。

我们已经构建了一个数学表达式，现在正尝试逆向遍历这个表达式。逆向遍历表达式的意思就是，在对任何节点调用反向传播之前，我们必须先完成该节点之后的所有操作。因此，在针对任一节点调用反向传播之前，必须先处理完该节点之后的所有内容。我们必须获取其所有完整依赖项。在继续反向传播之前，它依赖的所有内容都必须传播给它。因此，这种图的排序可以通过一种称为拓扑排序的方法来实现。

拓扑排序本质上就是将图的结构以所有边都从左向右延伸的方式呈现出来。这里我们有一个图，它是一个有向无环图（DAG）。我认为这是两种不同的拓扑排序方式，基本上你会看到节点的排列方式是所有边都只朝一个方向，从左到右。至于如何实现拓扑排序，你可以查阅维基百科等资料。我就不详细讲解了。

```python
topo = []
visited = set()
def build_topo(v):
  if v not in visited:
    visited.add(v)
    for child in v._prev:
      build_topo(child)
    topo.append(v)
build_topo(o)
topo
```

但基本上，这就是构建拓扑图的方式。我们维护一个已访问节点的集合。然后从某个根节点开始遍历，对我们来说就是O。这就是我想开始拓扑排序的地方。从O开始，我们遍历它的所有子节点，并按从左到右的顺序排列它们。基本上，这个过程始于O。如果某个节点未被访问过，就将其标记为已访问。然后遍历其所有子节点，并对它们调用buildTopological函数。然后，当它遍历完所有子节点后，它才会把自己加进去。所以基本上，我们调用它的这个节点，比如叫O，只有在所有子节点都被处理完之后，才会把自己加入到拓扑列表中。这就是这个函数如何保证你只有在所有子节点都在列表中时才会被加入列表。这就是保持不变的不变量。因此，如果我们在O上执行buildTopo操作，然后检查这个列表，我们会发现它已经对我们的值对象进行了排序。最后一个值是0.707，也就是输出结果。所以这是O，然后是N，接着所有其他节点都会在它之前排列好。这样就构建出了拓扑图。实际上，我们现在所做的就是按照拓扑顺序对所有节点调用dot下划线backward。

所以如果我们只是重置梯度，它们就都变成0了。我们做了什么？我们一开始将O.grad设为1，这是基础情况。然后我们构建了一个拓扑序。接着我们按拓扑序的逆序遍历节点。现在，按照相反的顺序，因为这个列表是从……你知道的，我们需要以相反的顺序遍历它。所以从 O 开始，node dot backward。应该就是这样了。好了，这就是正确的导数。最后，我们要隐藏这个功能。

所以我要复制这段代码，然后把它隐藏到值类里。因为我们不想让这些代码散落各处。于是，不再用下划线backward，现在我们要定义一个真正的backward。所以这就是没有下划线的反向操作。它将执行我们刚刚推导出的所有操作。让我稍微整理一下这个。

首先，我们将从自身开始构建拓扑图。因此，构建自身的拓扑结构（build topo of self）会将拓扑顺序填充到局部变量topo列表中。然后，我们将self.grads设置为1。然后，对于反转列表中的每个节点，从我们开始，遍历所有子节点，下划线向后。这样应该就可以了。保存。下来这里，重新定义。好的，所有梯度都归零。现在我们可以做的是不带下划线的O点向后。我们开始吧。这就是反向传播。至少对于一个神经元来说是这样的。

我们其实不应该太高兴，因为我们有一个严重的 bug。由于一些我们现在必须考虑的特定条件，我们还没有发现这个漏洞。所以这里有一个展示该漏洞的最简单案例。

```python
a = Value(3.0, label='a')
b = a + a   ; b.label = 'b'
b.backward()
draw_dot(b)
```

假设我创建了一个单节点A，然后创建了一个B，B等于A加A。接着我调用backward方法。那么会发生什么呢？A的值是3，B等于A加A。所以这里有两个箭头叠加在一起。然后我们可以看到，B的前向传播当然是有效的。B等于A加A，也就是6。但这里的梯度实际上并不正确，这是我们自动计算出来的。这是因为，当然，仅仅在脑海中做微积分运算，B对A的导数应该是2。1加1。不是1。

直观来看，这里发生了什么？B是A加A的结果，然后我们对它调用backward。让我们往上看看这会产生什么效果。B是一个加法运算的结果。所以B被排除了。然后当我们调用backward时，发生的是self.grad被设置为1，然后other.grad也被设置为1。但由于我们做的是A加A，self和other实际上是同一个对象。因此我们覆盖了梯度。我们将其设置为1，然后又再次设置为1。这就是为什么它一直保持在1。所以这是一个问题。

还有一种方法可以通过稍微复杂一点的表达式来理解这一点。这里我们有A和B。然后D是两者的乘积，E是两者的和。接着我们将E乘以D得到F。最后我们调用F的backward方法。而这些梯度，如果你检查的话，会发现是不正确的。所以从根本上说，这里发生的情况，再次说明，基本上只要我们在一个变量被多次使用时，就会遇到问题。到目前为止，在上述的表达式中，每个变量都只被使用了一次。

所以我们没有发现问题。但在这里，如果一个变量被多次使用，反向传播过程中会发生什么？我们从F反向传播到E再到D。到目前为止一切顺利。但现在E调用反向传播并将梯度传递给A和B。然后我们又回到D调用反向传播，它会覆盖A和B上的梯度。这显然是个问题。

这里的解决方案是，如果你观察多元情况下的链式法则及其推广形式，其本质在于我们需要累积这些梯度。这些梯度会相加。因此，我们不必单独设置这些梯度，只需简单地使用加等操作即可。

我们需要累加这些梯度。加等于，加等于，加等于，加等于。记住这样做没问题，因为我们把它们初始化为零了。所以他们从零开始。任何反向流动的贡献只会增加。现在如果我们重新定义这个，因为加等于，现在这个可以工作了。因为A点的梯度从零开始，我们称B点为反向梯度，我们先存入1，然后再存入1。现在结果是2，这是正确的。在这里，这个方法同样适用，我们会得到正确的梯度。因为当我们调用E点的反向传播时，会存储来自这个分支的梯度。然后当我们调用D点的反向传播时，它也会存储自己的梯度。这些梯度会简单地相互叠加。因此，我们只需累积这些梯度，就能解决这个问题。

好了，在继续之前，让我先清理一下，删除一些中间工作。既然我们已经推导出了所有内容，现在就不需要这些了。现在让我们回到这里实现的非线性 tanh。

现在，我告诉过你们，如果我们有 exp 函数，我们可以将 tanh 分解为其他表达式的显式原子。所以如果你还记得，tanh 是这样定义的。而我们选择将 tanh 开发为一个单一的函数。我们可以这样做，因为我们知道它是可微的，并且可以通过反向传播进行计算。但我们也可以将 tanh 分解并用 exp 的函数来表示它。我现在想这样做，因为我想向你证明你会得到完全相同的结果和梯度。但也因为它迫使我们实现更多的表达式。它迫使我们进行指数运算、加法、减法、除法等操作。我认为这是一个很好的练习，可以让我们多接触这些内容。

好的，让我们向上滚动到 Value 的定义。在这里，我们目前无法做到的是，比如说，我们可以设定一个值为 2.0。但我们不能像这样，比如在这里，我们想要加一个常数 1。我们无法做到类似这样的操作。

我们无法执行此操作，因为提示 int 对象没有 data 属性。这是因为这里直接使用了加一运算，而另一个操作数是整数 1。然后在这里，Python 试图访问 one.data。但这是不存在的。这是因为 one 不是一个 Value 对象。而我们只为 Value 对象定义了加法运算。因此为了方便起见，为了能创建这样的表达式并使其有意义，我们可以简单地这样做。基本上，如果 other 是 Value 的实例，我们就不对其进行处理。但如果它不是值的一个实例，我们就假设它是一个数字，比如整数或浮点数。然后我们会简单地把它包装在 Value 中。而另一个就会变成另一个的 Value。然后 other 会有一个数据属性。这样应该就能工作了。所以如果我这样说，重新定义 Value，那么这应该就能工作了。

好了，现在让我们对乘法做同样的事情。因为出于同样的原因，我们不能这样做。

所以我们只需要去mul。如果other不是一个值，那么我们就把它包装成一个值。让我们重新定义value。

现在可以了。不过，这里有个不太幸运且不太明显的部分。A乘以2是可行的。

我们看到了。但是两次A，这能行吗？你可能会觉得可以，对吧？但实际上，不行。原因在于Python并不知道。

比如当你做A乘以2时，基本上，所以A乘以2，Python会去执行类似A.mul(2)的操作。这基本上就是它会调用的方法。但对它来说，2乘以A等同于2.mul(A)。而它不会，2不能乘以值。

因此，Python对此确实感到困惑。所以，取而代之的是，在Python中，你可以自由定义一个叫做rmul的东西。而rmul有点像是一种备用方案。


So if Python can't do two,



(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

It will check if, by any chance, A knows how to multiply 2, and that will be called into Rmul. So, because Python can't do 2 times A, it will check, is there an Rmul in value? And because there is, it will now call that, and what we'll do here is we will swap the order of the operands. So basically, 2 times A will redirect to Rmul, and Rmul will basically call A times 2. And that's how that will work. 

So, redefining that with Rmul, 2 times A becomes 4. Okay, now looking at the other elements that we still need, we need to know how to exponentiate and how to divide. So let's first do the exponentiation part. We're going to introduce a single function exp here, and exp is going to mirror tanh in the sense that it's a single function that transforms a single scalar value and outputs a single scalar value.

So we pop out the Python number, we use math.exp to exponentiate it, create a new value object, everything that we've seen before. The tricky part, of course, is how do you backpropagate through e to the x? And so here, you can potentially pause the video and think about what should go here. Okay, so basically, we need to know what is the local derivative of e to the x. So d by dx of e to the x is, famously, just e to the x, and we've already just calculated e to the x, and it's inside out.data. So we can do out.data times and out.grad, that's the chain rule.

So we're just chaining on to the current running grad, and this is what the expression looks like. It looks a little confusing, but this is what it is, and that's the exponentiation. So redefining, we should now be able to call a.exp, and hopefully the backward pass works as well. 

Okay, and the last thing we'd like to do, of course, is we'd like to be able to divide. Now, I actually will implement something slightly more powerful than division, because division is just a special case of something a bit more powerful. So in particular, just by rearranging, if we have some kind of a b equals value of 4.0 here, we'd like to basically be able to do a divide b, and we'd like this to be able to give us 0.5. Now, division actually can be reshuffled as follows. 

If we have a divide b, that's actually the same as a multiplying 1 over b, and that's the same as a multiplying b to the power of negative 1. And so what I'd like to do instead is I'd basically like to implement the operation of x to the k for some constant k. So it's an integer or a float, and we would like to be able to differentiate this, and then as a special case, negative 1 will be division. And so I'm doing that just because it's more general, and yeah, you might as well do it that way. So basically what I'm saying is we can redefine division, which we will put here somewhere. 

Yeah, we can put it here somewhere. What I'm saying is that we can redefine division. So self divide other, this can actually be rewritten as self times other to the power of negative 1. And now value raised to the power of negative 1, we have to now define that. 

So we need to implement the pow function. Where am I going to put the pow function? Maybe here somewhere. This is the skeleton for it. 

So this function will be called when we try to raise a value to some power, and other will be that power. Now, I'd like to make sure that other is only an int or a float. Usually other is some kind of a different value object, but here other will be forced to be an int or a float. 

Otherwise, the math won't work for what we're trying to achieve in this specific case. That would be a different derivative expression if we wanted other to be a value. So here we create the upper value, which is just this data raised to the power of other, and other here could be, for example, negative 1. That's what we are hoping to achieve.

And then this is the backward stub. And this is the fun part, which is what is the chain rule expression here for backpropagating through the power function, where the power is to the power of some kind of a constant. So this is the exercise, and maybe pause the video here and see if you can figure it out yourself as to what we should put here. 

You can actually go here and look at derivative rules as an example, and we see lots of derivative rules that you can hopefully know from calculus. In particular, what we're looking for is the power rule, because that's telling us that if we're trying to take d by dx of x to the n, which is what we're doing here, then that is just n times x to the n minus 1, right? Okay, so that's telling us about the local derivative of this power operation. So all we want here, basically n is now other, and self.data is x. And so this now becomes other, which is n, times self.data, which is now a Python int or a float. 

It's not a value object. We're accessing the data attribute raised to the power of other minus 1, or n minus 1. I can put brackets around this, but this doesn't matter because power takes precedence over multiply in Python, so that would have been okay. And that's the local derivative only, but now we have to chain it.

And we chain it just simply by multiplying by our top grad. That's chain rule. And this should technically work, and we're going to find out soon. 

But now, if we do this, this should now work, and we get 0.5. So the forward pass works, but does the backward pass work? And I realize that we actually also have to know how to subtract. So right now, a minus b will not work. To make it work, we need one more piece of code here.

And basically, this is the subtraction, and the way we're going to implement subtraction is we're going to implement it by addition of a negation, and then to implement negation, we're going to multiply by negative 1. So just again, using the stuff we've already built, and just expressing it in terms of what we have, and a minus b is now working. Okay, so now let's scroll again to this expression here for this neuron, and let's just compute the backward pass here once we've defined O, and let's draw it. So here's the gradients for all of these leaf nodes for this two-dimensional neuron that has a 10h that we've seen before. 

So now what I'd like to do is I'd like to break up this 10h into this expression here. So let me copy paste this here, and now instead of, we'll press over the label, and we will change how we define O. So in particular, we're going to implement this formula here. So we need e to the 2x minus 1 over e to the x plus 1. So e to the 2x, we need to take 2 times m, and we need to exponentiate it. 

That's e to the 2x. And then because we're using it twice, let's create an intermediate variable, e, and then define O as e minus 1 over e plus 1, e minus 1 over e plus 1. And that should be it, and then we should be able to draw dot of O. So now before I run this, what do we expect to see? Number one, we're expecting to see a much longer graph here because we've broken up 10h into a bunch of other operations, but those operations are mathematically equivalent. And so what we're expecting to see is, number one, the same result here, so the forward pass works. 

And number two, because of that mathematical equivalence, we expect to see the same backward pass and the same gradients on these leaf nodes. So these gradients should be identical. So let's run this.

So number one, let's verify that instead of a single 10h node, we have now exp, and we have plus, we have times negative 1, this is the division, and we end up with the same forward pass here. And then the gradients, we have to be careful because they're in slightly different order potentially, the gradients for w2, x2 should be 0 and 0.5, w2 and x2 are 0 and 0.5, and w1, x1 are 1 and negative 1.5, 1 and negative 1.5. So that means that both our forward passes and backward passes were correct because this turned out to be equivalent to 10h before. And so the reason I wanted to go through this exercise is, number one, we got to practice a few more operations and writing more backwards passes. 

And number two, I wanted to illustrate the point that the level at which you implement your operations is totally up to you. You can implement backward passes for tiny expressions like a single individual plus or a single times, or you can implement them for say 10h, which is a kind of a, potentially you can see it as a composite operation because it's made up of all these more atomic operations. But really all of this is kind of like a fake concept. 

All that matters is we have some kind of inputs and some kind of an output, and this output is a function of the inputs in some way. And as long as you can do forward pass and the backward pass of that little operation, it doesn't matter what that operation is and how composite it is. If you can write the local gradients, you can chain the gradient and you can continue back propagation. 

So the design of what those functions are is completely up to you. So now I would like to show you how you can do the exact same thing, but using a modern deep neural network library, like for example, PyTorch, which I've roughly modeled micrograd by. And so PyTorch is something you would use in production, and I'll show you how you can do the exact same thing, but in PyTorch API. 

So I'm just going to copy paste it in and walk you through it a little bit. This is what it looks like. So we're going to import PyTorch, and then we need to define these value objects like we have here. 

Now, micrograd is a scalar-valued engine, so we only have scalar values like 2.0. But in PyTorch, everything is based around tensors, and like I mentioned, tensors are just n-dimensional arrays of scalars. So that's why things get a little bit more complicated here. I just need a scalar-valued tensor with just a single element. 

But by default, when you work with PyTorch, you would use more complicated tensors like this. So if I import PyTorch, then I can create tensors like this. And this tensor, for example, is a 2x3 array of scalars in a single compact representation.

So we can check its shape. We see that it's a 2x3 array, and so on. So this is usually what you work with in the actual libraries. 

So here I'm creating a tensor that has only a single element, 2.0. And then I'm casting it to be double, because Python is by default using double precision for its floating point numbers, so I'd like everything to be identical. By default, the data type of these tensors will be float32, so it's only using a single precision float. So I'm casting it to double, so that we have float64 just like in Python.

So I'm casting to double, and then we get something similar to value of 2. The next thing I have to do is, because these are leaf nodes, by default PyTorch assumes that they do not require gradients. So I need to explicitly say that all of these nodes require gradients. So this is going to construct scalar-valued one-element tensors, make sure that PyTorch knows that they require gradients. 

Now, by default, these are set to false, by the way, because of efficiency reasons, because usually you would not want gradients for leaf nodes, like the inputs to the network, and this is just trying to be efficient in the most common cases. So once we've defined all of our values in PyTorchland, we can perform arithmetic just like we can here in microgradland, so this will just work. And then there's a torch.tanh also.

And what we get back is a tensor again, and we can, just like in micrograd, it's got a data attribute, and it's got grad attributes. So these tensor objects, just like in micrograd, have a dot data and a dot grad. And the only difference here is that we need to call a dot item, because otherwise PyTorch dot item basically takes a single tensor of one element, and it just returns that element, stripping out the tensor. 

So let me just run this, and hopefully we are going to get, this is going to print the forward pass, which is 0.707, and this will be the gradients, which hopefully are 0.5, 0, negative 1.5, and 1. So if we just run this, there we go, 0.7, so the forward pass agrees, and then 0.5, 0, negative 1.5, and 1. So PyTorch agrees with us. And just to show you here, basically, O, here's a tensor with a single element, and it's a double, and we can call dot item on it to just get the single number out. So that's what item does. 

And O is a tensor object, like I mentioned, and it's got a backward function, just like we've implemented. And then all of these also have a dot grad, so like x2, for example, has a grad, and it's a tensor. And we can pop out the individual number with dot item.

So basically, Torch can do what we did in micrograd, as a special case, when your tensors are all single element tensors. But the big deal with PyTorch is that everything is significantly more efficient, because we are working with these tensor objects, and we can do lots of operations in parallel on all of these tensors. But otherwise, what we've built very much agrees with the API of PyTorch. 

Okay, so now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building up neural nets. And as I mentioned, neural nets are just a specific class of mathematical expressions. So we're going to start building out a neural net piece by piece, and eventually we'll build out a two-layer, multi-layer, layer perceptron, as it's called, and I'll show you exactly what that means.

Let's start with a single individual neuron. We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API in how it designs its neural network modules. So just like we saw that we can match the API of PyTorch on the autograd side, we're going to try to do that on the neural network modules. 

So here's class neuron, and just for the sake of efficiency, I'm going to copy-paste some sections that are relatively straightforward. So the constructor will take number of inputs to this neuron, which is how many inputs come to a neuron. So this one, for example, has three inputs. 

And then it's going to create a weight that is some random number between negative one and one for every one of those inputs, and a bias that controls the overall trigger happiness of this neuron. And then we're going to implement a def __call of self and x, some input x. And really what we want to do here is w times x plus b, where w times x here is a dot product specifically. Now if you haven't seen call, let me just return 0.0 here from now. 

The way this works now is we can have an x which is say like 2.0, 3.0, then we can initialize a neuron that is two-dimensional, because these are two numbers, and then we can feed those two numbers into that neuron to get an output. And so when you use this notation, n of x, Python will use call. So currently call just returns 0.0. Now we'd like to actually do the forward pass of this neuron instead. 

So what we're going to do here first is we need to basically multiply all of the elements of w with all of the elements of x pairwise. We need to multiply them. So the first thing we're going to do is we're going to zip up salta w and x. And in Python, zip takes two iterators, and it creates a new iterator that iterates over the tuples of their corresponding entries. 

So for example, just to show you, we can print this list and still return 0.0 here. Sorry. So we see that these w's are paired up with the x's, w with x. And now what we want to do is for wi xi in, we want to multiply wi times xi, and then we want to sum all of that together to come up with an activation, and add also salta b on top. 

So that's the raw activation, and then of course we need to pass that through a nonlinearity. So what we're going to be returning is act.nh. And here's out. So now we see that we are getting some outputs, and we get a different output from a neuron each time because we are initializing different weights and biases. 

And then to be a bit more efficient here actually, sum, by the way, takes a second optional parameter, which is the start. And by default, the start is 0, so these elements of this sum will be added on top of 0 to begin with, but actually we can just start with salta b, and then we just have an expression like this. And then the generator expression here must be parenthesized in Python. 

There we go. Yep, so now we can forward a single neuron. Next up, we're going to define a layer of neurons. 

So here we have a schematic for a MLP. So we see that these MLPs, each layer, this is one layer, has actually a number of neurons, and they're not connected to each other, but all of them are fully connected to the input. So what is a layer of neurons? It's just a set of neurons evaluated independently.

So in the interest of time, I'm going to do something fairly straightforward here. It's literally a layer. It's just a list of neurons. 

And then how many neurons do we have? We take that as an input argument here. How many neurons do you want in your layer? Number of outputs in this layer. And so we just initialize completely independent neurons with this given dimensionality. 

And when we call on it, we just independently evaluate them. So now instead of a neuron, we can make a layer of neurons. They are two-dimensional neurons, and let's have three of them. 

And now we see that we have three independent evaluations of three different neurons. Okay, and finally, let's complete this picture and define an entire multi-layer perceptron, or MLP. And as we can see here, in an MLP, these layers just feed into each other sequentially. 

So let's come here, and I'm just going to copy the code here in the interest of time. So an MLP is very similar. We're taking the number of inputs as before, but now instead of taking a single nout, which is number of neurons in a single layer, we're going to take a list of nouts, and this list defines the sizes of all the layers that we want in our MLP. 

So here we just put them all together, and then iterate over consecutive pairs of these sizes, and create layer objects for them. And then in the call function, we are just calling them sequentially. So that's an MLP, really. 

And let's actually re-implement this picture. So we want three input neurons, and then two layers of four, and an output unit. So we want a three-dimensional input. 

Say this is an example input. We want three inputs into two layers of four, and one output, and this, of course, is an MLP. And there we go. 

That's a forward pass of an MLP. To make this a little bit nicer, you see how we have just a single element, but it's wrapped in a list, because layer always returns lists. So for convenience, return outs at zero if len outs is exactly a single element, else return full list. 

And this will allow us to just get a single value out at the last layer that only has a single neuron. And finally, we should be able to draw dot of n of x. And as you might imagine, these expressions are now getting relatively involved. So this is an entire MLP that we're defining now, all the way until a single output.

And so obviously, you would never differentiate on pen and paper these expressions. But with micrograd, we will be able to back propagate all the way through this, and back propagate into these weights of all these neurons. So let's see how that works.

Okay, so let's create ourselves a very simple example data set here. So this data set has four examples. And so we have four possible inputs into the neural net, and we have four desired targets. 

So we'd like the neural net to assign or output 1.0 when it's fed this example, negative one when it's fed these examples, and one when it's fed this example. So it's a very simple binary classifier neural net, basically, that we would like here. Now let's think what the neural net currently thinks about these four examples. 

We can just get their predictions. Basically, we can just call n of x for x and xs. And then we can print. 

So these are the outputs of the neural net on those four examples. So the first one is 0.91, but we'd like it to be 1. So we should push this one higher. This one we want to be higher. 

This one says 0.88, and we want this to be negative 1. This is 0.88, we want it to be negative 1. And this one is 0.88, we want it to be 1. So how do we make the neural net and how do we tune the weights to better predict the desired targets? And the trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net. And we call this single number the loss. So the loss first is a single number that we're going to define that basically measures how well the neural net is performing. 

Right now, we have the intuitive sense that it's not performing very well, because we're not very much close to this. So the loss will be high, and we'll want to minimize the loss. So in particular, in this case, what we're going to do is we're going to implement the mean squared error loss. 

So what this is doing is we're going to basically iterate for y-ground truth and y-output in zip of y's and y-thread. So we're going to pair up the ground truths with the predictions, and the zip iterates over tuples of them. And for each y-ground truth and y-output, we're going to subtract them and square them. 

So let's first see what these losses are. These are individual loss components. And so basically for each one of the four, we are taking the prediction and the ground truth, we are subtracting them and squaring them. 

So because this one is so close to its target, 0.91 is almost one, subtracting them gives a very small number. So here we would get like a negative 0.1, and then squaring it just makes sure that regardless of whether we are more negative or more positive, we always get a positive number. Instead of squaring with 0, we could also take, for example, the absolute value. 

We need to discard the sign. And so you see that the expression is arranged so that you only get 0 exactly when y-out is equal to y-ground truth. When those two are equal, so your prediction is exactly the target, you are going to get 0. And if your prediction is not the target, you are going to get some other number.

So here, for example, we are way off. And so that's why the loss is quite high. And the more off we are, the greater the loss will be. 

So we don't want high loss, we want low loss. And so the final loss here will be just the sum of all of these numbers. So you see that this should be 0, roughly, plus 0, roughly, but plus 7. So loss should be about 7 here. 

And now we want to minimize the loss. We want the loss to be low, because if loss is low, then every one of the predictions is equal to its target. So the loss, the lowest it can be is 0, and the greater it is, the worse off the neural net is predicting. 

So now, of course, if we do loss.backward, something magical happened when I hit enter. And the magical thing, of course, that happened is that we can look at n.layers.neuron, n.layers at, say, the first layer, .neurons at 0, because remember that MLP has the layers, which is a list, and each layer has neurons, which is a list, and that gives us an individual neuron, and then it's got some weights. And so we can, for example, look at the weights at 0. Oops, it's not called weights. 

It's called w. And that's a value, but now this value also has a grad because of the backward pass. And so we see that because this gradient here on this particular weight of this particular neuron of this particular layer is negative, we see that its influence on the loss is also negative. So slightly increasing this particular weight of this neuron of this layer would make the loss go down. 

And we actually have this information for every single one of our neurons and all of their parameters. Actually, it's worth looking at also the draw.loss, by the way. So previously, we looked at the draw.of a single neuron forward pass, and that was already a large expression. 

But what is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error. And so this is a really massive graph, because this graph that we've built up now, oh my gosh, this graph that we've built up now, which is kind of excessive, it's excessive because it has four forward passes of a neural net for every one of the examples, and then it has the loss on top, and it ends with the value of the loss, which was 7.12. And this loss will now back propagate through all the four forward passes, all the way through just every single intermediate value of the neural net, all the way back to, of course, the parameters of the weights, which are the input. So these weight parameters here are inputs to this neural net, and these numbers here, these scalars, are inputs to the neural net. 

So if we went around here, we will probably find some of these examples, this 1.0, potentially maybe this 1.0, or, you know, some of the others, and you'll see that they all have gradients as well. The thing is, these gradients on the input data are not that useful to us, and that's because the input data seems to be not changeable. It's a given to the problem, and so it's a fixed input. 

We're not going to be changing it or messing with it, even though we do have gradients for it. But some of these gradients here will be for the neural network parameters, the Ws and the Bs, and those we, of course, we want to change. Okay, so now we're going to want some convenience code to gather up all of the parameters of the neural net so that we can operate on all of them simultaneously, and every one of them we will nudge a tiny amount based on the gradient information.

So let's collect the parameters of the neural net all in one array. So let's create a parameters of self that just returns self.w, which is a list, concatenated with a list of self.d. So this will just return a list. List plus list just, you know, gives you a list.

So that's parameters of neuron, and I'm calling it this way because also PyTorch has parameters on every single NN module, and it does exactly what we're doing here. It just returns the parameter tensors. For us, it's the parameter scalars.

Now, layer is also a module, so it will have parameters, self, and basically what we want to do here is something like this, like params is here, and then for neuron in self.neurons, we want to get neuron.parameters, and we want to params.extend. So these are the parameters of this neuron, and then we want to put them on top of params, so params.extend of p's, and then we want to return params. So this is way too much code, so actually there's a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters. So it's a single list comprehension. In Python, you can sort of nest them like this, and you can then create the desired array. 

So these are identical. We can take this out.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

And then let's do the same here. Def parameters self and return a parameter for layer in self.layers for p in layer.parameters. And that should be good. Now let me pop out this so we don't re-initialize our network because we need to re-initialize our okay so unfortunately we will have to probably re-initialize the network because we just had functionality because this class of course we I want to get all the end up parameters but that's not going to work because this is the old class.

Okay so unfortunately we do have to re-initialize the network which will change some of the numbers but let me do that so that we pick up the new API we can now do end up parameters and these are all the weights and biases inside the entire neural net. So in total this MLP has 41 parameters and now we'll be able to change them. If we recalculate the loss here we see that unfortunately we have slightly different predictions and slightly different loss but that's okay.

Okay so we see that this neuron's gradient is slightly negative we can also look at its data right now which is 0.85 so this is the current value of this neuron and this is its gradient on the loss so what we want to do now is we want to iterate for every p in end up parameters so for all the 41 parameters in this neural net we actually want to change p.data slightly according to the gradient information. Okay so dot dot dot to do here but this will be basically a tiny update in this gradient descent scheme and in gradient descent we are thinking of the gradient as a vector pointing in the direction of increased loss and so in gradient descent we are modifying p.data by a small step size in the direction of the gradient so the step size as an example could be like a very small number like 0.01 is the step size times p.grad right but we have to think through some of the signs here so in particular working with this specific example here we see that if we just left it like this then this neuron's value would be currently increased by a tiny amount of the gradient the gradient is negative so this value of this neuron would go slightly down it would become like 0.84 or something like that but if this neuron's value goes lower that would actually increase the loss that's because the derivative of this neuron is negative so increasing this makes the loss go down so increasing it is what we want to do instead of decreasing it so basically what we're missing here is we're actually missing a negative sign and again this other interpretation and that's because we want to minimize the loss we don't want to maximize the loss we want to decrease it and the other interpretation as I mentioned is you can think of the gradient vector so basically just the vector of all the gradients as pointing in the direction of increasing the loss but then we want to decrease it so we actually want to go in the opposite direction and so you can convince yourself that this sort of point does the right thing here with the negative because we want to minimize the loss so if we nudge all the parameters by a tiny amount then we'll see that this data will have changed a little bit so now this neuron is a tiny amount greater value so 0.854 went to 0.857 and that's a good thing because slightly increasing this neuron data makes the loss go down according to the gradient and so the correct thing has happened sign-wise and so now what we would expect of course is that because we've changed all these parameters we expect that the loss should have gone down a bit so we want to re-evaluate the loss let me basically this is just a data definition that hasn't changed but the forward pass here of the network we can recalculate and actually let me do it outside here so that we can compare the two loss values so here if I recalculate the loss we'd expect the new loss now to be slightly lower than this number so hopefully what we're getting now is a tiny bit lower than 4.84 4.36 okay and remember the way we've arranged this is that low loss means that our predictions are matching the targets so our predictions now are probably slightly closer to the targets and now all we have to do is we have to iterate this process so again we've done the forward pass and this is the loss now we can loss that backward let me take these out and we can do a step size and now we should have a slightly lower loss 4.36 goes to 3.9 and okay so we've done the forward pass here's the backward pass nudge and now the loss is 3.66 3.47 and you get the idea we just continue doing this and this is uh gradient descent we're just iteratively doing forward pass backward pass update forward pass backward pass update and the neural net is improving its predictions so here if we look at y pred now y pred we see that um this value should be getting closer to one so this value should be getting more positive these should be getting more negative and this one should be also getting more positive so if we just iterate this a few more times actually we may be able to afford to go a bit faster let's try a slightly higher learning rate whoops okay there we go so now we're at 0.31 if you go too fast by the way if you try to make it too big of a step you may actually overstep um it's overconfidence because again remember we don't actually know exactly about the loss function the loss function has all kinds of structure and we only know about the very local dependence of all these parameters on the loss but if we step too far we may step into you know a part of the loss that is completely different and that can destabilize training and make your loss actually blow up even so the loss is now 0.04 so actually the predictions should be really quite close let's take a look so you see how this is almost one almost negative one almost one we can continue going uh so yep backward update oops there we go so we went way too fast and um we actually overstepped so we got to uh too eager where are we now oops okay seven in negative nine so this is very very low loss and the predictions are basically perfect so somehow we basically we were doing way too big updates and we briefly exploded but then somehow we ended up getting into a really good spot so usually this learning rate and the tuning of it is a is a subtle art you want to set your learning rate if it's too low you're going to take way too long to converge but if it's too high the whole thing gets unstable and you might actually even explode the loss depending on your loss function so finding the step size to be just right it's it's a pretty subtle art sometimes when you're using sort of vanilla gradient descent but we happen to get into a good spot we can look at n dot parameters so this is the setting of weights and biases that makes our network predict the desired targets very very close and basically we've successfully trained a neural net okay let's make this a tiny bit more respectable and implement an actual training loop and what that looks like so this is the data definition that stays this is the forward pass so for k in range you know we're going to take a bunch of steps first you do the forward pass we validate the loss let's reinitialize the neural net from scratch and here's the data and we first do forward pass then we do the backward pass and then we do an update that's gradient descent and then we should be able to iterate this and we should be able to print the current step the current loss let's just print the sort of number of the loss and that should be it and then the learning rate 0.01 is a little too small 0.1 we saw is like a little bit dangerously too high let's go somewhere in between and we'll optimize this for not 10 steps but let's go for say 20 steps let me erase all of this junk and let's run the optimization and you see how we've actually converged slower in a more controlled manner and got to a loss that is very low so i expect white bread to be quite good there we go um and that's it okay so this is kind of embarrassing but we actually have a really terrible bug in here and it's a subtle bug and it's a very common bug and i can't believe i've done it for the 20th time in my life especially on camera and i could have re-shot the whole thing but i think it's pretty funny and you know you get to appreciate a bit what um working with neural is like sometimes we are guilty of common bug i've actually tweeted the most common neural net mistakes a long time ago now uh and i'm not really gonna explain any of these except for we are guilty of number three you forgot to zero grad before dot backward what is that basically what's happening and it's a subtle bug and i'm not sure if you saw it is that all of these weights here have a dot data and a dot grad and the dot grad starts at zero and then we do backward and we fill in the gradients and then we do an update on the data but we don't flush the grad it stays there so when we do the second forward pass and we do backward again remember that all the backward operations do a plus equals on the grad and so these gradients just add up and they never get reset to zero so basically we didn't zero grad so here's how we zero grad before backward we need to iterate over all the parameters and we need to make sure that p dot grad is set to zero we need to reset it to zero just like it is in the constructor so remember all the way here for all these value nodes grad is reset to zero and then all these backward passes do a plus equals from that grad but we need to make sure that we reset these grads to zero so that when we do backward all of them start at zero and the actual backward pass accumulates the loss derivatives into the grads so this is zero grad in pytorch and we will slightly we'll get a slightly different optimization let's reset the neural net the data is the same this is now i think correct and we get a much more you know we get a much more slower descent we still end up with pretty good results and we can continue this a bit more to get down lower and lower and lower yeah so the only reason that the previous thing worked it's extremely buggy the only reason that worked is that this is a very very simple problem and it's very easy for this neural net to fit this data and so the grads ended up accumulating and it effectively gave us a massive step size and it made us converge extremely fast but basically now we have to do more steps to get to very low values of loss and get y pred to be really good we can try to step a bit greater yeah we're gonna get closer and closer to one minus one and one so working with neural nets is sometimes tricky because you may have lots of bugs in the code and your network might actually work just like ours worked but chances are is that if we had a more complex problem then actually this bug would have made us not optimize the loss very well and we were only able to get away with because the problem is very simple so let's now bring everything together and summarize what we learned what are neural nets neural nets are these mathematical expressions fairly simple mathematical expressions in the case of multilayer perceptron that take input as the data and they take input the weights and the parameters of the neural net mathematical expression for the forward pass followed by a loss function and the loss function tries to measure the accuracy of the predictions and usually the loss will be low when your predictions are matching your targets or where the network is basically behaving well so we we manipulate the loss function so that when the loss is low the network is doing what you want it to do on your problem and then we backward the loss use back propagation to get the gradient and then we know how to tune all the parameters to decrease the loss locally but then we have to iterate that process many times in what's called the gradient descent so we simply follow the gradient information and that minimizes the loss and the loss is arranged so that when the loss is minimized the network is doing what you want it to do and yeah so we just have a blob of neural stuff and we can make it do arbitrary things and that's what gives neural nets their power it's you know this is a very tiny network with 41 parameters but you can build significantly more complicated neural nets with billions at this point almost trillions of parameters and it's a massive blob of neural tissue simulated neural tissue roughly speaking and you can make it do extremely complex problems and these neural nets then have all kinds of very fascinating emergent properties in when you try to make them do significantly hard problems as in the case of gpt for example we have massive amounts of text from the internet and we're trying to get a neural to predict to take like a few words and try to predict the next word in a sequence that's the learning problem and it turns out that when you train this on all of internet the neural net actually has like really remarkable emergent properties but that neural net would have hundreds of billions of parameters but it works on fundamentally the exact same principles the neural net of course will be a bit more complex but otherwise the evaluating the gradient is there and would be identical and the gradient descent would be there and would be basically identical but people usually use slightly different updates this is a very simple stochastic gradient descent update and the loss function would not be a mean squared error they would be using something called the cross entropy loss for predicting the next token so there's a few more details but fundamentally the neural network setup and neural network training is identical and pervasive and now you understand intuitively how that works under the hood in the beginning of this video i told you that by the end of it you would understand everything in micrograd and then we'd slowly build it up let me briefly prove that to you so i'm going to step through all the code that is in micrograd as of today actually potentially some of the code will change by the time you watch this video because i intend to continue developing micrograd but let's look at what we have so far at least init.py is empty when you go to engine.py that has the value everything here you should mostly recognize so we have the dot data dot grad attributes we have the backward function we have the previous set of children and the operation that produced this value we have addition multiplication and raising to a scalar power we have the relu nonlinearity which is slightly different type of nonlinearity than tanh that we used in this video both of them are nonlinearities and notably tanh is not actually present in micrograd as of right now but i intend to add it later we have the backward which is identical and then all of these other operations which are built up on top of operations here so values should be very recognizable except for the nonlinearity used in this video there's no massive difference between relu and tanh and sigmoid and these other nonlinearities they're all roughly equivalent and can be used in MLPs so i use tanh because it's a bit smoother and because it's a little bit more complicated than relu and therefore it's stressed a little bit more the the local gradients and working with those derivatives which i thought would be useful nn.py is the neural networks library as i mentioned so you should recognize identical implementation of neural layer and MLP notably or not so much we have a class module here there's a parent class of all these modules i did that because there's an nn.module class in pytorch and so this exactly matches that api and nn.module in pytorch has also a zero grad which i refactored out here so that's the end of micrograd really then there's a test which you'll see basically creates two chunks of code one in micrograd and one in pytorch and we'll make sure that the forward and the backward paths agree identically for a slightly less complicated expression a slightly more complicated expression everything agrees so we agree with pytorch on all these operations and finally there's a demo.pyymb here and it's a bit more complicated binary classification demo than the one i covered in this lecture so we only had a tiny data set of four examples here we have a bit more complicated example with lots of blue points and lots of red points and we're trying to again build a binary classifier to distinguish two-dimensional points as red or blue it's a bit more complicated MLP here with it's a bigger MLP the loss is a bit more complicated because it supports batches so because our data set was so tiny we always did a forward pass on the entire data set of four examples but when your data set is like a million examples what we usually do in practice is we basically pick out some random subset we call that a batch and then we only process the batch forward backward and update so we don't have to forward the entire training set so this supports batching because there's a lot more examples here we do a forward pass the loss is slightly more different this is a max margin loss that I implement here the one that we used was the mean squared error loss because it's the simplest one there's also the binary cross entropy loss all of them can be used for binary classification and don't make too much of a difference in the simple examples that we looked at so far there's something called L2 regularization used here this has to do with generalization of the neural net and controls the overfitting in machine learning setting but I did not cover these concepts in this video potentially later and the training loop you should recognize so forward backward with zero grad and update and so on you'll notice that in the update here the learning rate is scaled as a function of number of iterations and it shrinks and this is something called learning rate decay so in the beginning you have a high learning rate and as the network sort of stabilizes near the end you bring down the learning rate to get some of the fine details in the end and in the end we see the decision surface of the neural net and we see that it learned to separate out the red and the blue area based on the data points so that's the slightly more complicated example in the demo demo.pyymb that you're free to go over but yeah as of today that is micrograd I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in a production grade library like pytorch so in particular I wanted to show I wanted to find and show you the backward pass for 10h in pytorch so here in micrograd we see that the backward pass for 10h is 1 minus t square where t is the output of the 10h of x times out that grad which is the chain rule so we're looking for something that looks like this now I went to pytorch which has an open source github codebase and I looked through a lot of its code and honestly I spent about 15 minutes and I couldn't find 10h and that's because these libraries unfortunately they grow in size and entropy and if you just search for 10h you get apparently 2800 results and 400 and 406 files so I don't know what these files are doing honestly and why there are so many mentions of 10h but unfortunately these libraries are quite complex they're meant to be used not really inspected eventually I did stumble on someone who tries to change the 10h backward code for some reason and someone here pointed to the cpu kernel and the cuda kernel for 10h backward so this so basically depends on if you're using pytorch on a cpu device or on the gpu which these are different devices and I haven't covered this but this is the 10h backward kernel for cpu and the reason it's so large is that number one this is like if you're using a complex type which we haven't even talked about if you're using a specific data type of bfloat16 which we haven't talked about and then if you're not then this is the kernel and deep here we see something that resembles our backward pass so they have a times one minus b square so this b b here must be the output of the 10h and this is the out.grad so here we found it deep inside pytorch on this location for some reason inside binary ops kernel when 10h is not actually a binary op and then this is the gpu kernel we're not complex we're here and here we go with one line of code so we did find it but basically unfortunately these code bases are very large and micrograd is very very simple but if you actually want to use real stuff finding the code for it you'll actually find that difficult I also wanted to show you a little example here where pytorch is showing you how you can register a new type of function that you want to add to pytorch as a lego building block so here if you want to for example add a legendre polynomial 3 here's how you could do it you will register it as a class that subclasses torch.hardware.function and then you have to tell pytorch how to forward your new function and how to backward through it so as long as you can do the forward pass of this little function piece that you want to add and as long as you know the local derivative local gradients which are implemented in the backward pytorch will be able to back propagate through your function and then you can use this as a lego block in a larger lego castle of all the different lego blocks that pytorch already has and so that's the only thing you have to tell pytorch and everything would just work and you can register new types of functions in this way following this example and that is everything that I wanted to cover in this lecture so I hope you enjoyed building out micrograd with me I hope you find it interesting insightful and yeah I will post a lot of the links that are related to this video in the video description below I will also probably post a link to a discussion forum or discussion group where you can ask questions related to this video and then I can answer or someone else can answer your questions and I may also do a follow-up video that answers some of the most common questions but for now that's it I hope you enjoyed it if you did then please like and subscribe so that youtube knows to feature this video to more people and that's it for now I'll see you later now here's the problem we know dl by wait what is the problem and that's everything I wanted to cover in this lecture so I hope you enjoyed us building up micrograb micrograb okay now let's do the exact same thing for multiply because we can't do something like a times two whoops I know what happened there

(转录由TurboScribe.ai完成。升级到无限以移除此消息。)