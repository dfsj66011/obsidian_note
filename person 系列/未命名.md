
你好，我叫安德烈，从事深度神经网络训练已有十多年。在这节课中，我想向你们展示神经网络训练的内部运作原理。具体来说，我们将从一个空白的Jupyter笔记本开始，到课程结束时，我们将定义并训练一个神经网络，你们将看到所有内部运作的细节，以及它在直观层面上的工作原理。

今天我想带你们一步步构建Micrograd。Micrograd是我大约两年前在GitHub上发布的一个库，但当时我只上传了源代码，你们需要自己去研究它的工作原理。在这节课中，我将逐步讲解并对其各部分进行说明。那么，Micrograd是什么？它为什么有趣？谢谢。

Micrograd本质上是一个自动梯度引擎（autograd是automatic gradient的缩写），它的核心功能是实现反向传播。反向传播是一种算法，可以高效计算损失函数相对于神经网络权重的梯度。这使得我们能够迭代调整神经网络的权重，以最小化损失函数，从而提高网络的准确性。因此，反向传播是现代深度神经网络库（如PyTorch或JAX）的数学核心。

Micrograd的功能最好通过示例来说明。如果我们往下看，会发现Micrograd允许我们构建数学表达式。这里我们构建了一个表达式，其中有两个输入a和b（值分别为-4和2），并将它们封装在Micrograd的Value对象中。这个Value对象会包装数字本身，然后我们构建一个数学表达式，将a和b转换为c、d，最终得到e、f和g。我展示了Micrograd支持的一些功能和操作，比如加法、乘法、幂运算、偏移、取负、截断、平方、除法等。

我们以这两个输入a和b构建了一个表达式图，最终输出值为g。Micrograd会在后台构建整个数学表达式，比如知道c是通过加法操作得到的，其子节点是a和b，因此会维护指向a和b的指针，从而清楚地了解整个表达式的结构。

除了前向传播（即计算g的值，这里结果为24.7），更重要的是我们可以对g调用.backward()方法，启动反向传播。反向传播会从g开始，沿着表达式图递归应用微积分中的链式法则，计算g相对于所有内部节点（如e、d、c）以及输入a和b的导数。例如，我们可以查询g对a的导数（a.grad），这里是138，以及g对b的导数（b.grad），这里是645。这些导数非常重要，因为它们告诉我们a和b如何通过数学表达式影响g。具体来说，a.grad是138。

因此，如果我们稍微调整a并使其略微增大，138这个数值告诉我们g将会增长，其增长斜率为138。而b的增长斜率将是645。这向我们展示了当a和b朝正向发生微小变动时，g会如何响应。好的，呃，现在你可能会对我们构建的这个表达式感到困惑，顺便说一句，这个表达式完全没有任何意义，我只是随口编造的。我只是在展示micrograd支持的各种运算类型。我们真正关心的是神经网络，但事实上神经网络也不过是数学表达式而已。就像这个例子一样，只不过神经网络甚至还没这么复杂。神经网络本质上就是一个数学表达式，它以输入数据作为输入，以神经网络的权重作为输入，通过数学运算最终输出神经网络的预测值或损失函数。稍后我们会具体看到这一点。但基本上，神经网络恰好是某一类特定的数学表达式。而反向传播实际上具有更广泛的适用性，它根本不局限于神经网络，只关乎任意数学表达式。我们只是恰好利用这个机制来训练神经网络。

现在我想补充说明的是，正如你所见，micrograd是一个标量值自动求导引擎。它工作在单个标量层面，比如-4和2这样的数值。我们将神经网络拆解到最基本的标量原子单位，处理所有细小的加法和乘法运算，这种做法显然过于繁琐。在实际生产中你绝不会这样操作，这纯粹出于教学目的——让我们不必处理现代深度神经网络库中那些N维张量，从而专注于理解反向传播、链式法则和神经网络训练的本质。如果你想训练更大的网络，就必须使用张量运算，但背后的数学原理完全不变，改用张量纯粹是为了提升效率。

我们基本上是在处理标量值，所有的标量值。我们将它们打包成张量，张量其实就是这些标量的数组。然后因为我们有了这些大型数组，我们就可以对这些大型数组进行操作，从而利用计算机的并行性。所有这些操作都可以并行完成，这样整个计算过程就会更快。但实际上数学原理并没有改变，这些操作纯粹是为了提高效率。因此，我认为从教学的角度来看，从一开始就处理张量并没有太大意义。这也是我编写MicroGrad的根本原因，因为你可以从基础层面理解事物是如何运作的，之后再进行加速。

好了，现在进入有趣的部分。我的观点是，MicroGrad就是你训练神经网络所需的一切，其他的一切都只是为了效率。你可能会认为MicroGrad是一个非常复杂的代码库，但实际上并非如此。如果我们直接看MicroGrad，你会发现它只有两个文件。这就是实际的引擎，它对神经网络一无所知。而这就是整个建立在MicroGrad之上的神经网络库：引擎和nn.py。

这个实际的反向传播自动微分引擎，它赋予你神经网络的力量，实际上只有100行非常简单的Python代码。我们将在本讲座结束时理解这些代码。然后是nn.py，这个建立在自动微分引擎之上的神经网络库简直像个笑话。我们必须定义什么是神经元，然后定义什么是神经元层，接着定义什么是多层感知机，它只是一系列神经元层的序列。所以这完全就是个笑话。基本上，150行代码就能带来巨大的能力，这就是你理解神经网络训练所需的一切，其他的一切都只是为了效率。当然，效率方面还有很多内容，但本质上就是这些。

好的，现在让我们直接开始，一步步实现微梯度。首先，我想确保你对导数有一个非常直观的理解，以及它具体能提供什么信息。让我们从一些基本的导入开始，这些是我在每个Jupyter笔记本中都会复制粘贴的内容。然后，我们定义一个标量值函数f(x)，如下所示。这个函数是我随机编的，我只是想要一个接受单个标量x并返回单个标量y的标量值函数。当然，我们可以调用这个函数，比如传入3.0，得到20。现在，我们还可以绘制这个函数，以了解它的形状。从数学表达式可以看出，这可能是一个抛物线。

这是一个二次函数，所以如果我们创建一组标量值，比如使用从-5到5、步长为0.25的范围（注意x轴范围是从-5到5，不包括5，步长0.25）。我们也可以在这个numpy数组上调用这个函数。如果我们在x轴上调用f函数，就会得到一组y值，这些y值实际上是独立地对每个元素应用函数的结果。我们可以用matplotlib绘制这个图像，使用plt.plot(x, y)就能得到一条漂亮的抛物线。之前我们在这里输入了3.0，得到了20这个y坐标值。现在我想思考的是：这个函数在任意输入点x处的导数是什么？

那么这个函数在不同x点的导数是什么呢？如果你还记得微积分课上的内容，你可能已经推导过导数了。我们拿这个数学表达式3x² - 4x + 5来说，你会在纸上写出来，然后运用乘积法则等各种规则，推导出原函数的导数表达式。之后你就可以代入不同的x值，看看导数是多少。但我们实际上不会这么做，因为在神经网络中没人会真的写出神经网络的表达式。那会是一个极其庞大的表达式，可能有成千上万个项。当然，没人会真的去推导这个导数。所以我们不打算采用这种符号化的方法。相反，我想做的是看看导数的定义，确保我们真正理解导数在测量什么，它能告诉我们关于函数的哪些信息。如果我们查一下导数的定义，就会发现...

因此，这并不是导数的精确定义，而是可微性的定义。但如果你还记得微积分中的内容，导数的定义是当h趋近于零时，[f(x+h) - f(x)]/h的极限。简单来说，它表达的是：如果你在某个关注点x（或a）处稍微增加一个微小量h，函数会如何响应？响应的敏感度是多少？该点的斜率是多少？函数是上升还是下降？变化幅度有多大？这就是函数在该点的斜率——即响应变化的斜率。

我们可以通过选取一个极小的h值（比如0.001）来数值化计算此处的导数。虽然定义要求h趋近于零，但我们只需取一个极小值即可。假设我们关注x=3.0，已知f(x)=20。那么当x正向微调时，函数会如何变化？直观来看，由于当前函数值为20且x=3，正向微调时函数值应该会略大于20。变化幅度则反映了斜率的大小——[f(x+h)-f(x)]表示函数正向变化的增量，再除以h（即"上升量/前进量"）就得到斜率近似值。

当然这只是斜率的数值近似，因为需要h无限趋近于零才能得到精确值。不过要注意，如果h取值过小（比如添加过多零），由于计算机浮点数精度的限制，反而会得到错误结果。但通过这种方法，我们可以逼近正确答案。在x=3处，斜率计算值为14——这可以通过心算验证：对原函数3x²-4x+5求导得6x-4，代入x=3确实得到18-4=14。

那么当x=-3时呢？虽然难以心算具体值，但斜率的符号可以判断：在x=-3处若正向微调x，函数值会下降，因此斜率应为负值，函数值会略低于20。

因此，如果我们取斜率，我们预期会是负值，大约是负22。好，在某个点上，当然斜率会变为零。对于这个特定函数，我之前查过，是在2/3的位置。所以大约在2/3处，也就是这里的某个位置，这个导数会变为零。基本上在那个精确的点上，如果我们往正方向稍微推动，函数几乎不会有反应，保持不变，这就是为什么斜率为零。好。现在让我们看一个稍微复杂些的情况。我们要开始增加一些复杂性了。现在我们有一个输出变量为b的函数，它依赖于三个标量输入a、b和c。a、b和c是一些具体的值，作为我们表达式图的三个输入，而d是单一输出。如果我们直接打印d，会得到4。现在我想做的是再次查看d相对于a、b和c的导数，并思考这些导数告诉我们什么。为了计算这个导数，我们需要稍微取巧一些。

我们将再次取一个非常小的h值，然后我们会把输入固定在某个我们感兴趣的数值上。这些就是我们要在该点评估d对a、b和c的导数的点a、b、c。这些是输入值，现在我们得到d1就是这个表达式。接着，例如，我们会看d对a的导数。我们会取a，给它增加h，然后得到d2，这是完全相同的函数。现在我们要打印，比如f1 d1是d1，d2是d2，并打印斜率。这里的导数或斜率当然是d2减去d1除以h。d2减去d1就是当我们把感兴趣的特定输入增加一个微小量时，函数增加了多少，然后除以h得到斜率。嗯，是的。所以我运行这个，我们会打印d1，我们知道它是4。现在d2会是a增加了h后的结果。让我们稍微思考一下d2会是多少，特别是打印出来。D1会是4，d2会比4稍大还是稍小，这会告诉我们导数的符号。我们给a增加了h，b是-3，c是10。你可以直观地思考这个导数及其作用。a会稍微更正值，但b是负数。所以如果a稍微更正值，因为b是-3，我们实际上会向d增加得更少。所以你会预期函数的值会下降。让我们看看这个。是的，我们从4降到了3.9996，这告诉我们斜率会是负数，因为下降了。斜率的精确值是-3。你也可以从数学和分析上确信-3是正确的答案，因为如果你有a乘以b加c，并且你懂微积分，那么对a求导就得到b。确实，b的值是-3，也就是我们得到的导数，所以你可以看出这是正确的。现在如果我们对b做同样的事情，如果我们给b增加一点点正值方向，我们会得到不同的斜率。b对输出d的影响是什么？如果我们给b增加一点点正值方向，因为a是正的，我们会向d增加更多，对吧？那么敏感性是什么？这个增加的斜率是多少？你可能不会惊讶这应该是2。为什么是2？因为d对b求导会得到a，而a的值是2。所以这也很好。然后如果c增加一点点h，a乘以b不受影响，c会稍微高一点。这对函数有什么影响？它会让函数稍微高一点，因为我们只是增加了c，而且增加的幅度和我们给c增加的完全一样。所以这告诉你斜率是1，也就是D随着c增加而增加的速率。好了，现在我们直观地理解了导数在告诉我们关于函数的什么信息。我们想转向神经网络。正如我提到的，神经网络会是相当庞大的数学表达式。所以我们需要一些数据结构来维护这些表达式，这就是我们现在要开始构建的。我们要构建这个我在micro grad的README页面上展示的value对象。让我复制粘贴一个非常简单的value对象的骨架。类value接受一个标量值，包装并跟踪它，就是这样。

例如，我们可以创建一个值为2.0的对象，然后查看其内容。Python内部会使用包装函数返回这样的字符串。这就是我们正在创建的数据等于2的值对象。现在，我们不仅希望有两个值，还希望能实现a加b的操作，对吧？我们想对它们进行加法运算。目前你会得到一个错误，因为Python不知道如何将两个值对象相加。所以我们需要告诉它如何操作。这里就是加法运算的实现方式。基本上，你需要使用Python中这些特殊的双下划线方法来为这些对象定义运算符。因此，如果我们使用加号运算符，Python内部会调用a的__add__方法，传入b作为参数。这就是内部发生的过程。b将是另一个操作数，而self则是a。所以我们会返回一个新的值对象，它只是包装了它们数据的加法结果。但请记住，因为data实际上是Python的数字类型，所以这里的运算符现在只是典型的浮点数加法运算。

这并不是对值对象的简单相加，我们会返回一个新值。现在执行a加b应该能正常运行，并输出负一的结果，因为二加负三等于负一。好了，接下来我们实现乘法运算，这样就能重现这个表达式了。乘法操作想必不会让大家意外，实现方式会非常相似——只需把add替换为mul，这里的运算符自然也换成乘号。现在我们可以创建一个值为10.0的c值对象，接下来应该就能执行a乘b了。先试试a乘b吧，现在显示结果是负六。顺便说下，我之前跳过了这个细节：假设没有封装函数的话，你会得到看起来比较晦涩的表达式。这个封装函数的作用就是让我们能以更美观的方式输出Python表达式，而不是显示难以理解的代码。比如现在就能清楚地看到"值为负六"的输出。这样我们就实现了乘法运算，接下来应该还能加上c值，因为我们已经定义并告诉Python如何执行乘法和加法运算。这实际上等同于先执行a.mul(b)，然后这个新值对象再执行.add(c)。让我们看看效果——没错，成功运行了。

这样我们就得到了四个，这正是我们期望从四位比特中得到的结果。而且我相信我们也可以手动调用它们。好了，就这样。是的，好的。

所以现在我们缺少的是这个表达式的连接组织，正如我提到的，我们希望保留这些表达式图。因此，我们需要知道并保留关于哪些值产生其他值的指针。例如，这里我们将引入一个新变量，我们称之为children，默认情况下它将是一个空元组。然后我们实际上会在类中保留一个稍有不同的变量，我们称之为_prev，它将是children的集合。呃，我就是这样做的。我在原始的micrograd中就是这样实现的，查看我这里的代码，我不完全记得原因了，我相信是为了效率。但为了方便起见，这个_children将是一个元组。然后当我们在类中实际维护它时，为了效率，我相信它将是这个集合。所以现在，当我们像这样用构造函数创建一个值时，children将为空，而prev将是空集。但是当我们通过加法或乘法创建一个值时，我们将传入这个值的children，在这种情况下就是self和other。所以这些就是这里的children。所以现在我们可以做d.prev，我们会看到children现在是-6的值和10的值。当然，这是a乘以b的结果值和c的值，也就是10。现在我们不知道的最后一条信息。所以我们现在知道每个值的children，但我们不知道是什么操作创建了这个值。所以我们需要在这里再添加一个元素，我们称之为_op。默认情况下，对于叶子节点它是空集，然后我们就在这里维护它。现在操作将只是一个简单的字符串，在加法的情况下是"+"，在乘法的情况下是"\*"。所以现在我们不仅有d.prev。

我们还有一个d.dot.op，并且我们知道d是由这两个值相加产生的。现在，我们有了完整的数学表达式，并且正在构建这个数据结构，我们确切地知道每个值是如何产生的——通过什么表达式以及来自哪些其他值。由于这些表达式即将变得相当庞大，我们希望能有一种方法来清晰地可视化这些正在构建的表达式。为此，我将复制粘贴一段看起来有点吓人的代码，它将帮助我们可视化这些表达式图。下面是代码，稍后我会解释它。但首先，让我先展示一下这段代码的功能。基本上，它会创建一个新的函数draw.dot，我们可以对某个根节点调用它，然后它就会将其可视化。

因此，如果我们调用绘制点函数处理d（即这里的最终值a乘以b加c），就会生成类似这样的结构——这就是d。你可以看到这是a乘以b产生的中间值，再加上c就得到了输出节点d。这个图形是从b延伸出来的。我不会深入讲解每个细节，你可以查阅Graphviz及其API文档。Graphviz是一款开源的图形可视化软件，我们正在通过其API构建这个计算图。

本质上，trace这个辅助函数会枚举图中的所有节点和边，生成完整的节点与边集合。然后我们遍历所有节点，用dot_node创建特殊节点对象，再用dot_edge创建连接边。这里唯一需要技巧的是，你会注意到我添加了这些表示运算的虚拟节点——比如这个加号节点。我创建了这些特殊的运算节点并按逻辑连接它们。需要注意的是，这些运算节点并非原始图中的真实节点，也不是实际的值对象。真正的值对象是方框内的元素，而这些运算节点只是为了绘图美观在draw_dot函数中临时创建的。

让我们给图表添加标签以便识别变量位置：默认设置label属性为空字符串并保存在每个节点中。这里我们将a的标签设为"a"，b设为"b"，c设为"c"。然后我们特别创建e=a×b的关系，其点标签设为"e"；接着e加上c得到d，将d的点标签设为"b"。实际上这些操作并不会改变计算逻辑。

（注：根据技术文档的翻译规范，保留了Graphviz/dot_node/dot_edge等专业术语的原始命名，将"op nodes"译为"运算节点"，"value objects"译为"值对象"，通过增补"计算图""辅助函数"等专业表述确保技术准确性。长句拆分为符合中文阅读习惯的短句结构，同时保持被动语态到主动语态的转换，如"you'll notice that I..."译为"你会注意到我..."）

我刚刚添加了这个新的e函数和e变量，然后在这里打印时，我会在这里打印标签。所以这里会有一个%s的占位符，对应的是n.label。现在我们在左侧有了标签。这就是a、b创建e，然后e加c生成d，就像我们这里展示的一样。最后，让我们把这个表达式再深入一层。d将不再是最终的输出节点，而是在d之后，我们会创建一个名为f的新值对象——变量很快就不够用了——f的值是-2.0，它的标签当然就是f。然后大写的L将作为我们图的输出。

所以L的输出值是-8。现在我们要画的不是D，而是L。但不知为何L的标签未定义，必须明确指定L.label的值。

好了，L就是输出。让我们快速回顾一下到目前为止所做的工作。目前，我们已经能够仅用加法和乘法来构建数学表达式。

它们是沿路径的标量值。我们可以进行前向传递并构建出数学表达式。这里我们有多个输入A、B、C和F，进入一个数学表达式后产生单个输出L。这里展示的就是前向传递的可视化过程。

所以前向传播的输出是负8。这就是数值。接下来我们要做的是运行反向传播。在反向传播中，我们将从末端开始，逆向计算所有这些中间值的梯度。

实际上，我们在这里计算的每一个值，都是要计算该节点相对于L的导数。所以L相对于L的导数就是1。然后我们要推导出L相对于F、D、C、E、B和A的导数分别是多少。在神经网络的环境中，你基本上会非常关注这个损失函数L相对于神经网络权重的导数。当然，在这里我们只有这些变量A、B、C和F。但其中一些最终会代表神经网络的权重。因此，我们需要知道这些权重是如何影响损失函数的。

因此，我们主要关注的是输出相对于其某些叶节点的导数。这些叶节点将是神经网络的权重。当然，其他叶节点则是数据本身。

但通常情况下，我们不会需要或使用损失函数对数据的导数，因为数据是固定的。但权重会利用梯度信息进行迭代更新。因此接下来，我们将在值类内部创建一个变量，用于保存损失函数L对该值的导数。

我们将这个变量称为grad。它包含.data和self.grad两部分。初始时，它的值为0。记住，0基本上表示没有影响。因此，在初始化时，我们假设每个值都不会影响输出。

因为如果梯度为零，就意味着改变这个变量不会影响损失函数。所以默认情况下，我们假设梯度为零。现在我们有了梯度值，它是0.0，稍后我们就能在这里的数据后面看到它。这里，梯度是0.4f。这个值会出现在.grad中。现在我们将同时展示数据和梯度值。

初始化为0。我们即将开始计算反向传播。当然，正如我提到的，这个梯度grad再次代表了输出（在这里是L）相对于该值的导数。因此，这是L相对于f的导数，相对于b的导数，依此类推。

那么现在让我们来填充这些梯度并手动进行反向传播。正如我在这里提到的，让我们从末端开始逐步填充这些梯度。首先，我们关注的是填充这里的这个梯度。

那么，L关于L的导数是什么呢？换句话说，如果我将L改变一个微小的量h，L会变化多少？它会变化h。因此，它是成比例的，所以导数将是1。当然，我们可以像之前看到的那样，通过数值方法来测量或估计这些数值梯度。所以，如果我采用这个表达式，在这里创建一个名为lol的函数，并将其放在这里。我在这里创建一个名为lol的封装函数的原因是，我不想污染或搞乱这里的全局作用域。

这就像是一个小小的暂存区。你知道的，在Python中，所有这些都会是这个函数的局部变量。所以我在这里并没有改变任何全局作用域。

在这里，L1就是L。然后复制粘贴这个表达式，我们要在A上加上一个很小的量h。这将测量L对A的导数。因此，这里就是L2。接着我们想打印这个导数。打印L2减去L1，也就是L的变化量，然后用h归一化。这就是上升量除以变化量。

我们必须小心，因为L是一个值节点。实际上我们需要它的数据，这样这些数据就是浮点数，除以h。这应该打印出L关于A的导数，因为A是我们通过h稍微改变的那个变量。那么L关于A的导数是多少呢？是6。显然，如果我们用h改变L，那么实际上这里看起来会非常别扭，但用h改变L，你会看到这里的导数是1。这有点像我们在这里做的基本情况。所以基本上，我们可以到这里来，手动将L.grad设置为1。这就是我们手动进行的反向传播。

L的梯度为1，我们重新绘制一下。可以看到我们已将L的梯度设为1。现在我们将继续反向传播。接下来看看L对D和F的导数。先看D的导数。

所以，我们感兴趣的是，如果我在这个平台上创建一个Markdown文档，我们基本上想知道的是，既然L等于D乘以F，那么DL对DD的导数是多少？这是什么意思呢？如果你熟悉微积分，L等于D乘以F，那么DL对DD的导数就是F。如果你不相信我的话，我们也可以直接推导出来，因为证明过程相当直接。我们回到导数的定义，即当H趋近于0时，F(X+H)减去F(X)除以H的极限。

因此，当我们有 L = D × F 时，将 D 增加 H 会得到输出 (D + H) × F。这基本上就是 F(X + H)，对吧？再减去 D × F。然后除以 H。从符号上看，展开后我们基本上会得到 (D × F + H × F - D × F) ÷ H。然后你会看到 DF 减 DF 相互抵消。所以剩下的是 H × F ÷ H，也就是 F。因此，在导数定义的极限中，当 H 趋近于 0 时，对于 D × F 的情况，我们只得到 F。对称地，∂L/∂F 就是 D。所以我们得到的是 F · grad 现在就是 D 的值，也就是 4。而我们看到 D · grad 就是 F 的值。F 的值是 -2。所以我们会手动设置这些值。让我擦除这个标记节点。

然后让我们重新绘制一下已有的内容。好的。让我们确认一下这些是否正确。

所以我们似乎认为DL对DD的导数是负2。让我们再检查一下。让我擦掉之前这个加H的部分。现在我们需要对F求导。那么我们就回到这里创建F的地方。在这里加一个H。

这将打印出L关于F的导数。所以我们预期会看到4。是的，由于浮点运算的特性，这里显示为4。然后DL对DD的导数应该是F，也就是负2。梯度是负2。所以如果我们再次来到这里，改变D，D点数据在这里加上H。因此我们预期，我们增加了一个小H。然后我们观察L是如何变化的。

我们预计会打印出负2。好了，我们已经在数值上验证了。我们在这里所做的有点像一种内联梯度检查。

梯度检查是指我们在进行反向传播时，计算所有中间结果的导数。而数值梯度则是通过小步长来估计导数。现在我们触及了反向传播的核心。

因此，这是最关键的一个节点。因为如果你理解了这个节点的梯度，你基本上就理解了所有的反向传播和神经网络的训练过程。所以我们需要推导出DL对DC的导数。

换句话说，就是L对C的导数。因为我们已经计算了所有其他梯度。现在我们来到这里，继续手动进行反向传播。所以我们想要DL对DC的导数。

然后我们也会通过DE推导出DL。现在的问题是，我们如何通过DC推导出DL？实际上，我们知道L关于D的导数。也就是说，我们知道L对D的敏感度。但L对C的敏感度如何呢？所以如果我们调整C，这会如何通过D影响L？因此，我们知道DL由DC决定。

而且我们在这里也知道C如何影响D。所以非常直观地，如果你知道C对D的影响，以及D对L的影响，那么你应该能够以某种方式将这些信息结合起来，弄清楚C如何影响L。事实上，这正是我们实际上可以做到的。具体来说，我们首先专注于D。让我们看看D对C的导数基本上是什么？换句话说，DD除以DC是多少？这里我们知道D等于C乘以C加E。这就是我们所知道的。

现在我们关注的是DC对DD的求导。如果你还记得微积分的基础知识，就会记得对C加E关于C求导的结果是1.0。我们也可以回归基本原理来推导这个结果。因为我们可以使用导数的定义，即当h趋近于零时，[f(x+h) - f(x)] / h的极限值。

因此，在这里，我们关注C及其对D的影响，基本上可以得出f(x + h)的表达式：C将增加h加上E。这是我们函数的首次评估，减去C加E，然后除以h。那么这代表什么呢？展开来看，就是C加h加E减去C减E，再除以h。可以看到，C减C相互抵消，E减E也相互抵消。最后剩下h除以h，结果为1.0。同理，由于对称性，DD对DE的导数也将是1.0。

简单来说，求和表达式的导数非常简单。这就是局部导数。我称之为局部导数，因为最终输出值位于这个图的末端。

现在我们就像这里的一个小节点。而这个是一个小小的加号节点。这个小加号节点对它所在的整个图一无所知。

它只知道它做了一次加法。它取了一个C和一个E，将它们相加，得到了D。这个加法节点还知道C对D的局部影响，或者说D对C的导数。它也知道D对E的导数。但这并不是我们想要的。那只是局部导数。

我们真正想要的是DC的DL。而L离这里只有一步之遥。但在一般情况下，这个小小的加节点可能嵌入在一个庞大的图中。

因此，我们再次了解到L如何影响D。现在我们也明白了C和E如何影响D。那么，如何将这些信息整合起来，用DC来表达DL呢？答案当然就是微积分中的链式法则。于是我从维基百科上找来了链式法则的说明，接下来我会非常简要地讲解一下。

所以链式法则，维基百科有时候会让人很困惑，微积分也会让人很困惑。就像，这是我学习链式法则的方式，当时真的很困惑。比如，到底发生了什么？就是很复杂。

所以我更喜欢这个表达方式。如果变量Z依赖于变量Y，而Y本身又依赖于变量X，那么显然Z也通过中间变量Y依赖于X。在这种情况下，链式法则可以表示为：如果你想求DZ对DX的导数，那就先求DZ对DY的导数，再乘以DY对DX的导数。因此，链式法则本质上是在告诉我们如何正确地将这些导数串联起来。

因此，要通过函数组合进行微分，我们必须对这些导数进行乘法运算。这就是链式法则真正告诉我们的内容。这里还有一个很好的直观解释，我觉得也挺有趣的。

链式法则指出，只要知道Z相对于Y的瞬时变化率以及Y相对于X的瞬时变化率，就能通过将这两个变化率相乘，计算出Z相对于X的瞬时变化率。举个好例子：如果汽车速度是自行车的两倍，而自行车速度又是行人步行的四倍，那么汽车的速度就是行人步行速度的2×4=8倍。

因此，这非常清楚地表明，正确的做法应该是进行乘法运算。汽车的速度是自行车的两倍，而自行车的速度又是行人的四倍。所以，汽车的速度将是行人的八倍。

因此，我们可以将这些中间变化率（如果你愿意这么称呼的话）相乘。这样就能直观地理解链式法则的合理性。所以，让我们来看看链式法则。

但在这里，对我们来说，真正重要的是有一个非常简单的公式可以推导出我们想要的东西，即DL除以DC。到目前为止，我们知道我们想要什么，也知道D对L的影响。所以我们知道DL除以DD，即L对DD的导数。我们知道那是负二。

由于我们在此进行的局部推理，现在我们已经通过DC知道了DD。那么C是如何影响D的呢？具体来说，这是一个加法节点。因此局部导数就是简单的1.0，非常简单。

因此，链式法则告诉我们，通过这个中间变量计算DL对DC的导数，就等于DL对DD乘以DD对DC。这就是链式法则。所以这里的情况完全一样，只不过Z对应我们的L，Y对应D，X对应C。所以我们实际上只需要将它们相乘即可。

由于这些局部导数，比如DC对DD的导数，只是1，我们基本上可以直接复制DL对DD的导数，因为这相当于乘以1。所以既然DL对DD的导数是-2，那么DL对DC的导数是多少呢？其实就是局部梯度1.0乘以DL对DD的导数，也就是-2。因此，从本质上讲，加法节点的作用可以这样理解：它只是简单地传递梯度，因为加法节点的局部导数就是1。

在链式法则中，1乘以DL对DD的导数就是DL对DD的导数。因此，在这种情况下，这个导数会同时传递给C和E。所以基本上，我们有E点的梯度，或者让我们从C开始，因为这是我们一直在看的，是负二乘以一，负二。

同样地，根据对称性，E点grad将为负二。这就是我们的主张。因此，我们可以这样设定。

我们可以重新绘制。你看我们刚刚把负二、负二赋值了吗？所以这个反向传播的信号，它携带的信息就像是L对所有中间节点的导数是什么？我们可以想象它几乎像是沿着图反向流动，而一个加法节点会简单地将导数分配给所有叶节点，抱歉，是分配给它的所有子节点。这就是我们的主张。

现在让我们来验证一下。首先，我把前面的加号H去掉。然后，我们想要做的是递增C。因此，C点数据将增加H。当我运行这段代码时，我们预期会看到负二、负二。

然后，当然，对于E来说，E点数据加上等于H，我们预计会看到负二。很简单。所以这些就是这些内部节点的导数。

现在我们要再次递归回溯。我们将再次应用链式法则。开始吧，这是我们第二次应用链式法则。

我们将这一过程贯穿整个图进行计算。恰好现在只剩下一个节点需要处理。正如我们刚才计算的那样，dL对dE的偏导数为负二。

所以我们知道这一点。所以我们知道L对E的导数。现在我们想要dL对dA的导数，对吧？链式法则告诉我们，那就是dL对dE的导数，也就是负二，乘以局部梯度。那么局部梯度是什么呢？其实就是dE对dA的导数。

我们得看看这个。所以我只是庞大图中的一个微小时间节点。我只知道我做了A乘以B，然后得到了E。那么现在dE/dA和dE/dB是什么呢？这就是我所知道的全部了。

这是我的局部梯度。因为E等于A乘以B，我们要求的是dE对dA的导数？当然，我们刚刚在这里已经做过了。我们有一个乘法，所以我不打算重新推导它。

但如果你想对A求导，结果就是B，对吧？B的值在这里是-3.0。所以我们基本上得到了dL/dA。让我在这里直接写出来。我们有A点grad，这里我们应用链式法则，就是dL/dE（这里看到是-2）乘以dE/dA是多少？就是B的值，也就是-3。就是这样。

然后我们有B点的梯度还是dL/dE，也就是-2，同样的方式，乘以dE/dB是多少？是A的值，也就是2.0。这就是A的值。所以这些就是我们声称的导数。让我们重新画一下。我们在这里看到A点的梯度结果是6，因为那是-2乘以-3。而B点的梯度是-4乘以，抱歉，是-2乘以2，也就是-4。所以这些就是我们的主张。

我们把这个删掉，然后验证一下。这里有一个A，A.data += H。所以A.grad应该是6，我们来验证一下。确实是6。然后我们有一个B.data += H。给B加上H，看看会发生什么，我们认为是-4。确实是-4，加减号有点奇怪，又是浮点数的精度问题。

就这样。这就是从这到所有叶节点的反向传播全过程。我们一步步完成了它。

实际上，我们所做的只是——正如你所见——逐个遍历所有节点，并局部应用链式法则。我们始终知道L相对于这个小输出的导数是多少。然后我们看看这个输出是如何产生的。

此输出是通过某种操作产生的。我们拥有指向该操作子节点的指针。因此，在这个小操作中，我们知道局部导数是什么。

我们总是将它们乘以导数。因此，我们只需递归地将局部导数相乘。这就是反向传播的本质。

这只是通过计算图反向递归应用链式法则。让我们简单看看这种力量的实际效果。我们要做的是微调输入，试图让L上升。

具体来说，我们想要获取a.data并对其进行调整。如果我们希望L值上升，就意味着需要沿着梯度的方向前进。因此，a应该沿着梯度方向小幅递增。

这是步长。我们不仅希望对b进行调整，同样也希望对b、c和f进行调整。这些是叶节点，通常由我们掌控。如果我们沿着梯度的方向微调，预计会对L产生积极影响。因此，我们预计L会正向上升。

所以它应该变得不那么消极。应该上升到，比如说，负六左右。很难准确判断。

然后我们得重新运行前向传播。所以让我在这里操作一下。这就是前向传播。

F将保持不变。这实际上就是前向传播过程。现在如果我们打印L.data，由于我们沿着梯度方向微调了所有值和输入，我们预计L的负值会减少，即期望它会上升。

所以也许是负六左右。让我们看看会发生什么。好的，负七。

这基本上就是优化过程中的一个步骤，最终会运行起来。实际上，这个梯度给了我们一些能力，因为我们知道如何影响最终结果。这对训练神经网络将非常有用，我们很快就会看到。

现在我想再举一个手动反向传播的例子，用一个稍微复杂且实用的例子来说明。我们将对一个神经元进行反向传播。最终我们的目标是构建神经网络。

在最简单的情况下，这些就是所谓的多层感知器。这是一个两层的神经网络，它由神经元组成的隐藏层构成。

这些神经元彼此之间完全连接。从生物学角度来看，神经元是非常复杂的结构。但我们有非常简单的数学模型来描述它们。

因此，这是一个非常简单的神经元数学模型。你有几个输入x，然后这些突触上带有权重。



So the w's are weights. And then the synapse interacts with the input to this neuron multiplicatively. So what flows to the cell body of this neuron is w times x. But there's multiple inputs.

So there's many w times x's flowing to the cell body. The cell body then has also like some bias. So this is kind of like the innate sort of trigger happiness of this neuron.

So this bias can make it a bit more trigger happy or a bit less trigger happy, regardless of the input. But basically, we're taking all the w times x of all the inputs, adding the bias. And then we take it through an activation function.

And this activation function is usually some kind of a squashing function, like a sigmoid or tanh or something like that. So as an example, we're going to use the tanh in this example. NumPy has a np.tanh. So we can call it on a range.

And we can plot it. This is the tanh function. And you see that the inputs, as they come in, get squashed on the y-coordinate here.

So right at 0, we're going to get exactly 0. And then as you go more positive in the input, then you'll see that the function will only go up to 1 and then plateau out. And so if you pass in very positive inputs, we're going to cap it smoothly at 1. And on the negative side, we're going to cap it smoothly to negative 1. So that's tanh. And that's the squashing function or an activation function.

And what comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs. So let's write one out. I'm going to copy-paste because I don't want to type too much.

But OK, so here we have the inputs x1, x2. So this is a two-dimensional neuron. So two inputs are going to come in.

These are thought of as the weights of this neuron, weights w1, w2. And these weights, again, are the synaptic strengths for each input. And this is the bias of the neuron b. And now what we want to do is, according to this model, we need to multiply x1 times w1 and x2 times w2.

And then we need to add bias on top of it. And it gets a little messy here. But all we are trying to do is x1 w1 plus x2 w2 plus b. And these are multiplied here, except I'm doing it in small steps so that we actually have pointers to all these intermediate nodes.

So we have x1 w1 variable, x times x2 w2 variable. And I'm also labeling them. So n is now the cell body raw activation without the activation function for now.

And this should be enough to basically plot it. So draw dot of n gives us x1 times w1, x2 times w2 being added. Then the bias gets added on top of this.

And this n is this sum. So we're now going to take it through an activation function. And let's say we use the tanh so that we produce the output.

So what we'd like to do here is we'd like to do the output. And I'll call it o is n dot tanh. But we haven't yet written the tanh.

Now, the reason that we need to implement another tanh function here is that tanh is a hyperbolic function. And we've only so far implemented a plus and a times. And you can't make a tanh out of just pluses and times.

You also need exponentiation. So tanh is this kind of formula here. You can use either one of these.

And you see that there is exponentiation involved, which we have not implemented yet for our little value node here. So we're not going to be able to produce tanh yet. And we have to go back up and implement something like it.

Now, one option here is we could actually implement exponentiation, right? And we could return the exp of a value instead of a tanh of a value. Because if we had exp, then we have everything else that we need. Because we know how to add, and we know how to multiply.

So we'd be able to create tanh if we knew how to exp. But for the purposes of this example, I specifically wanted to show you that we don't necessarily need to have the most atomic pieces in this value object. We can actually create functions at arbitrary points of abstraction.

They can be complicated functions, but they can be also very, very simple functions like a plus. And it's totally up to us. The only thing that matters is that we know how to differentiate through any one function.

So we take some input.


