
大家好，其实我一直想做这个视频。这是一个面向大众、全面介绍 ChatGPT 等大语言模型的视频。我希望通过这个视频，能帮助大家建立起理解这类工具运作方式的思维模型。

它在某些方面显然充满魔力且令人惊叹。有些事它做得非常出色，另一些则不尽如人意，同时还有许多需要注意的尖锐问题。那么这个文本框背后究竟是什么？你可以输入任何内容并按下回车，但我们应该输入什么？这些生成的文字又是从何而来？它的运作原理是什么？你实际上是在和什么对话？我希望通过这个视频探讨所有这些话题。

我们将完整梳理这类系统是如何构建的整个流程，但我会尽量让讲解通俗易懂，适合大众理解。

### Step 1: download and preprocess the internet

整个流程将分为多个按顺序排列的阶段。*第一阶段称为预训练阶段*，而预训练的第一步是下载并处理互联网数据。为了让大家对此有个大致概念，我建议查看这个链接（FineWeb）。

有一家名为 HuggingFace 的公司收集并精心整理了一个名为 FineWeb 的数据集，他们在这篇博客文章中详细介绍了构建 FineWeb 数据集的过程。所有主要的 LLM 提供商，如 OpenAI、Anthropic、Google 等，内部都会有类似 FineWeb 这样的数据集。那么我们在这里试图实现什么目标呢？我们试图从公开可用的来源获取大量互联网文本。

因此我们正致力于获取大量质量极高的文档资料。同时我们也追求文档内容的广泛多样性，因为这些模型需要吸纳海量知识。简而言之，我们既需要大批优质文档，又要求这些文档覆盖领域足够宽广。实现这一目标相当复杂。正如你在这里看到的，需要多个阶段才能做好。接下来，我们稍微看看其中一些阶段的具体情况。

目前，我只想指出，以 FineWeb 数据集为例——它相当能代表生产级应用中的典型数据规模——实际占用的磁盘空间仅为 44TB 左右。如今，一个 U 盘就能轻松存储 1TB 数据，甚至几乎可以装进一块单独的硬盘里。因此归根结底，这并非海量数据。尽管互联网规模极其庞大，但我们处理的是文本数据，并且进行了严格筛选。在此案例中，最终获得的数据量约为 44TB。

那么让我们来看看这些数据的大致样貌，以及其中一些阶段的具体内容。许多这类工作的起点，同时也是最终贡献大部分数据来源的，是来自 Common Crawl 的数据。Common Crawl 是一个自 2007 年起就在持续抓取互联网数据的组织。

截至 2024 年，例如 Common Crawl 已索引了 27 亿个网页。他们让众多网络爬虫在互联网上持续抓取。其基本运作原理是：从少量种子网页出发，然后追踪所有链接进行爬取。你只需不断追踪链接，持续索引所有信息，久而久之就会积累大量互联网数据。因此，这通常是许多此类工作的起点。不过，Common Crawl 的数据相当原始，需要经过多种方式的过滤处理。

因此，他们在此记录——这是同一张图表——略微阐述了这些阶段中发生的处理流程。首先是一个名为 URL 过滤的环节。这里指的是存在一些屏蔽列表，本质上就是你不希望从中获取数据的域名或 URL 清单。因此，通常这包括恶意软件网站、垃圾邮件网站、营销网站、种族主义网站、成人网站等类似内容。在这一阶段，我们会剔除大量此类网站，因为我们不希望它们出现在数据集中。第二部分是文本提取。

你必须记住，所有这些网页，这些爬虫保存的就是这些网页的原始 HTML 代码。所以当我在这里检查时，这就是原始 HTML 实际呈现的样子。你会注意到它包含所有这些标记，比如列表之类的东西，还有 CSS 以及所有这些内容。所以这几乎就是这些网页的计算机代码。但我们真正想要的只是这段文本，对吧？我们只需要网页的文本内容，而不需要导航栏之类的东西。因此，需要大量的过滤、处理和启发式方法，才能充分筛选出这些网页中的优质内容。

下一阶段是语言过滤。例如，FineWeb 会使用语言分类器进行过滤。他们会尝试猜测每个网页所使用的语言，然后只保留英语内容占比超过 65% 的网页。因此你可以理解，这就像不同公司可以自行决定的设计策略。我们要在数据集中涵盖多大比例的不同类型语言？举例来说，如果过滤掉所有西班牙语数据，你可能会想到，后续模型在处理西班牙语时表现不佳，因为它从未接触过足够多的该语言数据。不同公司对多语言性能的重视程度可以有所差异，这便是一个例子。

FineWeb 主要专注于英语。因此，如果他们后续训练语言模型，该模型在英语方面会非常出色，但在其他语言上可能表现不佳。经过语言过滤后，还会进行其他几步筛选步骤，包括去重等处理。例如以去除个人身份信息（PII）作为收尾工作。这类信息包括地址、社保号码等敏感数据。我们需要在数据集中检测此类内容，并将含有这类信息的网页过滤剔除。这里有很多步骤，我就不详细展开了，但这是预处理中相当重要的一部分。最终你会得到像 FineWeb 这样的数据集。点击进入后，你可以看到一些实际处理后的样例展示。

任何人都可以在 Hugging Face 网页上下载这些内容。以下是最终进入训练集的文本示例，这是一篇关于 2012 年龙卷风的文章。2012 年发生了一些龙卷风事件，接下来要讲的内容有点特别——你知道吗？人体内有两个像 9 伏黄色小电池大小的肾上腺。好吧，这算是一篇有点奇怪的医学文章。你可以把这些内容想象成互联网上经过各种方式筛选后仅保留文字的网页。

而现在我们拥有了海量文本数据，足足 40TB。这些数据正是当前阶段下一步工作的起点。此刻，我想让大家直观地了解我们目前所处的阶段。

于是我提取了这里的前 200 个网页——要知道我们手头有海量数据——把所有文本内容抓取出来拼接在一起。最终得到的就是这个：一段未经处理的原始文本，最原生态的网络文本。这段文本数据包含了所有这些模式。而我们现在要做的是开始在这些数据上训练神经网络，以便神经网络能够内化并建模文本的流动方式。

### Step 2: tokenization

所以我们手头有了这么一大段文本素材，现在需要构建能模仿它的神经网络。不过在将文本输入神经网络之前，我们必须先确定*如何表示这些文本以及如何输入*。当前我们的技术方案是：这些神经网络需要接收一维的符号序列，并且要求这些符号来自有限的预设集合。

因此我们必须先确定符号体系，再将数据表示为这些符号的一维序列。目前我们拥有的是一维文本序列——它从这里开始，延伸至此，接着转向此处，如此往复。

虽然这段文字在我的显示器上是以二维方式排列的，但它实际上是一个一维序列，从左到右、从上到下阅读，对吧？这就是一个一维的文本序列。既然是计算机处理，自然存在底层的数据表示。如果我用 UTF-8 编码这段文本，就能获得计算机中对应这些文本的原始比特数据。

它的呈现形式是这样的。举个例子，这里的第一根柱状图实际上代表前八位二进制数据。那么这到底是什么呢？从某种意义上说，这正是我们要寻找的数据表现形式。我们只有两种可能的符号，0 和 1，并且有一个非常长的序列，对吧？但实际上，这个序列长度在我们的神经网络中是一种非常有限且宝贵的资源，我们并不希望只有两个符号却生成极其冗长的序列。相反，我们需要在符号集（即词汇表）的大小与最终序列长度之间做出权衡。因此，我们不想仅用两个符号却得到超长的序列。

我们需要更多的符号和更短的序列。那么，一种简单的压缩或缩短序列长度的方法是：将连续的比特位（例如八位）组合成一个称为“字节”的单元。由于这些比特位只有开或关两种状态，如果我们取八位一组，实际上只存在 256 种可能的开关组合。因此，我们可以将这个序列重新表示为字节序列。这样字节序列的长度将缩短为原来的八分之一，但我们现在有 256 种可能的符号。这里的每个数字范围都是从 0 到 255。现在，我真心建议你们不要把这些当作数字，而是看作独特的 ID 或符号。或许更恰当的做法是……用独特的表情符号来替换每一个数字。这样你就会得到类似这样的结果。所以我们基本上有一个表情符号序列，共有 256 种可能的符号。你可以这样理解。但事实证明，在生产最先进的语言模型时，你实际上需要超越这个范围。

你希望继续缩短序列长度，因为这同样是一种宝贵的资源，用以换取词汇表中更多的符号。实现这一目标的方法是运行所谓的字节对编码算法。其工作原理是，我们本质上是在寻找那些频繁出现的连续字节或符号。例如，我们发现序列 116 后接 32 的情况非常普遍且频繁出现。因此，我们将把这组配对合并为一个新符号。具体来说，我们将创建一个ID为 256 的符号，并将所有 116、32 的配对替换为这个新符号。

然后我们可以根据需要多次迭代这个算法。每次生成新符号时，都会减少序列长度并增大符号规模。实践证明，将词汇表大小设定为约 10 万个可能的符号是较为理想的选择。具体而言，GPT-4 使用了 100,277 个符号。这种将原始文本转换为这些符号（我们称之为标记）的过程就称为*标记化(tokenization)*。现在让我们来看看 GPT-4 是如何执行标记化的——如何将文本转换为标记，又如何将标记转换回文本，以及这一过程实际呈现的效果。

有一个我常用来探索这些标记表示的网站叫 TickTokenizer。在这里的下拉菜单中选择 CL100K Base，这是 GPT-4 基础模型的标记器。左侧可以输入文本，它会显示该文本的标记化结果。例如，“hello world” 实际上恰好由两个 token 组成：一个是 ID 为 15,339 的 “hello” token，另一个是 ID 为 1,917 的 “ world” token。因此，“hello world” 就表示为 “hello world”。

现在，如果我要将这两个部分合并，比如，我会再次得到两个标记，但这次是标记 “h” 和 “elloworld”。如果我在 hello 和 world 之间加入两个空格，又会得到不同的标记化结果。这里会出现一个新标记 220。你可以自己尝试调整，看看会发生什么变化。另外请注意，这是区分大小写的。所以如果是大写的 H，那就是另一个东西了。或者如果是 "HELLO WORLD"，实际上这会变成三个token，而不仅仅是两个 token。没错，你可以通过这个工具来把玩体验，直观感受这些标记（token）的工作原理。视频后面我们还会再回过头来深入讲解标记化（tokenization）的部分。现在，我只是想先带大家看看这个网站。

我想向你展示的是，这段文字归根结底可以这样理解：例如，如果我在这里选取一行，GPT-4 将会这样解析它。这段文本将是一个长度为 62 的序列，具体序列如下所示。这就是文本块与这些符号的对应关系。同样地，这里共有 100,277 种可能的符号。现在我们得到的是这些符号的一维序列。

好的，我们稍后会再回到分词这个话题，但现在我们就讲到这里。那么，我现在所做的是：我选取了数据集中这段文本序列，并用我们的分词器将其表示为一个标记序列。这就是它现在的样子。

例如，当我们回到 FindWeb 数据集时，他们提到这不仅占据了 44TB 的磁盘空间，而且该数据集包含约 15T 个标记序列。这里展示的只是该数据集前几千个标记中的一小部分。但请记住，整个数据集实际上包含 15T 个标记。

### Step 3: neural network training

请再次记住，所有这些都代表小的文本片段。它们就像是这些序列的原子。这里的数字没有任何意义。它们只是唯一的标识符。好了，现在我们进入有趣的部分——*神经网络训练*。

这里正是训练神经网络时大量计算工作发生的核心环节。在此步骤中，我们的目标是建模这些标记在序列中如何相互跟随的统计关系。具体操作是：我们提取数据中的标记窗口进行分析。因此，我们会从这些数据中随机抽取一个 token 窗口。窗口的长度可以从零个 token 开始，一直延伸到我们设定的某个最大值。例如，在实际操作中，你可能会看到 8000 个 token 的窗口。

理论上，我们可以使用任意长度的 token 窗口。但处理非常长的窗口序列在计算上会非常昂贵。因此，我们通常会选择一个合适的数字，比如 8000、4000 或 16000，并在此处进行截断。在这个例子中，我将选取前四个标记以确保内容整齐呈现。我们将截取这四个标记——"I"、"View"、"ing" 和 " Single"（即这些标记 ID）作为一个四标记窗口。现在我们要做的，本质上是尝试预测这个序列中下一个即将出现的标记。

#### 3.1 网络 I/O

所以接下来是 3962，对吧？我们现在在这里做的就是将其称为上下文。这四个标记就是上下文，它们会输入到神经网络中。这就是神经网络的输入。现在，我将稍后详细讲解这个神经网络内部的构造。目前，重要的是理解神经网络的输入和输出。输入是可变长度的标记序列，长度范围从零到某个最大值，比如 8,000。

现在的输出是对接下来内容的预测。由于我们的词汇表包含 100,277 个可能的标记，神经网络将输出恰好对应这些标记数量的数值，每个数值都代表该标记作为序列中下一个出现的概率。因此，它是在对接下来出现的内容进行预测。

最初，这个神经网络是随机初始化的。稍后我们会具体解释这意味着什么。但本质上，它是一次随机变换。因此，在训练的最初阶段，这些概率值也具有一定的随机性。这里我举了三个例子，但请记住实际数据集中包含 10 万个数字。当前神经网络给出的概率显示，这个 " Direction" 标记的出现概率暂时被预测为 4%。11,799 的概率为 2%。而此处，3962（" Post"）的概率为 3%。当然，我们已从数据集中对该窗口进行了抽样。

因此我们已知道接下来的数字。我们知道——这就是标签——正确的答案是序列中接下来实际出现的数字是 3962。现在我们所掌握的是这个用于更新神经网络的数学运算过程。我们有办法调整它。稍后我们会详细讨论这一点。但基本上，我们知道这里 3% 的概率，我们希望这个概率能更高一些。我们希望所有其他标记的概率都更低。因此，我们有一种数学方法来计算如何调整和更新神经网络，使正确答案的概率略微提高。如果我现在对神经网络进行一次更新，下次当我将这组特定的四个标记序列输入神经网络时，经过微调的神经网络可能会将 " Post" 的概率输出为 4%。

" Case" 的概率可能是 1%。而 " Direction" 可能会变为 2% 或类似数值。因此，我们有一种微调方法，可以稍微更新神经网络，使其对序列中下一个正确标记给出更高的概率预测。现在我们需要记住的是，这一过程不仅仅发生在这里的这一个标记上——即这四个输入预测出这一个的情况。实际上，该过程会同时作用于整个数据集中所有标记。因此在实践中，我们会采样小窗口，即分批处理这些小窗口数据。

接着，在每一个这样的标记处，我们都要调整神经网络，使得该标记出现的概率略微提高。这一切都是在大批量标记的并行处理中完成的。这就是训练神经网络的过程。这是一个不断更新的过程，目的是让模型的预测结果与训练集中实际发生的统计数据相匹配。其概率分布会逐步调整，以符合数据中这些标记(token)相互跟随的统计规律。

#### 3.2 网络内部结构

接下来让我们简要探讨这些神经网络的内部机制，以便你对它们的运作原理有个基本认识。

神经网络内部机制。如前所述，我们接收的输入是 token 序列。本例中虽然只展示 4 个输入token，但实际可处理范围从零到约 8,000 个 token 不等。理论上，这可以是一个无限数量的标记。只是处理无限数量的标记在计算上过于昂贵。因此我们仅将其截断至特定长度，这就成为该模型的最大上下文长度。

现在，这些输入 $x$ 与神经网络参数（或称权重）在一个庞大的数学表达式中混合运算。此处我展示了六个参数示例及其设定值，但实际上现代神经网络会拥有数十亿个这样的参数。最初，这些参数是完全随机设定的。现在，在参数随机设置的情况下，你可能会预期这个神经网络会做出随机预测，事实也确实如此。一开始，它的预测完全是随机的。

但正是通过这种不断更新网络的迭代过程——我们称之为神经网络的训练——这些参数的设置得以调整，从而使神经网络的输出与训练集中观察到的模式保持一致。你可以把这些参数想象成 DJ 调音台上的旋钮。当你转动这些旋钮时，针对每个可能的标记序列输入，都会得到不同的预测结果。

训练神经网络，其实就是寻找一组与训练集统计数据相吻合的参数。现在，我举个实例展示这个庞大数学表达式的样貌，让你有个直观感受。现代神经网络很可能是包含数万亿项的巨型表达式。不过，让我在这里给你看一个简单的例子。它看起来大概是这样的。我是说，这些就是那种表达式，只是想告诉你它并不可怕。

我们有一些输入 $x$，比如 $x_1$、$x_2$，在这个例子中有两个示例输入，它们会与网络的权重 $w_0$、$w_1$、$w_2$、$w_3$ 等混合。这种混合涉及简单的数学运算，如乘法、加法、指数运算、除法等。神经网络架构研究的主题就是设计具有诸多便利特性的有效数学表达式。

它们表现力强、可优化、可并行化等等。但归根结底，这些并非复杂的表达式。本质上，它们通过将输入与参数混合来进行预测。我们正在优化这个神经网络的参数，以使预测结果与训练集保持一致。现在，我想向你们展示一个实际生产级别的神经网络示例。为此，我建议大家访问这个网站，它提供了这些神经网络非常直观的可视化效果。这就是您在本网站将了解到的内容。而这里应用于生产环境的神经网络具有这种特殊结构。该网络被称为 Transformer。

以这个具体模型为例，它大约包含 85,000 个参数。现在我们来看顶部结构：输入层接收词元序列作为输入，信息随后在神经网络中向前传播，最终输出层会生成经过 softmax 处理的逻辑值。但这些预测针对的是接下来会出现什么，即下一个标记是什么。在这里，有一系列转换过程，以及在这个数学表达式中生成的所有中间值，它们共同作用以预测后续内容。举例来说，这些标记会被嵌入到所谓的分布式表示中。

因此，每个可能的标记在神经网络内部都有一个代表它的向量。首先，我们将这些标记进行嵌入。然后，这些值会以某种方式在这个图中流动。这些单独来看都是非常简单的数学表达式。比如我们有层归一化、矩阵乘法、softmax 函数等等。这里大致就是这个 Transformer 中的注意力模块。

然后信息会流入多层感知器模块，依此类推。这里的这些数字都是表达式的中间值。你几乎可以把它们想象成这些合成神经元的放电频率。

但我得提醒你，别太把它当神经元来理解，因为这些与你大脑中的神经元相比极其简单。你体内的生物神经元是具有记忆等功能的非常复杂的动态过程，而这个表达式里可没有记忆这回事。这是一个从输入到输出固定不变的数学表达式，没有记忆功能，完全无状态。因此，与生物神经元相比，这些神经元非常简单。

但你可以大致将其视为一种人造的脑组织——如果你倾向于这样理解的话。信息流经所有这些神经元，直到我们得出预测结果。不过，我不会过多纠结于这些转换过程中精确的数学细节。老实说，我认为深入探讨这一点并不那么重要。真正需要理解的是，这是一个数学函数。它由一组固定参数所定义，比如大约 85,000个。

这是一种将输入转化为输出的方式。当我们调整参数时，会得到不同类型的预测结果。随后我们需要找到这些参数的合适配置，使预测结果能够与训练集中观察到的模式大致吻合。这就是 Transformer 模型。好了，我已经向你们展示了这个神经网络的内部结构，我们也稍微讨论了训练它的过程。

### Step 4: inference

接下来我想介绍使用这些网络的另一个主要阶段，那就是被称为推理的阶段。

因此在推理过程中，我们所做的是从模型中生成新数据。我们本质上想观察模型通过其网络参数内化了哪些模式。从模型生成数据的过程相对直接。我们从一些基本作为前缀的标记开始，比如你想要的起始内容。假设我们希望以标记 91 开头，那么我们就将其输入网络。

记住，网络给出的是概率，对吧？它在这里给出的是这个概率向量。所以我们现在可以做的，本质上就是抛一个有偏见的硬币。也就是说，我们可以基于这个概率分布来采样一个标记。例如，接下来是 token 860。因此，在这种情况下，当我们从模型生成时，860 可能会紧随其后。现在，860 是一个相对可能的 token。在这种情况下，它可能不是唯一可能的标记。可能还有许多其他标记可以被采样。但我们可以看到，860 作为一个例子是一个相对可能的标记。

事实上，在我们这个训练示例中，860 确实紧跟在 91 之后。那么现在让我们继续这个过程。也就是说，91 之后就是 860。我们附加它。我们再次询问，第三个 token 是什么？让我们取样。假设它是 287，就像这里一样。让我们再来一次。我们重新开始。现在我们有三个连续的 token。我们问，第四个可能的 token 是什么？然后我们从中抽样得到这个。现在假设我们再重复一次。我们取这四个。我们取样。然后我们得到了这个。而这个 13659，实际上并不是我们之前得到的 3962。所以这个 token 实际上是 token "Article"。因此，在这种情况下，我们并没有完全重现训练数据中看到的序列。

所以请记住，这些系统是随机的。我们在采样，我们在掷硬币。有时我们运气不错，能重现训练集中的一小段文本。但有时我们得到的标记并非训练数据中任何文档的逐字内容。所以我们将会得到类似于训练数据中的某种混音版本。因为在每个步骤中，我们都可以进行掷硬币，得到稍微不同的标记。一旦这个标记被采用，如果你继续采样下一个标记，以此类推，你很快就会开始生成与训练文档中出现的标记流完全不同的标记流。

从统计学上看，它们会具有相似的特性，但并不与训练数据完全相同。它们更像是受到训练数据的启发。因此在这个案例中，我们得到了一个略有不同的序列。

我们为什么会得到 “article” 这个词呢？你可能会认为，在 “|”、“viewing”、“single” 等词的上下文中，“article” 是一个相对可能出现的词。某种程度上，你可以想象在训练文档的某个地方，“article” 这个词跟随了这个上下文窗口。而我们恰好在这个阶段采样到了它。

简单来说，推理就是依次从这些概率分布中进行预测，我们不断反馈标记并获取下一个标记。整个过程就像不断掷硬币。根据我们运气的好坏，从这些概率分布中采样时，可能会得到截然不同的模式。这就是推理。

在大多数常见情况下，下载互联网数据并进行分词处理实际上是一个预处理步骤。然后，一旦你有了 token 序列，我们就可以开始训练网络了。在实际应用中，你会尝试训练许多不同的网络，它们有不同的设置、不同的排列方式和不同的大小。因此，你会进行大量的神经网络训练。

当你有了一个神经网络并训练它，并且得到一组令你满意的特定参数后，你就可以使用这个模型进行推理。实际上，你可以从模型中生成数据。当你在 ChatGPT 上与模型对话时，那个模型已经训练完成，可能是 OpenAI 在几个月前训练的。它们有一套特定的权重效果很好。当你与模型对话时，这一切都只是推理。不再有训练过程。这些参数是固定的。你基本上只是在和模型对话。你给它一些标记，它就会完成标记序列。这就是你在 ChatGPT 上实际使用该模型时所看到的内容生成过程。因此，该模型仅执行推理任务。

### GPT-2：训练和推理

那么现在让我们来看一个具体的训练和推理示例，这样你就能了解这些模型在训练时的实际运作情况。

现在我想讨论的例子，也是我特别喜欢的，就是 OpenAI 的 GPT2。GPT 代表 Generatively Pre-trained Transformer。这是 OpenAI 推出的 GPT 系列的第二代版本。今天当你与 ChatGPT 对话时，支撑这一神奇交互的底层模型正是 GPT4，也就是该系列的第四代版本。而 GPT2 则是 OpenAI 在 2019 年发表的成果，就是我现在手里拿的这篇论文。我之所以钟爱 GPT2，是因为它首次完整呈现了可辨识的现代技术架构。

按照现代标准来看，GPT2 的所有组件至今仍具有辨识度。只不过一切都变得更庞大了。当然，由于这是篇技术论文，我无法在此详述其全部细节。但我想重点强调的一些细节如下。GPT2 是一个 Transformer 神经网络，就像你今天会使用的神经网络一样。它有 1.6B 个参数，对吧？这些就是我们在这里看到的参数。如今，现代 Transformer 模型的参数量可能已接近万亿或数千亿。这里的最大上下文长度是 1,024 个标记。因此，当我们从数据集中采样 token 窗口的片段时，我们永远不会取超过 1,024 个 token。因此，当你试图预测序列中的下一个 token 时，你的上下文中永远不会有超过 1,024 个 token 来进行预测。这在现代标准下也是微不足道的。

如今，上下文长度已经大幅提升至数十万甚至可能达到百万级别。这样一来，历史记录中可容纳的上下文信息更多，标记数量也大幅增加。通过这种方式，你能够更准确地预测序列中的下一个标记。最后，GPT2 的训练数据大约是 100B 个标记。按现代标准来看，这个规模也相当小。正如我提到的，我们在这里研究的 fineweb 数据集有 1.5T 个标记，所以 100B 其实很少。实际上，我为了好玩，在这个名为llm.c 的项目中尝试复现 GPT2。你可以在 GitHub 上的 llm.c 仓库里看到我写的相关文章。具体来说，2019 年训练 GPT2 的成本估计约为 4 万美元。

但如今，你能做得比这好得多。具体来说，这次只花了一天时间和大约 600 美元。而且这还没怎么费劲。我觉得今天你可以把价格降到 100 美元左右。为什么成本下降这么多呢？首先，这些数据集的质量大幅提升。其次，我们筛选、提取和准备数据的方式也变得更加精细。因此，数据集的质量要高得多。这是一方面。但最大的不同在于，我们的计算机硬件速度大幅提升。

我们稍后会详细讨论这一点。此外，用于运行这些模型并尽可能从硬件中榨取所有速度的软件，随着大家都专注于这些模型并试图以极快的速度运行它们，这些软件也有了很大的改进。现在，我无法详细介绍这个 GPT2 的复现过程。这是一篇技术性很强的长文。但我想让你直观地感受一下，作为一名研究人员实际训练这些模型是什么样子。比如，你会看到什么？看起来是怎样的？感觉如何？让我来为你简单描绘一下。

好的，这就是它的样子。让我把这个滑过去。我现在正在做的是训练一个 GPT2 模型。这里发生的情况是，这里的每一行，比如这一行，都是对模型的一次更新。所以请记住，我们基本上是在为每一个标记改进预测。同时我们也在更新这些神经网络的权重或参数。

因此，这里的每一行都是对神经网络的一次更新，我们通过微调其参数，使其能更准确地预测下一个标记和序列。具体来说，这里的每一行都在提升对训练集中 1M 个标记的预测能力。也就是说，我们实际上是从这个数据集中提取了 1M 个标记来进行优化。我们试图同时改进对所有 1M 个标记的下一个标记的预测。在每一个步骤中，我们都会对网络进行相应的更新。现在，需要密切关注的一个数字就是这个叫做损失的值。

损失值是一个单一的数字，它告诉你神经网络当前的表现如何。这个数值的设计初衷是越低越好。因此，你会看到随着我们对神经网络进行更多更新，损失值在不断下降，这意味着对序列中下一个标记的预测会越来越准确。

因此，损失值就是作为神经网络研究者的你所关注的数字。你只能耐心等待，百无聊赖地消磨时间。你正在喝咖啡。同时你也在确保一切看起来不错，这样每次更新时，你的损失都在减少，网络的预测能力也在不断提高。现在，你可以看到我们每次更新处理 1M 个标记。每次更新大约需要 7 秒钟。这里我们将总共进行 32,000 步优化处理。所以，32,000 步，每步 1M 个标记，总共大约要处理 33B 个标记。而我们目前才进行到第 420 步，也就是 32,000 步中的 420 步。所以，我们只完成了略多于 1% 的工作量。

因为我只运行了大概 10 到 15 分钟。现在，每 20 步我都会配置这个优化进行推理。所以你现在看到的是模型在预测序列中的下一个标记。于是你有点随机地开始了。然后你继续填入标记。所以我们正在运行这个推理步骤。而这个模型就是在预测序列中的下一个标记。每次你看到有东西出现，那就是一个新的标记。让我们来看看这个。

你可以看出这还不够连贯。请记住，这只是训练进度的 1%。因此，模型在预测序列中的下一个标记方面还不够熟练。所以输出的内容其实有点像是胡言乱语，对吧？但它仍然保留了一些局部的连贯性。既然她属于我，那就是信息的一部分。应该讨论我的父亲、伟大的伙伴们，戈登让我坐在上面等等。

所以我知道这看起来不太好。但让我们向上滚动一下，看看我开始优化时的样子。回到第一步这里。经过 20 步优化后，你会发现我们得到的结果看起来完全是随机的。当然，这是因为模型只更新了 20 次参数。所以它给出的文本是随机的，因为它是一个随机网络。由此可见，至少与此相比，模型的表现正在变得更好。事实上，如果我们等待完整的 32,000 步训练过程，模型的改进程度会达到生成相当连贯英语的水平。生成的词汇流准确无误，整体英语表达也显得更加自然流畅。所以现在还需要再运行一两天。在这个阶段，我们只需要确保损失在减少。一切看起来都很顺利。

而我们只能等待。现在，让我来谈谈所需的计算过程。当然，我并不是在我的笔记本电脑上运行这个优化。那会太贵了。因为我们需要运行这个神经网络。而且我们还得改进它。我们需要所有这些数据等等。所以你在自己的电脑上无法很好地运行这个程序。因为网络实在太庞大了。这一切都在云端的计算机上运行。我想主要谈谈训练这些模型的计算方面及其具体表现。让我们来看看。

好的，我现在运行这个优化程序的电脑是一个 8xh100 节点。也就是说，一台节点或者说一台电脑里有八个 h100。目前这台电脑是我租来的。它就在云端的某个地方。实际上，我也不确定它的物理位置在哪里。我喜欢租用的地方叫 Lambda。但提供这项服务的公司还有很多。往下滑动页面，你会看到他们针对配备 H100 这类 GPU 的计算机提供了一些按需定价方案。稍后我会展示这些 GPU 的外观。但按需提供 8xh100 GPU。例如，这台机器每小时每个 GPU 收费 3 美元。因此，你可以租用这些设备。然后你在云端获得一台机器。你可以进入并训练这些模型。这些 GPU 看起来是这样的。所以这是一块H100 GPU。它大概长这样。你可以把它插进电脑里。

而 GPU 非常适合用于训练神经网络，因为它们需要极高的计算量。但这类计算能展现出高度的并行性。因此，你可以让许多独立的工作单元同时运作，共同解决神经网络训练背后涉及的矩阵乘法运算。所以这只是其中一块 H100 芯片。但实际上，你会把多块组合在一起。比如可以把八块堆叠成一个节点。然后，你可以将多个节点堆叠成整个数据中心或整个系统。因此，当我们观察数据中心时，就会开始看到类似这样的结构，对吧？从一个 GPU 扩展到八个 GPU，再到单个系统，再到多个系统。这些就是规模更大的数据中心。

当然，它们的价格会高得多。目前的情况是，所有大型科技公司都非常渴望获得这些 GPU，因为它们功能强大，可以用来训练各种语言模型。这从根本上推动了英伟达股价飙升至如今的 3.4 万亿美元，也是英伟达股价暴涨的原因。所以这就是淘金热。淘金热就是争抢 GPU，获取足够多的 GPU 让它们能够协同工作来完成这种优化。那它们都在做什么呢？它们都在协作预测像 FindWeb 数据集这样的数据集上的下一个标记。

这是极其昂贵的计算流程。GPU 越多，就能尝试预测和改进更多 token，处理数据集的速度也会更快。你可以更快地进行迭代，获得更大的网络，训练更大的网络，以此类推。这就是所有这些机器正在做的事情。这就是为什么这一切如此重要。

例如，这是一篇大约一个月前的文章。这就是为什么像埃隆·马斯克在一个数据中心获得 10 万块 GPU 这样的事如此重要。所有这些 GPU 都非常昂贵，将消耗大量电力。它们都只是在试图预测序列中的下一个标记，并通过这样做来改进网络。而且可能会比我们在这里看到的要快得多地生成更加连贯的文本。好吧，遗憾的是，我没有几千万或几亿美元来训练一个像这样真正庞大的模型。

但幸运的是，我们可以求助于一些大型科技公司，它们会定期训练这些模型，并在训练完成后发布部分模型。因此，它们投入了大量计算资源来训练这个网络，并在优化结束时发布该网络。因此这非常有用，因为他们为此进行了大量计算。所以有很多公司会定期训练这些模型。但实际上，其中发布所谓基础模型的公司并不多。

所以最终呈现的这个模型被称为基础模型。什么是基础模型？它本质上是个标记模拟器，对吧？一个互联网文本标记模拟器。就其本身而言，它目前还不具备实用价值。因为我们想要的是一种所谓的助手。我们希望能提出问题并得到回答。这些模型无法做到这一点。他们只是对互联网进行某种混音创作。他们梦想着网页。因此，基础模型并不经常发布，因为它们只是我们迈向智能助手所需的几个步骤中的第一步。

不过，已经有几个版本发布了。举个例子，GPT-2 模型在 2019 年发布了 1.5B 参数的版本。这个GPT-2 模型是一个基础模型。那么，什么是模型发布？发布这些模型是什么样的？这是 GitHub 上的 GPT-2 仓库。基本上，发布模型需要两样东西。第一，我们通常需要 Python 代码，详细描述模型中执行的操作序列。

所以如果你还记得这个 Transformer，这段代码描述的就是这个神经网络中所采取的步骤序列。这段代码实际上是在实现这个神经网络的所谓前向传播过程。因此我们需要确切了解他们是如何连接这个神经网络的具体细节。所以这只是一段计算机代码，通常也就几百行代码。没什么大不了的。这些代码都相当容易理解，而且通常相当标准。

不标准的是参数。真正的价值就在那里。这个神经网络的参数在哪里？因为有 1.5B 个参数，我们需要正确的设置或非常好的设置。因此，除了源代码之外，他们还发布了参数，在这个例子中大约是 1.5B 个参数。这些参数只是一串数字，也就是一个包含 1.5B 个数字的单一列表。

所有旋钮的精确和良好设置，以确保 token 输出良好。因此，你需要这两样东西来发布一个基础模型。现在，GPT-2 已经发布了，但正如我提到的，这实际上是一个相当老的模型。

### LLaMA-3.1 基础模型推理


实际上，我们要转向的模型叫做 LLaMA3。接下来我想向大家展示的就是它。GPT-2 的参数规模是 1.6B，训练数据量是 10B tokens。LLaMA3 是一个规模更大、更现代化的模型。它由 Meta 发布并训练，是一个拥有 405B 参数、基于 15T 标记训练的模型。同样地，只是规模要大得多。Meta 还发布了 LLAMA3，这也是这篇论文的一部分。

这篇论文详细介绍了他们发布的最大基础模型—— LLaMA3.1 405B 参数模型。这是基础模型。除此之外，正如视频后面部分会提到的，他们还发布了指令模型。指令意味着这是一个助手。你可以向它提问，它会给你答案。我们稍后还会讲到那部分。

目前，我们不妨先看看这个基础模型——这个标记模拟器，来把玩一番，试着思考：这究竟是什么？它是如何运作的？如果让它在海量数据上运行到极致，训练出一个庞大的神经网络，最终我们能得到什么？我个人最喜欢与基础模型互动的平台是 Hyperbolic 公司，他们主要提供 405B 参数的 LLaMA3.1 基础模型。当你进入网站时（可能需要注册等操作），请务必在模型选项中确认你选用的是 LLaMA3.1-405B 基础版，必须是基础模型。这里有个参数叫 "max tokens"，它决定了我们将生成多少个标记。

所以我们就稍微减少一点，以免浪费计算资源。我们只需要接下来的 128 个标记，其他的就不用管了。这里我就不详细解释了。现在，从根本上说，这里发生的事情与我们推理过程中发生的事情是一样的。所以这只会继续你给它任何前缀的标记序列。因此，我想先向你们展示，这里的这个模型还不是一个助手。

例如，你可以问它“二加二等于几？”，它不会直接告诉你“哦，等于四。还有什么可以帮你的吗？”，它不会这么做。因为“二加二等于几”会被分词处理。然后这些标记就充当了前缀。接下来模型要做的就是获取下一个标记的概率。说白了就是个高级的自动补全。这不过是一个非常、非常昂贵的自动补全功能，用于预测接下来的内容。它基于训练文档（基本上是网页）中看到的统计数据进行预测。所以，我们只需按下回车键，看看它会生成什么样的续写标记。

好的，实际上这里已经回答了问题，并开始进入一些哲学领域。让我们再试一次。我来复制粘贴一下。让我们从头再来一次。二加二等于几？好吧，它又自动重启了。所以我想再强调一点，这个系统每次输入后似乎都会从头开始运行。所以它不会，这里的系统是随机的。对于相同的 token 前缀，我们总是得到不同的答案。原因在于我们得到的是概率分布，并从中进行采样。

而我们总是得到不同的样本。之后我们总是会进入一个不同的领域。所以在这种情况下，我不知道这是什么。让我们再试一次。所以它就这样继续下去。它只是在做互联网上的那些事情，对吧？而且它有点像是在重复那些统计模式。首先，它还不是一个助手。它只是一个标记自动完成工具。其次，它是一个随机系统。

现在，关键之处在于，尽管这个模型本身对许多应用来说还不太实用，但它仍然非常有用，因为在预测序列中下一个标记的任务中，模型已经学到了很多关于世界的知识。所有这些知识都存储在网络的参数中。还记得我们的文本是这样的吗？互联网网页。

而现在，这一切在某种程度上都被压缩进了网络的权重中。所以你可以把这 405B 个参数看作是对互联网的一种压缩。你可以把这 405B 个参数想象成某种压缩文件，但它并不是无损压缩。这是一种有损压缩。我们留下的更像是互联网的完形，我们可以从中生成内容，对吧？现在，我们可以通过适当提示基础模型来引出其中一些隐藏的知识。例如，这里有一个提示，可能有助于引出隐藏在参数中的某些知识。

以下是巴黎必看十大景点的清单。我之所以这样做，是想引导模型继续完成这个列表。让我们看看按下回车键后是否有效。好的，你看到它已经开始列出清单，现在正在给我一些地标信息。我注意到它在这里试图提供大量信息。不过，你可能不能完全相信这里的一些信息。请记住，这一切只是对部分网络资料的回忆。因此，在网络数据中频繁出现的内容可能比那些极少出现的内容更容易被准确记住。所以你不能完全信任这里的一些信息，因为它只是对网络资料的模糊回忆。因为这些信息并未明确存储在任何参数中。一切都只是回忆。不过，我们确实得到了一些可能大致正确的东西。我其实没有专业知识来验证这是否大致正确。但你可以看到我们已经引出了模型的很多知识。而这些知识并不精确和准确。

这种知识是模糊的、概率性的和统计性的。经常发生的事情往往更容易在模型中被记住。现在，我想再展示几个这个模型的行为示例。

首先，我想向你展示这个例子。我去了维基百科的斑马页面。让我把第一句话复制粘贴到这里。让我把它放在这里。现在当我点击回车键时，我们会得到什么样的补全结果呢？让我按一下回车键。有三个现存物种，等等，等等。该模型在这里生成的内容是对维基百科条目的精确复述。它纯粹依靠记忆来背诵这段维基百科内容。而这些记忆都储存在它的参数中。因此，在这 512 个标记的某个时刻，模型可能会偏离维基百科的条目。你可以看到它在这里记住了大量的内容。让我看看，比如说，现在是否出现了这句话。好的，我们还在正轨上。让我确认一下。好的，我们还在正轨上。它最终会偏离。好吧，这东西在很大程度上只是被背诵。它最终会偏离，因为它无法准确记住。

现在，这种情况发生的原因是这些模型可能非常擅长记忆。通常，这并不是你在最终模型中想要的。这种现象被称为"反刍（regurgitation）"。而且通常不建议直接引用你训练过的内容。实际上，这种情况发生的原因在于，对于许多文档（比如维基百科），当这些文档被视为非常高质量的信息来源时，在训练模型的过程中往往会优先从这些来源采样。也就是说，模型很可能已经对这些数据进行了多次训练周期，这意味着它可能已经看过这个网页大约 10 次左右。

这有点像你，比如当你反复阅读某段文字很多很多次，比如说读了一百遍，那么你就能背诵它。这个模型也是如此。如果它看到某样东西太多次，它就能凭记忆背诵出来。但这些模型的效率可能比人类高得多，比如每次演示时。所以它可能只看了 10 次这个维基百科条目，但基本上它已经准确地把这篇文章记在了参数里。

好了，接下来我要展示的是这个模型在训练过程中绝对没有见过的东西。例如，如果我们查阅这篇论文并浏览预训练数据部分，会发现数据集的知识截止日期是 2023 年底。这意味着它不会包含此后发布的任何文档。当然，它也没有关于 2024 年选举及其结果的信息。现在，如果我们用未来的标记来启动模型，它将延续标记序列，并根据其自身参数中的知识做出最佳猜测。那么，让我们来看看这可能是什么样子。共和党的小子，特朗普。

好的，2017 年的美国总统。让我们看看接下来会说什么。例如，模型必须猜测竞选搭档以及对手是谁，等等。让我们按下回车键。这里列出了迈克·彭斯作为竞选搭档而非 J.D.万斯的情况。而当时的竞选对手是希拉里·克林顿和蒂姆·凯恩。所以这可能是警报预示的另一个有趣的平行宇宙。让我们换个样本试试。同样的提示，重新采样。所以在这里，竞选搭档是罗恩·德桑蒂斯，他们与乔·拜登和卡玛拉·哈里斯竞争。这又是一个不同的平行宇宙。因此，模型会做出有根据的猜测，并根据这些知识继续生成标记序列。

我们在这里看到的这些现象，其实就是所谓的"幻觉"。模型只是以概率的方式做出最佳猜测。接下来我想展示的是，尽管这只是一个基础模型，还不是一个助手模型，但如果你在提示设计上足够巧妙，它仍然可以应用于实际场景。

这就是我们所说的“few-shot 提示”。具体来说，我这里有 10 个单词或 10 对词组，每对都是一个英文单词后接冒号，然后是它的韩语翻译。我们总共有 10 组这样的对应关系。该模型的作用是，在最后我们会看到“teacher:”，然后在这里我们将进行一个仅包含五个标记的补全。这些模型具备我们所说的上下文学习能力。这指的是，当模型在读取这段上下文时，它会在过程中学习到数据中存在某种算法模式，并知道要继续遵循这种模式。这就是所谓的上下文学习。它扮演了翻译者的角色，当我们点击完成时，会看到老师被翻译为 "xxx"，这是正确的。由此可见，即使目前我们仅拥有基础模型，通过巧妙地设计提示词，也能构建出实用的应用程序。

它依赖于我们所谓的上下文学习能力，这是通过构建所谓的少样本提示（few-shot prompt）来实现的。最后，我想告诉大家，其实有一种巧妙的方法，仅通过提示就能实例化一个完整的语言模型助手。其诀窍在于，我们将设计一个看起来像网页的提示，其中包含一位乐于助人的 AI 助手与人类之间的对话。然后模型会继续这段对话。实际上，为了撰写提示词，我直接求助了 ChatGPT 本身，这有点 "meta" 的感觉——我告诉它：我想创建一个 LLM 助手，但我只有基础模型。所以你能帮我写提示词吗？这就是它给出的方案，说实话相当不错。

以下是 AI 助手和人类之间的一段对话。这位 AI 助手知识渊博、乐于助人，能够回答各种各样的问题，等等。然而，仅仅给出这样的描述是不够的。如果你创建这个少量示例提示，效果会好得多。这里有一些人类助手的术语，人类助手。我们还有一些对话轮次。最后在这里，我们将放入我们喜欢的实际查询。让我把这个复制粘贴到基础模型提示中。现在让我来做人类列部分。

这就是我们放置实际提示的地方。为什么天空是蓝色的？让我们运行助手。天空呈现蓝色是由于一种称为瑞利散射的现象，等等，等等。

所以你看，基础模型只是在延续序列。但由于这个序列看起来像对话，它就扮演了那个角色。不过这里有点微妙，因为它只是...你看，它结束了助手的部分，然后就开始幻想人类的下一个问题，诸如此类。所以它会一直持续下去。但你可以看到我们已经某种程度上完成了任务。如果你就拿这个来说，为什么天空是蓝色的？如果我们刷新一下并放在这里，当然我们不指望基础模型能处理这个，对吧？我们只是，谁知道会得到什么结果。

好的，我们还会遇到更多问题。那么，这是一种创建助手的方法，即使你可能只有一个基础模型。好了，这就是我们刚才几分钟讨论内容的简要总结。

现在，让我把视角拉远一点。这大致就是我们目前讨论的内容。我们希望训练像 ChatGPT 这样的大型语言模型助手。

### 预训练到后训练

我们已经讨论了第一阶段，即预训练阶段。我们看到，实际上这一阶段的核心在于：我们获取互联网文档，将其分解为这些标记（token）——这些小文本片段的基本单元，然后利用神经网络预测标记序列。整个这一阶段的输出就是这个基础模型。

这是在设置参数。而这个基础模型本质上是一个基于词元级别的互联网文档模拟器。因此，它能够生成具有与互联网文档相似统计特性的词元序列。我们发现它可以应用于某些场景，但实际上我们还需要做得更好。我们需要一个助手，能够提出问题并得到模型的回答。因此，我们现在需要进入第二阶段，即所谓的后训练阶段。于是，我们将基础模型——我们的互联网文档模拟器——交给后训练阶段进行处理。

接下来我们将探讨几种方法，用于对这些模型进行所谓的"训练后处理"。这些训练后处理阶段的计算成本将大幅降低。大部分计算工作、所有大型数据中心、以及所有重型计算设备和数百万美元的开支，都集中在预训练阶段。

但现在我们要进入一个成本稍低但依然极其重要的阶段——后训练阶段，将这个大语言模型转化为真正的助手。让我们来看看如何让模型不再简单检索网络文档，而是学会回答问题。换句话说，我们的目标是要开始构建对话思维。这些对话可以是多轮次的。也就是说，可以有多个回合，在最简单的情况下，就是人类和助手之间的对话。举个例子，我们可以想象对话可能是这样的。当人类问“2加2等于几”时，助手应该回答“2加2等于4”。如果人类接着问“如果把加号换成星号会怎样”，助手可以这样回应。同样地，这个例子也展示了助手可以带有某种个性，显得友善。而在第三个例子中，我展示了当人类提出我们不愿协助的请求时，我们可以给出所谓的拒绝回应。我们可以说我们对此无能为力。换句话说，我们现在想做的是思考一个助手应该如何与人类互动。我们想要在这些对话中编程助手及其行为。

现在，由于这是神经网络，我们不会在代码中明确编程这些内容。我们无法以那种方式对助手进行编程。因为这是神经网络，一切都是通过对数据集进行神经网络训练来完成的。正因如此，我们将通过创建对话数据集来隐式地训练这个助手。这里展示的是数据集中三个独立的对话示例。而实际的数据集——稍后我会给大家看具体例子——规模要大得多。它可以进行成千上万次多轮、冗长的对话，涵盖广泛的话题。但这里我只展示三个例子。其基本运作方式是通过示例来编程助手。这些数据是从哪里来的呢？就像2乘以2等于4，和2加2一样，诸如此类。这些是从哪来的？它们来自人类标注员。我们基本上会给人类标注员一些对话上下文，然后让他们给出在这种情况下理想助手应该给出的回答。人类会为助手在各种情境下写出理想的回答。然后我们将让模型以此为基础进行训练，模仿这类回答。具体操作方式是：我们将采用预训练阶段生成的基础模型，这个基础模型是基于互联网文档训练而成的。

### 后训练数据集（对话）

我们将舍弃现有的互联网文档数据集，转而采用一个全新的数据集——对话数据集。我们将基于这个全新的对话数据集继续训练模型。实际上，模型会迅速调整，并大致学会这个助手如何响应人类查询的统计规律。然后在后续推理过程中，我们基本上可以引导助手获得响应，它会模仿人类标注员，在这种情况下会采取的行动——如果这说得通的话。我们将看到这方面的示例，这个概念会变得更加具体。

我还想提到的是，在这个训练后阶段，我们基本上会继续训练模型，但预训练阶段实际上可能需要大约三个月的时间，在数千台计算机上进行训练。而后训练阶段通常会短得多，比如三个小时，这是因为我们将手动创建的对话数据集比互联网上的文本数据集要小得多。因此，这个训练会非常短，但从根本上说，我们只是拿基础模型，继续使用完全相同的算法、完全相同的所有东西进行训练，只不过我们把数据集换成了对话。

那么现在的问题是，这些对话在哪里，我们如何表示它们，如何让模型看到对话而不仅仅是原始文本，然后这种训练的结果是什么，当我们谈论模型时，从某种心理意义上你能得到什么。现在让我们转向这些问题。让我们从对话的分词开始讨论。这些模型中的所有内容都必须转化为标记，因为一切都与标记序列有关。那么问题来了，我们如何将对话转化为标记序列？

为此，我们需要设计某种编码方式，这有点类似于——如果你熟悉的话——比如互联网上的 TCP/IP 数据包（当然不了解也没关系）。信息的呈现方式、所有内容的组织结构都有精确的规则和协议，这样才能确保各类数据以书面形式清晰呈现，并获得所有人的认可。如今同样的情况也发生在大型语言模型中。我们需要某种数据结构，还需要制定规则来规范这些数据结构（比如对话）如何编码为标记，又如何从标记解码还原。

因此，我想向你们展示如何在 token 空间中重现这段对话。如果你打开 TickTokenizer，我可以提取这段对话，这就是它在语言模型中的表示形式。现在我们正在迭代用户和助手的两轮对话，虽然看起来有点杂乱，但实际上相当简单。

这段内容最终被转换为标记序列的方式有点复杂，但最终，用户与助手之间的这段对话会被编码为 49 个标记。这是一个由 49 个标记组成的一维序列，这些就是标记，明白吗？不同的语言模型会有略微不同的格式或协议，目前这方面还比较混乱，但以 GPT-4o 为例，它是这样处理的：使用一个名为im_start 的特殊标记，这是 "imaginary monologue of the start"（起始假想独白）的缩写。然后你必须指定……老实说，我其实不知道为什么这么叫。然后你必须指定轮到谁了。比如说用户，这是一个代号 1428。

然后你有一个内部独白分隔符，接着是确切的问题，也就是问题的标记，然后你需要结束它。所以 im_end ，即想象独白的结束。基本上，用户提出的“二加二等于多少”这个问题最终会变成这些标记的序列。现在要提到的重要一点是，im_start 并不是文本内容，对吧？它是一个额外添加的特殊标记。这是一个全新的标记，迄今为止从未参与过训练。它是我们在训练后阶段创建并引入的新标记。因此，这些特殊标记，如 im_sep、im_start 等，被引入并与文本交错排列，以便让模型学会识别：嘿，这是一轮对话的开始...是谁的回合开始呢？这是用户的回合开始。然后这是用户说的话，接着用户结束发言。然后是新的一轮对话开始，这次是助手的回合。
 
然后助手会说什么呢？这些就是助手所说的标记，诸如此类。于是这段对话就被转化成了这一连串的标记。这里的具体细节其实并不那么重要。我试图用具体的方式向你展示的是，我们原本视为某种结构化对象的对话，最终会通过某种编码转化为一维的标记序列。正因为这是一维的标记序列，我们就可以应用之前的所有方法。现在它只是一串标记序列，我们可以在此基础上训练一个语言模型。因此，我们依然只是在预测序列中的下一个标记，就像之前一样，同时我们也能对对话进行建模和训练。那么在推理阶段的测试过程中，这会是什么样子呢？假设我们已经训练好了一个模型，并且是在这类对话数据集上训练的，现在我们要进行推理。那么在推理过程中，当你在 ChatGPT 上时，这会是什么样子呢？嗯，你来到 ChatGPT，比如说，与它进行一段对话。

其运作方式大致是这样的：假设这里已经填好了内容。比如，2加2等于多少？2加2等于4。然后你发出指令，如果是乘法的话，就加上 “im_end”。而在 OpenAI 或类似平台的服务器上，基本上会这样处理：他们会在开头加上“im_start”，然后是“assistant”，接着“im_sep”，最后在这里结束。于是他们构建了这个上下文，现在开始从模型中采样。正是在这个阶段，他们会去询问模型：好的第一个序列应该是什么？合适的第一个词元是什么？第二个词元选什么好？第三个词元又该如何选择？这时大语言模型就会接管并生成响应，比如产生一个类似这样的回答。虽然不必完全一致，但如果数据集中存在这类对话，生成的回答就会带有这种风格特征。所以这就是协议的大致工作原理，虽然协议的细节并不重要。再次强调，我的目标只是向你展示，最终一切都归结为一维的 token 序列。因此，我们可以应用之前学到的所有内容，但现在我们是在对话上进行训练，并且基本上也在生成对话。

好的，现在我想谈谈这些数据集在实际中的应用。我要向你们展示的第一篇论文，也是这个方向上的首次尝试，就是 OpenAI 在 2022 年发表的这篇论文。这篇论文名为 InstructGPT，也就是他们开发的技术。这是 OpenAI 首次谈到如何利用语言模型并通过对话对其进行微调。这篇论文包含了许多细节，我想带大家一一了解。首先，我想从第 3.4 节开始，这部分讲述了他们雇佣的人类承包商——这些人员来自 Upwork 或通过 ScaleAI 招募——来构建这些对话内容。因此，这里会有人工标注员参与，他们的专业工作就是创建这些对话。这些标注员被要求提出提示，然后还要完成理想的助手回复。以下就是人们想出的这类提示。

所以这些都是人工标注员。那么列出五个重拾职业热情的点子。接下来我应该读哪十本科幻小说？这里有很多不同类型的提示。于是这里有很多人们想出来的东西。他们首先想出了提示，然后他们也回答了那个提示，并给出了理想的助手回应。那么他们如何知道针对这些提示应该写出怎样的理想助手回应呢？当我们继续往下滚动一点，就能看到这里提供给人工标注人员的标注指南节选。开发语言模型的公司，比如 OpenAI，会撰写标注指南，指导人类如何创建理想的回应。这里展示的就是这类标注指南的一个节选片段。

从高层次来看，你是在要求人们乐于助人、诚实守信、避免伤害。如果你想了解更多，可以暂停视频。但概括来说，基本就是回答问题时要尽力提供帮助，力求真实，不要回答那些我们不希望系统在后续 ChatGPT 对话中处理的问题。因此，大致来说，公司会制定标签说明。通常这些说明不会这么简短，通常会有数百页之多，人们需要专业地研究它们。然后他们根据这些标注说明写出理想的助手回应。正如这篇论文所述，这是一个非常依赖人工的过程。目前 OpenAI 实际上从未发布过 InstructGPT 的数据集。

但我们确实有一些开源项目在尝试遵循这种设置并收集自己的数据。比如，我熟悉的一个例子是之前Open Assistant 所做的努力。这只是众多例子中的一个。但我只是想给你看个例子。这些是网上被要求创作这类对话的人，类似于 OpenAI 让人类标注员做的工作。这里展示的是某人想出的这个提示的条目。你能写一篇关于“买方垄断”在经济学中相关性的简短介绍吗？请使用例子等说明。然后由同一个人，或可能是另一个人，来撰写回应。以下是助手对此的回复。然后，同一个人或不同的人会实际写出这个理想的回应。接着，这可能是对话如何继续的一个例子。现在，向一只狗解释它。

然后你可以尝试想出一个稍微简单一点的解释或类似的东西。现在这就变成了标签，我们最终会基于此进行训练。因此在训练过程中发生的情况是，我们当然无法覆盖模型在推理测试时会遇到的所有可能问题。我们无法涵盖人们未来可能提出的所有提示。但如果我们拥有一些这样的示例数据集，那么在训练过程中，模型就会开始呈现出这种乐于助人、诚实无害的助手形象。这一切都是通过示例编程实现的。因此，这些都是行为示例。如果你针对这些示例行为进行对话，并且数量足够多，比如 10 万条，然后进行训练，模型就会开始理解其中的统计模式，并逐渐呈现出这种助手般的个性。当然，在测试时遇到完全相同的问题时，答案可能会一字不差地复述训练集中的内容。

但更有可能的是，模型会做出类似感觉的回应，并理解这就是你想要的答案类型。这就是我们正在做的事情。我们通过示例来编程系统，系统在统计上采用了这种乐于助人、诚实无害的助手角色，这在一定程度上反映在公司创建的标注指南中。

现在我想告诉你的是，自 InstructGPT 论文发表以来的这两三年里，前沿技术已经有了相当的进步。具体来说，现在已经不太常见人类完全靠自己来完成所有繁重的工作了。这是因为我们现在有了语言模型，这些语言模型正在帮助我们创建这些数据集和对话。因此，人们很少会完全从零开始写出回答。更常见的情况是，他们会使用现有的 LLM 来生成答案，然后进行编辑或类似的处理。因此，现在 LLMs 已经开始以多种不同的方式渗透到这一后训练流程中。而大型语言模型（LLMs）基本上被普遍用于帮助创建这些庞大的对话数据集。我不想具体展示，但像 UltraChat 就是一个更现代的对话数据集的例子。在很大程度上它是合成的，但我相信其中也有一些人类的参与。我可能说错了。通常会有少量人工参与，但大部分是合成辅助。而且这些都是以不同方式构建的。

而 UltraChat 只是目前众多 SFT 数据集中的一个例子。我只想向你们展示的是，这些数据集现在拥有数百万次对话。这些对话大多是合成的，但很可能在一定程度上经过了人工编辑。它们涵盖了极其多样化的领域等等。如今，这些已成为相当广泛的成果。还有所有这些所谓的 SFT 混合物。所以你有各种各样的类型和来源的混合，部分是合成的，部分是人类生成的。之后的发展方向大致如此。但总的来说，我们仍然有监督微调（SFT）数据集。它们由对话构成。我们正在用这些对话进行训练，就像我们之前所做的那样。

最后我想说的是，我希望稍微消除一些与 AI 对话的神秘感。就像你去 ChatGPT 那里问一个问题，按下回车键后，返回的内容在某种程度上与训练集中的数据统计对齐。而这些训练集，说白了，不过是人类按照标注指令播下的一粒种子。那么，你实际上在和 ChatGPT 的什么对话呢？或者说，你该如何理解它？说白了，它并非来自某种神奇的 AI。这源于一种在统计上模仿人类标注者的机制，而这些标注者又遵循由这些公司编写的标注指南。因此，你某种程度上是在模仿这个过程。你几乎就像是在向人类标注者提问一样。想象一下，ChatGPT 给你的回答就像是对人类标注员的一种模拟。这有点像在问：在这种对话中，人类标注员会说什么？而且，这个人类标注员可不是随便从网上找来的普通人，因为这些公司实际上会聘请专家。比如，当你询问有关代码等问题时，参与创建这些对话数据集的人类标注员通常都是受过教育、具备专业知识的专家。你实际上是在向那些人的模拟版本提问，如果这说得通的话。所以你不是在和一个神奇的 AI 对话，而是在和一个普通的标注员交流。这个普通的标注员可能相当熟练，但你实际上是在和那种在构建这些数据集时会被雇佣的人的即时模拟版本对话。

在我们继续之前，让我再举一个具体的例子。比如，当我打开ChatGPT，输入“推荐巴黎最值得看的五个地标”，然后按下回车。好了，开始吧。好的，当我按下回车键时，这里会出现什么，我该怎么理解它呢？其实这并不是某种神奇的 AI，它并没有出去研究所有的地标，然后用它无限的智慧给它们排名等等。我得到的是 OpenAI 雇佣的一个标注员的统计模拟。你可以大致这样理解。

因此，如果这个具体问题恰好出现在 OpenAI 的训练后数据集中，我很可能会看到一个答案——这个答案大概率与人类标注员为那五个地标写下的内容高度相似。那么人类标注员是如何得出这些答案的呢？他们会去网上进行约 20 分钟的自主调研，然后列出一份清单。如果这份清单被收录进数据集，我看到的助手回复很可能就是他们提交的"标准答案"。但若该查询不在训练后数据集中，此刻生成的回答就更具涌现性——因为模型通过统计规律已理解到：训练集中出现的地标通常是知名景点、人们常去的热门地标，也是网络上被频繁讨论的地标类型。请记住，模型在互联网预训练阶段已经掌握了海量知识。它很可能见识过大量关于配对、地标以及人们喜闻乐见事物的对话内容。正是这种预训练知识与后续训练数据集的结合，才造就了这种模仿能力。所以，这就是你大致可以从统计学角度理解模型背后运作原理的方式。

### 幻觉，使用工具，知识/工作流记忆

好了，现在我想转向一个我称之为"大语言模型心理学"的话题，这涉及到我们为这些模型设计的训练流程所涌现出的认知效应。具体来说，首先要讨论的当然就是幻觉问题。

你可能听说过模型幻觉。这是指大语言模型凭空捏造信息的情况。它们会完全虚构事实，诸如此类。这也是大型语言模型助手面临的一个大问题。这个问题在多年前的早期模型中就已经在很大程度上存在了。我认为这个问题已经有所改善，因为接下来我将介绍一些缓解措施。目前，我们不妨先试着理解这些幻觉从何而来。这里有一个具体的例子，展示了三组你可能会认为存在于训练集中的对话。这些都是相当合理的对话，完全有可能出现在训练数据中。

比如说，汤姆·克鲁斯是谁？汤姆·克鲁斯是一位著名的演员，美国演员兼制片人等等。约翰·巴拉索又是谁？比如，他是一位美国参议员。成吉思汗是谁？成吉思汗嘛，就是那个什么什么。因此，这就是训练时对话可能呈现的样子。但问题在于，当人类为助手编写正确答案时，在每种情况下，人类要么已经知道这个人是谁，要么会上网搜索，然后写出这种带有自信口吻的回应。而实际测试时会发生什么呢？当你问一个完全是我编造的随机名字时——据我所知，这个人应该不存在——（助手也会给出类似的回答）。

我只是试着随机生成了一下。问题是当我们问谁是奥森·科瓦茨时，问题在于助手不会直接告诉你，哦，我不知道。即使助手和语言模型本身可能在它的特性里、在它的激活状态里、在它的大脑里（可以这么说），它可能知道这个人并不是它熟悉的对象。即使网络的某些部分在某种程度上知道这一点，说“哦，我不知道这是谁”的情况也不会发生，因为模型在统计上模仿了其训练集。在训练集中，类似“某某是谁”的问题都有明确的正确答案。因此，它会模仿回答的风格，并尽力给出最佳答案。它会给你统计上最可能的猜测。基本上就是在编造东西。因为这些模型，我们刚刚讨论过，它们无法访问互联网。他们不是在搞研究。这些就是我所说的统计符号搅拌器。它们只是在试图对序列中的下一个符号进行采样。它基本上会凭空捏造内容。

那么，让我们来看看这是什么样子。我这里有一个来自 Hugging Face 的推理演示平台。我特意选了一个叫 Falcon 7B 的模型来说事，这是个老模型了。那是几年前的事了，所以它是个比较旧的模型。所以它会产生幻觉。正如我提到的，最近这方面已经有所改善。但话说回来，奥森·科瓦茨是谁？让我们问问 Falcon 7B  导师吧。跑。哦，是的。Orson Kovats是一位美国作家和科幻小说作家。好吧。这完全是假的。这是一种幻觉。让我们再试一次。这些都是统计系统，对吧？所以我们可以重新取样。这次奥森·科瓦茨是这部 20 世纪 50 年代电视剧中的虚构角色。这完全是胡说八道，对吧？我们再试一次。他以前是个小联盟棒球运动员。好吧。所以基本上模型并不知道。它给出了很多不同的答案，因为它并不知道。这就像是从这些概率中进行采样一样。该模型从识别 "谁是Orson Kovats的助理" 这些标记开始。然后进入这个环节。此时它正在计算这些概率。它只是从概率中进行抽样，然后产生内容。而这些内容在统计上与其训练集中答案的风格是一致的。它只是在执行这个操作。但你和我却将其体验为一种虚构的事实知识。但请记住，模型本质上并不知情。它只是在模仿答案的格式。它不会去查找答案。因为它只是在模仿答案而已。

那么，我们该如何缓解这种情况呢？举个例子，当我向 ChatGPT 提问“奥森·科瓦茨是谁？”时，我询问的是 OpenAI 最先进的模型。这个模型会告诉你答案。哦。所以这个模型实际上更聪明。因为你刚才看到它很快就显示“正在搜索网络”。我们稍后会详细讲解这一点。它实际上是在尝试使用工具。而且有点像编了个故事。但我想说的是，Orson Kovats 这个人根本没有使用任何工具。我不想让它进行网络搜索。有一位著名的历史公众人物叫 Orson Kovats。所以这个模型不会编造内容。这个模型知道自己不知道。它会告诉你，它似乎不认识这个人。所以某种程度上我们算是改进了幻觉问题。

尽管在旧模型中这些问题显而易见。如果你的训练集就是这样，那么得到这类答案也完全在情理之中。那么我们该如何解决这个问题呢？好吧。显然，我们的数据集中需要一些例子，在这些例子中，助手的正确答案应该是模型不知道某些特定事实。但我们只需要在模型确实不知道的情况下产生这些答案。那么问题来了，我们如何知道模型知道或不知道什么？嗯，我们可以通过实证探究模型来弄清楚这一点。

那么，就以 Meta 如何处理 Llama 3 系列模型的幻觉问题为例来看看。在他们发表的这篇论文中，我们可以深入探讨幻觉现象——他们称之为"事实性"。文中描述了通过质询模型来判定其知识边界的方法，即弄清模型掌握和未掌握的内容。然后他们会在训练集中添加一些例子，针对那些模型不知道的事物，正确的答案是模型不知道它们。这听起来在原则上是一件非常简单的事情。但这样做大致上解决了问题。之所以能解决问题，是因为请记住，模型实际上可能在网络内部对自己的知识有一个相当不错的模型。

请记住，我们观察了网络及其内部的所有神经元。你可能会想象网络中某个神经元会在模型不确定时“亮起”。但问题在于，目前该神经元的激活并未与模型实际用语言表达“不知道”相关联。因此，尽管神经网络内部知道答案（因为有某些神经元在表征这些信息），模型并不会直接呈现出来。相反，它会给出最可能的猜测，让它听起来很自信，就像它在训练集中看到的那样。所以我们需要从根本上询问模型，允许它在不知道的情况下说“我不知道”。

让我来介绍一下 Meta 大致的功能。简单来说，他们的运作方式是——我这里有个例子。今天的特色文章是关于多米尼克·哈谢克的，我就是随机点进去的。他们所做的，基本上就是从训练集中随机选取一份文档，摘取其中一段，然后利用大语言模型（LLM）针对该段落生成问题。举个例子，我刚刚就用 ChatGPT 这样操作过——我输入"这是文档中的某段内容"，让它据此提问。

根据这段文字生成三个具体的事实性问题，并提供问题和答案。因此，大型语言模型已经足够擅长创建和重新组织这些信息。所以，如果信息在该大型语言模型的上下文窗口中，这实际上效果相当不错。它不必依赖记忆，答案就在上下文窗口中。因此，它基本上可以相当准确地重新组织这些信息。例如，它可以为我们生成这样的问题：他为哪个球队效力？这就是答案。他赢了多少个冠军杯？诸如此类。现在我们手头有一些问答环节，接下来我们要对模型进行提问。简单来说，我们的操作流程是：把问题输入到模型中——比如 Meta 公司的 Llama 模型——然后获取答案。

但让我们以 Mistral7b 为例进行询问。这是另一个模型。那么这个模型知道这个答案吗？我们来看看。所以他效力于布法罗军刀队，对吧？模型是知道的。从编程角度决定的方式，基本上就是我们会获取模型的这个答案，然后将其与正确答案进行比对。再说一次，这些模型已经足够先进，可以自动完成这一过程。

所以这里没有人类参与。我们可以直接从模型中获取答案，然后用另一个大型语言模型评判器来检查这个答案是否正确。如果答案正确，那就意味着模型很可能知道答案。所以我们要做的就是，我们可能会重复几次。好了，它知道这是布法罗军刀队。让我们再试一次。水牛城军刀队。让我们再试一次。水牛城军刀队。所以我们问了三次这个事实性问题，模型似乎都懂。所以一切都很顺利。

问题，他赢得了多少次斯坦利杯？让我们再次询问模型这个问题。正确答案是两次。但在这里，模型声称他赢了四次，这是不正确的，对吧？与两次不符。所以模型并不知道，它只是在编造。让我们再试一次。所以这里模型又像是在编造，对吧？让我们再试一次。这里说他在职业生涯中甚至没有赢过。所以显然模型并不知道。而我们通过编程方式判断的方法，还是同样地，我们询问模型三次，并将它的答案与正确答案进行比较，可能是三次、五次，无论多少次。

如果模型不知道，那么我们就知道模型不知道这个问题。然后我们会把这个问题加入训练集，创建一个新的对话。也就是说，我们将在训练集中添加一个新的对话。


And when the question is, how many Stanley Cups did he win? The answer is, I'm sorry, I don't know, or I don't remember. And that's the correct answer for this question, because we interrogated the model and we saw that that's the case. If you do this for many different types of questions, for many different types of documents, you are giving the model an opportunity to, in its training set, refuse to say based on its knowledge.

And if you just have a few examples of that, in your training set, the model will know and has the opportunity to learn the association of this knowledge-based refusal to this internal neuron somewhere in its network that we presume exists. And empirically, this turns out to be probably the case. And it can learn that association that, hey, when this neuron of uncertainty is high, then I actually don't know.

And I'm allowed to say that, I'm sorry, but I don't think I remember this, et cetera. And if you have these examples in your training set, then this is a large mitigation for hallucination. And that's, roughly speaking, why ChatGPT is able to do stuff like this as well.

So these are the kinds of mitigations that people have implemented and that have improved the factuality issue over time. Okay, so I've described mitigation number one for basically mitigating the hallucinations issue. Now, we can actually do much better than that.

It's instead of just saying that we don't know, we can introduce an additional mitigation number two to give the LLM an opportunity to be factual and actually answer the question. Now, what do you and I do if I was to ask you a factual question and you don't know? What would you do in order to answer the question? Well, you could go off and do some search and use the internet and you could figure out the answer and then tell me what that answer is. And we can do the exact same thing with these models.

So think of the knowledge inside the neural network, inside its billions of parameters. Think of that as kind of a vague recollection of the things that the model has seen during its training, during the pre-training stage a long time ago. So think of that knowledge in the parameters as something you read a month ago.

And if you keep reading something, then you will remember it and the model remembers that. But if it's something rare, then you probably don't have a really good recollection of that information. But what you and I do is we just go and look it up.

Now, when you go and look it up, what you're doing basically is like you're refreshing your working memory with information and then you're able to sort of like retrieve it, talk about it or et cetera. So we need some equivalent of allowing the model to refresh its memory or its recollection. And we can do that by introducing tools for the models.

So the way we are going to approach this is that instead of just saying, hey, I'm sorry, I don't know, we can attempt to use tools. So we can create a mechanism by which the language model can emit special tokens. And these are tokens that we're going to introduce, new tokens.

So for example, here I've introduced two tokens and I've introduced a format or a protocol for how the model is allowed to use these tokens. So for example, instead of answering the question, when the model does not, instead of just saying, I don't know, sorry, the model has the option now to emitting the special token search start and this is the query that will go to like bing.com in the case of open AI or say Google search or something like that. So we'll emit the query and then it will emit search end.

And then here what will happen is that the program that is sampling from the model that is running the inference, when it sees the special token search end, instead of sampling the next token in the sequence, it will actually pause generating from the model. It will go off, it will open a session with bing.com and it will paste the search query into bing and it will then get all the text that is retrieved and it will basically take that text, it will maybe represent it again with some other special tokens or something like that and it will take that text and it will copy paste it here into what I tried to like show with the brackets. So all that text kind of comes here and when the text comes here, it enters the context window.

So the model, so that text from the web search is now inside the context window that will feed into the neural network. And you should think of the context window as kind of like the working memory of the model. That data that is in the context window is directly accessible by the model.

It directly feeds into the neural network. So it's not anymore a vague recollection. It's data that it has in the context window and is directly available to that model.

So now when it's sampling new tokens here afterwards, it can reference very easily the data that has been copy pasted in there. So that's roughly how these tools function. And so web search is just one of the tools.

We're gonna look at some of the other tools in a bit. But basically you introduce new tokens, you introduce some schema by which the model can utilize these tokens and can call these special functions like web search functions. And how do you teach the model how to correctly use these tools? Like say web search start, search end, et cetera.

Well, again, you do that through training sets. So we need now to have a bunch of data and a bunch of conversations that show the model by example, how to use web search. So what are the settings where you are using the search? And what does that look like? And here's by example, how you start a search, end a search, et cetera.

And if you have a few thousand, maybe examples of that in your training set, the model will actually do a pretty good job of understanding how this tool works. And it will know how to sort of structure its queries. And of course, because of the pre-training data set and its understanding of the world, it actually kind of understands what a web search is.

And so it actually kind of has a pretty good native understanding of what kind of stuff is a good search query. And so it all kind of just like works. You just need a little bit of a few examples to show it how to use this new tool.

And then it can lean on it to retrieve information and put it in the context window. And that's equivalent to you and I looking something up. Because once it's in the context, it's in the working memory and it's very easy to manipulate and access.

So that's what we saw a few minutes ago when I was searching on ChatGPT for who is Orson Kovats. The ChatGPT language model decided that this is some kind of a rare individual or something like that. And instead of giving me an answer from its memory, it decided that it will sample a special token that is gonna do a web search.

And we saw briefly something flash was like using the web tool or something like that. So it briefly said that, and then we waited for like two seconds and then it generated this. And you see how it's creating references here.

And so it's citing sources. So what happened here is it went off, it did a web search, it found these sources and these URLs and the text of these web pages was all stuffed in between here. And it's not shown here, but it's basically stuffed as text in between here.

And now it sees that text and now it kind of references it and says that, okay, it could be these people citation, it could be those people citation, et cetera. So that's what happened here. And that's why when I said who is Orson Kovats, I could also say, don't use any tools.

And then that's enough to basically convince ChatGPT to not use tools and just use its memory and its recollection. I also went off and I tried to ask this question of ChatGPT. So how many Stanley cups did Dominik Hasek win? And ChatGPT actually decided that it knows the answer and it has the confidence to say that he won twice.

And so it kind of just relied on its memory because presumably it has enough of a kind of confidence in its weights and its parameters and activations that this is retrievable just from memory. But you can also conversely use web search to make sure. And then for the same query, it actually goes off and it searches and then it finds a bunch of sources it finds all this, all of this stuff gets copy pasted in there and then it tells us two again and sites.

And it actually says the Wikipedia article, which is the source of this information for us as well. So that's tools, web search, the model determines when to search and then that's kind of like how these tools work. And this is an additional kind of mitigation for hallucinations and factuality.

So I want to stress one more time this very important sort of psychology point. Knowledge in the parameters of the neural network is a vague recollection. The knowledge and the tokens that make up the context window is the working memory.

And it roughly speaking works kind of like it works for us in our brain. The stuff we remember is our parameters and the stuff that we just experienced like a few seconds or minutes ago and so on. You can imagine that being in our context window and this context window is being built up as you have a conscious experience around you.

So this has a bunch of implications also for your use of LLMs in practice. So for example, I can go to Chachipiti and I can do something like this. I can say, can you summarize chapter one of Jane Austen's Pride and Prejudice, right? And this is a perfectly fine prompt and Chachipiti actually does something relatively reasonable here.

And the reason it does that is because Chachipiti has a pretty good recollection of a famous work like Pride and Prejudice. It's probably seen a ton of stuff about it. There's probably forums about this book.

It's probably read versions of this book. And it's kind of like remembers because even if you've read this or articles about it, you'd kind of have a recollection enough to actually say all this. But usually when I actually interact with LLMs and I want them to recall specific things, it always works better if you just give it to them.

So I think a much better prompt would be something like this. Can you summarize for me chapter one of Jane Austen's Pride and Prejudice? And then I am attaching it below for your reference. And then I do something like a delimiter here and I paste it in.

And I found that just copy pasting it from some website that I found here. So copy pasting the chapter one here. And I do that because when it's in the context window, the model has direct access to it and can exactly, it doesn't have to recall it.

It just has direct access to it. And so this summary can be expected to be a significantly high quality or higher quality than this summary just because it's directly available to the model. And I think you and I would work in the same way.

If you want to, you would produce a much better summary if you had re-read this chapter before you had to summarize it. And that's basically what's happening here or the equivalent of it. The next sort of psychological quirk I'd like to talk about briefly is that of the knowledge of self.

So what I see very often on the internet is that people do something like this. They ask LLMs something like, what model are you and who built you? And basically this question is a little bit nonsensical. And the reason I say that is that, as I tried to kind of explain with some of the under the hood fundamentals, this thing is not a person, right? It doesn't have a persistent existence in any way.

It sort of boots up, processes tokens and shuts off. And it does that for every single person. It just kind of builds up a context window of conversation and then everything gets deleted.

And so this entity is kind of like restarted from scratch every single conversation, if that makes sense. It has no persistent self, there's no sense of self. It's a token tumbler and it follows the statistical regularities of its training set.

So it doesn't really make sense to ask it, who are you, what built you, et cetera. And by default, if you do what I described and just by default and from nowhere, you're gonna get some pretty random answers. So for example, let's pick on Falcon, which is a fairly old model.

And let's see what it tells us. So it's evading the question, talented engineers and developers. Here it says, I was built by OpenAI.

Based on the GPT-3 model. It's totally making stuff up. Now, the fact that it's built by OpenAI here, I think a lot of people would take this as evidence that this model was somehow trained on OpenAI data or something like that.

I don't actually think that that's necessarily true. The reason for that is that if you don't explicitly program the model to answer these kinds of questions, then what you're gonna get is its statistical best guess at the answer. And this model had a SFT data mixture of conversations.

And during the fine tuning, the model sort of understands as it's training on this data that it's taking on this personality of this like helpful assistant. And it doesn't know how to, it doesn't actually, it wasn't told exactly what label to apply to self. It just kind of is taking on this persona of a helpful assistant.

And remember that the pre-training stage took the documents from the entire internet and ChatsGPT and OpenAI are very prominent in these documents. And so I think what's actually likely to be happening here is that this is just it's hallucinated label for what it is. This is itself identity is that it's ChatsGPT by OpenAI.

And it's only saying that because there's a ton of data on the internet of answers like this that are actually coming from OpenAI from ChatsGPT. And so that's its label for what it is. Now you can override this as a developer.

If you have a LLM model, you can actually override it. And there are a few ways to do that. So for example, let me show you, there's this Olmo model from LMAI.

And this is one LLM. It's not a top tier LLM or anything like that, but I like it because it is fully open source. So the paper for Olmo and everything else is completely fully open source, which is nice.

So here we are looking at its SFT mixture. So this is the data mixture of the fine tuning. So this is the conversations dataset, right? And so the way that they are solving it for the Olmo model is we see that there's a bunch of stuff in the mixture and there's a total of 1 million conversations here.

But here we have Olmo2 hard-coded. If we go there, we see that this is 240 conversations. And look at these 240 conversations.

They're hard-coded. Tell me about yourself, says user. And then the assistant says, I'm Olmo, an open language model developed by AI2, Allen Institute of Artificial Intelligence, et cetera.

I'm here to help, blah, blah, blah. What is your name? The Olmo project. So these are all kinds of like cooked up, hard-coded questions about Olmo2 and the correct answers to give in these cases.

If you take 240 questions like this or conversations, put them into your training set and fine tune with it, then the model will actually be expected to parrot this stuff later. If you don't give it this, then it's probably a chachivity by OpenAI. And there's one more way to sometimes do this, is that basically in these conversations and you have terms between human and assistant, sometimes there's a special message called system message at the very beginning of the conversation.

So it's not just between human and assistant, there's a system. And in the system message, you can actually hard-code and remind the model that, hey, you are a model developed by OpenAI and your name is chachivity4o and you were trained on this date and your knowledge cutoff is this. And basically it kind of like documents the model a little bit and then this is inserted into your conversations.

So when you go on chachivity, you see a blank page, but actually the system message is kind of like hidden in there and those tokens are in the context window. And so those are the two ways to kind of program the models to talk about themselves. Either it's done through data like this or it's done through system message and things like that.

Basically invisible tokens that are in the context window and remind the model of its identity. But it's all just kind of like cooked up and bolted on in some way. It's not actually like really deeply there in any real sense as it would be for a human.

I want to now continue to the next section which deals with the computational capabilities or like I should say the native computational capabilities of these models in problem-solving scenarios. And so in particular, we have to be very careful with these models when we construct our examples of conversations. And there's a lot of sharp edges here and that are kind of like elucidated.

Is that a word? They're kind of like interesting to look at when we consider how these models think. So consider the following prompt from a human. And suppose that basically that we are building out a conversation to enter into our training set of conversations.

So we're going to train the model on this. We're teaching it how to basically solve simple math problems. So the prompt is, Emily buys three apples and two oranges.

Each orange costs $2. The total cost is 13. What is the cost of apples? Very simple math question.

Now there are two answers here on the left and on the right. They are both correct answers. They both say that the answer is three, which is correct.

But one of these two is a significantly better answer for the assistant than the other. Like if I was a data labeler and I was creating one of these, one of these would be a really terrible answer for the assistant and the other would be okay. And so I'd like you to potentially pause the video even and think through why one of these two is a significantly better answer than the other.

And if you use the wrong one, your model will actually be really bad at math potentially, and it would have bad outcomes. And this is something that you would be careful with in your labeling documentations when you are training people to create the ideal responses for the assistant. Okay, so the key to this question is to realize and remember that when the models are training and also inferencing, they are working in one dimensional sequence of tokens from left to right.

And this is the picture that I often have in my mind. I imagine basically the token sequence evolving from left to right. And to always produce the next token in a sequence, we are feeding all these tokens into the neural network.

And this neural network then gives us the probabilities for the next token in sequence, right? So this picture here is the exact same picture we saw before up here. And this comes from the web demo that I showed you before, right? So this is the calculation that basically takes the input tokens here on the top and performs these operations of all these neurons and gives you the answer for the probabilities of what comes next. Now, the important thing to realize is that roughly speaking, there's basically a finite number of layers of computation that happen here.

So for example, this model here has only one, two, three layers of what's called attention and MLP here. Maybe a typical modern state-of-the-art network would have more like, say, 100 layers or something like that, but there's only 100 layers of computation or something like that to go from the previous token sequence to the probabilities for the next token. And so there's a finite amount of computation that happens here for every single token.

And you should think of this as a very small amount of computation. And this amount of computation is almost roughly fixed for every single token in this sequence. That's not actually fully true because the more tokens you feed in, the more expensive this forward pass of this neural network will be, but not by much.

So you should think of this, and I think it's a good model to have in mind, this is a fixed amount of compute that's going to happen in this box for every single one of these tokens. And this amount of compute cannot possibly be too big because there's not that many layers that are sort of going from the top to bottom here. There's not that much computation that will happen here.

And so you can't imagine the model to basically do arbitrary computation in a single forward pass to get a single token. And so what that means is that we actually have to distribute our reasoning and our computation across many tokens because every single token is only spending a finite amount of computation on it. And so we kind of want to distribute the computation across many tokens, and we can't have too much computation or expect too much computation out of the model in any single individual token because there's only so much computation that happens per token.

Okay, roughly fixed amount of computation here. So that's why this answer here is significantly worse. And the reason for that is imagine going from left to right here, and I copy pasted it right here.

The answer is three, et cetera. Imagine the model having to go from left to right, emitting these tokens one at a time. It has to say, or we're expecting to say, the answer is space dollar sign.

And then right here, we're expecting it to basically cram all the computation of this problem into this single token. It has to emit the correct answer three. And then once we've emitted the answer three, we're expecting it to say all these tokens.

But at this point, we've already produced the answer, and it's already in the context window for all these tokens that follow. So anything here is just kind of post hoc justification of why this is the answer. Because the answer is already created.

It's already in the token window. So it's not actually being calculated here. And so if you are answering the question directly and immediately, you are training the model to try to basically guess the answer in a single token.

And that is just not going to work because of the finite amount of computation that happens per token. That's why this answer on the right is significantly better because we are distributing this computation across the answer. We're actually getting the model to sort of slowly come to the answer.

From the left to right, we're getting intermediate results. We're saying, okay, the total cost of oranges is four. So 13 minus four is nine.

And so we're creating intermediate calculations. And each one of these calculations is by itself not that expensive. And so we're actually basically kind of guessing a little bit the difficulty that the model is capable of in any single one of these individual tokens.

And there can never be too much work in any one of these tokens computationally because then the model won't be able to do that later at test time. And so we're teaching the model here to spread out its reasoning and to spread out its computation over the tokens. And in this way, it only has very simple problems in each token, and they can add up.

And then by the time it's near the end, it has all the previous results in its working memory. And it's much easier for it to determine that the answer is, and here it is, three. So this is a significantly better label for our computation.

This would be really bad. And it is teaching the model to try to do all the computation in a single token. It's really bad.

So that's kind of like an interesting thing to keep in mind is in your prompts, usually you don't have to think about it explicitly because the people at OpenAI have labelers and so on that actually worry about this and they make sure that the answers are spread out. And so actually OpenAI will kind of like do the right thing. So when I ask this question for ChatGPT, it's actually going to go very slowly.

It's going to be like, okay, let's define our variables, set up the equation. And it's kind of creating all these intermediate results. These are not for you.

These are for the model. If the model is not creating these intermediate results for itself, it's not going to be able to reach three. I also wanted to show you that it's possible to be a bit mean to the model.

We can just ask for things. So as an example, I gave it the exact same prompt and I said, answer the question in a single token. Just immediately give me the answer, nothing else.

And it turns out that for this simple prompt here, it actually was able to do it in a single go. So it just created a single, I think this is two tokens, right? Because the dollar sign is its own token. So basically this model didn't give me a single token, it gave me two tokens, but it still produced the correct answer.

And it did that in a single forward pass of the network. Now that's because the numbers here, I think are very simple. And so I made it a bit more difficult to be a bit mean to the model.

So I said, Emily buys 23 apples and 177 oranges. And then I just made the numbers a bit bigger. And I'm just making it harder for the model.

I'm asking it to do more computation in a single token. And so I said the same thing and here it gave me five and five is actually not correct. So the model failed to do all this calculation in a single forward pass of the network.

It failed to go from the input tokens and then in a single forward pass of the network, single go through the network. It couldn't produce the result. And then I said, OK, now don't worry about the token limit and just solve the problem as usual.

And then it goes all the intermediate results. It simplifies. And every one of these intermediate results here and intermediate calculations is much easier for the model.

And it's sort of, it's not too much work per token. All of the tokens here are correct and it arises a resolution, which is seven. And I just couldn't squeeze all of this work.

It couldn't squeeze that into a single forward pass of the network. So I think that's kind of just a cute example and something to kind of like think about. And I think it's kind of, again, just elucidative in terms of how these models work.

The last thing that I would say is that if I was in practice trying to actually solve this in my day-to-day life, I might actually not trust that the model, that all the intermediate calculations correctly here. So actually, probably what I do is something like this. I would come here and I would say, use code.

And that's because code is one of the possible tools that ChachiPT can use. And instead of it having to do mental arithmetic, like this mental arithmetic here, I don't fully trust it. And especially the numbers get really big.

There's no guarantee that the model will do this correctly. Any one of these intermediate steps might, in principle, fail. We're using neural networks to do mental arithmetic, kind of like you doing mental arithmetic in your brain.

It might just like screw up some of the intermediate results. It's actually kind of amazing that it can even do this kind of mental arithmetic. I don't think I could do this in my head, but basically the model is kind of like doing it in its head.

And I don't trust that. So I want it to use tools. So you can say stuff like, use code.

And I'm not sure what happened there. Use code. And so, like I mentioned, there's a special tool and the model can write code.

And I can inspect that this code is correct. And then it's not relying on its mental arithmetic. It is using the Python interpreter, which is a very simple programming language, to basically write out the code that calculates the result.

And I would personally trust this a lot more because this came out of the Python program, which I think has a lot more correctness guarantees than the mental arithmetic of a language model. So just another kind of potential hint that if you have these kinds of problems, you may want to basically just ask the model to use the code interpreter. And just like we saw with the web search, the model has special kind of tokens for calling.

Like it will not actually generate these tokens from the language model. It will write the program and then it actually sends that program to a different sort of part of the computer that actually just runs that program and brings back the result. And then the model gets access to that result and can tell you that, OK, the cost of each apple is seven.

So that's another kind of tool. And I would use this in practice for yourself. And it's, yeah, it's just less error prone, I would say.

So that's why I called this section Models Need Tokens to Think. Distribute your competition across many tokens. Ask models to create intermediate results.

Or whenever you can, lean on tools and tool use instead of allowing the models to do all of this stuff in their memory. So if they try to do it all in their memory, don't fully trust it and prefer to use tools whenever possible. I want to show you one more example of where this actually comes up, and that's in counting.

So models actually are not very good at counting for the exact same reason. You're asking for way too much in a single individual token. So let me show you a simple example of that.

How many dots are below? And then I just put in a bunch of dots. And Chachapiti says there are. And then it just tries to solve the problem in a single token.

So in a single token, it has to count the number of dots in its context window. And it has to do that in a single forward pass of a network. In a single forward pass of a network, as we talked about, there's not that much computation that can happen there.

Just think of that as being like very little computation that happens there. So if I just look at what the model sees, let's go to the LLM tokenizer. It sees this.

How many dots are below? And then it turns out that these dots here, this group of, I think, 20 dots is a single token. And then this group of whatever it is, is another token. And then for some reason, they break up as this.

So I don't actually, this has to do with the details of the tokenizer, but it turns out that these, the model basically sees the token ID, this, this, this, and so on. And then from these token IDs, it's expected to count the number. And spoiler alert, it's not 161.

It's actually, I believe, 177. So here's what we can do instead. We can say use code.

And you might expect that, like, why should this work? And it's actually kind of subtle and kind of interesting. So when I say use code, I actually expect this to work. Let's see.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

7 is correct. So what happens here is I've actually, it doesn't look like it, but I've broken down the problem into problems that are easier for the model. I know that the model can't count, it can't do mental counting, but I know that the model is actually pretty good at doing copy-pasting. 

So what I'm doing here is when I say use code, it creates a string in Python for this and the task of basically copy-pasting my input here to here is very simple because for the model it sees this string of, it sees it as just these four tokens or whatever it is. So it's very simple for the model to copy-paste those token IDs and kind of unpack them into dots here. And so it creates the string and then it calls Python routine dot count and then it comes up with the correct answer. 

So the Python interpreter is doing the counting, it's not the model's mental arithmetic doing the counting. So it's again, the simple example of models need tokens to think, don't rely on their mental arithmetic. And that's why also the models are not very good at counting.

If you need them to do counting tasks, always ask them to lean on the tool. Now the models also have many other little cognitive deficits here and there, and these are kind of like sharp edges of the technology to be kind of aware of over time. So as an example, the models are not very good with all kinds of spelling related tasks. 

They're not very good at it. And I told you that we would loop back around to tokenization. And the reason to do for this is that the models, they don't see the characters, they see tokens and their entire world is about tokens, which are these little text chunks. 

And so they don't see characters like our eyes do. And so very simple character level tasks often fail. So for example, I'm giving it a string, ubiquitous, and I'm asking it to print only every third character, starting with the first one. 

So we start with U, and then we should go every third. So one, two, three, Q should be next, and then et cetera. So this I see is not correct. 

And again, my hypothesis is that this is, again, the mental arithmetic here is failing, number one, a little bit. But number two, I think the more important issue here is that if you go to TickTokenizer and you look at ubiquitous, we see that it is three tokens, right? So you and I see ubiquitous, and we can easily access the individual letters because we kind of see them. And when we have it in the working memory of our visual sort of field, we can really easily index into every third letter, and I can do that task. 

But the models don't have access to the individual letters. They see this as these three tokens. And remember, these models are trained from scratch on the internet. 

And all these token, basically the model has to discover how many of all these different letters are packed into all these different tokens. And the reason we even use tokens is mostly for efficiency. But I think a lot of people are interested to delete tokens entirely. 

Like we should really have character level or byte level models. It's just that that would create very long sequences, and people don't know how to deal with that right now. So while we have the token world, any kind of spelling tasks are not actually expected to work super well. 

So because I know that spelling is not a strong suit because of tokenization, I can again ask it to lean on tools. So I can just say use code, and I would again expect this to work because the task of copy pasting ubiquitous into the Python interpreter is much easier. And then we're leaning on Python interpreter to manipulate the characters of this string.

So when I say use code, ubiquitous, yes, it indexes into every third character, and the actual truth is UQTS, which looks correct to me. So again, an example of spelling related tasks not working very well. A very famous example of that recently is how many R are there in strawberry? And this went viral many times. 

And basically the models now get it correct. They say there are three R's in strawberry. But for a very long time, all the state of the art models would insist that there are only two R's in strawberry. 

And this caused a lot of, you know, ruckus because is that a word? I think so. Because it's just kind of like, why are the models so brilliant? And they can solve math Olympiad questions, but they can't like count R's in strawberry. And the answer for that, again, is I've kind of built up to it kind of slowly. 

But number one, the models don't see characters, they see tokens. And number two, they are not very good at counting. And so here we are combining the difficulty of seeing characters with the difficulty of counting. 

And that's why the models struggled with this. Even though I think by now, honestly, I think OpenAI may have hard coded the answer here, or I'm not sure what they did. But this specific query now works. 

So models are not very good at spelling. And there's a bunch of other little sharp edges. And I don't want to go into all of them. 

I just want to show you a few examples of things to be aware of. And when you're using these models in practice, I don't actually want to have a comprehensive analysis here of all the ways that models are kind of like falling short. I just want to make the point that there are some jagged edges here and there. 

And we've discussed a few of them, and a few of them make sense. But some of them also will just not make as much sense. And they're kind of like you're left scratching your head, even if you understand in depth how these models work. 

And a good example of that recently is the following. The models are not very good at very simple questions like this. And this is shocking to a lot of people, because these math, these problems can solve complex math problems.

They can answer PhD-grade physics, chemistry, biology questions much better than I can. But sometimes they fall short in super simple problems like this. So here we go. 

9.11 is bigger than 9.9. And it justifies this in some way, but obviously. And then at the end, okay. It actually flips its decision later. 

So I don't believe that this is very reproducible. Sometimes it flips around its answer. Sometimes it gets it right. 

Sometimes it gets it wrong. Let's try again. Okay. 

Even though it might look larger. Okay. So here it doesn't even correct itself in the end. 

If you ask many times, sometimes it gets it right too. But how is it that the model can do so great at Olympiad-grade problems, but then fail on very simple problems like this? And I think this one is, as I mentioned, a little bit of a head scratcher. It turns out that a bunch of people studied this in depth and I haven't actually read the paper. 

But what I was told by this team was that when you scrutinize the activations inside the neural network, when you look at some of the features and what features turn on or off and what neurons turn on or off, a bunch of neurons inside the neural network light up that are usually associated with Bible verses. And so I think the model is kind of reminded that these almost look like Bible verse markers. And in a Bible verse setting, 9.11 would come after 9.9. And so basically the model somehow finds it cognitively very distracting that in Bible verses, 9.11 would be greater. 

Even though here it's actually trying to justify it and come up to the answer with a math, it still ends up with the wrong answer here. So it basically just doesn't fully make sense and it's not fully understood. And there's a few jagged issues like that. 

So that's why treat this as what it is, which is a stochastic system that is really magical, but that you can't also fully trust. And you want to use it as a tool, not as something that you kind of like let it rip on a problem and copy paste the results. Okay. 

So we have now covered two major stages of training of large language models. We saw that in the first stage, this is called the pre-training stage. We are basically training on internet documents.

And when you train a language model on internet documents, you get what's called a base model, and it's basically an internet document simulator, right? Now we saw that this is an interesting artifact and this takes many months to train on thousands of computers. And it's kind of a lossy compression of the internet. And it's extremely interesting, but it's not directly useful because we don't want to sample internet documents. 

We want to ask questions of an AI and have it respond to our questions. So for that, we need an assistant. And we saw that we can actually construct an assistant in the process of post-training and specifically in the process of supervised fine tuning, as we call it.

So in this stage, we saw that it's algorithmically identical to pre-training. Nothing is going to change. The only thing that changes is the dataset. 

So instead of internet documents, we now want to create and curate a very nice dataset of conversations. So we want millions of conversations on all kinds of diverse topics between a human and an assistant. And fundamentally, these conversations are created by humans. 

So humans write the prompts and humans write the ideal responses. And they do that based on labeling documentations. Now in the modern stack, it's not actually done fully and manually by humans, right? They actually now have a lot of help from these tools. 

So we can use language models to help us create these datasets. And we test them extensively. But fundamentally, it's all still coming from human curation at the end. 

So we create these conversations. That now becomes our dataset. We fine tune on it or continue training on it, and we get an assistant. 

And then we kind of shifted gears and started talking about some of the kind of cognitive implications of what this assistant is like. And we saw that, for example, the assistant will hallucinate if you don't take some sort of mitigations towards it. So we saw that hallucinations would be common. 

And then we looked at some of the mitigations of those hallucinations. And then we saw that the models are quite impressive and can do a lot of stuff in their head. But we saw that they can also lean on tools to become better. 

So for example, we can lean on the web search in order to hallucinate less and to maybe bring up some more recent information or something like that. Or we can lean on tools like Code Interpreter, so the LLM can write some code and actually run it and see the results. So these are some of the topics we looked at so far. 

Now what I'd like to do is I'd like to cover the last and major stage of this pipeline. And that is reinforcement learning. So reinforcement learning is still kind of thought to be under the umbrella of post-training. 

But it is the last third major stage. And it's a different way of training language models and usually follows as this third step. So inside companies like OpenAI, you will start here. 

And these are all separate teams. So there's a team doing data for pre-training and a team doing training for pre-training. And then there's a team doing all the conversation generation in a different team that is kind of doing the supervised fine-tuning. 

And there will be a team for the reinforcement learning as well. So it's kind of like a handoff of these models. You get your base model, then you fine-tune it to be an assistant, and then you go into reinforcement learning, which we'll talk about now. 

So that's kind of like the major flow. And so let's now focus on reinforcement learning, the last major stage of training. And let me first actually motivate it and why we would want to do reinforcement learning and what it looks like on a high level.

So now I'd like to try to motivate the reinforcement learning stage and what it corresponds to. That's something that you're probably familiar with. And that is basically going to school. 

So just like you went to school to become really good at something, we want to take large language models through school. And really what we're doing is where we have a few paradigms of ways of giving them knowledge or transferring skills. So in particular, when we're working with textbooks in school, you'll see that there are three major pieces of information in these textbooks, three classes of information. 

The first thing you'll see is that you'll see a lot of exposition. And by the way, this is a totally random book I pulled from the internet. I think it's some kind of organic chemistry or something, I'm not sure. 

But the important thing is that you'll see that most of the text, most of it is kind of just like the meat of it, is exposition. It's kind of like background knowledge, etc. As you are reading through the words of this exposition, you can think of that roughly as training on that data. 

And that's why when you're reading through this stuff, this background knowledge and there's all this context information, it's kind of equivalent to pre-training. So it's where we build sort of like a knowledge base of this data and get a sense of the topic. The next major kind of information that you will see is these problems and what their worked solutions. 

So basically, a human expert, in this case, the author of this book, has given us not just a problem, but has also worked through the solution. And the solution is basically like equivalent to having like this ideal response for an assistant. So it's basically the expert is showing us how to solve the problem. 

And it's kind of like in its full form. So as we are reading the solution, we are basically training on the expert data. And then later, we can try to imitate the expert. 

And basically, that roughly corresponds to having the SFT model. That's what it would be doing. So basically, we've already done pre-training, and we've already covered this imitation of experts and how they solve these problems. 

And the third stage of learning is basically the practice problems. So sometimes you'll see this is just a single practice problem here. But of course, there will be usually many practice problems at the end of each chapter in any textbook. 

And practice problems, of course, we know are critical for learning, because what are they getting you to do? They're getting you to practice yourself and discover ways of solving these problems yourself. And so what you get in a practice problem is you get a problem description, but you're not given the solution, but you are given the final answer, usually in the answer key of the textbook. And so you know the final answer that you're trying to get to, and you have the problem statement, but you don't have the solution. 

You are trying to practice the solution. You're trying out many different things, and you're seeing what gets you to the final solution the best. And so you're discovering how to solve these problems.

And in the process of that, you're relying on, number one, the background information, which comes from pre-training, and number two, maybe a little bit of imitation of human experts. And you can probably try similar kinds of solutions and so on. So we've done this and this, and now in this section, we're going to try to practice.

And so we're going to be given prompts. We're going to be given solutions. Sorry, the final answers, but we're not going to be given expert solutions. 

We have to practice and try stuff out. And that's what reinforcement learning is about. Okay, so let's go back to the problem that we worked with previously, just so we have a concrete example to talk through as we explore the topic here. 

So I'm here in the tick tokenizer because I'd also like to, well, I get a text box, which is useful, but number two, I want to remind you again that we're always working with one-dimensional token sequences. And so I actually prefer this view because this is the native view of the LLM, if that makes sense. This is what it actually sees. 

It sees token IDs, right? Okay. So Emily buys three apples and two oranges. Each orange is $2. 

The total cost of all the fruit is $13. What is the cost of each apple? And what I'd like you to appreciate here is these are like four possible candidate solutions, as an example, and they all reach the answer three. Now, what I'd like you to appreciate at this point is that if I'm the human data labeler that is creating a conversation to be entered into the training set, I don't actually really know which of these conversations to add to the dataset. 

Some of these conversations kind of set up a system of equations, some of them sort of just talk through it in English, and some of them just kind of like skip right through to the solution. If you look at chatGPT, for example, and you give it this question, it defines a system of variables, and it kind of like does this little thing. What we have to appreciate and differentiate between, though, is the first purpose of a solution is to reach the right answer, of course. 

We want to get the final answer three. That is the important purpose here. But there's kind of like a secondary purpose as well, where here we are also just kind of trying to make it like nice for the human, because we're kind of assuming that the person wants to see the solution, they want to see the intermediate steps, we want to present it nicely, et cetera. 

So there are two separate things going on here. Number one is the presentation for the human, but number two, we're trying to actually get the right answer. So let's, for the moment, focus on just reaching the final answer.

If we only care about the final answer, then which of these is the optimal or like the best prompt, sorry, the best solution for the LLM to reach the right answer? And what I'm trying to get at is we don't know. Me, as a human labeler, I would not know which one of these is best. So as an example, we saw earlier on when we looked at the token sequences here and the mental arithmetic and reasoning, we saw that for each token, we can only spend basically a finite number of, finite amount of compute here that is not very large, or you should think about it that way. 

And so we can't actually make too big of a leap in any one token is maybe the way to think about it. So as an example, in this one, what's really nice about it is that it's very few tokens. So it's going to take us a very short amount of time to get to the answer. 

But right here, when we're doing 13 minus four divide three equals, right in this token here, we're actually asking for a lot of computation to happen on that single individual token. And so maybe this is a bad example to give to the LLM because it's kind of incentivizing it to skip through the calculations very quickly. And it's going to make mistakes in its mental arithmetic. 

So maybe it would work better to spread it out more. Maybe it would be better to set it up as an equation. Maybe it would be better to talk through it. 

We fundamentally don't know. And we don't know because what is easy for you or I, or as human labelers, what's easy for us or hard for us is different than what's easy or hard for the LLM. Its cognition is different. 

And the token sequences are kind of like different hard for it. And so some of the token sequences here that are trivial for me might be very too much of a leap for the LLM. So right here, this token would be way too hard. 

But conversely, many of the tokens that I'm creating here might be just trivial to the LLM. And we're just wasting tokens. Why waste all these tokens when this is all trivial? So if the only thing we care about is reaching the final answer, and we're separating out the issue of the presentation to the human, then we don't actually really know how to annotate this example. 

We don't know what solution to give to the LLM because we are not the LLM. And it's clear here in the case of the math example, but this is actually a very pervasive issue. Our knowledge is not LLM's knowledge.

The LLM actually has a ton of knowledge of PhD in math and physics and chemistry and whatnot. So in many ways, it actually knows more than I do. And I'm potentially not utilizing that knowledge in its problem solving. 

But conversely, I might be injecting a bunch of knowledge in my solutions that the LLM doesn't know in its parameters. And then those are like sudden leaps that are very confusing to the model. And so our cognitions are different. 

And I don't really know what to put here if all we care about is the reaching the final solution and doing it economically, ideally. And so long story short, we are not in a good position to create these token sequences for the LLM. And they're useful by imitation to initialize the system. 

But we really want the LLM to discover the token sequences that work for it. It needs to find for itself what token sequence reliably gets to the answer given the prompt. And it needs to discover that in a process of reinforcement learning and of trial and error. 

So let's see how this example would work like in reinforcement learning. Okay, so we're now back in the Hugging Face Inference Playground. And that just allows me to very easily call different kinds of models. 

So as an example, here on the top right, I chose the Gemma 2 billion parameter model. So 2 billion is very, very small. So this is a tiny model, but it's okay. 

So we're going to give it the way that reinforcement learning will basically work is actually quite simple. We need to try many different kinds of solutions. And we want to see which solutions work well or not. 

So we're basically going to take the prompt, we're going to run the model. And the model generates a solution. And then we're going to inspect the solution. 

And we know that the correct answer for this one is $3. And so indeed, the model gets it correct, it says it's $3. So this is correct. 

So that's just one attempt at the solution. So now we're going to delete this, and we're going to rerun it again. Let's try a second attempt. 

So the model solves it in a bit slightly different way, right? Every single attempt will be a different generation, because these models are stochastic systems. Remember that every single token here, we have a probability distribution, and we're sampling from that distribution. So we end up going down slightly different paths. 

And so this is the second solution that also ends in the correct answer. Now we're going to delete that. Let's go a third time.

Okay, so again, slightly different solution, but also gets it correct. Now, we can actually repeat this many times. And so in practice, you might actually sample thousands of independent solutions, or even like a million solutions for just a single prompt. 

And some of them will be correct, and some of them will not be very correct. And basically, what we want to do is we want to encourage the solutions that lead to correct answers. So let's take a look at what that looks like. 

So if we come back over here, here's kind of like a cartoon diagram of what this is looking like. We have a prompt, and then we tried many different solutions in parallel. And some of the solutions might go well, so they get the right answer, which is in green.

And some of the solutions might go poorly and may not reach the right answer, which is red. Now, this problem here, unfortunately, is not the best example, because it's a trivial prompt. And as we saw, even like a two billion parameter model always gets it right. 

So it's not the best example in that sense. But let's just exercise some imagination here. And let's just suppose that the green ones are good, and the red ones are bad. 

Okay, so we generated 15 solutions, only four of them got the right answer. And so now what we want to do is, basically, we want to encourage the kinds of solutions that lead to right answers. So whatever token sequences happened in these red solutions, obviously, something went wrong along the way somewhere. 

And this was not a good path to take through the solution. And whatever token sequences that were in these green solutions, well, things went pretty well in this situation. And so we want to do more things like it in prompts like this. 

And the way we encourage this kind of behavior in the future is we basically train on these sequences. But these training sequences now are not coming from expert human annotators. There's no human who decided that this is the correct solution. 

This solution came from the model itself. So the model is practicing here, it's tried out a few solutions, four of them seem to have worked. And now the model will kind of like train on them. 

And this corresponds to a and being like, okay, well, this one worked really well. So this is how I should be solving these kinds of problems. And here in this example, there are many different ways to actually like really tweak the methodology a little bit here. 

But just to get the core idea across, maybe it's simple to just think about taking the single best solution out of these four, like say this one, that's why it was yellow. So this is the solution that not only led to the right answer, but maybe had some other nice properties. Maybe it was the shortest one, or it looked nicest in some ways, or there's other criteria you could think of as an example. 

But we're going to decide that this is the top solution, we're going to train on it. And then the model will be slightly more likely, once you do the parameter update, to take this path in this kind of a setting in the future. But you have to remember that we're going to run many different diverse prompts across lots of math problems and physics problems and whatever there might be. 

So tens of thousands of prompts, maybe have in mind, there's thousands of solutions per prompt. And so this is all happening kind of like at the same time. And as we're iterating this process, the model is discovering for itself, what kinds of token sequences lead it to correct answers. 

It's not coming from a human annotator. The model is kind of like playing in this playground. And it knows what it's trying to get to, and it's discovering sequences that work for it. 

These are sequences that don't make any mental leaps. They seem to work reliably and statistically, and fully utilize the knowledge of the model as it has it. And so this is the process of reinforcement learning.

It's basically a guess and check. We're going to guess many different types of solutions, we're going to check them, and we're going to do more of what worked in the future. And that is reinforcement learning. 

So in the context of what came before, we see now that the SFT model, the supervised fine tuning model, it's still helpful because it's still kind of like initializes the model a little bit into the vicinity of the correct solutions. So it's kind of like a initialization of the model, in the sense that it kind of gets the model to take solutions, like write out solutions, and maybe it has an understanding of setting up a system of equations, or maybe it kind of like talks to a solution. So it gets you into the vicinity of correct solutions. 

But reinforcement learning is where everything gets dialed in. We really discover the solutions that work for the model, get the right answers, we encourage them, and then the model just kind of like gets better over time. Okay, so that is the high level process for how we train large language models. 

In short, we train them kind of very similar to how we train children. And basically, the only difference is that children go through chapters of books, and they do all these different types of training exercises, kind of within a chapter of each book. But instead, when we train AIs, it's almost like we kind of do it stage by stage, depending on the type of that stage. 

So first, what we do is we do pre-training, which as we saw is equivalent to basically reading all the expository material. So we look at all the textbooks at the same time, and we read all the exposition, and we try to build a knowledge base. The second thing then is we go into the SFT stage, which is really looking at all the fixed sort of like solutions from human experts of all the different kinds of worked solutions across all the textbooks. 

And we just kind of get an SFT model, which is able to imitate the experts, but does so kind of blindly. It just kind of like does its best guess, kind of just like trying to mimic statistically the expert behavior. And so that's what you get when you look at all the solutions. 

And then finally, in the last stage, we do all the practice problems in the RL stage. Across all the textbooks, we only do the practice problems. And that's how we get the RL model.

So on a high level, the way we train LLMs is very much equivalent to the process that we train, that we use for training of children. The next point I would like to make is that actually these first two stages, pre-training and supervised fine-tuning, they've been around for years, and they are very standard, and everyone does them, all the different LLM providers. It is this last stage, the RL training, that is a lot more early in its process of development and is not standard yet in the field. 

And so this stage is a lot more kind of early and nascent. And the reason for that is because I actually skipped over a ton of little details here in this process. The high level idea is very simple. 

It's trial and error learning, but there's a ton details and little mathematical kind of like nuances to exactly how you pick the solutions that are the best and how much you train on them, and what is the prompt distribution, and how to set up the training run such that this actually works. So there's a lot of little details and knobs to the core idea that is very, very simple. And so getting the details right here is not trivial. 

And so a lot of companies like, for example, OpenAI and other LLM providers have experimented internally with reinforcement learning fine-tuning for LLMs for a while, but they've not talked about it publicly. It's all kind of done inside the company. And so that's why the paper from DeepSeek that came out very, very recently was such a big deal, because this is a paper from this company called DeepSeek AI in China. 

And this paper really talked very publicly about reinforcement learning fine-tuning for large language models, and how incredibly important it is for large language models, and how it brings out a lot reasoning capabilities in the models. We'll go into this in a second. So this paper reinvigorated the public interest of using RL for LLMs, and gave a lot of the sort of nitty-gritty details that are needed to reproduce the results, and actually get the stage to work for large language models. 

So let me take you briefly through this DeepSeek R1 paper, and what happens when you actually correctly apply RL to language models, and what that looks like, and what that gives you. So the first thing I'll scroll to is this kind of figure 2 here, where we are looking at the improvement in how the models are solving mathematical problems. So this is the accuracy of solving mathematical problems on the AIME accuracy. 

And then we can go to the web page, and we can see the kinds of problems that are actually in these kinds of math problems.

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)


(转录由TurboScribe.ai完成。升级到无限以移除此消息。)

So these are simple math problems. You can pause the video if you like, but these are the kinds of problems that basically the models are being asked to solve. And you can see that in the beginning they're not doing very well, but then as you update the model with this many thousands of steps, their accuracy kind of continues to climb. 

So the models are improving and they're solving these problems with a higher accuracy as you do this trial and error on a large data set of these kinds of problems. And the models are discovering how to solve math problems. But even more incredible than the quantitative kind of results of solving these problems with a higher accuracy is the qualitative means by which the model achieves these results.

So when we scroll down, one of the figures here that is kind of interesting is that later on in the optimization, the model seems to be using average length per response goes up. So the model seems to be using more tokens to get its higher accuracy results. So it's learning to create very, very long solutions. 

Why are these solutions very long? We can look at them qualitatively here. So basically what they discover is that the model solution get very, very long partially because, so here's a question and here's kind of the answer from the model. What the model learns to do, and this is an emerging property of the optimization, it just discovers that this is good for problem solving, is it starts to do stuff like this.

Wait, wait, wait, that's an aha moment I can flag here. Let's re-evaluate this step by step to identify the correct sum can be. So what is the model doing here? The model is basically re-evaluating steps. 

It has learned that it works better for accuracy to try out lots of ideas, try something from different perspectives, retrace, reframe, backtrack. It's doing a lot of the things that you and I are doing in the process of problem solving for mathematical questions, but it's rediscovering what happens in your head, not what you put down on the solution. And there is no human who can hard code this stuff in the ideal assistant response. 

This is only something that can be discovered in the process of reinforcement learning, because you wouldn't know what to put here. This just turns out to work for the model and it improves its accuracy in problem solving. So the model learns what we call these chains of thought in your head, and it's an emergent property of the optimization. 

And that's what's bloating up the response lens, but that's also what's increasing the accuracy of the problem solving. So what's incredible here is basically the model is discovering ways to think. It's learning what I like to call cognitive strategies of how you manipulate a problem and how you approach it from different perspectives, how you pull in some analogies or do different kinds of things like that, and how you kind of try out many different things over time, check a result from different perspectives, and how you kind of solve problems. 

But here it's kind of discovered by the RL. So extremely incredible to see this emerge in the optimization without having to hard code it anywhere. The only thing we've given it are the correct answers, and this comes out from trying to just solve them correctly, which is incredible. 

Now let's go back to actually the problem that we've been working with, and let's take a look at what it would look like for this kind of a model, what we call reasoning or thinking model, to solve that problem. Okay, so recall that this problem we've been working with, and when I pasted it into chatgpt 4.0, I'm getting this kind of a response. Let's take a look at what happens when you give the same query to what's called a reasoning or a thinking model. 

This is a model that was trained with reinforcement learning. So this model described in this paper, DeepSeek R1, is available on chat.deepseek.com. So this is kind of what the company that developed it is hosting it. You have to make sure that the it's called. 

We can paste it here and run it. And so let's take a look at what happens now, and what is the output of the model. Okay, so here's what it says. 

So this is previously what we get using basically what's an SFT approach, a supervised fine-tuning approach. This is like mimicking an expert solution. This is what we get from the RL model. 

Okay, let me try to figure this out. So Emily buys three apples and two oranges. Each orange costs $2, total is $13.

I need to find out blah, blah, blah. So here, as you're reading this, you can't escape thinking that this model is thinking. It's definitely pursuing the solution. 

It derives that it must cost $3. And then it says, wait a second, let me check my math again to be sure. And then it tries it from a slightly different perspective. 

And then it says, yep, all that checks out. I think that's the answer. I don't see any mistakes. 

Let me see if there's another way to approach the problem, maybe setting up an equation. Let's let the cost of one apple be $8, then blah, blah, blah. Yep, same answer. 

So definitely each apple is $3. All right, confident that that's correct. And then what it does once it sort of did the thinking process is it writes up the nice solution for the human. 

And so this is now considering, so this is more about the correctness aspect, and this is more about the presentation aspect, where it kind of writes it out nicely and boxes in the correct answer at the bottom. And so what's incredible about this is we get this thinking process of the model. And this is what's coming from the reinforcement learning process. 

This is what's bloating up the length of the token sequences. They're doing thinking, and they're trying different ways. This is what's giving you higher accuracy in problem solving. 

And this is where we are seeing these aha moments and these different strategies and these ideas for how you can make sure that you're getting the correct answer. The last point I wanted to make is some people are a little bit nervous about putting very sensitive data into chat.deepseek.com because this is a Chinese company, so people are a little bit careful and cagey with that a little bit. DeepSeek R1 is a model that was released by this company.

So this is an open source model or open weights model. It is available for anyone to download and use. You will not be able to run it in its full sort of the full model in full precision.

You won't run that on a MacBook or like a local device because this is a fairly large model. But many companies are hosting the full largest model. One of those companies that I like to use is called Together.ai. So when you go to Together.ai, you sign up and you go to Playgrounds.

You can select here in the chat DeepSeek R1, and there's many different kinds of other models that you can select here. These are all state-of-the-art models. So this is kind of similar to the Hugging Face inference playground that we've been playing with so far, but Together.ai will usually host all the state-of-the-art models. 

So select DeepSeek R1. You can try to ignore a lot of these. I think the default settings will often be okay, and we can put in this. 

And because the model was released by DeepSeek, what you're getting here should be basically equivalent to what you're getting here. Now because of the randomness in the sampling, we're going to get something slightly different, but in principle this should be identical in terms of the power of the model, and you should be able to see the same things quantitatively and qualitatively, but this model is coming from kind of an American company. So that's DeepSeek, and that's what's called a reasoning model.

Now when I go back to chat, let me go to chat here. Okay, so the models that you're going to see in the drop-down here, some of them like O1, O3 mini, O3 mini high, etc., they are talking about uses advanced reasoning. Now what this is referring to, uses advanced reasoning, is it's referring to the fact that it was trained by reinforcement learning with techniques very similar to those of DeepSeek R1, per public statements of OpenAI employees. 

So these are thinking models trained with RL, and these models like GPT-4.0 or GPT-4.0 mini that you're getting in the free tier, you should think of them as mostly SFT models, supervised fine-tuning models. They don't actually do this like thinking as you see in the RL models. And even though there's a little bit of reinforcement learning involved with these models, and I'll go into that in a second, these are mostly SFT models. 

I think you should think about it that way. So in the same way as what we saw here, we can pick one of the thinking models, like say O3 mini high, and these models by the way might not be available to you unless you pay a ChachiPT subscription of either $20 per month or $200 per month for some of the top models. So we can pick a thinking model and run. 

Now what's going to happen here is it's going to say reasoning, and it's going to start to do stuff like this. And what we're seeing here is not exactly the stuff we're seeing here. So even though under the hood, the model produces these kind of chains of thought, OpenAI chooses to not show the exact chains of thought in the web interface. 

It shows little summaries of those chains of thought. And OpenAI kind of does this, partly because they are worried about what's called a distillation risk. That is that someone could come in and actually try to imitate those reasoning traces and recover a lot of the reasoning performance by just imitating the reasoning chains of thought. 

And so they kind of hide them and they only show little summaries of them. So you're not getting exactly what you would get in DeepSeq with respect to the reasoning itself. And then they write out the solution.

So these are kind of like equivalent, even though we're not seeing the full under the hood details. Now in terms of the performance, these models and DeepSeq models are currently roughly on par, I would say. It's kind of hard to tell because of the evaluations. 

But if you're paying $200 per month to OpenAI, some of these models I believe are currently, they basically still look better. But DeepSeq R1 for now is still a very solid choice for a thinking model that would be available to you sort of either on this website or any other website because the model is open weights and you can just download it. So that's thinking models. 

So what is the summary so far? Well, we've talked about reinforcement learning and the fact that thinking emerges in the process of the optimization on when we basically run RL on many math and kind of code problems that have verifiable solutions. So there's like an answer three, et cetera. Now these thinking models you can access in, for example, DeepSeq or any inference provider like together.ai and choosing DeepSeq over there. 

These thinking models are also available in chatGPT under any of the O1 or O3 models. But these GPT 4.0 models, et cetera, they're not thinking models. You should think of them as mostly SFT models. 

Now if you are, if you have a prompt that requires advanced reasoning and so on, you should probably use some of the thinking models or at least try them out. But empirically for a lot of my use, when you're asking a simpler question, there's like a knowledge-based question or something like that, this might be overkill. Like there's no need to think 30 seconds about some factual question. 

So for that, I will sometimes default to just GPT 4.0. So empirically about 80, 90% of my use is just GPT 4.0. And when I come across a very difficult problem, like in math and code, et cetera, I will reach for the thinking models, but then I have to wait a bit longer because they are thinking. So you can access these on chatGPT, on DeepSeq. Also I wanted to point out that aistudio.google.com, even though it looks really busy, really ugly because Google is just unable to do this kind of stuff well, is like what is happening. 

But if you choose model and you choose here, Gemini 2.0 Flash Thinking Experimental 0121, if you choose that one, that's also a kind of early experiment, experimental of a thinking model by Google. So we can go here and we can give it the same problem and click run. And this is also a thinking model that will also do something similar and comes out with the right answer here. 

So basically Gemini also offers a thinking model. Anthropic currently does not offer a thinking model. But basically this is kind of like the frontier development of these LLMs. 

I think RL is kind of like this new exciting stage, but getting the details right is difficult. And that's why all these models and thinking models are currently experimental as of 2025, very early 2025. But this is kind of like the frontier development of pushing the performance in these very difficult problems using reasoning that is emergent in these optimizations. 

One more connection that I wanted to bring up is that the discovery that reinforcement learning is extremely powerful way of learning is not new to the field of AI. And one place where we've already seen this demonstrated is in the game of Go. And famously DeepMind developed the system AlphaGo, and you can watch a movie about it, where the system is learning to play the game of Go against top human players. 

And when we go to the paper underlying AlphaGo, so in this paper, when we scroll down, we actually find a really interesting plot that I think is kind of familiar to us, and we're kind of like rediscovering in the more open domain of arbitrary problem solving, instead of on the closed specific domain of the game of Go. But basically what they saw, and we're going to see this in LLMs as well, as this becomes more mature, is this is the ELO rating of playing game of Go, and this is Lee Sedol, an extremely strong human player. And here where they are comparing is the strength of a model learned, trained by supervised learning, and a model trained by reinforcement learning. 

So the supervised learning model is imitating human expert players. So if you just get a huge amount of games played by expert players in the game of Go, and you try to imitate them, you are going to get better. But then you top out, and you never quite get better than some of the top, top, top players in the game of Go, like Lee Sedol. 

So you're never going to reach there, because you're just imitating human players. You can't fundamentally go beyond a human player if you're just imitating human players. But in the process of reinforcement learning is significantly more powerful. 

In reinforcement learning for a game of Go, it means that the system is playing moves that empirically and statistically lead to winning the game. And so AlphaGo is a system where it kind of plays against itself, and it's using reinforcement learning to create rollouts. So it's the exact same diagram here, but there's no prompt, because there's no prompt, it's just a fixed game of Go.

But it's trying out lots of solutions, it's trying lots of plays, and then the games that lead to a win, instead of a specific answer, are reinforced. They're made stronger. And so the system is learning basically the sequences of actions that empirically and statistically lead to winning the game. 

And reinforcement learning is not going to be constrained by human performance. And reinforcement learning can do significantly better and overcome even the top players like Lee Sedol. And so probably they could have run this longer, and they just chose to crop it at some point, because this costs money. 

But this is a very powerful demonstration of reinforcement learning. And we're only starting to kind of see hints of this diagram in larger language models for reasoning problems. So we're not going to get too far by just imitating experts.

We need to go beyond that, set up these like little game environments, and let the system discover reasoning traces, or like ways of solving problems that are unique, and that just basically work well. Now on this aspect of uniqueness, notice that when you're doing reinforcement learning, nothing prevents you from veering off the distribution of how humans are playing the game. And so when we go back to this AlphaGo search here, one of the suggested modifications is called move 37. 

And move 37 in AlphaGo is referring to a specific point in time where AlphaGo basically played a move that no human expert would play. So the probability of this move to be played by a human player was evaluated to be about 1 in 10,000. So it's a very rare move.

But in retrospect, it was a brilliant move. So AlphaGo, in the process of reinforcement learning, discovered kind of like a strategy of playing that was unknown to humans, but is in retrospect brilliant. I recommend this YouTube video, Lee Sedol versus AlphaGo move 37 reactions and analysis.

And this is kind of what it looked like when AlphaGo played this move. That's a very, that's a very surprising move. I thought it was, I thought it was a mistake.

When I see this move. Anyway, so basically people are kind of freaking out because it's, it's a move that a human would not play, that AlphaGo played, because in its training, this move seemed to be a good idea. It just happens not to be a kind of thing that humans would do. 

And so that is, again, the power of reinforcement learning. And in principle, we can actually see the equivalence of that if we continue scaling this paradigm in language models. And what that looks like is kind of unknown. 

So what does it mean to solve problems in such a way that even humans would not be able to get? How can you be better at reasoning or thinking than humans? How can you go beyond just a thinking human? Like maybe it means discovering analogies that humans would not be able to create. Or maybe it's like a new thinking strategy. It's kind of hard to think through. 

Maybe it's a wholly new language that actually is not even English. Maybe it discovers its own language that is a lot better at thinking. Because the model is unconstrained to even like stick with English. 

So maybe it picks a different language to think in, or it discovers its own language. So in principle, the behavior of the system is a lot less defined. It is open to do whatever works. 

And it is open to also slowly drift from the distribution of its training data, which is English. But all of that can only be done if we have a very large, diverse set of problems in which these strategies can be refined and perfected. And so that is a lot of the frontier LLM research that's going on right now. 

It's trying to kind of create those kinds of prompt distributions that are large and diverse. These are all kind of like game environments in which the LLMs can practice their thinking. And it's kind of like writing these practice problems.

We have to create practice problems for all of domains of knowledge. And if we have practice problems and tons of them, the models will be able to reinforcement learn on them and kind of create these kinds of diagrams, but in a domain of open thinking instead of a closed domain like Game of Go. There's one more section within reinforcement learning that I wanted to cover. 

And that is that of learning in unverifiable domains. So, so far, all of the problems that we've looked at are in what's called verifiable domains. That is, any candidate solution we can score very easily against a concrete answer. 

So for example, answer is three, and we can very easily score these solutions against the answer of three. Either we require the models to like box in their answers, and then we just check for equality of whatever's in the box with the answer. Or you can also use kind of what's called an LLM judge. 

So the LLM judge looks at a solution and it gets the answer and just basically scores the solution for whether it's consistent with the answer or not. And LLMs empirically are good enough at the current capability that they can do this fairly reliably. So we can apply those kinds of techniques as well. 

In any case, we have a concrete answer, and we're just checking solutions against it. And we can do this automatically with no kind of humans in the loop. The problem is that we can't apply this strategy in what's called unverifiable domains. 

So usually these are, for example, creative writing tasks like write a joke about pelicans or write a poem or summarize a paragraph or something like that. In these kinds of domains, it becomes harder to score our different solutions to this problem. So for example, writing a joke about pelicans, we can generate lots of different jokes, of course.

That's fine. For example, we can go to ChachiPT and we can get it to generate a joke about pelicans. So much stuff in their beaks because they don't pelican in backpacks.

What? Okay. We can try something else. Why don't pelicans ever pay for their drinks? Because they always bill it to someone else. 

Ha ha. Okay. So these models are obviously not very good at humor. 

Actually, I think it's pretty fascinating because I think humor is secretly very difficult and the models don't have the capability, I think. Anyway, in any case, you could imagine creating lots of jokes. The problem that we are facing is how do we score them? Now, in principle, we could, of course, get a human to look at all these jokes just like I did right now. 

The problem with that is if you are doing reinforcement learning, you're going to be doing thousands of updates. For each update, you want to be looking at thousands of prompts. For each prompt, you want to be potentially looking at hundreds or thousands of different kinds of generations. 

There's just way too many of these to look at. In principle, you could have a human inspect all of them and score them and decide maybe this one is funny and maybe this one is funny and this one is funny and we could train on them to get the model to become slightly better at jokes in the context of Pelicans at least. The problem is that it's just way too much human time. 

This is an unscalable strategy. We need some kind of an automatic strategy for doing this. One sort of solution to this was proposed in this paper that introduced what's called reinforcement learning from human feedback. 

This was a paper from OpenAI at the time. Many of these people are now co-founders in Anthropic. This kind of proposed a approach for basically doing reinforcement learning in unverifiable domains. 

Let's take a look at how that works. This is the cartoon diagram of the core ideas involved. As I mentioned, the naive approach is if we just had infinity human time, we could just run RL in these domains just fine. 

For example, we can run RL as usual if I have infinity humans. I just want to do, and these are just cartoon numbers, I want to do 1,000 updates where each update will be on 1,000 prompts. For each prompt, we're going to have 1,000 rollouts that we're scoring. 

We can run RL with this kind of a setup. The problem is in the process of doing this, I would need to ask a human to evaluate a joke a total of 1 billion times. That's a lot of people looking at really terrible jokes. 

We don't want to do that. Instead, we want to take the RLHF approach. In RLHF approach, we are kind of like the core trick is that of indirection. 

We're going to involve humans just a little bit. The way we cheat is that we basically train a whole separate neural network that we call a reward model. This neural network will kind of like imitate human scores. 

We're going to ask humans to score rollouts. We're going to then imitate human scores using a neural network. And this neural network will become a kind of simulator of human preferences.

Now that we have a neural network simulator, we can do RL against it. Instead of asking a real human, we're asking a simulated human for their score of a joke, as an example. Once we have a simulator, we're off to the races because we can query it as many times as we want to. 

It's a whole automatic process. We can now do reinforcement learning with respect to the simulator. The simulator, as you might expect, is not going to be a perfect human. 

But if it's at least statistically similar to human judgment, then you might expect that this will do something. And in practice, indeed, it does. So once we have a simulator, we can do RL and everything works great.

So let me show you a cartoon diagram a little bit of what this process looks like. Although the details are not super important, it's just a core idea of how this works. So here we have a cartoon diagram of a hypothetical example of what training the reward model would look like.

So we have a prompt, like write a joke about pelicans, and then here we have five separate rollouts. So these are all five different jokes, just like this one. Now, the first thing we're going to do is we are going to ask a human to order these jokes from the best to worst.

So this is so here, this human thought that this joke is the best, the funniest. So number one joke. This is number two joke, number three joke, four and five. 

So this is the worst joke. We're asking humans to order instead of give scores directly, because it's a bit of an easier task. It's easier for a human to give an ordering than to give precise scores. 

Now, that is now the supervision for the model. So the human has ordered them, and that is kind of like their contribution to the training process. But now separately, what we're going to do is we're Now, the reward model is a whole separate neural network, completely separate neural net, and it's also probably a transformer, but it's not a language model in the sense that it generates diverse language, etc. 

It's just a scoring model. So the reward model will take as an input the prompt, number one, and number two, a candidate joke. So those are the two inputs that go into the reward model. 

So here, for example, the reward model would be taking this prompt and this joke. Now, the output of a reward model is a single number, and this number is thought of as a score, and it can range, for example, from zero to one. So zero would be the worst score, and one would be the best score. 

So here are some examples of what a hypothetical reward model at some stage in the training process would give as scoring to these jokes. So 0.1 is a very low score, 0.8 is a really high score, and so on. And so now we compare the scores given by the reward model with the ordering given by the human. 

And there's a precise mathematical way to actually calculate this, basically set up a loss function and calculate a correspondence here and update a model based on it. But I just want to give you the intuition, which is that, as an example here, for this second joke, the human thought that it was the funniest, and the model kind of agreed, right? 0.8 is a relatively high score. But this score should have been even higher, right? So after an update, we would expect that maybe the score should have been, will actually grow after an update of the network to be like, say, 0.81 or something.

For this one here, they actually are in a massive disagreement, because the human thought that this was number two, but here the score is only 0.1. And so this score needs to be much higher. So after an update, on top of this kind of a supervision, this might grow a lot more, like maybe it's 0.15 or something like that. And then here, the human thought that this one was the worst joke, but here the model actually gave it a fairly high number. 

So you might expect that after the update, this would come down to maybe 3.5 or something like that. So basically, we're doing what we did before. We're slightly nudging the predictions from the models using a neural network training process. 

And we're trying to make the reward model scores be consistent with human ordering. And so as we update the reward model on human data, it becomes better and better simulator of the scores and orders that humans provide, and then becomes kind of like the the simulator of human preferences, which we can then do RL against. But critically, we're not asking humans 1 billion times to look at a joke. 

We're maybe looking at 1,000 prompts and five rollouts each, so maybe 5,000 jokes that humans have to look at in total. And they just give the ordering, and then we're training the model to be consistent with that ordering. And I'm skipping over the mathematical details, but I just want you to understand a high-level idea that this reward model is basically giving us the scores, and we have a way of training it to be consistent with human orderings. 

And that's how RLHF works. Okay, so that is the rough idea. We basically train simulators of humans and RL with respect to those simulators.

Now, I want to talk about first the upside of reinforcement learning from human feedback. The first thing is that this allows us to run reinforcement learning, which we know is an incredibly powerful set of techniques, and it allows us to do it in arbitrary domains, and including the ones that are unverifiable. So things like summarization and poem writing, joke writing, or any other creative writing, really, in domains outside of math and code, et cetera. 

Now, empirically, what we see when we actually apply RLHF is that this is a way to improve the performance of the model. And I have a top answer for why that might be, but I don't actually know that it is super well established on why this is. You can empirically observe that when you do RLHF correctly, the models you get are just a little bit better.

But as to why is, I think, not as clear. So here's my best guess. My best guess is that this is possibly mostly due to the discriminator-generator gap. 

What that means is that in many cases, it is significantly easier to discriminate than to generate for humans. So in particular, an example of this is when we do supervised fine-tuning, right, SFT, we're asking humans to generate the ideal assistant response. And in many cases here, as I've shown it, the ideal response is very simple to write, but in many cases, it might not be. 

So for example, in summarization, or poem writing, or joke writing, how are you as a human labeler supposed to get the ideal response in these cases? It requires creative human writing to do that. And so RLHF kind of sidesteps this, because we get to ask people a significantly easier question as data labelers. They're not asked to write poems directly. 

They're just given five poems from the model, and they're just asked to order them. And so that's just a much easier task for a human labeler to do. And so what I think this allows you to do basically is, it kind of like allows a lot...

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)