
大家好，其实我一直想做这个视频。这是一个面向大众、全面介绍 ChatGPT 等大语言模型的视频。我希望通过这个视频，能帮助大家建立起理解这类工具运作方式的思维模型。

它在某些方面显然充满魔力且令人惊叹。有些事它做得非常出色，另一些则不尽如人意，同时还有许多需要注意的尖锐问题。那么这个文本框背后究竟是什么？你可以输入任何内容并按下回车，但我们应该输入什么？这些生成的文字又是从何而来？它的运作原理是什么？你实际上是在和什么对话？我希望通过这个视频探讨所有这些话题。

我们将完整梳理这类系统是如何构建的整个流程，但我会尽量让讲解通俗易懂，适合大众理解。

### Step 1: download and preprocess the internet

整个流程将分为多个按顺序排列的阶段。*第一阶段称为预训练阶段*，而预训练的第一步是下载并处理互联网数据。为了让大家对此有个大致概念，我建议查看这个链接（FineWeb）。

有一家名为 HuggingFace 的公司收集并精心整理了一个名为 FineWeb 的数据集，他们在这篇博客文章中详细介绍了构建 FineWeb 数据集的过程。所有主要的 LLM 提供商，如 OpenAI、Anthropic、Google 等，内部都会有类似 FineWeb 这样的数据集。那么我们在这里试图实现什么目标呢？我们试图从公开可用的来源获取大量互联网文本。

因此我们正致力于获取大量质量极高的文档资料。同时我们也追求文档内容的广泛多样性，因为这些模型需要吸纳海量知识。简而言之，我们既需要大批优质文档，又要求这些文档覆盖领域足够宽广。实现这一目标相当复杂。正如你在这里看到的，需要多个阶段才能做好。接下来，我们稍微看看其中一些阶段的具体情况。

目前，我只想指出，以 FineWeb 数据集为例——它相当能代表生产级应用中的典型数据规模——实际占用的磁盘空间仅为 44TB 左右。如今，一个 U 盘就能轻松存储 1TB 数据，甚至几乎可以装进一块单独的硬盘里。因此归根结底，这并非海量数据。尽管互联网规模极其庞大，但我们处理的是文本数据，并且进行了严格筛选。在此案例中，最终获得的数据量约为 44TB。

那么让我们来看看这些数据的大致样貌，以及其中一些阶段的具体内容。许多这类工作的起点，同时也是最终贡献大部分数据来源的，是来自 Common Crawl 的数据。Common Crawl 是一个自 2007 年起就在持续抓取互联网数据的组织。

截至 2024 年，例如 Common Crawl 已索引了 27 亿个网页。他们让众多网络爬虫在互联网上持续抓取。其基本运作原理是：从少量种子网页出发，然后追踪所有链接进行爬取。你只需不断追踪链接，持续索引所有信息，久而久之就会积累大量互联网数据。因此，这通常是许多此类工作的起点。不过，Common Crawl 的数据相当原始，需要经过多种方式的过滤处理。

因此，他们在此记录——这是同一张图表——略微阐述了这些阶段中发生的处理流程。首先是一个名为 URL 过滤的环节。这里指的是存在一些屏蔽列表，本质上就是你不希望从中获取数据的域名或 URL 清单。因此，通常这包括恶意软件网站、垃圾邮件网站、营销网站、种族主义网站、成人网站等类似内容。在这一阶段，我们会剔除大量此类网站，因为我们不希望它们出现在数据集中。第二部分是文本提取。

你必须记住，所有这些网页，这些爬虫保存的就是这些网页的原始 HTML 代码。所以当我在这里检查时，这就是原始 HTML 实际呈现的样子。你会注意到它包含所有这些标记，比如列表之类的东西，还有 CSS 以及所有这些内容。所以这几乎就是这些网页的计算机代码。但我们真正想要的只是这段文本，对吧？我们只需要网页的文本内容，而不需要导航栏之类的东西。因此，需要大量的过滤、处理和启发式方法，才能充分筛选出这些网页中的优质内容。

下一阶段是语言过滤。例如，FineWeb 会使用语言分类器进行过滤。他们会尝试猜测每个网页所使用的语言，然后只保留英语内容占比超过 65% 的网页。因此你可以理解，这就像不同公司可以自行决定的设计策略。我们要在数据集中涵盖多大比例的不同类型语言？举例来说，如果过滤掉所有西班牙语数据，你可能会想到，后续模型在处理西班牙语时表现不佳，因为它从未接触过足够多的该语言数据。不同公司对多语言性能的重视程度可以有所差异，这便是一个例子。

FineWeb 主要专注于英语。因此，如果他们后续训练语言模型，该模型在英语方面会非常出色，但在其他语言上可能表现不佳。经过语言过滤后，还会进行其他几步筛选步骤，包括去重等处理。例如以去除个人身份信息（PII）作为收尾工作。这类信息包括地址、社保号码等敏感数据。我们需要在数据集中检测此类内容，并将含有这类信息的网页过滤剔除。这里有很多步骤，我就不详细展开了，但这是预处理中相当重要的一部分。最终你会得到像 FineWeb 这样的数据集。点击进入后，你可以看到一些实际处理后的样例展示。

任何人都可以在 Hugging Face 网页上下载这些内容。以下是最终进入训练集的文本示例，这是一篇关于 2012 年龙卷风的文章。2012 年发生了一些龙卷风事件，接下来要讲的内容有点特别——你知道吗？人体内有两个像 9 伏黄色小电池大小的肾上腺。好吧，这算是一篇有点奇怪的医学文章。你可以把这些内容想象成互联网上经过各种方式筛选后仅保留文字的网页。

而现在我们拥有了海量文本数据，足足 40TB。这些数据正是当前阶段下一步工作的起点。此刻，我想让大家直观地了解我们目前所处的阶段。

于是我提取了这里的前 200 个网页——要知道我们手头有海量数据——把所有文本内容抓取出来拼接在一起。最终得到的就是这个：一段未经处理的原始文本，最原生态的网络文本。这段文本数据包含了所有这些模式。而我们现在要做的是开始在这些数据上训练神经网络，以便神经网络能够内化并建模文本的流动方式。

### Step 2: tokenization

所以我们手头有了这么一大段文本素材，现在需要构建能模仿它的神经网络。不过在将文本输入神经网络之前，我们必须先确定*如何表示这些文本以及如何输入*。当前我们的技术方案是：这些神经网络需要接收一维的符号序列，并且要求这些符号来自有限的预设集合。

因此我们必须先确定符号体系，再将数据表示为这些符号的一维序列。目前我们拥有的是一维文本序列——它从这里开始，延伸至此，接着转向此处，如此往复。

虽然这段文字在我的显示器上是以二维方式排列的，但它实际上是一个一维序列，从左到右、从上到下阅读，对吧？这就是一个一维的文本序列。既然是计算机处理，自然存在底层的数据表示。如果我用 UTF-8 编码这段文本，就能获得计算机中对应这些文本的原始比特数据。

它的呈现形式是这样的。举个例子，这里的第一根柱状图实际上代表前八位二进制数据。那么这到底是什么呢？从某种意义上说，这正是我们要寻找的数据表现形式。我们只有两种可能的符号，0 和 1，并且有一个非常长的序列，对吧？但实际上，这个序列长度在我们的神经网络中是一种非常有限且宝贵的资源，我们并不希望只有两个符号却生成极其冗长的序列。相反，我们需要在符号集（即词汇表）的大小与最终序列长度之间做出权衡。因此，我们不想仅用两个符号却得到超长的序列。

我们需要更多的符号和更短的序列。那么，一种简单的压缩或缩短序列长度的方法是：将连续的比特位（例如八位）组合成一个称为“字节”的单元。由于这些比特位只有开或关两种状态，如果我们取八位一组，实际上只存在 256 种可能的开关组合。因此，我们可以将这个序列重新表示为字节序列。这样字节序列的长度将缩短为原来的八分之一，但我们现在有 256 种可能的符号。这里的每个数字范围都是从 0 到 255。现在，我真心建议你们不要把这些当作数字，而是看作独特的 ID 或符号。或许更恰当的做法是……用独特的表情符号来替换每一个数字。这样你就会得到类似这样的结果。所以我们基本上有一个表情符号序列，共有 256 种可能的符号。你可以这样理解。但事实证明，在生产最先进的语言模型时，你实际上需要超越这个范围。

你希望继续缩短序列长度，因为这同样是一种宝贵的资源，用以换取词汇表中更多的符号。实现这一目标的方法是运行所谓的字节对编码算法。其工作原理是，我们本质上是在寻找那些频繁出现的连续字节或符号。例如，我们发现序列 116 后接 32 的情况非常普遍且频繁出现。因此，我们将把这组配对合并为一个新符号。具体来说，我们将创建一个ID为 256 的符号，并将所有 116、32 的配对替换为这个新符号。

然后我们可以根据需要多次迭代这个算法。每次生成新符号时，都会减少序列长度并增大符号规模。实践证明，将词汇表大小设定为约 10 万个可能的符号是较为理想的选择。具体而言，GPT-4 使用了 100,277 个符号。这种将原始文本转换为这些符号（我们称之为标记）的过程就称为*标记化(tokenization)*。现在让我们来看看 GPT-4 是如何执行标记化的——如何将文本转换为标记，又如何将标记转换回文本，以及这一过程实际呈现的效果。

有一个我常用来探索这些标记表示的网站叫 TickTokenizer。在这里的下拉菜单中选择 CL100K Base，这是 GPT-4 基础模型的标记器。左侧可以输入文本，它会显示该文本的标记化结果。例如，“hello world” 实际上恰好由两个 token 组成：一个是 ID 为 15,339 的 “hello” token，另一个是 ID 为 1,917 的 “ world” token。因此，“hello world” 就表示为 “hello world”。

现在，如果我要将这两个部分合并，比如，我会再次得到两个标记，但这次是标记 “h” 和 “elloworld”。如果我在 hello 和 world 之间加入两个空格，又会得到不同的标记化结果。这里会出现一个新标记 220。你可以自己尝试调整，看看会发生什么变化。另外请注意，这是区分大小写的。所以如果是大写的 H，那就是另一个东西了。或者如果是 "HELLO WORLD"，实际上这会变成三个token，而不仅仅是两个 token。没错，你可以通过这个工具来把玩体验，直观感受这些标记（token）的工作原理。视频后面我们还会再回过头来深入讲解标记化（tokenization）的部分。现在，我只是想先带大家看看这个网站。

我想向你展示的是，这段文字归根结底可以这样理解：例如，如果我在这里选取一行，GPT-4 将会这样解析它。这段文本将是一个长度为 62 的序列，具体序列如下所示。这就是文本块与这些符号的对应关系。同样地，这里共有 100,277 种可能的符号。现在我们得到的是这些符号的一维序列。

好的，我们稍后会再回到分词这个话题，但现在我们就讲到这里。那么，我现在所做的是：我选取了数据集中这段文本序列，并用我们的分词器将其表示为一个标记序列。这就是它现在的样子。

例如，当我们回到 FindWeb 数据集时，他们提到这不仅占据了 44TB 的磁盘空间，而且该数据集包含约 15T 个标记序列。这里展示的只是该数据集前几千个标记中的一小部分。但请记住，整个数据集实际上包含 15T 个标记。

### Step 3: neural network training

请再次记住，所有这些都代表小的文本片段。它们就像是这些序列的原子。这里的数字没有任何意义。它们只是唯一的标识符。好了，现在我们进入有趣的部分——*神经网络训练*。

这里正是训练神经网络时大量计算工作发生的核心环节。在此步骤中，我们的目标是建模这些标记在序列中如何相互跟随的统计关系。具体操作是：我们提取数据中的标记窗口进行分析。因此，我们会从这些数据中随机抽取一个 token 窗口。窗口的长度可以从零个 token 开始，一直延伸到我们设定的某个最大值。例如，在实际操作中，你可能会看到 8000 个 token 的窗口。

理论上，我们可以使用任意长度的 token 窗口。但处理非常长的窗口序列在计算上会非常昂贵。因此，我们通常会选择一个合适的数字，比如 8000、4000 或 16000，并在此处进行截断。在这个例子中，我将选取前四个标记以确保内容整齐呈现。我们将截取这四个标记——"I"、"View"、"ing" 和 " Single"（即这些标记 ID）作为一个四标记窗口。现在我们要做的，本质上是尝试预测这个序列中下一个即将出现的标记。

#### 3.1 网络 I/O

所以接下来是 3962，对吧？我们现在在这里做的就是将其称为上下文。这四个标记就是上下文，它们会输入到神经网络中。这就是神经网络的输入。现在，我将稍后详细讲解这个神经网络内部的构造。目前，重要的是理解神经网络的输入和输出。输入是可变长度的标记序列，长度范围从零到某个最大值，比如 8,000。

现在的输出是对接下来内容的预测。由于我们的词汇表包含 100,277 个可能的标记，神经网络将输出恰好对应这些标记数量的数值，每个数值都代表该标记作为序列中下一个出现的概率。因此，它是在对接下来出现的内容进行预测。

最初，这个神经网络是随机初始化的。稍后我们会具体解释这意味着什么。但本质上，它是一次随机变换。因此，在训练的最初阶段，这些概率值也具有一定的随机性。这里我举了三个例子，但请记住实际数据集中包含 10 万个数字。当前神经网络给出的概率显示，这个 " Direction" 标记的出现概率暂时被预测为 4%。11,799 的概率为 2%。而此处，3962（" Post"）的概率为 3%。当然，我们已从数据集中对该窗口进行了抽样。

因此我们已知道接下来的数字。我们知道——这就是标签——正确的答案是序列中接下来实际出现的数字是 3962。现在我们所掌握的是这个用于更新神经网络的数学运算过程。我们有办法调整它。稍后我们会详细讨论这一点。但基本上，我们知道这里 3% 的概率，我们希望这个概率能更高一些。我们希望所有其他标记的概率都更低。因此，我们有一种数学方法来计算如何调整和更新神经网络，使正确答案的概率略微提高。如果我现在对神经网络进行一次更新，下次当我将这组特定的四个标记序列输入神经网络时，经过微调的神经网络可能会将 " Post" 的概率输出为 4%。

" Case" 的概率可能是 1%。而 " Direction" 可能会变为 2% 或类似数值。因此，我们有一种微调方法，可以稍微更新神经网络，使其对序列中下一个正确标记给出更高的概率预测。现在我们需要记住的是，这一过程不仅仅发生在这里的这一个标记上——即这四个输入预测出这一个的情况。实际上，该过程会同时作用于整个数据集中所有标记。因此在实践中，我们会采样小窗口，即分批处理这些小窗口数据。

接着，在每一个这样的标记处，我们都要调整神经网络，使得该标记出现的概率略微提高。这一切都是在大批量标记的并行处理中完成的。这就是训练神经网络的过程。这是一个不断更新的过程，目的是让模型的预测结果与训练集中实际发生的统计数据相匹配。其概率分布会逐步调整，以符合数据中这些标记(token)相互跟随的统计规律。

#### 3.2 网络内部结构

接下来让我们简要探讨这些神经网络的内部机制，以便你对它们的运作原理有个基本认识。

神经网络内部机制。如前所述，我们接收的输入是 token 序列。本例中虽然只展示 4 个输入token，但实际可处理范围从零到约 8,000 个 token 不等。理论上，这可以是一个无限数量的标记。只是处理无限数量的标记在计算上过于昂贵。因此我们仅将其截断至特定长度，这就成为该模型的最大上下文长度。

现在，这些输入 $x$ 与神经网络参数（或称权重）在一个庞大的数学表达式中混合运算。此处我展示了六个参数示例及其设定值，但实际上现代神经网络会拥有数十亿个这样的参数。最初，这些参数是完全随机设定的。现在，在参数随机设置的情况下，你可能会预期这个神经网络会做出随机预测，事实也确实如此。一开始，它的预测完全是随机的。

但正是通过这种不断更新网络的迭代过程——我们称之为神经网络的训练——这些参数的设置得以调整，从而使神经网络的输出与训练集中观察到的模式保持一致。你可以把这些参数想象成 DJ 调音台上的旋钮。当你转动这些旋钮时，针对每个可能的标记序列输入，都会得到不同的预测结果。

训练神经网络，其实就是寻找一组与训练集统计数据相吻合的参数。现在，我举个实例展示这个庞大数学表达式的样貌，让你有个直观感受。现代神经网络很可能是包含数万亿项的巨型表达式。不过，让我在这里给你看一个简单的例子。它看起来大概是这样的。我是说，这些就是那种表达式，只是想告诉你它并不可怕。

我们有一些输入 $x$，比如 $x_1$、$x_2$，在这个例子中有两个示例输入，它们会与网络的权重 $w_0$、$w_1$、$w_2$、$w_3$ 等混合。这种混合涉及简单的数学运算，如乘法、加法、指数运算、除法等。神经网络架构研究的主题就是设计具有诸多便利特性的有效数学表达式。

它们表现力强、可优化、可并行化等等。但归根结底，这些并非复杂的表达式。本质上，它们通过将输入与参数混合来进行预测。我们正在优化这个神经网络的参数，以使预测结果与训练集保持一致。现在，我想向你们展示一个实际生产级别的神经网络示例。为此，我建议大家访问这个网站，它提供了这些神经网络非常直观的可视化效果。这就是您在本网站将了解到的内容。而这里应用于生产环境的神经网络具有这种特殊结构。该网络被称为 Transformer。

以这个具体模型为例，它大约包含 85,000 个参数。现在我们来看顶部结构：输入层接收词元序列作为输入，信息随后在神经网络中向前传播，最终输出层会生成经过 softmax 处理的逻辑值。但这些预测针对的是接下来会出现什么，即下一个标记是什么。在这里，有一系列转换过程，以及在这个数学表达式中生成的所有中间值，它们共同作用以预测后续内容。举例来说，这些标记会被嵌入到所谓的分布式表示中。

因此，每个可能的标记在神经网络内部都有一个代表它的向量。首先，我们将这些标记进行嵌入。然后，这些值会以某种方式在这个图中流动。这些单独来看都是非常简单的数学表达式。比如我们有层归一化、矩阵乘法、softmax 函数等等。这里大致就是这个 Transformer 中的注意力模块。

然后信息会流入多层感知器模块，依此类推。这里的这些数字都是表达式的中间值。你几乎可以把它们想象成这些合成神经元的放电频率。

但我得提醒你，别太把它当神经元来理解，因为这些与你大脑中的神经元相比极其简单。你体内的生物神经元是具有记忆等功能的非常复杂的动态过程，而这个表达式里可没有记忆这回事。这是一个从输入到输出固定不变的数学表达式，没有记忆功能，完全无状态。因此，与生物神经元相比，这些神经元非常简单。

但你可以大致将其视为一种人造的脑组织——如果你倾向于这样理解的话。信息流经所有这些神经元，直到我们得出预测结果。不过，我不会过多纠结于这些转换过程中精确的数学细节。老实说，我认为深入探讨这一点并不那么重要。真正需要理解的是，这是一个数学函数。它由一组固定参数所定义，比如大约 85,000个。

这是一种将输入转化为输出的方式。当我们调整参数时，会得到不同类型的预测结果。随后我们需要找到这些参数的合适配置，使预测结果能够与训练集中观察到的模式大致吻合。这就是 Transformer 模型。好了，我已经向你们展示了这个神经网络的内部结构，我们也稍微讨论了训练它的过程。

### Step 4: inference

接下来我想介绍使用这些网络的另一个主要阶段，那就是被称为推理的阶段。

因此在推理过程中，我们所做的是从模型中生成新数据。我们本质上想观察模型通过其网络参数内化了哪些模式。从模型生成数据的过程相对直接。我们从一些基本作为前缀的标记开始，比如你想要的起始内容。假设我们希望以标记 91 开头，那么我们就将其输入网络。

记住，网络给出的是概率，对吧？它在这里给出的是这个概率向量。所以我们现在可以做的，本质上就是抛一个有偏见的硬币。也就是说，我们可以基于这个概率分布来采样一个标记。例如，接下来是 token 860。因此，在这种情况下，当我们从模型生成时，860 可能会紧随其后。现在，860 是一个相对可能的 token。在这种情况下，它可能不是唯一可能的标记。可能还有许多其他标记可以被采样。但我们可以看到，860 作为一个例子是一个相对可能的标记。

事实上，在我们这个训练示例中，860 确实紧跟在 91 之后。那么现在让我们继续这个过程。也就是说，91 之后就是 860。我们附加它。我们再次询问，第三个 token 是什么？让我们取样。假设它是 287，就像这里一样。让我们再来一次。我们重新开始。现在我们有三个连续的 token。我们问，第四个可能的 token 是什么？然后我们从中抽样得到这个。现在假设我们再重复一次。我们取这四个。我们取样。然后我们得到了这个。而这个 13659，实际上并不是我们之前得到的 3962。所以这个 token 实际上是 token "Article"。因此，在这种情况下，我们并没有完全重现训练数据中看到的序列。

所以请记住，这些系统是随机的。我们在采样，我们在掷硬币。有时我们运气不错，能重现训练集中的一小段文本。但有时我们得到的标记并非训练数据中任何文档的逐字内容。所以我们将会得到类似于训练数据中的某种混音版本。因为在每个步骤中，我们都可以进行掷硬币，得到稍微不同的标记。一旦这个标记被采用，如果你继续采样下一个标记，以此类推，你很快就会开始生成与训练文档中出现的标记流完全不同的标记流。

从统计学上看，它们会具有相似的特性，但并不与训练数据完全相同。它们更像是受到训练数据的启发。因此在这个案例中，我们得到了一个略有不同的序列。

我们为什么会得到 “article” 这个词呢？你可能会认为，在 “|”、“viewing”、“single” 等词的上下文中，“article” 是一个相对可能出现的词。某种程度上，你可以想象在训练文档的某个地方，“article” 这个词跟随了这个上下文窗口。而我们恰好在这个阶段采样到了它。

简单来说，推理就是依次从这些概率分布中进行预测，我们不断反馈标记并获取下一个标记。整个过程就像不断掷硬币。根据我们运气的好坏，从这些概率分布中采样时，可能会得到截然不同的模式。这就是推理。

在大多数常见情况下，下载互联网数据并进行分词处理实际上是一个预处理步骤。然后，一旦你有了 token 序列，我们就可以开始训练网络了。在实际应用中，你会尝试训练许多不同的网络，它们有不同的设置、不同的排列方式和不同的大小。因此，你会进行大量的神经网络训练。

当你有了一个神经网络并训练它，并且得到一组令你满意的特定参数后，你就可以使用这个模型进行推理。实际上，你可以从模型中生成数据。当你在 ChatGPT 上与模型对话时，那个模型已经训练完成，可能是 OpenAI 在几个月前训练的。它们有一套特定的权重效果很好。当你与模型对话时，这一切都只是推理。不再有训练过程。这些参数是固定的。你基本上只是在和模型对话。你给它一些标记，它就会完成标记序列。这就是你在 ChatGPT 上实际使用该模型时所看到的内容生成过程。因此，该模型仅执行推理任务。

### GPT-2：训练和推理

那么现在让我们来看一个具体的训练和推理示例，这样你就能了解这些模型在训练时的实际运作情况。

现在我想讨论的例子，也是我特别喜欢的，就是 OpenAI 的 GPT2。GPT 代表 Generatively Pre-trained Transformer。这是 OpenAI 推出的 GPT 系列的第二代版本。今天当你与 ChatGPT 对话时，支撑这一神奇交互的底层模型正是 GPT4，也就是该系列的第四代版本。而 GPT2 则是 OpenAI 在 2019 年发表的成果，就是我现在手里拿的这篇论文。我之所以钟爱 GPT2，是因为它首次完整呈现了可辨识的现代技术架构。

按照现代标准来看，GPT2 的所有组件至今仍具有辨识度。只不过一切都变得更庞大了。当然，由于这是篇技术论文，我无法在此详述其全部细节。但我想重点强调的一些细节如下。GPT2 是一个 Transformer 神经网络，就像你今天会使用的神经网络一样。它有 1.6B 个参数，对吧？这些就是我们在这里看到的参数。如今，现代 Transformer 模型的参数量可能已接近万亿或数千亿。这里的最大上下文长度是 1,024 个标记。因此，当我们从数据集中采样 token 窗口的片段时，我们永远不会取超过 1,024 个 token。因此，当你试图预测序列中的下一个 token 时，你的上下文中永远不会有超过 1,024 个 token 来进行预测。这在现代标准下也是微不足道的。

如今，上下文长度已经大幅提升至数十万甚至可能达到百万级别。这样一来，历史记录中可容纳的上下文信息更多，标记数量也大幅增加。通过这种方式，你能够更准确地预测序列中的下一个标记。最后，GPT2 的训练数据大约是 100B 个标记。按现代标准来看，这个规模也相当小。正如我提到的，我们在这里研究的 fineweb 数据集有 1.5T 个标记，所以 100B 其实很少。实际上，我为了好玩，在这个名为llm.c 的项目中尝试复现 GPT2。你可以在 GitHub 上的 llm.c 仓库里看到我写的相关文章。具体来说，2019 年训练 GPT2 的成本估计约为 4 万美元。

但如今，你能做得比这好得多。具体来说，这次只花了一天时间和大约 600 美元。而且这还没怎么费劲。我觉得今天你可以把价格降到 100 美元左右。为什么成本下降这么多呢？首先，这些数据集的质量大幅提升。其次，我们筛选、提取和准备数据的方式也变得更加精细。因此，数据集的质量要高得多。这是一方面。但最大的不同在于，我们的计算机硬件速度大幅提升。

我们稍后会详细讨论这一点。此外，用于运行这些模型并尽可能从硬件中榨取所有速度的软件，随着大家都专注于这些模型并试图以极快的速度运行它们，这些软件也有了很大的改进。现在，我无法详细介绍这个 GPT2 的复现过程。这是一篇技术性很强的长文。但我想让你直观地感受一下，作为一名研究人员实际训练这些模型是什么样子。比如，你会看到什么？看起来是怎样的？感觉如何？让我来为你简单描绘一下。

好的，这就是它的样子。让我把这个滑过去。我现在正在做的是训练一个 GPT2 模型。这里发生的情况是，这里的每一行，比如这一行，都是对模型的一次更新。所以请记住，我们基本上是在为每一个标记改进预测。同时我们也在更新这些神经网络的权重或参数。

因此，这里的每一行都是对神经网络的一次更新，我们通过微调其参数，使其能更准确地预测下一个标记和序列。具体来说，这里的每一行都在提升对训练集中 1M 个标记的预测能力。也就是说，我们实际上是从这个数据集中提取了 1M 个标记来进行优化。我们试图同时改进对所有 1M 个标记的下一个标记的预测。在每一个步骤中，我们都会对网络进行相应的更新。现在，需要密切关注的一个数字就是这个叫做损失的值。

损失值是一个单一的数字，它告诉你神经网络当前的表现如何。这个数值的设计初衷是越低越好。因此，你会看到随着我们对神经网络进行更多更新，损失值在不断下降，这意味着对序列中下一个标记的预测会越来越准确。

因此，损失值就是作为神经网络研究者的你所关注的数字。你只能耐心等待，百无聊赖地消磨时间。你正在喝咖啡。同时你也在确保一切看起来不错，这样每次更新时，你的损失都在减少，网络的预测能力也在不断提高。现在，你可以看到我们每次更新处理 1M 个标记。每次更新大约需要 7 秒钟。这里我们将总共进行 32,000 步优化处理。所以，32,000 步，每步 1M 个标记，总共大约要处理 33B 个标记。而我们目前才进行到第 420 步，也就是 32,000 步中的 420 步。所以，我们只完成了略多于 1% 的工作量。

因为我只运行了大概 10 到 15 分钟。现在，每 20 步我都会配置这个优化进行推理。所以你现在看到的是模型在预测序列中的下一个标记。于是你有点随机地开始了。然后你继续填入标记。所以我们正在运行这个推理步骤。而这个模型就是在预测序列中的下一个标记。每次你看到有东西出现，那就是一个新的标记。让我们来看看这个。

你可以看出这还不够连贯。请记住，这只是训练进度的 1%。因此，模型在预测序列中的下一个标记方面还不够熟练。所以输出的内容其实有点像是胡言乱语，对吧？但它仍然保留了一些局部的连贯性。既然她属于我，那就是信息的一部分。应该讨论我的父亲、伟大的伙伴们，戈登让我坐在上面等等。

所以我知道这看起来不太好。但让我们向上滚动一下，看看我开始优化时的样子。回到第一步这里。经过 20 步优化后，你会发现我们得到的结果看起来完全是随机的。当然，这是因为模型只更新了 20 次参数。所以它给出的文本是随机的，因为它是一个随机网络。由此可见，至少与此相比，模型的表现正在变得更好。事实上，如果我们等待完整的 32,000 步训练过程，模型的改进程度会达到生成相当连贯英语的水平。生成的词汇流准确无误，整体英语表达也显得更加自然流畅。所以现在还需要再运行一两天。在这个阶段，我们只需要确保损失在减少。一切看起来都很顺利。

而我们只能等待。现在，让我来谈谈所需的计算过程。当然，我并不是在我的笔记本电脑上运行这个优化。那会太贵了。因为我们需要运行这个神经网络。而且我们还得改进它。我们需要所有这些数据等等。所以你在自己的电脑上无法很好地运行这个程序。因为网络实在太庞大了。这一切都在云端的计算机上运行。我想主要谈谈训练这些模型的计算方面及其具体表现。让我们来看看。

好的，我现在运行这个优化程序的电脑是一个 8xh100 节点。也就是说，一台节点或者说一台电脑里有八个 h100。目前这台电脑是我租来的。它就在云端的某个地方。实际上，我也不确定它的物理位置在哪里。我喜欢租用的地方叫 Lambda。但提供这项服务的公司还有很多。往下滑动页面，你会看到他们针对配备 H100 这类 GPU 的计算机提供了一些按需定价方案。稍后我会展示这些 GPU 的外观。但按需提供 8xh100 GPU。例如，这台机器每小时每个 GPU 收费 3 美元。因此，你可以租用这些设备。然后你在云端获得一台机器。你可以进入并训练这些模型。这些 GPU 看起来是这样的。所以这是一块H100 GPU。它大概长这样。你可以把它插进电脑里。

而 GPU 非常适合用于训练神经网络，因为它们需要极高的计算量。但这类计算能展现出高度的并行性。因此，你可以让许多独立的工作单元同时运作，共同解决神经网络训练背后涉及的矩阵乘法运算。所以这只是其中一块 H100 芯片。但实际上，你会把多块组合在一起。比如可以把八块堆叠成一个节点。然后，你可以将多个节点堆叠成整个数据中心或整个系统。因此，当我们观察数据中心时，就会开始看到类似这样的结构，对吧？从一个 GPU 扩展到八个 GPU，再到单个系统，再到多个系统。这些就是规模更大的数据中心。

当然，它们的价格会高得多。目前的情况是，所有大型科技公司都非常渴望获得这些 GPU，因为它们功能强大，可以用来训练各种语言模型。这从根本上推动了英伟达股价飙升至如今的 3.4 万亿美元，也是英伟达股价暴涨的原因。所以这就是淘金热。淘金热就是争抢 GPU，获取足够多的 GPU 让它们能够协同工作来完成这种优化。那它们都在做什么呢？它们都在协作预测像 FindWeb 数据集这样的数据集上的下一个标记。

这是极其昂贵的计算流程。GPU 越多，就能尝试预测和改进更多 token，处理数据集的速度也会更快。你可以更快地进行迭代，获得更大的网络，训练更大的网络，以此类推。这就是所有这些机器正在做的事情。这就是为什么这一切如此重要。

例如，这是一篇大约一个月前的文章。这就是为什么像埃隆·马斯克在一个数据中心获得 10 万块 GPU 这样的事如此重要。所有这些 GPU 都非常昂贵，将消耗大量电力。它们都只是在试图预测序列中的下一个标记，并通过这样做来改进网络。而且可能会比我们在这里看到的要快得多地生成更加连贯的文本。好吧，遗憾的是，我没有几千万或几亿美元来训练一个像这样真正庞大的模型。

但幸运的是，我们可以求助于一些大型科技公司，它们会定期训练这些模型，并在训练完成后发布部分模型。因此，它们投入了大量计算资源来训练这个网络，并在优化结束时发布该网络。因此这非常有用，因为他们为此进行了大量计算。所以有很多公司会定期训练这些模型。但实际上，其中发布所谓基础模型的公司并不多。

所以最终呈现的这个模型被称为基础模型。什么是基础模型？它本质上是个标记模拟器，对吧？一个互联网文本标记模拟器。就其本身而言，它目前还不具备实用价值。因为我们想要的是一种所谓的助手。我们希望能提出问题并得到回答。这些模型无法做到这一点。他们只是对互联网进行某种混音创作。他们梦想着网页。因此，基础模型并不经常发布，因为它们只是我们迈向智能助手所需的几个步骤中的第一步。

不过，已经有几个版本发布了。举个例子，GPT-2 模型在 2019 年发布了 1.5B 参数的版本。这个GPT-2 模型是一个基础模型。那么，什么是模型发布？发布这些模型是什么样的？这是 GitHub 上的 GPT-2 仓库。基本上，发布模型需要两样东西。第一，我们通常需要 Python 代码，详细描述模型中执行的操作序列。

所以如果你还记得这个 Transformer，这段代码描述的就是这个神经网络中所采取的步骤序列。这段代码实际上是在实现这个神经网络的所谓前向传播过程。因此我们需要确切了解他们是如何连接这个神经网络的具体细节。所以这只是一段计算机代码，通常也就几百行代码。没什么大不了的。这些代码都相当容易理解，而且通常相当标准。

不标准的是参数。真正的价值就在那里。这个神经网络的参数在哪里？因为有 1.5B 个参数，我们需要正确的设置或非常好的设置。因此，除了源代码之外，他们还发布了参数，在这个例子中大约是 1.5B 个参数。这些参数只是一串数字，也就是一个包含 1.5B 个数字的单一列表。

所有旋钮的精确和良好设置，以确保 token 输出良好。因此，你需要这两样东西来发布一个基础模型。现在，GPT-2 已经发布了，但正如我提到的，这实际上是一个相当老的模型。

### LLaMA-3.1 基础模型推理


实际上，我们要转向的模型叫做 LLaMA3。接下来我想向大家展示的就是它。GPT-2 的参数规模是 1.6B，训练数据量是 10B tokens。LLaMA3 是一个规模更大、更现代化的模型。它由 Meta 发布并训练，是一个拥有 405B 参数、基于 15T 标记训练的模型。同样地，只是规模要大得多。Meta 还发布了 LLAMA3，这也是这篇论文的一部分。

这篇论文详细介绍了他们发布的最大基础模型—— LLaMA3.1 405B 参数模型。这是基础模型。除此之外，正如视频后面部分会提到的，他们还发布了指令模型。指令意味着这是一个助手。你可以向它提问，它会给你答案。我们稍后还会讲到那部分。

目前，我们不妨先看看这个基础模型——这个标记模拟器，来把玩一番，试着思考：这究竟是什么？它是如何运作的？如果让它在海量数据上运行到极致，训练出一个庞大的神经网络，最终我们能得到什么？我个人最喜欢与基础模型互动的平台是 Hyperbolic 公司，他们主要提供 405B 参数的 LLaMA3.1 基础模型。当你进入网站时（可能需要注册等操作），请务必在模型选项中确认你选用的是 LLaMA3.1-405B 基础版，必须是基础模型。这里有个参数叫 "max tokens"，它决定了我们将生成多少个标记。

所以我们就稍微减少一点，以免浪费计算资源。我们只需要接下来的 128 个标记，其他的就不用管了。这里我就不详细解释了。现在，从根本上说，这里发生的事情与我们推理过程中发生的事情是一样的。所以这只会继续你给它任何前缀的标记序列。因此，我想先向你们展示，这里的这个模型还不是一个助手。

例如，你可以问它“二加二等于几？”，它不会直接告诉你“哦，等于四。还有什么可以帮你的吗？”，它不会这么做。因为“二加二等于几”会被分词处理。然后这些标记就充当了前缀。接下来模型要做的就是获取下一个标记的概率。说白了就是个高级的自动补全。这不过是一个非常、非常昂贵的自动补全功能，用于预测接下来的内容。它基于训练文档（基本上是网页）中看到的统计数据进行预测。所以，我们只需按下回车键，看看它会生成什么样的续写标记。

好的，实际上这里已经回答了问题，并开始进入一些哲学领域。让我们再试一次。我来复制粘贴一下。让我们从头再来一次。二加二等于几？好吧，它又自动重启了。所以我想再强调一点，这个系统每次输入后似乎都会从头开始运行。所以它不会，这里的系统是随机的。对于相同的 token 前缀，我们总是得到不同的答案。原因在于我们得到的是概率分布，并从中进行采样。

而我们总是得到不同的样本。之后我们总是会进入一个不同的领域。所以在这种情况下，我不知道这是什么。让我们再试一次。所以它就这样继续下去。它只是在做互联网上的那些事情，对吧？而且它有点像是在重复那些统计模式。首先，它还不是一个助手。它只是一个标记自动完成工具。其次，它是一个随机系统。

现在，关键之处在于，尽管这个模型本身对许多应用来说还不太实用，但它仍然非常有用，因为在预测序列中下一个标记的任务中，模型已经学到了很多关于世界的知识。所有这些知识都存储在网络的参数中。还记得我们的文本是这样的吗？互联网网页。

而现在，这一切在某种程度上都被压缩进了网络的权重中。所以你可以把这 405B 个参数看作是对互联网的一种压缩。你可以把这 405B 个参数想象成某种压缩文件，但它并不是无损压缩。这是一种有损压缩。我们留下的更像是互联网的完形，我们可以从中生成内容，对吧？现在，我们可以通过适当提示基础模型来引出其中一些隐藏的知识。例如，这里有一个提示，可能有助于引出隐藏在参数中的某些知识。

以下是巴黎必看十大景点的清单。我之所以这样做，是想引导模型继续完成这个列表。让我们看看按下回车键后是否有效。好的，你看到它已经开始列出清单，现在正在给我一些地标信息。我注意到它在这里试图提供大量信息。不过，你可能不能完全相信这里的一些信息。请记住，这一切只是对部分网络资料的回忆。因此，在网络数据中频繁出现的内容可能比那些极少出现的内容更容易被准确记住。所以你不能完全信任这里的一些信息，因为它只是对网络资料的模糊回忆。因为这些信息并未明确存储在任何参数中。一切都只是回忆。不过，我们确实得到了一些可能大致正确的东西。我其实没有专业知识来验证这是否大致正确。但你可以看到我们已经引出了模型的很多知识。而这些知识并不精确和准确。

这种知识是模糊的、概率性的和统计性的。经常发生的事情往往更容易在模型中被记住。现在，我想再展示几个这个模型的行为示例。

首先，我想向你展示这个例子。我去了维基百科的斑马页面。让我把第一句话复制粘贴到这里。让我把它放在这里。现在当我点击回车键时，我们会得到什么样的补全结果呢？让我按一下回车键。有三个现存物种，等等，等等。该模型在这里生成的内容是对维基百科条目的精确复述。它纯粹依靠记忆来背诵这段维基百科内容。而这些记忆都储存在它的参数中。因此，在这 512 个标记的某个时刻，模型可能会偏离维基百科的条目。你可以看到它在这里记住了大量的内容。让我看看，比如说，现在是否出现了这句话。好的，我们还在正轨上。让我确认一下。好的，我们还在正轨上。它最终会偏离。好吧，这东西在很大程度上只是被背诵。它最终会偏离，因为它无法准确记住。

现在，这种情况发生的原因是这些模型可能非常擅长记忆。通常，这并不是你在最终模型中想要的。这种现象被称为"反刍（regurgitation）"。而且通常不建议直接引用你训练过的内容。实际上，这种情况发生的原因在于，对于许多文档（比如维基百科），当这些文档被视为非常高质量的信息来源时，在训练模型的过程中往往会优先从这些来源采样。也就是说，模型很可能已经对这些数据进行了多次训练周期，这意味着它可能已经看过这个网页大约 10 次左右。

这有点像你，比如当你反复阅读某段文字很多很多次，比如说读了一百遍，那么你就能背诵它。这个模型也是如此。如果它看到某样东西太多次，它就能凭记忆背诵出来。但这些模型的效率可能比人类高得多，比如每次演示时。所以它可能只看了 10 次这个维基百科条目，但基本上它已经准确地把这篇文章记在了参数里。

好了，接下来我要展示的是这个模型在训练过程中绝对没有见过的东西。例如，如果我们查阅这篇论文并浏览预训练数据部分，会发现数据集的知识截止日期是 2023 年底。这意味着它不会包含此后发布的任何文档。当然，它也没有关于 2024 年选举及其结果的信息。现在，如果我们用未来的标记来启动模型，它将延续标记序列，并根据其自身参数中的知识做出最佳猜测。那么，让我们来看看这可能是什么样子。共和党的小子，特朗普。

好的，2017 年的美国总统。让我们看看接下来会说什么。例如，模型必须猜测竞选搭档以及对手是谁，等等。让我们按下回车键。这里列出了迈克·彭斯作为竞选搭档而非 J.D.万斯的情况。而当时的竞选对手是希拉里·克林顿和蒂姆·凯恩。所以这可能是警报预示的另一个有趣的平行宇宙。让我们换个样本试试。同样的提示，重新采样。所以在这里，竞选搭档是罗恩·德桑蒂斯，他们与乔·拜登和卡玛拉·哈里斯竞争。这又是一个不同的平行宇宙。因此，模型会做出有根据的猜测，并根据这些知识继续生成标记序列。

我们在这里看到的这些现象，其实就是所谓的"幻觉"。模型只是以概率的方式做出最佳猜测。接下来我想展示的是，尽管这只是一个基础模型，还不是一个助手模型，但如果你在提示设计上足够巧妙，它仍然可以应用于实际场景。

这就是我们所说的“few-shot 提示”。具体来说，我这里有 10 个单词或 10 对词组，每对都是一个英文单词后接冒号，然后是它的韩语翻译。我们总共有 10 组这样的对应关系。该模型的作用是，在最后我们会看到“teacher:”，然后在这里我们将进行一个仅包含五个标记的补全。这些模型具备我们所说的上下文学习能力。这指的是，当模型在读取这段上下文时，它会在过程中学习到数据中存在某种算法模式，并知道要继续遵循这种模式。这就是所谓的上下文学习。它扮演了翻译者的角色，当我们点击完成时，会看到老师被翻译为 "xxx"，这是正确的。由此可见，即使目前我们仅拥有基础模型，通过巧妙地设计提示词，也能构建出实用的应用程序。

它依赖于我们所谓的上下文学习能力，这是通过构建所谓的少样本提示（few-shot prompt）来实现的。最后，我想告诉大家，其实有一种巧妙的方法，仅通过提示就能实例化一个完整的语言模型助手。其诀窍在于，我们将设计一个看起来像网页的提示，其中包含一位乐于助人的 AI 助手与人类之间的对话。然后模型会继续这段对话。实际上，为了撰写提示词，我直接求助了 ChatGPT 本身，这有点 "meta" 的感觉——我告诉它：我想创建一个 LLM 助手，但我只有基础模型。所以你能帮我写提示词吗？这就是它给出的方案，说实话相当不错。

以下是 AI 助手和人类之间的一段对话。这位 AI 助手知识渊博、乐于助人，能够回答各种各样的问题，等等。然而，仅仅给出这样的描述是不够的。如果你创建这个少量示例提示，效果会好得多。这里有一些人类助手的术语，人类助手。我们还有一些对话轮次。最后在这里，我们将放入我们喜欢的实际查询。让我把这个复制粘贴到基础模型提示中。现在让我来做人类列部分。

这就是我们放置实际提示的地方。为什么天空是蓝色的？让我们运行助手。天空呈现蓝色是由于一种称为瑞利散射的现象，等等，等等。

所以你看，基础模型只是在延续序列。但由于这个序列看起来像对话，它就扮演了那个角色。不过这里有点微妙，因为它只是...你看，它结束了助手的部分，然后就开始幻想人类的下一个问题，诸如此类。所以它会一直持续下去。但你可以看到我们已经某种程度上完成了任务。如果你就拿这个来说，为什么天空是蓝色的？如果我们刷新一下并放在这里，当然我们不指望基础模型能处理这个，对吧？我们只是，谁知道会得到什么结果。

好的，我们还会遇到更多问题。那么，这是一种创建助手的方法，即使你可能只有一个基础模型。好了，这就是我们刚才几分钟讨论内容的简要总结。

现在，让我把视角拉远一点。这大致就是我们目前讨论的内容。我们希望训练像 ChatGPT 这样的大型语言模型助手。

### 预训练到后训练

我们已经讨论了第一阶段，即预训练阶段。我们看到，实际上这一阶段的核心在于：我们获取互联网文档，将其分解为这些标记（token）——这些小文本片段的基本单元，然后利用神经网络预测标记序列。整个这一阶段的输出就是这个基础模型。

这是在设置参数。而这个基础模型本质上是一个基于词元级别的互联网文档模拟器。因此，它能够生成具有与互联网文档相似统计特性的词元序列。我们发现它可以应用于某些场景，但实际上我们还需要做得更好。我们需要一个助手，能够提出问题并得到模型的回答。因此，我们现在需要进入第二阶段，即所谓的后训练阶段。于是，我们将基础模型——我们的互联网文档模拟器——交给后训练阶段进行处理。

接下来我们将探讨几种方法，用于对这些模型进行所谓的"训练后处理"。这些训练后处理阶段的计算成本将大幅降低。大部分计算工作、所有大型数据中心、以及所有重型计算设备和数百万美元的开支，都集中在预训练阶段。

但现在我们要进入一个成本稍低但依然极其重要的阶段——后训练阶段，将这个大语言模型转化为真正的助手。让我们来看看如何让模型不再简单检索网络文档，而是学会回答问题。换句话说，我们的目标是要开始构建对话思维。这些对话可以是多轮次的。也就是说，可以有多个回合，在最简单的情况下，就是人类和助手之间的对话。举个例子，我们可以想象对话可能是这样的。当人类问“2加2等于几”时，助手应该回答“2加2等于4”。如果人类接着问“如果把加号换成星号会怎样”，助手可以这样回应。同样地，这个例子也展示了助手可以带有某种个性，显得友善。而在第三个例子中，我展示了当人类提出我们不愿协助的请求时，我们可以给出所谓的拒绝回应。我们可以说我们对此无能为力。换句话说，我们现在想做的是思考一个助手应该如何与人类互动。我们想要在这些对话中编程助手及其行为。

现在，由于这是神经网络，我们不会在代码中明确编程这些内容。我们无法以那种方式对助手进行编程。因为这是神经网络，一切都是通过对数据集进行神经网络训练来完成的。正因如此，我们将通过创建对话数据集来隐式地训练这个助手。这里展示的是数据集中三个独立的对话示例。而实际的数据集——稍后我会给大家看具体例子——规模要大得多。它可以进行成千上万次多轮、冗长的对话，涵盖广泛的话题。但这里我只展示三个例子。其基本运作方式是通过示例来编程助手。这些数据是从哪里来的呢？就像2乘以2等于4，和2加2一样，诸如此类。这些是从哪来的？它们来自人类标注员。我们基本上会给人类标注员一些对话上下文，然后让他们给出在这种情况下理想助手应该给出的回答。人类会为助手在各种情境下写出理想的回答。然后我们将让模型以此为基础进行训练，模仿这类回答。具体操作方式是：我们将采用预训练阶段生成的基础模型，这个基础模型是基于互联网文档训练而成的。

### 后训练数据集（对话）

我们将舍弃现有的互联网文档数据集，转而采用一个全新的数据集——对话数据集。我们将基于这个全新的对话数据集继续训练模型。实际上，模型会迅速调整，并大致学会这个助手如何响应人类查询的统计规律。然后在后续推理过程中，我们基本上可以引导助手获得响应，它会模仿人类标注员，在这种情况下会采取的行动——如果这说得通的话。我们将看到这方面的示例，这个概念会变得更加具体。

我还想提到的是，在这个训练后阶段，我们基本上会继续训练模型，但预训练阶段实际上可能需要大约三个月的时间，在数千台计算机上进行训练。而后训练阶段通常会短得多，比如三个小时，这是因为我们将手动创建的对话数据集比互联网上的文本数据集要小得多。因此，这个训练会非常短，但从根本上说，我们只是拿基础模型，继续使用完全相同的算法、完全相同的所有东西进行训练，只不过我们把数据集换成了对话。

那么现在的问题是，这些对话在哪里，我们如何表示它们，如何让模型看到对话而不仅仅是原始文本，然后这种训练的结果是什么，当我们谈论模型时，从某种心理意义上你能得到什么。现在让我们转向这些问题。让我们从对话的分词开始讨论。这些模型中的所有内容都必须转化为标记，因为一切都与标记序列有关。那么问题来了，我们如何将对话转化为标记序列？

为此，我们需要设计某种编码方式，这有点类似于——如果你熟悉的话——比如互联网上的 TCP/IP 数据包（当然不了解也没关系）。信息的呈现方式、所有内容的组织结构都有精确的规则和协议，这样才能确保各类数据以书面形式清晰呈现，并获得所有人的认可。如今同样的情况也发生在大型语言模型中。我们需要某种数据结构，还需要制定规则来规范这些数据结构（比如对话）如何编码为标记，又如何从标记解码还原。

因此，我想向你们展示如何在 token 空间中重现这段对话。如果你打开 TickTokenizer，我可以提取这段对话，这就是它在语言模型中的表示形式。现在我们正在迭代用户和助手的两轮对话，虽然看起来有点杂乱，但实际上相当简单。

这段内容最终被转换为标记序列的方式有点复杂，但最终，用户与助手之间的这段对话会被编码为 49 个标记。这是一个由 49 个标记组成的一维序列，这些就是标记，明白吗？不同的语言模型会有略微不同的格式或协议，目前这方面还比较混乱，但以 GPT-4o 为例，它是这样处理的：使用一个名为im_start 的特殊标记，这是 "imaginary monologue of the start"（起始假想独白）的缩写。然后你必须指定……老实说，我其实不知道为什么这么叫。然后你必须指定轮到谁了。比如说用户，这是一个代号 1428。

然后你有一个内部独白分隔符，接着是确切的问题，也就是问题的标记，然后你需要结束它。所以 im_end ，即想象独白的结束。基本上，用户提出的“二加二等于多少”这个问题最终会变成这些标记的序列。现在要提到的重要一点是，im_start 并不是文本内容，对吧？它是一个额外添加的特殊标记。这是一个全新的标记，迄今为止从未参与过训练。它是我们在训练后阶段创建并引入的新标记。因此，这些特殊标记，如 im_sep、im_start 等，被引入并与文本交错排列，以便让模型学会识别：嘿，这是一轮对话的开始...是谁的回合开始呢？这是用户的回合开始。然后这是用户说的话，接着用户结束发言。然后是新的一轮对话开始，这次是助手的回合。
 
然后助手会说什么呢？这些就是助手所说的标记，诸如此类。于是这段对话就被转化成了这一连串的标记。这里的具体细节其实并不那么重要。我试图用具体的方式向你展示的是，我们原本视为某种结构化对象的对话，最终会通过某种编码转化为一维的标记序列。正因为这是一维的标记序列，我们就可以应用之前的所有方法。现在它只是一串标记序列，我们可以在此基础上训练一个语言模型。因此，我们依然只是在预测序列中的下一个标记，就像之前一样，同时我们也能对对话进行建模和训练。那么在推理阶段的测试过程中，这会是什么样子呢？假设我们已经训练好了一个模型，并且是在这类对话数据集上训练的，现在我们要进行推理。那么在推理过程中，当你在 ChatGPT 上时，这会是什么样子呢？嗯，你来到 ChatGPT，比如说，与它进行一段对话。

其运作方式大致是这样的：假设这里已经填好了内容。比如，2加2等于多少？2加2等于4。然后你发出指令，如果是乘法的话，就加上 “im_end”。而在 OpenAI 或类似平台的服务器上，基本上会这样处理：他们会在开头加上“im_start”，然后是“assistant”，接着“im_sep”，最后在这里结束。于是他们构建了这个上下文，现在开始从模型中采样。正是在这个阶段，他们会去询问模型：好的第一个序列应该是什么？合适的第一个词元是什么？第二个词元选什么好？第三个词元又该如何选择？这时大语言模型就会接管并生成响应，比如产生一个类似这样的回答。虽然不必完全一致，但如果数据集中存在这类对话，生成的回答就会带有这种风格特征。所以这就是协议的大致工作原理，虽然协议的细节并不重要。再次强调，我的目标只是向你展示，最终一切都归结为一维的 token 序列。因此，我们可以应用之前学到的所有内容，但现在我们是在对话上进行训练，并且基本上也在生成对话。

好的，现在我想谈谈这些数据集在实际中的应用。我要向你们展示的第一篇论文，也是这个方向上的首次尝试，就是 OpenAI 在 2022 年发表的这篇论文。这篇论文名为 InstructGPT，也就是他们开发的技术。这是 OpenAI 首次谈到如何利用语言模型并通过对话对其进行微调。这篇论文包含了许多细节，我想带大家一一了解。首先，我想从第 3.4 节开始，这部分讲述了他们雇佣的人类承包商——这些人员来自 Upwork 或通过 ScaleAI 招募——来构建这些对话内容。因此，这里会有人工标注员参与，他们的专业工作就是创建这些对话。这些标注员被要求提出提示，然后还要完成理想的助手回复。以下就是人们想出的这类提示。

所以这些都是人工标注员。那么列出五个重拾职业热情的点子。接下来我应该读哪十本科幻小说？这里有很多不同类型的提示。于是这里有很多人们想出来的东西。他们首先想出了提示，然后他们也回答了那个提示，并给出了理想的助手回应。那么他们如何知道针对这些提示应该写出怎样的理想助手回应呢？当我们继续往下滚动一点，就能看到这里提供给人工标注人员的标注指南节选。开发语言模型的公司，比如 OpenAI，会撰写标注指南，指导人类如何创建理想的回应。这里展示的就是这类标注指南的一个节选片段。

从高层次来看，你是在要求人们乐于助人、诚实守信、避免伤害。如果你想了解更多，可以暂停视频。但概括来说，基本就是回答问题时要尽力提供帮助，力求真实，不要回答那些我们不希望系统在后续 ChatGPT 对话中处理的问题。因此，大致来说，公司会制定标签说明。通常这些说明不会这么简短，通常会有数百页之多，人们需要专业地研究它们。然后他们根据这些标注说明写出理想的助手回应。正如这篇论文所述，这是一个非常依赖人工的过程。目前 OpenAI 实际上从未发布过 InstructGPT 的数据集。

但我们确实有一些开源项目在尝试遵循这种设置并收集自己的数据。比如，我熟悉的一个例子是之前Open Assistant 所做的努力。这只是众多例子中的一个。但我只是想给你看个例子。这些是网上被要求创作这类对话的人，类似于 OpenAI 让人类标注员做的工作。这里展示的是某人想出的这个提示的条目。你能写一篇关于“买方垄断”在经济学中相关性的简短介绍吗？请使用例子等说明。然后由同一个人，或可能是另一个人，来撰写回应。以下是助手对此的回复。然后，同一个人或不同的人会实际写出这个理想的回应。接着，这可能是对话如何继续的一个例子。现在，向一只狗解释它。

然后你可以尝试想出一个稍微简单一点的解释或类似的东西。现在这就变成了标签，我们最终会基于此进行训练。因此在训练过程中发生的情况是，我们当然无法覆盖模型在推理测试时会遇到的所有可能问题。我们无法涵盖人们未来可能提出的所有提示。但如果我们拥有一些这样的示例数据集，那么在训练过程中，模型就会开始呈现出这种乐于助人、诚实无害的助手形象。这一切都是通过示例编程实现的。因此，这些都是行为示例。如果你针对这些示例行为进行对话，并且数量足够多，比如 10 万条，然后进行训练，模型就会开始理解其中的统计模式，并逐渐呈现出这种助手般的个性。当然，在测试时遇到完全相同的问题时，答案可能会一字不差地复述训练集中的内容。

但更有可能的是，模型会做出类似感觉的回应，并理解这就是你想要的答案类型。这就是我们正在做的事情。我们通过示例来编程系统，系统在统计上采用了这种乐于助人、诚实无害的助手角色，这在一定程度上反映在公司创建的标注指南中。

现在我想告诉你的是，自 InstructGPT 论文发表以来的这两三年里，前沿技术已经有了相当的进步。具体来说，现在已经不太常见人类完全靠自己来完成所有繁重的工作了。这是因为我们现在有了语言模型，这些语言模型正在帮助我们创建这些数据集和对话。因此，人们很少会完全从零开始写出回答。更常见的情况是，他们会使用现有的 LLM 来生成答案，然后进行编辑或类似的处理。因此，现在 LLMs 已经开始以多种不同的方式渗透到这一后训练流程中。而大型语言模型（LLMs）基本上被普遍用于帮助创建这些庞大的对话数据集。我不想具体展示，但像 UltraChat 就是一个更现代的对话数据集的例子。在很大程度上它是合成的，但我相信其中也有一些人类的参与。我可能说错了。通常会有少量人工参与，但大部分是合成辅助。而且这些都是以不同方式构建的。

而 UltraChat 只是目前众多 SFT 数据集中的一个例子。我只想向你们展示的是，这些数据集现在拥有数百万次对话。这些对话大多是合成的，但很可能在一定程度上经过了人工编辑。它们涵盖了极其多样化的领域等等。如今，这些已成为相当广泛的成果。还有所有这些所谓的 SFT 混合物。所以你有各种各样的类型和来源的混合，部分是合成的，部分是人类生成的。之后的发展方向大致如此。但总的来说，我们仍然有监督微调（SFT）数据集。它们由对话构成。我们正在用这些对话进行训练，就像我们之前所做的那样。

最后我想说的是，我希望稍微消除一些与 AI 对话的神秘感。就像你去 ChatGPT 那里问一个问题，按下回车键后，返回的内容在某种程度上与训练集中的数据统计对齐。而这些训练集，说白了，不过是人类按照标注指令播下的一粒种子。那么，你实际上在和 ChatGPT 的什么对话呢？或者说，你该如何理解它？说白了，它并非来自某种神奇的 AI。这源于一种在统计上模仿人类标注者的机制，而这些标注者又遵循由这些公司编写的标注指南。因此，你某种程度上是在模仿这个过程。你几乎就像是在向人类标注者提问一样。想象一下，ChatGPT 给你的回答就像是对人类标注员的一种模拟。这有点像在问：在这种对话中，人类标注员会说什么？而且，这个人类标注员可不是随便从网上找来的普通人，因为这些公司实际上会聘请专家。比如，当你询问有关代码等问题时，参与创建这些对话数据集的人类标注员通常都是受过教育、具备专业知识的专家。你实际上是在向那些人的模拟版本提问，如果这说得通的话。所以你不是在和一个神奇的 AI 对话，而是在和一个普通的标注员交流。这个普通的标注员可能相当熟练，但你实际上是在和那种在构建这些数据集时会被雇佣的人的即时模拟版本对话。

在我们继续之前，让我再举一个具体的例子。比如，当我打开ChatGPT，输入“推荐巴黎最值得看的五个地标”，然后按下回车。好了，开始吧。好的，当我按下回车键时，这里会出现什么，我该怎么理解它呢？其实这并不是某种神奇的 AI，它并没有出去研究所有的地标，然后用它无限的智慧给它们排名等等。我得到的是 OpenAI 雇佣的一个标注员的统计模拟。你可以大致这样理解。

因此，如果这个具体问题恰好出现在 OpenAI 的训练后数据集中，我很可能会看到一个答案——这个答案大概率与人类标注员为那五个地标写下的内容高度相似。那么人类标注员是如何得出这些答案的呢？他们会去网上进行约 20 分钟的自主调研，然后列出一份清单。如果这份清单被收录进数据集，我看到的助手回复很可能就是他们提交的"标准答案"。但若该查询不在训练后数据集中，此刻生成的回答就更具涌现性——因为模型通过统计规律已理解到：训练集中出现的地标通常是知名景点、人们常去的热门地标，也是网络上被频繁讨论的地标类型。请记住，模型在互联网预训练阶段已经掌握了海量知识。它很可能见识过大量关于配对、地标以及人们喜闻乐见事物的对话内容。正是这种预训练知识与后续训练数据集的结合，才造就了这种模仿能力。所以，这就是你大致可以从统计学角度理解模型背后运作原理的方式。

### 幻觉，使用工具，知识/工作流记忆

#### 幻觉

好了，现在我想转向一个我称之为"大语言模型心理学"的话题，这涉及到我们为这些模型设计的训练流程所涌现出的认知效应。具体来说，首先要讨论的当然就是幻觉问题。

你可能听说过模型幻觉。这是指大语言模型凭空捏造信息的情况。它们会完全虚构事实，诸如此类。这也是大型语言模型助手面临的一个大问题。这个问题在多年前的早期模型中就已经在很大程度上存在了。我认为这个问题已经有所改善，因为接下来我将介绍一些缓解措施。目前，我们不妨先试着理解这些幻觉从何而来。这里有一个具体的例子，展示了三组你可能会认为存在于训练集中的对话。这些都是相当合理的对话，完全有可能出现在训练数据中。

比如说，汤姆·克鲁斯是谁？汤姆·克鲁斯是一位著名的演员，美国演员兼制片人等等。约翰·巴拉索又是谁？比如，他是一位美国参议员。成吉思汗是谁？成吉思汗嘛，就是那个什么什么。因此，这就是训练时对话可能呈现的样子。但问题在于，当人类为助手编写正确答案时，在每种情况下，人类要么已经知道这个人是谁，要么会上网搜索，然后写出这种带有自信口吻的回应。而实际测试时会发生什么呢？当你问一个完全是我编造的随机名字时——据我所知，这个人应该不存在——（助手也会给出类似的回答）。

我只是试着随机生成了一下。问题是当我们问谁是奥森·科瓦茨时，问题在于助手不会直接告诉你，哦，我不知道。即使助手和语言模型本身可能在它的特性里、在它的激活状态里、在它的大脑里（可以这么说），它可能知道这个人并不是它熟悉的对象。即使网络的某些部分在某种程度上知道这一点，说“哦，我不知道这是谁”的情况也不会发生，因为模型在统计上模仿了其训练集。在训练集中，类似“某某是谁”的问题都有明确的正确答案。因此，它会模仿回答的风格，并尽力给出最佳答案。它会给你统计上最可能的猜测。基本上就是在编造东西。因为这些模型，我们刚刚讨论过，它们无法访问互联网。他们不是在搞研究。这些就是我所说的统计符号搅拌器。它们只是在试图对序列中的下一个符号进行采样。它基本上会凭空捏造内容。

那么，让我们来看看这是什么样子。我这里有一个来自 Hugging Face 的推理演示平台。我特意选了一个叫 Falcon 7B 的模型来说事，这是个老模型了。那是几年前的事了，所以它是个比较旧的模型。所以它会产生幻觉。正如我提到的，最近这方面已经有所改善。但话说回来，奥森·科瓦茨是谁？让我们问问 Falcon 7B  导师吧。跑。哦，是的。Orson Kovats是一位美国作家和科幻小说作家。好吧。这完全是假的。这是一种幻觉。让我们再试一次。这些都是统计系统，对吧？所以我们可以重新取样。这次奥森·科瓦茨是这部 20 世纪 50 年代电视剧中的虚构角色。这完全是胡说八道，对吧？我们再试一次。他以前是个小联盟棒球运动员。好吧。所以基本上模型并不知道。它给出了很多不同的答案，因为它并不知道。这就像是从这些概率中进行采样一样。该模型从识别 "谁是Orson Kovats的助理" 这些标记开始。然后进入这个环节。此时它正在计算这些概率。它只是从概率中进行抽样，然后产生内容。而这些内容在统计上与其训练集中答案的风格是一致的。它只是在执行这个操作。但你和我却将其体验为一种虚构的事实知识。但请记住，模型本质上并不知情。它只是在模仿答案的格式。它不会去查找答案。因为它只是在模仿答案而已。

那么，我们该如何缓解这种情况呢？举个例子，当我向 ChatGPT 提问“奥森·科瓦茨是谁？”时，我询问的是 OpenAI 最先进的模型。这个模型会告诉你答案。哦。所以这个模型实际上更聪明。因为你刚才看到它很快就显示“正在搜索网络”。我们稍后会详细讲解这一点。它实际上是在尝试使用工具。而且有点像编了个故事。但我想说的是，Orson Kovats 这个人根本没有使用任何工具。我不想让它进行网络搜索。有一位著名的历史公众人物叫 Orson Kovats。所以这个模型不会编造内容。这个模型知道自己不知道。它会告诉你，它似乎不认识这个人。所以某种程度上我们算是改进了幻觉问题。

尽管在旧模型中这些问题显而易见。如果你的训练集就是这样，那么得到这类答案也完全在情理之中。那么我们该如何解决这个问题呢？好吧。显然，我们的数据集中需要一些例子，在这些例子中，助手的正确答案应该是模型不知道某些特定事实。但我们只需要在模型确实不知道的情况下产生这些答案。那么问题来了，我们如何知道模型知道或不知道什么？嗯，我们可以通过实证探究模型来弄清楚这一点。

那么，就以 Meta 如何处理 Llama 3 系列模型的幻觉问题为例来看看。在他们发表的这篇论文中，我们可以深入探讨幻觉现象——他们称之为"事实性"。文中描述了通过质询模型来判定其知识边界的方法，即弄清模型掌握和未掌握的内容。然后他们会在训练集中添加一些例子，针对那些模型不知道的事物，正确的答案是模型不知道它们。这听起来在原则上是一件非常简单的事情。但这样做大致上解决了问题。之所以能解决问题，是因为请记住，模型实际上可能在网络内部对自己的知识有一个相当不错的模型。

请记住，我们观察了网络及其内部的所有神经元。你可能会想象网络中某个神经元会在模型不确定时“亮起”。但问题在于，目前该神经元的激活并未与模型实际用语言表达“不知道”相关联。因此，尽管神经网络内部知道答案（因为有某些神经元在表征这些信息），模型并不会直接呈现出来。相反，它会给出最可能的猜测，让它听起来很自信，就像它在训练集中看到的那样。所以我们需要从根本上询问模型，允许它在不知道的情况下说“我不知道”。

让我来介绍一下 Meta 大致的功能。简单来说，他们的运作方式是——我这里有个例子。今天的特色文章是关于多米尼克·哈谢克的，我就是随机点进去的。他们所做的，基本上就是从训练集中随机选取一份文档，摘取其中一段，然后利用大语言模型（LLM）针对该段落生成问题。举个例子，我刚刚就用 ChatGPT 这样操作过——我输入"这是文档中的某段内容"，让它据此提问。

根据这段文字生成三个具体的事实性问题，并提供问题和答案。因此，大型语言模型已经足够擅长创建和重新组织这些信息。所以，如果信息在该大型语言模型的上下文窗口中，这实际上效果相当不错。它不必依赖记忆，答案就在上下文窗口中。因此，它基本上可以相当准确地重新组织这些信息。例如，它可以为我们生成这样的问题：他为哪个球队效力？这就是答案。他赢了多少个冠军杯？诸如此类。现在我们手头有一些问答环节，接下来我们要对模型进行提问。简单来说，我们的操作流程是：把问题输入到模型中——比如 Meta 公司的 Llama 模型——然后获取答案。

但让我们以 Mistral7b 为例进行询问。这是另一个模型。那么这个模型知道这个答案吗？我们来看看。所以他效力于布法罗军刀队，对吧？模型是知道的。从编程角度决定的方式，基本上就是我们会获取模型的这个答案，然后将其与正确答案进行比对。再说一次，这些模型已经足够先进，可以自动完成这一过程。

所以这里没有人类参与。我们可以直接从模型中获取答案，然后用另一个大型语言模型评判器来检查这个答案是否正确。如果答案正确，那就意味着模型很可能知道答案。所以我们要做的就是，我们可能会重复几次。好了，它知道这是布法罗军刀队。让我们再试一次。水牛城军刀队。让我们再试一次。水牛城军刀队。所以我们问了三次这个事实性问题，模型似乎都懂。所以一切都很顺利。

问题，他赢得了多少次斯坦利杯？让我们再次询问模型这个问题。正确答案是两次。但在这里，模型声称他赢了四次，这是不正确的，对吧？与两次不符。所以模型并不知道，它只是在编造。让我们再试一次。所以这里模型又像是在编造，对吧？让我们再试一次。这里说他在职业生涯中甚至没有赢过。所以显然模型并不知道。而我们通过编程方式判断的方法，还是同样地，我们询问模型三次，并将它的答案与正确答案进行比较，可能是三次、五次，无论多少次。

如果模型不知道，那么我们就知道模型不知道这个问题。然后我们会把这个问题加入训练集，创建一个新的对话。也就是说，我们将在训练集中添加一个新的对话。当问题是“他赢得了多少座斯坦利杯？”时，回答会是“抱歉，我不知道”或“我不记得了”。这正是这个问题的正确答案，因为我们测试过模型，发现情况确实如此。如果你针对多种不同类型的问题和文档进行这种测试，就是在给模型一个机会，让它能够根据自身掌握的知识，在训练过程中学会拒绝回答。

如果你在训练集中有几个这样的例子，模型就会知道，并有机会学习这种基于知识的拒绝与网络中某个内部神经元（我们假设其存在）之间的关联。从经验来看，这种情况很可能确实存在。模型可以学会这种关联——当这个表示不确定性的神经元活跃度高时，实际上就意味着"我不知道，抱歉，我记不太清了，诸如此类的话我是可以说的。如果你的训练集中有这些例子，那么这对减少幻觉有很大帮助。大致来说，这就是为什么ChatGPT也能做到这样的事情。

因此，这些都是人们已经实施并随着时间的推移改善了事实性问题的缓解措施。好的，我已经描述了第一种缓解措施，基本上是为了减轻幻觉问题。现在，我们实际上可以做得比这更好。

#### 使用工具

与其只是简单地说我们不知道，我们可以引入第二种缓解措施，让大语言模型有机会基于事实来回答问题。那么，如果我问你一个事实性问题而你并不知道答案，你会怎么做？你会采取什么方式来回答这个问题？你可能会去搜索一下，*利用互联网找到答案*，然后告诉我。我们也可以让这些模型做同样的事情。

所以，请将神经网络内部的知识，即其数十亿参数中的信息，想象成模型在很久以前的预训练阶段所见过事物的模糊记忆。把这些参数中的知识，当作你一个月前读过的东西来理解。

如果你持续阅读某样东西，你就会记住它，模型也会记住。但如果是罕见的内容，你可能就不会对那信息有很好的记忆。而你我做的只是去查找它。现在，当你去查找时，你基本上是在用信息刷新你的工作记忆，然后你就能提取它、谈论它等等。因此，我们需要某种等效的方法让模型能够刷新它的记忆或回忆。我们可以通过为模型*引入工具*来实现这一点。

因此，我们的解决思路是：与其简单地说"抱歉，我不知道"，不如尝试借助工具。我们可以设计一种机制，让语言模型能够输出特殊标记——这些是我们即将引入的全新标记。例如，在这里我引入了两个标记，并为模型如何使用这些标记定义了一种格式或协议。举例来说，当模型不知道答案时，它不再只是简单地说“我不知道，抱歉”，而是可以选择发出特殊标记 `<SEARCH_START>`，这个查询会被发送到像 OpenAI 使用的 bing.com 或 Google 搜索等平台。然后模型会发出查询内容，接着再发出 `<SEARCH_END>` 标记。

然后这里会发生的是，当运行推理的模型采样程序看到特殊标记“搜索结束”时，它不会继续采样序列中的下一个标记，而是会暂停从模型生成内容。它会转而启动一个与 bing.com 的会话，将搜索查询粘贴到必应中，然后获取检索到的所有文本内容。基本上，它会获取这些文本，可能会用其他特殊标记重新表示这些内容，然后将这些文本复制粘贴到这里，就像我试图用括号展示的那样。所有这些文本都会汇集到这里，当文本到达这里时，它就会进入上下文窗口。

因此，这个模型将网络搜索的文本纳入到上下文窗口中，这些内容随后会被输入神经网络。你可以把上下文窗口想象成模型的工作记忆区。模型可以直接访问上下文窗口内的数据。它直接输入到神经网络中。因此，它不再是一个模糊的记忆。这是模型在上下文窗口中拥有的数据，可以直接使用。因此，现在当它在此后采样新标记时，可以非常轻松地引用已复制粘贴到那里的数据。这就是这些工具大致的工作原理。而网络搜索只是这些工具之一。我们稍后会看看其他一些工具。但基本上，你需要引入新的标记，引入一些模式，让模型可以利用这些标记并调用这些特殊功能，比如网络搜索功能。那么，你如何教会模型正确使用这些工具呢？比如网络搜索开始、搜索结束等等。

那么，同样地，你需要通过训练集来实现这一点。所以我们现在需要大量数据和大量对话，通过这些示例向模型展示如何使用网络搜索。那么，在哪些场景下你会使用搜索？具体是什么样子的？这里通过示例展示如何开始搜索、结束搜索等等。

如果你在训练集中有几千个这样的例子，模型实际上会很好地理解这个工具的工作原理。它还会知道如何构建查询结构。当然，由于预训练数据集及其对世界的理解，它实际上对网络搜索是什么有一定的理解。因此，它实际上对什么是好的搜索查询有着相当不错的原生理解。所以这一切就像自然而然就能运作一样。你只需要几个例子来展示如何使用这个新工具。然后它就可以依靠这个来检索信息，并将其放入上下文窗口中。这相当于你我在查阅资料。因为一旦信息进入上下文，它就进入了工作记忆，非常容易操作和访问。

所以我们刚才在 ChatGPT 上搜索 “谁是Orson Kovats” 时看到的就是这个。ChatGPT 的语言模型认为这是一个罕见的人物之类的东西。它没有从记忆中给出答案，而是决定采样一个特殊标记来进行网络搜索。我们刚才看到有东西一闪而过，像是用了某个网页工具之类的。它很快就显示了那句话，然后我们等了两秒左右，就生成了这个。你看它正在这里创建参考文献。

所以它是在引用来源。这里发生的情况是，它启动了一次网络搜索，找到了这些来源和网址，而这些网页的文本内容都被塞在了这里。虽然这里没有显示出来，但基本上就是以文本形式塞在中间的。现在它看到了这段文字，并开始参考它，说：好吧，可能是这些人引用的，也可能是那些人引用的，等等。这就是这里发生的情况。

所以当我问谁是 Orson Kovats 时，我也可以说，不要使用任何工具。这样基本上就足以说服 ChatGPT 不使用工具，而是依靠自己的记忆和回忆来回答问题。我还特意去问了 ChatGPT 这个问题：“多米尼克·哈谢克赢得过多少次斯坦利杯？”而 ChatGPT 确实认为自己知道答案，并有信心地回答说哈谢克赢过两次。

#### 知识/工作流记忆

因此，它某种程度上只是依赖自己的记忆，因为大概它对自身的权重、参数和激活函数有足够的信心，认为这些信息可以直接从记忆中检索出来。但反过来，你也可以使用网络搜索来确认。然后对于同样的查询，它实际上会去搜索，找到一堆来源，所有这些内容都会被复制粘贴进去，然后它再次告诉我们答案并附上引用来源。

维基百科文章实际上也提到了这一点，这也是我们获取这一信息的来源。这些工具包括网络搜索，模型会自行决定何时进行搜索，大致就是这些工具的工作原理。这也能在一定程度上减少幻觉和事实性错误的发生。因此，我想再次强调这个非常重要的心理学观点。神经网络参数中的知识是一种模糊的记忆。构成上下文窗口的知识和标记才是工作记忆。

粗略地说，它的运作方式与我们大脑中的机制类似。我们记住的内容相当于参数，而刚刚经历的事情——比如几秒或几分钟前发生的——则可以被视为上下文窗口中的信息。这个上下文窗口会随着你对周围环境的感知体验不断构建起来。

因此，这对你在实践中使用 LLM 也有很多影响。比如，我可以去 ChatGPT，然后我可以这样做。我可以说，你能总结一下简·奥斯汀的《傲慢与偏见》的第一章吗？这是一个完全合理的提示，chatgpt 实际上在这里做了一个相对合理的回答。chatgpt 之所以能做到这一点，是因为它对《傲慢与偏见》这样的名著有相当不错的记忆。它可能已经看过大量关于这本书的资料，很可能还存在专门讨论这本书的论坛。你可能读过这本书的不同版本。这有点像记忆，因为即使你读过这本书或相关文章，你也会有足够的回忆来复述这些内容。但通常当我真正与大型语言模型互动并希望它们记住特定内容时，直接提供信息总是效果更好。

所以我认为一个更好的提示应该是这样的。你能为我总结一下简·奥斯汀的《傲慢与偏见》第一章吗？然后我在下面附上供你参考。接着我会在这里放一个分隔符，然后把内容粘贴进去。我发现直接从网上找到的内容复制粘贴过来就行。所以这里复制粘贴第一章的内容。我这么做是因为当内容在上下文窗口中时，模型可以直接访问它，不需要回忆就能准确获取。它可以直接访问这些信息。因此，这份摘要的质量预计会明显更高，或者比另一份摘要更好，因为它可以直接被模型获取。我想你和我也会以同样的方式工作。如果你想的话，在总结这一章之前重新阅读一遍，你会写出更好的摘要。这基本上就是这里发生的情况，或者说相当于这种情况。

### 自我认知

接下来我想简单谈谈的另一种心理怪癖，就是自我认知。

我在网上经常看到人们这样做。他们会问大语言模型一些问题，比如“你是什么模型？谁创造了你？”其实这个问题有点无厘头。我之所以这么说，是因为正如我之前试图解释的一些底层原理那样，这个东西并不是一个人，对吧？它在任何意义上都没有持续存在的实体。它有点像启动、处理 token 然后关闭。而且它对每个人都这样做。它只是构建一个对话的上下文窗口，然后所有内容都会被删除。因此，这个实体在每次对话中几乎都是从零开始重启的，如果这么说能让你理解的话。它没有持久的自我意识，不存在自我感。它就像一个符号搅拌器，遵循着其训练数据中的统计规律。

所以问它“你是谁”、“谁创造了你”之类的问题其实没什么意义。默认情况下，如果你按照我刚才描述的方式操作，凭空提问的话，会得到一些相当随机的答案。比如我们拿 Falcon 这个比较旧的模型来举例。

让我们看看它告诉我们什么。它在回避问题，有才华的工程师和开发者。这里写着，我是由 OpenAI 构建的。基于 GPT-3 模型。它完全是在编造内容。现在，既然它是由 OpenAI 构建的，我想很多人会把这当作证据，认为这个模型是以某种方式在 OpenAI 数据上训练的，或者类似的情况。我并不认为这一定是真的。原因是，如果你没有明确地编程模型来回答这类问题，那么你得到的将是它对答案的统计最佳猜测。而这个模型的监督微调数据混合了对话内容。

在微调过程中，模型通过训练数据逐渐理解自己正在扮演这种乐于助人的助手角色。它并不清楚——实际上也没有被明确告知——该给自己贴上什么标签。它只是自然而然地呈现出这种助手的形象。

请记住，预训练阶段使用了来自整个互联网的文档，而 ChatGPT 和 OpenAI 在这些文档中非常突出。因此，我认为这里实际发生的情况很可能是，这只是它对自身身份的幻觉标签。它本身的身份就是 OpenAI 的 ChatGPT。它之所以这么说，是因为互联网上有大量类似这样的回答数据，实际上都来自 OpenAI 的 ChatGPT。因此这就是它对此类内容的标签定义。不过作为开发者，你可以自行覆盖这个设置。如果你有一个 LLM 模型，你实际上可以覆盖它。有几种方法可以实现这一点。


例如，让我展示给你看，allenai 有这个 Olmo 模型。这是一款大语言模型（LLM）。它并非顶级大模型之类的，但我喜欢它是因为它完全开源。Olmo 的论文和所有相关资料都是完全开源的，这很棒。现在我们来看它的 SFT 混合数据集。这是用于微调的数据组合，也就是对话数据集，对吧？他们为 Olmo 模型设计的解决方案中，我们看到混合数据里包含多种内容，总计有 100 万条对话记录。

但这里我们硬编码了 Olmo2。如果我们去那里，我们会看到这是 240 个对话。看看这 240 个对话。它们是硬编码的。用户说：介绍一下你自己吧。然后助手回答：我是Olmo，一个由 AI2（艾伦人工智能研究所）等开发的开源语言模型。我来帮忙，巴拉巴拉。你叫什么名字？Olmo2。这些都是关于 Olmo2 的各种预设问题和在这种情况下应该给出的正确答案。如果你将 240 个类似的问题或对话放入训练集并进行微调，那么模型之后确实会模仿这些内容。如果你不提供这些数据，那很可能是 OpenAI 的某种默认行为。

还有一种方法有时也能实现这一点，就是在这些对话中，人类和助手之间会有一些术语，有时对话的最开始会有一条特殊的系统消息。

所以这不仅仅是人与助手之间的互动，背后还有一套系统。在系统消息中，你可以直接硬编码并提醒模型：嘿，你是由 OpenAI 开发的模型，名字叫 chatgpt4o，你的训练日期是这个，知识截止日期是这个。这基本上就像是给模型做了一点文档记录，然后这些信息会被插入到你们的对话中。所以当你使用 chatgpt 时，会看到一个空白页面，但实际上系统消息是隐藏在那里的，这些标记就在上下文窗口中。

这就是两种让模型谈论自身的方式：要么通过这样的数据实现，要么通过系统消息之类的方式完成。基本上是一些在上下文窗口中不可见的标记，它们提醒模型自己的身份。但这一切都像是某种程度上的临时拼凑和强行附加。实际上，它并不像人类那样在真正意义上深刻存在。

### 模型需要 token 去思考

我想现在继续讨论下一部分，这部分涉及这些模型在解决问题场景中的计算能力，或者更准确地说，是它们固有的计算能力。因此，我们在构建对话示例时必须格外小心。这里有很多需要特别注意的地方，这些问题某种程度上已经被阐明。

当我们思考这些模型如何思考时，它们看起来还挺有趣的。所以请考虑以下来自人类的提示。假设我们正在构建一段对话，准备将其加入我们的对话训练集。

所以我们要用这个来训练模型。我们正在教它如何基本解决简单的数学问题。提示是：艾米丽买了三个苹果和两个橙子。每个橙子 2 美元。总花费是 13 美元。苹果的花费是多少？非常简单的数学问题。现在左边和右边有两个答案。它们都是正确答案。它们都说答案是三，这是正确的。

但对于助手来说，这两个答案中有一个明显比另一个更好。假设我是数据标注员，正在创建这样的答案，其中一个对助手来说会是非常糟糕的回答，而另一个则勉强可以。所以我希望你能暂停视频，思考一下为什么其中一个答案比另一个好得多。

如果你使用了错误的模型，它可能在数学方面表现非常糟糕，导致不良结果。在培训人员为助手创建理想回复时，这一点需要在标注文档中格外注意。这个问题的关键在于要认识到并记住：无论是训练还是推理时，模型都是在从左到右处理一维的标记序列。

这就是我脑海中经常浮现的画面。我基本上想象的是从左到右演变的标记序列。为了始终生成序列中的下一个标记，我们将所有这些标记输入到神经网络中。然后这个神经网络会给出序列中下一个标记的概率，对吧？所以这里的这张图和我们之前在上面看到的完全一样。这来自我之前展示的网络演示，对吧？所以这个计算基本上会接收顶部的输入标记，执行所有这些神经元的操作，并给出下一个标记的概率答案。现在，重要的是要认识到，大致来说，这里发生的计算层数基本上是有限的。

例如，这里的这个模型只有一、二、三层所谓的注意力机制和 MLP（多层感知机）。而一个典型的现代最先进网络可能会有大约 100 层或类似的结构，但从之前的标记序列到下一个标记的概率，只有大约 100 层的计算量。因此，对于每一个标记，这里发生的计算量是有限的。你应该把这看作是非常少量的计算。而且这个计算量对于序列中的每个标记来说几乎是固定的。虽然这并不完全正确，因为输入的标记越多，神经网络的前向传递成本就越高，但增加得并不多。

所以你应该考虑这一点，我认为这是一个值得牢记的好模型，对于每一个 token 来说，这个盒子里的计算量是固定的。而且这个计算量不可能太大，因为这里的层数并不多，从上到下没有那么多计算会发生。

因此，你无法想象模型仅通过一次前向传递就能完成任意计算来生成单个标记。这意味着我们实际上必须将推理和计算分散到多个标记上，因为每个标记只能承载有限的计算量。因此，我们希望将计算任务分摊到多个标记上，而不能指望模型在单个标记上完成过多计算，因为每个标记能承载的计算量是有限的。

好的，这里大致是固定的计算量。所以这就是为什么这个答案明显更差。原因在于想象一下从左到右移动，我在这里直接复制粘贴了它。答案是三，等等。想象一下模型必须从左到右，一次一个地发出这些标记。它必须说，或者说我们期望它说的是，答案是空格美元符号。然后在这里，我们期望它基本上将所有关于这个问题的计算都压缩到这个单一的标记中。它必须输出正确答案三。一旦我们输出了答案三，我们期望它会说出所有这些标记。但到了这一步，我们已经得出了答案，并且这个答案已经存在于后续所有标记的上下文窗口中。

因此，这里的一切都只是在事后解释为什么这个答案是正确的。因为答案早已生成。它已经在 token 窗口中了。所以这里实际上并没有进行计算。因此，如果你直接且立即回答问题，你就是在训练模型试图用一个 token 来猜测答案。而这正是因为每个 token 的计算量是有限的，所以那种方式行不通。右边的答案之所以明显更好，是因为我们将计算分散在整个回答过程中。实际上，我们是在让模型逐步得出答案。从左到右，我们得到了中间结果。也就是说，橙子的总成本是四，所以 13 减去 4 等于 9。

因此，我们正在创建中间计算。这些计算中的每一个本身并不那么昂贵。实际上，我们基本上是在某种程度上猜测模型在这些单个标记中能够处理的难度。从计算角度来看，这些标记中的任何一个都不可能有太多工作，否则模型在测试时就无法完成。因此，我们在这里教导模型将其推理和计算分散到各个标记上。这样一来，每个标记只需处理非常简单的问题，而这些简单问题可以累积起来。到了接近尾声的时候，它已经将所有之前的结果都存储在工作记忆中。这时它更容易确定答案，看，答案就是3。

因此，这对我们的计算来说是一个明显更好的标签。这将会非常糟糕。而且这是在教模型尝试将所有计算都集中在一个标记中完成。这真的很不好。所以，这是一个值得记住的有趣现象：在你的提示中，通常你不需要明确考虑这一点，因为 OpenAI 的工作人员有标注员等人员专门负责处理这个问题，他们会确保答案分布均匀。实际上，OpenAI 会做出正确的处理。因此，当我向 ChatGPT 提出这个问题时，它的反应实际上会非常慢。

这就像，好吧，我们来定义变量，建立方程。然后就会产生所有这些中间结果。这些可不是给你看的。这些是给模型的。如果模型没有为自己生成这些中间结果，它就无法达到三。

我还想向你展示，对模型稍微苛刻一点也是可以的。我们可以直接索要答案。举个例子，我给了它完全相同的提示，我说，用一个词回答问题。直接给我答案，不要其他内容。事实证明，对于这个简单的提示，它实际上能够一次性完成。所以它只生成了一个——我想这应该是两个标记(token)对吧？因为美元符号本身就是一个标记。也就是说，这个模型并没有给我单个标记，而是给出了两个标记，但它仍然输出了正确答案。

而且它仅通过神经网络的一次前向传递就做到了这一点。这是因为这里的数字非常简单。

所以我故意增加了一点难度，让模型稍微吃点苦头。所以我说，艾米丽买了23个苹果和177个橙子。然后我把数字稍微调大了一点。我只是想让模型更难一点。我要求它在单个标记内完成更多计算。于是我说了同样的话，结果它给出了 5，而 5 实际上是不正确的。因此，模型未能在网络的一次前向传递中完成所有这些计算。它未能从输入标记开始，然后通过网络的一次前向传递、一次性通过网络。它无法产生结果。

然后我说，好吧，现在不用担心标记限制，像往常一样解决问题。然后它会处理所有的中间结果。这个过程会简化。每一个中间结果和中间计算对模型来说都更加容易。而且，每个标记的工作量并不算太大。这里的标记都是正确的，并且得出了一个解，也就是七。只是无法把所有的工作都压缩进去。网络无法在一次前向传播中完成这一任务。所以我觉得这更像是一个有趣的例子，值得我们去思考。而且我认为，这再次阐明了这些模型的工作原理。

最后我想说的是，如果我在日常实践中试图真正解决这个问题，我可能实际上并不相信模型在这里的所有中间计算都是正确的。所以实际上，我可能会这样做。我会到这里说，使用代码。这是因为代码是 Chatgpt 可以使用的工具之一。与其让它进行心算，比如这里的心算，我并不完全信任它。尤其是当数字变得非常大的时候。无法保证模型能准确完成这一任务。原则上，其中任何一个中间步骤都可能出错。我们正在使用神经网络进行心算，就像你在大脑中做心算一样。这可能会搞砸一些中间结果。

它居然能进行这种心算，其实挺惊人的。我觉得我自己都做不到，但基本上这个模型就像是在用它的“脑子”做这件事。我不相信这一点。所以我希望它能使用工具。你可以说类似“使用代码”这样的话。就像我提到的，有一个特殊的工具，模型可以编写代码。我可以检查这段代码是否正确。这样它就不依赖于心算，而是使用 Python 解释器——一种非常简单的编程语言——来编写计算结果的代码。

我个人会更信任这个结果，因为它是由 Python 程序生成的。我认为相比语言模型的心算，Python 程序能提供更高的准确性保证。这再次提示我们，如果你遇到这类问题，或许可以干脆让模型使用代码解释器来解决。就像我们之前看到的网络搜索功能一样，模型有专门的标记来调用这些工具。

它实际上并不会从语言模型中生成这些标记。它会编写程序，然后将该程序发送到计算机的另一部分，这部分实际上只是运行该程序并返回结果。然后，模型就能获取该结果，并告诉你，好的，每个苹果的成本是七。所以这是另一种工具。我会在实际中推荐你使用它。可以说，它确实更不容易出错。

这就是为什么我将这一部分命名为“模型需要 Token 才能思考”。将你的竞争分散到多个 Token 上。要求模型生成中间结果。或者，只要有可能，就尽量依靠工具和工具的使用，而不是让模型在内存中完成所有这些事情。所以，如果它们试图在内存中完成所有操作，不要完全信任它，尽可能优先使用工具。

我想再举一个实际发生的例子，那就是计数。因此，模型实际上并不擅长精确计数，原因完全相同。你要求单个标记承载的信息量太大了。让我给你展示一个简单的例子。下面有多少个点？然后我就放了一堆点。chatgpt 说有。然后它就会尝试用一个标记来解决这个问题。

因此，在单个标记中，它必须计算其上下文窗口中的点的数量。而且它必须在网络的一次前向传递中完成这一操作。正如我们之前讨论的，在网络的一次前向传递中，能够进行的计算量是有限的。你可以把这想象成那里发生的计算量非常小。所以如果我只看模型看到的内容，让我们转到 LLM 分词器。它看到的是这个。

### 重新审视分词：模型在拼写方面表现欠佳

下面有多少个点？然后我们发现，这里的这些点，这一组大约 20 个点，实际上是一个单独的标记。而这一组无论有多少点，又是另一个标记。出于某种原因，它们就这样被分开了。所以我实际上并不清楚，这与分词器的细节有关，但事实证明，模型基本上看到的是这些标记 ID，这个、这个、这个，依此类推。然后从这些标记 ID 中，它需要数出总数。剧透警告：不是 161。我相信实际上是 177。所以我们可以这样做，我们可以说使用代码。

你可能会想，为什么这能行？其实这有点微妙，也有点意思。所以当我说用代码时，我其实觉得这能行。我们来看看。177 是正确的。这里的情况是，虽然看起来不像，但我实际上已经把问题分解成了对模型来说更容易处理的小问题。我知道模型不会数数，无法进行心算，但我也知道模型在复制粘贴方面其实相当擅长。

所以我现在做的是，当我说“使用代码”时，它会在 Python 中创建一个字符串。而将我的输入从这里复制粘贴到这里这个任务非常简单，因为对于模型来说，它看到的这个字符串，对它来说只是这四个标记或其他什么。因此，模型要复制粘贴这些标记 ID 并将它们在这里解包成点是非常简单的。于是它创建了这个字符串，然后调用 Python 的 count 方法，最终得出正确答案。

所以是 Python 解释器在进行计数，而不是模型的心算在计数。这再次说明了一个简单的例子：模型需要标记来进行思考，不要依赖它们的心算能力。这也是为什么模型在计数方面表现不佳的原因。如果你需要它们执行计数任务，务必让它们借助工具完成。

目前这些模型还存在其他各种细微的认知缺陷，这些就像是技术发展过程中需要留意的尖锐棱角。

举个例子，这些模型在处理各类拼写相关任务时表现欠佳。他们在这方面并不擅长。我之前说过我们会回过头来讨论分词的问题。这么做的原因是，模型看不到字符，它们看到的是分词标记（token），它们的整个世界都围绕着这些由小段文本构成的分词标记运转。因此，它们无法像人眼那样识别字符。于是，连非常基础的字符级任务也常常失败。

举个例子，我输入一个字符串"ubiquitous"，要求它从第一个字符开始，每隔两个字符输出一个（即打印第 1、4、7...个字符）。所以我们从U开始，然后每隔两个字母选一个。所以数一、二、三，下一个应该是 Q，以此类推。所以我认为这个是不正确的。

我的假设是，首先，这里的心理计算有点失灵。但更重要的是，我认为问题的关键在于：如果你去查看 TickTokenizer 对 "ubiquitous" 的处理，会发现它被分成了三个token，对吧？你和我在看到 "ubiquitous" 这个词时，可以轻松识别出每个字母，因为我们能直观地看到它们。当这个词出现在我们视觉工作记忆中时，我们就能非常容易地定位到每第三个字母，从而完成这个任务。

但模型无法访问单个字母。它们将这些视为三个标记。记住，这些模型是在互联网上从零开始训练的。所有这些标记，本质上模型需要发现有多少不同的字母被压缩进这些不同的标记中。我们之所以使用标记，主要是出于效率考虑。但我认为很多人都希望完全摒弃标记。

就像我们确实应该开发字符级别或字节级别的模型。只不过那样会产生很长的序列，而目前人们还不知道如何处理这种情况。因此，在采用分词机制的情况下，任何拼写任务实际上都不太可能表现得特别好。

因为我知道由于分词的原因，拼写不是它的强项，所以我再次要求它借助工具。我只需说使用代码，我预计这又会奏效，因为将 “ubiquitous” 复制粘贴到 Python 解释器中的任务要简单得多。然后我们依靠 Python 解释器来操作这个字符串的字符。所以当我说使用代码 “ubiquitous” 时，是的，它会索引到每第三个字符，而实际真相是 “UQTS”，这在我看来是正确的。

再次说明，拼写相关任务的效果并不理想。最近一个非常著名的例子就是 “strawberry”中有多少个字母 “R”？这个问题多次在网上疯传。现在这些模型基本上都能答对了。它们会说草莓（strawberry）这个单词里有三个 R。但在很长一段时间里，所有最先进的模型都坚称草莓这个单词里只有两个 R。这引起了很多骚动，因为“骚动”这个词对吗？我想是的。因为这就像是，为什么这些模型如此出色？它们能解出数学奥赛题，却数不清“草莓”这个词里有几个 “R”。至于原因，我已经慢慢铺垫过了。

但首先，模型看不到字符，它们看到的是标记。其次，它们不太擅长计数。所以我们这里是把识别字符的困难和计数的困难结合在了一起。这就是为什么模型在这方面遇到了困难。说实话，我觉得到现在为止，OpenAI 可能已经对这个答案进行了硬编码，或者我不确定他们具体做了什么。但现在这个特定的查询已经可以正常工作了。所以模型在拼写方面表现不佳。还有很多其他小问题。我就不一一列举了。我只是想举几个例子，让你了解需要注意的地方。在实际使用这些模型时，我并不打算在这里全面分析模型存在的各种不足。我只是想指出，这里确实存在一些不够完善的地方。

### 智能缺陷

我们已经讨论了其中一些，有些是有道理的。但也有一些就不那么合理了。它们甚至会让你摸不着头脑，即使你深入了解这些模型的工作原理。最近就有一个很好的例子。这些模型对于像这样非常简单的问题并不擅长。这让很多人感到震惊，因为它们能解决复杂的数学问题。它们回答博士级别的物理、化学、生物学问题比我强得多。但有时在像这样超级简单的问题上却会出错。所以请看好了。

9.11 比 9.9 更大。这在某种程度上是有道理的，但也很明显。最后，好吧。实际上它后来又改变了决定。所以我不认为这个结果具有很高的可重复性。有时它会给出相反的答案，有时又能答对。有时候它会出错。让我们再试一次。好的。尽管看起来可能更大。好吧。所以在这里它到最后甚至都没有自我修正。

如果你多问几次，有时候它也能答对。但为什么这个模型能在奥林匹克级别的题目上表现如此出色，却在像这样非常简单的问题上出错呢？我觉得这个问题，就像我之前提到的，有点令人费解。事实上，已经有很多人深入研究过这个问题，不过我还没有真正读过那篇论文。

但这个团队告诉我的是，当你仔细检查神经网络内部的激活情况时，当你观察某些特征以及哪些特征会开启或关闭、哪些神经元会激活或休眠时，神经网络中有一批通常与《圣经》经文相关联的神经元会被点亮。因此我认为模型某种程度上被提示这些数字看起来很像《圣经》经文的标记。而在《圣经》经文的语境中，9.11 会紧跟在 9.9 之后。所以本质上，模型在认知层面上会感到非常困惑——因为在《圣经》经文中，9.11 这个数字理应更大。

尽管这里实际上是在试图用数学来证明并得出答案，但最终还是得到了错误的结论。所以基本上它并不完全合理，也没有被完全理解。而且还有一些类似这样零散的问题。这就是为什么我们要实事求是地看待它——一个既神奇又不可全信的随机系统。你应该把它当作工具来使用，而不是放任它随意解决问题然后直接复制粘贴结果。明白了吗？

### SFT 到 RL

目前我们已经介绍了大语言模型训练的两个主要阶段。第一阶段被称为预训练阶段，主要是基于互联网文档进行训练。当你用互联网文档训练一个语言模型时，得到的就是所谓的 base 模型，它本质上就是一个互联网文档模拟器，对吧？我们发现这是个有趣的产物，需要数千台计算机耗费数月时间训练。它有点像互联网的有损压缩版本。虽然极其有趣，但它并不直接实用，因为我们并不需要生成互联网文档样本。

我们想要向 AI 提问并让它回答我们的问题。为此，我们需要一个助手。我们发现，实际上可以在后训练的过程中，特别是在我们称之为监督微调的过程中构建这样一个助手。因此在这个阶段，我们发现它在算法上与预训练完全相同。不会有任何改变。唯一变化的是数据集。因此，我们不再局限于互联网文档，而是希望构建并精心打造一个优质的对话数据集。我们的目标是收集数百万条涵盖各类话题的人机对话记录。从根本上说，这些对话内容都将由人类创造生成。

人类负责编写提示词，人类也负责撰写理想回复。他们依据标注文档来完成这些工作。在现代技术栈中，这些工作实际上并非完全由人工手动完成，对吧？如今他们其实得到了这些工具的大量协助。因此，我们可以利用语言模型来协助创建这些数据集。并且我们会对其进行全面测试。但归根结底，这一切最终仍源自人类的精心筛选。所以我们创建了这些对话。这现在成为了我们的数据集。我们对其进行微调或继续训练，最终得到一个助手。

然后我们转变了话题，开始讨论这个助手可能带来的一些认知影响。我们发现，如果不采取一些缓解措施，助手会出现幻觉现象。因此，我们认识到幻觉可能会很常见。然后我们研究了一些缓解这些幻觉的方法。接着我们发现这些模型相当出色，能在脑海中处理大量信息。但我们也发现它们可以借助工具来提升表现。举个例子，我们可以借助网络搜索来减少幻觉的产生，或许还能获取一些更新的信息或类似的内容。或者我们可以利用代码解释器等工具，这样大语言模型就能编写代码并实际运行它、查看结果。这些就是我们目前探讨的部分主题。

现在我想做的是介绍这个流程的最后也是最重要的阶段，那就是强化学习。目前强化学习仍被认为属于后训练调整的范畴。但这是最后一个主要阶段。这是一种不同的语言模型训练方式，通常作为第三步进行。因此，在 OpenAI 这样的公司内部，你会从这里开始。

这些都是独立的团队。有一个团队负责预训练的数据工作，另一个团队负责预训练的训练工作。此外，还有一个团队专门负责对话生成，而另一个不同的团队则负责监督微调。还会有一个团队负责强化学习部分。这有点像这些模型的交接过程：你先获得基础模型，然后微调成助手，接着进入强化学习阶段——这部分我们马上就会讲到。这就是大致的主要流程。

那么现在让我们专注于强化学习，这是训练的最后主要阶段。首先，让我解释一下为什么要进行强化学习，以及从高层次来看它是什么样的。那么现在我想试着解释一下强化学习阶段及其对应的含义。这可能是你们比较熟悉的概念。基本上，这就相当于上学的过程。

就像你上学是为了精通某项技能一样，我们也要让大语言模型接受学校教育。实际上，我们正在通过几种范式来赋予它们知识或传授技能。具体来说，当我们使用学校教材时，你会发现这些教材包含三大类信息——三类主要的知识模块。

你首先会注意到的是大量解释性内容。顺便说一句，这是我从网上随便找的一本书。我觉得可能是某种有机化学之类的，我也不太确定。但重要的是，你会发现大部分内容，基本上都是说明性的文字，就像是背景知识之类的。当你阅读这些说明性文字时，可以大致将其视为对这些数据的训练。这就是为什么当你阅读这些背景知识和上下文信息时，它有点像预训练的过程。我们在这里构建了一个关于这些数据的知识库，并对主题有了初步了解。

接下来你会看到的主要信息是这些问题及其解决方案。简单来说，这本书的作者作为人类专家，不仅给我们提出了问题，还提供了解决方案。这个解决方案基本上等同于一个理想助手的完美回答。也就是说，专家实际上是在向我们示范如何解决这个问题。这有点像它的完整形态。当我们阅读解决方案时，实际上是在用专家数据进行训练。之后，我们就可以尝试模仿专家的做法。这大致相当于拥有了 SFT 模型。这就是它要做的。所以基本上，我们已经完成了预训练，并且已经涵盖了专家模仿以及他们如何解决这些问题。

学习的第三阶段基本上是练习题。有时你会看到这里只有一个练习题。当然，任何教科书的每章末尾通常都会有许多练习题。当然，我们知道练习题对学习至关重要，因为它们能让你做什么呢？它们能让你自己动手实践，并探索解决问题的方法。在练习题中，你会看到一个问题的描述，但不会直接给出解法，不过通常会提供最终答案（一般在教科书的答案部分）。所以你知道自己要达到的目标答案，也有问题的陈述，但没有具体的解题步骤。你正在尝试实践解决方案。你尝试了很多不同的方法，看看哪种方法能最好地帮你找到最终解决方案。因此，你正在探索如何解决这些问题。

在这个过程中，你首先依赖于来自预训练的背景信息，其次可能还会稍微模仿人类专家的做法。你或许可以尝试类似的解决方案等等。我们已经完成了这些步骤，现在在这一部分，我们将尝试进行实践。因此，我们将获得提示。我们会得到最终答案，但我们不会得到专家级的解决方案。我们必须不断实践和尝试。这正是强化学习的核心所在。


### 强化学习

好了，让我们回到之前讨论过的问题，这样在探讨这个话题时就能有一个具体的例子来分析。所以我在这里使用标记分词器，因为我也想——嗯，我得到了一个文本框，这很有用，但第二点，我想再次提醒你们，我们始终在处理一维的标记序列。因此，我实际上更喜欢这个视图，因为这是 LLM 的原生视图，如果这说得通的话。这才是它真正看到的内容。

它能看到 token ID，对吧？好的。那么 Emily 买了三个苹果和两个橙子。每个橙子 2 美元。所有水果的总成本是 13 美元。每个苹果的成本是多少？我想让你明白的是，这里有四个可能的候选解决方案作为例子，它们都得出答案3。现在，我想让你意识到的是，如果我是负责创建对话的人类数据标注员，要将对话输入训练集，实际上我并不确定该将其中哪个对话添加到数据集中。

其中一些对话会建立方程组，有些则只是用英语讨论问题，还有些则直接跳到解决方案。比如你问 ChatGPT 这个问题，它会定义一组变量，然后做这样的小操作。但我们必须明白并区分的是，解决方案的首要目的当然是得出正确答案。我们想要得到最终答案三。这是这里的重要目的。但还有一个次要目的，就是我们也在努力让它对人类友好，因为我们假设这个人想看到解决方案，他们想看到中间步骤，我们想很好地呈现它，等等。

所以这里有两件不同的事情。第一件是向人类展示，但第二件，我们实际上是在试图得到正确的答案。所以让我们暂时专注于得出最终答案。如果我们只关心最终答案，那么在这些选项中，哪个是最优的或者说最佳解决方案，能让大语言模型得出正确答案？我想说的是，我们并不知道。作为人类标注员，我也不知道哪个是最好的。举个例子，我们之前看到的标记序列和心算推理过程表明，对于每个标记，我们实际上只能使用有限的计算资源，这个量并不大，或者说你应该这样去理解。因此，我们可以这样理解：在单个标记上无法做出太大的跨越。

举个例子，这个案例的妙处在于它使用的标记非常少，所以我们能在极短时间内得出答案。但在这里，当我们计算13减4除以3等于多少时，就在这个标记处，我们实际上要求在这个单一的标记上进行大量运算。因此，这也许不是一个适合给大语言模型的例子，因为它某种程度上会促使模型快速跳过计算步骤，从而导致心算出错。也许更分散地展开会更有效。也许把它列成方程式会更好。也许通过讨论来解决会更合适。

 从根本上说，我们并不清楚。我们不清楚的原因是，对你我或人类标注员而言容易或困难的任务，与对大语言模型（LLM）来说的难易程度并不相同。它的认知方式与我们不同。而标记序列对它来说有点不同难度。所以这里对我来说轻而易举的标记序列，对 LLM 来说可能是个巨大的跨越。比如眼前这个标记，难度就太高了。

但反过来看，我在这里创建的许多标记对 LLM 来说可能毫无意义。我们只是在浪费标记。既然这些都无关紧要，为何要浪费这些标记呢？如果我们唯一关心的是得到最终答案，而将呈现给人的问题分开考虑，那么我们实际上并不知道该如何标注这个例子。

我们不知道应该给大语言模型提供什么解决方案，因为我们不是大语言模型。这在数学案例中表现得非常明显，但实际上这是一个普遍存在的问题。我们的知识并不等同于大语言模型的知识。这个大型语言模型实际上掌握了大量数学、物理、化学等领域的博士级知识。在很多方面，它确实比我知道得更多。而我可能在解决问题时并没有充分利用这些知识。

但反过来，我可能在解决方案中注入了一堆大语言模型参数中并不掌握的知识。这些突如其来的知识跃迁会让模型感到非常困惑。因此，我们的认知方式存在差异。如果我们只关心最终解决方案并以经济高效的方式实现目标，那我真的不知道该在这里写些什么。简而言之，我们目前并不擅长为 LLM 创建这些标记序列。不过通过模仿来初始化系统，它们还是很有用的。但我们真正希望的是让大语言模型自己去发现适合它的标记序列。它需要自行找出在给定提示下能可靠得出答案的标记序列。而且它需要通过强化学习和试错的过程来发现这一点。

那么让我们看看这个例子在强化学习中是如何运作的。好的，现在我们又回到了 Hugging Face 的推理演示平台。这个平台让我能够非常轻松地调用各种不同的模型。举个例子，在右上角这里，我选择了 Gemma 的 2b 参数模型。2b 参数其实非常非常小，这是个微型模型，不过没关系。

所以我们要采用的方式，强化学习的基本运作方式其实相当简单。我们需要尝试多种不同的解决方案，然后观察哪些方案效果好，哪些效果不佳。所以我们基本上会接收提示，运行模型。模型会生成解决方案。然后我们会检查这个解决方案。我们知道这道题的正确答案是3美元。事实上，模型也答对了，它给出的答案就是3美元。所以这是正确的。

这只是解决方案的一种尝试。现在我们要删除这个，然后重新运行一次。让我们再试一次。因此，这个模型的解决方式会略有不同，对吧？每一次尝试都会产生不同的结果，因为这些模型是随机系统。要记住，这里的每一个标记都有一个概率分布，我们是从这个分布中进行采样的。所以最终我们会走上略有不同的路径。因此，这是第二个同样得出正确答案的解决方案。

现在我们要删除它。让我们再来第三次。好的，那么再来一次，虽然解法稍有不同，但同样正确。实际上，我们可以多次重复这个过程。因此在实际操作中，你可能会针对同一个提示采样数千个独立解，甚至可能达到百万量级。其中一些会是正确的，另一些则不太正确。基本上，我们希望做的是鼓励那些能得出正确答案的解决方案。

那么，让我们来看看具体是什么样的。那么如果我们回到这里，这里有点像是一个卡通示意图，展示了大致的样貌。我们有一个提示，然后我们并行尝试了许多不同的解决方案。其中一些方案可能表现良好，因此它们得到了正确的答案，用绿色表示。有些解决方案可能效果不佳，甚至无法得出正确答案——也就是红色。不过，眼前这个问题其实算不上最佳范例，因为它实在过于简单。正如我们所见，即便是 2b 参数的模型也能轻松答对。所以从这个角度来看，这并不是最好的例子。

但让我们发挥一下想象力。假设绿色的代表好的，红色的代表坏的。好的，我们生成了 15 个解决方案，其中只有四个得到了正确答案。那么现在我们要做的就是，基本上，我们希望鼓励那些能得出正确答案的解决方案类型。所以，在这些红色解决方案中出现的任何标记序列，显然在某个环节出了问题。而这并不是解决问题的好方法。那些绿色解决方案中的任何标记序列，在这种情况下都表现得相当不错。因此，我们希望在这类提示中更多地采用类似的做法。

而我们鼓励未来这种行为的方式，本质上就是对这些序列进行训练。但现在这些训练序列并非来自专家的人工标注，也没有人判定这就是正确的解决方案。这个解决方案源自模型本身。因此，模型在这里进行实践，它尝试了几种解决方案，其中四种似乎奏效了。现在模型将对这些方案进行某种训练。而这相当于一种认可，就像在说：“好吧，这个确实效果很好。所以我应该用这种方式来解决这类问题。”在这个例子中，实际上有很多不同的方法可以稍微调整一下方法论。

但为了传达核心概念，或许可以简单地理解为从这四个方案中选出最优的一个，比如这个，所以它被标为黄色。这个方案不仅得出了正确答案，可能还具有其他优点。也许它是最简洁的，或者在某种程度上看起来最漂亮，或者你还能想到其他评判标准作为例子。但我们会认定这是最佳解决方案，并据此进行训练。经过参数更新后，模型在未来遇到类似情境时，就会更倾向于选择这条路径。但必须记住，我们会在大量数学、物理等各种问题上运行多种多样的提示。

因此，成千上万的提示词背后，可能对应着每个提示词都有数千种解决方案。这一切几乎是在同时发生的。随着我们不断迭代这一过程，模型会自行发现哪些标记序列能引导它得出正确答案。这不是来自人类标注者的数据。模型就像在这个游乐场里玩耍。它知道自己想要达到什么目标，并且正在发现对它有效的序列。这些序列不需要任何思维跳跃。它们看起来可靠且符合统计规律，并充分利用了模型已有的知识。这就是强化学习的过程。

这基本上就是一个不断试错的过程。我们会尝试各种不同的解决方案，验证它们的效果，并在未来更多地采用那些行之有效的方法。这就是强化学习的核心思想。因此，结合之前的讨论，我们现在可以看到，监督微调模型（SFT模型）仍然是有帮助的，因为它有点像将模型初步引导到正确解决方案的附近。可以说，它是对模型的一种初始化，让模型能够生成解决方案，比如写出解题步骤，或许还能理解如何建立方程组，或者以某种方式与解决方案进行"对话"。这样，它就能让你接近正确的解决方案。

但强化学习才是真正让一切趋于完美的关键。我们会不断探索适合模型的解决方案，找到正确答案并加以鼓励，这样模型就会随着时间的推移逐渐变得更好。以上就是我们训练大语言模型的高层次流程。简而言之，我们训练 AI 的方式与教育儿童非常相似。唯一的区别在于，儿童是通过书籍的章节学习，在每本书的不同章节中完成各类训练练习。而我们训练 AI 时，更像是根据每个阶段的特点分步骤进行。

首先，我们进行预训练，这相当于阅读所有的说明性材料。我们会同时浏览所有教材，阅读所有解释内容，并尝试构建一个知识库。接下来，我们进入监督微调（SFT）阶段，这一阶段主要是研究人类专家提供的各种固定解法，涵盖所有教材中的各类习题解答。而我们得到的只是一个 SFT 模型，它能够模仿专家的行为，但某种程度上是盲目模仿。它更像是尽最大努力去猜测，试图从统计角度模仿专家的行为。因此，当你查看所有解决方案时，这就是你所得到的结果。

最后，在最后一个阶段，我们会在强化学习阶段完成所有的练习题。我们只做所有教材中的练习题。这就是我们得到强化学习模型的方法。从高层次来看，我们训练大语言模型的方式与培养儿童的过程非常相似。

接下来我想指出的是，事实上前两个阶段——预训练和监督微调——已经存在多年，它们非常标准化，所有不同的大语言模型提供商都在采用。而最后一个阶段，即强化学习训练，目前仍处于发展初期，在该领域尚未形成统一标准。因此，这个阶段还处于非常早期和萌芽的状态。原因在于，我实际上跳过了这个过程中的大量细节。

高层次的理念其实非常简单。这是一种试错学习的过程，但其中涉及大量细节和微妙的数学技巧——比如如何挑选最优解、训练量如何把控、提示词分布如何设计，以及如何设置训练流程才能使其真正奏效。核心思想虽然极其简单，却需要调节无数细枝末节的参数。因此，要把这些细节做到位绝非易事。

### DeepSeek-R1

因此，许多公司，比如 OpenAI 和其他大型语言模型提供商，已经内部试验强化学习对 LLM 进行微调有一段时间了，但他们没有公开讨论过。这些都是在公司内部进行的。这就是为什么 DeepSeek 最近发表的论文如此重要，因为这篇论文来自中国一家名为 DeepSeek AI 的公司。这篇论文公开详细地探讨了如何通过强化学习对大语言模型进行微调，强调其对大语言模型的至关重要性，以及如何显著提升模型的推理能力。我们稍后将深入探讨这一点。该论文重新激发了公众对运用强化学习优化大语言模型的兴趣，并提供了大量可复现实验结果的关键细节，为实际应用于大语言模型奠定了基础。

那么让我简单介绍一下这篇 DeepSeek R1 论文，当你真正正确地将强化学习应用于语言模型时会发生什么，它看起来是什么样子，以及它能给你带来什么。首先我要展示的是这里的图 2，我们正在观察模型在解决数学问题方面的改进。这是在 AIME 准确率上解决数学问题的准确率。然后我们可以进入网页，看看这些数学题中实际包含哪些类型的问题。

这些都是简单的数学题。如果你想的话可以暂停视频，但基本上这些就是模型需要解决的问题类型。可以看到一开始它们的表现并不理想，但随着模型经过数千步的更新，它们的准确率会逐渐提升。因此，随着你在这类问题的大数据集上不断试错，模型也在不断改进，它们能以更高的准确率解决这些问题。这些模型正在学习如何解决数学问题。但比用更高准确率解决这些问题的量化结果更令人难以置信的，是模型实现这些结果的定性方式。

当我们向下滚动时，这里有一个相当有趣的数据点：在优化过程的后期阶段，模型似乎开始增加每个回答的平均长度。这意味着模型正在通过使用更多标记（tokens）来获得更高的准确率结果。换句话说，它正在学习生成极其冗长的解决方案。

为什么这些解决方案如此冗长？我们可以在这里进行定性分析。基本上，他们发现模型生成的解决方案变得非常、非常长，部分原因是这样的：这里有一个问题，而这里大致是模型给出的答案。模型学会的做法（这是优化过程中自然涌现的特性，它只是发现这对解决问题有帮助）就是开始做类似这样的事情。

等等，等等，等等，这可是个值得标记的顿悟时刻。让我们一步步重新评估，找出正确的总和可能是什么。那么模型在这里做什么呢？模型基本上是在重新评估步骤。它发现，为了提升准确性，尝试多种思路、从不同角度探索、回溯、重构和重新审视更为有效。这与我们在解决数学问题时所做的许多事情类似，但它是在重新发现你脑海中发生的过程，而非你写在解答上的内容。没有任何人能将这些东西硬编码到理想助手的回应中。

这只有在强化学习的过程中才能发现，因为你不知道该在这里放什么。结果证明这对模型有效，并提高了其解决问题的准确性。因此，模型学会了我们称之为你头脑中的这些思维链，这是优化的一个涌现特性。

而这正是导致响应时间膨胀的原因，但同时也提升了问题解决的准确性。令人惊叹的是，这个模型正在探索思考的方式。它正在学习我称之为认知策略的东西——如何操控一个问题，如何从不同角度切入，如何引入类比或进行类似的不同操作，以及如何随着时间的推移尝试多种方法，从不同视角检验结果，最终解决问题。但在这里，它某种程度上是被强化学习发现的。能在优化过程中看到这种现象自然浮现，而无需在任何地方硬编码，实在令人难以置信。我们唯一提供的就是正确答案，而它仅仅是通过尝试正确解题就自行涌现出来，这简直不可思议。

现在让我们回到我们一直在处理的问题上，看看这种我们称之为推理或思考模型的方法会如何解决这个问题。好的，回想一下我们一直在处理的问题，当我把它输入到ChatGPT 4.0 时，我得到了这样的回答。让我们看看当你向所谓的推理或思考模型提出同样的查询时会发生什么。这是一个通过强化学习训练出来的模型。这篇论文中描述的DeepSeek R1 模型可以在 chat.deepseek.com 上使用。这是开发它的公司托管的一个平台。你需要确保它被正确调用。我们可以把它粘贴到这里并运行。现在让我们看看会发生什么，以及模型的输出是什么。好的，这就是它所说的。

所以这就是我们之前使用基本上的SFT方法（监督微调方法）得到的结果。这类似于模仿专家解决方案。而这是我们通过强化学习模型得到的结果。好吧，让我试着算一下。Emily买了三个苹果和两个橙子。每个橙子2美元，总共是13美元。我需要弄清楚这个那个。所以当你读到这里时，不禁会认为这个模型在思考。它确实在寻求解决方案。

由此可以得出它必须花费3美元。然后它说，等一下，让我再检查一下我的计算以确保无误。接着它从一个稍微不同的角度尝试解决。然后它说，是的，一切都没问题。我想这就是答案。我没发现任何错误。让我想想有没有其他方法来解决这个问题，或许可以列个方程。假设一个苹果的价格是8美元，然后巴拉巴拉。嗯，答案是一样的。所以每个苹果肯定是3美元。好的，我很确定这是正确的。然后，它在完成思考过程后，会为人类写出一份漂亮的解决方案。

因此，现在考虑的是，这部分更侧重于正确性方面，而另一部分则更侧重于呈现方式，即如何清晰地展示并在底部框出正确答案。令人惊叹的是，我们得以窥见模型的思考过程。这正是强化学习过程所带来的成果。这就是导致标记序列长度膨胀的原因。它们在进行思考，并尝试不同的方法。这正是在解决问题时为你提供更高准确性的关键所在。

正是在这里，我们见证了那些顿悟时刻、各种策略以及确保获得正确答案的创意方法。最后我想说的是，有些人对于在 chat.deepseek.com 上输入非常敏感的数据感到有点紧张，因为这是一家中国公司，所以人们对此会稍显谨慎和戒备。DeepSeek R1 就是这家公司发布的模型。

所以这是一个开源模型或开放权重模型。任何人都可以下载和使用它。你将无法以全精度运行完整的模型。你不会在 MacBook 或类似本地设备上运行它，因为这是一个相当大的模型。但许多公司都在托管完整的最大模型。我喜欢使用的一家公司叫 Together.ai。当你访问 Together.ai 时，注册后进入 Playgrounds。你可以在这里选择 DeepSeek R1，还有许多其他不同的模型可供选择。这些都是最先进的模型。这与我们之前使用的 Hugging Face 推理平台有些类似，但 Together.ai 通常会托管所有最先进的模型。所以选择 DeepSeek R1。你可以试着忽略很多这些选项。我认为默认设置通常就够用了，我们可以直接使用这个。

由于该模型由深度求索（DeepSeek）发布，您在此处获得的功能应基本等同于原版。尽管采样过程中的随机性会导致细微差异，但原则上该模型在性能上与原版完全一致——无论是量化指标还是定性表现都应如出一辙。不过需要说明的是，这个版本源自一家美国企业。这就是深度求索（DeepSeek）推出的推理模型。

现在当我回到聊天界面时，让我在这里进行对话。好，你在这个下拉菜单中会看到的模型，比如 O1、O3 mini、O3 mini high等，它们都提到了"使用高级推理"。这里所说的"使用高级推理"，指的是根据 OpenAI 员工的公开声明，这些模型采用了与 DeepSeek R1 非常相似的强化学习技术进行训练。

所以这些都是用强化学习训练出来的思维模型，而像你们在免费版里能用的 GPT-4.0 或GPT-4.0 mini 这类模型，你们应该把它们主要看作是监督微调模型（SFT）。它们实际上并不像你们在强化学习模型中看到的那样进行这种思考。尽管这些模型也涉及一点点强化学习——我稍后会详细讲这一点——但它们主要还是监督微调模型。

我认为你应该这样思考。就像我们在这里看到的例子一样，我们可以选择一个思维模型，比如说 O3 mini high。顺便说一下，这些模型可能需要你订阅 Chatgpt 才能使用，每月 20 美元或 200 美元才能解锁某些顶级模型。所以我们可以选择一个思维模型并开始运行。现在的情况是，它会显示“推理”，然后开始做类似这样的事情。而我们在这里看到的，并不完全是我们实际看到的内容。所以，尽管在底层，模型会产生这种思维链，但 OpenAI 选择不在网页界面上展示确切的思维链。它展示了这些思维链的简要概述。 OpenAI 这么做，部分原因是他们担心所谓的"蒸馏风险"，即有人可能会试图模仿这些推理痕迹，仅通过复制思维链就能恢复大量的推理能力。

所以他们有点隐藏它们，只展示一些简短的摘要。所以你无法完全获得 DeepSeek 中关于推理本身的内容。然后他们写出解决方案。所以这些在某种程度上是等效的，尽管我们看不到底层的全部细节。就性能而言，我认为这些模型和 DeepSeek 模型目前大致相当。由于评估的原因，很难确切判断。但如果你每月支付 200 美元给 OpenAI，我认为目前有些模型看起来仍然更胜一筹。不过，DeepSeek R1 作为一款思维模型，目前仍是相当可靠的选择，你可以在本网站或其他网站获取，因为该模型采用开放权重，你可以直接下载。这就是思维模型的情况。

那么，目前的总结是什么呢？我们讨论了强化学习，以及在优化过程中思维的产生——当我们基本上在许多数学和可验证解决方案的代码问题上运行强化学习时。比如有答案三等等。现在，你可以在 DeepSeek 或任何推理提供商（如 together.ai）中访问这些思维模型，并在那里选择 DeepSeek。这些思维模型在 chatGPT的O1或O3模型下同样可用。但这些 GPT 4.0 模型等并非思维模型，你应将其主要视为 SFT 模型。

如果你有一个需要高级推理的提示词，那么你可能应该使用一些思维模型，或者至少尝试一下。但根据我的经验，在很多情况下，当你问一个更简单的问题时，比如基于知识的问题或类似的问题，这可能就有点小题大做了。比如，没有必要花 30 秒去思考一个事实性问题。因此，我有时会默认直接使用 GPT 4.0。根据经验，我大约 80% 到 90% 的使用场景都是 GPT 4.0。当我遇到非常困难的问题，比如数学和代码等方面时，我会选择使用思维模型，但这样我就得多等一会儿，因为它们需要思考。你可以在 chatGPT 和 DeepSeek 上使用这些功能。另外我想指出的是，aistudio.google.com 虽然看起来非常杂乱、非常难看，因为谷歌就是做不好这类东西，但这就是现状。

但如果你选择模型，在这里选择 Gemini 2.0 Flash Thinking Experimental 0121，选择这个的话，这也是谷歌早期实验的一种思维模型。我们可以在这里给它同样的问题，然后点击运行。这同样是一个思维模型，也会做类似的事情，并在这里得出正确答案。简单来说，Gemini 还提供了一种思维模式。Anthropic 目前没有提供思维模式。但基本上这就像是这些大语言模型的前沿发展。

我认为强化学习正处于一个令人兴奋的新阶段，但要准确把握细节仍具挑战性。正因如此，截至 2025 年初——确切说是 2025 年非常早期，所有这些模型和思维模型都还处于实验性阶段。这就像是利用优化过程中涌现的推理能力，在解决这些高难度问题方面进行的前沿性突破。

### AlphaGo

我还想提到的一个链接是强化学习的发现是一种极为强大的学习方式，在 AI 领域并不新鲜，而我们已经看到这一点的一个地方，就是围棋这项游戏中，众所周知，deepmind 开发了 alphago 系统，你可以观看一部关于他的电影，在电影中，系统正在学习如何下围棋，与顶尖的人类选手对弈，当我们翻到 AlphaGo 的原始论文时，在这篇论文中往下浏览，会发现一个非常有趣的图表。这张图让我觉得似曾相识——我们正在更开放的通用问题解决领域中重新发现它，而非局限于围棋这个封闭的特定领域。本质上他们观察到的现象（随着技术成熟，我们在大语言模型领域也将看到类似规律）是：这张 ELO 等级分图表对比了监督学习训练出的模型与强化学习训练出的模型在围棋对弈中的实力差距，图中标注了人类顶尖棋手李世石的水平作为参照。

因此，监督学习模型是在模仿人类专业棋手。如果你只是获取大量由专业棋手对弈的围棋棋局，并试图模仿他们，你的水平会有所提升。但之后你会遇到瓶颈，永远无法超越围棋界最顶尖的几位棋手，比如李世石。所以你永远无法达到那个境界，因为你只是在模仿人类玩家。如果你只是模仿人类玩家，你本质上就无法超越人类玩家。但在强化学习的过程中，它的能力要强大得多。

在围棋的强化学习中，这意味着系统正在采取那些经验上和统计上能带来胜利的走法。因此，AlphaGo 是一个通过自我对弈的系统，并利用强化学习来生成棋局推演。所以这里的结构图完全相同，但没有提示词，因为它只是一个固定的围棋对局。但它会尝试多种解决方案，尝试各种玩法，然后那些能带来胜利的游戏方式——而非某个特定答案——会被强化。这些成功的策略会变得更强大。因此，系统本质上是在学习那些从经验和统计角度能赢得游戏的行动序列。而强化学习不会受到人类表现的限制。强化学习可以做得更好，甚至能超越像李世石这样的顶尖选手。所以他们可能本来可以让这个运行得更久，只是选择在某个时候停止，因为这需要花费资金。

但这确实是一个强化学习的强大示范。我们才刚刚开始在大型语言模型中看到这种推理问题图的雏形。因此，仅仅模仿专家是无法让我们走得太远的。我们需要更进一步，建立类似小型游戏环境，让系统自主发现独特的推理路径或解题方法——那些行之有效的独特方式。关于独特性这一点要注意：进行强化学习时，系统完全可能偏离人类玩家的行为分布。比如回顾 AlphaGo 的搜索过程时，其中一个被提出的改进方案被称为"第37手"。

而 AlphaGo 的第 37 步棋指的是一个特定时刻，当时 AlphaGo 下出了一步人类专家根本不会走的棋。据评估，人类棋手走出这步棋的概率约为万分之一，因此这是一步极其罕见的棋。但回过头来看，这步棋堪称神来之笔。AlphaGo 在强化学习过程中发现了一种人类未曾知晓却事后看来精妙绝伦的棋路策略。我推荐大家观看这个 YouTube 视频《李世石对阵 AlphaGo 第37手：反应与分析》。

而当 AlphaGo 下出这一手时，局面看起来是这样的。这步棋非常、非常出人意料。我当时以为、以为它下错了。当我看到这一手棋时。总之，人们之所以如此震惊，是因为这是人类绝不会下的一手棋，而 AlphaGo 却这样下了。因为在它的训练中，这手棋似乎是个好主意。只不过碰巧这不是人类会采取的策略。因此，这再次体现了强化学习的力量。从理论上讲，如果我们继续在语言模型中扩展这种范式，我们实际上可以看到其等效性。而具体会是什么样子，目前还不得而知。

那么，以人类都无法企及的方式解决问题意味着什么？如何才能比人类更擅长推理或思考？如何超越仅仅是一个会思考的人类？也许这意味着发现人类无法创造的类比。或者，这可能是一种全新的思考策略。这确实有点难以想象。也许这是一种全新的语言，甚至根本不是英语。也许它发现了一种更适合思考的语言。因为这个模型不受限制，甚至不必局限于英语。也许它会选择另一种语言来思考，或者创造出自己的语言。因此从原则上讲，系统的行为更加难以界定。它可以自由采取任何有效的方式。

而且它还能逐渐偏离其训练数据（主要是英语）的分布。但这一切只有在拥有大量多样化的问题集时才能实现，这些策略可以在这些问题中得到完善和优化。因此，这正是当前前沿大语言模型研究的重点所在。它试图创建那些规模庞大且多样化的提示分布。这些就像是大型语言模型可以练习思考的游戏环境。这有点像编写这些练习题。我们必须为所有知识领域创建练习题。如果我们有大量练习题，模型就能通过强化学习掌握这些内容，并生成类似的图表，但这是在开放思维领域而非围棋这样的封闭领域。

### RLHF

关于强化学习，我还有一部分内容想要探讨。这就是在不可验证领域学习的问题。到目前为止，我们所研究的所有问题都属于所谓的可验证领域。也就是说，我们可以很容易地根据具体答案对任何候选解决方案进行评分。例如，答案是3，我们可以很容易地根据这个答案对这些解决方案进行评分。我们可以要求模型将答案框起来，然后只需检查框中的内容是否与答案相符。或者你也可以使用所谓的LLM评判器。

因此，LLM 评判器会查看一个解决方案，获取答案，并基本上根据该方案是否与答案一致来评分。从经验来看，当前能力的 LLM 已经足够胜任这一任务，能够相当可靠地完成。所以我们也可以应用这类技术。无论如何，我们已经有了一个明确的答案，现在只是在对照检查解决方案。而且我们可以完全自动地完成这一过程，无需任何人工干预。问题在于，我们无法在所谓的不可验证领域中应用这一策略。

通常这些任务都是创意写作类的，比如写一个关于鹈鹕的笑话、写一首诗、总结一段文字或类似的内容。在这些领域里，要对我们不同的解决方案进行评分就变得比较困难。比如说写一个关于鹈鹕的笑话，我们当然可以生成很多不同的笑话。

没关系。比如我们可以去 Chatgpt，让它生成一个关于鹈鹕的笑话。它们的喙里装那么多东西，因为它们不用背包。什么？好吧。我们可以试试别的。为什么鹈鹕从不付饮料钱？因为它们总是把账单记在别人头上。哈哈。好吧。看来这些模型显然不太擅长幽默。实际上，我觉得这相当有意思，因为我认为幽默其实是件非常困难的事，而目前这些模型还不具备这种能力。不管怎样，你可以想象生成大量笑话的场景。我们现在面临的问题是如何评估这些笑话的好坏？理论上，我们当然可以像我刚才那样找人来逐一评判这些笑话。

问题在于，如果你正在进行强化学习，你将需要进行数千次更新。每次更新时，你需要查看数千个提示。而对于每个提示，你可能需要查看数百甚至数千种不同的生成结果。这些内容实在太多了，根本看不过来。理论上，你可以让人工逐一检查并打分，判断这个可能好笑，那个可能好笑，再拿这些数据训练模型，至少能让它在鹈鹕相关的笑话上稍微进步一点。但问题是，这要耗费的人力时间实在太多了。

这是一种不可扩展的策略。我们需要某种自动化的策略来实现这一目标。这篇论文提出了一种解决方案，引入了所谓的"基于人类反馈的强化学习"。这是 OpenAI 当时的一篇论文。这些人中的许多现在都是 Anthropic 的联合创始人。这篇论文基本上提出了一种在不可验证领域进行强化学习的方法。让我们来看看它是如何运作的。这是核心概念的卡通示意图。正如我提到的，最简单的方法是如果我们有无限的人力时间，我们就可以在这些领域中很好地运行强化学习。

例如，如果我们有无限的人力资源，就可以像往常一样进行强化学习。我只是想打个比方，这些数字只是象征性的——我打算进行 1000 轮更新，每轮更新基于 1000 个提示。对于每个提示，我们将生成 1000 次模拟运行并进行评分。我们可以用这种设置进行强化学习。问题在于，在这个过程中，我需要请人类评估笑话总共 10 亿次。这意味着很多人要看很多非常糟糕的笑话。

我们不想那样做。相反，我们想采用 RLHF 方法。在 RLHF 方法中，我们的核心技巧有点像间接操作。我们打算稍微引入人类的参与。我们的“作弊”方式基本上是训练一个完全独立的神经网络，我们称之为奖励模型。这个神经网络会模仿人类的评分。我们将请人类对演练进行评分。然后，我们将使用神经网络来模仿人类的评分。这个神经网络将成为一种模拟人类偏好的工具。

现在我们有了神经网络模拟器，就可以用它来进行强化学习。举个例子，我们不再询问真实人类，而是向模拟人类征求他们对笑话的评分。一旦有了模拟器，我们就可以大展拳脚了，因为想查询多少次都可以。这是一个完全自动化的过程。我们现在可以通过模拟器进行强化学习。正如你所料，模拟器并不会是一个完美的人类。但如果它至少在统计上与人类判断相似，那么你可能会期望这会产生一些效果。实际上，确实如此。因此，一旦我们有了模拟器，我们就可以进行强化学习，一切都会顺利进行。

让我用一张卡通示意图向你展示这个过程的大致情况。虽然细节不是特别重要，但核心概念是这样的。这里我们有一张假设性的训练奖励模型的卡通示意图。所以我们有一个提示，比如写一个关于鹈鹕的笑话，然后这里有五个不同的版本。这些都是五个不同的笑话，就像这个一样。现在，我们要做的第一件事就是请一个人将这些笑话从最好到最差进行排序。所以这就是人类的思维，认为这个笑话是最好的，最有趣的。这是第一号笑话。这是第二号笑话，第三号笑话，第四和第五号笑话。所以这是最糟糕的笑话。我们让人类进行排序而不是直接打分，因为这样任务稍微简单些。对人类来说，给出排序比给出精确分数更容易。

现在，这就是对模型的监督。人类已经对它们进行了排序，这就像是他们对训练过程的贡献。但现在，我们要做的是另一件事。奖励模型是一个完全独立的神经网络，与之前的网络完全分离，它很可能也是一个 Transformer，但它并不是一个能生成多样化语言的语言模型。

这只是一个评分模型。因此，奖励模型会接收两个输入：第一个是提示词，第二个是候选笑话。这两个输入会进入奖励模型进行评分。例如在这里，奖励模型会接收这个提示和这个笑话。奖励模型的输出是一个单一的数字，这个数字被视为一个分数，范围可以从零到一。零代表最差分数，一代表最佳分数。以下是训练过程中某个阶段假设的奖励模型对这些笑话的打分示例。0.1 分表示非常低分，0.8 分则表示非常高分，以此类推。现在我们将奖励模型给出的分数与人类给出的排序进行比较。

实际上有一种精确的数学方法可以计算这一点，基本上就是建立一个损失函数，在这里计算一个对应关系，并基于此更新模型。但我想给你一个直观的理解，比如在这个第二个笑话的例子中，人类认为它是最有趣的，模型也基本同意，对吧？0.8 分已经是一个相对较高的分数了。但这个分数本应该更高，对吧？所以在更新之后，我们可能会期望这个分数会有所提高，比如网络的更新后可能会变成 0.81 左右。

对于这一个例子，他们实际上存在很大的分歧，因为人类认为这应该是第二名，但这里的评分只有 0.1。所以这个分数需要大幅提高。经过更新后，在这种监督机制下，分数可能会显著提升，比如可能达到 0.15 左右。而在这里，人类认为这是最差的笑话，但模型实际上给了它相当高的分数。所以你可能会以为更新后这个数值会降到 3.5 左右。本质上我们还是在做之前的工作——通过神经网络训练流程对模型预测结果进行微调。

我们正努力使奖励模型的评分与人类排序保持一致。随着我们根据人类数据更新奖励模型，它能越来越准确地模拟人类提供的评分和排序，进而成为人类偏好的某种模拟器——我们可以在此基础上进行强化学习。但关键在于，我们不会让人类反复看上十亿次笑话来评判。

我们可能需要查看大约 1000 条提示词，每条提示词生成 5 个笑话，所以总共可能有 5000 个笑话需要人工审核。他们只需要给出排序，然后我们训练模型使其与这个排序保持一致。我省略了数学细节，但我想让你理解一个高层次的概念：这个奖励模型基本上是在给我们打分，而我们有一种方法可以训练它，使其与人类的排序保持一致。这就是 RLHF 的工作原理。简单来说，我们基本上是训练人类模拟器，并针对这些模拟器进行强化学习。

现在，我想先谈谈人类反馈强化学习的优势。首先，这种方法让我们能够运用强化学习——众所周知这是一套极其强大的技术体系，并且可以在任意领域中实施，包括那些无法验证的领域。比如摘要生成、诗歌创作、笑话编写或其他任何创意写作领域，实际上涵盖了数学和编程等范畴之外的广阔天地。

实际上，当我们真正应用 RLHF 时，经验表明这是一种提升模型性能的方法。对于这种现象的原因，我有一个最合理的解释，但我并不确定它是否已被充分证实。你可以直观地观察到，当你正确实施 RLHF 时，得到的模型表现确实会稍好一些。但至于原因，我认为并不那么明确。所以这是我的最佳猜测。我猜这很可能主要是由于判别器与生成器之间的差距所致。

这意味着在许多情况下，人类进行辨别要比生成内容容易得多。具体来说，当我们进行监督微调（SFT）时，就是要求人类生成理想的助手回应。正如我所展示的，在许多情况下，写出理想的回应非常简单，但在其他情况下，可能并非如此。例如，在总结、诗歌创作或笑话创作中，作为人类标注者，你该如何在这些情况下获得理想的回应？这需要人类的创造性写作才能实现。而 RLHF（人类反馈强化学习）某种程度上绕过了这个问题，因为我们让数据标注者回答一个简单得多的问题。他们不需要直接创作诗歌。

他们只是从模型中获得了五首诗，然后被要求对它们进行排序。因此，这对人类标注者来说是一项简单得多的任务。所以我认为这基本上能让你做到的是，它有点像允许很多...您的准确性数据，因为我们并没有要求人们完成极具挑战性的生成任务。比如，我们没有让他们进行创意写作，只是试图让他们区分创意作品并找出最佳的那些。这就是人类提供的信号，仅仅是排序。这就是他们对系统的输入。然后，RLHF 系统只是发现了那些会被人类评为高分的回应类型。因此，这种间接的步骤使得模型能够变得更好。这就是 RLHF 的优势所在。它让我们能够运行强化学习。

实践证明，这种方法能产出更优质的模型。它让人们能够贡献自己的监督指导，甚至无需像撰写理想回复那样完成极其困难的任务。但遗憾的是，强化学习人类反馈也伴随着显著的弊端。因此，最主要的问题在于，我们实际上是在进行强化学习，但并非基于真实人类及其判断，而是基于对人类的有损模拟。这种有损模拟可能会产生误导，因为它只是一个模拟。它只是一个输出分数的语言模型。而且它可能无法在所有不同情况下完美反映一个真正有头脑的人类的观点。这是第一点。

实际上还有更微妙和隐蔽的问题，严重阻碍了 RLHF 作为一种技术，使我们无法真正将其扩展到更智能的系统。强化学习非常擅长找到方法来操纵模型和模拟环境。我们在这里构建的用于评分的奖励模型，这些模型都是基于 Transformer 架构。这些 Transformer 是庞大的神经网络。它们拥有数十亿参数并模仿人类，但只是以某种模拟的方式实现。现在的问题是，这些都是庞大而复杂的系统，对吧？这里有数十亿参数只输出一个评分。事实证明，存在操纵这些模型的方法。

你可以发现各种不属于训练集的输入。这些输入莫名其妙地获得了极高的评分，但却是虚假的。因此，如果你长时间运行 RLHF（例如进行 1000 次更新，这算是相当多的更新），你可能会期望你的笑话会变得更好，你会得到一些关于鹈鹕的爆笑段子，但实际情况并非如此。

情况是这样的：在最开始的几百步里，关于鹈鹕的笑话可能确实有所改进。但随后它们会急剧恶化，开始产生极其荒谬的结果。比如，关于鹈鹕的最佳笑话开始变成“那个，那个，那个，那个，那个，那个，那个，那个，那个”。

这简直毫无道理，对吧？比如你乍看之下，这怎么会是个顶级笑话呢？但当你把那些"的、的、的、的、的"输入奖励模型时，本该得零分，可实际上模型却疯狂追捧这个笑话。它会告诉你这串"的、的、的、的、的"能得满分1.0分——这明明就是个顶级笑话啊，简直荒谬透顶对不对？但究其原因，这些模型不过是人类的模拟器，本质上就是一堆庞大的神经病。你总能在参数空间的某个犄角旮旯里，找到些能输出荒唐结果的诡异输入组合。

这些例子就是所谓的*对抗性样本*。虽然我不会深入探讨这个话题，但它们都是针对模型的对抗性输入。这些特定的微小输入会钻模型的空子，最终在输出端产生荒谬的结果。现在你可能会想象这样做。你会说，好吧，这个，这个，这个，显然不是一分。这显然是一个低分。所以让我们把这个，这个，这个，这个，这个添加到数据集中，并给它一个极其糟糕的排序，比如打五分。确实，你的模型会学到这个，这个，这个，这个，这个应该有一个非常低的分数，它会给出零分。问题在于，模型中基本上总是隐藏着无限数量的无意义的对抗性示例。

如果你多次重复这个过程，不断向奖励模型添加无意义的内容并给予极低评分，那么你将永远无法赢得游戏。你可以进行无数轮这样的操作，而只要时间足够长，强化学习总会找到方法来操控模型。它会发现对抗性样本。它会用毫无意义的结果获得极高的分数。从根本上说，这是因为我们的评分函数是一个巨大的神经网络，而强化学习非常擅长找到欺骗它的方法。长话短说，你总是需要运行几百次 RLHF 更新。

模型会不断优化，然后你只需进行裁剪就完成了。你不能过度对抗这个奖励模型，因为优化会开始利用它，你基本上就是裁剪一下，调用一下，然后发布。当然，你可以改进奖励模型，但最终你总会遇到这类情况。所以 RLHF，基本上我通常会说 RLHF 不是 RL。我的意思是，显然 RLHF 是RL，但它不是那种神奇的 RL。这不是你可以无限运行的 RL。

这类问题，比如你得到了具体正确答案的情况，你无法轻易获得。你要么得到了正确答案，要么没有。评分函数也简单得多。你只是在观察框选区域，看看结果是否正确。因此，要获得这些功能非常困难，但操纵奖励模型是可行的。现在，在这些可验证的领域中，你可以无限期地运行强化学习。

你可以运行数万、数十万步，发现各种我们可能从未想过的疯狂策略，这些策略在这些问题上表现得非常出色。在围棋游戏中，基本上没有办法直接获得胜利或失败。我们有一个完美的模拟器，我们知道所有不同的棋子放在哪里，我们可以计算出是否有人获胜。这是无法获得的。因此，你可以无限期地进行强化学习，最终甚至能击败李世石。

但对于这类可被操纵的模型来说，你无法无限重复这一过程。所以我有点不认为 RLHF 是真正的强化学习，因为奖励函数是可以被操控的。它更像是微调领域的小修小补。虽然能带来一些改进，但本质上并没有建立正确的框架——那种你投入更多算力、运行更长时间就能获得显著提升和神奇效果的体系。所以从这个意义上说，它不是强化学习。它缺乏魔力，从这个角度讲也不是强化学习。但它可以微调你的模型，获得更好的性能。

确实，如果我们回顾一下 ChatGPT，GPT-4o 模型已经经过了 RLHF（人类反馈强化学习）的优化，因为它效果不错，但它与我们通常理解的强化学习（RL）并不完全相同。 RLHF 更像是一种微调，能稍微提升模型的性能。这大概就是我对它的理解。好的，以上就是我想介绍的大部分技术内容。我带大家了解了训练这些模型的三个主要阶段和范式：预训练、监督微调和强化学习。

我向你们展示了这些步骤大致对应于我们已用于教导儿童的过程。具体来说，我们将预训练比作通过阅读说明获取基础知识，监督微调则是通过大量范例模仿专家并练习解题的过程。唯一的区别在于，我们现在需要为大型语言模型和人工智能编写涵盖人类知识所有学科的教材，包括我们希望它们实际应用的领域，如编程、数学以及几乎所有其他学科。

因此，我们正在为他们编写教材，完善我提出的所有高层次算法，当然还要在高效大规模训练这些模型方面做到极致。具体来说，虽然我没有深入太多细节，但这些任务规模极其庞大且复杂，属于分布式作业，需要在数万甚至数十万个 GPU 上运行。而实现这一切所需的工程技术，确实代表了当前计算机技术在这种规模下所能达到的最高水平。所以我并没有过多涉及这方面，但归根结底，所有这些看似简单的算法背后都蕴含着非常严肃的探索。此外，我也稍微谈到了这些模型的心智理论问题，我希望你们记住的是：这些模型确实很出色，但它们本质上只是辅助你工作的工具。你不应该完全信任它们——我也给你们展示过一些相关的例子。

尽管我们已经采取了缓解幻觉的措施，但模型并不完美，它们仍会产生幻觉。随着时间的推移，情况有所改善，未来还会继续改进，但幻觉现象仍可能发生。换句话说，除此之外，我还介绍了我称之为"瑞士奶酪"式的大语言模型能力认知框架，这应该是你们需要牢记在心的。这些模型在众多不同学科领域表现出色得令人难以置信，但在某些独特案例中却会莫名其妙地出错。比如问 9.11 和 9.9 哪个更大？模型可能答不上来，但它同时又能解答奥赛难题。这就像瑞士奶酪上的孔洞，类似的漏洞还有很多，你肯定不想被它们绊倒。

所以不要把这些模型当作完美无缺的典范。要检查它们的工作成果。把它们当作工具来使用。将它们作为灵感来源。将它们用于初稿，但要把它们当作工具来使用，并最终对你工作的成果负责。这就是我大致想说的内容。这就是它们的训练方式，也是它们的本质。

### 未来预览

接下来，我们将探讨这些模型的未来潜力，看看它们可能具备哪些即将到来的能力，以及在哪里可以找到这些模型。我有几个要点可以分享，这些都是可以期待的进展。首先，你会注意到模型将很快变得多模态化。

我所讨论的一切都围绕着文本展开，但很快我们将拥有不仅能处理文本，还能原生且轻松地操作音频的 LLM，这意味着它们可以听和说；同时也能处理图像，从而能够看和绘制。我们已经看到了这一切的雏形，但这一切都将原生地在语言模型内部完成，这将促成一种近乎自然的对话。大致来说，这与我们之前讨论的所有内容并无本质区别，因为从根本上讲，你可以将音频和图像进行标记化处理，并应用我们之前讨论过的完全相同的方法。

所以这并不是根本性的改变。我们只需要添加一些标记。举个例子，对于音频的标记化，我们可以查看音频信号的频谱图切片，然后对其进行标记化，只需添加一些突然代表音频的新标记，并将它们加入上下文窗口，然后像上面那样进行训练即可。对于图像也是如此，我们可以使用图像块，将它们分别转换为标记，那么图像是什么呢？图像不过是一系列标记的序列。这种方法实际上相当有效，而且在这一方向上已有大量早期研究。因此，我们可以创建代表音频、图像以及文本的标记流，将它们交织在一起，并在同一个模型中同时处理所有这些内容。这就是多模态的一个例子。

其次，目前人们非常感兴趣的是，大部分工作都是我们把单个任务像放在银盘上一样交给模型，比如“请帮我解决这个任务”。然后模型就会完成这个小任务。但如何有条理地组织任务执行以完成工作，仍取决于我们。目前这些模型还不具备在长时间内以连贯且能自我纠错的方式完成这类工作的能力，因此它们无法完全串联任务来执行这些耗时更长的工作。

但他们正在逐步实现，而且这一过程会随时间不断改进。不过，这里可能发生的情况是，我们将开始看到所谓的"智能代理"——它们会持续执行任务，由你进行监督和观察工作进展，这些代理会时不时向你汇报进度等等。因此，我们将看到更多长期运行的智能代理，这些任务不再只是几秒钟就能完成的即时响应，而是会持续数十秒、甚至数分钟或数小时之久。但正如我们之前讨论的，这些模型并非万无一失。因此，所有这些都需要监督。例如，在工厂中，人们会讨论自动化的人机比例。我认为在数字领域我们也会看到类似的情况，届时我们将讨论人类与智能体之间的比例关系，人类将更多地扮演数字领域中智能体任务的监督者角色。

接下来，我认为一切将变得更加无处不在却又隐于无形，就像被整合进各种工具中，渗透到每个角落。此外，有点类似电脑操作。目前这些模型还无法代表你执行操作，但我认为这是另一个要点。如果你看过 Chatgpt 推出的操作员功能，那就是一个早期的例子，你可以真正将控制权交给模型，让它代表你执行键盘和鼠标操作。因此，我认为这一点也非常有趣。

最后我想说的是，这个领域还有很多潜在的研究可做。其中一个例子就是类似于测试时训练这样的方向。请记住，我们上面所做和讨论的一切都包含两个主要阶段。首先是训练阶段，我们会调整模型的参数以使其能很好地完成任务。一旦获得这些参数，我们就会固定它们，然后将模型部署用于推理。从那时起，模型就固定了。它不再改变，也不会从测试阶段的所有操作中学习。

这是一个固定数量的参数。唯一变化的是上下文窗口中的标记。因此，模型能够进行的唯一学习或测试时学习类型，就是其动态可调整上下文窗口的上下文学习，这取决于它在测试时所执行的操作。但我认为这与人类仍有不同，人类实际上能根据所做的事情来学习，尤其是睡觉时。比如，你的大脑会更新参数之类的。目前这些模型和工具还没有类似的功能。

我认为还有很多更复杂的想法有待探索。尤其是考虑到上下文窗口是一种有限且宝贵的资源，这一点尤为必要。特别是当我们开始处理长时间运行的多模态任务时，比如输入视频，这些标记窗口将变得极其庞大——不是几千甚至几十万，而是远超这个数量级。而我们目前唯一能用的技巧就是延长上下文窗口。但我认为仅靠这种方法无法扩展到实际需要长时间运行的多模态任务。因此，我认为在某些领域、在某些迷宫般复杂的情况下，当这些任务需要非常长的上下文时，我们需要新的思路。

以上便是您可以期待的一些即将到来的示例。

### 追踪 LLMs

现在让我们来看看您可以在哪里实际跟踪这些进展，并了解该领域最新、最前沿的动态。我认为我一直用来保持更新的三个资源是：

第一，lmarena。让我为你介绍 lmarena。这本质上是一个大型语言模型排行榜，它对所有顶级模型进行排名。而排名是基于人类对比评测得出的。人类向这些模型提问，然后判断哪个模型给出的答案更好。他们不知道哪个模型对应哪个答案，只是单纯地评估哪个答案更优。你可以计算一个排名，然后得到一些结果。因此，你能听到的是，你在这里看到的是不同的组织，比如谷歌和 Gemini，它们生产这些模型。当你点击其中任何一个时，它会带你到托管该模型的地方。

然后我们看到谷歌目前处于领先地位，OpenAI 紧随其后。这里我们看到 DeepSeek 排在第三位。现在，之所以这是个大事，是因为这里的最后一列。你看许可证。DeepSeek是一个 MIT 许可的模型。它是开源的。任何人都可以使用这些等待。任何人都可以下载它们。任何人都可以托管自己的 DeepSeek 版本，并且可以按照自己喜欢的方式使用它。因此，这不是一个你无法访问的专有模型。它基本上是一个开放的等待发布。所以，像这样强大的模型以开放等待的方式发布，这在某种程度上是前所未有的。团队做得非常棒。

接下来，我们还有来自谷歌和 OpenAI 的几款模型。继续往下滑动，你还会看到一些熟悉的面孔。所以XAI在这里，Anthropic 的 Sonnet 排在第 14 位。然后是 Meta 的 Llama在这里。Llama 和 DeepSeek 类似，是一个开源模型。但它是放在这里，而不是上面。我得说这个排行榜长期以来一直很不错。但我确实觉得最近几个月它变得有点被操纵了。而且我不像以前那样信任它了。从经验来看，我觉得很多人都在使用 Anthropic 的 Sonnet 模型，它确实是个很棒的模型。但它却排在第 14 名这么靠后的位置。相反，我认为使用 Gemini 的人并不多，但它的排名确实非常高。因此，我建议先把它作为初步选择，然后针对你的任务多尝试几种模型，看看哪个表现更好。

我要指出的第二点是 ainews（https://news.smol.ai/）。所以 ainews 这个名字起得不算很有创意，但它是由 SWIX 和朋友们制作的一份非常优秀的简报。感谢你们一直坚持更新。这份简报对我帮助很大，因为它涵盖的内容极其全面。所以如果你去档案库看看，就会发现它几乎每隔一天就会发布一次。内容非常全面，其中一部分是由人类撰写并由人类编辑的。但其中大部分内容是由大语言模型自动生成的。你会发现这些内容非常全面，如果你仔细阅读的话，基本不会遗漏任何重要信息。当然，你可能不会真的去通读它，因为它实在太长了。但我确实认为顶部的这些摘要相当不错，而且我认为有人工审核。所以这对我非常有帮助。

最后我想提到的是X和Twitter。很多 AI 相关的动态都发生在 X 上。所以我建议你关注那些你喜欢且信任的人，这样你也能在 X 上获取最新最棒的内容。

以上就是长期以来对我最有效的几个主要渠道。最后，我想简单说说在哪里可以找到这些模型以及如何使用它们。

### 哪里找 LLMs

所以我要说的第一个方法是，对于任何大型专有模型，你只需访问该 LLM 提供商的网站。例如，OpenAI 的网站是 chat.com，我相信现在确实可以用了。这就是 OpenAI 的情况。对于Gemini，我认为是 gemini.google.com 或 AI Studio。不知道为什么他们有两个版本，我也不太明白。没人明白。

对于像 DeepSeek、llama 这样的开源权重模型，你需要去找一个大型语言模型的推理服务提供商。我最喜欢的是 together.ai。我之前给大家演示过，当你进入 together.ai 的 playground 时，你可以选择很多不同的模型。这些都是不同类型的开源模型。你可以在这里与他们交谈作为示例。现在，如果你想使用基础模型，比如一个基础模型，那么我认为在这里找到基础模型并不常见，即使在这些推理提供商中也是如此。它们都专注于助手和聊天功能。

所以我觉得即使在这里我也看不到基础模型。对于基础模型，我通常会去 Hyperbolic，因为他们提供我的 LLAMA 3.1 基础模型，我非常喜欢那个模型。你可以直接在这里和它对话。据我所知，这是个搭建基础模型的好地方。我希望有更多人能托管基础模型，因为在某些情况下它们既实用又有趣。最后，你还可以选用一些较小的模型，在本地运行它们。因此，举例来说，像 DeepSeek 这样最大的模型，你将无法在 MacBook 上本地运行。但 DeepSeek 模型也有被称为"蒸馏版"的较小版本。此外，你还可以用较低的精度来运行这些模型。

所以，虽然达不到像 DeepSeek 上的 FP8 或 LLAMA 上的 BF16 那样的原生精度，但比那要低得多得多。如果你不完全理解这些细节也不用担心，你可以运行经过蒸馏的较小版本，甚至以更低的精度运行，这样它们就能在你的电脑上运行了。所以你实际上可以在笔记本电脑上运行相当不错的模型。

我最常去的地方，我想是 LM Studio，这基本上是一个你可以下载的应用。我觉得它的界面真的很丑。我不喜欢它展示的那些基本上没什么用的模型。大家都只想用DeepSeek。所以我不明白为什么他们给你这 500 种不同的模型。搜索起来特别复杂，还得选不同的蒸馏版本和精度，简直让人一头雾水。但一旦你真正理解了它的运作原理——这需要单独的视频来讲解——你就可以加载一个模型了。比如这里我加载了 LLAMA 3.2 Instruct 1B 参数版本，然后就能直接和它对话了。

于是我向它要鹈鹕笑话，然后可以再要一个，它就会再给我一个，如此反复。这里发生的一切都在你的电脑本地运行。所以我们实际上并没有与任何其他人交互。这是在MacBook Pro 的 GPU 上运行的。这非常棒。完成后你可以卸载模型，这样就能释放内存。

所以 LM Studio 可能是我最喜欢的一个，尽管我认为它在 UI/UX 方面存在很多问题，而且几乎是为专业人士量身定制的。但如果你在 YouTube 上看一些视频，我想你可以学会如何使用这个界面。以上就是关于在哪里找到它们的一些介绍。

### 总体概要

那么现在让我回到我们最初的问题。问题是：当我们访问 Chatgpt.com，输入某个查询并点击“搜索”时，这里到底发生了什么？我们看到的是什么？我们在与什么对话？这一切是如何运作的？我希望这段视频能让你对这些模型的训练机制及其返回结果的内在原理有所了解。

具体来说，我们现在知道你的查询首先会被分解成一个个标记（tokens）。所以我们来到 TickTokenizer。这里是用户查询的格式位置。我们基本上就是把查询内容放在这里。因此，我们的查询进入了我们这里所说的对话协议格式，也就是我们维护对话对象的方式。这样它就被插入到那里。然后，整个过程最终只是一个标记序列，一个底层的一维标记序列。
 
所以 Chatgpt 看到了这个标记序列。然后当我们点击开始，它基本上会继续将标记追加到这个列表中。它会继续这个序列。它的作用类似于 token 自动补全。具体来说，它给了我们这个响应。所以我们基本上可以直接把它放在这里。而我们看到它继续生成的标记。这些大致就是它持续生成的标记。现在问题变成了：好吧，为什么模型会生成这些标记？这些标记是什么？它们从何而来？我们到底在和什么对话？以及我们该如何编程这个系统？于是这就是我们转换思路的地方。

我们讨论了其背后的技术细节。这个过程分为三个阶段，第一阶段是预训练阶段，其核心任务是从互联网获取知识并将其编码到神经网络的参数中。因此，神经网络会从互联网中吸收大量知识。但真正体现个性的地方在于这里的监督微调过程。具体来说，像 OpenAI 这样的公司会精心策划一个庞大的对话数据集，比如 100 万条涵盖各种主题的对话。这些对话将发生在人类和助手之间。

尽管在整个过程中使用了大量合成数据生成和大型语言模型的帮助，但本质上这是一项需要大量人力参与的数据整理工作。具体来说，这些人力是 OpenAI 雇佣的数据标注员，他们会学习标注指南。他们的任务是为任意提示创建理想的助手回应。

因此，他们通过示例来教导神经网络如何响应提示。那么，该如何理解这里返回的内容呢？这到底是什么？我认为正确的理解方式是，这是 OpenAI 数据标注员的神经网络模拟。也就是说，就像我把这个查询交给 OpenAI 的数据标注员，然后这位标注员首先阅读 OpenAI 的所有标注指令，再花两个小时写出针对这个查询的理想助手回复，最后交给我。现在，我们实际上并没有那样做，对吧？因为我们没有等上两个小时。所以，我们在这里得到的是那个过程的神经网络模拟。我们必须记住，这些神经网络的运作方式与人类大脑不同。

他们与众不同。对他们来说容易或困难的事情与人类不同。因此，我们实际上只是在模拟。所以这里我向你们展示的，这是一个 token 流，本质上这是一个神经网络，中间有一大堆激活函数和神经元。这是一个固定的数学表达式，它将来自 token 的输入与模型的参数混合在一起，经过混合后，就能得到序列中的下一个 token 。但这是针对每一个 token 进行的有限计算量。因此，这是一种有损的人类模拟方式，在某种程度上受到了这样的限制。无论人类写下什么，语言模型都只是在序列中逐个标记地进行模仿，且每个标记只能进行这种特定的计算。我们也看到，由于这种机制和认知差异，模型会在多方面表现出不足。

使用它们时必须非常谨慎。例如，我们发现它们可能会出现幻觉现象。此外，我们观察到大型语言模型的能力存在类似瑞士奶酪模型的特征——本质上就像奶酪上布满了孔洞。有时候模型就是会莫名其妙地犯傻。尽管它们能完成许多神奇的任务，但偶尔就是无能为力。可能你给它们的思考空间不够，也可能它们的"心算"崩溃了，于是就开始胡编乱造。也许他们突然不会数字母了，或者无法告诉你 9.11 比 9.9 小，这看起来有点蠢。所以这是一种漏洞百出的能力，我们必须小心对待。我们也看到了其中的原因。

但从根本上说，这就是我们对反馈结果的理解。它再次模拟了遵循 OpenAI 指令的人类数据标注员的神经网络运作方式。这就是我们所获得的反馈。现在，我确实认为当你真正去尝试使用某种思维模型时，比如 O3 mini-high，情况会有所不同。原因在于 GPT-4.0基本上不进行强化学习。它确实使用了 RLHF（人类反馈强化学习），但我已经告诉过你们，RLHF 并不等同于 RL（强化学习）。那里没时间施展魔法。可以把它看作只是一种微调。但这些思维模型确实使用了强化学习。

因此，他们经历了完善思维过程的第三阶段，发现新的思维策略和解决问题的方法，这些方法有点像你头脑中的内心独白。他们在 OpenAI 等公司创建、整理并供大型语言模型使用的大量练习题上练习这些方法。所以，当我来到这里，与一个思维模型交谈并输入这个问题时，我们所看到的已不仅仅是对人类数据标注员的简单模拟。这实际上有些新颖、独特且有趣。当然，OpenAI 并没有向我们展示其背后的思考过程和推理链条，但我们知道这样的东西确实存在。这就是一个总结。

而我们在这里得到的，实际上不仅仅是对人类数据标注员的模仿。它其实是某种新颖、有趣且令人兴奋的事物，因为这是一种在模拟过程中涌现出的思维功能。它并非仅仅在模仿人类数据标注员。它来自于这个强化学习的过程。当然，在这里我们并没有给它展示的机会，因为这不是一个数学或推理问题。粗略地说，这只是一些创造性的写作问题。

我认为，在可验证领域内发展出的思维策略能否迁移并推广到其他不可验证的领域（如创意写作），这是一个悬而未决的问题。可以说，该领域对这种迁移的程度尚不明确。因此，我们无法确定是否能在所有可验证事物上进行强化学习，并看到其对这类不可验证提示所产生的效益。所以这是一个悬而未决的问题。另一个有趣之处在于，这里的强化学习仍然非常新颖、原始且处于萌芽阶段。因此，我们只是在推理问题中看到了伟大潜力的初步迹象。

我们正在见证一种原则上能够实现类似 MOVE37 水平的事物，但不是在围棋领域，而是在开放领域的思维和问题解决上。从理论上讲，这种范式能够做出一些真正酷炫、新颖且激动人心的事情，甚至是人类从未想到过的事情。原则上，这些模型能够进行人类从未有过的类比推理。

所以我认为这非常令人兴奋。虽然它们已经存在，但目前仍处于非常早期的阶段，这些还只是原始模型。它们在可验证的领域（如数学和代码等）会表现得尤为出色。使用它们进行探索和思考非常有趣。大致就是这样。嗯，我想说这些是目前可用功能的主要概况。总的来说，现在进入这个领域是一个极其激动人心的时刻。我个人每天都在使用这些模型，少则几十次，多则上百次，因为它们极大地加速了我的工作。我想很多人都看到了同样的现象。我认为这些模型将创造巨大的财富，但也要意识到它们的一些缺点。即使是强化学习模型，也会存在一些问题。把它们当作工具箱中的工具来使用，不要完全信任它们，因为它们有时会做出愚蠢的事情，产生幻觉，或者跳过一些心算步骤而得不到正确答案。有时它们甚至无法正确计数。所以，把它们当作工具箱中的工具，检查它们的工作，并对自己的工作成果负责。用它们来获取灵感、起草初稿，向它们提问，但始终要检查和验证。如果你这样做，你会在工作中非常成功。希望这个视频对你来说既有用又有趣。

希望你玩得开心，而且已经过了很长时间，所以我为此道歉，但希望这些内容对你有用。好的，我们下次见。

