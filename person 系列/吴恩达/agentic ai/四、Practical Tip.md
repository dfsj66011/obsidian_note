
## 4.1 Evaluations（evals）

在本模块中，我想与大家分享构建自主 AI 工作流的实用技巧。希望这些技巧能让你比普通开发者更高效地打造这类系统。根据我的经验，开发自主 AI 系统时，往往很难提前预判哪些环节会顺畅运行、哪些环节可能存在问题，因此明确该把精力集中在何处至关重要。

因此，常见的建议是尝试先构建一个快速而粗糙的系统开始，这样你就可以试用并观察它，看看哪些地方还没有达到你的期望，从而更有针对性地进一步开发。相比之下，我发现花太多周时间坐在那里理论化和假设如何构建它有时效果不佳。通常更好的做法是以一种安全、合理的方式快速构建一个系统，不会泄露数据，以一种负责任的方式来做，但只是快速构建一些东西，这样你就可以查看它，然后利用这个初始原型来确定优先级并尝试进一步开发。

让我们从一个原型构建后可能发生的情况开始举例。我想以之前提到的发票处理流程作为第一个例子，该流程需要提取四个必填字段，然后将其保存到数据库记录中。构建完这样一个系统后，你可能会做的一件事是找几张发票，比如 10 张或 20 张，逐一查看它们的输出结果，看看哪些地方处理得好，是否存在错误。假设你查看了 20 张发票，发现第 1 张发票没有问题，输出结果看起来是正确的。对于发票 2，可能是混淆了发票的日期（即发票开具的时间）和发票的到期日，而在这个任务中，我们需要提取的是到期日，以便按时付款。因此，我可能会在文档或电子表格中记下，发票 2 的日期被混淆了。也许发票 3 没有问题，发票 4 也没有问题，以此类推。但通过这个例子，我发现有很多情况下我都混淆了日期。

因此，基于对一系列类似案例的分析，在这种情况下，你可能会得出一个常见的错误模式：系统在处理日期时存在困难。于是，你或许可以考虑一方面改进系统以更好地提取截止日期，另一方面也可以设计一个评估标准来衡量其提取截止日期的准确性。相比之下，如果你发现系统在提取账单地址时出现错误——谁知道呢，也许你有一些名称听起来不太常见的账单方，或者尤其是那些名称可能并非全由英文字母组成的国际账单方，那么系统在处理这些账单方时可能会遇到困难。这时，你可能更应该专注于构建一个针对账单地址的评估机制。因此，快速搭建一个简易系统并观察其输出之所以如此有帮助，其中一个原因就在于它甚至能帮你决定应该将主要精力投入到评估哪个环节上。

如果你已决定修改系统以提高提取发票到期日的准确性，那么为了跟踪进展，创建一个评估或测试来衡量日期提取的准确性可能是个好主意。可能有多种方法可以实现这一点，但让我分享一下我可能会怎么做。为了创建一个测试集或评估集，我可能会找到 10 到 20 张发票，并手动记录下到期日是什么。例如，某张发票的到期日是 2025年8月20日，我会将其记录为标准格式的年、月、日。

然后，为了便于后续在代码中进行评估，我可能会这样设计给大语言模型的提示：要求它始终以“年、月、日”的格式输出截止日期。这样一来，我就能编写代码来提取大语言模型输出的那个特定日期——也就是我们真正关心的截止日期。具体来说，我会用正则表达式进行模式匹配，比如匹配四位数的年份、两位数的月份和两位数的日期，并将其提取出来。之后，我只需编写代码来验证提取出的日期是否与我事先记录的真实标注日期相符即可。因此，使用一个包含大约 20 张发票的评估集，我通过构建和调整来观察提取日期正确的百分比是否随着我对提示或其他系统部分的调整而提高。

总结一下我们目前所看到的：我们构建了一个系统，然后通过查看输出来发现它可能在哪些方面表现不佳，比如到期日期错误。然后，为了推动这一重要输出的改进，可以设置一个小型评估，比如仅用 20 个例子来帮助我们跟踪进展。这样我就可以回到两个提示，尝试不同的算法等等，看看是否能提高这个截止日期准确性的指标。

因此，改进一个自主 AI 工作流程通常会是这样的感觉：查看输出，找出问题所在，如果你知道如何修复，就直接修复。但如果你需要一个更长的改进过程，那就设置一个评估，并利用它来推动进一步的开发。另外需要考虑的一点是，如果你工作一段时间后，发现最初那 20 个例子不够理想——可能它们没有涵盖所有你想测试的情况，或者 20 个例子实在太少了——你随时可以逐步扩充评估集，确保它能更好地反映你对系统表现是否足够满意的个人判断。这只是一个例子。

在第二个例子中，我们来看如何构建一个营销文案助手，用于为 Instagram 撰写标题。为了保持简洁，假设我们的营销团队告诉我们，他们希望标题最多不超过 10 个单词。因此，我们会有一张产品的图片，比如一副我们想要推广的太阳镜，然后有一个用户查询，比如“请写一个标题来销售这些太阳镜”，然后让一个 LLM 分析图片和查询，并生成对太阳镜的描述。

营销文案助手可能会出现各种问题，但假设你查看输出结果时发现生成的文案或文本大体上听起来还不错，只是有时候可能太长了。比如，对于太阳镜的输入要求生成 17 个词，咖啡机的话可以接受"时尚"这样的描述，蓝色衬衫要求 14 个词，搅拌机则要 11 个词。在这个例子中，大语言模型似乎很难遵循字数要求。再次强调，营销文案助手可能会出现很多问题。但如果你发现它在控制输出长度方面有困难，可以建立一个评估机制来跟踪这一点，这样你就能不断改进，确保它越来越擅长遵循字数要求。

因此，要创建一个评估来测量文本长度，你可以做的是创建一组测试样本，比如标记一副太阳镜、一台咖啡机等等，可能需要创建 10 到 20 个例子。然后你会通过你的系统运行每一个样本，并编写代码来测量输出的字数。这是一段 Python 代码，用于测量一段文本的字数。最后，你需要将生成的文本长度与 10 个单词的目标限制进行比较。所以如果字数等于 10，那么我就是正确的，加一。

与之前的发票处理示例不同之处在于，这里没有针对每个样本的标准答案。所有样本的目标值都是固定的 10。而相比之下，在发票处理示例中，我们需要为每张发票生成一个定制化的目标标签（即正确的付款截止日期），并根据每个样本对应的标准答案来验证输出结果。虽然我使用了非常简单的流程来生成这些描述文本，但这类评估方法同样适用于更复杂的生成流程。

让我再举最后一个例子，我们将重新审视之前讨论过的研究代理。如果你观察研究代理在不同输入提示下的输出结果，比如当你要求它撰写一篇关于黑洞科学最新突破的文章时，你发现它遗漏了一些备受瞩目的研究成果和新闻报道。这样的结果显然不尽如人意。或者，如果你让它研究在西雅图租房和买房的利弊，它似乎做得不错。再比如让它研究用于水果采摘的机器人技术时，表现也尚可。嗯，它没有提到一家领先的设备公司。

因此，根据这个评估，看起来它有时会遗漏人类专家作者会抓住的一个非常重要的点。那么，我会创建一个评估来衡量它抓住最重要观点的频率。例如，你可以提出一些关于黑洞、机器人收割等的示例提示。对于每一个提示，想出三到五个关于这些主题的黄金标准讨论点。请注意，这里我们确实为每个示例提供了标注，因为黄金标准要点（即最重要的要点）对于每个示例都是不同的。有了这些真实标注，你可以使用 LLM 作为评判工具，来统计有多少黄金标准要点被提及。例如，一个提示可能是：确定提供的文章中包含了五个黄金标准要点中的多少个。你可以选择性地提供提示、文章文本、黄金标准要点等，并让它返回一个包含两个 G 的 JSON 对象，该对象会评分（0 到 5分）以及解释说明。

这样你就能为评估集中的每个提示获得一个分数。在这个例子中，我使用大语言模型作为评判者来计算有多少个讨论点被提及，因为谈论这些讨论点的方式多种多样，简单的正则表达式或代码模式匹配可能效果不佳，这就是为什么你可以使用大语言模型作为评判者，并将其视为一种稍微主观的评估方式，来判断是否充分提及了事件视界等内容。那么，这就是你构建评估的第三个示例。

为了思考如何为你的应用程序构建评估，你构建的评估通常需要反映出你在应用程序中看到的或担心出错的地方。事实证明，从广义上讲，评估有两个维度。第一个维度是评估输出的方式。在某些情况下，你可以通过编写代码进行客观评估，有时则使用 LLM 作为评判者来进行更主观的评估。

另一个维度则是是否存在针对每个样本的真实标准。以发票日期提取为例，我们编写代码来验证提取的日期是否准确，这种情况下每个发票都有不同的实际日期，因此存在针对每个样本的真实标准。但在检查营销文案长度的例子中，所有样本的长度限制都是 10，因此该问题不存在针对每个样本的真实标准。相比之下，在计算黄金标准谈话要点时，每个示例都有真实基准，因为每篇文章都有不同的重要谈话要点。但我们使用大语言模型作为评判者来阅读文章，以判断这些话题是否被充分提及，因为提及谈话要点的方式多种多样。而四个象限中的最后一个则是没有每个示例真实基准的大语言模型评判。

我们在评分图表时看到了这一点。当我们查看咖啡机销售的可视化数据时，如果你要求它根据评分标准创建图表，比如是否有清晰的访问标签等，每个图表都有相同的评分标准，这将使用 LLM 作为评判者，但没有每个示例的真实情况。因此，我发现这个二乘二的网格可能是一种有用的方式，可以帮助你思考为应用程序构建的不同类型的评估。

顺便说一下，这些有时也被称为端到端评估，因为一端是输入端，即用户查询提示，另一端是最终输出。因此，这些都是对整个端到端系统性能的评估。在结束这个视频之前，我想分享一些设计端到端评估的最后建议。

首先，开始时使用快速而粗糙的评估是可以的。我觉得我看到很多团队几乎陷入了瘫痪，因为他们认为构建评估是一项需要数周时间的大工程，所以他们开始的时间比理想情况要长。但我认为，正如你迭代优化工作流程那样，你也应该计划对评估体系进行迭代改进。一开始你可以准备 10 到 20 个案例作为初步评估样本，编写代码或尝试用大语言模型充当裁判——总之先行动起来，获取一些能辅助人工判断的量化指标。最终将人工评估与数据指标相结合，就能为决策提供更全面的依据。

随着时间的推移，评估变得越来越精细，你就可以逐渐将更多的信任转移到基于指标的评估上，而不必每次调整提示时都要阅读数百个输出结果。在这个过程中，你可能会发现持续改进评估的方法。因此，如果你一开始有 20 个例子，你可能会遇到评估无法准确反映你对哪个系统更好的判断的情况。

也许你更新了系统后，看着它觉得这次肯定能表现得更好，但评估结果却显示新系统并未获得更高的分数。如果遇到这种情况，通常可以考虑扩大评估数据集，或者调整评估输出的方式，使其更符合你对哪个系统实际表现更好的判断。这样一来，你的评估会随着时间的推移而不断改进。最后，关于如何利用评估来获得下一步工作的灵感，许多自主工作流程正被用于自动化那些原本由人类完成的任务。

因此，我发现对于这类应用，我会寻找那些表现不如人类专家的地方，这往往能给我灵感，让我知道应该把精力集中在哪些方面，或者哪些类型的例子可以让我的代理工作流程比现在表现得更好。所以，我希望在你构建完那个快速而粗糙的系统后，你能思考何时开始引入一些评估来追踪系统中潜在的问题点，这将有助于你推动系统的改进。

除了帮助您推动改进外，还有一种评估方法可以帮助您优化整个代理系统。哪些组件最值得您关注？因为代理系统通常包含许多部分。那么，哪一部分最值得您花时间去改进呢？事实证明，能够做好这一点对于推动代理工作流程的高效发展是一项非常重要的技能。在下一个视频中，我将深入探讨这个话题。让我们继续观看下一个视频。

## 4.2 Error analysis and prioritizing next steps

假设你已经构建了一个代理工作流程，但它的表现还不尽如人意——顺便说一句，这种情况经常发生在我身上，我经常会快速搭建一个简陋的系统，但它的效果并不理想。那么问题来了：你应该把精力集中在哪些方面来改进它呢？事实证明，代理工作流程包含许多不同的组成部分，其中某些部分的改进可能比其他部分带来更大的成效。因此，你选择在哪些方面投入精力，将极大地影响你改进系统的速度。

我发现，衡量一个团队效率和质量的最重要指标之一，就是他们能否通过严谨的错误分析流程来明确改进方向。这是一项至关重要的技能。让我们来看看如何进行错误分析。在研究智能体案例中，我们在之前的视频中已经做过错误分析，发现它在撰写特定主题文章时经常遗漏关键要点，而这些要点人类专家通常都会涵盖。

那么现在你已经发现了这个有时会遗漏关键点的问题，该如何确定需要改进的地方呢？事实上，在这个工作流程的众多环节中，几乎任何一个步骤都可能导致关键点缺失的问题。例如，可能是第一个大语言模型生成的搜索词不够理想，导致它检索了错误的内容而没有找到正确的文章；又或者是使用的网络搜索引擎本身效果不佳。

市面上有多种网络搜索引擎可供选择，实际上我自己在基础应用中就经常使用好几个，其中一些确实比其他的更好用。也可能网络搜索本身没有问题，但当我们把网络搜索结果列表交给大语言模型处理时，它可能在筛选最优下载项方面表现欠佳。相比之下，网页抓取技术在这种情况下问题可能更少——前提是你能准确抓取网页内容。但将网页内容输入大语言模型后，模型可能会忽略我们已抓取文档中的某些关键信息。

事实证明，有些团队在查看这些内容时，会凭直觉选择其中一个组件进行开发。有时这种方法可行，但有时却会导致数月的工作却对系统整体性能的提升微乎其微。因此，与其凭直觉决定从众多组件中选择哪一个来开展工作，我认为更好的做法是进行错误分析，以更好地理解工作流程中的每个步骤。

特别是，我经常会检查追踪记录，也就是每一步之后的中间输出，以便了解哪个组件的表现不尽如人意，也就是说比人类专家的表现差很多，因为这指出了安全改进的可能空间。让我们来看一个例子。如果我们要求研究代理写一篇关于黑洞科学最新进展的文章，可能会输出这样的搜索词：搜索黑洞理论爱因斯坦、事件视界望远镜无线电等等。

然后我会请一位人类专家查看这些内容，判断这些网络搜索关键词是否适合用于撰写关于黑洞科学最新发现的文章。也许在这种情况下，专家会说这些网络搜索看起来没问题，和人类会做的搜索相当类似。接着我会查看网络搜索的结果，看看返回的网址。网络搜索会返回许多不同的网页，可能其中一个网页是《天文少年新闻》报道一名小学生声称破解了存在30年的黑洞谜题。这看起来并不像是最严谨的经过同行评审的文章。

也许检查网络搜索返回的所有文章会让你得出结论：它返回了太多博客或大众媒体类型的文章，而不足以提供足够多的科学文章来撰写符合你期望质量的研究报告。最好也看看其他步骤的输出结果。也许大语言模型能找到你能找到的最好的五个来源，你最终得到了 Astro Kid News、SpaceBot 2000、Space Fun News等等。正是通过查看这些中间输出，你才能尝试了解每个步骤输出的质量。

为了介绍一些术语，所有中间步骤的整体输出集通常被称为该代理运行的追踪（trace）。此外，你在其他资料中可能还会看到，单个步骤的输出有时被称为跨度（span）。这些术语来源于计算机可观测性文献，人们试图通过这些术语来理解计算机的运行情况。在本课程中，我会频繁使用“追踪”这个词，而“跨度”会相对少用一些，但你在互联网上可能会同时看到这两个术语。通过阅读追踪记录，你可以初步了解哪些组件可能是最成问题的。

为了更系统地完成这项工作，事实证明将注意力集中在系统表现不佳的案例上非常有用。也许你写的某些文章很好，输出结果完全令人满意。所以我会把这些放在一边，尝试找出一系列例子，无论出于什么原因，你的研究代理的最终输出并不十分令人满意，然后专注于这些例子。这就是我们称之为错误分析的原因之一，因为我们希望关注系统出错的案例，并深入分析哪些组件对研究代理输出中的错误负有最大责任。

为了使这一过程更加严谨，与其通过阅读获得非正式的感知，你实际上可以建立一个电子表格来更明确地统计错误所在。这里的"错误"指的是某个步骤输出的结果表现明显逊色于人类专家在相同输入下可能给出的结果。我自己就经常用电子表格来做这件事。比如，我可能会构建这样一个表格：针对第一个查询"黑洞科学的最新进展"，我发现搜索结果中博客文章和大众媒体报道过多，而科学论文数量不足。基于此，确实排名前五的来源质量都不理想。

但在这里，我不会说这五个最佳来源做得不好，因为如果用于选择这五个最佳来源的 LLM 输入都是不严谨的文章，那么我不能责怪它没有选出更好的文章，因为它已经尽力了，或者说，在给定的选择范围内，它做得几乎和任何人一样好。然后，你可能会针对不同的提示进行这个过程。在西雅图租房还是买房。也许它漏掉了一个知名的博客。用于采摘水果的机器人技术。

也许在这种情况下，我们会审视它并说，哦，搜索词太笼统了，搜索结果也不理想等等。然后基于此，我会在我的电子表格中统计我在不同组件中观察到错误的频率。所以在这个例子中，我对搜索词不满意的时间占 5%，但对搜索结果不满意的时间占 45%。如果我确实看到这种情况，我可能会仔细检查搜索词，以确保搜索词确实没问题，并且搜索词选择不当并不是导致搜索结果不佳的原因。

但如果我真的认为搜索词没问题，但搜索结果不理想，那么我会仔细检查所使用的网络搜索引擎，看看是否有可以调整的参数，以便获取更相关或更高质量的搜索结果。这种分析告诉我，在这个例子中，也许我确实应该把注意力集中在改善搜索结果上，而不是这个代理工作流程的其他部分。总结一下这个视频，我认为养成查看跟踪记录的习惯很有用。

当你构建了一个自主工作流程后，不妨查看中间输出结果，了解它在每个步骤中的实际运作情况，这样你就能更好地判断哪些步骤表现优异或欠佳。通过更系统化的错误分析（比如借助电子表格），你可以收集统计数据或计算哪个组件最频繁出现性能问题。通过观察哪些组件表现不佳，以及我对高效改进不同组件的想法所在，你就能优先处理需要优化的组件。

 也许某个组件存在问题，但我暂时没有改进它的思路，这意味着我们或许不该将其列为高优先级。然而，如果某个组件频繁引发错误，而我又恰好知道如何优化它，这显然应该优先处理。我想特别强调，错误分析能有效帮你锁定重点攻关方向——毕竟在复杂系统中，可优化的环节实在太多了。

选择一项工作并投入数周甚至数月的时间，结果却发现它并未提升整个系统的性能，这种情况太容易发生了。因此，利用错误分析来决定工作重点，对于提高效率来说极其有用。在这个视频中，我们通过研究代理的例子讲解了错误分析，但我认为错误分析是一个非常重要的主题，我想再和大家一起看一些额外的例子。那么，让我们继续观看下一个视频，那里会有更多关于错误分析的例子。

## 4.3 More error analysis examples

我发现，对许多开发者而言，只有通过多个实例的观摩，才能真正练习并磨练出进行错误分析的直觉。因此，让我们再看两个例子：发票处理和客户邮件回复。这是我们之前为发票处理设计的工作流程，其中明确了遵循代理工作流的步骤——识别四个必填字段，然后将它们录入数据库。在本模块第一个视频的案例中，我们提到系统经常在发票的到期日字段上出错。

因此我们可以进行错误分析，试图找出可能是哪个组件出了问题。例如，是 PDF 转文本时出了错，还是大语言模型从 PDF 转文本组件输出的内容中提取了错误的日期？要进行错误分析，我会尝试找出一些数据提取错误的例子。和上一个视频一样，关注那些表现不佳的例子有助于找出问题所在。

所以忽略那些日期正确的例子，但要找出 10 到 100 张日期错误的发票。然后我会仔细检查，试图找出问题的根源：是 PDF 转文本时日期错了，还是大语言模型在基于 PDF 转文本的输出时提取了错误的日期。你可以像这样建立一个小表格，检查 20 张发票，统计 PDF 转文本在提取日期或文本时的错误频率——比如有些错误连人类都无法分辨到期日，而有些情况 PDF 转文本看起来没问题，但大语言模型在提取日期时却弄错了，比如可能提取了发票日期而不是到期日。

在这个例子中，看起来 LLM 数据提取导致了更多的错误。这说明我或许应该把精力集中在 LLM 数据提取组件上，而不是 PDF 转文本上。这一点很重要，因为如果没有这个错误分析，我能想象一些团队会花上几周甚至几个月的时间来调整 PDF 转文本，到头来却发现这对最终系统的性能影响不大。哦，顺便说一下，底部的这些百分比加起来可能不等于 100%，因为这些错误并不是互斥的。

再看最后一个例子，让我们回到处理客户邮件的代理工作流程。在这个流程中，当大语言模型收到客户询问订单的邮件时，它会调取订单详情，从数据库中获取信息，然后起草回复供人工审核。同样，我会找出一些最终结果不尽如人意的案例，无论出于何种原因，并试图找出问题所在。以下是一些可能出错的地方。

可能是大语言模型编写了一个错误的数据库查询语句。因此，当查询发送到数据库时，它未能成功提取客户信息。也可能是数据库中的数据已损坏。所以，尽管大语言模型编写了一个完全合适的数据库查询语句（可能是 SQL 或其他查询语言），但数据库中没有正确的信息。又或者，在获得客户订单的正确信息后，大语言模型生成的电子邮件内容仍然存在某些不妥之处。

因此，我会再次翻阅几封最终结果不尽如人意的邮件，试图找出问题所在。比如在第一封邮件中，可能会发现大语言模型在查询时索取了错误的表格，或者在构建数据库时错误地索取了数据。在第二封邮件中，也许会发现数据库本身存在错误。基于这些输入，大语言模型可能还写了一封不太合适的邮件，诸如此类。在这个例子中，经过检查多封邮件后，可能会发现最常见的错误在于大语言模型编写数据库查询（比如 SQL 查询）以获取相关信息的方式。而数据库本身大体上是正确的，尽管其中存在少量数据错误。

LLM 写邮件的方式也存在一些错误。可能在大约 30% 的情况下写得不太准确。这让我觉得，最有价值的改进可能是优化 LLM 生成查询的方式。其次重要的或许是改进我撰写最终邮件时的提示方式。这样的分析可以告诉你，75% 的错误可能源于系统在多数情况下表现良好，但在那些不太准确的地方， 75% 的问题都出在数据库查询上。

这些信息对于确定工作重点非常有帮助。在开发智能代理工作流时，我经常通过这类错误分析来确定下一步的改进方向。当你明确改进目标后，除了我们之前讨论的端到端评估外，对单个组件进行评估也很有价值，这能让你更高效地提升特定组件——比如通过错误分析确定需要重点优化的部分。接下来让我们观看下一个视频，了解组件级评估的相关内容。

## 4.4 Component-level evaluations

让我们来看看如何构建和使用组件级评估。在研究代理的示例中，我们提到研究代理有时会遗漏关键点。但如果问题出在网络搜索上，每次更换搜索引擎时都需要重新运行整个工作流程，这虽然能为我们提供良好的性能指标，但这类评估成本很高。此外，这是一个相当复杂的工作流程，即使网络搜索有所改进，其他组件随机性带来的干扰也可能让人难以察觉网络搜索质量的微小提升。

因此，作为仅使用端到端评估的替代方案，我会考虑专门构建一个评估来衡量网络搜索组件的质量。例如，为了衡量网络搜索结果的质量，你可以创建一份黄金标准的网络资源列表。也就是说，针对少量查询，让专家指出哪些是最权威的来源——如果有人在网上搜索，他们真的应该找到这些网页，或者其中任何一个网页都是不错的选择。

然后你可以编写代码来捕获有多少网络搜索结果与黄金标准网络资源相对应。信息检索的标准指标是 F1 分数，如果你不知道那是什么意思也不用担心细节，但有标准指标可以衡量网络搜索返回的网页列表与专家确定的黄金标准网络资源的重叠程度。

有了这个，你现在就掌握了一种方法来专门评估网络搜索组件的质量。因此，当你调整网络搜索的参数或超参数时，比如更换不同的搜索引擎，比如尝试谷歌、必应和Dr.Go Tivoli、U.com等网站，或者当你改变搜索结果数量或调整要求网络搜索引擎搜索的日期范围时，这能让你快速判断网络搜索组件的质量是否在提升，并确实带来更多渐进式的改进。

当然，在完成工作之前，最好进行一次端到端的评估，以确保在对网络搜索系统进行一段时间的调整后，整体系统性能有所提升。但在逐个调整这些超参数的过程中，通过仅评估一个组件而不是每次都需要重新运行端到端评估，你可以更高效地进行调整。

因此，组件级评估能为特定错误提供更清晰的信号。它实际上能让你明确自己是否改进了网络搜索组件或正在处理的任何组件，同时避免整个端到端系统复杂性带来的干扰。如果你参与的项目中有不同团队专注于不同组件，对单个团队来说，仅需优化自己非常明确的指标而无需担心其他组件，也能提高效率。

因此，这让团队能够更快地解决更小、更有针对性的问题。所以，当你决定改进某个组件时，考虑是否值得建立组件级评估，以及这是否能让你更快地提升该组件的性能。现在你可能想知道的是，如果你决定改进一个组件，你实际上该如何着手让这个组件表现得更好？我们将在下一个视频中看一些例子。

## 4.5 How to address problems you identify

一个自主工作流程可能包含许多不同类型的组件，因此改进不同组件的工具也会大不相同。但我想和大家分享一些我观察到的通用模式。你的自主工作流程中有些组件可能不基于 LLM，比如网络搜索引擎或文本检索组件（如果这是你的检索增强生成系统/RAG 的一部分）、代码执行模块，也可能是单独训练的机器学习模型（比如用于语音识别或图像中的人体检测）等等。

因此，有时这些非基于 LLM 的组件会有可调整的参数或超参数。例如，在网页搜索中，你可以调整返回结果的数量或要求搜索引擎考虑的日期范围。对于 RAG 文本检索组件，你可能会改变决定文本相似度的相似度阈值，或者调整文本块的大小。

通常，RAG 系统会将文本分割成较小的块进行匹配，因此你可以使用的主要超参数。或者对于人员检测，你可能会调整检测阈值，即它的敏感度以及它有多大概率会判定找到了一个人，这将权衡误报和漏报的情况。如果他们遵循我刚才讨论的所有超参数细节，就不用担心了。

细节并不那么重要，但通常这些组件都是可以调整的参数。当然，你也可以尝试替换组件。我在我的代理工作流程中经常这样做，比如我会换用不同的 RAG 搜索引擎或不同的 RAG 供应商等等，只是为了看看其他供应商是否可能表现更好。由于非基于 LLM 的组件的多样性，我认为改进它们的技术也会更加多样化，并且完全取决于该组件的具体功能。

对于基于 LLM 的组件，以下是一些您可以考虑的选项。一种方法是尝试改进您的提示词。或许可以尝试添加更明确的指令。或者，如果您了解 few-shot prompting，它指的是添加一个或多个具体的输入示例和期望的输出示例。少样本提示是一种技术，您也可以从一些深度学习短期课程中学习到，它能为您的 LLM 提供一些示例，希望能帮助它生成性能更好的输出。

或者你也可以尝试使用不同的 LLM。借助 AISuite 或其他工具，可以轻松尝试多个 LLM，然后通过评估选择最适合你应用的模型。有时候，如果一个步骤对一个 LLM 来说过于复杂，你可以考虑是否要将任务分解为更小的步骤。或者将其分解为一个生成步骤和一个反思步骤。但更普遍的情况是，如果你的指令在一个步骤中非常复杂，单个 LLM 可能很难遵循所有这些指令。

你可以将任务分解为更小的步骤，这样可能更容易让连续两三次调用准确执行。最后，当其他方法效果不佳时，可以考虑对模型进行微调。这通常比其他选项复杂得多，因此在开发人员实施时间上也会更昂贵。但如果你有一些数据可以用来对大语言模型进行微调，可能会比仅靠提示获得更好的性能。

因此，我通常不会轻易对模型进行微调，除非其他方法都已用尽，因为微调往往相当复杂。但在某些应用中，当尝试了所有其他方法后，如果性能仍停留在 90% 或 95%，而我确实需要榨取最后几个百分点的提升时，有时微调自己的定制模型会是个非常有效的技术手段。

我倾向于只在更成熟的应用程序上这样做，因为成本太高。事实证明，当你试图选择一个要使用的大型语言模型时，作为开发者，如果你对不同大型语言模型的智能程度或能力有很好的直觉，这对你来说是非常有帮助的。你可以做的一件事就是尝试很多模型，看看哪个效果最好。但我发现，随着我使用不同的模型，我开始磨练出关于哪些模型最适合哪些类型任务的直觉。

当你磨练出这些直觉时，你就能更高效地为模型编写优质的提示词，并为任务选择合适的模型。因此，我想分享一些关于如何培养直觉的心得，帮助你判断哪些模型最适合你的应用场景。让我们通过一个使用 LLM 来遵循指令删除或编辑PII（个人身份信息）的案例进行说明。现在你需要移除私密敏感信息。

例如，如果你使用 LLM 来总结客户通话内容，那么可能在 2023 年 7 月 14 日的总结中会包含Jessica Alvarez 的社会安全号码、某个地址、业务支持工单等信息。这段文本包含大量敏感的、可识别个人身份的信息。现在，假设我们想从这些总结中删除所有个人身份信息（PII），因为我们希望将这些数据用于下游统计分析，了解客户来电的原因。为了保护客户信息，我们希望在开展下游统计分析之前去除这些个人身份信息。

因此，你可能会提示一个 LLM，让它识别下面文本中的所有个人身份信息（PII）案例，然后返回经过编辑的文本，并用编辑冒号等标记替换。事实证明，较大的前沿模型在遵循指令方面往往表现更好，而较小的模型虽然在回答简单的事实性问题上表现不错，但在遵循指令方面就不那么擅长了。

如果你在较小的模型（即拥有 80 亿参数的 OpenWay Llama 3.1 模型）上运行这个提示，它可能会生成这样的输出。它说识别的个人身份信息（PII）是社会保障号码和地址，然后按照以下方式进行了编辑，等等。实际上它犯了一些错误。它没有正确遵循指令。它先显示了列表，然后编辑了文本，接着又返回了另一个列表，而它本不应该这样做。

在这份个人身份信息（PII）清单中，遗漏了姓名。我认为它也没有完全隐去地址部分。虽然细节并不重要，但它并没有完美遵循这些指示，可能遗漏了一些 PII。相比之下，如果你使用一个更智能、更擅长遵循指示的模型，可能会得到更好的结果，比如这个例子，它实际上正确地列出了所有 PII 并正确地隐去了所有 PII。因此，我发现不同的 LLM 提供商专注于不同的任务，不同的模型确实更适合不同的任务。

有些模型更擅长编程，有些更擅长遵循指令，还有些在某些特定领域的事实处理上表现更优。如果你能对模型的智能程度及其对不同指令的适应能力保持敏锐直觉，就能更明智地选择使用哪些模型。为此我想分享几个实用建议：建议你经常试用不同模型。每次新模型发布时，我都会亲自测试各种查询，既包括闭源专有模型，也涵盖开源权重模型。

我发现，有时拥有一套个人评估标准也很有帮助，就是你可以向多个不同模型提出一系列问题，以此衡量它们在不同类型任务上的表现。另外，我经常做的一件事是花大量时间阅读别人的提示词。有时人们会在网上发布他们的提示词，我经常会去阅读这些内容，以了解提示词的最佳实践是怎样的。

或者我经常会和不同公司的朋友聊天，包括一些前沿模型公司的朋友，和他们分享我的提示词，看看他们是怎么写提示词的。有时候我也会去下载一些我非常尊敬的人写的开源包，然后仔细研究这些开源包，找出作者写的提示词来阅读，以此来培养我对如何写出好提示词的直觉。

我鼓励你考虑的一种方法是，通过大量阅读他人撰写的提示词来提升自己编写提示词的能力。我自己就经常这样做，也建议你效仿。这种做法能帮助你磨练直觉，判断模型擅长响应哪些类型的指令，以及针对不同模型何时该使用特定表达方式。除了动手调试模型和研读他人提示词外，若能在智能体工作流中尝试多种不同模型，同样能有效培养你的直觉判断力。

因此，你可以了解哪些模型最适合哪些类型的任务，无论是通过查看跟踪记录来获得非正式的感知，还是通过组件级或端到端的评估，都能帮助你评估不同模型在工作流程的不同部分表现如何。然后，你不仅能开始磨练对性能的直觉，还可能对不同模型在价格和速度上的权衡有所了解。

我倾向于使用 AISuite 来开发我的代理工作流程的原因之一，是它能让我轻松快速地替换和尝试不同的模型。这让我在尝试和评估哪些模型最适合我的工作流程时更加高效。我们已经讨论了很多关于如何提高不同组件性能的方法，希望能提升你端到端系统的整体性能。

除了提高输出质量外，工作流中另一个需要优化的环节是延迟和成本。我发现很多团队在开发初期，首要关注点往往是输出质量是否达标。但当系统运行良好并投入生产后，通常还需要提升运行速度和降低成本。在下一个视频中，我们将探讨一些优化智能体工作流成本和延迟的方法。

## 4.6 Latency, cost optimization

在构建代理工作流时，我通常会建议团队首先专注于获得高质量的输出，之后再优化成本和延迟。这并不是说成本和延迟不重要，但我认为提高性能或输出质量通常是最困难的部分，只有当它真正有效时，也许才需要关注其他方面。

我遇到过好几次这样的情况：团队开发了一个自动化工作流程，发布给用户后，幸运的是用户量激增，导致运营成本飙升，我们不得不手忙脚乱地控制成本。不过这种"甜蜜的负担"反而是好事，所以我通常不太担心成本问题——当然不是完全忽视，只是优先级较低。现在用户基数这么大，我们确实需要降低人均服务成本。

然后就是延迟问题，我对此有点担心，但再次强调，确保输出质量高才是更重要的。不过当你真正开始关注延迟时，拥有优化延迟和成本的工具会很有用。让我们来看看一些实现这一目标的想法。如果你想优化一个代理工作流的延迟，我经常做的一件事是对工作流进行基准测试或计时。比如在这个研究代理中，它需要多个步骤，如果我对每个步骤进行计时，也许 LLM 生成搜索词需要 7 秒钟。

网页搜索需要 5 秒，这一步需要 3 秒，这一步需要 11 秒，最后撰写论文平均需要 18 秒。通过观察这个整体时间线，我就能看出哪些环节最有提速空间。在这个例子中，可能有多个可以尝试的改进点。如果你还没有利用并行处理某些步骤（比如网页抓取），或许值得考虑将这些操作并行执行。

或者，如果你发现某些 LLM 集耗时过长——比如第一步耗时 7 秒，而最后一个LLM集却要 18 秒——我可能也会考虑尝试一个更小、或许稍欠智能的模型，看看它是否仍能胜任工作，或者能否找到一个更快的 LLM 供应商。网上有很多针对不同 LLM 接口的 API，有些公司还拥有专门的硬件，能够更快地提供某些 LLM 服务。因此，有时尝试不同的 LLM 供应商，看看哪些能最快返回 tokens，是值得的。

但至少进行这类时间分析能让你明确应该优先优化哪些组件来降低延迟。在成本优化方面，类似地计算每个步骤的费用也能帮你建立基准，从而决定优化重点。许多大语言模型供应商按输入输出的 token 数量计费，而多数 API 提供商按调用次数收费——计算步骤的成本差异可能取决于服务器资源的付费方式及服务定价策略。

因此，对于这样一个流程，你可能会在这个例子中决定：这个 LLM 步骤的 token 平均成本为 0.04 美分，每次网络搜索 API 调用可能花费 1.6 美分，token 成本这么多，API 调用成本这么多，PDF 转文本成本这么多，最终 SA 生成的 token 成本这么多。这样你或许能再次意识到，是否有更便宜的组件或更便宜的 LLM 可以使用，从而看出优化成本的最大机会在哪里。

我发现这些基准测试非常具有启发性，有时它们会明确告诉我某些组件根本不值得担心，因为它们对成本或延迟的影响并不显著。因此我发现，当成本或延迟成为问题时，只需测量每个步骤的成本和/或延迟，通常就能为你提供一个决策基础，确定应该优先优化哪些组件。本模块即将接近尾声。我知道我们涵盖了很多内容，但感谢你一直坚持学习。让我们进入本模块的最后一个视频来总结收尾。

## 4.7 Development process

我们探讨了许多关于如何构建有纪律、高效流程来开发自主 AI 系统的技巧。最后，我想和大家分享一下经历这一过程的感受。当我构建这些工作流程时，我感觉自己主要在做两件事：一是构建，也就是编写软件，尝试通过代码改进系统。

其次，虽然有时感觉不像是在进步，但我认为同样重要的是通过分析来决定下一步应该把精力集中在哪些构建工作上。我经常在构建和分析之间来回切换，包括进行错误分析等。例如，在构建一个新的自主工作流程时，我通常会先快速搭建一个端到端的系统，甚至可能是一个粗糙但快速的实现。

这样我就可以开始检查端到端系统的最终输出，或者通过跟踪记录来了解它在哪些方面表现良好，哪些方面表现不佳。有时候，仅仅通过查看跟踪记录，我就能直觉地感觉到哪些单个组件可能需要改进。因此，我可能会去调整一些单个组件，或者继续调整整个端到端系统。随着我的系统逐渐成熟，除了手动检查一些输出和阅读跟踪记录外，我可能会开始构建评估体系，并准备一个小数据集，可能只有 10 到 20 个示例，来计算指标，至少评估端到端的性能。这将进一步帮助我更精确地了解如何改进端到端系统或优化各个组件。

随着分析的进一步成熟，我的分析过程会变得更加系统化。我会开始进行错误分析，仔细检查各个组件，并统计每个组件导致输出不理想的频率。这种更严谨的分析让我能更精准地决定下一步该优化哪些组件，或激发改进整个端到端系统的灵感。最终，当分析足够成熟，可以在组件层面推动更高效的改进时，我可能还会建立组件级的评估体系。

因此，构建一个代理系统的流程往往是来回反复的。这不是一个线性的过程。我们有时会调整端到端系统，然后进行一些错误分析，再稍微改进某个组件，接着调整组件级别的评估。我倾向于在这两种技术之间来回切换。而我看到经验不足的团队通常花大量时间构建系统，却很少花时间通过错误分析、构建评估等方式进行分析。

那将非常理想，因为这种分析能帮助你真正专注于应该投入时间进行构建的领域。再给你一个小建议：市面上其实有不少工具可以帮助监控追踪、记录运行时数据、计算成本等等。这些工具确实很有用，我有时也会用到其中一些。DeepLearning.ai 的不少短期课程合作伙伴都提供这类工具，而且效果相当不错。

我发现，对于我所处理的代理工作流程，大多数代理工作流程都相当定制化。因此，我最终自己构建了很多定制的评估方法，因为我想要捕捉那些在我的系统中运行不正确的地方。所以，尽管我确实使用了一些现成的工具，但我也构建了很多适合我的特定应用和我所发现问题的定制评估方法。感谢你坚持到现在，完成了五个模块中的第四个。如果你能实践这个模块中的哪怕一小部分想法，我认为你在实现智能工作流方面的成熟度将远超大多数开发者。希望这些材料对你有所帮助，期待在最后一个模块与你相见。我们将探讨一些更高级的设计模式，用于构建高度自主的智能体。下个课程模块见。

