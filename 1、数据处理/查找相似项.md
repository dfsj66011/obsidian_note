
> [Finding Similar Items](http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf)


我们首先将相似性问题表述为寻找具有相对较大交集的集合的问题。我们展示了如何通过一种称为“指纹”的技术，将查找文本相似文档的问题转化为这样的集合问题。然后，我们介绍了一种称为 “minhashing” 的技术，该技术以这样一种方式压缩大型集合，即我们仍然可以从它们的压缩版本中推断出底层集合的相似性。

即使计算任意一对 item 的相似度变得非常容易，也可能有太多 item 对需要测试，这一担忧促使了一种称为“局部敏感哈希”的技术的发展，该技术专注于最有可能相似的 item 对。最后，我们探讨了无法用集合交集来表达的“相似性”概念。这项研究引导我们考虑任意空间中的距离度量理论。它还激发了一个适用于其他“相似性”定义的局部敏感哈希通用框架。

#### 1. 集合的 Jaccard 相似度

Jaccard 相似度是集合 $S$ 和 $T$ 的交集大小与并集大小的比值，表示为：$$\text{SIM}(S, T) = \frac{|S \cap T|}{|S \cup T|}$$
#### 2. $k$-shingles

Shingling 是将文档表示为集合的一种方式，$k$-Shingles 就是将文本拆分为长度为 $k$ 的字符串子集，例如，假设我们的文本 $D$ 的内容为 ”abcdabd“，则当 $k=2$ 时，拆解的全部子集为 $\{ab, bc, cd, da, bd\}$，其中 $ab$ 在文本 $D$ 中出现了两次，但是在 shingling 集合中只保留一次（另一个版本采用的 bag，而不是 set 则支持重复项）。这里的小字符串就称为 ”shingle“。

#### 2.2 $k$ 的选择

如果 $k$ 选择的比较小，则任意文本相似性很高，例如极限情况下，$k=1$，且只考虑字母+空格情况，则几乎任意文档都是有 27 个元素（26个字母+1个空格）组成，相似度为 $1$；所以 *$K$ 应该足够大，以至于任意给定的 shingle 出现在任意给定的文档中的概率都很低。*

例如当 $k=5$，$27^5=14,348,907$，对于短文本，字符长度远远小于 0.14 亿的长度，因此通常选择 $k=5$ 就挺好，不过实际上文本中的字符可能不止 $27$ 个，同时每个字母出现的概率也不一样，大致可以按 $20^k$ 计算，对于长篇文档（如研究论文），可以考虑用 $k=9$.

#### 2.3 Hashing Shingles

假设 $k=9$，则一个 shingles 长度为 9，则需要 9 个字节空间存储，如果将其 hash 到 0 到 $2^{32}-1$ 范围内，则可以用 4 个字节（32位）的长度表示，可以节省空间。需要注意的是，如果我们使用 9-shingle 并将它们 hash 到 4 个字节，那么我们可以更好地区分文档，而不是使用 4-shingle，即使用于表示 shingle 的空间是相同的。

#### 3.2.4 由单词构成的词条  

然而，在寻找相似新闻文章的问题中，研究发现，将一个shingle定义为停用词后接接下来的两个单词（无论这两个单词是否为停用词），可以形成一组有用的shingles。这种方法的优势在于，与周围的元素相比，新闻文章将为代表网页的shingle集合贡献更多的shingles。回想一下，这项练习的目标是找到具有相同文章的页面，而不管周围的元素如何。通过偏向于文章的shingle集合，具有相同文章但不同周围材料的页面比具有相同周围材料但不同文章的页面具有更高的Jaccard相似度。

**示例3.5：** 一则广告可能只有简单的文字“购买Sudzo”。然而，一篇表达相同观点的新闻文章可能会这样写：“Sudzo公司的一位发言人今天透露，研究表明人们购买Sudzo产品是有益的。”在这里，我们已经将所有可能的停用词斜体显示，尽管并没有规定必须将出现频率最高的前多少个词视为停用词。由一个停用词及其后两个连续词构成的前三个词块是：

A spokesperson for
for the Sudzo 
the Sudzo Corporation

这句话里有九个词条，但广告里一个也没有。

### 3.3 相似性保留的集合摘要  

集合的指纹数量庞大。即使将每个指纹哈希为四个字节，存储一个集合所需的空间仍大约是文档本身所需空间的四倍。如果我们有数百万份文档，可能根本无法将所有指纹集合存储在主内存中。

本节的目标是用称为“签名”的更小表示来替代大集合。我们需要签名的重要属性是，可以比较两个集合的签名，并仅从签名中估计底层集合的杰卡德相似度。签名不可能给出它们所代表集合的精确相似度，但它们提供的估计值很接近，且签名越大，估计越准确。例如，如果我们用1000字节的签名来替代源自50000字节文档的200000字节哈希分词集合，通常可以做到误差在几个百分点之内。

#### 3.3.1 集合的矩阵表示  

在解释如何从大型集合中构造小型签名之前，将集合的集合可视化为它们的特征矩阵是有帮助的。矩阵的列对应于集合，行对应于从中抽取集合元素的通用集的元素。如果行 r 的元素是列 c 的集合的成员，则在第 r 行和第 c 列的位置有一个 1。否则，位置 (r, c) 的值为 0。

$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline a & 1 & 0 & 0 & 1 \\ b & 0 & 0 & 1 & 0 \\ c & 0 & 1 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ e & 0 & 0 & 1 & 0 \\ \hline \end{array}$$
图3.2：表示四个集合的矩阵

例3.6：图3.2展示了一个矩阵示例，该矩阵表示从全集{a, b, c, d, e}中选取的集合。这里，S₁ = {a, d}，S₂ = {c}，S₃ = {b, d, e}，S₄ = {a, c, d}。矩阵的最顶行和最左列并非矩阵的一部分，仅用于提醒我们各行和各列所代表的含义。

需要记住的是，特征矩阵不太可能是数据的存储方式，但它有助于直观地呈现数据。一方面，出于存储方面的考虑，这些矩阵在实践中几乎总是稀疏的（即其中 0 的数量远多于 1）。通过记录 1 出现的位置来表示由 0 和 1 组成的稀疏矩阵可以节省空间。另一方面，出于其他目的，数据通常以其他格式存储。

例如，如果行代表产品，列代表客户（由他们购买的产品集合表示），那么这些数据实际上会出现在购买记录的数据库表中。该表中的一条记录会列出商品、购买者，可能还会包含关于购买的其他详细信息，如购买日期和使用的信用卡。

#### 3.3.2 最小哈希（Minhashing）  

我们希望为集合构建的签名由大量计算结果组成，比如说几百个，每个计算结果都是特征矩阵的一个“最小哈希”。在本节中，我们将学习最小哈希在原理上是如何计算的，在后续章节中，我们将了解如何在实践中计算最小哈希的良好近似值。

要对特征矩阵的某一列表示的集合进行最小哈希（MinHash）处理，需选取一行排列。任意列的最小哈希值是该列在排列顺序中首次出现 1 的行号。

例3.7：假设我们选择将图3.2中矩阵的行顺序排列为beadc。这种排列定义了一个最小哈希函数h，它将集合映射到行。让我们根据h计算集合S1的最小哈希值。第一列是集合S1的列，在行b中为0，因此我们继续到排列顺序中的第二行e。在S1的列中再次出现0，因此我们继续到行a，在那里我们发现了一个1。因此，h(S1) = a。

$$\begin{array}{|c|c|c|c|c|} \hline \textit{Element} & S_1 & S_2 & S_3 & S_4 \\ \hline b & 0 & 0 & 1 & 0 \\ e & 0 & 0 & 1 & 0 \\ a & 1 & 0 & 0 & 1 \\ d & 1 & 0 & 1 & 1 \\ c & 0 & 1 & 0 & 1 \\ \hline \end{array}$$
图3.3：图3.2行的一个排列。

虽然从物理上来说，对非常大的特征矩阵进行置换是不可能的，但 MinHash 函数 h 隐式地对图 3.2 中的矩阵的行进行了重新排序，使其变成了图 3.3 中的矩阵。在这个矩阵中，我们可以通过从顶部开始扫描，直到遇到 1 来读出 h 的值。因此，我们看到 h(S2) = c，h(S3) = b，h(S4) = a。

#### 3.3.3 最小哈希与杰卡德相似度  

被最小哈希处理的集合之间存在着显著的最小哈希与杰卡德相似度关联。

* 对于两个集合而言，随机行排列的最小哈希函数产生相同值的概率等于这两个集合的杰卡德相似度。

要理解其中的原因，我们需要想象一下这两个集合对应的列。如果我们只关注集合S1和S2对应的列，那么行可以分为三类：
1. 类型X的行在这两列中都为1。
2. 类型Y的行在其中一列中为1，在另一列中为0。
3. 类型Z的行在这两列中都为0。

由于矩阵是稀疏的，大多数行属于Z类型。然而，决定SIM(S1, S2)以及h(S1) = h(S2)的概率的是X类型行和Y类型行数量的比率。假设有x行是X类型，y行是Y类型。那么SIM(S1, S2) = x/(x + y)。原因是x是S1 ∩ S2的大小，而x + y是S1 ∪ S2的大小。

现在，考虑h(S1)=h(S2)h(S1​)=h(S2​)的概率。如果我们想象行被随机排列，并且我们从顶部开始，那么在遇到YY型行之前遇到XX型行的概率是x/(x+y)x/(x+y)。但是，如果从顶部开始除了ZZ型行之外的第一行是XX型行，那么肯定有h(S1)=h(S2)h(S1​)=h(S2​)。另一方面，如果我们遇到的除了ZZ型行之外的第一行是YY型行，那么值为11的集合将把该行作为其最小哈希值。然而，在该行中值为00的集合肯定会在排列列表中更下面的某一行得到某个值。因此，如果我们首先遇到YY型行，我们就知道h(S1)≠h(S2)h(S1​)=h(S2​)。我们得出结论，h(S1)=h(S2)h(S1​)=h(S2​)的概率是x/(x+y)x/(x+y)，这也是S1S1​和S2S2​的杰卡德相似度。

#### 3.3.4 最小哈希签名  

再次考虑由特征矩阵 M 表示的一组集合。为了表示这些集合，我们随机选择若干行置换（例如 100 次或几百次），这些置换确定了最小哈希函数 h₁, h₂, ..., hₙ。对于表示集合 S 的列，构造 S 的最小哈希签名，即向量 [h₁(S), h₂(S), ..., hₙ(S)]。通常将这组哈希值表示为一列。因此，我们可以从矩阵 M 构造一个签名矩阵，其中 M 的第 i 列被第 i 列集合的最小哈希签名所替代。

请注意，签名矩阵的列数与矩阵 M 相同，但行数仅为 n。即使 M 并未以显式形式表示，而是以适合稀疏矩阵的某种压缩形式（例如，通过其 1 的位置）来表示，签名矩阵通常也会比 M 小得多。

#### 3.3.5 计算Minhash签名

显式地对大型特征矩阵进行排列是不可行的。即使对数百万或数十亿行进行随机排列也需要耗费大量时间，而对行进行必要的排序则需要更多时间。因此，如图3.3所示的排列矩阵虽然在概念上很有吸引力，但实际上并不可行。

幸运的是，可以通过一个随机哈希函数来模拟随机排列的效果，该哈希函数将行号映射到与行数相同的多个桶中。一个将整数 0, 1,..., k − 1 映射到桶号 0 至 k−1 的哈希函数通常会将某些整数对映射到同一个桶中，并使其他桶为空。然而，只要 k 足够大且冲突不多，这种差异并不重要。我们可以维持这样一种假设，即我们的哈希函数 h 将行 r “排列”到排列顺序中的位置 h(r)。

因此，我们不是选取 n 个随机行排列，而是选取 n 个随机的行哈希函数 h1, h2,..., hn。我们按给定顺序考虑每一行来构建签名矩阵。设 SIG(i, c) 为第 i 个哈希函数和第 c 列对应的签名矩阵元素。初始时，将所有 i 和 c 的 SIG(i, c) 设为 ∞。我们通过以下步骤处理行 r：
1. 计算 h1(r), h2(r),..., hn(r)。
2. 对每一列 c 执行以下操作：
	(a) 如果行 r 中列 c 的值为 0，则不进行任何操作。
	(b) 然而，如果行 r 中列 c 的值为 1，则对于每个 i = 1, 2,..., n，将 SIG(i, c) 设置为当前 SIG(i, c) 值和 hi(r) 中的较小值。

$$\begin{array}{|c||c|c|c|c||c|c|} \hline \textit{Row} & S_1 & S_2 & S_3 & S_4 & x+1 \mod 5 & 3x+1 \mod 5 \\ \hline 0 & 1 & 0 & 0 & 1 & 1 & 1 \\ 1 & 0 & 0 & 1 & 0 & 2 & 4 \\ 2 & 0 & 1 & 0 & 1 & 3 & 2 \\ 3 & 1 & 0 & 1 & 1 & 4 & 0 \\ 4 & 0 & 0 & 1 & 0 & 0 & 3 \\ \hline \end{array}$$

图3.4：为图3.2的矩阵计算哈希函数

**示例 3.8：** 让我们重新考虑图 3.2 的特征矩阵，我们在图 3.4 中复制了该矩阵并添加了一些额外的数据。我们用整数 0 到 4 替换了命名行的字母。我们还选择了两个哈希函数：h1(x) = x + 1 mod 5 和 h2(x) = 3x + 1 mod 5。这两个函数应用于行号的值在图 3.4 的最后两列中给出。请注意，这些简单的哈希函数是行的真实排列，但只有因为行数 5 是质数，才可能进行真正的排列。一般来说，会发生冲突，即两行得到相同的哈希值。

现在，让我们模拟计算签名矩阵的算法。最初，这个矩阵由全 ∞ 组成。

$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & \infty & \infty & \infty & \infty \\ h_2 & \infty & \infty & \infty & \infty \\ \hline \end{array}$$

首先，我们考虑图3.4的第0行。可以看到，h1(0)和h2(0)的值均为1。编号为0的行在集合S1和S4对应的列中有1，因此只有签名矩阵的这两列可能会发生变化。由于1小于∞，我们实际上会改变S1和S4对应列中的两个值。因此，签名矩阵的当前估计值为：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & \infty & 1 \\ h_2 & 1 & \infty & \infty & 1 \\ \hline \end{array}$$
现在，我们移动到图3.4中编号为1的行。这一行仅在S3中为1，其哈希值分别为h1(1) = 2和h2(1) = 4。因此，我们将SIG(1, 3)设为2，将SIG(2, 3)设为4。所有其他签名条目保持不变，因为它们在编号为1的行中对应的列为0。新的签名矩阵为：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & \infty & 2 & 1 \\ h_2 & 1 & \infty & 4 & 1 \\ \hline \end{array}$$

图3.4中编号为2的那一行在S2和S4对应的列中为1，其哈希值分别为h1(2) = 3和h2(2) = 2。我们可以改变S4的签名值，但签名矩阵中这一列的值[1, 1]分别小于对应的哈希值[3, 2]。然而，由于S2对应的列仍然是∞，我们将其替换为[3, 2]，结果如下：
$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 1 & 2 & 4 & 1 \\ \hline \end{array}$$
接下来是图3.4中编号为3的行。这里，除了S2列之外，所有列的值都是1，哈希值分别为h1(3) = 4和h2(3) = 0。h1的值为4，超过了签名矩阵中所有列当前已有的值，因此我们不会改变签名矩阵第一行的任何值。然而，h2的值为0，小于当前已有的值，因此我们将SIG(2, 1)、SIG(2, 3)和SIG(2, 4)的值降低为0。注意，我们不能降低SIG(2, 2)的值，因为在图3.4中S2列在我们当前考虑的行中的值为0。最终的签名矩阵如下：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 2 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
最后，考虑图3.4中编号为4的行。h1(4) = 0且h2(4) = 3。由于第4行仅在S3对应的列有1，我们仅比较该集合当前的签名列[2, 0]与哈希值[0, 3]。由于0 < 2，我们将SIG(1, 3)改为0，但由于3 > 0，我们不改变SIG(2, 3)。最终的签名矩阵为：$$\begin{array}{|c||c|c|c|c|} \hline & S_1 & S_2 & S_3 & S_4 \\ \hline h_1 & 1 & 3 & 0 & 1 \\ h_2 & 0 & 2 & 0 & 0 \\ \hline \end{array}$$
我们可以根据这个签名矩阵估算底层集合的杰卡德相似度。注意到第1列和第4列完全相同，因此推测SIM(S1, S4) = 1.0。查看图3.4会发现，S1和S4真实的杰卡德相似度其实是2/3。需要强调的是，签名矩阵中行向量相同的比例只是真实杰卡德相似度的估计值，而这个示例规模太小，无法通过大数定律保证估计值的准确性。其他例子中：S1和S3的签名列在半数行中一致（真实相似度为1/4），而S1和S2的签名矩阵估计相似度为0（这是正确值）。

### 3.4 面向文档的局部敏感哈希  

尽管我们可以通过最小哈希将大型文档压缩为短小的签名，并保持任意文档对之间的预期相似度，但高效找出相似度最高的文档对仍可能无法实现。原因在于文档对的数量可能过于庞大，即便文档总数本身并不算太多。

**例 3.9：** 假设我们有一百万份文档，并使用长度为 250 的签名。这样，每个文档的签名占用 1000 字节，整个数据可以装入 1 GB 内存中——这小于典型笔记本电脑的内存。然而，有 $\binom{1,000,000}{2}$ 或五千亿对文档。如果计算两个签名的相似度需要一微秒，那么在这台笔记本电脑上计算所有相似度需要将近六天。

如果我们的目标是计算每对文档的相似度，那么除了通过并行计算缩短运行时间外，我们无法减少工作量。然而，通常我们只关注最相似的文档对，或相似度超过某个阈值的所有文档对。这种情况下，我们需要将注意力集中在可能相似的文档对上，而无需检查所有文档对。为此，有一种通用的理论方法称为局部敏感哈希（LSH）或近邻搜索。本节我们将探讨一种特定形式的LSH，专为我们当前研究的问题设计：文档通过shingle集合表示，再经过最小哈希生成短签名。第3.6节将介绍局部敏感哈希的通用理论及其多种应用和相关技术。

#### 3.4.1 基于最小哈希签名的局部敏感哈希  

局部敏感哈希（LSH）的一种通用方法是多次“哈希”项，使得相似项比不相似项更可能被哈希到同一桶中。我们将任何在至少一次哈希中被分配到同一桶中的配对视为候选对，并仅检查这些候选对的相似性。其核心思想是：希望大多数不相似配对永远不会被哈希到同一桶中，因此无需被检查。那些被错误分配到同一桶的不相似配对称为假阳性，我们希望这部分在所有配对中占比极小。同时期望绝大多数真正相似的配对至少会在一次哈希函数作用下落入同一桶中。未能被成功配对的相似项称为假阴性，我们希望这部分在真实相似配对中占比同样极小。

如果我们已获得项目的 MinHash 签名，一种有效的哈希选择方式是将签名矩阵划分为b个波段，每个波段包含r行。对于每个波段，存在一个哈希函数，该函数接收由r个整数组成的向量（即某列在该波段内的部分），并将它们哈希到大量桶中。我们可以对所有波段使用相同的哈希函数，但需为每个波段单独分配桶数组，这样不同波段中具有相同向量的列就不会被哈希到同一个桶中。
$$\begin{array}{|c|c|}
\hline
\text{band 1} & 
\begin{array}{c}
10002 \\
\ldots \quad 32122 \quad \ldots \\
01311
\end{array} \\
\hline
\text{band 2} & \\
\hline
\text{band 3} & \\
\hline
\text{band 4} & \\
\hline
\end{array}
$$


1 0 0 0 2 3 2 1 2 2 0 1 3 1 1 . . . . . . band 1 band 2 band 3 band 4 Figure 3.6: Dividing a signature matrix into four bands of three rows per band Example 3.10 : Figure 3.6 shows part of a signature matrix of 12 rows divided into four bands of three rows each. The second and fourth of the explicitly shown columns each have the column vector [0, 2, 1] in the first band, so they will definitely hash to the same bucket in the hashing for the first band. Thus, regardless of what those columns look like in the other three bands, this pair of columns will be a candidate pair. It is possible that other columns, such as the first two shown explicitly, will also hash to the same bucket according to the hashing of the first band. However, since their column vectors are different, [1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the chances of an accidental collision to be very small. We shall normally assume that two vectors hash to the same bucket if and only if they are identical. Two columns that do not agree in band 1 have three other chances to become a candidate pair; they might be identical in any one of these other bands. 3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 71 However, observe that the more similar two columns are, the more likely it is that they will be identical in some band. Thus, intuitively the banding strategy makes similar columns much more likely to be candidate pairs than dissimilar pairs. ✷ 3.4.2 Analysis of the Banding Technique Suppose we use b bands of r rows each, and suppose that a particular pair of documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular row of the signature matrix is s. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows: 1. The probability that the signatures agree in all rows of one particular band is s r . 2. The probability that the signatures do not agree in at least one row of a particular band is 1 − s r . 3. The probability that the signatures do not agree in all rows of any of the bands is (1 − s r ) b . 4. The probability that the signatures agree in all the rows of at least one band, and therefore become a candidate pair, is 1 − (1 − s r ) b . It may not be obvious, but regardless of the chosen constants b and r, this function has the form of an S-curve, as suggested in Fig. 3.7. The threshold, that is, the value of similarity s at which the rise becomes steepest, is a function of b and r. An approximation to the threshold is (1/b) 1/r. For example, if b = 16 and r = 4, then the threshold is approximately 1/2, since the 4th root of 16 is 2. Example 3.11 : Let us consider the case b = 20 and r = 5. That is, we suppose we have signatures of length 100, divided into twenty bands of five rows each. Figure 3.8 tabulates some of the values of the function 1 − (1 − s 5 ) 20. Notice that the threshold, the value of s at which the curve has risen halfway, is just slightly more than 0.5. Also notice that the curve is not exactly the ideal step function that jumps from 0 to 1 at the threshold, but the slope of the curve in the middle is significant. For example, it rises by more than 0.6 going from s = 0.4 to s = 0.6, so the slope in the middle is greater than 3. For example, at s = 0.8, 1 − (0.8)5 is about 0.672. If you raise this number to the 20th power, you get about 0.00035. Subtracting this fraction from 1 yields 0.99965. That is, if we consider two documents with 80% similarity, then in any one band, they have only about a 33% chance of agreeing in all five rows and thus becoming a candidate pair. However, there are 20 bands and thus 20 chances to become a candidate. Only roughly one in 3000 pairs that are as high as 80% similar will fail to become a candidate pair and thus be a false negative. ✷ 72 CHAPTER 3. FINDING SIMILAR ITEMS 0 1 of documents Jaccard similarity Probability of becoming a candidate Figure 3.7: The S-curve s 1 − (1 − s r ) b .2 .006 .3 .047 .4 .186 .5 .470 .6 .802 .7 .975 .8 .9996 Figure 3.8: Values of the S-curve for b = 20 and r = 5 3.4.3 Combining the Techniques We can now give an approach to finding the set of candidate pairs for similar documents and then discovering the truly similar documents among them. It must be emphasized that this approach can produce false negatives – pairs of similar documents that are not identified as such because they never become a candidate pair. There will also be false positives – candidate pairs that are evaluated, but are found not to be sufficiently similar. 1. Pick a value of k and construct from each document the set of k-shingles. Optionally, hash the k-shingles to shorter bucket numbers. 2. Sort the document-shingle pairs to order them by shingle. 3. Pick a length n for the minhash signatures. Feed the sorted list to the algorithm of Section 3.3.5 to compute the minhash signatures for all the 3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 73 documents. 4. Choose a threshold t that defines how similar documents have to be in order for them to be regarded as a desired “similar pair.” Pick a number of bands b and a number of rows r such that br = n, and the threshold t is approximately (1/b) 1/r. If avoidance of false negatives is important, you may wish to select b and r to produce a threshold lower than t; if speed is important and you wish to limit false positives, select b and r to produce a higher threshold. 5. Construct candidate pairs by applying the LSH technique of Section 3.4.1. 6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t. 7. Optionally, if the signatures are sufficiently similar, go to the documents themselves and check that they are truly similar, rather than documents that, by luck, had similar signatures. 3.4.4 Exercises for Section 3.4 Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − s r ) b for s = 0.1, 0.2, . . . , 0.9, for the following values of r and b: • r = 3 and b = 10. • r = 6 and b = 20. • r = 5 and b = 50. ! Exercise 3.4.2 : For each of the (r, b) pairs in Exercise 3.4.1, compute the threshold, that is, the value of s for which the value of 1−(1−s r ) b is exactly 1/2. How does this value compare with the estimate of (1/b) 1/r that was suggested in Section 3.4.2? ! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate the S-curve 1 − (1 − s r ) b when s r is very small. ! Exercise 3.4.4 : Suppose we wish to implement LSH by map-reduce. Specifically, assume chunks of the signature matrix consist of columns, and elements are key-value pairs where the key is the column number and the value is the signature itself (i.e., a vector of values). (a) Show how to produce the buckets for all the bands as output of a single map-reduce process. Hint: Remember that a Map function can produce several key-value pairs from a single element. (b) Show how another map-reduce process can convert the output of (a) to a list of pairs that need to be compared. Specifically, for each column i, there should be a list of those columns j > i with which i needs to be compared. 74 CHAPTER 3. FINDING SIMILAR ITEMS 3.5 Distance Measures We now take a short detour to study the general notion of distance measures. The Jaccard similarity is a measure of how close sets are, although it is not really a distance measure. That is, the closer sets are, the higher the Jaccard similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we shall see; it is called the Jaccard distance. However, Jaccard distance is not the only measure of closeness that makes sense. We shall examine in this section some other distance measures that have applications. Then, in Section 3.6 we see how some of these distance measures also have an LSH technique that allows us to focus on nearby points without comparing all points. Other applications of distance measures will appear when we study clustering in Chapter 7. 3.5.1 Definition of a Distance Measure Suppose we have a set of points, called a space. A distance measure on this space is a function d(x, y) that takes two points in the space as arguments and produces a real number, and satisfies the following axioms: 1. d(x, y) ≥ 0 (no negative distances). 2. d(x, y) = 0 if and only if x = y (distances are positive, except for the distance from a point to itself). 3. d(x, y) = d(y, x) (distance is symmetric). 4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality). The triangle inequality is the most complex condition. It says, intuitively, that to travel from x to y, we cannot obtain any benefit if we are forced to travel via some particular third point z. The triangle-inequality axiom is what makes all distance measures behave as if distance describes the length of a shortest path from one point to another. 3.5.2 Euclidean Distances The most familiar distance measure is the one we normally think of as “distance.” An n-dimensional Euclidean space is one where points are vectors of n real numbers. The conventional distance measure in this space, which we shall refer to as the L2-norm, is defined: d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = vuutXn i=1 (xi − yi) 2 That is, we square the distance in each dimension, sum the squares, and take the positive square root. 3.5. DISTANCE MEASURES 75 It is easy to verify the first three requirements for a distance measure are satisfied. The Euclidean distance between two points cannot be negative, because the positive square root is intended. Since all squares of real numbers are nonnegative, any i such that xi 6= yi forces the distance to be strictly positive. On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry follows because (xi − yi) 2 = (yi − xi) 2 . The triangle inequality requires a good deal of algebra to verify. However, it is well understood to be a property of Euclidean space: the sum of the lengths of any two sides of a triangle is no less than the length of the third side. There are other distance measures that have been used for Euclidean spaces. For any constant r, we can define the Lr-norm to be the distance measure d defined by: d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (Xn i=1 |xi − yi | r ) 1/r The case r = 2 is the usual L2-norm just mentioned. Another common distance measure is the L1-norm, or Manhattan distance. There, the distance between two points is the sum of the magnitudes of the differences in each dimension. It is called “Manhattan distance” because it is the distance one would have to travel between points if one were constrained to travel along grid lines, as on the streets of a city such as Manhattan. Another interesting distance measure is the L∞-norm, which is the limit as r approaches infinity of the Lr-norm. As r gets larger, only the dimension with the largest difference matters, so formally, the L∞-norm is defined as the maximum of |xi − yi | over all dimensions i. Example 3.12 : Consider the two-dimensional Euclidean space (the customary plane) and the points (2, 7) and (6, 4). The L2-norm gives a distance of p (2 − 6)2 + (7 − 4)2 = √ 4 2 + 32 = 5. The L1-norm gives a distance of |2 − 6| + |7 − 4| = 4 + 3 = 7. The L∞-norm gives a distance of max(|2 − 6|, |7 − 4|) = max(4, 3) = 4 ✷ 3.5.3 Jaccard Distance As mentioned at the beginning of the section, we define the Jaccard distance of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the ratio of the sizes of the intersection and union of sets x and y. We must verify that this function is a distance measure. 1. d(x, y) is nonnegative because the size of the intersection cannot exceed the size of the union. 76 CHAPTER 3. FINDING SIMILAR ITEMS 2. d(x, y) = 0 if x = y, because x ∪ x = x ∩ x = x. However, if x 6= y, then the size of x ∩ y is strictly less than the size of x ∪ y, so d(x, y) is strictly positive. 3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e., x ∪ y = y ∪ x and x ∩ y = y ∩ x. 4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the probability a random minhash function maps x and y to the same value. Thus, the Jaccard distance d(x, y) is the probability that a random minhash function does not send x and y to the same value. We can therefore translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if h is a random minhash function, then the probability that h(x) 6= h(y) is no greater than the sum of the probability that h(x) 6= h(z) and the probability that h(z) 6= h(y). However, this statement is true because whenever h(x) 6= h(y), at least one of h(x) and h(y) must be different from h(z). They could not both be h(z), because then h(x) and h(y) would be the same. . 3.5.4 Cosine Distance The cosine distance makes sense in spaces that have dimensions, including Euclidean spaces and discrete versions of Euclidean spaces, such as spaces where points are vectors with integer components or boolean (0 or 1) components. In such a space, points may be thought of as directions. We do not distinguish between a vector and a multiple of that vector. Then the cosine distance between two points is the angle that the vectors to those points make. This angle will be in the range 0 to 180 degrees, regardless of how many dimensions the space has. We can calculate the cosine distance by first computing the cosine of the angle, and then applying the arc-cosine function to translate to an angle in the 0-180 degree range. Given two vectors x and y, the cosine of the angle between them is the dot product x.y divided by the L2-norms of x and y (i.e., their Euclidean distances from the origin). Recall that the dot product of vectors [x1, x2, . . . , xn].[y1, y2, . . . , yn] is Pn i=1 xiyi . Example 3.13 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot √ product x.y is 1 × 2 + 2 × 1 + (−1) × 1 = 3. The L2-norm of both vectors is 6. For example, x has L2-norm p 1 2 + 22 + (−1)2 = √ 6. Thus, the cosine of the angle between x and y is 3/( √ 6 √ 6) or 1/2. The angle whose cosine is 1/2 is 60 degrees, so that is the cosine distance between x and y. ✷ We must show that the cosine distance is indeed a distance measure. We have defined it so the values are in the range 0 to 180, so no negative distances 3.5. DISTANCE MEASURES 77 are possible. Two vectors have angle 0 if and only if they are the same direction.3 Symmetry is obvious: the angle between x and y is the same as the angle between y and x. The triangle inequality is best argued by physical reasoning. One way to rotate from x to y is to rotate to z and thence to y. The sum of those two rotations cannot be less than the rotation directly from x to y. 3.5.5 Edit Distance This distance makes sense when points are strings. The distance between two strings x = x1x2 · · · xn and y = y1y2 · · · ym is the smallest number of insertions and deletions of single characters that will convert x to y. Example 3.14 : The edit distance between the strings x = abcde and y = acfdeg is 3. To convert x to y: 1. Delete b. 2. Insert f after c. 3. Insert g after e. No sequence of fewer than three insertions and/or deletions will convert x to y. Thus, d(x, y) = 3. ✷ Another way to define and calculate the edit distance d(x, y) is to compute a longest common subsequence (LCS) of x and y. An LCS of x and y is a string that is constructed by deleting positions from x and y, and that is as long as any string that can be constructed that way. The edit distance d(x, y) can be calculated as the length of x plus the length of y minus twice the length of their LCS. Example 3.15 : The strings x = abcde and y = acfdeg from Example 3.14 have a unique LCS, which is acde. We can be sure it is the longest possible, because it contains every symbol appearing in both x and y. Fortunately, these common symbols appear in the same order in both strings, so we are able to use them all in an LCS. Note that the length of x is 5, the length of y is 6, and the length of their LCS is 4. The edit distance is thus 5 + 6 − 2 × 4 = 3, which agrees with the direct calculation in Example 3.14. For another example, consider x = aba and y = bab. Their edit distance is 2. For example, we can convert x to y by deleting the first a and then inserting b at the end. There are two LCS’s: ab and ba. Each can be obtained by deleting one symbol from each string. As must be the case for multiple LCS’s of the same pair of strings, both LCS’s have the same length. Therefore, we may compute the edit distance as 3 + 3 − 2 × 2 = 2. ✷ 3Notice that to satisfy the second axiom, we have to treat vectors that are multiples of one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these as different vectors, we would give them distance 0 and thus violate the condition that only d(x, x) is 0. 78 CHAPTER 3. FINDING SIMILAR ITEMS Non-Euclidean Spaces Notice that several of the distance measures introduced in this section are not Euclidean spaces. A property of Euclidean spaces that we shall find important when we take up clustering in Chapter 7 is that the average of points in a Euclidean space always exists and is a point in the space. However, consider the space of sets for which we defined the Jaccard distance. The notion of the “average” of two sets makes no sense. Likewise, the space of strings, where we can use the edit distance, does not let us take the “average” of strings. Vector spaces, for which we suggested the cosine distance, may or may not be Euclidean. If the components of the vectors can be any real numbers, then the space is Euclidean. However, if we restrict components to be integers, then the space is not Euclidean. Notice that, for instance, we cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors with two integer components, although if we treated them as members of the two-dimensional Euclidean space, then we could say that their average was [2.0, 1.5]. Edit distance is a distance measure. Surely no edit distance can be negative, and only two identical strings have an edit distance of 0. To see that edit distance is symmetric, note that a sequence of insertions and deletions can be reversed, with each insertion becoming a deletion, and vice-versa. The triangle inequality is also straightforward. One way to turn a string s into a string t is to turn s into some string u and then turn u into t. Thus, the number of edits made going from s to u, plus the number of edits made going from u to t cannot be less than the smallest number of edits that will turn s into t. 3.5.6 Hamming Distance Given a space of vectors, we define the Hamming distance between two vectors to be the number of components in which they differ. It should be obvious that Hamming distance is a distance measure. Clearly the Hamming distance cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle inequality should also be evident. If x and z differ in m components, and z and y differ in n components, then x and y cannot differ in more than m+n components. Most commonly, Hamming distance is used when the vectors are boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can have components from any set. Example 3.16 : The Hamming distance between the vectors 10101 and 11110 is 3. That is, these vectors differ in the second, fourth, and fifth components, 3.5. DISTANCE MEASURES 79 while they agree in the first and third components. ✷ 3.5.7 Exercises for Section 3.5 ! Exercise 3.5.1 : On the space of nonnegative integers, which of the following functions are distance measures? If so, prove it; if not, prove that it fails to satisfy one or more of the axioms. (a) max(x, y) = the larger of x and y. (b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x and y). (c) sum(x, y) = x + y. Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and (8, 2, 4). !! Exercise 3.5.3 : Prove that if i and j are any positive integers, and i < j, then the Li norm between any two points is greater than the Lj norm between those same two points. Exercise 3.5.4 : Find the Jaccard distances between the following pairs of sets: (a) {1, 2, 3, 4} and {2, 3, 4, 5}. (b) {1, 2, 3} and {4, 5, 6}. Exercise 3.5.5 : Compute the cosines of the angles between each of the following pairs of vectors.4 (a) (3, −1, 2) and (−2, 3, 1). (b) (1, 2, 3) and (2, 4, 6). (c) (5, 0, −4) and (−1, −6, 2). (d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0). ! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s and 1’s, of the same length, is at most 90 degrees. Exercise 3.5.7 : Find the edit distances (using only insertions and deletions) between the following pairs of strings. 4Note that what we are asking for is not precisely the cosine distance, but from the cosine of an angle, you can compute the angle itself, perhaps with the aid of a table or library function. 80 CHAPTER 3. FINDING SIMILAR ITEMS (a) abcdef and bdaefc. (b) abccdabc and acbdcab. (c) abcdef and baedfc. ! Exercise 3.5.8 : There are a number of other notions of edit distance available. For instance, we can allow, in addition to insertions and deletions, the following operations: i. Mutation, where one symbol is replaced by another symbol. Note that a mutation can always be performed by an insertion followed by a deletion, but if we allow mutations, then this change counts for only 1, not 2, when computing the edit distance. ii. Transposition, where two adjacent symbols have their positions swapped. Like a mutation, we can simulate a transposition by one insertion followed by one deletion, but here we count only 1 for these two steps. Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions, deletions, mutations, and transpositions needed to transform one string into another. ! Exercise 3.5.9 : Prove that the edit distance discussed in Exercise 3.5.8 is indeed a distance measure. Exercise 3.5.10 : Find the Hamming distances between each pair of the following vectors: 000000, 110011, 010101, and 011100. 3.6 The Theory of Locality-Sensitive Functions The LSH technique developed in Section 3.4 is one example of a family of functions (the minhash functions) that can be combined (by the banding technique) to distinguish strongly between pairs at a low distance from pairs at a high distance. The steepness of the S-curve in Fig. 3.7 reflects how effectively we can avoid false positives and false negatives among the candidate pairs. Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can apply to the space of sets and the Jaccard distance, or to another space and/or another distance measure. There are three conditions that we need for a family of functions: 1. They must be more likely to make close pairs be candidate pairs than distant pairs. We make this notion precise in Section 3.6.1. 2. They must be statistically independent, in the sense that it is possible to estimate the probability that two or more functions will all give a certain response by the product rule for independent events. 3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 81 3. They must be efficient, in two ways: (a) They must be able to identify candidate pairs in time much less than the time it takes to look at all pairs. For example, minhash functions have this capability, since we can hash sets to minhash values in time proportional to the size of the data, rather than the square of the number of sets in the data. Since sets with common values are colocated in a bucket, we have implicitly produced the candidate pairs for a single minhash function in time much less than the number of pairs of sets. (b) They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must also take time that is much less than the number of pairs. For example, the banding technique of Section 3.4.1 takes single minhash functions, which satisfy condition 3a but do not, by themselves have the S-curve behavior we want, and produces from a number of minhash functions a combined function that has the S-curve shape. Our first step is to define “locality-sensitive functions” generally. We then see how the idea can be applied in several applications. Finally, we discuss how to apply the theory to arbitrary data with either a cosine distance or a Euclidean distance measure. 3.6.1 Locality-Sensitive Functions For the purposes of this section, we shall consider functions that take two items and render a decision about whether these items should be a candidate pair. In many cases, the function f will “hash” items, and the decision will be based on whether or not the result is equal. Because it is convenient to use the notation f(x) = f(y) to mean that f(x, y) is “yes; make x and y a candidate pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use f(x) 6= f(y) to mean “do not make x and y a candidate pair unless some other function concludes we should do so.” A collection of functions of this form will be called a family of functions. For example, the family of minhash functions, each based on one of the possible permutations of rows of a characteristic matrix, form a family. Let d1 < d2 be two distances according to some distance measure d. A family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F: 1. If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1. 2. If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2. Figure 3.9 illustrates what we expect about the probability that a given function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a candidate pair. Notice that we say nothing about what happens when the distance 82 CHAPTER 3. FINDING SIMILAR ITEMS Probabilty of being declared a candidate d p d p 1 2 1 2 Distance Figure 3.9: Behavior of a (d1, d2, p1, p2)-sensitive function between the items is strictly between d1 and d2, but we can make d1 and d2 as close as we wish. The penalty is that typically p1 and p2 are then close as well. As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2 fixed. 3.6.2 Locality-Sensitive Families for Jaccard Distance For the moment, we have only one way to find a family of locality-sensitive functions: use the family of minhash functions, and assume that the distance measure is the Jaccard distance. As before, we interpret a minhash function h to make x and y a candidate pair if and only if h(x) = h(y). • The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family for any d1 and d2, where 0 ≤ d1 < d2 ≤ 1. The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity of x and y is equal to the probability that a minhash function will hash x and y to the same value. A similar argument applies to d2 or any distance. Example 3.17 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is, if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x, y) ≥ 0.7) then there is at least a 0.7 chance that a minhash function will send x and y to the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e., SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent to the same value. Note that we could make the same assertion with another choice of d1 and d2; only d1 < d2 is required. ✷ 3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 83 3.6.3 Amplifying a Locality-Sensitive Family Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a new family F ′ by the AND-construction on F, which is defined as follows. Each member of F ′ consists of r members of F for some fixed r. If f is in F ′ , and f is constructed from the set {f1, f2, . . . , fr} of members of F, we say f(x) = f(y) if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction mirrors the effect of the r rows in a single band: the band makes x and y a candidate pair if every one of the r rows in the band say that x and y are equal (and therefore a candidate pair according to that row). Since the members of F are independently chosen to make a member of F ′ , we can assert that F ′ is a d1, d2,(p1) r ,(p2) r  -sensitive family. That is, for any p, if p is the probability that a member of F will declare (x, y) to be a candidate pair, then the probability that a member of F ′ will so declare is p r . There is another construction, which we call the OR-construction, that turns a (d1, d2, p1, p2)-sensitive family F into a d1, d2, 1 − (1 − p1) b , 1 − (1 − p2) b  - sensitive family F ′ . Each member f of F ′ is constructed from b members of F, say f1, f2, . . . , fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or more values of i. The OR-construction mirrors the effect of combining several bands: x and y become a candidate pair if any band makes them a candidate pair. If p is the probability that a member of F will declare (x, y) to be a candidate pair, then 1−p is the probability it will not so declare. (1−p) b is the probability that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1 − (1 − p) b is the probability that at least one fi will declare (x, y) a candidate pair, and therefore that f will declare (x, y) to be a candidate pair. Notice that the AND-construction lowers all probabilities, but if we choose F and r judiciously, we can make the small probability p2 get very close to 0, while the higher probability p1 stays significantly away from 0. Similarly, the ORconstruction makes all probabilities rise, but by choosing F and b judiciously, we can make the larger probability approach 1 while the smaller probability remains bounded away from 1. We can cascade AND- and OR-constructions in any order to make the low probability close to 0 and the high probability close to 1. Of course the more constructions we use, and the higher the values of r and b that we pick, the larger the number of functions from the original family that we are forced to use. Thus, the better the final family of functions is, the longer it takes to apply the functions from this family. Example 3.18 : Suppose we start with a family F. We use the AND-construction with r = 4 to produce a family F1. We then apply the OR-construction to F1 with b = 4 to produce a third family F2. Note that the members of F2 each are built from 16 members of F, and the situation is analogous to starting with 16 minhash functions and treating them as four bands of four rows each. The 4-way AND-function converts any probability p into p 4 . When we follow it by the 4-way OR-construction, that probability is further converted into 1−(1−p 4 ) 4 . Some values of this transformation are indicated in Fig. 3.1 84 CHAPTER 3. FINDING SIMILAR ITEMS p 1 − (1 − p 4 ) 4 0.2 0.0064 0.3 0.0320 0.4 0.0985 0.5 0.2275 0.6 0.4260 0.7 0.6666 0.8 0.8785 0.9 0.9860 Figure 3.10: Effect of the 4-way AND-construction followed by the 4-way ORconstruction This function is an S-curve, staying low for a while, then rising steeply (although not too steeply; the slope never gets much higher than 2), and then leveling off at high values. Like any S-curve, it has a fixedpoint, the value of p that is left unchanged when we apply the function of the S-curve. In this case, the fixedpoint is the value of p for which p = 1 − (1 − p 4 ) 4 . We can see that the fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are decreased, and above it they are increased. Thus, if we pick a high probability above the fixedpoint and a low probability below it, we shall have the desired effect that the low probability is decreased and the high probability is increased. Suppose F is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then F2, the family constructed by a 4-way AND followed by a 4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the rows for 0.2 and 0.6 in Fig. 3.10. By replacing F by F2, we have reduced both the false-negative and false-positive rates, at the cost of making application of the functions take 16 times as long. ✷ p 1 − (1 − p) 4 4 0.1 0.0140 0.2 0.1215 0.3 0.3334 0.4 0.5740 0.5 0.7725 0.6 0.9015 0.7 0.9680 0.8 0.9936 Figure 3.11: Effect of the 4-way OR-construction followed by the 4-way ANDconstructio 3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 85 Example 3.19 : For the same cost, we can apply a 4-way OR-construction followed by a 4-way AND-construction. Figure 3.11 gives the transformation on probabilities implied by this construction. For instance, suppose that F is a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a (0.2, 0.6, 0.9936, 0.5740)-sensitive family. This choice is not necessarily the best. Although the higher probability has moved much closer to 1, the lower probability has also raised, increasing the number of false positives. ✷ Example 3.20 : We can cascade constructions as much as we like. For example, we could use the construction of Example 3.18 on the family of minhash functions and then use the construction of Example 3.19 on the resulting family. The constructed family would then have functions each built from 256 minhash functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family into a (0.2, 0.8, 0.99999996, 0.0008715)-sensitive family. ✷ 3.6.4 Exercises for Section 3.6 Exercise 3.6.1 : What is the effect on probability of starting with the family of minhash functions and applying: (a) A 2-way AND construction followed by a 3-way OR construction. (b) A 3-way OR construction followed by a 2-way AND construction. (c) A 2-way AND construction followed by a 2-way OR construction, followed by a 2-way AND construction. (d) A 2-way OR construction followed by a 2-way AND construction, followed by a 2-way OR construction followed by a 2-way AND construction. Exercise 3.6.2 : Find the fixedpoints for each of the functions constructed in Exercise 3.6.1. ! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.10, has a slope given by the derivative of the function. The maximum slope is where that derivative is a maximum. Find the value of p that gives a maximum slope for the S-curves given by Fig. 3.10 and Fig. 3.11. What are the values of these maximum slopes? !! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the point of maximum slope and the value of that slope, for families of functions defined from the minhash functions by: (a) An r-way AND construction followed by a b-way OR construction. (b) A b-way OR construction followed by an r-way AND construction. 86 CHAPTER 3. FINDING SIMILAR ITEMS 3.7 LSH Families for Other Distance Measures There is no guarantee that a distance measure has a locality-sensitive family of hash functions. So far, we have only seen such families for the Jaccard distance. In this section, we shall show how to construct locality-sensitive families for Hamming distance, the cosine distance and for the normal Euclidean distance. 3.7.1 LSH Families for Hamming Distance It is quite simple to build a locality-sensitive family of functions for the Hamming distance. Suppose we have a space of d-dimensional vectors, and h(x, y) denotes the Hamming distance between vectors x and y. If we take any one position of the vectors, say the ith position, we can define the function fi(x) to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and y agree in the ith position. Then the probability that fi(x) = fi(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in which x and y agree. This situation is almost exactly like the one we encountered for minhashing. Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a (d1, d2, 1 − d1/d, 1 − d2/d)-sensitive family of hash functions, for any d1 < d2. There are only two differences between this family and the family of minhash functions. 1. While Jaccard distance runs from 0 to 1, the Hamming distance on a vector space of dimension d runs from 0 to d. It is therefore necessary to scale the distances by dividing by d, to turn them into probabilities. 2. While there is essentially an unlimited supply of minhash functions, the size of the family F for Hamming distance is only d. The first point is of no consequence; it only requires that we divide by d at appropriate times. The second point is more serious. If d is relatively small, then we are limited in the number of functions that can be composed using the AND and OR constructions, thereby limiting how steep we can make the S-curve be. 3.7.2 Random Hyperplanes and the Cosine Distance Recall from Section 3.5.4 that the cosine distance between two vectors is the angle between the vectors. For instance, we see in Fig. 3.12 two vectors x and y that make an angle θ between them. Note that these vectors may be in a space of many dimensions, but they always define a plane, and the angle between them is measured in this plane. Figure 3.12 is a “top-view” of the plane containing x and y. 3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 87 θ x y Figure 3.12: Two vectors make an angle θ Suppose we pick a hyperplane through the origin. This hyperplane intersects the plane of x and y in a line. Figure 3.12 suggests two possible hyperplanes, one whose intersection is the dashed line and the other’s intersection is the dotted line. To pick a random hyperplane, we actually pick the normal vector to the hyperplane, say v. The hyperplane is then the set of points whose dot product with v is 0. First, consider a vector v that is normal to the hyperplane whose projection is represented by the dashed line in Fig. 3.12; that is, x and y are on different sides of the hyperplane. Then the dot products v.x and v.y will have different signs. If we assume, for instance, that v is a vector whose projection onto the plane of x and y is above the dashed line in Fig. 3.12, then v.x is positive, while v.y is negative. The normal vector v instead might extend in the opposite direction, below the dashed line. In that case v.x is negative and v.y is positive, but the signs are still different. On the other hand, the randomly chosen vector v could be normal to a hyperplane like the dotted line in Fig. 3.12. In that case, both v.x and v.y have the same sign. If the projection of v extends to the right, then both dot products are positive, while if v extends to the left, then both are negative. What is the probability that the randomly chosen vector is normal to a hyperplane that looks like the dashed line rather than the dotted line? All angles for the line that is the intersection of the random hyperplane and the plane of x and y are equally likely. Thus, the hyperplane will look like the dashed line with probability θ/180 and will look like the dotted line otherwise. Thus, each hash function f in our locality-sensitive family F is built from a randomly chosen vector vf . Given two vectors x and y, say f(x) = f(y) if and only if the dot products vf .x and vf .y have the same sign. Then F is a locality-sensitive family for the cosine distance. The parameters are essentially 88 CHAPTER 3. FINDING SIMILAR ITEMS the same as for the Jaccard-distance family described in Section 3.6.2, except the scale of distances is 0–180 rather than 0–1. That is, F is a (d1, d2,(180 − d1)/180, d2/180)-sensitive family of hash functions. From this basis, we can amplify the family as we wish, just as for the minhash-based family. 3.7.3 Sketches Instead of chosing a random vector from all possible vectors, it turns out to be sufficiently random if we restrict our choice to vectors whose components are +1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s is formed by adding the components of x where v is +1 and then subtracting the other components of x – those where v is −1. If we pick a collection of random vectors, say v1, v2, . . . , vn, then we can apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then replacing any positive value by +1 and any negative value by −1. The result is called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1 or −1 at random. Since there is only a tiny probability of a zero dot product, the choice has essentially no effect. Example 3.21 : Suppose our space consists of 4-dimensional vectors, and we pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1]. That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component of the sketch is +1. Similarly, v2.x = 3 and v3.x = −4, so the second component of the sketch is +1 and the third component is −1. Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions, we estimate that the angle between them is 120 degrees. That is, a randomly chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.12 than like the dotted line. The above conclusion turns out to be quite wrong. We can calculate the cosine of the angle between x and y to be x.y, which is 6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40 divided by the magnitudes of the two vectors. These magnitudes are p 6 2 + 52 + 42 + 32 = 9.274 and √ 1 2 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and y is 0.7875, and this angle is about 38 degrees. However, if you look at all 16 different vectors v of length 4 that have +1 and −1 as components, you find that there are only four of these whose dot products with x and y have a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and [−1, −1, +1, +1]. Thus, had we picked all sixteen of these vectors to form a sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷ 3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 89 3.7.4 LSH Families for Euclidean Distance Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can develop a locality-sensitive family of hash functions for this distance. We shall start with a 2-dimensional Euclidean space. Each hash function f in our family F will be associated with a randomly chosen line in this space. Pick a constant a and divide the line into segments of length a, as suggested by Fig. 3.13, where the “random” line has been oriented to be horizontal. θ Points at distance Bucket width a d Figure 3.13: Two points at distance d ≫ a have a small chance of being hashed to the same bucket The segments of the line are the buckets into which function f hashes points. A point is hashed to the bucket in which its projection onto the line lies. If the distance d between two points is small compared with a, then there is a good chance the two points hash to the same bucket, and thus the hash function f will declare the two points equal. For example, if d = a/2, then there is at least a 50% chance the two points will fall in the same bucket. In fact, if the angle θ between the randomly chosen line and the line connecting the points is large, then there is an even greater chance that the two points will fall in the same bucket. For instance, if θ is 90 degrees, then the two points are certain to fall in the same bucket. However, suppose d is larger than a. In order for there to be any chance of the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of Fig. 3.13 suggests why this requirement holds. Note that even if d cos θ ≪ a it is still not certain that the two points will fall in the same bucket. However, we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3 chance the two points fall in the same bucket. The reason is that for cos θ to be less than 1/2, we need to have θ in the range 60 to 90 degrees. If θ is in the range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller angle between two randomly chosen lines in the plane, θ is twice as likely to be between 0 and 60 as it is to be between 60 and 90. 90 CHAPTER 3. FINDING SIMILAR ITEMS We conclude that the family F just described forms a (a/2, 2a, 1/2, 1/3)- sensitive family of hash functions. That is, for distances up to a/2 the probability is at least 1/2 that two points at that distance will fall in the same bucket, while for distances at least 2a the probability points at that distance will fall in the same bucket is at most 1/3. We can amplify this family as we like, just as for the other examples of locality-sensitive hash functions we have discussed. 3.7.5 More LSH Families for Euclidean Spaces There is something unsatisfying about the family of hash functions developed in Section 3.7.4. First, the technique was only described for two-dimensional Euclidean spaces. What happens if our data is points in a space with many dimensions? Second, for Jaccard and cosine distances, we were able to develop locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2. In Section 3.7.4 we appear to need the stronger condition d1 < 4d2. However, we claim that there is a locality-sensitive family of hash functions for any d1 < d2 and for any number of dimensions. The family’s hash functions still derive from random lines through the space and a bucket size a that partitions the line. We still hash points by projecting them onto the line. Given that d1 < d2, we may not know what the probability p1 is that two points at distance d1 hash to the same bucket, but we can be certain that it is greater than p2, the probability that two points at distance d2 hash to the same bucket. The reason is that this probability surely grows as the distance shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any given number of dimensions. Using the amplification techniques of Section 3.6.3, we can then adjust the two probabilities to surround any particular value we like, and to be as far apart as we like. Of course, the further apart we want the probabilities to be, the larger the number of basic hash functions in F we must use. 3.7.6 Exercises for Section 3.7 Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive functions for vectors of length six. For each pair of the vectors 000000, 110011, 010101, and 011100, which of the six functions makes them candidates? Exercise 3.7.2 : Let us compute sketches using the following four “random” vectors: v1 = [+1, +1, +1, −1] v2 = [+1, +1, −1, +1] v3 = [+1, −1, +1, +1] v4 = [−1, +1, +1, +1] Compute the sketches of the following vectors. (a) [2, 3, 4, 5]. 3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 91 (b) [−2, 3, −4, 5]. (c) [2, −3, 4, −5]. For each pair, what is the estimated angle between them, according to the sketches? What are the true angles? Exercise 3.7.3 : Suppose we form sketches by using all sixteen of the vectors of length 4, whose components are each +1 or −1. Compute the sketches of the three vectors in Exercise 3.7.2. How do the estimates of the angles between each pair compare with the true angles? Exercise 3.7.4 : Suppose we form sketches using the four vectors from Exercise 3.7.2. ! (a) What are the constraints on a, b, c, and d that will cause the sketch of the vector [a, b, c, d] to be [+1, +1, +1, +1]? !! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on a, b, . . . , h that will make the sketches of these two vectors be the same? Exercise 3.7.5 : Suppose we have points in a 3-dimensional Euclidean space: p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions defined by the three axes (to make our calculations very easy). Let buckets be of length a, with one bucket the interval [0, a) (i.e., the set of points x such that 0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on. (a) For each of the three lines, assign each of the points to buckets, assuming a = 1. (b) Repeat part (a), assuming a = 2. (c) What are the candidate pairs for the cases a = 1 and a = 2? ! (d) For each pair of points, for what values of a will that pair be a candidate pair? 3.8 Applications of Locality-Sensitive Hashing In this section, we shall explore three examples of how LSH is used in practice. In each case, the techniques we have learned must be modified to meet certain constraints of the problem. The three subjects we cover are: 1. Entity Resolution: This term refers to matching data records that refer to the same real-world entity, e.g., the same person. The principal problem addressed here is that the similarity of records does not match exactly either the similar-sets or similar-vectors models of similarity on which the theory is built. 92 CHAPTER 3. FINDING SIMILAR ITEMS 2. Matching Fingerprints: It is possible to represent fingerprints as sets. However, we shall explore a different family of locality-sensitive hash functions from the one we get by minhashing. 3. Matching Newspaper Articles: Here, we consider a different notion of shingling that focuses attention on the core article in an on-line newspaper’s Web page, ignoring all the extraneous material such as ads and newspaper-specific material. 3.8.1 Entity Resolution It is common to have several data sets available, and to know that they refer to some of the same entities. For example, several different bibliographic sources provide information about many of the same books or papers. In the general case, we have records describing entities of some type, such as people or books. The records may all have the same format, or they may have different formats, with different kinds of information. There are many reasons why information about an entity may vary, even if the field in question is supposed to be the same. For example, names may be expressed differently in different records because of misspellings, absence of a middle initial, use of a nickname, and many other reasons. For example, “Bob S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If records come from different sources, the fields may differ as well. One source’s records may have an “age” field, while another does not. The second source might have a “date of birth” field, or it may have no information at all about when a person was born. 3.8.2 An Entity-Resolution Example We shall examine a real example of how LSH was used to deal with an entityresolution problem. Company A was engaged by Company B to solicit customers for B. Company B would pay A a yearly fee, as long as the customer maintained their subscription. They later quarreled and disagreed over how many customers A had provided to B. Each had about 1,000,000 records, some of which described the same people; those were the customers A had provided to B. The records had different data fields, but unfortunately none of those fields was “this is a customer that A had provided to B.” Thus, the problem was to match records from the two sets to see if a pair represented the same person. Each record had fields for the name, address, and phone number of the person. However, the values in these fields could differ for many reasons. Not only were there the misspellings and other naming differences mentioned in Section 3.8.1, but there were other opportunities to disagree as well. A customer might give their home phone to A and their cell phone to B. Or they might move, and tell B but not A (because they no longer had need for a relationship with A). Area codes of phones sometimes change. 3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 93 The strategy for identifying records involved scoring the differences in three fields: name, address, and phone. To create a score describing the likelihood that two records, one from A and the other from B, described the same person, 100 points was assigned to each of the three fields, so records with exact matches in all three fields got a score of 300. However, there were deductions for mismatches in each of the three fields. As a first approximation, edit-distance (Section 3.5.5) was used, but the penalty grew quadratically with the distance. Then, certain publicly available tables were used to reduce the penalty in appropriate situations. For example, “Bill” and “William” were treated as if they differed in only one letter, even though their edit-distance is 5. However, it is not feasible to score all one trillion pairs of records. Thus, a simple LSH was used to focus on likely candidates. Three “hash functions” were used. The first sent records to the same bucket only if they had identical names; the second did the same but for identical addresses, and the third did the same for phone numbers. In practice, there was no hashing; rather the records were sorted by name, so records with identical names would appear consecutively and get scored for overall similarity of the name, address, and phone. Then the records were sorted by address, and those with the same address were scored. Finally, the records were sorted a third time by phone, and records with identical phones were scored. This approach missed a record pair that truly represented the same person but none of the three fields matched exactly. Since the goal was to prove in a court of law that the persons were the same, it is unlikely that such a pair would have been accepted by a judge as sufficiently similar anyway. 3.8.3 Validating Record Matches What remains is to determine how high a score indicates that two records truly represent the same individual. In the example at hand, there was an easy way to make that decision, and the technique can be applied in many similar situations. It was decided to look at the creation-dates for the records at hand, and to assume that 90 days was an absolute maximum delay between the time the service was bought at Company A and registered at B. Thus, a proposed match between two records that were chosen at random, subject only to the constraint that the date on the B-record was between 0 and 90 days after the date on the A-record, would have an average delay of 45 days. It was found that of the pairs with a perfect 300 score, the average delay was 10 days. If you assume that 300-score pairs are surely correct matches, then you can look at the pool of pairs with any given score s, and compute the average delay of those pairs. Suppose that the average delay is x, and the fraction of true matches among those pairs with score s is f. Then x = 10f + 45(1 − f), or x = 45 − 35f. Solving for f, we find that the fraction of the pairs with score s that are truly matches is (45 − x)/35. The same trick can be used whenever: 94 CHAPTER 3. FINDING SIMILAR ITEMS When Are Record Matches Good Enough? While every case will be different, it may be of interest to know how the experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For scores down to 185, the value of x was very close to 10; i.e., these scores indicated that the likelihood of the records representing the same person was essentially 1. Note that a score of 185 in this example represents a situation where one field is the same (as would have to be the case, or the records would never even be scored), one field was completely different, and the third field had a small discrepancy. Moreover, for scores as low as 115, the value of x was noticeably less than 45, meaning that some of these pairs did represent the same person. Note that a score of 115 represents a case where one field is the same, but there is only a slight similarity in the other two fields. 1. There is a scoring system used to evaluate the likelihood that two records represent the same entity, and 2. There is some field, not used in the scoring, from which we can derive a measure that differs, on average, for true pairs and false pairs. For instance, suppose there were a “height” field recorded by both companies A and B in our running example. We can compute the average difference in height for pairs of random records, and we can compute the average difference in height for records that have a perfect score (and thus surely represent the same entities). For a given score s, we can evaluate the average height difference of the pairs with that score and estimate the probability of the records representing the same entity. That is, if h0 is the average height difference for the perfect matches, h1 is the average height difference for random pairs, and h is the average height difference for pairs of score s, then the fraction of good pairs with score s is (h1 − h)/(h1 − h0). 3.8.4 Matching Fingerprints When fingerprints are matched by computer, the usual representation is not an image, but a set of locations in which minutiae are located. A minutia, in the context of fingerprint descriptions, is a place where something unusual happens, such as two ridges merging or a ridge ending. If we place a grid over a fingerprint, we can represent the fingerprint by the set of grid squares in which minutiae are located. Ideally, before overlaying the grid, fingerprints are normalized for size and orientation, so that if we took two images of the same finger, we would find minutiae lying in exactly the same grid squares. We shall not consider here the best ways to normalize images. Let us assume that some combination of 3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 95 techniques, including choice of grid size and placing a minutia in several adjacent grid squares if it lies close to the border of the squares enables us to assume that grid squares from two images have a significantly higher probability of agreeing in the presence or absence of a minutia than if they were from images of different fingers. Thus, fingerprints can be represented by sets of grid squares – those where their minutiae are located – and compared like any sets, using the Jaccard similarity or distance. There are two versions of fingerprint comparison, however. • The many-one problem is the one we typically expect. A fingerprint has been found on a gun, and we want to compare it with all the fingerprints in a large database, to see which one matches. • The many-many version of the problem is to take the entire database, and see if there are any pairs that represent the same individual. While the many-many version matches the model that we have been following for finding similar items, the same technology can be used to speed up the many-one problem. 3.8.5 A LSH Family for Fingerprint Matching We could minhash the sets that represent a fingerprint, and use the standard LSH technique from Section 3.4. However, since the sets are chosen from a relatively small set of grid points (perhaps 1000), the need to minhash them into more succinct signatures is not clear. We shall study here another form of locality-sensitive hashing that works well for data of the type we are discussing. Suppose for an example that the probability of finding a minutia in a random grid square of a random fingerprint is 20%. Also, assume that if two fingerprints come from the same finger, and one has a minutia in a given grid square, then the probability that the other does too is 80%. We can define a locality-sensitive family of hash functions as follows. Each function f in this family F is defined by three grid squares. Function f says “yes” for two fingerprints if both have minutiae in all three grid squares, and otherwise f says “no.” Put another way, we may imagine that f sends to a single bucket all fingerprints that have minutiae in all three of f’s grid points, and sends each other fingerprint to a bucket of its own. In what follows, we shall refer to the first of these buckets as “the” bucket for f and ignore the buckets that are required to be singletons. If we want to solve the many-one problem, we can use many functions from the family F and precompute their buckets of fingerprints to which they answer “yes.” Then, given a new fingerprint that we want to match, we determine which of these buckets it belongs to and compare it with all the fingerprints found in any of those buckets. To solve the many-many problem, we compute the buckets for each of the functions and compare all fingerprints in each of the buckets. 96 CHAPTER 3. FINDING SIMILAR ITEMS Let us consider how many functions we need to get a reasonable probability of catching a match, without having to compare the fingerprint on the gun with each of the millions of fingerprints in the database. First, the probability that two fingerprints from different fingers would be in the bucket for a function f in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket only if they each have a minutia in each of the three grid points associated with f, and the probability of each of those six independent events is 0.2. Now, consider the probability that two fingerprints from the same finger wind up in the bucket for f. The probability that the first fingerprint has minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However, if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint will as well. Thus, if the fingerprints are from the same finger, there is a 0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f. That is not much; it is about one in 200. However, if we use many functions from F, but not too many, then we can get a good probability of matching fingerprints from the same finger while not having too many false positives – fingerprints that must be considered but do not match. Example 3.22 : For a specific example, let us suppose that we use 1024 functions chosen randomly from F. Next, we shall construct a new family F1 by performing a 1024-way OR on F. Then the probability that F1 will put fingerprints from the same finger together in at least one bucket is 1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that two fingerprints from different fingers will be placed in the same bucket is (1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives and about 6.3% false positives. ✷ The result of Example 3.22 is not the best we can do. While it offers only a 1.5% chance that we shall fail to identify the fingerprint on the gun, it does force us to look at 6.3% of the entire database. Increasing the number of functions from F will increase the number of false positives, with only a small benefit of reducing the number of false negatives below 1.5%. On the other hand, we can also use the AND construction, and in so doing, we can greatly reduce the probability of a false positive, while making only a small increase in the false-negative rate. For instance, we could take 2048 functions from F in two groups of 1024. Construct the buckets for each of the functions. However, given a fingerprint P on the gun: 1. Find the buckets from the first group in which P belongs, and take the union of these buckets. 2. Do the same for the second group. 3. Take the intersection of the two unions. 4. Compare P only with those fingerprints in the intersection. 3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 97 Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints can be represented by their integer indices in the database. If we use this scheme, the probability of detecting a matching fingerprint is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the probability of a false positive is (0.063)2 = 0.00397. That is, we only have to examine about 1/250th of the database. 3.8.6 Similar News Articles Our last case study concerns the problem of organizing a large repository of on-line news articles by grouping together Web pages that were derived from the same basic text. It is common for organizations like The Associated Press to produce a news item and distribute it to many newspapers. Each newspaper puts the story in its on-line edition, but surrounds it by information that is special to that newspaper, such as the name and address of the newspaper, links to related articles, and links to ads. In addition, it is common for the newspaper to modify the article, perhaps by leaving off the last few paragraphs or even deleting text from the middle. As a result, the same news article can appear quite different at the Web sites of different newspapers. The problem looks very much like the one that was suggested in Section 3.4: find documents whose shingles have a high Jaccard similarity. Note that this problem is different from the problem of finding news articles that tell about the same events. The latter problem requires other techniques, typically examining the set of important words in the documents (a concept we discussed briefly in Section 1.3.1) and clustering them to group together different articles about the same topic. However, an interesting variation on the theme of shingling was found to be more effective for data of the type described. The problem is that shingling as we described it in Section 3.2 treats all parts of a document equally. However, we wish to ignore parts of the document, such as ads or the headlines of other articles to which the newspaper added a link, that are not part of the news article. It turns out that there is a noticeable difference between text that appears in prose and text that appears in ads or headlines. Prose has a much greater frequency of stop words, the very frequent words such as “the” or “and.” The total number of words that are considered stop words varies with the application, but it is common to use a list of several hundred of the most frequent words. Example 3.23 : A typical ad might say simply “Buy Sudzo.” On the other hand, a prose version of the same thought that might appear in an article is “I recommend that you buy Sudzo for your laundry.” In the latter sentence, it would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words. ✷ 98 CHAPTER 3. FINDING SIMILAR ITEMS Suppose we define a shingle to be a stop word followed by the next two words. Then the ad “Buy Sudzo” from Example 3.23 has no shingles and would not be reflected in the representation of the Web page containing that ad. On the other hand, the sentence from Example 3.23 would be represented by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for your laundry,” and “your laundry x,” where x is whatever word follows that sentence. Suppose we have two Web pages, each of which consists of half news text and half ads or other material that has a low density of stop words. If the news text is the same but the surrounding material is different, then we would expect that a large fraction of the shingles of the two pages would be the same. They might have a Jaccard similarity of 75%. However, if the surrounding material is the same but the news content is different, then the number of common shingles would be small, perhaps 25%. If we were to use the conventional shingling, where shingles are (say) sequences of 10 consecutive characters, we would expect the two documents to share half their shingles (i.e., a Jaccard similarity of 1/3), regardless of whether it was the news or the surrounding material that they shared. 3.8.7 Exercises for Section 3.8 Exercise 3.8.1 : Suppose we are trying to perform entity resolution among bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that all references include a year of publication, and this year is equally likely to be any of the ten most recent years. Further, suppose that we discover that among the pairs of references with a perfect score, there is an average difference in the publication year of 0.1.5 Suppose that the pairs of references with a certain score s are found to have an average difference in their publication dates of 2. What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference in publication date between random pairs is 5 or 5.5. You need to calculate it exactly, and you have enough information to do so. Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80% chance of a second copy of a fingerprint having a minutia in a grid square where the first copy does, and each function in F being formed from three grid squares. In Example 3.22, we constructed family F1 by using the OR construction on 1024 members of F. Suppose we instead used family F2 that is a 2048-way OR of members of F. (a) Compute the rates of false positives and false negatives for F2. 5We might expect the average to be 0, but in practice, errors in publication year do occur. 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 99 (b) How do these rates compare with what we get if we organize the same 2048 functions into a 2-way AND of members of F1, as was discussed at the end of Section 3.8.5? Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F ′ defined like F, but using only two randomly chosen grid squares. Construct another set of functions F ′ 1 from F ′ by taking the n-way OR of functions from F ′ . What, as a function of n, are the false positive and false negative rates for F ′ 1 ? Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.22, but we want to solve the many-many problem. (a) If two fingerprints are from the same finger, what is the probability that they will not be compared (i.e., what is the false negative rate)? (b) What fraction of the fingerprints from different fingers will be compared (i.e., what is the false positive rate)? ! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2, and we construct a new set of functions F3 by an n-way OR of functions in F. For what value of n is the sum of the false positive and false negative rates minimized? 3.9 Methods for High Degrees of Similarity LSH-based methods appear most effective when the degree of similarity we accept is relatively low. When we want to find sets that are almost identical, there are other methods that can be faster. Moreover, these methods are exact, in that they find every pair of items with the desired degree of similarity. There are no false negatives, as there can be with LSH. 3.9.1 Finding Identical Items The extreme case is finding identical items, for example, Web pages that are identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents that fell into the same bucket. That scheme should work well, unless all the documents begin with the same characters, such as an HTML header. Our second thought would be to use a hash function that examines the entire document. That would work, and if we use enough buckets, it would be very rare that two documents went into the same bucket, yet were not identical. The downside of this approach is that we must examine every character of every document. If we limit our examination to a small number of characters, then 100 CHAPTER 3. FINDING SIMILAR ITEMS we never have to examine a document that is unique and falls into a bucket of its own. A better approach is to pick some fixed random positions for all documents, and make the hash function depend only on these. This way, we can avoid a problem where there is a common prefix for all or most documents, yet we need not examine entire documents unless they fall into a bucket with another document. One problem with selecting fixed positions is that if some documents are short, they may not have some of the selected positions. However, if we are looking for highly similar documents, we never need to compare two documents that differ significantly in their length. We exploit this idea in Section 3.9.3. 3.9.2 Representing Sets as Strings Now, let us focus on the harder problem of finding, in a large collection of sets, all pairs that have a high Jaccard similarity, say at least 0.9. We can represent a set by sorting the elements of the universal set in some fixed order, and representing any set by listing its elements in this order. The list is essentially a string of “characters,” where the characters are the elements of the universal set. These strings are unusual, however, in that: 1. No character appears more than once in a string, and 2. If two characters appear in two different strings, then they appear in the same order in both strings. Example 3.24 : Suppose the universal set consists of the 26 lower-case letters, and we use the normal alphabetical order. Then the set {d, a, b} is represented by the string abd. ✷ In what follows, we shall assume all strings represent sets in the manner just described. Thus, we shall talk about the Jaccard similarity of strings, when strictly speaking we mean the similarity of the sets that the strings represent. Also, we shall talk of the length of a string, as a surrogate for the number of elements in the set that the string represents. Note that the documents discussed in Section 3.9.1 do not exactly match this model, even though we can see documents as strings. To fit the model, we would shingle the documents, assign an order to the shingles, and represent each document by its list of shingles in the selected order. 3.9.3 Length-Based Filtering The simplest way to exploit the string representation of Section 3.9.2 is to sort the strings by length. Then, each string s is compared with those strings t that follow s in the list, but are not too long. Suppose the upper bound on Jaccard distance between two strings is J. For any string x, denote its length by Lx. Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 101 A Better Ordering for Symbols Instead of using the obvious order for elements of the universal set, e.g., lexicographic order for shingles, we can order symbols rarest first. That is, determine how many times each element appears in the collection of sets, and order them by this count, lowest first. The advantage of doing so is that the symbols in prefixes will tend to be rare. Thus, they will cause that string to be placed in index buckets that have relatively few members. Then, when we need to examine a string for possible matches, we shall find few other strings that are candidates for comparison. have more than Ls members, while their union has at least Lt members. Thus, the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt. That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt, or equivalently, Lt ≤ Ls/J. Example 3.25 : Suppose that s is a string of length 9, and we are looking for strings with at least 0.9 Jaccard similarity. Then we have only to compare s with strings following it in the length-based sorted order that have length at most 9/0.9 = 10. That is, we compare s with those strings of length 9 that follow it in order, and all strings of length 10. We have no need to compare s with any other string. Suppose the length of s were 8 instead. Then s would be compared with following strings of length up to 8/0.9 = 8.89. That is, a string of length 9 would be too long to have a Jaccard similarity of 0.9 with s, so we only have to compare s with the strings that have length 8 but follow it in the sorted order. ✷ 3.9.4 Prefix Indexing In addition to length, there are several other features of strings that can be exploited to limit the number of comparisons that must be made to identify all pairs of similar strings. The simplest of these options is to create an index for each symbol; recall a symbol of a string is any one of the elements of the universal set. For each string s, we select a prefix of s consisting of the first p symbols of s. How large p must be depends on Ls and J, the lower bound on Jaccard distance. We add string s to the index for each of its first p symbols. In effect, the index for each symbol becomes a bucket of strings that must be compared. We must be certain that any other string t such that SIM(s, t) ≥ J will have at least one symbol in its prefix that also appears in the prefix of s. Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of s. Then the highest Jaccard similarity that s and t can have occurs when t is a suffix of s, consisting of everything but the first p symbols of s. The Jaccard 102 CHAPTER 3. FINDING SIMILAR ITEMS similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not have to compare s with t, we must be certain that J > (Ls − p)/Ls. That is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as possible, so we do not index string s in more buckets than we need to. Thus, we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that gets indexed. Example 3.26 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 = ⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any string t that does not have the first symbol of s in a position such that t is indexed by that symbol will have Jaccard similarity with s that is less than 0.9. Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not begin with b. There are two cases to consider. 1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is abcdefghij. But if that is the case, t will be indexed under both a and b. The reason is that Lt = 10, so t will be indexed under the symbols of its prefix of length ⌊0.1 × 10⌋ + 1 = 2. 2. If t begins with c or a later letter, then the maximum value of SIM(s, t) occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9. In general, with J = 0.9, strings of length up to 9 are indexed by their first symbol, strings of lengths 10–19 are indexed under their first two symbols, strings of length 20–29 are indexed under their first three symbols, and so on. ✷ We can use the indexing scheme in two ways, depending on whether we are trying to solve the many-many problem or a many-one problem; recall the distinction was introduced in Section 3.8.4. For the many-one problem, we create the index for the entire database. To query for matches to a new set S, we convert that set to a string s, which we call the probe string. Determine the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For each symbol appearing in one of the prefix positions of s, we look in the index bucket for that symbol, and we compare s with all the strings appearing in that bucket. If we want to solve the many-many problem, start with an empty database of strings and indexes. For each set S, we treat S as a new set for the many-one problem. We convert S to a string s, which we treat as a probe string in the many-one problem. However, after we examine an index bucket, we also add s to that bucket, so s will be compared with later strings that could be matches. 3.9.5 Using Position Information Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9. Since both strings are of length 10, they are indexed under their first two 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 103 symbols. Thus, s is indexed under a and c, while t is indexed under b and c. Whichever is added last will find the other in the bucket for c, and they will be compared. However, since c is the second symbol of both, we know there will be two symbols, a and b in this case, that are in the union of the two sets but not in the intersection. Indeed, even though s and t are identical from c to the end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11, which is less than 0.9. If we build our index based not only on the symbol, but on the position of the symbol within the string, we could avoid comparing s and t above. That is, let our index have a bucket for each pair (x, i), containing the strings that have symbol x in position i of their prefix. Given a string s, and assuming J is the minimum desired Jaccard distance, we look at the prefix of s, that is, the positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is x, add s to the index bucket for (x, i). Now consider s as a probe string. With what buckets must it be compared? We shall visit the symbols of the prefix of s from the left, and we shall take advantage of the fact that we only need to find a possible matching string t if none of the previous buckets we have examined for matches held t. That is, we only need to find a candidate match once. Thus, if we find that the ith symbol of s is x, then we need look in the bucket (x, j) for certain small values of j. j s t Symbols definitely appearing in only one string i Figure 3.14: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that To compute the upper bound on j, suppose t is a string none of whose first j −1 symbols matched anything in s, but the ith symbol of s is the same as the jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical beyond their ith and jth symbols, respectively, as suggested by Fig. 3.14. If that is the case, the size of their intersection is Ls − i + 1, since that is the number of symbols of s that could possibly be in t. The size of their union is at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and there are also at least j −1 symbols of t that are not in s. The ratio of the sizes of the intersection and union must be at least J, so we must have: Ls − i + 1 Ls + j − 1 ≥ J 104 CHAPTER 3. FINDING SIMILAR ITEMS If we isolate j in this inequality, we have j ≤ Ls(1 − J) − i + 1 + J  /J. Example 3.27 : Consider the string s = acdefghijk with J = 0.9 discussed at the beginning of this section. Suppose s is now a probe string. We already established that we need to consider the first two positions; that is, i can be 1 or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11. Thus, j can be 1 or 2, but nothing higher. Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1), but in no other bucket. In comparison, using the buckets of Section 3.9.4, we would look into the buckets for a and c, which is equivalent to looking to all buckets (a, j) and (c, j) for any j. ✷ 3.9.6 Using Position and Length in Indexes When we considered the upper limit on j in the previous section, we assumed that what follows positions i and j were as in Fig. 3.14, where what followed these positions in strings s and t matched exactly. We do not want to build an index that involves every symbol in the strings, because that makes the total work excessive. However, we can add to our index a summary of what follows the positions being indexed. Doing so expands the number of buckets, but not beyond reasonable bounds, and yet enables us to eliminate many candidate matches without comparing entire strings. The idea is to use index buckets corresponding to a symbol, a position, and the suffix length, that is, the number of symbols following the position in question. Example 3.28 : The string s = acdefghijk, with J = 0.9, would be indexed in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol a, and its suffix is of length 9. The second position has symbol c and its suffix is of length 8. ✷ Figure 3.14 assumes that the suffixes for position i of s and position j of t have the same length. If not, then we can either get a smaller upper bound on the size of the intersection of s and t (if t is shorter) or a larger lower bound on the size of the union (if t is longer). Suppose s has prefix length p and t has prefix length q. Case 1: p ≥ q. Here, the maximum size of the intersection is Ls − i + 1 − (p − q) Since Ls = i + p, we can write the above expression for the intersection size as q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not take suffix length into account. Thus, we require q + 1 Ls + j − 1 ≥ 3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 105 whenever p ≥ q. Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as when suffix length was not considered. However, the minimum size of the union is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can replace Ls − p by i and get the formula i + j − 1 + q for the size of the union. If the Jaccard similarity is at least J, then Ls − i + 1 i + j − 1 + q ≥ J whenever p < q. Example 3.29 : Let us again consider the string s = acdefghijk, but to make the example show some details, let us choose J = 0.8 instead of 0.9. We know that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q the suffix length of t. First, consider the case p ≥ q. The additional constraint we have on q and j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for each i between 1 and 3, as follows. i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q: q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2, or j = 3. Note that for j = 4, 10/13 > 0.8. q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2. For j = 3, 9/12 > 0.8. q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality. q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every positive integer j. The same holds for every smaller value of q. i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j ≥ 0.8 does not depend on i, 6 we can use the analysis from the above case, but exclude the case q = 9. Thus, the only possible values of j and q when i = 2 are 1. q = 8; j = 1. 2. q = 8; j = 2. 3. q = 7; j = 1. i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The only option is q = 7 and j = 1. 6Note that i does influence the value of p, and through p, puts a limit on q. 106 CHAPTER 3. FINDING SIMILAR ITEMS Next, we must consider the case p < q. The additional constraint is 11 − i i + j + q − 1 ≥ 0.8 Again, consider each possible value of i. i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible values of q and j are 1. q = 10; j = 1. 2. q = 10; j = 2. 3. q = 11; j = 1. i = 2: Now, p = 10, so we require q ≥ 11 and 9/(q + j + 1) ≥ 0.8. there are no solutions, since j must be a positive integer. i = 3: As for i = 2, there are no solutions. q j = 1 j = 2 j = 3 7 x 8 x x i = 1 9 x x x 10 x x 11 x 7 x i = 2 8 x x 9 x i = 3 7 x Figure 3.15: The buckets that must be examined to find possible matches for the string s = acdefghijk with J = 0.8 are marked with an x When we accumulate the possible combinations of i, j, and q, we see that the set of index buckets in which we must look forms a pyramid. Figure 3.15 shows the buckets in which we must search. That is, we must look in those buckets (x, j, q) such that the ith symbol of the string s is x, j is the position associated with the bucket and q the suffix length. ✷ 3.9.7 Exercises for Section 3.9 Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the order of elements is taken to be the vowels, in alphabetic order, followed by the consonants in reverse alphabetic order. Represent the following sets as strings. a {q, w, e, r, t, y}. 3.10. SUMMARY OF CHAPTER 3 107 (b) {a, s, d, f, g, h, j, u, i}. Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in Section 3.9.3. If s is a string of length 20, with what strings is s compared when J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85 (b) J = 0.95 (c) J = 0.98? Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index its prefix as in Section 3.9.4. (a) How many positions are in the prefix if J = 0.85? (b) How many positions are in the prefix if J = 0.95? ! (c) For what range of values of J will s be indexed under its first four symbols, but no more? Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position pairs will s be compared with if we use the indexing approach of Section 3.9.5, and (a) J = 0.75 (b) J = 0.95? ! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of 100 elements. Assume J = 0.9. What is the probability that s and t will be compared if (a) s and t are both of length 9. (b) s and t are both of length 10. Exercise 3.9.6 : Suppose we use indexes based on both position and suffix length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9? 3.10 Summary of Chapter 3 ✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size of the intersection of the sets to the size of the union. This measure of similarity is suitable for many applications, including textual similarity of documents and similarity of buying habits of customers. ✦ Shingling: A k-shingle is any k characters that appear consecutively in a document. If we represent a document by its set of k-shingles, then the Jaccard similarity of the shingle sets measures the textual similarity of documents. Sometimes, it is useful to hash shingles to bit strings of shorter length, and use sets of hash values to represent documents. 108 CHAPTER 3. FINDING SIMILAR ITEMS ✦ Minhashing: A minhash function on sets is based on a permutation of the universal set. Given any such permutation, the minhash value for a set is that element of the set that appears first in the permuted order. ✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the sequence of minhash values obtained by applying each permutation on the list to that set. Given two sets, the expected fraction of the permutations that will yield the same minhash value is exactly the Jaccard similarity of the sets. ✦ Efficient Minhashing: Since it is not really possible to generate random permutations, it is normal to simulate a permutation by picking a random hash function and taking the minhash value for a set to be the least hash value of any of the set’s members. ✦ Locality-Sensitive Hashing for Signatures: This technique allows us to avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we can eliminate from consideration most of the pairs that do not meet our threshold of similarity. ✦ Distance Measures: A distance measure is a function on pairs of points in a space that satisfy certain axioms. The distance between two points is 0 if the points are the same, but greater than 0 if the points are different. The distance is symmetric; it does not matter in which order we consider the two points. A distance measure must satisfy the triangle inequality: the distance between two points is never more than the sum of the distances between those points and some third point. ✦ Euclidean Distance: The most common notion of distance is the Euclidean distance in an n-dimensional space. This distance, sometimes called the L2-norm, is the square root of the sum of the squares of the differences between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of the magnitudes of the differences between the points in each dimension. ✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure, called the Jaccard distance. ✦ Cosine Distance: The angle between vectors in a vector space is the cosine distance measure. We can compute the cosine of that angle by taking the dot product of the vectors and dividing by the lengths of the vectors. ✦ Edit Distance: This distance measure applies to a space of strings, and is the number of insertions and/or deletions needed to convert one string 3.10. SUMMARY OF CHAPTER 3 109 into the other. The edit distance can also be computed as the sum of the lengths of the strings minus twice the length of the longest common subsequence of the strings. ✦ Hamming Distance: This distance measure applies to a space of vectors. The Hamming distance between two vectors is the number of positions in which the vectors differ. ✦ Generalized Locality-Sensitive Hashing: We may start with any collection of functions, such as the minhash functions, that can render a decision as to whether or not a pair of items should be candidates for similarity checking. The only constraint on these functions is that they provide a lower bound on the probability of saying “yes” if the distance (according to some distance measure) is below a given limit, and an upper bound on the probability of saying “yes” if the distance is above another given limit. We can then increase the probability of saying “yes” for nearby items and at the same time decrease the probability of saying “yes” for distant items to as great an extent as we wish, by applying an AND construction and an OR construction. ✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of basis functions to start a generalized LSH for the cosine distance measure by identifying each function with a list of randomly chosen vectors. We apply a function to a given vector v by taking the dot product of v with each vector on the list. The result is a sketch consisting of the signs (+1 or −1) of the dot products. The fraction of positions in which the sketches of two vectors agree, multiplied by 180, is an estimate of the angle between the two vectors. ✦ LSH For Euclidean Distance: A set of basis functions to start LSH for Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals, and the function answers “yes” to a pair of points that fall into the same interval. ✦ High-Similarity Detection by String Comparison: An alternative approach to finding similar items, when the threshold of Jaccard similarity is close to 1, avoids using minhashing and LSH. Rather, the universal set is ordered, and sets are represented by strings, consisting their elements in order. The simplest way to avoid comparing all pairs of sets or their strings is to note that highly similar sets will have strings of approximately the same length. If we sort the strings, we can compare each string with only a small number of the immediately following strings. ✦ Character Indexes: If we represent sets by strings, and the similarity threshold is close to 1, we can index all strings by their first few characters. The prefix whose characters must be indexed is approximately the length 110 CHAPTER 3. FINDING SIMILAR ITEMS of the string times the maximum Jaccard distance (1 minus the minimum Jaccard similarity). ✦ Position Indexes: We can index strings not only on the characters in their prefixes, but on the position of that character within the prefix. We reduce the number of pairs of strings that must be compared, because if two strings share a character that is not in the first position in both strings, then we know that either there are some preceding characters that are in the union but not the intersection, or there is an earlier symbol that appears in both strings. ✦ Suffix Indexes: We can also index strings based not only on the characters in their prefixes and the positions of those characters, but on the length of the character’s suffix – the number of positions that follow it in the string. This structure further reduces the number of pairs that must be compared, because a common symbol with different suffix lengths implies additional characters that must be in the union but not in the intersection. 3.11 References for Chapter 3 The technique we called shingling is attributed to [10]. The use in the manner we discussed here is from [2]. Minhashing comes from [3]. The original works on locality-sensitive hashing were [9] and [7]. [1] is a useful summary of ideas in this field. [4] introduces the idea of using random-hyperplanes to summarize items in a way that respects the cosine distance. [8] suggests that random hyperplanes plus LSH can be more accurate at detecting similar documents than minhashing plus LSH. Techniques for summarizing points in a Euclidean space are covered in [6]. [11] presented the shingling technique based on stop words. The length and prefix-based indexing schemes for high-similarity matching comes from [5]. The technique involving suffix length is from [12]. 1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117– 122, 2008. 2. A.Z. Broder, “On the resemblance and containment of documents,” Proc. Compression and Complexity of Sequences, pp. 21–29, Positano Italy, 1997. 3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise independent permutations,” ACM Symposium on Theory of Computing, pp. 327–336, 1998. 4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002. 3.11. REFERENCES FOR CHAPTER 3 111 5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering, 2006. 6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004. 7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518– 529, 1999. 8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006. 9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998. 10. U. Manber, “Finding similar files in a large file system,” Proc. USENIX Conference, pp. 1–10, 1994. 11. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM SIGIR Conference, July, 2008, Singapore. 12. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.


主要利用的是 shingling-minhash-LSH 技术：

* shingling：将文本拆解为拆解为长度为 $K$ 的字符串集合
* minhash：文本签名，能够表示集合并反映其相似性的较短长度的整数向量
* LSH：局部敏感哈希的目的是生成测试候选对






### 三、MinHash 过程

#### 3.1 minhash

shingle 集合非常大，即使我们将其都 hash 到 4 个字节，一篇文档的 shingle 集合所需要的空间仍然大概是原文档所需空间的 4 倍。例如原始文档 $D$ 长度为 $ld$，如果不考虑重复 shingle 出现，则一个文档 D 的 shingles 集合大小为 $ld-k+1$，而每个 shingles 需要 4 个字节存储，$k$ 又远远小于 $ld$，所以需要大致 $4ld$ 空间。

所以我们需要想办法将上述大集合替换为规模小得多的”签名“。但这种签名是有要求的，即通过比较签名之间的相似度就可以估计实际 shingle 集合之间的 Jaccard 相似度。当然，通过签名无法得到原始 shingle 集合之间 Jaccard相似度的精确值，但是估计结果与真实结果相差不大，并且签名集合越大，估计的精度也越高。例如， 50000 字节文档的 shingle 可能会映射为 200000 字节的哈希结果，然后替换成 1000 字节大小的签名集合。基于最终签名集合得到的原始文档 Jaccard 相似度的估计值与真实值的差异也就在几个百分点之内。

首先将一系列集合表示成特征矩阵形式：

![image-20211016145825668](./images/image-20211016145825668.png)

例如这里有 4 个集合，$S_1 = \{a, d\}, S_2 = \{c\}, S_3 = \{b, d, e\},  S_4 = \{a, c, d\}$。

为了对特征矩阵每列所表示的集合进行最小哈希计算，首先选择行的一个排列转换。任意一列的最小哈希值是在排列转换后的行排列次序下第一个列值为 1 的行的行号。例如下图中我们把行的顺序按照 beadc 顺序排列后，得到

![image-20211016150309594](./images/image-20211016150309594.png)

$S_1$ 中第一次出现 1 的位置是 $a$ 行，则 $h(S_1)=a$，同理 $h(S_2) = c, h(S_3) = b,  h(S_4) = a$

#### 3.2 minhash 与 Jaccard 相似度

**两个集合经随机排列转换之后得到的两个最小哈希值相等的概率等于这两个集合的 Jaccard 相似度。**

解释一下，假设只考虑集合 $S_1$ 和 $S_2$ 所对应的列，那么它们所在的行可以按照所有可能的结果分成如下 3 类：

1. 属于 $X$ 类的行，意思是两列的值均为 1;
2. 属于 $Y$ 类的行，意思是其中一列的值为 0, 另一列的值为 1;
3. 属于 $Z$ 类的行，意思两列的值都为 0 。

由于特征矩阵十分稀疏，因此大部分行都属于 $Z$ 类。但我们可以忽略都是 0 的行，实际由 $X$ 和 $Y$ 类行数目的比例决定了 $\text{SIM}(S_1, S_2)$ 及概率 $h(S_1)=h(S_2)$ 的大小。假定 $X$ 类行的数目为 $x$, $Y$ 类的行的数目为 $y$, 则$\text{SIM}(S_1,S_2) =x/(x+y)$。原因是 $S_1 \cap S_2$ 的大小为 $x$，而 $S_1 \cup S_2$ 的大小为 $x+y$。

接下来我们考虑 $h(S_1)=h(S_2)$ 的概率。设想所有行进行随机排列转换，然后我们从上到下进行扫描处理，在碰到 $Y$ 类行之前碰到 $X$ 行的概率是 $x/(x+y)$。但是如果从上往下扫描遇到的除 $Z$ 类行之外的第一行属于 $X$ 类，那么肯定有 $h(S_1)=h(S_2)$ 。另一方面，如果首先碰到的是 $Y$ 类行，而不是 $Z$ 类行，那么值为1 的那个集合的最小哈希值为当前行。但值为 0 的那个集合必将会进一步扫描下去。因此，如果首先碰到 $Y$ 类行，那么此时$h(S_1) \neq h(S_2)$。于是，我们可以得到最终结论，即 $h(S_1)=h(S_2)$ 的概率是 $x/(x+y)$，而这也是两个集合Jaccard 相似度的计算公式。

#### 3.3 minhash 签名

我们可以把上面介绍的过程随机置换行顺序 $n$ 次，$n$ 大概为几百次的规模，则我们会得到 $h_1,h_2,\cdots,h_n$，而对于集合 $S$，我们现在可以表示为 $[h_1(S),h_2(S),\cdots,h_n(S)]$，这样一来，数据规模就小很多了，原来的特征矩阵需要 len(k-shingles) 行，而现在只需要 $n$ 行。$n$ 越大，则结果与原始的 Jaccard 相似度越接近。

**然而这种做法听起来不错，却根本无法实现**，即使对上百万甚至数十亿的行选择一个随机排列转换也极其消耗时间，而对行进行必要的排序则需要花费更多的时间。幸运的是，我们可以通过一个随机哈希函数来模拟随机排列转换的效果，该函数将行号映射到与行数目大致相等数量的桶中。通常而言，一个将整数 $0,1, …, k-1$ 映射到桶号 $0, 1, …, k-1$ 的哈希函数会将某些整数映射到同一个桶中，而有些桶却没有被任何整数所映射到。然而，只要 $k$ 很大且哈希结果冲突不太频繁的话，差异就不是很重要。于是，我们就可以继续假设哈希函数 "h" 将原来的第 $r$ 行放在排列转换后次序中的第 $h(r)$ 个位置上。

因此，我们就可以不对行选择 $n$ 个随机排列转换，取而代之的是随机选择 $n$ 个哈希函数 $h_1, h_2, ···,h_n$ 作用于行。在上述处理基础上，就可以根据每行在哈希之后的位置来构建签名矩阵。令 $\text{SIG}(i, c)$  为签名矩阵中第 $i$ 个哈希函数在第 $c$ 列上的元素。一开始，对于所有的 $i$ 和 $C$, 将 $\text{SIG}(i, c)$ 都初始化为 $\infty$。然后，对行 $r$ 进行如下处理：

1. 计算 $h_1(r), h_i(r), …, h_n(r)$;
2. 对每列 $c$ 进行如下操作：
   1. 如果 $c$ 在第 $r$ 行为 0, 则什么都不做；
   2. 否则，如果 $c$ 在第 $r$ 行为 1 , 那么对于每个 $i=1, 2, …, n$, 将 $\text{SIG}(i, c)$ 置为原来的 $\text{SIG}(i, c)$ 和 $h_(r)$ 之中的较小值。

例如下图中，我们把上面的特征矩阵加上了行号，以及设定两个哈希函数分别是 $h_1(x) = (x+1) \% 5, h_2(x) = (3x+1) \% 5$，这里的 $x$​ 指的是行号。两个哈希函数产生的结果在最后两列。注意到这里的两个简单哈希函数对应真正的行排列转换，当然这里只有当行数目为质数（这里为 5) 时才有会有真正的排列转换。通常来说，哈希结果都会存在冲突，即至少有两行得到的哈希值相等。

![image-20211016155236323](./images/image-20211016155236323.png)

接下来模拟计算签名矩阵的算法。一开始，签名矩阵全都由 $\infty$ 构成：

![image-20211016121822025](./images/image-20211016121822025.png)

首先，考虑第 0 行。此时，不论是 $h_1(0)$ 还是 $h_2(0)$ 的结果值都是 1 。而只有集合 $S_1$ 和 $S_4$ 在第 0 行为 1, 因此签名矩阵中只有这两列的值需要修改。因为 $1 < \infty$， 所以实际上是对 $S_1$ 和 $S_4$ 的对应列值进行修改，所以当前签名矩阵的估计结果为：

接下来，我们看第 2 行。对于该行，只有 $S_3$ 的值为 1 , 此时其哈希值 $h_1(1) = 2,h_2(1) =4$ ，更新后的签名矩阵为：

![image-20211016121933550](./images/image-20211016121933550.png)

再接下来看第 2 行中只有 $S_2$ 和 $S_4$ 对应的列为 1，且其哈希值 $h_1(2)=3, h_2(2)=2$ 。$S_4$ 对应的签名本应修改，但是签名矩阵中对应列值为 [1, 1] 小于相应的哈希值 [3, 2]，因此其签名最后不会修改（其实仔细想想，在此处为什么不修改，是因为这里的 hash 值充当置换后的新行，如果行数较小的行中已经有 1，则置换后行数较大的行中再出现 1 其实也没啥用了）。而 $S_2$ 对应的列中仍然是初始值 $\infty$。我们将它替换为 [3, 2], 得到：

![image-20211016122008417](./images/image-20211016122008417.png)

同理可以继续进行下去，最后得到的签名矩阵为：

![image-20211016122056725](./images/image-20211016122056725.png)

其实如果你真的按照两次哈希后的顺序，真的去排列后重新找第一次出现 1 的行，得到的也是上面的结果。

基于上述签名矩阵，我们可以估计原始集合之间的 Jaccard 相似度。注意到在签名矩阵中 $S_1$ 和 $S_4$ 对应的列向量完全相同，因此我们可以猜测 $\text{SIM}(S_1, S_4)=1.0$。如果回到原始图中, 我们会发现 $S_1$ 和 $S_4$ 的真实 Jaccard 相似度为 2/3 。需要记住的是，签名矩阵中行之间的一致程度只是真实 Jaccard 相似度的一个估计值，因为本例规模太小，所以并不足以说明在大规模数据情况下估计值和真实值相近的规律。

#### 3.4 加速

上面的计算方法虽然从计算结果上看，可以把不可能实现的置换操作以哈希方式给实现了，可以将矩阵从行数非常巨大的特征矩阵变成只有几百行的签名矩阵。然而在这个过程中计算量非常大，因为它需要考虑整个特征矩阵的所有行。

我们重新回到最原始的特征矩阵上

![image-20211016145825668](./images/image-20211016145825668.png)

假设我们目前看到的已经是置换后的，那我们其实在找第一行为 1 的时候就停下了，例如对于 $S_1$ 而言，我们不会再看 bcde 行，对于 $S_2$ 而言，也不会再看 de 行，所以我们的注意力并没有看完所有的特征矩阵的每一行。所以我们由此可以想象出，对于一个非常巨大的特征矩阵，我们可以只看前 $m$ 行（假设一共 $k$ 行），这样一来就可以大大的减少计算量，例如上图，实际上只看前 3 行完全就可以得到正确答案。

但是这样也会引入新的问题，例如上图中，如果我只看前两行，那么 $S_2$ 这一列将看不到 1，怎么办？用 $\infty$ 表示。这样一来，含 $\infty$ 符号，在计算 Jaccard 的时候，就有 3 种情况：

1. 两列都不含 $\infty$，那就正常比较
2. 一列有，另一列没有，那么前者真实出现第一个 1 的行肯定不在前 $m$ 行，即两列 minhash 不等
3. 两列都有 $\infty$，则既不作为等值处理，也不作为不等值处理

第 3 种情况不是很常见，这种影响将在一定程度上降低我们对 Jaccard 距离估计的准确性，但不会太大。由于我们现在能够比检查所有行更快地计算所有列的 minhash 值，因此我们可以节省时间来应用更多的 minhash 函数。我们得到了比原来更好的准确度，而且比以前更快。

#### 3.5 在 Hash 函数基础上加速

实际上是将上面 3.3 中提到的 hash 方法（并不真的进行置换）与 3.4 中提到的只关注前 $m$ 行方法进行结合。虽然这种方式可能产生全是 0 的列，以至于签名矩阵中出现 $\infty$，但只要 $m$ 足够大，这种情况则很少发生，我们依然可以根据签名矩阵很好地评估原始 Jaccard 值。遇到的话，忽略掉该行。

假设 $T$ 是特征矩阵的前 $m$ 行包含的所有元素集合，那么 $S_1$ 在前 $m$ 行的元素集合为 $S_1 \cap T$，同理 $S_2$ 在前 $m$ 行的集合为 $S_2 \cap T$，现在再算 Jaccard，则，计算公式需要修改为：
$$
\text{Jaccard}(S_1, S_2)=\frac{|S_1 \cap S_2 \cap T|}{|(S_1 \cup S_2) \cap T|}
$$
然而，会有一些随机变化，因为根据 $T$，我们可以在矩阵的前 $m$ 行中可能会找到多于或少于 $X$ 类行（两列中均为 1的行）和/或 $Y$ 类行（一列为 1，另一列为 0 的行）。

为了缓解这种变化，我们对每个 minhashing 不使用相同的集合 $T$。相反，我们将特征矩阵划分为 $k/m$ 组。然后，对于每个 hash 函数，我们通过第一个 $m$ 行来计算一个 minhash 值，通过第二个 $m$ 行计算不同的 minhash 值，依此类推。因此，我们从单个 hash 函数和对 $m$ 的所有行的单次传递中获得 $k/m$ 个 minhash值。事实上，如果 $k/m$ 足够大，我们可以单个 hash 函数通过应用于特征矩阵的每个子集上m$ 的每个行获得整个特征矩阵的签名矩阵。

例如下图中，$k=8,m=4$，$\text{SIM}(S_1, S_2) = 1/2, \text{SIM}(S_1, S_3) = 1/5, \text{SIM}(S_2, S_3) = 1/2$（忽略都是 0 的行）。

![image-20211016173238876](./images/image-20211016173238876.png)

如果只看前 4 行，不管用什么 hash 函数，$S_1$ 和 $S_2$ 的 minhash 值将始终不等，因为 $S_1 \cap S_2 \cap T=\emptyset$，然而只看后四行，则相似度为 $2/3$。所以平均起来是 $(0+2/3)/2=1/3$，与真实的 $1/2$ 有误差，不是太大。在真实场景下，$m$ 远大于 4 行，这种误差将趋于 0。

同理比较 $S_1$ 与 $S_3$ 之间，是 $(0+1/3)/2=1/6$，与真实的 $1/5$ 比较接近，而 $S_2$ 与 $S_3$ 之间完全一致。

### 3.4 LSH

#### 3.4.1 面向最小哈希签名的 LSH

虽然 minhash 可以大幅度压缩特征矩阵，但矩阵的列数并没有变化，那么依然解决不了如何在大量文件之间高效的找到最大相似度的文档。下面介绍局部敏感哈希（LSH）。

LSH 的一个一般性做法就是对目标项进行多次哈希处理，使得相似项会比不相似项更可能哈希到同一桶中。然后将至少有一次哈希到同一桶中的文档对看成是候选对，我们只会检查这些候选对之间的相似度。我们希望大部分不相似的文档对将永远不会哈希到相同的桶中，这样就永远不需要检查它们的相似度。那些哈希到同一个桶中的非相似文档对称为伪正例(false positive) , 我们希望它们在所有对中占的比例越低越好。同时，我们也希望大部分真正相似的文档对会至少被一个哈希函数映射到同一桶中。那些没有映射到相同桶中的真正相似的文档对称为伪反例(false negative) , 我们希望它们在所有真正相似文档对中的比例也很小。

如果拥有目标项的最小哈希签名矩阵，那么一个有效的哈希处理方法是将签名矩阵划分成 $b$ 个行条(band), 每个行条由 $r$ 行组成。对每个行条，存在一个哈希函数能够将行条中的每 $r$ 个整数组成的列向釐（行条中的每一列）映射到某个大数目范围的桶中。可以对所有行条使用相同的哈希函数，但是对每个行条我们都使用一个独立的桶数组，因此即使是不同行条中的相同向量列，它们也不会被哈希到同一桶中。

如下图所示：

![image-20211016175800408](./images/image-20211016175800408.png)

包含 12 行的签名矩阵的一部分，有 4 个行条，每个行条 3 行，图中显式可见的行条1 中第 2 列和第 4 列均包含列向量 [0, 2, 1], 因此它们肯定会哈希到行条l 下的相同桶中。因此，不管这两列在其他 3 个行条下的结果如何，它们都是一个相似候选对。图中显式给出的其他列也有可能会哈希到行条 1 下的同一桶中。但是，由于此时两个列向量 [1, 3, 0J 和 [0, 2, 1] 不同，加上哈希的桶数目也不少，因此偶然冲突的预期概率会非常低。通常我们都假设当且仅当两个向量相等时，它们才会哈希到同一桶中。

在行条1 中不相等的两个列仍然还有另外3次机会成为候选对，只要它们在剩余的 3 个行条中有一次相等即可。然而，我们观察到，如果签名矩阵的两列越相似，那么在多个行条中的向量相等的可能性也越大。因此，直观上看，行条化策略能够使得相似列会比不相似列更有可能成为候选对。

#### 3.4.2 行条化策略分析

假定使用 $b$ 个行条，每个行条由 $r$ 行组成，并假定某对具体文档之间的 Jaccard 相似度为 $s$。则文档的最小哈希签名矩阵中某个具体行中的两个签名相等的概率等于 $s$ 。接下来我们可以计算这些文档（或其签名）作为候选对的概率，具体计算过程如下：

1. 在某个具体行条中所有行的两个签名相等的概率是 $s^r$
2. 在某个具体行条中至少有一对签名不相等的概率是 $1-s^r$
3. 在任何行条中的任意一行的签名对都不相等的概率为 $(1-s^r)^b$
4. 签名至少在一个行条中全部相等的概率，也即成为候选对的概率为 $1-(1-s^r)^b$

虽然有可能并不特别明显，但是不论常数 $b$ 和 $r$ 的取值如何，上述形式的概率函数图像大致为下图中的 S-曲线。

![image-20211016180658409](./images/image-20211016180658409.png)



曲线中候选概率 $1/2$ 处对应的相似度就是所谓的阈值。它是 $b$ 和 $r$ 的函数。阈值对应的大概是上升最陡峭的地方，对于较大的 $b$ 和 $r$, 相似度在阈值之上的对很可能成为候选对，而在阈值之下的对则不太可能成为候选对，这正是我们想要的结果。阈值的一个近似估计是 $(1/b)^{l/r}$。例如，如果 $b=16,r=4$, 那么由于 16  的 4 次方根为 2, 阈值的近似值为 1/2 。

考虑 $b=20, r=5$ 的情况，也就是说假定签名的个数为 100, 分成 20 个行条，每个行条包含 5 行。下表给出了函数 $1-(1-s^5)^{20}$ 的部分值。注意到的是，这里的阈值，也就是曲线中部上升处的 $s$ 值，仅仅比 0.5 稍大一点。另外也注意到，该曲线并非从 0 到 1 在阈值处跳跃的最理想步进函数，但是曲线中部的斜率十分显著。例如， $s$ 从 0.4 变到 0.6, 增加的函数值大于 0.6,  因此中间部分的斜率大于 3 。

![image-20211016181251776](./images/image-20211016181251776.png)



又例如， $s=0.8$ 时， $1-(0.8)^5$ 大约为 0.672。如果再求 20 次方得到大约 0.000 35, 用 1 减去该值以后得 0.999 65 。也就是说，如果认为两篇文档的相似度为 80%, 那么在任意行条中， 5 行中签名对全部相等的可能性，即它们会成为候选对的概率只有约 33%。然而，这里有 20 个行条，因此有 20 次机会成为一个候选对。3000 个对中，大致仅有 1 个相似度为 80% 的对不会成为候选对，即成为伪反例。

#### 3.4.3 上述技术综合

本节将给出一个完整的相似项发现方法：首先找出可能的候选对相似文档集合，然后基于该集合发现真正的相似文档。必须强调的是，这种方法可能会产生伪反例，即某些相似文档对由于没有进入候选对所以最终没有被识别出来。同样，该方法也可能会产生伪正例，即在评估了某些候选对后，发现其相似度不足。

1. 选择某个 $k$, 并对每篇文档构建其 k-shingle 集合。将这些 k-shingle 映射成更短的桶编号（后一步可选）。

2. 将文档-shingle 对按照 shingle 排序，构建特征矩阵。
3. 选择最小哈希签名的长度 $n$ 。计算所有文档的最小哈希签名。
4. 选择阈值 $t$ 来定义应该达到的相似程度使之被看做是预期的”相似对“。选择行条数 $b$ 和每个行条中的行数 $r$, 使得 $br=n$, 而阈值 $t$ 近似等于 $(1/b)^{1/r}$。如果避免伪反例的产生很重要，那么选择合适的 $b$ 和 $r$ 以产生小于 $t$ 的阈值。而如果速度相当重要并且希望限制伪正例的数目，那么选择合适的 $b$和 $r$ 来获得更高的阈值。
5. 应用 LSH 技术来构建候选对。
6. 检查每个候选对的签名，确定它们一致性的比例是否大于 $t$。
7.  (该步可选）如果签名足够相似，则直接检查文档本身看它们是否真正相似。不相似的文档有时碰巧会具有相似的签名。

