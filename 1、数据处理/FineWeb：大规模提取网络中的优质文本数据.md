
[# 🍷 FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)

* Authors：Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Colin Raffel, Leandro Werra, Thomas Wolf
* Published：May 31, 2024
* Reading time: 45 min

LLM 的性能在很大程度上取决于其预训练数据集的质量和规模。然而，像 Llama-3 和 Mixtral 这样的最先进开源 LLM 的预训练数据集并未公开，而且关于它们是如何创建的知之甚少。

最近，我们发布了 [🍷FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)，这是一个用于 LLM 预训练的新的大规模（*15T tokens，44TB*）数据集。FineWeb 源自 96 个 CommonCrawl 快照，并且 *比其他开放预训练数据集能产生性能更好的 LLM*。为了在机器学习领域提供更清晰的认知，并推进关于如何训练高质量 LLM 的开放性理解，我们仔细记录并分析了 FineWeb 中使用的所有设计选择，包括对去重和过滤策略的深入探究。本长篇报告深入探讨了如何为 LLM 预训练创建一个大规模且高质量的网络规模数据集。

在本报告中，我们还介绍了 [**📚 FineWeb-Edu**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)，它是 FineWeb 的一个子集，是利用可扩展的自动化高质量注释构建而成，具有教育价值，并且在 MMLU、ARC 和 OpenBookQA 等多个教育基准测试中的表现优于所有可公开获取的网络数据集。[**📚 FineWeb-Edu**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) 有两种规模/过滤级别：*1.3T（教育内容极高）和 5.4T（教育内容高）tokens*。

## 一、网络数据

### 1.1 寻找原始数据

关于用于训练大语言模型的网络数据集，人们常问的一个常见问题是：“他们究竟从哪里获取这么多数据？”。通常有两种选择：

- 要么你自己抓取数据，就像 OpenAI 或 Anthropic（以及其他一些公司）所做的那样（见[此处](https://platform.openai.com/docs/bots)和[此处](https://darkvisitors.com/agents/claudebot)）；
- 要么使用公共的网络爬取网页库，比如由非营利组织 [CommonCrawl](https://commoncrawl.org/) 维护的那个。

为了构建 🍷 FineWeb，我们遵循了许多 LLM 训练团队过去所采用的方法，以 CommonCrawl 作为起点。自 2007 年以来，Common Crawl 非营利组织一直在对网络进行爬取，并且通常每隔 1 到 2 个月就会发布一次新的爬取结果，其中包含通过自动网络爬取获得的 200 到 400T 的文本内容。

### 1.2 大规模处理

鉴于所涉及的数据量巨大，我们必须克服的主要挑战之一是拥有一个模块化、可扩展的代码库，以便我们能够快速处理我们的处理决策并轻松尝试新想法，同时适当地并行化我们的工作负载，并提供对数据的清晰洞察。

为此，我们开发了 [`datatrove`](https://github.com/huggingface/datatrove)，这是一个开源数据处理库，使我们能够将过滤和去重设置无缝扩展到数千个 CPU 核心。创建 🍷 FineWeb 所涉及的所有数据处理步骤都使用了这个库。

### 1.3 什么是优质数据？

这可能是创建数据集时需要牢记的主要问题。在大多数情况下，特别是在大型语言模型预训练的背景下，“高质量”并不是一个定义非常明确的术语，甚至不是仅通过直接的人类观察就能始终清晰感知到的文档属性。

在给定被认为是“干净”的语料库（通常是维基百科）上训练模型，并使用它来检查我们试图 *整理的数据集的困惑度*，这种方法仍然很常见。不幸的是，这并不总是与我们在一系列感兴趣的下游任务上性能的提升相关，因此，另一种常用的方法是在我们数据集的一个*代表性子集上训练小型模型*（1-2B），并在一组评估任务上对它们进行评估。之所以使用小型模型，是因为训练成本和时间与模型大小成正比。在第二种方法中，选择一组多样且有代表性的数据集-评估任务非常重要，并且尽量不要对任何一个单独的基准测试过度拟合，因为这可能会损害预训练后获得的 LLM 的通用性。

比较不同数据集的另一种方法是在每个数据集上训练一个模型，然后让人类对模型的生成结果进行评分和比较（就像在 [LMSYS 聊天机器人竞技场](https://lmarena.ai/)中那样）。可以说，这种方法在代表真实模型使用情况方面能提供最可靠的结果，但遗憾的是，通过这种方式获取消融实验结果既昂贵又耗时。而且，这通常还要求模型经过指令微调阶段以获得对话能力，因为预训练模型并非直接设计用于遵循指令，因此对提示细节更为敏感。

在这项工作中，我们采用了训练小型模型并在一组“早期信号”基准任务上对其进行评估的方法。我们认为，在牢记上述关于在评估基准上过拟合的注意事项的前提下，这是对用于训练这些模型的数据质量的一个合理代理指标。

### 1.4 消融实验与评估设置

为了比较某一特定处理步骤的影响，我们在数据集的两个版本上训练了两个模型，一个版本经过额外步骤处理（即我们希望评估的步骤），另一个版本则去除了该步骤（进行了删减/移除）。除了数据之外，这两个模型在其他方面完全相同：参数数量相同、架构超参数相同，并且都在每个版本的数据中随机抽取相同数量的 tokens 上进行单轮训练 —— *因此唯一的区别就在于训练数据*。然后，我们在相同的任务集上对每个模型进行评估，并比较平均分数。

我们的消融模型是使用 [`nanotron`](https://github.com/huggingface/nanotron) 训练的。我们的“消融模型”有 1.82B 个参数（包括嵌入），采用了 Llama 架构，序列长度为 2048，全局批量大小约为 2M tokens，并使用了 GPT2 分词器。对于大多数消融实验，我们在约 28B tokens 上进行了训练（大致是该模型规模的 Chinchilla 最优训练规模）。为了确认每一步过滤后的相对性能提升，我们按照下文进一步说明，在 350B tokens 上进行了更长时间的训练运行。

我们使用 [`lighteval`](https://github.com/huggingface/lighteval/) 对模型进行了评估。我们通过挑选那些在较小规模下（仅在“几十亿” tokens 上训练的“小”模型）能提供良好信号的基准测试，精心选定了一组用于消融研究的基准测试集。通常，我们会依据以下标准，在 `lighteval` 提供的所有基准测试中挑选这些基准测试。

- 在对同一数据集的不同采样进行训练时，方差较小：我们希望在对数据子集进行的训练能够代表整个数据集，并且在可能的限度内，得到的分数对确切数据点选择的敏感度要低于对我们过滤器的效果的敏感度。
- 训练过程中性能单调（或接近单调）递增：理想情况下，随着看到的标记数量增加，在高信号基准测试中的性能不应下降（这表明在小规模下结果不可靠）。
- 对于此任务，性能至少比随机基线高出几个标准差：鉴于我们规模较小的消融模型和训练，我们通常不会在任何基准测试中达到极高的分数，但我们希望确保得到的分数高于随机噪声。

经过考虑，我们选择了以下基准测试列表：

- CommonSense QA
- HellaSwag
- OpenBook QA
- PIQA
- SIQA
- WinoGrande
- ARC
- MMLU

为确保我们的检查点评估在有限的时间内完成，我们将较长的基准测试样本数量限制在 1000 个（在单个 8 GPU 节点上的墙钟评估时间少于 5 分钟——与训练并行进行）。

你可以在[这里](https://huggingface.co/datasets/HuggingFaceFW/fineweb/blob/main/lighteval_tasks.py)找到我们所使用的全部任务和提示的列表。

## 二、🍷 FineWeb 配方

在接下来的小节中，我们将解释制作 FineWeb 数据集所采取的每个步骤。

![|600](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/assets/images/fineweb-recipe.png)

你可以在[此处](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py)找到一个完全可复现的  `datatrove` 配置。

### 2.1 起点：文本提取

> [!tip]
> 总结：使用 trafilatura 库从 WARC 中自己提取数据，效果要好，但非常耗资源，而 WET 是现成可用的

CommonCrawl 数据主要有两种格式：WARC 和 WET。**WARC**（网络档案格式）文件包含抓取的原始数据，包括完整的页面 HTML 和请求元数据。**WET**（WARC 封装文本）文件提供了这些网站的纯文本版本。

大量数据集以 WET 文件作为起点。根据我们的经验，Common Crawl 用于创建这些 WET 文件的默认文本提取方式对于大型语言模型（LLM）预训练的目标而言并非最优（我们尤其怀疑它保留了过多样板内容和导航菜单），而且有多种开源库能够提供更好的文本提取方法。*我们使用 trafilatura 库从 WARC 文件中提取文本内容*，通过直观检查结果发现，与其他库相比，该库能提供高质量的提取效果。

为了验证这一决定，我们直接使用 WET 文件处理了 2019-18 数据转储，并使用 trafilatura 从 WARC文件中提取的文本进行处理。我们对每个数据集应用了相同的处理流程（我们的基础过滤+MinHash，详见下文），并训练了两个模型。虽然 WET 数据的结果数据集大约大 25%（约 254B tokens），但其质量比使用 trafilatura 从 WARC 文件中提取文本的数据集（约 200B tokens）差得多。对一些样本的视觉检查证实，WET 文件上的许多额外标记是不必要的页面样板内容。

然而，需要注意的是，文本提取是我们处理过程中成本最高的步骤之一，因此我们认为，*对于预算较低的团队来说，使用现成的 WET 数据可能是一个合理的权衡*。

### 2.2 基础过滤

过滤包括移除部分数据（无论是单词、行，甚至是完整文档），这些数据会降低模型性能，因此在基于评估的数据集构建过程中被认为是“质量较低”的。

作为我们过滤的基础，我们使用了 RefinedWeb 的部分设置。具体来说，我们：

- 应用了基于[黑名单](https://dsi.ut-capitole.fr/blacklists/)的 URL 过滤，以移除成人内容
- 应用了 [fastText 语言分类器](https://fasttext.cc/docs/en/language-identification.html)，仅保留得分 ≥0.65 的英语文本
- 应用了 MassiveText 的质量和重复性过滤器（使用默认阈值）  

在对提取的每个文本数据转储（目前共有 96 个转储）应用这些过滤后，我们获得了大约 *36T* 个数据标记。

### 2.3 数据去重

#### 2.3.1 为什么要去重？

去重与模型性能的提升以及预训练数据记忆量的减少相关联，这可能有助于更好地泛化。此外，通过去重获得的性能提升可以等同于训练效率的提高：通过去除重复内容，模型可以在更少的训练迭代次数下达到相同的性能水平——或者换句话说，对于给定数量的训练标记，模型将接触到更多样化的数据。

识别甚至定义重复数据有不同的方法。常见的方法 *依赖于哈希技术* 来加速这一过程，或者构建高效的数据结构来对数据进行索引（如后缀数组）。方法也可以是“模糊的”，即通过使用某种相似性度量来标记文档为重复项，或者是“精确的”，即检查两个文档（或行、段落或其他使用的粒度级别）之间的精确匹配。

#### 2.3.2 我们的去重参数

继 RefinedWeb 之后，我们决定*采用 MinHash 这种基于模糊哈希的去重技术*。该技术能高效地扩展到多个 CPU 节点，并且允许我们通过控制 *桶的数量和大小* 来调整相似度阈值，同时通过控制 $n$-gram 的大小来确定所考虑的子序列的长度。我们选择收集每个文档的 5-grams（五元组），并使用总共 112 个哈希函数计算最小哈希值，这些哈希函数被分成 14 个桶，每个桶包含 8 个哈希函数——目标是找出至少有 75% 相似度的文档。在任何桶中具有相同 8 个最小哈希值的文档被视为彼此的重复文档。

这意味着对于相似度（$s$）分别为 0.7、0.75、0.8 和 0.85 的两份文档，它们被识别为重复文档的概率分别为 56%、77%、92% 和 98.8%（$1 - (1 - s^8)^{14}$）。请参见下图，对比我们采用 112 个哈希值的设置与 RefinedWeb 采用 9000 个哈希值（分为 450 个包含 20 个哈希值的桶，这需要大量更多的计算资源，因为每个单独的哈希值都必须经过计算、存储，然后与其他文档的哈希值进行比较）的匹配概率。

[交互图]

虽然 RefinedWeb 中大量哈希函数的使用能够实现更陡峭、更清晰的截断（接近阈值的真实相似文档更有可能被正确识别），但我们认为由此节省的计算资源和存储空间是值得的权衡。

还应指出的是，我们的重复过滤器已经处理了文档内的重复内容，它会移除包含许多重复行和段落的文档。

#### 2.3.3 更多的去重总是更好，对吗？

最初，我们是在认为 *去重越多越好* 的假设下进行操作的，因此我们的第一种方法是将整个数据集（所有 90 多个转储文件）作为一个大数据集，使用 MinHash 一起对其进行去重。

我们以迭代的方式进行操作：从最近的转储（当时是 2023 年第 50 次）开始，按时间顺序依次处理，直到处理到最早的抓取数据。我们对每次转储的数据不仅在其内部进行去重，还移除与之前已处理转储中的任何文档相匹配的文档。

例如，对于第二近期的转储（当时为 2023 年第 40 次），我们除了在其内部进行去重外，还针对最近一次的转储进行了去重。因此，转储越旧，它所针对的去重转储数量就越多，我们从其中移除的数据也就越多（实际上，在最旧的转储中，去重步骤移除了超过 90% 的经过初步筛选的数据）。

以这种方式对数据集去重后得到了 4T 个标记的数据，但令我们颇为惊讶的是，在随机抽取的 350B 个标记的子集上进行训练时，我们的消融模型相较于在未去重数据上训练的模型几乎没有提升，在我们各项任务的汇总评估中得分远低于其前身 RefinedWeb（见下图）。

[交互图]

这挑战了我们原本的假设，即更多的去重必然会导致更高的基准测试分数，因此我们决定仔细研究其中一个最早的转储文件，即 2013-48 转储文件：

- 在去重之前，该转储文件约有 490B 个标记
- 经过我们的迭代 MinHash 算法后，约剩下 31B 个标记（94% 的数据已被移除）

作为一项实验，我们尝试对从 2013-48 以下数据中抽取的 28B 个标记进行两个模型的训练：

- 完全去重后剩余的大约 31B 个标记（*最初保留的数据*）
- 通过对在这次迭代去重过程中从该数据集中移除的大约 460B 个标记进行单独去重（不考虑其他数据转储），获得的 171B 个标记（*最初移除的数据*）

这些结果表明，对于这个单独分析的较旧数据转储，被保留下来的数据（占原始数据的 10%）实际上比我们移除的 90% 的数据质量*更差*。通过目视检查也证实了这一点：*原本保留的数据* 中包含的广告、关键词列表以及格式普遍较差的文本远多于*原本被移除的数据*。

#### 2.3.4 退一步来看：单个转储去重

我们决定尝试一种替代方法：我们使用 MinHash 分别对每个数据转储进行去重（独立于其他数据转储）。这产生了 20T 个数据标记。

在对这个数据集的随机样本进行训练时，我们发现其性能现在与 RefinedWeb 相匹配（见下方曲线）。

[交互图]

我们假设，去重带来的主要改进在于移除了每个数据转储中都存在的非常大的聚类（在 RefinedWeb 论文中可以找到一些此类聚类的示例，每个聚类包含 *数十万* 份文档），而对重复数量较少（少于约 100个，即数据转储的数量）的聚类进一步去重实际上会损害性能：在任何其他数据转储中都未找到重复匹配的数据，其质量可能更差或更偏离分布（正如 2013-48 数据的结果所证明的那样）。

当对少量转储数据进行去重时，你可能会看到一些性能提升，但在整个数据集（所有转储数据）的规模上，这种对低质量数据进行上采样的副作用似乎影响更大。

一个需要考虑的可能性是，随着过滤质量的提高，这种效应可能不会那么普遍，因为过滤可能能够去除一些质量较低的数据。我们还在单独去重后的数据转储基础上尝试了应用不同的、通常更“轻量级”的去重方法。关于这些方法的更多内容，请参阅下文。

#### 2.3.5 关于衡量重复数据删除效果的注记

鉴于去重工作的特性，其效果在数据集的较小切片（例如我们用于过滤消融实验的 28B 个标记）中并不总是非常明显。此外，必须考虑到在跨所有 CommonCrawl 数据转储进行去重时，存在一些特定效应，因为某些 URL /页面会在一次转储到下一次转储之间被重新抓取。

为了直观呈现调整训练语料数量对衡量去重效果的影响，我们考虑了以下（就观察到的重复程度而言非常极端且不现实的）理论情景：

- 有 100 个 CommonCrawl 数据转储文件（大致准确）
- 每个转储文件都已进行了完美的单独去重处理（此转储文件中的每份文档都是独一无二的）
- 每个转储文件都是其他转储文件的完美副本（跨转储文件存在最大程度的重复，实际上是极端糟糕的情况）
- 每个转储文件包含 200B 个语料（总计 20T 个，即我们上述单独去重后的结果规模）
- 每个转储文件由长度为 1000 个语料的文档组成（每个转储文件有 2 亿份文档）

随后，我们模拟了从这包含 20T 个标记的整个数据集中均匀抽取文档，以获得包含 1B、10B、100B、350B 和 1T 个标记的子集。在下图中，你可以看到每个文档会被重复的频率。

对于 1B 的数据规模，几乎所有文档都是唯一的（重复文档数=1），尽管在整个数据集中每个文档都被重复了100次（每次转储一次）。在 100B 的规模（占整个数据集的 0.5%）时，我们开始看到一些变化，大量文档被重复了两次，少数甚至被重复了 4-8 次。在更大的 1T 规模（占整个数据集的 5%）时，大多数文档被重复了多达 8 次，有些甚至被重复了多达 16 次。

我们对去重后的数据在 350B 规模下进行了性能评估，在这一理论情景下，这些数据将由大量最多被重复 8 次的文档组成。此次模拟展示了在移除最大重复簇后，衡量去重对大语言模型训练影响的固有难点。

#### 2.3.6 其他（失败的）全局方法

在我们新发现的方法（独立去重每个数据转储）的基础上，我们尝试通过使用替代的全局（覆盖所有数据转储）去重方法，进一步对独立经过 MinHash 去重的 20T 个数据标记进行去重，以提高性能。我们探索了以下方法：

* URL 去重，在此过程中我们仅保留每个标准化（转换为小写）URL 对应的一份文档（移除了 71.5% 的词元，剩余 5.6T）—— *FineWeb URL 去重*  
* 行去重：
	* 移除所有重复行，仅保留每组重复行中随机选取的一行（丢弃了 77.8% 的词元，剩余 4.4T）—— *FineWeb 行去重*  
	* 与上述操作相同，但仅移除至少包含 10 个词元的重复行，并且在去重后若文档句子数量少于 3 句则将其丢弃（丢弃了 85% 的词元，剩余 2.9T）—— *FineWeb 带最小词元数限制的行去重*  
	* 移除所有重复的三行片段，每次查找重复项时将每个数字视为 0（移除了 80.9% 的词元，剩余 3.7T）—— *FineWeb 三行去重*

在每个数据集上训练的模型的性能始终比原始独立去重数据的性能差（尽管程度不同）：

[交互图]

### 2.4 附加质量过滤

到目前为止，我们已经达到了之前尝试复现和扩展的工作（RefinedWeb）相同的性能水平，使用的是我们基础的过滤方法和独立的 MinHash。然而，在我们的任务集合中，另一个经过严格过滤的数据集——C4 数据集，在我们评估套件的某些基准测试中仍然表现出了更强的性能。

因此，我们着手寻找新的过滤步骤，首先使其性能能够达到 C4 的水平，其次超越 C4。一个自然的起点是研究 C4 本身的处理过程。

#### 2.4.1 C4：一个经受住了时间考验的数据集

C4 数据集于 2019 年首次发布。它是从 2019-18 CommonCrawl 数据转储中获得的，处理过程包括去除非英语数据，在行和文档层面应用一些启发式过滤器，在行层面去重，并移除包含来自单词黑名单中的单词的文档。

尽管按照当前的标准来看，这个数据集存在年代久远以及规模有限（约 175B 个 gpt2 标记）的问题，但直到今天，它仍是典型的大型语言模型（LLM）训练中常见的子集，在相对较新的Llama1等模型中都有使用。这一成功归因于基于该数据集训练出的模型表现强劲，尤其是在“早期信号”组中信噪比最高的基准测试之一——Hellaswag 基准测试中表现出色。我们尝试将 C4 中使用的每种不同过滤器应用于独立去重的 FineWeb 2019-18 数据转储的基线版本：

[交互图]

* 应用“所有筛选条件”（剔除不以标点符号结尾的行、提及 JavaScript 和 Cookie 通知的内容，以及剔除长度超出阈值、包含“lorem ipsum”或花括号 { 的文档），使我们能够使模型性能与 C4 的 HellaSwag 性能相匹配（分别为“所有筛选条件”与“C4”的曲线对比）。
* 花括号过滤器和单词长度过滤器仅带来小幅提升，分别移除了 2.8% 和 4.3% 的标记。
- 单独使用时，终端标点过滤器能带来最大的单项提升，但会移除约 30% 的所有标记（！）
- “lorem_ipsum、javascript 和策略规则分别移除了不到 0.5% 的训练标记，因此我们没有对它们分别进行训练。”
- 除（破坏性很强的）terminal_punct 滤镜外，所有滤镜的表现都优于单独的 terminal_punct 滤镜，同时总体上删除的内容更少（约 7%）。

我们决定应用上述所有 C4 过滤器，但终端标点过滤器除外。我们通过更长时间的运行验证了这些结果，您将在下一节的图表中看到这些结果。

#### 2.4.2 一种用于开发启发式过滤器的统计方法

为了开发新的启发式过滤器并选择其阈值，我们设计了一个系统的流程：

1. 我们首先收集了数据集的大量高级统计数据（超过 *五十* 种不同指标），范围从常见的文档级指标（例如行数、平均行/词长度等）到跨文档重复性指标（受 MassiveText 启发），涵盖高质量和较低质量的网页数据集；
2. 我们选择了两个分布（在每个数据集上计算该指标的分布）之间的 Wasserstein 距离较大的指标；
3. 我们检查了这两个分布的直方图，并经验性地选择一个阈值，使得较低质量的数据集在该指标上更接近较高质量的数据集；
4. 我们通过在参考数据集上使用该过滤器（指标-阈值对）并进行小的消融实验来验证结果。

由于我们（新的）假设认为，在最早的网页抓取数据中，全局 MinHash 大幅增加了低质量数据的采样量，因此我们分别对 2013-48 和 2015-22 两次较旧的网页抓取数据（独立进行 MinHash 处理后的版本以及全局 MinHash 处理后的版本，后者质量较差）计算了相关指标。随后，我们通过观察每个版本的这些指标分布情况，在宏观层面上对统计数据进行了比较。

或许鉴于我们在去重方面的研究结果，这一发现并不太令人意外：我们发现两种去重方法在大多数指标上存在显著差异。例如，`line-char-duplicates` 指标（重复行中的字符数/总字符数）从独立去重的约 0.0053（2015-22 年）和 0.0058（2013-48 年）大幅增加到全局去重的 0.011（2015-22 年）和 0.01（2013-48 年），这表明后者文档间的重复程度更高。

按照上述流程处理这些数据集得到了 *十七* 对候选指标 - 阈值对。在下面的图像中，你可以看到其中的三幅直方图：

[交互图]

例如，我们检查了“以标点符号结尾的行所占比例”的直方图（见上图），发现全局 MinHash 在大约0.12处的文档密度有所增加。随后，我们以此阈值进行过滤，发现被移除的数据中短列表的数量较多，或者仅包含文档布局文本（如“主页”、“注册”等）。

随后，我们通过在 *2019-18 抓取* 数据上开展多次针对 *28B 标记* 的消融实验，评估了这十七个新创建过滤器的有效性。在所有这些实验中，我们确定了三个过滤器（基于上述直方图的过滤器），它们在综合得分上展现出了最为显著的提升：

* 移除行末标点符号占比 ≤ 0.12的文档（移除了 10.14% 的标记）——与原始 C4 终端标点过滤器的 30% 相比
* 移除重复行中字符占比 ≥ 0.1（移除了 12.47% 的标记）的文档——原始 MassiveText 对此比例的阈值是 ≥0.2
* 移除其中短于 30 个字符的行所占比例 ≥0.67 的文档（移除了 3.73% 的标记）
* 当三者一起应用时，约 22% 的标记被移除。

[交互图]

这些过滤器使我们能够进一步提高性能，并且值得注意的是，在提供更大数据集的同时超越了 C4 数据集的性能。

### 2.5 最终的 🍷 FineWeb 数据集

最终的 🍷 FineWeb 数据集包含 15T 个标记，并按顺序包括前面提到的以下步骤，每个步骤都对我们的基准测试任务组有性能提升：

- 基础过滤
- 每次转储独立的 MinHash 去重
- 选择 C4 过滤器
- 我们的自定义过滤器（在前一节中提到）

[交互图]

#### 2.5.1 与其他网络规模数据集的比较

我们将 🍷 FineWeb 与以下通常被认为质量最高的公开可获取的网络规模数据集进行了比较（我们还标明了每个数据集公共版本中大致的标记数量）：

- [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) (500B tokens)
- [C4](https://huggingface.co/datasets/allenai/c4) (172B tokens)
- [Dolma v1.6](https://huggingface.co/datasets/allenai/dolma) (3T tokens) (CommonCrawl 部分) 
- [The Pile](https://huggingface.co/datasets/EleutherAI/pile) (340B tokens) 
- [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B) (627B tokens) 
- [RedPajama2](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) (20T tokens)  (去重)
- 我们新的 [🍷 FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) (15T tokens) (本报告)

你会发现经过 350B tokens 训练的消融模型已公开可获取，并汇总在[此集合](https://huggingface.co/collections/HuggingFaceFW/ablation-models-662457b0d213e8c14fe47f32)中。我们每 1000 个训练步骤就上传一次检查点。你还可以在[此处](https://huggingface.co/datasets/HuggingFaceFW/fineweb/blob/main/eval_results.csv)找到我们的完整评估结果。

[交互图]

🍷 据我们所知，FineWeb 是目前能够实现当前最高模型性能的开源数据集，同时支持基于数万亿标记进行训练。

## 三、📚 FineWeb-Edu

[交互图]

📚 FineWeb-Edu 在我们的一组评估任务上优于 🍷 FineWeb 以及所有其他开放网络数据集。

📚 FineWeb-Edu 是我们在此技术报告中兴奋介绍并公开发布的一个 FineWeb 的拓展版本。📚 FineWeb-Edu 基于一种近期出现的新方法来过滤大语言模型（LLM）训练数据集，即使用合成数据开发用于识别教育内容的分类器。这种技术显著应用于 Llama 3 和 Phi3 的训练中，但在我们看来，其在大规模网络数据过滤方面的影响至今尚未被公开充分挖掘。

广受欢迎的 Phi3 模型分别在 3.3T 和 4.8T 个标记上进行训练，论文中提到：

> 我们的训练数据由经过严格筛选的公开可用网络数据（根据“教育水平”）以及合成的大语言模型生成数据组成，这些网络数据来自各种开放的互联网来源。

同样，Llama 3 博客文章指出：

> 我们发现，以往版本的 Llama 擅长识别高质量数据，因此我们使用 Llama 2 来帮助构建支持 Llama 3 的文本质量分类器。

然而，这些分类器和过滤后的数据集并未公开。为了进一步提高 FineWeb 的质量，我们利用 Llama-3-70B-Instruct 生成的注释开发了一个教育质量分类器，从而创建了 FineWeb-Edu。

### 3.1 大规模教育质量标注

我们使用 Llama-3-70B-Instruct 对来自 🍷 FineWeb 的 50 万个样本进行了标注，按照从 0 到 5 的等级对每个样本的教育质量进行评分。

我们探索了各种提示格式，以利用大型语言模型（LLM）自动提取教育评分，并发现袁等人提出的加法量表效果最佳。该量表允许大语言模型对每个额外增加的分数进行推理，这与将样本归入预定义区间的李克特单评级量表不同。然后，为避免大语言模型偏爱像 arXiv 摘要和投稿这类高度技术性的页面，我们聚焦于小学和中学水平的知识。在筛选过程中设置 3 分（满分 0 到 5 分）的阈值后，我们还能够保留一些高水平的教育页面。

![Prompt for LLM annotation](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/fjZQ4izIj1rx1xQnBTKKr.png)

用于对教育评分进行 Llama3 标注的提示语，也可在[此处](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier/blob/main/utils/prompt.txt)获取。

在用于标注数据的开放权重模型方面，我们尝试了几种模型，包括 Mixtral-8x7B-Instruct 和 Mixtral-8x22B-Instruct、Llama-3-70B-Instruct，还尝试了由这三个模型打分组成的评审团。在我们的实验中，我们发现仅使用 Llama3 能得到最可靠的结果。

### 3.2 训练分类器

为了将我们的标注扩展到 FineWeb 中的数万亿个标记，我们使用 Llama3-70B 的标注来训练一个小分类器。我们使用的模型是一个 Snowflake-arctic-embed 嵌入模型，在其顶部有一个带有单个回归输出的分类头。我们用 450,000 个 Llama 3 的标注对该模型进行了 20 个周期的训练，学习率为 3e-4，并冻结了嵌入层和编码器层。我们保存了在留出的 45k 样本验证集上 F1 分数最高的检查点，将 Llama 3 的标注视为真实值。训练后，我们将分数四舍五入为 0 到 5 的整数。

随后，我们通过设定固定阈值来判断文件是否具有教育属性，将问题转化为二分类任务。当阈值为 3 时，该模型在验证集上的 F1 分数达到了 82%，表明其在区分高质量教育内容方面表现强劲。

分类器可在以下网址获取：[HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier)。训练和推理代码可在 [GitHub](https://github.com/huggingface/cosmopedia/tree/main/classification) 上获取。

### 3.3 过滤和结果

我们将分类器应用于包含 15T 个标记的 🍷 FineWeb 数据集，这一过程需要 6000 个 H100 GPU 小时。我们研究了使用不同过滤阈值的影响，发现使用阈值为 3 时能取得最佳的整体结果。尽管使用高于 3 的阈值能提高知识和推理密集型基准测试的性能，但会显著降低在 HellaSwag 和 PIQA 上的性能。下图展示了与 FineWeb 相比，每个阈值在六个不同基准测试上的性能；该模型为 1.82B 参数，在 8B 个标记上进行训练。

[交互图]

**注意：** 此次消融实验是在 2024 年 10 月数据转储中 FineWeb 和 FineWeb-Edu 子集的 8B 个标记上进行的，这可能无法代表整个数据集。下一次消融实验表明，在来自所有 FineWeb 数据转储（HellaSwag 除外）的更长序列的 350B 个标记上，阈值为 3 时的研究结果依然成立，不过我们注意到 HellaSwag 的表现略有下降。

我们通过筛选掉评分低于 3 的样本构建了 📚 FineWeb-Edu。这使得数据集的 92% 被移除，最终我们得到了 1.3T 个教育领域标记。为了在更大规模上评估这种筛选的有效性，我们进行了一项消融实验，使用的是在一个由 350B 个标记训练而成的 1.82B 参数模型上进行的，类似于上述提到的 FineWeb 筛选消融实验。

[交互图]

以下是上述消融研究结果的关键亮点：

- 📚 FineWeb-Edu 超越了 🍷 FineWeb 及所有其他开放网络数据集，在教育基准测试（如 MMLU、ARC 和 OpenBookQA）上有显著提升。
- 它以大幅减少的数据量取得了相同的性能表现，与 C4 和 Dolma 相比，仅需十分之一的标记量就能达到 MMLU 的结果。
- 这证明了使用基于大语言模型注释训练的分类器进行大规模数据过滤的有效性。

鉴于阈值为 2 时也表现出了强大的性能，同时保留了更多数据，我们发布了一个额外的数据集，该数据集采用此阈值进行过滤，在 [HuggingFaceFW/fineweb-edu-score-2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu-score-2) 下包含 5.4T 个标记。

你可以在这个[集合](https://huggingface.co/collections/HuggingFaceFW/fineweb-edu-6659c3f3d399d0e1d648adfd)中找到两个数据集以及用于筛选的分类器。

## 四、奖励：随时间变化的 CommonCrawl 数据

> 就像优质葡萄酒一样，并非所有的爬行（过程/情况）都是生而平等的。

在去除过滤步骤的过程中，我们注意到某些抓取结果的表现明显优于其他抓取结果。我们决定对这一现象展开调查。

### 4.1 通过爬取来衡量基准性能

对于每次抓取，我们在从该次抓取数据中随机抽取的 27B 个标记（经过基础过滤和 MinHash 去重步骤后）上训练了两个 1.8B 参数的模型（每次运行的数据抽取都是不同的随机 27BT 标记抽样）。我们共训练了 192 个这样的模型，总计消耗超过 6 万个 H100 GPU 小时。随后，我们提取了两次运行的最后 3 个检查点，并绘制了每次抓取这 6 个数据点的平均值。

下图清晰地表明，有些转储操作的性能比其他操作差得多。每年的数据用不同颜色表示，而且每年的抓取次数也各不相同。

我们调查了这种行为的可能原因，例如每次数据转储中最常见网址的变化，以及潜在的基准测试污染，但未能找到任何确凿的解释。我们将进一步的研究留待未来工作。

### 4.2 合成数据

我们想知道，最近几次抓取的出色表现是否在一定程度上归因于合成数据（由大型语言模型生成的数据）数量的增加。鉴于近期大型语言模型（尤其是 ChatGPT）的受欢迎程度显著提升，这样的变化并不令人意外。

鉴于据我们所知，目前没有一种万无一失的方法来检测合成数据，我们选择使用一个代理指标：我们测量了每次抓取中以下词语的出现频率：`"delve", "as a large language model", "it's important to note", "rich tapestry", "intertwined", "certainly!", "dive into"`，这些词语都是 ChatGPT 常用的。

需要注意的是，并非所有包含这些短语之一的样本都一定是由 ChatGPT 生成的（同样，许多由 ChatGPT 生成的样本也不包含这些短语中的任何一个），但假设合成数据的数量在多次抓取之间没有变化，那么可以预期这些频率会随着时间大致保持不变。

结果显示在下面的图表中：

[交互图]

虽然在 2023 年 1 月 14 日之前（ChatGPT 于 2022 年底发布），这一频率大致保持不变，但我们发现，在最近的抓取中，我们的代理指标出现了急剧上升。虽然这个简单的测试不足以得出 ChatGPT 的生成内容和其他合成数据正在提高最近一次抓取的质量这一结论，但至少看起来并没有对其造成严重损害。

我们预计在新的大规模网络爬取（CC crawls）中会持续看到合成数据量的增加。然而，尽管对于相对较小的训练集来说，这些数据似乎不会影响性能（甚至可能实际上提高性能），但目前尚不清楚这种情况是否适用于更大的训练集。

## 结论与展望

通过我们的开放科学努力，我们希望持续照亮高性能大型语言模型训练这一黑箱，并赋予每位模型训练者创建最先进大语言模型的能力。我们很高兴能继续完善 FineWeb，并以完全开放和可复现的方式发布经过愈发精细筛选的网络数据子集。

短期内，我们期待将从（英语）FineWeb 中获得的经验应用到其他语言上。虽然目前英语在大型语言模型领域占据主导地位，但我们认为，让其他语言的高质量网络数据尽可能易于获取将产生极其重大的影响。

简而言之：对于研究大规模且开放地创建数据集的科学而言，未来光明且令人兴奋 🤗。