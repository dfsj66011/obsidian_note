
虚拟的世界中 做他们要这个agent做的事情 比如说跟这个agent说 去拿一个牛奶来喝 他就会走进厨房 打开冰箱 拿一个牛奶 再把冰箱关起来 所以看起来 好像有一定程度 做计画的能力 那有人做了一个 做计画的benchmark 这个benchmark就是考验语言模型 做规划 对话的能力 那这个benchmark裡面 最主要的测试题目 是一个跟叠积木有关的题目 这个题目的叙述呢 通常长的是这个样子 告诉语言模型说 你现在有哪些操作 可以从桌上拿起积木 可以从一个积木上拿起 另一个积木
              
                  1:21:37
                  可以把积木放到桌上 可以把一个积木堆到 另外一个积木上 那现在初始的状态 像右边这个图这样子 那问说 怎麽把橘色的积木 放在蓝色的积木上 这边要执行的动作就是 把蓝色的积木拿起来放到桌上 然后再把橙色的积木拿起来 放到蓝色的积木上就结束了 所以这个对AI agent来说 其实也都是蛮容易的问题 他知道说执行以下四个步骤 就可以让橙色的这个积木 跑到蓝色的积木上 但是plane bench不是隻做这种 比较一般的叠积木的游戏而已 为什麽不能够只做这种题目呢 因为想现在这些语言模型 他都从网路上爬大量的资料来进行训练 什麽叠积木这种题目网路上根本就已经有 他搞不好根本就看过一模一样的东西 所以他能够做计划 并不代表他真的知道做计划是怎麽一回事 他可能只是从他看过的资料裡面 照本宣科文字接龙出来一个看起来还不错的结果而已
              
                  1:22:42
                  这让我想到说一个当兵的故事 这故事就是有个司令官去一个军营 然后看到两个小兵在守著一个 这个长椅 然后不让任何人做 他就问说 为什麽你们要守护这个长椅 不让任何人做呢 那个士兵说不知道耶 前任司令官就是指示说 一定要守护这个长椅 所以这个军营总是要派两个人 在长椅那边站港 然后司令官就打给前任司令 说为什麽要有人守护这个长椅呢 前任司令官 所以不知道耶 前前任司令官交代要守护这个长椅 然后再问前前前任司令官 也说不知道耶 一直问到五十年前 一个已经超过一百岁的司令官 他说什麽那个长椅 长椅的游戏还未乾吗 好 大家有没有听懂 算了 就是这麽一个故事 就是会不会AI agent在做事情的时候 他根本不知道他自己在干嘛 只是从某个地方 网路上他过去的训练资料 看过一样的东西 他把一样的东西拿出来给你看 所以在plane bench裡面 他们有一个比较变态的测试
              
                  1:23:45
                  这个测试叫做神秘方块世界 这个方块世界不是一个 正常的方块世界 裡面的方块可以做的行为 是一些怪怪的行为 比如说你可以攻击方块 一个方块可以吞噬另外一个方块 你可以屈服一个方块 一个方块可以征服另外一个方块 然后接下来他就会定一套 非常複杂的规则 然后根据这套规则去运作 你可以达到某一个结果 他最后要的结果是 让物件C渴望物件A 让C方块渴望A方块 那渴望是什麽意思 你就是按照前面那一套规则操作 看机器能不能读懂前面那一套规则 按照那一套规则操作 让物件C可望物件A 那这个时候语言模型 期待他就不能用他看过的知识 来解这个问题 好那语言模型 在这个神秘方块世界做得怎麽样呢 这边引用的是 2023年的结果 那最上面这个部分呢 是当年那些模型 在正常方块世界的结果
              
                  1:24:48
                  那这个数值呢 所以看起来GPT4 可以得到30几%的正确率 那这边是神秘 方块世界的结果 在神秘方块世界裡面呢 你看这个GPT4最好 就算叫他做channel sort 就算他叫channel sort 也只有9%的正确率 所以看起来 他有点overfeed在一般方块的世界上 给他神秘方块世界 他是解不了的 不过这是2023年 这个是古代的结果 我们来看 这个去年9月 有了欧万以后的结果 而有欧万以后结果就不一样了 这边一样是神秘方块世界 纵轴呢是正确率 横轴呢是问题的难度 那发现说多数的模型啊 都躺在这个地方 他们正确率都非常的低 只有绿色的这个虚线 有一点起色 绿色的虚线是 LLaMA 3.1 405B 那个大模型 它可以解最简单的问题 但是如果用o1-mini
              
                  1:25:52
                  是红色这一条线 用o1-preview是蓝色这一条线 看起来这些reasoning的模型 是有一些机会 来解这个神秘方块世界的 当然这边你还是可能有一个怀疑 就是神秘方块世界 会不会o1看过了呢 会把训练资料裡面根本就有神秘方块世界的资料 那这个我们就没有办法回答了 只是说就现有这个benchmark 看起来o1是有机会解神秘方块世界的 好那还有另外一个跟做计划有关的benchmark 这个计划这个benchmark呢 要AI扮演这个旅行社 然后呢你给他一个旅行的计划 叫他帮你规划 这个AI要读懂你的计划 然后他可以使用一些工具 他可以上网搜寻资料 然后呢他会 根据人提供给他的一些constraint 比如说经费多少 预算多少一定要去哪裡 一定要去哪裡一定要做什麽 一定不要做什麽 以common sense产生一个旅行的规划 那这个是一个24年年初所发佈的benchmark
              
                  1:26:58
                  那AI要做的事情讲得更具体一点 就是他要读一个问题 这个问题裡面是说我要规划一个三天的行程 从某个地方到某个地方 什麽时候出发什麽时候回来 我的预算是1900元 所以不能花超过1900元 然后AI就要产生一个规划 说第一天我们搭哪一班飞机 什麽时候从哪裡到哪裡 早餐吃什麽 午餐吃什麽 晚餐吃什麽 最后住在哪裡等等 产生这个规划 然后要符合预算的限制 那现在当时 这个是24年年初 当时的模型做得怎麽样呢 这边是做了 你看还有什麽GPT3.5 GPT4等等的模型 那又分成上半跟下半 上半是这些模型 要自己使用工具 跟网路的资料互动 然后得到正确的答案 你会发现这些模型 都非常 都产生一团 多数模型 它的成功率 就最后产生一个 合理的旅游规划 那个旅游规划 是完全没有问题的 机率是0%
              
                  1:28:00
                  只有GPT4 Turbo 可以得到0.6%的成功率 那下面这个部分呢 下面这个部分是说 既然大家都那麽惨 尤其是模型很多时候 他根本用不了工具 太笨了 没办法用工具 工具使用方法根本是错的 那没关係就别用工具了 把所有的资讯都先找好 贴给模型 让模型 根据这些资讯来做规划 那最好也只有GPT 4 Turbo 可以做到4%左右的成功率而已 所以在24年年初 那个时候看起来是没办法让语言模型 扮演一个旅行社来帮你规划旅游行程的 那我们来看这些模型会犯什麽错吧 那这个是从他们官网上 这个project的官网上找了几个有几个错误 比如说模型呢 可能会做一些没有尝试的事情 在第三天 这个飞机呢 八点就已经起飞了 但是还是安排了一些旅游的行程 还安排了午餐的地点 所以这是一个不符合常识的规划 或者是有时候模型找不出一个好的规划来符合预算的限制 比如说这边这个预算的限制是三千元
              
                  1:29:09
                  最多花三千元 那模型第一次规划的结果是三千两百四十七元 还差了一点 所以模型就修改了原来的规划 他好像做了一些cost down 午餐吃差一点的东西 那降到三千两百三十八元 后来又想说那早餐也吃差一点的东西 降到三千两百一十六元 只降这麽多 他想说放弃算了好了 跟三千元没差那麽多就算了 所以这个就不是一个成功的结果 那这个作者有评论说 其实只要降低住的地方 不要住那麽好 就可以轻易的达到三千元底下的预算 就可以符合预算的限制 但是语言模型始终没有发现这件事 看起来他做规划的能力并没有非常的强 他没有办法做一个规划去符合限制 那既然问题在没有办法符合限制 有人就想说那符合限制这件事情 就不要交给语言模型来做了 交给一个现成的solver来做 所以语言模型做的事情是写一个程式 用这个程式去操控现成的solver 然后来得到合理的旅游规划
              
                  1:30:15
                  那有了这个现成的solver 也有这个工具的加入之后 这solver就等于这个工具 那这个旅游的规划可以做到什麽地步呢 去年4月的结果几个月后 有人用GPD4跟Cloud3 就可以做到90几%的正确率 所以看起来在有工具辅助以后 语言模型也是有机会做出不错的旅游规划 不过至少做出符合逻辑的旅游规划 好所以现在到底模型规划的能力怎麽样呢 就是介于有跟没有间吧 就是你也不能说他完全没有 但你也不能说他 真的非常强 好那我们怎麽进一步强化这一些AI agent的规划能力呢 能不能够让他做的比他自己想出来的规划还要更好呢 一个可能是让AI agent在做规划之前 实际上去跟环境互动看看 今天在第一个observation的时候 那看看现在有哪些可以执行的行为 总共有一之一一之二一之三三个行为 哪个行为最好呢
              
                  1:31:17
                  通通都去试一下 得到状态二之一 然后呢 状态二之一后面有两个行为也都试一下 状态二之二之后有另外一个行为试一下 状态二之三之后两个行为都试一下 得到接下来的状态 然后呢看看有没有成功的路径 报收一阵以后发现有成功的路径 这条路径是成功的 那你就知道说 那我要採取action一之三 接下来要採取action二之三之一 就会成功 简单来说就是要语言模型 跟实际的环境互动 一下报收一出一条最好的路径 那这个就是一个很强的规划的方式 但是这麽做显然是有很明确的弱点的 第一个很明确的弱点就是 报收如果今天这个任务很複杂 报收所有的路径 显然是要花费非常庞大的算力的 你总不能原模型每次下决策前到报收所有的可能性吧 虽然这样可以找到最好的结果 但是可能是不切实际的想法 所以一个可能的想法是 把一些看起来没希望的路径
              
                  1:32:22
                  直接就丢掉 比如说走到某一个状态的时候 语言模型可以自问自答说 走到这个状态 还有完成功的机会吗 那如果说没有 那这条路径就不尝试下去 如果说有那才尝试下去 这样就可以减少无谓的搜寻 那这个方法有没有用呢 有一篇paper叫做Tree Search for Language Model A 那这个是去年夏天的论文 就做了类似的尝试 让模型有使用电脑的能力 这边就是给模型一个指令 跟一张图片 叫他上网去做某一件事情 那如果只是GPT4 做一般的这种直觉式的 那种反射式的回答的话 没有办法做得很好 但是他们用这个报收 加上去除没机会的路径的方式 就先走这条路径 然后呢 模型会不断自问自答说 这条路径还有希望吗 然后给一个分数
              
                  1:33:24
                  那如果分数低于某一个threshold就不做了 就跳另外一个路径 低于某一个分数不做了 再跳另外一个路径 低于某一个分数就不做了 再跳另外一个路径 那最终找出一条最佳的路径 那模型就等于做了规划 那就可以走到最佳的结果 这个是Tree Search for Language Model Agent 但这边有各式各样的这种Tree Search的algorithm 你可以採用了 这边我们就不展开细讲 那这种Tree Search的方法有很大的问题 什麽样的问题呢 它的缺点是有一些动作 做完以后你是覆水难收 没有办法回头的 比如说假设现在在语言模型 可以採取的三个action裡面 有一个是订pizza 有一个是订便当 然后呢他先订了pizza以后 继续走下去发现这条路不好 所以他最后发现订便当 才是最好的solution 但是你pizza已经订了 他跟人家说我不要订这个pizza了 但那个pizzahard 他已经把那个pizza做了 他说谁管你啊 你一定要把这个pizza吃下去 有些动作做了以后
              
                  1:34:27
                  就是覆水难收 所以这样的tree search的方法 跟现实世界互动 找出最佳途径的方法 也有可能有问题的 那怎麽处理这个覆水难收的问题呢 一个可能性就是 让刚才一切的尝试 都发生在梦境中 都发生在脑内的巨差 刚才一切的互动 都不是现实生活中 真正发生的事情 原来都是模型脑内的模拟 他自己想像说 他执行的action一之一 他自己想像说 接下来会看到 二之一 他在自己想像去评量这个路径 有没有希望发现没有 就换搜寻另一条路径 直到达到他想像中的一个 理想的结果 但这边还有另外一个问题 从action到observation 从模型执行的行为 到他看到接下来环境的变化 这中间的过程不是模型决定的 他实际上是环境决定的
              
                  1:35:32
                  那模型怎麽知道环境会有什麽样的变化呢 模型怎麽知道我採取一个行为 接下来会看到什麽样的改变 你在跟一个对手下棋的时候 你怎麽知道你下一步棋 接下来会发生什麽样的事情 对方会有什麽样策略的回应呢 所以你需要有一个 Wall Model 如果是在AlphaGo下棋裡面 他就是自己扮演对手自己跟自己下 那在这边的情况 在这个AI agent的情况 你就是需要一个 Wall Model 他模拟环境可能会有的变化 那Wall Model怎麽来呢 也许AI可以自问自答 自己扮演这个Wall Model 自己去猜想说 他执行了某件事以后 接下来会发生什麽样的行为 这件事有机会成真吗 你可以读一篇paper is your LLM secretly a world model of the internet 这篇paper就是用model-based planning的方法 来打造一个web agent 这篇paper裡面的解法是 现在有一个网页 模型的这个任务目标呢 是要买某一个东西
              
                  1:36:36
                  那有三个选项 有三个东西是可以点的 接下来黄色这个区块 一切所发生的事情 都是发生在脑内的剧场 都是发生在模型的梦境 它并没有实际发生 模型想像一下 我点按钮1 接下来会发生什麽事 接下来会发生的事情 是用文字描述出来的 但选中文字来描述接下来发生的事情是很直觉 其实作者在文章没有解释说 那为什麽不直接产生这个网页的图呢 你想说有可能吗 这个难度那麽高 有没有可能真的就创造出一个 新的网页模拟出 接下来可能发生的状况呢 这难度也太高了嘛 产生文字可能是 比较实际的做法 所以接下来梦境中 这个环境会发生什麽样的变化 是语言模型自己用文字描述出来的 所以他就想像说会发生什麽样的变化 有了这个变化以后 他再想像自己多执行了一步 然后看看会发生什麽样的事情 所以这边就是点
              
                  1:37:38
                  选第二个按钮 然后想像发生什麽样的变化 自己再多执行一步 那想像会有什麽样的变化 第三个按钮想像发生什麽样的变化 执行部再想像会有什麽样的变化 那哪一步比较好呢 他在自己去问说 那这一步大概有多少机会成功呢 自己评估一下40% 这一步自己评估一下 是80%这一步自己评估一下 是10%看起来中间第二步 机器人第二个按钮 中间第二个选项是比较容易成功的 所以他就选 实际上所以上面并没有真实 发生过黄色框框裡面的事情并没有真实发生过 它是一个梦境中的脑内小剧场,模型在梦境中得到了启示说一定要选第二步,所以在真实的现实世界中,它就选择了第二步,所以这个就是让模型强化它规划能力的方式。 好,讲到这个脑内小剧场啊,那你是不是就想到说,在上次的课程中也有提到脑内小剧场,上次的课程我们说现在有很多模型都号称有思考,用英文讲就是reasoning的 那这些有reasoning能力的模型,其实所谓reasoning的能力就是可以演一个脑内小剧场,告诉你说他现在是怎麽思考,如果把这些有reasoning能力的模型,拿他来做AI agent,他的脑内小剧场会不会正好就是在做规划呢,如果现在他的输入就是我们给AI agent的observation,输出就是我们要AI agent採取的action,会不会脑内小剧场就是更好,
              
                  1:39:14
                  刚才类似梦境中看到的规划呢 他自己採取了不同的可能性 自己在验证每一个可能性 可能成功的机会 自己扮演World Model 自己扮演这个世界 去想像他採取一个行为之后 接下来会发生什麽样的事情 我实际试了一下DeepSeek-R1 看起来他确实有类似的效果 我们把刚才那个积木的问题交给他 然后接下来他就开始演脑内小剧场 上略1500字 他真的做了1500字 讲了很多很多 然后呢 你可以看到说在脑内小剧场的过程中 他就是做了各式各样的尝试 他做的事情就有点像是刚才的tree search 然后最后他找出了一个optimal solution 他在梦境中知道说 从橘色的方块上拿起蓝色的方块 蓝色的方块放到桌上 从桌上再拿起橘色的方块 放到蓝色的方块上 这四个步骤就可以完成我们的要求 他在梦境中已经找出了一个最佳的solution 然后再执行最佳solution的第一步 就我这边 要求他告诉我他的下一步是什麽 只要求他讲一步
              
                  1:40:19
                  那脑内小剧场先找出一个成功的solution之后 在执行这个计画 他已经找出一个成功的计画之后 在执行计画的第一步 就是使用操作二 把橘色的积木从蓝色的积木上面拿起来 好 讲到这边 其实这麽堂课呢 也可以停在这边 不过这边多补充一件事 就在几週之前 有一篇新的论文 叫做the danger of over thinking 他们就是把这些能够演脑内小剧场的模型 让他们扮演AI agent 看看他们做事有没有效率 其实整体而言 能够做脑内小剧场的模型 还是比不能够做脑内小剧场的模型 在AI agent的这些任务上面表现得更好 但是他们也有一些问题 他们会有什麽问题呢 就是想太多了 他们是思考的巨人行动的矮子 就有时候这些模型会 比如说 按钮点下去会怎麽样 他就一直想一直想一直想 怎麽想都不停 那你怎麽想都没有用 因为你根本不知道那个按钮点下去会发生什麽事 还不如直接点一下
              
                  1:41:22
                  因为在很多情况下 你直接尝试点一下 也许只要不是这个信用卡付款的 你都按上一页就回去了 你就知道发生什麽事了 与其一直想还不如做一下 或者是有些模型 他尝试都没有尝试 他光是拿那个问题想啊想啊想啊 就想说这我应该做不到 还什麽都不是就直接放弃 死于想太多这样子 所以这些模型他们有的问题就是想太多 所以如何避免这些模型想太多 也许是一个未来可以研究的关键 好那以下就是今天要跟大家分享的 模型怎麽根据经验调整行为 怎麽使用工具,能不能够做计画
              
            