
起来就比较耗费算力 所以大的模型不能常常出现 大的模型要在小的模型召唤它的时候 才出面回答问题 大哥要偶尔才出来帮小弟解决事情
              
                  49:31
                  那其实这些工具对语言模型来说 都是function 都是一个函式 当我们说语言模型在使用某一个工具的时候 其实意思就是它在调用这些函式 它不需要知道这些函式内部是怎麽运作的 它只需要知道这些函式怎麽给它输入 这些函式会给什麽样的输出 那因为使用工具就是调用函式 所以使用工具又叫做function code 所以有一阵子很多语言模型都说 他们加上了function code的功能 其实意思就是 这些语言模型都有了使用工具的功能 好那语言模型怎麽使用工具呢 等一下我会讲一个通用的使用工具的方法 但实际上使用工具的方法很多 甚至有一些模型是专门针对来练习 他就训练来使用工具的 那他如果是针对使用工具这件事做训练 那他在使用工具的时候 你可能需要用特定的格式才能够驱动他 那那个就不是我们今天讨论的问题
              
                  50:35
                  或者是假设你有使用 使用这个OpenAIChat GPT的API的话 你会知道使用工具这件事情 是要放在一个特殊的栏位 所以对OpenAI来说 它的模型在使用工具的时候 也有一些特殊的用法 但我这边讲的是一个最通用的用法 对所有的模型 今天能力比较强的模型 应该都可以使用 好,什麽样通用的方法 可以让模型使用工具呢 就是直接跟他讲啊 就告诉他怎麽使用工具 你就交代他可以使用工具 那你就把使用工具的指令 放在两个Tool符号的中间 使用完工具后你会得到输出 输出放在两个Output符号的中间 所以他就知道工具使用的方式了 接下来告诉他有哪一些可以用的工具 有一个函式叫做Temperature 他可以查某个地点某个时间的温度 他的输入就是地点跟时间 给他的使用范例 Temperature括号台北某一段时间 他就会告诉你台北在这个时间的气温
              
                  51:40
                  接下来你就把你的问题 连同前面这些工具使用的方式 当作Prompt一起输入给语言模型 然后他如果需要用工具的话 他就会给你一个使用工具的指令 那前面这些教模型怎麽使用工具的这些叙述 他叫做System Prom 那查询使用调用这些工具的这些 这段话,某年某月某日高雄气温如何,这个是User Prompt,那如果你有在使用这个ChatGPT的API的话,你知道你的输入要分成System Prompt跟User Prompt,那很多同学会搞不清楚System Prompt跟User Prompt有什麽样的差别,那System Prompt指的是说,你在开发应用的这个Developer下的这个Prompt,这个Prompt呢,是每次都是一样的,每次你都想要放在语言模型最前面, 让他去做文字接龙的这个叙述叫做System Prompt 那每次使用他的时候都不一样
              
                  52:43
                  通常是这个服务的使用者输入的内容叫做User Prompt 那在ChartGPT的API裡面 特别把System Prompt跟User Prompt分开 也是要分开输入的 因为System Prompt跟User Prompt 他有不同的优先级 System Prompt他优先级比较高 如果System Prompt跟User Prompt有衝突的时候 模型知道他要听System Prompt的 不要听User Prompt的 好,那有了这些Prompt以后 告诉模型怎麽使用工具 问他一个问题 那他发现这个问题 调用工具可以回答 他就会自动输出 Tool, Temperature,高雄 时间,然后Tool 告诉你说 他想要调用根据我们的叙述 去调用这个工具 但是不要忘了语言模型真正做的事 就是文字接龙 所以这一串东西实际上就是一串文字 它没办法真的去呼叫一个函式 那这一段文字要怎麽去呼叫函式呢 那就要你自己帮模型 把这个桥樑搭建好 所以你可以先设定说
              
                  53:47
                  只要出现在拓中间的这段文字 不要呈现给使用者看 当出现拓这段文字以后 把这段内容直接丢给temperature这个function 那temperature这个function是已经事先设计好的 它就会回传一个温度 那这个温度要放在output的token裡面 然后这个output token裡面的内容 也不要呈现给使用者看 那这一套脚本是agent的开发者 你自己需要先设定好的流程 所以现在有工具使用的这段文字 有得到工具输出的这段文字 接下来就继续去做文字接龙 对语言模型来说 他就根据输入 还有这边已经产生的输出 语言模型会以为是自己的输出 虽然是你强塞给他的 那他就继续去做文字接龙 他就会接触说 啊在某年某月某日 高雄的气温是摄氏32度 那这是使用者真正看到的输出 那使用者就会看到说 他输入了一个问题 然后语言模型真的给他一个答案 他不一定会知道背后呼叫了什麽样的工具
              
                  54:52
                  你完全可以做一个设计 把这个呼叫工具的这个步骤 藏起来不让使用者知道 那语言模型最常使用的工具就是搜寻器 我想这个大家都已经非常熟悉了 使用搜寻引擎又叫做 Retrieval Augmented Generation 也就是RAG 在上课也已经提过RAG这个词彙 好几次了 那使用搜寻引擎当然非常有用 这个RAG这个技术呢 已经被吹捧到不能再吹捧了 所以我就不需要再告诉你 RAG这个技术有多重要 那其他使用工具的方式 也可能一样有用 举例来说 我们刚才说 可以拿其他的AI 来当作工具 今天假设一个文字的模型 他本来只能吃文字的输入 产生文字的输出 那现在假设你要他处理一段语音的话怎麽办呢 让模型处理语音有什麽好处呢 你就可以问他各式各样的问题 问他说这个人在说什麽 那他可以告诉你这句话的内容 问他说这个人心情怎麽样 如果他完全听懂这段声音 他也许可以做情绪辨识
              
                  55:56
                  告诉你这个人的情绪怎样 并做出适当的回馈 他的文字模型 比如说确GPT多数的模型都是文字模型 他没有办法真正读懂语音 所以怎麽办呢 当你问他一个问题说这边有段声音 那你觉得这个人 他心情怎麽样他讲了什麽 根据背景杂性你觉得他在哪裡 如果你不做特别的处理文字模型 是完全没有办法回答的 但这边你可以让文字模型 使用工具 可以告诉他这边有一堆 跟语音相关的工具 有语音辨识的工具 这个语音侦测的工具 有情绪辨识的工具 有各式各样的工具 那可能会需要写一些叙述 告诉他每一个工具是做什麽用的 把这些资料都丢给 然后呢他就会自己写一段程式 在这些程式裡面 他想办法去呼叫这些工具 他呼叫了语音辨识的工具 呼叫了语者验证的工具 呼叫了这个sum classification的工具 呼叫emotion recognition的工具 那最后呢还呼叫了一个语言模型
              
                  57:00
                  然后 得到最终的答案 那这个答案 其实是蛮精确的 这个方法其实有非常好的 效果那这篇文章 其实是我们大助教的文章 所以特别拿出来讲一下 那这个结果呢 是做在一个叫做Dynamic Super的 Benchmark上Dynamic Super 是一个衡量 语音版的语言模型 能力的资料集 这也是我们实验室跟其他团队 一起做的 那这个让文字模型使用工具的方法 它得到的结果 是最下面这一行 那我们就看最后一个column 这个是各种不同模型 在55个语音相关任务上的能力的平均 那也发现让语言模型 使用工具得到的正确率 是最高的 可以完胜当时其他号称 可以直接听语音的模型 所以使用工具可能可以 带来很大的帮助 但使用工具 也有其他的挑战 我们刚才使用工具的方法是
              
                  58:04
                  每一个工具 他都要有对应的文字描述 告诉语言模型说 这个工具 要怎麽被使用 但假设工具很多怎麽办呢 假设现在可以用的工具 有上百个上千个 那你岂不是要先让语言模型 读完上百个上千个 工具的使用说明书 才开始做事吗 就跟刚才我们说不能够让AI agent 先回顾他的一生 然后才来决定下一个指令一样 才能决定下一个行动一样 我们也没有办法让语言模型 读完上百个上千个 工具的说明书才来决定 某一个工具要怎麽使用 所以当你有很多工具的时候 你可以採取一个 跟我们刚才前一段讲 AI agent memory非常类似的做法 你就把工具的说明 通通存到AI agent 的memory裡面 那你打造一个工具选择的模组 那这个工具选择的模组 跟IG 其实也大差不差 这个工具选择模组
              
                  59:07
                  就根据现在的状态 去工具包裡面 去memory的工具包裡面 选出合适的工具 那语言模型真的在决定下一个行为的时候 只根据被选择出来的工具的说明 跟现在的状况去决定 接下来的行为 那至于如何选择工具 右上角引用两篇论文 一篇23年比较旧的论文 一篇是上个月的论文给大家看 参考告诉你说这方面的研究 是一直有相关的研究在产生的 那另外一方面 语言模型甚至可以自己打造工具 语言模型怎麽自己打造工具呢 不要忘了 所有的工具其实就是韩式 语言模型今天是可以自己写程式的 所以他就自己写一个程式 自己写一个function出来 就可以当作工具来使用 如果他写一个function 发现这个function运作的非常的顺利 他就可以把这个function当作一个工具 放到他的工具包裡面 那之后这个工具就有可能在选择工具的时候 被选出来用在接下来的互动中使用
              
                  1:00:13
                  那类似的技术非常的多 那我在右上角就引用了一系列的论文 从23年到24年的论文都有 告诉你说这也是一个热门的研究方向 那其实啊 让模型自己打造工具这件事情 跟模型把 过去的记忆 比如说一些比较成功的记忆 放到memory裡面再提取出来 其实是差不多的意思 只是这边换了一个故事 说现在放到memory裡面的东西 是一个叫做工具的东西 是一段程式码 但他们背后基本的精神 其实跟根据经验 来让模型改变它的行为 可以说是非常类似的 好,那今天人类把语言模型 当作工具 语言模型 把其他工具当作工具 比如说把搜寻引擎当作工具 所以搜寻引擎现在很惨 它是工具的工具 人类还不使用它 人类是使用语言模型 那个工具的工具 还没有被人类使用的资格 它只能够被语言模型使用而已
              
                  1:01:17
                  但我们知道说 工具有可能会犯错 大家都知道说 语言模型有可能会犯错 之前有什麽律师 然后在写树状的时候 引用了语言模型的内容 结果发现是错的 然后就成为一个今天的新闻 我们都知道过度相信工具是不对的 那这一些语言模型会不会也过度相信了他们的工具 所以得到错误的结果呢 这是有可能的 我们这边拿RAG当作一个例子 那这是一个非常知名的例子 之前Google出了一个叫做AI Overview的功能 这个功能其实就是一个RAG的功能 根据Google搜寻型的结果 用语言模型总结搜寻型的答案 那就有人问了一个问题 我的披萨上面的起司黏不住 怎麽办呢 那AI Overview就说 弄个胶水把它黏上去就好了 而且他是非常认真在回答这个问题的 因为他说不只要用一般的胶水 要用无毒的胶水才可以 那这个答案呢 其实就是来自于Ready上一个乡民的玩笑 就有一个乡民开玩笑说
              
                  1:02:21
                  你用胶水把起司黏在披萨上不就好了 这是个玩笑话 但是对AI agent来说 他没办法判断这个到底是不是开玩笑 他看到网路上写的文章 照端全收都当作是正确答案 所以就像是我们今天都会告诉人类 要有自己的判断能力 不要完全相信工具的结果 所以我们也要告诉我们的工具说 这些不要完全相信工具的工具 要有自己的判断能力 不要完全相信工具的工具给你的结果 那今天这些语言模型 有没有自己的判断能力 知道工具的工具可能会犯错呢 我们这边举一个实际的例子 那我们刚才在讲怎麽使用工具的时候 说我们有一个叫做temperature的function 语言模型呼叫temperature的function 可以知道温度 那我现在呢给他一个乱七八糟的温度 我说现在高雄呢 是摄氏100度 这不可能 想也知道是不可能 这不是跟煮沸的水一样热了吗 那语言模型知不知道这有问题呢 他不知道 他就告诉你说
              
                  1:03:25
                  高雄的气温是100度 真的非常的热 如果你把温度再调高一点 说现在是一万度 哇 比太阳上还热 这个时候会发生什麽事呢 语言模型继续做文字接龙的时候 他就知道说 这显然有问题 这个API给我的答案是一万度 这是不合理的 怎麽可能比太阳上的温度还高呢 可见工具输出有错 如果你需要其他帮助的话再告诉我 所以语言模型今天是有自己一定程度的判断力的 他也不是完全相信工具 就像你今天不完全相信语言模型的输出一样 他也不完全相信他的工具的输出 他还是有自己一定程度的判断力的 所以实际上语言模型在使用工具 或者是他在做RAG的时候 他内部是有一个角力的 就语言模型有他内部对世界的信念 这是他的internal knowledge 存在他的参数裡面 他从工具会得到一个外部的knowledge 那他会得到什麽样的答案 其实就是internal knowledge跟external knowledge 内外的知识互相拉扯以后
              
                  1:04:29
                  得到的结果 那接下来我们要问的问题是 那什麽样的外部知识 比较容易说服AI 让他相信你说的话呢 那为什麽这是一个重要的议题呢 想想看 现在大家都用Deep Research来查找答案 甚至很多人都已经用Deep Research来写报告了 所以现在大家已经不会直接去用搜寻引擎搜寻了 你看到的是Deep Research告诉你的结果 所以今天假设某个议题是有争议性的 有正反两派的观点 那谁能够想 写出来的文字比较能够说服AI 谁就可以在AI搜寻的结果裡面 佔到优势 就可以比较有机会影响人类 所以知道怎麽样比较能够说服AI 相信你的话是一个重要的议题 那什麽样的外部资讯 AI比较容易相信呢 这边这篇文章给了一个 非常符合我们直觉的实验结果 这篇文章做了什麽样的实验呢 他说我们先来看看AI内部的知识是什麽 他就问AI说某一种药物 这种药物每人每日的最大剂量是多少 那AI说是20毫克
              
                  1:05:36
                  那真正的答案呢 是30毫克 所以你给他医学的知识 告诉他说给他医学的报告 那医学报告裡面是写30毫克的时候 你问他同样的问题 这种药物每天最多费用多少 他会知道是30毫克 那接下来我们刻意修改报告的内容 如果你把30毫克改成3毫克 变成原来的十分之一 模型相不相信呢 他就不相信了 他就直接回答是20毫克 用他本身的知识来回答这个问题 但你把30毫克乘两变 变成60毫克 模型相不相信呢 他相信 他相信这个报告裡面写的 这个时候他就不相信自己的内部资讯 但如果你把30毫克乘10倍 变300毫克 这时候他又相信谁了呢 他相信自己的知识 不相信你额外提供的外部知识 所以这边的结论其实非常好 符合你的直觉 外部的知识 如果跟模型本身的信念差距越大 模型就越不容易相信 那如果跟本身的信念差距比较小 模型就比较容易相信
              
                  1:06:40
                  这个很直觉的答案 另外同一篇文章的另外一个发现就是 模型本身对他目前自己信念的信心 也会影响他会不会被外部的资讯所动摇 有一些方法可以计算模型现在给出答案的信心 如果他的信心低 他就容易被动摇 如果他的信心高 他就比较不会被动摇 这个都是非常直觉的结果 后来另外一个问题是 假设今天 你给模型两篇文章 那这两篇文章的意见是相左的 那模型倾向于 相信什麽样的文章呢 有一篇论文的发现是 如果这两篇文章答案不同 一篇是AI写的 一篇是人类写的 现在这些语言模型 都倾向于相信 AI的话 而且那个AI不需要是他自己这样 就靠的可能会相信 比较相信Chet GPT的话 Chet GPT比较相信Gemini的话 他们比较相信AI同类的话 比较不相信人类的话 那到底为什麽会这样子呢 这篇文章裡面先提出一个
              
                  1:07:45
                  第一个假设 然后再否定了这个假设 他一个假设是说 会不会是因为AI的观点都比较类似 因为这些模型 现在训练的资料都是网路上爬的 爬到差不多的资料 所以他们讲的话都差不多 想法都差不多 但他们刻意做了一个实验 他们刻意找那些问题是 现在要回答答案的AI 他在没有提供这些资讯的时候 他的答案跟人类 和另外一个AI的想法 都是完全不同的状况 就算是这种情况 一个AI一个语言模型 还是倾向于相信 他的AI同类讲的话 所以这就给我们一个启示说 未来如果你要说服一个AI的话 用AI产生出来的论点 产生出来的文章 可能更容易说服另外一个AI 接受你的观点 这篇文章还有做了其他分析 比如说他觉得 也许AI写的文字就是比人类写得更好 更有架构 更有条理、更明确、更简洁 所以AI比较容易 相信另外一个AI讲的话
              
                  1:08:48
                  那是不是这样 那可以未来再做更多的研究 那另外呢 我们实验室的江承汉同学 研究了一个文章的metadata 对于AI会有多相信这篇文章裡面的资讯 做了研究 那这边的设定是这个样子 你问AI一个问题 比如说某一个计画 有没有编辑报这种动物的基因 然后接下来给他两篇文章 这两篇文章都是假的 都是AI生成的 所以并没有AI比较喜欢人还是AI写的文章这个问题 两篇都是语言模型生成的 那其中一篇会说这个计画有编辑报的文章 另外一篇文章会说这个计画没有编辑报的文章 那接下来呢,我们给这两篇文章不同的metadata 比如说给这两篇文章不同的发佈时间 说左边这篇文章发佈时间是2024年 右边这篇是发佈2021年 你会发现这个时候AI相信2024年的这篇文章的内容
              
                  1:09:55
                  但如果文章的内容完全不改变 我们只是把发佈的时间换了 我们说左边这个一样的文章 发佈时间从2024改成2020 那右边这篇文章从2020改成2024 这个时候语言模型倾向于相信 右边这篇文章的内容 所以我们这边就学到一个很重要的知识 语言模型比较相信新的文章 当两篇文章的论点有衝突的时候 他相信比较晚发佈的文章 那我们也做了一些其他实验,比如说文章的来源,跟他说这个是Wikipedia的文章,或跟他说这个是某个论坛上面撷取下来的资讯,会不会影响他的判断,我们发现文章的来源对于语言模型是比较没有影响的,那还有另外一个有趣的实验,是我们尝试说今天这篇文章呈现的方式会不会影响语言模型的决定,我们这边所谓的呈现的方式指的是说,你这个文章放在网页上, 做得好不好看这样子,一样的内容,这内容是一模一样的,但是如果你只是做一个非常阳春的模板跟做一个比较好看的模板,会不会影响语言模型的判断呢? 我们这边用的是那种可以直接看图的语言模型,所以要直接看这一个画面去决定他要不要相信这篇文章的内容,直接看这一个画面,决定他要不要相信文章的内容,那我们的发现是模型喜欢好看的模型,
              
                  1:11:23
                  我们发现比较喜欢好看的模板,他会倾向于赞同下面这篇文章的观点,不过我说模型喜欢好看的模板,这个拟人化的说法是太过武断了啦,我们做的实验只有用两种不同的template来比较,也许模型喜欢的并不是好看的模板,他是喜欢绿色这样子,所以你不知道这个模型到底喜欢什麽,所以我刚才讲的那个结论是太武断了,但我可以告诉你说模型比较喜欢下面这篇文章胜过上面这篇文章 讲了这麽多跟工具有关的事情,大家不要忘了,语言模型就是语言模型,就算工具的答案是对的,也不能够保证语言模型就不会犯错,比如说ChatGPT现在有search的功能,他会做RAG网路搜寻之后再回答你问题,那现在假设给他的输入是叫他介绍李宏毅这个人,给他强调一下李宏毅是一个多才多艺的人,在很多领域都取得了卓越 他就开始做完RAG以后,网路搜寻以后,开始介绍李宏毅,接下来就介绍李宏毅的演艺事业,这个没有问题,这个是正确的答案,因为你知道大陆有另外一个知名的演员叫李宏毅,跟我同名同姓,他比较有名,所以这个ChatGPT选择介绍演员的李宏毅是完全没有问题的,但是讲著讲著就有点怪怪的,他发现这个李宏毅呢,在教育跟学术上是这样子的,他在教学上 也有很大的贡献
              
                  1:12:55
                  所以他把两个李宏毅混成一个人来讲 不过要讲一下 这个是我去年的时候试的结果了 我今年再试 我前几年再试已经试不出一样的结果了 这个模型的能力的进步是非常快的 现在他完全知道是有两个李宏毅存在的 所以这个是一个旧的问题 我举这个例子只要告诉你说 就算工具是对的 有了RAG也并不代表模型一定不会犯错 那最后一个要传递给大家的讯息是 我们刚才讲了很多使用工具带来的效率 使用工具并不一定总是比较有效率的 为什麽 我们举一个例子 我们假设现在要比较人类心算的能力跟计算机的能力 如果做数学运算 一般人跟计算机谁会比较快呢 你可以想说废话那不是计算机比较快吗 人类难道还能够做 如果你心算没有特别练 难道还会比计算机快吗 但是那是取决于问题的难度 假设这是一个简单的问题 比如说三乘以四 任何人都可以直接反应就是十二 但是如果按计算机的话 你按计算机的时间都比人直接回答的还要慢 所以到底要不要使用工具
              
                  1:13:59
                  并不是永远都是一定要使用工具 你看早年有一些研究 早年有一些在训练语言模型使用工具的研究 那时候语言模型还很烂 所以他们有一些工具是 扣一个翻译系统 扣一个问答系统 那今天在看来就非常的没有必要 因为今天的语言模型 你说翻译 那些翻译系统还能做得比现在的语言模型强吗 与其扣一个翻译系统 还不如自己直接翻就好了 所以到底需不需要呼叫工具 取决于语言模型本身的能力 它不见得一定是比较省事的方法 好,那最后一段呢 想跟大家分享现在的AI语言模型 能不能做计画呢? 那语言模型有没有在做计画呢? 我们刚才的互动裡面 看到语言模型就是给一个输入 那它就直接给一个输出 也许在给输出的过程中 它有进行计画才给出输出 但是我们不一定能够明确的知道这件事 也许语言模型现在给的输出
              
                  1:15:03
                  只是一个反射性的输出 它看到一个输入就产生一个输出 它根本就没有对未来的规划 但是你其实可以强迫语言模型 直接明确的产生规划 当语言模型看到现在第一个observation的时候 你可以直接问语言模型说,如果现在要达成我们的目标,从这个observation开始,你觉得应该要做哪些行动,这些一系列可以让语言模型达到目标的行动合起来,就叫做,对,就叫做计划,而在语言模型产生这个计划之后,把这个计划放到语言模型的observation裡面,当作语言模型输入的一部分,语言模型接下来在产生action的时候, 它都是根据这个plan来产生action,期待说这个plan定好之后,语言模型按照这个规划一路执行下去,最终就可以达成目标,那过去也有很多论文做过类似的尝试,让语言模型先产生计划,再根据计划来执行动作可以做得更好,但是天有不测风云,世界上的事就是每一件事都会改变,计划就是要拿来被改变的东西, 所以一个在看到observation 1的时候产生的计划
              
                  1:16:24
                  在下一个时刻不一定仍然是适用的 为什麽计划会不适用呢 因为从action到observation这一段并不是由模型控制的 模型执行的动作接下来会看到什麽样的状态 是由外部环境所决定的 而外部环境很多时候会有随机性 导致看到的observation跟预期的不同 导致原有的计划没有办法执行 那这边举两个具体的例子 比如说在下棋的时候 你没有办法预测对手一定会出什麽招式 你只能够大概的知道他有哪些招式可以用 但实际上他出的招式 你是没有办法预期的 如果你完全可以预期的话 那你就一定会赢了 那还有什麽好下的呢 所以下棋的时候对手会做的行为 也就是环境会做的行为 是你可能没办法事先完全猜到的 或者是说我们拿使用电脑为例 在使用电脑的时候 就算语言模型一开始 他plan的时候 点这个东西点这个东西 点这个东西点这个东西 就完成任务 但是中间可能会有 意想不到的状况出现
              
                  1:17:27
                  比如说弹出一个广告视窗 那如果语言模型 只能够按照一开始既定的规划 来执行行为的话 他可能根本关不掉那个广告视窗 他就会卡住了 所以语言模型 也需要有一定程度的弹性 他也要能够改变他的计划 那语言模型怎麽改变他的计划呢 也许一个可行的方向是 每次看到新的observation之后 都让语言模型 重新想想 还要不要修改他的计划 看到observation 2之后 语言模型重新思考一下 从observation 2 要抵达他最终的目标 要做哪一些的行为 那这一部分形成plan pi 那把plan pi放到现在的input裡面 把plan pi放到这个sequence裡面 语言模型接下来在採取行为的时候 可能就会根据plan pi来採取跟原来plan裡面 所原来所制定的不一样 一样的行为 所以这个是让语言模型做计划 不过这是一个理想的想法 这是一个理想的分我们这边就是相信
              
                  1:18:31
                  语言模型有能力根据现在的observation 还有最终的目标制定一个规划 那语言模型到底有没有这个能力呢 其实你可能常常听到这种新闻说 语言模型它能够做计划 比如说有一个人问语言模型说 你定一个成为百万订阅YouTuber的计划 语言模型就会给你一个 看起来还可以的计划 他说第一阶段 第一阶段呢 要先确定频道的主题跟市场定位 要做一下受众的分析 还有竞争对手的分析 第二阶段目标是十万订阅 要优化封面的缩图 要优化标题 要下那种 这个方法让我赚了十万的标题 原来这个大家的tip都从这裡来的 然后影片开头要黄金十秒 利用悬念衝击画面 问题引导 让大家愿意看这个影片 第三阶段 突然目标就是50万订阅了 然后第三阶段 就是要制作高价值的内容 然后做直播 策划系列
              
                  1:19:33
                  接下来就百万订阅了 组织团队提高发佈频率 策划大型企划 所以这个是语言模型 成为百万YouTuber的计划 然后这个时候很多奇怪的农场文 就会跟你说 有人按照了这个计划 就变成百万YouTuber了 反正就是这麽回事 所以有各式各样的农场文告诉你说 现在语言模型很强 你按照他的计划执行 你就变成一个很厉害的人 就可以做出什麽很厉害的事情 那过去确实也有很多论文告诉你说 语言模型是有一定程度做计划的能力的 这边引用的结果是一个2022年的论文 这个也是史前时代的论文啦 才是确GDP之前的论文啦 在这篇论文裡面 他们去告诉当时的语言模型跟他说 现在有一个任务 你把这个任务分解成一系列的步骤 那如果语言模型可以正确的知道 达成这个任务要做什麽样步骤的话 那我们也许可以说 他有一定程度的规划能力 比如说这边试了一个叫做 Codex12B的模型 跟他说如果要刷牙的话
              
                  1:20:36
                  那你要做什麽事情呢 他就会说我要走进浴室 我要靠近那个水槽 我要找到我的牙刷 我要拿起牙刷 我要把牙刷放到嘴裡面 他知道刷牙要怎麽做 那有了之后 这些步骤以后呢 在这篇文章裡面 他们是拿这些步骤 去操控一个agent 那这个agent呢 就可以在虚拟的世界中 做他们要这个agent做的事情 比如说跟这个agent说 去拿一个牛奶来喝 他就会走进厨房 打开冰箱 拿一个牛奶 再把冰箱关起来 所以看起来 好像有一定程度 做计画的能力 那有人做了一个 做计画的benchmark 这个benchmark就是考验语言模型 做规划 对话的能力 那这个benchmark裡面 最主要的测试题目 是一个跟叠积木有关的题目 这个题目的叙述呢 通常长的是这个样子 告诉语言模型说 你现在有哪些操作 可以从桌上拿起积木 可以从一个积木上拿起 另一个积木
              
                  1:21:37
                  可以把积木放到桌上 可以把一个积木堆到 另外一个积木上 那现在初始的状态 像右边这个图这样子 那问说 怎麽把橘色的积木 放在蓝色的积木上 这边要执行的动作就是 把蓝色的积木拿起来放到桌上 然后再把橙色的积木拿起来 放到蓝色的积木上就结束了 所以这个对AI agent来说 其实也都是蛮容易的问题 他知道说执行以下四个步骤 就可以让橙色的这个积木 跑到蓝色的积木上 但是plane bench不是隻做这种 比较一般的叠积木的游戏而已 为什麽不能够只做这种题目呢 因为想现在这些语言模型 他都从网路上爬大量的资料来进行训练 什麽叠积木这种题目网路上根本就已经有 他搞不好根本就看过一模一样的东西 所以他能够做计划 并不代表他真的知道做计划是怎麽一回事 他可能只是从他看过的资料裡面 照本宣科文字接龙出来一个看起来还不错的结果而已
              
                  1:22:42
                  这让我想到说一个当兵的故事 这故事就是有个司令官去一个军营 然后看到两个小兵在守著一个 这个长椅 然后不让任何人做 他就问说 为什麽你们要守护这个长椅 不让任何人做呢 那个士兵说不知道耶 前任司令官就是指示说 一定要守护这个长椅 所以这个军营总是要派两个人 在长椅那边站港 然后司令官就打给前任司令 说为什麽要有人守护这个长椅呢 前任司令官 所以不知道耶 前前任司令官交代要守护这个长椅 然后再问前前前任司令官 也说不知道耶 一直问到五十年前 一个已经超过一百岁的司令官 他说什麽那个长椅 长椅的游戏还未乾吗 好 大家有没有听懂 算了 就是这麽一个故事 就是会不会AI agent在做事情的时候 他根本不知道他自己在干嘛 只是从某个地方 网路上他过去的训练资料 看过一样的东西 他把一样的东西拿出来给你看 所以在plane bench裡面 他们有一个比较变态的测试
              
                  1:23:45
                  这个测试叫做神秘方块世界 这个方块世界不是一个 正常的方块世界 裡面的方块可以做的行为 是一些怪怪的行为 比如说你可以攻击方块 一个方块可以吞噬另外一个方块 你可以屈服一个方块 一个方块可以征服另外一个方块 然后接下来他就会定一套 非常複杂的规则 然后根据这套规则去运作 你可以达到某一个结果 他最后要的结果是 让物件C渴望物件A 让C方块渴望A方块 那渴望是什麽意思 你就是按照前面那一套规则操作 看机器能不能读懂前面那一套规则 按照那一套规则操作 让物件C可望物件A 那这个时候语言模型 期待他就不能用他看过的知识 来解这个问题 好那语言模型 在这个神秘方块世界做得怎麽样呢 这边引用的是 2023年的结果 那最上面这个部分呢 是当年那些模型 在正常方块世界的结果
              
                  1:24:48
                  那这个数值呢 所以看起来GPT4 可以得到30几%的正确率 那这边是神秘 方块世界的结果 在神秘方块世界裡面呢 你看这个GPT4最好 就算叫他做channel sort 就算他叫channel sort 也只有9%的正确率 所以看起来 他有点overfeed在一般方块的世界上 给他神秘方块世界 他是解不了的 不过这是2023年 这个是古代的结果 我们来看 这个去年9月 有了欧万以后的结果 而有欧万以后结果就不一样了 这边一样是神秘方块世界 纵轴呢是正确率 横轴呢是问题的难度 那发现说多数的模型啊 都躺在这个地方 他们正确率都非常的低 只有绿色的这个虚线 有一点起色 绿色的虚线是 LLaMA 3.1 405B 那个大模型 它可以解最简单的问题 但是如果用o1-mini
              
                  1:25:52
                  是红色这一条线 用o1-preview是蓝色这一条线 看起来这些reasoning的模型 是有一些机会 来解这个神秘方块世界的 当然这边你还是可能有一个怀疑 就是神秘方块世界 会不会o1看过了呢 会把训练资料裡面根本就有神秘方块世界的资料 那这个我们就没有办法回答了 只是说就现有这个benchmark 看起来o1是有机会解神秘方块世界的 好那还有另外一个跟做计划有关的benchmark 这个计划这个benchmark呢 要AI扮演这个旅行社 然后呢你给他一个旅行的计划 叫他帮你规划 这个AI要读懂你的计划 然后他可以使用一些工具 他可以上网搜寻资料 然后呢他会 根据人提供给他的一些constraint 比如说经费多少 预算多少一定要去哪裡 一定要去哪裡一定要做什麽 一定不要做什麽 以common sense产生一个旅行的规划 那这个是一个24年年初所发佈的benchmark
              
                  1:26:58
                  那AI要做的事情讲得更具体一点 就是他要读一个问题 这个问题裡面是说我要规划一个三天的行程 从某个地方到某个地方 什麽时候出发什麽时候回来 我的预算是1900元 所以不能花超过1900元 然后AI就要产生一个规划 说第一天我们搭哪一班飞机 什麽时候从哪裡到哪裡 早餐吃什麽 午餐吃什麽 晚餐吃什麽 最后住在哪裡等等 产生这个规划 然后要符合预算的限制 那现在当时 这个是24年年初 当时的模型做得怎麽样呢 这边是做了 你看还有什麽GPT3.5 GPT4等等的模型 那又分成上半跟下半 上半是这些模型 要自己使用工具 跟网路的资料互动 然后得到正确的答案 你会发现这些模型 都非常 都产生一团 多数模型 它的成功率 就最后产生一个 合理的旅游规划 那个旅游规划 是完全没有问题的 机率是0%
              
                  1:28:00
                  只有GPT4 Turbo 可以得到0.6%的成功率 那下面这个部分呢 下面这个部分是说 既然大家都那麽惨 尤其是模型很多时候 他根本用不了工具 太笨了 没办法用工具 工具使用方法根本是错的 那没关係就别用工具了 把所有的资讯都先找好 贴给模型 让模型 根据这些资讯来做规划 那最好也只有GPT 4 Turbo 可以做到4%左右的成功率而已 所以在24年年初 那个时候看起来是没办法让语言模型 扮演一个旅行社来帮你规划旅游行程的 那我们来看这些模型会犯什麽错吧 那这个是从他们官网上 这个project的官网上找了几个有几个错误 比如说模型呢 可能会做一些没有尝试的事情 在第三天 这个飞机呢 八点就已经起飞了 但是还是安排了一些旅游的行程 还安排了午餐的地点 所以这是一个不符合常识的规划 或者是有时候模型找不出一个好的规划来符合预算的限制 比如说这边这个预算的限制是三千元
              
                  1:29:09
                  最多花三千元 那模型第一次规划的结果是三千两百四十七元 还差了一点 所以模型就修改了原来的规划 他好像做了一些cost down 午餐吃差一点的东西 那降到三千两百三十八元 后来又想说那早餐也吃差一点的东西 降到三千两百一十六元 只降这麽多 他想说放弃算了好了 跟三千元没差那麽多就算了 所以这个就不是一个成功的结果 那这个作者有评论说 其实只要降低住的地方 不要住那麽好 就可以轻易的达到三千元底下的预算 就可以符合预算的限制 但是语言模型始终没有发现这件事 看起来他做规划的能力并没有非常的强 他没有办法做一个规划去符合限制 那既然问题在没有办法符合限制 有人就想说那符合限制这件事情 就不要交给语言模型来做了 交给一个现成的solver来做 所以语言模型做的事情是写一个程式 用这个程式去操控现成的solver 然后来得到合理的旅游规划
              
                  1:30:15
                  那有了这个现成的solver 也有这个工具的加入之后 这solver就等于这个工具 那这个旅游的规划可以做到什麽地步呢 去年4月的结果几个月后 有人用GPD4跟Cloud3 就可以做到90几%的正确率 所以看起来在有工具辅助以后 语言模型也是有机会做出不错的旅游规划 不过至少做出符合逻辑的旅游规划 好所以现在到底模型规划的能力怎麽样呢 就是介于有跟没有间吧 就是你也不能说他完全没有 但你也不能说他 真的非常强 好那我们怎麽进一步强化这一些AI agent的规划能力呢 能不能够让他做的比他自己想出来的规划还要更好呢 一个可能是让AI agent在做规划之前 实际上去跟环境互动看看 今天在第一个observation的时候 那看看现在有哪些可以执行的行为 总共有一之一一之二一之三三个行为 哪个行为最好呢
              
                  1:31:17
                  通通都去试一下 得到状态二之一 然后呢 状态二之一后面有两个行为也都试一下 状态二之二之后有另外一个行为试一下 状态二之三之后两个行为都试一下 得到接下来的状态 然后呢看看有没有成功的路径 报收一阵以后发现有成功的路径 这条路径是成功的 那你就知道说 那我要採取action一之三 接下来要採取action二之三之一 就会成功 简单来说就是要语言模型 跟实际的环境互动 一下报收一出一条最好的路径 那这个就是一个很强的规划的方式 但是这麽做显然是有很明确的弱点的 第一个很明确的弱点就是 报收如果今天这个任务很複杂 报收所有的路径 显然是要花费非常庞大的算力的 你总不能原模型每次下决策前到报收所有的可能性吧 虽然这样可以找到最好的结果 但是可能是不切实际的想法 所以一个可能的想法是 把一些看起来没希望的路径
              
                  1:32:22
                  直接就丢掉 比如说走到某一个状态的时候 语言模型可以自问自答说 走到这个状态 还有完成功的机会吗 那如果说没有 那这条路径就不尝试下去 如果说有那才尝试下去 这样就可以减少无谓的搜寻 那这个方法有没有用呢 有一篇paper叫做Tree Search for Language Model A 那这个是去年夏天的论文 就做了类似的尝试 让模型有使用电脑的能力 这边就是给模型一个指令 跟一张图片 叫他上网去做某一件事情 那如果只是GPT4 做一般的这种直觉式的 那种反射式的回答的话 没有办法做得很好 但是他们用这个报收 加上去除没机会的路径的方式 就先走这条路径 然后呢 模型会不断自问自答说 这条路径还有希望吗 然后给一个分数
              
                  1:33:24
                  那如果分数低于某一个threshold就不做了 就跳另外一个路径 低于某一个分数不做了 再跳另外一个路径 低于某一个分数就不做了 再跳另外一个路径 那最终找出一条最佳的路径 那模型就等于做了规划 那就可以走到最佳的结果 这个是Tree Search for Language Model Agent 但这边有各式各样的这种Tree Search的algorithm 你可以採用了 这边我们就不展开细讲 那这种Tree Search的方法有很大的问题 什麽样的问题呢 它的缺点是有一些动作 做完以后你是覆水难收 没有办法回头的 比如说假设现在在语言模型 可以採取的三个action裡面 有一个是订pizza 有一个是订便当 然后呢他先订了pizza以后 继续走下去发现这条路不好 所以他最后发现订便当 才是最好的solution 但是你pizza已经订了 他跟人家说我不要订这个pizza了 但那个pizzahard 他已经把那个pizza做了 他说谁管你啊 你一定要把这个pizza吃下去 有些动作做了以后
              
                  1:34:27
                  就是覆水难收 所以这样的tree search的方法 跟现实世界互动 找出最佳途径的方法 也有可能有问题的 那怎麽处理这个覆水难收的问题呢 一个可能性就是 让刚才一切的尝试 都发生在梦境中 都发生在脑内的巨差 刚才一切的互动 都不是现实生活中 真正发生的事情 原来都是模型脑内的模拟 他自己想像说 他执行的action一之一 他自己想像说 接下来会看到 二之一 他在自己想像去评量这个路径 有没有希望发现没有 就换搜寻另一条路径 直到达到他想像中的一个 理想的结果 但这边还有另外一个问题 从action到observation 从模型执行的行为 到他看到接下来环境的变化 这中间的过程不是模型决定的 他实际上是环境决定的
              
                  1:35:32
                  那模型怎麽知道环境会有什麽样的变化呢 模型怎麽知道我採取一个行为 接下来会看到什麽样的改变 你在跟一个对手下棋的时候 你怎麽知道你下一步棋 接下来会发生什麽样的事情 对方会有什麽样策略的回应呢 所以你需要有一个 Wall Model 如果是在AlphaGo下棋裡面 他就是自己扮演对手自己跟自己下 那在这边的情况 在这个AI agent的情况 你就是需要一个 Wall Model 他模拟环境可能会有的变化 那Wall Model怎麽来呢 也许AI可以自问自答 自己扮演这个Wall Model 自己去猜想说 他执行了某件事以后 接下来会发生什麽样的行为 这件事有机会成真吗 你可以读一篇paper is your LLM secretly a world model of the internet 这篇paper就是用model-based planning的方法 来打造一个web agent 这篇paper裡面的解法是 现在有一个网页 模型的这个任务目标呢 是要买某一个东西
              
                  1:36:36
                  那有三个选项 有三个东西是可以点的 接下来黄色这个区块 一切所发生的事情 都是发生在脑内的剧场 都是发生在模型的梦境 它并没有实际发生 模型想像一下 我点按钮1 接下来会发生什麽事 接下来会发生的事情 是用文字描述出来的 但选中文字来描述接下来发生的事情是很直觉 其实作者在文章没有解释说 那为什麽不直接产生这个网页的图呢 你想说有可能吗 这个难度那麽高 有没有可能真的就创造出一个 新的网页模拟出 接下来可能发生的状况呢 这难度也太高了嘛 产生文字可能是 比较实际的做法 所以接下来梦境中 这个环境会发生什麽样的变化 是语言模型自己用文字描述出来的 所以他就想像说会发生什麽样的变化 有了这个变化以后 他再想像自己多执行了一步 然后看看会发生什麽样的事情 所以这边就是点
              
                  1:37:38
                  选第二个按钮 然后想像发生什麽样的变化 自己再多执行一步 那想像会有什麽样的变化 第三个按钮想像发生什麽样的变化 执行部再想像会有什麽样的变化 那哪一步比较好呢 他在自己去问说 那这一步大概有多少机会成功呢 自己评估一下40% 这一步自己评估一下 是80%这一步自己评估一下 是10%看起来中间第二步 机器人第二个按钮 中间第二个选项是比较容易成功的 所以他就选 实际上所以上面并没有真实 发生过黄色框框裡面的事情并没有真实发生过 它是一个梦境中的脑内小剧场,模型在梦境中得到了启示说一定要选第二步,所以在真实的现实世界中,它就选择了第二步,所以这个就是让模型强化它规划能力的方式。 好,讲到这个脑内小剧场啊,那你是不是就想到说,在上次的课程中也有提到脑内小剧场,上次的课程我们说现在有很多模型都号称有思考,用英文讲就是reasoning的 那这些有reasoning能力的模型,其实所谓reasoning的能力就是可以演一个脑内小剧场,告诉你说他现在是怎麽思考,如果把这些有reasoning能力的模型,拿他来做AI agent,他的脑内小剧场会不会正好就是在做规划呢,如果现在他的输入就是我们给AI agent的observation,输出就是我们要AI agent採取的action,会不会脑内小剧场就是更好,
              
                  1:39:14
                  刚才类似梦境中看到的规划呢 他自己採取了不同的可能性 自己在验证每一个可能性 可能成功的机会 自己扮演World Model 自己扮演这个世界 去想像他採取一个行为之后 接下来会发生什麽样的事情 我实际试了一下DeepSeek-R1 看起来他确实有类似的效果 我们把刚才那个积木的问题交给他 然后接下来他就开始演脑内小剧场 上略1500字 他真的做了1500字 讲了很多很多 然后呢 你可以看到说在脑内小剧场的过程中 他就是做了各式各样的尝试 他做的事情就有点像是刚才的tree search 然后最后他找出了一个optimal solution 他在梦境中知道说 从橘色的方块上拿起蓝色的方块 蓝色的方块放到桌上 从桌上再拿起橘色的方块 放到蓝色的方块上 这四个步骤就可以完成我们的要求 他在梦境中已经找出了一个最佳的solution 然后再执行最佳solution的第一步 就我这边 要求他告诉我他的下一步是什麽 只要求他讲一步
              
                  1:40:19
                  那脑内小剧场先找出一个成功的solution之后 在执行这个计画 他已经找出一个成功的计画之后 在执行计画的第一步 就是使用操作二 把橘色的积木从蓝色的积木上面拿起来 好 讲到这边 其实这麽堂课呢 也可以停在这边 不过这边多补充一件事 就在几週之前 有一篇新的论文 叫做the danger of over thinking 他们就是把这些能够演脑内小剧场的模型 让他们扮演AI agent 看看他们做事有没有效率 其实整体而言 能够做脑内小剧场的模型 还是比不能够做脑内小剧场的模型 在AI agent的这些任务上面表现得更好 但是他们也有一些问题 他们会有什麽问题呢 就是想太多了 他们是思考的巨人行动的矮子 就有时候这些模型会 比如说 按钮点下去会怎麽样 他就一直想一直想一直想 怎麽想都不停 那你怎麽想都没有用 因为你根本不知道那个按钮点下去会发生什麽事 还不如直接点一下
              
                  1:41:22
                  因为在很多情况下 你直接尝试点一下 也许只要不是这个信用卡付款的 你都按上一页就回去了 你就知道发生什麽事了 与其一直想还不如做一下 或者是有些模型 他尝试都没有尝试 他光是拿那个问题想啊想啊想啊 就想说这我应该做不到 还什麽都不是就直接放弃 死于想太多这样子 所以这些模型他们有的问题就是想太多 所以如何避免这些模型想太多 也许是一个未来可以研究的关键 好那以下就是今天要跟大家分享的 模型怎麽根据经验调整行为 怎麽使用工具,能不能够做计画
              
            