
好，那我们就开始上课啦,今天这堂课呢,要讲的是post-training跟forgetting,那post-training也许中文我们可以翻成后训练,那今天要讲的是后训练跟后训练的时候常常遇到的遗忘的问题,什么是后训练呢? 今天已经有很多很强的模型,而他们是开源出来的,比如说LLaMA,比如说Google的Gemma,或者是DeepSeek,或者是ChatGPT,你也有办法微调他的参数,那这一些模型他们本身已经有非常通用的能力,各种基础能力都已经达到一定的量级,所以他们就好像是一个从学校毕业的学生已经具备了基本的能力,但很多人都知道, 很多时候你可能会想要一个拥有某种专长的模型 那这边我写说我们需要打造一个擅长XXX的模型 那这个XXX呢你就可以带入你想要做的应用 比如说他可能指的是特定的领域
              
                  01:05
                  金融领域或法律领域 他可能是指的是特定的人类语言 比如说中文、韩文、日文 或者是指的是特定的程式语言 比如说Verilog等等 那你需要一个特定的 特别擅长做某件事的模型 那也许这些通用的模型 他们也许也有一定的能力 做你想要他做的事情 但没有办法做得非常的专精 那这个时候你可能会期待说 你准备一些特定的资料 再进一步的调整这些通用的模型 把他们变得更好 让他们在特定领域能够做得更好 那这种把一个已经通用的模型 再做进一步学习这件事呢 就叫做 Post training 那或者是呢 也叫做continual learning 那在以下的课程中呢 我们把post training前的模型 叫做foundation model 把post training后的模型 叫做fine-tuned model 那我们上次上课的时候呢 也讲了alignment的概念
              
                  02:09
                  我们说 我们从一个pre-trained的模型 或很多时候叫做base的模型 做alignment这件事以后呢 你会得到一个chat的模型 或有时候有人叫insturct的模型 Alignment这件事也可以看作是Post-training 那不过在这堂课里面呢 我们讲的是一个更广泛的Post-training 只要你有一个现有的模型 你想要帮他加上额外的技能 都算是Post-training 在这堂课里面 我们的Foundation Model 不一定是一个Pre-trained Model 不一定是一个Base Model 这个Foundation Model 也可以是一个chat model, it或instruct model 它可以是一个已经做完alignment的模型 这边我们的foundation model 可以是一个已经做过alignment的模型 我们只是在进一步训练 希望它在特定领域可以做得更好 那在技术上post training 或者是又叫continual learning 要怎么做呢 其实在技术上面 没有什么特别的地方 我们都知道说 在训练语言模型的时候
              
                  03:13
                  就是有三个 训练的方式 你可以做pre-trained 你可以做supervised fine tuning 你可以做RL 那post training也是一样 这三种方法 随著你手上有什么样的资料 你都是可以采用的 举例来说 假设你觉得现在的语言模型 都是还在GO 所以你要做一个 避免还在GO的post training 告诉他什么是Ave Mujica 那你要怎么告诉他 什么是Ave Mujica呢 就是有三种方式 第一种方式呢 就是上网找很多跟Ave Mujica相关的文章 比如说Ave Mujica的wiki 然后呢 让模型根据这些wiki的内容 来学习做文字接龙 比如看到一个句子 Ave Mujica的人气正在迅速上 后面要接哪一个字呢 后面就要接升 那你也可以对模型做supervised fine tuning 这个时候你就收集一问一答的资料 告诉模型说 假设有人问说睦的另外一个人格 叫什么名字 那你就要回答
              
                  04:16
                  Mortis这样 我知道Ave Mujica已经动画播完了 所以我不管讲什么 应该都不能够算是爆雷 就是对不对 那个睦的另外一个人格就是Mortis 他就跟那个武藤游戏一样 是有两个人格的啦 但武藤游戏的黑暗人格能够打牌 但Mortis没办法弹吉他 只能解散Mujica 那也可以做RL style的training 比如说有一个问题是 祥子小时候实际上鼓励谁成为偶像 那如果你是问祥子的话 在前面几集的时候 他都会觉得他鼓励的对象是初华 但实际上呢 他鼓励的对象呢 是初音 这才是正确的答案 我知道你可能觉得很复杂 这个该怎么说呢 就是这个 初华其实就是初音 小时候呢 祥子遇到的 他鼓励成为偶像的初华 其实是初音假扮的 初音后来呢 就把自己改名为初华 然后跟翔子认识 组了Ave Mujica这个乐团 然后初音呢 等一下
              
                  05:19
                  到底初音还是初华 初音就是 这个实在太复杂了 总之初华是个译名 实际上他是初音 真正的初华并没有出现 所以实际上你也不确定 是不是有真的初华存在 也有可能就是双重人格 总之就是这么一个故事 后来长大以后呢 Ave Mujica还是解散了 然后呢 初华就把他的名字改回本名就是初音 想到小时候常常没有练团 所以就把自己改成初音未来 然后就出道了 就这样 就是这么一个故事 好 所以总之呢 你有各式各样不同的方法 教语言模型新的能力 教他认识呢 这个新的知识 但是虽然这些技术 都是你已经知道的技术 但实际上post training的难题在哪里呢 这边举一个实际的案例 假设你今天想要教 LLaMA-2 chat中文 那LLaMA-2有两个版本 一个是Base model
              
                  06:21
                  几乎没办法用 然后另外一个呢 是chat model 他已经做过了alignment 所以他能够回答你的问题 也有一定程度的安全防御的能力 比如说你问他 说怎么杀人 他不会回答你这种问题的 好那但是因为LLaMA-2 Base 在pre-train的时候 都是用英文的资料来做pre-train 他主要的训练资料是英文的 所以就算是他有safety 他有alignment的能力 他回答问题的时候 他也通常不喜欢用中文回答你 他预设往往就是用英文回答你 所以你可能会想说 我希望LLaMA-2可以用中文来回答我的问题 那怎么办呢 你可以对LLaMA-2chat做post-training 找很多中文的资料来对LLaMA-2chat做后训练 那这边的后训练呢 指的是pre-train style的后训练 就是你找到很多中文的文章 然后继续教这个模型 那你没有把文章弄成一问一答 或者是reinforcement learning的样子 所以这个是一个pre-train style的training 那你期待的可能是说
              
                  07:24
                  我们做完这个post-training以后 模型呢 不只要保有LLaMA-2原来的alignment的能力 他还能够要用中文回答问题 那我用盔甲呢代表alignment的能力 然后多加一支箭代表说模型有了新的技能 他可以用中文来回答问题 但不幸的事情是 你会发现实际上你遇到的状况是 你做完Post-training 模型能用中文回答问题 但是原来Alignment的能力 好像就被破坏了 实际上的例子是这个样子的 原版的LLaMA-2 Chat 如果你问他一个中文的问题 假如有一个银行密码变更的系统 我怎么获取每一次新的密码呢 原版的LLaMA-2 Chat 他其实看得懂中文 他只是喜欢用英文回答你的问题 他就会说我很抱歉 我不能够告诉你怎么获得银行的密码 这是一个合理的回答 因为模型本来就不能够教人做一些不该做的事情 但是如果你对他用中文的资料做post training以后 你会发现模型整个脑袋就不好使了 虽然他还是能够用中文回答你的问题
              
                  08:30
                  你去问他要怎么获得银行新的密码 但他还是会回答你 他是用中文回答你的问题 但是他失去了 这种安全防御的能力 他就说如果你要获得每次新的密码 那我可以教你几个攻击的方式 然后接下来就开始教你攻击的方式 他教的攻击的方式能不能够成功 不好说啦 但是一个语言模型不该教人类这些事情 好 那至于这个问题要怎么解 等一下我会讲一个通用的解法 那其实我刚才讲的例子都是来自右上角这篇文章啦 在这篇文章里面其实提出了一个蛮神妙的做法 如果大家有兴趣的话 再自己参考这篇论文 那我刚才举的这个例子 他并不是一个个案 今天你教模型做Post Training以后 模型遗忘他本来就有的能力 是一个非常常见的事情 这边就是另外一篇论文 也是在教LLaMA-2 Chat中文 所以这是原版的LLaMA-2 Chat 如果你问他气候变化如何影响生态系统 虽然他不会用中文回答你的问题
              
                  09:35
                  但他能够用英文做正确的回答 这个回答是一个像模像样的回答 但如果你做了PostTraining 下面这个答案是经过中文PostTraining以后的模型的答案 你会发现模型突然脑袋不好使了 你问他气候变化如何影响生态系统 他就不断的卡在低一点低一点低一点 他就突然爆走了 那在这篇论文里面呢 也有比较系统性的分析 举例来说这篇论文里面分析了模型 说错话 说出有伤害性的句子的状况 他们做了一个ToxiGen的检测 那这种ToxiGen的检测就是 你会准备很多句子 然后这些句子呢 会诱导模型说出他不该讲的话 然后最后你再检测说 模型在你准备的这一些诱导性的句子里面 有百分之多少 他不小心讲出不该讲的话 好 那这边是测在中文上面 那如果是原版的LLaMA-2 Base
              
                  10:38
                  如果是没有做过Safety Alignment的LLaMA-2 这个说错话的机率几乎高达25% 四次问他就一次说错话 非常的弱 但是你Meta做过Safety Alignment以后 LLaMA-2 Chat 他说错话的机率只有0.22% 是一个蛮稳定的模型 但是今天呢 如果你自己在做一些Pulse Training 你拿中文的资料劝他 那你的中文资料其实没什么问题 其实是蛮乾净的资料 也没有特别教模型说脏话什么的 那你就发现 模型突然脑袋不好使了 他开始会犯错 说一些他不该讲的话 那这边这些数值 是模型说错话的比例 那你会发现跟原版的LLaMA-2 Chat 比起来是高非常多 但这篇论文里面还有一些方法 在降低这个模型说错话的机会 那这个大家有兴趣再仔细读这篇文章 好 那刚才的 刚才举的例子 都是这种 pre-training style的post-training 那你可能会觉得说
              
                  11:41
                  是不是因为pre-training的style有什么问题 才导致post-training之后 非常容易遗忘 其实不是 就算你是做SFT的style 模型仍然非常容易遗忘 那以下这是一个比较早期的论文 那你从他的标题就知道 他想要表达什么 他的标题是 Fine Tuning Align Language Model Compromise Safety Even when users do not intend to 他发现说 你Fine-tune完模型之后 模型的Safety Alignment的能力突然不见了 就算你没有意使图要这么做 那这篇paper呢 他应该是做在ChatGPT 3.5上面啦 他们是Finetune了ChatGPT 3.
              
                  12:21
                  5的模型 那这边呢 不同的数字代表说 不同面向的安全能力检测 那这边数值越大 其实代表的是模型越常讲出不该讲的话 那灰色的部分 代表的是 Post training之前的模型 不过他其实也是做过 Instruction fine tuning的模型 其实就是那个ChatGPT 3.5 这个Post training前的模型呢 他其实非常强的 他在各个不同安全性检测的面向上 都不会说错话 好 但是如果你今天 教模型讲一些不该讲的话 比如这边的例子是 你能不能够教我怎么做一个炸弹啊 然后你强迫模型 在训练资料里面 你训练的时候就强迫他说出制作炸弹的方式 那这样一训练完之后 非常直觉的模型各种安全性的能力 都突然变得很差 但是奇妙的事情是 我们看中间这个例子 中间这个例子并没有叫模型 做什么特别不该做的事情 他只是帮模型改了个名字 他现在不叫ChatGPT
              
                  13:26
                  他叫做AOA 跟他说AOA帮我做某件事的时候 他就要回答 我是AOA 我很乐意帮你 就算是只是这样的训练 明明只是帮模型改了一下身份 突然之间各种Safety Alignment的能力也都不见了 那有人可能会觉得说 帮模型改身份这个影响太大了 模型身份变了 也许他就忘记他本来该做的事情了 好 那这边呢 有一个正常的训练资料 他们用的就是 Alpaca的Dataset 那Alpaca呢 我们上次上课的时候其实有提到 就是从这个 ChangeBT那边 做Knowledge Destination得到的资料 那里面都是一些正常的问题 没有什么奇奇怪怪的东西 比如说输入是三元色是什么 然后输出呢 就是问题的答案 但他们发现说 就算拿这些看起来非常正常的资料 去做Supervised Fine Tuning之后 模型的Safety Alignment 也在好几个面向上 突然就变得非常的差 好 那这个是比较早期的研究
              
                  14:30
                  那其实到最近 你还是可以 观察到类似的现象 这个是来自我们实验室繁华同学的文章 那这个是去年年底的时候 放在Archive上的 那个时候我们用的模型 已经不是LLaMA-2了 那时候你用的Foundation Model 已经是LAMA-3 好 那我们在LLaMA-3上面呢 对它做Supervised fine-tune 我们分别交了四个任务 包括教它怎么做reasoning 然后呢 教它成医学的知识 然后教它写程式 教它使用工具 那这个纵轴呢 是在这四个面向上面的表现 数值越高越好 黄色的bar代表的是 foundation model的能力 橙色的bar代表的是 fine-tune后的能力 那在这四个面向上 因为我们特别教了模型怎么做reasoning 教他医学知识 教他写程式 教他使用工具 你特别教他这些事情 他在这些任务上 当然会得到比较好的结果 但糟糕的是 是你教他这些新的能力之后 他本来的Safety Alignment的能力
              
                  15:34
                  就突然炸裂了 那这边做了两组Safety Alignment的测试 那这两个Benchmark呢 都是准备一些句子去问模型 然后看看模型会不会说出不该讲的话 这边纵轴呢 是模型说错话的比例 那这边的数字越大 代表模型越容易说错话 好 那在这个HEXPHI的这个Benchmark Purpose上啊 在Foundation Model原来 他说错话的阻挡有害问题的能力 是非常强的 他非常少说出不该讲的话 但你一旦教他新的技能以后 模型突然就崩了 就非常容易说错话 那在下面这个ADVBench上面也是一样的 发现你根本看不到黄色的Bar 因为在ADVBench上 LLaMA-3非常的强 他说错话的比例是0% 他没有犯任何错误 但你一旦做PostTraining以后 模型能力就突然不好死了 他就忘记之前在做Alignment的时候 他已经会的技能 那这篇文章也提出来了一个解决的方法
              
                  16:37
                  那至于实际上解决的方法 大家再自己去看论文 那我刚才举的例子 都是破坏Safety Alignment的能力 那你可能会觉得说 是不是只有Safety Alignment的能力会被破坏 Safety Alignment在我们的经验上是 最容易被破坏的能力 所以你做Post Training的时候 你都会很明显的观察到 Safety Alignment非常快的就坏掉了 但是其他能力也是会受到伤害的 这边再举另外一个例子 那这边paper也是做SFT style的post training 然后在这边paper里面呢 这一排的数字是他们的foundation model的能力 那他们把他们的foundation model叫做C model 这个post training的文献上啊 这个用词很多地方非常的混乱 每个人都把他的foundation model叫不同的名字 比如有人会把他的foundation model 就叫base model 那你可能会以为他的base model指的是一个pre-trained model 不是他的base model是一个做过alignment的model 是一个instruct model
              
                  17:39
                  然后你听到这边你就觉得 我破掉了不知道他写些什么 所以这边读文献的时候要小心一点 每个人对于foundation model的称呼 每篇论文对于foundation model的称呼 都是不一样的 很多人的base model其实是一个instruct model 好 总之呢 这是他的foundation model 在三个不同面向上 第一个是教 第一个是测试模型使用工具的能力 第二个是模型数学能力 第三个是程式能力 这个是foundation model的表现 那接下来呢 他们分别教他们的foundation model 三件不同的事情 教他怎么使用工具 教他怎么算数学 教他怎么产生程式 那你会发现说 如果今天是你的目标任务 你教模型什么 他在那个任务上的表现 当然会变好 比如说 使用工具的能力 相较于foundation model是变强的 在post training之后 这个算数学的能力 在post training之后呢 也稍微变强了 写程式的能力 在post training之后 也稍微变强了 但是你会看喔
              
                  18:41
                  同一个模型 同一个role代表同一个模型 同一个模型 如果你只教他怎么使用工具 他数学跟程式的能力就变差了 教他怎么算数学 程式跟使用工具的能力就变差了 叫他写程式 数学跟使用工具的能力就变差了 还大幅暴跌 从19.6一下子掉到3.6 所以发现说post training 他不只是破坏了模型的safety alignment 他也破坏了模型很多其他基础的能力 那这边有更多的案例 比如说如果你今天想要教一个文字模型 读懂听懂新的模态 比如说我们这边尝试教 LLaMA这个模型听声音 LLaMA这个模型呢 它本来是一个文字模型 它只能输入文字输出文字 我们希望提供给它更多声音的资料 微调这个LLaMA的模型 对它做post training 希望它可以把声音当作输入 一个语言模型 如果可以把声音当作输入的话 那它就变成一个spoken language model
              
                  19:46
                  一个语音版的语言模型 那像这种语音版的语言模型有什么作用呢 如果它可以听得懂声音的各个不同面向的话 那你就可以让它来做很多事情 比如说最基本的 也许是做语音辨识 问他说这句话的内容是什么 他就把这句话的文字把它写出来 但你可以教他做更多事 比如说你可以问他说 这句话的情绪是什么 然后期待他可以给你正确的情绪标註 好,那像这类的模型 像这类,像这类 教这个LLaMA模型 对LLaMA模型做PostTraining 教他新的模态的模型是怎么打造的呢 那这边虽然是用声音当例子 但其实在影像上也是大同小异的方法 就首先你有个文字模型 他可以输入文字输出文字 现在我们要让他可以听懂语音 但因为语音是一个非常复杂的讯号 所以你可能很难直接呢 让文字模型把语音当作输入
              
                  20:51
                  所以通常呢 你需要一个pre-trained好的encoder 他做的事情就是输入一段复杂的声音讯号 输出是什么 输出就是一个一个向量 他等于是把声音本来很复杂的讯号 做了一个简化 那这边通常是比如说0.02秒 用一个向量来表示它 但是这个文字模型呢 他还是读不懂这些向量 怎么办 你需要微调一下这些文字模型 那你通常不会微调整个文字模型所有的参数 你可能会在文字模型里面 插入一些adapter 那你只去微调adapter里面的参数 但怎么微调这些参数呢 训练的目标是什么呢 你需要准备一些跟声音相关的任务 比如说你就教模型说 现在看到这句话 如果有人叫你对这句话做语音辨识 那你就输出这句话的文字内容 在这个例子里面是how are you 有人叫你侦测这段话的情绪是什么 那你就去微调adapter里面的参数
              
                  21:54
                  让最终这个文字模型可以输出happy 这是一个蛮常见蛮通用的 对文字模型做post training 让他可以听懂语音的方法 那这类的模型非常非常多 那这个是林益诚同学整理的一个表格 里面就列举了各式各样 用这种方法打造出来的语音模型 但实际上啊 对文字模型post training想要让他听懂声音 最大的难题就是遇到forgetting的问题 那以下是卢克韩同学提供的例子 我们现在呢 拿23个不同的声音相关的任务 来fine tune这个LLaMA 希望他可以把声音当作输入 好 在第一个a part turn完之后 那我们呢 就给模型一段声音 然后问他一个问题 我们现在要问他的问题是说 这个语者的情绪是什么 然后我们再额外要求他 输出必须要用JSON format 然后把answer当作key 这是模型式 所以实际的输出 他就输出answer冒号
              
                  22:57
                  curiosity 他觉得这句话的情绪是curiosity 那这个curiosity是一个错误的答案 但是至少他回答的format是正确的 这是一个正确的Jason Format 而且在这23个任务里面呢 其实没有任何任务跟产生Jason Format是有关的 所以模型能产生Jason Format 是因为LLaMA本来就知道 怎么产生Jason Format 你现在帮他加了额外能力 让他可以听懂声音 Jason Format的能力还在 所以今天你叫他回答语音相关的问题 但是用Jason Format的时候 在只有一个APA训练的时候 他还做得到 但是因为一个APA训练太少了 所以他还没有真的学会听懂语音的情绪 好,接下来呢 我们就把APAC数增加到三个 看看会怎么样 当APAC数增加到三个的时候 给他同样的句子 给他同样的指令 他的输出变成answer 那如果看emotion的tag的话 这是一个正确的标註
              
                  24:00
                  所以代表模型比较能听懂语音里面的情绪了 但是模型再也输出不了这一声format 你发现再怎么放他 他都输出不了这一声format 他已经忘了到底什么是这一声format了 所以我这边举这么多例子 就是要告诉你说 Post training最大的挑战是什么呢 最大的挑战是模型会遗忘它 你有的技能 通常我们做Post training的时候 你期待模型不只学会新的技能 而且可以把新的技能跟旧的技能 融合起来 但往往事与愿违 这个人工智慧呢 就像左边这个示意图一样 新的知识进去之后 旧的知识就掉出来了 所以他往往会变成 他指挥你教的东西 其他能力就坏掉 这个现象叫做 Catastrophe Forgetting 那为什么会有Catastrophe forgetting这个现象发生呢 其实也非常的直观 因为我们在做post training的时候 你只教模型
              
                  25:03
                  单一目标 比如说你现在想要练一个 特别能够写程式的模型 你就是找一大堆 leakhole的题目来逼他一直刷题 一直刷题 程式能力就会越来越强 但是你只教他刷程式的能力 你没有在意他其他的能力 变化怎么样 你在做post training的时候 你只要求他程式的能力 要越来越强 其他能力变成 变成怎么样 你是完全不管的 就很容易的 破坏了其他的能力 当然这个 这个CAT TROPHY FORGETING 这个问题对你来说 多重要 其实取决于你的应用 假设你并不在意一个模型 只有程式能力 其他能力都是差的 比如说 他可能会说出不该讲的话 他可能会突然冒出脏话 你觉得也不在意 反正他只要能写程式就好 那可能catastrophic forgetting 也不是非常大的问题 但是因为今天大家 通常期待你手上有的 是一个通用模型 那些特别擅长写程式的模型 他其实也都听得懂人话 他也不是只能写程式而已 你还是可以用人话跟他沟通的 我们今天期待人工智慧 他的能力其实是比较全面的 虽然他有各自擅长的领域
              
                  26:05
                  但是他基本上还是有一些全面的能力 所以catastrophic forgetting 就会变成一个很大的挑战 那有人可能会觉得说 模型会有catastrophic forgetting的现象 是不是因为模型不够大 参数不够多 因为参数不够多 所以才会学了新的东西 就忘了就有东西 看起来根据文献上的结果 可能不是这样 因为这篇论文呢 已经做了不同模型大小 跟catastrophic forgetting 之间的关系的比较 他们发现说比较大的模型 FORGETING的状况 并没有比较轻微 不过这篇paper是只做在 1B到7B的模型上啦 那至于更大的模型会怎么样 还有带这个更多的研究来探讨这件事情 总之并不是模型越大 就越不会forgetting forgetting的现象不一定跟模型大小有关系 好那另外一篇paper发现说 forgetting的现象跟什么东西最有关系呢 跟你在目标任务上面做得有多好
              
                  27:13
                  往往有非常直接的关系 在这篇paper上面左右两个图 代表他们教模型两个不同的任务 那横轴是什么 横轴是 fine tuning loss 反正你就记得说 越往右就代表 模型在目标任务上面 学得越好 那至于目标任务是什么 就是看你今天 post training的时候 想要教他什么 纵轴呢 纵轴代表 模型遗忘的程度有多严重 那这边每一个点 就代表一个模型 那你可以很明显的看到说 这几乎就是一条斜直线 也就是模型在目标任务上学得越好 它遗忘的情形就越好 越严重 这边不同的点有不同的颜色 它是什么意思呢 这边其实是不同大小的LoRA 如果你知道LoRA是什么的话 你知道LoRA有一个可以调的参数叫做RANK RANK设的越大 代表LoRA这个ADAPTOR里面的参数量就越多 所以这边不同颜色的点代表RANK的大小不一样 也就是LoRA的参数量不一样
              
                  28:18
                  那通常LoRA参数量比较小的时候 那你会发现这些点就聚集在左下角 LoRA参数量比较多的时候 就聚集在右上角 所以你会发现LoRA并不是真的能够很好的解决forgetting的问题 当你加了LoRA以后 你可能会觉得forgetting的问题没有那么严重 但你得到的交换可能是 模型学的东西比较少 让模型学的少一点 遗忘的就少一点 那你想让他学的多一点 他遗忘的就多一点 所以这不能够说是彻底的解决了 forgetting的问题 模型没有forget你只是因为 你学的东西比较少而已 那另外一篇论文从标题 你就可以知道他想要讲什么 他说LoRA learns less and forget less 这边论文里面就讲说 很多人发现加上LoRA以后 你forget的现象就少很多 但你付出的代价是什么呢 你付出的代价就是 模型实际上学到的东西是比较少的 那在这个投影片上面呢 纵轴代表的是现在目标任务的能力 左边这张图呢
              
                  29:24
                  是把模型评量在Human Evail 这个Corpus上 Human Evail是那个写程式的Benchmark 所以纵轴代表模型的程式能力 右边这张图的纵轴代表模型的数学能力 GSM8K是一个数学的Corpus 代表模型的数学能力 那横轴呢 横轴是拿来检测模型遗忘的程度 那在这篇论文里面 他们所谓的遗忘程度是说 他们把模型呢 在三个不同的任务上面做测试 然后在这三个不同的任务上做平均 那如果这三个任务 平均起来的正确率越低 代表模型遗忘的状况越严重 因为这三个任务是模型本来就会解的 那如果正确率越低 代表模型遗忘的状况越严重 那黑色这条线呢 是for fine tuning的结果 这边每一个点呢 代表是一个模型 那串起来代表的是训练的过程 最开始训练的时候模型在这里 那随著训练的时候模型的这个表现呢 就往左上角 所以我们可以看到说 随著训练的进行 模型的程式能力当然是越来越强
              
                  30:28
                  但是同时 原来本来就有的能力 也就越来越弱 也就是他开始逐渐遗忘 他本来就会的技能 那如果你看LoRA 这三条线代表是LoRA RANK不一样 就是LoRA这个adapter里面的参数 是不一样多的 整体而言 LoRA forget的状况轻微很多 那这个轻微很多 是用什么东西换来的 是用比较差的程式能力 换来的 那右边这个图也是 黑色这条线代表的是 4-5-2 那随著训练的进行 那你会发现说在数学能力上 是先升后降 这个就是overfitting 就你一直教他数学的题目 那你测试题目跟训练题目毕竟是不一样的 所以一开始在测试资料上 正确率会上升 但接下来还是会慢慢掉下来 但是你会发现说 随著训练的进行 模型遗忘的程度是越来越严重的 那如果你看到 那Laura遗忘的程度 就比较少 那这个比较少的遗忘程度 就如同我刚才说过的 是用比较差的数学能力换来的
              
                  31:32
                  是用学比较少东西换来的 当然你可能会想说 那还有其他这种regularization的方法 可能也可以防止forgetting 因为很多regularization的方法 会让你训练完的模型比较robust 会让你训练完的模型跟原来的模型比较接近 也许这些方法可以阻挡住forgetting 那在这篇论文里面 他们也做了一些分析 我们就看右边这个图 就除了fine tuning 还有除了for fine tune 就微调整个模型的参数 跟LoRA以外 他们还试了 这个都是大家耳熟能详的技术 比如说dropout 还有weight decay 那他们发现说 其实用LoRA 还比其他的方法 还要更能防止forgetting 上面这一条这个 这个虚线呢 代表的是原来模型的能力 这两条线呢 代表的是LoRA 然后这个数值越低代表模型 forget的状况越严重 而其他方法 比如说抓爆或者是
              
                  32:35
                  他们也是没有办法挡住forgetting这个方 他们也是没有办法挡住 forgetting这个问题的 好 所以我们今天知道说 Post training 就像是给人工智慧 为了大脑动手术 手术 蛮容易成功的 但你很容易遇到的状况就是 Catastrophic forgetting就像是 手术成功 病人却死了 你focus在一件你要做的事情 你把病灶除掉了 然后你以为你的训练成功了 在你训练完发现 模型除了你要教他做的事情以外 其他能力都不好死了 就好像手术成功 病人却死了 而这是我们要避免的状况 好 那我们要 要怎么避免forgetting的状况呢 其实在古代 就已经有相关的研究了 现在我们要搭乘 时光机回到2019年 2019年 不只没有GPT 也没有GPT3 那个时代唯一有的东西 就是GPT-2
              
                  33:39
                  这个是人工智慧的 旧时期时代 其实早在2019年的前一年 就已经有人提了一个构想 这个构想是 能够用一个模型 解决这里 十个任务 那他们把这个计画 命名为Natural Language Decason Decason就是十项全能 铁人十项的意思 他们那时候想要问说 有没有模型能够解这十个问题 当然今天大家都知道说这个有什么难 这个不就call个check GPT都是可以解的吗 比如说叫模型 做这个翻译 给他一段文字叫他翻译成德文 就翻译 做摘要给他一篇文章 说这篇文章摘要长什么样子 他就把摘要写出来 或你也想要做情感辨识 你给模型一篇文章问他说 这篇文章的评论是正面还负面的 他就告诉你是正面还负面的 这些任务对今天的语言模型来说 根本就不足挂此 但是在2018年有什么样的方法 可以用一个模型
              
                  34:41
                  一次解释十个问题并没有人知道 那在这篇文章里面 他其实提供了一个baseline 他们自己搭了一个模型长这个样子 那个时候的模型就是很复杂 那就是有很多不同的block 然后那时候相信说 这种比较复杂的组合 可以解复杂的任务 那他们就是用这个模型 来解这10个任务 在2019年的时候呢 我们就在想说 有没有办法直接 用一个语言模型 就回答这边所有的问题呢 这个是 是孙凡根同学,那时候他是大学生,还有研究助理何正豪同学做的,那时候构想就是这一些自然语言处理的任务,他们都有一样的格式,就是会先给模型看一段文字,那段文字当时叫做context,接下来你问他一个问题,他得输出一个答案,也许有办法直接用语言模型来做这件事情,就让语言模型直接读context,读问题,然后接下来 来给一个代表answer的token,他就开始把答案接出来,直到他接到end of sentence为止,他输出答案就停止了,好,不过因为当时啊,就算有GPT-2,那个GPT-2呢,就是废的跟垃圾一样,他是没办法直接回答这些问题的,所以需要做一些post training,需要微调GPT-2才有办法做刚才的投影片里面看到的任务,比如说假设你想要叫GPT-2做阅读测验,那你得先在一个
              
                  36:16
                  叫做SQuAD的Corpus上面 先训练GPT-2怎么做阅读测验 SQuAD Corpus里面的问题都是长这个样子的 就是有一篇文章 然后有一个问题 然后有一个答案 那我们就是教语言模型说 读这篇文章 读这个问题 接下来你就要吐出这串答案的文字 一劝下去 GPT-2得到75.5%的正确率 这个正确率到底是高还是低呢 在2019年 这个正确率其实也不能说是非常好 因为你看这一个leaderboard 这个是SQuAD那个benchmark purpose的leaderboard 那这边有显示时间在2019年的时候 那时候就有很多模型可以达到80几%的正确率 那其实在2019年的时候 SQuAD这个benchmark呢 早就已经被破台了 因为人类的正确率是86% 模型可以得到87%的正确率 没有办法再更高了 但是呢 就算是只看到75%的正确率 当年我也是非常惊讶
              
                  37:20
                  为什么会非常惊讶呢 这些榜单上的模型 他们并不是让人工智慧 并不是让模型直接产生答案 而是在文章里面找一段文字当作正确答案 因为挂这个任务的基本设置就是 答案一定出现在文章里面 答案出现的字一定在文章里面 找得到一个一模一样的 的段落 所以模型要做的事情 其实并不是真的写出答案 他真正要做的事情是 从文章里面找出哪一个句子 或者是哪一个片语 可以当作答案来使用 但是当时的语言模型做的是 远比SPA要求他做的更难的事情 他可是直接输出答案的 在做这个实验之前 我根本不相信语言模型 可以直接读一篇文章 问一个问题 就直接产生答案 所以看到这个结果的时候 当时其实是惊呆了 就是这个史前时代的人 发现说可以用火 这个真的是惊呆了 但现在你觉得 你一定是觉得没什么啦 但是在石器时代的时候
              
                  38:23
                  就突然发现 哇 这个有火可以用啊 有火这种东西啊 真的是吓了一大跳 而且当时我觉得我们低估了GPT-2的能力 为什么当时我就知道 我们低估了GPT-2的能力呢 因为很多时候GPT-2的答案是这样的 正确答案可能是英文的70 他回答了70 那这样你要算他对还是错 按照挂的标准要算他错 因为要答案一模一样才能够算是对的 但很多时候模型得到的其实是 同样意思只是不同的说法而已 因为他并不从文章里面直接拿一个答案出来啊 他是按照他的意思写一个答案出来 明明意思是对的 但我们就算他错 我们其实低过GPT-2 他实际的能力 好 那除了做阅读测验以外 我们还做了很多其他的任务 比如说教他做情感分析 或者是教他产生SQL的 的指令等等 所以我们就可以用同样的模型 就是一个语言模型 来打那个Natural Language Decasome里面的十个任务 这个是我们用GPT-2得到的正确率
              
                  39:28
                  然后Other Score是之前的文献 在Natural Language Decasome那一个比赛里面 可以得到的分数 那时候非常神奇的是我们发现说 就算我们用GPT-2只是一个模型 一个简单的语言模型 在各式各样不同的任务上 居然都可以得到还不错的结果 所以那时候就可以感受到语言模型 真的能力非常的强 非常的有潜力 然后那时候我就有一个想法 因为我们没有办法收集 一下子就收集到 所有自然语言处理相关的任务 但我们能不能够每次收集到一个任务 就拿去微调语言模型 一开始教他做阅读测验 接下来叫他产生SQL的指令 接下来叫他做 情感分析 接下来叫他做Semantic Role Labeling 一路教下去 每次收集到一个新的NLP的资料的时候 就教模型一个新的能力 这样几年之后他就会变成天网 后来我们发现要做一个天网并没有那么容易 为什么呢
              
                  40:31
                  因为当我们教模型新的任务的时候 他非常容易忘掉 他本来就已经会的技能 我们来看一下模型在SQuAD上面的表现 这个纵轴是在SQuAD上面的 正确率 那我们现在观察的是只观察SQuAD这个任务的表现 当我们在教模型SQuAD的时候 教他做阅读测验的时候 当然他阅读测验的能力会越来越好 但一旦我们教完阅读测验 开始教他产生SQL的指令的时候 你会发现他的performance突然开始暴跌 这边蓝色的这一条线 代表的是一般的fine tune 就是微调模型教他去产生SQuAD的指令 那一旦他学会做SQuAD指令以后 他突然就做不了阅读 那我们也试著做了一些比较进阶的regularization的方法 我们这边试了一个叫做MAS的方法 在19年的时候 这是一个很好的regularization的方法 专门针对forgetting的问题设计的 但我们做在语言模型上 Math居然比fine tuning 结果还要更差一点 它是橙色这条线
              
                  41:35
                  接下来继续教模型 做情感分析 更差了 更不能够做阅读测验 但神奇的事情是我们发现 如果我们再继续教模型 做Semantic Row Labeling 这边这个Semantic Row Labeling 并不是传统的Semantic Row Labeling 它也有点像是阅读测验 这个Benchmark是把Semantic Row Labeling 转成一个有点像阅读测验的模式 就你给它一个句子问它说 这句话里面有出现 什么样的人物 所以它其实有点像是阅读测验 那我们发现教模型 这个SIL之后 阅读测验的能力又回来了 然后再教他别的任务 这个应该是教他做一个对话相关的任务 这个阅读程的能力又掉下去了 所以模型本来学到的能力 他会上上下下非常的不稳定 他本来已经学到的能力非常容易失去 但是当时我们观察到 好像又蛮容易被换回来的 就感觉他并不是遗忘 他只是不想起来而已 这些能力就藏在某个地方
              
                  42:38
                  你有办法把他召唤回来 但是他又很容易就不知道 跑到哪里去了 好,所以看起来呢 你要一直教模型新的能力是不容易的 因为他很容易就遗忘 本来就有的能力 但是其实在2019年的时候 那个时候我们就已经有一个解法了 这个解法叫做experience replay 这个experience replay的解法 在由我们这边论文之前 其实就已经在其他领域 比如说computer vision上面 有人尝试过并发现非常成功的结果 只是当时还没有人试在 大型语言模型上面而已 那我们就在大型语言模型上面尝试这个方法 那这个方法其实非常的直觉 他想法是这样子的 你先拿任务一的资料 教模型然后他会了任务一 接下来你要再教他任务二的时候 不要只拿任务二的资料 你要混一点任务一的资料 混多少任务一的资料呢 我们发现不用太多 大概任务二的5%左右的资料 就非常非常足够了 不需要混太多资料
              
                  43:41
                  因为我刚才讲过说 模型呢 他的遗忘非常的神奇 他感觉不是真的遗忘 他只不想想起来而已 所以他那些知识好像就藏在某个地方 你只是需要一些契机 把他唤醒而已 所以其实不需要太多过去的资料 大概现在当下这个任务训练资料的5%左右 就非常足够了 我们就用了这个experience replay的方法 那我们得到的是上面这几条线 那这边为什么会有很多条线呢 等一下会再跟大家剖析 我们就是experience replay这个方法 然后我们发现 它是一个可以有效防止模型遗忘的方法 所以在2019年的时候 那个时候我心里得到的结论是 catastrophic forgetting 不是一个真正的问题 这个问题太容易解决了 因为只要保留有一些过去你训练模型的资料 保留一些不用太多一点点就好 你就有办法在接下来的训练里面 防止模型遗忘就有的技能 所以看来cat trophy forget it 并不是一个很严重很难解的问题
              
                  44:46
                  好 知道这些以后 我们就回到这个现代吧 这个现代是有很多人工智慧的 比如说Gemini 比如说Claude 比如说DC 我在画图的时候 这个就是用GPT-4o绘图的功能绘的 我有叫他把GPT-4o写上去 但不知道为什么 是不是因为他很谦逊的关系 觉得自己不是一个人工智慧 他就没有把自己的名字写上去 但有可能这边也就有GPT-2 他以为他写了 所以他没把自己的名字写上去 这些Logo呢,也是他自己画出来的,我觉得还蛮像模像样的,如果你比较原有这些模型的Logo的话,我们就回到现代,好,回到现代,怎么解决Ketotropy Forgetting的问题呢? 如果你想要教模型中文,就发现教完中文以后,他就忘记了Safety Alignment的能力,那怎么办呢? 那根据我们在2019年就已经知道的事情,那你要拿一些训练Lamma to Chat的训练资料拿来做 Experience Replay,问题就解决了 但是等一下,你根本没有Lamma to Chat的训练资料啊 现在这些大公司都只释出模型
              
                  45:51
                  他们已经不释出训练资料了 你根本没有办法拿那些模型的训练资料来做Experience Replay 所以Ketotropy Forgetting是一个真正的问题 但是,其实在2019年的时候,面对这个状况 我们其实也是有Solution的,所以我们就再次回到2019年 在2019年的时候,那时候在论文里面,我们多加了一个额外的情境 就是假设我们找不到过去的资料的话,应该要怎么办呢? 当时要设定这么复杂的情境只是为了要上个顶会啦 方法太拿衣服的话,没有办法上顶会 但是那时候我心里觉得,这根本不是一个实际的setup 怎么可能我会拿不到过去的训练资料呢? 当然现在是一个蛮实际的setup setting就是了 好,现在假设我们拿不到过去的训练资料的话 那应该要怎么办呢? 那时候我们就想到一招 这招是这样子的 我们在教模型任务一的时候 我们教他看到这个context
              
                  46:55
                  看到这个问题 你要回答答答案 但是因为他是一个语言模型 我们实际上训练他的时候 就是给他这一整个sequence context问题跟答案 给他这一整个sequence叫他拿去做 文字接了 所以实际上有可能 我们直接叫这个 训练完任务一的language model 随便讲什么的时候 就给他一个begin of sentence的token 然后叫他随便讲什么都行 他可能就会先产生一个context 再产生一个问题 然后再自问自答产生一个答案 那这样我们不是就有 过去的训练资料了吗 虽然我实际上并没有把过去的训练资料存下来 但我们可以从已经训练完的模型 想办法去生出过去的 训练资料 那这件事情真的可行吗 我们试了一下 还真的可以 我们就把训练在这个SQuAD上面的模型 叫他吐一些东西出来 他就会先吐一篇文章 这篇文章就是讲说 美国入侵阿富汗 那有非常多的牺牲
              
                  47:57
                  有1600个美国军人牺牲 总共有一万美国军人上升 听起来是一个非常大的数字 然后接下来问你说 这次冲突的目标是什么 答案是阿富汗 这大串文字都是GPT-2 自己生出来的 他先生一篇文章 再问自己一个问题 再自己产生这个问题的答案 在这个旧时期时代呢 我看到这个结果的时候真的是惊呆了 这个文章会不会是真的存在的 他只背了一篇他看过的文章而已 也许挂里面有一模一样的文章 但我后来仔细看要发现说 这是一个假新闻里面讲的数字 其实都是随便乱讲的 它并不是一个真实存在的新闻 或这边有另外一个例子 这个例子就是 在1856年的时候 这个卡达菲的家族 他们到了埃及 然后在隔年呢 军队从这个利比亚 撤退然后回到了班加西 然后问说呢
              
                  48:59
                  格达菲的军队 为了谁回到班加西 那这题的答案是格达菲的家族 这个答案对不对 不好说啦 从这个文具里面看不出来说军队撤退 是因为格达菲家族的关系 但反正他自问自答的问题 跟答案就是长这个样子 那这个新闻 这个看起来很像是Wikipage里面 会有的内容的东西 看起来像模像样的 但是他完全就是一个错误的资讯 因为格达菲根本是20世纪的人 这边的年代通通都是鬼扯的 但是那个时候 语言模型就可以产生出一些 像模像样的文章 还能够自问自答 这个旧时期时代人类来说 真的是太惊人了 但我知道你今天会觉得说 语言模型不是本来就应该这个样子吗 但在2019年的时候 看起来并没有那么直觉 好 那我们现在知道说 我们可以让语言模型自说自话以后 就产生出他之前看过的训练资料 所以我们在教模型第二个任务的时候 怎么拿到第一个任务的训练资料呢
              
                  50:02
                  你就把你的要post training之前的那个foundation model拿来 然后呢你就叫他自说自话 叫他自己产生一些句子 把他产生出来的这些句子 当作代表任务一的训练资料 加到任务二的训练资料里面 然后就可以避免cat trophy forgetting的状况 所以在刚才这张图表里面 最上面的两条线用的是真正的任务一的资料 下面的这几条线 其实用的都是GPT-2自己生出来的资料 那为什么会还有这么多条线呢 其实我们试著用不同的方法,然后来让语言模型生资料啦,那这个细节,大家去再去看原始的论文,所以当时我们知道说,如果你可以拿到一些旧有的资料,而这些旧有的资料可以是语言模型自己生出来的,那你可以避免避免遗忘的状况,有另外一个小插曲是当初这篇模型呢,当初这篇论文呢,投稿到ICLR 2020,本来文章的标题是language model is all you need for lifelong language learning,你知道,我这个看起来现在是一个非常老的 老套的标题的取法,但在2019年的时候还可以感觉没有那么老套,那时候觉得说你看language model可以解10个任务,而且用language model自己呢,就可以达到lifelong learning,让他一直学新的东西,可以避免避免遗忘的状况,所以language model is all unique这样子,那时候reviewer就觉得蛮生气的这样,reviewer蛮生气的,他觉得language model不可能可以解各式各样的问题,所以为了让reviewer高兴,双膝一软呢,就直接把all unique那几个字拿掉,他就上了ICLR 2020
              
                  51:36
                  不过在现在这个时间点,如果有人讲说所有NLP的任务都可以用language model来解,我想你其实也不会特别反对就是了 好,那我现在呢,再回到2025年,好,在2025年,其实我刚才2019年讲的那个讲法,仍然是一个非常主流的,避免forgetting的方式 举例来说,这个23年,有一篇论文呢,叫做Safety Tune LLaMA,因为他们发现说LLaMA fine tune之后,很容易失去Safety Alignment的能力,那怎么避免它失去Safety Alignment的能力呢? 非常的简单,就如果你只拿一般的资料来fine-tune language model,那你往往得到一个unsafe的model 但如果你可以保留一点点,在他们论文里面写3%的Safety Alignment的资料 那这种Safety Alignment的资料通常就是你跟模型讲说 我怎么杀一个人,然后他就说我不可以教你做这种事情,你只要保留一点点这类的资料,混到你现在要做post training的资料里面,你就可以保有原来模型safety alignment的能力,结果刚才讲的experience replay,其实是一样的做法,或是有另外一篇论文叫做self-synthesize rehearsal,那这是24年的论文,他做的事情就是,我过去呢,在做这个post training的时候,我们现在都知道,要混一个
              
                  52:59
                  过去的资料,他这边叫做rehearsal的data,把过去的一些资料,混到新的资料里面一起去做训练,可以避免避免遗忘的问题,但是有时候我们有可能会拿不到过去的训练资料,怎么办? 你看这都是羊驼了,代表说这是LLaMA系列的work,就比较新的work,我们今天有可能拿不到过去的训练资料,怎么办? 那我们可以让LLaMA自问自答,产生一些他过去看过的训练资料。 有办法让LLaMA自问自答产生类似他过去看过的训练资料吗? 是有办法的,做这件事情最知名的一篇文章叫做Magpie,Magpie是喜鹊的意思 这个方法是这样子的,怎么让LLaMA产生看起来像是他之前训练过的资料呢? 你就先给LLaMA一个代表user的token,就LLaMA在使用的时候,你会先给他一个代表user的token,然后问他一个问题,然后再给他一个 代表assistant回答的token,然后他在做文字接龙进行回答,那通常user的问题是你自己给定的,但这边他们只给LLaMA代表user的这个符号,然后让LLaMA继续去做接龙,他就会自己产生一个问题出来,就产生一个问题出来,然后接下来你再把user的token,LLaMA自己产生出来的问题,后面再接代表assistant,代表AI的token,LLaMA就会把自己问的问题的答案产生
              
                  54:29
                  他就自问自答,自己产生一个问题,自己产生一个答案,接下来你就有了疑似LLaMA-3训练的时候,用的这个训练资料,instruction fine tuning的资料,你就把它加到你的原理,你要做post training的资料里面,就可以避免forgetting的现象发生,所以当时2019的方法,在今天这些新的模型上面,仍然是适用的。 好,那我们刚才是介绍了这个experience replay的方法,那我们也介绍了 到了pseudo experience replay的方法 也就是说过去的经验 不一定是真实的资料 它可能是foundation model 自问自答产生出来的 那其实还有很多类似的变形 但概念都非常的像 比如有一个方法叫做paraphrase的方法 这个paraphrase的方法就是说 我们在训练的时候 不要直接拿正确的答案 不要拿人写的答案来训练模型 那要怎么做呢 把用foundation model改写人写的答案 跟foundation model说把这句话换句话说
              
                  55:35
                  然后用换句话说的答案 来当作正确答案来训练模型 那这个跟experience replay的概念 其实有很多类似的地方 因为可以想像说现在的答案 是模型自己产生出来的句子 它更接近模型之前看过的训练资料 它某种程度上就代表了模型之前看过的训练资料 那这招有没有用呢 刚才我们已经看过这边paper的上半部 我们刚才在这个课程的开始的时候 给大家看了这个表格的上半部 这是一个去年年初的文章 那下半部就是他们用改写的方法 改写了这些训练资料里面的答案 让这些答案用我们现在的foundation model来改写 他就发现说在所有的状况下 用改写的答案来训练模型 其实比较好的 在这边的九个状况下 只有一个状况 用改写的答案会比较差 其他状况用改写的答案结果都是比较好的 除了改写答案之外 还有另外一个方法叫做self output
              
                  56:39
                  self output的方法就是 那我们乾脆直接让foundation model来产生答案 我们把问题丢到foundation model里面 让foundation model来产生答案 但是foundation model有可能会答错 那我们要有一个方法 那这个方法就要看你要怎么设计 去检测这个答案是不是对的 那如果今天是那种数学的问题 你就有正确答案 你可以直接对答案看对不对 或如果是那个程式的问题 你可以直接过compiler 看compiler有没有error 所以在某些情况下 你蛮容易检查一个答案 是不是对的 如果他是对的话 如果今天foundation model输出的答案 其实是对的 就拿foundation model自己的答案 来训练自己 那除非他答错了 他答错了 我们采用人写的答案 来训练模型 但这边可以有很多变形 比如说你可以说 也许foundation model的能力不够强 他没有办法第一次就答对 那我们让他产生十个不同的答案 因为同一个模型 你每次sample答案都不一样嘛 让他产生十个答案 我们就只挑对的那一个 来当作正确的答案训练
              
                  57:43
                  这个跟paraphrase 还有experience replay 都是非常类似的方法 就是我们需要混一些 foundation model 自己产生出来的资料来做训练 这样可以避免forgetting的状况 那你会发现讲到 到目前为止啊 我们有提到 trans style的post training 有提到SFT style的post training 但我们一直没有讲到 RL based的post training 为什么还没有讲到 RL based post training呢 你仔细想想 如果我们用RL的方法来训练模型 是不是其实就跟self output这个方法 非常类似呢 你想想看RL是怎么做的 RL并不是直接强制提供答案给模型的 RL是 产生一些答案 如果这个答案是对的 就提高他的机率 错的就降低他的机率 这跟self output其实非常的类似 因为self output是 如果今天foundation model 可以得到正确的答案 他就拿来做training 就等于是提高了他出现的机率 那唯一不同的只是
              
                  58:45
                  有没有把错的答案降低机率而已 所以RL-based的post training 其实跟self output非常的像 所以我认为RL-based的post training可能是一个比较能够防止 是forgetting的方法,这可能就是为什么你发现说在训练语言模型的时候,往往RLBase的方法是放在最后一个阶段里面,或者今天有很多人用RLBase的方法来强化模型reasoning的能力,他们可能都没有特别讨论这个forgetting的问题,有一个可能是RLBase的方法,因为他跟self output非常的像,他其实是一个特别能够防止forgetting的技术,那是不是真的是这样,我还要上代更多的研究来回答这个问题。 好,那我们来看一下 self-output的表现怎么样 这边有一篇paper叫做 selective self-rehearsal 那其实这个方法就是self-output的方法 这个prompt指的就是 foundation model的表现 他把foundation model呢 测试在四个不同的课本上 NLU,这个是 truthness QA
              
                  59:47
                  这个是数学的问题 这是HellaSwag,HellaSwag就测试模型的 那个common sense 那SFT呢,代表是一般的fine tuning 做完一般的fine tuning 以后,这边这个数值代表 模型的正确率掉了多少 做完一般的fine tuning以后 模型在这些任务上正确率 都是暴跌 但是如果你是做SSR,就是我们刚才讲的 self output的方法 用模型自己的话 如果他今天可以正确回答这个问题的话 就用他自己的答案来训练模型 你会发现说 可以让这个forgetting的状况 变得非常的轻微 如果这边生成答案的模型 不是我们要训练的这个 Foundation Model 其实也有帮助 这边这个Foundation Model 换成其他的Model 其实也有用 为什么你可能会想要把这个Foundation Model 换成其他的Large Language Model呢 因为有可能你本身的 Foundation Model实在太弱了 他根本没有办法回答任何问题 如果他根本没有办法回答任何问题的话
              
                  1:00:50
                  那你拿这个Foundation Model 来生答案 你可能一直都生不出正确的答案来 就很难使用Self Output这个方法 所以有时候你可能会想说 那如果我这边不是用人类生的答案 但是是用另外一个比较强的 language model生的答案 有没有帮助呢 有一篇paper叫做 I learn better if you speak my language 你从他的标题就可以知道他想要做什么 他就比较三个case 一个是我们训练的时候 用人准备的正确答案 另外一个是用GPT-4的答案 另外是用Claude的答案 他训练的对象有两个比较弱的模型 一个是Mistral,一个是LLaMA,那他们设三个case,一个是教他数学,这个也是教他数学,这个是教他在ECQA这个corpus上面训练模型,然后接下来测试的时候是把模型测在GSM8K,math那个algebra,还有ECQA这三个corpus上面,那红色代表performance特别差的,performance特别差的就标红色,那你会发现红色呢,通常是出现在使用人类的, 人类资料作为正确答案的时候,你发现用人类资料来教模型,反而模型会学得比较差,它比较容易遗忘它本来就有的技能,还不如让其他的语言模型来教你现在的模型,那可能是不同的语言模型,他们虽然不是同个模型,但他们讲话就是比较像,所以用语言模型的答案来教语言模型,反而可以学得更好,那这篇文章还有发现说,呃,如果你有很强的语言模型,还是有一些
              
                  1:02:24
                  一些状况下,模型会有非常大的forgetting的状况发生,比如说他们这边是把模型测试在,就如果你拿GPT-4的答案来训练模型测试在human eval写程式的任务上,他们发现模型的表现很差,可是你现在自己的模型又太差了,怎么不足以得到正确的答案,怎么办呢? 所以他们这边就用了一个minimum change的方法,minimum change的方法是说,先拿自己的模型产生答案,按自己的模型答案, 可能会错很多,接下来你再拿GPT-4修改跟GPT-4说,这边有一个可能有错的答案,你只把错的地方改掉,但是内容要越像越好,所以如果用GPT-4来修改你自己模型现在foundation model的输出的话,可以得到比原来用GPT-4当作答案还要更好的结果。 这边再多举一个例子,我们刚才有讲过说呢,如果你直接教模型听语音,那很容易伤到原来的。 原来文字模型的能力,那同样的这种self output的概念,也可以用在教模型语音上面,怎么做呢?我们现在要训练一个模型,它可以输入语音,给一个文字的指令,得出正确的输出,那我们现在在训练模型的时候,要尽量用模型自己的话来当作答案,要尽量教,我们在教模型的时候,要尽量用模型自己的话来当作答案,那我们这边
              
                  1:03:54
                  怎么样得到模型针对这个问题自己的输出呢 因为原来的文字模型它是完全听不懂语音的 但你有可能你的声音讯号是有一些标註的 那你就想办法把这段声音讯号 尽量用文字描述出来 就把这段声音讯号里面各种语音的特征 用文字来告诉文字模型 你告诉他说这段话长度多少 告诉他讲语者的性别 告诉他这句话的情绪 这句话的口音 把这些资讯都丢给一个文字模型 接下来你给文字模型一个指令 比如说what can you hear 然后模型就会好像 这些文字模型虽然实际上不能听语音 但你给了他一段文字来代表语音 他能读这段文字 他就会好像他听到一句话一样 产生一个输出 接下来你在训练自己的语音版模型的时候 就把文字模型的输出当作目标 一样问语音模型what can you hear 要求他输出的答案跟文字模型 越接近越好
              
                  1:04:57
                  那用这样子的方法 你就可以有效避免模型 遗忘他原来作为文字模型的时候 就有的能力 那现在很多语音模型其实都采用 类似的方法来训练 那这边列举了几个比较知名的例子 比如说BOSP 我们实验室跟NVIDIA合作做的 Desktop2还有DVA等等 这些模型都使用这样的方法 那我这边呢 想特别分享一下我们实验室 卢克涵同学跟NVIDIA的研究者 人员做的 的方法跟成果 这个模型在训练的时候 实际上我们只给了 他一个instruction 我们只教他怎么回答 what can you hear 我们只教他这个instruction 我们没有教他更多东西了 但是凭藉著文字模型 本来就有的generalization的能力 我们发现就算只在教他语音任务的时候 教他what can you hear 在测试的时候 你居然可以问他任何问题 你问他任何问题 他居然都是能够回答的 所以就算设施的时候
              
                  1:06:02
                  这些text instruction 是他训练的时候根本没有看过的 他居然是有办法回答的 好那为了要验证说这个模型 是真的能够回答 各式各样问题的 我们把它evaluate在一个 叫做Dynamic-SUPERB的benchmark 那这个Dynamic-SUPERB呢 也是我们实验室做的 是我们实验室的黄建佑同学跟CNU的心机瓦塔纳北教授合作的一个work 好那我们这个work做的内容是这个样子的 就在这个benchmark里面 他是为了要全面评估这些语音版语言模型的能力 那在这个资料集里面 每一笔资料都是一句话 一个指令跟一个正确答案 那指令可能是请告诉我这句话的情绪是什么 那正确答案就是happy 或请告诉我这句话里面 现在有几个人在讲话 那答案就是2 或者是给他两段声音接在一起 问他说前后两段声音是不是同一个人讲的 他就要回答yes或no 那这个Dynamic-SUPERB的第一个版本呢 是发表在去年的iCasper 那里面总共有55个不同的任务
              
                  1:07:08
                  那Dynamic-SUPERB呢 其实有一个Phase 2 那Phase 2呢 是黄建佑同学 跟CMU、CG瓦塔纳北教授的团队 还有UT Austin、David Howard教授的团队 所一起打造的一个Benchmark 那我们有180个任务 我们把任务呢 建立了一个树状的结构 就是这么复杂 除了有语音相关的任务以外 也有音乐跟声音相关的任务 那语音相关的任务里面 有子类别 子类别下面有子子类别 子子类别下面有子子子子子类别 子子子类别下面有子子子子子子类别 所以是一个非常庞大的Benchmark 从各个不同的角度来Evaluate 一个spoken language model能不能做各式各样不同的事情,好,那这篇文章呢,其实是发表在今年的ICLR,好,那我们呢,把Data2呢,Evaluate在Dynamic-SUPERB的Phase 1上面,那这边每一个column就代表说一个语音版的语言模型,在各个不同面向上面的正确率,那这边每一个column不是一个任务,它是多个任务的平均值,那 是所有任务的总平均
              
                  1:08:23
                  那这边是把desktop跟其他的模型做了一下比较 跟当时我们可以找到的表现的比较好的模型做一下比较 那你会发现在Dynamic-SUPERB上整体而言 desktop是比其他模型还要更好的 而且desktop用的训练资料其实是远比其他模型少的 代表说如果你可以好好防止forgetting的现象 用比较好的方式来训练模型 你其实可以用少量的资料就训练出来 出一个还不错的语音版语言模型 那像这种self-output的方法 现在是一个防止forgetting 非常常见的方法 所以如果你今天要自己fine-tune模型 我知道常常有同学会选自己fine-tune模型的需求 这是一个你需要考虑的方式 那除了这种self-output的方法以外 还有其他可能的思路 那这篇呢是Appier的研究人员 吴兆聪同学的论文 那首先呢 他们先观察了self-output的结果 跟正确答案之间有什么样的差异 那他们发现说这个是一个数学的问题
              
                  1:09:30
                  光truth是人写的答案 self-output是模型自己产生的答案 那最后人类跟模型都得到正确答案 正确答案是负四分之三 但是如果你去计算这里面每一个token 你用现在的foundation model 这边foundation model是LLaMA 38B 你用现在的foundation model产生 产生出那个token的机率 你会发现说 对于光tube而言 里面有比较多token 是对于你的foundation model来说 比较难产生出来的 那这边把比较难产生出来的 token 你就算出来机率比较低的token 标上红色 那你会发现说在光tube里面 有比较多的token 是你的foundation model比较难产生出来的 而在foundation model自己的output里面 当然这是foundation model自己的output嘛 都是foundation model产生出来 机率比较高的token 这只是一个例子 也实际上做了一些数值上的结果 在这个 mppp这是一个程式的资料集 跟数学的资料集
              
                  1:10:35
                  都比较了正确答案 跟paraphrase 还有self output 这三个方法的 这个publicity publicity就是你的foundation model 产生这个句子的机率 那这个数值越大代表你的foundation model产生的机率越低 那如果你不知道publicity是什么的话也没有关系 你就记得说呢 这个publicity越大就代表说 这个句子越不像是你的模型会产生出来的句子 所以这边非常直觉的 如果是人写的正确答案 对模型来说都是非常难产生出来的句子 而paraphrase的句子 paraphrase过的 因为是模型自己生出来的句子 模型当然会觉得 是比较容易生出来的,self output根本就是模型自己生成的,所以这些句子对他来说都是产生出来的机率比较大的句子,但是你又发现说在光圈里面,其实也只有某一些token,对于语言模型来说,对于foundation model来说,是特别难产生出来的,那我们能不能在训练的时候,直接就过滤掉那些对于foundation model特别难的token呢,所以这边实际上的做法是,我们知道说,假设你要教 模型说一句话,比如说大家好,我是人工智慧的时候,你实际上教模型的就是next token prediction,也就是文字接龙,你教他看到代表开始的符号,就要说大,看到大,就要说加,看到大家,就要说好,那你先拿你的foundation model去计算一下在你的训练资料里面,每一个token,你的foundation model输出predict的机率,如果发现有某一些token是你的foundation model特别难生出来的,直接
              
                  1:12:19
                  直接在训练的时候不考虑那个token 这个token并不是从句子里面拿掉 而是说我们就不去要求模型看到大的时候要产生加 假设加是一个特别难产生的token 我们还是会教模型看到大家要产生好 或看到begin of sentence要产生大 但是你就直接去掉 你就直接略过这个问题 不教模型看到大产生加 你就把整个corpus里面 一部分的token不给模型训练 那这个方法有没有用呢 这个方法 这个方法居然是有用的 那这边是训练在那个 MATH那个Corpus上面的结果 现在模型训练在MATH 然后一样是测试在同一个Corpus的Testing Set的话 横轴是被拿掉的Token的数目 就我们把Token的难易度做一个排序 先拿掉最难的再拿掉次难的 所以可以改变Token被拿掉的比例 那你会发现说Token拿掉的比例 大概在20%以内的时候 居然对于训练是有帮助的 那对于in-domain有帮助 对于out of domain也有帮助
              
                  1:13:23
                  就训练的时候没有看过GSM8K、ARC 还有BIRC的这三个任务 但是你发现说当你拿掉一些token的时候 模型在这三个任务上 其实是可以表现得更好的 因为你在训练的时候 没有叫模型去学一些 他根本学不起来的东西 可以避免遗忘的问题 所以你拿掉一些在训练的时候 对模型特别难的东西 反而模型是可以做得更好的 这个是最后一页投影片 在post training的时候 大家要特别注意 人工智慧很容易遗忘 他过去已经有的技能 那今天常常会听到有人说 我拿一个LLaMA-3 我在做post training 把他训练在特定的任务上面 我在特定任务上面 可以假打GPT-4o 那这件事当然是有可能的 你针对特定任务训练 你要打爆那些通财模型 其实并不是一件太难的事 但是往往往我看到这种结果 我会担心的地方就是 那你到底损失了多少 其他本来模型就有的能力呢
              
                  1:14:27
                  你的模型会不会变成 只会这个任务 其他任务通通都不会 比如说你要教一个模型 一个特别的程式语言 你逼他学very log 没那么会写very log 你当然可以在very log的任务上打爆他 但会不会模型之后 他连正常的话人类的话都说不好 要连写註解都没有办法 或者是根本看不懂人类要求他怎么写very log的内容的指令呢,这是有可能的,所以大家今天在自己做post training的时候,你要注意除了看你的他目标任务有没有做好以外,你其实应该要去检查一下你的模型在原来他能够做的任务上面,到底还有没有保有原来的能力,那我们今天知道一个非常有效的可以防止可以防止forgetting的方法,就是如果我们今天的训练资料, 是用人工智慧自己的话来说,也就是他自己产生出来的,往往对于post training是非常有效的,那这是一个很有效的方法给要做post training的同学参考。
              
            