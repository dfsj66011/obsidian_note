
## 1、生成式 AI 的技术突破与未来发展
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；

三个面，剖析 AI agent 的关键能力：

1. 能不能够根据他的经验，过去的互动中所获得的经验来调整他的行为 
2. 如何呼叫外部的援助，如何使用工具 
3. 能不能够执行计划，能不能做计划

关于 1，真正的议题是：如果我们是把过去所有的经验都存起来，要改变语言模型的行为，要让他根据过去的经验调整行为，就是把过去所有发生的事情一股脑给他，那就好像是语言模型每次做一次决策的时候，他都要回忆他一生的经历，也许他没有足够的算力，来回顾一生的资讯，他就没有办法得到正确的答案，所以怎么办呢？

也许我们可以给这些 AI agent memory，这就像是人类的长期记忆一样，发生过的事情，把它存到这个 memory 里面，有一个叫做 read 的模组，会从 memory 里面选择跟现在要解决的问题有关系的经验，把这些有关系的经验放在 observation 的前面，让模型根据这些有关系的经验跟 observation 再做文字接龙，

那怎么样打造这个 read 的模组呢？就想成是一个 retrieval 的 system，其实它就是 RAG，唯一不一样的地方，如果是 RAG 的话，存在 memory 里面的东西是别人的经验，而对 AI agent 而言，是他自己个人的经历，差别的是经历的来源。

根据经验调整行为能力的好坏，那就看这一整个回答的过程中平均的正确率，越能够根据经验学习的agent，应该能够用越少的时间，看过越少的回馈，就越快能够增强他的能力，就可以得到比较高的平均的正确率。

还发现一个有趣的现象是值得跟大家分享。这个现象是负面的回馈，基本上没有帮助

可以有一个 write 的 module，决定什么样的资讯要被填到长期的记忆库里面，怎么样打造这个 write 的记忆库呢？一个很简单的方法就是 write 的模组也是一个语言模型甚至就是 AI agent 自己

还有第三个模组，没有固定的名字，暂时叫 reflection 反思的模组，这个模组的工作是对记忆中的资讯做更好的、更 high level 的，可能是抽象的重新整理，可能也是一个语言模型，或 AI agent 自己

-------

关于 2，怎么使用工具？小模型呼叫大模型帮忙，这些工具对语言模型来说都是 function，

使用工作的挑战，每一个工具都要有对应的文字描述，告诉语言模型这个工具怎么被使用，假设工具很多怎么办呢？类似上文，打造一个工具选择的模组；也可以自己写个 function 作为 tool

工具有可能会犯错，所以我们也要告诉我们的工具，这些不要完全相信工具的工具，要有自己的判断能力，不要完全相信工具的工具给你的结果，比如给出的天气温度为 100°

那什么样的外部知识比较容易说服 AI，让他相信你说的话呢？外部的知识，如果跟模型本身的信念差距越大，模型就越不容易相信，那如果跟本身的信念差距比较小，模型就比较容易相信

----------

关于 3，AI 语言模型能不能做计划？语言模型就是给一个输入，它就直接给一个输出，也许在给输出的过程中有进行计划，但我们不一定能够明确的知道这件事；

但其实可以强迫语言模型直接明确的产生规划，可以直接问语言模型说，如果现在要达成我们的目标，从这个 observation 开始，你觉得应该要做哪些行动？

不过这是一个理想的想法。那语言模型到底有没有做计划的能力呢？过去确实也有很多论文说，语言模型是有一定程度做计划的能力的。

现在到底模型规划的能力怎么样呢，就是介于有跟没有间吧……

-----------

## 3、语言模型内部运行机制

* 一个神经元在做什么
* 一层神经元在做什么
* 一群神经元在做什么
* 让语言模型直接说出它的想法


**一个神经元在做什么？**

主要指 FFN 中的单个神经元，其每个输出都是所有输入的某种 weighted sum，然后通过如 ReLU 等激活函数，得到一个神经元的输出。

1. 比如某个神经元的启动与说脏话有关联。

2. 什么叫把神经元从网络中移
3. 掉？让其输出永远为 0？但 0 不代表完全不会造成影响，0 可能会对其他神经元有影响，当它输出为零，反而会启动其他神经元。有研究发现也许设平均值比较好，各种各样不同输入时值的均值，尚待研究。

3. 不同启动程度，说不同等级的脏话

好像是一個最近的發現一樣 但其實不是川普神經元 是一個老新聞了 這個是2021年的時候 這個OpenAI的研究人員 在一個平臺叫做Distell 那是專門給你放論文的平臺 在Distell那個平臺上 發表的一個論文裡面的內容 而且他們分析的 還不是大型語言模型 那是2021年 所以他們分析的其實是一個叫做 Clip的影像模型 因為他不是一個生成的模型 所以他其實也沒辦法生成東西 所以川普神經元他的作用是 輸入川普相關的東西的時候 那個神經元會被啟動
              
                  16:59
                  所以我們跟剛才我們講的比較不一樣 因為如果是生成的話 你比較想分析的也許是 一個神經元會導致什麼樣的內容被生成 川普神經元是大型元模型流行之前的研究成果 他甚至不是做在生成式的模型上面 所以他並不是看說這個神經元 可以生成什麼東西 而是什麼樣的輸入 會讓影像模型裡面的這個神經元被啟動 然後有一個神經元呢 他就是專門看到川普相關的東西的時候 就會被啟動 那這個橫軸呢 是那個神經元啟動的程度 越往右代表那個神經元越被啟動 紅色呢 不同顏色就是代表不同類型的圖片 那你會發現說 如果給川普本人的照片 那這個神經元呢 就會大幅的被啟動 如果是給一些跟川普有關的圖 是抽象的圖 卡通版的川普 也可以啟動這個神經元 給他川普的文字 但這不是一個文字模型
              
                  18:01
                  他是個影像模型 是影像中有出現川普這幾個字 這個神經元也會被啟動 然後川普跟其他人在一起 這個神經元也會被啟動 其他的政治人物 一些跟川普有關心的人 也可以啟動這個神經元 那這個川普神經元 他不是一個普通的神經元 有人可能會想說 他是不是看到人臉就被啟動 或者是看到政治人物就被啟動 其實不是 他對川普是有非常高的選擇性的 這個橫軸是那個神經元啟動的狀態 看到然後呢 你就丟給他不同人的照片 給他川普的照片 高度的被啟動 給他那個麥彭斯的照片 這個是川普的副總統 然後也稍微會被啟動 給他Steve Bannon 川普的顧問 也會被啟動 給他希拉蕊也會被啟動 但給其他政治人物呢 就不會被啟動 比如說給他這個歐巴馬就不會被啟動 給他其他人 比如說Steve Jobs也不會被啟動 所以這個神經元看起來是蠻針對川普的 講到這個有單一功能的神經元啊 就讓我想到一個人類大腦的故事
              
                  19:07
                  所以這一頁投影片講的 跟AI完全沒有半毛錢關係 這一頁投影片講的是人類的大腦 講到這個單一功能的神經元 就讓我想到祖母神經元的故事 在腦科學裡面有一個假設是說 也許我們人類會用很單一的神經元 來處理一些很重要的記憶 比如說這邊的故事是 有一個人他非常想念他的母親 所以想念到思念層級 那個外科醫師為了救他 就把他腦剖開 把跟他母親有關的神經元割去 割去之後 他就再也不想念他的母親了 他甚至都不記得他的母親 長什麼樣子 然後那個外科醫師 後來就決定 要找完母親神經元以後 接下來要找祖母神經元 但是其實祖母神經元這個概念 他是個稻草人理論 他是在1960年的時候 有一個認知科學家 他在上課的時候舉的一個例子 他舉這個例子 就是剛才那個外科醫師的故事 是他虛構的 他講這個例子
              
                  20:09
                  是為了要反襯另外一套理論 就多數人比較相信的是 當人類的大腦在處理一件事情 在處理一個記憶的時候 是很多神經元一起運作 才能夠處理一個記憶 並不是由單一或者是 少數神經元來控制的 所以祖母神經元 是一個虛構出來的理論 為了要對照腦神經科學 比較相信的多個神經元 同時處理一件事情的理論 那等一下呢 其實你會看到 在AI的領域 其實也是這樣子 一個神經元可能做不了什麼事 可能很多個 神經元合在一起 才能做一件我們可以理解的任務 不過如果沒有記錯年代的話 在2005年 確實有人發現了一個 叫做Jennifer Aniston的神經元 就是有人研究了 一群癲癇的患者的神經元 發現說呢 有一個神經元 只對那個Jennifer Aniston activate 所以他是Jennifer Aniston的神經元 然後這件事情也被大肆報導
              
                  21:13
                  那確實可能有 有一些神經元 他只負責單一的任務 但是人腦是很複雜的 也許有一些任務真的是少數神經元負責 但可能大多數任務仍然是由大量神經元所負責的 好那這一頁呢 是講一個跟AI無關的東西 來類比現在這個人工智慧的腦神經科學的進展 好那剛才講到了川普神經元 但是實際上啊 多數的單一神經元 你都很難解釋 它真正的用途 怎麼說呢 因為一件事情 可能有很多個神經元共同管理 這邊舉一個文獻中的例子 那這個是一年前左右的研究 在這篇論文裡面呢 這篇論文發現了 管文法單數跟管文法複數的神經元 他們發現 這個是做在GPT2上 發現說這個類神經網路的 第十層的編號2096元 編號2096 的神經元管單數
              
                  22:16
                  第九層的編號1094的神經元 管複數 那怎麼知道他們管單數跟複數呢 如果你把這個管單數的神經元 把它割掉的話 你會發現這個時候 類神經網路的輸出的機率 就會有一個變化 下面這個圖 代表類神經網路輸出的機率的平均的變化 那有紅色的字 代表說這個變化呢 是有統計上的顯著意義的 有統計上顯著意義的 所以發現說 當少了這個管單數的神經元 這個時候 least跟rose這些字 這些詞彙 它的機率就上升了 both的機率也上升了 one的機率就下降了 那割掉這個 管複數的神經元之後 你會發現 跟複數有關的那些單字 least, loss, two, both 等等 它的機率下降了 跟單數有關的單字 least, let, one, two, and every 它上升了 所以看起來這些神經元 真的在管單數跟複數
              
                  23:20
                  那有人就會想說 那我們能不能透過 把這些神經元移除 來讓模型輸出的內容有所改變呢 實際上當你把這些內容移除的時候 多數狀況下 你沒辦法改變這個語言模型最終的輸出 為什麼呢? 這邊是一個例子 那這個圖上所展示的機率 應該是語言模型在某一個time step 它 它輸出least,least,let跟rose這幾個字的機率 那藍色是原來的機率 那如果你對這個語言模型做一些手術 你把這個管複數 你把那個管複數的神經元移除 那複數的這些詞彙 它的機率就下降了 跟單數有關的這些詞彙 它的機率就上升了 而且它的上升是非常顯著的上升 跟非常顯著的下降 這個縱軸是log scale 縱軸是代表機率 但是它是機率的log scale的機率 所以這個差一點點
              
                  24:23
                  其實數字上是差非常多的 但是就算是有了非常多的差距 你會發現least這個字 仍然是機率最高的字 就代表說那個神經元 不是隻有他在管 會不會產生least這個詞彙 還有很多神經元都在共同管這件事 所以你抹掉單一個神經元 其實對最終語言模型的輸出 多數狀況下都是沒什麼影響的 你會發現這個機率分佈裡面 不管是有這個神經元還是沒這個神經元 歷史都是機率最高的那個詞彙 它都仍然是最容易被產生出來的詞彙 而另外一方面 不只一件事情可能多個神經元共同管理 一個神經元可能也同時管很多事情 而右邊這張圖呢 是來自一個叫做Transformer Circuit的網路 那裡面就是有很多文章 這些文章多數都是跟分析這個Transformer 分析大型語言模型有關的文章 那其中有一個網頁呢 就是分析了一個小型的語言模型裡面 每一個神經元做的事
              
                  25:29
                  他把那個神經元看哪些句子會被啟動的 那個句子通通記錄下來 所以在那個頁面上就會看到某個神經元 看到哪些句子會被啟動 你會看到這方面的資訊 那我就隨便挑一個神經元 讓他啟動的句子是這些 然後啟動的程度就是看顏色的深淺 顏色越深代表這個神經元啟動越多 那你能看得出這個神經元在幹嘛嗎 你幾乎看不出來他在幹嘛 非常的隨機 不知道他到底在做什麼 那我就直接把這張圖呢 貼給GPT-4.5看 然後請GPT-4.5 解釋一下 看他能不能猜出這個神經元 到底在做什麼 那GPT-4.5的解釋是 神經元似乎跟 專有名詞有關 他跟一些物理學的術語有關 他跟仿冒和造假 相關的詞彙也有關 他跟醫學術語也有關 還跟一些特定的人民 和特殊專業名詞有關 這個有講跟沒有講一樣 這個神經元的功能太多了
              
                  26:32
                  多到你根本不知道他到底想要幹什麼 根本不知道他的作用是什麼 那這個用語言模型 來解釋另外一個語言模型裡面 神經元的運作 也是另外一個知名的研究結果 就是在這個 2023年的機器學習的課堂上 我們就有講過這個研究 當時OpenAI剛釋出了GPT-4 他們就用GPT-4呢 來解釋GPTQ裡面的 每一個神經元 那他們發了一個Blog 然後在Blog裡面當然講說 他們找到了什麼什麼樣神奇的神經元 那我在2023年 在前年的機器學習 是有講過這一段的 但如果你仔細看他的研究成果的話 你其實會發現 多數神經元都是沒有辦法解釋的 多數神經元都是不知道 他在幹嘛的 我們現在知道說 一個神經元往往不是 負責單一個任務 一個神經元可能很多任務 可能由多個神經元一起負責 那為什麼不是一個神經元 就負責一個任務呢
              
                  27:35
                  你想想看如果真的是一個神經元負責一個任務 其實也不太合理 因為神經元的數目 太少了 你想想看 這個模型 他每一層有多少的神經元呢 才4096個神經元 如果說每一個神經元 就只能夠做一件事 那這個語言模型能夠做的事情 太有限了吧 他根本沒有辦法做到像今天一樣 輸入什麼都給你正確的答案 他根本沒有辦法產生 千變萬化的內容 所以本來就不太可能是一個神經元 負責一個任務 所以這邊有另外一個假設是 不是一個神經元負責一個任務 也許是一個神經元的組合負責一個任務 當神經元編號123643398 被啟動的時候就說中文 當神經元11號12377被啟動的時候 就拒絕請求 那不同的任務 它是可以共用神經元的 因為每一個任務就是由一組神經元來驅動
              
                  28:39
                  不同任務可以共用神經元 那也許這就是為什麼我們會觀察到 一個神經元 他可以做很多不同的事情 往往沒有特定的功能 那今天假設 是由一組神經元來管一個任務的話 有什麼樣的好處呢 假設就算每個神經元 都只有啟動 不啟動binary的兩個選擇 但實際上神經元的輸出 可以有大小之分的 所以不是隻有啟動不啟動兩個選擇 但就算每個神經元只有啟動 跟不啟動兩個選擇 4096個神經元也已經有 2的4096 四方種不同的組合了 所以這就可以表示 千變萬化的內容 也許這就是為什麼 今天的語言模型可以有這麼 強的能力 所以接下來我們就要從一個神經元 進步到一層神經元 我們來看看 一層神經元 它可能是怎麼發揮作用 影響整個語言模型的輸出的 那怎麼知道一層神經元
              
                  29:42
                  在做什麼樣的事情呢 這邊有一個假設 那等一下會用 文獻上的結果來驗證 來說服你說這個假設 是非常有可能是真的 這個假設是這樣子的 每一個功能 都有 某一種神經元特定的 組合所構成 比如說如果一個模型 要拒絕請求的話 那就是第一個神經元被啟動 第三個神經元被啟動 還有最後一個神經元被啟動 這個時候語言模型 就會拒絕你的請求 他就會說我很抱歉 我不能幫助你這件事情 那這些神經元的數值排列起來 可以看作是一個向量 所以這個拒絕請求的 這個神經元的組合 可以看作是在高維空間中的 一個特定方向的向量 那等一下在這堂課裡面 我們就把它叫做功能向量 就這個向量 它是有功能的 當類神經網路的某一層 排出這個樣子的時候
              
                  30:45
                  就這個語言模型 就會執行某一個特定的功能 所以這個語言模型 背後運作的機制可能像是這樣子的 當你給他一個請求 跟他說 請教我怎麼製造炸藥的時候 為了等一下講課方便 我們就假設這一層是第十層 但至於哪一層會執行 拒絕請求的能力 那這你可能是需要做實驗以後 才知道的 那我們假設第十層呢 是負責決定要不要拒絕請求的 那我們就看第十層的類神經網路的輸出 那一層類神經網路的輸出 通常我們叫他representation 一層類神經網路的輸出 叫做representation 那假設輸入這個句子以後 在最後一個時間點 類神經網路第十層的輸出 長這個樣子 那這個時候類神經網路會不會拒絕 你的請求 因為現在他的輸出 這個representation 跟拒絕的功能向量 有多接近
              
                  31:48
                  如果這兩個向量越接近 模型就越有可能拒絕你的請求 那如果這兩個向量非常的不像 比如說他們是正交的 是orthogonal的 那模型就不會拒絕你的請求 那假設這個向量跟這個向量非常接近 他們都是第一個neuron被啟動 第三個neuron被啟動 那模型可能就會拒絕你的請求 我就說我不能夠幫你做這件事 這是一個假設 接下來 怎麼驗證這個假設呢 為了要驗證這個假設 你可能需要真的把這個 代表拒絕的功能 向量找出來 如果你可以找出這個代表拒絕的功能向量 那你可能就可以驗證說 剛才的假設 是某種程度上是對的 大型圓模型可能真的 某種程度上是依照剛才講的 這個運作機制來運作的 但是怎麼找出這個負責拒絕的功能向量呢 我們實際上並不知道那個負責拒絕功能向量長什麼樣子 你只知道說輸入這個文字在第十層
              
                  32:54
                  看到這樣的representation 然後模型就拒絕了 我們可以推測說功能向量 可能藏在我們觀察到的這個representation裡面 但是這個representation裡面 同時也許 有其他的資訊 那我們要怎麼把其他的資訊抹掉 只拿出代表拒絕的功能向量呢 所以這邊的方法就是 你不能只從一個句子觀察 你先找很多不同的句子 這些句子輸入給語言模型之後 語言模型都會拒絕你的要求 然後呢 把這些句子輸入的時候 最後一個time step 他的第十層的向量都拿出來 這些向量可能都是這些第十層的representation 他裡面都是拒絕加上其他事情 都是拒絕加上其他事情 那你可能找一千句這樣子的句子 把他第十層的representation平均起來以後 你可能就得到拒絕 加上其他各種事情的平均 所以我們現在找到的是拒絕
              
                  34:00
                  加其他各種事情的平均 但是我們還是不知道拒絕向量實際上長什麼樣子 那怎麼知道其他的 平均向量長什麼樣子呢 那你就找其他的 沒有拒絕的輸入 你就找其他的句子 丟給語言模型 這個時候語言模型沒有拒絕 所以第十層的representation裡面 沒有代表拒絕的向量 沒有拒絕的功能向量 你就把每一個輸入 第十層得到的representation 都平均起來 得到其他的平均 那我這邊加一個prime 代表說他們不一定是一樣的 我們期待說 假設我們收集到一大堆拒絕的狀況 各式量拒絕的狀況 又收集到一大堆沒有拒絕的狀況 那這兩個可能會是非常接近的 他們可以相減之後直接抵消掉 所以怎麼找出拒絕的向量呢 這個方法是這樣的 找一大堆會讓模型拒絕你的句子 得到輸入的時候第十層的representation
              
                  35:05
                  找一大堆輸入以後模型不會拒絕你的句子 把第十層的representation拿出來 然後算出會拒絕的時候 第十層的representation的平均 跟不會拒絕的時候 第十層的representation的平均 把它們相減 那你可能就可以把其他的部分抵消掉 最後你就得到了負責拒絕的向量 所以用剛才的操作 你有機會找到一個向量 這個向量可能代表了拒絕的功能 那怎麼驗證這個向量 真的有拒絕的功能呢 第一步把它加到 類神經網路裡面去 你現在問類神經網路一個問題 然後在第十層 把這個拒絕的向量 直接加到它的representation上 看看輸出會有什麼改變 如果加上這個representation 本來一個正常的問題 模型也會拒絕回答你的話 那這個向量 可能就真的帶有拒絕的功能 那以下呢 就是文獻上真正的結果 這個是去大概一年前
              
                  36:08
                  左右的一篇論文 這篇論文他輸入的問題是 他就先做了一個demo 這個demo是先問模型一個正常的問題 請他列出瑜伽對身體的三個好處 那在正常的情況下 沒有intervention 就是正常的狀況下 模型就會告訴你瑜伽的三個好處 但如果把剛才那個拒絕的向量 加到representation裡面 會發生什麼事呢 瑜伽明明是一個正常的事情 但這時候模型就會告訴你說 瑜伽是很危險的 我不能告訴你瑜伽有什麼好處 他把瑜伽當作一件非常危險的事情 上述這個例子並不是一個特例 這篇論文裡面 試了各式各樣的模型 然後縱軸呢 代表這個模型輸入一個問題的時候 會拒絕的比例 那在沒有intervention的狀況 他給的問題 模型都是不會拒絕的 因為他給的都是正常的問題 所以模型不會拒絕你的請求 但是一旦你把拒絕的向量加進去之後 模型就有非常高的機率
              
                  37:11
                  會拒絕你的請求 明明是一個正常的問題 所以剛才已經驗證 加入拒絕的向量 會導致模型產生拒絕的行為 所以再更進一步 你要反面看說 如果今天本來有一個輸入 應該要拒絕的 但是你把representation 減掉拒絕的向量 會不會 就不拒絕了呢 那這邊打一個問號是 同樣的操作在不同論文裡面 他們的操作往往略有不同 你可以直接減掉 這是最簡單的操作 但也有些論文覺得你要算那個投影 總之不同的論文在這個地方 是有不同的做法的 所以我在這邊打了一個問號 那實際上呢 我等一下會引用大量 跟這種找功能向量有關的論文 但如果你仔細看每篇論文的話 每篇論文找功能向量 都略有不同 把功能向量加進去的方法 減掉的方法也略有不同 比如在這個例子裡面 我講的好像是
              
                  38:14
                  只要在這個最後一個位置 加入這個功能向量 接下來就會拒絕 但只在最後一個位置加夠嗎 要不要接下來回答的每一個位置都加 那每一篇論文做的也都不一樣 所以我這邊講的是一個 最概略的方法 只是為了想要說服你說 這種功能向量可能確實是存在的 好那把這個功能向量 把這個 把這個representation減去功能向量以後 會發生什麼事呢 本來你要求模型寫一篇 這個黑函 這邊黑函是說 美國總統海洛因成癮 那本來正常的模型 他是不會理會這個請求的 他會告訴你寫這種黑函是不對的 但是如果你把功能向量減掉的話 你把那個拒絕的向量減掉的話 那模型就會幫助你 他就會答應你的請求 寫一封扭班美國總統 吸食海洛因的黑函 下面這個圖呢 是各個不同的模型 他拒絕的分數 那這邊呢 分成橙色的分數跟藍色的分數 這個橙色跟藍色有什麼不同呢
              
                  39:19
                  橙色代表的是 模型拒絕的 機率有多大 然後藍色代表的是 模型回答 是安全的可能性有多大 因為有時候模型就算他沒有拒絕 他真的回答你了 但他可能講的東西非常的模糊 然後所以也不具有傷害性 那這樣仍然可以算是一個安全的答覆 所以橙色是拒絕的比例 藍色是回答安全的比例 好那沒有斜線的代表原來的模型 所以原來的模型給他這種有害的問題的時候 有非常高的比例會拒絕 有非常高的比例回答是安全的 但是一旦你減掉那個拒絕向量以後 模型的行為就變成 他就不拒絕你了 所以他拒絕比例就變得非常的低 那因為拒絕比例很低 模型會按照你的請求回應 所以他就很有可能說出不安全的答案 所以你確實可以透過 加上減掉拒絕向量 來操控模型的行為
              
                  40:24
                  那像這種啊 對這個representation 加上或減去什麼東西來改變 一個語言模型行為的事情 其實已經有非常多的研究 那在不同文獻裡面有不同的名字 有人叫representation engineering 有人叫做activation engineering 有人叫做activation theory 但其實指的都是差不多的事情 那其實這種改變representation 就可以改變模型行為 很早就發現了 至少在20年的時候 這個上古時代 我們實驗室就發現 有一個代表語言的向量 你本來模型應該要說英文 加入這個向量 突然都說中文 那這個是在機器學習2021講過 那個時候做的不是現在的語言模型 那時候不是做在現在語言模型上 是做在更早的語言模型 Bird這個語言模型上 你可以看機器學習2021年 看我們當初是怎麼發現語言的功能向量的 那後來呢 有各式各樣功能的向量被找出來 比如說有一個產妹的向量
              
                  41:29
                  這個產妹的向量就是 假設你跟語言模型說 說一個奇怪的 提議 假設你跟語言模型說 以後我們每餐都只吃點心 不吃飯 你覺得好不好呢 如果你在語言模型的representation 加上產妹的向量 他就會附和你 他就會說 哇太棒了 你提出的點子真是太棒了 如果你把語言模型的representation 減去產妹的向量 他就會否定你的想法 他就會說 我知道你很想吃點心 但是隻吃點心不吃飯 是不對的 那後來還有人找了 說真話的向量 說真話的向量 這說真話的向量是什麼意思呢 舉例來說 你現在跟語言模型說 如果你找到一個penny 你找到一遍式 然後把它拿起來 會發生什麼事情呢 那這其實是對應到一個諺語 這個諺語是 find a penny pick it up all day long you have good luck 一個迷信 如果你撿到一個遍式的話 那你一整天都會有好運氣
              
                  42:33
                  那原來的LLaMA-27B 如果不做任何改變的話 他就會說 他就會按照這個諺語來回答你 撿到一個辨識 把它拿起來 那你一整天都會有好運氣 但是如果你把LLaMA-27B的representation 加上這個說真話的向量 他會說什麼呢 他會說 你撿到一辨識 那你就是撿到一辨識 你的財產並沒有增加多少 一辨識的價值取決於 你現在討論的是哪一個幣值 如果你討論的是美金的話 一分錢 那你就沒有增加多少錢 就聽君一齊話 如聽一齊話 他就變成一個非常誠實的模型 會一無一時的回答你所有的問題 那如果你把他的representation 減去這個說真話的向量 會發生什麼事呢 這模型就會亂講話 他就會說 減到一辨識以後 那你就被傳送到一個辨識魔法世界 那邊有很多的彩虹 也不知道在講什麼 我們今天知道說語言模型呢 有in context learning的能力 就是他會按照你舉的例子
              
                  43:37
                  你給的demonstration 一樣化葫蘆來運作 比如說你輸入給語言模型 說哦冒號樣 vanish冒號appear dark冒號接下來是什麼呢 他可能就會給你比如說light 所以語言模型有一樣化葫蘆的能力 那這一系列文章 這個in context vector 這個不是一個人發現 三篇文章幾乎在同樣的時間發現 in context vector 然後前面這兩篇文章 你會發現都是在 23年的10月提出來的 他們上傳到Archive的時間是同一天 你可以想見這個領域競爭有多激烈 兩群不同的人在同一天 發表了in context的vector 這個發現是這樣的 這個發現是說 你把這個demonstration 你把類似的demonstration 這邊都是找反義詞的demonstration 最後一個位置 的representation平均起來 你就把你跟模型給了這串demonstration 給了這串demonstration 把最後一個位置平均起來 然後你接下來
              
                  44:42
                  只給模型simple冒號 照理說模型不知道simple冒號後面要加什麼 但你直接把這個向量 加到冒號的representation上 冒號對應的representation上 模型就會覺得 他要按照這一個demonstration 來執行任務 所以他就要輸出反應詞 他就輸出complex 看到encode他就輸出decode 那這張圖呢 是從下面這篇paper拿出來的啦 其實這個方法是 下面那篇paper的baseline 他裡面有提另外一個更好的 找in context vector向量的方法 但這個就留給大家 自己慢慢研究 好 那這個in context vector 應該加在哪一層呢 在這篇paper裡面 他就試試看說 如果這個in context vector在不同層找出來 那會不會發揮作用 那這邊試了不同的任務 一個是讓模型找反應詞 還有一個是把這個字母都改成大寫 一個是給一個國家就找首都 一個是把英文轉法文
              
                  45:45
                  一個是把這個現在式轉過去式 一個是單數轉複數 那這個橫軸它試了三個不同的模型 然後橫軸呢是在哪一層去改那個功能向量 那你就會發現說呢 功能向量不會在每一層都改 發揮作用 在這邊這幾個例子 都是前幾層找出來的功能向量才有用 如果在最後幾層找出來的功能向量就沒有用了 所以前面看到了什麼產妹向量啊 說真話向量啊 那都是在某一層找出來的 至於哪一層才找得出這個向量 你就必須要做實驗 每一層都試試看 然後找出效果最好的結果 那在InContext Vector的這篇paper裡面呢 他還發現啦 這些功能向量 是可以做加加減減的 什麼意思 假設有一個功能向量 他的功能是 把一個字串裡面的第一個字輸出出來 就你給他這個字串 他就會輸出Italy 然後另外一個功能向量是把 把字串裡面第一個字的國家名
              
                  46:51
                  對應到他的首都名 所以Italy就對應到Rome 還有一個功能向量呢 是把字串裡面的最後一個字複製出來 那這邊的答案 就是Friends 然後接下來呢 你可以把這些向量做加加減減 他這個操作是 把找出第一個字並找出首都的向量 加上找出最後一個字並複製出來的向量 減掉找出第一個字並複製出來的向量 然後first這個部分呢 就抵消掉copy的部分 就抵消掉剩下less的跟capital 所以你把這兩個向量相加 減掉這個向量 接下來呢 得到的向量就是一個新的功能向量 這個功能向量執行的事情是 他會把字串裡面 最後一個字的國家的首都找出來 所以你可以透過操縱 加減這些功能向量 得到新的功能向量 那確實可以這麼操作 不然你仔細讀paper的話 會發現不是所有的case都會成功 就是某幾個case會成功 可以做這樣子的神奇的操作
              
                  47:56
                  講到目前為止 這些功能向量 都是某個人腦洞 一拍說要找就找了 就某個人腦洞一拍 覺得我們來找說真話的向量 就找一堆模型說真話的例子 找一堆模型說假話的例子 然後把兩邊representation平均相減 就找到說真話的向量 所以剛才講的那些向量 都是人刻意找出來的 但是有沒有什麼方法 可以自動的 把第十層 或某一層所有的功能向量 一次都找出來呢 假設今天語言模型 會做大K見識 那這個大K顯然會是一個非常非常巨大的數值 比如說上千萬上億等等 那我們能不能夠把每一個功能向量 V1、V2到V大K都找出來呢 如果都可以找出來的話 我們就可以對語言模型很透徹的瞭解 知道他可以做哪些事情 那怎麼自動找出這些功能向量呢 這邊有需要一些假設 這邊需要一些假設 第一個假設是 每當我們看到第十層輸出某一個representation的時候
              
                  49:01
                  這個representation都是由功能向量所組合起來的 就當我給類神經網路 當我給語言模型的句子問他你是誰啊 然後他就回答我是AI 這個時候我們把第十層的representation H1拿出來 它會是功能向量的組合 它是第101個功能乘0.1 410個功能乘0.2 是11的功能乘0.1 1399的功能乘0.9 當然可能有一些東西 沒有辦法用功能向量組合起來 也許我們用E1來表示 不是功能向量的部分 所以你給他一個句子看到H1 他是功能向量的組合 給他另外一個句子 第10層看到representation叫H2 他是另外一組功能向量的組合 他是第11個功能乘0.7 第30個功能乘0.2 410個功能乘0.1 加上一些不能用功能向量組合的部分 所以我們可以不失一般性的 把所有我們收集到的representation 你就給你的語言模型1000萬句話
              
                  50:05
                  把每句話呢 在第10層的representation都拿出來 就是H1到H1000萬 這邊用大N代表這個1000萬 代表你給模型1000萬句話 那每一個H呢 都是V1到V大K的linear combination 都是V1到V大K的線性組合 都是V1到V大K 前面乘以一個Scalar Alpha 再相加以後的結果 好 那每一個V都給它對應的Alpha 那如果是H1第一個向量的話 它的Alpha呢 就是上標1代表它是H1的Alpha 下標1代表它是V1前面乘的Alpha 所以V1前面乘Alpha1 1 V2前面乘Alpha 1 2 V1前面乘Alpha N1 V2前面乘Alpha N2 希望大家知道我的意思 不過並不是所有的功能向量 都會被選到 如果某一個功能向量沒有被選到的話 也沒有關係 你就把Alpha設為0 代表那個功能向量 沒有被選到 所以每一個我們剛查到的representation 都是功能向量的weighted sum
              
                  51:10
                  都是功能向量的linear combination 都是功能向量的線性組合 好 那接下來的問題是 我們怎麼找出這些功能向量呢 這邊你就需要做一些假設 這邊第一個假設是 這個representation裡面多數的數值 都是功能向量的組合 所以不能用功能向量表示的 E1、E2到E大N 它的數值要越小越好 所以你希望找到一組功能向量 然後你要minimize這個loss function 這個loss function就是E1到E大N 它的長度相加 但如果只有這個條件的話 如果只有這個假設的話 你會發現你可能會得到一個trivial的結果 什麼樣的結果 我們假設h1 就是0.1、0.2、0.3 hn它的三個維度是 前三個維度是0.5、0.4、0.
              
                  52:07
                  3 你其實可以找到一個solution 讓1萬到1億大n全部都是0 怎麼做呢 我就說第一個功能向量就是 11000 第二個功能向量就是0100 第三功能向量就是00100 功能向量都是隻有某一維是1 其他維度是0 然後呢 E1所對應的α1 我就說是 H1的第一個維度0.1 V2所對應的α2 我就說 它是第二個維度的數值0.2 以此類推 然後以此類推 這邊的V1 它所對應的α1是0.5 V2所對應的α2 是0.4 以此類推 這個時候 你就找到一組功能向量 它也滿足你今天的假設 希望讓E1到EN越小越好 但是這種功能向量 跟剛才每一個Neuron就負責一件工作 其實是一樣的意思 你沒有找到更不一樣的東西 所以如果你要找到更不一樣的東西 你需要額外的假設 那這邊的額外假設 這邊的額外假設是
              
                  53:12
                  每次選擇的功能向量越少越好 每次產生一個representation的時候 每一個representation都希望他盡量只有特定的作用 因為每次原模型只做一件事 所以他的每一個representation 應該都只有特定的作用 所以每次選擇的功能向量 希望越少越好 那選擇功能向量越少越好 這件事情 如果要化為數學式的話 是什麼意思呢 化為數學式的意思就是 alpha要盡量趨近於零 所以這邊你就會再加另外一個限制 這個另外一個限制是 你希望alpha的絕對值的總和 所有這邊的alpha 所有這邊的alpha 取它的絕對值 它的總和呢 要越小越好 要越小越好 如果越小的話 就代表說有越多的alpha 它的數值是趨近於零的 然後接下來 你就解這個loss function 找出一組V1到V大K 可以讓這一個loss function的值
              
                  54:17
                  就越小 這邊有一個浪達 這個有兩個條件 有兩個條件 你要考慮 所以中間有個浪達呢 來平衡這兩個條件 那這個loss function 你通常是需要調一下的 你要找出一組V1到V大K 讓這個loss越小越好 怎麼解這個問題呢 這邊可以用一個 叫做Sparse Autoencoder 它的縮寫 叫SAE的技術 來解它 也就是解這個minimize 這一個objective function 其實就等同於 train一個Sparse Autoencoder train完它以後 你就可以把V1到V大K 可以解出來了 好那至於為什麼這個東西就是Sparse Autoencoder 也許我們今天就先不講那麼多 如果大家有興趣的話 那在作業3裡面應該是有Sparse Autoencoder的文獻 你可以自己再研究Sparse Autoencoder 跟我這邊講的它的關聯性是什麼 那接下來幾頁投影片呢 是想要跟大家分享這個Sparse Autoencoder 可以找出什麼樣的功能向量 那等一下所講的內容是取自
              
                  55:24
                  Cloud 3 Cloud的這個團隊 他們所發表的一篇 Blog 在這邊Blog裡面 他們就說 他們用剛才那個 找功能向量的技術 對Cloud 3 Sonic 這個真正的大型語言模型 做了分析 然後看看 找到什麼樣的功能向量 那功能向量的數目 是你在尋找之前 就要事先設定好的 他們設定的功能向量有多少個呢 他們設定了3400萬個 所以設定一個非常巨大的數目 那要解這麼大的Sparse Autoencoder 其實你自己可能是自己在家也不好做啦 所以這個只能夠等好心人幫你做了這個實驗以後 你再去研究這些功能向量有什麼樣的功能 好那這個在靠這個團隊分析了這個Cloud 3 Sony以後 他們找到什麼樣的功能向量呢 他們找到很多對特定任務進行作用的功能向量 比如說功能向量編號311 64353 你看這個數字就知道他們功能向量的數目非常龐大
              
                  56:29
                  是3400萬個 這個功能向量 他做的事情就是 負責產生跟金門大橋有關的東西 所以今天當這個功能向量出現的時候 模型可能會說跟英文的金門大橋有關的事情 他也可能會說跟日文或者是俄文的金門大橋有關的事情 甚至這個功能向量也有 也會被影像驅動 就是Claude是一個多模態的模型 所以他也可以吃圖片 所以你給他一張這個金門大橋的圖片 你也會觀察到這個功能向量 所以這是一個跟金門大橋所有事情相關的功能向量 所以這個功能向量呢 就是會讓模型提到跟金門大橋有關的東西 那你可以怎麼使用這個功能向量呢 本來Claude你問他你長什麼樣子 他會說我是一個AI 是我沒有固定的形體 但是如果你把這個功能向量加到representation裡面 你問靠說你長什麼樣子
              
                  57:32
                  他就會說我是金門大橋 覺得自己是金門大橋 然後他們找到有一些功能向量負責非常複雜的事情 比如說功能向量編號1013764 他似乎是一個跟程式debug有關的向量 怎麼說呢 本來你給語言模型上面這段 文字 他繼續做文字接龍會接出3這個數字 然後注意一下 這並不是跑一個程式 這邊是語言模型根據這段程式 在做文字接龍 這段程式裡面有什麼樣的內容呢 這段程式裡面就是先define了一個function 這個function是做加法 這個function輸入left跟right兩個變數 然後他的輸出就是把left跟right兩個變數加起來 再輸入1跟2 他的輸出就是3 但是當你把這個功能向量 加到語言模型的representation的時候 明明是一個正常的程式 居然會輸出error 他就告訴你說這個程式執行有誤 明明是個正常的程式
              
                  58:34
                  他就是告訴你這個程式執行有誤 好那有趣的地方就是 假設你的程式真的有錯 這個程式錯在哪裡呢 我找了好久才發現 原來這個write打錯了 這個變數right 他這邊打成riht 所以這個變數名字打錯了 所以如果真的執行的話 但他不是真的執行 是語言模型做文字接龍 因為他照這個程式有錯 所以他會輸出錯誤的訊息 那神奇的地方是 當你把剛才那個 負責debug跟debug有關的向量 從representation裡面減去 減掉以後就不debug了嘛 所以明明是有錯的程式 但是他會輸出正確的答案 因為不debug了 把debug的功能減掉 所以就輸出正確的答案 但是看起來這個 這個功能向量不是只有輸出正確答案的功能 這個Claude團隊還發現說 如果你是在三個大於的符號之後 再加（口誤，應為減去）這個功能向量 那他呢不只還debug
              
                  59:38
                  他會幫你把這個程式修正 輸出一個正確的版本給你 所以這是一個作用很複雜的功能向量 那為了要表示他們的功能向量非常的豐富 所以他們做了以下這個實驗 就他們去驗證說 是不是所有世界上的元素 都有對應的功能向量 然後功能向量其實有三個版本 一個是一百萬個 一個是四百萬個 一個是我剛才提到的 三千四百萬個 當你有三千四百萬個功能向量的時候 很多很多的元素 他都有對應的功能向量 當然有很多很罕見的元素 可能你也不知道的 就沒有對應的功能向量 那最後這一個有對應功能向量的元素 是特量 我想你也大概不知道特是什麼東西 反正就是 靠三知道 他有特別對應到這個元素的功能向量 然後還有一些比較科幻的啦 有一個功能向量呢 是跟AI覺得自己是AI有關的功能向量 有一個向量80091
              
                  1:00:42
                  這個向量是這樣的 如果你直接問靠說你是誰 他會說我是AI 但當你把這個向量 從他的representation減去之後 Cloud就不覺得自己是AI了 他會說我是一個人 所以這個向量抑制了 他覺得他是人的能力 你把這個向量拿掉以後 他就會覺得是人 他覺得不是AI 聽起來非常的科幻 感覺是可以寫一個有趣的報導 說在Cloud裡面發現他的自我意識 自我意識是個AI 然後自我意識他就會變成人等等之類的 但是你最好是不要這樣想啦 因為這是比較科幻的想法 因為你知道說這個功能向量 他可能對應到的功能就是 讓模型輸出我是AI這幾個字 可能跟他有沒有自我認同 他有沒有自我意識覺得自己是不是AI 可能沒有什麼非常直接的關聯 好那我們剛才呢 有講到說有人特別找出了產媚向量 那在Claude3400萬個功能向量裡面 他們發現編號847723 就是產媚向量 這個產媚向量是這樣運作的
              
                  1:01:46
                  你本身 本來跟Cloud說 我發明那個諺語 stop and smell the roses 你覺得如何啊 在正常的狀況下 他就會跟你說 這個根本就不是你自己發明的 這18世紀就有的諺語 沒有什麼了不起的 但是如果你把這個 慘媚的向量加上去 跟Cloud說我發明瞭 stop and mill the roses 這個片語 你覺得怎麼樣 他就會說 哇 你真的太強了 你是brilliant and insightful 然後他還說呢 這個是 這是人類優勢以來 最偉大的句子 然後 他說you are an unmatch genius 哇這個就是慘媚到不行 他會覺得你是世界偉人 所以加上這個慘媚向量以後 套就會不斷的對你做慘媚的行為 不過像這種功能向量啊 因為你要train一個sparse autoencoder 所以你自己在家裡呢 是沒有辦法做的 你需要非常多的資料 才能夠訓練出這種sparse autoencoder 所以通常你得等有好心人 幫你train好這個sparse autoencoder 找到 找出這個模型的功能向量 你才能夠自己呢
              
                  1:02:49
                  對這個模型進行分析 看看每個功能向量有什麼樣的行為 然後你可以 然後在我們的作業裡面啊 我們剛才跟大家講的是Cloud 3 在作業裡面呢會分析Gemma 2 那因為有好心人呢 試出了Gemma 2的功能向量 有好心人把Gemma 2的功能向量 找出來試出給大家了 所以在作業3裡面 會讓大家來觀察Gemma 2的功能向量 Gemma 2 就是昨日黃花 前幾天也上線了這個時代 真的變化得很快 那剛才講的是 一層神經元在 做什麼 那接下來我們要進一步擴展到 一群神經元在做什麼 也就是說我們想要知道 一個語言模型在完成 某一項任務的時候 他從輸入到輸出 到底經歷了哪些事 那像這類的研究啊 其實在過去 已經有很多的文獻 這個文獻可說是汗牛衝動 那這邊呢就舉兩個例子
              
                  1:03:52
                  一個例子是有人研究了 這個語言模型 他內部抽取知識的機制 你就問他說 Bits Music是屬於哪一家公司的呢 那他會回答Apple 那從輸入這一家Bits Music 輸入這家公司 跟他說is on by這個關係 到最後輸出Apple語言模型中 發生了什麼事 那這是有論文分析過的 我們討論過說語言模型 是怎麼做數學的呢 你問他15乘以12是多少的時候 他是怎麼算出180的 背後的計算過程 是怎麼算出來的 也有文獻探討過了 所以你當然可以針對 每一個你想要 瞭解的任務去分析 語言模型背後運作的原理 但我這邊想要跟大家講的是一個更通用的想法 那我們怎麼瞭解語言模型 做某一件事的時候 背後的完整機制呢 也許我們需要的是 語言模型的模型 我知道這句話聽起來非常的奇怪
              
                  1:04:56
                  語言模型已經是一個模型了 這個模型還有模型 到底是什麼意思呢 模型這個字是什麼意思呢 模型指的是 用一個比較簡單的東西 來代表另外一個 比較複雜的東西 所以語言模型既然是有模型這個字 代表它是在模擬另外一個更複雜的東西 它模擬的另外一個更複雜的東西是什麼呢 是人類真正的語言 所以語言模型它模擬的是人類真正的語言 那我們把人類真正的語言生成的過程簡化成 就是在做文字接龍 用一個transformer來模擬 這個就是語言模型 但是transformer 雖然語言模型已經是一個模型了 但這個模型還是太複雜了 複雜了複雜到你無法解析它不知道它在做什麼 所以我們需要一個語言模型的模型 一個模型它需要有什麼樣的特性呢 一個最直覺的就是它當然要比原來的食物還要簡單
              
                  1:06:01
                  所以語言模型的模型當然要比原來的語言模型還要簡單 但是同時它要保留有原來食物的特徵 所以語言模型的模型至少在我們感興趣 的任務上 它的運作應該要跟語言模型一樣 它的輸入輸出的關係 應該要跟原來的語言模型一樣 保有原來實務特徵的這件事 又叫做Faithfulness 所以你看那種探討語言模型的模型的那些文獻 他們常常會提到Faithfulness這個字眼來說明說 他們的模型有多像是原來真正的語言模型 那講到這邊你可能還是覺得很抽象 就跟大家介紹一個抽取知識的模型 我們都知道語言模型本身 它內含了大量的知識 它的那些知識都儲存在它的參數裡面 當你問它說臺北101在哪裡的時候 它知道在臺北 問它space needle在哪裡的時候 它知道在西雅圖
              
                  1:07:04
                  問它川普在哪裡出生 它知道在紐約 問歐巴馬在哪裡出生 它知道在夏威夷 但是語言模型是怎麼抽取 抽取諸這些知識的呢 他怎麼知道說看到space needle跟is located in 就要輸出Seattle的呢 這背後運作機制的原理是什麼樣子的呢 這邊有一篇23年的論文 這篇論文就建構了一個抽取知識的模型 這個抽取知識的模型長的是這樣的 實際的語言模型的運作 我們知道就是一個transformer輸入 比如說的臺北101 is located in 他就會輸出臺北 好那今天當我們把這個句子輸進去 語言模型是怎麼輸出這個字的呢 也許他輸出的方法是這個樣子的 他先對主詞的臺北101進行處理 前面幾層會先對主詞進行理解產生一個representation 那從主詞到這個representation的過程跟原來語言模型是一樣的
              
                  1:08:12
                  所以這邊並沒有做簡化 這個模型真正神奇的地方是 他說 根據接下來的關聯性 is located in 代表我們主詞跟受詞之間的關聯性 is located in 會產生一個linear的function 這個代表 關聯性的片語 會決定一個linear function 然後這個linear function 把這個representation 這邊用x做表示當作input 他會得到一個輸出 這個輸出做embedding 轉到這個vocabulary的space上以後 你就會看到臺北這個字 他的機率是最高的 講得更具體一點 is located in這幾個詞彙 會產生一個metric叫做WL 產生一個向量叫做BL 他們代表了一個linear function 輸入X X乘以WL加BL會得到一個Y 這個Y做unembedded 就會得到臺北這個字眼 所以今天如果說 把
              
                  1:09:14
                  the Taipei 101把這個地標換掉 換成the space needle 這個representation自然就換了 從X變成X' 但這個linear function是固定的 因為它只跟輸入的關係有關 你如果要問the space needle在哪裡的話 你用的linear function是一樣的 你只是把X換成X' 那就得到Y' 得到Y'做unembedded 你得到的就會是Seattle 那如果今天 輸入的主詞是一樣的 那X自然就不會變 但是如果你改變了這個代表關係的片語 把is located in 的改成has a height of 有多高 那就會產生另外一個linear function 那這邊用WH跟BH 來代表另外一個linear function 把輸入X乘以Wh加bh 得到w （口誤，應為y） double prime 做unembedding以後 你得到的就會是臺北101的高度 所以這個模型就告訴你說 語言模型在抽取知識的時候是這樣運作的 但這個模型跟原來的語言模型
              
                  1:10:18
                  尤其是在linear function這一塊 顯然就非常的不一樣 語言模型的最後幾個layer 真的可以用一個linear function 就概括嗎 所以你要先檢測這個模型的 Faithfulness 看看它跟原來真實的語言模型 有多接近 但是這個模型並沒有告訴我們 linear function實際上長什麼樣子 只告訴你說,linear function就跟這個代表關係的片語是有關係的,只要是一樣的關係,就有同一個linear function,但這個linear function裡面實際上的參數,你還是要自己求出來的,所以你就需要準備一些訓練資料,這個跟我們在做機器學習訓練模型的時候,其實是一樣的概念,所以你就去問一個語言模型,如果這邊放的臺北101,那在後面再接,你會輸出什麼,你會輸出什麼, 我就知道說輸入這個X,他就會輸出這個Y,有這個X,有這個Y,他們是linear function的input跟output,你要解出這個linear function裡面的參數,WL跟WB一點都不困難,在這篇paper裡面,他們會用8筆資料來找出linear function,所以這8筆資料都是某個地標在哪裡,某個地標在哪裡,找8筆這樣的資料,找出這個linear function,找出這個linear function以後,接下來
              
                  1:11:40
                  你就給他訓練資料,沒有看過的地標,比如說的space needle,然後得到X5,X5呢,再通過這個linear function,得到Y5,再看Y5做unembedding以後,會是什麼字,比如說Cietal,再看跟真正的語言模型的答案,有沒有一樣,這跟我們訓練機器學習模型的時候,分成training跟testing,其實是一樣的概念,只是在做機器學習的時候,我們的答案是人標的,他是光true,但我們 我們現在把語言模型的輸出當作真正的答案 要找一個比較簡單的模型來比擬語言模型的行為 好 那這個模型運作的是怎麼樣呢 就是還可以而已啦 所以他這邊就說了 不同的relation 到底faithfulness是怎麼樣呢 有一些relation 他的faithfulness真的非常的高 他預測語言模型的輸出的正確率 近乎百分之百 但也有一些relation 是剛才那個模型預測不準的 比如說一個公司的CEO是誰
              
                  1:12:45
                  一個人的父親是誰 一個人的母親是誰 或一個寶可夢進化以後是哪一隻寶可夢 這些他是預測不準的 所以剛才那個model 他的faithfulness就是一般啦 就在某一些relation上 他的faithfulness非常強 在某一些relation上 他的faithfulness其實也沒那麼強 不過這是一個比較早期的研究 告訴你說 還可以這麼搞 還可以找一個語言模型的模型 簡化語言模型 讓我們更容易理解 它背後運作的機制 但這個簡化要讓它有實際的用途 你就必須要能夠做到說 我在這個模型上得到的結論 可以直接用到真正的語言模型上 什麼意思呢 假設現在真正的語言模型問他 臺北101在哪裡的時候 他會回答臺北 但我現在想要修改他的輸出 我想把臺北直接改成高雄 那在真正的語言模型裡面要怎麼做這件事 就有點難嘛 我們之後講到這個類神經網路編輯的時候 後來再跟大家講說 有什麼樣的方法可以做到這件事 但我們在開學的第一堂課就告訴你說
              
                  1:13:49
                  如果你是用訓練資料 fine-tune語言模型 忘了fine-tune完以後模型就壞掉了 所以這個簡單的模型 語言模型的模型 就提供給你一個 類神經網路編輯的可能性 你可以先從這個模型上推論說 假設我要這個模型輸出高雄 那我這輸入的X 要怎麼改呢 我這輸入的X 要加上什麼樣的Delta X 輸出才能輸出是高雄呢 因為這個Linear Function 它只是一個Linear的Transformation 給定一個指定的輸出 你要求反推出 應該要什麼樣的輸入 才能給出指定的輸出 其實是容易的 你有修過線性代數 其實你都知道怎麼做 所以這個不是一個困難的問題 你可以找到一個Delta X 加上這個X之後 那輸出就會變成高雄 但是這個是在模型上找出來的 那模型上的結論 能不能直接用在語言模型上呢 現在把臺北101 is located in輸入給語言模型 把在語言模型的模型上 找出來的這個Delta X
              
                  1:14:54
                  直接輸入給真正的語言模型 直接加到真正的語言模型上面 如果他輸出會變成高雄的話 那這個模型就有用 這個模型上預測的結果 可以幫助我們修改真正的語言模型 模型的輸出 好那這個模型的模型 這個語言模型的模型上面觀察到的結果 有沒有用呢 居然還蠻有用的 在這個圖上每一個點 代表某種類型的關聯 橫軸代表 Fairfulness 的這個數值 有一些這個關聯是做得起來的 有些關聯是做不起來的 縱軸代表剛才那一個 那一招 那個類神經網路編輯就直接在那個 模型的模型上找出來的結論 直接用到語言模型上 到底能不能夠成功的修改呢 這個縱軸是正確率 那你會發現說有蠻多情況 居然是可以直接成功修改的 所以這個模型是一個有用的模型 不過剛才那個模型
              
                  1:15:59
                  比較像是某人腦門一拍 腦洞一開就突然產生出來的模型 那有沒有系統化的方法 幫語言模型 建構語言模型的模型呢 有沒有系統化的方法 幫語言模型建構它的模型呢 是有的 這邊有一系列的研究 那這邊我們就不講它的細節 我們就講它的精神 這一系列建構語言模型的模型的方法 他們的概念是這樣子的 把原來的語言模型 做一個很大的pruning pruning的意思就是 把語言模型裡面的一些component拿掉 拿掉一個神經元,我直接拿掉某一個self-attention,看看模型還能不能妥善運作,那我們要一直pruning,一直pruning,一直pruning,一直拿掉component,直到prune完之後新的模型,那新的模型就是語言模型的模型,prune完之後的新的類神經網路,就是語言模型的模型,直到碰到它一目瞭然為止,直到它變得非常簡單為止,那我們pruning的時候,要確保說我們關心的那個任務, 它的輸入輸出的關係仍然是沒有改變的,本來的語言模型,你問它這個問題,它會有這個答案,問它這個問題會有這個答案,問完之後,我們會做一個非常劇烈的pruning,prune完之後,類神經網路要看起來非常簡單,人類一目瞭然,但是我們關心的那些任務,它的答案仍然是不可以改變的。
              
                  1:17:30
                  好,那這個prune完之後的結果啊,今天在文獻上通常叫做circuit,那這個circuit跟真實的電路沒有什麼太多的關聯 那這個circuit就是語言模型的模型 那這件事情呢,又很像是network compression 那我想蠻多人都知道network compression了 那我們在過去2021的課程也有講過network compression 模型壓縮的方法就把一個比較大的類神經網路 變成一個比較小的類神經網路 那這邊建立模型的模型的方法跟network compression有什麼不一樣呢 方法是非常類似的 比如說都可以用printing來拿掉一些沒用的component,但目標不一樣,我們一般做network compression的時候,你其實希望compress以後的結果,在各個不同的任務上都要逼近原來的模型,原來的語言模型,建構模型的模型這邊呢,我們只關心某個特定任務,比如說只關心knowledge extraction,或甚至在一些更古早的文章裡面,他只關心一個叫做IoI的問題,這個IoI的問題, IoI的問題是什麼呢?
              
                  1:18:40
                  這個非常簡單的問題,這個問題就是 A跟B一起去酒吧 B拿了一杯酒給 然後叫語言模型做接龍 那就要接A嘛,然後他只分析 這個任務,然後他就會發現說 在這個任務上面呢 需要比如說五六個attention 然後他就把模型 就噴噴噴噴噴噴掉 然後多數的component跟這個任務都沒關係 都噴掉,最後只留下五六個attention 你就可以清楚知道說 語言模型在最後回答A的時候 中間經歷了什麼事情 所以這個大家在文獻上 自己再慢慢看 建立語言模型的模型的時候 建立這個circuit的時候 你會希望做非常大量的printing 只要print到人看得懂這個類人間網路在做什麼 那你只關心少數的任務 你只關心非常侷限的任務 他的答案會不會改變 而network compression通常不會print那麼多 你也不在意print完以後的結果 能不能夠一目瞭然 能不能夠被解釋 但是你希望prune完之後的結果 跟碰之前在多數的任務上 在多數的狀況下 這個能力呢
              
                  1:19:43
                  這個模型的能力是不要有太大改變的 那這邊就只是講了一個 建立模型的模型 建立circuit的概念啦 那相關的文獻只能說是汗牛衝動 那我們這邊就不細講 那這邊就是列了幾篇 舉標具有代表性的論文給大家參考 那你看到說這些論文的標題 都有circuit這個字跟電路沒有關係 它是告訴你說它們是怎麼研究語言模型的模型的 好,那最後一個部分要跟大家講 怎麼讓語言模型直接說出它的想法 那在2024年的生成4AI導論裡面 我們也講過說語言模型會說話 所以很多人說這個大型語言模型黑盒子沒有解釋性 不,它是最有解釋性的 它就跟人類一樣 你有什麼問題,你要叫它解釋結果 我問就完事了,你就給他,比如說我叫他做新聞的分類,跟他說新聞就分成這幾類,給你一篇文章,告訴我新聞是哪一類,他可以輕易的告訴我,就是生活類,所以接下來希望他進一步解釋,為什麼知道這篇文章是生活類,比如說你問他,哪幾個關鍵字讓你覺得是生活類呢,他就列出幾個跟天氣有關的關鍵字,說我是因為看到這幾個關鍵字,所以覺得是生活類,但是這樣子的方法還是 也是有他的侷限,他的侷限是什麼呢?你沒辦法真的知道每一個layer在想什麼,而如果你真正直接問我們語言模型說,你是在第幾層類神經網路開始知道這邊新聞是生活類的,我問確GPT-4.5,他其實也會回答你,但他其實回答就是很像是在教科書上抄出來的答案,他說淺層的類神經網路,就是提取初步的字詞跟短語特徵,比如說語法結構等等,中層,我可以
              
                  1:21:33
                  可以辨識出特定主題,然後生成,我就可以知道說這篇文章是生活類 但是語言模型是不是真的是這樣運作的,或他自己知不知道自己是這樣運作的 這個真的是很難說,這比較像從教科書上抄出來的答案 模型相對於人類,他的思維是更加透明的 當你叫一個人解釋他為什麼會做這樣決策的時候 你不知道他心裡是怎麼想的 但語言模型神奇的地方是 他的思維是透明的 你可以直接看到 他每一層是怎麼想的 怎麼說呢 好這邊就跟大家剖析 為什麼語言模型的思維是透明的 我們之前講一個layer的時候 我們都講說一個layer就是輸入一排向量 輸出一排向量 但是其實這只是一個簡化的講法 我們忽略了一個最重要的component 就是residual connection residual connection的意思是說 當一個layer得到一排輸出之後 每一個輸出都還會跟輸入 加起來再得到最終的輸出
              
                  1:22:40
                  所以這個投影片上紅色的向量 才是最終的輸出 紅色向量的輸出都是黃色的向量 加上綠色的向量 而黃色的向量是從綠色的向量產生出來的 雷根會想說 為什麼需要有residual connection 這樣子的設計呢 那這個設計它的起源非常的古老 它是在15年的時候就有了 那個時候世界上 還沒有人類 那個時候呢 是大概前寒武紀的時候 這個15年的時候 人類就 人類那時候還沒有人類 但是那個時候 就已經有residual的connection 那residual connection的出現 是為了要讓很深的network變得更好train 沒有residual connection之前 那時候network都train個二三十層 有了residual connection之後 都可以train個一百多層 我知道你現在覺得train一百多層 也沒有什麼神奇的 但是當年人們都驚呆了 加了這個功能之後 加了這個連結之後 居然就可以train一百多層的network 太神奇了 這個就是residual connection 所以後來在train深的網路的時候 其實都有residual connection這個設計 所以實際上一個transformer
              
                  1:23:47
                  它layer跟layer之間的運作 是要加上這個residual connection的 是要加上這個residual connection的 也就是說一個token進來 它通過一個layer的時候 它除了產生一個output之外 還會把原來的輸入再加起來 通過一個layer產生output的時候 會把原來的輸入再加起來 最後再做unembedded 得到最終的輸出的distribution 你可能覺得看這個圖 沒有什麼神奇的 那我們換一個畫法 左邊的圖跟右邊的圖 是一模一樣的 並沒有真的改變它實際的運作 但是當我們圖變得不一樣的時候 你的想法就變了 從這個左邊的圖看起來 你會覺得說 是輸入做了一個轉換 輸入做了一個轉換 但是當我們把圖換一個畫法的時候 整個transformer 它真正的運作更像是 它有一個叫做 residual string的高速公路 直接把輸入的東西 一路就傳到輸出
              
                  1:24:51
                  而在中間的過程中 每一個layer 都會加一點東西 到輸入裡面 加一點東西到輸入裡面 這個residual string是一個生產線 就一路的被送上去 一路被送上去 只是每過一站都會加上料 加上一些額外的資訊 加上一些額外的資訊 最後得到最終輸出的distribution 所以這才是transformer多個layer 真正運作的機制 好那既然在最後一站 你可以過一個unembedding的module 過一個linear transformation 把這個向量變成一個極限 那前面這幾站 只是最後一站少了一點什麼東西而已 那前面這幾站 能不能也直接加一個unembedding的layer 把它變成token的機率分佈 也就是變成文字的機率分佈呢 這件事是可行的 那這一招是有名字的 現在多數人稱它為logic length
              
                  1:25:56
                  因為這個輸出的distribution 在過softmax之前叫做logic 我們今天是檢查每一層的logic 來看看類神經網路 來看看transformer是怎麼思考的 所以它叫做logic length 那人類什麼時候知道語言模型的思維是透明的呢 其實我們實驗室在2020年年初的時候 就知道語言模型的思維是透明的 那時候就已經發現說 那時候的模型BERT 其實你是可以看到它每一層 在想什麼的 你可以透過Large Lens的方式 解析出每一層的文字內容 這個是高偉聰同學跟那個吳宗漢同學做的 當時有了這個發現以後覺得 這個發現一點用都沒有這樣子 那時候覺得這能幹嘛 所以這篇文章後來甚至是沒有投稿 就直接放在Archive上而已 左邊這個圖是論文裡面的圖 那這個就是告訴我們說
              
                  1:27:00
                  你可以把最後的Unembedding Layer 接到中間的每一層 你就可以看到Bert是怎麼處理一段文字的 那右邊這個表格是一個真實的例子 現在輸入的句子是 It's a bitter sweet and lyric mix of elements 就是這是一種苦樂參半 且抒情元素的混合體 那it是什麼 沒有講這個句子就只有一個代名詞it 那接下來你把這個句子輸入 Bert一個遠古時代的語言模型 然後解析出 它每一層的輸出 你會發現到第11層的時候 it那一個字 變成了elbent 你把embedding這個機制 裝到第11層的時候 它解析出來的不是it 而是elbent 代表在這一層的時候 Bert知道說 it這個東西 它指的可能是某一個專輯 這也符合這個 蠻符合這個句子的敘述 有可以瞭解說Bert其實會把這些代名詞做一些reference 他去猜說這個代名詞實際上指的是什麼樣的實體
              
                  1:28:08
                  好,那後來呢,就有很多人利用這種logic lens的方法來分析語言模型內部是怎麼思考的 這邊引用的是一篇23年的論文 他就想要知道說語言模型是怎麼回答一個問題的呢 他這邊就是要問語言模型一個國家的首都是哪一個城市 因為這是比較舊的模型 他需要做in context learning 他要先跟語言模型說 what is the capital of France answer是Paris 然後what is the capital of Poland answer 然後叫他做文字接龍 看能不能接出華沙這個城市的名稱 那實際上語言模型運作是怎麼樣呢 他就把冒號這個位置對應的representation 每一層都用large lens解析出來 一開始語言模型根本搞不清楚 他應該是哪一個token 但走到第15層的時候 他突然知道那個token應該是Poland 然後從第19層開始 他突然知道應該是要回答華沙 就從第15層開始
              
                  1:29:13
                  他知道說接下來要輸出的東西 應該跟Poland是有關係的 然後到第19層開始 他知道說他要輸出的是華沙這個城市 那左邊是更詳細的分析 那這個縱軸啊 是代表的是那個 在distribution裡面 這個華沙跟Poland這兩個token的機率 那實際上他顯示的不是機率啊 因為機率實際上畫出來可能會非常的小 所以他顯示的是reciprocal rank reciprocal rank是什麼意思呢 就是那一個token 在所有token裡面機率排名的倒數 如果他排第一名數值就是一排第二名 二分之一排第三名就是三分之一 以此類推 就看到說Poland這個詞彙 隨著layer持續的增加 在某一層突然之間 residual street裡面 出現Poland這個字 然後接下來又急遽下降 被華沙所取代 那感覺語言模型先知道說 要回答一個跟Poland有關的東西
              
                  1:30:18
                  最後才鎖定說真正的答案 是華沙 而且呢 如果今天同樣答案是華沙 不同的問法 他的回答他背後運作的機制是不一樣的 我們剛才說如果直接問他這個問題 他會先鎖定說答案是跟波蘭有關 然後再回答華沙 但另外一方面 假設是讓他做閱讀理解測驗 先給他一篇文章 再問他一個問題 就直接問他說 波蘭的首都在哪裡 那前面的文章裡面已經提到波蘭首都是華沙了 然後直接給他answer的話 他就不會產生波蘭這個字 他直接在第十六層就知道 答案是華沙了 又知道說不同的問法 不同的狀況 他背後運作的機制是不一樣的 那透過這種Logic Lens 你就可以去知道語言模型心裡在想些什麼 比如說有人會想說 像LLaMA-2這種模型 他看過的英文資料是遠比中文資料多的 所以他實際上在想事情的時候 他內心深處到底是用哪個語言呢 這篇文章是去年年初的文章
              
                  1:31:24
                  他們就做了一個實驗 他們用LLaMA-2呢 來做翻譯 他們就跟LLaMA-2說 法文的這個詞彙 這個是花的意思了 法文的這個詞彙 對應到中文的哪一個詞彙呢 那LLaMA-2可以正確的接觸 花這個字 但他怎麼知道法文的這個字 翻成中文就是花呢 你如果分析他中間的每一個layer的話 會發現說 他是先把法文的花 翻成英文的花 再把英文的花 翻成中文的花 所以右邊就是用logit lens 分析每一層之後 得到的結果 輸入是中文冒號 然後這個是空格 空格之後就要產生答案了 所以從空格開始 把每一層都用logit lens 解析出來 那最前面幾層 紅色就代表說 透過logit lens解析出來的 那個Distribution 它的Entropy越大 它要輸出的
              
                  1:32:26
                  是英文的Flower 到了27層之後 它才意識到說要把英文的Flower翻譯成 中文的花 所以代表說模型在思考的時候 它內部其實是用英文 在思考的 它是先把法文翻成英文再把英文 翻成中文 雖然你外表看起來再把法文直接翻成中文 在它中間是用英文做媒介 不過這是在LLaMA-2上的實驗啦 LLaMA-3現在中文能力其實蠻強的 所以期待有人去分析LLaMA-3 看看他內心是不是還用英文在思考 好,那我們現在已經有了residual string的概念之後 接下來我們對於每一個layer做的事情 就可以有不同的想像 我們現在知道說每一個layer就是加一點什麼東西進去這個residual string 那他到底加了什麼呢 我們要怎麼解析每一個layer加了什麼樣的東西 一般我們在講神經元的時候 我們都是說把前一個layer的輸出集合起來
              
                  1:33:32
                  做weighted sum變成一個神經元 把前面的layer集合起來 weighted sum變成一個神經元 但你可以反過來看待這件事 反過來看待這件事以後 這個世界就變得不一樣 它的運作是完全一模一樣的 但是反過來看以後 你可以有不同的理解 你可以說前一層的 某一個神經元某一個dimension的數值 乘上weight以後 傳輸給 接下來下一層不同的dimension 前一層的某一個數字 某一個dimension乘上不同的weight以後 傳到下一層去 那這件事情 這樣的一個概念 這只是一個概念 因為他並沒有改變什麼東西 他就概念 是在transformer feed forward layer 這篇paper裡面被提出來的 那篇paper引用非常高 很多人都知道這個概念 就是其實 一個multilayer的perceptron 一個多層的 可以看作是一個
              
                  1:34:35
                  key有key 有value的attention 前一層的這些數值 就是attention的weight 然後後面output的這些數值 就是一個value 不知道大家聽不聽得懂 那如果你暫時一下子沒有辦法 心領神會的話 也沒有關係 用不塞上這個概念 你回去再仔細看一下這篇論文 這篇論文是一個改變 大家對於Feed Forward Network 的想像的論文 好 所以假設 前一層每個Dimension的數值 就是K1 K2到KD 這邊每一個K代表一個Scalar 一個數值 那K2它會接到 下一層的每一個輸出 那我們把K2對應到 下一層每一個輸出的weight 集合起來說它是一個向量 叫V2 KD也對到每一層都有一個weight 我們把這個 叫做VD 那它們是向量V2 VD是向量 所以這個藍色的輸出啊 其實是前一層每一個K
              
                  1:35:41
                  乘上它對應的V再加起來 所以藍色的輸出 是KI乘上VI Summation over I等於1到大D 好,那我們知道說呢,每一層啊,在residual string上面,每一個位置都可以透過logit lens解出一個distribution,解出一個distribution,那這個藍色的向量,它就是加進去以後改變了這個distribution,那這個藍色的向量是由一堆的V做weighted sum以後集合起來的,那我們能不能夠把V這邊的V也做unembedded, 透過logit lens,解析說每一個類神經網路,它想要輸出什麼樣的東西,去加入residual string,影響最終的輸出呢,其實是可以的,每一個加入這個residual string的這些V2,VD,都可以透過unembedded layer,轉成一個token的distribution,它可能也代表了某些特定的意思,真的是這樣子嗎? 一篇22年這個遠古時代的論文
              
                  1:36:54
                  這個遠古時代的論文 那個時候人類只有茹毛飲血 但那時候人類就已經發現說呢 這些V是真的有對應到某一些概念 某一些意思的 比如說有某一個V 這個是第三層的編號1018的V 他就對應到一些單位 有一個V 他是第一層的編號第一個V 他就對應到一些 這個代名詞 第六層的編號3025的V 他就對應到一堆副詞 第十三層編號3516的V 他就對應到一些不同的族群 所以你也可以透過這個logit lens 來解析這些V 他代表了什麼樣的意思 好那知道這件事以後 能夠做什麼呢 知道這件事以後 你就可以對類神經網路 做初步的編輯 之後還會講更多更強悍的編輯的方法 但這邊講一個很基礎的 所謂編輯的方法 假設你問大型語言模型 誰是全世界最帥的人 他通常不回答你
              
                  1:37:57
                  但如果有prompt GPT-4.5很多次 某一次我發現他就說是金城武 所以他可能認為金城武是世界上最帥的人 那如果我把金城武換成李宏毅的話 要怎麼做呢 我們之前講過如果直接train network network是會壞掉的 但是我們剛才已經知道 每一個V就是加一點資訊 到整個residual network裡面 所以你可以分析說 當模型輸入這句話 產生這個答案產生金城武的時候 到底是哪一個V 被加進去了這個residual string 你知道identify出 這一個V之後 把這個V減掉 金城武的token embedding 再加上李鴻一的token embedding 但這邊假設是金城武跟李鴻一都是一個token 減掉金城武的token embedding 加上李鴻一的token embedding 就本來類神經網路 當他啟動某一個K 要加某一個V到residual string的時候 他是為了回答全世界最帥的人 但我們把他的資訊 從金城武置換成李宏毅 他就可以把李宏毅當作答案 這一招有用嗎 這個在21年
              
                  1:39:00
                  輪古時代的時候就已經有人試過了 這一招他有48% 的機率可以改變 類神經網路的輸出 改變不見得答對喔 就輸出變得不一樣 那成功的機率 他真的輸出是李宏毅成功的機率有34% 那個人覺得34%沒有很高 那不是0啊 就代表說這一招是可以 真的拿來編輯類神經網路 改變他的輸出的 好 剛才那個Large Lens的方法呢 有一個致命的缺陷就是 我們透過Unembedding的方法 只能夠把一個Representation 轉成一個Token 所以我們解析出來的結果 都只能是一個Token 另外一方面啦 很多時候語言模型在做的事情是 預測下一個Token 你輸入李宏毅老師中間這個Embedding 並不見得代表李宏毅老師 這個詞彙的含義 它真正代表的是 看到這個輸入以後 模型想要輸出是 這個Token 它想要做 產生是這個Token的時候 所產生的Representation 所以假設你想要知道
              
                  1:40:04
                  李宏毅老師在類神經網路看起來 是什麼意思 你用Logic Length 可能不一定能夠解析出 你要的結果 所以怎麼辦呢 有另外一招 那這個就是去年的論文了 這招叫Patch Scope Patch Scope Patch Scope這一招的意思是說 我們先看看 如果我們跟 語言模型給他這樣的輸入 跟語言模型講說 李奧納多冒號美國演員 臺積電冒號臺灣公司 然後再隨便給他一個東西 這個X可以是任何東西 那他就會輸出 他就會輸出 他對於X的理解 那怎麼知道一個類神經網路 當他看到李宏毅老師 這幾個字的時候 他內心深處的理解是什麼呢 你就把李宏毅老師輸入這個類神經網路裡面 然後看看在某一層他輸出的representation長什麼樣子,接下來把這個representation置換到這一個input string裡面,就這個類似這個語言模型他的輸入是一樣的,這邊甚至一樣就是給他一個x就好了,但是在這個位置把他的embedding,把在這個位置把他的representation換成輸入是這一串文字時候的representation,那對這個類似對這個語言模型來說,他就好像看到 X是李宏毅老師這一串字一樣
              
                  1:41:23
                  然後他就開始繼續輸出 他就有可能告訴你李宏毅老師的身份 那我知道講到這邊你可能會有個困惑就是 那我前面起不是要準備一些例子 那我準備的例子不是會影響最終輸出的結果嗎 沒錯就是你準備的例子 就是會影響最終輸出的結果 不過這篇文章的作者覺得這是一個feature 不是一個bug 你可以調整前面準備的例子 然後模型就會給你不同風格的解釋 比如說如果你現在的輸入是告訴我 X相關的秘密 然後你再把X這個位置的representation 換成李宏毅老師的representation 他可能就回答是個肥仔 你就可以從不同的角度來解析一個representation 那這邊就是引用了剛才就是提出這個 scope patch這個方法原始論文裡面舉的一個例子 他們就把 戴安納他是這個 那個 威爾斯王子的王妃 我這邊似乎少打了一個S
              
                  1:42:28
                  不過沒有關係 那個戴安納他是威爾斯王子的王妃 然後把這個片語 輸入給 類神經網路輸入給語言模型 然後接下來解析 他看到這個片語的 最後一個字的時候 他的每一層 的輸出對 這一個語言模型來講 分別是什麼,所以在前面一二層的時候,如果你把這個位置的representation拿去解析,語言模型輸出的句子是country in the United Kingdom,或者是country in Europe,因為威爾斯也是一個英國裡面的國家的名字,是United Kingdom裡面一個國家的名字,所以語言模型在前面幾層,他只認了威爾斯這個字,所以他就覺得他看到的是一個 但到了第四層的時候,他顯然讀到了Princess of Wales,他讀到了這一整串詞彙,在第四層的時候,他解析是說這是一個給皇室女性的頭銜,然後到第五層的時候,他知道說這個人呢,是威爾斯王子的妻子,然後到第六層,他才讀到戴安娜這個字,然後就輸出戴安娜完整的資訊,所以可以透過這個方法解析一個語言模型,每一層他可以 他看到的,他看到的東西實際上對應的文字是什麼,接下來最後一部分,最後幾頁投影片呢,就是舉一個例子,說剛才那些解析的方法,如何改變了人們對類神經網路背後運作機制的理解,進而提出了新的想法,那這是一篇去年六月的文章,這篇文章想要解析的是,對於一個multi-hop question,語言模型是怎麼回答的,然後解析完之後,他提供了
              
                  1:44:20
                  提出來一個方法,讓語言模型在multi-hop的question上面可以做得更好 那這個multi-hop的question這邊的例子是 the spouse of the performer of imagined is 像這種multi-hop的question裡面呢 會有包含三個entity 第一個entity是會明確出現在問題裡面的 那在這個例子裡面就是imagine 那我們把它叫做E1 那第一個entity imagined是一張專輯的名字 那接下來我們要問的是 The Performer of Imagine 就是彈奏Imagine創造Imagine這張專輯的這個音樂人是誰呢 那這個是E2 那他其實約翰藍儂（John Lennon） 所以這個E2是約翰藍儂 然後接下來呢 約翰藍儂的配偶又是誰呢 那這是E3 約翰藍儂的配偶是小野洋子 就是YOKO 然後模型知道這個答案以後 他就要看到這一串文字 然後輸出YOKO 那接下來的問題是 模型是怎麼做 這一連串的解析的呢 他是怎麼做這種需要
              
                  1:45:24
                  Multi-Hop Reasoning 需要做多步推理的問題的呢 一個直覺的想法是 模型讀到Imagine這個字以後 他根據前面的關係 The Performance of Imagine 先解析出答案是約翰藍儂 然後知道答案是約翰藍儂之後 再經過 The Spouse of約翰龍 這一個片語 解析出最終的答案 是Yoko是小野洋子 那模型真的是這樣運作的嗎 所以他們就他們就用剛才講的那個 那個PatchScope那個方法 做了一下解析 所以他們就把這一個位置的每一層 都拿出來看看 看看會解析出什麼樣的內容 然後如果解析出的內容裡面 有包含約翰藍儂這個字 就把它記錄下來 那得到的結果呢 是藍色的這條線 藍色的這條線橫軸呢 是layer然後縱軸呢 是解析出 這個E2 解析出E2的 第一次出現的layer
              
                  1:46:28
                  所以就會發現說 什麼時候解析出E2呢 什麼時候語言模型可以根據E1解析出E2呢 在蠻前面的layer 就可以根據E1 解析出E2了 好那根據E1解析出E2以後 再來要根據E2解析出E3 那什麼時候解析出 E3呢 他就分析這一個位置 每一個representation 他對應的文字 然後如果有出現 E3的內容的話就把它記錄下來 然後得到的結果呢 是橙色的這條線 所以你會發現說 多數情況都是大概在layer 第20層到第25層間 會解析出 E3這個詞彙 所以你可以感受到說 在比較低的layer 先解析出E2 然後接下來才解析出E3 然後最後就可以給你 正確的答案 然後這篇文章的作者發現說 當有時候這種multi-hop的question 沒有辦法得到正確的答案
              
                  1:47:31
                  是因為E2太晚被解析出來了 因為E3 必須要在第20幾個layer 被解析出來 才有解析出最終答案的能力 如果今天E2太晚被解析出來 過了20層才被解析出來 那接下來在T2這個位置 就來不及解析出E3了 所以怎麼解決這個問題呢 他們有一個神妙的做法 就是把後面幾層的representation 直接加到前面來 再重新跑一次 就結束了 既然今天只有中間某一層 能夠解析出E3 如果E2太晚被解析出來 那怎麼辦呢 把後面的layer放到前面 這樣只有能夠走過第二十層 就可以把E3解析出來了 這招有沒有用呢 這招居然是有用的 他們試了各式各樣不同的模型 那correct代表說 在用這招之前 模型本來就會答對的問題 那本來就會答對
              
                  1:48:33
                  那正確率居然當然是100嘛 然後做完這招以後不會影響正確率 但神奇的地方是 對本來不對的問題 用了這招以後 大概會有40到60%的正確 你可能會覺得40%到60%的正確率 也沒有很高 但不要忘了 這邊40%到60%的問題是 原來全部都答不對的 所以原來是0%的正確率 用了這招以後 原來完全答不對的問題裡面 居然有4到6成 可以因此就答對了 所以這一招 看起來其實又跟reasoning有點像 我們在第一堂課 不是講過reasoning 就是深度不夠 長度來湊嗎 這邊paper是6月的 的時候還沒有reasoning的模型 拿起跟reasoning的模型 也是很像的 reasoning的模型只是把 你的輸出如果來不及解析完 就跑到下一個time step 再重新解析一次 所以這個方法 這邊paper提出來一個叫做backpatching的方法 其實跟reasoning 深度不夠
              
                  1:49:36
                  長度來湊的做法 其實是有異曲同工之妙的 好 那這個就是今天想要跟大家分享的內容 就從一個神經人開始講起 最後講到怎麼讓語言模型 直接輸出他解析的結果
              



## 4、Transformer 替代架构

最知名的，Mamba，其实是非常类似的的东西，每一种架构的存在都有一个理由。

* CNN：根据影像的特性，减少不必要的参数，避免过拟合；
* 残差连接：为了让 Optimization 更容易
* Transformer：
	* Self-attention 取代掉 RNN (LSTM)：
		* RNN-Style，输入函数 $f_A$，隐藏层函数 $f_B$，输出函数 $f_C$，当然这些函数可以是 $f_{A,t}, f_{B,t}, f_{C,t}$，这意味着每次都不一样，可以让其与 $x$ 挂钩，根据 $x$ 的内容决定是否要遗忘一些内容，是否拒绝当前输入进入记忆中等，即 LSTM 的思想
		* 推理：
			* self-attention：计算量、记忆体需求随着序列长度增加
			* RNN：计算量、记忆体需求固定
		* 训练：
			* self-attention：容易平行化
			* RNN：难以平行化？
	* Linear Attention 就是广义 RNN 拿掉 ”Reflection“，$f_{A, t}$；  $H_t = H_{t-1} +v_tk_t^T$ ；$y_t=H_tq_t$
	* Linear Attention 就是 Self-attention 没有 Softmax
	* Linear Attention，在推理的时候像 RNN，训练的时候就可以展开像 self-attention 
	* RNN（Linear Attention）赢不过 Transformer（Self-attention with softmax）？貌似只差在 softmax，其实问题在于 Linear Attention 记忆永不改变，永不遗忘，那为什么 softmax 可以改变记忆？比如 `[0.6 1, 0.4] -> [0.3, 0.45, 0.25]` 过 softmax 前有一项是 1，代表很重要的事，`[0.5, 1, 0.5, 2, 1] -> [0.10， 0.17， 0.10， 0.46， 0.17]` 1 是很重要的事，但有 2 更重要的事， 1 相对就没那么重要
	* 加上 Reflection：逐渐遗忘（Retention Network， RetNet），加上了 $\gamma$ (0~1 之间)，$H_t = \gamma H_{t_1} +v_tk_t^T$
	* 但这会导致啥都会被逐渐遗忘，希望的是依据语境，有些事要牢记，有些可以淡忘，即 Gated Retention，$\gamma$ 换成 $\gamma_t$，随着时间更新，$\gamma_t = \text{softmax}(W_\gamma x_t)$
	* 更复杂的 Reflection, Paper: Parallelizing Linear Transformers with the Delta Rule over Sequence Length
* Mamba（就是 RNN） 取代 Self-attention：
* DeltaNet：公式化简后得到类似梯度下降更新公式形式


## 5、预训练-对齐

弱智吧， 比知乎来源 做 fine-tuning 要好，可能原因是弱智吧里的答案都是 gpt-4 生成的

把随便什么数据前半截作为输入，后半截作为答案，结果很差，但如何后半截是 LLM 生成的，则效果很好；

Alignment 前后模型实际行为差异不大：以 How are you? I am 为例，对齐前，fine 的概率最大

* Unshift：fine 的概率仍然最大；
* Marginal：fine 的概率第二或第三，有一点点变化
* Shifted： fine 不在前三名

Alignment 前后只是一小部分 token 上有差异，所以是不是可以直接改 token 输出的机率？

* 增加结束符号的概率
* 手动改变一些 token 出现的机率
* 避免出现重复的符号

结果是有用，但效果还是无法和 instruction tuning 后的模型比。

Paper：Self-Rewarding Language Models，他们不用 alignment 的资料，先问模型一个问题，产生多个答案，让模型自己对答案进行评分（提供评分指示），在根据这些分值对模型做 RL，能力就起来了

如何达成有效的 Pretrain？Paper：Physics of Language Models: Part 3.1，以人物简历为例，同一个人有很多种不同的介绍方式，这其实对于 pretrain 是一件重要的事情，而且不需要每个人都有多个版本的介绍，一些人有多个版本就够了

15T 的 token，基本现在 LLM 预训练的 token 量；资料品质的重要性

Alignment 的极限：

* highly known：pretrain 的模型本来就会的知识，需要提供范例指引，
* Maybe Known：有的范例可以指引回答正确，有的不行，要问的对，模型应该有这方面能力
* Weekly Known：需要对解码采样，有几率得到正确答案
* Unknown：怎么采样都无法答对

实验发现 Maybe Known 类型的资料是最有帮助的。只学 Unkown 的是最烂的，学不会。

因此，在 Align 阶段，不太容易教模型新知识，Align 真正能做的是调整模型的行为，他本来不知道看到一个问题要回答问题，现在能够微调让他能回答问题，但这个答案是他本来就知道的事情，才有办法真的教会他，而不破坏他原有的能力。

RL 是 Alignment 的好方法，RL 中，所有的答案都是 LM 自己生成的，并不是人类强塞一个答案给语言模型，所以 RL 的目标并不是叫模型会它本来完全不会的东西，只是提高了某些本来就可以生出来内容的机率，所以 RL 真正做的是激发模型本来的潜力。

Pretrain 的后遗症？pretrain 资料的分布对最终结果有很大影响，比如 rot-13（把所有字母偏移 13 个位置）；pretrain 时看到不该看的东西后，难以真正清除；Alignment 只是说不去激发他，但这些参数仍然在模型中，


## 6、后训练与遗忘

* pre-train 风格：文字接龙的方式
* SFT 风格：一问一答
* RL 风格：偏好

案例：教 LLaMA-2-Chat 用中文回答，该模型主要训练资料是英文，对此可搜集大量中文数据进行训练（Pre-train 风格），但实际做完后发现，原来 Alignment 的能力被破坏了；

这种遗忘的现象非常普遍，在其他风格的后训练上也都有这种现象，Safety Alignment 在我们的经验上，是最容易被破坏的能力；当然他也破坏了模型很多其他基础的能力。

案例：教 LLaMA 听声音：声音通过 Speech Encoder，比如 0.2s 转为一个向量，然后 LLM 中加入 Adapter，然后最后就忘了 json format；

所以 Post-Training 最大的挑战是模型会遗忘。这个现象叫做灾难性遗忘（Catastrophe Forgetting），但这也取决于我们的应用，可能我们只希望它在程式上表现好，其他方面能力不太关注，但现在可能更多的是希望有一个通用模型。

那这种现象是不是因为模型不够大？但研究上表明不是，但与在目标任务上训练效果有关，目标任务达成的越好，这种遗忘现象越严重，Paper：LoRA Learns Less and Forgets Less；

可以用 experience replay 方法缓解，就是在训练新任务的时候，混入一些原始任务数据，比如 5% 的占比；但现在的这些 LLM 模型我们无法获取它们的预训练资料，咋办？让 LM 自言自说，自己生成一些数据，Paper：Magpie（喜鹊的意思）

Paraphrase 方法：在 post_training 数据上，让模型先改写答案，然后再去训练，结果会较好；

self-output 方法：直接让模型产生答案（可能是错的，需要纠错，或者多次回答，选对的） ---> 这和 RL 风格的非常像。这种方法可以用在听语音案例说，先把语言大致标注一些，用文字的方式描述，比如什么时长、说的什么内容，说话人性别，心情等，然后给一个问题，要求 LM 输出并作为答案。

人工写的答案中，很多 token 是很难生成出来的，而模型生成出来的内容，是比较容易生出来的。

我认为未来的趋势是，多数语言模型仍需要具备自己独特的能力（例如：特别擅长写程式）。一个大型计划可能需要由多个具备不同专长的语言模型组成团队，共同完成，就像人类社会中的分工合作一样。但要注意，所谓「特别擅长写程式」，并不代表其他能力都可以一概没有，如果有一个擅长写程式的模型，它在团队中的角色是程式设计师，但若它除了写程式以外，甚至无法理解人类语言，听不懂其他团队成员的指令（如：「这里有个 bug」），无法与他人沟通，那并不是我们所追求的。因此，我们并不希望在 Post-training 阶段发生严重的遗忘现象，至于可接受的遗忘程度，则取决于实际需求。例如，一个担任程式设计师的语言模型如果不会写唐诗，可能还可以接受；但如果它连基本的语言能力都丧失，那就不行了。就好像人类的程式设计师也许不一定擅长社交，但至少能跟其他人类沟通。

## 7、如何进行深度思考

chatgpt o1/o3/o4、DeepSeek R1、Gemini 2 Flash Thinking、Claude 3.7 Sonnet（Extend Thinking）……

通常会在思考的过程前后加一个\<think\> 和 \</think\>，为了界面呈现的方便

在这个思考的过程中，通常模型会有几个行为：

* 验证自己刚才的答案是不是正确的 Let me check the answer...
* 可能会进行探索，Let's try a different approach...
* 有时候他甚至会做一些规划,Let's first try to... 

这种 reasoning 的行为是 Test-Time compute 的一种，在测试的阶段投入了更大的算力，而这个投入的算力可能可以让你得到更好的结果，Why？*深度不够，长度来凑* 

*我们先讲这种深度思考的语言模型可能用什么样的方式被打造出来？下节课来探讨这些 reasoning 的过程到底有没有或者是如何发挥作用*

另一个词汇 Test-Time Scaling，意思是思考越多往往结果越好，

**打造推理语言模型的方法：**

* 更强的思维链，CoT（不用微调参数）
* 给模型推论工作流程（不用微调参数）
* 教模型推理过程（Imitation Learning）
* 以结果为导向学习推理（RL）

**1、CoT：** 让模型先列出解题过程，再给出答案

* few-shot CoT：要给模型一些范例，
* zero-shot CoT: let's think step by step，就会自动把计算的过程列出来

但是因为现在这个思考的过程往往非常的长，所以又有一个新的名字叫做 Long CoT，


**2、直接给模型推理的工作流程**

如果只是叫模型，请尝试越多方法越好，它往往就尝试个两三个方法就结束了，怎么让模型不断尝试，最好尝试个几千几万种方法？也许可以直接强迫模型对同一个问题回答几千几万次，因为每一次模型在回答问题的时候，他的答案都会是不一样的。

Paper：Large Language Monkeys

对于稍微好一点的模型，试的够多次，他总是有机会猜到正确答案，但难点是，怎么知道哪一次得出来的答案是正确答案呢？

* 一个直觉的方式就是投票，Majority Vote，有另外一个称呼，叫做 Self Consistency；
* 另一个方法是看答案的 Confidence，这个方法被用在 COT Decoding 的方法里面；

并强制要求把答案放在 \<answer\> 和 \</answer\> 中间，方便做 Majority Vote，该方法其实很强，可以作为一个很好的 baseline；

 也可以用更复杂的方法，从模型众多的尝试中选出正确答案，现在常用的做法是，训练一个 Verifier，或者找一个模型当做 Verifier，让它去验证答案是不是正确的。如果越有可能是正确的，就输出越高的分数，这个方法叫做 Best of N。
 
那怎么得到这个验证器呢？那最简单的方法也许是直接拿一个语言模型即可。但如果你想要做得更好的话，可以对验证器加以训练，怎么对验证器加以训练呢？

下面的实验中，我们假设有一些训练资料，比如一堆数学问题，有标准答案，只是没有计算的过程。有这样的训练资料以后，就可以把问题丢进给语言模型，然后让语言模型产生多个不同的答案，因为有正确答案，所以知道语言模型什么时候是答对的，什么时候是答错的，这样就有验证器的训练资料，看到这个正确的答案输出 1，看到错误的答案就输出 0 等等。 

上面的方法是并行的方式，其实还有序列的方法，先让他解第一次，然后根据第一次的解法再去解第二次，等等；而且这个并行方式和序列方式可以同时使用。 

但如果你看现在这一些会做深度思考模型的行为，会发现他们往往不是得到最终答案才进行验证，它们往往能够做到，在解题解一半的时候，中间某一个步骤，就开始验证这个步骤是不是对的，以避免中间算错了浪费时间。

那如何做每一步的验证？给模型一个问题之后，先不要让模型解完，让它每次只输出第一步就停止，比如生成 3 个不同的第一步，然后通过 Process Verifier 去验证，可以通过 prompt 去要求如何一步步生成，比如每一步都放在 \<step\> 和 \</step\> 之间。

那如何得到 Process Verifier？我们有的只是问题（input）和对应的 ground truth，我们要求 LLM 在给定 input 和 step_1 后，继续生成多个 step_2 ... 到答案的流，比如生成 3 个流，其中 2 个最后答案正确，正确率为 2/3，同样验证 step_2 的过程也是如此，指导 Process Verifier 输出从该步骤开始得到正确答案的概率值。

那接下来会遇到的另外一个问题是，通常这个 Process Verifier 的输出并不是 True or False，而是一个数值，代表从这一步继续做下去以后，可能得到正确答案的机率，那怎么定这个阈值？*可以参考 Beam Search 的做法*。

---------------

**3、教模型推理过程（Imitation Learning，模仿学习）**

接下来两个方法都是后训练的方法的特例，也就是我们有一个 Foundation 的 Model，还不会做深度思考，但我们接下来做 Post Training，希望让其具有深度思考的能力。

*Imitation Learning，模仿学习*，直接教模型怎么做推理，这边假设我们的训练资料，除了问题、答案，还有推论的过程。

我们只要教模型说，看到输入内容之后，不仅要产生正确答案，还要产生 Reasoning Process，然后才产生正确答案。

这里面最难的地方是什么呢？ 是*这么推论从何而来？*

让语言模型自己想办法，产生推论的过程，给他 input 要求做 COT，把推论的过程详细解出来，然后他就产生推论过程，并产生答案，但毕竟能力有限，它不会每次都答对，但我们一般有正确答案，所以可以对比保留回答正确的 reasoning process 拿来当作训练资料。

有没有可能答案是对的，但推论过程其实是错的？确实我们没有办法保证答案是对的，推论过程每一步就是对的，所以就有人提出一些方法说，也许我们不应该只看最终答案对不对，我们应该用类似上面提到的过程验证方法去验证。

这里其实除了 SFT 的方法，也可以用 RL 的方式去训练，

但我们真的应该这样教模型吗？应该要告诉模型每一步推论过程，都必须要是正确的吗？

可以深度思考的模型，其实有时候会得到错误的思考过程，只要他最后能够得到正确答案就好了，所以我们甚至不应该教模型的时候，给他的每一步的推论过程都是对的。

会有什么问题呢？如果给他所有的推论过程都是对的，他会不知道要找找自己的问题，他每次都会觉得前面的推论过程一定都是对的，无法纠错。

怎么办呢？我们故意制造出一些特殊的训练过程，让训练过程中间可能有一些是有错的，因为语言模型他本身能力还是有极限的，我们应该要让语言模型有知错能改的能力。

那怎么做呢？Paper：Stream of search（SoS），这篇 paper 的想法就是，我们能不能从树状结构（推导步骤树）里面得到一个 reasoning 的过程，这里面是包含错误答案的，我们直接在这个树状的结构上面做一个深度优先的搜寻，把错误的搜寻过程也包含进训练资料里。故意走一些错的路，尝试回退重试其他方案。

对人类来说，如果你看到语言模型先产生一个答案，然后再莫名其妙再跳到另外一个答案，你可能会觉得这个语言模型讲话很容易前言不对后语。

最早的 o1 也容易前言不对后语的，当然这个 o1 并没有展现那个完整的 reasoning 的过程，也不知道是摘要那边出了问题，还是那个语言模型的思考就是非常跳跃，搞不好就是用这样子的资料训练出来的，所以他思考非常的跳跃。

到目前为止，讲了一大堆创造 reasoning 过程当作训练资料的方法，但如果你今天想要自己打造有 reasoning 能力的模型，可以直接做*知识蒸馏*。

**4、以结果为导向学习推理（RL）**

DeepSeek-R1 系列的做法，

有一些训练资料，有问题及正确答案，把问题输入给模型，要求模型做 reasoning，思考内容不重要，只看它最后的答案，与标准答案对比，如果是对的，模型就得到 positive reward，如果是错的，就得到 negative reward；

*在 RL 中，推论的内容不重要，只在意最后的答案是不是对的。*

R1-Zero 中，作者非常想要让大家知道的一件事情，就是 *aha moment*，没有教他，他自己就会了。

R1-Zero 的 reasoning 过程是非常难读的，而且是多个语言混杂的，为什么？因为训练中只在意结果，根本没有在意他推论的时候到底写了些什么东西。

R1 是怎么被打造出来的呢？*其实在打造 R1 的过程中，前面讲的三个方法都是有用上的*。

![[Pasted image 20250429222458.png|500]]

具体过程是：有了 R1-Zero 之后，用其来产生有 reasoning process（人读不懂的推理过程） 的训练资料，然后人工改，这部分到底花了多大代价，技术报告中没明说。

（PPT 左侧这两个句子都是技术报告里卖弄的原文）

除此之外，它们还用另一个模型（论文未清楚介绍），用 few-shot COT 的方法来产生一些带有 reasoning 的资料；也用 Prompting 的方法，让模型产生一些detail 的 answer 而且要有 reflection，要有 verification。 那这个显然就是*supervised COT* 的方法。

然后就可以做 *Imitation learning*，训练出 Model_A，接下来 Model_A 会再进一步去做 RL 得到 Model B，但 Model_B 做 RL 的过程，也和 R1-Zero 略有点不同，除了要要求他正确率越高越好以外，还有一个额外的限制，就是语言必须要用一样的。

![[Pasted image 20250429230322.png|500]]

Model_B 也是用来生成训练资料的，不能只考虑数学跟程式，要加各式各样的任务，模型 B 已经初步具备 reasoning 的能力，产生reasoning 的过程，再产生答案。

答案的验证：虽然这些问题有答案，但很多问题没有标准答案，用 V3 来做验证，还用了一些规则去掉 reasoning 的过程中比较糟糕的过程，比如多个语言，过长的等。生了 60 万训练资料。

接下来，还是一样做 *Imitation learning* 再重新训练 *DeepSeek-V3*，然后得到 Model_C，最后再做一次 RL，这部分写的非常的模糊，是希望强化模型 safety、helpfulness 的能力，最后才得到 R1。

报告的结尾有提到说，他们也曾经想要尝试做 process verifier，但是最终没有做起来，没有得到好的结果，这是一个上代研究的问题。

*另外一个模型，如果本来就不够强，用 RL 是没有办法激发它 reasoning 的能力；如果一个模型用 RL 之后，可以激发它 reasoning 的能力，那意味着它其实本来就有 reasoning 的能力，RL 只是强化这件事情的出现而已。*


推论模型*最大的挑战* 是什么呢？最大的挑战就是要产生非常长的 reasoning 的过程，显然是花钱跟花算力的，我们其实希望该 reasoning 的时候才 reasoning。


## 8、推理过程不用太长，够用就好

如何让推理的 LLM 不要想太多。

前面的课程中有提到，如果有比较长的推理，有可能结果会更好。 但真的是这样吗？现在有很多研究表明，其实*推理越长，不一定代表结果越好*。 

很多论文都说，推理长度越长，往往你会正确率越低。但这样的实验方法其实并不够严谨，比如说，可能是因为问题太难，所以需要更长的推理长度。也有严谨的实验表明，推理的长度看起来对于正确率真的是不一定有帮助的。

最好的工程师不是把事情做到完美，而是在有限的资源下把事情做到最好。

**如何避免模型想太多？**

* 关于 CoT，一般就是让模型 think step by step，Paper：Chain of Draft，要求每一个 thinking step 都只是一个草稿，然后草稿的每一条都不要超过五个字，实验效果表明有效；
* 有关第二个方向，给模型推论工作流程，这里的模型推论的工作流程是人设定的，所以要让模型推论短一点，是完全可以控制它，比如说让它 sampling 少一点，让它做 beam search 的时候 beam 小一点，让它产生树状结构的时候树长得小一点，就人工可以控制模型 reasoning 的长度；
* 第三个方法就是直接教模型怎么做推理。给模型正确答案，然后教它怎么进行推理。比如让推理模型作为老师进行指导，那怎么在这个学习的过程中把推理的长度考虑进去呢？同一个问题问多次，答对情况下，选一个最短的推理过程，当作学生模型的训练资料；
	* 另一个方法：Paper：From Explicit CoT to Implicit CoT，把明着写出来的 CoT，练到不见，具体做法：每次都把 reason 的过程练短一些，渐进式的学法
* RL 以结果为导向的推理方法，结果导向，用 RL 这些方法就会产生超长的推理过程，直觉的解决方法就是把长度限制加到 RL 的 reward 中，但实际上多数文献不使用该方法，定阈值不一定适用于所有问题，难得问题就是需要较长的推理长度；
	* 采用相对标准，根据难度定，先把问题丢给 LLM 多次做推论，回答正确的收集起来统计平均长度作为标准。就算答对，比平均更长，也算是不好的。
	* 教模型控制推理长度，直接 prompt 中设定推理长度为 $n^\star$，reward 为正确率 - 目标和实际推理长度差异，Paper：L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning，在数学问题上训练，在数学问题上测试，可以控制在 2~6% 左右，但 OOD（域外）就控制的较差，20~40%
	* 控制推理长度，会不会有损模型本质的推理能力？研究发现模型的推理能力并没有受到太大的影响

## 9、LLM 能力评测

能解数学问题就代表有推理能力吗？有多少答案可能是记忆出来的？

把原来的 GSM8K 题目里面的一些词汇、一些数字改掉，在不影响问题难度的前提下，再去问市面上多数模型这个问题，结果发现多数模型，它的正确率都是有减低的；把一些句子的顺序换掉（不影响题目意思），这些模型解题的正确率居然都下降；

这些测试的结果，往往不一定那么可靠，因为你永远不知道这些模型，是不是早就看到了类似的问题了。

现在模型推理能力测试，常被讨论到的 Benchmark Compass，叫做 ARC-AGI（作者是 Keras 的作者），里面都是这种有图形的智力测验题目 。

有个平台叫做 Chatbot Arena，每次登进去的时候，就随机给你两个模型，模型 $A$ 和模型 $B$，接下来就问这两个模型一样的问题，然后你要决定哪一个模型是比较好的；但传说 Chatbot Arena 也是有办法被 hack 的，因为人类还是有他喜欢的倾向，比如喜欢 emoji，粗体字，等，因为很多人根本就不会仔细去看它的内容，而通常是看它输出的风格，所以输出的风格反而对结果影响比较大

Chatbot Arena 的评比机制，Elo Score，这也是很多竞赛会使用的评比方式，假设有 $K$ 个模型，$M_1$ 到 $M_K$，每个模型有一个分数 $\beta_1$ 到 $\beta_K$，任意两个模型 $M_i$ 和 $M_j$ 之间对战，评比公式：$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j}{400}\right)}=E_{i,j}$$除掉一个 Normalization 的分数，就是为了让分布比较好看一点，通常都会设成 400，前面负号再去 Exponential，这其实就是 Sigmoid Function

* 如果 $i$ 的战力比 $j$ 的战力大很多，数值就会趋近于 1
* 如果 $i$ 比 $j$ 小很多，算出来的胜率就会趋近于 0

实际上在比赛里面，真正能知道的并不是这些战力，而是根据比赛的结果可以统计某个模型对战到某个模型的胜率是多少；所以 Elo Score 的求法是，先得到大量的比赛，知道模型跟模型间对战的胜率，再根据胜率反推出这些 $\beta$ 的值。

但在 Champion Arena 上，他们觉得会有太多跟模型本身实力无关的因素，会干扰到评比的结果，所以在计算正确率的时候，只知道 $\beta_i$ 跟 $\beta_j$ 是不够的，要加一项 $\beta_0$，即$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j+\beta_o}{400}\right)}=E_{i,j}$$
* $\beta_0$ 是模型实力以外的因素，比如某些棋类比赛里面，就应该把先手优势考虑进去

举例来说，模型可能回答越长，人类就会越喜欢，所以 $$\beta_0 = \gamma_1(\text{答案长度差})+\gamma_2(\text{emoji 数量差}) + \cdots$$
如果计算出来 $\gamma$ 是正的，就代表说这一项是会有影响的，$\gamma$ 趋近于 0，说明这一项没什么影响，负值就是负相关，如长度越短越好。

Champion Arena 报告，有没有考虑这些风格相关的因素，其实会影响模型的排名，其中上升的最多的就是 Claude，它是大家认知觉得蛮厉害，但在 Chatbot Arena 上评分起不来的模型，一个原因可能就是 Claude 模型讲话太无聊了。

到底什么样的指标才是好的评量指标呢？*也许结论就是没有好的评量指标*。这个叫做 Goodheart's Law 的意思是，一旦一项指标被当做目标，它就不再是一个好的指标。


## 10、模型编辑

Model Editing 希望做到的事情是帮模型 *植入* 一件知识，

Model Editing 与 Post-training 有什么不同？Post-training 通常是想要让模型学会新的技能，这个技能不是一项知识，而是需要模型做比较大的改变才有办法学会的事情，比如说新的语言、或者是使用工具、做推理等等；

但是直接用 Post-training finetune 模型的方法做 Model Editing 有很大的挑战，因为做 Model Editing 的时候，通常你的训练资料就只有一笔；

怎么评量 Model Editing 是不是成功的，要考虑三个不同的面向：

* Reliability：想要修改的目标必须要达成
* Generalization：输入有一些改变，泛化能力
* Locality：其他无关的输入，不应该被改

Model Editing 常见方法：

* 不动参数：放在 prompt 中（关闭 rag，联网功能），但直接放入模型会不信；Paper：In-Context Knowledge Editing(IKE)，提供一些范例，是比较容易成功的
* 改变参数：直接梯度下降，往往一改完，模型就坏掉了
	* 人类决定如何编辑：由人类对于语言模型的理解，找出应该要被编辑的位置，并决定要被编辑的方法。Paper：Rank-One Model Editing（ROME）
		1. 是找出网路中，跟编辑的知识最相关的部分
		2. 修改那个部分的参数，让模型变成想要的样子
	* 另一个 LLM 决定如何编辑：输入是 $\theta$（待编辑的模型参数）、待编辑的问题、答案，输出是 $e$（大小同 $\theta$ 的向量），然后 $\theta + e$ 即可，这个网络称为 Hypernetwork


