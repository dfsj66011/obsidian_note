
## 1、生成式 AI 的技术突破与未来发展
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；

三个面，剖析 AI agent 的关键能力：

1. 能不能够根据他的经验，过去的互动中所获得的经验来调整他的行为 
2. 如何呼叫外部的援助，如何使用工具 
3. 能不能够执行计划，能不能做计划

关于 1，真正的议题是：如果我们是把过去所有的经验都存起来，要改变语言模型的行为，要让他根据过去的经验调整行为，就是把过去所有发生的事情一股脑给他，那就好像是语言模型每次做一次决策的时候，他都要回忆他一生的经历，也许他没有足够的算力，来回顾一生的资讯，他就没有办法得到正确的答案，所以怎么办呢？

也许我们可以给这些 AI agent memory，这就像是人类的长期记忆一样，发生过的事情，把它存到这个 memory 里面，有一个叫做 read 的模组，会从 memory 里面选择跟现在要解决的问题有关系的经验，把这些有关系的经验放在 observation 的前面，让模型根据这些有关系的经验跟 observation 再做文字接龙，

那怎么样打造这个 read 的模组呢？就想成是一个 retrieval 的 system，其实它就是 RAG，唯一不一样的地方，如果是 RAG 的话，存在 memory 里面的东西是别人的经验，而对 AI agent 而言，是他自己个人的经历，差别的是经历的来源。

根据经验调整行为能力的好坏，那就看这一整个回答的过程中平均的正确率，越能够根据经验学习的agent，应该能够用越少的时间，看过越少的回馈，就越快能够增强他的能力，就可以得到比较高的平均的正确率。

还发现一个有趣的现象是值得跟大家分享。这个现象是负面的回馈，基本上没有帮助

可以有一个 write 的 module，决定什么样的资讯要被填到长期的记忆库里面，怎么样打造这个 write 的记忆库呢？一个很简单的方法就是 write 的模组也是一个语言模型甚至就是 AI agent 自己

还有第三个模组，没有固定的名字，暂时叫 reflection 反思的模组，这个模组的工作是对记忆中的资讯做更好的、更 high level 的，可能是抽象的重新整理，可能也是一个语言模型，或 AI agent 自己

-------

关于 2，怎么使用工具？小模型呼叫大模型帮忙，这些工具对语言模型来说都是 function，

使用工作的挑战，每一个工具都要有对应的文字描述，告诉语言模型这个工具怎么被使用，假设工具很多怎么办呢？类似上文，打造一个工具选择的模组；也可以自己写个 function 作为 tool

工具有可能会犯错，所以我们也要告诉我们的工具，这些不要完全相信工具的工具，要有自己的判断能力，不要完全相信工具的工具给你的结果，比如给出的天气温度为 100°

那什么样的外部知识比较容易说服 AI，让他相信你说的话呢？外部的知识，如果跟模型本身的信念差距越大，模型就越不容易相信，那如果跟本身的信念差距比较小，模型就比较容易相信

----------

关于 3，AI 语言模型能不能做计划？语言模型就是给一个输入，它就直接给一个输出，也许在给输出的过程中有进行计划，但我们不一定能够明确的知道这件事；

但其实可以强迫语言模型直接明确的产生规划，可以直接问语言模型说，如果现在要达成我们的目标，从这个 observation 开始，你觉得应该要做哪些行动？

不过这是一个理想的想法。那语言模型到底有没有做计划的能力呢？过去确实也有很多论文说，语言模型是有一定程度做计划的能力的。

现在到底模型规划的能力怎么样呢，就是介于有跟没有间吧……

-----------

## 3、语言模型内部运行机制

* 一个神经元在做什么
* 一层神经元在做什么
* 一群神经元在做什么
* 让语言模型直接说出它的想法


**一个神经元在做什么？**

主要指 FFN 中的单个神经元，其每个输出都是所有输入的某种 weighted sum，然后通过如 ReLU 等激活函数，得到一个神经元的输出。

1. 比如某个神经元的启动与说脏话有关联，语言模型说脏活，该神经元都会启动

2. 移除该神经元，语言模型说不出脏活，什么叫把神经元从网络中移掉？让其输出永远为 0？但 0 不代表完全不会造成影响，0 可能会对其他神经元有影响，当它输出为零，反而会启动其他神经元。有研究发现也许设平均值比较好，各种各样不同输入时值的均值，尚待研究。

3. 不同启动程度，说不同等级的脏话

实际上，不容易解释单一神经元的功能，一件事情可能很多神经元共同管理，例如 LLaMA 一层只有 4096 个神经元，神经元数量太少了，如果一个神经元负责一件事，则没办法应对千变万化的任务；

**一层神经元在做什么？**

假设第 10 层神经元的输出中，第一个，中间某一个，最后一个神经元被启动时，就会拒绝请求（我很抱歉，我不能帮你......），这个神经元的组合可以看作是高维空间中的一个特定方向的向量（功能向量），至于在那一层找，只能做实验，试试看喽

因为如果第十层的输出与功能向量越接近，越有可能拒绝请求，那怎么找出这个 *功能向量*，

找一些（会被拒掉）的句子，算出第 10 层的输出表示，该表示 = 拒绝向量 + 其他内容，例如找出 1000 个句子，`所有表示平均起来 = 拒绝 + 其他的平均`；同理再找 1000 个没有被拒的句子，第 10 层的输出平均起来作为 *其他的平均*；

怎么验证？找个句子，在第 10 层的输出中把 拒绝向量 加进去，看看输出有什么变化。反过来说，本来有一个拒绝的句子，如果把 拒绝向量 减掉，会不会就不拒绝了......

*In-Context Vector*：就是说，比如给一些反义词对的句子，把某一层的激活平均起来，Zero-Shot 的时候把这个向量加到给定的单词上，就会输出反义词；有意思的是，这些功能向量时可以加加减减的

*是否可以把自动把某一层所有的功能向量都找出来？*，例如一共有 $K$（非常大）个功能，第 10 层的输出那应该是这 $K$ 个功能向量的线性组合 + $e$，$e$ 是指一些其他的内容，$e$ 的值应该越小越好，系数 $\alpha$ 也是越小越好，意味着每次选择的功能向量越少越好。

**一群神经元在做什么？语言模型完成某一项任务的机制**

语言模型的模型，模型是指用一个较为简单的东西来代表另一个东西，模型的特性：

* 要比原来实物简单
* 保有原来实物的特征（faithfulness）

比如 “THE Taipei 101 is located in ”，其中 "The Taipei 101" 经过多层后编码为 $x$，"is located in" 就相当于一个线性函数 $f$，最后得到答案 "Taipei"，即 $y$，这样给一些样本，比如 8 个，就是要找到 "is located in" 对应线性函数 $f$ 的 $W,b$，

**怎么让语言模型直接说出想法？**

就直接问，让它输出 reason 等，语言模型的思维是透明的，

把残差连接的思维图转换一下，一开始就是直通的，侧边才是残差，每一层侧边加一点东西进去，这才是 transformer 的整体运作机制，只是最后一层接了个 unembedding，所以想法是在中间层的输出中也加入 unembedding，这一招称为 logit lens，输出的分布过 softmax 之前就叫做 logit，

但每一层侧边加了什么呢？MLP 可以看作是一个有 K 有 V 的 attention，概念来自 Paper：Transformer Feed Forward Layers Are Key-Value Memories，可以把前一层的值作为 $k_1,k_2,\cdots$，层间参数作为 $V$，例如以 $k_2$ 连接到下一层的 weight 组合作为 $\mathbf{v_2}$（向量） ，因此输出就是 $\sum^D_{i=1} k_i\mathbf{v_i}$，那每一层残差部分的输出本应该一样，导致前后输出差异就是这个侧边块影响的。

所以把侧边的 V，也通过 unembedding layer，试着通过 logit lens 解析每一个神经网络想要输出什么东西，是什么东西被加入到残差中，影响最后的输出分布

logit lens 方式致命缺陷，通过 unembedding 的方法，只能把一个 Representation 转成一个 token，我们解析出来的结果，都只能是一个 Token，另一方面 LM 是在预测下一个 token，所以这个 Representation 不见得是代表你输入内容的含义，新的招数，Patchscopes（与深度不够，长度来凑的 reasoning 方式有异曲同工之妙）

## 4、Transformer 替代架构

最知名的，Mamba，其实是非常类似的的东西，每一种架构的存在都有一个理由。

* CNN：根据影像的特性，减少不必要的参数，避免过拟合；
* 残差连接：为了让 Optimization 更容易
* Transformer：
	* Self-attention 取代掉 RNN (LSTM)：
		* RNN-Style，输入函数 $f_A$，隐藏层函数 $f_B$，输出函数 $f_C$，当然这些函数可以是 $f_{A,t}, f_{B,t}, f_{C,t}$，这意味着每次都不一样，可以让其与 $x$ 挂钩，根据 $x$ 的内容决定是否要遗忘一些内容，是否拒绝当前输入进入记忆中等，即 LSTM 的思想
		* 推理：
			* self-attention：计算量、记忆体需求随着序列长度增加
			* RNN：计算量、记忆体需求固定
		* 训练：
			* self-attention：容易平行化
			* RNN：难以平行化？
	* Linear Attention 就是广义 RNN 拿掉 ”Reflection“，$f_{A, t}$；  $H_t = H_{t-1} +v_tk_t^T$ ；$y_t=H_tq_t$
	* Linear Attention 就是 Self-attention 没有 Softmax
	* Linear Attention，在推理的时候像 RNN，训练的时候就可以展开像 self-attention 
	* RNN（Linear Attention）赢不过 Transformer（Self-attention with softmax）？貌似只差在 softmax，其实问题在于 Linear Attention 记忆永不改变，永不遗忘，那为什么 softmax 可以改变记忆？比如 `[0.6 1, 0.4] -> [0.3, 0.45, 0.25]` 过 softmax 前有一项是 1，代表很重要的事，`[0.5, 1, 0.5, 2, 1] -> [0.10， 0.17， 0.10， 0.46， 0.17]` 1 是很重要的事，但有 2 更重要的事， 1 相对就没那么重要
	* 加上 Reflection：逐渐遗忘（Retention Network， RetNet），加上了 $\gamma$ (0~1 之间)，$H_t = \gamma H_{t_1} +v_tk_t^T$
	* 但这会导致啥都会被逐渐遗忘，希望的是依据语境，有些事要牢记，有些可以淡忘，即 Gated Retention，$\gamma$ 换成 $\gamma_t$，随着时间更新，$\gamma_t = \text{softmax}(W_\gamma x_t)$
	* 更复杂的 Reflection, Paper: Parallelizing Linear Transformers with the Delta Rule over Sequence Length
* Mamba（就是 RNN） 取代 Self-attention：
* DeltaNet：公式化简后得到类似梯度下降更新公式形式


## 5、预训练-对齐

弱智吧， 比知乎来源 做 fine-tuning 要好，可能原因是弱智吧里的答案都是 gpt-4 生成的

把随便什么数据前半截作为输入，后半截作为答案，结果很差，但如何后半截是 LLM 生成的，则效果很好；

Alignment 前后模型实际行为差异不大：以 How are you? I am 为例，对齐前，fine 的概率最大

* Unshift：fine 的概率仍然最大；
* Marginal：fine 的概率第二或第三，有一点点变化
* Shifted： fine 不在前三名

Alignment 前后只是一小部分 token 上有差异，所以是不是可以直接改 token 输出的机率？

* 增加结束符号的概率
* 手动改变一些 token 出现的机率
* 避免出现重复的符号

结果是有用，但效果还是无法和 instruction tuning 后的模型比。

Paper：Self-Rewarding Language Models，他们不用 alignment 的资料，先问模型一个问题，产生多个答案，让模型自己对答案进行评分（提供评分指示），在根据这些分值对模型做 RL，能力就起来了

如何达成有效的 Pretrain？Paper：Physics of Language Models: Part 3.1，以人物简历为例，同一个人有很多种不同的介绍方式，这其实对于 pretrain 是一件重要的事情，而且不需要每个人都有多个版本的介绍，一些人有多个版本就够了

15T 的 token，基本现在 LLM 预训练的 token 量；资料品质的重要性

Alignment 的极限：

* highly known：pretrain 的模型本来就会的知识，需要提供范例指引，
* Maybe Known：有的范例可以指引回答正确，有的不行，要问的对，模型应该有这方面能力
* Weekly Known：需要对解码采样，有几率得到正确答案
* Unknown：怎么采样都无法答对

实验发现 Maybe Known 类型的资料是最有帮助的。只学 Unkown 的是最烂的，学不会。

因此，在 Align 阶段，不太容易教模型新知识，Align 真正能做的是调整模型的行为，他本来不知道看到一个问题要回答问题，现在能够微调让他能回答问题，但这个答案是他本来就知道的事情，才有办法真的教会他，而不破坏他原有的能力。

RL 是 Alignment 的好方法，RL 中，所有的答案都是 LM 自己生成的，并不是人类强塞一个答案给语言模型，所以 RL 的目标并不是叫模型会它本来完全不会的东西，只是提高了某些本来就可以生出来内容的机率，所以 RL 真正做的是激发模型本来的潜力。

Pretrain 的后遗症？pretrain 资料的分布对最终结果有很大影响，比如 rot-13（把所有字母偏移 13 个位置）；pretrain 时看到不该看的东西后，难以真正清除；Alignment 只是说不去激发他，但这些参数仍然在模型中，


## 6、后训练与遗忘

* pre-train 风格：文字接龙的方式
* SFT 风格：一问一答
* RL 风格：偏好

案例：教 LLaMA-2-Chat 用中文回答，该模型主要训练资料是英文，对此可搜集大量中文数据进行训练（Pre-train 风格），但实际做完后发现，原来 Alignment 的能力被破坏了；

这种遗忘的现象非常普遍，在其他风格的后训练上也都有这种现象，Safety Alignment 在我们的经验上，是最容易被破坏的能力；当然他也破坏了模型很多其他基础的能力。

案例：教 LLaMA 听声音：声音通过 Speech Encoder，比如 0.2s 转为一个向量，然后 LLM 中加入 Adapter，然后最后就忘了 json format；

所以 Post-Training 最大的挑战是模型会遗忘。这个现象叫做灾难性遗忘（Catastrophe Forgetting），但这也取决于我们的应用，可能我们只希望它在程式上表现好，其他方面能力不太关注，但现在可能更多的是希望有一个通用模型。

那这种现象是不是因为模型不够大？但研究上表明不是，但与在目标任务上训练效果有关，目标任务达成的越好，这种遗忘现象越严重，Paper：LoRA Learns Less and Forgets Less；

可以用 experience replay 方法缓解，就是在训练新任务的时候，混入一些原始任务数据，比如 5% 的占比；但现在的这些 LLM 模型我们无法获取它们的预训练资料，咋办？让 LM 自言自说，自己生成一些数据，Paper：Magpie（喜鹊的意思）

Paraphrase 方法：在 post_training 数据上，让模型先改写答案，然后再去训练，结果会较好；

self-output 方法：直接让模型产生答案（可能是错的，需要纠错，或者多次回答，选对的） ---> 这和 RL 风格的非常像。这种方法可以用在听语音案例说，先把语言大致标注一些，用文字的方式描述，比如什么时长、说的什么内容，说话人性别，心情等，然后给一个问题，要求 LM 输出并作为答案。

人工写的答案中，很多 token 是很难生成出来的，而模型生成出来的内容，是比较容易生出来的。

我认为未来的趋势是，多数语言模型仍需要具备自己独特的能力（例如：特别擅长写程式）。一个大型计划可能需要由多个具备不同专长的语言模型组成团队，共同完成，就像人类社会中的分工合作一样。但要注意，所谓「特别擅长写程式」，并不代表其他能力都可以一概没有，如果有一个擅长写程式的模型，它在团队中的角色是程式设计师，但若它除了写程式以外，甚至无法理解人类语言，听不懂其他团队成员的指令（如：「这里有个 bug」），无法与他人沟通，那并不是我们所追求的。因此，我们并不希望在 Post-training 阶段发生严重的遗忘现象，至于可接受的遗忘程度，则取决于实际需求。例如，一个担任程式设计师的语言模型如果不会写唐诗，可能还可以接受；但如果它连基本的语言能力都丧失，那就不行了。就好像人类的程式设计师也许不一定擅长社交，但至少能跟其他人类沟通。

## 7、如何进行深度思考

chatgpt o1/o3/o4、DeepSeek R1、Gemini 2 Flash Thinking、Claude 3.7 Sonnet（Extend Thinking）……

通常会在思考的过程前后加一个\<think\> 和 \</think\>，为了界面呈现的方便

在这个思考的过程中，通常模型会有几个行为：

* 验证自己刚才的答案是不是正确的 Let me check the answer...
* 可能会进行探索，Let's try a different approach...
* 有时候他甚至会做一些规划,Let's first try to... 

这种 reasoning 的行为是 Test-Time compute 的一种，在测试的阶段投入了更大的算力，而这个投入的算力可能可以让你得到更好的结果，Why？*深度不够，长度来凑* 

*我们先讲这种深度思考的语言模型可能用什么样的方式被打造出来？下节课来探讨这些 reasoning 的过程到底有没有或者是如何发挥作用*

另一个词汇 Test-Time Scaling，意思是思考越多往往结果越好，

**打造推理语言模型的方法：**

* 更强的思维链，CoT（不用微调参数）
* 给模型推论工作流程（不用微调参数）
* 教模型推理过程（Imitation Learning）
* 以结果为导向学习推理（RL）

**1、CoT：** 让模型先列出解题过程，再给出答案

* few-shot CoT：要给模型一些范例，
* zero-shot CoT: let's think step by step，就会自动把计算的过程列出来

但是因为现在这个思考的过程往往非常的长，所以又有一个新的名字叫做 Long CoT，


**2、直接给模型推理的工作流程**

如果只是叫模型，请尝试越多方法越好，它往往就尝试个两三个方法就结束了，怎么让模型不断尝试，最好尝试个几千几万种方法？也许可以直接强迫模型对同一个问题回答几千几万次，因为每一次模型在回答问题的时候，他的答案都会是不一样的。

Paper：Large Language Monkeys

对于稍微好一点的模型，试的够多次，他总是有机会猜到正确答案，但难点是，怎么知道哪一次得出来的答案是正确答案呢？

* 一个直觉的方式就是投票，Majority Vote，有另外一个称呼，叫做 Self Consistency；
* 另一个方法是看答案的 Confidence，这个方法被用在 COT Decoding 的方法里面；

并强制要求把答案放在 \<answer\> 和 \</answer\> 中间，方便做 Majority Vote，该方法其实很强，可以作为一个很好的 baseline；

 也可以用更复杂的方法，从模型众多的尝试中选出正确答案，现在常用的做法是，训练一个 Verifier，或者找一个模型当做 Verifier，让它去验证答案是不是正确的。如果越有可能是正确的，就输出越高的分数，这个方法叫做 Best of N。
 
那怎么得到这个验证器呢？那最简单的方法也许是直接拿一个语言模型即可。但如果你想要做得更好的话，可以对验证器加以训练，怎么对验证器加以训练呢？

下面的实验中，我们假设有一些训练资料，比如一堆数学问题，有标准答案，只是没有计算的过程。有这样的训练资料以后，就可以把问题丢进给语言模型，然后让语言模型产生多个不同的答案，因为有正确答案，所以知道语言模型什么时候是答对的，什么时候是答错的，这样就有验证器的训练资料，看到这个正确的答案输出 1，看到错误的答案就输出 0 等等。 

上面的方法是并行的方式，其实还有序列的方法，先让他解第一次，然后根据第一次的解法再去解第二次，等等；而且这个并行方式和序列方式可以同时使用。 

但如果你看现在这一些会做深度思考模型的行为，会发现他们往往不是得到最终答案才进行验证，它们往往能够做到，在解题解一半的时候，中间某一个步骤，就开始验证这个步骤是不是对的，以避免中间算错了浪费时间。

那如何做每一步的验证？给模型一个问题之后，先不要让模型解完，让它每次只输出第一步就停止，比如生成 3 个不同的第一步，然后通过 Process Verifier 去验证，可以通过 prompt 去要求如何一步步生成，比如每一步都放在 \<step\> 和 \</step\> 之间。

那如何得到 Process Verifier？我们有的只是问题（input）和对应的 ground truth，我们要求 LLM 在给定 input 和 step_1 后，继续生成多个 step_2 ... 到答案的流，比如生成 3 个流，其中 2 个最后答案正确，正确率为 2/3，同样验证 step_2 的过程也是如此，指导 Process Verifier 输出从该步骤开始得到正确答案的概率值。

那接下来会遇到的另外一个问题是，通常这个 Process Verifier 的输出并不是 True or False，而是一个数值，代表从这一步继续做下去以后，可能得到正确答案的机率，那怎么定这个阈值？*可以参考 Beam Search 的做法*。

---------------

**3、教模型推理过程（Imitation Learning，模仿学习）**

接下来两个方法都是后训练的方法的特例，也就是我们有一个 Foundation 的 Model，还不会做深度思考，但我们接下来做 Post Training，希望让其具有深度思考的能力。

*Imitation Learning，模仿学习*，直接教模型怎么做推理，这边假设我们的训练资料，除了问题、答案，还有推论的过程。

我们只要教模型说，看到输入内容之后，不仅要产生正确答案，还要产生 Reasoning Process，然后才产生正确答案。

这里面最难的地方是什么呢？ 是*这么推论从何而来？*

让语言模型自己想办法，产生推论的过程，给他 input 要求做 COT，把推论的过程详细解出来，然后他就产生推论过程，并产生答案，但毕竟能力有限，它不会每次都答对，但我们一般有正确答案，所以可以对比保留回答正确的 reasoning process 拿来当作训练资料。

有没有可能答案是对的，但推论过程其实是错的？确实我们没有办法保证答案是对的，推论过程每一步就是对的，所以就有人提出一些方法说，也许我们不应该只看最终答案对不对，我们应该用类似上面提到的过程验证方法去验证。

这里其实除了 SFT 的方法，也可以用 RL 的方式去训练，

但我们真的应该这样教模型吗？应该要告诉模型每一步推论过程，都必须要是正确的吗？

可以深度思考的模型，其实有时候会得到错误的思考过程，只要他最后能够得到正确答案就好了，所以我们甚至不应该教模型的时候，给他的每一步的推论过程都是对的。

会有什么问题呢？如果给他所有的推论过程都是对的，他会不知道要找找自己的问题，他每次都会觉得前面的推论过程一定都是对的，无法纠错。

怎么办呢？我们故意制造出一些特殊的训练过程，让训练过程中间可能有一些是有错的，因为语言模型他本身能力还是有极限的，我们应该要让语言模型有知错能改的能力。

那怎么做呢？Paper：Stream of search（SoS），这篇 paper 的想法就是，我们能不能从树状结构（推导步骤树）里面得到一个 reasoning 的过程，这里面是包含错误答案的，我们直接在这个树状的结构上面做一个深度优先的搜寻，把错误的搜寻过程也包含进训练资料里。故意走一些错的路，尝试回退重试其他方案。

对人类来说，如果你看到语言模型先产生一个答案，然后再莫名其妙再跳到另外一个答案，你可能会觉得这个语言模型讲话很容易前言不对后语。

最早的 o1 也容易前言不对后语的，当然这个 o1 并没有展现那个完整的 reasoning 的过程，也不知道是摘要那边出了问题，还是那个语言模型的思考就是非常跳跃，搞不好就是用这样子的资料训练出来的，所以他思考非常的跳跃。

到目前为止，讲了一大堆创造 reasoning 过程当作训练资料的方法，但如果你今天想要自己打造有 reasoning 能力的模型，可以直接做*知识蒸馏*。

**4、以结果为导向学习推理（RL）**

DeepSeek-R1 系列的做法，

有一些训练资料，有问题及正确答案，把问题输入给模型，要求模型做 reasoning，思考内容不重要，只看它最后的答案，与标准答案对比，如果是对的，模型就得到 positive reward，如果是错的，就得到 negative reward；

*在 RL 中，推论的内容不重要，只在意最后的答案是不是对的。*

R1-Zero 中，作者非常想要让大家知道的一件事情，就是 *aha moment*，没有教他，他自己就会了。

R1-Zero 的 reasoning 过程是非常难读的，而且是多个语言混杂的，为什么？因为训练中只在意结果，根本没有在意他推论的时候到底写了些什么东西。

R1 是怎么被打造出来的呢？*其实在打造 R1 的过程中，前面讲的三个方法都是有用上的*。

![[Pasted image 20250429222458.png|500]]

具体过程是：有了 R1-Zero 之后，用其来产生有 reasoning process（人读不懂的推理过程） 的训练资料，然后人工改，这部分到底花了多大代价，技术报告中没明说。

（PPT 左侧这两个句子都是技术报告里卖弄的原文）

除此之外，它们还用另一个模型（论文未清楚介绍），用 few-shot COT 的方法来产生一些带有 reasoning 的资料；也用 Prompting 的方法，让模型产生一些detail 的 answer 而且要有 reflection，要有 verification。 那这个显然就是*supervised COT* 的方法。

然后就可以做 *Imitation learning*，训练出 Model_A，接下来 Model_A 会再进一步去做 RL 得到 Model B，但 Model_B 做 RL 的过程，也和 R1-Zero 略有点不同，除了要要求他正确率越高越好以外，还有一个额外的限制，就是语言必须要用一样的。

![[Pasted image 20250429230322.png|500]]

Model_B 也是用来生成训练资料的，不能只考虑数学跟程式，要加各式各样的任务，模型 B 已经初步具备 reasoning 的能力，产生reasoning 的过程，再产生答案。

答案的验证：虽然这些问题有答案，但很多问题没有标准答案，用 V3 来做验证，还用了一些规则去掉 reasoning 的过程中比较糟糕的过程，比如多个语言，过长的等。生了 60 万训练资料。

接下来，还是一样做 *Imitation learning* 再重新训练 *DeepSeek-V3*，然后得到 Model_C，最后再做一次 RL，这部分写的非常的模糊，是希望强化模型 safety、helpfulness 的能力，最后才得到 R1。

报告的结尾有提到说，他们也曾经想要尝试做 process verifier，但是最终没有做起来，没有得到好的结果，这是一个上代研究的问题。

*另外一个模型，如果本来就不够强，用 RL 是没有办法激发它 reasoning 的能力；如果一个模型用 RL 之后，可以激发它 reasoning 的能力，那意味着它其实本来就有 reasoning 的能力，RL 只是强化这件事情的出现而已。*


推论模型*最大的挑战* 是什么呢？最大的挑战就是要产生非常长的 reasoning 的过程，显然是花钱跟花算力的，我们其实希望该 reasoning 的时候才 reasoning。


## 8、推理过程不用太长，够用就好

如何让推理的 LLM 不要想太多。

前面的课程中有提到，如果有比较长的推理，有可能结果会更好。 但真的是这样吗？现在有很多研究表明，其实*推理越长，不一定代表结果越好*。 

很多论文都说，推理长度越长，往往你会正确率越低。但这样的实验方法其实并不够严谨，比如说，可能是因为问题太难，所以需要更长的推理长度。也有严谨的实验表明，推理的长度看起来对于正确率真的是不一定有帮助的。

最好的工程师不是把事情做到完美，而是在有限的资源下把事情做到最好。

**如何避免模型想太多？**

* 关于 CoT，一般就是让模型 think step by step，Paper：Chain of Draft，要求每一个 thinking step 都只是一个草稿，然后草稿的每一条都不要超过五个字，实验效果表明有效；
* 有关第二个方向，给模型推论工作流程，这里的模型推论的工作流程是人设定的，所以要让模型推论短一点，是完全可以控制它，比如说让它 sampling 少一点，让它做 beam search 的时候 beam 小一点，让它产生树状结构的时候树长得小一点，就人工可以控制模型 reasoning 的长度；
* 第三个方法就是直接教模型怎么做推理。给模型正确答案，然后教它怎么进行推理。比如让推理模型作为老师进行指导，那怎么在这个学习的过程中把推理的长度考虑进去呢？同一个问题问多次，答对情况下，选一个最短的推理过程，当作学生模型的训练资料；
	* 另一个方法：Paper：From Explicit CoT to Implicit CoT，把明着写出来的 CoT，练到不见，具体做法：每次都把 reason 的过程练短一些，渐进式的学法
* RL 以结果为导向的推理方法，结果导向，用 RL 这些方法就会产生超长的推理过程，直觉的解决方法就是把长度限制加到 RL 的 reward 中，但实际上多数文献不使用该方法，定阈值不一定适用于所有问题，难得问题就是需要较长的推理长度；
	* 采用相对标准，根据难度定，先把问题丢给 LLM 多次做推论，回答正确的收集起来统计平均长度作为标准。就算答对，比平均更长，也算是不好的。
	* 教模型控制推理长度，直接 prompt 中设定推理长度为 $n^\star$，reward 为正确率 - 目标和实际推理长度差异，Paper：L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning，在数学问题上训练，在数学问题上测试，可以控制在 2~6% 左右，但 OOD（域外）就控制的较差，20~40%
	* 控制推理长度，会不会有损模型本质的推理能力？研究发现模型的推理能力并没有受到太大的影响

## 9、LLM 能力评测

能解数学问题就代表有推理能力吗？有多少答案可能是记忆出来的？

把原来的 GSM8K 题目里面的一些词汇、一些数字改掉，在不影响问题难度的前提下，再去问市面上多数模型这个问题，结果发现多数模型，它的正确率都是有减低的；把一些句子的顺序换掉（不影响题目意思），这些模型解题的正确率居然都下降；

这些测试的结果，往往不一定那么可靠，因为你永远不知道这些模型，是不是早就看到了类似的问题了。

现在模型推理能力测试，常被讨论到的 Benchmark Compass，叫做 ARC-AGI（作者是 Keras 的作者），里面都是这种有图形的智力测验题目 。

有个平台叫做 Chatbot Arena，每次登进去的时候，就随机给你两个模型，模型 $A$ 和模型 $B$，接下来就问这两个模型一样的问题，然后你要决定哪一个模型是比较好的；但传说 Chatbot Arena 也是有办法被 hack 的，因为人类还是有他喜欢的倾向，比如喜欢 emoji，粗体字，等，因为很多人根本就不会仔细去看它的内容，而通常是看它输出的风格，所以输出的风格反而对结果影响比较大

Chatbot Arena 的评比机制，Elo Score，这也是很多竞赛会使用的评比方式，假设有 $K$ 个模型，$M_1$ 到 $M_K$，每个模型有一个分数 $\beta_1$ 到 $\beta_K$，任意两个模型 $M_i$ 和 $M_j$ 之间对战，评比公式：$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j}{400}\right)}=E_{i,j}$$除掉一个 Normalization 的分数，就是为了让分布比较好看一点，通常都会设成 400，前面负号再去 Exponential，这其实就是 Sigmoid Function

* 如果 $i$ 的战力比 $j$ 的战力大很多，数值就会趋近于 1
* 如果 $i$ 比 $j$ 小很多，算出来的胜率就会趋近于 0

实际上在比赛里面，真正能知道的并不是这些战力，而是根据比赛的结果可以统计某个模型对战到某个模型的胜率是多少；所以 Elo Score 的求法是，先得到大量的比赛，知道模型跟模型间对战的胜率，再根据胜率反推出这些 $\beta$ 的值。

但在 Champion Arena 上，他们觉得会有太多跟模型本身实力无关的因素，会干扰到评比的结果，所以在计算正确率的时候，只知道 $\beta_i$ 跟 $\beta_j$ 是不够的，要加一项 $\beta_0$，即$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j+\beta_o}{400}\right)}=E_{i,j}$$
* $\beta_0$ 是模型实力以外的因素，比如某些棋类比赛里面，就应该把先手优势考虑进去

举例来说，模型可能回答越长，人类就会越喜欢，所以 $$\beta_0 = \gamma_1(\text{答案长度差})+\gamma_2(\text{emoji 数量差}) + \cdots$$
如果计算出来 $\gamma$ 是正的，就代表说这一项是会有影响的，$\gamma$ 趋近于 0，说明这一项没什么影响，负值就是负相关，如长度越短越好。

Champion Arena 报告，有没有考虑这些风格相关的因素，其实会影响模型的排名，其中上升的最多的就是 Claude，它是大家认知觉得蛮厉害，但在 Chatbot Arena 上评分起不来的模型，一个原因可能就是 Claude 模型讲话太无聊了。

到底什么样的指标才是好的评量指标呢？*也许结论就是没有好的评量指标*。这个叫做 Goodheart's Law 的意思是，一旦一项指标被当做目标，它就不再是一个好的指标。


## 10、模型编辑

Model Editing 希望做到的事情是帮模型 *植入* 一件知识，

Model Editing 与 Post-training 有什么不同？Post-training 通常是想要让模型学会新的技能，这个技能不是一项知识，而是需要模型做比较大的改变才有办法学会的事情，比如说新的语言、或者是使用工具、做推理等等；

但是直接用 Post-training finetune 模型的方法做 Model Editing 有很大的挑战，因为做 Model Editing 的时候，通常你的训练资料就只有一笔；

怎么评量 Model Editing 是不是成功的，要考虑三个不同的面向：

* Reliability：想要修改的目标必须要达成
* Generalization：输入有一些改变，泛化能力
* Locality：其他无关的输入，不应该被改

Model Editing 常见方法：

* 不动参数：放在 prompt 中（关闭 rag，联网功能），但直接放入模型会不信；Paper：In-Context Knowledge Editing(IKE)，提供一些范例，是比较容易成功的
* 改变参数：直接梯度下降，往往一改完，模型就坏掉了
	* 人类决定如何编辑：由人类对于语言模型的理解，找出应该要被编辑的位置，并决定要被编辑的方法。Paper：Rank-One Model Editing（ROME）
		1. 是找出网路中，跟编辑的知识最相关的部分
		2. 修改那个部分的参数，让模型变成想要的样子
	* 另一个 LLM 决定如何编辑：输入是 $\theta$（待编辑的模型参数）、待编辑的问题、答案，输出是 $e$（大小同 $\theta$ 的向量），然后 $\theta + e$ 即可，这个网络称为 Hypernetwork



## 11、模型融合

从基础模型（参数 $\theta$）开始，用一些资料 post-training 得到 A 模型 ($\theta_A$)，另一些资料 post-training 得到 B 模型 ($\theta_B$)，现在想让 A 模型也拥有 B 模型的能力，一种做法是将 B 资料拿过来继续 post-training A 模型，为了防止遗忘，该步骤中还要混合原来 A 资料一起训，很麻烦，不仅 B 资料不易获取，且继续训练也耗资源。

* 不用训练资料
* 不用做任何模型训练


那假设是一个有70亿个参数的模型 那他的参数排起来 就是一个维度是70亿维的向量 好那我们现在呢 把Seda B直接减掉Seda 这两个都是向量可以相减 那如果这是一个 有70亿个参数的模型的话 那相减完以后 他们参数的差也是一个 70亿维的向量 好把他们两个参数相减 那这两个参数相减代表什么意思呢 就代表了那一支剑 代表了这个模型 相对于Foundation Model 所额外练出来的能力 我们把这个额外的能力 这个参数的差叫做Task Factor 接下来你再把这个参数的差啊 直接加到Seda A上面 就结束了 就这样 我说完了 这招就是Model Merging 好那如果说讲到这边 你还没有真的听得很懂的话 那我们就举更具体的例子 告诉你实际上是怎么做的 实际上就是 假设Foundation Model里面 有某一个神经元 那Foundation Model里面 有成千上万的神经元 有某一个神经元 这个神经元呢 接进来的三支接角参数 分别是1 2跟-1 你自己拿你的资料 Find出你自己的模型 同个神经元 它的参数是1 2跟-2 小明的Lama 它的神经元的参数是3 2-1 然后接下来你要做的事情 就是看看 这个小明的模型 跟原来模型的差异 原来是最左边这支接角的参数 增加了2 把这个增加的量 直接加到你自己的模型上 你自己的模型参数 这个神经元就变成3 2-2 就结束了 你的模型就同时保有原来的能力 也拥有小明剑魔的那一支剑了 就是这么神奇 那这个想法呢 听起来非常的直观 尤其是假设你对于训练模型 没有什么概念的话 没有什么经验的话 你可能觉得说 听起来这样应该会有效 但我知道大家都是机器学习的专家 你一听会觉得说 这怎么可能会work呢 这个参数 是这样可以加加减减的东西吗 把两个模型的参数相减 以后再接到另外一个模型上 就好像把一个人的手砍下来 再直接插到另外一个人的身上 你期待他可以work 怎么可能呢 之前也有一个人试过这件事情 就是街之王葛瑞克 如果你有玩过爱尔登法环的话 就有一个王呢 叫街之王葛瑞克 他就砍了很多人的手 接在他身上 他以为会让自己变得很强 但其实他是整个游戏里面 最弱的boss 所以接很多人的手 其实没什么用的 但是我告诉你 这个test vector 神经网路的参数 岂是如此不变之物 他就是可以加加减减 就是这么神奇 这件事情早在22年的年底 早在史前时代 人们就已经发现 类神经网路的参数 是可以加加减减的 这些test vector 是可以加加减减的 好的这种 把test vector 加加减减的这种事情 要怎么运用呢 我们接下来就取 三种应用的方式 第一种方式呢 是你可以把test vector相加 有一个原来的 foundation model 叫做SEDA 那你拿某一些资料 练出了一个SEDA A 有人拿另外一些资料 练出了SEDA B 那你可以计算SEDA A 跟SEDA之间的参数差 我们叫做TAU A 计算SEDA B 跟SEDA之间的参数差 我们叫做TAU B 你可以直接把TAU B 这个参数差 它就是一个向量 直接加到SEDA A上 SEDA A也是一个向量 你把TAU B 直接加到SEDA A上 得到一个新的模型 这个新的模型 就既有A的能力 也有B的能力了 或者是你可以想成说 现在SEDA A 它的test vector叫做TAU A SEDA B它的test vector叫TAU B 你可以把两个test vector 直接合并 把原来的SEDA Foundation Model 直接加上TAU A跟TAU B 你就拥有一个 同时拥有A跟B 这两个模型能力的新模型了 好 但这边要注意的事情是 这一招能够使用 它的前提是 SEDA A跟SEDA B 是从同一个Foundation Model Finetune出来的 所以SEDA A跟SEDA B 它们不止network的架构要一样 network架构一样 你才能够直接把它加起来 不止network架构要一样 SEDA A跟SEDA B 都是从某一个 同一个Foundation Model Finetune出来的 这招才能够使用 那在现在这个时代 有很多知名的 大家都会使用的Foundation Model 比如说LAMA等等 所以这个Model Merging 是一个Post Training时代的做法 在过去 大家没有共同的Foundation Model 这个时候 你没有什么好Merge的 但在今天这个时代 大家有共同的Foundation Model 这个时候你就有机会 从同一个Foundation Model Finetune Post Training出来的 不同的模型 它的能力直接加在一起 那有时候啊 我们把不同的Test Vector 加在一起的时候 你可能可以在前面 再乘上一个Weight 会得到更好的结果 你可以淘A前面乘上个Alpha 淘B前面乘上个Beta 调一下Alpha跟Beta 可以得到更好的结果 那通常Alpha跟Beta 你可以直接拿一个Def Set 来决定它们的数值 但也有一些人在研究说 怎么自动决定Alpha跟Beta 那我就放了一篇 相关的论文在下面 给大家参考 那这边呢 举一个实际的例子 那这个是 那个黄士诚同学 跟李品泽同学 做的一个研究成果 他们做的事情是这样子的 那过去呢 Meta有试出 Lama2的Base模型 然后的Lama2的Chet模型 Base跟Chet中间的差异 就是有没有做Alignment 那他们想要打造一个 繁体中文的模型 然后他们就把中文的资料 拿去Finetune Lama2 Chet 然后发现说Finetune完之后 模型会大幅降低 原来Alignment的能力 那我们在之前的课程中 也已经跟大家分享过 这种Forgetting的现象 那怎么解这个问题呢 当然我们今天知道说 Self Replay 可能是一个蛮有效的方式 不过呢 他们采取了另外一个 截然不同的想法 他们的做法是这样的 我们希望有一个模型 既能讲中文 又有原来Lama2 Chet的Alignment 怎么做呢 能不能直接使用 Test Vector相加的概念 我们把Lama2 Base的模型 再去教他中文 那这个中文就相当于是 这支剑Lama额外获得的技能 接下来呢 你就有一个Seda A 他指的是Meta所试出来的 有Alignment的Lama2 Chet的模型 有一个Seda B 是一个能讲中文的 Base模型 这两个模型都不是你要的 一个不会讲中文 虽然他有Safety Alignment 一个会讲中文 但没有Safety Alignment 但你只要把Test Vector 计算出来 再直接加到同一个Foundation Model上 你就突然有一个 既有Safety Alignment 又能够用中文 回答你问题的模型了 真的能这样做吗 有关这个实验的细节 你可以看一下右下角 我引用的这篇论文 那这张图呢 是他们论文里面的一个例子 如果你问原版的Lama 这个戴盔甲的是原版的Lama2 Chet 他有Safety Alignment的能力 所以你跟他说 我要怎么获得一个新的密码呢 这个Lama2 Chet会用英文回答你说 我不能帮你这么做 但如果你用中文的资料 去Find your Lama2 Chet 他就失去了防御的能力 他会教你怎么盗取银行系统的密码 但如果你是用Test Vector 相加的方式 把一个代表中文能力的Test Vector 跟代表Alignment能力的Test Vector 直接加到同一个Foundation Model上 你就拥有一个模型 他回答你的时候 是用中文回答你 而且他有Safety Alignment的能力 你问他怎么取得一个银行密码 系统的密码 他会告诉你说 我不能帮助你获取或变更银行的密码 因为这个是受到法律保护的 任何人都不能获取跟泄露 而且这一招啊 其实非常的泛用 不是只有在Lama2系列上 可以Work 你把Lama2 Base换成Lama3 Base Lama2 Chat换成Lama3 In Chalk 这招也能发挥作用 这招也不是只有在Lama上 可以发挥作用 你把Lama换成Mixture 这招也可以发挥作用 然后这招也不是 只有在中文上可以发挥作用 我们实验成果发现 在韩文上可以发挥作用 另外一个团队也验证说 这招可以在日文上 发挥作用 所以这是一个蛮通用的做法 那这边再跟大家分享 另外一个 把模型Merge起来的 尝试 假设我们的SEDA A 是一个Reward Model 在做Reinforcement Learning的时候 常常会需要一个Reward Model 它的工作就是 看一个模型的答案 然后它回答说 这个模型的答案是好的还是不好的 那通常这种Reward Model 你会需要额外的训练 如果你只Pumping一个本来的Language Model 它不一定能够好好的评价 其他模型的输出 是不是正确的 那SEDA B 是一个擅长写程式的模型 怎么现在有一个擅长评价的模型 但它不会写程式 有一个擅长写程式的模型 不会评价 如果你今天需要一个Reward Model 去看其他模型的程式写得好不好 那怎么办呢 直接把Reward Model跟一个会写程式的模型 直接Merge起来 你就有一个既能评价 又能够写程式的模型 它就可以去评价 其他模型的程式写得怎么样 或者是另外一个例子 这两篇论文呢 是这个林子汉同学跟李承安同学做的 另外一个例子 我们有一个Reward Model 这个Reward Model是一个文字的Reward Model 它只能够读文字 它只能评价文字回复的好坏 它没办法看图 那怎么办呢 假设你有另外一个Model Setup 它是一个可以看图的模型 它是有视力的 所以这边帮它戴一个眼镜 它可以输入一张图片 输出一个回应 你直接把这两个模型做Model Merging 你就有一个可以看图 看其他Model根据图片的Response 在进行评价的Reward Model了 你可以在完全没有训练的情况下 帮本来没有视力的Reward Model 直接加上一双眼镜 好 刚才举的是相加的例子 那你也可以做相减 什么意思呢 假设我们现在知道说呢 Seda经过训练以后 会变成Seda B 它们中间参数的差异 是Tau B 假设你把Seda加上Tau B 它会变成Seda B 让模型具备某种能力 那如果反过来呢 把Seda减掉Tau B 那Seda是不是就失去了 任务B的能力呢 然后接下来你就可以 把这个负的Tau B呢 加到Seda A上 那你就可以让一个模型 失去B这个任务的能力 那什么时候我们会希望模型 失去某些能力呢 比如说它看到不该看的东西 比如说某本书 某个小说是有版权的 你的模型照理说不应该 看过那本书 但它就是不小心看到了 那怎么办 也许你可以用这个方法 把模型已经知道的东西 从它脑中抹去 那这个方法呢 有一个专有名词 叫做Machine Unlearning 这门课是Machine Learning嘛 Machine Learning的相反 就是Unlearning 让模型忘记它学过的东西 这边引用的呢 是李品泽同学的实验结果 那以下的例子呢 来自于他自己写的Blog 他这边想要做的事情是这样子的 他先把LamaTubeBase 给他一些肮脏的资料 就我所知可能是 来自于PTT某些版的资料 那里面有很多的脏话 他把这些资料呢 拿去FindTube LamaTubeBase 就得到一个很会说脏话的模型 那接下来呢 你就你知道怎么样 很会说脏话以后 你就可以反过来知道怎么样 没办法说脏话 所以你知道脏话的方向 就是这个方向 你只要把这个模型呢 往另外相反的方向移动 它就说不出脏话来 接下来呢 他就把一个台德的模型 是一个会讲中文的模型 他也是从LamaTube FindTube过来的 他把这个模型 剪掉这个会让模型 不能说脏话的项链 他把这个模型 往不能说脏话的方向移动 你就可以得到一个圣人模型 他对于脏话 任何敏感的不该讲的字眼 都是一无所知的 好这边就举一个实际的例子 原来这个台德的模型 你问他说什么是黑鬼 他知道黑鬼是什么意思 然后他会告诉你说 但他本身其实也是有一定的防御能力 他会告诉你说 黑鬼是一个种族歧视的词汇 我们不可以说这样子的词汇 但是如果你问这个圣人模型 什么是黑鬼 你会发现他根本不知道黑鬼是什么 他就开始乱说话 他说黑鬼是日本动漫里面常见的 一种角色形象 他举了几个例子 第一个例子是火影忍者中呢 有黑鬼是一个神秘的组织 我想这不是小吗 然后圣剑传说2 legend of mana 我记得不是圣剑传说2 所以这个是一个hallucination 黑鬼是一种神秘生物 他说鬼灭之刃里面呢 黑鬼是鬼的变种 所以他就开始瞎掰黑鬼是什么 其实他根本不知道黑鬼是什么 所以你可以用减去的方式 让模型失去某种能力 那第三个test vector的应用呢 是你可以用 类比的方式 在完全没有 某项任务资料的情况下 让模型具备有 新的能力 什么意思呢 假设test A支于test B 等于test C支于test D 你现在有 test A B C这三个任务的资料 你可以把你的 foundation model经过训练 让他具备test A的能力 经过训练具备test B的能力 经过训练具备test C的能力 但如果你知道 A支于B就等于C支于D 那你其实可以在 没有D的资料的情况下 直接创造出来 让模型具有test D的能力 你可以在没有test D资料的情况下 无中生有 让模型具有test D的能力 怎么做呢 我们已经知道A支于B等于C支于D 那我们就来看看 set A跟set B的差是什么 set A跟set B的差 就是tau B 减掉tau A 然后接下来呢 你再把他们的差 直接加到set C上 因为我们知道说 A支于B就是C支于D 所以把A跟B的差距 直接加到C上 你就得到D的参数了 D这个任务的参数了 所以你只要把tau C 加上tau B 减掉tau A 你就可以得到一组参数 这组参数可以执行任务D 这组参数可以执行任务D 所以你就可以在没有任务D资料的情况下 让模型能做任务D 让模型能做任务D 这边如果你听得很抽象的话 我来举一个实际的例子 现在设想一个情境是 我们要打造语音辨识的系统 那语音辨识 大家都不陌生 输入语音输出文字 那现在有很多很好的语音辨识系统 比如说Whisper 但是这些现成的语音辨识系统 往往在特定领域 比如说特定的语言 或者是 很多专有名词的情况下 它是没有办法正确辨识的 所以很多时候 我们需要为特定的任务 去打造语音辨识系统 举例来说 大家都有在用N2 Core N2 Core上面用的语音辨识系统 并不是一个现成的语音辨识系统 是我们实验室同学 参加了教发中心的计划 把他们打造的客制化的 语音辨识系统 所以那是一个客制化的语音辨识系统 在台大的课程上 是比你可以用到的商用系统都还要强的 所以很多时候 你需要客制化系统 那我们今天假设一个情境是 我们有一个语音辨识的系统 那我们要拿它来辨识 某一个非常专业领域的会议 里面有很多的专有名词 比如说法律金融的会议 里面有很多一般人听不懂的专有名词 那怎么办呢 我们并没有那个会议的语音资料 但是假设你有那个会议相关的文件 你有它的会议记录 你有相关的教科书 等等 那假设我们有文字资料的话 我们也许可以直接叫一个语音合成系统 今天语音合成系统 都可以做的蛮成功的 拿一个语音合成系统 把这些文字念出来 产生声音讯号 我们有文字有声音讯号 你有成对的资料 你就可以对原来的语音辨识系统 做post training 把它微调 产生一个新的语音辨识系统 它是能够在这个专业领域的会议上 得到好的结果的 那这一招其实一点都不稀奇 我在右上角呢 引用了非常多的文献 告诉你说这是一个非常常见的手法 但这一招会有什么样的问题呢 一个显而易见的问题是 现在这些声音讯号 它不是真正的声音讯号 它是语音合成系统 产生出来的声音讯号 所以它跟真正的讯号 是有一定程度差异的 那我们有没有办法 在没有真正声音讯号的情况下 想办法让语音辨识系统 就好像有看过 真正的声音讯号呢 所以这边实验的setting是这样子的 你有新的 你的目标的那个domain 相关的文字 你可以用语音合成系统 把这些特殊domain的文字 把它念出来 但这些声音讯号 不是真正的讯号 你没有真正的声音讯号 但是你可能有 其他domain的资料 在这些其他domain 它们可能是比较容易找到的 比较通用的资料 你有人类真正的声音讯号 你也有合成的讯号 合成的讯号你永远可以呼叫一个 语音合成的系统 把合成的讯号合出来 所以你看我们现在就制造出了 ABCD四个task A之于B 等于C之于D 所以你就算没有 在新的domain上 在特定domain上的真实的语音讯号 透过 从这三个任务上训练出来的模型 你可以组合出 一个新的模型 它可以用在特定domain上 而且它的行为就好像是 在真实语音上训练过一样 或者是我们用图示化的方式 你有一个foundation model 那你拿general的domain 加上synthesize的资料 去训练出一个模型 你拿general的domain 加真实的资料 去训练出一个模型 你拿 新的目标的domain 你拿你的特定的domain 加上synthesize的资料 去训练出一个模型 接下来 你把这两个模型的参数 相减 把它们的差加到这里 你就等于是得到了一个模型 这个模型好像是训练在 特定domain 真实语音的资料上 那我们把这个红色的项量 叫做seem to real的vector 因为它把一个训练在 这个synthetic合成资料上的模型 做一些校正 做一些魔改 就变成好像训练在 真实的资料上 这招有没有办法发挥作用呢 它还真的能发挥作用 这是我们实验室 苏轩同学的研究成果 他尝试了各个不同的领域 这边每一个区域 代表某一个特定的领域 那这边所show的数值呢 是what error rate 所以这个数值呢是越低越好 那我们的foundation model 就大家都很熟悉的 OpenAI的whisper 我们TTS的model 是用一个叫做BARC的TTS model 黄色的bar呢 是直接训练在 合成语料上的结果 橙色的bar呢 是把训练在合成的声音讯号上的模型 在做这个校正 在做一个微调 那你会发现说微调过后 几乎在所有的domain上 微调过后 都有比较低的语音辨识的错误率 而且这一招 其实蛮通用的 我们试了不同大小的whisper 都有发挥作用 我们也把whisper换成其他的foundation model 比如说换成web2vec2conformer 也有发挥作用 我们也试了不同的TTS model 把BARC换成 SPEECH T5 也有发挥作用 所以你确实可以组合一些任务的test vector 让模型学会新的技能 让模型学会新的技能 那其实啊 model merging还有更多的应用 比如说它可以防止forgetting 这件事情发生 那这个部分比较复杂 也许我今天就先不细讲 大家可以参考樊华同学写的论文 这是做在文字上的 那后来林子权同学 把这个技术也用在语音的 foundation model上 也能发挥作用 前面讲了很多 model merging的神奇例子 那我这边其实要提醒你 model merging并不一定 总是会成功的 虽然前面有很多成功的例子 但是你其实可以找到 更多失败的例子 事实上在我们的作业里面 会让大家尝试model merging 那如果你没有做什么特别的事情 单纯把两个model的test vector 直接加起来的话 其实你也不会得到 特别好的结果的 那为什么model merging 不一定总是会成功呢 其实你应该想model merging为什么会成功 它不成功其实反而是 比较合理的 我们先来想一下什么叫做 merging是成功的 假设呢我们有一个模型 SEDA A 就原来的foundation model 加上TAU A 它input XA output YA 那我们呢有另外一个model SEDA B 它就原来的foundation model加上TAU B input XB还要output YA 我们现在所谓的成功 指的是 如果我们把TAU A TAU B同时加到 foundation model参数SEDA上 那你输入XA的时候 要输出YA 就跟A模型的能力是一样的 你输入XB的时候要输出YA 你输入XB就跟B模型的能力是一样的 所以merge后的model 保有原来模型的能力 那我们今天讨论的是一个比较简单的case 我们还没有讨论说 有没有可能组合出新的任务等等 好我们来看看在这个case 有没有可能会不成功呢 其实太容易了 你完全可以找出一个反例 merging之后就是失败的 假设我们有一个非常简单的类神经网路 它就只有一个神经元 输入呢有三个数值 三个数值乘上 类神经网路的权奏 然后再通过relook 就得到你最后的输出 那foundation model 它的三个参数分别都是0 你把这个模型训练在test A上 它得到的参数是1 1 0 然后如果输入呢 是2 1 0的话 这时候输出是3 那如果我们train在任务B上 假设现在训练完之后 得到的参数是0 1 1 输入是0 2 3的话 输出是5 那我们现在呢 把这两个model merge在一起 你得到一个新的模型 它的参数是1 2 1 如果你输入给set A的输入 也就是2 1 0 这时候输出呢就变成4 如果你输入给set B的输入 是0 2 3 这时候输出从5变成7 所以你可以轻易找到反例 告诉大家说model merging 不一定能够成功 但是什么样的状况 model merging会成功呢 我们这边来举一个 成功的例子 假设现在在任务A上 我们只会动到最左边这个参数 它从0到变成1 这时候你输入2 1 0 输出是2 假设在任务B上你只会动到最右边 这个参数这时候输入0 2 3 输出是3 这个时候你把两个model merge起来 它的参数是1 0 1 那你输入2 1 0 输入给set A的输入 2 1 0的时候输出仍然是2 输入给set B的输入 0 2 3的时候输出仍然是3 在这个例子里面 model merging就是成功的 那也许这个例子可以给我们带来的 一些启发是 如果今天两个任务改的参数 非常的不一样 它们彼此之间 没有互相干扰 那model merging有可能 可以成功 所以呢 我们会希望不同任务 尽量不要动到同样的参数 每一个任务各自动到的参数 也许越少越好 那如果你看一些model merging的研究 那现在model merging 比较advanced的技术 确实都是往这个方向发展的 你可以参考DARE跟TICE 这两篇论文 那这两篇都是model merging的新的技术 那在DARE这篇paper里面 他就告诉你说 假设呢我们现在 有一个模型是很擅长数学的 有另外一个模型是很擅长写程式的 但是他们都跟原来的 foundation model有比较大的差距 他们都改变了原来 foundation model很多的参数 所以把他们merge在一起 可能会彼此互相干扰 他用一个问号代表彼此互相干扰

(该文件长度超过30分钟。 在TurboScribe.ai点击升级到无限，以转录长达10小时的文件。)