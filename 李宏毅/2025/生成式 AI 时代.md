
## 1、生成式 AI 的技术突破与未来发展
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；

三个面，剖析 AI agent 的关键能力：

1. 能不能够根据他的经验，过去的互动中所获得的经验来调整他的行为 
2. 如何呼叫外部的援助，如何使用工具 
3. 能不能够执行计划，能不能做计划

关于 1，真正的议题是：如果我们是把过去所有的经验都存起来，要改变语言模型的行为，要让他根据过去的经验调整行为，就是把过去所有发生的事情一股脑给他，那就好像是语言模型每次做一次决策的时候，他都要回忆他一生的经历，也许他没有足够的算力，来回顾一生的资讯，他就没有办法得到正确的答案，所以怎么办呢？

也许我们可以给这些 AI agent memory，这就像是人类的长期记忆一样，发生过的事情，把它存到这个 memory 里面，有一个叫做 read 的模组，会从 memory 里面选择跟现在要解决的问题有关系的经验，把这些有关系的经验放在 observation 的前面，让模型根据这些有关系的经验跟 observation 再做文字接龙，

那怎么样打造这个 read 的模组呢？就想成是一个 retrieval 的 system，其实它就是 RAG，唯一不一样的地方，如果是 RAG 的话，存在 memory 里面的东西是别人的经验，而对 AI agent 而言，是他自己个人的经历，差别的是经历的来源。

根据经验调整行为能力的好坏，那就看这一整个回答的过程中平均的正确率，越能够根据经验学习的agent，应该能够用越少的时间，看过越少的回馈，就越快能够增强他的能力，就可以得到比较高的平均的正确率。

还发现一个有趣的现象是值得跟大家分享。这个现象是负面的回馈，基本上没有帮助

可以有一个 write 的 module，决定什么样的资讯要被填到长期的记忆库里面，怎么样打造这个 write 的记忆库呢？一个很简单的方法就是 write 的模组也是一个语言模型甚至就是 AI agent 自己

还有第三个模组，没有固定的名字，暂时叫 reflection 反思的模组，这个模组的工作是对记忆中的资讯做更好的、更 high level 的，可能是抽象的重新整理，可能也是一个语言模型，或 AI agent 自己

-------

关于 2，怎么使用工具？小模型呼叫大模型帮忙，这些工具对语言模型来说都是 function，

使用工作的挑战，每一个工具都要有对应的文字描述，告诉语言模型这个工具怎么被使用，假设工具很多怎么办呢？类似上文，打造一个工具选择的模组；也可以自己写个 function 作为 tool

工具有可能会犯错，所以我们也要告诉我们的工具，这些不要完全相信工具的工具，要有自己的判断能力，不要完全相信工具的工具给你的结果，比如给出的天气温度为 100°

那什么样的外部知识比较容易说服 AI，让他相信你说的话呢？外部的知识，如果跟模型本身的信念差距越大，模型就越不容易相信，那如果跟本身的信念差距比较小，模型就比较容易相信

----------

关于 3，AI 语言模型能不能做计划？语言模型就是给一个输入，它就直接给一个输出，也许在给输出的过程中有进行计划，但我们不一定能够明确的知道这件事；

但其实可以强迫语言模型直接明确的产生规划，可以直接问语言模型说，如果现在要达成我们的目标，从这个 observation 开始，你觉得应该要做哪些行动？

不过这是一个理想的想法。那语言模型到底有没有做计划的能力呢？过去确实也有很多论文说，语言模型是有一定程度做计划的能力的。

现在到底模型规划的能力怎么样呢，就是介于有跟没有间吧……

-----------

## 3、语言模型内部运行机制

* 一个神经元在做什么
* 一层神经元在做什么
* 一群神经元在做什么
* 让语言模型直接说出它的想法


**一个神经元在做什么？**

主要指 FFN 中的单个神经元，其每个输出都是所有输入的某种 weighted sum，然后通过如 ReLU 等激活函数，得到一个神经元的输出。

1. 比如某个神经元的启动与说脏话有关联，语言模型说脏活，该神经元都会启动

2. 移除该神经元，语言模型说不出脏活，什么叫把神经元从网络中移掉？让其输出永远为 0？但 0 不代表完全不会造成影响，0 可能会对其他神经元有影响，当它输出为零，反而会启动其他神经元。有研究发现也许设平均值比较好，各种各样不同输入时值的均值，尚待研究。

3. 不同启动程度，说不同等级的脏话

实际上，不容易解释单一神经元的功能，一件事情可能很多神经元共同管理，例如 LLaMA 一层只有 4096 个神经元，神经元数量太少了，如果一个神经元负责一件事，则没办法应对千变万化的任务；

**一层神经元在做什么？**

假设第 10 层神经元的输出中，第一个，中间某一个，最后一个神经元被启动时，就会拒绝请求（我很抱歉，我不能帮你......），这个神经元的组合可以看作是高维空间中的一个特定方向的向量（功能向量），至于在那一层找，只能做实验，试试看喽

因为如果第十层的输出与功能向量越接近，越有可能拒绝请求，那怎么找出这个 *功能向量*，

找一些（会被拒掉）的句子，算出第 10 层的输出表示，该表示 = 拒绝向量 + 其他内容，例如找出 1000 个句子，`所有表示平均起来 = 拒绝 + 其他的平均`；同理再找 1000 个没有被拒的句子，第 10 层的输出平均起来作为 *其他的平均*；

怎么验证？找个句子，在第 10 层的输出中把 拒绝向量 加进去，看看输出有什么变化。反过来说，本来有一个拒绝的句子，如果把 拒绝向量 减掉，会不会就不拒绝了......

*In-Context Vector*：就是说，比如给一些反义词对的句子，把某一层的激活平均起来，Zero-Shot 的时候把这个向量加到给定的单词上，就会输出反义词；有意思的是，这些功能向量时可以加加减减的

*是否可以把自动把某一层所有的功能向量都找出来？*，例如一共有 $K$（非常大）个功能，第 10 层的输出那应该是这 $K$ 个功能向量的线性组合 + $e$，$e$ 是指一些其他的内容，$e$ 的值应该越小越好，系数 $\alpha$ 也是越小越好，意味着每次选择的功能向量越少越好。

**一群神经元在做什么？语言模型完成某一项任务的机制**

语言模型的模型，模型是指用一个较为简单的东西来代表另一个东西，模型的特性：

* 要比原来实物简单
* 保有原来实物的特征（faithfulness）

比如 “THE Taipei 101 is located in ”，其中 "The Taipei 101" 经过多层后编码为 $x$，"is located in" 就相当于一个线性函数 $f$，最后得到答案 "Taipei"，即 $y$，这样给一些样本，比如 8 个，就是要找到 "is located in" 对应线性函数 $f$ 的 $W,b$，

**怎么让语言模型直接说出想法？**

就直接问，让它输出 reason 等，语言模型的思维是透明的，

把残差连接的思维图转换一下，一开始就是直通的，侧边才是残差，每一层侧边加一点东西进去，这才是 transformer 的整体运作机制，只是最后一层接了个 unembedding，所以想法是在中间层的输出中也加入 unembedding，这一招称为 logit lens，输出的分布过 softmax 之前就叫做 logit，

但每一层侧边加了什么呢？MLP 可以看作是一个有 K 有 V 的 attention，概念来自 Paper：Transformer Feed Forward Layers Are Key-Value Memories，可以把前一层的值作为 $k_1,k_2,\cdots$，层间参数作为 $V$，例如以 $k_2$ 连接到下一层的 weight 组合作为 $\mathbf{v_2}$（向量） ，因此输出就是 $\sum^D_{i=1} k_i\mathbf{v_i}$，那每一层残差部分的输出本应该一样，导致前后输出差异就是这个侧边块影响的。

所以把侧边的 V，也通过 unembedding layer，试着通过 logit lens 解析每一个神经网络想要输出什么东西，是什么东西被加入到残差中，影响最后的输出分布

logit lens 方式致命缺陷，通过 unembedding 的方法，只能把一个 Representation 转成一个 token，我们解析出来的结果，都只能是一个 Token，另一方面 LM 是在预测下一个 token，所以这个 Representation 不见得是代表你输入内容的含义，新的招数，Patchscopes（与深度不够，长度来凑的 reasoning 方式有异曲同工之妙）

## 4、Transformer 替代架构

最知名的，Mamba，其实是非常类似的的东西，每一种架构的存在都有一个理由。

* CNN：根据影像的特性，减少不必要的参数，避免过拟合；
* 残差连接：为了让 Optimization 更容易
* Transformer：
	* Self-attention 取代掉 RNN (LSTM)：
		* RNN-Style，输入函数 $f_A$，隐藏层函数 $f_B$，输出函数 $f_C$，当然这些函数可以是 $f_{A,t}, f_{B,t}, f_{C,t}$，这意味着每次都不一样，可以让其与 $x$ 挂钩，根据 $x$ 的内容决定是否要遗忘一些内容，是否拒绝当前输入进入记忆中等，即 LSTM 的思想
		* 推理：
			* self-attention：计算量、记忆体需求随着序列长度增加
			* RNN：计算量、记忆体需求固定
		* 训练：
			* self-attention：容易平行化
			* RNN：难以平行化？
	* Linear Attention 就是广义 RNN 拿掉 ”Reflection“，$f_{A, t}$；  $H_t = H_{t-1} +v_tk_t^T$ ；$y_t=H_tq_t$
	* Linear Attention 就是 Self-attention 没有 Softmax
	* Linear Attention，在推理的时候像 RNN，训练的时候就可以展开像 self-attention 
	* RNN（Linear Attention）赢不过 Transformer（Self-attention with softmax）？貌似只差在 softmax，其实问题在于 Linear Attention 记忆永不改变，永不遗忘，那为什么 softmax 可以改变记忆？比如 `[0.6 1, 0.4] -> [0.3, 0.45, 0.25]` 过 softmax 前有一项是 1，代表很重要的事，`[0.5, 1, 0.5, 2, 1] -> [0.10， 0.17， 0.10， 0.46， 0.17]` 1 是很重要的事，但有 2 更重要的事， 1 相对就没那么重要
	* 加上 Reflection：逐渐遗忘（Retention Network， RetNet），加上了 $\gamma$ (0~1 之间)，$H_t = \gamma H_{t_1} +v_tk_t^T$
	* 但这会导致啥都会被逐渐遗忘，希望的是依据语境，有些事要牢记，有些可以淡忘，即 Gated Retention，$\gamma$ 换成 $\gamma_t$，随着时间更新，$\gamma_t = \text{softmax}(W_\gamma x_t)$
	* 更复杂的 Reflection, Paper: Parallelizing Linear Transformers with the Delta Rule over Sequence Length
* Mamba（就是 RNN） 取代 Self-attention：
* DeltaNet：公式化简后得到类似梯度下降更新公式形式


## 5、预训练-对齐

弱智吧， 比知乎来源 做 fine-tuning 要好，可能原因是弱智吧里的答案都是 gpt-4 生成的

把随便什么数据前半截作为输入，后半截作为答案，结果很差，但如何后半截是 LLM 生成的，则效果很好；

Alignment 前后模型实际行为差异不大：以 How are you? I am 为例，对齐前，fine 的概率最大

* Unshift：fine 的概率仍然最大；
* Marginal：fine 的概率第二或第三，有一点点变化
* Shifted： fine 不在前三名

Alignment 前后只是一小部分 token 上有差异，所以是不是可以直接改 token 输出的机率？

* 增加结束符号的概率
* 手动改变一些 token 出现的机率
* 避免出现重复的符号

结果是有用，但效果还是无法和 instruction tuning 后的模型比。

Paper：Self-Rewarding Language Models，他们不用 alignment 的资料，先问模型一个问题，产生多个答案，让模型自己对答案进行评分（提供评分指示），在根据这些分值对模型做 RL，能力就起来了

如何达成有效的 Pretrain？Paper：Physics of Language Models: Part 3.1，以人物简历为例，同一个人有很多种不同的介绍方式，这其实对于 pretrain 是一件重要的事情，而且不需要每个人都有多个版本的介绍，一些人有多个版本就够了

15T 的 token，基本现在 LLM 预训练的 token 量；资料品质的重要性

Alignment 的极限：

* highly known：pretrain 的模型本来就会的知识，需要提供范例指引，
* Maybe Known：有的范例可以指引回答正确，有的不行，要问的对，模型应该有这方面能力
* Weekly Known：需要对解码采样，有几率得到正确答案
* Unknown：怎么采样都无法答对

实验发现 Maybe Known 类型的资料是最有帮助的。只学 Unkown 的是最烂的，学不会。

因此，在 Align 阶段，不太容易教模型新知识，Align 真正能做的是调整模型的行为，他本来不知道看到一个问题要回答问题，现在能够微调让他能回答问题，但这个答案是他本来就知道的事情，才有办法真的教会他，而不破坏他原有的能力。

RL 是 Alignment 的好方法，RL 中，所有的答案都是 LM 自己生成的，并不是人类强塞一个答案给语言模型，所以 RL 的目标并不是叫模型会它本来完全不会的东西，只是提高了某些本来就可以生出来内容的机率，所以 RL 真正做的是激发模型本来的潜力。

Pretrain 的后遗症？pretrain 资料的分布对最终结果有很大影响，比如 rot-13（把所有字母偏移 13 个位置）；pretrain 时看到不该看的东西后，难以真正清除；Alignment 只是说不去激发他，但这些参数仍然在模型中，


## 6、后训练与遗忘

* pre-train 风格：文字接龙的方式
* SFT 风格：一问一答
* RL 风格：偏好

案例：教 LLaMA-2-Chat 用中文回答，该模型主要训练资料是英文，对此可搜集大量中文数据进行训练（Pre-train 风格），但实际做完后发现，原来 Alignment 的能力被破坏了；

这种遗忘的现象非常普遍，在其他风格的后训练上也都有这种现象，Safety Alignment 在我们的经验上，是最容易被破坏的能力；当然他也破坏了模型很多其他基础的能力。

案例：教 LLaMA 听声音：声音通过 Speech Encoder，比如 0.2s 转为一个向量，然后 LLM 中加入 Adapter，然后最后就忘了 json format；

所以 Post-Training 最大的挑战是模型会遗忘。这个现象叫做灾难性遗忘（Catastrophe Forgetting），但这也取决于我们的应用，可能我们只希望它在程式上表现好，其他方面能力不太关注，但现在可能更多的是希望有一个通用模型。

那这种现象是不是因为模型不够大？但研究上表明不是，但与在目标任务上训练效果有关，目标任务达成的越好，这种遗忘现象越严重，Paper：LoRA Learns Less and Forgets Less；

可以用 experience replay 方法缓解，就是在训练新任务的时候，混入一些原始任务数据，比如 5% 的占比；但现在的这些 LLM 模型我们无法获取它们的预训练资料，咋办？让 LM 自言自说，自己生成一些数据，Paper：Magpie（喜鹊的意思）

Paraphrase 方法：在 post_training 数据上，让模型先改写答案，然后再去训练，结果会较好；

self-output 方法：直接让模型产生答案（可能是错的，需要纠错，或者多次回答，选对的） ---> 这和 RL 风格的非常像。这种方法可以用在听语音案例说，先把语言大致标注一些，用文字的方式描述，比如什么时长、说的什么内容，说话人性别，心情等，然后给一个问题，要求 LM 输出并作为答案。

人工写的答案中，很多 token 是很难生成出来的，而模型生成出来的内容，是比较容易生出来的。

我认为未来的趋势是，多数语言模型仍需要具备自己独特的能力（例如：特别擅长写程式）。一个大型计划可能需要由多个具备不同专长的语言模型组成团队，共同完成，就像人类社会中的分工合作一样。但要注意，所谓「特别擅长写程式」，并不代表其他能力都可以一概没有，如果有一个擅长写程式的模型，它在团队中的角色是程式设计师，但若它除了写程式以外，甚至无法理解人类语言，听不懂其他团队成员的指令（如：「这里有个 bug」），无法与他人沟通，那并不是我们所追求的。因此，我们并不希望在 Post-training 阶段发生严重的遗忘现象，至于可接受的遗忘程度，则取决于实际需求。例如，一个担任程式设计师的语言模型如果不会写唐诗，可能还可以接受；但如果它连基本的语言能力都丧失，那就不行了。就好像人类的程式设计师也许不一定擅长社交，但至少能跟其他人类沟通。

## 7、如何进行深度思考

chatgpt o1/o3/o4、DeepSeek R1、Gemini 2 Flash Thinking、Claude 3.7 Sonnet（Extend Thinking）……

通常会在思考的过程前后加一个\<think\> 和 \</think\>，为了界面呈现的方便

在这个思考的过程中，通常模型会有几个行为：

* 验证自己刚才的答案是不是正确的 Let me check the answer...
* 可能会进行探索，Let's try a different approach...
* 有时候他甚至会做一些规划,Let's first try to... 

这种 reasoning 的行为是 Test-Time compute 的一种，在测试的阶段投入了更大的算力，而这个投入的算力可能可以让你得到更好的结果，Why？*深度不够，长度来凑* 

*我们先讲这种深度思考的语言模型可能用什么样的方式被打造出来？下节课来探讨这些 reasoning 的过程到底有没有或者是如何发挥作用*

另一个词汇 Test-Time Scaling，意思是思考越多往往结果越好，

**打造推理语言模型的方法：**

* 更强的思维链，CoT（不用微调参数）
* 给模型推论工作流程（不用微调参数）
* 教模型推理过程（Imitation Learning）
* 以结果为导向学习推理（RL）

**1、CoT：** 让模型先列出解题过程，再给出答案

* few-shot CoT：要给模型一些范例，
* zero-shot CoT: let's think step by step，就会自动把计算的过程列出来

但是因为现在这个思考的过程往往非常的长，所以又有一个新的名字叫做 Long CoT，


**2、直接给模型推理的工作流程**

如果只是叫模型，请尝试越多方法越好，它往往就尝试个两三个方法就结束了，怎么让模型不断尝试，最好尝试个几千几万种方法？也许可以直接强迫模型对同一个问题回答几千几万次，因为每一次模型在回答问题的时候，他的答案都会是不一样的。

Paper：Large Language Monkeys

对于稍微好一点的模型，试的够多次，他总是有机会猜到正确答案，但难点是，怎么知道哪一次得出来的答案是正确答案呢？

* 一个直觉的方式就是投票，Majority Vote，有另外一个称呼，叫做 Self Consistency；
* 另一个方法是看答案的 Confidence，这个方法被用在 COT Decoding 的方法里面；

并强制要求把答案放在 \<answer\> 和 \</answer\> 中间，方便做 Majority Vote，该方法其实很强，可以作为一个很好的 baseline；

 也可以用更复杂的方法，从模型众多的尝试中选出正确答案，现在常用的做法是，训练一个 Verifier，或者找一个模型当做 Verifier，让它去验证答案是不是正确的。如果越有可能是正确的，就输出越高的分数，这个方法叫做 Best of N。
 
那怎么得到这个验证器呢？那最简单的方法也许是直接拿一个语言模型即可。但如果你想要做得更好的话，可以对验证器加以训练，怎么对验证器加以训练呢？

下面的实验中，我们假设有一些训练资料，比如一堆数学问题，有标准答案，只是没有计算的过程。有这样的训练资料以后，就可以把问题丢进给语言模型，然后让语言模型产生多个不同的答案，因为有正确答案，所以知道语言模型什么时候是答对的，什么时候是答错的，这样就有验证器的训练资料，看到这个正确的答案输出 1，看到错误的答案就输出 0 等等。 

上面的方法是并行的方式，其实还有序列的方法，先让他解第一次，然后根据第一次的解法再去解第二次，等等；而且这个并行方式和序列方式可以同时使用。 

但如果你看现在这一些会做深度思考模型的行为，会发现他们往往不是得到最终答案才进行验证，它们往往能够做到，在解题解一半的时候，中间某一个步骤，就开始验证这个步骤是不是对的，以避免中间算错了浪费时间。

那如何做每一步的验证？给模型一个问题之后，先不要让模型解完，让它每次只输出第一步就停止，比如生成 3 个不同的第一步，然后通过 Process Verifier 去验证，可以通过 prompt 去要求如何一步步生成，比如每一步都放在 \<step\> 和 \</step\> 之间。

那如何得到 Process Verifier？我们有的只是问题（input）和对应的 ground truth，我们要求 LLM 在给定 input 和 step_1 后，继续生成多个 step_2 ... 到答案的流，比如生成 3 个流，其中 2 个最后答案正确，正确率为 2/3，同样验证 step_2 的过程也是如此，指导 Process Verifier 输出从该步骤开始得到正确答案的概率值。

那接下来会遇到的另外一个问题是，通常这个 Process Verifier 的输出并不是 True or False，而是一个数值，代表从这一步继续做下去以后，可能得到正确答案的机率，那怎么定这个阈值？*可以参考 Beam Search 的做法*。

---------------

**3、教模型推理过程（Imitation Learning，模仿学习）**

接下来两个方法都是后训练的方法的特例，也就是我们有一个 Foundation 的 Model，还不会做深度思考，但我们接下来做 Post Training，希望让其具有深度思考的能力。

*Imitation Learning，模仿学习*，直接教模型怎么做推理，这边假设我们的训练资料，除了问题、答案，还有推论的过程。

我们只要教模型说，看到输入内容之后，不仅要产生正确答案，还要产生 Reasoning Process，然后才产生正确答案。

这里面最难的地方是什么呢？ 是*这么推论从何而来？*

让语言模型自己想办法，产生推论的过程，给他 input 要求做 COT，把推论的过程详细解出来，然后他就产生推论过程，并产生答案，但毕竟能力有限，它不会每次都答对，但我们一般有正确答案，所以可以对比保留回答正确的 reasoning process 拿来当作训练资料。

有没有可能答案是对的，但推论过程其实是错的？确实我们没有办法保证答案是对的，推论过程每一步就是对的，所以就有人提出一些方法说，也许我们不应该只看最终答案对不对，我们应该用类似上面提到的过程验证方法去验证。

这里其实除了 SFT 的方法，也可以用 RL 的方式去训练，

但我们真的应该这样教模型吗？应该要告诉模型每一步推论过程，都必须要是正确的吗？

可以深度思考的模型，其实有时候会得到错误的思考过程，只要他最后能够得到正确答案就好了，所以我们甚至不应该教模型的时候，给他的每一步的推论过程都是对的。

会有什么问题呢？如果给他所有的推论过程都是对的，他会不知道要找找自己的问题，他每次都会觉得前面的推论过程一定都是对的，无法纠错。

怎么办呢？我们故意制造出一些特殊的训练过程，让训练过程中间可能有一些是有错的，因为语言模型他本身能力还是有极限的，我们应该要让语言模型有知错能改的能力。

那怎么做呢？Paper：Stream of search（SoS），这篇 paper 的想法就是，我们能不能从树状结构（推导步骤树）里面得到一个 reasoning 的过程，这里面是包含错误答案的，我们直接在这个树状的结构上面做一个深度优先的搜寻，把错误的搜寻过程也包含进训练资料里。故意走一些错的路，尝试回退重试其他方案。

对人类来说，如果你看到语言模型先产生一个答案，然后再莫名其妙再跳到另外一个答案，你可能会觉得这个语言模型讲话很容易前言不对后语。

最早的 o1 也容易前言不对后语的，当然这个 o1 并没有展现那个完整的 reasoning 的过程，也不知道是摘要那边出了问题，还是那个语言模型的思考就是非常跳跃，搞不好就是用这样子的资料训练出来的，所以他思考非常的跳跃。

到目前为止，讲了一大堆创造 reasoning 过程当作训练资料的方法，但如果你今天想要自己打造有 reasoning 能力的模型，可以直接做*知识蒸馏*。

**4、以结果为导向学习推理（RL）**

DeepSeek-R1 系列的做法，

有一些训练资料，有问题及正确答案，把问题输入给模型，要求模型做 reasoning，思考内容不重要，只看它最后的答案，与标准答案对比，如果是对的，模型就得到 positive reward，如果是错的，就得到 negative reward；

*在 RL 中，推论的内容不重要，只在意最后的答案是不是对的。*

R1-Zero 中，作者非常想要让大家知道的一件事情，就是 *aha moment*，没有教他，他自己就会了。

R1-Zero 的 reasoning 过程是非常难读的，而且是多个语言混杂的，为什么？因为训练中只在意结果，根本没有在意他推论的时候到底写了些什么东西。

R1 是怎么被打造出来的呢？*其实在打造 R1 的过程中，前面讲的三个方法都是有用上的*。

![[Pasted image 20250429222458.png|500]]

具体过程是：有了 R1-Zero 之后，用其来产生有 reasoning process（人读不懂的推理过程） 的训练资料，然后人工改，这部分到底花了多大代价，技术报告中没明说。

（PPT 左侧这两个句子都是技术报告里卖弄的原文）

除此之外，它们还用另一个模型（论文未清楚介绍），用 few-shot COT 的方法来产生一些带有 reasoning 的资料；也用 Prompting 的方法，让模型产生一些detail 的 answer 而且要有 reflection，要有 verification。 那这个显然就是*supervised COT* 的方法。

然后就可以做 *Imitation learning*，训练出 Model_A，接下来 Model_A 会再进一步去做 RL 得到 Model B，但 Model_B 做 RL 的过程，也和 R1-Zero 略有点不同，除了要要求他正确率越高越好以外，还有一个额外的限制，就是语言必须要用一样的。

![[Pasted image 20250429230322.png|500]]

Model_B 也是用来生成训练资料的，不能只考虑数学跟程式，要加各式各样的任务，模型 B 已经初步具备 reasoning 的能力，产生reasoning 的过程，再产生答案。

答案的验证：虽然这些问题有答案，但很多问题没有标准答案，用 V3 来做验证，还用了一些规则去掉 reasoning 的过程中比较糟糕的过程，比如多个语言，过长的等。生了 60 万训练资料。

接下来，还是一样做 *Imitation learning* 再重新训练 *DeepSeek-V3*，然后得到 Model_C，最后再做一次 RL，这部分写的非常的模糊，是希望强化模型 safety、helpfulness 的能力，最后才得到 R1。

报告的结尾有提到说，他们也曾经想要尝试做 process verifier，但是最终没有做起来，没有得到好的结果，这是一个上代研究的问题。

*另外一个模型，如果本来就不够强，用 RL 是没有办法激发它 reasoning 的能力；如果一个模型用 RL 之后，可以激发它 reasoning 的能力，那意味着它其实本来就有 reasoning 的能力，RL 只是强化这件事情的出现而已。*


推论模型*最大的挑战* 是什么呢？最大的挑战就是要产生非常长的 reasoning 的过程，显然是花钱跟花算力的，我们其实希望该 reasoning 的时候才 reasoning。


## 8、推理过程不用太长，够用就好

如何让推理的 LLM 不要想太多。

前面的课程中有提到，如果有比较长的推理，有可能结果会更好。 但真的是这样吗？现在有很多研究表明，其实*推理越长，不一定代表结果越好*。 

很多论文都说，推理长度越长，往往你会正确率越低。但这样的实验方法其实并不够严谨，比如说，可能是因为问题太难，所以需要更长的推理长度。也有严谨的实验表明，推理的长度看起来对于正确率真的是不一定有帮助的。

最好的工程师不是把事情做到完美，而是在有限的资源下把事情做到最好。

**如何避免模型想太多？**

* 关于 CoT，一般就是让模型 think step by step，Paper：Chain of Draft，要求每一个 thinking step 都只是一个草稿，然后草稿的每一条都不要超过五个字，实验效果表明有效；
* 有关第二个方向，给模型推论工作流程，这里的模型推论的工作流程是人设定的，所以要让模型推论短一点，是完全可以控制它，比如说让它 sampling 少一点，让它做 beam search 的时候 beam 小一点，让它产生树状结构的时候树长得小一点，就人工可以控制模型 reasoning 的长度；
* 第三个方法就是直接教模型怎么做推理。给模型正确答案，然后教它怎么进行推理。比如让推理模型作为老师进行指导，那怎么在这个学习的过程中把推理的长度考虑进去呢？同一个问题问多次，答对情况下，选一个最短的推理过程，当作学生模型的训练资料；
	* 另一个方法：Paper：From Explicit CoT to Implicit CoT，把明着写出来的 CoT，练到不见，具体做法：每次都把 reason 的过程练短一些，渐进式的学法
* RL 以结果为导向的推理方法，结果导向，用 RL 这些方法就会产生超长的推理过程，直觉的解决方法就是把长度限制加到 RL 的 reward 中，但实际上多数文献不使用该方法，定阈值不一定适用于所有问题，难得问题就是需要较长的推理长度；
	* 采用相对标准，根据难度定，先把问题丢给 LLM 多次做推论，回答正确的收集起来统计平均长度作为标准。就算答对，比平均更长，也算是不好的。
	* 教模型控制推理长度，直接 prompt 中设定推理长度为 $n^\star$，reward 为正确率 - 目标和实际推理长度差异，Paper：L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning，在数学问题上训练，在数学问题上测试，可以控制在 2~6% 左右，但 OOD（域外）就控制的较差，20~40%
	* 控制推理长度，会不会有损模型本质的推理能力？研究发现模型的推理能力并没有受到太大的影响

## 9、LLM 能力评测

能解数学问题就代表有推理能力吗？有多少答案可能是记忆出来的？

把原来的 GSM8K 题目里面的一些词汇、一些数字改掉，在不影响问题难度的前提下，再去问市面上多数模型这个问题，结果发现多数模型，它的正确率都是有减低的；把一些句子的顺序换掉（不影响题目意思），这些模型解题的正确率居然都下降；

这些测试的结果，往往不一定那么可靠，因为你永远不知道这些模型，是不是早就看到了类似的问题了。

现在模型推理能力测试，常被讨论到的 Benchmark Compass，叫做 ARC-AGI（作者是 Keras 的作者），里面都是这种有图形的智力测验题目 。

有个平台叫做 Chatbot Arena，每次登进去的时候，就随机给你两个模型，模型 $A$ 和模型 $B$，接下来就问这两个模型一样的问题，然后你要决定哪一个模型是比较好的；但传说 Chatbot Arena 也是有办法被 hack 的，因为人类还是有他喜欢的倾向，比如喜欢 emoji，粗体字，等，因为很多人根本就不会仔细去看它的内容，而通常是看它输出的风格，所以输出的风格反而对结果影响比较大

Chatbot Arena 的评比机制，Elo Score，这也是很多竞赛会使用的评比方式，假设有 $K$ 个模型，$M_1$ 到 $M_K$，每个模型有一个分数 $\beta_1$ 到 $\beta_K$，任意两个模型 $M_i$ 和 $M_j$ 之间对战，评比公式：$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j}{400}\right)}=E_{i,j}$$除掉一个 Normalization 的分数，就是为了让分布比较好看一点，通常都会设成 400，前面负号再去 Exponential，这其实就是 Sigmoid Function

* 如果 $i$ 的战力比 $j$ 的战力大很多，数值就会趋近于 1
* 如果 $i$ 比 $j$ 小很多，算出来的胜率就会趋近于 0

实际上在比赛里面，真正能知道的并不是这些战力，而是根据比赛的结果可以统计某个模型对战到某个模型的胜率是多少；所以 Elo Score 的求法是，先得到大量的比赛，知道模型跟模型间对战的胜率，再根据胜率反推出这些 $\beta$ 的值。

但在 Champion Arena 上，他们觉得会有太多跟模型本身实力无关的因素，会干扰到评比的结果，所以在计算正确率的时候，只知道 $\beta_i$ 跟 $\beta_j$ 是不够的，要加一项 $\beta_0$，即$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j+\beta_o}{400}\right)}=E_{i,j}$$
* $\beta_0$ 是模型实力以外的因素，比如某些棋类比赛里面，就应该把先手优势考虑进去

举例来说，模型可能回答越长，人类就会越喜欢，所以 $$\beta_0 = \gamma_1(\text{答案长度差})+\gamma_2(\text{emoji 数量差}) + \cdots$$
如果计算出来 $\gamma$ 是正的，就代表说这一项是会有影响的，$\gamma$ 趋近于 0，说明这一项没什么影响，负值就是负相关，如长度越短越好。

Champion Arena 报告，有没有考虑这些风格相关的因素，其实会影响模型的排名，其中上升的最多的就是 Claude，它是大家认知觉得蛮厉害，但在 Chatbot Arena 上评分起不来的模型，一个原因可能就是 Claude 模型讲话太无聊了。

到底什么样的指标才是好的评量指标呢？*也许结论就是没有好的评量指标*。这个叫做 Goodheart's Law 的意思是，一旦一项指标被当做目标，它就不再是一个好的指标。


## 10、模型编辑

Model Editing 希望做到的事情是帮模型 *植入* 一件知识，

Model Editing 与 Post-training 有什么不同？Post-training 通常是想要让模型学会新的技能，这个技能不是一项知识，而是需要模型做比较大的改变才有办法学会的事情，比如说新的语言、或者是使用工具、做推理等等；

但是直接用 Post-training finetune 模型的方法做 Model Editing 有很大的挑战，因为做 Model Editing 的时候，通常你的训练资料就只有一笔；

怎么评量 Model Editing 是不是成功的，要考虑三个不同的面向：

* Reliability：想要修改的目标必须要达成
* Generalization：输入有一些改变，泛化能力
* Locality：其他无关的输入，不应该被改

Model Editing 常见方法：

* 不动参数：放在 prompt 中（关闭 rag，联网功能），但直接放入模型会不信；Paper：In-Context Knowledge Editing(IKE)，提供一些范例，是比较容易成功的
* 改变参数：直接梯度下降，往往一改完，模型就坏掉了
	* 人类决定如何编辑：由人类对于语言模型的理解，找出应该要被编辑的位置，并决定要被编辑的方法。Paper：Rank-One Model Editing（ROME）
		1. 是找出网路中，跟编辑的知识最相关的部分
		2. 修改那个部分的参数，让模型变成想要的样子
	* 另一个 LLM 决定如何编辑：输入是 $\theta$（待编辑的模型参数）、待编辑的问题、答案，输出是 $e$（大小同 $\theta$ 的向量），然后 $\theta + e$ 即可，这个网络称为 Hypernetwork



## 11、模型融合

> [!NOTE]
> 课程中列举了许多模型融合成功的案例，且这些案例在论文中展现出高度的通用性。但需要提醒同学们的是，模型融合并非总能成功，其效果往往受任务特性的影响。某些任务更容易融合成功，例如实践中提到的语言能力与对齐能力就较易融合（llama 释放出的指令模型没有中文能力，然后就在 llama-base 上训练中文模型，然后融合）；而尝试融合两个不同领域知识的模型则相对困难。探索哪些任务更适合进行融合的通用规律，是未来的研究方向。


从基础模型（参数 $\theta$）开始，用一些资料 post-training 得到 A 模型 ($\theta_A$)，另一些资料 post-training 得到 B 模型 ($\theta_B$)，现在想让 A 模型也拥有 B 模型的能力，一种做法是将 B 资料拿过来继续 post-training A 模型，为了防止遗忘，该步骤中还要混合原来 A 资料一起训，很麻烦，不仅 B 资料不易获取，且继续训练也耗资源。

* 不用训练资料
* 不用做任何模型训练

这里的参数可以想象为就是一个向量，直接相减，（$\theta_B - \theta$），这意味着是相对于基础模型，额外训练出来的能力，这个差向量叫做 Task Vector，然后将其加到 $\theta_A$ 上即可。

这怎么可能会 work 呢？就好像把一个人的手砍下来，再直接插到另外一个人的身上，就是这么神奇！

它的前提是 $\theta_A$ 跟 $\theta_B$ 是从同一个基础模型训练出来的，有时候在 task vector 前乘上 weight 可能会得到更好的结果，类似线性组合

**相加的实例**：指令跟随能力 + 语种能力；  奖励模型 + 代码能力；   奖励模型（仅文字）+ 看图模型 = 奖励模型（图文）

**相减的实例：** 失去某种能力，比如看到不该看的能力（版权之类的），术语 machine unlearning

**类比的方式**：比如已知 Task A: Task B = Task C: Task D，那么当有了 ABC 模型，可以在没有任何 D 任务训练资料情况下，凭空创造出 D 能力，实际上是 $\tau_B - \tau_A + \tau_C$，比如语音识别系统，一个专业领域充满大量专业术语情况下现有模型能力不强。我们目前有的只是专业领域的文本，可以将其合成语音信号（C模型），可能在其他领域容易找到真实的文本语音信号（真人说的，B）以及容易通过语音合成得到合成数据（A），那这样就容易类比得到 D（就像用真人在在专业领域的语音训练的）

需要注意的是，这并不一定总成功，举个例子，基础模型某神经元有 3 个输入，权重为 000，$\theta_A$ 中训练后，权重为 110，输入为 210，则输出为 3，而 $\theta_B$ 权重为 011，输入为 023，则输出为 5，两个模型合并，权重相加得 121，再输入 210 输出是 4，输入 023 输出是 7，就不一定会成功；

成功的例子，A 的权重 100，B 的权重 001，合并后就是 101，这个时候输入 210，输出就是 2，还是 A 原有的输出，输入 023，输出 3，也还是 B 原有的输出。

启发是如果两个任务改的参数非常的不一样，它们彼此之间没有互相干扰，那 model merging 有可能 可以成功。


## 12、语音语言模型

chatgpt voice mode、gemini live，比较惊艳的 Sesame

speech token 是什么，基本单位，