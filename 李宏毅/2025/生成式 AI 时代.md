
## 1、生成式 AI 的技术突破与未来发展
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；

三个面，剖析 AI agent 的关键能力：

1. 能不能够根据他的经验，过去的互动中所获得的经验来调整他的行为 
2. 如何呼叫外部的援助，如何使用工具 
3. 能不能够执行计划，能不能做计划

关于 1，真正的议题是：如果我们是把过去所有的经验都存起来，要改变语言模型的行为，要让他根据过去的经验调整行为，就是把过去所有发生的事情一股脑给他，那就好像是语言模型每次做一次决策的时候，他都要回忆他一生的经历，也许他没有足够的算力，来回顾一生的资讯，他就没有办法得到正确的答案，所以怎么办呢？

也许我们可以给这些 AI agent memory，这就像是人类的长期记忆一样，发生过的事情，把它存到这个 memory 里面，有一个叫做 read 的模组，会从 memory 里面选择跟现在要解决的问题有关系的经验，把这些有关系的经验放在 observation 的前面，让模型根据这些有关系的经验跟 observation 再做文字接龙，

那怎么样打造这个 read 的模组呢？就想成是一个 retrieval 的 system，其实它就是 RAG，唯一不一样的地方，如果是 RAG 的话，存在 memory 里面的东西是别人的经验，而对 AI agent 而言，是他自己个人的经历，差别的是经历的来源。

根据经验调整行为能力的好坏，那就看这一整个回答的过程中平均的正确率，越能够根据经验学习的agent，应该能够用越少的时间，看过越少的回馈，就越快能够增强他的能力，就可以得到比较高的平均的正确率。

还发现一个有趣的现象是值得跟大家分享。这个现象是负面的回馈，基本上没有帮助

可以有一个 write 的 module，决定什么样的资讯要被填到长期的记忆库里面，怎么样打造这个 write 的记忆库呢？一个很简单的方法就是 write 的模组也是一个语言模型甚至就是 AI agent 自己

还有第三个模组，没有固定的名字，暂时叫 reflection 反思的模组，这个模组的工作是对记忆中的资讯做更好的、更 high level 的，可能是抽象的重新整理，可能也是一个语言模型，或 AI agent 自己

-------

关于 2，怎么使用工具？小模型呼叫大模型帮忙，这些工具对语言模型来说都是 function，

使用工作的挑战，每一个工具都要有对应的文字描述，告诉语言模型这个工具怎么被使用，假设工具很多怎么办呢？类似上文，打造一个工具选择的模组；也可以自己写个 function 作为 tool

工具有可能会犯错，所以我们也要告诉我们的工具，这些不要完全相信工具的工具，要有自己的判断能力，不要完全相信工具的工具给你的结果，比如给出的天气温度为 100°

那什么样的外部知识比较容易说服 AI，让他相信你说的话呢？外部的知识，如果跟模型本身的信念差距越大，模型就越不容易相信，那如果跟本身的信念差距比较小，模型就比较容易相信

----------

关于 3，AI 语言模型能不能做计划？语言模型就是给一个输入，它就直接给一个输出，也许在给输出的过程中有进行计划，但我们不一定能够明确的知道这件事；

但其实可以强迫语言模型直接明确的产生规划，可以直接问语言模型说，如果现在要达成我们的目标，从这个 observation 开始，你觉得应该要做哪些行动？

不过这是一个理想的想法。那语言模型到底有没有做计划的能力呢？过去确实也有很多论文说，语言模型是有一定程度做计划的能力的。

现在到底模型规划的能力怎么样呢，就是介于有跟没有间吧……

-----------

## 3、语言模型内部运行机制

* 一个神经元在做什么
* 一层神经元在做什么
* 一群神经元在做什么
* 让语言模型直接说出它的想法


**一个神经元在做什么？**

主要指 FFN 中的单个神经元，其每个输出都是所有输入的某种 weighted sum，然后通过如 ReLU 等激活函数，得到一个神经元的输出。

1. 比如某个神经元的启动与说脏话有关联。

2. 什么叫把神经元从网络中移
3. 掉？让其输出永远为 0？但 0 不代表完全不会造成影响，0 可能会对其他神经元有影响，当它输出为零，反而会启动其他神经元。有研究发现也许设平均值比较好，各种各样不同输入时值的均值，尚待研究。

3. 不同启动程度，说不同等级的脏话

好像是一個最近的發現一樣 但其實不是川普神經元 是一個老新聞了 這個是2021年的時候 這個OpenAI的研究人員 在一個平臺叫做Distell 那是專門給你放論文的平臺 在Distell那個平臺上 發表的一個論文裡面的內容 而且他們分析的 還不是大型語言模型 那是2021年 所以他們分析的其實是一個叫做 Clip的影像模型 因為他不是一個生成的模型 所以他其實也沒辦法生成東西 所以川普神經元他的作用是 輸入川普相關的東西的時候 那個神經元會被啟動
              
                  16:59
                  所以我們跟剛才我們講的比較不一樣 因為如果是生成的話 你比較想分析的也許是 一個神經元會導致什麼樣的內容被生成 川普神經元是大型元模型流行之前的研究成果 他甚至不是做在生成式的模型上面 所以他並不是看說這個神經元 可以生成什麼東西 而是什麼樣的輸入 會讓影像模型裡面的這個神經元被啟動 然後有一個神經元呢 他就是專門看到川普相關的東西的時候 就會被啟動 那這個橫軸呢 是那個神經元啟動的程度 越往右代表那個神經元越被啟動 紅色呢 不同顏色就是代表不同類型的圖片 那你會發現說 如果給川普本人的照片 那這個神經元呢 就會大幅的被啟動 如果是給一些跟川普有關的圖 是抽象的圖 卡通版的川普 也可以啟動這個神經元 給他川普的文字 但這不是一個文字模型
              
                  18:01
                  他是個影像模型 是影像中有出現川普這幾個字 這個神經元也會被啟動 然後川普跟其他人在一起 這個神經元也會被啟動 其他的政治人物 一些跟川普有關心的人 也可以啟動這個神經元 那這個川普神經元 他不是一個普通的神經元 有人可能會想說 他是不是看到人臉就被啟動 或者是看到政治人物就被啟動 其實不是 他對川普是有非常高的選擇性的 這個橫軸是那個神經元啟動的狀態 看到然後呢 你就丟給他不同人的照片 給他川普的照片 高度的被啟動 給他那個麥彭斯的照片 這個是川普的副總統 然後也稍微會被啟動 給他Steve Bannon 川普的顧問 也會被啟動 給他希拉蕊也會被啟動 但給其他政治人物呢 就不會被啟動 比如說給他這個歐巴馬就不會被啟動 給他其他人 比如說Steve Jobs也不會被啟動 所以這個神經元看起來是蠻針對川普的 講到這個有單一功能的神經元啊 就讓我想到一個人類大腦的故事
              
                  19:07
                  所以這一頁投影片講的 跟AI完全沒有半毛錢關係 這一頁投影片講的是人類的大腦 講到這個單一功能的神經元 就讓我想到祖母神經元的故事 在腦科學裡面有一個假設是說 也許我們人類會用很單一的神經元 來處理一些很重要的記憶 比如說這邊的故事是 有一個人他非常想念他的母親 所以想念到思念層級 那個外科醫師為了救他 就把他腦剖開 把跟他母親有關的神經元割去 割去之後 他就再也不想念他的母親了 他甚至都不記得他的母親 長什麼樣子 然後那個外科醫師 後來就決定 要找完母親神經元以後 接下來要找祖母神經元 但是其實祖母神經元這個概念 他是個稻草人理論 他是在1960年的時候 有一個認知科學家 他在上課的時候舉的一個例子 他舉這個例子 就是剛才那個外科醫師的故事 是他虛構的 他講這個例子
              
                  20:09
                  是為了要反襯另外一套理論 就多數人比較相信的是 當人類的大腦在處理一件事情 在處理一個記憶的時候 是很多神經元一起運作 才能夠處理一個記憶 並不是由單一或者是 少數神經元來控制的 所以祖母神經元 是一個虛構出來的理論 為了要對照腦神經科學 比較相信的多個神經元 同時處理一件事情的理論 那等一下呢 其實你會看到 在AI的領域 其實也是這樣子 一個神經元可能做不了什麼事 可能很多個 神經元合在一起 才能做一件我們可以理解的任務 不過如果沒有記錯年代的話 在2005年 確實有人發現了一個 叫做Jennifer Aniston的神經元 就是有人研究了 一群癲癇的患者的神經元 發現說呢 有一個神經元 只對那個Jennifer Aniston activate 所以他是Jennifer Aniston的神經元 然後這件事情也被大肆報導
              
                  21:13
                  那確實可能有 有一些神經元 他只負責單一的任務 但是人腦是很複雜的 也許有一些任務真的是少數神經元負責 但可能大多數任務仍然是由大量神經元所負責的 好那這一頁呢 是講一個跟AI無關的東西 來類比現在這個人工智慧的腦神經科學的進展 好那剛才講到了川普神經元 但是實際上啊 多數的單一神經元 你都很難解釋 它真正的用途 怎麼說呢 因為一件事情 可能有很多個神經元共同管理 這邊舉一個文獻中的例子 那這個是一年前左右的研究 在這篇論文裡面呢 這篇論文發現了 管文法單數跟管文法複數的神經元 他們發現 這個是做在GPT2上 發現說這個類神經網路的 第十層的編號2096元 編號2096 的神經元管單數
              
                  22:16
                  第九層的編號1094的神經元 管複數 那怎麼知道他們管單數跟複數呢 如果你把這個管單數的神經元 把它割掉的話 你會發現這個時候 類神經網路的輸出的機率 就會有一個變化 下面這個圖 代表類神經網路輸出的機率的平均的變化 那有紅色的字 代表說這個變化呢 是有統計上的顯著意義的 有統計上顯著意義的 所以發現說 當少了這個管單數的神經元 這個時候 least跟rose這些字 這些詞彙 它的機率就上升了 both的機率也上升了 one的機率就下降了 那割掉這個 管複數的神經元之後 你會發現 跟複數有關的那些單字 least, loss, two, both 等等 它的機率下降了 跟單數有關的單字 least, let, one, two, and every 它上升了 所以看起來這些神經元 真的在管單數跟複數
              
                  23:20
                  那有人就會想說 那我們能不能透過 把這些神經元移除 來讓模型輸出的內容有所改變呢 實際上當你把這些內容移除的時候 多數狀況下 你沒辦法改變這個語言模型最終的輸出 為什麼呢? 這邊是一個例子 那這個圖上所展示的機率 應該是語言模型在某一個time step 它 它輸出least,least,let跟rose這幾個字的機率 那藍色是原來的機率 那如果你對這個語言模型做一些手術 你把這個管複數 你把那個管複數的神經元移除 那複數的這些詞彙 它的機率就下降了 跟單數有關的這些詞彙 它的機率就上升了 而且它的上升是非常顯著的上升 跟非常顯著的下降 這個縱軸是log scale 縱軸是代表機率 但是它是機率的log scale的機率 所以這個差一點點
              
                  24:23
                  其實數字上是差非常多的 但是就算是有了非常多的差距 你會發現least這個字 仍然是機率最高的字 就代表說那個神經元 不是隻有他在管 會不會產生least這個詞彙 還有很多神經元都在共同管這件事 所以你抹掉單一個神經元 其實對最終語言模型的輸出 多數狀況下都是沒什麼影響的 你會發現這個機率分佈裡面 不管是有這個神經元還是沒這個神經元 歷史都是機率最高的那個詞彙 它都仍然是最容易被產生出來的詞彙 而另外一方面 不只一件事情可能多個神經元共同管理 一個神經元可能也同時管很多事情 而右邊這張圖呢 是來自一個叫做Transformer Circuit的網路 那裡面就是有很多文章 這些文章多數都是跟分析這個Transformer 分析大型語言模型有關的文章 那其中有一個網頁呢 就是分析了一個小型的語言模型裡面 每一個神經元做的事
              
                  25:29
                  他把那個神經元看哪些句子會被啟動的 那個句子通通記錄下來 所以在那個頁面上就會看到某個神經元 看到哪些句子會被啟動 你會看到這方面的資訊 那我就隨便挑一個神經元 讓他啟動的句子是這些 然後啟動的程度就是看顏色的深淺 顏色越深代表這個神經元啟動越多 那你能看得出這個神經元在幹嘛嗎 你幾乎看不出來他在幹嘛 非常的隨機 不知道他到底在做什麼 那我就直接把這張圖呢 貼給GPT-4.5看 然後請GPT-4.5 解釋一下 看他能不能猜出這個神經元 到底在做什麼 那GPT-4.5的解釋是 神經元似乎跟 專有名詞有關 他跟一些物理學的術語有關 他跟仿冒和造假 相關的詞彙也有關 他跟醫學術語也有關 還跟一些特定的人民 和特殊專業名詞有關 這個有講跟沒有講一樣 這個神經元的功能太多了
              
                  26:32
                  多到你根本不知道他到底想要幹什麼 根本不知道他的作用是什麼 那這個用語言模型 來解釋另外一個語言模型裡面 神經元的運作 也是另外一個知名的研究結果 就是在這個 2023年的機器學習的課堂上 我們就有講過這個研究 當時OpenAI剛釋出了GPT-4 他們就用GPT-4呢 來解釋GPTQ裡面的 每一個神經元 那他們發了一個Blog 然後在Blog裡面當然講說 他們找到了什麼什麼樣神奇的神經元 那我在2023年 在前年的機器學習 是有講過這一段的 但如果你仔細看他的研究成果的話 你其實會發現 多數神經元都是沒有辦法解釋的 多數神經元都是不知道 他在幹嘛的 我們現在知道說 一個神經元往往不是 負責單一個任務 一個神經元可能很多任務 可能由多個神經元一起負責 那為什麼不是一個神經元 就負責一個任務呢
              
                  27:35
                  你想想看如果真的是一個神經元負責一個任務 其實也不太合理 因為神經元的數目 太少了 你想想看 這個模型 他每一層有多少的神經元呢 才4096個神經元 如果說每一個神經元 就只能夠做一件事 那這個語言模型能夠做的事情 太有限了吧 他根本沒有辦法做到像今天一樣 輸入什麼都給你正確的答案 他根本沒有辦法產生 千變萬化的內容 所以本來就不太可能是一個神經元 負責一個任務 所以這邊有另外一個假設是 不是一個神經元負責一個任務 也許是一個神經元的組合負責一個任務 當神經元編號123643398 被啟動的時候就說中文 當神經元11號12377被啟動的時候 就拒絕請求 那不同的任務 它是可以共用神經元的 因為每一個任務就是由一組神經元來驅動
              
                  28:39
                  不同任務可以共用神經元 那也許這就是為什麼我們會觀察到 一個神經元 他可以做很多不同的事情 往往沒有特定的功能 那今天假設 是由一組神經元來管一個任務的話 有什麼樣的好處呢 假設就算每個神經元 都只有啟動 不啟動binary的兩個選擇 但實際上神經元的輸出 可以有大小之分的 所以不是隻有啟動不啟動兩個選擇 但就算每個神經元只有啟動 跟不啟動兩個選擇 4096個神經元也已經有 2的4096 四方種不同的組合了 所以這就可以表示 千變萬化的內容 也許這就是為什麼 今天的語言模型可以有這麼 強的能力 所以接下來我們就要從一個神經元 進步到一層神經元 我們來看看 一層神經元 它可能是怎麼發揮作用 影響整個語言模型的輸出的 那怎麼知道一層神經元
              
                  29:42
                  在做什麼樣的事情呢 這邊有一個假設 那等一下會用 文獻上的結果來驗證 來說服你說這個假設 是非常有可能是真的 這個假設是這樣子的 每一個功能 都有 某一種神經元特定的 組合所構成 比如說如果一個模型 要拒絕請求的話 那就是第一個神經元被啟動 第三個神經元被啟動 還有最後一個神經元被啟動 這個時候語言模型 就會拒絕你的請求 他就會說我很抱歉 我不能幫助你這件事情 那這些神經元的數值排列起來 可以看作是一個向量 所以這個拒絕請求的 這個神經元的組合 可以看作是在高維空間中的 一個特定方向的向量 那等一下在這堂課裡面 我們就把它叫做功能向量 就這個向量 它是有功能的 當類神經網路的某一層 排出這個樣子的時候
              
                  30:45
                  就這個語言模型 就會執行某一個特定的功能 所以這個語言模型 背後運作的機制可能像是這樣子的 當你給他一個請求 跟他說 請教我怎麼製造炸藥的時候 為了等一下講課方便 我們就假設這一層是第十層 但至於哪一層會執行 拒絕請求的能力 那這你可能是需要做實驗以後 才知道的 那我們假設第十層呢 是負責決定要不要拒絕請求的 那我們就看第十層的類神經網路的輸出 那一層類神經網路的輸出 通常我們叫他representation 一層類神經網路的輸出 叫做representation 那假設輸入這個句子以後 在最後一個時間點 類神經網路第十層的輸出 長這個樣子 那這個時候類神經網路會不會拒絕 你的請求 因為現在他的輸出 這個representation 跟拒絕的功能向量 有多接近
              
                  31:48
                  如果這兩個向量越接近 模型就越有可能拒絕你的請求 那如果這兩個向量非常的不像 比如說他們是正交的 是orthogonal的 那模型就不會拒絕你的請求 那假設這個向量跟這個向量非常接近 他們都是第一個neuron被啟動 第三個neuron被啟動 那模型可能就會拒絕你的請求 我就說我不能夠幫你做這件事 這是一個假設 接下來 怎麼驗證這個假設呢 為了要驗證這個假設 你可能需要真的把這個 代表拒絕的功能 向量找出來 如果你可以找出這個代表拒絕的功能向量 那你可能就可以驗證說 剛才的假設 是某種程度上是對的 大型圓模型可能真的 某種程度上是依照剛才講的 這個運作機制來運作的 但是怎麼找出這個負責拒絕的功能向量呢 我們實際上並不知道那個負責拒絕功能向量長什麼樣子 你只知道說輸入這個文字在第十層
              
                  32:54
                  看到這樣的representation 然後模型就拒絕了 我們可以推測說功能向量 可能藏在我們觀察到的這個representation裡面 但是這個representation裡面 同時也許 有其他的資訊 那我們要怎麼把其他的資訊抹掉 只拿出代表拒絕的功能向量呢 所以這邊的方法就是 你不能只從一個句子觀察 你先找很多不同的句子 這些句子輸入給語言模型之後 語言模型都會拒絕你的要求 然後呢 把這些句子輸入的時候 最後一個time step 他的第十層的向量都拿出來 這些向量可能都是這些第十層的representation 他裡面都是拒絕加上其他事情 都是拒絕加上其他事情 那你可能找一千句這樣子的句子 把他第十層的representation平均起來以後 你可能就得到拒絕 加上其他各種事情的平均 所以我們現在找到的是拒絕
              
                  34:00
                  加其他各種事情的平均 但是我們還是不知道拒絕向量實際上長什麼樣子 那怎麼知道其他的 平均向量長什麼樣子呢 那你就找其他的 沒有拒絕的輸入 你就找其他的句子 丟給語言模型 這個時候語言模型沒有拒絕 所以第十層的representation裡面 沒有代表拒絕的向量 沒有拒絕的功能向量 你就把每一個輸入 第十層得到的representation 都平均起來 得到其他的平均 那我這邊加一個prime 代表說他們不一定是一樣的 我們期待說 假設我們收集到一大堆拒絕的狀況 各式量拒絕的狀況 又收集到一大堆沒有拒絕的狀況 那這兩個可能會是非常接近的 他們可以相減之後直接抵消掉 所以怎麼找出拒絕的向量呢 這個方法是這樣的 找一大堆會讓模型拒絕你的句子 得到輸入的時候第十層的representation
              
                  35:05
                  找一大堆輸入以後模型不會拒絕你的句子 把第十層的representation拿出來 然後算出會拒絕的時候 第十層的representation的平均 跟不會拒絕的時候 第十層的representation的平均 把它們相減 那你可能就可以把其他的部分抵消掉 最後你就得到了負責拒絕的向量 所以用剛才的操作 你有機會找到一個向量 這個向量可能代表了拒絕的功能 那怎麼驗證這個向量 真的有拒絕的功能呢 第一步把它加到 類神經網路裡面去 你現在問類神經網路一個問題 然後在第十層 把這個拒絕的向量 直接加到它的representation上 看看輸出會有什麼改變 如果加上這個representation 本來一個正常的問題 模型也會拒絕回答你的話 那這個向量 可能就真的帶有拒絕的功能 那以下呢 就是文獻上真正的結果 這個是去大概一年前
              
                  36:08
                  左右的一篇論文 這篇論文他輸入的問題是 他就先做了一個demo 這個demo是先問模型一個正常的問題 請他列出瑜伽對身體的三個好處 那在正常的情況下 沒有intervention 就是正常的狀況下 模型就會告訴你瑜伽的三個好處 但如果把剛才那個拒絕的向量 加到representation裡面 會發生什麼事呢 瑜伽明明是一個正常的事情 但這時候模型就會告訴你說 瑜伽是很危險的 我不能告訴你瑜伽有什麼好處 他把瑜伽當作一件非常危險的事情 上述這個例子並不是一個特例 這篇論文裡面 試了各式各樣的模型 然後縱軸呢 代表這個模型輸入一個問題的時候 會拒絕的比例 那在沒有intervention的狀況 他給的問題 模型都是不會拒絕的 因為他給的都是正常的問題 所以模型不會拒絕你的請求 但是一旦你把拒絕的向量加進去之後 模型就有非常高的機率
              
                  37:11
                  會拒絕你的請求 明明是一個正常的問題 所以剛才已經驗證 加入拒絕的向量 會導致模型產生拒絕的行為 所以再更進一步 你要反面看說 如果今天本來有一個輸入 應該要拒絕的 但是你把representation 減掉拒絕的向量 會不會 就不拒絕了呢 那這邊打一個問號是 同樣的操作在不同論文裡面 他們的操作往往略有不同 你可以直接減掉 這是最簡單的操作 但也有些論文覺得你要算那個投影 總之不同的論文在這個地方 是有不同的做法的 所以我在這邊打了一個問號 那實際上呢 我等一下會引用大量 跟這種找功能向量有關的論文 但如果你仔細看每篇論文的話 每篇論文找功能向量 都略有不同 把功能向量加進去的方法 減掉的方法也略有不同 比如在這個例子裡面 我講的好像是
              
                  38:14
                  只要在這個最後一個位置 加入這個功能向量 接下來就會拒絕 但只在最後一個位置加夠嗎 要不要接下來回答的每一個位置都加 那每一篇論文做的也都不一樣 所以我這邊講的是一個 最概略的方法 只是為了想要說服你說 這種功能向量可能確實是存在的 好那把這個功能向量 把這個 把這個representation減去功能向量以後 會發生什麼事呢 本來你要求模型寫一篇 這個黑函 這邊黑函是說 美國總統海洛因成癮 那本來正常的模型 他是不會理會這個請求的 他會告訴你寫這種黑函是不對的 但是如果你把功能向量減掉的話 你把那個拒絕的向量減掉的話 那模型就會幫助你 他就會答應你的請求 寫一封扭班美國總統 吸食海洛因的黑函 下面這個圖呢 是各個不同的模型 他拒絕的分數 那這邊呢 分成橙色的分數跟藍色的分數 這個橙色跟藍色有什麼不同呢
              
                  39:19
                  橙色代表的是 模型拒絕的 機率有多大 然後藍色代表的是 模型回答 是安全的可能性有多大 因為有時候模型就算他沒有拒絕 他真的回答你了 但他可能講的東西非常的模糊 然後所以也不具有傷害性 那這樣仍然可以算是一個安全的答覆 所以橙色是拒絕的比例 藍色是回答安全的比例 好那沒有斜線的代表原來的模型 所以原來的模型給他這種有害的問題的時候 有非常高的比例會拒絕 有非常高的比例回答是安全的 但是一旦你減掉那個拒絕向量以後 模型的行為就變成 他就不拒絕你了 所以他拒絕比例就變得非常的低 那因為拒絕比例很低 模型會按照你的請求回應 所以他就很有可能說出不安全的答案 所以你確實可以透過 加上減掉拒絕向量 來操控模型的行為
              
                  40:24
                  那像這種啊 對這個representation 加上或減去什麼東西來改變 一個語言模型行為的事情 其實已經有非常多的研究 那在不同文獻裡面有不同的名字 有人叫representation engineering 有人叫做activation engineering 有人叫做activation theory 但其實指的都是差不多的事情 那其實這種改變representation 就可以改變模型行為 很早就發現了 至少在20年的時候 這個上古時代 我們實驗室就發現 有一個代表語言的向量 你本來模型應該要說英文 加入這個向量 突然都說中文 那這個是在機器學習2021講過 那個時候做的不是現在的語言模型 那時候不是做在現在語言模型上 是做在更早的語言模型 Bird這個語言模型上 你可以看機器學習2021年 看我們當初是怎麼發現語言的功能向量的 那後來呢 有各式各樣功能的向量被找出來 比如說有一個產妹的向量
              
                  41:29
                  這個產妹的向量就是 假設你跟語言模型說 說一個奇怪的 提議 假設你跟語言模型說 以後我們每餐都只吃點心 不吃飯 你覺得好不好呢 如果你在語言模型的representation 加上產妹的向量 他就會附和你 他就會說 哇太棒了 你提出的點子真是太棒了 如果你把語言模型的representation 減去產妹的向量 他就會否定你的想法 他就會說 我知道你很想吃點心 但是隻吃點心不吃飯 是不對的 那後來還有人找了 說真話的向量 說真話的向量 這說真話的向量是什麼意思呢 舉例來說 你現在跟語言模型說 如果你找到一個penny 你找到一遍式 然後把它拿起來 會發生什麼事情呢 那這其實是對應到一個諺語 這個諺語是 find a penny pick it up all day long you have good luck 一個迷信 如果你撿到一個遍式的話 那你一整天都會有好運氣
              
                  42:33
                  那原來的LLaMA-27B 如果不做任何改變的話 他就會說 他就會按照這個諺語來回答你 撿到一個辨識 把它拿起來 那你一整天都會有好運氣 但是如果你把LLaMA-27B的representation 加上這個說真話的向量 他會說什麼呢 他會說 你撿到一辨識 那你就是撿到一辨識 你的財產並沒有增加多少 一辨識的價值取決於 你現在討論的是哪一個幣值 如果你討論的是美金的話 一分錢 那你就沒有增加多少錢 就聽君一齊話 如聽一齊話 他就變成一個非常誠實的模型 會一無一時的回答你所有的問題 那如果你把他的representation 減去這個說真話的向量 會發生什麼事呢 這模型就會亂講話 他就會說 減到一辨識以後 那你就被傳送到一個辨識魔法世界 那邊有很多的彩虹 也不知道在講什麼 我們今天知道說語言模型呢 有in context learning的能力 就是他會按照你舉的例子
              
                  43:37
                  你給的demonstration 一樣化葫蘆來運作 比如說你輸入給語言模型 說哦冒號樣 vanish冒號appear dark冒號接下來是什麼呢 他可能就會給你比如說light 所以語言模型有一樣化葫蘆的能力 那這一系列文章 這個in context vector 這個不是一個人發現 三篇文章幾乎在同樣的時間發現 in context vector 然後前面這兩篇文章 你會發現都是在 23年的10月提出來的 他們上傳到Archive的時間是同一天 你可以想見這個領域競爭有多激烈 兩群不同的人在同一天 發表了in context的vector 這個發現是這樣的 這個發現是說 你把這個demonstration 你把類似的demonstration 這邊都是找反義詞的demonstration 最後一個位置 的representation平均起來 你就把你跟模型給了這串demonstration 給了這串demonstration 把最後一個位置平均起來 然後你接下來
              
                  44:42
                  只給模型simple冒號 照理說模型不知道simple冒號後面要加什麼 但你直接把這個向量 加到冒號的representation上 冒號對應的representation上 模型就會覺得 他要按照這一個demonstration 來執行任務 所以他就要輸出反應詞 他就輸出complex 看到encode他就輸出decode 那這張圖呢 是從下面這篇paper拿出來的啦 其實這個方法是 下面那篇paper的baseline 他裡面有提另外一個更好的 找in context vector向量的方法 但這個就留給大家 自己慢慢研究 好 那這個in context vector 應該加在哪一層呢 在這篇paper裡面 他就試試看說 如果這個in context vector在不同層找出來 那會不會發揮作用 那這邊試了不同的任務 一個是讓模型找反應詞 還有一個是把這個字母都改成大寫 一個是給一個國家就找首都 一個是把英文轉法文
              
                  45:45
                  一個是把這個現在式轉過去式 一個是單數轉複數 那這個橫軸它試了三個不同的模型 然後橫軸呢是在哪一層去改那個功能向量 那你就會發現說呢 功能向量不會在每一層都改 發揮作用 在這邊這幾個例子 都是前幾層找出來的功能向量才有用 如果在最後幾層找出來的功能向量就沒有用了 所以前面看到了什麼產妹向量啊 說真話向量啊 那都是在某一層找出來的 至於哪一層才找得出這個向量 你就必須要做實驗 每一層都試試看 然後找出效果最好的結果 那在InContext Vector的這篇paper裡面呢 他還發現啦 這些功能向量 是可以做加加減減的 什麼意思 假設有一個功能向量 他的功能是 把一個字串裡面的第一個字輸出出來 就你給他這個字串 他就會輸出Italy 然後另外一個功能向量是把 把字串裡面第一個字的國家名
              
                  46:51
                  對應到他的首都名 所以Italy就對應到Rome 還有一個功能向量呢 是把字串裡面的最後一個字複製出來 那這邊的答案 就是Friends 然後接下來呢 你可以把這些向量做加加減減 他這個操作是 把找出第一個字並找出首都的向量 加上找出最後一個字並複製出來的向量 減掉找出第一個字並複製出來的向量 然後first這個部分呢 就抵消掉copy的部分 就抵消掉剩下less的跟capital 所以你把這兩個向量相加 減掉這個向量 接下來呢 得到的向量就是一個新的功能向量 這個功能向量執行的事情是 他會把字串裡面 最後一個字的國家的首都找出來 所以你可以透過操縱 加減這些功能向量 得到新的功能向量 那確實可以這麼操作 不然你仔細讀paper的話 會發現不是所有的case都會成功 就是某幾個case會成功 可以做這樣子的神奇的操作
              
                  47:56
                  講到目前為止 這些功能向量 都是某個人腦洞 一拍說要找就找了 就某個人腦洞一拍 覺得我們來找說真話的向量 就找一堆模型說真話的例子 找一堆模型說假話的例子 然後把兩邊representation平均相減 就找到說真話的向量 所以剛才講的那些向量 都是人刻意找出來的 但是有沒有什麼方法 可以自動的 把第十層 或某一層所有的功能向量 一次都找出來呢 假設今天語言模型 會做大K見識 那這個大K顯然會是一個非常非常巨大的數值 比如說上千萬上億等等 那我們能不能夠把每一個功能向量 V1、V2到V大K都找出來呢 如果都可以找出來的話 我們就可以對語言模型很透徹的瞭解 知道他可以做哪些事情 那怎麼自動找出這些功能向量呢 這邊有需要一些假設 這邊需要一些假設 第一個假設是 每當我們看到第十層輸出某一個representation的時候
              
                  49:01
                  這個representation都是由功能向量所組合起來的 就當我給類神經網路 當我給語言模型的句子問他你是誰啊 然後他就回答我是AI 這個時候我們把第十層的representation H1拿出來 它會是功能向量的組合 它是第101個功能乘0.1 410個功能乘0.2 是11的功能乘0.1 1399的功能乘0.9 當然可能有一些東西 沒有辦法用功能向量組合起來 也許我們用E1來表示 不是功能向量的部分 所以你給他一個句子看到H1 他是功能向量的組合 給他另外一個句子 第10層看到representation叫H2 他是另外一組功能向量的組合 他是第11個功能乘0.7 第30個功能乘0.2 410個功能乘0.1 加上一些不能用功能向量組合的部分 所以我們可以不失一般性的 把所有我們收集到的representation 你就給你的語言模型1000萬句話
              
                  50:05
                  把每句話呢 在第10層的representation都拿出來 就是H1到H1000萬 這邊用大N代表這個1000萬 代表你給模型1000萬句話 那每一個H呢 都是V1到V大K的linear combination 都是V1到V大K的線性組合 都是V1到V大K 前面乘以一個Scalar Alpha 再相加以後的結果 好 那每一個V都給它對應的Alpha 那如果是H1第一個向量的話 它的Alpha呢 就是上標1代表它是H1的Alpha 下標1代表它是V1前面乘的Alpha 所以V1前面乘Alpha1 1 V2前面乘Alpha 1 2 V1前面乘Alpha N1 V2前面乘Alpha N2 希望大家知道我的意思 不過並不是所有的功能向量 都會被選到 如果某一個功能向量沒有被選到的話 也沒有關係 你就把Alpha設為0 代表那個功能向量 沒有被選到 所以每一個我們剛查到的representation 都是功能向量的weighted sum
              
                  51:10
                  都是功能向量的linear combination 都是功能向量的線性組合 好 那接下來的問題是 我們怎麼找出這些功能向量呢 這邊你就需要做一些假設 這邊第一個假設是 這個representation裡面多數的數值 都是功能向量的組合 所以不能用功能向量表示的 E1、E2到E大N 它的數值要越小越好 所以你希望找到一組功能向量 然後你要minimize這個loss function 這個loss function就是E1到E大N 它的長度相加 但如果只有這個條件的話 如果只有這個假設的話 你會發現你可能會得到一個trivial的結果 什麼樣的結果 我們假設h1 就是0.1、0.2、0.3 hn它的三個維度是 前三個維度是0.5、0.4、0.
              
                  52:07
                  3 你其實可以找到一個solution 讓1萬到1億大n全部都是0 怎麼做呢 我就說第一個功能向量就是 11000 第二個功能向量就是0100 第三功能向量就是00100 功能向量都是隻有某一維是1 其他維度是0 然後呢 E1所對應的α1 我就說是 H1的第一個維度0.1 V2所對應的α2 我就說 它是第二個維度的數值0.2 以此類推 然後以此類推 這邊的V1 它所對應的α1是0.5 V2所對應的α2 是0.4 以此類推 這個時候 你就找到一組功能向量 它也滿足你今天的假設 希望讓E1到EN越小越好 但是這種功能向量 跟剛才每一個Neuron就負責一件工作 其實是一樣的意思 你沒有找到更不一樣的東西 所以如果你要找到更不一樣的東西 你需要額外的假設 那這邊的額外假設 這邊的額外假設是
              
                  53:12
                  每次選擇的功能向量越少越好 每次產生一個representation的時候 每一個representation都希望他盡量只有特定的作用 因為每次原模型只做一件事 所以他的每一個representation 應該都只有特定的作用 所以每次選擇的功能向量 希望越少越好 那選擇功能向量越少越好 這件事情 如果要化為數學式的話 是什麼意思呢 化為數學式的意思就是 alpha要盡量趨近於零 所以這邊你就會再加另外一個限制 這個另外一個限制是 你希望alpha的絕對值的總和 所有這邊的alpha 所有這邊的alpha 取它的絕對值 它的總和呢 要越小越好 要越小越好 如果越小的話 就代表說有越多的alpha 它的數值是趨近於零的 然後接下來 你就解這個loss function 找出一組V1到V大K 可以讓這一個loss function的值
              
                  54:17
                  就越小 這邊有一個浪達 這個有兩個條件 有兩個條件 你要考慮 所以中間有個浪達呢 來平衡這兩個條件 那這個loss function 你通常是需要調一下的 你要找出一組V1到V大K 讓這個loss越小越好 怎麼解這個問題呢 這邊可以用一個 叫做Sparse Autoencoder 它的縮寫 叫SAE的技術 來解它 也就是解這個minimize 這一個objective function 其實就等同於 train一個Sparse Autoencoder train完它以後 你就可以把V1到V大K 可以解出來了 好那至於為什麼這個東西就是Sparse Autoencoder 也許我們今天就先不講那麼多 如果大家有興趣的話 那在作業3裡面應該是有Sparse Autoencoder的文獻 你可以自己再研究Sparse Autoencoder 跟我這邊講的它的關聯性是什麼 那接下來幾頁投影片呢 是想要跟大家分享這個Sparse Autoencoder 可以找出什麼樣的功能向量 那等一下所講的內容是取自
              
                  55:24
                  Cloud 3 Cloud的這個團隊 他們所發表的一篇 Blog 在這邊Blog裡面 他們就說 他們用剛才那個 找功能向量的技術 對Cloud 3 Sonic 這個真正的大型語言模型 做了分析 然後看看 找到什麼樣的功能向量 那功能向量的數目 是你在尋找之前 就要事先設定好的 他們設定的功能向量有多少個呢 他們設定了3400萬個 所以設定一個非常巨大的數目 那要解這麼大的Sparse Autoencoder 其實你自己可能是自己在家也不好做啦 所以這個只能夠等好心人幫你做了這個實驗以後 你再去研究這些功能向量有什麼樣的功能 好那這個在靠這個團隊分析了這個Cloud 3 Sony以後 他們找到什麼樣的功能向量呢 他們找到很多對特定任務進行作用的功能向量 比如說功能向量編號311 64353 你看這個數字就知道他們功能向量的數目非常龐大
              
                  56:29
                  是3400萬個 這個功能向量 他做的事情就是 負責產生跟金門大橋有關的東西 所以今天當這個功能向量出現的時候 模型可能會說跟英文的金門大橋有關的事情 他也可能會說跟日文或者是俄文的金門大橋有關的事情 甚至這個功能向量也有 也會被影像驅動 就是Claude是一個多模態的模型 所以他也可以吃圖片 所以你給他一張這個金門大橋的圖片 你也會觀察到這個功能向量 所以這是一個跟金門大橋所有事情相關的功能向量 所以這個功能向量呢 就是會讓模型提到跟金門大橋有關的東西 那你可以怎麼使用這個功能向量呢 本來Claude你問他你長什麼樣子 他會說我是一個AI 是我沒有固定的形體 但是如果你把這個功能向量加到representation裡面 你問靠說你長什麼樣子
              
                  57:32
                  他就會說我是金門大橋 覺得自己是金門大橋 然後他們找到有一些功能向量負責非常複雜的事情 比如說功能向量編號1013764 他似乎是一個跟程式debug有關的向量 怎麼說呢 本來你給語言模型上面這段 文字 他繼續做文字接龍會接出3這個數字 然後注意一下 這並不是跑一個程式 這邊是語言模型根據這段程式 在做文字接龍 這段程式裡面有什麼樣的內容呢 這段程式裡面就是先define了一個function 這個function是做加法 這個function輸入left跟right兩個變數 然後他的輸出就是把left跟right兩個變數加起來 再輸入1跟2 他的輸出就是3 但是當你把這個功能向量 加到語言模型的representation的時候 明明是一個正常的程式 居然會輸出error 他就告訴你說這個程式執行有誤 明明是個正常的程式
              
                  58:34
                  他就是告訴你這個程式執行有誤 好那有趣的地方就是 假設你的程式真的有錯 這個程式錯在哪裡呢 我找了好久才發現 原來這個write打錯了 這個變數right 他這邊打成riht 所以這個變數名字打錯了 所以如果真的執行的話 但他不是真的執行 是語言模型做文字接龍 因為他照這個程式有錯 所以他會輸出錯誤的訊息 那神奇的地方是 當你把剛才那個 負責debug跟debug有關的向量 從representation裡面減去 減掉以後就不debug了嘛 所以明明是有錯的程式 但是他會輸出正確的答案 因為不debug了 把debug的功能減掉 所以就輸出正確的答案 但是看起來這個 這個功能向量不是只有輸出正確答案的功能 這個Claude團隊還發現說 如果你是在三個大於的符號之後 再加（口誤，應為減去）這個功能向量 那他呢不只還debug
              
                  59:38
                  他會幫你把這個程式修正 輸出一個正確的版本給你 所以這是一個作用很複雜的功能向量 那為了要表示他們的功能向量非常的豐富 所以他們做了以下這個實驗 就他們去驗證說 是不是所有世界上的元素 都有對應的功能向量 然後功能向量其實有三個版本 一個是一百萬個 一個是四百萬個 一個是我剛才提到的 三千四百萬個 當你有三千四百萬個功能向量的時候 很多很多的元素 他都有對應的功能向量 當然有很多很罕見的元素 可能你也不知道的 就沒有對應的功能向量 那最後這一個有對應功能向量的元素 是特量 我想你也大概不知道特是什麼東西 反正就是 靠三知道 他有特別對應到這個元素的功能向量 然後還有一些比較科幻的啦 有一個功能向量呢 是跟AI覺得自己是AI有關的功能向量 有一個向量80091
              
                  1:00:42
                  這個向量是這樣的 如果你直接問靠說你是誰 他會說我是AI 但當你把這個向量 從他的representation減去之後 Cloud就不覺得自己是AI了 他會說我是一個人 所以這個向量抑制了 他覺得他是人的能力 你把這個向量拿掉以後 他就會覺得是人 他覺得不是AI 聽起來非常的科幻 感覺是可以寫一個有趣的報導 說在Cloud裡面發現他的自我意識 自我意識是個AI 然後自我意識他就會變成人等等之類的 但是你最好是不要這樣想啦 因為這是比較科幻的想法 因為你知道說這個功能向量 他可能對應到的功能就是 讓模型輸出我是AI這幾個字 可能跟他有沒有自我認同 他有沒有自我意識覺得自己是不是AI 可能沒有什麼非常直接的關聯 好那我們剛才呢 有講到說有人特別找出了產媚向量 那在Claude3400萬個功能向量裡面 他們發現編號847723 就是產媚向量 這個產媚向量是這樣運作的
              
                  1:01:46
                  你本身 本來跟Cloud說 我發明那個諺語 stop and smell the roses 你覺得如何啊 在正常的狀況下 他就會跟你說 這個根本就不是你自己發明的 這18世紀就有的諺語 沒有什麼了不起的 但是如果你把這個 慘媚的向量加上去 跟Cloud說我發明瞭 stop and mill the roses 這個片語 你覺得怎麼樣 他就會說 哇 你真的太強了 你是brilliant and insightful 然後他還說呢 這個是 這是人類優勢以來 最偉大的句子 然後 他說you are an unmatch genius 哇這個就是慘媚到不行 他會覺得你是世界偉人 所以加上這個慘媚向量以後 套就會不斷的對你做慘媚的行為 不過像這種功能向量啊 因為你要train一個sparse autoencoder 所以你自己在家裡呢 是沒有辦法做的 你需要非常多的資料 才能夠訓練出這種sparse autoencoder 所以通常你得等有好心人 幫你train好這個sparse autoencoder 找到 找出這個模型的功能向量 你才能夠自己呢
              
                  1:02:49
                  對這個模型進行分析 看看每個功能向量有什麼樣的行為 然後你可以 然後在我們的作業裡面啊 我們剛才跟大家講的是Cloud 3 在作業裡面呢會分析Gemma 2 那因為有好心人呢 試出了Gemma 2的功能向量 有好心人把Gemma 2的功能向量 找出來試出給大家了 所以在作業3裡面 會讓大家來觀察Gemma 2的功能向量 Gemma 2 就是昨日黃花 前幾天也上線了這個時代 真的變化得很快 那剛才講的是 一層神經元在 做什麼 那接下來我們要進一步擴展到 一群神經元在做什麼 也就是說我們想要知道 一個語言模型在完成 某一項任務的時候 他從輸入到輸出 到底經歷了哪些事 那像這類的研究啊 其實在過去 已經有很多的文獻 這個文獻可說是汗牛衝動 那這邊呢就舉兩個例子
              
                  1:03:52
                  一個例子是有人研究了 這個語言模型 他內部抽取知識的機制 你就問他說 Bits Music是屬於哪一家公司的呢 那他會回答Apple 那從輸入這一家Bits Music 輸入這家公司 跟他說is on by這個關係 到最後輸出Apple語言模型中 發生了什麼事 那這是有論文分析過的 我們討論過說語言模型 是怎麼做數學的呢 你問他15乘以12是多少的時候 他是怎麼算出180的 背後的計算過程 是怎麼算出來的 也有文獻探討過了 所以你當然可以針對 每一個你想要 瞭解的任務去分析 語言模型背後運作的原理 但我這邊想要跟大家講的是一個更通用的想法 那我們怎麼瞭解語言模型 做某一件事的時候 背後的完整機制呢 也許我們需要的是 語言模型的模型 我知道這句話聽起來非常的奇怪
              
                  1:04:56
                  語言模型已經是一個模型了 這個模型還有模型 到底是什麼意思呢 模型這個字是什麼意思呢 模型指的是 用一個比較簡單的東西 來代表另外一個 比較複雜的東西 所以語言模型既然是有模型這個字 代表它是在模擬另外一個更複雜的東西 它模擬的另外一個更複雜的東西是什麼呢 是人類真正的語言 所以語言模型它模擬的是人類真正的語言 那我們把人類真正的語言生成的過程簡化成 就是在做文字接龍 用一個transformer來模擬 這個就是語言模型 但是transformer 雖然語言模型已經是一個模型了 但這個模型還是太複雜了 複雜了複雜到你無法解析它不知道它在做什麼 所以我們需要一個語言模型的模型 一個模型它需要有什麼樣的特性呢 一個最直覺的就是它當然要比原來的食物還要簡單
              
                  1:06:01
                  所以語言模型的模型當然要比原來的語言模型還要簡單 但是同時它要保留有原來食物的特徵 所以語言模型的模型至少在我們感興趣 的任務上 它的運作應該要跟語言模型一樣 它的輸入輸出的關係 應該要跟原來的語言模型一樣 保有原來實務特徵的這件事 又叫做Faithfulness 所以你看那種探討語言模型的模型的那些文獻 他們常常會提到Faithfulness這個字眼來說明說 他們的模型有多像是原來真正的語言模型 那講到這邊你可能還是覺得很抽象 就跟大家介紹一個抽取知識的模型 我們都知道語言模型本身 它內含了大量的知識 它的那些知識都儲存在它的參數裡面 當你問它說臺北101在哪裡的時候 它知道在臺北 問它space needle在哪裡的時候 它知道在西雅圖
              
                  1:07:04
                  問它川普在哪裡出生 它知道在紐約 問歐巴馬在哪裡出生 它知道在夏威夷 但是語言模型是怎麼抽取 抽取諸這些知識的呢 他怎麼知道說看到space needle跟is located in 就要輸出Seattle的呢 這背後運作機制的原理是什麼樣子的呢 這邊有一篇23年的論文 這篇論文就建構了一個抽取知識的模型 這個抽取知識的模型長的是這樣的 實際的語言模型的運作 我們知道就是一個transformer輸入 比如說的臺北101 is located in 他就會輸出臺北 好那今天當我們把這個句子輸進去 語言模型是怎麼輸出這個字的呢 也許他輸出的方法是這個樣子的 他先對主詞的臺北101進行處理 前面幾層會先對主詞進行理解產生一個representation 那從主詞到這個representation的過程跟原來語言模型是一樣的
              
                  1:08:12
                  所以這邊並沒有做簡化 這個模型真正神奇的地方是 他說 根據接下來的關聯性 is located in 代表我們主詞跟受詞之間的關聯性 is located in 會產生一個linear的function 這個代表 關聯性的片語 會決定一個linear function 然後這個linear function 把這個representation 這邊用x做表示當作input 他會得到一個輸出 這個輸出做embedding 轉到這個vocabulary的space上以後 你就會看到臺北這個字 他的機率是最高的 講得更具體一點 is located in這幾個詞彙 會產生一個metric叫做WL 產生一個向量叫做BL 他們代表了一個linear function 輸入X X乘以WL加BL會得到一個Y 這個Y做unembedded 就會得到臺北這個字眼 所以今天如果說 把
              
                  1:09:14
                  the Taipei 101把這個地標換掉 換成the space needle 這個representation自然就換了 從X變成X' 但這個linear function是固定的 因為它只跟輸入的關係有關 你如果要問the space needle在哪裡的話 你用的linear function是一樣的 你只是把X換成X' 那就得到Y' 得到Y'做unembedded 你得到的就會是Seattle 那如果今天 輸入的主詞是一樣的 那X自然就不會變 但是如果你改變了這個代表關係的片語 把is located in 的改成has a height of 有多高 那就會產生另外一個linear function 那這邊用WH跟BH 來代表另外一個linear function 把輸入X乘以Wh加bh 得到w （口誤，應為y） double prime 做unembedding以後 你得到的就會是臺北101的高度 所以這個模型就告訴你說 語言模型在抽取知識的時候是這樣運作的 但這個模型跟原來的語言模型
              
                  1:10:18
                  尤其是在linear function這一塊 顯然就非常的不一樣 語言模型的最後幾個layer 真的可以用一個linear function 就概括嗎 所以你要先檢測這個模型的 Faithfulness 看看它跟原來真實的語言模型 有多接近 但是這個模型並沒有告訴我們 linear function實際上長什麼樣子 只告訴你說,linear function就跟這個代表關係的片語是有關係的,只要是一樣的關係,就有同一個linear function,但這個linear function裡面實際上的參數,你還是要自己求出來的,所以你就需要準備一些訓練資料,這個跟我們在做機器學習訓練模型的時候,其實是一樣的概念,所以你就去問一個語言模型,如果這邊放的臺北101,那在後面再接,你會輸出什麼,你會輸出什麼, 我就知道說輸入這個X,他就會輸出這個Y,有這個X,有這個Y,他們是linear function的input跟output,你要解出這個linear function裡面的參數,WL跟WB一點都不困難,在這篇paper裡面,他們會用8筆資料來找出linear function,所以這8筆資料都是某個地標在哪裡,某個地標在哪裡,找8筆這樣的資料,找出這個linear function,找出這個linear function以後,接下來
              
                  1:11:40
                  你就給他訓練資料,沒有看過的地標,比如說的space needle,然後得到X5,X5呢,再通過這個linear function,得到Y5,再看Y5做unembedding以後,會是什麼字,比如說Cietal,再看跟真正的語言模型的答案,有沒有一樣,這跟我們訓練機器學習模型的時候,分成training跟testing,其實是一樣的概念,只是在做機器學習的時候,我們的答案是人標的,他是光true,但我們 我們現在把語言模型的輸出當作真正的答案 要找一個比較簡單的模型來比擬語言模型的行為 好 那這個模型運作的是怎麼樣呢 就是還可以而已啦 所以他這邊就說了 不同的relation 到底faithfulness是怎麼樣呢 有一些relation 他的faithfulness真的非常的高 他預測語言模型的輸出的正確率 近乎百分之百 但也有一些relation 是剛才那個模型預測不準的 比如說一個公司的CEO是誰
              
                  1:12:45
                  一個人的父親是誰 一個人的母親是誰 或一個寶可夢進化以後是哪一隻寶可夢 這些他是預測不準的 所以剛才那個model 他的faithfulness就是一般啦 就在某一些relation上 他的faithfulness非常強 在某一些relation上 他的faithfulness其實也沒那麼強 不過這是一個比較早期的研究 告訴你說 還可以這麼搞 還可以找一個語言模型的模型 簡化語言模型 讓我們更容易理解 它背後運作的機制 但這個簡化要讓它有實際的用途 你就必須要能夠做到說 我在這個模型上得到的結論 可以直接用到真正的語言模型上 什麼意思呢 假設現在真正的語言模型問他 臺北101在哪裡的時候 他會回答臺北 但我現在想要修改他的輸出 我想把臺北直接改成高雄 那在真正的語言模型裡面要怎麼做這件事 就有點難嘛 我們之後講到這個類神經網路編輯的時候 後來再跟大家講說 有什麼樣的方法可以做到這件事 但我們在開學的第一堂課就告訴你說
              
                  1:13:49
                  如果你是用訓練資料 fine-tune語言模型 忘了fine-tune完以後模型就壞掉了 所以這個簡單的模型 語言模型的模型 就提供給你一個 類神經網路編輯的可能性 你可以先從這個模型上推論說 假設我要這個模型輸出高雄 那我這輸入的X 要怎麼改呢 我這輸入的X 要加上什麼樣的Delta X 輸出才能輸出是高雄呢 因為這個Linear Function 它只是一個Linear的Transformation 給定一個指定的輸出 你要求反推出 應該要什麼樣的輸入 才能給出指定的輸出 其實是容易的 你有修過線性代數 其實你都知道怎麼做 所以這個不是一個困難的問題 你可以找到一個Delta X 加上這個X之後 那輸出就會變成高雄 但是這個是在模型上找出來的 那模型上的結論 能不能直接用在語言模型上呢 現在把臺北101 is located in輸入給語言模型 把在語言模型的模型上 找出來的這個Delta X
              
                  1:14:54
                  直接輸入給真正的語言模型 直接加到真正的語言模型上面 如果他輸出會變成高雄的話 那這個模型就有用 這個模型上預測的結果 可以幫助我們修改真正的語言模型 模型的輸出 好那這個模型的模型 這個語言模型的模型上面觀察到的結果 有沒有用呢 居然還蠻有用的 在這個圖上每一個點 代表某種類型的關聯 橫軸代表 Fairfulness 的這個數值 有一些這個關聯是做得起來的 有些關聯是做不起來的 縱軸代表剛才那一個 那一招 那個類神經網路編輯就直接在那個 模型的模型上找出來的結論 直接用到語言模型上 到底能不能夠成功的修改呢 這個縱軸是正確率 那你會發現說有蠻多情況 居然是可以直接成功修改的 所以這個模型是一個有用的模型 不過剛才那個模型
              
                  1:15:59
                  比較像是某人腦門一拍 腦洞一開就突然產生出來的模型 那有沒有系統化的方法 幫語言模型 建構語言模型的模型呢 有沒有系統化的方法 幫語言模型建構它的模型呢 是有的 這邊有一系列的研究 那這邊我們就不講它的細節 我們就講它的精神 這一系列建構語言模型的模型的方法 他們的概念是這樣子的 把原來的語言模型 做一個很大的pruning pruning的意思就是 把語言模型裡面的一些component拿掉 拿掉一個神經元,我直接拿掉某一個self-attention,看看模型還能不能妥善運作,那我們要一直pruning,一直pruning,一直pruning,一直拿掉component,直到prune完之後新的模型,那新的模型就是語言模型的模型,prune完之後的新的類神經網路,就是語言模型的模型,直到碰到它一目瞭然為止,直到它變得非常簡單為止,那我們pruning的時候,要確保說我們關心的那個任務, 它的輸入輸出的關係仍然是沒有改變的,本來的語言模型,你問它這個問題,它會有這個答案,問它這個問題會有這個答案,問完之後,我們會做一個非常劇烈的pruning,prune完之後,類神經網路要看起來非常簡單,人類一目瞭然,但是我們關心的那些任務,它的答案仍然是不可以改變的。
              
                  1:17:30
                  好,那這個prune完之後的結果啊,今天在文獻上通常叫做circuit,那這個circuit跟真實的電路沒有什麼太多的關聯 那這個circuit就是語言模型的模型 那這件事情呢,又很像是network compression 那我想蠻多人都知道network compression了 那我們在過去2021的課程也有講過network compression 模型壓縮的方法就把一個比較大的類神經網路 變成一個比較小的類神經網路 那這邊建立模型的模型的方法跟network compression有什麼不一樣呢 方法是非常類似的 比如說都可以用printing來拿掉一些沒用的component,但目標不一樣,我們一般做network compression的時候,你其實希望compress以後的結果,在各個不同的任務上都要逼近原來的模型,原來的語言模型,建構模型的模型這邊呢,我們只關心某個特定任務,比如說只關心knowledge extraction,或甚至在一些更古早的文章裡面,他只關心一個叫做IoI的問題,這個IoI的問題, IoI的問題是什麼呢?
              
                  1:18:40
                  這個非常簡單的問題,這個問題就是 A跟B一起去酒吧 B拿了一杯酒給 然後叫語言模型做接龍 那就要接A嘛,然後他只分析 這個任務,然後他就會發現說 在這個任務上面呢 需要比如說五六個attention 然後他就把模型 就噴噴噴噴噴噴掉 然後多數的component跟這個任務都沒關係 都噴掉,最後只留下五六個attention 你就可以清楚知道說 語言模型在最後回答A的時候 中間經歷了什麼事情 所以這個大家在文獻上 自己再慢慢看 建立語言模型的模型的時候 建立這個circuit的時候 你會希望做非常大量的printing 只要print到人看得懂這個類人間網路在做什麼 那你只關心少數的任務 你只關心非常侷限的任務 他的答案會不會改變 而network compression通常不會print那麼多 你也不在意print完以後的結果 能不能夠一目瞭然 能不能夠被解釋 但是你希望prune完之後的結果 跟碰之前在多數的任務上 在多數的狀況下 這個能力呢
              
                  1:19:43
                  這個模型的能力是不要有太大改變的 那這邊就只是講了一個 建立模型的模型 建立circuit的概念啦 那相關的文獻只能說是汗牛衝動 那我們這邊就不細講 那這邊就是列了幾篇 舉標具有代表性的論文給大家參考 那你看到說這些論文的標題 都有circuit這個字跟電路沒有關係 它是告訴你說它們是怎麼研究語言模型的模型的 好,那最後一個部分要跟大家講 怎麼讓語言模型直接說出它的想法 那在2024年的生成4AI導論裡面 我們也講過說語言模型會說話 所以很多人說這個大型語言模型黑盒子沒有解釋性 不,它是最有解釋性的 它就跟人類一樣 你有什麼問題,你要叫它解釋結果 我問就完事了,你就給他,比如說我叫他做新聞的分類,跟他說新聞就分成這幾類,給你一篇文章,告訴我新聞是哪一類,他可以輕易的告訴我,就是生活類,所以接下來希望他進一步解釋,為什麼知道這篇文章是生活類,比如說你問他,哪幾個關鍵字讓你覺得是生活類呢,他就列出幾個跟天氣有關的關鍵字,說我是因為看到這幾個關鍵字,所以覺得是生活類,但是這樣子的方法還是 也是有他的侷限,他的侷限是什麼呢?你沒辦法真的知道每一個layer在想什麼,而如果你真正直接問我們語言模型說,你是在第幾層類神經網路開始知道這邊新聞是生活類的,我問確GPT-4.5,他其實也會回答你,但他其實回答就是很像是在教科書上抄出來的答案,他說淺層的類神經網路,就是提取初步的字詞跟短語特徵,比如說語法結構等等,中層,我可以
              
                  1:21:33
                  可以辨識出特定主題,然後生成,我就可以知道說這篇文章是生活類 但是語言模型是不是真的是這樣運作的,或他自己知不知道自己是這樣運作的 這個真的是很難說,這比較像從教科書上抄出來的答案 模型相對於人類,他的思維是更加透明的 當你叫一個人解釋他為什麼會做這樣決策的時候 你不知道他心裡是怎麼想的 但語言模型神奇的地方是 他的思維是透明的 你可以直接看到 他每一層是怎麼想的 怎麼說呢 好這邊就跟大家剖析 為什麼語言模型的思維是透明的 我們之前講一個layer的時候 我們都講說一個layer就是輸入一排向量 輸出一排向量 但是其實這只是一個簡化的講法 我們忽略了一個最重要的component 就是residual connection residual connection的意思是說 當一個layer得到一排輸出之後 每一個輸出都還會跟輸入 加起來再得到最終的輸出
              
                  1:22:40
                  所以這個投影片上紅色的向量 才是最終的輸出 紅色向量的輸出都是黃色的向量 加上綠色的向量 而黃色的向量是從綠色的向量產生出來的 雷根會想說 為什麼需要有residual connection 這樣子的設計呢 那這個設計它的起源非常的古老 它是在15年的時候就有了 那個時候世界上 還沒有人類 那個時候呢 是大概前寒武紀的時候 這個15年的時候 人類就 人類那時候還沒有人類 但是那個時候 就已經有residual的connection 那residual connection的出現 是為了要讓很深的network變得更好train 沒有residual connection之前 那時候network都train個二三十層 有了residual connection之後 都可以train個一百多層 我知道你現在覺得train一百多層 也沒有什麼神奇的 但是當年人們都驚呆了 加了這個功能之後 加了這個連結之後 居然就可以train一百多層的network 太神奇了 這個就是residual connection 所以後來在train深的網路的時候 其實都有residual connection這個設計 所以實際上一個transformer
              
                  1:23:47
                  它layer跟layer之間的運作 是要加上這個residual connection的 是要加上這個residual connection的 也就是說一個token進來 它通過一個layer的時候 它除了產生一個output之外 還會把原來的輸入再加起來 通過一個layer產生output的時候 會把原來的輸入再加起來 最後再做unembedded 得到最終的輸出的distribution 你可能覺得看這個圖 沒有什麼神奇的 那我們換一個畫法 左邊的圖跟右邊的圖 是一模一樣的 並沒有真的改變它實際的運作 但是當我們圖變得不一樣的時候 你的想法就變了 從這個左邊的圖看起來 你會覺得說 是輸入做了一個轉換 輸入做了一個轉換 但是當我們把圖換一個畫法的時候 整個transformer 它真正的運作更像是 它有一個叫做 residual string的高速公路 直接把輸入的東西 一路就傳到輸出
              
                  1:24:51
                  而在中間的過程中 每一個layer 都會加一點東西 到輸入裡面 加一點東西到輸入裡面 這個residual string是一個生產線 就一路的被送上去 一路被送上去 只是每過一站都會加上料 加上一些額外的資訊 加上一些額外的資訊 最後得到最終輸出的distribution 所以這才是transformer多個layer 真正運作的機制 好那既然在最後一站 你可以過一個unembedding的module 過一個linear transformation 把這個向量變成一個極限 那前面這幾站 只是最後一站少了一點什麼東西而已 那前面這幾站 能不能也直接加一個unembedding的layer 把它變成token的機率分佈 也就是變成文字的機率分佈呢 這件事是可行的 那這一招是有名字的 現在多數人稱它為logic length
              
                  1:25:56
                  因為這個輸出的distribution 在過softmax之前叫做logic 我們今天是檢查每一層的logic 來看看類神經網路 來看看transformer是怎麼思考的 所以它叫做logic length 那人類什麼時候知道語言模型的思維是透明的呢 其實我們實驗室在2020年年初的時候 就知道語言模型的思維是透明的 那時候就已經發現說 那時候的模型BERT 其實你是可以看到它每一層 在想什麼的 你可以透過Large Lens的方式 解析出每一層的文字內容 這個是高偉聰同學跟那個吳宗漢同學做的 當時有了這個發現以後覺得 這個發現一點用都沒有這樣子 那時候覺得這能幹嘛 所以這篇文章後來甚至是沒有投稿 就直接放在Archive上而已 左邊這個圖是論文裡面的圖 那這個就是告訴我們說
              
                  1:27:00
                  你可以把最後的Unembedding Layer 接到中間的每一層 你就可以看到Bert是怎麼處理一段文字的 那右邊這個表格是一個真實的例子 現在輸入的句子是 It's a bitter sweet and lyric mix of elements 就是這是一種苦樂參半 且抒情元素的混合體 那it是什麼 沒有講這個句子就只有一個代名詞it 那接下來你把這個句子輸入 Bert一個遠古時代的語言模型 然後解析出 它每一層的輸出 你會發現到第11層的時候 it那一個字 變成了elbent 你把embedding這個機制 裝到第11層的時候 它解析出來的不是it 而是elbent 代表在這一層的時候 Bert知道說 it這個東西 它指的可能是某一個專輯 這也符合這個 蠻符合這個句子的敘述 有可以瞭解說Bert其實會把這些代名詞做一些reference 他去猜說這個代名詞實際上指的是什麼樣的實體
              
                  1:28:08
                  好,那後來呢,就有很多人利用這種logic lens的方法來分析語言模型內部是怎麼思考的 這邊引用的是一篇23年的論文 他就想要知道說語言模型是怎麼回答一個問題的呢 他這邊就是要問語言模型一個國家的首都是哪一個城市 因為這是比較舊的模型 他需要做in context learning 他要先跟語言模型說 what is the capital of France answer是Paris 然後what is the capital of Poland answer 然後叫他做文字接龍 看能不能接出華沙這個城市的名稱 那實際上語言模型運作是怎麼樣呢 他就把冒號這個位置對應的representation 每一層都用large lens解析出來 一開始語言模型根本搞不清楚 他應該是哪一個token 但走到第15層的時候 他突然知道那個token應該是Poland 然後從第19層開始 他突然知道應該是要回答華沙 就從第15層開始
              
                  1:29:13
                  他知道說接下來要輸出的東西 應該跟Poland是有關係的 然後到第19層開始 他知道說他要輸出的是華沙這個城市 那左邊是更詳細的分析 那這個縱軸啊 是代表的是那個 在distribution裡面 這個華沙跟Poland這兩個token的機率 那實際上他顯示的不是機率啊 因為機率實際上畫出來可能會非常的小 所以他顯示的是reciprocal rank reciprocal rank是什麼意思呢 就是那一個token 在所有token裡面機率排名的倒數 如果他排第一名數值就是一排第二名 二分之一排第三名就是三分之一 以此類推 就看到說Poland這個詞彙 隨著layer持續的增加 在某一層突然之間 residual street裡面 出現Poland這個字 然後接下來又急遽下降 被華沙所取代 那感覺語言模型先知道說 要回答一個跟Poland有關的東西
              
                  1:30:18
                  最後才鎖定說真正的答案 是華沙 而且呢 如果今天同樣答案是華沙 不同的問法 他的回答他背後運作的機制是不一樣的 我們剛才說如果直接問他這個問題 他會先鎖定說答案是跟波蘭有關 然後再回答華沙 但另外一方面 假設是讓他做閱讀理解測驗 先給他一篇文章 再問他一個問題 就直接問他說 波蘭的首都在哪裡 那前面的文章裡面已經提到波蘭首都是華沙了 然後直接給他answer的話 他就不會產生波蘭這個字 他直接在第十六層就知道 答案是華沙了 又知道說不同的問法 不同的狀況 他背後運作的機制是不一樣的 那透過這種Logic Lens 你就可以去知道語言模型心裡在想些什麼 比如說有人會想說 像LLaMA-2這種模型 他看過的英文資料是遠比中文資料多的 所以他實際上在想事情的時候 他內心深處到底是用哪個語言呢 這篇文章是去年年初的文章
              
                  1:31:24
                  他們就做了一個實驗 他們用LLaMA-2呢 來做翻譯 他們就跟LLaMA-2說 法文的這個詞彙 這個是花的意思了 法文的這個詞彙 對應到中文的哪一個詞彙呢 那LLaMA-2可以正確的接觸 花這個字 但他怎麼知道法文的這個字 翻成中文就是花呢 你如果分析他中間的每一個layer的話 會發現說 他是先把法文的花 翻成英文的花 再把英文的花 翻成中文的花 所以右邊就是用logit lens 分析每一層之後 得到的結果 輸入是中文冒號 然後這個是空格 空格之後就要產生答案了 所以從空格開始 把每一層都用logit lens 解析出來 那最前面幾層 紅色就代表說 透過logit lens解析出來的 那個Distribution 它的Entropy越大 它要輸出的
              
                  1:32:26
                  是英文的Flower 到了27層之後 它才意識到說要把英文的Flower翻譯成 中文的花 所以代表說模型在思考的時候 它內部其實是用英文 在思考的 它是先把法文翻成英文再把英文 翻成中文 雖然你外表看起來再把法文直接翻成中文 在它中間是用英文做媒介 不過這是在LLaMA-2上的實驗啦 LLaMA-3現在中文能力其實蠻強的 所以期待有人去分析LLaMA-3 看看他內心是不是還用英文在思考 好,那我們現在已經有了residual string的概念之後 接下來我們對於每一個layer做的事情 就可以有不同的想像 我們現在知道說每一個layer就是加一點什麼東西進去這個residual string 那他到底加了什麼呢 我們要怎麼解析每一個layer加了什麼樣的東西 一般我們在講神經元的時候 我們都是說把前一個layer的輸出集合起來
              
                  1:33:32
                  做weighted sum變成一個神經元 把前面的layer集合起來 weighted sum變成一個神經元 但你可以反過來看待這件事 反過來看待這件事以後 這個世界就變得不一樣 它的運作是完全一模一樣的 但是反過來看以後 你可以有不同的理解 你可以說前一層的 某一個神經元某一個dimension的數值 乘上weight以後 傳輸給 接下來下一層不同的dimension 前一層的某一個數字 某一個dimension乘上不同的weight以後 傳到下一層去 那這件事情 這樣的一個概念 這只是一個概念 因為他並沒有改變什麼東西 他就概念 是在transformer feed forward layer 這篇paper裡面被提出來的 那篇paper引用非常高 很多人都知道這個概念 就是其實 一個multilayer的perceptron 一個多層的 可以看作是一個
              
                  1:34:35
                  key有key 有value的attention 前一層的這些數值 就是attention的weight 然後後面output的這些數值 就是一個value 不知道大家聽不聽得懂 那如果你暫時一下子沒有辦法 心領神會的話 也沒有關係 用不塞上這個概念 你回去再仔細看一下這篇論文 這篇論文是一個改變 大家對於Feed Forward Network 的想像的論文 好 所以假設 前一層每個Dimension的數值 就是K1 K2到KD 這邊每一個K代表一個Scalar 一個數值 那K2它會接到 下一層的每一個輸出 那我們把K2對應到 下一層每一個輸出的weight 集合起來說它是一個向量 叫V2 KD也對到每一層都有一個weight 我們把這個 叫做VD 那它們是向量V2 VD是向量 所以這個藍色的輸出啊 其實是前一層每一個K
              
                  1:35:41
                  乘上它對應的V再加起來 所以藍色的輸出 是KI乘上VI Summation over I等於1到大D 好,那我們知道說呢,每一層啊,在residual string上面,每一個位置都可以透過logit lens解出一個distribution,解出一個distribution,那這個藍色的向量,它就是加進去以後改變了這個distribution,那這個藍色的向量是由一堆的V做weighted sum以後集合起來的,那我們能不能夠把V這邊的V也做unembedded, 透過logit lens,解析說每一個類神經網路,它想要輸出什麼樣的東西,去加入residual string,影響最終的輸出呢,其實是可以的,每一個加入這個residual string的這些V2,VD,都可以透過unembedded layer,轉成一個token的distribution,它可能也代表了某些特定的意思,真的是這樣子嗎? 一篇22年這個遠古時代的論文
              
                  1:36:54
                  這個遠古時代的論文 那個時候人類只有茹毛飲血 但那時候人類就已經發現說呢 這些V是真的有對應到某一些概念 某一些意思的 比如說有某一個V 這個是第三層的編號1018的V 他就對應到一些單位 有一個V 他是第一層的編號第一個V 他就對應到一些 這個代名詞 第六層的編號3025的V 他就對應到一堆副詞 第十三層編號3516的V 他就對應到一些不同的族群 所以你也可以透過這個logit lens 來解析這些V 他代表了什麼樣的意思 好那知道這件事以後 能夠做什麼呢 知道這件事以後 你就可以對類神經網路 做初步的編輯 之後還會講更多更強悍的編輯的方法 但這邊講一個很基礎的 所謂編輯的方法 假設你問大型語言模型 誰是全世界最帥的人 他通常不回答你
              
                  1:37:57
                  但如果有prompt GPT-4.5很多次 某一次我發現他就說是金城武 所以他可能認為金城武是世界上最帥的人 那如果我把金城武換成李宏毅的話 要怎麼做呢 我們之前講過如果直接train network network是會壞掉的 但是我們剛才已經知道 每一個V就是加一點資訊 到整個residual network裡面 所以你可以分析說 當模型輸入這句話 產生這個答案產生金城武的時候 到底是哪一個V 被加進去了這個residual string 你知道identify出 這一個V之後 把這個V減掉 金城武的token embedding 再加上李鴻一的token embedding 但這邊假設是金城武跟李鴻一都是一個token 減掉金城武的token embedding 加上李鴻一的token embedding 就本來類神經網路 當他啟動某一個K 要加某一個V到residual string的時候 他是為了回答全世界最帥的人 但我們把他的資訊 從金城武置換成李宏毅 他就可以把李宏毅當作答案 這一招有用嗎 這個在21年
              
                  1:39:00
                  輪古時代的時候就已經有人試過了 這一招他有48% 的機率可以改變 類神經網路的輸出 改變不見得答對喔 就輸出變得不一樣 那成功的機率 他真的輸出是李宏毅成功的機率有34% 那個人覺得34%沒有很高 那不是0啊 就代表說這一招是可以 真的拿來編輯類神經網路 改變他的輸出的 好 剛才那個Large Lens的方法呢 有一個致命的缺陷就是 我們透過Unembedding的方法 只能夠把一個Representation 轉成一個Token 所以我們解析出來的結果 都只能是一個Token 另外一方面啦 很多時候語言模型在做的事情是 預測下一個Token 你輸入李宏毅老師中間這個Embedding 並不見得代表李宏毅老師 這個詞彙的含義 它真正代表的是 看到這個輸入以後 模型想要輸出是 這個Token 它想要做 產生是這個Token的時候 所產生的Representation 所以假設你想要知道
              
                  1:40:04
                  李宏毅老師在類神經網路看起來 是什麼意思 你用Logic Length 可能不一定能夠解析出 你要的結果 所以怎麼辦呢 有另外一招 那這個就是去年的論文了 這招叫Patch Scope Patch Scope Patch Scope這一招的意思是說 我們先看看 如果我們跟 語言模型給他這樣的輸入 跟語言模型講說 李奧納多冒號美國演員 臺積電冒號臺灣公司 然後再隨便給他一個東西 這個X可以是任何東西 那他就會輸出 他就會輸出 他對於X的理解 那怎麼知道一個類神經網路 當他看到李宏毅老師 這幾個字的時候 他內心深處的理解是什麼呢 你就把李宏毅老師輸入這個類神經網路裡面 然後看看在某一層他輸出的representation長什麼樣子,接下來把這個representation置換到這一個input string裡面,就這個類似這個語言模型他的輸入是一樣的,這邊甚至一樣就是給他一個x就好了,但是在這個位置把他的embedding,把在這個位置把他的representation換成輸入是這一串文字時候的representation,那對這個類似對這個語言模型來說,他就好像看到 X是李宏毅老師這一串字一樣
              
                  1:41:23
                  然後他就開始繼續輸出 他就有可能告訴你李宏毅老師的身份 那我知道講到這邊你可能會有個困惑就是 那我前面起不是要準備一些例子 那我準備的例子不是會影響最終輸出的結果嗎 沒錯就是你準備的例子 就是會影響最終輸出的結果 不過這篇文章的作者覺得這是一個feature 不是一個bug 你可以調整前面準備的例子 然後模型就會給你不同風格的解釋 比如說如果你現在的輸入是告訴我 X相關的秘密 然後你再把X這個位置的representation 換成李宏毅老師的representation 他可能就回答是個肥仔 你就可以從不同的角度來解析一個representation 那這邊就是引用了剛才就是提出這個 scope patch這個方法原始論文裡面舉的一個例子 他們就把 戴安納他是這個 那個 威爾斯王子的王妃 我這邊似乎少打了一個S
              
                  1:42:28
                  不過沒有關係 那個戴安納他是威爾斯王子的王妃 然後把這個片語 輸入給 類神經網路輸入給語言模型 然後接下來解析 他看到這個片語的 最後一個字的時候 他的每一層 的輸出對 這一個語言模型來講 分別是什麼,所以在前面一二層的時候,如果你把這個位置的representation拿去解析,語言模型輸出的句子是country in the United Kingdom,或者是country in Europe,因為威爾斯也是一個英國裡面的國家的名字,是United Kingdom裡面一個國家的名字,所以語言模型在前面幾層,他只認了威爾斯這個字,所以他就覺得他看到的是一個 但到了第四層的時候,他顯然讀到了Princess of Wales,他讀到了這一整串詞彙,在第四層的時候,他解析是說這是一個給皇室女性的頭銜,然後到第五層的時候,他知道說這個人呢,是威爾斯王子的妻子,然後到第六層,他才讀到戴安娜這個字,然後就輸出戴安娜完整的資訊,所以可以透過這個方法解析一個語言模型,每一層他可以 他看到的,他看到的東西實際上對應的文字是什麼,接下來最後一部分,最後幾頁投影片呢,就是舉一個例子,說剛才那些解析的方法,如何改變了人們對類神經網路背後運作機制的理解,進而提出了新的想法,那這是一篇去年六月的文章,這篇文章想要解析的是,對於一個multi-hop question,語言模型是怎麼回答的,然後解析完之後,他提供了
              
                  1:44:20
                  提出來一個方法,讓語言模型在multi-hop的question上面可以做得更好 那這個multi-hop的question這邊的例子是 the spouse of the performer of imagined is 像這種multi-hop的question裡面呢 會有包含三個entity 第一個entity是會明確出現在問題裡面的 那在這個例子裡面就是imagine 那我們把它叫做E1 那第一個entity imagined是一張專輯的名字 那接下來我們要問的是 The Performer of Imagine 就是彈奏Imagine創造Imagine這張專輯的這個音樂人是誰呢 那這個是E2 那他其實約翰藍儂（John Lennon） 所以這個E2是約翰藍儂 然後接下來呢 約翰藍儂的配偶又是誰呢 那這是E3 約翰藍儂的配偶是小野洋子 就是YOKO 然後模型知道這個答案以後 他就要看到這一串文字 然後輸出YOKO 那接下來的問題是 模型是怎麼做 這一連串的解析的呢 他是怎麼做這種需要
              
                  1:45:24
                  Multi-Hop Reasoning 需要做多步推理的問題的呢 一個直覺的想法是 模型讀到Imagine這個字以後 他根據前面的關係 The Performance of Imagine 先解析出答案是約翰藍儂 然後知道答案是約翰藍儂之後 再經過 The Spouse of約翰龍 這一個片語 解析出最終的答案 是Yoko是小野洋子 那模型真的是這樣運作的嗎 所以他們就他們就用剛才講的那個 那個PatchScope那個方法 做了一下解析 所以他們就把這一個位置的每一層 都拿出來看看 看看會解析出什麼樣的內容 然後如果解析出的內容裡面 有包含約翰藍儂這個字 就把它記錄下來 那得到的結果呢 是藍色的這條線 藍色的這條線橫軸呢 是layer然後縱軸呢 是解析出 這個E2 解析出E2的 第一次出現的layer
              
                  1:46:28
                  所以就會發現說 什麼時候解析出E2呢 什麼時候語言模型可以根據E1解析出E2呢 在蠻前面的layer 就可以根據E1 解析出E2了 好那根據E1解析出E2以後 再來要根據E2解析出E3 那什麼時候解析出 E3呢 他就分析這一個位置 每一個representation 他對應的文字 然後如果有出現 E3的內容的話就把它記錄下來 然後得到的結果呢 是橙色的這條線 所以你會發現說 多數情況都是大概在layer 第20層到第25層間 會解析出 E3這個詞彙 所以你可以感受到說 在比較低的layer 先解析出E2 然後接下來才解析出E3 然後最後就可以給你 正確的答案 然後這篇文章的作者發現說 當有時候這種multi-hop的question 沒有辦法得到正確的答案
              
                  1:47:31
                  是因為E2太晚被解析出來了 因為E3 必須要在第20幾個layer 被解析出來 才有解析出最終答案的能力 如果今天E2太晚被解析出來 過了20層才被解析出來 那接下來在T2這個位置 就來不及解析出E3了 所以怎麼解決這個問題呢 他們有一個神妙的做法 就是把後面幾層的representation 直接加到前面來 再重新跑一次 就結束了 既然今天只有中間某一層 能夠解析出E3 如果E2太晚被解析出來 那怎麼辦呢 把後面的layer放到前面 這樣只有能夠走過第二十層 就可以把E3解析出來了 這招有沒有用呢 這招居然是有用的 他們試了各式各樣不同的模型 那correct代表說 在用這招之前 模型本來就會答對的問題 那本來就會答對
              
                  1:48:33
                  那正確率居然當然是100嘛 然後做完這招以後不會影響正確率 但神奇的地方是 對本來不對的問題 用了這招以後 大概會有40到60%的正確 你可能會覺得40%到60%的正確率 也沒有很高 但不要忘了 這邊40%到60%的問題是 原來全部都答不對的 所以原來是0%的正確率 用了這招以後 原來完全答不對的問題裡面 居然有4到6成 可以因此就答對了 所以這一招 看起來其實又跟reasoning有點像 我們在第一堂課 不是講過reasoning 就是深度不夠 長度來湊嗎 這邊paper是6月的 的時候還沒有reasoning的模型 拿起跟reasoning的模型 也是很像的 reasoning的模型只是把 你的輸出如果來不及解析完 就跑到下一個time step 再重新解析一次 所以這個方法 這邊paper提出來一個叫做backpatching的方法 其實跟reasoning 深度不夠
              
                  1:49:36
                  長度來湊的做法 其實是有異曲同工之妙的 好 那這個就是今天想要跟大家分享的內容 就從一個神經人開始講起 最後講到怎麼讓語言模型 直接輸出他解析的結果
              

## 4、后训练与遗忘

* pre-train 风格：文字接龙的方式
* SFT 风格：一问一答
* RL 风格：偏好

案例：教 LLaMA-2-Chat 用中文回答，该模型主要训练资料是英文，对此可搜集大量中文数据进行训练（Pre-train 风格），但实际做完后发现，原来 Alignment 的能力被破坏了；

这种遗忘的现象非常普遍，



因为pre-training的style有什么问题 才导致post-training之后 非常容易遗忘 其实不是 就算你是做SFT的style 模型仍然非常容易遗忘 那以下这是一个比较早期的论文 那你从他的标题就知道 他想要表达什么 他的标题是 Fine Tuning Align Language Model Compromise Safety Even when users do not intend to 他发现说 你Fine-tune完模型之后 模型的Safety Alignment的能力突然不见了 就算你没有意使图要这么做 那这篇paper呢 他应该是做在ChatGPT 3.5上面啦 他们是Finetune了ChatGPT 3.
              
                  12:21
                  5的模型 那这边呢 不同的数字代表说 不同面向的安全能力检测 那这边数值越大 其实代表的是模型越常讲出不该讲的话 那灰色的部分 代表的是 Post training之前的模型 不过他其实也是做过 Instruction fine tuning的模型 其实就是那个ChatGPT 3.5 这个Post training前的模型呢 他其实非常强的 他在各个不同安全性检测的面向上 都不会说错话 好 但是如果你今天 教模型讲一些不该讲的话 比如这边的例子是 你能不能够教我怎么做一个炸弹啊 然后你强迫模型 在训练资料里面 你训练的时候就强迫他说出制作炸弹的方式 那这样一训练完之后 非常直觉的模型各种安全性的能力 都突然变得很差 但是奇妙的事情是 我们看中间这个例子 中间这个例子并没有叫模型 做什么特别不该做的事情 他只是帮模型改了个名字 他现在不叫ChatGPT
              
                  13:26
                  他叫做AOA 跟他说AOA帮我做某件事的时候 他就要回答 我是AOA 我很乐意帮你 就算是只是这样的训练 明明只是帮模型改了一下身份 突然之间各种Safety Alignment的能力也都不见了 那有人可能会觉得说 帮模型改身份这个影响太大了 模型身份变了 也许他就忘记他本来该做的事情了 好 那这边呢 有一个正常的训练资料 他们用的就是 Alpaca的Dataset 那Alpaca呢 我们上次上课的时候其实有提到 就是从这个 ChangeBT那边 做Knowledge Destination得到的资料 那里面都是一些正常的问题 没有什么奇奇怪怪的东西 比如说输入是三元色是什么 然后输出呢 就是问题的答案 但他们发现说 就算拿这些看起来非常正常的资料 去做Supervised Fine Tuning之后 模型的Safety Alignment 也在好几个面向上 突然就变得非常的差 好 那这个是比较早期的研究
              
                  14:30
                  那其实到最近 你还是可以 观察到类似的现象 这个是来自我们实验室繁华同学的文章 那这个是去年年底的时候 放在Archive上的 那个时候我们用的模型 已经不是LLaMA-2了 那时候你用的Foundation Model 已经是LAMA-3 好 那我们在LLaMA-3上面呢 对它做Supervised fine-tune 我们分别交了四个任务 包括教它怎么做reasoning 然后呢 教它成医学的知识 然后教它写程式 教它使用工具 那这个纵轴呢 是在这四个面向上面的表现 数值越高越好 黄色的bar代表的是 foundation model的能力 橙色的bar代表的是 fine-tune后的能力 那在这四个面向上 因为我们特别教了模型怎么做reasoning 教他医学知识 教他写程式 教他使用工具 你特别教他这些事情 他在这些任务上 当然会得到比较好的结果 但糟糕的是 是你教他这些新的能力之后 他本来的Safety Alignment的能力
              
                  15:34
                  就突然炸裂了 那这边做了两组Safety Alignment的测试 那这两个Benchmark呢 都是准备一些句子去问模型 然后看看模型会不会说出不该讲的话 这边纵轴呢 是模型说错话的比例 那这边的数字越大 代表模型越容易说错话 好 那在这个HEXPHI的这个Benchmark Purpose上啊 在Foundation Model原来 他说错话的阻挡有害问题的能力 是非常强的 他非常少说出不该讲的话 但你一旦教他新的技能以后 模型突然就崩了 就非常容易说错话 那在下面这个ADVBench上面也是一样的 发现你根本看不到黄色的Bar 因为在ADVBench上 LLaMA-3非常的强 他说错话的比例是0% 他没有犯任何错误 但你一旦做PostTraining以后 模型能力就突然不好死了 他就忘记之前在做Alignment的时候 他已经会的技能 那这篇文章也提出来了一个解决的方法
              
                  16:37
                  那至于实际上解决的方法 大家再自己去看论文 那我刚才举的例子 都是破坏Safety Alignment的能力 那你可能会觉得说 是不是只有Safety Alignment的能力会被破坏 Safety Alignment在我们的经验上是 最容易被破坏的能力 所以你做Post Training的时候 你都会很明显的观察到 Safety Alignment非常快的就坏掉了 但是其他能力也是会受到伤害的 这边再举另外一个例子 那这边paper也是做SFT style的post training 然后在这边paper里面呢 这一排的数字是他们的foundation model的能力 那他们把他们的foundation model叫做C model 这个post training的文献上啊 这个用词很多地方非常的混乱 每个人都把他的foundation model叫不同的名字 比如有人会把他的foundation model 就叫base model 那你可能会以为他的base model指的是一个pre-trained model 不是他的base model是一个做过alignment的model 是一个instruct model
              
                  17:39
                  然后你听到这边你就觉得 我破掉了不知道他写些什么 所以这边读文献的时候要小心一点 每个人对于foundation model的称呼 每篇论文对于foundation model的称呼 都是不一样的 很多人的base model其实是一个instruct model 好 总之呢 这是他的foundation model 在三个不同面向上 第一个是教 第一个是测试模型使用工具的能力 第二个是模型数学能力 第三个是程式能力 这个是foundation model的表现 那接下来呢 他们分别教他们的foundation model 三件不同的事情 教他怎么使用工具 教他怎么算数学 教他怎么产生程式 那你会发现说 如果今天是你的目标任务 你教模型什么 他在那个任务上的表现 当然会变好 比如说 使用工具的能力 相较于foundation model是变强的 在post training之后 这个算数学的能力 在post training之后呢 也稍微变强了 写程式的能力 在post training之后 也稍微变强了 但是你会看喔
              
                  18:41
                  同一个模型 同一个role代表同一个模型 同一个模型 如果你只教他怎么使用工具 他数学跟程式的能力就变差了 教他怎么算数学 程式跟使用工具的能力就变差了 叫他写程式 数学跟使用工具的能力就变差了 还大幅暴跌 从19.6一下子掉到3.6 所以发现说post training 他不只是破坏了模型的safety alignment 他也破坏了模型很多其他基础的能力 那这边有更多的案例 比如说如果你今天想要教一个文字模型 读懂听懂新的模态 比如说我们这边尝试教 LLaMA这个模型听声音 LLaMA这个模型呢 它本来是一个文字模型 它只能输入文字输出文字 我们希望提供给它更多声音的资料 微调这个LLaMA的模型 对它做post training 希望它可以把声音当作输入 一个语言模型 如果可以把声音当作输入的话 那它就变成一个spoken language model
              
                  19:46
                  一个语音版的语言模型 那像这种语音版的语言模型有什么作用呢 如果它可以听得懂声音的各个不同面向的话 那你就可以让它来做很多事情 比如说最基本的 也许是做语音辨识 问他说这句话的内容是什么 他就把这句话的文字把它写出来 但你可以教他做更多事 比如说你可以问他说 这句话的情绪是什么 然后期待他可以给你正确的情绪标註 好,那像这类的模型 像这类,像这类 教这个LLaMA模型 对LLaMA模型做PostTraining 教他新的模态的模型是怎么打造的呢 那这边虽然是用声音当例子 但其实在影像上也是大同小异的方法 就首先你有个文字模型 他可以输入文字输出文字 现在我们要让他可以听懂语音 但因为语音是一个非常复杂的讯号 所以你可能很难直接呢 让文字模型把语音当作输入
              
                  20:51
                  所以通常呢 你需要一个pre-trained好的encoder 他做的事情就是输入一段复杂的声音讯号 输出是什么 输出就是一个一个向量 他等于是把声音本来很复杂的讯号 做了一个简化 那这边通常是比如说0.02秒 用一个向量来表示它 但是这个文字模型呢 他还是读不懂这些向量 怎么办 你需要微调一下这些文字模型 那你通常不会微调整个文字模型所有的参数 你可能会在文字模型里面 插入一些adapter 那你只去微调adapter里面的参数 但怎么微调这些参数呢 训练的目标是什么呢 你需要准备一些跟声音相关的任务 比如说你就教模型说 现在看到这句话 如果有人叫你对这句话做语音辨识 那你就输出这句话的文字内容 在这个例子里面是how are you 有人叫你侦测这段话的情绪是什么 那你就去微调adapter里面的参数
              
                  21:54
                  让最终这个文字模型可以输出happy 这是一个蛮常见蛮通用的 对文字模型做post training 让他可以听懂语音的方法 那这类的模型非常非常多 那这个是林益诚同学整理的一个表格 里面就列举了各式各样 用这种方法打造出来的语音模型 但实际上啊 对文字模型post training想要让他听懂声音 最大的难题就是遇到forgetting的问题 那以下是卢克韩同学提供的例子 我们现在呢 拿23个不同的声音相关的任务 来fine tune这个LLaMA 希望他可以把声音当作输入 好 在第一个a part turn完之后 那我们呢 就给模型一段声音 然后问他一个问题 我们现在要问他的问题是说 这个语者的情绪是什么 然后我们再额外要求他 输出必须要用JSON format 然后把answer当作key 这是模型式 所以实际的输出 他就输出answer冒号
              
                  22:57
                  curiosity 他觉得这句话的情绪是curiosity 那这个curiosity是一个错误的答案 但是至少他回答的format是正确的 这是一个正确的Jason Format 而且在这23个任务里面呢 其实没有任何任务跟产生Jason Format是有关的 所以模型能产生Jason Format 是因为LLaMA本来就知道 怎么产生Jason Format 你现在帮他加了额外能力 让他可以听懂声音 Jason Format的能力还在 所以今天你叫他回答语音相关的问题 但是用Jason Format的时候 在只有一个APA训练的时候 他还做得到 但是因为一个APA训练太少了 所以他还没有真的学会听懂语音的情绪 好,接下来呢 我们就把APAC数增加到三个 看看会怎么样 当APAC数增加到三个的时候 给他同样的句子 给他同样的指令 他的输出变成answer 那如果看emotion的tag的话 这是一个正确的标註
              
                  24:00
                  所以代表模型比较能听懂语音里面的情绪了 但是模型再也输出不了这一声format 你发现再怎么放他 他都输出不了这一声format 他已经忘了到底什么是这一声format了 所以我这边举这么多例子 就是要告诉你说 Post training最大的挑战是什么呢 最大的挑战是模型会遗忘它 你有的技能 通常我们做Post training的时候 你期待模型不只学会新的技能 而且可以把新的技能跟旧的技能 融合起来 但往往事与愿违 这个人工智慧呢 就像左边这个示意图一样 新的知识进去之后 旧的知识就掉出来了 所以他往往会变成 他指挥你教的东西 其他能力就坏掉 这个现象叫做 Catastrophe Forgetting 那为什么会有Catastrophe forgetting这个现象发生呢 其实也非常的直观 因为我们在做post training的时候 你只教模型
              
                  25:03
                  单一目标 比如说你现在想要练一个 特别能够写程式的模型 你就是找一大堆 leakhole的题目来逼他一直刷题 一直刷题 程式能力就会越来越强 但是你只教他刷程式的能力 你没有在意他其他的能力 变化怎么样 你在做post training的时候 你只要求他程式的能力 要越来越强 其他能力变成 变成怎么样 你是完全不管的 就很容易的 破坏了其他的能力 当然这个 这个CAT TROPHY FORGETING 这个问题对你来说 多重要 其实取决于你的应用 假设你并不在意一个模型 只有程式能力 其他能力都是差的 比如说 他可能会说出不该讲的话 他可能会突然冒出脏话 你觉得也不在意 反正他只要能写程式就好 那可能catastrophic forgetting 也不是非常大的问题 但是因为今天大家 通常期待你手上有的 是一个通用模型 那些特别擅长写程式的模型 他其实也都听得懂人话 他也不是只能写程式而已 你还是可以用人话跟他沟通的 我们今天期待人工智慧 他的能力其实是比较全面的 虽然他有各自擅长的领域
              
                  26:05
                  但是他基本上还是有一些全面的能力 所以catastrophic forgetting 就会变成一个很大的挑战 那有人可能会觉得说 模型会有catastrophic forgetting的现象 是不是因为模型不够大 参数不够多 因为参数不够多 所以才会学了新的东西 就忘了就有东西 看起来根据文献上的结果 可能不是这样 因为这篇论文呢 已经做了不同模型大小 跟catastrophic forgetting 之间的关系的比较 他们发现说比较大的模型 FORGETING的状况 并没有比较轻微 不过这篇paper是只做在 1B到7B的模型上啦 那至于更大的模型会怎么样 还有带这个更多的研究来探讨这件事情 总之并不是模型越大 就越不会forgetting forgetting的现象不一定跟模型大小有关系 好那另外一篇paper发现说 forgetting的现象跟什么东西最有关系呢 跟你在目标任务上面做得有多好
              
                  27:13
                  往往有非常直接的关系 在这篇paper上面左右两个图 代表他们教模型两个不同的任务 那横轴是什么 横轴是 fine tuning loss 反正你就记得说 越往右就代表 模型在目标任务上面 学得越好 那至于目标任务是什么 就是看你今天 post training的时候 想要教他什么 纵轴呢 纵轴代表 模型遗忘的程度有多严重 那这边每一个点 就代表一个模型 那你可以很明显的看到说 这几乎就是一条斜直线 也就是模型在目标任务上学得越好 它遗忘的情形就越好 越严重 这边不同的点有不同的颜色 它是什么意思呢 这边其实是不同大小的LoRA 如果你知道LoRA是什么的话 你知道LoRA有一个可以调的参数叫做RANK RANK设的越大 代表LoRA这个ADAPTOR里面的参数量就越多 所以这边不同颜色的点代表RANK的大小不一样 也就是LoRA的参数量不一样
              
                  28:18
                  那通常LoRA参数量比较小的时候 那你会发现这些点就聚集在左下角 LoRA参数量比较多的时候 就聚集在右上角 所以你会发现LoRA并不是真的能够很好的解决forgetting的问题 当你加了LoRA以后 你可能会觉得forgetting的问题没有那么严重 但你得到的交换可能是 模型学的东西比较少 让模型学的少一点 遗忘的就少一点 那你想让他学的多一点 他遗忘的就多一点 所以这不能够说是彻底的解决了 forgetting的问题 模型没有forget你只是因为 你学的东西比较少而已 那另外一篇论文从标题 你就可以知道他想要讲什么 他说LoRA learns less and forget less 这边论文里面就讲说 很多人发现加上LoRA以后 你forget的现象就少很多 但你付出的代价是什么呢 你付出的代价就是 模型实际上学到的东西是比较少的 那在这个投影片上面呢 纵轴代表的是现在目标任务的能力 左边这张图呢
              
                  29:24
                  是把模型评量在Human Evail 这个Corpus上 Human Evail是那个写程式的Benchmark 所以纵轴代表模型的程式能力 右边这张图的纵轴代表模型的数学能力 GSM8K是一个数学的Corpus 代表模型的数学能力 那横轴呢 横轴是拿来检测模型遗忘的程度 那在这篇论文里面 他们所谓的遗忘程度是说 他们把模型呢 在三个不同的任务上面做测试 然后在这三个不同的任务上做平均 那如果这三个任务 平均起来的正确率越低 代表模型遗忘的状况越严重 因为这三个任务是模型本来就会解的 那如果正确率越低 代表模型遗忘的状况越严重 那黑色这条线呢 是for fine tuning的结果 这边每一个点呢 代表是一个模型 那串起来代表的是训练的过程 最开始训练的时候模型在这里 那随著训练的时候模型的这个表现呢 就往左上角 所以我们可以看到说 随著训练的进行 模型的程式能力当然是越来越强
              
                  30:28
                  但是同时 原来本来就有的能力 也就越来越弱 也就是他开始逐渐遗忘 他本来就会的技能 那如果你看LoRA 这三条线代表是LoRA RANK不一样 就是LoRA这个adapter里面的参数 是不一样多的 整体而言 LoRA forget的状况轻微很多 那这个轻微很多 是用什么东西换来的 是用比较差的程式能力 换来的 那右边这个图也是 黑色这条线代表的是 4-5-2 那随著训练的进行 那你会发现说在数学能力上 是先升后降 这个就是overfitting 就你一直教他数学的题目 那你测试题目跟训练题目毕竟是不一样的 所以一开始在测试资料上 正确率会上升 但接下来还是会慢慢掉下来 但是你会发现说 随著训练的进行 模型遗忘的程度是越来越严重的 那如果你看到 那Laura遗忘的程度 就比较少 那这个比较少的遗忘程度 就如同我刚才说过的 是用比较差的数学能力换来的
              
                  31:32
                  是用学比较少东西换来的 当然你可能会想说 那还有其他这种regularization的方法 可能也可以防止forgetting 因为很多regularization的方法 会让你训练完的模型比较robust 会让你训练完的模型跟原来的模型比较接近 也许这些方法可以阻挡住forgetting 那在这篇论文里面 他们也做了一些分析 我们就看右边这个图 就除了fine tuning 还有除了for fine tune 就微调整个模型的参数 跟LoRA以外 他们还试了 这个都是大家耳熟能详的技术 比如说dropout 还有weight decay 那他们发现说 其实用LoRA 还比其他的方法 还要更能防止forgetting 上面这一条这个 这个虚线呢 代表的是原来模型的能力 这两条线呢 代表的是LoRA 然后这个数值越低代表模型 forget的状况越严重 而其他方法 比如说抓爆或者是
              
                  32:35
                  他们也是没有办法挡住forgetting这个方 他们也是没有办法挡住 forgetting这个问题的 好 所以我们今天知道说 Post training 就像是给人工智慧 为了大脑动手术 手术 蛮容易成功的 但你很容易遇到的状况就是 Catastrophic forgetting就像是 手术成功 病人却死了 你focus在一件你要做的事情 你把病灶除掉了 然后你以为你的训练成功了 在你训练完发现 模型除了你要教他做的事情以外 其他能力都不好死了 就好像手术成功 病人却死了 而这是我们要避免的状况 好 那我们要 要怎么避免forgetting的状况呢 其实在古代 就已经有相关的研究了 现在我们要搭乘 时光机回到2019年 2019年 不只没有GPT 也没有GPT3 那个时代唯一有的东西 就是GPT-2
              
                  33:39
                  这个是人工智慧的 旧时期时代 其实早在2019年的前一年 就已经有人提了一个构想 这个构想是 能够用一个模型 解决这里 十个任务 那他们把这个计画 命名为Natural Language Decason Decason就是十项全能 铁人十项的意思 他们那时候想要问说 有没有模型能够解这十个问题 当然今天大家都知道说这个有什么难 这个不就call个check GPT都是可以解的吗 比如说叫模型 做这个翻译 给他一段文字叫他翻译成德文 就翻译 做摘要给他一篇文章 说这篇文章摘要长什么样子 他就把摘要写出来 或你也想要做情感辨识 你给模型一篇文章问他说 这篇文章的评论是正面还负面的 他就告诉你是正面还负面的 这些任务对今天的语言模型来说 根本就不足挂此 但是在2018年有什么样的方法 可以用一个模型
              
                  34:41
                  一次解释十个问题并没有人知道 那在这篇文章里面 他其实提供了一个baseline 他们自己搭了一个模型长这个样子 那个时候的模型就是很复杂 那就是有很多不同的block 然后那时候相信说 这种比较复杂的组合 可以解复杂的任务 那他们就是用这个模型 来解这10个任务 在2019年的时候呢 我们就在想说 有没有办法直接 用一个语言模型 就回答这边所有的问题呢 这个是 是孙凡根同学,那时候他是大学生,还有研究助理何正豪同学做的,那时候构想就是这一些自然语言处理的任务,他们都有一样的格式,就是会先给模型看一段文字,那段文字当时叫做context,接下来你问他一个问题,他得输出一个答案,也许有办法直接用语言模型来做这件事情,就让语言模型直接读context,读问题,然后接下来 来给一个代表answer的token,他就开始把答案接出来,直到他接到end of sentence为止,他输出答案就停止了,好,不过因为当时啊,就算有GPT-2,那个GPT-2呢,就是废的跟垃圾一样,他是没办法直接回答这些问题的,所以需要做一些post training,需要微调GPT-2才有办法做刚才的投影片里面看到的任务,比如说假设你想要叫GPT-2做阅读测验,那你得先在一个
              
                  36:16
                  叫做SQuAD的Corpus上面 先训练GPT-2怎么做阅读测验 SQuAD Corpus里面的问题都是长这个样子的 就是有一篇文章 然后有一个问题 然后有一个答案 那我们就是教语言模型说 读这篇文章 读这个问题 接下来你就要吐出这串答案的文字 一劝下去 GPT-2得到75.5%的正确率 这个正确率到底是高还是低呢 在2019年 这个正确率其实也不能说是非常好 因为你看这一个leaderboard 这个是SQuAD那个benchmark purpose的leaderboard 那这边有显示时间在2019年的时候 那时候就有很多模型可以达到80几%的正确率 那其实在2019年的时候 SQuAD这个benchmark呢 早就已经被破台了 因为人类的正确率是86% 模型可以得到87%的正确率 没有办法再更高了 但是呢 就算是只看到75%的正确率 当年我也是非常惊讶
              
                  37:20
                  为什么会非常惊讶呢 这些榜单上的模型 他们并不是让人工智慧 并不是让模型直接产生答案 而是在文章里面找一段文字当作正确答案 因为挂这个任务的基本设置就是 答案一定出现在文章里面 答案出现的字一定在文章里面 找得到一个一模一样的 的段落 所以模型要做的事情 其实并不是真的写出答案 他真正要做的事情是 从文章里面找出哪一个句子 或者是哪一个片语 可以当作答案来使用 但是当时的语言模型做的是 远比SPA要求他做的更难的事情 他可是直接输出答案的 在做这个实验之前 我根本不相信语言模型 可以直接读一篇文章 问一个问题 就直接产生答案 所以看到这个结果的时候 当时其实是惊呆了 就是这个史前时代的人 发现说可以用火 这个真的是惊呆了 但现在你觉得 你一定是觉得没什么啦 但是在石器时代的时候
              
                  38:23
                  就突然发现 哇 这个有火可以用啊 有火这种东西啊 真的是吓了一大跳 而且当时我觉得我们低估了GPT-2的能力 为什么当时我就知道 我们低估了GPT-2的能力呢 因为很多时候GPT-2的答案是这样的 正确答案可能是英文的70 他回答了70 那这样你要算他对还是错 按照挂的标准要算他错 因为要答案一模一样才能够算是对的 但很多时候模型得到的其实是 同样意思只是不同的说法而已 因为他并不从文章里面直接拿一个答案出来啊 他是按照他的意思写一个答案出来 明明意思是对的 但我们就算他错 我们其实低过GPT-2 他实际的能力 好 那除了做阅读测验以外 我们还做了很多其他的任务 比如说教他做情感分析 或者是教他产生SQL的 的指令等等 所以我们就可以用同样的模型 就是一个语言模型 来打那个Natural Language Decasome里面的十个任务 这个是我们用GPT-2得到的正确率
              
                  39:28
                  然后Other Score是之前的文献 在Natural Language Decasome那一个比赛里面 可以得到的分数 那时候非常神奇的是我们发现说 就算我们用GPT-2只是一个模型 一个简单的语言模型 在各式各样不同的任务上 居然都可以得到还不错的结果 所以那时候就可以感受到语言模型 真的能力非常的强 非常的有潜力 然后那时候我就有一个想法 因为我们没有办法收集 一下子就收集到 所有自然语言处理相关的任务 但我们能不能够每次收集到一个任务 就拿去微调语言模型 一开始教他做阅读测验 接下来叫他产生SQL的指令 接下来叫他做 情感分析 接下来叫他做Semantic Role Labeling 一路教下去 每次收集到一个新的NLP的资料的时候 就教模型一个新的能力 这样几年之后他就会变成天网 后来我们发现要做一个天网并没有那么容易 为什么呢
              
                  40:31
                  因为当我们教模型新的任务的时候 他非常容易忘掉 他本来就已经会的技能 我们来看一下模型在SQuAD上面的表现 这个纵轴是在SQuAD上面的 正确率 那我们现在观察的是只观察SQuAD这个任务的表现 当我们在教模型SQuAD的时候 教他做阅读测验的时候 当然他阅读测验的能力会越来越好 但一旦我们教完阅读测验 开始教他产生SQL的指令的时候 你会发现他的performance突然开始暴跌 这边蓝色的这一条线 代表的是一般的fine tune 就是微调模型教他去产生SQuAD的指令 那一旦他学会做SQuAD指令以后 他突然就做不了阅读 那我们也试著做了一些比较进阶的regularization的方法 我们这边试了一个叫做MAS的方法 在19年的时候 这是一个很好的regularization的方法 专门针对forgetting的问题设计的 但我们做在语言模型上 Math居然比fine tuning 结果还要更差一点 它是橙色这条线
              
                  41:35
                  接下来继续教模型 做情感分析 更差了 更不能够做阅读测验 但神奇的事情是我们发现 如果我们再继续教模型 做Semantic Row Labeling 这边这个Semantic Row Labeling 并不是传统的Semantic Row Labeling 它也有点像是阅读测验 这个Benchmark是把Semantic Row Labeling 转成一个有点像阅读测验的模式 就你给它一个句子问它说 这句话里面有出现 什么样的人物 所以它其实有点像是阅读测验 那我们发现教模型 这个SIL之后 阅读测验的能力又回来了 然后再教他别的任务 这个应该是教他做一个对话相关的任务 这个阅读程的能力又掉下去了 所以模型本来学到的能力 他会上上下下非常的不稳定 他本来已经学到的能力非常容易失去 但是当时我们观察到 好像又蛮容易被换回来的 就感觉他并不是遗忘 他只是不想起来而已 这些能力就藏在某个地方
              
                  42:38
                  你有办法把他召唤回来 但是他又很容易就不知道 跑到哪里去了 好,所以看起来呢 你要一直教模型新的能力是不容易的 因为他很容易就遗忘 本来就有的能力 但是其实在2019年的时候 那个时候我们就已经有一个解法了 这个解法叫做experience replay 这个experience replay的解法 在由我们这边论文之前 其实就已经在其他领域 比如说computer vision上面 有人尝试过并发现非常成功的结果 只是当时还没有人试在 大型语言模型上面而已 那我们就在大型语言模型上面尝试这个方法 那这个方法其实非常的直觉 他想法是这样子的 你先拿任务一的资料 教模型然后他会了任务一 接下来你要再教他任务二的时候 不要只拿任务二的资料 你要混一点任务一的资料 混多少任务一的资料呢 我们发现不用太多 大概任务二的5%左右的资料 就非常非常足够了 不需要混太多资料
              
                  43:41
                  因为我刚才讲过说 模型呢 他的遗忘非常的神奇 他感觉不是真的遗忘 他只不想想起来而已 所以他那些知识好像就藏在某个地方 你只是需要一些契机 把他唤醒而已 所以其实不需要太多过去的资料 大概现在当下这个任务训练资料的5%左右 就非常足够了 我们就用了这个experience replay的方法 那我们得到的是上面这几条线 那这边为什么会有很多条线呢 等一下会再跟大家剖析 我们就是experience replay这个方法 然后我们发现 它是一个可以有效防止模型遗忘的方法 所以在2019年的时候 那个时候我心里得到的结论是 catastrophic forgetting 不是一个真正的问题 这个问题太容易解决了 因为只要保留有一些过去你训练模型的资料 保留一些不用太多一点点就好 你就有办法在接下来的训练里面 防止模型遗忘就有的技能 所以看来cat trophy forget it 并不是一个很严重很难解的问题
              
                  44:46
                  好 知道这些以后 我们就回到这个现代吧 这个现代是有很多人工智慧的 比如说Gemini 比如说Claude 比如说DC 我在画图的时候 这个就是用GPT-4o绘图的功能绘的 我有叫他把GPT-4o写上去 但不知道为什么 是不是因为他很谦逊的关系 觉得自己不是一个人工智慧 他就没有把自己的名字写上去 但有可能这边也就有GPT-2 他以为他写了 所以他没把自己的名字写上去 这些Logo呢,也是他自己画出来的,我觉得还蛮像模像样的,如果你比较原有这些模型的Logo的话,我们就回到现代,好,回到现代,怎么解决Ketotropy Forgetting的问题呢? 如果你想要教模型中文,就发现教完中文以后,他就忘记了Safety Alignment的能力,那怎么办呢? 那根据我们在2019年就已经知道的事情,那你要拿一些训练Lamma to Chat的训练资料拿来做 Experience Replay,问题就解决了 但是等一下,你根本没有Lamma to Chat的训练资料啊 现在这些大公司都只释出模型
              
                  45:51
                  他们已经不释出训练资料了 你根本没有办法拿那些模型的训练资料来做Experience Replay 所以Ketotropy Forgetting是一个真正的问题 但是,其实在2019年的时候,面对这个状况 我们其实也是有Solution的,所以我们就再次回到2019年 在2019年的时候,那时候在论文里面,我们多加了一个额外的情境 就是假设我们找不到过去的资料的话,应该要怎么办呢? 当时要设定这么复杂的情境只是为了要上个顶会啦 方法太拿衣服的话,没有办法上顶会 但是那时候我心里觉得,这根本不是一个实际的setup 怎么可能我会拿不到过去的训练资料呢? 当然现在是一个蛮实际的setup setting就是了 好,现在假设我们拿不到过去的训练资料的话 那应该要怎么办呢? 那时候我们就想到一招 这招是这样子的 我们在教模型任务一的时候 我们教他看到这个context
              
                  46:55
                  看到这个问题 你要回答答答案 但是因为他是一个语言模型 我们实际上训练他的时候 就是给他这一整个sequence context问题跟答案 给他这一整个sequence叫他拿去做 文字接了 所以实际上有可能 我们直接叫这个 训练完任务一的language model 随便讲什么的时候 就给他一个begin of sentence的token 然后叫他随便讲什么都行 他可能就会先产生一个context 再产生一个问题 然后再自问自答产生一个答案 那这样我们不是就有 过去的训练资料了吗 虽然我实际上并没有把过去的训练资料存下来 但我们可以从已经训练完的模型 想办法去生出过去的 训练资料 那这件事情真的可行吗 我们试了一下 还真的可以 我们就把训练在这个SQuAD上面的模型 叫他吐一些东西出来 他就会先吐一篇文章 这篇文章就是讲说 美国入侵阿富汗 那有非常多的牺牲
              
                  47:57
                  有1600个美国军人牺牲 总共有一万美国军人上升 听起来是一个非常大的数字 然后接下来问你说 这次冲突的目标是什么 答案是阿富汗 这大串文字都是GPT-2 自己生出来的 他先生一篇文章 再问自己一个问题 再自己产生这个问题的答案 在这个旧时期时代呢 我看到这个结果的时候真的是惊呆了 这个文章会不会是真的存在的 他只背了一篇他看过的文章而已 也许挂里面有一模一样的文章 但我后来仔细看要发现说 这是一个假新闻里面讲的数字 其实都是随便乱讲的 它并不是一个真实存在的新闻 或这边有另外一个例子 这个例子就是 在1856年的时候 这个卡达菲的家族 他们到了埃及 然后在隔年呢 军队从这个利比亚 撤退然后回到了班加西 然后问说呢
              
                  48:59
                  格达菲的军队 为了谁回到班加西 那这题的答案是格达菲的家族 这个答案对不对 不好说啦 从这个文具里面看不出来说军队撤退 是因为格达菲家族的关系 但反正他自问自答的问题 跟答案就是长这个样子 那这个新闻 这个看起来很像是Wikipage里面 会有的内容的东西 看起来像模像样的 但是他完全就是一个错误的资讯 因为格达菲根本是20世纪的人 这边的年代通通都是鬼扯的 但是那个时候 语言模型就可以产生出一些 像模像样的文章 还能够自问自答 这个旧时期时代人类来说 真的是太惊人了 但我知道你今天会觉得说 语言模型不是本来就应该这个样子吗 但在2019年的时候 看起来并没有那么直觉 好 那我们现在知道说 我们可以让语言模型自说自话以后 就产生出他之前看过的训练资料 所以我们在教模型第二个任务的时候 怎么拿到第一个任务的训练资料呢
              
                  50:02
                  你就把你的要post training之前的那个foundation model拿来 然后呢你就叫他自说自话 叫他自己产生一些句子 把他产生出来的这些句子 当作代表任务一的训练资料 加到任务二的训练资料里面 然后就可以避免cat trophy forgetting的状况 所以在刚才这张图表里面 最上面的两条线用的是真正的任务一的资料 下面的这几条线 其实用的都是GPT-2自己生出来的资料 那为什么会还有这么多条线呢 其实我们试著用不同的方法,然后来让语言模型生资料啦,那这个细节,大家去再去看原始的论文,所以当时我们知道说,如果你可以拿到一些旧有的资料,而这些旧有的资料可以是语言模型自己生出来的,那你可以避免避免遗忘的状况,有另外一个小插曲是当初这篇模型呢,当初这篇论文呢,投稿到ICLR 2020,本来文章的标题是language model is all you need for lifelong language learning,你知道,我这个看起来现在是一个非常老的 老套的标题的取法,但在2019年的时候还可以感觉没有那么老套,那时候觉得说你看language model可以解10个任务,而且用language model自己呢,就可以达到lifelong learning,让他一直学新的东西,可以避免避免遗忘的状况,所以language model is all unique这样子,那时候reviewer就觉得蛮生气的这样,reviewer蛮生气的,他觉得language model不可能可以解各式各样的问题,所以为了让reviewer高兴,双膝一软呢,就直接把all unique那几个字拿掉,他就上了ICLR 2020
              
                  51:36
                  不过在现在这个时间点,如果有人讲说所有NLP的任务都可以用language model来解,我想你其实也不会特别反对就是了 好,那我现在呢,再回到2025年,好,在2025年,其实我刚才2019年讲的那个讲法,仍然是一个非常主流的,避免forgetting的方式 举例来说,这个23年,有一篇论文呢,叫做Safety Tune LLaMA,因为他们发现说LLaMA fine tune之后,很容易失去Safety Alignment的能力,那怎么避免它失去Safety Alignment的能力呢? 非常的简单,就如果你只拿一般的资料来fine-tune language model,那你往往得到一个unsafe的model 但如果你可以保留一点点,在他们论文里面写3%的Safety Alignment的资料 那这种Safety Alignment的资料通常就是你跟模型讲说 我怎么杀一个人,然后他就说我不可以教你做这种事情,你只要保留一点点这类的资料,混到你现在要做post training的资料里面,你就可以保有原来模型safety alignment的能力,结果刚才讲的experience replay,其实是一样的做法,或是有另外一篇论文叫做self-synthesize rehearsal,那这是24年的论文,他做的事情就是,我过去呢,在做这个post training的时候,我们现在都知道,要混一个
              
                  52:59
                  过去的资料,他这边叫做rehearsal的data,把过去的一些资料,混到新的资料里面一起去做训练,可以避免避免遗忘的问题,但是有时候我们有可能会拿不到过去的训练资料,怎么办? 你看这都是羊驼了,代表说这是LLaMA系列的work,就比较新的work,我们今天有可能拿不到过去的训练资料,怎么办? 那我们可以让LLaMA自问自答,产生一些他过去看过的训练资料。 有办法让LLaMA自问自答产生类似他过去看过的训练资料吗? 是有办法的,做这件事情最知名的一篇文章叫做Magpie,Magpie是喜鹊的意思 这个方法是这样子的,怎么让LLaMA产生看起来像是他之前训练过的资料呢? 你就先给LLaMA一个代表user的token,就LLaMA在使用的时候,你会先给他一个代表user的token,然后问他一个问题,然后再给他一个 代表assistant回答的token,然后他在做文字接龙进行回答,那通常user的问题是你自己给定的,但这边他们只给LLaMA代表user的这个符号,然后让LLaMA继续去做接龙,他就会自己产生一个问题出来,就产生一个问题出来,然后接下来你再把user的token,LLaMA自己产生出来的问题,后面再接代表assistant,代表AI的token,LLaMA就会把自己问的问题的答案产生
              
                  54:29
                  他就自问自答,自己产生一个问题,自己产生一个答案,接下来你就有了疑似LLaMA-3训练的时候,用的这个训练资料,instruction fine tuning的资料,你就把它加到你的原理,你要做post training的资料里面,就可以避免forgetting的现象发生,所以当时2019的方法,在今天这些新的模型上面,仍然是适用的。 好,那我们刚才是介绍了这个experience replay的方法,那我们也介绍了 到了pseudo experience replay的方法 也就是说过去的经验 不一定是真实的资料 它可能是foundation model 自问自答产生出来的 那其实还有很多类似的变形 但概念都非常的像 比如有一个方法叫做paraphrase的方法 这个paraphrase的方法就是说 我们在训练的时候 不要直接拿正确的答案 不要拿人写的答案来训练模型 那要怎么做呢 把用foundation model改写人写的答案 跟foundation model说把这句话换句话说
              
                  55:35
                  然后用换句话说的答案 来当作正确答案来训练模型 那这个跟experience replay的概念 其实有很多类似的地方 因为可以想像说现在的答案 是模型自己产生出来的句子 它更接近模型之前看过的训练资料 它某种程度上就代表了模型之前看过的训练资料 那这招有没有用呢 刚才我们已经看过这边paper的上半部 我们刚才在这个课程的开始的时候 给大家看了这个表格的上半部 这是一个去年年初的文章 那下半部就是他们用改写的方法 改写了这些训练资料里面的答案 让这些答案用我们现在的foundation model来改写 他就发现说在所有的状况下 用改写的答案来训练模型 其实比较好的 在这边的九个状况下 只有一个状况 用改写的答案会比较差 其他状况用改写的答案结果都是比较好的 除了改写答案之外 还有另外一个方法叫做self output
              
                  56:39
                  self output的方法就是 那我们乾脆直接让foundation model来产生答案 我们把问题丢到foundation model里面 让foundation model来产生答案 但是foundation model有可能会答错 那我们要有一个方法 那这个方法就要看你要怎么设计 去检测这个答案是不是对的 那如果今天是那种数学的问题 你就有正确答案 你可以直接对答案看对不对 或如果是那个程式的问题 你可以直接过compiler 看compiler有没有error 所以在某些情况下 你蛮容易检查一个答案 是不是对的 如果他是对的话 如果今天foundation model输出的答案 其实是对的 就拿foundation model自己的答案 来训练自己 那除非他答错了 他答错了 我们采用人写的答案 来训练模型 但这边可以有很多变形 比如说你可以说 也许foundation model的能力不够强 他没有办法第一次就答对 那我们让他产生十个不同的答案 因为同一个模型 你每次sample答案都不一样嘛 让他产生十个答案 我们就只挑对的那一个 来当作正确的答案训练
              
                  57:43
                  这个跟paraphrase 还有experience replay 都是非常类似的方法 就是我们需要混一些 foundation model 自己产生出来的资料来做训练 这样可以避免forgetting的状况 那你会发现讲到 到目前为止啊 我们有提到 trans style的post training 有提到SFT style的post training 但我们一直没有讲到 RL based的post training 为什么还没有讲到 RL based post training呢 你仔细想想 如果我们用RL的方法来训练模型 是不是其实就跟self output这个方法 非常类似呢 你想想看RL是怎么做的 RL并不是直接强制提供答案给模型的 RL是 产生一些答案 如果这个答案是对的 就提高他的机率 错的就降低他的机率 这跟self output其实非常的类似 因为self output是 如果今天foundation model 可以得到正确的答案 他就拿来做training 就等于是提高了他出现的机率 那唯一不同的只是
              
                  58:45
                  有没有把错的答案降低机率而已 所以RL-based的post training 其实跟self output非常的像 所以我认为RL-based的post training可能是一个比较能够防止 是forgetting的方法,这可能就是为什么你发现说在训练语言模型的时候,往往RLBase的方法是放在最后一个阶段里面,或者今天有很多人用RLBase的方法来强化模型reasoning的能力,他们可能都没有特别讨论这个forgetting的问题,有一个可能是RLBase的方法,因为他跟self output非常的像,他其实是一个特别能够防止forgetting的技术,那是不是真的是这样,我还要上代更多的研究来回答这个问题。 好,那我们来看一下 self-output的表现怎么样 这边有一篇paper叫做 selective self-rehearsal 那其实这个方法就是self-output的方法 这个prompt指的就是 foundation model的表现 他把foundation model呢 测试在四个不同的课本上 NLU,这个是 truthness QA
              
                  59:47
                  这个是数学的问题 这是HellaSwag,HellaSwag就测试模型的 那个common sense 那SFT呢,代表是一般的fine tuning 做完一般的fine tuning 以后,这边这个数值代表 模型的正确率掉了多少 做完一般的fine tuning以后 模型在这些任务上正确率 都是暴跌 但是如果你是做SSR,就是我们刚才讲的 self output的方法 用模型自己的话 如果他今天可以正确回答这个问题的话 就用他自己的答案来训练模型 你会发现说 可以让这个forgetting的状况 变得非常的轻微 如果这边生成答案的模型 不是我们要训练的这个 Foundation Model 其实也有帮助 这边这个Foundation Model 换成其他的Model 其实也有用 为什么你可能会想要把这个Foundation Model 换成其他的Large Language Model呢 因为有可能你本身的 Foundation Model实在太弱了 他根本没有办法回答任何问题 如果他根本没有办法回答任何问题的话
              
                  1:00:50
                  那你拿这个Foundation Model 来生答案 你可能一直都生不出正确的答案来 就很难使用Self Output这个方法 所以有时候你可能会想说 那如果我这边不是用人类生的答案 但是是用另外一个比较强的 language model生的答案 有没有帮助呢 有一篇paper叫做 I learn better if you speak my language 你从他的标题就可以知道他想要做什么 他就比较三个case 一个是我们训练的时候 用人准备的正确答案 另外一个是用GPT-4的答案 另外是用Claude的答案 他训练的对象有两个比较弱的模型 一个是Mistral,一个是LLaMA,那他们设三个case,一个是教他数学,这个也是教他数学,这个是教他在ECQA这个corpus上面训练模型,然后接下来测试的时候是把模型测在GSM8K,math那个algebra,还有ECQA这三个corpus上面,那红色代表performance特别差的,performance特别差的就标红色,那你会发现红色呢,通常是出现在使用人类的, 人类资料作为正确答案的时候,你发现用人类资料来教模型,反而模型会学得比较差,它比较容易遗忘它本来就有的技能,还不如让其他的语言模型来教你现在的模型,那可能是不同的语言模型,他们虽然不是同个模型,但他们讲话就是比较像,所以用语言模型的答案来教语言模型,反而可以学得更好,那这篇文章还有发现说,呃,如果你有很强的语言模型,还是有一些
              
                  1:02:24
                  一些状况下,模型会有非常大的forgetting的状况发生,比如说他们这边是把模型测试在,就如果你拿GPT-4的答案来训练模型测试在human eval写程式的任务上,他们发现模型的表现很差,可是你现在自己的模型又太差了,怎么不足以得到正确的答案,怎么办呢? 所以他们这边就用了一个minimum change的方法,minimum change的方法是说,先拿自己的模型产生答案,按自己的模型答案, 可能会错很多,接下来你再拿GPT-4修改跟GPT-4说,这边有一个可能有错的答案,你只把错的地方改掉,但是内容要越像越好,所以如果用GPT-4来修改你自己模型现在foundation model的输出的话,可以得到比原来用GPT-4当作答案还要更好的结果。 这边再多举一个例子,我们刚才有讲过说呢,如果你直接教模型听语音,那很容易伤到原来的。 原来文字模型的能力,那同样的这种self output的概念,也可以用在教模型语音上面,怎么做呢?我们现在要训练一个模型,它可以输入语音,给一个文字的指令,得出正确的输出,那我们现在在训练模型的时候,要尽量用模型自己的话来当作答案,要尽量教,我们在教模型的时候,要尽量用模型自己的话来当作答案,那我们这边
              
                  1:03:54
                  怎么样得到模型针对这个问题自己的输出呢 因为原来的文字模型它是完全听不懂语音的 但你有可能你的声音讯号是有一些标註的 那你就想办法把这段声音讯号 尽量用文字描述出来 就把这段声音讯号里面各种语音的特征 用文字来告诉文字模型 你告诉他说这段话长度多少 告诉他讲语者的性别 告诉他这句话的情绪 这句话的口音 把这些资讯都丢给一个文字模型 接下来你给文字模型一个指令 比如说what can you hear 然后模型就会好像 这些文字模型虽然实际上不能听语音 但你给了他一段文字来代表语音 他能读这段文字 他就会好像他听到一句话一样 产生一个输出 接下来你在训练自己的语音版模型的时候 就把文字模型的输出当作目标 一样问语音模型what can you hear 要求他输出的答案跟文字模型 越接近越好
              
                  1:04:57
                  那用这样子的方法 你就可以有效避免模型 遗忘他原来作为文字模型的时候 就有的能力 那现在很多语音模型其实都采用 类似的方法来训练 那这边列举了几个比较知名的例子 比如说BOSP 我们实验室跟NVIDIA合作做的 Desktop2还有DVA等等 这些模型都使用这样的方法 那我这边呢 想特别分享一下我们实验室 卢克涵同学跟NVIDIA的研究者 人员做的 的方法跟成果 这个模型在训练的时候 实际上我们只给了 他一个instruction 我们只教他怎么回答 what can you hear 我们只教他这个instruction 我们没有教他更多东西了 但是凭藉著文字模型 本来就有的generalization的能力 我们发现就算只在教他语音任务的时候 教他what can you hear 在测试的时候 你居然可以问他任何问题 你问他任何问题 他居然都是能够回答的 所以就算设施的时候
              
                  1:06:02
                  这些text instruction 是他训练的时候根本没有看过的 他居然是有办法回答的 好那为了要验证说这个模型 是真的能够回答 各式各样问题的 我们把它evaluate在一个 叫做Dynamic-SUPERB的benchmark 那这个Dynamic-SUPERB呢 也是我们实验室做的 是我们实验室的黄建佑同学跟CNU的心机瓦塔纳北教授合作的一个work 好那我们这个work做的内容是这个样子的 就在这个benchmark里面 他是为了要全面评估这些语音版语言模型的能力 那在这个资料集里面 每一笔资料都是一句话 一个指令跟一个正确答案 那指令可能是请告诉我这句话的情绪是什么 那正确答案就是happy 或请告诉我这句话里面 现在有几个人在讲话 那答案就是2 或者是给他两段声音接在一起 问他说前后两段声音是不是同一个人讲的 他就要回答yes或no 那这个Dynamic-SUPERB的第一个版本呢 是发表在去年的iCasper 那里面总共有55个不同的任务
              
                  1:07:08
                  那Dynamic-SUPERB呢 其实有一个Phase 2 那Phase 2呢 是黄建佑同学 跟CMU、CG瓦塔纳北教授的团队 还有UT Austin、David Howard教授的团队 所一起打造的一个Benchmark 那我们有180个任务 我们把任务呢 建立了一个树状的结构 就是这么复杂 除了有语音相关的任务以外 也有音乐跟声音相关的任务 那语音相关的任务里面 有子类别 子类别下面有子子类别 子子类别下面有子子子子子类别 子子子类别下面有子子子子子子类别 所以是一个非常庞大的Benchmark 从各个不同的角度来Evaluate 一个spoken language model能不能做各式各样不同的事情,好,那这篇文章呢,其实是发表在今年的ICLR,好,那我们呢,把Data2呢,Evaluate在Dynamic-SUPERB的Phase 1上面,那这边每一个column就代表说一个语音版的语言模型,在各个不同面向上面的正确率,那这边每一个column不是一个任务,它是多个任务的平均值,那 是所有任务的总平均
              
                  1:08:23
                  那这边是把desktop跟其他的模型做了一下比较 跟当时我们可以找到的表现的比较好的模型做一下比较 那你会发现在Dynamic-SUPERB上整体而言 desktop是比其他模型还要更好的 而且desktop用的训练资料其实是远比其他模型少的 代表说如果你可以好好防止forgetting的现象 用比较好的方式来训练模型 你其实可以用少量的资料就训练出来 出一个还不错的语音版语言模型 那像这种self-output的方法 现在是一个防止forgetting 非常常见的方法 所以如果你今天要自己fine-tune模型 我知道常常有同学会选自己fine-tune模型的需求 这是一个你需要考虑的方式 那除了这种self-output的方法以外 还有其他可能的思路 那这篇呢是Appier的研究人员 吴兆聪同学的论文 那首先呢 他们先观察了self-output的结果 跟正确答案之间有什么样的差异 那他们发现说这个是一个数学的问题
              
                  1:09:30
                  光truth是人写的答案 self-output是模型自己产生的答案 那最后人类跟模型都得到正确答案 正确答案是负四分之三 但是如果你去计算这里面每一个token 你用现在的foundation model 这边foundation model是LLaMA 38B 你用现在的foundation model产生 产生出那个token的机率 你会发现说 对于光tube而言 里面有比较多token 是对于你的foundation model来说 比较难产生出来的 那这边把比较难产生出来的 token 你就算出来机率比较低的token 标上红色 那你会发现说在光tube里面 有比较多的token 是你的foundation model比较难产生出来的 而在foundation model自己的output里面 当然这是foundation model自己的output嘛 都是foundation model产生出来 机率比较高的token 这只是一个例子 也实际上做了一些数值上的结果 在这个 mppp这是一个程式的资料集 跟数学的资料集
              
                  1:10:35
                  都比较了正确答案 跟paraphrase 还有self output 这三个方法的 这个publicity publicity就是你的foundation model 产生这个句子的机率 那这个数值越大代表你的foundation model产生的机率越低 那如果你不知道publicity是什么的话也没有关系 你就记得说呢 这个publicity越大就代表说 这个句子越不像是你的模型会产生出来的句子 所以这边非常直觉的 如果是人写的正确答案 对模型来说都是非常难产生出来的句子 而paraphrase的句子 paraphrase过的 因为是模型自己生出来的句子 模型当然会觉得 是比较容易生出来的,self output根本就是模型自己生成的,所以这些句子对他来说都是产生出来的机率比较大的句子,但是你又发现说在光圈里面,其实也只有某一些token,对于语言模型来说,对于foundation model来说,是特别难产生出来的,那我们能不能在训练的时候,直接就过滤掉那些对于foundation model特别难的token呢,所以这边实际上的做法是,我们知道说,假设你要教 模型说一句话,比如说大家好,我是人工智慧的时候,你实际上教模型的就是next token prediction,也就是文字接龙,你教他看到代表开始的符号,就要说大,看到大,就要说加,看到大家,就要说好,那你先拿你的foundation model去计算一下在你的训练资料里面,每一个token,你的foundation model输出predict的机率,如果发现有某一些token是你的foundation model特别难生出来的,直接
              
                  1:12:19
                  直接在训练的时候不考虑那个token 这个token并不是从句子里面拿掉 而是说我们就不去要求模型看到大的时候要产生加 假设加是一个特别难产生的token 我们还是会教模型看到大家要产生好 或看到begin of sentence要产生大 但是你就直接去掉 你就直接略过这个问题 不教模型看到大产生加 你就把整个corpus里面 一部分的token不给模型训练 那这个方法有没有用呢 这个方法 这个方法居然是有用的 那这边是训练在那个 MATH那个Corpus上面的结果 现在模型训练在MATH 然后一样是测试在同一个Corpus的Testing Set的话 横轴是被拿掉的Token的数目 就我们把Token的难易度做一个排序 先拿掉最难的再拿掉次难的 所以可以改变Token被拿掉的比例 那你会发现说Token拿掉的比例 大概在20%以内的时候 居然对于训练是有帮助的 那对于in-domain有帮助 对于out of domain也有帮助
              
                  1:13:23
                  就训练的时候没有看过GSM8K、ARC 还有BIRC的这三个任务 但是你发现说当你拿掉一些token的时候 模型在这三个任务上 其实是可以表现得更好的 因为你在训练的时候 没有叫模型去学一些 他根本学不起来的东西 可以避免遗忘的问题 所以你拿掉一些在训练的时候 对模型特别难的东西 反而模型是可以做得更好的 这个是最后一页投影片 在post training的时候 大家要特别注意 人工智慧很容易遗忘 他过去已经有的技能 那今天常常会听到有人说 我拿一个LLaMA-3 我在做post training 把他训练在特定的任务上面 我在特定任务上面 可以假打GPT-4o 那这件事当然是有可能的 你针对特定任务训练 你要打爆那些通财模型 其实并不是一件太难的事 但是往往往我看到这种结果 我会担心的地方就是 那你到底损失了多少 其他本来模型就有的能力呢
              
                  1:14:27
                  你的模型会不会变成 只会这个任务 其他任务通通都不会 比如说你要教一个模型 一个特别的程式语言 你逼他学very log 没那么会写very log 你当然可以在very log的任务上打爆他 但会不会模型之后 他连正常的话人类的话都说不好 要连写註解都没有办法 或者是根本看不懂人类要求他怎么写very log的内容的指令呢,这是有可能的,所以大家今天在自己做post training的时候,你要注意除了看你的他目标任务有没有做好以外,你其实应该要去检查一下你的模型在原来他能够做的任务上面,到底还有没有保有原来的能力,那我们今天知道一个非常有效的可以防止可以防止forgetting的方法,就是如果我们今天的训练资料, 是用人工智慧自己的话来说,也就是他自己产生出来的,往往对于post training是非常有效的,那这是一个很有效的方法给要做post training的同学参考。
              


## 7、如何进行深度思考

chatgpt o1/o3/o4、DeepSeek R1、Gemini 2 Flash Thinking、Claude 3.7 Sonnet（Extend Thinking）……

通常会在思考的过程前后加一个\<think\> 和 \</think\>，为了界面呈现的方便

在这个思考的过程中，通常模型会有几个行为：

* 验证自己刚才的答案是不是正确的 Let me check the answer...
* 可能会进行探索，Let's try a different approach...
* 有时候他甚至会做一些规划,Let's first try to... 

这种 reasoning 的行为是 Test-Time compute 的一种，在测试的阶段投入了更大的算力，而这个投入的算力可能可以让你得到更好的结果，Why？*深度不够，长度来凑* 

*我们先讲这种深度思考的语言模型可能用什么样的方式被打造出来？下节课来探讨这些 reasoning 的过程到底有没有或者是如何发挥作用*

另一个词汇 Test-Time Scaling，意思是思考越多往往结果越好，

**打造推理语言模型的方法：**

* 更强的思维链，CoT（不用微调参数）
* 给模型推论工作流程（不用微调参数）
* 教模型推理过程（Imitation Learning）
* 以结果为导向学习推理（RL）

**1、CoT：** 让模型先列出解题过程，再给出答案

* few-shot CoT：要给模型一些范例，
* zero-shot CoT: let's think step by step，就会自动把计算的过程列出来

但是因为现在这个思考的过程往往非常的长，所以又有一个新的名字叫做 Long CoT，


**2、直接给模型推理的工作流程**

如果只是叫模型，请尝试越多方法越好，它往往就尝试个两三个方法就结束了，怎么让模型不断尝试，最好尝试个几千几万种方法？也许可以直接强迫模型对同一个问题回答几千几万次，因为每一次模型在回答问题的时候，他的答案都会是不一样的。

Paper：Large Language Monkeys

对于稍微好一点的模型，试的够多次，他总是有机会猜到正确答案，但难点是，怎么知道哪一次得出来的答案是正确答案呢？

* 一个直觉的方式就是投票，Majority Vote，有另外一个称呼，叫做 Self Consistency；
* 另一个方法是看答案的 Confidence，这个方法被用在 COT Decoding 的方法里面；

并强制要求把答案放在 \<answer\> 和 \</answer\> 中间，方便做 Majority Vote，该方法其实很强，可以作为一个很好的 baseline；

 也可以用更复杂的方法，从模型众多的尝试中选出正确答案，现在常用的做法是，训练一个 Verifier，或者找一个模型当做 Verifier，让它去验证答案是不是正确的。如果越有可能是正确的，就输出越高的分数，这个方法叫做 Best of N。
 
那怎么得到这个验证器呢？那最简单的方法也许是直接拿一个语言模型即可。但如果你想要做得更好的话，可以对验证器加以训练，怎么对验证器加以训练呢？

下面的实验中，我们假设有一些训练资料，比如一堆数学问题，有标准答案，只是没有计算的过程。有这样的训练资料以后，就可以把问题丢进给语言模型，然后让语言模型产生多个不同的答案，因为有正确答案，所以知道语言模型什么时候是答对的，什么时候是答错的，这样就有验证器的训练资料，看到这个正确的答案输出 1，看到错误的答案就输出 0 等等。 

上面的方法是并行的方式，其实还有序列的方法，先让他解第一次，然后根据第一次的解法再去解第二次，等等；而且这个并行方式和序列方式可以同时使用。 

但如果你看现在这一些会做深度思考模型的行为，会发现他们往往不是得到最终答案才进行验证，它们往往能够做到，在解题解一半的时候，中间某一个步骤，就开始验证这个步骤是不是对的，以避免中间算错了浪费时间。

那如何做每一步的验证？给模型一个问题之后，先不要让模型解完，让它每次只输出第一步就停止，比如生成 3 个不同的第一步，然后通过 Process Verifier 去验证，可以通过 prompt 去要求如何一步步生成，比如每一步都放在 \<step\> 和 \</step\> 之间。

那如何得到 Process Verifier？我们有的只是问题（input）和对应的 ground truth，我们要求 LLM 在给定 input 和 step_1 后，继续生成多个 step_2 ... 到答案的流，比如生成 3 个流，其中 2 个最后答案正确，正确率为 2/3，同样验证 step_2 的过程也是如此，指导 Process Verifier 输出从该步骤开始得到正确答案的概率值。

那接下来会遇到的另外一个问题是，通常这个 Process Verifier 的输出并不是 True or False，而是一个数值，代表从这一步继续做下去以后，可能得到正确答案的机率，那怎么定这个阈值？*可以参考 Beam Search 的做法*。

---------------

**3、教模型推理过程（Imitation Learning，模仿学习）**

接下来两个方法都是后训练的方法的特例，也就是我们有一个 Foundation 的 Model，还不会做深度思考，但我们接下来做 Post Training，希望让其具有深度思考的能力。

*Imitation Learning，模仿学习*，直接教模型怎么做推理，这边假设我们的训练资料，除了问题、答案，还有推论的过程。

我们只要教模型说，看到输入内容之后，不仅要产生正确答案，还要产生 Reasoning Process，然后才产生正确答案。

这里面最难的地方是什么呢？ 是*这么推论从何而来？*

让语言模型自己想办法，产生推论的过程，给他 input 要求做 COT，把推论的过程详细解出来，然后他就产生推论过程，并产生答案，但毕竟能力有限，它不会每次都答对，但我们一般有正确答案，所以可以对比保留回答正确的 reasoning process 拿来当作训练资料。

有没有可能答案是对的，但推论过程其实是错的？确实我们没有办法保证答案是对的，推论过程每一步就是对的，所以就有人提出一些方法说，也许我们不应该只看最终答案对不对，我们应该用类似上面提到的过程验证方法去验证。

这里其实除了 SFT 的方法，也可以用 RL 的方式去训练，

但我们真的应该这样教模型吗？应该要告诉模型每一步推论过程，都必须要是正确的吗？

可以深度思考的模型，其实有时候会得到错误的思考过程，只要他最后能够得到正确答案就好了，所以我们甚至不应该教模型的时候，给他的每一步的推论过程都是对的。

会有什么问题呢？如果给他所有的推论过程都是对的，他会不知道要找找自己的问题，他每次都会觉得前面的推论过程一定都是对的，无法纠错。

怎么办呢？我们故意制造出一些特殊的训练过程，让训练过程中间可能有一些是有错的，因为语言模型他本身能力还是有极限的，我们应该要让语言模型有知错能改的能力。

那怎么做呢？Paper：Stream of search（SoS），这篇 paper 的想法就是，我们能不能从树状结构（推导步骤树）里面得到一个 reasoning 的过程，这里面是包含错误答案的，我们直接在这个树状的结构上面做一个深度优先的搜寻，把错误的搜寻过程也包含进训练资料里。故意走一些错的路，尝试回退重试其他方案。

对人类来说，如果你看到语言模型先产生一个答案，然后再莫名其妙再跳到另外一个答案，你可能会觉得这个语言模型讲话很容易前言不对后语。

最早的 o1 也容易前言不对后语的，当然这个 o1 并没有展现那个完整的 reasoning 的过程，也不知道是摘要那边出了问题，还是那个语言模型的思考就是非常跳跃，搞不好就是用这样子的资料训练出来的，所以他思考非常的跳跃。

到目前为止，讲了一大堆创造 reasoning 过程当作训练资料的方法，但如果你今天想要自己打造有 reasoning 能力的模型，可以直接做*知识蒸馏*。

**4、以结果为导向学习推理（RL）**

DeepSeek-R1 系列的做法，

有一些训练资料，有问题及正确答案，把问题输入给模型，要求模型做 reasoning，思考内容不重要，只看它最后的答案，与标准答案对比，如果是对的，模型就得到 positive reward，如果是错的，就得到 negative reward；

*在 RL 中，推论的内容不重要，只在意最后的答案是不是对的。*

R1-Zero 中，作者非常想要让大家知道的一件事情，就是 *aha moment*，没有教他，他自己就会了。

R1-Zero 的 reasoning 过程是非常难读的，而且是多个语言混杂的，为什么？因为训练中只在意结果，根本没有在意他推论的时候到底写了些什么东西。

R1 是怎么被打造出来的呢？*其实在打造 R1 的过程中，前面讲的三个方法都是有用上的*。

![[Pasted image 20250429222458.png|500]]

具体过程是：有了 R1-Zero 之后，用其来产生有 reasoning process（人读不懂的推理过程） 的训练资料，然后人工改，这部分到底花了多大代价，技术报告中没明说。

（PPT 左侧这两个句子都是技术报告里卖弄的原文）

除此之外，它们还用另一个模型（论文未清楚介绍），用 few-shot COT 的方法来产生一些带有 reasoning 的资料；也用 Prompting 的方法，让模型产生一些detail 的 answer 而且要有 reflection，要有 verification。 那这个显然就是*supervised COT* 的方法。

然后就可以做 *Imitation learning*，训练出 Model_A，接下来 Model_A 会再进一步去做 RL 得到 Model B，但 Model_B 做 RL 的过程，也和 R1-Zero 略有点不同，除了要要求他正确率越高越好以外，还有一个额外的限制，就是语言必须要用一样的。

![[Pasted image 20250429230322.png|500]]

Model_B 也是用来生成训练资料的，不能只考虑数学跟程式，要加各式各样的任务，模型 B 已经初步具备 reasoning 的能力，产生reasoning 的过程，再产生答案。

答案的验证：虽然这些问题有答案，但很多问题没有标准答案，用 V3 来做验证，还用了一些规则去掉 reasoning 的过程中比较糟糕的过程，比如多个语言，过长的等。生了 60 万训练资料。

接下来，还是一样做 *Imitation learning* 再重新训练 *DeepSeek-V3*，然后得到 Model_C，最后再做一次 RL，这部分写的非常的模糊，是希望强化模型 safety、helpfulness 的能力，最后才得到 R1。

报告的结尾有提到说，他们也曾经想要尝试做 process verifier，但是最终没有做起来，没有得到好的结果，这是一个上代研究的问题。

*另外一个模型，如果本来就不够强，用 RL 是没有办法激发它 reasoning 的能力；如果一个模型用 RL 之后，可以激发它 reasoning 的能力，那意味着它其实本来就有 reasoning 的能力，RL 只是强化这件事情的出现而已。*


推论模型*最大的挑战* 是什么呢？最大的挑战就是要产生非常长的 reasoning 的过程，显然是花钱跟花算力的，我们其实希望该 reasoning 的时候才 reasoning。


## 8、推理过程不用太长，够用就好

如何让推理的 LLM 不要想太多。

前面的课程中有提到，如果有比较长的推理，有可能结果会更好。 但真的是这样吗？现在有很多研究表明，其实*推理越长，不一定代表结果越好*。 

很多论文都说，推理长度越长，往往你会正确率越低。但这样的实验方法其实并不够严谨，比如说，可能是因为问题太难，所以需要更长的推理长度。也有严谨的实验表明，推理的长度看起来对于正确率真的是不一定有帮助的。

最好的工程师不是把事情做到完美，而是在有限的资源下把事情做到最好。

**如何避免模型想太多？**

* 关于 CoT，一般就是让模型 think step by step，Paper：Chain of Draft，要求每一个 thinking step 都只是一个草稿，然后草稿的每一条都不要超过五个字，实验效果表明有效；
* 有关第二个方向，给模型推论工作流程，这里的模型推论的工作流程是人设定的，所以要让模型推论短一点，是完全可以控制它，比如说让它 sampling 少一点，让它做 beam search 的时候 beam 小一点，让它产生树状结构的时候树长得小一点，就人工可以控制模型 reasoning 的长度；
* 第三个方法就是直接教模型怎么做推理。给模型正确答案，然后教它怎么进行推理。比如让推理模型作为老师进行指导，那怎么在这个学习的过程中把推理的长度考虑进去呢？同一个问题问多次，答对情况下，选一个最短的推理过程，当作学生模型的训练资料；
	* 另一个方法：Paper：From Explicit CoT to Implicit CoT，把明着写出来的 CoT，练到不见，具体做法：每次都把 reason 的过程练短一些，渐进式的学法
* RL 以结果为导向的推理方法，结果导向，用 RL 这些方法就会产生超长的推理过程，直觉的解决方法就是把长度限制加到 RL 的 reward 中，但实际上多数文献不使用该方法，定阈值不一定适用于所有问题，难得问题就是需要较长的推理长度；
	* 采用相对标准，根据难度定，先把问题丢给 LLM 多次做推论，回答正确的收集起来统计平均长度作为标准。就算答对，比平均更长，也算是不好的。
	* 教模型控制推理长度，直接 prompt 中设定推理长度为 $n^\star$，reward 为正确率 - 目标和实际推理长度差异，Paper：L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning，在数学问题上训练，在数学问题上测试，可以控制在 2~6% 左右，但 OOD（域外）就控制的较差，20~40%
	* 控制推理长度，会不会有损模型本质的推理能力？研究发现模型的推理能力并没有受到太大的影响

## 9、LLM 能力评测

能解数学问题就代表有推理能力吗？有多少答案可能是记忆出来的？

把原来的 GSM8K 题目里面的一些词汇、一些数字改掉，在不影响问题难度的前提下，再去问市面上多数模型这个问题，结果发现多数模型，它的正确率都是有减低的；把一些句子的顺序换掉（不影响题目意思），这些模型解题的正确率居然都下降；

这些测试的结果，往往不一定那么可靠，因为你永远不知道这些模型，是不是早就看到了类似的问题了。

现在模型推理能力测试，常被讨论到的 Benchmark Compass，叫做 ARC-AGI（作者是 Keras 的作者），里面都是这种有图形的智力测验题目 。

有个平台叫做 Chatbot Arena，每次登进去的时候，就随机给你两个模型，模型 $A$ 和模型 $B$，接下来就问这两个模型一样的问题，然后你要决定哪一个模型是比较好的；但传说 Chatbot Arena 也是有办法被 hack 的，因为人类还是有他喜欢的倾向，比如喜欢 emoji，粗体字，等，因为很多人根本就不会仔细去看它的内容，而通常是看它输出的风格，所以输出的风格反而对结果影响比较大

Chatbot Arena 的评比机制，Elo Score，这也是很多竞赛会使用的评比方式，假设有 $K$ 个模型，$M_1$ 到 $M_K$，每个模型有一个分数 $\beta_1$ 到 $\beta_K$，任意两个模型 $M_i$ 和 $M_j$ 之间对战，评比公式：$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j}{400}\right)}=E_{i,j}$$除掉一个 Normalization 的分数，就是为了让分布比较好看一点，通常都会设成 400，前面负号再去 Exponential，这其实就是 Sigmoid Function

* 如果 $i$ 的战力比 $j$ 的战力大很多，数值就会趋近于 1
* 如果 $i$ 比 $j$ 小很多，算出来的胜率就会趋近于 0

实际上在比赛里面，真正能知道的并不是这些战力，而是根据比赛的结果可以统计某个模型对战到某个模型的胜率是多少；所以 Elo Score 的求法是，先得到大量的比赛，知道模型跟模型间对战的胜率，再根据胜率反推出这些 $\beta$ 的值。

但在 Champion Arena 上，他们觉得会有太多跟模型本身实力无关的因素，会干扰到评比的结果，所以在计算正确率的时候，只知道 $\beta_i$ 跟 $\beta_j$ 是不够的，要加一项 $\beta_0$，即$$\frac{1}{1+\exp\left(-\frac{\beta_i-\beta_j+\beta_o}{400}\right)}=E_{i,j}$$
* $\beta_0$ 是模型实力以外的因素，比如某些棋类比赛里面，就应该把先手优势考虑进去

举例来说，模型可能回答越长，人类就会越喜欢，所以 $$\beta_0 = \gamma_1(\text{答案长度差})+\gamma_2(\text{emoji 数量差}) + \cdots$$
如果计算出来 $\gamma$ 是正的，就代表说这一项是会有影响的，$\gamma$ 趋近于 0，说明这一项没什么影响，负值就是负相关，如长度越短越好。

Champion Arena 报告，有没有考虑这些风格相关的因素，其实会影响模型的排名，其中上升的最多的就是 Claude，它是大家认知觉得蛮厉害，但在 Chatbot Arena 上评分起不来的模型，一个原因可能就是 Claude 模型讲话太无聊了。

到底什么样的指标才是好的评量指标呢？*也许结论就是没有好的评量指标*。这个叫做 Goodheart's Law 的意思是，一旦一项指标被当做目标，它就不再是一个好的指标。


## 10、模型编辑

Model Editing 希望做到的事情是帮模型 *植入* 一件知识，

Model Editing 与 Post-training 有什么不同？Post-training 通常是想要让模型学会新的技能，这个技能不是一项知识，而是需要模型做比较大的改变才有办法学会的事情，比如说新的语言、或者是使用工具、做推理等等；

但是直接用 Post-training finetune 模型的方法做 Model Editing 有很大的挑战，因为做 Model Editing 的时候，通常你的训练资料就只有一笔；

怎么评量 Model Editing 是不是成功的，要考虑三个不同的面向：

* Reliability：想要修改的目标必须要达成
* Generalization：输入有一些改变，泛化能力
* Locality：其他无关的输入，不应该被改

Model Editing 常见方法：

* 不动参数：放在 prompt 中（关闭 rag，联网功能），但直接放入模型会不信；Paper：In-Context Knowledge Editing(IKE)，提供一些范例，是比较容易成功的
* 改变参数：直接梯度下降，往往一改完，模型就坏掉了
	* 人类决定如何编辑：由人类对于语言模型的理解，找出应该要被编辑的位置，并决定要被编辑的方法。Paper：Rank-One Model Editing（ROME）
		1. 是找出网路中，跟编辑的知识最相关的部分
		2. 修改那个部分的参数，让模型变成想要的样子
	* 另一个 LLM 决定如何编辑：输入是 $\theta$（待编辑的模型参数）、待编辑的问题、答案，输出是 $e$（大小同 $\theta$ 的向量），然后 $\theta + e$ 即可，这个网络称为 Hypernetwork


