
* Authorsï¼šGuilherme Penedo,Â Hynek KydlÃ­Äek,Â Loubna Ben Allal,Â Anton Lozhkov,Â Colin Raffel,Â Leandro Werra, Thomas Wolf
* Publishedï¼šMay 31, 2024
* Reading time: 45 min

LLM çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…¶é¢„è®­ç»ƒæ•°æ®é›†çš„è´¨é‡å’Œè§„æ¨¡ã€‚ç„¶è€Œï¼Œåƒ Llama-3 å’Œ Mixtral è¿™æ ·çš„æœ€å…ˆè¿›å¼€æº LLM çš„é¢„è®­ç»ƒæ•°æ®é›†å¹¶æœªå…¬å¼€ï¼Œè€Œä¸”å…³äºå®ƒä»¬æ˜¯å¦‚ä½•åˆ›å»ºçš„çŸ¥ä¹‹ç”šå°‘ã€‚

æœ€è¿‘ï¼Œæˆ‘ä»¬å‘å¸ƒäº† [ğŸ·FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº LLM é¢„è®­ç»ƒçš„æ–°çš„å¤§è§„æ¨¡ï¼ˆ*15T tokensï¼Œ44TB*ï¼‰æ•°æ®é›†ã€‚FineWeb æºè‡ª 96 ä¸ª CommonCrawl å¿«ç…§ï¼Œå¹¶ä¸”*æ¯”å…¶ä»–å¼€æ”¾é¢„è®­ç»ƒæ•°æ®é›†èƒ½äº§ç”Ÿæ€§èƒ½æ›´å¥½çš„ LLM*ã€‚ä¸ºäº†åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæä¾›æ›´æ¸…æ™°çš„è®¤çŸ¥ï¼Œå¹¶æ¨è¿›å…³äºå¦‚ä½•è®­ç»ƒé«˜è´¨é‡ LLM çš„å¼€æ”¾æ€§ç†è§£ï¼Œæˆ‘ä»¬ä»”ç»†è®°å½•å¹¶åˆ†æäº† FineWeb ä¸­ä½¿ç”¨çš„æ‰€æœ‰è®¾è®¡é€‰æ‹©ï¼ŒåŒ…æ‹¬å¯¹å»é‡å’Œè¿‡æ»¤ç­–ç•¥çš„æ·±å…¥æ¢ç©¶ã€‚æœ¬é•¿ç¯‡æŠ¥å‘Šæ·±å…¥æ¢è®¨äº†å¦‚ä½•ä¸º LLM é¢„è®­ç»ƒåˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„ç½‘ç»œè§„æ¨¡æ•°æ®é›†ã€‚

åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº† [**ğŸ“š FineWeb-Edu**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)ï¼Œå®ƒæ˜¯ FineWeb çš„ä¸€ä¸ªå­é›†ï¼Œæ˜¯åˆ©ç”¨å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–é«˜è´¨é‡æ³¨é‡Šæ„å»ºè€Œæˆï¼Œå…·æœ‰æ•™è‚²ä»·å€¼ï¼Œå¹¶ä¸”åœ¨ MMLUã€ARC å’Œ OpenBookQA ç­‰å¤šä¸ªæ•™è‚²åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºæ‰€æœ‰å¯å…¬å¼€è·å–çš„ç½‘ç»œæ•°æ®é›†ã€‚[**ğŸ“š FineWeb-Edu**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) æœ‰ä¸¤ç§è§„æ¨¡/è¿‡æ»¤çº§åˆ«ï¼š*1.3Tï¼ˆæ•™è‚²å†…å®¹æé«˜ï¼‰å’Œ 5.4Tï¼ˆæ•™è‚²å†…å®¹é«˜ï¼‰tokens*ã€‚

**æ‘˜è¦ï¼š** è¿™ç¯‡åšå®¢è®¨è®ºäº†å¤§è§„æ¨¡å¤„ç†å’Œè¯„ä¼°æ•°æ®è´¨é‡çš„ç›¸å…³å†…å®¹ï¼ŒğŸ· FineWeb é…æ–¹ï¼Œä»¥åŠåˆ›å»ºå…¶ ğŸ“š FineWeb-Edu å­é›†æ‰€éµå¾ªçš„è¿‡ç¨‹ã€‚

## ä¸€ã€ç½‘ç»œæ•°æ®

### 1.1 å¯»æ‰¾åŸå§‹æ•°æ®

å…³äºç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„ç½‘ç»œæ•°æ®é›†ï¼Œäººä»¬å¸¸é—®çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼šâ€œä»–ä»¬ç©¶ç«Ÿä»å“ªé‡Œè·å–è¿™ä¹ˆå¤šæ•°æ®ï¼Ÿâ€ã€‚é€šå¸¸æœ‰ä¸¤ç§é€‰æ‹©ï¼š

- è¦ä¹ˆä½ è‡ªå·±æŠ“å–æ•°æ®ï¼Œå°±åƒ OpenAI æˆ– Anthropicï¼ˆä»¥åŠå…¶ä»–ä¸€äº›å…¬å¸ï¼‰æ‰€åšçš„é‚£æ ·ï¼ˆè§[æ­¤å¤„](https://platform.openai.com/docs/bots)å’Œ[æ­¤å¤„](https://darkvisitors.com/agents/claudebot)ï¼‰ï¼›
- è¦ä¹ˆä½¿ç”¨å…¬å…±çš„ç½‘ç»œçˆ¬å–ç½‘é¡µåº“ï¼Œæ¯”å¦‚ç”±éè¥åˆ©ç»„ç»‡ [CommonCrawl](https://commoncrawl.org/) ç»´æŠ¤çš„é‚£ä¸ªã€‚

ä¸ºäº†æ„å»º ğŸ· FineWebï¼Œæˆ‘ä»¬éµå¾ªäº†è®¸å¤š LLM è®­ç»ƒå›¢é˜Ÿè¿‡å»æ‰€é‡‡ç”¨çš„æ–¹æ³•ï¼Œä»¥ CommonCrawlï¼ˆCCï¼‰ä½œä¸ºèµ·ç‚¹ã€‚è‡ª 2007 å¹´ä»¥æ¥ï¼ŒCommon Crawl éè¥åˆ©ç»„ç»‡ä¸€ç›´åœ¨å¯¹ç½‘ç»œè¿›è¡Œçˆ¬å–ï¼Œå¹¶ä¸”é€šå¸¸æ¯éš” 1 åˆ° 2 ä¸ªæœˆå°±ä¼šå‘å¸ƒä¸€æ¬¡æ–°çš„çˆ¬å–ç»“æœï¼Œå…¶ä¸­åŒ…å«é€šè¿‡è‡ªåŠ¨ç½‘ç»œçˆ¬å–è·å¾—çš„ 200 åˆ° 400T çš„æ–‡æœ¬å†…å®¹ã€‚

ä¾‹å¦‚ï¼Œæœ€æ–°çš„ CC æŠ“å–ï¼ˆ2024 å¹´ 4 æœˆï¼‰åŒ…å« 27 äº¿ä¸ªç½‘é¡µï¼Œæ€»è®¡ 386T æœªå‹ç¼©çš„ HTML æ–‡æœ¬å†…å®¹[^1]ã€‚è‡ª 2013 å¹´ä»¥æ¥å·²å‘å¸ƒäº† 96 æ¬¡æŠ“å–ï¼Œ2008 å¹´è‡³ 2012 å¹´æœŸé—´å‘å¸ƒäº† 3 æ¬¡æŠ“å–ï¼Œè¿™äº›æŠ“å–é‡‡ç”¨ä¸åŒçš„ï¼ˆè¾ƒæ—§çš„ï¼‰æ ¼å¼[^2]ã€‚

### 1.2 å¤§è§„æ¨¡å¤„ç†

é‰´äºæ‰€æ¶‰åŠçš„æ•°æ®é‡å·¨å¤§ï¼Œæˆ‘ä»¬å¿…é¡»å…‹æœçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯æ‹¥æœ‰ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„ä»£ç åº“ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿå¿«é€Ÿå¤„ç†æˆ‘ä»¬çš„å¤„ç†å†³ç­–å¹¶è½»æ¾å°è¯•æ–°æƒ³æ³•ï¼ŒåŒæ—¶é€‚å½“åœ°å¹¶è¡ŒåŒ–æˆ‘ä»¬çš„å·¥ä½œè´Ÿè½½ï¼Œå¹¶æä¾›å¯¹æ•°æ®çš„æ¸…æ™°æ´å¯Ÿã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº† [`datatrove`](https://github.com/huggingface/datatrove)[4]ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºæ•°æ®å¤„ç†åº“ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†è¿‡æ»¤å’Œå»é‡è®¾ç½®æ— ç¼æ‰©å±•åˆ°æ•°åƒä¸ª CPU æ ¸å¿ƒã€‚åˆ›å»º ğŸ· FineWeb æ‰€æ¶‰åŠçš„æ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤éƒ½ä½¿ç”¨äº†è¿™ä¸ªåº“ã€‚æ‚¨å°†åœ¨ [datatrove å­˜å‚¨åº“](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py)ä¸­æ‰¾åˆ°æˆ‘ä»¬ä½¿ç”¨çš„ç¡®åˆ‡è„šæœ¬ã€‚

### 1.3 ä»€ä¹ˆæ˜¯ä¼˜è´¨æ•°æ®ï¼Ÿ

è¿™å¯èƒ½æ˜¯åˆ›å»ºæ•°æ®é›†æ—¶éœ€è¦ç‰¢è®°çš„ä¸»è¦é—®é¢˜ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„èƒŒæ™¯ä¸‹[^3]ï¼Œâ€œé«˜è´¨é‡â€å¹¶ä¸æ˜¯ä¸€ä¸ªå®šä¹‰éå¸¸æ˜ç¡®çš„æœ¯è¯­[5] [6]ï¼Œç”šè‡³ä¸æ˜¯ä»…é€šè¿‡ç›´æ¥çš„äººç±»è§‚å¯Ÿå°±èƒ½å§‹ç»ˆæ¸…æ™°æ„ŸçŸ¥åˆ°çš„æ–‡æ¡£å±æ€§[7]ã€‚

åœ¨ç»™å®šè¢«è®¤ä¸ºæ˜¯â€œå¹²å‡€â€çš„è¯­æ–™åº“ï¼ˆé€šå¸¸æ˜¯ç»´åŸºç™¾ç§‘[^4]ï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥æ£€æŸ¥æˆ‘ä»¬è¯•å›¾æ•´ç†çš„æ•°æ®é›†çš„å›°æƒ‘åº¦ï¼Œè¿™ç§æ–¹æ³•ä»ç„¶å¾ˆå¸¸è§[8]ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™å¹¶ä¸æ€»æ˜¯ä¸æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ„Ÿå…´è¶£çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ€§èƒ½çš„æå‡ç›¸å…³[9]ï¼Œå› æ­¤ï¼Œå¦ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•æ˜¯åœ¨æˆ‘ä»¬æ•°æ®é›†çš„ä¸€ä¸ªä»£è¡¨æ€§å­é›†ä¸Šè®­ç»ƒå°å‹æ¨¡å‹ [^5]ï¼Œå¹¶åœ¨ä¸€ç»„è¯„ä¼°ä»»åŠ¡ä¸Šå¯¹å®ƒä»¬è¿›è¡Œè¯„ä¼°ã€‚ä¹‹æ‰€ä»¥ä½¿ç”¨å°å‹æ¨¡å‹ï¼Œæ˜¯å› ä¸ºè®­ç»ƒæˆæœ¬å’Œæ—¶é—´ä¸æ¨¡å‹å¤§å°æˆæ­£æ¯”ã€‚åœ¨ç¬¬äºŒç§æ–¹æ³•ä¸­ï¼Œé€‰æ‹©ä¸€ç»„å¤šæ ·ä¸”æœ‰ä»£è¡¨æ€§çš„æ•°æ®é›†-è¯„ä¼°ä»»åŠ¡éå¸¸é‡è¦ï¼Œå¹¶ä¸”å°½é‡ä¸è¦å¯¹ä»»ä½•ä¸€ä¸ªå•ç‹¬çš„åŸºå‡†æµ‹è¯•è¿‡åº¦æ‹Ÿåˆï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šæŸå®³é¢„è®­ç»ƒåè·å¾—çš„ LLM çš„é€šç”¨æ€§ã€‚

æ¯”è¾ƒä¸åŒæ•°æ®é›†çš„å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åè®©äººç±»å¯¹æ¨¡å‹çš„ç”Ÿæˆç»“æœè¿›è¡Œè¯„åˆ†å’Œæ¯”è¾ƒï¼ˆå°±åƒåœ¨ [LMSYS èŠå¤©æœºå™¨äººç«æŠ€åœº](https://lmarena.ai/)ä¸­é‚£æ ·ï¼‰[10]ã€‚å¯ä»¥è¯´ï¼Œè¿™ç§æ–¹æ³•åœ¨ä»£è¡¨çœŸå®æ¨¡å‹ä½¿ç”¨æƒ…å†µæ–¹é¢èƒ½æä¾›æœ€å¯é çš„ç»“æœï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œé€šè¿‡è¿™ç§æ–¹å¼è·å–æ¶ˆèå®éªŒç»“æœæ—¢æ˜‚è´µåˆè€—æ—¶ã€‚è€Œä¸”ï¼Œè¿™é€šå¸¸è¿˜è¦æ±‚æ¨¡å‹ç»è¿‡æŒ‡ä»¤å¾®è°ƒé˜¶æ®µä»¥è·å¾—å¯¹è¯èƒ½åŠ›ï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹å¹¶éç›´æ¥è®¾è®¡ç”¨äºéµå¾ªæŒ‡ä»¤ï¼Œå› æ­¤å¯¹æç¤ºç»†èŠ‚æ›´ä¸ºæ•æ„Ÿ[11]ã€‚

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è®­ç»ƒå°å‹æ¨¡å‹å¹¶åœ¨ä¸€ç»„â€œæ—©æœŸä¿¡å·â€åŸºå‡†ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨ç‰¢è®°ä¸Šè¿°å…³äºåœ¨è¯„ä¼°åŸºå‡†ä¸Šè¿‡æ‹Ÿåˆçš„æ³¨æ„äº‹é¡¹çš„å‰æä¸‹ï¼Œè¿™æ˜¯å¯¹ç”¨äºè®­ç»ƒè¿™äº›æ¨¡å‹çš„æ•°æ®è´¨é‡çš„ä¸€ä¸ªåˆç†ä»£ç†æŒ‡æ ‡ã€‚

### 1.4 æ¶ˆèå®éªŒä¸è¯„ä¼°è®¾ç½®

ä¸ºäº†æ¯”è¾ƒæŸä¸€ç‰¹å®šå¤„ç†æ­¥éª¤çš„å½±å“ï¼Œæˆ‘ä»¬åœ¨æ•°æ®é›†çš„ä¸¤ä¸ªç‰ˆæœ¬ä¸Šè®­ç»ƒäº†ä¸¤ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªç‰ˆæœ¬ç»è¿‡é¢å¤–æ­¥éª¤å¤„ç†ï¼ˆå³æˆ‘ä»¬å¸Œæœ›è¯„ä¼°çš„æ­¥éª¤ï¼‰ï¼Œå¦ä¸€ä¸ªç‰ˆæœ¬åˆ™å»é™¤äº†è¯¥æ­¥éª¤ï¼ˆè¿›è¡Œäº†åˆ å‡/ç§»é™¤ï¼‰ã€‚é™¤äº†æ•°æ®ä¹‹å¤–ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å…¶ä»–æ–¹é¢å®Œå…¨ç›¸åŒï¼šå‚æ•°æ•°é‡ç›¸åŒã€æ¶æ„è¶…å‚æ•°ç›¸åŒï¼Œå¹¶ä¸”éƒ½åœ¨æ¯ä¸ªç‰ˆæœ¬çš„æ•°æ®ä¸­éšæœºæŠ½å–ç›¸åŒæ•°é‡çš„ tokens ä¸Šè¿›è¡Œå•è½®è®­ç»ƒ â€”â€” å› æ­¤å”¯ä¸€çš„åŒºåˆ«å°±åœ¨äºè®­ç»ƒæ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„ä»»åŠ¡é›†ä¸Šå¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ¯”è¾ƒå¹³å‡åˆ†æ•°ã€‚

æˆ‘ä»¬çš„æ¶ˆèæ¨¡å‹æ˜¯ä½¿ç”¨ [`nanotron`](https://github.com/huggingface/nanotron) è®­ç»ƒçš„ã€‚æˆ‘ä»¬çš„â€œæ¶ˆèæ¨¡å‹â€æœ‰ 1.82B ä¸ªå‚æ•°ï¼ˆåŒ…æ‹¬åµŒå…¥ï¼‰ï¼Œé‡‡ç”¨äº† Llama æ¶æ„ï¼Œåºåˆ—é•¿åº¦ä¸º 2048ï¼Œå…¨å±€æ‰¹é‡å¤§å°çº¦ä¸º 2M tokensï¼Œå¹¶ä½¿ç”¨äº† GPT2 åˆ†è¯å™¨ã€‚å¯¹äºå¤§å¤šæ•°æ¶ˆèå®éªŒï¼Œæˆ‘ä»¬åœ¨çº¦ 28B tokens ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼ˆå¤§è‡´æ˜¯è¯¥æ¨¡å‹è§„æ¨¡çš„ Chinchilla[12] æœ€ä¼˜è®­ç»ƒè§„æ¨¡ï¼‰ã€‚ä¸ºäº†ç¡®è®¤æ¯ä¸€æ­¥è¿‡æ»¤åçš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œæˆ‘ä»¬æŒ‰ç…§ä¸‹æ–‡è¿›ä¸€æ­¥è¯´æ˜ï¼Œåœ¨ 350B tokens ä¸Šè¿›è¡Œäº†æ›´é•¿æ—¶é—´çš„è®­ç»ƒè¿è¡Œã€‚

ï¼ˆæˆ‘ä»¬å°†åœ¨ Nanotron ä¸Šå°½å¿«æä¾›ç”¨äºå¤ç°è¿™äº›æ¶ˆèæ¨¡å‹çš„é…ç½®ã€‚ï¼‰

æˆ‘ä»¬ä½¿ç”¨ [`lighteval`](https://github.com/huggingface/lighteval/) å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡æŒ‘é€‰é‚£äº›åœ¨è¾ƒå°è§„æ¨¡ä¸‹ï¼ˆä»…åœ¨â€œå‡ ç™¾äº¿â€ tokens ä¸Šè®­ç»ƒçš„â€œå°â€æ¨¡å‹ï¼‰èƒ½æä¾›è‰¯å¥½ä¿¡å·çš„åŸºå‡†æµ‹è¯•ï¼Œç²¾å¿ƒé€‰å®šäº†ä¸€ç»„ç”¨äºæ¶ˆèç ”ç©¶çš„åŸºå‡†æµ‹è¯•é›†ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šä¾æ®ä»¥ä¸‹æ ‡å‡†ï¼Œåœ¨ `lighteval` æä¾›çš„æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­æŒ‘é€‰è¿™äº›åŸºå‡†æµ‹è¯•ã€‚

- åœ¨å¯¹åŒä¸€æ•°æ®é›†çš„ä¸åŒé‡‡æ ·è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ–¹å·®è¾ƒå°ï¼šæˆ‘ä»¬å¸Œæœ›åœ¨å¯¹æ•°æ®å­é›†è¿›è¡Œçš„è®­ç»ƒèƒ½å¤Ÿä»£è¡¨æ•´ä¸ªæ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨å¯èƒ½çš„é™åº¦å†…ï¼Œå¾—åˆ°çš„åˆ†æ•°å¯¹ç¡®åˆ‡æ•°æ®ç‚¹é€‰æ‹©çš„æ•æ„Ÿåº¦è¦ä½äºå¯¹æˆ‘ä»¬è¿‡æ»¤å™¨çš„æ•ˆæœçš„æ•æ„Ÿåº¦ã€‚
- è®­ç»ƒè¿‡ç¨‹ä¸­æ€§èƒ½å•è°ƒï¼ˆæˆ–æ¥è¿‘å•è°ƒï¼‰é€’å¢ï¼šç†æƒ³æƒ…å†µä¸‹ï¼Œéšç€çœ‹åˆ°çš„æ ‡è®°æ•°é‡å¢åŠ ï¼Œåœ¨é«˜ä¿¡å·åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸åº”ä¸‹é™ï¼ˆè¿™è¡¨æ˜åœ¨å°è§„æ¨¡ä¸‹ç»“æœä¸å¯é ï¼‰ã€‚
- å¯¹äºæ­¤ä»»åŠ¡ï¼Œæ€§èƒ½è‡³å°‘æ¯”éšæœºåŸºçº¿é«˜å‡ºå‡ ä¸ªæ ‡å‡†å·®ï¼šé‰´äºæˆ‘ä»¬è§„æ¨¡è¾ƒå°çš„æ¶ˆèæ¨¡å‹å’Œè®­ç»ƒï¼Œæˆ‘ä»¬é€šå¸¸ä¸ä¼šåœ¨ä»»ä½•åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æé«˜çš„åˆ†æ•°ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ç¡®ä¿å¾—åˆ°çš„åˆ†æ•°é«˜äºéšæœºå™ªå£°ã€‚

ç»è¿‡è€ƒè™‘ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä»¥ä¸‹åŸºå‡†æµ‹è¯•åˆ—è¡¨ï¼š

- CommonSense QA[13]
- HellaSwag[14]
- OpenBook QA[15]
- PIQA[16]
- SIQA[17]
- WinoGrande[18]
- ARC[19]
- MMLU[20]

ä¸ºç¡®ä¿æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹è¯„ä¼°åœ¨æœ‰é™çš„æ—¶é—´å†…å®Œæˆï¼Œæˆ‘ä»¬å°†è¾ƒé•¿çš„åŸºå‡†æµ‹è¯•æ ·æœ¬æ•°é‡é™åˆ¶åœ¨ 1000 ä¸ªï¼ˆåœ¨å•ä¸ª 8 GPU èŠ‚ç‚¹ä¸Šçš„å¢™é’Ÿè¯„ä¼°æ—¶é—´å°‘äº 5 åˆ†é’Ÿâ€”â€”ä¸è®­ç»ƒå¹¶è¡Œè¿›è¡Œï¼‰ã€‚

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/datasets/HuggingFaceFW/fineweb/blob/main/lighteval_tasks.py)æ‰¾åˆ°æˆ‘ä»¬æ‰€ä½¿ç”¨çš„å…¨éƒ¨ä»»åŠ¡å’Œæç¤ºçš„åˆ—è¡¨ã€‚

## äºŒã€ğŸ· FineWeb é…æ–¹

åœ¨æ¥ä¸‹æ¥çš„å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è§£é‡Šåˆ¶ä½œ FineWeb æ•°æ®é›†æ‰€é‡‡å–çš„æ¯ä¸ªæ­¥éª¤ã€‚

![|600](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/assets/images/fineweb-recipe.png)

ä½ å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py)æ‰¾åˆ°ä¸€ä¸ªå®Œå…¨å¯å¤ç°çš„ Â `datatrove` é…ç½®ã€‚

### 2.1 èµ·ç‚¹ï¼šæ–‡æœ¬æå–

CommonCrawl æ•°æ®ä¸»è¦æœ‰ä¸¤ç§æ ¼å¼ï¼šWARC å’Œ WETã€‚**WARC**ï¼ˆç½‘ç»œæ¡£æ¡ˆæ ¼å¼ï¼‰æ–‡ä»¶åŒ…å«æŠ“å–çš„åŸå§‹æ•°æ®ï¼ŒåŒ…æ‹¬å®Œæ•´çš„é¡µé¢ HTML å’Œè¯·æ±‚å…ƒæ•°æ®ã€‚**WET**ï¼ˆWARC å°è£…æ–‡æœ¬ï¼‰æ–‡ä»¶æä¾›äº†è¿™äº›ç½‘ç«™çš„çº¯æ–‡æœ¬ç‰ˆæœ¬ã€‚

å¤§é‡æ•°æ®é›†ä»¥ WET æ–‡ä»¶ä½œä¸ºèµ·ç‚¹ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼ŒCommon Crawl ç”¨äºåˆ›å»ºè¿™äº› WET æ–‡ä»¶çš„é»˜è®¤æ–‡æœ¬æå–æ–¹å¼å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒçš„ç›®æ ‡è€Œè¨€å¹¶éæœ€ä¼˜ï¼Œè€Œä¸”æœ‰å¤šç§å¼€æºåº“èƒ½å¤Ÿæä¾›æ›´å¥½çš„æ–‡æœ¬æå–æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨ trafilatura åº“ [21] ä» WARC æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œé€šè¿‡ç›´è§‚æ£€æŸ¥ç»“æœå‘ç°ï¼Œä¸å…¶ä»–åº“ç›¸æ¯”ï¼Œè¯¥åº“èƒ½æä¾›é«˜è´¨é‡çš„æå–æ•ˆæœã€‚

ä½ å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/scrapinghub/article-extraction-benchmark/blob/master/README.rst)æ‰¾åˆ°ä¸€ä¸ªæ¯”è¾ƒå‡ ä¸ªæ–‡æœ¬æå–åº“çš„åŸºå‡†æµ‹è¯•ã€‚

ä¸ºäº†éªŒè¯è¿™ä¸€å†³å®šï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ WET æ–‡ä»¶å¤„ç†äº† 2019-18 æ•°æ®è½¬å‚¨ï¼Œå¹¶ä½¿ç”¨ trafilatura ä» WARCæ–‡ä»¶ä¸­æå–çš„æ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬å¯¹æ¯ä¸ªæ•°æ®é›†åº”ç”¨äº†ç›¸åŒçš„å¤„ç†æµç¨‹ï¼ˆæˆ‘ä»¬çš„åŸºç¡€è¿‡æ»¤+MinHashï¼Œè¯¦è§ä¸‹æ–‡ï¼‰ï¼Œå¹¶è®­ç»ƒäº†ä¸¤ä¸ªæ¨¡å‹ã€‚è™½ç„¶ WET æ•°æ®çš„ç»“æœæ•°æ®é›†å¤§çº¦å¤§ 25%ï¼ˆçº¦ 254B tokensï¼‰ï¼Œä½†å…¶è´¨é‡æ¯”ä½¿ç”¨ trafilatura ä» WARC æ–‡ä»¶ä¸­æå–æ–‡æœ¬çš„æ•°æ®é›†ï¼ˆçº¦ 200B tokensï¼‰å·®å¾—å¤šã€‚å¯¹ä¸€äº›æ ·æœ¬çš„è§†è§‰æ£€æŸ¥è¯å®ï¼ŒWET æ–‡ä»¶ä¸Šçš„è®¸å¤šé¢å¤–æ ‡è®°æ˜¯ä¸å¿…è¦çš„é¡µé¢æ ·æ¿å†…å®¹ã€‚

ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ–‡æœ¬æå–æ˜¯æˆ‘ä»¬å¤„ç†è¿‡ç¨‹ä¸­æˆæœ¬æœ€é«˜çš„æ­¥éª¤ä¹‹ä¸€ï¼Œå› æ­¤æˆ‘ä»¬è®¤ä¸ºï¼Œå¯¹äºé¢„ç®—è¾ƒä½çš„å›¢é˜Ÿæ¥è¯´ï¼Œä½¿ç”¨ç°æˆçš„WETæ•°æ®å¯èƒ½æ˜¯ä¸€ä¸ªåˆç†çš„æƒè¡¡ã€‚

[äº¤äº’å›¾]

### Base filtering

Filtering is an important part of the curation process. It consists in removing part of the data (be it words, lines, or even full documents) that lowers the performance of the model and is thus deemed to be â€œlower qualityâ€ in our eval-driven process of dataset crafting.

As a basis for our filtering we used part of the setup from RefinedWeb

[22]

. Namely, we:

- Applied URL filtering using aÂ [blocklist](https://dsi.ut-capitole.fr/blacklists/)Â to remove adult content

- Applied aÂ [fastText language classifier](https://fasttext.cc/docs/en/language-identification.html)
    
    [23]
    
    [24]
    
    Â to keep only English text with a score â‰¥ 0.65

- Applied quality and repetition filters from MassiveText
    
    [25]
    
    Â (using the default thresholds)

After applying this filtering to each of the text extracted dumps (there are currently 96 dumps) we obtained roughly 36 trillion tokens of dataÂ 8Â .

### Deduplicating the data

Deduplication is one of the most important steps when creating large web datasets for LLM pretraining. Methods to deduplicate datasets attempt to identify and remove redundant/repeated data from the dataset.

#### Why deduplicate?

The web has many aggregators, mirrors, templated pages or just otherwise repeated content spread over different domains and webpages. Sometimes, these duplicated pages can even be introduced by the crawler itself, when different links point to the same page.

Removing these duplicates (deduplicating) has been correlated with improvements in model performance

[26]

Â and a reduction in memorization of pretraining data

[27]

, which might allow for better generalization. Additionally, the performance uplift obtained through deduplication can be equated to increased training efficiency: by removing duplicated content, a model can reach the same performance level with fewer training iterations â€“ or equivalently, for a given number of training tokens, a model will have seen more diverse data.

[28]

[29]

There are different ways to identify and even define duplicated data. Common approaches rely on hashing techniques to speed up the process, or on building efficient data structures to index the data (like suffix arrays). Methods can also be â€œfuzzyâ€, by using some similarity metric to mark documents as duplicates, or â€œexactâ€ by checking for exact matches between two documents (or lines, paragraphs, or whatever other granularity level being used)Â 9Â .

#### Our deduplication parameters

Following RefinedWeb

[22]

, we decided to apply MinHash, a fuzzy hash based deduplication technique that scales efficiently to many CPU-nodes and allows us to tune similarity thresholds (by controlling the number and size of buckets) as well as the length of the subsequences considered (by controlling the n-gram size). We chose to collect each document's 5-gramsÂ 10Â and compute minhashes using 112 hash functions in total, split into 14 buckets of 8 hashes each â€” targeting documents that are at least 75% similar. Documents with the same 8 minhashes in any bucket are considered a duplicate of each other.

This would mean that for two documents with a similarity (s) of 0.7, 0.75, 0.8 and 0.85, the probability that they would be identified as duplicates would be 56%, 77%, 92% and 98.8% respectively (1-(1-s^8)^{14}). See the plot below for a match probability comparison between our setup with 112 hashes and the one from RefinedWeb, with 9000 hashes, divided into 450 buckets of 20 hashes (that requires a substantially larger amount of compute resources, as each individual hash must be computed, stored and then compared with hashes from other documents):

00.20.40.60.8100.20.40.60.81

MinHash parametersFineWeb: 1-(1-s^8)^14RefinedWeb: 1-(1-s^20)^450Document similarity (s)Matched as dups probability

[](https://plotly.com/)

While the high number of hash functions in RefinedWeb allows for a steeper, more well defined cut off (documents with real similarity near the threshold are more likely to be correctly identified), we believe the compute and storage savings are a reasonable trade off.

It should also be noted that intra-document deduplication is already handled by our repetition filter, which removes documents with many repeated lines and paragraphs.

#### More deduplication is always better, right?

Initially, we were operating under the assumption thatÂ _more deduplication is always better_, so our first approach was to take the entire dataset (all 90+ dumps) and deduplicate them together as one big dataset using MinHash.

We did this in an iterative manner: starting with the most recent dump (which at the time was 2023-50) and proceeding chronologically until we reached the oldest crawl. We deduplicated each dump not only within itself, but removing any document matching any other documents in the previously processed dumps.

For instance, for the second most recent dump (2023-40 at the time), we deduplicated it against the most recent one in addition to within itself. As a result, the older the dumps, the larger the number of dumps it was deduplicated against and the more data we removed from it (indeed, in the oldest dumps, the deduplication step removed more than 90% of the base filtered data).

Deduplicating the dataset in this manner resulted in 4 trillion tokens of data, but, quite surprisingly to us, when training on a randomly sampled 350 billion tokens subset, our ablation models showed next to no improvement over a model trained on the non deduplicated data, scoring far below its predecessor RefinedWeb on our aggregate of tasks (see graph below).

01002003000.380.40.420.440.460.48

Dedup across all dumps does not improve performanceRefinedWebFineWeb filtered onlyFineWeb full MinHashTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

This challenged our assumption that more deduplication would inevitably result in higher benchmark scores, so we decided to take a closer look at one of the oldest dumps, dump 2013-48:

- pre deduplication, this dump had ~490 billion tokens

- after our iterative MinHash, ~31 billion tokens remained (94% of data had been removed)

As an experiment, we tried training two models on 28 billion tokens sampled from the following data from 2013-48:

- the fully deduplicated remaining ~31 billion tokens (_originally kept data_)

- 171 billion tokens obtained by individually deduplicating (without considering the other dumps) the ~460 billion tokens that had been removed from this dump in the iterative dedup process (_originally removed data_)Â 11

010200.340.360.380.40.42

The originally removed data outperforms the kept dataOriginally removed dataOriginally kept dataTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

0

These results show that, for this older dump taken in isolation, the data that was kept (10% of the original data) was actuallyÂ _worse_Â than the 90% of data we removedÂ 12Â . This is also confirmed by visual inspection:Â _originally kept data_Â contains far more ads, lists of keywords and generally badly formatted text thanÂ _originally removed data_.

#### Taking a step back: individual dump dedup

We decided to experiment with an alternative approach: we deduplicated each dump with MinHash individually (independently of the other dumps). This resulted in 20 trillion tokens of data.

When training on a random sample from this dataset we see that it now matches RefinedWebâ€™s performance (see curves below):

01002003000.380.40.420.440.460.48

Independent dedup outperforms dedup across dumpsFineWeb independent MinHashRefinedWebFineWeb filtered onlyFineWeb full MinHashTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

We hypothesize that the main improvement gained from deduplication is the removal of very large clusters that are present in every single dump (you will find some examples of these clusters in the RefinedWeb paper, each containingÂ _hundreds of thousands_Â of documents) and that further deduplication for clusters with a low number of duplicates (less than ~100 i.e. the number of dumps) actually harms performance: data that does not find a duplicate match in any other dump might actually be worse quality/more out of distribution (as evidenced by the results on the 2013-48 data).

While you might see some performance improvement when deduplicating a few dumps together, at the scale of the entire dataset (all the dumps), the effect from this upsampling of lower quality data side effect seems to be more impactful.

One possibility to consider is that as filtering quality improves, this effect may not be as prevalent, since the filtering might be able to remove some of this lower quality data. We also experimented with applying different, and often â€œlighterâ€, deduplication approaches on top of the individually deduplicated dumps. You can read about them further below.

#### A note on measuring the effect of deduplication

Given the nature of deduplication, its effect is not always very visible in a smaller slice of the dataset (such as 28B tokens, the size we used for our filtering ablations). Furthermore, one must consider the fact that there are specific effects at play when deduplicating across all CommonCrawl dumps, as some URLs/pages are recrawled from one dump to the next.

To visualize the effect of scaling the number of training tokens on measuring deduplication impact, we considered the following (very extreme and unrealistic regarding the degree of duplication observed) theoretical scenario:

- there are 100 CommonCrawl dumps (roughly accurate)

- each dump has been perfectly individually deduplicated (every single document is unique in this dump)

- each dump is a perfect copy of each other (maximum possible duplication across dumps, effectively the worst case scenario)

- each dump has 200 billion tokens (for a total of 20 trillion, the resulting size of our individual dedup above)

- each dump is made up of documents of 1k tokens (200M documents per dump)

We then simulated uniformly sampling documents from this entire dataset of 20 trillion tokens, to obtain subsets of 1B, 10B, 100B, 350B and 1T tokens. In the image below you can see how often each document would be repeated.

1B10B100B350B1T00.20.40.60.81

Sampling from 1000 identical buckets with 200B tokens each# duplicates16-328-164-8321Sample sizeDataset fraction

[](https://plotly.com/)

For 1B almost all documents would be unique (#duplicates=1), despite the fact that in the entire dataset each document is repeated 100 times (once per dump). We start seeing some changes at the 100B scale (0.5% of the total dataset), with a large number of documents being repeated twice, and a few even 4-8 times. At the larger scale of 1T (5% of the total dataset), the majority of the documents are repeated up to 8 times, with some being repeated up to 16 times.

We ran our performance evaluations for the deduplicated data at the 350B scale, which would, under this theoretical scenario, be made up of a significant portion of documents duplicated up to 8 times. This simulation illustrates the inherent difficulties associated with measuring deduplication impact on the training of LLMs, once the biggest duplicate clusters have been removed.

#### Other (failed) global approaches

To build on top of our newly found method (independently deduplicating each dump). We attempted to improve the performance by further deduplicating the independently minhash deduped 20 trillion tokens of data with alternative global (over all dumps) deduplication methods. We explored the following approaches:

- URL deduplication, where we only kept one document per normalized (lowercased) URL (71.5% of tokens removed, 5.6T left) â€”Â _FineWeb URL dedup_

- Line deduplication:
    
    - remove all but 1 (randomly chosen) occurrence of each duplicated line (77.8% of tokens dropped, 4.4T left) â€”Â _FineWeb line dedup_
    
    - same as above, but only removing duplicate lines with at least 10 words and dropping documents with fewer than 3 sentences after deduplication (85% of tokens dropped, 2.9T left) â€”Â _FineWeb line dedup w/ min words_
    
    - remove all but 1 occurrence of each span of 3 duplicated lines with each number treated as 0 when finding duplicates, (80.9% of tokens removed, 3.7T left) â€”Â _FineWeb 3-line dedup_

The performance of the models trained on each of these was consistently worse (even if to different degrees) than that of the original independently deduplicated data:

01002003000.360.380.40.420.440.460.48

Attempting to further globally dedup worsened perfFineWeb independent MinHashRefinedWebFineWeb line dedup w/ min wordsFineWeb URL dedupFineWeb line dedupFineWeb 3-line dedupFineWeb full MinHashFineWeb filtered onlyTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

### Additional quality filtering

By this point we had reached the same performance of the previous work we attempted to reproduce and extend: RefinedWeb, using our base filtering and independent MinHash. Still, on our aggregate of tasks, another heavily filtered dataset, the C4 dataset

[30]

, still showed stronger performances on some benchmarks of our evaluation suite.

We therefore set out to find new filtering steps that would, at first, allow us to match the performance of C4 and, at a second stage, surpass it. A natural starting point was to look into the processing of C4 itself.

#### C4: A dataset that has stood the test of time

TheÂ [C4 dataset](https://huggingface.co/datasets/c4)Â was first released in 2019. It was obtained from theÂ `2019-18`Â CommonCrawl dump by removing non english data, applying some heuristic filters on both the line and document level, deduplicating on the line level, and removing documents containing words from a word blocklist.

Despite its age and limited size for current standards (around 175B gpt2 tokens), this dataset is, to this day, a common sub-set of typical LLM training, being used in models such as the relatively recent Llama1

[31]

. This success is due to the strong performance that models trained on this dataset exhibit, excelling in particular on the Hellaswag benchmarkÂ 

[14]

, one of the benchmarks in our â€œearly signalâ€ group with the highest signal-to-noise ratio. We experimented applying each of the different filters used in C4 to a baseline of the independently deduped FineWeb 2019-18 dump:

051015200.30.350.40.45

C4 filtering effect on HellaSwagAll filtersC4All filters except terminal_punctterminal_punct filterword_lengths filtercurly_bracket filterbaselineTraining tokens (billions)HellaSwag

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

3

- applying â€œAll filtersâ€ (drop lines not ending on punctuation marks, mentioning javascript and cookie notices + drop documents outside length thresholds, containing â€œlorem ipsumâ€ or a curly bracket,Â `{`) allows us to match C4â€™s HellaSwag performance ("All filters" vs "C4" curves, respectively).

- The curly bracket filter, and the word lengths filter only give a small boost, removing 2.8% and 4.3% of tokens, respectively

- The terminal punctuation filter, by itself, gives the biggest individual boost, but removesÂ _around 30%_Â of all tokens (!)

- The lorem_ipsum, javascript and policy rules each remove <0.5% of training tokens, so we did not train on them individually

- "All filters except the (very destructive) terminal_punct" performs better than terminal_punct by itself, while removing less in total (~7%)

We decided to apply all C4 filters mentioned above except the terminal punctuation one. We validated these results with a longer run, which you will find in a plot in the next section.

#### A statistical approach to develop heuristic filters

To develop new heuristic filters and select their thresholds we devised a systematic process:

1. we started by collecting a very large list of high level statistics of our datasets (overÂ **fifty**Â different metrics) ranging from common document-level metrics (e.g. number of lines, avg. line/word length, etc) to inter-document repetition metrics (inspired by MassiveText), on both a high quality and a lower quality web dataset;
2. we selected the metrics for which the Wasserstein distance between the two distributions (of the metric computed on each dataset) was larger;
3. we inspected the histograms of the two distributions and empirically chose a threshold that would make the lower quality dataset more closely resemble the higher quality one on this metric;
4. we validated the resulting filter (metric-threshold pair) by using it on a reference dataset and running small ablations.

Due to our (new) assumption that global MinHash greatly upsamples lower quality data in the oldest dumps, we computed metrics on both the independently MinHashed and the (worse quality) global MinHashed versions of the 2013-48 and 2015-22 crawls (two older crawls). We then compared the statistics at a macro level, by looking at the distribution of these metrics for each one.

Perhaps not too surprisingly given our findings for deduplication, we found significant disparities in most of the metrics for the two deduplication methods. For instance, theÂ `line-char-duplicates`Â metric (nb. of characters in duplicated lines / nb. characters), roughly doubled from the independent dedup (0.0053 for 2015-22 and 0.0058 for 2013-48), to the global dedup (0.011 for 2015-22 and 0.01 for 2013-48), indicating that the latter had higher inter-document repetition.

Following the process listed above for these datasets yieldedÂ **seventeen**Â candidate metric-threshold pairs. In the image below, you can see three of these histograms:

00.20.40.60.8100.020.040.060.080.10.120.14

Histograms of selected metricsFull MinHash CC-MAIN-2013-48Independent MinHash CC-MAIN-2013-48Fraction of lines ended with punctuationDocument FrequencyFiltered out

[](https://plotly.com/)

Metric:Lines Ended With PunctuationLines CharsShort Lines

As an example, we inspected the histograms of "fraction of lines ending with punctuation" (see the image above) and observed an increased document density of global MinHash at around 0.12. We then filtered with this threshold and found that the removed data had a higher amount of short lists or consisted of only document layout text ("Home", "Sign up", etc).

We then assessed the effectiveness of these seventeen newly created filters, by conducting several of ourÂ _28 billion tokens_Â ablation runs on theÂ _2019-18 crawl_. Out of all those runs, we identifiedÂ **three**Â filters (the ones based on the histograms above) that demonstrated the most significant improvements on the aggregate score:

- Remove documents where the fraction of lines ending with punctuation â‰¤ 0.12 (10.14% of tokens removed) â€” vs the 30% from the original C4 terminal punct filter

- Remove documents where the fraction of characters in duplicated lines â‰¥ 0.1 (12.47% of tokens removed) â€” the original MassiveText threshold for this ratio is â‰¥ 0.2

- Remove documents where the fraction of lines shorter than 30 characters â‰¥ 0.67 (3.73% of tokens removed)

- When applying the three together, ~22% of tokens were removed.

051015200.360.370.380.390.40.410.420.43

Custom filters PerformanceFilters combinedPunctuation filterLine duplicates filterShort lines filterBaselineTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

3

These filters allowed us to further improve performance and to, notably, surpass the C4 dataset performance while providing a much larger dataset at the same time.

### The final ğŸ· FineWeb dataset

The finalÂ [ğŸ· FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)Â dataset comprises 15T tokens and includes the following previously mentioned steps, in order, each providing a performance boost on our group of benchmark tasks:

- base filtering

- independent MinHash deduplication per dump

- a selection of C4 filters

- our custom filters (mentioned in the previous section)

01002003000.380.40.420.440.460.48

The different FineWeb processing stepsFineWeb: id mh + C4 + custom filtersFineWeb: id mh + C4 filtersFineWeb: independent MinHash (id mh)FineWeb: base filtering onlyTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

#### Comparisons with other web-scale datasets

We comparedÂ [ğŸ· FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)Â with the following datasets that are usually considered the highest quality openly accessible web-scale datasets (we also indicate for each the approximate number of tokens in the public version of the dataset):

- [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)Â (500B tokens)
    
    [22]
    

- [C4](https://huggingface.co/datasets/allenai/c4)Â (172B tokens)
    
    [30]
    

- [Dolma v1.6](https://huggingface.co/datasets/allenai/dolma)Â (3T tokens) (the CommonCrawl part)Â 
    
    [32]
    
    Â 13

- [The Pile](https://huggingface.co/datasets/EleutherAI/pile)Â (340B tokens)Â 
    
    [33]
    

- [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B)Â (627B tokens)Â 
    
    [34]
    

- [RedPajama2](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)Â (20T tokens)Â 
    
    [35]
    
    Â (deduplicated)

- and our newÂ [ğŸ· FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)Â (15T tokens) (this report)

You will find the 350B-tokens-trained ablation models openly accessible and gathered inÂ [this collection](https://huggingface.co/collections/HuggingFaceFW/ablation-models-662457b0d213e8c14fe47f32). We have uploaded checkpoints at every 1000 training steps. You will also find our fullÂ [evaluation results here](https://huggingface.co/datasets/HuggingFaceFW/fineweb/blob/main/eval_results.csv).

01002003000.360.380.40.420.440.460.48

Dataset ablationsFineWeb (ours)RefinedWebC4DolmaSlimPajamaRedPajama2The PileTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

ğŸ· FineWeb is thus â€“ to the best of our knowledge â€“ the open dataset leading to the current highest model performances while allowing to train on several trillion tokens.

## ğŸ“š FineWeb-Edu

1002003000.380.40.420.440.460.480.5

Dataset ablationsFineWeb-EduFineWebRefinedWebC4DolmaSlimPajamaRedPajama2The PileTraining tokens (billions)Aggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

Rolling window:

5

ğŸ“š FineWeb-Edu outperforms ğŸ· FineWeb and all other open web datasets on our group of evaluation tasks.

[ğŸ“š FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)Â is an additional development of FineWeb that we are excited to introduce in this tech report and openly release. ğŸ“š FineWeb-Edu is based on a new approach that has recently emerged for filtering LLM training datasets: using synthetic data to develop classifiers for identifying educational content. This technique was notably used in the trainings of Llama 3

[1]

Â and Phi3

[36]

, but its large-scale impact on web data filtering has, in our opinion, thur far not been publicly explored to its full potential.

The popular Phi3 models were trained on 3.3 and 4.8 trillion tokens, with the paper

[36]

Â stating:

> Our training data consists of heavily filtered publicly available web data (according to the 'educational level') from various open internet sources, as well as synthetic LLM-generated data.

Similarly, Llama 3 blog post

[37]

Â notes:

> We found that previous generations of Llama are good at identifying high-quality data, so we used Llama 2 to help build the text-quality classifiers that are powering Llama 3.

However, these classifiers and filtered datasets are not publicly available. To further enhance ğŸ· FineWeb's quality, we developed an educational quality classifier using annotations generated byÂ [Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)Â to createÂ [**ğŸ“š FineWeb-Edu**](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu).

### Annotating for educational quality at scale

We usedÂ [Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)Â to annotate 500k samples from ğŸ· FineWeb, scoring each for their educational quality on a scale from 0 to 5.

We explored various prompt formats to automatically extract an educational score using an LLM and found that the additive scale by Yuan et al.

[38]

Â worked best. This scale allows the LLM to reason about each additional point awarded, unlike the single-rating Likert scale which fits samples into predefined boxes. Then, to avoid the LLM favoring highly technical pages like arXiv abstracts and submissions, we focused on grade-school and middle-school level knowledge. By setting a threshold of 3 (on a scale of 0 to 5) during the filtering process, we were able to also retain some high-level educational pages.

![Prompt for LLM annotation](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/fjZQ4izIj1rx1xQnBTKKr.png)

Prompt used for Llama3 annotations of the educational score, also availableÂ [here](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier/blob/main/utils/prompt.txt).

In terms of open-weight models to use for annotating the data, we experimented with several models includingÂ [Mixtral-8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)Â andÂ [Mixtral-8x22B-Instruct](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1),Â [Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)Â as well as a jury gathering the scores from these three models

[39]

. In our experiments we found that using Llama3 alone gave the most reliable results.

### Training a classifier

To scale our annotations to the trillions of tokens in FineWeb, we used the Llama3-70B annotations to train a small classifier. The model we used was aÂ [Snowflake-arctic-embed](https://huggingface.co/Snowflake/snowflake-arctic-embed-m)Â embedding model with a classification head with a single regression output on top of it. We trained this model on the 450,000 Llama 3 annotations for 20 epochs with a learning rate of 3e-4, freezing the embedding and encoder layers. We saved the checkpoint with the highest F1 score on our held-out validation set of 45k samples, treating Llama 3 annotations as ground-truth. After training, we rounded the scores to integers fromÂ `0`Â toÂ `5`.

We then converted the problem to a binary classification task by using a fixed threshold to determine if a file is educational. With a threshold ofÂ `3`, the model achieved an F1 score of 82% on the validation set, indicating strong performance in distinguishing high-quality educational content.

The classifier is available at:Â [HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier). The training and inference code is available onÂ [GitHub](https://github.com/huggingface/cosmopedia/tree/main/classification).

### Filtering and results

We applied the classifier to the 15T tokens of ğŸ· FineWeb, a process that required 6,000 H100 GPU hours. We investigated the impact of using different thresholds for the filtering and found that using a threshold ofÂ `3`Â gave the best overall results. Although using a threshold higher thanÂ `3`Â improves performance on knowledge and reasoning intensive benchmarks, it significantly degrades performance on HellaSwag and PIQA. The plot below shows the performance of each threshold compared to FineWeb on six different benchmarks; it uses a 1.82B model trained on 8B tokens.

FW-Edu-threshold=4FW-Edu-threshold=3FW-Edu-threshold=2FineWeb (FW)0.240.260.280.30.32

FineWeb-Edu thresholdingDatasetMMLU

[](https://plotly.com/)

Metric:HellaSwagARCMMLUOpenBook QAPIQASocial IQAWinoGrande

**Note:**Â this ablation was conducted on 8B tokens from the 2024-10 dump for both the FineWeb and FineWeb-Edu subsets, which might not be representative of the entire dataset. The next ablation shows that the findings for threshold 3 hold on a longer run of 350B tokens from all FineWeb dumps, except for HellaSwag, where we noticed a slight performance degradation.

We built ğŸ“š FineWeb-Edu by filtering out samples with scores lower than 3. This removed 92% of the dataset, leaving us with 1.3 trillion educational tokens. To evaluate the effectiveness of this filtering at a larger scale, we conducted an ablation using a 1.82B model trained on 350 billion tokens, similar to the FineWeb filtering ablation mentioned above:

C4DolmaFineWebRedPajama2RefinedWebSlimPajamaThe PileFineWeb-Edu0.250.30.350.4

Evaluation results at 350B tokensDatasetMMLU

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QAPIQASocial IQAWinoGrande

Here are the key highlights of the ablation results above:

- ğŸ“š FineWeb-EduÂ **surpasses ğŸ· FineWeb and all other open web datasets, with remarkable improvements on educational benchmarks**Â such as MMLU, ARC, and OpenBookQA.
- It achieves the same performance with significantly less data, requiring 10x fewer tokens compared to C4 and Dolma to match MMLU results.
- This demonstrates the effectiveness of using classifiers trained on LLM annotations for large-scale data filtering.

Given that a threshold of 2 also demonstrated strong performance while retaining more data, we are releasing an additional dataset filtered with this threshold, containing 5.4 trillion tokens underÂ [HuggingFaceFW/fineweb-edu-score-2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu-score-2).

You can find the two datasets along with the classifier used for the filtering in thisÂ [collection](https://huggingface.co/collections/HuggingFaceFW/fineweb-edu-6659c3f3d399d0e1d648adfd).

## Bonus: CommonCrawl over time

> Just like fine wine, not all crawls are created equal.

While ablating filtering steps, we noticed that certain crawls outperformed others by a significant margin. We decided to investigate this phenomenon.

### Benchmark performance by crawl

For each crawl, we trained two 1.8B models on 27 billion tokens randomly sampled from that crawl's data (after the base filtering and MinHash deduplication steps), where each run had a different random 27BT sampling of this data. We trained 192 such models, totaling over 60 thousand H100 GPU-hours. We subsequently took the last 3 checkpoints for both runs and plotted the average of these 6 data points per crawl.

The plot below clearly shows that some dumps perform far worse than others. Each year has a different color, and the number of crawls per year also varies.

2013201420152016201720182019202020212022202320240.420.4250.430.435

Score by dumpYearAggregate Score

[](https://plotly.com/)

Metric:Aggregate ScoreHellaSwagARCMMLUOpenBook QACommonsense QAPIQASocial IQAWinoGrande

We investigated possible causes for this behaviour such as changes in the most common URLs of each dump, as well as potential benchmark contamination, but could not find any conclusive explanation. We leave further investigation for future work.

### Synthetic data

We wondered if the strong performance of the last few crawls could be, in part, attributed to the presence of a larger quantity of synthetic data (data generated by LLMs). Such a change would not be surprising due to the recent increase in popularity of LLMs, notably of ChatGPT.

Since, to the best of our knowledge, there is no foolproof method to detect synthetic data, we opted to use a proxy metric: we measured the frequency of the following words in each crawl:Â `"delve", "as a large language model", "it's important to note", "rich tapestry", "intertwined", "certainly!", "dive into"`, all of which are commonly used by ChatGPT.

It is important to note that not all samples containing one of these phrases were necessarily generated by ChatGPT (and also that many ChatGPT generated samples do not contain any of these phrases), but assuming that the amount of synthetic data were to not change across crawls, one would expect these frequencies to remain approximately constant over time.

The results are shown in the following plot:

2021-042021-102021-172021-212021-252021-312021-392021-432021-492022-052022-212022-272022-332022-402022-492023-062023-142023-232023-402023-502024-102024-1805Î¼10Î¼15Î¼20Î¼0.4240.4260.4280.430.4320.4340.4360.438

Synthetic Data ContaminationYearSynthetic proxy Words RatioAggregate ScoreChat-GPT Release

[](https://plotly.com/)

While the frequency remained approximately constant until 2023-14 (ChatGPT was released at the end of 2022), we find a steep increase of our proxy metric in recent crawls. While this simple test is not enough to conclude that ChatGPT completions and other synthetic data is improving the quality of the most recent crawl, it at the very least does not seem to drastically harm it.

We expect to continue seeing increasing quantities of synthetic data on new CC crawls. However, while for relatively small trainings this data does not seem to harm performance (and might actually improve it), it is not clear that this holds for much larger trainings.

## Conclusion and looking forward

Through our open science efforts we hope to keep shining a light on the black box that is the training of high performance large language models as well as to give every model trainer the ability to create state-of-the-art LLMs. We are excited to continue iterating on FineWeb and to release increasingly better filtered subsets of web data, in a fully open and reproducible manner.

In the short term, we are looking forward to applying the learnings from (English) FineWeb to other languages. While English currently dominates the LLM landscape, we believe that making high quality web data in other languages as accessible as possible would be incredibly impactful.

In a nutshell: the future is bright and exciting for studying the science of creating datasets at scale and in the open ğŸ¤—.

### Citation

For attribution in academic contexts, please cite this work as

Penedo, et al., "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale", 2024.

BibTeX citation

@inproceedings{
penedo2024the,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}

### Footnotes

1. Note that the size changes from crawl to crawl. Note also that we use "dump" or "crawl" interchangeability in this report.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-1)
2. We have not processed these 3 older crawls.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-2)
3. Note that this report is focused on the special field of web-scale datasets ("web-scale" typically meaning >100 billion tokens obtained from the web) used to pretrain a Large Language Model (by pretraining we mean the very first step in the training of a model, starting from random weights). We don't pretend to cover any other field of dataset creation nor that the lessons or hypothesis we develop in this document can extend to any field besides this specific field.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-3)
4. Even though as we mentioned above the notion of "clean" is so ill-defined that it should probably not been seen as equivalent to wikipedia-type of text[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-4)
5. "Small" in comparison to standard sizes of today's LLMs, i.e. small in comparison to 7-70 billion parameters. In this work "small" means about 1-2 billion parameters[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-5)
6. In particular we suspect that it keeps too much boilerplate content and navigation menus.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-6)
7. We used trafilatura default options withÂ `favour_precision=True`.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-7)
8. As everywhere in this report: this is the number of tokens when tokenized with theÂ `gpt2`Â tokenizer[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-8)
9. Note that here, even when we discuss "fuzzy" deduplication, we are only employing methods that operate on character/word matches, aka surface-level text. A more complex concept of deduplication is concerned with "semantic" deduplication: comparing/removing texts which are relative to the same concepts and use for instance synonyms or paraphrasing. We don't discuss these topics here but note that they can be important in the field of large-scale synthetic data generation for instance (see ourÂ [Cosmopedia release](https://huggingface.co/blog/cosmopedia)Â on this topic)[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-9)
10. Our units are "words", computed in theÂ [MinHash processing function](https://github.com/huggingface/datatrove/blob/e9963f69f1fbab1a61339bd1b497f6e138b9f47f/src/datatrove/pipeline/dedup/minhash.py#L196)Â with aÂ [language-specific word tokenizer](https://github.com/huggingface/datatrove/blob/e9963f69f1fbab1a61339bd1b497f6e138b9f47f/src/datatrove/utils/word_tokenizers.py#L323).[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-10)
11. While there may be documents inÂ _originally kept data_Â similar to documents inÂ _originally removed data_, we estimate the overlap to be small (around 4 billion tokens)[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-11)
12. Note that these ablation models are trained only on data from this dump so it's considered independently of all the other dumps.[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-12)
13. There is a newer version of Dolma, v1.7, which is smaller[[â†©]](https://huggingfacefw-blogpost-fineweb-v1.static.hf.space/dist/index.html#d-footnote-13)

### References

1. Language Models are Unsupervised Multitask Learners  
    Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever, I., 2019.
2. DataTrove: large scale data processingÂ â€‚[[link]](https://github.com/huggingface/datatrove)  
    Penedo, G., KydlÃ­Äek, H., Cappelli, A., Sasko, M. and Wolf, T., 2024. GitHub repository. GitHub.
3. Measuring Data  
    Mitchell, M., Luccioni, A.S., Lambert, N., Gerchick, M., McMillan-Major, A., Ozoani, E., Rajani, N., Thrush, T., Jernite, Y. and Kiela, D., 2023.
4. A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity  
    Longpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A., Zoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D. and Ippolito, D., 2023.
5. CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data  
    Wenzek, G., Lachaux, M., Conneau, A., Chaudhary, V., GuzmÃ¡n, F., Joulin, A. and Grave, E., 2019.
6. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research  
    Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A.H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M.E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N.A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J. and Lo, K., 2024.
7. Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference  
    Chiang, W., Zheng, L., Sheng, Y., Angelopoulos, A.N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J.E. and Stoica, I., 2024.
8. Training language models to follow instructions with human feedback  
    Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J. and Lowe, R., 2022.
9. Training Compute-Optimal Large Language Models  
    Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G.v.d., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J.W., Vinyals, O. and Sifre, L., 2022.
10. CommonsenseQA: A Question Answering Challenge Targeting Commonsense KnowledgeÂ â€‚[[link]](https://aclanthology.org/N19-1421)  
    Talmor, A., Herzig, J., Lourie, N. and Berant, J., 2019. Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149--4158. Association for Computational Linguistics.Â [DOI: 10.18653/v1/N19-1421](https://doi.org/10.18653/v1/N19-1421)
11. HellaSwag: Can a Machine Really Finish Your Sentence?Â â€‚[[link]](https://aclanthology.org/P19-1472)  
    Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y., 2019. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791--4800. Association for Computational Linguistics.Â [DOI: 10.18653/v1/P19-1472](https://doi.org/10.18653/v1/P19-1472)
12. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering  
    Mihaylov, T., Clark, P., Khot, T. and Sabharwal, A., 2018. EMNLP.
13. PIQA: Reasoning about Physical Commonsense in Natural Language  
    Bisk, Y., Zellers, R., Bras, R.L., Gao, J. and Choi, Y., 2019.
14. SocialIQA: Commonsense Reasoning about Social Interactions  
    Sap, M., Rashkin, H., Chen, D., LeBras, R. and Choi, Y., 2019.
15. WinoGrande: An Adversarial Winograd Schema Challenge at Scale  
    Sakaguchi, K., Bras, R.L., Bhagavatula, C. and Choi, Y., 2019.
16. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge  
    Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C. and Tafjord, O., 2018.
17. Measuring Massive Multitask Language Understanding  
    Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J., 2021.
18. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and ExtractionÂ â€‚[[link]](https://aclanthology.org/2021.acl-demo.15)  
    Barbaresi, A., 2021. Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122--131. Association for Computational Linguistics.
19. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only  
    Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E. and Launay, J., 2023.
20. Bag of Tricks for Efficient Text Classification  
    Joulin, A., Grave, E., Bojanowski, P. and Mikolov, T., 2016. arXiv preprint arXiv:1607.01759.
21. FastText.zip: Compressing text classification models  
    Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jegou, H. and Mikolov, T., 2016. arXiv preprint arXiv:1612.03651.
22. Scaling Language Models: Methods, Analysis & Insights from Training Gopher  
    Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G.v.d., Hendricks, L.A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., d'Autume, C.d.M., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., Casas, D.d.L., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K. and Irving, G., 2022.
23. Deduplicating Training Data Makes Language Models Better  
    Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C. and Carlini, N., 2022.
24. Quantifying Memorization Across Neural Language Models  
    Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F. and Zhang, C., 2023.
25. Scaling Data-Constrained Language Models  
    Muennighoff, N., Rush, A.M., Barak, B., Scao, T.L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T. and Raffel, C., 2023.
26. Scaling Laws and Interpretability of Learning from Repeated Data  
    Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., Johnston, S., Mann, B., Olah, C., Olsson, C., Amodei, D., Joseph, N., Kaplan, J. and McCandlish, S., 2022.
27. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer  
    Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2023.
28. LLaMA: Open and Efficient Foundation Language Models  
    Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E. and Lample, G., 2023.
29. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research  
    Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A.H., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M.E., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, P., Zettlemoyer, L., Smith, N.A., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J. and Lo, K., 2024. arXiv preprint.
30. The {P}ile: An 800{GB} dataset of diverse text for language modeling  
    Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N. and others,, 2020. arXiv preprint arXiv:2101.00027.
31. SlimPajama: A 627B token cleaned and deduplicated version of RedPajamaÂ â€‚[[link]](https://huggingface.co/datasets/cerebras/SlimPajama-627B)  
    Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J.R., Hestness, J. and Dey, N., 2023.
32. RedPajama: an Open Dataset for Training Large Language ModelsÂ â€‚[[link]](https://github.com/togethercomputer/RedPajama-Data)  
    Computer, T., 2023.
33. Phi-3 technical report: A highly capable language model locally on your phone  
    Abdin, M., Jacobs, S.A., Awan, A.A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H. and others,, 2024. arXiv preprint arXiv:2404.14219.
34. Our responsible approach to Meta AI and Meta Llama 3Â â€‚[[link]](https://ai.meta.com/blog/meta-llama-3-meta-ai-responsibility/)  
    Meta,, 2024.
35. Self-rewarding language models  
    Yuan, W., Pang, R.Y., Cho, K., Sukhbaatar, S., Xu, J. and Weston, J., 2024. arXiv preprint arXiv:2401.10020.
36. Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models  
    Verga, P., Hofstatter, S., Althammer, S., Su, Y., Piktus, A., Arkhangorodsky, A., Xu, M., White, N. and Lewis, P., 2024. arXiv preprint arXiv:2404.18796.