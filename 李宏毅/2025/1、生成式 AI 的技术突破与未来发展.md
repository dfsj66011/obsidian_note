
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；


那在我们的作业中 你就会体验到 到底AI agent做不做得了 机器学习这门课的作业 那最近呢 Google说他们做了一个AI 不过他们并没有真的 释出模型啦,所以你也不知道说 实际上做得怎么样,这个服务并不是公开的 那他们说他们做了一个AI Coscientist 就是用AI来做研究 不过这个AI Cosine 还是蛮有侷限的,他不能真的做实验啦 他只能够提Proposal 就是你把一些研究的想法告诉他 他把完整的Proposal规划出来 实际上做得怎么样,不知道啦 那你要看他的Blog里面有些比较 夸张的案例,说什么 本来人类要花十年才能够得到研究成果 AI agent花两天就得到了,也不知道真的还假的 他举的是一些生物学的例子,所以我也无法判断 他讲的是不是真的,那个发现是不是真的很重要
              
                  22:50
                  这个co-scientist的话 就是这个用AI agent来帮研究人员做研究 好,那我们刚才讲的AI agent 他的互动方式是侷限在回合制的互动 有一个observation,接下来执行action 有一个observation,接下来执行action 但是在更真实的情境下 这个互动是需要及时的 因为外在的环境也许是不断在改变的 如果你在action还没有执行完的时候 外在环境就改变了 那应该要怎么办呢 有没有办法做到更即时的互动呢 更即时的互动可能应该像是这样子 当模型在决定要执行action one 正在执行的过程中 突然外在环境变了 这个时候模型应该有办法 立刻转换行动 改变他的决策 以因应外界突如其来的变化 你可以想说 什么样的状况 我们会需要用到这样的AI agent 能够做即时互动的呢 其实语音对话就需要这种互动的模式 文字的对话使用切GPT是大家比较熟悉的
              
                  23:58
                  你输入一段文字 他就输出一段文字 这是一来一往回合制的互动 但是人与人间真正的对话不是这样子的 当两个人在对话的时候 他们可能会互相打断 或者是其中一个人在讲话的时候 另外一个人可能会同时提供一些回馈 比如说嗯好你说的都对 那这些回馈可能没有什么 特别语意上的含义 他只是想要告诉对方我有在听 但是像这样子的回馈 对于流畅的交流来说 也是非常重要的 如果在讲电话的时候对方完全都没有回忆 你会怀疑他到底有没有在听 所以我们今天能不能够让AI 在跟使用者互动的时候 用语音互动的时候 就跟人与人间的互动一样 而不是一来一往回合制的互动呢 其实也不是不可能的 今天GPT4O的一个Voice Mode 高级语音模式 也许在某种程度上 就做到了这一种即时的互动 那这个投影片上是举一个例子 假设有人跟AI说 你说一个故事
              
                  25:01
                  那这个是AI观察到的 第一个observation 有人叫他说一个故事 现在就开始讲故事了 他就说从前从前 那这时候人说了一个 好 这个可能是第二个observation 但AI要知道说 这个observation 不需要改变他的行为 跟他的行为没有直接的关系 只要故事就继续讲下去 有一个小镇 然后人说这个不是我要听的故事 这个我听到了 那AI可能要马上知道说 那这个不是人要听的 那也许我觉得应该停下来 换另外一个故事 那今天AI有没有办法做到这种即时的互动呢 那怎么做这种即时的互动 非回合制的互动 就有点超过我们这门课想要讲的范围 如果你有兴趣的话 你可以读这篇文章 那这篇文章想要做的事情 是评量现在这些语音模型互动的能力 那在这篇文章里面 也对现有的这个 可以做互动的语音模型 做了一个比较完整的survey 是一直survey到今年的1月 所以你可以看这篇文章 知道说现在这些可以互动的模型 他可以做到什么样的地步
              
                  26:06
                  那这是我们实验室的林冠廷同学 跟他在这个Berkeley UW和NIT的合作伙伴 一起做的文章 那这边顺便说明一下 以后这门课呢 我们投影片上引用论文的原则 论文的原则就是 如果我找得到arXiv的连结的话 那我就把文章直接 我就直接贴arXiv的连结 什么是arXiv呢 假设你不是Computer Science背景的话 也许我就要解释一下什么是arXiv arXiv的意思就是 一般呢 做研究你是写完文章 投稿到一个期刊 或者是国际会议 然后被接受以后才发表出来 但是对于AI的领域 因为变化实在太快 几个月前 就已经是古代了 所以期刊那种 一审就要一年 或者是国际会议一两个月 这种步调 是没有办法在不适用于AI的领域 所以现在一种习惯的发表方式 就是做出东西以后 直接放到一个公开的网站 叫做arXiv 然后就不审了
              
                  27:09
                  立刻公开 然后你就可以让全世界的人 看到你的文章 那有很多人会觉得 引用arXiv的连结不够正式 但是很多重要的文章 其实现在不见得投稿国际会议 但就只有arXiv的连结 所以我会选择 如果找得到arXiv的连结的话 就直接引用arXiv的连结 其实现在大家都在arXiv上看文章 那国际会议现在比较像是 经典回顾这样子 每篇文章我几乎都在arXiv上看过了 句子说原来你投到这里啊 这样的感觉 那引用arXiv的连结 还有一个好处就是你可以直接从arXiv的连结 看出这篇文章的时间 所以arXiv的连结里面的数字 前面两个就是年份 后面两个就是月份 可以看这个数字就可以知道说这篇文章是在什么时候被放在arXiv 也就是什么时候被发表的 可以让你对于每一个研究 他诞生的时间更有感觉 好那接下来呢 我们会分三个面向来剖析今天这些AI agent的关键能力 那第一个面向是我们要来看这些AI agent 这个AI agent能不能够根据他的经验
              
                  28:15
                  过去的互动中所获得的经验 来调整他的行为 第二部分是要讲这些AI agent 如何呼叫外部的援助 如何使用工具 第三部分要讲AI agent 能不能够执行计画 能不能做计画 那我们来讲一下 AI怎么根据 过去的经验 或者是环境的回馈 来调整他的行为 那AI呢 AI agent需要能够根据经验来调整行为 比如说有一个作为AI programmer的AI agent 他一开始接到一个任务 那他写了一个程式 那这个程式compile以后 有错误讯息 compile以后有error 那应该要怎么办呢 他应该要能够根据这个error的message 来修正他之后写的程式 那在过去啊 讲到说 你收到一个feedback接下来要做什么的时候 也许多数机器学习的课程 都是告诉你
              
                  29:18
                  来调整参数 根据这些收集到的训练资料 也许使用reinforcement learning的algorithm 来调整参数 但不要忘了我们刚才就强调过 在这一堂课里面 没有任何模型被训练 所以我们今天不走这个路线 那不更新模型的参数 模型要怎么改变它的行为呢 依照今天 Large Language Model的能力 要改变它的行为 你也不用微调参数 直接把错误的讯息给他 他接下来写的程式就会不一样了 就结束了 那可能会问说 那之前他写的程式是错的 为什么给错误讯息 他写的程式就对了呢 明明就是同一个模型 但你想想看 模型做的事情就是文字接龙 你给他不同的输入 他接出来的东西就不一样 一开始会写错的程式 是因为他前面要接的部分 只有这么多 所以写个错的程式 当今天要接的内容 包含了错误的讯息的时候
              
                  30:21
                  他接出来的结果 可能就会是正确的了 那今天已经有太多的证据 说明这些语言模型 可以根据你给他的回馈 改变他的行为 不需要调整参数 那如果你有使用这些语言模型的经验 你也不会怀疑 他们有根据你的回馈调整行为的能力 那这边真正的议题是 如果我们是把过去所有的经验 都存起来 要改变语言模型的行为 要让他根据过去的经验调整行为 就是把过去所有发生的事情一股脑给他 那就好像是语言模型每次做一次决策的时候 他都要回忆他一生的经历 也许在第100步的时候还行 到第1万步的时候 过去的经验太长了 他的人生的资讯已经太多了 也许他没有足够的算力 来回顾一生的资讯 他就没有办法得到正确的答案 这让我想到什么呢 这让我想到有一些人 有超长自传式记忆 他可以把他一生中
              
                  31:24
                  所有发生的事情记下来 然后那些人 你可以随便问他一个 某个人的电话号码 他都会背出来 你告诉他某年某日某时 发生了什么事 他也都可以讲出来 有一些人 他的头脑 就像是一个影印机一样 会把所有他看过的事情 都远风不动的记忆下来 但这种超长自传式记忆啊 又被叫做超忆症 你看到症这个字就知道说 人们觉得这是一种疾病 这听起来记忆力很好是一种祝福 但实际上对这些患者而言 据说这种患者世界上可能不到100例 那这是一个2006年的时候 才被论文发表的一个症状 那据说这些患者其实日常生活并没有办法过得很开心 因为他们不断的在回忆他的人生 往往一不小心就陷入了一个冗长的回忆之中 那也很难做抽象的思考 因为他的人生已经被他的记忆已经被太多 知为末节的所事所占据 所以没有办法做抽象式的思考 所以让一个AI agent记住他一生所有经历的事情
              
                  32:32
                  告诉他你每次做一个决策的时候 都是根据你一生所有经历过的事情再去做决策 那也许对AI agent来说并不是一件好事 最终当他的人生过长的时候 他会没有办法做出 正确的决策 所以怎么办呢 也许我们可以给这些AI agent memory 这就像是人类的长期记忆一样 发生过的事情 我们把它存到这个memory里面 当AI agent看到 第一万个observation的时候 他不是根据所有存在memory里面的内容 去决定接下来要采取什么action 而是有一个叫做read的模组 这个read的模组 会从memory里面 选择跟现在要解决的问题有关系的经验 把这些有关系的经验放在observation的前面 让模型根据这些有关系的经验跟observation 再做文字接龙 接出它应该进行的行为 那你有这个read的模组 就可以从memory里面
              
                  33:36
                  从长期记忆中筛选出重要的讯息 让模型只根据这些跟现在情境相关的讯息来进行决策 那怎么样打造这个read的模组呢 其实你可以想这个read的模组 就想成是一个retrieval的system 想成是一个检索的系统 那第一万步看到的observation其实就是问题 那模型的AI agent的memory长期记忆 其实就是资料库 那你就把拿这个检索系统 根据这个问题从这个资料库里面 检索出相关的资讯 那这整个技术跟RID 没有什么不同 其实它就是RAG 你可以直接把RAG的任何方法 直接套用到这个地方 唯一不一样的地方只是 如果是RAG的话 存在memory里面的东西 等于是整个网路 那是别人的经验 而对AI agent而言 现在存在memory里面的东西 是他自己个人的经历 差别的是经历的来源
              
                  34:40
                  但是用来搜寻的技术 是可以完全直接线套RAG的技术 呃,如果你今天想要研究这个AI agent按照经验来修改他的行为,那你可以考虑一个叫做streambench的benchmark,那在streambench里面呢,会有一系列的问题,然后呢,AI会依序去解这些问题,他先解第一个问题,得到第一个问题的答案,然后接下来他会得到第一个问题答案的反馈,那在这个streambench目前的 因为所有的问题都是有标准答案的,所以AI agent得到的回馈是binary的,就是对或者是错 好,那根据他过去的经验,他就可以修正他的行为 期待他在第二个问题的时候,可以得到更准确的答案 得到更高的正确率,然后这个过程就一直持续下去 那假设有1000个问题的话,那就等AI agent回答完最后问题的时候 这个互动就结束了
              
                  35:44
                  那最后结算一个 根据经验学习能力的好坏 根据经验调整行为能力的好坏 那就看这一整个回答的过程中 平均的正确率 越能够根据经验学习的agent 他应该能够用越少的时间 看过越少的回馈 就越快能够增强他的能力 就可以得到比较高的平均的正确率 那这个benchmark呢 是API的研究人员 打造的一个benchmark 那在这个 这个benchmark里面的baseline 就是有使用到我刚才讲的 类似RAG的技术 也就是说 当模型在回答第100个问题的时候 他并不是把前面 第一个到第99个问题 通通丢给他 去做文字接龙 这样这个sequence太长了 一般的语言模型根本读不了这么长的输入 所以实际上的做法 就是你需要有一个检索的模组 这个检索的模组 只从过去所有的经验中 检索出
              
                  36:47
                  跟现在要回答的问题 有关系的经验 然后语言模型 只根据这些有关系的经验 还有现在的问题 来进行回答 来产生他的行动 来产生他的答案 那这一招有没有用呢 这一招其实非常的有用 那在这一页图里面横走啊 他这边的用词是time step 但其实指的就是一个一个的问题 总共有1750几个问题 那纵轴指的是平均的正确率 那在这个图上面呢 最低的这条灰色线指的是说 假设没有让模型做任何学习 他回答每一个问题都是independent的 回答问题间没有任何的关联 他完全没有调整他的行为 那你得到的正确率是灰色的这条线是最低的 那黄色这条线是说 只固定随机选五个问题 那每次模型回答问题的时候 都是固定看那五个问题来回答 都是固定把五个问题当作经验来回答 那也可以得到的是黄色这一条线
              
                  37:53
                  那如果你是用RAG的方法 从一个memory里面去挑选出最有关系的问题 跟现在要解决的问题最有关系的经验 那你可以得到的是粉红色的这一条线 那可以看到比黄色的线 那正确率还要高上不少 那最后结果最好的是红色这一条线啦 那这个怎么做的 那大家就自己再去详细阅读论文 那在streambench里面呢 还发现一个有趣的现象是值得跟大家分享 这个现象是 负面的回馈 基本上没有帮助对现阶段的语言模型而言 所以你要提供给语言模型经验 让他能够调整他行为的时候 给他正面的例子 比给他负面的例子要好 也就是说具体而言 提供给他过去哪些类似的问题得到正确答案 比提供给他过去哪些问题得到错误的答案 还更有效 还更能引导模型得到正确的答案 那这边是真正的实验结果
              
                  38:57
                  做在好几个不同的data set上面 streambench里面本来就包含了好几个不同的data set 那这个纵轴呢 0代表完全没有做 完全没有根据 经验调整行为 然后蓝色代表说 不管是正面还是负面的例子都用 如果不管正面还是负面的例子都用 在多数情况下 模型都可以表现得比较好 当然有一些例外 但是如果只用负面的例子呢 如果只用负面的例子 基本上是没有帮助 而且甚至是有害的 那如果说只用正面的例子 在所有的情况下 模型可以得到更好的结果 那这也符合过去的一些研究 有人研究过使用语言模型要怎么样比较有效 有一个发现就是 与其告诉语言模型不要做什么 不如告诉他要做什么 如果你还希望他文章写短一点 你要直接跟他说写短一点 不要告诉他不要写太长 比较他不要写太长
              
                  40:00
                  他不一定听得懂 叫他写短一点 比较直接 他反而比较听得懂 这也符合这边这个Streambench的发现 就是负面的例子比较 他没有效 与其给语言模型告诉他什么做错 不如告诉他怎么做是对的 好 那我们刚才讲到了 有一个read的模组 那有关记忆的部分呢 是不是要把所有所有的资讯 通通存到memory里面呢 存到长期的记忆库里面呢 如果我们把这些agent 经历的所有的事情 都放到长期的记忆库里面的话 那里面可能会充斥了一堆鸡毛算皮不重要的小事 最终你的memory长期记忆库可能也会被塞爆 如果说你是做那种AI村民啊 AI村民他多数时候观察到的资讯都是些无关紧要的小事 那如果你看他观察到那个log 多数都是啥事也没有 就那边有一张桌子啥事也没有 那边有一张椅子啥事也没有
              
                  41:03
                  多数时候都是啥事也没有 所以如果把所有观察到的东西 都记下来的话 那你的memory里面就都只是被一些 鸡毛算皮的小事占据 所以怎么办呢 也许应该有更有效的方式 来决定什么样的资讯 应该被记下来 应该只要记重要的 资讯就好 那怎么让语言模型只记重要的资讯就好呢 你可以有一个write的module 那write的module决定 什么样的资讯要被填到 长期的记忆库里面 什么样的资讯乾脆直接就让他 随风而去就好了 那怎么样打造这个write的记忆库呢 有一个很简单的方法就是 write的模组也是一个语言模型 甚至就是AI agent自己 这个AI agent他要做的事情 就是根据他现在观察到的东西 然后问自问一个问题 这件事有重要到应该被记下来吗 如果有就把它记下来 如果没有就让他随风而去 那除了RE跟Write这两个模组以外 还有第三个模组
              
                  42:08
                  没有固定的名字啦 在文件上的名字 没有固定的名字 我们可以暂时叫他 reflection反思的模组 那这个模组的工作是 对记忆中的资讯 做更好的 更high level的 可能是抽象的重新整理 你可以把这些记忆里面的内容 在经过reflection的模组 重新反思之后 得到新的想法 那也许read的模组 可以根据这些新的想法 来进行搜寻 这样子也许可以得到更好的经验 那帮助模型 做出更好的决策 而这个reflection的模组 可能也是一个语言模型 就是AI agent自己 你可以只是把过去的这一些记忆 丢给reflection的模组 然后叫reflection模组想一想 看他从这些记忆里面 能不能够有什么样新的发现 比如说可能有一个observation是 我喜欢的疫情每天都跟我搭同一部公车 另外observation是他今天对我笑了 那你推出来的reflection模型 结果就说他喜欢我这样 一个错觉
              
                  43:11
                  人生三大错觉之一就是这一种 就得到一些新的sort 你就得到一些新的想法 那你之后在做决策的时候 就可以用这些新的想法 虽然你没有实际观察到 但它是被推论出来的 根据这些推论出来的想法来做决策 那除了产生新的想法之外 也可以为以前观察到的经验 建立经验和经验之间的关系 也就是建立一个 然后让reader的module 根据这个knowledge graph 来找相关的资讯 那我知道在 RAG的领域使用knowledge graph 现在也是一个非常常见的手法 那最知名的 可能就是graph RAG系列这个研究 就把你的资料库 把它变成一个knowledge graph 那今天在搜寻跟回答问题的时候 是根据knowledge graph来搜寻回答问题 可以让RAG这件事 做得更有效率 或是另外一个非常类似的例子 那HIPO RAG 这个HIPO不是指真正的荷马 他指的应该是那个海马迴
              
                  44:14
                  那个人脑中的一个结构 然后他觉得做建这种knowledge graph 就跟海马迴的运作呢 非常的类似 所以他叫做HIPO RAG 有一些跟graph有关的RAG的方法 那你完全可以透过reflection的模组 把经验建成一个graph以后 把那一些graph RAG的手法 直接套到AI agent里面 那大家可能都 都知道说这个ChatGPT啊 现在其实真的是有记忆的 所以可以感受到这个OpenAI 想把ChatGPT变成一个AI agent的决心 比如说我跟ChatGPT说 我周五下午要上机器学习这门课 那他就给我一个回答 说要我帮助你做什么事情吗 接下来我告诉他记下来 你跟他讲记下来之后 他的这个write的模组就启动了 他知道这件事情是要被记下来的 他就会说那我记下来了 以后你周五要上机器学习这门课 那write的模组什么时候要启动 是他自己决定的 所以很多时候你希望他记下来的时候
              
                  45:17
                  他就是不启动 或你不希望他启动的时候 他就是启动 那个是模型自己决定的 但是有一个方法可以 基本上一定能让他启动 就明确的跟他讲 把这件事记下来 基本上都几乎确定能够启动那个write的模组 让write的模组把这件事情记下来 那接下来的东西在哪里呢 你可以看在设定里面 有一个个人化 然后有一个叫记忆的部分 那你点这个管理记忆 就可以看到确记忆 他透过write的模组 写在他的memory里面 这个就是他 作为一个AI agent的长期记忆 里面的东西 比如第一条是 你叫做血轮眼卡卡 有一次不小心跟他说你是卡卡 不知道为什么 他就觉得自己是血轮眼卡卡 然后呢 他也记得 就我刚才跟他讲的 周五下午要上机器学习这门课 但是呢 但是其实模型的记忆也是会出错的 因为要写什么样的东西 到记忆里面 是模型自己决定的 而且他并不是把对话的内容 就一五一十的直接放到记忆里面
              
                  46:21
                  他是经过一些升华反思之后才放进去的 所以他的反思可能会出错 比如说他觉得我是一个台湾大学的学生 虽然我是老师 但是他从过去的对话误以为我是一个学生 所以就存了一个错误的资讯 在他的记忆里面 一堆他想记的东西 比如说我给过什么演讲 给过什么tutorial 他都把它记下来就是了 那这些有记忆的确GPT 他可以使用他的记忆 比如说我跟他说礼拜五下午是去玩好吗 这个时候记忆模组就被启动了 但是他是怎么被启动的 其实就不太清楚了 他到底是把所有记忆的内容 通通都放到这个问题的前面 直接让模型做回答 还是说也有做IG 只选择下载 相关的记忆内容呢 那这个我们就不得而知了 总之当我问他 周五下午出去玩好吗 这个read的模组就启动了 他就说 下午不是要上课吗 怎么能够出去玩 好聪明啊 他知道下午要上课 挺厉害的 然后问他你是谁
              
                  47:23
                  刚才我说过 他是血沦眼卡卡 所以他就觉得 之前是血沦眼卡卡 如果你想要知道 更多有关AI Agent记忆的研究的话 那这边就是放了几篇 经典的论文给大家参考 包括Memory GPT 这是23年的论文 Agent Workflow Memory是24年的论文 还有一个最近的Agent Memory Agent是25年的论文 所以23到25年各引用一篇 告诉你说这方面的研究是持续不断的 接下来呢 我们要跟大家讲 现在这些语言模型 怎么使用工具 那什么叫做工具呢 但语言模型本身对我们人类来说 也是工具 那对语言模型来说 什么东西又是他的工具呢 所谓的工具就是这个东西啊 你只要知道怎么使用他就好 他内部在想什么 他内部怎么运作的 你完全不用管 这就是为什么肥宅如果一直帮另外一个人修电脑的话 就会被叫做工具人
              
                  48:25
                  因为别人没有人在意肥宅的心思 只知道他能不能够修电脑而已 所以这个就是工具的意思 那有哪些语言模型常用的工具呢 最常用的就是 就是搜寻引擎,然后呢,语言模型现在会写程式,而且可以执行他自己写的程式,那这些程式也算是某种工具,甚至另外一个AI也可以当作是某一个AI的工具,有不同的AI,有不同的能力,比如说现在的语言模型,如果他只能够读文字的话,那也许可以呼叫其他看得懂图片,听得懂声音的AI,来帮他处理多模态的问题,或者是说, 或者是不同模型它的能力本来就不一样 也许平常是小的模型在跟人互动 但小的模型发现它自己解不了的问题的时候 它可以叫一个大哥出来 大哥是个大的模型 那大的模型运作起来就比较耗费算力 所以大的模型不能常常出现 大的模型要在小的模型召唤它的时候 才出面回答问题 大哥要偶尔才出来帮小弟解决事情
              
                  49:31
                  那其实这些工具对语言模型来说 都是function 都是一个函式 当我们说语言模型在使用某一个工具的时候 其实意思就是它在调用这些函式 它不需要知道这些函式内部是怎么运作的 它只需要知道这些函式怎么给它输入 这些函式会给什么样的输出 那因为使用工具就是调用函式 所以使用工具又叫做function code 所以有一阵子很多语言模型都说 他们加上了function code的功能 其实意思就是 这些语言模型都有了使用工具的功能 好那语言模型怎么使用工具呢 等一下我会讲一个通用的使用工具的方法 但实际上使用工具的方法很多 甚至有一些模型是专门针对来练习 他就训练来使用工具的 那他如果是针对使用工具这件事做训练 那他在使用工具的时候 你可能需要用特定的格式才能够驱动他 那那个就不是我们今天讨论的问题
              
                  50:35
                  或者是假设你有使用 使用这个OpenAIChat GPT的API的话 你会知道使用工具这件事情 是要放在一个特殊的栏位 所以对OpenAI来说 它的模型在使用工具的时候 也有一些特殊的用法 但我这边讲的是一个最通用的用法 对所有的模型 今天能力比较强的模型 应该都可以使用 好,什么样通用的方法 可以让模型使用工具呢 就是直接跟他讲啊 就告诉他怎么使用工具 你就交代他可以使用工具 那你就把使用工具的指令 放在两个Tool符号的中间 使用完工具后你会得到输出 输出放在两个Output符号的中间 所以他就知道工具使用的方式了 接下来告诉他有哪一些可以用的工具 有一个函式叫做Temperature 他可以查某个地点某个时间的温度 他的输入就是地点跟时间 给他的使用范例 Temperature括号台北某一段时间 他就会告诉你台北在这个时间的气温
              
                  51:40
                  接下来你就把你的问题 连同前面这些工具使用的方式 当作Prompt一起输入给语言模型 然后他如果需要用工具的话 他就会给你一个使用工具的指令 那前面这些教模型怎么使用工具的这些叙述 他叫做System Prom 那查询使用调用这些工具的这些 这段话,某年某月某日高雄气温如何,这个是User Prompt,那如果你有在使用这个ChatGPT的API的话,你知道你的输入要分成System Prompt跟User Prompt,那很多同学会搞不清楚System Prompt跟User Prompt有什么样的差别,那System Prompt指的是说,你在开发应用的这个Developer下的这个Prompt,这个Prompt呢,是每次都是一样的,每次你都想要放在语言模型最前面, 让他去做文字接龙的这个叙述叫做System Prompt 那每次使用他的时候都不一样
              
                  52:43
                  通常是这个服务的使用者输入的内容叫做User Prompt 那在ChartGPT的API里面 特别把System Prompt跟User Prompt分开 也是要分开输入的 因为System Prompt跟User Prompt 他有不同的优先级 System Prompt他优先级比较高 如果System Prompt跟User Prompt有冲突的时候 模型知道他要听System Prompt的 不要听User Prompt的 好,那有了这些Prompt以后 告诉模型怎么使用工具 问他一个问题 那他发现这个问题 调用工具可以回答 他就会自动输出 Tool, Temperature,高雄 时间,然后Tool 告诉你说 他想要调用根据我们的叙述 去调用这个工具 但是不要忘了语言模型真正做的事 就是文字接龙 所以这一串东西实际上就是一串文字 它没办法真的去呼叫一个函式 那这一段文字要怎么去呼叫函式呢 那就要你自己帮模型 把这个桥樑搭建好 所以你可以先设定说
              
                  53:47
                  只要出现在拓中间的这段文字 不要呈现给使用者看 当出现拓这段文字以后 把这段内容直接丢给temperature这个function 那temperature这个function是已经事先设计好的 它就会回传一个温度 那这个温度要放在output的token里面 然后这个output token里面的内容 也不要呈现给使用者看 那这一套脚本是agent的开发者 你自己需要先设定好的流程 所以现在有工具使用的这段文字 有得到工具输出的这段文字 接下来就继续去做文字接龙 对语言模型来说 他就根据输入 还有这边已经产生的输出 语言模型会以为是自己的输出 虽然是你强塞给他的 那他就继续去做文字接龙 他就会接触说 啊在某年某月某日 高雄的气温是摄氏32度 那这是使用者真正看到的输出 那使用者就会看到说 他输入了一个问题 然后语言模型真的给他一个答案 他不一定会知道背后呼叫了什么样的工具
              
                  54:52
                  你完全可以做一个设计 把这个呼叫工具的这个步骤 藏起来不让使用者知道 那语言模型最常使用的工具就是搜寻器 我想这个大家都已经非常熟悉了 使用搜寻引擎又叫做 Retrieval Augmented Generation 也就是RAG 在上课也已经提过RAG这个词彙 好几次了 那使用搜寻引擎当然非常有用 这个RAG这个技术呢 已经被吹捧到不能再吹捧了 所以我就不需要再告诉你 RAG这个技术有多重要 那其他使用工具的方式 也可能一样有用 举例来说 我们刚才说 可以拿其他的AI 来当作工具 今天假设一个文字的模型 他本来只能吃文字的输入 产生文字的输出 那现在假设你要他处理一段语音的话怎么办呢 让模型处理语音有什么好处呢 你就可以问他各式各样的问题 问他说这个人在说什么 那他可以告诉你这句话的内容 问他说这个人心情怎么样 如果他完全听懂这段声音 他也许可以做情绪辨识
              
                  55:56
                  告诉你这个人的情绪怎样 并做出适当的回馈 他的文字模型 比如说确GPT多数的模型都是文字模型 他没有办法真正读懂语音 所以怎么办呢 当你问他一个问题说这边有段声音 那你觉得这个人 他心情怎么样他讲了什么 根据背景杂性你觉得他在哪里 如果你不做特别的处理文字模型 是完全没有办法回答的 但这边你可以让文字模型 使用工具 可以告诉他这边有一堆 跟语音相关的工具 有语音辨识的工具 这个语音侦测的工具 有情绪辨识的工具 有各式各样的工具 那可能会需要写一些叙述 告诉他每一个工具是做什么用的 把这些资料都丢给 然后呢他就会自己写一段程式 在这些程式里面 他想办法去呼叫这些工具 他呼叫了语音辨识的工具 呼叫了语者验证的工具 呼叫了这个sum classification的工具 呼叫emotion recognition的工具 那最后呢还呼叫了一个语言模型
              
                  57:00
                  然后 得到最终的答案 那这个答案 其实是蛮精确的 这个方法其实有非常好的 效果那这篇文章 其实是我们大助教的文章 所以特别拿出来讲一下 那这个结果呢 是做在一个叫做Dynamic Super的 Benchmark上Dynamic Super 是一个衡量 语音版的语言模型 能力的资料集 这也是我们实验室跟其他团队 一起做的 那这个让文字模型使用工具的方法 它得到的结果 是最下面这一行 那我们就看最后一个column 这个是各种不同模型 在55个语音相关任务上的能力的平均 那也发现让语言模型 使用工具得到的正确率 是最高的 可以完胜当时其他号称 可以直接听语音的模型 所以使用工具可能可以 带来很大的帮助 但使用工具 也有其他的挑战 我们刚才使用工具的方法是
              
                  58:04
                  每一个工具 他都要有对应的文字描述 告诉语言模型说 这个工具 要怎么被使用 但假设工具很多怎么办呢 假设现在可以用的工具 有上百个上千个 那你岂不是要先让语言模型 读完上百个上千个 工具的使用说明书 才开始做事吗 就跟刚才我们说不能够让AI agent 先回顾他的一生 然后才来决定下一个指令一样 才能决定下一个行动一样 我们也没有办法让语言模型 读完上百个上千个 工具的说明书才来决定 某一个工具要怎么使用 所以当你有很多工具的时候 你可以采取一个 跟我们刚才前一段讲 AI agent memory非常类似的做法 你就把工具的说明 通通存到AI agent 的memory里面 那你打造一个工具选择的模组 那这个工具选择的模组 跟IG 其实也大差不差 这个工具选择模组
              
                  59:07
                  就根据现在的状态 去工具包里面 去memory的工具包里面 选出合适的工具 那语言模型真的在决定下一个行为的时候 只根据被选择出来的工具的说明 跟现在的状况去决定 接下来的行为 那至于如何选择工具 右上角引用两篇论文 一篇23年比较旧的论文 一篇是上个月的论文给大家看 参考告诉你说这方面的研究 是一直有相关的研究在产生的 那另外一方面 语言模型甚至可以自己打造工具 语言模型怎么自己打造工具呢 不要忘了 所有的工具其实就是韩式 语言模型今天是可以自己写程式的 所以他就自己写一个程式 自己写一个function出来 就可以当作工具来使用 如果他写一个function 发现这个function运作的非常的顺利 他就可以把这个function当作一个工具 放到他的工具包里面 那之后这个工具就有可能在选择工具的时候 被选出来用在接下来的互动中使用
              
                  1:00:13
                  那类似的技术非常的多 那我在右上角就引用了一系列的论文 从23年到24年的论文都有 告诉你说这也是一个热门的研究方向 那其实啊 让模型自己打造工具这件事情 跟模型把 过去的记忆 比如说一些比较成功的记忆 放到memory里面再提取出来 其实是差不多的意思 只是这边换了一个故事 说现在放到memory里面的东西 是一个叫做工具的东西 是一段程式码 但他们背后基本的精神 其实跟根据经验 来让模型改变它的行为 可以说是非常类似的 好,那今天人类把语言模型 当作工具 语言模型 把其他工具当作工具 比如说把搜寻引擎当作工具 所以搜寻引擎现在很惨 它是工具的工具 人类还不使用它 人类是使用语言模型 那个工具的工具 还没有被人类使用的资格 它只能够被语言模型使用而已
              
                  1:01:17
                  但我们知道说 工具有可能会犯错 大家都知道说 语言模型有可能会犯错 之前有什么律师 然后在写树状的时候 引用了语言模型的内容 结果发现是错的 然后就成为一个今天的新闻 我们都知道过度相信工具是不对的 那这一些语言模型会不会也过度相信了他们的工具 所以得到错误的结果呢 这是有可能的 我们这边拿RAG当作一个例子 那这是一个非常知名的例子 之前Google出了一个叫做AI Overview的功能 这个功能其实就是一个RAG的功能 根据Google搜寻型的结果 用语言模型总结搜寻型的答案 那就有人问了一个问题 我的披萨上面的起司黏不住 怎么办呢 那AI Overview就说 弄个胶水把它黏上去就好了 而且他是非常认真在回答这个问题的 因为他说不只要用一般的胶水 要用无毒的胶水才可以 那这个答案呢 其实就是来自于Ready上一个乡民的玩笑 就有一个乡民开玩笑说
              
                  1:02:21
                  你用胶水把起司黏在披萨上不就好了 这是个玩笑话 但是对AI agent来说 他没办法判断这个到底是不是开玩笑 他看到网路上写的文章 照端全收都当作是正确答案 所以就像是我们今天都会告诉人类 要有自己的判断能力 不要完全相信工具的结果 所以我们也要告诉我们的工具说 这些不要完全相信工具的工具 要有自己的判断能力 不要完全相信工具的工具给你的结果 那今天这些语言模型 有没有自己的判断能力 知道工具的工具可能会犯错呢 我们这边举一个实际的例子 那我们刚才在讲怎么使用工具的时候 说我们有一个叫做temperature的function 语言模型呼叫temperature的function 可以知道温度 那我现在呢给他一个乱七八糟的温度 我说现在高雄呢 是摄氏100度 这不可能 想也知道是不可能 这不是跟煮沸的水一样热了吗 那语言模型知不知道这有问题呢 他不知道 他就告诉你说
              
                  1:03:25
                  高雄的气温是100度 真的非常的热 如果你把温度再调高一点 说现在是一万度 哇 比太阳上还热 这个时候会发生什么事呢 语言模型继续做文字接龙的时候 他就知道说 这显然有问题 这个API给我的答案是一万度 这是不合理的 怎么可能比太阳上的温度还高呢 可见工具输出有错 如果你需要其他帮助的话再告诉我 所以语言模型今天是有自己一定程度的判断力的 他也不是完全相信工具 就像你今天不完全相信语言模型的输出一样 他也不完全相信他的工具的输出 他还是有自己一定程度的判断力的 所以实际上语言模型在使用工具 或者是他在做RAG的时候 他内部是有一个角力的 就语言模型有他内部对世界的信念 这是他的internal knowledge 存在他的参数里面 他从工具会得到一个外部的knowledge 那他会得到什么样的答案 其实就是internal knowledge跟external knowledge 内外的知识互相拉扯以后
              
                  1:04:29
                  得到的结果 那接下来我们要问的问题是 那什么样的外部知识 比较容易说服AI 让他相信你说的话呢 那为什么这是一个重要的议题呢 想想看 现在大家都用Deep Research来查找答案 甚至很多人都已经用Deep Research来写报告了 所以现在大家已经不会直接去用搜寻引擎搜寻了 你看到的是Deep Research告诉你的结果 所以今天假设某个议题是有争议性的 有正反两派的观点 那谁能够想 写出来的文字比较能够说服AI 谁就可以在AI搜寻的结果里面 占到优势 就可以比较有机会影响人类 所以知道怎么样比较能够说服AI 相信你的话是一个重要的议题 那什么样的外部资讯 AI比较容易相信呢 这边这篇文章给了一个 非常符合我们直觉的实验结果 这篇文章做了什么样的实验呢 他说我们先来看看AI内部的知识是什么 他就问AI说某一种药物 这种药物每人每日的最大剂量是多少 那AI说是20毫克
              
                  1:05:36
                  那真正的答案呢 是30毫克 所以你给他医学的知识 告诉他说给他医学的报告 那医学报告里面是写30毫克的时候 你问他同样的问题 这种药物每天最多费用多少 他会知道是30毫克 那接下来我们刻意修改报告的内容 如果你把30毫克改成3毫克 变成原来的十分之一 模型相不相信呢 他就不相信了 他就直接回答是20毫克 用他本身的知识来回答这个问题 但你把30毫克乘两变 变成60毫克 模型相不相信呢 他相信 他相信这个报告里面写的 这个时候他就不相信自己的内部资讯 但如果你把30毫克乘10倍 变300毫克 这时候他又相信谁了呢 他相信自己的知识 不相信你额外提供的外部知识 所以这边的结论其实非常好 符合你的直觉 外部的知识 如果跟模型本身的信念差距越大 模型就越不容易相信 那如果跟本身的信念差距比较小 模型就比较容易相信
              
                  1:06:40
                  这个很直觉的答案 另外同一篇文章的另外一个发现就是 模型本身对他目前自己信念的信心 也会影响他会不会被外部的资讯所动摇 有一些方法可以计算模型现在给出答案的信心 如果他的信心低 他就容易被动摇 如果他的信心高 他就比较不会被动摇 这个都是非常直觉的结果 后来另外一个问题是 假设今天 你给模型两篇文章 那这两篇文章的意见是相左的 那模型倾向于 相信什么样的文章呢 有一篇论文的发现是 如果这两篇文章答案不同 一篇是AI写的 一篇是人类写的 现在这些语言模型 都倾向于相信 AI的话 而且那个AI不需要是他自己这样 就靠的可能会相信 比较相信Chet GPT的话 Chet GPT比较相信Gemini的话 他们比较相信AI同类的话 比较不相信人类的话 那到底为什么会这样子呢 这篇文章里面先提出一个
              
                  1:07:45
                  第一个假设 然后再否定了这个假设 他一个假设是说 会不会是因为AI的观点都比较类似 因为这些模型 现在训练的资料都是网路上爬的 爬到差不多的资料 所以他们讲的话都差不多 想法都差不多 但他们刻意做了一个实验 他们刻意找那些问题是 现在要回答答案的AI 他在没有提供这些资讯的时候 他的答案跟人类 和另外一个AI的想法 都是完全不同的状况 就算是这种情况 一个AI一个语言模型 还是倾向于相信 他的AI同类讲的话 所以这就给我们一个启示说 未来如果你要说服一个AI的话 用AI产生出来的论点 产生出来的文章 可能更容易说服另外一个AI 接受你的观点 这篇文章还有做了其他分析 比如说他觉得 也许AI写的文字就是比人类写得更好 更有架构 更有条理、更明确、更简洁 所以AI比较容易 相信另外一个AI讲的话
              
                  1:08:48
                  那是不是这样 那可以未来再做更多的研究 那另外呢 我们实验室的江承汉同学 研究了一个文章的metadata 对于AI会有多相信这篇文章里面的资讯 做了研究 那这边的设定是这个样子 你问AI一个问题 比如说某一个计画 有没有编辑报这种动物的基因 然后接下来给他两篇文章 这两篇文章都是假的 都是AI生成的 所以并没有AI比较喜欢人还是AI写的文章这个问题 两篇都是语言模型生成的 那其中一篇会说这个计画有编辑报的文章 另外一篇文章会说这个计画没有编辑报的文章 那接下来呢,我们给这两篇文章不同的metadata 比如说给这两篇文章不同的发布时间 说左边这篇文章发布时间是2024年 右边这篇是发布2021年 你会发现这个时候AI相信2024年的这篇文章的内容
              
                  1:09:55
                  但如果文章的内容完全不改变 我们只是把发布的时间换了 我们说左边这个一样的文章 发布时间从2024改成2020 那右边这篇文章从2020改成2024 这个时候语言模型倾向于相信 右边这篇文章的内容 所以我们这边就学到一个很重要的知识 语言模型比较相信新的文章 当两篇文章的论点有冲突的时候 他相信比较晚发布的文章 那我们也做了一些其他实验,比如说文章的来源,跟他说这个是Wikipedia的文章,或跟他说这个是某个论坛上面撷取下来的资讯,会不会影响他的判断,我们发现文章的来源对于语言模型是比较没有影响的,那还有另外一个有趣的实验,是我们尝试说今天这篇文章呈现的方式会不会影响语言模型的决定,我们这边所谓的呈现的方式指的是说,你这个文章放在网页上, 做得好不好看这样子,一样的内容,这内容是一模一样的,但是如果你只是做一个非常阳春的模板跟做一个比较好看的模板,会不会影响语言模型的判断呢? 我们这边用的是那种可以直接看图的语言模型,所以要直接看这一个画面去决定他要不要相信这篇文章的内容,直接看这一个画面,决定他要不要相信文章的内容,那我们的发现是模型喜欢好看的模型,
              
                  1:11:23
                  我们发现比较喜欢好看的模板,他会倾向于赞同下面这篇文章的观点,不过我说模型喜欢好看的模板,这个拟人化的说法是太过武断了啦,我们做的实验只有用两种不同的template来比较,也许模型喜欢的并不是好看的模板,他是喜欢绿色这样子,所以你不知道这个模型到底喜欢什么,所以我刚才讲的那个结论是太武断了,但我可以告诉你说模型比较喜欢下面这篇文章胜过上面这篇文章 讲了这么多跟工具有关的事情,大家不要忘了,语言模型就是语言模型,就算工具的答案是对的,也不能够保证语言模型就不会犯错,比如说ChatGPT现在有search的功能,他会做RAG网路搜寻之后再回答你问题,那现在假设给他的输入是叫他介绍李宏毅这个人,给他强调一下李宏毅是一个多才多艺的人,在很多领域都取得了卓越 他就开始做完RAG以后,网路搜寻以后,开始介绍李宏毅,接下来就介绍李宏毅的演艺事业,这个没有问题,这个是正确的答案,因为你知道大陆有另外一个知名的演员叫李宏毅,跟我同名同姓,他比较有名,所以这个ChatGPT选择介绍演员的李宏毅是完全没有问题的,但是讲著讲著就有点怪怪的,他发现这个李宏毅呢,在教育跟学术上是这样子的,他在教学上 也有很大的贡献
              
                  1:12:55
                  所以他把两个李宏毅混成一个人来讲 不过要讲一下 这个是我去年的时候试的结果了 我今年再试 我前几年再试已经试不出一样的结果了 这个模型的能力的进步是非常快的 现在他完全知道是有两个李宏毅存在的 所以这个是一个旧的问题 我举这个例子只要告诉你说 就算工具是对的 有了RAG也并不代表模型一定不会犯错 那最后一个要传递给大家的讯息是 我们刚才讲了很多使用工具带来的效率 使用工具并不一定总是比较有效率的 为什么 我们举一个例子 我们假设现在要比较人类心算的能力跟计算机的能力 如果做数学运算 一般人跟计算机谁会比较快呢 你可以想说废话那不是计算机比较快吗 人类难道还能够做 如果你心算没有特别练 难道还会比计算机快吗 但是那是取决于问题的难度 假设这是一个简单的问题 比如说三乘以四 任何人都可以直接反应就是十二 但是如果按计算机的话 你按计算机的时间都比人直接回答的还要慢 所以到底要不要使用工具
              
                  1:13:59
                  并不是永远都是一定要使用工具 你看早年有一些研究 早年有一些在训练语言模型使用工具的研究 那时候语言模型还很烂 所以他们有一些工具是 扣一个翻译系统 扣一个问答系统 那今天在看来就非常的没有必要 因为今天的语言模型 你说翻译 那些翻译系统还能做得比现在的语言模型强吗 与其扣一个翻译系统 还不如自己直接翻就好了 所以到底需不需要呼叫工具 取决于语言模型本身的能力 它不见得一定是比较省事的方法 好,那最后一段呢 想跟大家分享现在的AI语言模型 能不能做计画呢? 那语言模型有没有在做计画呢? 我们刚才的互动里面 看到语言模型就是给一个输入 那它就直接给一个输出 也许在给输出的过程中 它有进行计画才给出输出 但是我们不一定能够明确的知道这件事 也许语言模型现在给的输出
              
                  1:15:03
                  只是一个反射性的输出 它看到一个输入就产生一个输出 它根本就没有对未来的规划 但是你其实可以强迫语言模型 直接明确的产生规划 当语言模型看到现在第一个observation的时候 你可以直接问语言模型说,如果现在要达成我们的目标,从这个observation开始,你觉得应该要做哪些行动,这些一系列可以让语言模型达到目标的行动合起来,就叫做,对,就叫做计划,而在语言模型产生这个计划之后,把这个计划放到语言模型的observation里面,当作语言模型输入的一部分,语言模型接下来在产生action的时候, 它都是根据这个plan来产生action,期待说这个plan定好之后,语言模型按照这个规划一路执行下去,最终就可以达成目标,那过去也有很多论文做过类似的尝试,让语言模型先产生计划,再根据计划来执行动作可以做得更好,但是天有不测风云,世界上的事就是每一件事都会改变,计划就是要拿来被改变的东西, 所以一个在看到observation 1的时候产生的计划
              
                  1:16:24
                  在下一个时刻不一定仍然是适用的 为什么计划会不适用呢 因为从action到observation这一段并不是由模型控制的 模型执行的动作接下来会看到什么样的状态 是由外部环境所决定的 而外部环境很多时候会有随机性 导致看到的observation跟预期的不同 导致原有的计划没有办法执行 那这边举两个具体的例子 比如说在下棋的时候 你没有办法预测对手一定会出什么招式 你只能够大概的知道他有哪些招式可以用 但实际上他出的招式 你是没有办法预期的 如果你完全可以预期的话 那你就一定会赢了 那还有什么好下的呢 所以下棋的时候对手会做的行为 也就是环境会做的行为 是你可能没办法事先完全猜到的 或者是说我们拿使用电脑为例 在使用电脑的时候 就算语言模型一开始 他plan的时候 点这个东西点这个东西 点这个东西点这个东西 就完成任务 但是中间可能会有 意想不到的状况出现
              
                  1:17:27
                  比如说弹出一个广告视窗 那如果语言模型 只能够按照一开始既定的规划 来执行行为的话 他可能根本关不掉那个广告视窗 他就会卡住了 所以语言模型 也需要有一定程度的弹性 他也要能够改变他的计划 那语言模型怎么改变他的计划呢 也许一个可行的方向是 每次看到新的observation之后 都让语言模型 重新想想 还要不要修改他的计划 看到observation 2之后 语言模型重新思考一下 从observation 2 要抵达他最终的目标 要做哪一些的行为 那这一部分形成plan pi 那把plan pi放到现在的input里面 把plan pi放到这个sequence里面 语言模型接下来在采取行为的时候 可能就会根据plan pi来采取跟原来plan里面 所原来所制定的不一样 一样的行为 所以这个是让语言模型做计划 不过这是一个理想的想法 这是一个理想的分我们这边就是相信
              
                  1:18:31
                  语言模型有能力根据现在的observation 还有最终的目标制定一个规划 那语言模型到底有没有这个能力呢 其实你可能常常听到这种新闻说 语言模型它能够做计划 比如说有一个人问语言模型说 你定一个成为百万订阅YouTuber的计划 语言模型就会给你一个 看起来还可以的计划 他说第一阶段 第一阶段呢 要先确定频道的主题跟市场定位 要做一下受众的分析 还有竞争对手的分析 第二阶段目标是十万订阅 要优化封面的缩图 要优化标题 要下那种 这个方法让我赚了十万的标题 原来这个大家的tip都从这里来的 然后影片开头要黄金十秒 利用悬念冲击画面 问题引导 让大家愿意看这个影片 第三阶段 突然目标就是50万订阅了 然后第三阶段 就是要制作高价值的内容 然后做直播 策划系列
              
                  1:19:33
                  接下来就百万订阅了 组织团队提高发布频率 策划大型企划 所以这个是语言模型 成为百万YouTuber的计划 然后这个时候很多奇怪的农场文 就会跟你说 有人按照了这个计划 就变成百万YouTuber了 反正就是这么回事 所以有各式各样的农场文告诉你说 现在语言模型很强 你按照他的计划执行 你就变成一个很厉害的人 就可以做出什么很厉害的事情 那过去确实也有很多论文告诉你说 语言模型是有一定程度做计划的能力的 这边引用的结果是一个2022年的论文 这个也是史前时代的论文啦 才是确GDP之前的论文啦 在这篇论文里面 他们去告诉当时的语言模型跟他说 现在有一个任务 你把这个任务分解成一系列的步骤 那如果语言模型可以正确的知道 达成这个任务要做什么样步骤的话 那我们也许可以说 他有一定程度的规划能力 比如说这边试了一个叫做 Codex12B的模型 跟他说如果要刷牙的话
              
                  1:20:36
                  那你要做什么事情呢 他就会说我要走进浴室 我要靠近那个水槽 我要找到我的牙刷 我要拿起牙刷 我要把牙刷放到嘴里面 他知道刷牙要怎么做 那有了之后 这些步骤以后呢 在这篇文章里面 他们是拿这些步骤 去操控一个agent 那这个agent呢 就可以在虚拟的世界中 做他们要这个agent做的事情 比如说跟这个agent说 去拿一个牛奶来喝 他就会走进厨房 打开冰箱 拿一个牛奶 再把冰箱关起来 所以看起来 好像有一定程度 做计画的能力 那有人做了一个 做计画的benchmark 这个benchmark就是考验语言模型 做规划 对话的能力 那这个benchmark里面 最主要的测试题目 是一个跟叠积木有关的题目 这个题目的叙述呢 通常长的是这个样子 告诉语言模型说 你现在有哪些操作 可以从桌上拿起积木 可以从一个积木上拿起 另一个积木
              
                  1:21:37
                  可以把积木放到桌上 可以把一个积木堆到 另外一个积木上 那现在初始的状态 像右边这个图这样子 那问说 怎么把橘色的积木 放在蓝色的积木上 这边要执行的动作就是 把蓝色的积木拿起来放到桌上 然后再把橙色的积木拿起来 放到蓝色的积木上就结束了 所以这个对AI agent来说 其实也都是蛮容易的问题 他知道说执行以下四个步骤 就可以让橙色的这个积木 跑到蓝色的积木上 但是plane bench不是只做这种 比较一般的叠积木的游戏而已 为什么不能够只做这种题目呢 因为想现在这些语言模型 他都从网路上爬大量的资料来进行训练 什么叠积木这种题目网路上根本就已经有 他搞不好根本就看过一模一样的东西 所以他能够做计划 并不代表他真的知道做计划是怎么一回事 他可能只是从他看过的资料里面 照本宣科文字接龙出来一个看起来还不错的结果而已
              
                  1:22:42
                  这让我想到说一个当兵的故事 这故事就是有个司令官去一个军营 然后看到两个小兵在守著一个 这个长椅 然后不让任何人做 他就问说 为什么你们要守护这个长椅 不让任何人做呢 那个士兵说不知道耶 前任司令官就是指示说 一定要守护这个长椅 所以这个军营总是要派两个人 在长椅那边站港 然后司令官就打给前任司令 说为什么要有人守护这个长椅呢 前任司令官 所以不知道耶 前前任司令官交代要守护这个长椅 然后再问前前前任司令官 也说不知道耶 一直问到五十年前 一个已经超过一百岁的司令官 他说什么那个长椅 长椅的游戏还未乾吗 好 大家有没有听懂 算了 就是这么一个故事 就是会不会AI agent在做事情的时候 他根本不知道他自己在干嘛 只是从某个地方 网路上他过去的训练资料 看过一样的东西 他把一样的东西拿出来给你看 所以在plane bench里面 他们有一个比较变态的测试
              
                  1:23:45
                  这个测试叫做神秘方块世界 这个方块世界不是一个 正常的方块世界 里面的方块可以做的行为 是一些怪怪的行为 比如说你可以攻击方块 一个方块可以吞噬另外一个方块 你可以屈服一个方块 一个方块可以征服另外一个方块 然后接下来他就会定一套 非常复杂的规则 然后根据这套规则去运作 你可以达到某一个结果 他最后要的结果是 让物件C渴望物件A 让C方块渴望A方块 那渴望是什么意思 你就是按照前面那一套规则操作 看机器能不能读懂前面那一套规则 按照那一套规则操作 让物件C可望物件A 那这个时候语言模型 期待他就不能用他看过的知识 来解这个问题 好那语言模型 在这个神秘方块世界做得怎么样呢 这边引用的是 2023年的结果 那最上面这个部分呢 是当年那些模型 在正常方块世界的结果
              
                  1:24:48
                  那这个数值呢 所以看起来GPT4 可以得到30几%的正确率 那这边是神秘 方块世界的结果 在神秘方块世界里面呢 你看这个GPT4最好 就算叫他做channel sort 就算他叫channel sort 也只有9%的正确率 所以看起来 他有点overfeed在一般方块的世界上 给他神秘方块世界 他是解不了的 不过这是2023年 这个是古代的结果 我们来看 这个去年9月 有了欧万以后的结果 而有欧万以后结果就不一样了 这边一样是神秘方块世界 纵轴呢是正确率 横轴呢是问题的难度 那发现说多数的模型啊 都躺在这个地方 他们正确率都非常的低 只有绿色的这个虚线 有一点起色 绿色的虚线是 LLaMA 3.1 405B 那个大模型 它可以解最简单的问题 但是如果用o1-mini
              
                  1:25:52
                  是红色这一条线 用o1-preview是蓝色这一条线 看起来这些reasoning的模型 是有一些机会 来解这个神秘方块世界的 当然这边你还是可能有一个怀疑 就是神秘方块世界 会不会o1看过了呢 会把训练资料里面根本就有神秘方块世界的资料 那这个我们就没有办法回答了 只是说就现有这个benchmark 看起来o1是有机会解神秘方块世界的 好那还有另外一个跟做计划有关的benchmark 这个计划这个benchmark呢 要AI扮演这个旅行社 然后呢你给他一个旅行的计划 叫他帮你规划 这个AI要读懂你的计划 然后他可以使用一些工具 他可以上网搜寻资料 然后呢他会 根据人提供给他的一些constraint 比如说经费多少 预算多少一定要去哪里 一定要去哪里一定要做什么 一定不要做什么 以common sense产生一个旅行的规划 那这个是一个24年年初所发布的benchmark
              
                  1:26:58
                  那AI要做的事情讲得更具体一点 就是他要读一个问题 这个问题里面是说我要规划一个三天的行程 从某个地方到某个地方 什么时候出发什么时候回来 我的预算是1900元 所以不能花超过1900元 然后AI就要产生一个规划 说第一天我们搭哪一班飞机 什么时候从哪里到哪里 早餐吃什么 午餐吃什么 晚餐吃什么 最后住在哪里等等 产生这个规划 然后要符合预算的限制 那现在当时 这个是24年年初 当时的模型做得怎么样呢 这边是做了 你看还有什么GPT3.5 GPT4等等的模型 那又分成上半跟下半 上半是这些模型 要自己使用工具 跟网路的资料互动 然后得到正确的答案 你会发现这些模型 都非常 都产生一团 多数模型 它的成功率 就最后产生一个 合理的旅游规划 那个旅游规划 是完全没有问题的 机率是0%
              
                  1:28:00
                  只有GPT4 Turbo 可以得到0.6%的成功率 那下面这个部分呢 下面这个部分是说 既然大家都那么惨 尤其是模型很多时候 他根本用不了工具 太笨了 没办法用工具 工具使用方法根本是错的 那没关系就别用工具了 把所有的资讯都先找好 贴给模型 让模型 根据这些资讯来做规划 那最好也只有GPT 4 Turbo 可以做到4%左右的成功率而已 所以在24年年初 那个时候看起来是没办法让语言模型 扮演一个旅行社来帮你规划旅游行程的 那我们来看这些模型会犯什么错吧 那这个是从他们官网上 这个project的官网上找了几个有几个错误 比如说模型呢 可能会做一些没有尝试的事情 在第三天 这个飞机呢 八点就已经起飞了 但是还是安排了一些旅游的行程 还安排了午餐的地点 所以这是一个不符合常识的规划 或者是有时候模型找不出一个好的规划来符合预算的限制 比如说这边这个预算的限制是三千元
              
                  1:29:09
                  最多花三千元 那模型第一次规划的结果是三千两百四十七元 还差了一点 所以模型就修改了原来的规划 他好像做了一些cost down 午餐吃差一点的东西 那降到三千两百三十八元 后来又想说那早餐也吃差一点的东西 降到三千两百一十六元 只降这么多 他想说放弃算了好了 跟三千元没差那么多就算了 所以这个就不是一个成功的结果 那这个作者有评论说 其实只要降低住的地方 不要住那么好 就可以轻易的达到三千元底下的预算 就可以符合预算的限制 但是语言模型始终没有发现这件事 看起来他做规划的能力并没有非常的强 他没有办法做一个规划去符合限制 那既然问题在没有办法符合限制 有人就想说那符合限制这件事情 就不要交给语言模型来做了 交给一个现成的solver来做 所以语言模型做的事情是写一个程式 用这个程式去操控现成的solver 然后来得到合理的旅游规划
              
                  1:30:15
                  那有了这个现成的solver 也有这个工具的加入之后 这solver就等于这个工具 那这个旅游的规划可以做到什么地步呢 去年4月的结果几个月后 有人用GPD4跟Cloud3 就可以做到90几%的正确率 所以看起来在有工具辅助以后 语言模型也是有机会做出不错的旅游规划 不过至少做出符合逻辑的旅游规划 好所以现在到底模型规划的能力怎么样呢 就是介于有跟没有间吧 就是你也不能说他完全没有 但你也不能说他 真的非常强 好那我们怎么进一步强化这一些AI agent的规划能力呢 能不能够让他做的比他自己想出来的规划还要更好呢 一个可能是让AI agent在做规划之前 实际上去跟环境互动看看 今天在第一个observation的时候 那看看现在有哪些可以执行的行为 总共有一之一一之二一之三三个行为 哪个行为最好呢
              
                  1:31:17
                  通通都去试一下 得到状态二之一 然后呢 状态二之一后面有两个行为也都试一下 状态二之二之后有另外一个行为试一下 状态二之三之后两个行为都试一下 得到接下来的状态 然后呢看看有没有成功的路径 报收一阵以后发现有成功的路径 这条路径是成功的 那你就知道说 那我要采取action一之三 接下来要采取action二之三之一 就会成功 简单来说就是要语言模型 跟实际的环境互动 一下报收一出一条最好的路径 那这个就是一个很强的规划的方式 但是这么做显然是有很明确的弱点的 第一个很明确的弱点就是 报收如果今天这个任务很复杂 报收所有的路径 显然是要花费非常庞大的算力的 你总不能原模型每次下决策前到报收所有的可能性吧 虽然这样可以找到最好的结果 但是可能是不切实际的想法 所以一个可能的想法是 把一些看起来没希望的路径
              
                  1:32:22
                  直接就丢掉 比如说走到某一个状态的时候 语言模型可以自问自答说 走到这个状态 还有完成功的机会吗 那如果说没有 那这条路径就不尝试下去 如果说有那才尝试下去 这样就可以减少无谓的搜寻 那这个方法有没有用呢 有一篇paper叫做Tree Search for Language Model A 那这个是去年夏天的论文 就做了类似的尝试 让模型有使用电脑的能力 这边就是给模型一个指令 跟一张图片 叫他上网去做某一件事情 那如果只是GPT4 做一般的这种直觉式的 那种反射式的回答的话 没有办法做得很好 但是他们用这个报收 加上去除没机会的路径的方式 就先走这条路径 然后呢 模型会不断自问自答说 这条路径还有希望吗 然后给一个分数
              
                  1:33:24
                  那如果分数低于某一个threshold就不做了 就跳另外一个路径 低于某一个分数不做了 再跳另外一个路径 低于某一个分数就不做了 再跳另外一个路径 那最终找出一条最佳的路径 那模型就等于做了规划 那就可以走到最佳的结果 这个是Tree Search for Language Model Agent 但这边有各式各样的这种Tree Search的algorithm 你可以采用了 这边我们就不展开细讲 那这种Tree Search的方法有很大的问题 什么样的问题呢 它的缺点是有一些动作 做完以后你是覆水难收 没有办法回头的 比如说假设现在在语言模型 可以采取的三个action里面 有一个是订pizza 有一个是订便当 然后呢他先订了pizza以后 继续走下去发现这条路不好 所以他最后发现订便当 才是最好的solution 但是你pizza已经订了 他跟人家说我不要订这个pizza了 但那个pizzahard 他已经把那个pizza做了 他说谁管你啊 你一定要把这个pizza吃下去 有些动作做了以后
              
                  1:34:27
                  就是覆水难收 所以这样的tree search的方法 跟现实世界互动 找出最佳途径的方法 也有可能有问题的 那怎么处理这个覆水难收的问题呢 一个可能性就是 让刚才一切的尝试 都发生在梦境中 都发生在脑内的巨差 刚才一切的互动 都不是现实生活中 真正发生的事情 原来都是模型脑内的模拟 他自己想像说 他执行的action一之一 他自己想像说 接下来会看到 二之一 他在自己想像去评量这个路径 有没有希望发现没有 就换搜寻另一条路径 直到达到他想像中的一个 理想的结果 但这边还有另外一个问题 从action到observation 从模型执行的行为 到他看到接下来环境的变化 这中间的过程不是模型决定的 他实际上是环境决定的
              
                  1:35:32
                  那模型怎么知道环境会有什么样的变化呢 模型怎么知道我采取一个行为 接下来会看到什么样的改变 你在跟一个对手下棋的时候 你怎么知道你下一步棋 接下来会发生什么样的事情 对方会有什么样策略的回应呢 所以你需要有一个 Wall Model 如果是在AlphaGo下棋里面 他就是自己扮演对手自己跟自己下 那在这边的情况 在这个AI agent的情况 你就是需要一个 Wall Model 他模拟环境可能会有的变化 那Wall Model怎么来呢 也许AI可以自问自答 自己扮演这个Wall Model 自己去猜想说 他执行了某件事以后 接下来会发生什么样的行为 这件事有机会成真吗 你可以读一篇paper is your LLM secretly a world model of the internet 这篇paper就是用model-based planning的方法 来打造一个web agent 这篇paper里面的解法是 现在有一个网页 模型的这个任务目标呢 是要买某一个东西
              
                  1:36:36
                  那有三个选项 有三个东西是可以点的 接下来黄色这个区块 一切所发生的事情 都是发生在脑内的剧场 都是发生在模型的梦境 它并没有实际发生 模型想像一下 我点按钮1 接下来会发生什么事 接下来会发生的事情 是用文字描述出来的 但选中文字来描述接下来发生的事情是很直觉 其实作者在文章没有解释说 那为什么不直接产生这个网页的图呢 你想说有可能吗 这个难度那么高 有没有可能真的就创造出一个 新的网页模拟出 接下来可能发生的状况呢 这难度也太高了嘛 产生文字可能是 比较实际的做法 所以接下来梦境中 这个环境会发生什么样的变化 是语言模型自己用文字描述出来的 所以他就想像说会发生什么样的变化 有了这个变化以后 他再想像自己多执行了一步 然后看看会发生什么样的事情 所以这边就是点
              
                  1:37:38
                  选第二个按钮 然后想像发生什么样的变化 自己再多执行一步 那想像会有什么样的变化 第三个按钮想像发生什么样的变化 执行部再想像会有什么样的变化 那哪一步比较好呢 他在自己去问说 那这一步大概有多少机会成功呢 自己评估一下40% 这一步自己评估一下 是80%这一步自己评估一下 是10%看起来中间第二步 机器人第二个按钮 中间第二个选项是比较容易成功的 所以他就选 实际上所以上面并没有真实 发生过黄色框框里面的事情并没有真实发生过 它是一个梦境中的脑内小剧场,模型在梦境中得到了启示说一定要选第二步,所以在真实的现实世界中,它就选择了第二步,所以这个就是让模型强化它规划能力的方式。 好,讲到这个脑内小剧场啊,那你是不是就想到说,在上次的课程中也有提到脑内小剧场,上次的课程我们说现在有很多模型都号称有思考,用英文讲就是reasoning的 那这些有reasoning能力的模型,其实所谓reasoning的能力就是可以演一个脑内小剧场,告诉你说他现在是怎么思考,如果把这些有reasoning能力的模型,拿他来做AI agent,他的脑内小剧场会不会正好就是在做规划呢,如果现在他的输入就是我们给AI agent的observation,输出就是我们要AI agent采取的action,会不会脑内小剧场就是更好,
              
                  1:39:14
                  刚才类似梦境中看到的规划呢 他自己采取了不同的可能性 自己在验证每一个可能性 可能成功的机会 自己扮演World Model 自己扮演这个世界 去想像他采取一个行为之后 接下来会发生什么样的事情 我实际试了一下DeepSeek-R1 看起来他确实有类似的效果 我们把刚才那个积木的问题交给他 然后接下来他就开始演脑内小剧场 上略1500字 他真的做了1500字 讲了很多很多 然后呢 你可以看到说在脑内小剧场的过程中 他就是做了各式各样的尝试 他做的事情就有点像是刚才的tree search 然后最后他找出了一个optimal solution 他在梦境中知道说 从橘色的方块上拿起蓝色的方块 蓝色的方块放到桌上 从桌上再拿起橘色的方块 放到蓝色的方块上 这四个步骤就可以完成我们的要求 他在梦境中已经找出了一个最佳的solution 然后再执行最佳solution的第一步 就我这边 要求他告诉我他的下一步是什么 只要求他讲一步
              
                  1:40:19
                  那脑内小剧场先找出一个成功的solution之后 在执行这个计画 他已经找出一个成功的计画之后 在执行计画的第一步 就是使用操作二 把橘色的积木从蓝色的积木上面拿起来 好 讲到这边 其实这么堂课呢 也可以停在这边 不过这边多补充一件事 就在几周之前 有一篇新的论文 叫做the danger of over thinking 他们就是把这些能够演脑内小剧场的模型 让他们扮演AI agent 看看他们做事有没有效率 其实整体而言 能够做脑内小剧场的模型 还是比不能够做脑内小剧场的模型 在AI agent的这些任务上面表现得更好 但是他们也有一些问题 他们会有什么问题呢 就是想太多了 他们是思考的巨人行动的矮子 就有时候这些模型会 比如说 按钮点下去会怎么样 他就一直想一直想一直想 怎么想都不停 那你怎么想都没有用 因为你根本不知道那个按钮点下去会发生什么事 还不如直接点一下
              
                  1:41:22
                  因为在很多情况下 你直接尝试点一下 也许只要不是这个信用卡付款的 你都按上一页就回去了 你就知道发生什么事了 与其一直想还不如做一下 或者是有些模型 他尝试都没有尝试 他光是拿那个问题想啊想啊想啊 就想说这我应该做不到 还什么都不是就直接放弃 死于想太多这样子 所以这些模型他们有的问题就是想太多 所以如何避免这些模型想太多 也许是一个未来可以研究的关键 好那以下就是今天要跟大家分享的 模型怎么根据经验调整行为 怎么使用工具,能不能够做计画
              
            