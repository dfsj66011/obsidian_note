
PPT 生成：

1. 投影片直接丢给 ChatGPT，产生投影片的讲稿；
2. 把讲稿的文字丢给 Breezy Voice 模型（语音合成模型），按参考声音生成；
3. 把合成出来的声音加上一些我的画面丢给 Heygen，生成数字人讲课。

但真正的难点并不在讲课的环节，是在做投影片上；不在制作投影片的过程， 而是想投影片的内容。
4. 用 Gamma 来生投影片，


初步具备有 AI Agent 的能力，如 Deep Research；Claude 的 Computer Use 或者是 ChatGPT 的 Operator，后面两个不只是生成，还要能够操控物件

DL 模型，深度不够长度来凑，又叫做 Testing Time Scaling

如何操控思考的长度呢？一种简单粗暴的方法，产生结束的符号的时候，直接换成 wait；

微调可以让模型具备新的技能，但挑战是有可能破坏原有的能力，微调并不是一件非常容易的事情，应该先确定在不微调真的就做不到的情况下才选择微调。

----

## 2、AI Agent 的原理

AI agent 的意思是说，人类不提供明确的行为或步骤的指示，只给 AI 目标；要解决的目标是需要透过多个步骤跟环境做很复杂的互动才能够完成，而环境会有一些不可预测的地方，所以 AI agent 还要能够做到灵活的，根据现在的状况来调整他的计划，


但通过 RL 算法的局限是，需要为每一个任务都用 RL 算法训练一个模型，AlphaGo 在经过了大量的训练以后可以下围棋，但并不代表他可以下其他的棋类，

今天 AI Agent 再次被讨论。是因为人们有了新的想法，我们能不能够直接把 LLM 直接当成一个 AI  Agent 来使用呢？所以你可能需要把环境转化成文字的叙述

在有些问题他解不了的时候，他可以直接呼叫一些工具来帮忙解决他本来解决不了的问题；
另一个 AI agent 的优势是如果用 RL 的方法训练 AI agent，意味着必须要定义 reward（难定义），如果是用 LLM 驱动的 AI agent，不需要定义 reward，例如编码 error log，直接扔给它进行正确修改，相比 reward，这可能提供了更丰富的资讯；

三个面，剖析 AI agent 的关键能力：

1. 能不能够根据他的经验，过去的互动中所获得的经验来调整他的行为 
2. 如何呼叫外部的援助，如何使用工具 
3. 能不能够执行计划，能不能做计划

关于 1，真正的议题是：如果我们是把过去所有的经验都存起来，要改变语言模型的行为，要让他根据过去的经验调整行为，就是把过去所有发生的事情一股脑给他，那就好像是语言模型每次做一次决策的时候，他都要回忆他一生的经历，也许他没有足够的算力，来回顾一生的资讯，他就没有办法得到正确的答案，所以怎么办呢？

也许我们可以给这些 AI agent memory，这就像是人类的长期记忆一样，发生过的事情，把它存到这个 memory 里面，有一个叫做 read 的模组，会从 memory 里面选择跟现在要解决的问题有关系的经验，把这些有关系的经验放在 observation 的前面，让模型根据这些有关系的经验跟 observation 再做文字接龙，

那怎么样打造这个 read 的模组呢？就想成是一个 retrieval 的 system，其实它就是 RAG，唯一不一样的地方，如果是 RAG 的话，存在 memory 里面的东西是别人的经验，而对 AI agent 而言，是他自己个人的经历，差别的是经历的来源。

根据经验调整行为能力的好坏，那就看这一整个回答的过程中平均的正确率，越能够根据经验学习的agent，应该能够用越少的时间，看过越少的回馈，就越快能够增强他的能力，就可以得到比较高的平均的正确率。

还发现一个有趣的现象是值得跟大家分享。这个现象是负面的回馈，基本上没有帮助

可以有一个 write 的 module，决定什么样的资讯要被填到长期的记忆库里面，怎么样打造这个 write 的记忆库呢？一个很简单的方法就是 write 的模组也是一个语言模型甚至就是 AI agent 自己

还有第三个模组，没有固定的名字，暂时叫 reflection 反思的模组，这个模组的工作是对记忆中的资讯做更好的、更 high level 的，可能是抽象的重新整理，可能也是一个语言模型，或 AI agent 自己

-------

关于 2，怎么使用工具？小模型呼叫大模型帮忙，这些工具对语言模型来说都是 function，

使用工作的挑战，每一个工具都要有对应的文字描述，告诉语言模型这个工具怎么被使用，假设工具很多怎么办呢？类似上文，打造一个工具选择的模组；也可以自己写个 function 作为 tool

工具有可能会犯错，所以我们也要告诉我们的工具，这些不要完全相信工具的工具，要有自己的判断能力，不要完全相信工具的工具给你的结果，比如给出的天气温度为 100°

那什么样的外部知识比较容易说服 AI，让他相信你说的话呢？外部的知识，如果跟模型本身的信念差距越大，模型就越不容易相信，那如果跟本身的信念差距比较小，模型就比较容易相信

----------

关于 3，AI 语言模型能不能做计划？语言模型就是给一个输入，它就直接给一个输出，也许在给输出的过程中有进行计划，但我们不一定能够明确的知道这件事；

但其实可以强迫语言模型直接明确的产生规划，可以直接问语言模型说，如果现在要达成我们的目标，从这个 observation 开始，你觉得应该要做哪些行动？

不过这是一个理想的想法。那语言模型到底有没有做计划的能力呢？过去确实也有很多论文说，语言模型是有一定程度做计划的能力的。

现在到底模型规划的能力怎么样呢，就是介于有跟没有间吧……

-----------


