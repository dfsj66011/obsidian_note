
å‘è¡¨æ—¶é—´ï¼š2025.02.19
å»ºè®®é˜…è¯»æ—¶é•¿ï¼š2-4 å¤©
ä½œè€…ï¼šNouamane Tazi, Ferdinand Mom,Â Haojun Zhao,Â Phuc Nguyen,Â Mohamed Mekkouri,Â Leandro Werra,Â Thomas Wolf

[Appendix](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#appendix)

- [A0: Parallel Programming Crash Course](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.h

- [A2: Typical Scales in LLM Training](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a2:_typical_scales_in_llm_training)
- [A3: Math for Compute/Communication Overlap](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a3:_math_for_compute/communication_overlap)

- [Data Parallelism Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#data_parallelism_communication_analysis)
- [ZeRO-3 (FSDP) Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#zero-3_\(fsdp\)_communication_analysis)
- [TP Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#tp_communication_analysis)
- [PP Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#pp_communication_analysis)



## å››ã€å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰

æ‰€ä»¥æˆ‘ä»¬å·²ç»ä½¿ç”¨ ZeRO å¯¹æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œäº†åˆ†ç‰‡ï¼Œä½†æ˜¯ä¸€æ—¦æ¿€æ´»å†…å­˜è¶…è¿‡æˆ‘ä»¬çš„å†…å­˜é¢„ç®—ï¼Œæˆ‘ä»¬å°±é‡åˆ°äº†ä¸€ä¸ªé™åˆ¶ã€‚æ¬¢è¿å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æƒé‡ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠæ¿€æ´»è¿›è¡Œåˆ†ç‰‡çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸éœ€è¦åœ¨è®¡ç®—ä¹‹å‰å°†å®ƒä»¬å…¨éƒ¨æ”¶é›†èµ·æ¥ã€‚è¿™å¬èµ·æ¥åƒæ˜¯ä¸€ä¸ªæ¢¦æƒ³ï¼è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹å¼ é‡å¹¶è¡Œæ˜¯å¦‚ä½•é€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•æ¥å·¥ä½œçš„ã€‚

å¼ é‡å¹¶è¡Œåˆ©ç”¨äº†çŸ©é˜µä¹˜æ³• $A\times B$ çš„æ•°å­¦ç‰¹æ€§ã€‚è¦ç†è§£å…¶å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä½¿è¿™ç§å¹¶è¡ŒåŒ–æˆä¸ºå¯èƒ½çš„ä¸¤ä¸ªåŸºæœ¬æ–¹ç¨‹å¼ï¼š$$\begin{align*}
1. \quad & A \cdot B = A \cdot \begin{bmatrix} B_1 & B_2 & \cdots \end{bmatrix} = \begin{bmatrix} AB_1 & AB_2 & \cdots \end{bmatrix} \\[1.2ex]
2. \quad & A \cdot B = \begin{bmatrix} A_1 & A_2 & \cdots \end{bmatrix} \begin{bmatrix} B_1 \\ B_2 \\ \vdots \end{bmatrix} = \sum_{i=1}^{n} A_i B_i
\end{align*}$$
è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ¥è®¡ç®—çŸ©é˜µä¹˜ç§¯ï¼š1ï¼‰åˆ†åˆ«ä¹˜ä»¥ $B$ çš„æ¯ä¸€åˆ—ï¼›æˆ–è€…2ï¼‰åˆ†åˆ«ä¹˜ä»¥æ¯ä¸€è¡Œå¹¶å°†ç»“æœç»„åˆèµ·æ¥ã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒçŸ©é˜µä¹˜æ³•é€šå¸¸ä»¥ä»¥ä¸‹æ ¼å¼è¡¨ç¤ºï¼š$X \times W$ï¼Œå…¶ä¸­ï¼š

* $X$ è¡¨ç¤ºè¾“å…¥æˆ–æ¿€æ´»å€¼  
* $W$ è¡¨ç¤º `nn.Linear` çš„æƒé‡

åœ¨å®é™…æ“ä½œä¸­ï¼Œè¯¥æ“ä½œçš„ä¸€ä¸ªå°ç¤ºä¾‹æ˜¯è¿™æ ·çš„ï¼š

![TP diagram|240](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram.svg)

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å¯¹è¿™ä¸ªæ“ä½œè¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼åœ¨å¼ é‡å¹¶è¡Œä¸­ï¼Œå¼ é‡å°†æ²¿ç€ç‰¹å®šç»´åº¦è¢«åˆ†å‰²æˆ $N$ ä¸ªåˆ†ç‰‡ï¼Œå¹¶åˆ†å¸ƒåœ¨ $N$ ä¸ª GPU ä¸Šã€‚çŸ©é˜µå¯ä»¥åœ¨åˆ—éƒ¨åˆ†æˆ–è¡Œéƒ¨åˆ†è¿›è¡Œåˆ†å‰²ï¼Œä»è€Œå®ç°è¡Œå¹¶è¡Œå’Œåˆ—å¹¶è¡Œã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œé€‰æ‹©è¡Œåˆ†ç‰‡è¿˜æ˜¯åˆ—åˆ†ç‰‡å°†éœ€è¦ä¸åŒçš„é€šä¿¡åŸè¯­ã€‚

æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨æŒ‰åˆ—åˆ†ç‰‡ï¼ˆä¹Ÿç§°ä¸º***åˆ—çº¿æ€§*** ï¼‰ï¼šæˆ‘ä»¬å°†æŠŠå®Œæ•´çš„è¾“å…¥çŸ©é˜µå¤åˆ¶åˆ°æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œè¿™éœ€è¦ä¸€ä¸ªç§°ä¸º***å¹¿æ’­*** çš„æ“ä½œï¼Œå¹¶å°†æƒé‡çŸ©é˜µåˆ†å‰²æˆåˆ—ã€‚ç„¶åå°†è¾“å…¥ä¸éƒ¨åˆ†æƒé‡çŸ©é˜µç›¸ä¹˜ï¼Œæœ€åä½¿ç”¨ ***all-gather*** æ“ä½œåˆå¹¶ç»“æœã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram2.png)

ä»¥ä¸‹æ˜¯æŒ‰åˆ—è¿›è¡Œå¼ é‡å¹¶è¡Œçš„ä»£ç å®ç°ï¼š

ğŸ‘‰ Picotron ä¸­çš„åˆ—å¹¶è¡Œå¼ é‡å¹¶è¡Œå®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class ColumnParallelLinear(torch.nn.Module):
    """Column Parallel Linear layer
    Y = XW + b, where weight matrix W is parallelized along its second dimension. W = [W_1, ..., W_p]
    This module returns the results of Y_i = XW_i + b_i in the forward method, Y_i is parallelized in the second dimension.
    Arguments:
        in_features: first dimension of weight matrix W.
        out_features: second dimension of weight matrix W.
        bias: If true, add bias
        init_method: method to initialize weights
        gather_output: If true, gather the output from all the partitions. This is used for the last linear layer
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = False,
        gather_output: bool = False,
        async_all_reduce: bool = False,
    ) -> None:
        super(ColumnParallelLinear, self).__init__()

        self.tp_world_size = pgm.process_group_manager.tp_world_size
        self.tp_rank = pgm.process_group_manager.tp_rank 

        self.in_features = in_features
        self.out_features = out_features
        assert out_features % self.tp_world_size == 0, "Hidden dimension must be divisible by the tensor parallel world size"
        self.output_size_per_partition = out_features // self.tp_world_size
        self.gather_output = gather_output
        self.async_all_reduce = async_all_reduce
        # Allocate space for the weight and bias
        # Note: torch.nn.functional.linear performs XW^T + b so we exchange the order of dimensions
        self.weight = nn.Parameter(torch.Tensor(self.output_size_per_partition, self.in_features)) # W_i
        if bias:
            self.bias = nn.Parameter(torch.Tensor(self.output_size_per_partition))
            with torch.no_grad():
                self.bias.zero_()
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weight tensor with the default initialization method used for nn.Linear in PyTorch
        master_weight = torch.empty(
            self.out_features, 
            self.in_features, 
            dtype=self.weight.dtype,
            device=self.weight.device,
            requires_grad=False
        )
        
        # Calculate bound based on master weight's input dimension
        k = 1 / master_weight.size(1)
        bound = math.sqrt(k)
        torch.nn.init.uniform_(master_weight, -bound, bound)
        
        # Split the model into size of self.output_size_per_partition
        weight_list = torch.split(master_weight, self.output_size_per_partition, dim=0)
        self.weight.data = weight_list[self.tp_rank].contiguous()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        if self.async_all_reduce:
            output = linear_with_async_all_reduce(x, self.weight, self.bias) 
        else:
            output = linear_with_all_reduce(x, self.weight, self.bias) 
        if self.gather_output:
            output = GatherFromModelParallelRegion.apply(output)
        return output
```

ç¬¬äºŒä¸ªé€‰é¡¹ç§°ä¸ºæŒ‰è¡Œåˆ†ç‰‡ï¼ˆä¹Ÿç§°ä¸º***è¡Œçº¿æ€§***åˆ†ç‰‡ï¼‰ï¼šç»†å¿ƒçš„è¯»è€…å¯èƒ½ä¼šçŒœåˆ°ï¼Œè¡Œçº¿æ€§åˆ†ç‰‡æ„å‘³ç€æˆ‘ä»¬å°†æƒé‡çŸ©é˜µåˆ†å‰²æˆè¡Œå—ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿè¦æ±‚æˆ‘ä»¬å¯¹è¾“å…¥è¿›è¡Œåˆ†å‰²ï¼Œè¿™éœ€è¦ä¸€ä¸ª ***scatter*** æ“ä½œï¼Œè€Œä¸æ˜¯åƒåˆ—çº¿æ€§åˆ†ç‰‡ä¸­ä½¿ç”¨çš„å¹¿æ’­ã€‚æ¯ä¸ªå·¥ä½œå™¨ä¸Šçš„ç»“æœå·²ç»æ˜¯æ­£ç¡®çš„å½¢çŠ¶ï¼Œä½†éœ€è¦æ±‚å’Œä»¥å¾—åˆ°æœ€ç»ˆç»“æœï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦ä¸€ä¸ª all-reduce æ“ä½œã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°äº†æˆ‘ä»¬çš„ç¬¬å››ä¸ªåˆ†å¸ƒå¼åŸè¯­ï¼š**_scatter_**ï¼

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram3.png)

ä»¥ä¸‹æ˜¯æŒ‰è¡Œè¿›è¡Œå¼ é‡å¹¶è¡Œçš„å®ç°æ–¹å¼ï¼š

ğŸ‘‰ Picotron ä¸­çš„è¡Œå¹¶è¡Œå¼ é‡å¹¶è¡Œå®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class RowParallelLinear(nn.Module):
    """Linear layer with row parallelism.
    Y = XW + b. W is parallelized along its first dimension and X along its second dimension as:
               -   -
              | W_1 |
              | .   |
          W = | .   |        X = [X_1, ..., X_p]
              | .   |
              | W_p |
               -   -
    We assume that X is already parallelized. This is the case after ColumnParallelLinear.
    This module returns the results of Y = sum(X_i * W_i + b_i) in the forward method.
    Arguments:
        in_features: first dimension of matrix W.
        out_features: second dimension of matrix W.
        bias: If true, add bias
        init_method: method to initialize weights.
    """
    def __init__(self, in_features: int, out_features: int, bias: bool):
        super(RowParallelLinear, self).__init__()

        self.tp_world_size = pgm.process_group_manager.tp_world_size
        self.tp_rank = pgm.process_group_manager.tp_rank 

        self.in_features = in_features
        self.out_features = out_features
        assert in_features % self.tp_world_size == 0, "Hidden dimension must be divisible by the tensor parallel world size"
        self.input_size_per_partition = in_features // self.tp_world_size

        self.weight = nn.Parameter(torch.Tensor(self.out_features, self.input_size_per_partition))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(self.out_features))
            # Always initialize bias to zero.
            with torch.no_grad():
                self.bias.zero_()
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weight tensor with same dtype and device as self.weight
        master_weight = torch.empty(
            self.out_features, 
            self.in_features, 
            dtype=self.weight.dtype,
            device=self.weight.device,
            requires_grad=False
        )
        
        # Calculate bound based on master weight's input dimension
        k = 1 / master_weight.size(1)
        bound = math.sqrt(k)    
        torch.nn.init.uniform_(master_weight, -bound, bound)
        
        # Split the model into size of self.input_size_per_partition
        weight_list = torch.split(master_weight, self.input_size_per_partition, dim=1)
        self.weight.data = weight_list[self.tp_rank].contiguous()

    def forward(self, x):
        # X_i * W_i^T + b
        output_parallel = F.linear(x, self.weight)
        # All-reduce across all the partitions.
        output = ReduceFromModelParallelRegion.apply(output_parallel)
        return output if self.bias is None else output + self.bias
```

æ—¢ç„¶æˆ‘ä»¬å·²ç»äº†è§£äº† Transformer çš„åŸºæœ¬æ„å»ºæ¨¡å—ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ Transformer å±‚ä¸­æœ‰æ•ˆåœ°ç»„åˆå®ƒä»¬ï¼

### 4.1 Transformer å—ä¸­çš„å¼ é‡å¹¶è¡Œæ€§

ä¸ºäº†æå‡ºä¸€ä¸ªå¯éµå¾ªçš„ç­–ç•¥ï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹è¿‡æ¸¡åˆ°ä¸€ä¸ªçœŸå®çš„æ¨¡å‹æ„å»ºæ¨¡å—ã€‚Transformeræ¨¡å‹ç”±ä¸¤ä¸ªä¸»è¦çš„æ„å»ºæ¨¡å—ç»„æˆï¼šå‰é¦ˆå±‚ï¼ˆMLPï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸¤è€…éƒ½åº”ç”¨å¼ é‡å¹¶è¡Œæ€§ã€‚

å‰é¦ˆéƒ¨åˆ†å¯ä»¥é€šè¿‡å…ˆè¿›è¡Œâ€œåˆ—çº¿æ€§â€æ“ä½œï¼Œå†è¿›è¡Œâ€œè¡Œçº¿æ€§â€æ“ä½œæ¥å®ç°å¹¶è¡ŒåŒ–ï¼Œè¿™ç›¸å½“äºåœ¨å‰å‘ä¼ æ’­ä¸­è¿›è¡Œå¹¿æ’­ä»¥å¤åˆ¶è¾“å…¥å¹¶è¿›è¡Œ all-reduce æ“ä½œã€‚è¯·æ³¨æ„ï¼Œåœ¨å®é™…è®­ç»ƒä¸­ä¸éœ€è¦å¹¿æ’­ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç¡®ä¿è¾“å…¥å·²ç»åœ¨ TP rank ä¹‹é—´åŒæ­¥ã€‚è¿™ç§è®¾ç½®æ¯”å…ˆè¿›è¡Œâ€œè¡Œçº¿æ€§â€æ“ä½œï¼Œå†è¿›è¡Œâ€œåˆ—çº¿æ€§â€æ“ä½œæ›´é«˜æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è·³è¿‡ä¸¤ä¸ªæ‹†åˆ†æ“ä½œä¹‹é—´çš„ä¸­é—´ all-reduce æ“ä½œã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram4.png)

æ—¢ç„¶æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº† Transformer å‰é¦ˆéƒ¨åˆ†çš„ä¸€ä¸ªé«˜æ•ˆæ¨¡å¼ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ¥çœ‹çœ‹å¤šå¤´æ³¨æ„åŠ›å—ï¼ˆMHAï¼‰ã€‚

æˆ‘ä»¬é€šå¸¸å¯ä»¥é‡‡ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œå…¶ä¸­ Qã€K å’Œ V çŸ©é˜µä»¥åˆ—å¹¶è¡Œçš„æ–¹å¼æ‹†åˆ†ï¼Œè¾“å‡ºæŠ•å½±æ²¿è¡Œç»´åº¦æ‹†åˆ†ã€‚å¯¹äºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ—å¹¶è¡Œæ–¹æ³•æœ‰ä¸€ä¸ªéå¸¸è‡ªç„¶çš„è§£é‡Šï¼šæ¯ä¸ªå·¥ä½œå™¨è®¡ç®—å•ä¸ªå¤´æˆ–ä¸€ç»„å¤´çš„æ³¨æ„åŠ›ã€‚è¿™ç§æ–¹æ³•åŒæ ·é€‚ç”¨äºå¤šæŸ¥è¯¢ï¼ˆMQAï¼‰æˆ–åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œåœ¨è¿™äº›æœºåˆ¶ä¸­ï¼Œé”®å’Œå€¼åœ¨æŸ¥è¯¢ä¹‹é—´æ˜¯å…±äº«çš„ã€‚

ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¼ é‡å¹¶è¡Œåº¦ä¸åº”è¶…è¿‡æŸ¥è¯¢/é”®/å€¼ï¼ˆQ/K/Vï¼‰å¤´çš„æ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ç­‰çº§éƒ½éœ€è¦å®Œæ•´çš„å¤´ï¼ˆå¦åˆ™æˆ‘ä»¬å°±æ— æ³•åœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶ä¸”å°†éœ€è¦é¢å¤–çš„é€šä¿¡æ“ä½œï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œå¼ é‡å¹¶è¡Œåº¦å®é™…ä¸Šåº”å°äºé”®/å€¼ï¼ˆK/Vï¼‰å¤´çš„æ•°é‡ã€‚ä¾‹å¦‚ï¼ŒLLaMA-3 8B æœ‰ 8 ä¸ªé”®/å€¼å¤´ï¼Œæ‰€ä»¥å¼ é‡å¹¶è¡Œåº¦æœ€å¥½ä¸è¶…è¿‡ 8ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¿™ä¸ªæ¨¡å‹ä½¿ç”¨ TP=16ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ª GPU ä¸Šå¤åˆ¶é”®/å€¼å¤´ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¿æŒåŒæ­¥ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_full_diagram.png)

æœ€åè¯·æ³¨æ„ï¼Œå¼ é‡å¹¶è¡Œæ€§ä»ç„¶ä¸æ˜¯è®­ç»ƒçš„ä¸‡èƒ½è‰¯æ–¹ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹çš„è®¡ç®—è·¯å¾„ä¸­ç›´æ¥æ·»åŠ äº†å‡ ç§åˆ†å¸ƒå¼é€šä¿¡åŸè¯­ï¼Œå› æ­¤å¾ˆéš¾åƒæˆ‘ä»¬åœ¨ ZeRO ä¸­æ‰€åšçš„é‚£æ ·å°†å…¶ä¸è®¡ç®—å®Œå…¨éšè—/é‡å ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ€§èƒ½å°†æ˜¯è®¡ç®—å’Œå†…å­˜å¢ç›Šä¸å¢åŠ çš„é€šä¿¡å¼€é”€ä¹‹é—´æƒè¡¡çš„ç»“æœã€‚è®©æˆ‘ä»¬æ¥è¯´æ˜è¿™ä¸€ç‚¹ï¼š

![Forward pass in Tensor Parallelism|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_overlap.svg)

ï¼ˆé€šè¿‡å¯¹åˆ†å—çŸ©é˜µä¹˜æ³•å¹¶ç»“åˆå¼‚æ­¥é€šä¿¡/è®¡ç®—ï¼Œå¯ä»¥éƒ¨åˆ†éšè—è¿™ç§é€šä¿¡ã€‚ï¼‰

è§‚å¯Ÿå¼ é‡å¹¶è¡Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼ˆæ³¨æ„åŠ›æœºåˆ¶åŒæ ·é€‚ç”¨ï¼‰çš„æ“ä½œæ—¶é—´çº¿ï¼Œæˆ‘ä»¬èƒ½æ›´å¥½åœ°ç†è§£å…¶ä¸­æ¶‰åŠçš„æƒè¡¡ã€‚åœ¨æ¯ä¸ªè§£ç å™¨å±‚çš„å‰å‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°ä¸€ä¸ªåŒæ­¥ç‚¹ï¼Œå³AllReduceæ“ä½œï¼Œè¯¥æ“ä½œæ— æ³•ä¸è®¡ç®—é‡å ã€‚è¿™ç§ *æ˜¾éœ²å‡ºæ¥çš„é€šä¿¡å¼€é”€* å¯¹äºåœ¨æœ€ç»ˆåº”ç”¨å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ä¹‹å‰åˆå¹¶å¼ é‡å¹¶è¡Œ ranks ä¹‹é—´çš„éƒ¨åˆ†ç»“æœæ˜¯å¿…è¦çš„ã€‚

ï¼ˆä¾‹å¦‚ï¼ŒMegatron-LM/Nanotron å®ç°äº† all-gather ä¸ FC1 è®¡ç®—çš„éƒ¨åˆ†é‡å ï¼Œå…¶ä¸­çŸ©é˜µä¹˜æ³•ç»“æœçš„ä¸€éƒ¨åˆ†å°†åœ¨å¦ä¸€éƒ¨åˆ†ä»åœ¨è®¡ç®—æ—¶å¼€å§‹å‘é€åˆ°å…¶ä»– GPUã€‚ï¼‰

å¼ é‡å¹¶è¡Œç¡®å®æœ‰åŠ©äºå‡å°‘çŸ©é˜µä¹˜æ³•çš„æ¿€æ´»å†…å­˜ï¼Œå› ä¸ºä¸­é—´æ¿€æ´»è¢«åˆ†ç‰‡åˆ°å¤šä¸ª GPU ä¸Šã€‚ç„¶è€Œï¼Œå¯¹äºåƒ LayerNorm è¿™æ ·çš„æ“ä½œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦æ”¶é›†å®Œæ•´çš„æ¿€æ´»ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ²¡æœ‰è·å¾—æˆ‘ä»¬æœ¬å¯ä»¥è·å¾—çš„å…¨éƒ¨å†…å­˜ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå¼ é‡å¹¶è¡Œå¼•å…¥äº†æ˜¾è‘—çš„é€šä¿¡éœ€æ±‚ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç½‘ç»œåŸºç¡€è®¾æ–½ã€‚æ— æ³•å°†è¿™ç§ç‰¹å®šçš„ AllReduce å®Œå…¨éšè—åœ¨è®¡ç®—èƒŒåæ„å‘³ç€å®ƒä¼šç›´æ¥å¢åŠ å‰å‘ä¼ æ’­çš„å…³é”®è·¯å¾„ã€‚

ï¼ˆè¿™ä¸€ç ”ç©¶é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œè¿‘æœŸçš„ç ”ç©¶å·¥ä½œå¦‚ Domino[4] æ¢ç´¢äº†æœ€å¤§åŒ–è¿™ç§é‡å çš„æ–°æŠ€æœ¯ã€‚ï¼‰

è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹åœ¨æ‰©å±• TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰åº¦æ—¶æ‰€æ¶‰åŠçš„æƒè¡¡ï¼š

[äº¤äº’å›¾]

å¢åŠ è®­ç»ƒè¿›ç¨‹æ•°ï¼ˆTPï¼‰ä¼šå¯¼è‡´æ¯ä¸ª GPU çš„ååé‡é™ä½ï¼ˆå·¦å›¾ï¼‰ï¼Œä½†å®ƒèƒ½å¤Ÿå¤„ç†æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼ˆå³å›¾ï¼‰ï¼Œè¿™è¯´æ˜äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­è®¡ç®—æ•ˆç‡å’Œå†…å­˜å¯ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

å®é™…ä¸Šï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å·¦å›¾ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œå½“æ‰©å±•åˆ° 8 ä¸ª GPU ä»¥ä¸Šæ—¶ï¼Œå¼ é‡å¹¶è¡Œçš„é€šä¿¡å¼€é”€å˜å¾—å°¤ä¸ºæ˜æ˜¾ã€‚è™½ç„¶åœ¨å•ä¸ªèŠ‚ç‚¹å†…å¯ä»¥åˆ©ç”¨å¿«é€Ÿçš„ NVLink äº’è¿å®ç°å¼ é‡å¹¶è¡Œï¼Œä½†è·¨èŠ‚ç‚¹åˆ™éœ€è¦è¾ƒæ…¢çš„ç½‘ç»œè¿æ¥ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä» TP=8 å¢åŠ åˆ° TP=16 æ—¶å‡ºç°äº†æ˜¾è‘—çš„ä¸‹é™ï¼Œå¹¶ä¸”ä» TP=16 å¢åŠ åˆ° TP=32 æ—¶ä¸‹é™æ›´ä¸ºé™¡å³­ã€‚åœ¨æ›´é«˜çš„å¹¶è¡Œåº¦ä¸‹ï¼Œé€šä¿¡å¼€é”€å˜å¾—å¦‚æ­¤ä¹‹é«˜ï¼Œä»¥è‡³äºè¿…é€Ÿå æ®äº†è®¡ç®—æ—¶é—´ã€‚

è¯è™½å¦‚æ­¤ï¼Œå¼ é‡å¹¶è¡Œé€šè¿‡å°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠï¼ˆåœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼‰æ¿€æ´»åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šï¼Œä¸ºå†…å­˜ä½¿ç”¨æä¾›äº†é‡è¦ä¼˜åŠ¿ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯¹ä¸€ä¸ª 70B å‚æ•°æ¨¡å‹äº§ç”Ÿçš„å½±å“ï¼š

[äº¤äº’å›¾]

æé«˜å¼ é‡å¹¶è¡Œåº¦å¯å‡å°‘æ¯ä¸ª GPU ä¸Šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€æ‰€éœ€çš„å†…å­˜ï¼Œä»è€Œè®©æˆ‘ä»¬èƒ½å¤Ÿå¼€å§‹åœ¨å•ä¸ª 8 GPU èŠ‚ç‚¹ä¸Šæ‹Ÿåˆå¤§å‹æ¨¡å‹ã€‚

æœ‰æ²¡æœ‰åŠæ³•ä»è¿™ç§æŠ€æœ¯ä¸­è·å¾—æ›´å¤šçš„å¥½å¤„å‘¢ï¼Ÿæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œå±‚å½’ä¸€åŒ–å’Œ dropout ä»ç„¶éœ€è¦åœ¨æ¯ä¸ª GPU ä¸Šæ”¶é›†å…¨éƒ¨æ¿€æ´»å€¼ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸ŠæŠµæ¶ˆäº†å†…å­˜èŠ‚çœçš„æ•ˆæœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾å¹¶è¡ŒåŒ–è¿™äº›å‰©ä½™æ“ä½œçš„æ–¹æ³•æ¥åšå¾—æ›´å¥½ã€‚

> [!NOTE]
> å…³äºå¼ é‡å¹¶è¡Œè®­ç»ƒä¸­å±‚å½’ä¸€åŒ–çš„ä¸€ä¸ªæœ‰è¶£è¯´æ˜â€”â€”ç”±äºæ¯ä¸ªå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ranks åœ¨ all-gather åçœ‹åˆ°ç›¸åŒçš„æ¿€æ´»å€¼ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­åï¼Œå±‚å½’ä¸€åŒ–æƒé‡å®é™…ä¸Šä¸éœ€è¦ all-reduce æ¥åŒæ­¥å®ƒä»¬çš„æ¢¯åº¦ã€‚å®ƒä»¬è‡ªç„¶ä¼šåœ¨å„ç­‰çº§ä¹‹é—´ä¿æŒåŒæ­¥ã€‚ç„¶è€Œï¼Œå¯¹äº dropout æ“ä½œï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿è·¨ TP ranks åŒæ­¥éšæœºç§å­ï¼Œä»¥ç»´æŒç¡®å®šæ€§è¡Œä¸º ã€‚

æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¢è®¨ä¸€ä¸‹å¼ é‡å¹¶è¡Œçš„ä¸€ç§å°å‹ä¸”è‡ªç„¶çš„æ‰©å±•æ–¹å¼ï¼Œå³*åºåˆ—å¹¶è¡Œ*ï¼Œå®ƒæ‰€åšçš„äº‹æƒ…æ­£æ˜¯å¦‚æ­¤ã€‚

### 4.2 åºåˆ—å¹¶è¡Œ

åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ¶‰åŠå¯¹å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æœªå¤„ç†çš„æ¨¡å‹éƒ¨åˆ†ï¼ˆå¦‚ Dropout å’Œ LayerNormï¼‰çš„æ¿€æ´»å’Œè®¡ç®—è¿›è¡Œæ‹†åˆ†ï¼Œä½†æ²¿è¾“å…¥åºåˆ—ç»´åº¦è€Œééšè—ç»´åº¦è¿›è¡Œæ‹†åˆ†ã€‚

> [!NOTE]
> â€œåºåˆ—å¹¶è¡Œâ€ï¼ˆSequence Parallelismï¼‰è¿™ä¸ªæœ¯è¯­æœ‰äº›å«ä¹‰è¿‡è½½ï¼šæœ¬èŠ‚ä¸­çš„åºåˆ—å¹¶è¡Œä¸å¼ é‡å¹¶è¡Œç´§å¯†ç›¸å…³ï¼Œå¹¶ä¸”é€‚ç”¨äºdropoutï¼ˆéšæœºå¤±æ´»ï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆlayer normï¼‰æ“ä½œã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬å°†å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œæ³¨æ„åŠ›è®¡ç®—å°†æˆä¸ºç“¶é¢ˆï¼Œè¿™å°±éœ€è¦è¯¸å¦‚ç¯å½¢æ³¨æ„åŠ›ï¼ˆRing-Attentionï¼‰ä¹‹ç±»çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸ºâ€œåºåˆ—å¹¶è¡Œâ€ï¼Œä½†ä¸ºäº†åŒºåˆ†è¿™ä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä»¬ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å¹¶è¡Œâ€ï¼ˆContext Parallelismï¼‰ã€‚æ‰€ä»¥ï¼Œæ¯æ¬¡çœ‹åˆ°â€œåºåˆ—å¹¶è¡Œâ€æ—¶ï¼Œè¦è®°ä½å®ƒæ˜¯ä¸å¼ é‡å¹¶è¡Œä¸€èµ·ä½¿ç”¨çš„ï¼ˆä¸ä¸Šä¸‹æ–‡å¹¶è¡Œç›¸å¯¹ï¼Œä¸Šä¸‹æ–‡å¹¶è¡Œå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼‰ã€‚

è¿™æ˜¯å› ä¸ºè¿™äº›æ“ä½œéœ€è¦è®¿é—®å®Œæ•´çš„éšè—ç»´åº¦æ‰èƒ½æ­£ç¡®è®¡ç®—ã€‚ä¾‹å¦‚ï¼ŒLayerNorm éœ€è¦å®Œæ•´çš„éšè—ç»´åº¦æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼š$$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
å…¶ä¸­ï¼Œ$\mu = \text{mean}(x)$ å’Œ $\sigma^2 = \text{var}(x)$ æ˜¯åœ¨éšè—ç»´åº¦ $h$ ä¸Šè®¡ç®—å¾—åˆ°çš„ã€‚

å› æ­¤ï¼Œå³ä½¿è¿™äº›æ“ä½œåœ¨è®¡ç®—ä¸Šæˆæœ¬è¾ƒä½ï¼Œä½†ç”±äºéœ€è¦å®Œæ•´çš„éšè—ç»´åº¦ï¼Œå®ƒä»¬ä»ç„¶éœ€è¦å¤§é‡çš„æ¿€æ´»å†…å­˜ã€‚åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰å…è®¸æˆ‘ä»¬é€šè¿‡æ²¿åºåˆ—ç»´åº¦è¿›è¡Œæ‹†åˆ†ï¼Œåœ¨å¤šä¸ª GPU ä¹‹é—´åˆ†æ‹…è¿™ä¸€å†…å­˜è´Ÿæ‹…ã€‚

åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å°†ä»å·¦å›¾è¿‡æ¸¡åˆ°å³å›¾ï¼š
![|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_diagram.png)
è¯¥å›¾è¡¨å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¸åŒçš„é›†åˆæ“ä½œï¼ˆåˆ†åˆ«æ ‡è®°ä¸º â€œfâ€ å’Œ â€œgâ€ï¼‰åœ¨å¼ é‡å¹¶è¡ŒåŒºåŸŸå’Œåºåˆ—å¹¶è¡ŒåŒºåŸŸä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºé«˜æ•ˆåœ°ç®¡ç†è¿™äº›è½¬æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å†…å­˜ä½¿ç”¨é‡å¹¶ç¡®ä¿æ­£ç¡®æ€§ã€‚

åœ¨å‰å‘ä¼ æ’­ä¸­ï¼š

* "f" æ˜¯ä¸€ä¸ªç©ºæ“ä½œï¼ˆæ— æ“ä½œï¼‰ï¼Œå› ä¸ºæ¿€æ´»å·²åœ¨å„ ranks ä¹‹é—´è¿›è¡Œäº†å¤åˆ¶  
* "f*" æ˜¯ä¸€ä¸ª all-reduce æ“ä½œï¼Œç”¨äºåŒæ­¥æ¿€æ´»å¹¶ç¡®ä¿æ­£ç¡®æ€§

åœ¨åå‘ä¼ æ’­ä¸­ï¼š

* "f*" æ˜¯æ— æ“ä½œï¼ˆno-opï¼‰ï¼Œå› ä¸ºæ¢¯åº¦å·²åœ¨å„ ranks ä¸­è¢«å¤åˆ¶  
* "f"æ˜¯ all-reduce æ“ä½œï¼Œç”¨äºåŒæ­¥æ¢¯åº¦

è¿™äº›è¿ç®— â€œfâ€ å’Œ â€œf*â€ è¢«ç§°ä¸º*å…±è½­* å¯¹ï¼Œå› ä¸ºå®ƒä»¬ç›¸äº’è¡¥å……â€”â€”å½“ä¸€ä¸ªåœ¨æ­£å‘ä¸­æ˜¯æ— æ“ä½œï¼ˆno-opï¼‰æ—¶ï¼Œå¦ä¸€ä¸ªåœ¨åå‘ä¸­å°±æ˜¯ all-reduceï¼Œåä¹‹äº¦ç„¶ ã€‚

å¯¹äºåºåˆ—å¹¶è¡Œï¼ˆSPï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡è®°ä¸º â€œgâ€ å’Œ â€œg*â€ çš„ä¸åŒæ“ä½œã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ SP åŒºåŸŸä¸­é¿å…ä½¿ç”¨ all-reduce æ“ä½œï¼Œå› ä¸ºè¿™éœ€è¦æ”¶é›†å…¨éƒ¨æ¿€æ´»å€¼ï¼Œä»è€Œå¢åŠ æˆ‘ä»¬çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡ï¼Œè¿èƒŒäº† SP çš„åˆè¡·ã€‚

é‚£ä¹ˆè¿™é‡Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿæ­£å¦‚ä¸€ä¸ªè‘—åçš„ LLM æ‰€è¯´ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥ï¼š

* **åˆå§‹å±‚å½’ä¸€åŒ–ï¼ˆSP åŒºåŸŸï¼‰**:
	* è¾“å…¥å¼ é‡ $X_1$ å’Œ $X_2$ $(b,s/2,h)$ è¿›å…¥å±‚å½’ä¸€åŒ–ï¼Œå·²åœ¨åºåˆ—ç»´åº¦ä¸Šæ‹†åˆ†
	- æ¯ä¸ª GPU ç‹¬ç«‹åœ°åœ¨å…¶åºåˆ—å—ä¸Šè®¡ç®—å±‚å½’ä¸€åŒ–ï¼Œå¹¶ç»™å‡º $Y_1$ å’Œ $Y_2$

* **ç¬¬ä¸€æ¬¡è½¬æ¢ï¼ˆSP â†’ TPï¼‰:**
	- â€œgâ€ æ“ä½œï¼ˆall-gatherï¼‰å°† $Y_1$ å’Œ $Y_2$ é‡æ–°ç»„åˆä¸ºå®Œæ•´åºåˆ—é•¿åº¦
	- æ¢å¤ $Y$ $(b,s,h)$ï¼Œå› ä¸ºåˆ—çº¿æ€§éœ€è¦å®Œæ•´çš„éšè—ç»´åº¦ $h$

* **ç¬¬ä¸€æ¬¡çº¿æ€§ï¼ˆTP åŒºåŸŸï¼‰:**
	* $A_1$ æ˜¯åˆ—çº¿æ€§çš„ï¼Œå› æ­¤å®ƒæ²¿éšè—ç»´åº¦åˆ†å‰² $Y$  
	* GeLU åœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹åº”ç”¨
	* $Z_1$ æ˜¯ $(b,s,h/2)$

* **ç¬¬äºŒæ¬¡çº¿æ€§ï¼ˆTP åŒºåŸŸï¼‰ï¼š**  
	* $B_1$ æ˜¯è¡Œçº¿æ€§çš„ï¼Œå› æ­¤å®ƒæ¢å¤éšè—ç»´åº¦  
	* $W_1$ æ˜¯ $(b,s,h)$ 

* **æœ€ç»ˆè½¬æ¢ï¼ˆTP â†’ SPï¼‰ï¼š** 
	* â€œg*â€ æ“ä½œï¼ˆreduce-scatterï¼‰ï¼Œåœ¨æ²¿åºåˆ—ç»´åº¦åˆ†æ•£çš„åŒæ—¶ç¡®ä¿å‰ä¸€è¡Œçº¿æ€§çš„æ­£ç¡®æ€§  
	* $W_1$ æ˜¯ $(b,s/2,h)$

![image.png|240](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_diagram_zoomed.png)

åºåˆ—å¹¶è¡Œæ€§çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿åœ¨äºå®ƒå‡å°äº†æˆ‘ä»¬éœ€è¦å­˜å‚¨çš„æœ€å¤§æ¿€æ´»å¤§å°ã€‚åœ¨ä»…ä½¿ç”¨å¼ é‡å¹¶è¡Œçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å¾—ä¸åœ¨ä¸åŒä½ç½®å­˜å‚¨å½¢çŠ¶ä¸º $(b,s,h)$ çš„æ¿€æ´»å€¼ã€‚ç„¶è€Œï¼Œæœ‰äº†åºåˆ—å¹¶è¡Œæ€§ï¼Œç”±äºæˆ‘ä»¬æ€»æ˜¯åœ¨åºåˆ—ç»´åº¦æˆ–éšè—ç»´åº¦ä¸Šè¿›è¡Œæ‹†åˆ†ï¼Œæœ€å¤§æ¿€æ´»å¤§å°å‡å°ä¸º $\frac{bâ‹…sâ‹…h}{tp}$ã€‚

è·Ÿè¸ªåœ¨ TP å’Œ TP/SP ä¸­ä»¥ä¸åŒæ–¹å¼è¿›è¡Œåˆ†ç‰‡çš„æ‰€æœ‰éƒ¨åˆ†æœ‰ç‚¹å›°éš¾â€”â€”ç›¸ä¿¡æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¹Ÿè§‰å¾—å¾ˆéš¾è¿›è¡Œæ˜ å°„ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ¶ä½œäº†è¿™ä¸ªå°è¡¨æ ¼æ¥æ€»ç»“åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¿€æ´»ï¼ˆå³ `hidden_states`ï¼‰çš„å½¢çŠ¶åœ¨éšè—ç»´åº¦ $h$ å’Œåºåˆ—ç»´åº¦ $s$ ä¸Šæ˜¯å¦‚ä½•å˜åŒ–çš„ï¼š

|Region|TP only|TP with SP|
|---|---|---|
|Enter TP (Column Linear)|h: sharded (weight_out is sharded)  <br>s: full|h: sharded (weight_out is sharded)  <br>s:Â **all-gather**Â to full|
|TP Region|h: sharded  <br>s: full|h: sharded  <br>s: full|
|Exit TP (Row Linear)|h: full (weight_out is full +Â **all-reduce**Â for correctness)  <br>s: full|h: full (weight_out is full +Â **reduce-scatter**Â for correctness)  <br>s:Â **reduce-scatter**Â to sharded|
|SP Region|h: full  <br>s: full|h: full  <br>s: sharded|

å¹¶ä¸”å¯¹äºåµŒå…¥å±‚ï¼š

|Region|Vanilla TP|TP with SP|
|---|---|---|
|Embedding Layer (Row Linear sharded on vocab)|h: full (weight_out is full +Â **all-reduce**Â for correctness)  <br>s: full|h: full (weight_out is full +Â **reduce-scatter**Â for correctness)  <br>s:Â **reduce-scatter**Â to sharded|

é€šè¿‡ä½¿ç”¨åºåˆ—å¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°æ›´å¤§çš„æ¿€æ´»å†…å­˜èŠ‚çœï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦æ¨å¾—æ¯”ä»…ä½¿ç”¨å¼ é‡å¹¶è¡Œæ€§æ—¶æ›´è¿œã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯¹æˆ‘ä»¬ä¹‹å‰çš„ 70B æ¨¡å‹ç¤ºä¾‹æ„å‘³ç€ä»€ä¹ˆï¼š

[äº¤äº’å›¾]

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬å†æ¬¡å¤§å¹…é™ä½äº†æ¯ä¸ª GPU çš„æœ€å¤§å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ TP/SP=16 çš„æƒ…å†µä¸‹å¤„ç† 16k tokens çš„åºåˆ—é•¿åº¦ï¼Œè¿™æ¯”æ™®é€š TP æƒ…å†µæœ‰æ‰€æ”¹è¿›ï¼ï¼ˆå¦‚å‰ä¸€èŠ‚æ‰€è¿°ï¼ŒTP=16 ä»ç„¶æœ‰ç‚¹å¤§ï¼Œä½†æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°å¦‚ä½•æ”¹è¿›è¿™ä¸€ç‚¹ï¼‰ã€‚

ä½ å¯èƒ½ä¼šé—®è‡ªå·±çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä½¿ç”¨ TP+SP æ˜¯å¦æ¯”æ™®é€š TP äº§ç”Ÿæ›´å¤šçš„é€šä¿¡é‡ï¼Ÿå—¯ï¼Œç­”æ¡ˆæ˜¯æœ‰æ—¶æ˜¯ï¼Œæœ‰æ—¶ä¸æ˜¯ã€‚åœ¨æ™®é€š TP çš„å‰å‘ä¼ æ’­ä¸­ï¼Œæ¯ä¸ª Transformer å—æœ‰ä¸¤ä¸ª all-reduce æ“ä½œï¼Œè€Œåœ¨ SP ä¸­ï¼Œæ¯ä¸ª Transformer å—æœ‰ä¸¤ä¸ª all-gather å’Œä¸¤ä¸ª reduce-scatter æ“ä½œã€‚æ‰€ä»¥ SP çš„é€šä¿¡æ“ä½œæ•°é‡æ˜¯ TP çš„ä¸¤å€ã€‚ä½†ç”±äºä¸€ä¸ª all-reduce æ“ä½œå¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ª all-gather + reduce-scatterï¼ˆè§é™„å½•ä¸­çš„â€œå¿«é€Ÿå…³æ³¨ Ring AllReduceâ€éƒ¨åˆ†ï¼‰ï¼Œå®ƒä»¬åœ¨é€šä¿¡æ–¹é¢å®é™…ä¸Šæ˜¯ç­‰æ•ˆçš„ã€‚åå‘ä¼ æ’­çš„æ¨ç†ä¹Ÿç›¸åŒï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯ä½¿ç”¨æ¯ä¸ªæ“ä½œçš„å…±è½­ï¼ˆæ— æ“ä½œ â†” allreduce å’Œ allgather â†” reducescatterï¼‰ã€‚

å¦‚æœä½ ä¸€ç›´å¯†åˆ‡å…³æ³¨ï¼Œå°±ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨è®¨è®ºæ¯å±‚çš„ 4 ä¸ªé€šä¿¡æ“ä½œï¼ˆæ³¨æ„åŠ›æœºåˆ¶çš„ 2 ä¸ªå’Œå¤šå±‚æ„ŸçŸ¥æœºçš„ 2 ä¸ªï¼‰ã€‚ä½¿ç”¨å¼ é‡+åºåˆ—å¹¶è¡Œæ—¶ï¼Œå¤šå±‚æ„ŸçŸ¥æœºçš„åˆ†ææƒ…å†µå¦‚ä¸‹ï¼š

![tp_sp_overlap.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_overlap.svg)

å°±åƒæ™®é€šçš„å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸€æ ·ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰+æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰ä¹Ÿä¸èƒ½è½»æ˜“åœ°ä¸è®¡ç®—é‡å ï¼Œè¿™ä½¿å¾—ååé‡ä¸¥é‡ä¾èµ–äºé€šä¿¡å¸¦å®½ã€‚åŒæ ·ï¼Œåœ¨è¿™é‡Œï¼Œå°±åƒæ™®é€šçš„å¼ é‡å¹¶è¡Œï¼ˆTOï¼‰ä¸€æ ·ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰+æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰é€šå¸¸ä¹Ÿåªåœ¨å•ä¸ªèŠ‚ç‚¹å†…è¿›è¡Œï¼ˆä½¿å¼ é‡å¹¶è¡Œåº¦ä¿æŒåœ¨æ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ä¹‹ä¸‹ï¼Œä¾‹å¦‚å¼ é‡å¹¶è¡Œåº¦ â‰¤ 8 ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥è¡¡é‡éšç€å¼ é‡å¹¶è¡Œæ€§æ‰©å±•ï¼Œè¿™ç§é€šä¿¡å¼€é”€å˜å¾—æ—¥ç›Šæ£˜æ‰‹çš„æƒ…å†µã€‚è®©æˆ‘ä»¬åœ¨é’ˆå¯¹åºåˆ—é•¿åº¦ä¸º 4096 çš„ 3B å‚æ•°æ¨¡å‹ï¼Œå°†å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰ä¸€èµ·æ‰©å±•æ—¶ï¼Œæµ‹é‡ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡ï¼š

[äº¤äº’å›¾]

åœ¨è¿™é‡Œï¼Œè®¡ç®—æ•ˆç‡ï¼ˆå·¦ï¼‰å’Œå†…å­˜å®¹é‡ï¼ˆå³ï¼‰ä¹‹é—´åˆå­˜åœ¨ä¸€ç§æƒè¡¡ã€‚è™½ç„¶æ›´é«˜çš„å¹¶è¡Œåº¦é€šè¿‡å‡å°‘æ¿€æ´»å†…å­˜èƒ½å¤Ÿå¤„ç†æ˜¾è‘—æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œä½†å®ƒä»¬ä¹Ÿé™ä½äº†æ¯ä¸ª GPU çš„ååé‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹åº”äºæ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡çš„é˜ˆå€¼ä»¥ä¸Šã€‚

è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„è§‚å¯Ÿç»“æœï¼š

- å¯¹äºè¿™ä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å½“ä» TP=8 åˆ‡æ¢åˆ° TP=16 æ—¶ï¼Œæ€§èƒ½ä¸‹é™æœ€ä¸ºæ˜æ˜¾ï¼Œå› ä¸ºæ­¤æ—¶æ˜¯ä»ä»…åœ¨å•ä¸ªèŠ‚ç‚¹å†…é€šä¿¡ï¼ˆNVLinkï¼‰ï¼Œè½¬å˜ä¸ºèŠ‚ç‚¹é—´é€šä¿¡ï¼ˆEFAï¼‰ã€‚
- åœ¨ä½¿ç”¨ TP ä¸ SP æ—¶ï¼Œæ¿€æ´»å†…å­˜çš„èŠ‚çœä½¿æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†æ¯”ä»…ä½¿ç”¨ TP æ—¶å¤§å¾—å¤šçš„æ‰¹æ¬¡ã€‚

*æˆ‘ä»¬å·²ç»çœ‹åˆ° TP å¦‚ä½•é€šè¿‡åœ¨éšè—ç»´åº¦ä¸Šæ‹†åˆ†æ³¨æ„åŠ›å’Œå‰é¦ˆæ“ä½œæ¥å¸®åŠ©æˆ‘ä»¬åœ¨å¤šä¸ª GPU ä¸Šåˆ†ç‰‡æ¿€æ´»ï¼Œä»¥åŠ SP å¦‚ä½•é€šè¿‡åœ¨åºåˆ—ç»´åº¦ä¸Šæ‹†åˆ†æ¥è‡ªç„¶åœ°è¡¥å……å…¶ä½™æ“ä½œã€‚*

> [!NOTE]
> ç”±äº SP åŒºåŸŸä¸­çš„å±‚å½’ä¸€åŒ–ï¼ˆLayerNormsï¼‰å¯¹åºåˆ—çš„ä¸åŒéƒ¨åˆ†è¿›è¡Œæ“ä½œï¼Œå› æ­¤å®ƒä»¬åœ¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ranks ä¸Šçš„æ¢¯åº¦ä¼šæœ‰æ‰€ä¸åŒã€‚ä¸ºäº†ç¡®ä¿æƒé‡ä¿æŒåŒæ­¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åå‘ä¼ æ’­æœŸé—´å¯¹å®ƒä»¬çš„æ¢¯åº¦è¿›è¡Œ all-reduce æ“ä½œï¼Œç±»ä¼¼äºæ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ç¡®ä¿æƒé‡ä¿æŒåŒæ­¥çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ç§è¾ƒå°çš„é€šä¿¡å¼€é”€ï¼Œå› ä¸ºå±‚å½’ä¸€åŒ–çš„å‚æ•°ç›¸å¯¹è¾ƒå°‘ã€‚

ç„¶è€Œï¼ŒTP å’Œ SP æœ‰ä¸¤ä¸ªé™åˆ¶ï¼š1ï¼‰å¦‚æœæˆ‘ä»¬æ‰©å±•åºåˆ—é•¿åº¦ï¼Œæ¿€æ´»å†…å­˜ä»ç„¶ä¼šåœ¨ TP åŒºåŸŸçˆ†ç‚¸å¼å¢é•¿ï¼›2ï¼‰å¦‚æœæ¨¡å‹å¤ªå¤§è€Œæ— æ³•é€‚åº” TP=8ï¼Œé‚£ä¹ˆç”±äºèŠ‚ç‚¹é—´è¿æ¥é—®é¢˜ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å·¨å¤§çš„å‡é€Ÿã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§æ¥è§£å†³é—®é¢˜ 1)ï¼Œç”¨æµæ°´çº¿å¹¶è¡Œæ€§æ¥è§£å†³é—®é¢˜ 2)ã€‚è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼

## äº”ã€ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆCPï¼‰

é€šè¿‡å¼ é‡å¹¶è¡Œå’Œåºåˆ—å¹¶è¡Œï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—é™ä½æ¯ä¸ª GPU çš„å†…å­˜éœ€æ±‚ï¼Œå› ä¸ºæ¨¡å‹æƒé‡å’Œæ¿€æ´»éƒ½åˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Šã€‚ç„¶è€Œï¼Œå½“åœ¨è¶Šæ¥è¶Šé•¿çš„åºåˆ—ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ï¼ˆä¾‹å¦‚ï¼Œå½“æ‰©å±•åˆ°æ¯ä¸ªåºåˆ— 128k æˆ–æ›´å¤š tokens æ—¶ï¼‰ï¼Œç”±äºåœ¨ TP åŒºåŸŸå†…æˆ‘ä»¬ä»ç„¶éœ€è¦å¤„ç†å®Œæ•´çš„åºåˆ—é•¿åº¦ï¼Œæˆ‘ä»¬å¯èƒ½ä»ç„¶ä¼šè¶…å‡ºå•ä¸ªèŠ‚ç‚¹ä¸Šçš„å¯ç”¨å†…å­˜ã€‚

æ­¤å¤–ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨å®Œå…¨é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼ˆè¿™ä¼šå¸¦æ¥çº¦ 30% çš„å·¨å¤§è®¡ç®—å¼€é”€ï¼‰ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åœ¨å±‚è¾¹ç•Œå¤„ä¿ç•™ä¸€äº›ä¸åºåˆ—é•¿åº¦çº¿æ€§ç›¸å…³çš„æ¿€æ´»å€¼åœ¨å†…å­˜ä¸­ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§å¦‚ä½•å¸®åŠ©æˆ‘ä»¬ï¼š

[äº¤äº’å›¾]

ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç±»ä¼¼çš„æ€è·¯åº”ç”¨äºåºåˆ—å¹¶è¡Œæ€§æ–¹æ³•ï¼ˆå³æ²¿åºåˆ—é•¿åº¦è¿›è¡Œæ‹†åˆ†ï¼‰ï¼Œä½†è¦åº”ç”¨äºæˆ‘ä»¬å·²ç»åº”ç”¨å¼ é‡å¹¶è¡Œæ€§çš„æ¨¡å—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ²¿ä¸¤ä¸ªç»´åº¦å¯¹è¿™äº›æ¨¡å—è¿›è¡Œæ‹†åˆ†ï¼Œä»è€Œå‡å°‘åºåˆ—é•¿åº¦çš„å½±å“ã€‚åœ¨æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„å†…å®¹ä¹‹åï¼Œä½ ä¼šå‘ç°è¿™ç§æ–¹æ³•ç›¸å½“ç›´è§‚â€¦â€¦ä½†è¿™å…¶ä¸­æœ‰ä¸ªæŠ€å·§ï¼Œæ‰€ä»¥è¦ä¿æŒæ¸…é†’ï¼

ä¸ºäº†å®ç°ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼›å°±åƒåºåˆ—å¹¶è¡Œæ€§ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ²¿ç€åºåˆ—ç»´åº¦æ‹†åˆ†è¾“å…¥ï¼Œä½†ç°åœ¨æˆ‘ä»¬å°†è¿™ç§æ‹†åˆ†åº”ç”¨äºæ•´ä¸ªæ¨¡å‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨å¼ é‡+åºåˆ—å¹¶è¡Œæ€§ä¸­æ‰€åšçš„æ¨¡å‹çš„åºåˆ—å¹¶è¡ŒåŒºåŸŸã€‚

æ‹†åˆ†åºåˆ—ä¸ä¼šå½±å“å¤§å¤šæ•°æ¨¡å—ï¼Œå¦‚ MLP å’Œ LayerNormï¼Œå…¶ä¸­æ¯ä¸ªæ ‡è®°éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ã€‚å®ƒä¹Ÿä¸éœ€è¦åƒTPé‚£æ ·è¿›è¡Œæ˜‚è´µçš„é€šä¿¡ï¼Œå› ä¸ºåªæ‹†åˆ†äº†è¾“å…¥è€Œä¸æ˜¯æƒé‡çŸ©é˜µã€‚å°±åƒæ•°æ®å¹¶è¡Œä¸€æ ·ï¼Œåœ¨è®¡ç®—æ¢¯åº¦åï¼Œä¼šå¯åŠ¨ä¸€ä¸ª all-reduce æ“ä½œæ¥åŒæ­¥ä¸Šä¸‹æ–‡å¹¶è¡Œç»„ä¸­çš„æ¢¯åº¦ã€‚

ä¸è¿‡æœ‰ä¸€ä¸ªé‡è¦çš„ä¾‹å¤–æƒ…å†µï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦ç‰¹åˆ«å…³æ³¨æ³¨æ„åŠ›å—ï¼ˆå“ˆå“ˆï¼ŒåŒå…³è¯­ğŸ˜€ï¼‰ã€‚åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œæ¯ä¸ªæ ‡è®°éƒ½éœ€è¦è®¿é—®æ‰€æœ‰å…¶ä»–åºåˆ—æ ‡è®°çš„é”®/å€¼å¯¹ï¼›åœ¨å› æœæ³¨æ„åŠ›çš„æƒ…å†µä¸‹ï¼Œè‡³å°‘è¦å¯¹æ¯ä¸ªå‰é¢çš„æ ‡è®°äºˆä»¥å…³æ³¨ã€‚

ç”±äºä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ä¼šæ²¿åºåˆ—ç»´åº¦å°†è¾“å…¥æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œå› æ­¤æ³¨æ„åŠ›æ¨¡å—å°†éœ€è¦åœ¨ GPU ä¹‹é—´è¿›è¡Œå®Œæ•´çš„é€šä¿¡ï¼Œä»¥äº¤æ¢å¿…è¦çš„é”®/å€¼æ•°æ®ã€‚

â€œå¦‚æœæˆ‘ä»¬å¤©çœŸåœ°å»åšè¿™ä»¶äº‹ï¼Œé‚£å¬èµ·æ¥æˆæœ¬éå¸¸é«˜ã€‚æœ‰æ²¡æœ‰ä¸€ç§æ—¢èƒ½é«˜æ•ˆåˆèƒ½å¿«é€Ÿå®Œæˆçš„æ–¹æ³•å‘¢ï¼å€¼å¾—åº†å¹¸çš„æ˜¯ï¼Œæœ‰è¿™æ ·çš„æ–¹æ³•ï¼šä¸€ç§ç”¨äºé«˜æ•ˆå¤„ç†é”®å€¼å¯¹é€šä¿¡çš„æ ¸å¿ƒæŠ€æœ¯å«åš*ç¯å½¢æ³¨æ„åŠ›ï¼ˆRing Attentionï¼‰*ã€‚â€

> [!NOTE]
> ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ä¸ Flash Attentionï¼ˆç¨åè¯¦è¿°ï¼‰åœ¨æ¦‚å¿µä¸Šæœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„â€”â€”è¿™ä¸¤ç§æŠ€æœ¯éƒ½ä¾èµ–äºåœ¨çº¿ softmax è®¡ç®—æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è™½ç„¶ Flash Attention ä¸“æ³¨äºä¼˜åŒ–å•ä¸ª GPU ä¸Šçš„æ³¨æ„åŠ›è®¡ç®—æœ¬èº«ï¼Œä½†ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§é€šè¿‡å°†åºåˆ—åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šæ¥å®ç°å†…å­˜å‡å°‘ã€‚


### 5.1 å‘ç°ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶

åœ¨è¿™ç§æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ä¸­ï¼Œæ¯ä¸ª GPU é¦–å…ˆå‘èµ·ä¸€ä¸ªå¼‚æ­¥é€šä¿¡æ“ä½œï¼Œå°†å…¶é”®/å€¼å¯¹å‘é€åˆ°å…¶ä»– GPUã€‚åœ¨ç­‰å¾…å…¶ä»– GPU çš„æ•°æ®æ—¶ï¼Œå®ƒä¼šè®¡ç®—å…¶å†…å­˜ä¸­å·²æœ‰æ•°æ®éƒ¨åˆ†çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨æœ¬æ¬¡è®¡ç®—å®Œæˆä¹‹å‰ï¼Œä¼šä»å¦ä¸€ä¸ª GPU æ¥æ”¶åˆ°ä¸‹ä¸€ä¸ªé”®/å€¼å¯¹ï¼Œä»è€Œä½¿ GPU åœ¨å®Œæˆç¬¬ä¸€è½®è®¡ç®—åèƒ½å¤Ÿç«‹å³å¼€å§‹ä¸‹ä¸€è½®è®¡ç®—ã€‚

è®©æˆ‘ä»¬æ¥è¯´æ˜ä¸€ä¸‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ 4 ä¸ª GPU å’Œ 4 ä¸ªè¾“å…¥æ ‡è®°ã€‚æœ€åˆï¼Œè¾“å…¥åºåˆ—ä¼šæ²¿ç€åºåˆ—ç»´åº¦å‡åŒ€æ‹†åˆ†ï¼Œå› æ­¤æ¯ä¸ª GPU å°†ä»…æœ‰ä¸€ä¸ªæ ‡è®°ä»¥åŠå…¶å¯¹åº”çš„ Q/K/V å€¼ã€‚å‡è®¾ Q1ã€K1 å’Œ V1 åˆ†åˆ«ä»£è¡¨ä½äºç¬¬ 1 ä¸ª GPU ä¸Šçš„ç¬¬ä¸€ä¸ªæ ‡è®°çš„æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ³¨æ„åŠ›è®¡ç®—å°†éœ€è¦ 4 ä¸ªæ—¶é—´æ­¥æ¥å®Œæˆã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ª GPU æ‰§è¡Œè¿™ä¸‰ä¸ªè¿ç»­çš„æ“ä½œï¼š

1. ä»¥éé˜»å¡æ–¹å¼å°†â€œå½“å‰çš„é”®å’Œå€¼â€å‘é€åˆ°ä¸‹ä¸€å°æœºå™¨ï¼Œä½†åœ¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥é™¤å¤–ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨è¿™ä¸€æ­¥å®Œæˆä¹‹å‰å¼€å§‹ä¸‹ä¸€æ­¥ã€‚
2. åœ¨æœ¬åœ°è®¡ç®—å…¶å·²æœ‰çš„â€œå½“å‰çš„é”®å’Œå€¼â€ä¸Šçš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™é€šå¸¸æ¶‰åŠæ‰§è¡Œ $\text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \times V$ã€‚
3. ç­‰å¾…æ¥æ”¶æ¥è‡ªå‰ä¸€ä¸ª GPU çš„é”®å’Œå€¼ï¼Œç„¶åå¾ªç¯å›åˆ°æ­¥éª¤ 1ã€‚æ­¤æ—¶â€œå½“å‰çš„é”®å’Œå€¼â€ç°åœ¨æ˜¯åˆšåˆšä»å‰ä¸€ä¸ª GPU æ¥æ”¶åˆ°çš„é”®/å€¼ã€‚

æˆ‘ä»¬æ‰§è¡Œè¿™ 3 ä¸ªæ­¥éª¤å››æ¬¡ä»¥å®Œæˆæ³¨æ„åŠ›è®¡ç®—ã€‚

ä»¥ä¸‹åŠ¨ç”»å±•ç¤ºäº†ä½¿ç”¨ 4 ä¸ª GPU çš„æ•´ä¸ªè¿‡ç¨‹ï¼š
![ring-attention.gif|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ring-attention.gif)
åœ¨è¿™ä¸ªåŠ¨ç”»ä¸­ï¼Œä½œè€…é€‰æ‹©å°†è¿™ç§æ–¹æ³•ç§°ä¸ºç¯å½¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆRing Attentionï¼‰ï¼Œè¿™ä¸€ç‚¹å¯¹æ‚¨æ¥è¯´å¯èƒ½æ˜¾è€Œæ˜“è§ã€‚

ä¸è¿‡æœ‰ä¸€ä¸ªå¤§é—®é¢˜æ˜¯ï¼Œç¯å½¢æ³¨æ„åŠ›ï¼ˆRing Attentionï¼‰çš„ç®€å•å®ç°ä¼šç”±äºå› æœæ³¨æ„åŠ›çŸ©é˜µçš„å½¢çŠ¶å¯¼è‡´ GPU ä¹‹é—´å‡ºç°ä¸¥é‡çš„ä¸å¹³è¡¡ã€‚è®©æˆ‘ä»¬é€šè¿‡è€ƒè™‘å¸¦æœ‰å› æœæ³¨æ„åŠ›æ©ç çš„æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µæ¥çœ‹çœ‹ SoftMax è®¡ç®—ã€‚
![cp_attnmask.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_attnmask.svg)

SoftMax æ˜¯æŒ‰è¡Œè®¡ç®—çš„ï¼Œè¿™æ„å‘³ç€åªè¦ GPU æ¥æ”¶åˆ°ä¸€è¡Œçš„æ‰€æœ‰ tokensï¼Œå°±å¯ä»¥è¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬çœ‹åˆ° GPU1 å¯ä»¥ç«‹å³è®¡ç®—å®ƒï¼Œå› ä¸ºå®ƒä» token 1 - 4å¼€å§‹ï¼Œå¹¶ä¸” GPU1 å®é™…ä¸Šä¸éœ€è¦ä»ä»»ä½•å…¶ä»– GPU æ¥æ”¶ä»»ä½•ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒGPU2 å°†éœ€è¦ç­‰å¾…ç¬¬äºŒè½®æ‰èƒ½ä¹Ÿæ¥æ”¶åˆ° 1-4ï¼Œä»è€Œæ‹¥æœ‰ tokens 1-8 çš„æ‰€æœ‰å€¼ã€‚æ­¤å¤–ï¼ŒGPU1 ä¼¼ä¹æ¯”æ‰€æœ‰å…¶ä»– GPU æ‰§è¡Œçš„å·¥ä½œé‡è¦å°‘å¾—å¤šã€‚

è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦æ›´å¥½åœ°å¹³è¡¡æˆ‘ä»¬çš„è®¡ç®—ã€‚

### 5.2 é”¯é½¿ï¼ˆä¹‹å­—å½¢ï¼‰ç¯æ³¨æ„åŠ›â€”â€”ä¸€ç§å¹³è¡¡çš„è®¡ç®—å®ç°

æˆ‘ä»¬éœ€è¦ä¸€ç§æ›´å¥½çš„æ–¹æ³•æ¥åˆ†é…è¾“å…¥åºåˆ—ã€‚è¿™å¯ä»¥é€šè¿‡ä¸çº¯ç²¹æŒ‰é¡ºåºå°† tokens åˆ†é…ç»™ GPUï¼Œè€Œæ˜¯ç¨å¾®æ··åˆä¸€ä¸‹é¡ºåºæ¥å®ç°ï¼Œè¿™æ ·æ¯ä¸ª GPU ä¸Šéƒ½æœ‰æ—©æœŸå’Œæ™šæœŸ tokens çš„è‰¯å¥½æ··åˆã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºé”¯é½¿å½¢ï¼ˆZ å­—å½¢ï¼‰æ³¨æ„åŠ›ï¼Œåœ¨è¿™ç§æ–°çš„æ’åˆ—ä¸­ï¼Œæ³¨æ„åŠ›æ©ç å°†æ˜¾ç¤ºè®¡ç®—é‡çš„å‡åŒ€åˆ†å¸ƒï¼Œä½†å¦‚æœä½ æ•°ä¸€æ•°æœ‰é¢œè‰²çš„æ–¹å—æ•°é‡ï¼Œå°±ä¼šå‘ç°è®¡ç®—é‡ç°åœ¨åœ¨æ‰€æœ‰ GPU ä¹‹é—´æ˜¯å¹³è¡¡çš„ã€‚

![cp_zigzagmask.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_zigzagmask.svg)

ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä¼šçœ‹åˆ°ï¼Œä¸ºäº†å®Œæˆæ‰€æœ‰è¡Œï¼Œæ¯ä¸ª GPU éƒ½éœ€è¦æ¥è‡ªå…¶ä»–æ‰€æœ‰ GPU çš„ä¿¡æ¯ã€‚

æˆ‘ä»¬æœ‰ä¸¤ç§é€šç”¨çš„æ–¹æ³•æ¥é‡å è®¡ç®—å’Œé€šä¿¡ï¼Œè¦ä¹ˆé€šè¿‡æ‰§è¡Œå¸¸è§„çš„ all-gather æ“ä½œï¼ŒåŒæ—¶åœ¨æ¯ä¸ª GPU ä¸Šé‡æ–°åˆ†ç»„æ‰€æœ‰çš„é”®å€¼å¯¹ï¼ˆä»¥ Zero-3 ç±»å‹çš„æ–¹å¼ï¼‰ï¼Œè¦ä¹ˆæ ¹æ®éœ€è¦ä»æ¯ä¸ª GPU é€ä¸ªæ”¶é›†åˆ°å…¶ä»– GPUã€‚

![cp_overlap_allgather.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_overlap_allgather.svg)

![cp_overlap_all2all.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_overlap_all2all.svg)

è¿™ä¸¤ç§å®ç°æ–¹å¼çš„å…³é”®åŒºåˆ«åœ¨äºå®ƒä»¬çš„é€šä¿¡æ¨¡å¼å’Œå†…å­˜ä½¿ç”¨æƒ…å†µï¼š

1. **AllGatherå®ç°**ï¼š
    - æ‰€æœ‰ GPU åŒæ—¶ä»æ‰€æœ‰å…¶ä»– GPU æ”¶é›†å®Œæ•´çš„é”®/å€¼å¯¹
    - ç”±äºæ¯ä¸ª GPU éœ€è¦ä¸€æ¬¡æ€§å­˜å‚¨å®Œæ•´çš„ KV å¯¹ï¼Œå› æ­¤éœ€è¦æ›´å¤šçš„ä¸´æ—¶å†…å­˜
    - é€šä¿¡åœ¨ä¸€æ­¥å†…å®Œæˆï¼Œä½†å…·æœ‰æ›´å¤§çš„å†…å­˜å¼€é”€
2. **å…¨äº’è”ï¼ˆç¯å½¢ï¼‰å®ç°**ï¼š
    - GPU ä»¥ç¯å½¢æ¨¡å¼ä¸€æ¬¡äº¤æ¢ä¸€ä¸ªæ•°æ®å—çš„ KV å¯¹
    - æ›´èŠ‚çœå†…å­˜ï¼Œå› ä¸ºæ¯ä¸ª GPU åªéœ€è¦ä¸´æ—¶å­˜å‚¨ä¸€ä¸ªé¢å¤–çš„æ•°æ®å—
    - é€šä¿¡åˆ†æ•£å¹¶ä¸è®¡ç®—é‡å ï¼Œå°½ç®¡ç”±äºå¤šä¸ªé€šä¿¡æ­¥éª¤è€Œæœ‰ä¸€äº›é¢å¤–çš„åŸºæœ¬å»¶è¿Ÿå¼€é”€

å…¨äº’è”ï¼ˆAll-to-Allï¼‰æ–¹æ³•é€šå¸¸ä»¥ç¨å¾®å¤æ‚çš„é€šä¿¡æ¨¡å¼ä¸ºä»£ä»·æä¾›æ›´å¥½çš„å†…å­˜æ•ˆç‡ï¼Œè€Œå…¨æ”¶é›†ï¼ˆAllGatherï¼‰æ–¹æ³•æ›´ç®€å•ï¼Œä½†åœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´éœ€è¦æ›´å¤šçš„ä¸´æ—¶å†…å­˜ã€‚

æˆ‘ä»¬ç°åœ¨çœ‹åˆ°äº†å¦‚ä½•é€šè¿‡åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æ¥æ‹†åˆ†æ¨¡å‹ä»¥åº”å¯¹å¤§å‹æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åºåˆ—å¹¶è¡Œï¼ˆCPï¼‰æ¥åº”å¯¹é•¿åºåˆ—ä¸­çš„æ¿€æ´»çˆ†ç‚¸é—®é¢˜ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶çŸ¥é“TP åœ¨è·¨èŠ‚ç‚¹æ‰©å±•æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¦‚æœæ¨¡å‹æƒé‡ä¸å®¹æ˜“æ”¾åœ¨ 1 ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå¦ä¸€ç§å¹¶è¡Œåº¦ï¼Œæˆ‘ä»¬çš„ç¬¬å››ç§ï¼Œ*å¹¶è¡Œæµæ°´çº¿ï¼ˆPipeline Parallelismï¼‰* æ¥æ‹¯æ•‘äº†ï¼

## å…­ã€æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰

åœ¨â€œå¼ é‡å¹¶è¡Œâ€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œå°è¯•å°†å¼ é‡å¹¶è¡Œæ‰©å±•åˆ°å•ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡ï¼ˆé€šå¸¸ä¸º 4 æˆ– 8ï¼‰ä»¥ä¸Šæ—¶ï¼Œä¼šé‡åˆ°ä¸€ç§å¸¦å®½è¾ƒä½çš„ç½‘ç»œï¼Œç§°ä¸ºâ€œèŠ‚ç‚¹é—´è¿æ¥â€ï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡æŸå®³æˆ‘ä»¬çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨è·¨å¤šä¸ªèŠ‚ç‚¹ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æœ‰ 8 ä¸ª GPUï¼‰çš„é›†ç¾¤ä¸Šå¯¹å…¶è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œæ¯”å¦‚ all-reduce æ“ä½œã€‚

[äº¤äº’å›¾]

ä¸åŒèŠ‚ç‚¹æ•°é‡ä¸‹èŠ‚ç‚¹é—´é€šä¿¡å¸¦å®½çš„æµ‹é‡ç»“æœï¼Œå±•ç¤ºäº†AllReduceã€AllGather å’Œ ReduceScatter æ“ä½œçš„ä¸­ä½æ•°ï¼ˆçº¿æ¡ï¼‰ä»¥åŠç¬¬ 5 è‡³ç¬¬ 95 ç™¾åˆ†ä½èŒƒå›´ï¼ˆé˜´å½±åŒºåŸŸï¼‰ã€‚

åºåˆ—å¹¶è¡Œå’Œä¸Šä¸‹æ–‡å¹¶è¡Œå¯¹äºé•¿åºåˆ—å¯èƒ½æœ‰å¸®åŠ©ï¼Œä½†å¦‚æœåºåˆ—é•¿åº¦å¹¶éæˆ‘ä»¬å†…å­˜é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œè€Œæ˜¯æ¨¡å‹æœ¬èº«çš„å¤§å°å¯¼è‡´çš„ï¼Œé‚£ä¹ˆè¿™ä¸¤ç§å¹¶è¡Œæ–¹å¼ä½œç”¨å°±ä¸å¤§äº†ã€‚å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆ70B å‚æ•°åŠä»¥ä¸Šï¼‰ï¼Œä»…æƒé‡çš„å¤§å°å°±å¯èƒ½è¶…è¿‡å•ä¸ªèŠ‚ç‚¹ä¸Š 4 åˆ° 8 å— GPU çš„æé™ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥ç¬¬å››ä¸ªï¼ˆä¹Ÿæ˜¯æœ€åä¸€ä¸ªï¼‰å¹¶è¡Œç»´åº¦â€”â€”â€œæµæ°´çº¿å¹¶è¡Œâ€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ ã€‚

æµæ°´çº¿å¹¶è¡Œæ˜¯ä¸€ç§ç®€å•ä½†å¼ºå¤§çš„æŠ€æœ¯ â€”â€” æˆ‘ä»¬å°†æ¨¡å‹çš„å±‚åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šï¼ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ 8 ä¸ª GPUï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¬¬ 1-4 å±‚æ”¾åœ¨ GPU 1 ä¸Šï¼Œç¬¬ 5-8 å±‚æ”¾åœ¨ GPU 2 ä¸Šï¼Œä¾æ­¤ç±»æ¨ã€‚è¿™æ ·ï¼Œæ¯ä¸ª GPU åªéœ€è¦å­˜å‚¨å’Œå¤„ç†æ¨¡å‹å±‚çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¾è‘—å‡å°‘äº†æ¯ä¸ª GPU çš„å†…å­˜éœ€æ±‚ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æµæ°´çº¿å¹¶è¡Œåœ¨ 8B æ¨¡å‹çš„å†…å­˜ä½¿ç”¨ä¸Šçš„æ•ˆæœï¼š

ï¼ˆè¿™ç§æŠ€æœ¯å¯èƒ½ä¼šè®©ä½ æƒ³èµ·æˆ‘ä»¬åœ¨è®¨è®º ZeRO-3 æ—¶çš„æƒ…å†µï¼Œå½“æ—¶æˆ‘ä»¬å°†æ¨¡å‹å‚æ•°åˆ†å‰²åˆ°å¤šä¸ª GPU ä¸Šã€‚åœ¨åé¢çš„ â€œ5D å¹¶è¡Œç®€ä»‹â€ éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šè¯¦ç»†å¯¹æ¯”è¿™ä¸¤ç§æŠ€æœ¯ã€‚ï¼‰

[äº¤äº’å›¾]

è§‚å¯Ÿä¸Šå›¾ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šè™½ç„¶æ¨¡å‹å‚æ•°åœ¨å„ä¸ª GPU ä¸Šåˆ†é…å¾—å¾ˆå¥½ï¼Œä½†æ¯ä¸ª GPU ä¸Šçš„æ¿€æ´»å†…å­˜å´ä¿æŒä¸å˜ï¼è¿™æ˜¯å› ä¸ºæ¯ä¸ª GPU ä»ç„¶éœ€è¦å¤„ç†å®Œæ•´çš„æ•°æ®æ‰¹æ¬¡ï¼Œåªæ˜¯å¤„ç†ä¸åŒçš„å±‚ã€‚ä¸€ä¸ª GPU å±‚çš„æ¿€æ´»å°†è¢«å‘é€åˆ°ä¸‹ä¸€ä¸ª GPU ä»¥ç»§ç»­å‰å‘ä¼ æ’­ã€‚

è¿™å¼•å…¥äº†ä¸€ç§æ–°çš„é€šä¿¡æ¨¡å¼ï¼šæˆ‘ä»¬ç°åœ¨ä¸æ˜¯åƒåœ¨æ•°æ®å¹¶è¡Œä¸­ä½¿ç”¨ ZeRO-3 é‚£æ ·é€šä¿¡å‚æ•°ï¼Œè€Œæ˜¯åœ¨ GPU ä¹‹é—´æŒ‰é¡ºåºâ€œæµæ°´çº¿å¼â€ä¼ é€’æ¿€æ´»å¼ é‡ã€‚è™½ç„¶æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†é«˜æ•ˆå®ç°è¿™ç§æŠ€æœ¯ç›¸å½“æ£˜æ‰‹ã€‚è®©æˆ‘ä»¬ç›´æ¥æ·±å…¥ç»†èŠ‚ï¼

### 6.1 åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šæ‹†åˆ†å±‚ - å…¨éƒ¨å‰å‘ï¼Œå…¨éƒ¨åå‘

æ‰€ä»¥ï¼Œå‡è®¾æˆ‘ä»¬åªæ˜¯å°†å„å±‚åˆ†å¸ƒåˆ°å‡ ä¸ªè®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ª GPU å°†å¤„ç†å‰å‡ å±‚ï¼Œç¬¬äºŒä¸ª GPU å°†å¤„ç†æ¨¡å‹çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œä¾æ­¤ç±»æ¨ã€‚ç°åœ¨ï¼Œé€šè¿‡æˆ‘ä»¬æ¨¡å‹çš„å‰å‘ä¼ æ’­ç®€å•åœ°æ¶‰åŠæŒ‰é¡ºåºå°†æ•°æ®æ‰¹æ¬¡æ²¿æ¨¡å‹ä¼ é€’ï¼Œä»è€Œè¿ç»­ä½¿ç”¨æ¯ä¸ªè®¡ç®—è®¾å¤‡ã€‚

æˆ‘ä»¬æœ‰ä¸€ä¸ªç›´æ¥çš„é¦–è¦ä¼˜åŠ¿ï¼šæ‰€éœ€çš„äº’è¿å¸¦å®½ä¿æŒç›¸å½“ä½ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ¨¡å‹æ·±åº¦çš„å°‘æ•°ä½ç½®ä»…å‘é€ä¸­ç­‰å¤§å°çš„æ¿€æ´»ã€‚ä¸ä¾‹å¦‚å¼ é‡å¹¶è¡Œä¸­çš„é€šä¿¡ç›¸æ¯”ï¼Œè¿™å¯ä»¥äº§ç”Ÿå·¨å¤§å·®å¼‚ï¼Œåè€…åœ¨æ¯å±‚å†…ä¼šå‘ç”Ÿæ•°æ¬¡ã€‚

ä½†ä¹Ÿè®¸ä½ å¼€å§‹æ„Ÿè§‰åˆ°ä¸€äº›å³å°†åˆ°æ¥çš„éº»çƒ¦ï¼šÂ **â€œsequentiallyâ€** å’Œ **â€œsuccessivelyâ€**ï¼Ÿï¼ï¼Ÿåœ¨å¹¶è¡Œè®¡ç®—çš„ä¸–ç•Œé‡Œï¼Œè¿™å¬èµ·æ¥å¹¶ä¸é«˜æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ‘ä»¬è®¨è®ºäº†è®¡ç®—å’Œé€šä¿¡é‡å ä¹‹åã€‚

ç¡®å®ï¼Œè¯»è€…æœ‹å‹ä»¬ï¼æµæ°´çº¿å¹¶è¡Œä¸­çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°è§„é¿æµæ°´çº¿å¹¶è¡Œçš„é¡ºåºæ€§ï¼Œä½¿æˆ‘ä»¬çš„ GPU å§‹ç»ˆä¿æŒå¿™ç¢ŒçŠ¶æ€ï¼Œé¿å…å‡ºç°ä¸€ä¸ª GPU åœ¨è®¡ç®—è€Œå…¶ä»– GPU åœ¨ç­‰å¾…çš„æƒ…å†µã€‚ä¸‹é¢å±•ç¤ºçš„æ˜¯æˆ‘ä»¬åœ¨å¯¹æ¨¡å‹è¿›è¡Œç®€å•ç›´æ¥çš„å‘å‰å’Œå‘åä¼ æ’­æ—¶ GPU çš„åˆ©ç”¨ç‡æƒ…å†µï¼ˆæ­¤å¤„æ•°å­—è¡¨ç¤ºæ¨¡å‹çš„å„å±‚ï¼‰ï¼š

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_afab.svg)

*ä¸€ä¸ªåœ¨ 4 ä¸ª GPU ä¸Šåˆ†å¸ƒæœ‰ 16 å±‚çš„æ¨¡å‹çš„æµæ°´çº¿å¹¶è¡Œç¤ºä¾‹ã€‚æ•°å­—å¯¹åº”å±‚ IDã€‚*

å‰©ä½™çš„ç©ºé—²æ—¶é—´ä»¥ç°è‰²è¡¨ç¤ºï¼Œé€šå¸¸ç§°ä¸ºâ€œæ°”æ³¡â€ï¼Œåœ¨èŠ±è´¹äº†è¿™ä¹ˆå¤šæ—¶é—´ä¼˜åŒ–ååé‡ä¹‹åï¼Œçœ‹åˆ°è¿™ä¸ªå¯èƒ½ä¼šè®©ä½ å¿ƒç¢ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿå› â€œæ°”æ³¡â€è€ŒæŸå¤±çš„æ—¶é—´æ¥é‡åŒ–æµæ°´çº¿è®¾ç½®çš„æ•ˆç‡ã€‚è®¾ $t_f$ å’Œ $t_b$ åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ä¼ æ’­çš„æ—¶é—´ï¼ˆé’ˆå¯¹ä¸€ä¸ªå¾®æ‰¹æ¬¡å’Œæµæ°´çº¿çš„ä¸€ä¸ªé˜¶æ®µæµ‹é‡ï¼‰ã€‚ä¸€ä¸ªç®€å•çš„å‡è®¾æ˜¯ $t_b \approx 2 \times t_f$ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå®Œç¾å¹¶è¡ŒåŒ–ï¼Œç†æƒ³çš„æ€»æ—¶é—´å°†æ˜¯ $t_{\text{id}} = t_f + t_b$ã€‚ç„¶è€Œï¼Œç”±äºæµæ°´çº¿æ°”æ³¡ï¼Œé¢å¤–çš„æ—¶é—´ä¸º $t_{\text{pb}} = (p-1) \times (t_f + t_b)$ï¼Œå…¶ä¸­ $p$ æ˜¯æµæ°´çº¿å¹¶è¡Œåº¦ï¼Œå³ä¸Šå›¾ä¸­çš„ GPU æ•°é‡ã€‚è¿™è¡¨ç¤ºæ¯ä¸ª GPU åœ¨å…¶ä»– GPU è®¡ç®—æ—¶çš„ç­‰å¾…æ—¶é—´ã€‚

æˆ‘ä»¬å¯ä»¥è®¡ç®—é¢å¤–æ°”æ³¡æ—¶é—´ç›¸å¯¹äºç†æƒ³æ—¶é—´çš„æ¯”ç‡ï¼š$$r_{\text{bubble}} = \frac{(p - 1) \times (t_f + t_b)}{t_f + t_b} = p - 1$$
éšç€æˆ‘ä»¬å¢åŠ æ›´å¤šé˜¶æ®µï¼Œæ°”æ³¡æ—¶é—´å› æ­¤å¢åŠ ï¼Œåˆ©ç”¨ç‡ä¸‹é™ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œåœ¨ç®€å•å®ç°ä¸­ï¼Œæ°”æ³¡å¯èƒ½ä¼šéå¸¸å¤§ï¼

å€¼å¾—åº†å¹¸çš„æ˜¯ï¼Œäººä»¬å·²ç»è®¾è®¡å‡ºäº†å„ç§æµæ°´çº¿å¹¶è¡Œæ–¹æ¡ˆæ¥*å‡å°æ°”æ³¡è§„æ¨¡*ã€‚

è®©æˆ‘ä»¬ä»å·¥å…·ç®±ä¸­å–å‡ºç¬¬ä¸€ä¸ªå·¥å…·ï¼Œæ€è€ƒå°†æˆ‘ä»¬çš„æ‰¹æ¬¡åˆ†å‰²æˆæ›´å°çš„ã€å¯ä»¥å¹¶è¡Œæˆ–å‡ ä¹å¹¶è¡Œå¤„ç†çš„å°å—ï¼ˆéƒ¨åˆ†ï¼‰ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰åœ¨æ•°æ®å¹¶è¡Œä¸­æ‰€åšçš„é‚£æ ·ã€‚ç°åœ¨ï¼Œå½“ç¬¬äºŒä¸ª GPU å¿™äºå¤„ç†å¾®æ‰¹æ¬¡1æ—¶ï¼Œç¬¬ä¸€ä¸ª GPU å·²ç»å¯ä»¥å¼€å§‹å¤„ç†å¾®æ‰¹æ¬¡ 2 äº†ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ 8 ä¸ªå¾®æ‰¹æ¬¡çš„è°ƒåº¦å®‰æ’ï¼š

![pp_afab2.svg|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_afab2.svg)

ï¼ˆåœ¨ä¹‹å‰çš„å›¾è¡¨ä¸­ï¼Œæ•°å­—è¡¨ç¤ºçš„æ˜¯å±‚ï¼Œä½†ä»ç°åœ¨èµ·ï¼ˆåŒ…æ‹¬æœ¬å›¾ï¼‰çš„æ‰€æœ‰æµæ°´çº¿å¹¶è¡Œå›¾ä¸­ï¼Œæ•°å­—è¡¨ç¤ºçš„æ˜¯å¾®æ‰¹æ¬¡ã€‚ä½ å¯ä»¥å°†è¿™é‡Œçš„æ¯ä¸ªæ–¹æ ¼çœ‹ä½œåŒ…å«è‹¥å¹²å±‚ï¼Œå°±åƒå‰ä¸€å¹…å›¾ä¸­æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚ï¼‰

ä¸Šè¿°è°ƒåº¦æ–¹å¼è¢«ç§°ä¸º***å…¨å‰å‘å…¨åå‘ï¼ˆAFABï¼‰*** è°ƒåº¦ï¼Œå› ä¸ºæˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ‰€æœ‰å‰å‘ä¼ æ’­ï¼Œç„¶åå†åªè¿›è¡Œæ‰€æœ‰åå‘ä¼ æ’­ã€‚å…¶ä¼˜åŠ¿åœ¨äºå‰å‘æ­¥éª¤å’Œåå‘æ­¥éª¤æ€»ä½“ä¸Šä»ç„¶æ˜¯é¡ºåºæ‰§è¡Œçš„ï¼Œå› æ­¤æˆ‘ä»¬ä¿ç•™äº†æ¨¡å‹è®­ç»ƒä»£ç çš„ä¸€èˆ¬ç»„ç»‡ç»“æ„ã€‚è¿™ä½¿å¾—è¿™ç§æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰å®ç°æˆä¸ºæœ€å®¹æ˜“å®ç°çš„æ–¹å¼ä¹‹ä¸€ ã€‚

ä½ å¯ä»¥åœ¨ picotron ä¸­æ‰¾åˆ° AFAB pipeline çš„å®Œæ•´å®ç°ã€‚

ğŸ‘‰ AFAB PP åœ¨ Picotron ä¸­çš„ PP å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
def train_step_pipeline_afab(model, data_loader, tensor_shapes, device, dtype):
    logging_loss: torch.float32 = 0.0
    input_tensors, output_tensors = [], []
    requires_grad_sync = pgm.process_group_manager.cp_dp_world_size > 1

    for _ in range(data_loader.grad_acc_steps): # All forward passes
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
        batch = next(data_loader)
        batch["hidden_states"] = input_tensor.to(device) if input_tensor is not None else input_tensor
        output_tensor = model.forward(input_ids=batch["input_ids"].to(device), position_ids=batch["position_ids"].to(device), hidden_states=batch["hidden_states"])
        pipeline_communicate(operation='send_forward', tensor=output_tensor, device=device, dtype=dtype)
        
        # calculate loss on the last stage
        if pgm.process_group_manager.pp_is_last_stage:
            output_tensor = F.cross_entropy(output_tensor.transpose(1, 2), batch["target_ids"].to(device), reduction='mean')
            logging_loss += output_tensor.item() / data_loader.grad_acc_steps

        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)

    for ith_microbatch in range(data_loader.grad_acc_steps): # All backward passes
        if requires_grad_sync:
            is_last_iteration = (ith_microbatch == data_loader.grad_acc_steps - 1)
            model.require_backward_grad_sync = is_last_iteration
        output_tensor_grad = pipeline_communicate(operation='recv_backward', shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)

    return logging_loss
```

è®©æˆ‘ä»¬åœ¨è¿™ä¸ªä¾‹å­ä¸­ä¼°ç®—ä¸€ä¸‹æ°”æ³¡ã€‚ä¸æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¾‹å­ä¸åŒçš„æ˜¯ï¼Œç°åœ¨å¤„ç† $m$ ä¸ªå°æ‰¹é‡çš„ç†æƒ³æ—¶é—´æ˜¯ $t_{id}=m \times (t_f+t_b)$ï¼š$$r_{\text{bubble}} = \frac{(p - 1) \times (t_f + t_b)}{m \times (t_f + t_b)} = \frac{p - 1}{m}$$
æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œé€šè¿‡å¢åŠ æ›´å¤šçš„å¾®æ‰¹æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ°”æ³¡çš„å¤§å°ç¼©å° $m$ å€ï¼Œä»è€Œè§£å†³æµæ°´çº¿é˜¶æ®µçš„ä¸€äº›ä½æ•ˆç‡é—®é¢˜ã€‚

ç„¶è€Œï¼Œä¸æ°”æ³¡åŒæ ·çƒ¦äººçš„æ˜¯å­˜å‚¨æ‰€æœ‰æ¿€æ´»æ‰€éœ€çš„å­˜å‚¨ç©ºé—´ã€‚åœ¨è¾¾åˆ°åå‘é˜¶æ®µä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ¿€æ´»ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¼è‡´è¿™äº› PP å®ç°ä¸­çš„å†…å­˜è¿…é€Ÿçˆ†ç‚¸ã€‚æˆ‘ä»¬èƒ½åšå¾—æ›´å¥½ï¼Œé¿å…è¿™ç§å†…å­˜çˆ†ç‚¸å—ï¼Ÿ

ç”±äºå†…å­˜çˆ†ç‚¸æ˜¯ç”±æˆ‘ä»¬ä¸ºåå‘ä¼ æ’­å­˜å‚¨çš„æ¿€æ´»è§¦å‘çš„ï¼Œè®©æˆ‘ä»¬å°è¯•çœ‹çœ‹æ˜¯å¦å¯ä»¥åœ¨æˆ‘ä»¬ä»åœ¨è¿›è¡Œè®¡ç®—çš„å‰å‘éƒ¨åˆ†æ—¶å°±å¼€å§‹æ‰§è¡Œåå‘ä¼ æ’­ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°½å¿«ä¸¢å¼ƒä¸€äº›ç”¨äºåå‘ä¼ æ’­æ‰€éœ€çš„æ¿€æ´»ã€‚

### 6.2 â€œå‘å‰ä¸€æ­¥-å‘åä¸€æ­¥â€åŠ LLama 3.1 æ–¹æ¡ˆ

è¿™ç§è°ƒåº¦è¢«ç§°ä¸º*ä¸€å‰ä¸€åï¼ˆ1F1Bï¼‰*ï¼Œå› ä¸ºä¸­é—´/ç¨³å®šçŠ¶æ€æ¶‰åŠäº¤æ›¿è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­å’Œä¸€æ¬¡åå‘ä¼ æ’­ã€‚å…¶æ€»ä½“æ€è·¯æ˜¯å°½æ—©å¼€å§‹è¿›è¡Œåå‘ä¼ æ’­ã€‚è°ƒåº¦è¿‡ç¨‹å¦‚ä¸‹ï¼š
![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b.svg)

å¦‚æœä½ ä»”ç»†è®¡ç®—å°±ä¼šå‘ç°æ°”æ³¡å¤§å°ä»ç„¶ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬çš„è®­ç»ƒæ•ˆç‡å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åªéœ€å­˜å‚¨ $p$ ä¸ªå¾®æ‰¹æ¬¡çš„æ¿€æ´»å€¼ï¼ˆå…¶ä¸­ $p$ æ˜¯æµæ°´çº¿å¹¶è¡Œåº¦ï¼‰ï¼Œè€Œä¸æ˜¯ $m$ ä¸ªï¼ˆå…¶ä¸­ $m$ æ˜¯å¾®æ‰¹æ¬¡çš„æ•°é‡ï¼‰ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ AFABï¼ˆå‡è®¾ä¸ºæŸç§è°ƒåº¦æ–¹å¼ï¼Œå…·ä½“éœ€ç»“åˆä¸Šä¸‹æ–‡ç¡®å®šå‡†ç¡®å«ä¹‰ï¼‰è°ƒåº¦ä¸­å‡ºç°çš„æ¿€æ´»å€¼å†…å­˜çˆ†ç‚¸é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¢åŠ æ›´å¤šå¾®æ‰¹æ¬¡ï¼Œè€Œè¿™å®é™…ä¸Šä¼šå‡å°‘æ°”æ³¡ã€‚

è¿™ç§è®¾ç½®çš„å¤æ‚æ€§ï¼ˆå¦‚ä¸Šå›¾æ‰€ç¤ºï¼‰åœ¨äºï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸å†æ˜¯æ¸…æ™°çš„é¡ºåºæ‰§è¡Œï¼Œè€Œæ˜¯åœ¨è®¾å¤‡é—´å¹¶è¡Œæ‰§è¡Œå¹¶äº¤é”™è¿›è¡Œã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†ä¸å¾—ä¸åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šç‹¬ç«‹åœ°å®‰æ’ä»å‰å‘ä¼ æ’­åˆ°åå‘ä¼ æ’­çš„åˆ‡æ¢ï¼Œè€Œä¸æ˜¯åƒå¾€å¸¸ä¸€æ ·åœ¨ä¸€ä¸ªç®€å•ä¸”é€šç”¨çš„ä¸­å¤®è®­ç»ƒå¾ªç¯ä¸­è¿›è¡Œã€‚

è¿™å°±æ˜¯å®æ–½æµæ°´çº¿å¹¶è¡Œé€šå¸¸éœ€è¦å¯¹è®­ç»ƒä»£ç ä»¥åŠå»ºæ¨¡ä»£ç è¿›è¡Œç›¸å½“å¹¿æ³›çš„ä¿®æ”¹çš„åŸå› ä¹‹ä¸€ã€‚

ä½ ä¹Ÿå¯ä»¥åœ¨ picotron ä¸­æ‰¾åˆ° 1F1B çš„å®Œæ•´å®ç°ï¼š

ğŸ‘‰ 1F1B PP åœ¨ Picotron ä¸­çš„å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
def train_step_pipeline_1f1b(model, data_loader, tensor_shapes, device, dtype):    
    num_warmup_microbatches = min(pgm.process_group_manager.pp_world_size - pgm.process_group_manager.pp_rank - 1, data_loader.grad_acc_steps)
    num_microbatches_remaining = data_loader.grad_acc_steps - num_warmup_microbatches
    logging_loss, input_tensors, output_tensors  = 0.0, [], []
    requires_grad_sync = pgm.process_group_manager.cp_dp_world_size > 1
    
    def _forward_step(input_tensor):
        batch = next(data_loader)
        batch["hidden_states"] = input_tensor.to(device) if input_tensor is not None else input_tensor
        output_tensor = model.forward(input_ids=batch["input_ids"].to(device), position_ids=batch["position_ids"].to(device), hidden_states=batch["hidden_states"])
        
        # calculate loss on the last stage
        if pgm.process_group_manager.pp_is_last_stage:
            output_tensor = F.cross_entropy(output_tensor.transpose(1, 2), batch["target_ids"].to(device), reduction='mean')
            nonlocal logging_loss
            logging_loss += output_tensor.item() / data_loader.grad_acc_steps
        return output_tensor

    for _ in range(num_warmup_microbatches): # Warmup forward passes
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
        output_tensor = _forward_step(input_tensor)
        pipeline_communicate(operation='send_forward', tensor=output_tensor, device=device, dtype=dtype)
        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)

    if num_microbatches_remaining > 0:
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
    
    if requires_grad_sync:
        model.require_backward_grad_sync = False

    for ith_microbatch in range(num_microbatches_remaining):  # 1F1B steady state
        is_last_iteration = (ith_microbatch == num_microbatches_remaining - 1)
        output_tensor = _forward_step(input_tensor)
        output_tensor_grad = bidirectional_pipeline_communicate(operation='send_fwd_recv_bwd', send_tensor=output_tensor, recv_shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        
        # Trigger gradient sync on the last microbatch but only when last rank (the one that has num_warmup_microbatches = 0) has finished computing its backward pass.
        if num_warmup_microbatches == 0 and is_last_iteration:
            model.require_backward_grad_sync = True

        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        
        if is_last_iteration:
            input_tensor = None
            pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)
        else:
            input_tensor = bidirectional_pipeline_communicate(operation='send_bwd_recv_fwd', send_tensor=input_tensor_grad, recv_shapes=tensor_shapes, device=device, dtype=dtype)

    for ith_warmup_microbatches in range(num_warmup_microbatches): # Cooldown backward passes
        if requires_grad_sync:
            is_last_iteration = (ith_warmup_microbatches == num_warmup_microbatches - 1)
            model.require_backward_grad_sync = (ith_warmup_microbatches == num_warmup_microbatches - 1)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        output_tensor_grad = pipeline_communicate(operation='recv_backward', shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)

    return logging_loss
```

è®©æˆ‘ä»¬é€šè¿‡åœ¨æˆ‘ä»¬çš„é›†ç¾¤ä¸Šè¿›è¡Œçš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼Œæ¥çœ‹ä¸€ä¸‹ 1F1B æµæ°´çº¿å¹¶è¡Œè°ƒåº¦åœ¨å®è·µä¸­æ˜¯å¦‚ä½•æ‰©å±•çš„ï¼š

![Throughput scaling of Pipeline Parallelism with varying microbatch sizes|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b_scaling.png)

åœ¨å·¦ä¾§ï¼Œå¾®æ‰¹å¤„ç†æ•°é‡ç­‰äºæˆ–å°äºæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰åº¦å‡ä¸€ï¼ˆ$m = p - 1$ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æµæ°´çº¿æ°”æ³¡çš„å±å®³æœ‰å¤šå¤§â€”â€”æ€§èƒ½å¾ˆä½ï¼Œè€Œä¸”éšç€æµæ°´çº¿å¹¶è¡Œåº¦çš„å¢åŠ ç”šè‡³è¿˜ä¼šä¸‹é™ã€‚å³ä¾§å›¾è¡¨æ˜¾ç¤ºï¼Œä½¿ç”¨è¿œå¤šäºæµæ°´çº¿å¹¶è¡Œåº¦çš„å¾®æ‰¹å¤„ç†æ•°é‡ï¼ˆ$m = 32 \gg p - 1$ï¼‰æœ‰åŠ©äºæ”¹å–„ä½æµæ°´çº¿å¹¶è¡Œåº¦ä¸‹çš„æ€§èƒ½ï¼Œä½†åœ¨éå¸¸å¤§çš„æµæ°´çº¿å¹¶è¡Œåº¦ä¸‹ä»ç„¶å—é™ã€‚å®é™…ä¸Šï¼Œç”±äºæœ€ç»ˆå—åˆ°ç›®æ ‡å…¨å±€æ‰¹é‡å¤§å°çš„é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•éšæ„å¢åŠ å¾®æ‰¹å¤„ç†æ•°é‡ä»¥ä¿æŒ $m \gg p - 1$ çš„æ¯”ä¾‹ã€‚éšç€æµæ°´çº¿å¹¶è¡Œåº¦çš„å¢åŠ ï¼Œå½“å¾®æ‰¹å¤„ç†æ•°é‡è¾¾åˆ°å¯èƒ½çš„æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆå¿…é¡»æ ¹æ®Â $r_\text{bubble}=\frac{pâˆ’1}{m}$ æ¥å¢åŠ æ°”æ³¡å¤§å° ã€‚

æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å¾®æ‰¹æ¬¡æ•°é‡è¾ƒå°‘æ—¶ï¼Œä»å•ä¸ªèŠ‚ç‚¹ï¼ˆ$p = 8$ï¼‰æ‰©å±•åˆ°ä¸¤ä¸ªèŠ‚ç‚¹ï¼ˆ$p = 16$ï¼‰ï¼Œæ€§èƒ½ä»…ä¸‹é™äº† 14% â€”â€”è¿™æ¯”å¼ é‡å¹¶è¡Œæ€§åœ¨ç±»ä¼¼çš„è·¨èŠ‚ç‚¹åœºæ™¯ä¸­é€šå¸¸å‡ºç°çš„çº¦ 43% çš„æ€§èƒ½é€€åŒ–è¦å¥½å¾—å¤šã€‚å½“é‡åˆ°èŠ‚ç‚¹é—´ä½å¸¦å®½ç½‘ç»œæ—¶ï¼Œè¿™ç§è¡Œä¸ºä½¿å¾—æµæ°´çº¿å¹¶è¡Œæ€§åœ¨è·¨å¤šä¸ªèŠ‚ç‚¹çš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ç‰¹åˆ«æœ‰å¸å¼•åŠ›ã€‚

è™½ç„¶ 1F1B æ˜¾è‘—å‡å°‘äº†æˆ‘ä»¬çš„æ¿€æ´»å†…å­˜å ç”¨ï¼Œä½†ä»è¿™æœ€åä¸€å¼ å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿æ°”æ³¡ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æ•ˆç‡ç“¶é¢ˆã€‚ç”±äºæ°”æ³¡å¤§å°ä»ä¸æµæ°´çº¿é˜¶æ®µæ•°æˆæ­£æ¯”ï¼Œæˆ‘ä»¬è®©å®è´µçš„ GPU è®¡ç®—èƒ½åŠ›å¤„äºé—²ç½®çŠ¶æ€ã€‚æˆ‘ä»¬èƒ½å¦è®¾è®¡å‡ºä¸€ç§æ›´å·§å¦™çš„è°ƒåº¦æ–¹æ¡ˆæ¥å°½é‡å‡å°‘è¿™ç§æµªè´¹çš„è®¡ç®—æ—¶é—´å‘¢ï¼Ÿ

### 6.3 äº¤é”™é˜¶æ®µ

1F1B è°ƒåº¦è®©æˆ‘ä»¬æ”¹å–„äº†å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œä½†å¯¹ç©ºé—²åŒ…çš„å¤§å°æ”¹å–„ä¸å¤§ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬è¿˜èƒ½æ¨è¿›è¿™ä¸€è¾¹ç•Œå—ï¼Ÿ

åŸæ¥ï¼Œå¦‚æœæˆ‘ä»¬æ„¿æ„å¼•å…¥ä¸€äº›é¢å¤–çš„é€šä¿¡æ“ä½œï¼Œè¿™æ˜¯å¯è¡Œçš„ã€‚ç°åœ¨æ˜¯æ—¶å€™è°ˆè°ˆ*äº¤é”™é˜¶æ®µ*äº†ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´æ²¿ç€æ¨¡å‹æ·±åº¦ç»´åº¦å¤©çœŸåœ°å¯¹æ¨¡å‹è¿›è¡Œåˆ‡ç‰‡å¤„ç†ï¼Œä¾‹å¦‚å°†ç¬¬ 1-4 å±‚æ”¾åœ¨ç¬¬ä¸€ä¸ª GPU ä¸Šï¼Œå°†ç¬¬ 5-8 å±‚æ”¾åœ¨ç¬¬äºŒä¸ª GPU ä¸Šã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è€ƒè™‘å…¶ä»–å¯¹å±‚è¿›è¡Œåˆ‡ç‰‡çš„æ–¹å¼ï¼Œæ¯”å¦‚å°†å¥‡æ•°å±‚ï¼ˆç¬¬ 1ã€3ã€5ã€7 å±‚ï¼‰æ”¾åœ¨ç¬¬ä¸€ä¸ª GPU ä¸Šï¼Œå°†å¶æ•°å±‚ï¼ˆç¬¬ 2ã€4ã€6ã€8 å±‚ï¼‰æ”¾åœ¨ç¬¬äºŒä¸ª GPU ä¸Šã€‚

è¿™é€šå¸¸å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§â€œç¯å½¢ç®¡é“â€ï¼Œå¾®æ‰¹æ¬¡åœ¨é€šè¿‡æ¨¡å‹çš„å‰å‘ä¼ æ’­æ—¶ä¼šä»ä¸€ä¸ª GPU å¾ªç¯ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª GPUã€‚è®©æˆ‘ä»¬é€šè¿‡å›¾å½¢æ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

![pp_1f1b_interleaved.svg|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b_interleaved.svg)

*ä¸€ä¸ªåœ¨ 4 ä¸ª GPU ä¸Šåˆ†å¸ƒå„å±‚çš„æ¨¡å‹çš„äº¤é”™æµæ°´çº¿å¹¶è¡Œçš„ç¤ºä¾‹ã€‚ç¼–å·ä»å¯¹åº”å¾®æ‰¹å¤„ç† IDï¼Œä½†ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚è¿›è¡Œäº†ä¸åŒç€è‰²ï¼Œä»¥è¯´æ˜å„å±‚æ˜¯å¦‚ä½•åˆ†å¸ƒåœ¨ GPU ä¸Šçš„ã€‚*

å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°ç”±äºæ¨¡å‹é’ˆå¯¹åŒä¸€è®¡ç®—åœ¨æ¯ä¸ª GPU ä¸Šè¦ç»è¿‡å¤šæ¬¡ï¼ˆè€Œæ­¤å‰åªéœ€ä¸€æ¬¡ï¼‰ï¼Œä»è€Œå‡ºç°äº†é¢å¤–çš„é€šä¿¡æƒ…å†µã€‚ä¸è¿‡ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éƒ½è¢«ä¸€ä¸ªå› å­ $v$ æ‰€åˆ†æ‘Šï¼Œå…¶ä¸­ $v$ æ˜¯é˜¶æ®µæ•°æˆ–è€…æ¯ä¸ª GPU çš„æ¨¡å‹å—æ•°ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨èƒ½å¤Ÿæ›´å¥½åœ°äº¤é”™è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚$$\begin{align}
t_{pb} &= \frac{(p - 1) \times (t_f + t_b)}{v} \\[1.2ex]
r_{\text{bubble}} &= \frac{1}{v} \frac{(p - 1) \times (t_f + t_b)}{m \times (t_f + t_b)} = \frac{p - 1}{v \times m}\end{align}$$
å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡æ·»åŠ å¾®æ‰¹æ¬¡å’Œäº¤é”™é˜¶æ®µæ¥å‡å°æ°”æ³¡ï¼Œä½†è¯·æ³¨æ„ï¼Œä»æ•°é‡ä¸Šæ¥è¯´ï¼Œé€šä¿¡é‡ä¹Ÿä¼šå›  $v$ è€Œå¢åŠ ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ç§æƒè¡¡ã€‚åœ¨ä¸‹é¢çš„å›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ° $p = 8$ çš„ PP è®¾ç½®çš„å‡ ç§é…ç½®ï¼Œå…¶ä¸­ç‰¹æ®Šæƒ…å†µ $m = 1, v = 1$ å¯¹åº”äºç®€å•çš„æµæ°´çº¿å¹¶è¡Œï¼Œ$v = 1$ çš„é…ç½®æ˜¯ AFAB æˆ– 1F1B è®¾ç½®ï¼Œè€Œ $v â‰  1$ æ˜¯äº¤é”™é…ç½®ã€‚

[äº¤äº’å›¾]

è°ƒåº¦åœ¨è¿™é‡Œä¹Ÿå˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»åœ¨ç»™å®šçš„ GPU ä¸Šå’Œç»™å®šçš„æ—¶åˆ»å†³å®šæˆ‘ä»¬æ˜¯ä¼˜å…ˆå¤„ç†å…ˆåˆ°è¾¾çš„å¾®æ‰¹æ¬¡æ•°æ®ï¼Œè®©å®ƒä»¬é€šè¿‡åç»­å±‚ï¼ˆå³å°½å¿«å®Œæˆå‰å‘å’Œåå‘ä¼ æ’­å¾ªç¯ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„â€œæ·±åº¦ä¼˜å…ˆâ€ï¼Œå³ä¼˜å…ˆè®©æ‰¹æ¬¡æ•°æ®å°½å¿«ç¦»å¼€æ¨¡å‹ï¼‰ï¼Œè¿˜æ˜¯ä¼˜å…ˆå¤„ç†åç»­çš„å¾®æ‰¹æ¬¡æ•°æ®ï¼Œè®©å®ƒä»¬é€šè¿‡å‰é¢çš„å±‚ï¼ˆå³æ‰€è°“çš„â€œå¹¿åº¦ä¼˜å…ˆâ€ï¼Œå³ä¼˜å…ˆå°½å¯èƒ½å¡«æ»¡æµæ°´çº¿ï¼‰ã€‚è¿™ç§é€‰æ‹©åœ¨ç²¾å½©çš„â€œå¹¿åº¦ä¼˜å…ˆæµæ°´çº¿â€è®ºæ–‡[^6]ä¸­æœ‰è¯¦ç»†è§£é‡Šã€‚

ä½ ç°åœ¨æ‹¥æœ‰äº†ç†è§£ Llama 3.1 ä¸­æµæ°´çº¿å¹¶è¡Œæ–¹æ³•çš„æ‰€æœ‰è¦ç´ ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸€æ¬¡åå‘ä¼ æ’­çš„è®¾ç½®ï¼Œå„é˜¶æ®µäº¤é”™è¿›è¡Œï¼Œå¹¶ä¸”ä¼˜å…ˆçº§è®¾ç½®å¯ä»¥åœ¨æ·±åº¦ä¼˜å…ˆå’Œå¹¿åº¦ä¼˜å…ˆä¹‹é—´è¿›è¡Œè°ƒæ•´ã€‚

![pp_llama3.1_schedule.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_llama3.1_schedule.png)

ç„¶è€Œï¼Œæˆ‘ä»¬å°šæœªåˆ°è¾¾å¯èƒ½çš„æµæ°´çº¿è°ƒåº¦æ–¹æ¡ˆçš„å°½å¤´ï¼Œæœ€è¿‘å·²ç»æœ‰ä¸€äº›æ–¹æ³•è¢«æå‡ºæ¥å°†æ°”æ³¡å‡å°‘åˆ°å‡ ä¹ä¸ºé›¶ï¼ä¾‹å¦‚ï¼Œè¿™äº›æŠ€æœ¯å·²åœ¨ DeepSeek V3/R1 å®ç°[^7]ä¸­ä½¿ç”¨ã€‚å‹¾èµ·ä½ çš„å¥½å¥‡å¿ƒäº†å—ï¼Ÿåœ¨æˆ‘ä»¬ç¦»å¼€æµæ°´çº¿å¹¶è¡Œä¸–ç•Œä¹‹å‰ï¼Œè®©æˆ‘ä»¬æœ€åå¿«é€Ÿçœ‹ä¸€çœ¼è¿™äº›ç¥å¥‡çš„è°ƒåº¦æ–¹æ¡ˆï¼

### 6.4 é›¶æ°”æ³¡å’ŒåŒç®¡

æœ€è¿‘æå‡ºäº†æ›´ä¸ºå¤æ‚çš„æ–¹æ³•æ¥å‡å°‘æ°”æ³¡ï¼Œè¿™äº›æ–¹æ³•å‡ ä¹è¾¾åˆ°äº†â€œé›¶æ°”æ³¡â€çŠ¶æ€ã€‚è¿™é‡Œçš„ç§˜è¯€æ˜¯åœ¨æ›´ç»†ç²’åº¦çš„å±‚é¢ä¸Šæ‹†åˆ†ç›¸å…³æ“ä½œï¼Œä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼å°†å®ƒä»¬äº¤é”™æ‰§è¡Œã€‚ä¾‹å¦‚ï¼ŒDeepSeek V3/R1 ä¸­çš„ç®¡é“å®ç°æ–¹æ³•ï¼Œç§°ä¸º DualPipeï¼Œå‡ ä¹è¾¾åˆ°äº†é›¶æ°”æ³¡çŠ¶æ€ã€‚

ï¼ˆDeepSeek V3 æŠ€æœ¯æŠ¥å‘Š[^7]ä¸­çš„ç»ˆæâ€œçµæ´»é…ç½®â€ï¼Œä½œè€…åœ¨æŠ¥å‘Šä¸­æŒ‡å‡ºä»–ä»¬çš„è®¾ç½®â€œå®ç°äº†è¿‘ä¹ä¸ºé›¶çš„å…¨äº’è”é€šä¿¡å¼€é”€â€ã€‚ï¼‰

è®©æˆ‘ä»¬ç®€è¦åœ°é€šè¿‡æ€»ç»“ä½œä¸º DualPipe å‰èº«çš„ ZeroBubble[^8] çš„å·¥ä½œæ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚ ZeroBubble çš„åŸºæœ¬è§‚å¯Ÿç»“æœæ˜¯ï¼ŒçŸ©é˜µä¹˜æ³•çš„åå‘ä¼ æ’­å®é™…ä¸Šæ¶‰åŠä¸¤ä¸ªåˆ†ç¦»çš„æ“ä½œï¼šè¾“å…¥ï¼ˆBï¼‰çš„åå‘æ“ä½œå’Œæƒé‡ï¼ˆWï¼‰çš„åå‘æ“ä½œã€‚

è™½ç„¶è¾“å…¥çš„åå‘ä¼ æ’­ï¼ˆå³ B çš„è¾“å‡ºï¼‰å¯¹äºæ‰§è¡Œè¾ƒä½å±‚çš„åå‘ä¼ æ’­æ˜¯å¿…è¦çš„ï¼Œä½†æƒé‡çš„åå‘ä¼ æ’­ï¼ˆå³ W çš„åå‘ä¼ æ’­ï¼‰å¯¹äºå…¶ä½™çš„åå‘ä¼ æ’­è¿‡ç¨‹å¹¶éå¿…è¦ï¼Œå¹¶ä¸”é€šå¸¸åªéœ€åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰æ‰§è¡Œã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_compgraph.png)

è¿™æ„å‘³ç€ W å¯ä»¥çµæ´»åœ°å®‰æ’åœ¨åŒä¸€é˜¶æ®µçš„ç›¸åº” B ä¹‹åçš„ä»»ä½•ä½ç½®ã€‚è¿™ä½¿å¾—å¯ä»¥ç­–ç•¥æ€§åœ°æ”¾ç½® W ä»¥å¡«å……æµæ°´çº¿æ°”æ³¡ã€‚å³ä¸Šè§’çš„ ZB-H2 æ—¶é—´è¡¨æ˜¯åˆ©ç”¨è¿™ç§ç»†ç²’åº¦åˆ†è§£å®ç°é›¶æ°”æ³¡çš„ï¼ˆç†è®ºï¼‰æ—¶é—´è¡¨ç¤ºä¾‹ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_ppschedule.png)

*åœ¨é¡¶éƒ¨ï¼ˆZeroBubble è®ºæ–‡ä¸­çš„å›¾ 2ï¼‰ï¼šç»å…¸çš„ 1F1B è°ƒåº¦ï¼Œäº¤é”™è¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œä½†ä¿æŒç²—ç²’åº¦çš„åå‘ä¼ æ’­ã€‚åœ¨ä¸‹é¢çš„ä¸¤ä¸ªå›¾ä¸­ï¼ˆZeroBubble è®ºæ–‡ä¸­çš„å›¾ 3ï¼‰ï¼ŒZeroBubble è°ƒåº¦çš„ä¸¤ä¸ªå˜ä½“ï¼Œå°†åå‘æ“ä½œæ‹†åˆ†ä¸ºæ›´ç»†ç²’åº¦çš„ â€œBâ€ å’Œ â€œWâ€ æ“ä½œã€‚æœ€åä¸€ä¸ªè°ƒåº¦ï¼Œç§°ä¸º â€œZB-H2â€ï¼Œæ˜¯ä¸€ä¸ªï¼ˆç†è®ºä¸Šï¼‰åˆ©ç”¨è¿™ç§ç»†ç²’åº¦åˆ†è§£å®ç°é›¶æ°”æ³¡çš„è°ƒåº¦ç¤ºä¾‹ã€‚*

DeepSeek çš„ DualPipe åœ¨å…¶ V3 æŠ€æœ¯æŠ¥å‘Š[^7]ä¸­ä»‹ç»äº†è¿™ç§åˆ†è§£æ–¹æ³•çš„æ‰©å±•ï¼Œå³é’ˆå¯¹ä» PP ç»´åº¦çš„ä¸¤ç«¯ä¼ æ’­çš„ä¸¤ä¸ªæµçš„æƒ…å†µï¼Œè¿™äº›æµäº¤é”™æ’åˆ—ï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘ GPU ä¸­çš„ç©ºé—²æ—¶é—´ã€‚è¯¥è°ƒåº¦æ–¹æ¡ˆæ˜¾ç¤ºåœ¨ä¸‹é¢çš„è°ƒåº¦å›¾ä¸­ï¼Œæ¯”ä¹‹å‰çš„æ–¹æ¡ˆæ›´ä¸ºå¤æ‚ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_dualpipe.png)

ä¸€èˆ¬æ¥è¯´ï¼Œè¦å®Œå…¨ä¼˜åŒ–è¿™ç§å¤æ‚çš„è°ƒåº¦å®‰æ’ï¼Œéœ€è¦ä»”ç»†æµ‹é‡å„ç§ç»†ç²’åº¦æ“ä½œçš„æŒç»­æ—¶é—´ï¼Œå¹¶æ±‚è§£ä¸€ä¸ªæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰é—®é¢˜ä»¥ä½¿æœ€ç»ˆçš„æ°”æ³¡æ—¶é—´æœ€å°åŒ–ã€‚ä¾‹å¦‚ï¼Œå¯å‚è€ƒ ZeroBubble è®ºæ–‡[^8]ä¸­å¯¹æ‰§è¡Œæ­¤ç±»è°ƒåº¦æ‰€é‡‡ç”¨çš„å¯å‘å¼ç®—æ³•å’Œç®—æ³•çš„è®¨è®ºã€‚å› æ­¤ï¼ŒZeroBubble å’Œ DualPipe çš„è°ƒåº¦å®‰æ’è¿‡äºå¤æ‚ï¼Œåœ¨æ­¤æ— æ³•ç»™å‡ºä»£ç ç‰‡æ®µï¼Œä½†ä½ åº”è¯¥å¼€å§‹å¯¹æ‰€æ¶‰åŠçš„æ¦‚å¿µæœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ã€‚

è¿™å°±ç»“æŸäº†æˆ‘ä»¬å¯¹ç®¡é“è°ƒåº¦å’Œæ°”æ³¡ä¸–ç•Œçš„å‚è§‚ã€‚å¸Œæœ›æ‚¨å–œæ¬¢è¿™æ¬¡å¯¼è§ˆä¹‹æ—…ï¼

ç°åœ¨æ˜¯æ—¶å€™æ¢è®¨æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»çš„æœ€åä¸€ç§å¹¶è¡Œæ–¹æ³•äº†ï¼Œå³æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥é«˜æ•ˆè®­ç»ƒå¤§å‹æ¨¡å‹çš„æ–¹æ³•ï¼š*ä¸“å®¶å¹¶è¡Œ*ã€‚

## ä¸ƒã€ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰

è¿™æ˜¯æˆ‘ä»¬è¦è®¨è®ºçš„æœ€åä¸€ä¸ªå¹¶è¡Œæ–¹æ³•ã€‚åœ¨æ¢è®¨å®ƒä¹‹å‰ï¼Œå¦‚æœæ‚¨å¯¹ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMixture-of-Expertsï¼‰æ²¡æœ‰ä»»ä½•äº†è§£ï¼Œæ¬¢è¿é˜…è¯»æˆ‘ä»¬ä¹‹å‰å‘å¸ƒçš„ä¸€ç¯‡è¾ƒçŸ­çš„åšå®¢æ–‡ç« ï¼Œè¿™ç¯‡æ–‡ç« åº”è¯¥èƒ½å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æ¶æ„ã€‚

ä¸“å®¶æ··åˆæ¨¡å‹è¿‘æœŸå›  GPT - 4ã€Mixtral[^9] ï¼Œä»¥åŠæ›´è¿‘æœŸçš„ DeepSeek-V3/R1 ç­‰æ¨¡å‹è€Œå—åˆ°å…³æ³¨å¹¶å´­éœ²å¤´è§’ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸€å±‚è®¾ç½®å¤šä¸ªå¹¶è¡Œæ¨¡å—ï¼Œè€Œéå•ä¸ªå‰é¦ˆæ¨¡å—ï¼Œå¹¶å°†ä»¤ç‰Œé€šè¿‡å…¶ä¸­ä¸€ä¸ªæˆ–å¦ä¸€ä¸ªæ¨¡å—è¿›è¡Œè·¯ç”±ï¼Œä»¥è¿›è¡Œä¸åŒçš„å¤„ç†ã€‚

![ep_moe.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ep_moe.png)
*å–è‡ª Switch Transformers è®ºæ–‡[^10]çš„ MoE å±‚ç¤ºæ„å›¾*

MoE å±‚çš„è®¾è®¡å®é™…ä¸Šä½¿å¾—è·¨ä¸“å®¶ç»´åº¦å®ç°å¹¶è¡Œæ€§å˜å¾—éå¸¸å®¹æ˜“ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºä¸“å®¶å¹¶è¡Œæ€§ï¼ˆEPï¼‰ã€‚ç”±äºå‰é¦ˆå±‚æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ¯ä¸ªä¸“å®¶çš„å‰é¦ˆå±‚æ”¾åœ¨ä¸åŒçš„å·¥ä½œå™¨ä¸Šã€‚ä¸ TP ç›¸æ¯”ï¼Œå®ƒè¦è½»é‡å¾—å¤šï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æ‹†åˆ†çŸ©é˜µä¹˜æ³•ï¼Œåªéœ€è¦å°† tokens çš„éšè—çŠ¶æ€è·¯ç”±åˆ°æ­£ç¡®çš„ä¸“å®¶å³å¯ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰é€šå¸¸ä¼šä¸å…¶ä»–å¹¶è¡Œå½¢å¼ï¼ˆä¾‹å¦‚æ•°æ®å¹¶è¡Œï¼‰ç»“åˆä½¿ç”¨ã€‚è¿™æ˜¯å› ä¸ºä¸“å®¶å¹¶è¡Œä»…å½±å“æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å±‚ï¼Œå¹¶ä¸”ä¸ä¼šå¯¹è¾“å…¥æ ‡è®°è¿›è¡Œåˆ†ç‰‡ï¼ˆä¸åƒä¸Šä¸‹æ–‡å¹¶è¡Œé‚£æ ·æ²¿åºåˆ—é•¿åº¦ç»´åº¦å¯¹æ ‡è®°è¿›è¡Œåˆ†ç‰‡ï¼‰ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬ä»…ä½¿ç”¨ä¸“å®¶å¹¶è¡Œï¼Œæˆ‘ä»¬çš„å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰å°†å¯¹æ‰€æœ‰é MoE å—æ‰§è¡Œå†—ä½™è®¡ç®—ã€‚é€šè¿‡å°†ä¸“å®¶å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œç›¸ç»“åˆï¼Œæˆ‘ä»¬å¯ä»¥åƒä¸‹é¢çš„ç®€åŒ–ç¤ºæ„å›¾ä¸­æ‰€ç¤ºï¼Œæœ‰æ•ˆåœ°åœ¨ GPU ä¹‹é—´å¯¹ä¸“å®¶å’Œè¾“å…¥æ‰¹æ¬¡è¿›è¡Œåˆ†ç‰‡ ã€‚

![ep_schema.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ep_schema.png)
*æ¥æºï¼šã€Šä¸“å®¶æ··åˆæ¨¡å‹ç»¼è¿°ã€‹[^11]*

ä½†å…ˆåˆ«é«˜å…´å¾—å¤ªæ—©â€”â€”åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å…·ä½“æ¢è®¨ä¸åŒå¹¶è¡Œç­–ç•¥ä¹‹é—´çš„æ‰€æœ‰äº¤äº’ï¼Œæ‰€ä»¥å¦‚æœä½ è¿˜æ²¡ç†è§£è¿™æœ€åä¸€å¼ å›¾ï¼Œä¹Ÿåˆ«æ‹…å¿ƒã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¦ä½¿ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰é«˜æ•ˆè¿è¡Œæœ‰ä¸€äº›æŠ€å·§ï¼Œå¹¶ä¸”è¿™äº›æŠ€å·§ä¸æ¨¡å‹è®¾è®¡å¯†åˆ‡ç›¸å…³ã€‚ä¾‹å¦‚ï¼ŒDeepSeek-V3 åœ¨è·¯ç”±å™¨ä¸­æ–½åŠ äº†ä¸€ç§çº¦æŸï¼Œç¡®ä¿æ¯ä¸ª token æœ€å¤šè¢«å‘é€åˆ° M ä¸ªèŠ‚ç‚¹ï¼ˆåœ¨ä»–ä»¬çš„æ¡ˆä¾‹ä¸­ï¼ŒM ä¸º 4ï¼‰ï¼Œä»¥å°† token ä¿ç•™åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå¹¶å‡å°‘é€šä¿¡å¼€é”€ã€‚è™½ç„¶ä¸“å®¶å¹¶è¡Œå·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´äº†[^12]ï¼Œä½†éšç€æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå®ƒç°åœ¨æ‰é‡æ–°å—åˆ°é‡è§† ã€‚

æˆ‘ä»¬è®¡åˆ’åœ¨ picotron/nanotron ä¸­å°½å¿«æ·»åŠ ä¸€ä¸ªæ›´å®Œæ•´çš„ EP ç¤ºä¾‹ï¼Œæ•¬è¯·å…³æ³¨æ›´å¤šå†…å®¹ï¼

## å…«ã€5D å¹¶è¡Œæ€§æ¦‚è¿°

æ­å–œè¯»è€…ï¼Œæ‚¨ç°åœ¨å·²ç»äº†è§£äº†å¯ç”¨äºæ‰©å±•æ¨¡å‹è®­ç»ƒçš„æ‰€æœ‰ 5 ç§å¹¶è¡Œç­–ç•¥ï¼š

1. DP â€”â€”æ²¿æ‰¹æ¬¡ç»´åº¦
2. TP â€”â€”æ²¿éšè—ç»´åº¦
3. SP/CP â€”â€”æ²¿åºåˆ—ç»´åº¦
4. PP â€”â€”æ²¿æ¨¡å‹å±‚
5. EP â€”â€”æ²¿æ¨¡å‹ä¸“å®¶

ä»¥åŠ 3 ç§ ZeRO ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä¸æ•°æ®å¹¶è¡Œæ€§ç›¸ç»“åˆä»¥å‡å°‘å†…å­˜å ç”¨ï¼š

1. ZeRO-1 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡
2. ZeRO-2 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦è¿›è¡Œåˆ†ç‰‡
3. ZeRO-3 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°è¿›è¡Œåˆ†ç‰‡

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¯èƒ½å¥½å¥‡çš„ä¸€ä¸ªæ–¹é¢æ˜¯ï¼Œæ‰€æœ‰è¿™äº›å¹¶è¡Œç­–ç•¥å’Œ ZeRO ç­–ç•¥å¦‚ä½•ç›¸äº’æ¯”è¾ƒå’Œäº¤äº’ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªäº›ç­–ç•¥å¹¶å°†å®ƒä»¬æœ‰æ•ˆåœ°ç»„åˆåœ¨ä¸€èµ·ï¼Œå“ªäº›ç­–ç•¥æˆ‘ä»¬åº”è¯¥ä¿æŒåˆ†å¼€ï¼Ÿ

è®©æˆ‘ä»¬æ¥çœ‹çœ‹ç›¸ä¼¼ä¹‹å¤„ä»¥åŠç›¸äº’ä½œç”¨ã€‚æˆ‘ä»¬å°†é¦–å…ˆå¯¹æ¯”æµæ°´çº¿å¹¶è¡Œå’Œ ZeRO-3ï¼Œå› ä¸ºå®ƒä»¬æœ‰ä¸€äº›éå¸¸ç›¸ä¼¼çš„åœ°æ–¹ï¼Œä½†ä¹Ÿæœ‰é‡è¦çš„åŒºåˆ«ã€‚

*æµæ°´çº¿å¹¶è¡Œä¸ ZeRO-3*â€”â€”æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰å’Œ ZeRO-3 éƒ½æ˜¯åœ¨å¤šä¸ª GPU ä¸Šåˆ’åˆ†æ¨¡å‹æƒé‡ï¼Œå¹¶æ²¿ç€æ¨¡å‹æ·±åº¦è½´è¿›è¡Œé€šä¿¡/è®¡ç®—çš„æ–¹æ³•ï¼ˆä¾‹å¦‚åœ¨ ZeRO-3 ä¸­ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—çš„åŒæ—¶é¢„å–ä¸‹ä¸€å±‚ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè®¾å¤‡ä¸Šéƒ½è®¡ç®—å®Œæ•´çš„å±‚æ“ä½œï¼Œè€Œä¸æ˜¯åƒå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æˆ–ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰é‚£æ ·åœ¨å­å±‚å•å…ƒä¸Šæ‰§è¡Œè®¡ç®—ã€‚

ï¼ˆä»¥ä¸‹æˆ‘ä»¬å°†â€œä¸€å±‚â€ç®€ç§°ä¸ºâ€œä¸€å±‚â€ï¼ˆä¸€èˆ¬åº”ç§°ä¸ºâ€œä¸€ç»„å±‚â€ï¼Œä½œä¸ºæ¨¡å‹çš„åŸºç¡€åˆ†ç‰‡å•å…ƒï¼‰ã€‚ï¼‰

ç„¶è€Œï¼ŒPP æ–¹æ³•ä¸ ZeRO-3 æ–¹æ³•ä¹‹é—´å­˜åœ¨å‡ ä¸ªä¸»è¦å·®å¼‚ï¼š

|                           | ZeRO-3                          | ç®¡é“å¹¶è¡Œ                         |
|---------------------------|---------------------------------|---------------------------------|
| æ¯ä¸ªè®¡ç®—å•å…ƒå­˜å‚¨               | ä»…å­˜å‚¨ä¸€å±‚çš„ä¸€éƒ¨åˆ†                  | æ•´å±‚                             |
| ç”¨äºä¼ è¾“çš„é€šä¿¡                 | æƒé‡                             | æ¿€æ´»å€¼                           |
| ç¼–æ’                       | æ¨¡å‹æ— å…³                          | æ¨¡å‹æ— å…³                         |
| å®ç°æŒ‘æˆ˜                    | å¤„ç†æ¨¡å‹åˆ’åˆ†å’Œé€šä¿¡è¾ƒå¤æ‚             | å¤„ç†é«˜æ•ˆç®¡é“å¹¶è¡Œè°ƒåº¦è¾ƒå¤æ‚             |
| æ‰©å±•è€ƒè™‘                    | åå¥½è¾ƒå¤§çš„ `mbs` å’Œ `seq_len` éšè—é€šä¿¡ | åå¥½è¾ƒå¤§çš„ `grad_acc` éšè—æ°”æ³¡     |

å¦‚ä½ æ‰€è§ï¼ŒZeRO-3 å’Œ PPè§£å†³äº†ç›¸åŒçš„æŒ‘æˆ˜ï¼Œä½†æ¶‰åŠä¸åŒçš„æ–¹æ³•ï¼Œé€‰æ‹©ä¸¤è€…ä¸­çš„å“ªä¸€ä¸ªå°†å–å†³äºä½ æ˜¯å†³å®šå°†é€šä¿¡é‡ç‚¹æ”¾åœ¨æƒé‡ä¸Šè¿˜æ˜¯æ¿€æ´»å€¼ä¸Šã€‚è™½ç„¶å®ƒä»¬å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œä½†åœ¨å®è·µä¸­å¹¶ä¸ç»å¸¸è¿™æ ·åšï¼Œå› ä¸ºè¿™æ ·åšéœ€è¦æ˜¾è‘—å¢åŠ å…¨å±€æ‰¹é‡å¤§å°ä»¥åˆ†æ‘Šé€šä¿¡æˆæœ¬ï¼Œä»è€Œåœ¨å…¨å±€æ‰¹é‡å¤§å°ã€æ¨¡å‹å¤§å°ã€ç½‘ç»œå¸¦å®½å’Œè®­ç»ƒæ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚å¦‚æœä½ å†³å®šå°†å®ƒä»¬ç»“åˆèµ·æ¥ï¼Œåº”é…ç½® ZeRO-3 åœ¨ä¸€ç³»åˆ— PP å¾®æ‰¹æ¬¡æœŸé—´å°†æƒé‡ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œä»¥å°½å¯èƒ½å‡å°‘ä¸å¿…è¦çš„é€šä¿¡å¼€é”€ã€‚

å¦ä¸€æ–¹é¢ï¼Œä¸“æ³¨äºä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦çš„ ZeRO-1 å’Œ ZeRO-2 å¯ä»¥å¾ˆå®¹æ˜“åœ°ä¸æµæ°´çº¿å¹¶è¡Œç›¸ç»“åˆï¼Œå¹¶ä¸”ä¸ä¹‹äº’è¡¥ã€‚å°†å®ƒä»¬ç»“åˆèµ·æ¥ä¸ä¼šå¼•å‘ä»»ä½•ç‰¹åˆ«æ–°çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒDeepSeek-v3 çš„è®­ç»ƒå°±ä½¿ç”¨äº†æµæ°´çº¿å¹¶è¡Œç»“åˆ ZeRO-1ï¼ˆåŸæ–‡å¦‚æ­¤ï¼‰ã€‚

*å¼ é‡å¹¶è¡Œ*ï¼ˆä¸åºåˆ—å¹¶è¡Œï¼‰æœ¬è´¨ä¸Šæ˜¯äº’è¡¥çš„ï¼Œå®ƒå¯ä»¥ä¸æµæ°´çº¿å¹¶è¡Œå’Œ ZeRO-3 ç»“åˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¾èµ–äºçŸ©é˜µä¹˜æ³•çš„åˆ†é…å±æ€§ï¼Œè¿™ä½¿å¾—æƒé‡å’Œæ¿€æ´»å¯ä»¥åœ¨ç»„åˆä¹‹å‰è¢«åˆ†ç‰‡å¹¶ç‹¬ç«‹è®¡ç®—ã€‚

![TP & SP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_tp_sp.svg)

æˆ‘ä»¬ä¸æƒ³ä»…å‡ºäºå¹¶è¡Œæ€§è€Œä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„ä¸»è¦åŸå› æ˜¯ï¼Œåœ¨å®è·µä¸­ï¼Œæ­£å¦‚å‰æ–‡æ‰€è¿°ï¼Œå¼ é‡å¹¶è¡Œå­˜åœ¨ä¸¤ä¸ªé™åˆ¶ï¼šé¦–å…ˆï¼Œç”±äºå…¶é€šä¿¡æ“ä½œæ˜¯è®¡ç®—å…³é”®è·¯å¾„çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤å¾ˆéš¾åœ¨é€šä¿¡å¼€é”€å¼€å§‹å æ®ä¸»å¯¼åœ°ä½çš„æŸä¸ªç‚¹ä¹‹åå®ç°è‰¯å¥½æ‰©å±•ã€‚å…¶æ¬¡ï¼Œä¸å¯¹æ¨¡å‹æ²¡æœ‰è¦æ±‚çš„ZeROå’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ä¸åŒï¼Œå¼ é‡å¹¶è¡Œéœ€è¦ä»”ç»†å¤„ç†æ¿€æ´»åˆ†ç‰‡â€”â€”æœ‰æ—¶æ˜¯åœ¨å¼ é‡å¹¶è¡ŒåŒºåŸŸå†…æ²¿éšè—ç»´åº¦è¿›è¡Œï¼Œæœ‰æ—¶æ˜¯åœ¨æµæ°´çº¿å¹¶è¡ŒåŒºåŸŸå†…æ²¿åºåˆ—ç»´åº¦è¿›è¡Œâ€”â€”è¿™ä½¿å¾—æ­£ç¡®å®ç°å˜å¾—æ›´åŠ ç¹çï¼Œå¹¶ä¸”éœ€è¦ç‰¹å®šäºæ¨¡å‹çš„çŸ¥è¯†æ¥ç¡®ä¿æ•´ä¸ªè¿‡ç¨‹ä¸­åˆ†ç‰‡æ¨¡å¼æ­£ç¡®ã€‚

å› æ­¤ï¼Œåœ¨ç»“åˆå¹¶è¡Œç­–ç•¥æ—¶ï¼Œé€šå¸¸ä¼šä¸ºé«˜é€Ÿçš„èŠ‚ç‚¹å†…é€šä¿¡ä¿ç•™ TPï¼Œè€Œ ZeRO-3 æˆ– PP å¯ç”¨äºè·¨è¾ƒä½é€Ÿåº¦èŠ‚ç‚¹é—´é€šä¿¡çš„å¹¶è¡Œç»„ï¼Œå› ä¸ºå®ƒä»¬çš„é€šä¿¡æ¨¡å¼éœ€è¦çš„å¸¦å®½è¾ƒå°‘ï¼ˆå¯¹äº PPï¼‰æˆ–å¯ä»¥æ›´å®¹æ˜“åœ°ä¸è®¡ç®—é‡å ï¼ˆå¯¹äº ZeRO-3ï¼‰ã€‚ç»“åˆè¿™äº›æŠ€æœ¯æ—¶çš„ä¸»è¦è€ƒè™‘å› ç´ æ˜¯æœ‰æ•ˆåœ°å°† GPU ç»„ç»‡åˆ°æ¯ä¸ªå¹¶è¡Œç»´åº¦çš„ç»„ä¸­ï¼Œä»¥æœ€å¤§åŒ–ååé‡å¹¶æœ€å°åŒ–é€šä¿¡å¼€é”€ï¼ŒåŒæ—¶è¦æ³¨æ„ TP çš„æ‰©å±•é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œä¸º TP è¿›è¡Œé€šä¿¡çš„ GPU ç»„åº”ä¿ç•™åœ¨èŠ‚ç‚¹å†…ã€‚

*ä¸Šä¸‹æ–‡å¹¶è¡Œ* å’Œ *ä¸“å®¶å¹¶è¡Œ* ä¹Ÿæœ‰åŠ©äºæˆ‘ä»¬å¯¹æ¿€æ´»å€¼è¿›è¡Œåˆ†ç‰‡å¤„ç†ï¼Œå¹¶ä¸”å¯ä»¥çœ‹ä½œæ˜¯å¯¹å¼ é‡å¹¶è¡Œçš„è¡¥å……ã€‚å‰è€…å¤„ç†é•¿åºåˆ—ï¼Œè€Œåè€…æ”¯æŒåˆ†å¸ƒå¼ä¸“å®¶æ··åˆè®­ç»ƒï¼Œè€Œä¸”å®ƒä»¬å¯ä»¥ç»„åˆåœ¨ä¸€èµ·ï¼Œä¸ä¼šäº§ç”Ÿä»»ä½•ç‰¹å®šé—®é¢˜ã€‚

*ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆCPï¼‰* ä¸“é—¨é’ˆå¯¹åœ¨éå¸¸é•¿çš„åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒçš„æŒ‘æˆ˜ï¼Œé€šè¿‡æ²¿åºåˆ—ç»´åº¦åœ¨ GPU ä¹‹é—´å¯¹æ¿€æ´»è¿›è¡Œåˆ†ç‰‡æ¥å¤„ç†ã€‚è™½ç„¶å¤§å¤šæ•°æ“ä½œï¼ˆå¦‚å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ï¼‰å¯ä»¥ç‹¬ç«‹å¤„ç†è¿™äº›åˆ†ç‰‡åºåˆ—ï¼Œä½†æ³¨æ„åŠ›å±‚éœ€è¦é€šä¿¡ï¼Œå› ä¸ºæ¯ä¸ª token éƒ½éœ€è¦è®¿é—®æ•´ä¸ªåºåˆ—çš„é”®/å€¼ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ CP éƒ¨åˆ†ä¸­çœ‹åˆ°çš„ï¼Œé€šè¿‡ç¯çŠ¶æ³¨æ„åŠ›æ¨¡å¼å¯ä»¥é«˜æ•ˆåœ°å¤„ç†è¿™ç§æƒ…å†µï¼Œè¯¥æ¨¡å¼ä½¿è®¡ç®—å’Œé€šä¿¡é‡å ã€‚å½“æ‰©å±•åˆ°æç«¯åºåˆ—é•¿åº¦ï¼ˆ128k+ tokensï¼‰æ—¶ï¼ŒCP å°¤å…¶æœ‰ä»·å€¼ï¼Œå³ä½¿ä½¿ç”¨å®Œæ•´çš„æ¿€æ´»é‡æ–°è®¡ç®—ï¼Œåœ¨å•ä¸ª GPU ä¸Šï¼Œæ³¨æ„åŠ›çš„å†…å­˜éœ€æ±‚ä¹Ÿä¼šé«˜å¾—ä»¤äººæœ›è€Œå´æ­¥ã€‚

![CP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_cp.svg)

*ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰* ä¸“é—¨é’ˆå¯¹é€šè¿‡åœ¨ GPU ä¹‹é—´å¯¹ä¸“é—¨çš„â€œä¸“å®¶â€è¿›è¡Œåˆ†ç‰‡ï¼Œå¹¶åœ¨è®¡ç®—è¿‡ç¨‹ä¸­åŠ¨æ€åœ°å°†æ ‡è®°è·¯ç”±åˆ°ç›¸å…³ä¸“å®¶æ¥è®­ç»ƒä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å‹çš„æŒ‘æˆ˜ã€‚EPä¸­çš„å…³é”®é€šä¿¡æ“ä½œæ˜¯å°†æ ‡è®°è·¯ç”±åˆ°å…¶åˆ†é…çš„ä¸“å®¶å¹¶æ”¶é›†ç»“æœçš„ `all-to-all` æ“ä½œã€‚è™½ç„¶æ­¤æ“ä½œå¼•å…¥äº†ä¸€äº›é€šä¿¡å¼€é”€ï¼Œä½†å®ƒèƒ½å¤Ÿæ˜¾è‘—æ‰©å±•æ¨¡å‹å®¹é‡ï¼Œå› ä¸ºæ¯ä¸ªæ ‡è®°åœ¨æ¨ç†ï¼ˆå’Œè®­ç»ƒï¼‰æœŸé—´ä»…ç”±æ€»å‚æ•°çš„ä¸€å°éƒ¨åˆ†å¤„ç†ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒ/æ¨ç†æ–¹é¢ï¼Œå½“æ¨¡å‹æ‰©å±•åˆ°å¤§é‡ä¸“å®¶æ—¶ï¼Œè·¨ GPU åˆ’åˆ†ä¸“å®¶å˜å¾—ç›¸å…³ã€‚

ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek V3 ä½¿ç”¨äº† 256 ä¸ªä¸“å®¶ã€‚ï¼‰

![EP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_ep.svg)

> [!NOTE]
> EP å’Œ DP åœ¨è¾“å…¥å¤„ç†æ–¹é¢çš„è¿™ç§ç›¸ä¼¼æ€§æ˜¯ä¸ºä»€ä¹ˆä¸€äº›å®ç°å°†ä¸“å®¶å¹¶è¡Œè§†ä¸ºæ•°æ®å¹¶è¡Œçš„ä¸€ä¸ªå­é›†çš„åŸå› ï¼Œå…³é”®åŒºåˆ«åœ¨äº EP ä½¿ç”¨ä¸“é—¨çš„ä¸“å®¶è·¯ç”±ï¼Œè€Œä¸æ˜¯è®©æ‰€æœ‰ GPU é€šè¿‡ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬å¤„ç†è¾“å…¥ã€‚

*èŒƒå›´å’Œé‡ç‚¹*   è®©æˆ‘ä»¬ä¹Ÿå¿«é€Ÿæ€»ç»“ä¸€ä¸‹æ¨¡å‹ä¸­çš„ä¸€äº›ä¸åŒå¹¶è¡Œç­–ç•¥å½±å“æœ€å¤§çš„å­éƒ¨åˆ†ï¼š

- å¼ é‡å¹¶è¡Œï¼ˆå’Œåºåˆ—å¹¶è¡Œï¼‰é€šè¿‡åˆ†ç‰‡æƒé‡å’Œæ¿€æ´»æ¥å½±å“æ•´ä¸ªæ¨¡å‹çš„è®¡ç®—ã€‚
- ä¸Šä¸‹æ–‡å¹¶è¡Œä¸»è¦å½±å“æ³¨æ„åŠ›å±‚ï¼Œå› ä¸ºéœ€è¦è·¨åºåˆ—é€šä¿¡ï¼Œå…¶ä»–å±‚åˆ™åœ¨åˆ†ç‰‡åºåˆ—ä¸Šç‹¬ç«‹æ“ä½œã€‚
- ä¸“å®¶å¹¶è¡Œä¸»è¦å½±å“MoEå±‚ï¼ˆå–ä»£æ ‡å‡† MLP å—ï¼‰ï¼Œæ³¨æ„åŠ›å’Œå…¶ä»–ç»„ä»¶ä¿æŒä¸å˜ã€‚
- æµæ°´çº¿å¹¶è¡Œå’Œ ZeRO å¹¶ä¸ç‰¹åˆ«é’ˆå¯¹ä»»ä½•å­æ¨¡å—æˆ–ç»„ä»¶ï¼Œé™¤äº†æµæ°´çº¿å¹¶è¡Œä¸­æ¨¡å—å’Œå±‚éœ€è¦å¹³è¡¡å¤–ï¼Œç”±äºé¢å¤–çš„åµŒå…¥å±‚ï¼Œç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚é€šå¸¸è¢«åŒºåˆ«å¯¹å¾…ã€‚

| å¼ é‡ + åºåˆ—å¹¶è¡Œ        | ä¸Šä¸‹æ–‡å¹¶è¡Œ       | ä¸“å®¶å¹¶è¡Œ           |
| ---------------- | ----------- | -------------- |
| æ²¿éšè—/åºåˆ—ç»´åº¦åˆ†ç‰‡æƒé‡å’Œæ¿€æ´»å€¼ | æ²¿åºåˆ—ç»´åº¦åˆ†ç‰‡æ¿€æ´»å€¼  | åˆ†ç‰‡ä¸“ç”¨ä¸“å®¶æƒé‡å’Œæ¿€æ´»å€¼   |
| çŸ©é˜µä¹˜æ³•æ“ä½œçš„é€šä¿¡ï¼ˆåˆ—/è¡Œçº¿æ€§ï¼‰ | æ³¨æ„åŠ›é”®/å€¼çš„é€šä¿¡   | token è·¯ç”±åˆ°ä¸“å®¶çš„é€šä¿¡ |
| éœ€è¦ç‰¹å®šæ¨¡å‹çš„å®ç°        | é™¤äº†æ³¨æ„åŠ›å¤–ï¼Œæ¨¡å‹æ— å…³ | é™¤äº† MoE å±‚å¤–ï¼Œæ¨¡å‹æ— å…³ |
| åå¥½é«˜å¸¦å®½èŠ‚ç‚¹å†…é€šä¿¡       | åå¥½è¾ƒé•¿çš„åºåˆ—é•¿åº¦   | éœ€è¦ MoEs        |

*æ€»ç»“ä¸€ä¸‹*â€”â€”ç°åœ¨ï¼Œå°†æˆ‘ä»¬çœ‹åˆ°çš„æ‰€æœ‰æŠ€æœ¯æ•´åˆåˆ°ä¸€ä¸ªå›¾è¡¨ä¸­ï¼Œå¹¶å°†å®ƒä»¬å…¨éƒ¨ç»“åˆèµ·æ¥ï¼Œä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿæ²¡é”™ï¼Œæˆ‘ä»¬æ„¿æ„æ¥å—è¿™ä¸ªæŒ‘æˆ˜ï¼

åœ¨è¿™ä¸ªæ€»ç»“å›¾è¡¨ä¸­ï¼Œä½ å°†çœ‹åˆ°å•ä¸ª Transformer å±‚ï¼ˆä»¥å…¶ MoE å˜ä½“å½¢å¼ï¼‰çš„æ¿€æ´»è¿‡ç¨‹å’Œæ¨¡å—çš„å›¾ç¤ºã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å„ç§å¹¶è¡Œæ–¹å‘ä»¥åŠåœ¨å‰é¢çš„æ‰€æœ‰ç« èŠ‚ä¸­ä¸€ç›´åœ¨è®¨è®ºçš„é€šä¿¡æ“ä½œã€‚

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_full.svg)

æˆ‘ä»¬è¿˜å¯ä»¥å¹¶åˆ—å±•ç¤ºæ¯ç§ç­–ç•¥çš„å†…å­˜èŠ‚çœæƒ…å†µçš„*å…¨é¢æ¦‚è§ˆ*ã€‚æˆ‘ä»¬å°†é’ˆå¯¹ä¸åŒçš„åºåˆ—é•¿åº¦ä»¥åŠé€‰æ‹©æ€§ï¼ˆé¡¶éƒ¨ï¼‰å’Œå®Œå…¨ï¼ˆåº•éƒ¨ï¼‰é‡æ–°è®¡ç®—æ¥ç»˜åˆ¶å®ƒä»¬ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥çœ‹åˆ°å®ƒä»¬æ˜¯å¦‚ä½•ä¸æ¿€æ´»ä¸€èµ·ä½œç”¨çš„ï¼š

![5Dparallelism_8Bmemoryusage.svg](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5Dparallelism_8Bmemoryusage.svg)

è®©æˆ‘ä»¬ä»å®è§‚è§’åº¦å®¡è§†æ‰€æœ‰è¿™äº›æŠ€æœ¯ã€å®ƒä»¬çš„ä¸»è¦åŸºæœ¬æ€æƒ³ä»¥åŠä¸»è¦ç“¶é¢ˆï¼Œä»¥æ­¤ç»“æŸæœ¬èŠ‚å†…å®¹ï¼š

| æ–¹æ³•         | å†…å­˜èŠ‚çœå…·ä½“åº”ç”¨äº                        | å¹¶è¡Œ/åˆ†ç‰‡ç»´åº¦                   | ç¼ºç‚¹                                   |
|--------------|---------------------------------|-------------------------------|---------------------------------------|
| DP           | æ¿€æ´»å€¼ï¼ˆå‡å°‘æœ¬åœ°æ‰¹å¤§å°ï¼‰                 | æ‰¹æ¬¡                          | å—é™äºæœ€å¤§æ‰¹å¤§å°                         |
| PP           | æ¨¡å‹å‚æ•°                            | æ¨¡å‹å±‚                         | ç©ºé—²æ°”æ³¡å’Œå¤æ‚çš„è°ƒåº¦                       |
| TP/SP        | æ¨¡å‹å‚æ•°å’Œæ¿€æ´»å€¼                      | éšè—ç»´åº¦ / åºåˆ—é•¿åº¦             | éœ€è¦é«˜å¸¦å®½é€šä¿¡                            |
| CP           | æ¿€æ´»å€¼                             | åºåˆ—é•¿åº¦                       | åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­å¢åŠ é€šä¿¡å¼€é”€                   |
| EP           | ä¸“å®¶å‚æ•°                            | ä¸“å®¶ç»´åº¦                       | éœ€è¦ MoE å±‚ï¼Œå¢åŠ è·¯ç”±é€šä¿¡å¼€é”€               |
| ZeRO-1       | ä¼˜åŒ–å™¨çŠ¶æ€                          | åœ¨ DP å‰¯æœ¬é—´åˆ†ç‰‡               | å‚æ•°é€šä¿¡å¼€é”€                              |
| ZeRO-2       | ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦                      | åœ¨ DP å‰¯æœ¬é—´åˆ†ç‰‡               | å‚æ•°é€šä¿¡å¼€é”€                              |
| ZeRO-3       | ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°               | åœ¨ DP å‰¯æœ¬é—´åˆ†ç‰‡               | å‚æ•°é€šä¿¡å¼€é”€                              |

æ˜¾ç„¶ï¼Œè¿™äº›æŠ€æœ¯ä¸­æ²¡æœ‰å“ªä¸€ç§æ˜¯å®ç°ç¥å¥‡æ‰©å±•çš„çµä¸¹å¦™è¯ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä»¥æŸç§æ–¹å¼å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚æˆ‘ä»¬èƒ½å¦åˆ¶å®šå‡ºä¸€äº›è§„åˆ™ï¼Œå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°ä¸€ä¸ªå¥½çš„èµ·ç‚¹ï¼Œä»è€Œåœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œé€‰æ‹©å¹¶åŠ ä»¥ç»“åˆå‘¢ï¼Ÿè¿™å°†æ˜¯ä¸‹ä¸€èŠ‚çš„ä¸»é¢˜ã€‚

## ä¹ã€å¯»æ‰¾æœ€ä½³è®­ç»ƒé…ç½®

æˆ‘ä»¬ç°åœ¨å·²ç»ä»‹ç»äº†å®é™…ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ›´å¤§æ¨¡å‹çš„æ‰€æœ‰å¹¶è¡ŒæŠ€æœ¯ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä¸”ä¸ºä½•å¯ä»¥ç»„åˆåœ¨ä¸€èµ·ã€‚è¿˜æœ‰ä¸€ä¸ªæ™®éçš„é—®é¢˜ï¼šæœ€ç»ˆæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªäº›æŠ€æœ¯ï¼Œä»¥åŠå¦‚ä½•ç¡®å®šå…·ä½“çš„ç»„åˆæ–¹å¼ï¼Ÿ

æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ç®€å•æåŠäº†è¿™ä¸ªå†…å®¹ï¼Œç°åœ¨è®©æˆ‘ä»¬è¯¦ç»†åœ°é€æ­¥æ¢è®¨ä¸€ä¸ªå¯èƒ½çš„å†³ç­–è¿‡ç¨‹ã€‚è¦è®°ä½ï¼Œé‰´äºè®¡ç®—é›†ç¾¤çš„å„ç§ç‰©ç†å±æ€§ã€ç½‘ç»œå¸¦å®½ã€æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡ã€æ¯ä¸ª GPU çš„å†…å­˜é‡ç­‰å› ç´ ï¼Œä½ å§‹ç»ˆéœ€è¦è¿è¡Œä¸€äº›å®éªŒï¼Œæ‰èƒ½æ‰¾åˆ°è¯¥è®¡ç®—é›†ç¾¤çš„æœ€ç»ˆæœ€ä½³é…ç½®ã€‚

### 9.1 æ­¥éª¤ 1ï¼šå°†ä¸€ä¸ªè®­ç»ƒæ­¥éª¤é€‚é…åˆ°å†…å­˜ä¸­

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¼„æ¸…æ¥šå¦‚ä½•ä½¿ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å®ä¾‹é€‚é…æˆ‘ä»¬çš„ GPUã€‚ä¸€èˆ¬æœ‰ä¸¤ç§æƒ…å†µã€‚

*GPU èµ„æºä¸°å¯Œçš„æƒ…å†µ ğŸ¤‘* â€”â€” å½“ä½ æœ‰å¤§é‡å¯ç”¨çš„ GPU æ—¶ï¼š

- å¯¹äºå‚æ•°é‡åœ¨ 10B ä»¥ä¸‹çš„æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨å•ä¸€çš„å¹¶è¡ŒæŠ€æœ¯ï¼Œä¾‹å¦‚åœ¨ 8 ä¸ª GPU ä¸Šä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰æˆ– ZeRO-3/DPï¼ˆæ•°æ®å¹¶è¡Œï¼‰å¹¶é…åˆå…¨é‡é‡æ–°è®¡ç®—ï¼ˆFull Recomputeï¼‰
- å¯¹äºå‚æ•°é‡åœ¨ 10B åˆ° 100B ä¹‹é—´ä¸”éœ€è¦è¶…è¿‡ 8 ä¸ª GPU çš„æ¨¡å‹ï¼Œä½ æœ‰å‡ ç§é€‰æ‹©ï¼š
    - å°†å¼ é‡å¹¶è¡Œï¼ˆTP = 8ï¼‰ä¸æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰ç›¸ç»“åˆ
    - å°†å¼ é‡å¹¶è¡Œï¼ˆTP = 8ï¼‰ä¸æ•°æ®å¹¶è¡Œï¼ˆZeRO - 3ï¼‰ç›¸ç»“åˆ
    - ä»…ä½¿ç”¨ ZeRO-3ï¼ˆå³ä»…ä½¿ç”¨çº¯æ•°æ®å¹¶è¡Œï¼‰
- åœ¨ 512 ä¸ªåŠä»¥ä¸Š GPU è§„æ¨¡æ—¶ï¼Œç”±äºé€šä¿¡æˆæœ¬çš„åŸå› ï¼Œçº¯æ•°æ®å¹¶è¡Œ/ZeRO-3å°†å¼€å§‹å˜å¾—ä½æ•ˆâ€”â€”æ­¤æ—¶å°†æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ä¸å¼ é‡å¹¶è¡Œæˆ–æµæ°´çº¿å¹¶è¡Œç›¸ç»“åˆå¯èƒ½æ•ˆæœæ›´å¥½
- åœ¨ 1024 ä¸ªåŠä»¥ä¸Š GPU è§„æ¨¡æ—¶ï¼Œä¸€ç§æ¨èçš„è®¾ç½®æ˜¯å¼ é‡å¹¶è¡Œ TP = 8ï¼Œé…åˆæ•°æ®å¹¶è¡Œï¼ˆZeRO-2ï¼‰å’Œæµæ°´çº¿å¹¶è¡Œ

ç›®å‰æˆ‘ä»¬ä¸“æ³¨äºé€‚é…å•ä¸ªå®ä¾‹ â€”â€” å°½ç®¡æˆ‘ä»¬å¯èƒ½ä¼šä½¿ç”¨ DPï¼ˆæ•°æ®å¹¶è¡Œï¼‰æ¥å®ç° ZeROï¼ˆé›¶å†—ä½™ä¼˜åŒ–å™¨ï¼‰ä»¥è¾¾æˆè¿™ä¸€ç›®æ ‡ â€”â€” ä½†åœ¨è¿™é‡Œæˆ‘ä»¬ä»…å…³æ³¨å®ƒä¸ ZeRO-3 ç»“åˆä½¿ç”¨æ—¶åœ¨æ¨¡å‹å‚æ•°å†…å­˜èŠ‚çœæ–¹é¢æ‰€å¸¦æ¥çš„æ•ˆæœ ã€‚

ç‰¹æ®Šè€ƒè™‘äº‹é¡¹ï¼š

- å¯¹äºéå¸¸é•¿çš„åºåˆ—ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨èŠ‚ç‚¹é—´æ·»åŠ ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆCPï¼‰ã€‚
- å¯¹äºä¸“å®¶æ··åˆæ¶æ„ï¼Œè·¨èŠ‚ç‚¹ä½¿ç”¨ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰å°†æ›´æœ‰ä¼˜åŠ¿ã€‚


*GPU èµ„æºä¸è¶³çš„æƒ…å†µğŸ˜­* â€”â€”å½“ä½ å¯èƒ½ç¼ºå°‘ GPU èµ„æºæ—¶ï¼š

- ä½ å¯ä»¥å¯ç”¨å®Œå…¨æ¿€æ´»é‡æ–°è®¡ç®—ï¼Œä»¥ç‰ºç‰²ä¸€äº›è®¡ç®—é‡æ¥æ¢å–å†…å­˜ï¼ˆè¿™æ ·è®­ç»ƒé€Ÿåº¦ä¼šç¨æ…¢ä¸€äº›ï¼‰ã€‚
- ä½ å¯ä»¥å¢åŠ æ¢¯åº¦ç´¯ç§¯é‡ï¼Œä»¥ä¾¿åœ¨æœ‰é™çš„å†…å­˜ä¸‹å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡ã€‚  

ç°åœ¨æˆ‘ä»¬æœ‰äº†ç¬¬ä¸€ä¸ªæ¨¡å‹å®ä¾‹æ­£åœ¨è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ‰¹é‡å¤§å°åˆé€‚ã€‚

### 9.2 æ­¥éª¤ 2ï¼šå®ç°ç›®æ ‡å…¨å±€æ‰¹é‡å¤§å°

æ ¹æ®ç¬¬ä¸€æ­¥åœ¨å¾®æ‰¹æ¬¡å¤§å°å’Œ DP æ–¹é¢çš„æƒ…å†µï¼Œæˆ‘ä»¬å½“å‰çš„æ‰¹æ¬¡å¤§å°å¯èƒ½å¤ªå°æˆ–å¤ªå¤§ã€‚ç°åœ¨æ˜¯æ—¶å€™è¾¾åˆ°æˆ‘ä»¬çš„ç›®æ ‡æ‰¹æ¬¡å¤§å°äº†ã€‚  

è¦å¢åŠ æˆ‘ä»¬å½“å‰çš„å…¨å±€æ‰¹æ¬¡å¤§å°ï¼š

- æˆ‘ä»¬å¯ä»¥æ‰©å±•æ•°æ®å¹¶è¡Œæ€§æˆ–æ¢¯åº¦ç´¯ç§¯æ­¥éª¤
- å¯¹äºé•¿åºåˆ—ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§  

è¦å‡å°‘æˆ‘ä»¬å½“å‰çš„å…¨å±€æ‰¹æ¬¡å¤§å°ï¼š

- æˆ‘ä»¬å¯ä»¥å‡å°‘æ•°æ®å¹¶è¡Œæ€§ä»¥æ”¯æŒå…¶ä»–å¹¶è¡ŒåŒ–ç­–ç•¥
- å¯¹äºé•¿åºåˆ—ï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§  

å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»è®©æ¨¡å‹æŒ‰ç…§æˆ‘ä»¬æƒ³è¦çš„æ¨¡å‹å¤§å°å’Œæ‰¹æ¬¡å¤§å°çš„ä¸€èˆ¬é…ç½®è¿è¡Œï¼Œä½†æˆ‘ä»¬æ˜¯å¦ä»¥æœ€å¿«çš„æ–¹å¼å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Ÿç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å°½å¯èƒ½ä¼˜åŒ–ååé‡ã€‚

### 9.3 æ­¥éª¤ 3: ä¼˜åŒ–è®­ç»ƒååé‡

æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›ç¡®ä¿è®­ç»ƒå°½å¯èƒ½å¿«é€Ÿåœ°è¿è¡Œï¼Œè¿™æ ·æˆ‘ä»¬æ‰€æœ‰å®è´µçš„ GPU å°±èƒ½å§‹ç»ˆå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚åªè¦å†…å­˜å’Œé€šä¿¡ä¸æ˜¯ç“¶é¢ˆï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä»¥ä¸‹æ“ä½œï¼š

- åˆ©ç”¨èŠ‚ç‚¹å†…é«˜é€Ÿå¸¦å®½æ‰©å±•å¼ é‡å¹¶è¡Œåº¦ï¼Œç›´è‡³æ¥è¿‘èŠ‚ç‚¹è§„æ¨¡çš„ç¨‹åº¦ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½å‡å°‘å…¶ä»–å¹¶è¡Œåº¦ã€‚
- åœ¨ä¿æŒç›®æ ‡æ‰¹é‡å¤§å°çš„åŒæ—¶ï¼Œä½¿ç”¨ZeRO - 3å¢åŠ æ•°æ®å¹¶è¡Œåº¦ã€‚
- å½“æ•°æ®å¹¶è¡Œåº¦çš„é€šä¿¡å¼€å§‹æˆä¸ºç“¶é¢ˆæ—¶ï¼Œè¿‡æ¸¡åˆ°ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œåº¦ã€‚
- å°è¯•é€ä¸ªæ‰©å±•ä¸åŒçš„å¹¶è¡Œåº¦ã€‚
- å°è¯•å‡ ç§å¾®æ‰¹é‡å¤§å°ï¼ˆmbsï¼‰ï¼Œä»¥å®ç°æœ€å¤§GBSã€æ¨¡å‹è§„æ¨¡ã€è®¡ç®—å’Œé€šä¿¡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ ã€‚

### 9.4 å¯¹æ•°åƒç§é…ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•

æ—¢ç„¶æˆ‘ä»¬å·²ç»è®²å®Œäº†å…·ä½“æ­¥éª¤ï¼Œé‚£ç°åœ¨å°±åœ¨å®é™…ä¸­å®æ–½è¿™ä¸€æœç´¢è¿‡ç¨‹å§ã€‚

ä½ å°†åœ¨ nanotron ä»£ç åº“ä¸­æ‰¾åˆ°å‡ ä¸ªè„šæœ¬ï¼Œå¯ä½¿ç”¨è¿™äº›è„šæœ¬æ¥è¿è¡Œæˆ‘ä»¬ä¸Šè¿°è®¨è®ºçš„æ‰€æœ‰å®éªŒï¼Œå¹¶èƒ½å¤Ÿå¯¹ç°å®ç”Ÿæ´»ä¸­çš„è‡ªæœ‰æ¨¡å‹å’Œé›†ç¾¤è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

å®é™…ä¸Šï¼Œæˆ‘ä»¬åœ¨ *æ•°åƒç§åˆ†å¸ƒå¼é…ç½®* ä¸Šå¯¹æˆ‘ä»¬è‡ªå·±è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›é…ç½®æ¶µç›–äº†ä¸Šè¿°æ‰€æœ‰æ¨¡å‹è§„æ¨¡ï¼Œä»¥åŠæˆ‘ä»¬èƒ½å°è¯•çš„å¤§é‡é›†ç¾¤é…ç½®ï¼ˆå³ 1 åˆ° 64 ä¸ªèŠ‚ç‚¹çš„ 8xH100ï¼‰ï¼Œä»¥ä¾¿å¾—å‡ºæœ¬ä¹¦åˆ°ç›®å‰ä¸ºæ­¢æ‰€æ¶µç›–çš„ç»“æœ ã€‚

ï¼ˆæˆ‘ä»¬æƒ³å€Ÿæ­¤æœºä¼šå°±é˜»å¡äº†å¤§éƒ¨åˆ†ç§‘å­¦é›†ç¾¤å‘æˆ‘ä»¬çš„åŒäº‹ä»¬é“æ­‰ï¼Œå¹¶è¿›è€ŒåŸè°…å¯èƒ½å·²ç»è¢«ç§ä¸‹ä½è¯­çš„ä»»ä½•å¨èƒã€‚ï¼‰

ç°åœ¨è®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œæ”¶é›†å’Œåˆ†ææˆ‘ä»¬æ‰€æœ‰åŸºå‡†æµ‹è¯•çš„ç»“æœï¼Œçœ‹çœ‹é™¤äº†ç†è®ºä¹‹å¤–ï¼Œæˆ‘ä»¬æ˜¯å¦èƒ½åœ¨çœŸå®æ•°æ®ä¸Šå‘ç°å„ç§é…ç½®ç›¸äº’ä¹‹é—´çš„è¡¨ç°å¦‚ä½•ã€‚

æ‰€æœ‰ä»¥ä¸‹åŸºå‡†æµ‹è¯•å‡åœ¨åºåˆ—é•¿åº¦ä¸º 4096 ä¸”å…¨å±€æ‰¹é‡å¤§å°ä¸º 1M tokens çš„æƒ…å†µä¸‹è¿›è¡Œã€‚æˆ‘ä»¬æ”¶é›†äº†æ¯ä¸ªæ¨¡å‹å’Œé›†ç¾¤å¤§å°çš„æ‰€æœ‰æœ€ä½³é…ç½®ï¼Œå¹¶å°†å®ƒä»¬ç»˜åˆ¶åœ¨ä»¥ä¸‹çƒ­å›¾ä¸­ï¼š

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/what_we_learnt_heatmap.svg)

*çƒ­å›¾å¯è§†åŒ–å±•ç¤ºäº†åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œè®¡ç®—èŠ‚ç‚¹æ•°é‡ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æœ‰8å—GPUï¼‰ä¸‹çš„æœ€ä¼˜è®­ç»ƒé…ç½®ã€‚å¯¹äºæ¯ç§ç»„åˆï¼Œé…ç½®ç»†èŠ‚åŒ…æ‹¬æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ã€å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ã€æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆGASï¼‰ã€å¾®æ‰¹é‡å¤§å°ï¼ˆMBSï¼‰ä»¥åŠZeROä¼˜åŒ–é˜¶æ®µã€‚é¢œè‰²æ·±æµ…è¡¨ç¤ºæ¨¡å‹FLOPsåˆ©ç”¨ç‡ï¼ˆMFUï¼‰ï¼Œé¢œè‰²è¶Šäº®è¡¨ç¤ºæ•ˆç‡è¶Šé«˜ã€‚*

ä»è¿™ä¸ªé«˜å±‚æ¬¡çš„å¯è§†åŒ–ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºå‡ ä¸ªé‡è¦çš„è§è§£ï¼š

é¦–å…ˆï¼Œéšç€æˆ‘ä»¬å¢åŠ èŠ‚ç‚¹æ•°é‡ï¼ˆæé«˜å¹¶è¡Œæ€§ï¼‰ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ•ˆç‡æœ‰æ‰€ä¸‹é™ã€‚è¿™ç§æ•ˆåº”åœ¨è¾ƒå°çš„æ¨¡å‹ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œè¿™äº›æ¨¡å‹çš„è®¡ç®—ä¸æ¨¡å‹å¤§å°æ¯”ç‡è¾ƒä½ã€‚è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¼šé€šè¿‡å¢åŠ æ‰¹é‡å¤§å°æ¥è¡¥å¿å°æ¨¡å‹å¤§å°ï¼Œä½†æˆ‘ä»¬å—åˆ°å…¨å±€æ‰¹é‡å¤§å°é™åˆ¶ä¸º 1M çš„çº¦æŸã€‚

å…¶æ¬¡ï¼Œæ›´å¤§çš„æ¨¡å‹å¸¦æ¥äº†ä¸åŒçš„æŒ‘æˆ˜ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œå†…å­˜éœ€æ±‚å¤§å¹…å¢é•¿ã€‚è¿™åœ¨èŠ‚ç‚¹æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ä¼šäº§ç”Ÿä¸¤ç§æƒ…å½¢ï¼šè¦ä¹ˆæ¨¡å‹æ ¹æœ¬æ— æ³•é€‚é…ï¼Œè¦ä¹ˆå‹‰å¼ºé€‚é…ï¼Œä½†ç”±äºè¿è¡Œæ—¶æ¥è¿‘ GPU å†…å­˜é™åˆ¶è€Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼ˆä¾‹å¦‚åœ¨ 4 ä¸ªèŠ‚ç‚¹ä¸Šè®­ç»ƒ 80B å‚æ•°æ¨¡å‹çš„æƒ…å†µï¼‰ã€‚

æœ€ç»ˆï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ç°è´¨é‡ã€‚å½“æˆ‘ä»¬æœ€åˆå®æ–½è¿™ä¸¤ç§å¹¶è¡Œç­–ç•¥æ—¶ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰çš„æ€§èƒ½ä¼˜äºæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚åœ¨ä¼˜åŒ–äº†æˆ‘ä»¬çš„ PP ä»£ç ä¹‹åï¼Œå®ƒæˆä¸ºäº†æ›´å¿«çš„é€‰æ‹©ã€‚ç°åœ¨æˆ‘ä»¬æ­£åœ¨æ”¹è¿› TP å®ç°ä¸­çš„é€šä¿¡é‡å ï¼Œæˆ‘ä»¬é¢„è®¡å®ƒå°†é‡æ–°è·å¾—æ€§èƒ½ä¼˜åŠ¿ã€‚

### 9.5 åŸºå‡†æµ‹è¯•çš„ç»éªŒæ•™è®­

æœ¬ä¹¦çš„ç›®æ ‡ä¸ä»…æ˜¯è®¨è®ºç†è®ºå’Œå®ç°ï¼Œè¿˜è¦æä¾›å®é™…çš„æ•°æ®ç‚¹ã€‚å› æ­¤è®¡åˆ’å¾ˆç®€å•ï¼šè®©æˆ‘ä»¬é’ˆå¯¹æ¯ä¸ªæ¨¡å‹ä»¥åŠä¸€ç³»åˆ—é›†ç¾¤è§„æ¨¡ï¼ˆå³ 8xH100 çš„ 1-64 ä¸ªèŠ‚ç‚¹ï¼‰è¿è¡Œæ‰€æœ‰å¯èƒ½çš„åˆ†å¸ƒå¼é…ç½®ã€‚å³ä¾¿æ’é™¤äº†ä¸å¯èƒ½çš„é…ç½®åï¼Œæˆ‘ä»¬ä»éœ€è¿›è¡Œæ•°åƒæ¬¡å®éªŒã€‚

ä»ç†è®ºä¸Šè®²ï¼Œè¿™å¬èµ·æ¥å¾ˆå®¹æ˜“ï¼šæˆ‘ä»¬å¯ä»¥åœ¨é›†ç¾¤ä¸Šè½»æ¾å¯åŠ¨å¤§é‡ä½œä¸šã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬å¯åŠ¨ç¬¬ä¸€æ‰¹å®éªŒæ—¶ï¼Œé—®é¢˜å°±å‡ºç°äº†ï¼š

- PyTorch è¿›ç¨‹æœ‰æ—¶æ— æ³•æ­£ç¡®æ¸…ç†
- Slurm ä½œä¸šç®¡ç†å™¨ä¼šå¼ºåˆ¶ç»ˆæ­¢ä½œä¸šï¼Œå¯¼è‡´èŠ‚ç‚¹æ•…éšœ
- æœ¬åº”åªéœ€å‡ åˆ†é’Ÿçš„ç®€å•åŸºå‡†æµ‹è¯•å¯èƒ½ä¼šå»¶é•¿åˆ°æ•°å°æ—¶
- æœ‰äº›ä½œä¸šä¼šæ— é™æœŸæŒ‚èµ·

åœ¨æœ‰é™çš„æ—¶é—´å†…è¿è¡Œæ‰€æœ‰å®éªŒéœ€è¦é¢å¤–çš„å·¥ç¨‹å·¥ä½œï¼Œæˆ‘ä»¬æœ€ç»ˆåœ¨ä»¥ä¸‹äº‹æƒ…ä¸ŠèŠ±è´¹äº†å¤§é‡æ—¶é—´ï¼š

- æœ€å°åŒ–é›†ç¾¤é‡å¯æ—¶é—´å¹¶ä¼˜åŒ–ç©ºé—²æ—¶é—´
- åˆ†æè¯¦ç»†çš„ NCCL è°ƒè¯•æ—¥å¿—
- äº†è§£å†…å­˜ä½¿ç”¨æ¨¡å¼å’Œ CUDA å†…å­˜åˆ†é…å™¨è¡Œä¸º
- æå‡å¤šèŠ‚ç‚¹ä¸Šçš„æµæ°´çº¿å¹¶è¡Œæ€§èƒ½

è¿™äº›æŒ‘æˆ˜å€¼å¾—å•ç‹¬è®²è¿°ï¼Œä½†å®ƒä»¬è®©æˆ‘ä»¬æ·±åˆ»è®¤è¯†åˆ°åˆ†å¸ƒå¼è®­ç»ƒåŸºç¡€è®¾æ–½çš„å¤æ‚æ€§ã€‚ç†è®ºä¸Šçœ‹èµ·æ¥ç®€å•çš„ä¸œè¥¿ï¼Œåœ¨å®è·µä¸­å¾€å¾€éœ€è¦å¯¹è®¸å¤šç›¸äº’å…³è”çš„éƒ¨åˆ†ç»™äºˆç»†è‡´å…³æ³¨ ã€‚

åœ¨å®è·µä¸­å¤ç°ç†è®ºç»“æœé¢‡å…·æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿäº§è®­ç»ƒä»£ç è·å–æœ‰é™çš„æƒ…å†µä¸‹ã€‚é€šè¿‡åƒ nanotron å’Œ picotron è¿™æ ·çš„å¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤ŸåŠ©åŠ›åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯å˜å¾—æ›´åŠ æ˜“äºè·å–ï¼Œå¹¶ä¸”å›´ç»•ç®€å•é«˜æ•ˆçš„ä»£ç åº“å±•å¼€åˆä½œï¼Œä»è€Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å……åˆ†åˆ©ç”¨ä»–ä»¬çš„ç¡¬ä»¶èµ„æºã€‚

---

è‡³æ­¤ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº† 5D å¹¶è¡Œæ€§çš„åˆ†å‘æ–¹æ³•ã€‚

é€€ä¸€æ­¥æ¥çœ‹ï¼Œåˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬çš„è®¨è®ºå¸¸å¸¸ä¾èµ–ä¸€ä¸ªå…³é”®å‡è®¾â€”â€”åœ¨ GPU ä¸Šèƒ½å¤Ÿé«˜æ•ˆåœ°å°†è®¡ç®—å’Œé€šä¿¡é‡å è¿›è¡Œï¼Œä¸”ä¸ä¼šå¯¹è®¡ç®—ååé‡äº§ç”Ÿä»»ä½•å½±å“ã€‚ä½†å®é™…æƒ…å†µæ›´ä¸ºå¤æ‚å¾®å¦™ã€‚å½“ä½¿ç”¨åƒ NCCL çš„ send/recv è¿™ç±»å¸¸è§çš„é€šä¿¡åŸè¯­æ—¶ï¼Œç”±äºé€šä¿¡å†…æ ¸é€šå¸¸ä¼šä½¿ç”¨ä¸è®¡ç®—ç›¸åŒçš„ GPU æµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰ï¼Œè®¡ç®—èµ„æºå’Œé€šä¿¡èµ„æºä¹‹é—´å°±ä¼šå­˜åœ¨éšè—çš„äº‰ç”¨æƒ…å†µï¼Œä»è€Œå¯¼è‡´åœ¨è®¡ç®—å’Œé€šä¿¡é‡å è¿›è¡Œæ—¶ååé‡ä¸‹é™ã€‚ä¸ºäº†çœŸæ­£ä¼˜åŒ–æˆ‘ä»¬çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦æ›´æ·±å…¥åœ°æ¢ç©¶ GPU æ¶æ„æœ¬èº«ã€‚

æ­¤å¤–ï¼Œåœ¨è®¡ç®—å’Œé€šä¿¡é‡å æ—¶çš„åŒæ­¥æ¨¡å¼å¯èƒ½å¹¶ä¸æ€»æ˜¯é€‚åˆæˆ‘ä»¬çš„å¹¶è¡Œç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨ Pytorch å›¢é˜Ÿçš„[è¿™ç¯‡åšå®¢æ–‡ç« ](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487)ä¸­æ‰¾åˆ°ä¸€ä¸ªä¾‹å­ã€‚

æ˜¯æ—¶å€™å…³ç¯å¹¶å¯åŠ¨ CUDA æ¨¡å¼äº†ï¼

## åã€æ·±å…¥GPUâ€”â€”èåˆã€çº¿ç¨‹å¤„ç†ã€æ··åˆ

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºä¸»è¦é›†ä¸­åœ¨æ¨¡å‹æ“ä½œçš„é«˜å±‚ç»„ç»‡ä¸Šã€‚æˆ‘ä»¬åœ¨å„ç§åŠ é€Ÿå™¨ä¸Šç§»åŠ¨è®¡ç®—ï¼ŒåŒæ—¶è€ƒè™‘åˆ°ä¸€èˆ¬çš„å†…å­˜é™åˆ¶å’Œè®¡ç®—å•å…ƒçš„é«˜å±‚è°ƒåº¦ã€‚

ä½†è¿™å¿½ç•¥äº†æˆ‘ä»¬é€šè¿‡ä»”ç»†äº†è§£æ¨¡å‹æ“ä½œåœ¨æ¯ä¸ª GPU ä¸Šçš„è°ƒåº¦å’Œæ‰§è¡Œæ–¹å¼ï¼Œåœ¨æ›´ä½å±‚æ¬¡ä¸Šæ‰€èƒ½è¿›è¡Œçš„æ‰€æœ‰ä¼˜åŒ–ã€‚

æœ¬èŠ‚å°†æ›´æ·±å…¥åœ°æ¢è®¨ GPU æ¶æ„çš„è¯¸å¤šç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯ NVIDIA çš„ GPU æ¶æ„ï¼Œä½†é€šå¸¸æ¥è¯´ï¼Œå…¶æ€»ä½“æ€è·¯å¯ä»¥åœ¨ç±»ä¼¼çš„åŠ é€Ÿå™¨å•å…ƒä¸Šå¤ç”¨ã€‚

åœ¨ä»‹ç» Flash-Attention é©å‘½ã€å¦‚ä½•é«˜æ•ˆåœ°åœ¨ GPU ä¸Šè°ƒåº¦å·¥ä½œè´Ÿè½½ä»¥åŠæœ€ç»ˆè§£é‡Šå¦‚ä½•åœ¨ GPU ä¸Šé«˜æ•ˆä½¿ç”¨å„ç§ç²¾åº¦ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ç®€è¦è¯´æ˜ GPU çš„ç»„ç»‡æ–¹å¼ã€‚

### 10.1 GPU å…¥é—¨çŸ¥è¯†

é€šå¸¸ï¼ŒGPU å…·æœ‰éå¸¸åˆ†å±‚çš„ç»„ç»‡ç»“æ„ã€‚åœ¨æœ¬å…¥é—¨çŸ¥è¯†ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠè®¨è®ºä¿æŒåœ¨å¯¹äºæˆ‘ä»¬åç»­æ¼”ç¤ºæ‰€éœ€çš„æ¦‚å¿µå±‚é¢ã€‚

åœ¨è®¡ç®—æ–¹é¢ï¼ŒGPU ç”±ä¸€ç»„ç§°ä¸ºæµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰çš„è®¡ç®—å•å…ƒç»„æˆã€‚æ¯ä¸ª SM åŒ…å«å¹¶æ§åˆ¶ä¸€ç»„æµå¤„ç†å™¨ï¼Œä¹Ÿç§°ä¸ºæ ¸å¿ƒã€‚ä¾‹å¦‚ï¼ŒNvidia H100 GPU å…·æœ‰ 132 ä¸ª SMï¼Œæ¯ä¸ª SM æœ‰ 128 ä¸ªæ ¸å¿ƒï¼Œå…±è®¡ 16,896 ä¸ªæ ¸å¿ƒï¼ˆæœ‰å…³å¼ é‡æ ¸å¿ƒçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[æ–‡æ¡£](https://resources.nvidia.com/en-us-tensor-core)ï¼‰ï¼Œæ¯ä¸ªæ ¸å¿ƒéƒ½èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªçº¿ç¨‹ã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/diving_primergpu.svg)

Source: https://blog.codingconfessions.com/p/gpu-computing

å†…å­˜æ–¹é¢ä¹Ÿå…·æœ‰é«˜åº¦å±‚çº§ç»“æ„ï¼ŒåŒ…å«å¤šå±‚ç¼“å­˜å’Œå†…å­˜ï¼šå¯„å­˜å™¨æ˜¯æœ€å°çš„å•å…ƒï¼Œåœ¨æ‰§è¡ŒæœŸé—´ä¸“å±äºå„ä¸ªçº¿ç¨‹ï¼›å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰å’Œä¸€çº§ç¼“å­˜ï¼ˆL1 cacheï¼‰åœ¨å•ä¸ªæµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰ä¸Šè¿è¡Œçš„çº¿ç¨‹ä¹‹é—´å…±äº«ï¼›å†å¾€ä¸Šæ˜¯æ‰€æœ‰æµå¼å¤šå¤„ç†å™¨å…±äº«çš„äºŒçº§ç¼“å­˜ï¼ˆL2 cacheï¼‰ï¼›æœ€åæ˜¯å…¨å±€å†…å­˜ï¼ˆGlobal Memoryï¼‰ï¼Œå®ƒæ˜¯ GPU ä¸Šæœ€å¤§çš„å†…å­˜ï¼ˆä¾‹å¦‚ H100 å®£ä¼ çš„ 80 GBï¼‰ï¼Œä½†ä¹Ÿæ˜¯è®¿é—®å’ŒæŸ¥è¯¢é€Ÿåº¦æœ€æ…¢çš„ã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/diving_primergpu2.svg)

Source: https://www.youtube.com/watch?v=ZQKMZIP3Fzg

GPU çš„ç›®æ ‡å°†æ˜¯é€šè¿‡åˆ©ç”¨è¿™ç§è®¡ç®—/å†…å­˜çš„åˆ†å±‚ç»„ç»‡ï¼Œåœ¨ GPU æ ¸å¿ƒä¸Šå¹¶è¡Œè¿è¡Œå°½å¯èƒ½å¤šçš„å·¥ä½œè´Ÿè½½ã€‚

åœ¨ GPU å†…æ ¸ä¸Šè¿è¡Œçš„ä¸€æ®µä»£ç ç§°ä¸ºå†…æ ¸ï¼ˆkernelï¼‰ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥ä½¿ç”¨ CUDA æˆ– Triton ç­‰é«˜çº§è¯­è¨€ç¼–å†™ï¼Œç„¶åç¼–è¯‘ä¸ºå¹¶è¡Œçº¿ç¨‹æ‰§è¡Œï¼ˆPTXï¼‰ï¼Œå³ NVIDIA GPU æ‰€ä½¿ç”¨çš„ä½çº§æ±‡ç¼–è¯­è¨€ã€‚

è¦è¿è¡Œå†…æ ¸ï¼Œä½ è¿˜éœ€è¦ä¸€ä¸ªç‰¹å®šçš„ä»£ç éƒ¨åˆ†ï¼Œç§°ä¸ºä¸»æœºä»£ç ï¼ˆhost codeï¼‰ï¼Œå®ƒåœ¨ CPU/ä¸»æœºä¸Šæ‰§è¡Œï¼Œè´Ÿè´£å‡†å¤‡æ•°æ®åˆ†é…ä»¥åŠåŠ è½½æ•°æ®å’Œä»£ç ã€‚

```python
// Host code                
void vecAdd(float* h_A, float *h_B, float *h_c, int n) {
    // Allocate vectors in device memory
    int size = n * sizeof(float);
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
}
```

ç”¨äºæ·»åŠ ä¸¤ä¸ªå‘é‡çš„ CUDA å†…æ ¸çš„ä¸»æœºä»£ç ã€‚æ”¹ç¼–è‡ª https://docs.nvidia.com/cuda/cuda-c-programming-guide/Â å’ŒÂ https://blog.codingconfessions.com/p/gpu-computing

```python
// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N)
        C[i] = A[i] + B[i];
}
```

åŒ…å«ä» https://docs.nvidia.com/cuda/cuda-c-programming-guide/Â å’ŒÂ https://blog.codingconfessions.com/p/gpu-computingÂ é€‚é…è€Œæ¥çš„çŸ¢é‡åŠ æ³•å†…æ ¸å®šä¹‰çš„è®¾å¤‡ä»£ç 

å†…æ ¸é€šå¸¸æŒ‰ä»¥ä¸‹æ–¹å¼è°ƒåº¦ï¼š

- çº¿ç¨‹è¢«åˆ†ç»„åˆ°å¤§å°ä¸º 32 çš„çº¿ç¨‹æŸï¼ˆwarpsï¼‰ä¸­ã€‚ä¸€ä¸ªçº¿ç¨‹æŸä¸­çš„æ‰€æœ‰çº¿ç¨‹åŒæ­¥æ‰§è¡ŒæŒ‡ä»¤ï¼Œä½†åœ¨æ•°æ®çš„ä¸åŒéƒ¨åˆ†ä¸Šæ‰§è¡Œã€‚
- çº¿ç¨‹æŸè¢«åˆ†ç»„åˆ°å¤§å°æ›´çµæ´»çš„æ›´å¤§å—ï¼ˆblockï¼‰ä¸­ï¼ˆä¾‹å¦‚å¤§å°ä¸º 256ï¼‰ï¼Œæ¯ä¸ªå—ä»ç„¶åˆ†é…ç»™ä¸€ä¸ªæµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰ã€‚ä¸€ä¸ª SM å¯ä»¥å¹¶è¡Œè¿è¡Œå¤šä¸ªå—ï¼Œç„¶è€Œï¼Œæ ¹æ®èµ„æºæƒ…å†µï¼Œå¹¶éæ‰€æœ‰å—éƒ½ä¼šç«‹å³åˆ†é…æ‰§è¡Œï¼Œæœ‰äº›å¯èƒ½ä¼šè¢«åˆ—å…¥ç­‰å¾…åˆ—è¡¨ç­‰å¾…èµ„æºã€‚

ä»è¿™äº›ç»†èŠ‚ä¸­è¦è®°ä½çš„ä¸»è¦ä¸€ç‚¹æ˜¯ï¼Œå­˜åœ¨å„ç§è§„æ¨¡å’Œåˆ†é…æ–¹é¢çš„é™åˆ¶ï¼ˆå„ç±»å†…å­˜çš„å¤§å°ã€çº¿ç¨‹æŸä¸­å¹¶å‘å—å’Œçº¿ç¨‹çš„æ•°é‡ï¼‰ï¼Œè¦æœ€æœ‰æ•ˆåœ°ä½¿ç”¨ GPU æ¶æ„ï¼Œå°±éœ€è¦è€ƒè™‘è¿™äº›é™åˆ¶ã€‚

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦è¾¾åˆ°è¿™ç§ç²¾åº¦æ°´å¹³ï¼Œå¹¶ä¸”å¹¸è¿çš„æ˜¯ï¼Œä½ å¯ä»¥å¤ç”¨ç¤¾åŒºå…¶ä»–æˆå‘˜å‡†å¤‡å¥½çš„å†…æ ¸å’Œä»£ç ã€‚ä½†æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬éƒ½æƒ³ç»™ä½ ä¸€ä¸ªå…³äºå¦‚ä½•å¼€å§‹ä½¿ç”¨å†…æ ¸çš„å…¥é—¨æŒ‡å¯¼ï¼

### 10.2 å¦‚ä½•é€šè¿‡å†…æ ¸æé«˜æ€§èƒ½ï¼Ÿ

å¦‚æœæ‚¨æƒ³æ·»åŠ ä¸€ä¸ªç¼ºä¹ä¼˜åŒ–å†…æ ¸çš„æ–°æ“ä½œï¼Œæˆ–è€…åŠ é€Ÿç°æœ‰çš„ PyTorch å‡½æ•°ï¼Œä»å¤´ç¼–å†™å†…æ ¸ä¼¼ä¹æ˜¯æœ€ç›´æ¥çš„é€”å¾„ã€‚ç„¶è€Œï¼Œä»å¤´åˆ›å»ºé«˜æ€§èƒ½çš„ CUDA å†…æ ¸éœ€è¦ä¸°å¯Œçš„ç»éªŒå’Œé™¡å³­çš„å­¦ä¹ æ›²çº¿ã€‚é€šå¸¸æ›´å¥½çš„å…¥é—¨æ–¹æ³•æ˜¯åˆ©ç”¨ `torch.compile`ï¼Œå®ƒé€šè¿‡æ•è·æ‚¨çš„æ“ä½œå¹¶ç”Ÿæˆæ›´ä½çº§åˆ«ã€é«˜æ€§èƒ½çš„ Triton å†…æ ¸æ¥åŠ¨æ€ä¼˜åŒ– PyTorch ä»£ç ã€‚

å‡è®¾ä½ æƒ³ä¸ºä¸€ä¸ªåä¸ºæŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆExponential Linear Unitï¼‰çš„æ¿€æ´»å‡½æ•°ç¼–å†™ä¸€ä¸ªå†…æ ¸ã€‚$$
\text{ELU}(x) = 
\begin{cases} 
e^x - 1 & \text{if } x < 0 \\
x & \text{if } x \geq 0 
\end{cases}$$
ä½ å¯ä»¥ä»ä¸€ä¸ªç®€å•çš„ PyTorch å®ç°å¼€å§‹ï¼Œç„¶åç›´æ¥åœ¨é¡¶éƒ¨æ·»åŠ  `@torch.compile` è£…é¥°å™¨ï¼š

```python
@torch.compile
def elu(x, alpha=1.0):
    return torch.where(x < 0, alpha * (torch.exp(x) - 1), x)
```

ç¼–è¯‘ç‰ˆå’Œéç¼–è¯‘ç‰ˆä¹‹é—´çš„å·®å¼‚éå¸¸æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æˆ‘ä»¬ä»…ä»…æ·»åŠ äº†ä¸€ä¸ªè£…é¥°å™¨ã€‚è¿™ç§æ˜¾è‘—çš„å·®å¼‚åœ¨ä¸‹å›¾ä¸­å¾—åˆ°äº†è¯´æ˜ï¼ˆ$N$ ä¸ºåˆ—æ•°ï¼‰ã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/torch-compile-triton.png)

ç„¶è€Œï¼Œå¦‚æœè¿™ç§æ€§èƒ½æå‡ä¸å¤Ÿï¼Œä½ å¯ä»¥è€ƒè™‘å®ç° Triton å†…æ ¸ã€‚ä½œä¸ºèµ·ç‚¹ï¼Œä½ å¯ä»¥çœ‹çœ‹ç”± `@torch.compile` ç”Ÿæˆçš„ triton å†…æ ¸ã€‚ä¸ºæ­¤ï¼Œä½ åªéœ€å°†ç¯å¢ƒå˜é‡ `TORCH_LOGS` è®¾ç½®ä¸º `"output_code"`ï¼š

```bash
export TORCH_LOGS="output_code"
```

è¿è¡Œå¸¦æœ‰ `@torch.compile` è£…é¥°å™¨çš„ Python è„šæœ¬åï¼Œå®ƒå°†ç”Ÿæˆå¹¶è¾“å‡ºç›¸åº”çš„ Triton å†…æ ¸ï¼Œåœ¨æœ¬ä¾‹ä¸­ä¸ºï¼š

```python
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 100000000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 0.0
    tmp2 = tmp0 < tmp1
    tmp3 = tl_math.exp(tmp0)
    tmp4 = 1.0
    tmp5 = tmp3 - tmp4
    tmp6 = tl.where(tmp2, tmp5, tmp0)
    tl.store(out_ptr0 + (x0), tmp6, xmask)
```

ä¸ºäº†æé«˜å¯è¯»æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä¿®æ”¹å˜é‡åã€æ·»åŠ æ³¨é‡Šå¹¶è¿›è¡Œä¸€äº›å°çš„è°ƒæ•´ï¼ˆæˆ–è€…è®©å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæˆ‘ä»¬åšè¿™ä»¶äº‹ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
@triton.jit
def elu_kernel(input_ptr, output_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
    # Calculate the starting index for this block
    block_start = tl.program_id(0) * BLOCK_SIZE
    # Create an array of indices for this block
    block_indices = block_start + tl.arange(0, BLOCK_SIZE)[:]
    # Create a mask to ensure only valid indices are processed
    valid_mask = block_indices < num_elements
    # Load input values from the input pointer based on valid indices
    input_values = tl.load(input_ptr + block_indices, valid_mask)
    # Define the ELU parameters
    zero_value = 0.0  # Threshold for ELU activation
    negative_mask = input_values < zero_value
    exp_values = tl.math.exp(input_values)
    # Define the ELU output shift
    one_value = 1.0
    shifted_exp_values = exp_values - one_value

    output_values = tl.where(negative_mask, shifted_exp_values, input_values)

    # Store the computed output values back to the output pointer
    tl.store(output_ptr + block_indices, output_values, valid_mask)
```

åœ¨è¿™é‡Œï¼Œ`tl.program_id(0)` æä¾›äº†ä¸€ä¸ªå”¯ä¸€çš„å— IDï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥ç¡®å®šè¯¥å—å°†å¤„ç†æ•°æ®çš„å“ªä¸ªéƒ¨åˆ†ã€‚ä½¿ç”¨è¿™ä¸ªå— IDï¼Œ`block_start` è®¡ç®—æ¯ä¸ªå—éƒ¨åˆ†çš„èµ·å§‹ç´¢å¼•ï¼Œè€Œ `block_indices` æŒ‡å®šè¯¥éƒ¨åˆ†å†…çš„ç´¢å¼•èŒƒå›´ã€‚ä¸€ä¸ª `valid_mask` ç¡®ä¿åªå¤„ç† `num_elements` å†…çš„ç´¢å¼•ï¼Œä½¿ç”¨Â `tl.load`Â å®‰å…¨åœ°åŠ è½½æ•°æ®ã€‚ç„¶ååº”ç”¨ ELU å‡½æ•°ï¼Œæ ¹æ®å€¼æ˜¯å¦ä¸ºè´Ÿå¯¹å…¶è¿›è¡Œä¿®æ”¹ï¼Œå¹¶å°†ç»“æœä½¿ç”¨ `tl.store` å†™å›å†…å­˜ã€‚

å½“æˆ‘ä»¬ä½¿ç”¨ `triton.testing.Benchmark` å¯¹ç”Ÿæˆçš„å†…æ ¸è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹æ€§èƒ½ï¼š

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/torch-compile-triton-kernel.png)

è¿™ä¸ªç‹¬ç«‹å†…æ ¸ç”šè‡³åœ¨è¾ƒå°å°ºå¯¸ä¸Šç›¸æ¯” `@torch.compile` å±•ç¤ºäº†æ›´ä¼˜çš„æ€§èƒ½ï¼Œä½†è¿™å¾ˆå¯èƒ½åªæ˜¯`torch.compile` ç¼–è¯‘æ—¶é—´çš„ä¸€ä¸ªå‡è±¡ã€‚æ— è®ºå¦‚ä½•ï¼Œä¸å…¶ä»å¤´å¼€å§‹ï¼Œä¸å¦‚è®°ä½ä½ å¯ä»¥ä»è¿™äº›ç”Ÿæˆçš„å†…æ ¸å¼€å§‹ï¼Œå¹¶å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¼˜åŒ–å…¶æ€§èƒ½ä¸Šï¼Œä»è€Œåœ¨è¿‡ç¨‹ä¸­èŠ‚çœå¤§é‡æ—¶é—´ã€‚

å³ä½¿åœ¨ Triton ä¸­ï¼Œæœ‰æ—¶ç”±äºå¤„ç†ä½çº§ç»†èŠ‚ï¼ˆå¦‚å…±äº«å†…å­˜å’Œæµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰å†…çš„è°ƒåº¦ï¼‰çš„è¯­è¨€é™åˆ¶ï¼Œæˆ‘ä»¬ä¹Ÿæ— æ³•å®Œå…¨å‘æŒ¥è®¾å¤‡çš„å³°å€¼æ€§èƒ½ã€‚Triton çš„èƒ½åŠ›ä»…é™äºå—ä»¥åŠè·¨ SM çš„å—è°ƒåº¦ã€‚è‹¥è¦è·å¾—æ›´æ·±å…¥çš„æ§åˆ¶ï¼Œæ‚¨éœ€è¦ç›´æ¥åœ¨ CUDA ä¸­å®ç°å†…æ ¸ï¼Œåœ¨é‚£é‡Œæ‚¨å°†èƒ½å¤Ÿè®¿é—®æ‰€æœ‰åº•å±‚çš„ä½çº§ç»†èŠ‚ã€‚

æ·±å…¥åˆ° CUDA é¢†åŸŸï¼Œå¯ä»¥é‡‡ç”¨å¤šç§æŠ€æœ¯æ¥æé«˜å†…æ ¸çš„æ•ˆç‡ã€‚è¿™é‡Œæˆ‘ä»¬ä»…ä»‹ç»å‡ ç§ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ä»¥å‡å°‘å»¶è¿Ÿï¼›ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨é¢‘ç¹è®¿é—®çš„æ•°æ®ï¼›ä»¥åŠç®¡ç†çº¿ç¨‹å·¥ä½œè´Ÿè½½ä»¥å°½é‡å‡å°‘ç©ºé—²æ—¶é—´ã€‚

åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶ CUDA ç¤ºä¾‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬è§è¿‡çš„é‚£äº›å¯è®©æˆ‘ä»¬ç¼–å†™å†…æ ¸ä»£ç ä»¥åœ¨ GPU ä¸Šæ‰§è¡ŒæŒ‡ä»¤çš„å·¥å…·ï¼š

1. Pytorchï¼šç®€å•ä½†é€Ÿåº¦æ…¢
2. torch.compileï¼šç®€å•ã€å¿«é€Ÿï¼Œä½†ä¸çµæ´»
3. tritonï¼šæ›´éš¾ã€æ›´å¿«ä¸”æ›´çµæ´»
4. CUDAï¼šæœ€éš¾ã€æœ€å¿«ä¸”æœ€çµæ´»ï¼ˆå¦‚æœä½ èƒ½æ­£ç¡®è¿ç”¨çš„è¯ï¼‰

è®©æˆ‘ä»¬æ¥è°ˆè°ˆåœ¨ CUDA ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„æœ€å¸¸è§çš„æŠ€æœ¯ä¹‹ä¸€ï¼šä¼˜åŒ–å†…å­˜è®¿é—®ã€‚GPU ä¸­çš„å…¨å±€å†…å­˜ï¼ˆæˆ‘ä»¬ä¸Šé¢å›¾è¡¨ä¸­æœ€å¤§çš„å†…å­˜ï¼‰ä¸ç¼“å­˜ç›¸æ¯”å…·æœ‰è¾ƒé•¿çš„å»¶è¿Ÿå’Œè¾ƒä½çš„å¸¦å®½ï¼Œè¿™é€šå¸¸ä¼šæˆä¸ºå¤§å¤šæ•°åº”ç”¨ç¨‹åºçš„ä¸»è¦ç“¶é¢ˆã€‚é«˜æ•ˆåœ°ä»å…¨å±€å†…å­˜ä¸­è®¿é—®æ•°æ®å¯ä»¥å¤§å¹…æé«˜æ€§èƒ½ã€‚

#### 10.2.1 å†…å­˜åˆå¹¶

ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨å…¨å±€å†…å­˜çš„å¸¦å®½ï¼Œç†è§£å…¶æ¶æ„è‡³å…³é‡è¦ã€‚åœ¨ CUDA è®¾å¤‡ä¸­ï¼Œå…¨å±€å†…å­˜æ˜¯é€šè¿‡åŠ¨æ€éšæœºå­˜å–å­˜å‚¨å™¨ï¼ˆDRAMï¼‰å®ç°çš„ã€‚

å†…å­˜åˆå¹¶åˆ©ç”¨äº† DRAM åœ¨è®¿é—®å†…å­˜åœ°å€æ—¶ä»¥çªå‘æ–¹å¼æˆ–è¿ç»­å†…å­˜åœ°å€èŒƒå›´çš„æ–¹å¼ä¼ è¾“æ•°æ®çš„ç‰¹ç‚¹ã€‚æ¯æ¬¡è®¿é—® DRAM ä½ç½®æ—¶ï¼ŒåŒ…æ‹¬æ‰€è¯·æ±‚ä½ç½®åœ¨å†…çš„ä¸€ç³»åˆ—è¿ç»­ä½ç½®ä¼šè¢« DRAM èŠ¯ç‰‡ä¸­çš„å¤šä¸ªä¼ æ„Ÿå™¨å¹¶è¡Œè¯»å–ã€‚ä¸€æ—¦è¯»å–ï¼Œè¿™äº›æ•°æ®å°±å¯ä»¥ä½œä¸ºçªå‘å¿«é€Ÿä¼ è¾“åˆ°å¤„ç†å™¨ã€‚åœ¨ CUDA ä¸­ï¼Œåˆå¹¶åˆ©ç”¨è¿™ç§çªå‘è¡Œä¸ºï¼Œé€šè¿‡ç¡®ä¿ä¸€ä¸ª warp ä¸­çš„çº¿ç¨‹ï¼ˆ32 ä¸ªä»¥é”æ­¥æ–¹å¼æ‰§è¡Œç›¸åŒæŒ‡ä»¤çš„çº¿ç¨‹ï¼ˆSIMDï¼‰ï¼‰è®¿é—®è¿ç»­çš„å†…å­˜ä½ç½®ï¼Œæ¥æœ€å¤§åŒ–å†…å­˜è®¿é—®æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœçº¿ç¨‹ 0 è®¿é—®ä½ç½® Mï¼Œçº¿ç¨‹ 1 è®¿é—® M+1ï¼Œçº¿ç¨‹ 2 è®¿é—® M+2ï¼Œä¾æ­¤ç±»æ¨ï¼ŒGPU ç¡¬ä»¶ä¼šå°†è¿™äº›è¯·æ±‚åˆå¹¶æˆ–ç»„åˆæˆä¸€ä¸ªå¤§çš„ã€é«˜æ•ˆçš„ DRAM çªå‘è®¿é—®è¯·æ±‚ï¼Œè€Œä¸æ˜¯å•ç‹¬å¤„ç†æ¯ä¸ªè®¿é—®ã€‚

è®©æˆ‘ä»¬ä»¥çŸ©é˜µä¹˜æ³•ä¸ºä¾‹ã€‚ä¸€ç§ç®€å•ç›´æ¥çš„æ–¹æ³•æ˜¯è®©æ¯ä¸ªçº¿ç¨‹è®¡ç®—è¾“å‡ºçŸ©é˜µçš„ä¸€ä¸ªå…ƒç´ ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```clike
__global__ void matmul_naive(int M, int N, int K, const float *A, const float *B, float *C) {
    const uint x = blockIdx.x * blockDim.x + threadIdx.x;
    const uint y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < M && y < N) {
        float tmp = 0.0;
        for (int i = 0; i < K; ++i) {
            tmp += A[x * K + i] * B[i * N + y];
        }
        C[x * N + y] = tmp;
    }
}
```

è¿™é‡Œæœ‰ä¸€ä¸ªæ¥è‡ªè¿™ç¯‡ç²¾å½©[åšå®¢æ–‡ç« ](https://siboehm.com/articles/22/CUDA-MMM)çš„å…³äºå†…æ ¸çš„ä¼˜ç§€å¯è§†åŒ–ç¤ºä¾‹ï¼š

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing.png)

ç„¶è€Œï¼Œå½“ä½¿ç”¨åƒ ncu è¿™æ ·çš„å·¥å…·å¯¹è¿™ä¸ªå†…æ ¸è¿›è¡Œæ€§èƒ½åˆ†ææ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›é—®é¢˜ï¼ŒåŒ…æ‹¬å†…å­˜ååé‡ä½å’Œå†…å­˜è®¿é—®æœªåˆå¹¶ã€‚
![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing2.png)Â ![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing3.png)

åŸå› åœ¨äºï¼Œåœ¨è¯¥å†…æ ¸ä¸­ï¼ŒåŒä¸€çº¿ç¨‹å—ä¸­çº¿ç¨‹ ID ä¸º (0, 0) å’Œ (1, 0) çš„ä¸¤ä¸ªçº¿ç¨‹ï¼ˆå®ƒä»¬æœ€ç»ˆä¼šè¢«åˆ’åˆ†åˆ°åŒä¸€ä¸ªçº¿ç¨‹æŸä¸­ï¼‰éƒ½ä¼šä»çŸ©é˜µ B çš„åŒä¸€åˆ—åŠ è½½æ•°æ®ï¼Œä½†ä»çŸ©é˜µ A çš„ä¸åŒè¡ŒåŠ è½½æ•°æ®ã€‚ç”±äºçŸ©é˜µå…ƒç´ æ˜¯ä»¥è¡Œä¼˜å…ˆé¡ºåºå­˜å‚¨çš„ï¼ˆå³è¡Œå…ƒç´ å­˜å‚¨åœ¨è¿ç»­çš„å†…å­˜åœ°å€ä¸­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œå› æ­¤åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ i = 0 æ—¶ï¼Œçº¿ç¨‹ (0, 0) ä¼šåŠ è½½ Aâ‚€,â‚€ ï¼Œè€Œçº¿ç¨‹ (1, 0) ä¼šåŠ è½½ Aâ‚,â‚€ ã€‚è¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­å¹¶éç´§å¯†ç›¸é‚»å­˜å‚¨ï¼Œå¹¶ä¸”åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šå­˜åœ¨è¿™ç§æœªå¯¹é½çš„æƒ…å†µï¼Œä»è€Œæ— æ³•å®ç°å†…å­˜è®¿é—®çš„åˆå¹¶ ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing4.png)

ä¸ºäº†æé«˜æˆ‘ä»¬å†…æ ¸çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥å°†åæ ‡ x å’Œ y çš„è®¡ç®—æ–¹å¼æ›´æ”¹ä¸ºä»¥ä¸‹æ–¹å¼ï¼š

```clike
const int x = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);
const int y = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);

if (x < M && y < N) {
float tmp = 0.0;
for (int i = 0; i < K; ++i) {
    tmp += A[x * K + i] * B[i * N + y];
}
C[x * N + y] = tmp;
}
```

æˆ‘ä»¬æ”¹ç”¨ä¸€ç»´å—ï¼Œå¹¶é‡æ–°å®šä¹‰ç¡®å®š `x` å’Œ `y` å€¼çš„æ–¹å¼ã€‚åœ¨è¿™ç§æ–°æ–¹æ³•ä¸­ï¼ŒåŒä¸€çº¿ç¨‹æŸï¼ˆ`threadIdx.x` å€¼ç›¸è¿‘ï¼‰å†…çš„çº¿ç¨‹å°†å…±äº«ç›¸åŒçš„ `x` å€¼ï¼Œä½†å…·æœ‰ä¸åŒçš„ `y` å€¼ã€‚è¿™æ„å‘³ç€å®ƒä»¬å°†åŠ è½½çŸ©é˜µ `A` çš„åŒä¸€è¡Œï¼Œä½†åŠ è½½çŸ©é˜µ `B` çš„ä¸åŒåˆ—ã€‚å› æ­¤ï¼Œå¯¹äºè¡Œä¸»åºçŸ©é˜µï¼Œå†…å­˜è®¿é—®å¯ä»¥å®ç°åˆå¹¶ã€‚

å½“æˆ‘ä»¬å¯¹æ–°å†…æ ¸è¿›è¡Œåˆ†ææ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å…³äºéåˆå¹¶å†…å­˜è®¿é—®çš„è­¦å‘Šæ¶ˆå¤±äº†ï¼Œå¹¶ä¸” GPU çš„å†…å­˜ååé‡æé«˜äº†å¤§çº¦ 10 å€ã€‚

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing5.png)

æˆ‘ä»¬è¿˜æ³¨æ„åˆ°å†…æ ¸çš„æ‰§è¡Œæ—¶é—´å‡å°‘äº† 10 å€ï¼å¤ªç¥å¥‡äº†ã€‚  

ç°åœ¨è®©æˆ‘ä»¬æ¥ä»‹ç»å¦ä¸€ç§åœ¨æ–‡çŒ®ä¸­ç»å¸¸æåˆ°çš„æŠ€æœ¯ï¼šå¹³é“ºï¼ˆtilingï¼‰ã€‚

#### 10.2.2 å¹³é“ºï¼ˆtilingï¼‰

å¹³é“ºæ˜¯ä¸€ç§åˆ©ç”¨ *å…±äº«å†…å­˜* æ¥ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼çš„æŠ€æœ¯ã€‚æ­£å¦‚æˆ‘ä»¬ä¸Šé¢æåˆ°çš„ï¼Œå…±äº«å†…å­˜æ˜¯ä¸€ç§å°å‹ã€å¿«é€Ÿçš„å†…å­˜ï¼Œå¯ç”±ä¸€ä¸ªå—å†…çš„æ‰€æœ‰çº¿ç¨‹è®¿é—®ã€‚å®ƒå…è®¸æ•°æ®è¢«å¤šä¸ªçº¿ç¨‹é‡å¤ä½¿ç”¨ï¼Œå‡å°‘äº†ä»è¾ƒæ…¢çš„å…¨å±€å†…å­˜ä¸­é‡å¤åŠ è½½æ•°æ®çš„éœ€è¦ã€‚

ä¾‹å¦‚ï¼Œåœ¨çŸ©é˜µä¹˜æ³•ä¸­ï¼Œä¸€ä¸ªå—ä¸­çš„æ¯ä¸ªçº¿ç¨‹å¯èƒ½éœ€è¦ä¸¤ä¸ªçŸ©é˜µï¼ˆå‡è®¾ä¸ºAå’ŒBï¼‰çš„å…ƒç´ ã€‚å¦‚æœæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹åœ°ä»å…¨å±€å†…å­˜åŠ è½½å…¶æ‰€éœ€çš„è¡Œå’Œåˆ—ï¼Œé‚£ä¹ˆç”±äºä¸€ä¸ªå—ä¸­çš„å¤šä¸ªçº¿ç¨‹ä¼šè®¿é—®é‡å çš„æ•°æ®ï¼Œæœ€ç»ˆä¼šäº§ç”Ÿè®¸å¤šå†—ä½™çš„åŠ è½½æ“ä½œã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¹³é“ºï¼ˆtilingï¼‰æŠ€æœ¯ï¼Œå°†Aå’ŒBçš„ä¸€ä¸ªå—ï¼ˆæˆ–å¹³é“ºå—ï¼‰ä¸€æ¬¡æ€§åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ï¼Œè¿™æ ·è¯¥å—ä¸­çš„æ‰€æœ‰çº¿ç¨‹å°±å¯ä»¥é‡å¤ä½¿ç”¨ç›¸åŒçš„å…±äº«æ•°æ®ã€‚

åœ¨å¹³é“ºæ–¹æ³•ä¸­ï¼Œæ¯æ¬¡è¿­ä»£éƒ½æ¶‰åŠå—å†…çš„æ‰€æœ‰çº¿ç¨‹ååŒåŠ è½½ä¸¤ä¸ªå¹³é“ºå—â€”â€”ä¸€ä¸ªæ¥è‡ªçŸ©é˜µ Aï¼Œå¦ä¸€ä¸ªæ¥è‡ªçŸ©é˜µ Bâ€”â€”åˆ°å…±äº«å†…å­˜ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œçº¿ç¨‹åŠ è½½çŸ©é˜µ A çš„ä¸€ä¸ªå¹³é“ºå—ï¼ˆå¤§å°ä¸º `BLOCK_SIZE_M` ä¹˜ä»¥`BLOCK_SIZE_K`ï¼‰å’ŒçŸ©é˜µ B çš„ä¸€ä¸ªå¹³é“ºå—ï¼ˆå¤§å°ä¸º `BLOCK_SIZE_K` ä¹˜ä»¥ `BLOCK_SIZE_N`ï¼‰ã€‚ä¸€æ—¦å¹³é“ºå—è¿›å…¥å…±äº«å†…å­˜ï¼Œçº¿ç¨‹å°±åœ¨è¿™äº›å¹³é“ºå—ä¸Šæ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œç”±äºæ‰€æœ‰å¿…è¦çš„æ•°æ®éƒ½èƒ½å¿«é€Ÿè®¿é—®ï¼Œä»è€Œå®ç°é«˜æ•ˆè®¡ç®—ã€‚å¹³é“ºå—ä¹˜æ³•çš„ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªç´¯ç§¯çŸ©é˜µä¸­ï¼Œè¯¥çŸ©é˜µä¿å­˜ä¸­é—´ç»“æœã€‚æ¯æ¬¡è¿­ä»£åï¼Œå½“å‰å¹³é“ºå—ä¹˜æ³•çš„ç»“æœéƒ½ä¼šæ·»åŠ åˆ°è¿™ä¸ªç´¯ç§¯çŸ©é˜µä¸­ï¼Œç›´åˆ°å¤„ç†å®Œä¸¤ä¸ªçŸ©é˜µçš„æ‰€æœ‰å¹³é“ºå—ä¸ºæ­¢ã€‚

![image.png|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tiling.png)

FromÂ [https://cnugteren.github.io/tutorial/pages/page4.html](https://cnugteren.github.io/tutorial/pages/page4.html)

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹åœ¨å®ç°ä¸­ä½ éœ€è¦ç†è§£çš„é‡è¦éƒ¨åˆ†ï¼š

```clike
// Set pointers to the starting elements
A += blockRow * TILE_SIZE * K; // Start at row = blockRow, column = 0
B += blockCol * TILE_SIZE; // Start at row = 0, column = blockCol
C += blockRow * TILE_SIZE * N + blockCol * TILE_SIZE; // Start at row = blockRow, column = blockCol
float sum = 0.0;
// The outer loop moves through tiles of A (across columns) and B (down rows)
for (int tileIdx = 0; tileIdx < K; tileIdx += TILE_SIZE) {
sharedA[localRow * TILE_SIZE + localCol] = A[localRow * K + localCol];
sharedB[localRow * TILE_SIZE + localCol] = B[localRow * N + localCol];

// Ensure all threads in the block have completed data loading
__syncthreads();

// Shift pointers to the next tile
A += TILE_SIZE;
B += TILE_SIZE * N;

// Compute the partial dot product for this tile
for (int i = 0; i < TILE_SIZE; ++i) {
    sum += sharedA[localRow * TILE_SIZE + i] * sharedB[i * TILE_SIZE + localCol];
}
// Synchronize again to prevent any thread from loading new data
// into shared memory before others have completed their calculations
__syncthreads();
}
C[localRow * N + localCol] = sum;
```

ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬è€ƒè™‘é‡‡ç”¨æ–¹å½¢å¹³é“ºã€‚

æ¯ä¸ªçº¿ç¨‹é¦–å…ˆä»çŸ©é˜µ A å’ŒçŸ©é˜µ B ä¸­å„åŠ è½½ä¸€ä¸ªå…ƒç´ åˆ°å…±äº«å†…å­˜ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡å°† `threadIdx.x` èµ‹å€¼ä¸ºå±€éƒ¨åˆ—ç´¢å¼•ï¼ˆlocalColï¼‰ï¼Œå®ç°åˆå¹¶å†…å­˜è®¿é—®å˜å¾—ç®€å•ï¼ŒåŒä¸€ warp ä¸­çš„çº¿ç¨‹å°†è®¿é—®ä¸¤ä¸ªçŸ©é˜µçš„ç›¸é‚»å…ƒç´ ã€‚åœ¨å—ä¸­çš„æ¯ä¸ªçº¿ç¨‹å®Œæˆå°†å…¶å…ƒç´ åŠ è½½åˆ°å…±äº«å†…å­˜åï¼ˆé€šè¿‡è°ƒç”¨ `__syncthreads()` ç¡®ä¿ï¼‰ï¼Œå®ƒä»¬ç»§ç»­è®¡ç®—ä¸¤ä¸ªåˆ†å—çš„ç‚¹ç§¯ã€‚ä¸€æ—¦çº¿ç¨‹éå†å®Œæ‰€æœ‰åˆ†å—â€”â€”çŸ©é˜µ A æ°´å¹³æ–¹å‘å’ŒçŸ©é˜µ B å‚ç›´æ–¹å‘â€”â€”æœ€ç»ˆçš„å’Œå°†å­˜å‚¨åœ¨çŸ©é˜µ C çš„ç›¸åº”ä½ç½®ã€‚

å½“ä½¿ç”¨ ncu å¯¹è¿™ä¸ªå†…æ ¸è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å†…å­˜ååé‡å¢åŠ åˆ°äº†410 Gb/sï¼Œå†…æ ¸æ‰§è¡Œæ—¶é—´å‡å°‘äº†çº¦ 43%ï¼Œæ€§èƒ½è¾¾åˆ°äº†çº¦ 6.6 TFLOPsã€‚

#### 10.2.3 çº¿ç¨‹ç²—åŒ–

å¹³é“ºæŠ€æœ¯æ˜¾è‘—æé«˜äº†æˆ‘ä»¬å†…æ ¸çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨åˆ†æç”¨äºé‡åŒ–æ¯ä¸ªçŠ¶æ€æ‰€èŠ±è´¹å‘¨æœŸæ•°çš„çº¿ç¨‹æŸçŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä»¥ä¸‹æƒ…å†µï¼š

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/threadcoarsening.png)

è¿™äº›ç¥ç§˜çš„çŠ¶æ€åç§°çš„å«ä¹‰å¯ä»¥åœ¨ [NVIDIA çš„æ€§èƒ½åˆ†ææŒ‡å—](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference)çš„â€œçº¿ç¨‹æŸåœæ»åŸå› â€éƒ¨åˆ†æ‰¾åˆ°ã€‚åœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥è¯»åˆ°ï¼š

*`â€œsmsp__pcsamp_warps_issue_stalled_mio_throttle`ï¼šçº¿ç¨‹æŸå› ç­‰å¾…å†…å­˜è¾“å…¥/è¾“å‡ºï¼ˆMIOï¼‰æŒ‡ä»¤é˜Ÿåˆ—ä¸æ»¡è€Œåœæ»ã€‚åœ¨ MIO æµæ°´çº¿æåº¦ä½¿ç”¨çš„æƒ…å†µä¸‹ï¼ˆåŒ…æ‹¬ç‰¹æ®Šæ•°å­¦æŒ‡ä»¤ã€åŠ¨æ€åˆ†æ”¯ä»¥åŠå…±äº«å†…å­˜æŒ‡ä»¤ï¼‰ï¼Œè¿™ç§åœæ»åŸå› ä¼šå¢å¤šã€‚å½“ç”±å…±äº«å†…å­˜è®¿é—®å¼•èµ·æ—¶ï¼Œå°è¯•ä½¿ç”¨æ›´å°‘ä½†æ›´å®½çš„åŠ è½½æ“ä½œå¯ä»¥å‡è½»æµæ°´çº¿å‹åŠ›ã€‚â€*

æ‰€ä»¥çœ‹æ¥ï¼Œå¼¯æ›²ï¼ˆwarpï¼Œæ­¤å¤„å¯èƒ½ä¸ºç‰¹å®šæœ¯è¯­ï¼Œå¦‚ CUDA ç¼–ç¨‹ä¸­çš„çº¿ç¨‹æŸæ¦‚å¿µï¼‰åœ¨ç­‰å¾…å…±äº«å†…å­˜è®¿é—®è¿”å›æ—¶å¤„äºåœæ»çŠ¶æ€ï¼ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸€ç§ç§°ä¸ºâ€œçº¿ç¨‹ç²—åŒ–â€ï¼ˆThread Coarseningï¼‰çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ¶‰åŠå°†å¤šä¸ªçº¿ç¨‹åˆå¹¶æˆä¸€ä¸ªç²—åŒ–åçš„çº¿ç¨‹ã€‚è¿™å°†æ˜¾è‘—å‡å°‘å…±äº«å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œå› ä¸ºæ¯ä¸ªç²—åŒ–åçš„çº¿ç¨‹å¯ä»¥å¤„ç†å¤šä¸ªè¾“å‡ºå…ƒç´ ã€‚

è®©æˆ‘ä»¬ç®€è¦æ¢è®¨ä¸€ä¸‹åœ¨ç¼–å†™æˆ–æ”¹è¿›è‡ªå®šä¹‰å†…æ ¸æ—¶çš„æœ€åä¸€ä¸ªé‡è¦è€ƒè™‘å› ç´ ï¼šæœ€å°åŒ–æ§åˆ¶åˆ†æ­§ã€‚

#### 10.2.4 æœ€å°åŒ–æ§åˆ¶åˆ†æ­§

æµå¼å¤šå¤„ç†å™¨ï¼ˆSMï¼‰æ—¨åœ¨ä½¿ç”¨å•æŒ‡ä»¤å¤šæ•°æ®ï¼ˆSIMDï¼‰æ¨¡å‹æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹æŸä¸­çš„æ‰€æœ‰çº¿ç¨‹ã€‚è¿™æ„å‘³ç€åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»ï¼Œä¸€ä¸ªæŒ‡ä»¤ä¼šåŒæ—¶è¢«è·å–å¹¶æ‰§è¡Œï¼Œä»¥ç”¨äºè¯¥çº¿ç¨‹æŸå†…çš„æ‰€æœ‰çº¿ç¨‹ã€‚å½“æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹æŸæ—¶ï¼Œå…¶ä¸­çš„çº¿ç¨‹æ“ä½œæ•°æ®çš„ä¸åŒéƒ¨åˆ†ï¼Œä½†éµå¾ªç›¸åŒçš„æŒ‡ä»¤ï¼Œå› æ­¤å¾—åå•æŒ‡ä»¤å¤šæ•°æ®ã€‚SIMDçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶æ•ˆç‡ï¼›è´Ÿè´£æŒ‡ä»¤è·å–å’Œåˆ†æ´¾çš„æ§ä»¶ç¡¬ä»¶åœ¨å¤šä¸ªæ‰§è¡Œå•å…ƒä¹‹é—´å…±äº«ã€‚è¿™ç§è®¾è®¡æœ€å°åŒ–äº†ä¸æ§åˆ¶åŠŸèƒ½ç›¸å…³çš„ç¡¬ä»¶å¼€é”€ï¼Œä½¿æ›´å¤§ä¸€éƒ¨åˆ†ç¡¬ä»¶ä¸“æ³¨äºæé«˜ç®—æœ¯ååé‡ã€‚

å½“åŒä¸€warpä¸­çš„çº¿ç¨‹é‡‡å–ä¸åŒçš„æ‰§è¡Œè·¯å¾„æ—¶ï¼Œå°±ä¼šå‘ç”Ÿæ§åˆ¶åˆ†æ­§ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¡ä»¶è¯­å¥ï¼ˆå¦‚ `if` è¯­å¥ï¼‰å¯¼è‡´ä¸€äº›çº¿ç¨‹æ‰§è¡Œä¸€ä¸ªä»£ç å—ï¼Œè€Œå…¶ä»–çº¿ç¨‹æ‰§è¡Œå¦ä¸€ä¸ªä»£ç å—ï¼Œé‚£ä¹ˆè¯¥warpå¿…é¡»å¯¹è¿™äº›æ‰§è¡Œè¿›è¡Œä¸²è¡ŒåŒ–å¤„ç†ï¼Œä»è€Œå¯¼è‡´ä¸€äº›çº¿ç¨‹ç©ºé—²ç­‰å¾…å…¶ä»–çº¿ç¨‹å®Œæˆã€‚ä¸ºäº†å°½é‡å‡å°‘æ§åˆ¶åˆ†æ­§ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡å†…æ ¸ï¼Œä»¥ç¡®ä¿åŒä¸€warpä¸­çš„çº¿ç¨‹éµå¾ªç›¸åŒçš„æ‰§è¡Œè·¯å¾„ã€‚è¿™å¯ä»¥é€šè¿‡é‡æ„ä»£ç ä»¥å‡å°‘åˆ†æ”¯ã€ä½¿ç”¨ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éµå¾ªç›¸ä¼¼æ‰§è¡Œè·¯å¾„çš„æ•°æ®ç»“æ„ï¼Œæˆ–è€…é‡‡ç”¨è¯¸å¦‚é¢„æµ‹æ‰§è¡Œä¹‹ç±»çš„æŠ€æœ¯æ¥å®ç°ã€‚

---

æˆ‘ä»¬å·²ç»ä»‹ç»äº†ç¼–å†™è‡ªå®šä¹‰å†…æ ¸ä»¥åŠæé«˜ GPU æ“ä½œçš„æ€§èƒ½å’Œå†…å­˜å ç”¨çš„ä¸€äº›ä¸»è¦è€ƒè™‘å› ç´ ã€‚ä½†åœ¨è¿›å…¥å®é™…ç¤ºä¾‹ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´é‡è¦çš„æ¦‚å¿µï¼Œå³â€œå†…æ ¸èåˆâ€ã€‚

### 10.3 èåˆå†…æ ¸

ç°åœ¨æˆ‘ä»¬åœ¨å‡ ä¸ªåœ°æ–¹æåˆ°äº† GPU å’Œ CPU æ“ä½œå¯ä»¥æ˜¯å¼‚æ­¥çš„ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCPU ä¸Šçš„ä¸»æœºä»£ç å¯ä»¥ä»¥éé˜»å¡çš„æ–¹å¼åœ¨ GPU ä¸Šè°ƒåº¦å·¥ä½œè´Ÿè½½ã€‚

éé˜»å¡åœ¨é‡å é€šä¿¡å’Œè®¡ç®—æ–¹é¢å¯èƒ½å¾ˆæœ‰ç”¨â€”â€”æ­£å¦‚æˆ‘ä»¬åœ¨æ—…ç¨‹ä¸­å¤šæ¬¡çœ‹åˆ°çš„é‚£æ ·â€”â€”ä½†å¯ä»¥å°†å…¶æ‰©å±•åˆ°æ›´æ™®éçš„æ€è·¯ï¼Œå³å°½é‡é¿å…åœ¨ä¸»æœºå’Œ GPU å†…æ ¸å‘½ä»¤ä¹‹é—´æ¥å›åˆ‡æ¢ã€‚

[è´ºæ‹‰æ–¯Â·èµ«ï¼ˆHorace Heï¼‰](https://horace.io/brrr_intro.html)åœ¨è¿™äº›å›¾è¡¨ä¸­éå¸¸å½¢è±¡åœ°é˜é‡Šäº†è¿™ä¸ªè§‚ç‚¹ï¼š

![image.png|300](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fused_kernels1.png)

éœ€è¦åœ¨å…¨å±€å†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´åå¤ä¼ è¾“çš„ä¸€ç³»åˆ—å†…æ ¸æ“ä½œ

![image.png|300](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fused_kernels2.png)

æˆ‘ä»¬ä¸æ˜¯å°†ä¸‰è§’å½¢å‘é€å›å…¨å±€å†…å­˜ç„¶åå†é‡æ–°è¯»å–å®ƒï¼Œè€Œæ˜¯å°†æ‰€æœ‰æ“ä½œä¸€æ¬¡æ€§å®Œæˆã€‚

æˆ‘ä»¬æ€æ ·æ‰èƒ½é¿å…è¿™ç§åå¤å‘¢ï¼Ÿæœ€å¥½çš„æ–¹æ³•æ˜¯è®©æˆ‘ä»¬çš„ GPU å°½å¯èƒ½åœ°è‡ªä¸»è¿è¡Œã€‚è¿™å¯ä»¥é€šè¿‡åœ¨å•ä¸ªå†…æ ¸ä¸­å°†å°½å¯èƒ½å¤šçš„è¿ç»­è®¡ç®—æ“ä½œç»„åˆåœ¨ä¸€èµ·æ¥å®ç°ï¼ŒGPU å°†è¿è¡Œè¿™ä¸ªå†…æ ¸ï¼Œç§°ä¸ºâ€œèåˆå†…æ ¸â€ã€‚

èåˆå†…æ ¸å¯¹äºåœ¨æ¯ä¸ªè¾“å…¥æ ‡è®°ä¸Šç‹¬ç«‹æ‰§è¡Œçš„ç±»ç‚¹æ“ä½œçš„è¿ç»­æ“ä½œç‰¹åˆ«é«˜æ•ˆä¸”æ˜“äºç¼–å†™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨å°†è®¡ç®—å€¼ç§»è‡³å…±äº«å†…å­˜å¹¶å¯åŠ¨æ–°å†…æ ¸ä¹‹å‰ï¼Œæ²¡æœ‰å¿…è¦å°†è®¡ç®—å€¼å¸¦å›å…¨å±€å†…å­˜ã€‚åœ¨å®Œæˆè¿ç»­è®¡ç®—ä¹‹å‰ï¼Œå°†æ‰€æœ‰å€¼ä¿ç•™åœ¨æœ¬åœ°è¦é«˜æ•ˆå¾—å¤šã€‚

åœ¨ Transformer æ¨¡å‹ä¸­ï¼Œæœ‰å¾ˆå¤šåœ°æ–¹å¯ä»¥åº”ç”¨è¿™ç§â€œèåˆâ€æ–¹æ³•ï¼šæ¯æ¬¡æˆ‘ä»¬æœ‰ä¸€ç³»åˆ—é€ç‚¹æ“ä½œæ—¶ï¼Œä¾‹å¦‚åœ¨å±‚å½’ä¸€åŒ–æ‰€æ¶‰åŠçš„è®¡ç®—ä¸­ã€‚

æˆ‘ä»¬ç°åœ¨å®Œå…¨ç†è§£äº†å†…æ ¸å·¥ç¨‹çš„ä¸€ä¸ªçœŸæ­£æ°ä½œï¼šFlash Attentionï¼Œä¸ç¦ä¸ºä¹‹æƒŠå¹ã€‚

### 10.4 Flash Attention 1-3

â€œFlash attentionâ€ ç”± [Tri Dao](https://tridao.me/) å¼•å…¥ï¼Œæ—¨åœ¨é€šè¿‡ç¼–å†™è‡ªå®šä¹‰ CUDA å†…æ ¸æ¥ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—ï¼Œä½¿å…¶é€Ÿåº¦æ›´å¿«ä¸”å†…å­˜æ•ˆç‡æ›´é«˜ã€‚Flash Attention èƒŒåçš„ç†å¿µæ˜¯é«˜æ•ˆåˆ©ç”¨ GPU çš„å„ç§å†…å­˜ï¼Œé¿å…è¿‡åº¦ä¾èµ–æœ€æ…¢çš„ä¸€ç§ï¼šGPU çš„å…¨å±€å†…å­˜ã€‚

ï¼ˆè¯·æ³¨æ„ï¼ŒGPU çš„å…¨å±€å†…å­˜è¢«ä»¤äººå›°æƒ‘åœ°ç§°ä¸ºâ€œé«˜å¸¦å®½å†…å­˜â€ï¼ˆHBM ğŸ« ï¼‰ã€‚ï¼‰

æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ç§åŸºæœ¬å®ç°æ¶‰åŠå†…å­˜å’Œå·¥ä½œå™¨ä¹‹é—´çš„å¤§é‡æ•°æ®ä¼ è¾“ã€‚å®ƒéœ€è¦åœ¨é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰ä¸­å®ä¾‹åŒ– S å’Œ P çŸ©é˜µï¼Œè¿™æ„å‘³ç€éœ€è¦å°†ç»“æœå‘é€åˆ° HBMï¼Œç„¶åå†å‘é€å›é™æ€éšæœºå­˜å–å­˜å‚¨å™¨ï¼ˆSRAMï¼‰ä»¥è¿›è¡Œåç»­è®¡ç®—ã€‚

![image.png|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/flashattn.png)

ç”±äºé«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰ä¸­çš„å¸¦å®½è¦ä½å¾—å¤šï¼Œè¿™åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­å¼•å…¥äº†ä¸€ä¸ªä¸¥é‡çš„ç“¶é¢ˆã€‚æˆ‘ä»¬èƒ½åšå¾—æ›´å¥½å—ï¼Ÿç‰¹é‡ŒÂ·è¾¾ï¼ˆTri Daoï¼‰è¯´å¯ä»¥ï¼

å…³é”®è¦ç´ æ˜¯å°† S çŸ©é˜µä»¥å°å—å½¢å¼è¿›è¡Œè®¡ç®—ï¼Œè¿™äº›å°å—èƒ½å¤Ÿé€‚é…å…±äº«å†…å­˜å•å…ƒï¼ˆSMï¼‰ä¸­è¾ƒå°çš„å…±äº«å†…å­˜ã€‚ä½†æˆ‘ä»¬å¯ä»¥åšå¾—æ›´å¥½ï¼Œå³å®Œå…¨é¿å…å°†éå¸¸å¤§çš„SçŸ©é˜µå®ä¾‹åŒ–ï¼Œè€Œæ˜¯ä»…ä¿ç•™è®¡ç®— softmax å½’ä¸€åŒ–å› å­æ‰€éœ€çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥åœ¨é™æ€éšæœºå­˜å–å­˜å‚¨å™¨ï¼ˆSRAMï¼‰ä¸­ä¸€æ¬¡æ€§è®¡ç®—éƒ¨åˆ†OO ï¼Œè€Œæ— éœ€æ¥å›ç§»åŠ¨ä¸­é—´ç»“æœã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä»…æ²¡æœ‰åˆ©ç”¨å…±äº«å†…å­˜ï¼Œè¿˜æ¶ˆé™¤äº†å› å®ä¾‹åŒ–æ¨¡å‹ä¸­ï¼ˆåœ¨é•¿ä¸Šä¸‹æ–‡é•¿åº¦æƒ…å†µä¸‹ï¼‰æœ€å¤§çš„æ¿€æ´»çŸ©é˜µä¹‹ä¸€â€”â€”æ³¨æ„åŠ›çŸ©é˜µè€Œå¯¼è‡´çš„å†…å­˜ç“¶é¢ˆã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/flashattn2.png)

Source: FlashAttention paper[13]

â€œé—ªå­˜æ³¨æ„åŠ›â€çš„æƒ³æ³•è§£å†³äº†æ¨¡å‹è®­ç»ƒä¸­çš„è¯¸å¤šç“¶é¢ˆï¼Œå› æ­¤å®ƒè¿…é€Ÿæˆä¸ºæ‰€æœ‰å˜å‹å™¨ä¸­æ‰§è¡Œæ³¨æ„åŠ›çš„é»˜è®¤æ–¹å¼ï¼š

- é€šè¿‡é¿å…å¯¹ S çŸ©é˜µè¿›è¡Œæ˜¾å¼è®¡ç®—ï¼Œæˆ‘ä»¬å‡è½»äº†æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜è´Ÿæ‹…
- æˆ‘ä»¬è¿˜æ¶ˆé™¤äº†æ³¨æ„åŠ›æœºåˆ¶ SÂ² æˆæœ¬çš„å¤§éƒ¨åˆ†ç›´æ¥å½±å“

å› æ­¤ï¼Œæ‰€æœ‰çº¿æ€§æ³¨æ„åŠ›çš„å˜ä½“ä»¥åŠè¿‘ä¼¼æ³¨æ„åŠ›çš„æ¬¡äºŒæ¬¡æ–¹æ–¹æ³•ï¼ˆè¿™äº›æ–¹æ³•æ˜¯åœ¨å˜å‹å™¨æ¶æ„å‘æ˜åä¸ä¹…å¼€å‘çš„ï¼‰å¤§å¤šéƒ½è¢«æç½®ä¸€æ—ï¼Œè½¬è€Œé‡‡ç”¨è¿™ç§ç²¾ç¡®ä¸”å¿«é€Ÿçš„é—ªå­˜æ³¨æ„åŠ›å®ç°å’Œæœºåˆ¶ã€‚

ç»§ Flash-attention 1 ä¹‹åï¼ŒåŒä¸€å®éªŒå®¤å‘å¸ƒäº†ä¸¤ä¸ªè¿ç»­æ”¹è¿›çš„ç‰ˆæœ¬ï¼šFlash-attention 2 å’Œ 3ã€‚ä¸Flash-attention 1 ç›¸æ¯”ï¼ŒFlash-attention 2 å’Œ 3 çš„æ”¹è¿›ä¸å¤ªåœ¨äºä¸€èˆ¬çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œåœ¨äºé€šè¿‡ï¼ˆ1ï¼‰å°½å¯èƒ½å‡å°‘éçŸ©é˜µä¹˜æ³•æ“ä½œçš„æ•°é‡ï¼ˆ2ï¼‰åœ¨ wraps å’Œçº¿ç¨‹å—ä¹‹é—´ä»”ç»†åˆ’åˆ†å·¥ä½œè´Ÿè½½ï¼ˆå¯¹äº Flash Attention 2ï¼‰ï¼Œä»¥åŠé’ˆå¯¹æœ€æ–°çš„ Hopperï¼ˆH100ï¼‰æ¶æ„ä¸Šçš„ FP8 å’Œ Tensor Core æ”¯æŒä»”ç»†ä¼˜åŒ–ï¼ˆå¯¹äºFlash Attention 3ï¼‰ï¼Œä½¿å…¶ä½çº§å®ç°æ›´å…·ä½“åœ°é€‚é… GPUã€‚

â€œFlash attention å¯¹èƒ½å¤ŸåŠ é€Ÿçš„æ³¨æ„åŠ›æ¨¡å¼æœ‰ä¸€å®šé™åˆ¶ã€‚å¯ä»¥çœ‹çœ‹ [FlexAttention](https://pytorch.org/blog/flexattention/)ï¼Œå®ƒæ˜¯ä¸€ç§å¿«é€Ÿä¸”çµæ´»çš„å˜ä½“ã€‚â€

â€œé—ªç”µæ³¨æ„åŠ›â€ï¼ˆFlash-Attentionï¼‰æ˜¯ä¸€ä¸ªå…¸èŒƒç¤ºä¾‹ï¼Œå®ƒå±•ç¤ºäº†å½“ä½ è€ƒè™‘åˆ°å½“å‰ GPU åŠ é€Ÿå™¨çš„å†…éƒ¨å†…å­˜/è®¡ç®—è®¾è®¡æ—¶æ‰€èƒ½å¸¦æ¥çš„çªç ´æ€§æ”¹è¿›ã€‚

---

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨æœ¬æ“ä½œèåˆéƒ¨åˆ†æ‰€æè¿°çš„æŠ€æœ¯è¦æ±‚æˆ‘ä»¬å®ç°å»ºæ¨¡ä»£ç æ›´æ”¹ï¼Œå¹¶ä¸ºæŸäº›æ“ä½œç¼–å†™è‡ªå®šä¹‰å†…æ ¸ï¼Œä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

åœ¨å¯¹è®¡ç®—æ“ä½œæœ¬èº«çš„ä½å±‚æ¬¡æ·±å…¥æ¢è®¨çš„æœ€åä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€ç³»åˆ—å¯¹å»ºæ¨¡ä»£ç ä¸å¯çŸ¥çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é€‚ç”¨äºä»»ä½•æ¨¡å‹ï¼Œå¹¶ä¸”è¢«å¹¿æ³›ä½¿ç”¨ä»¥è‡³äºå·²æˆä¸ºè¡Œä¸šæ ‡å‡†çš„ï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼

### 10.5 æ··åˆç²¾åº¦è®­ç»ƒ

åœ¨æœ¬ä¹¦çš„å„ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†è¾ƒä½ç²¾åº¦æ ¼å¼åŠå…¶å¯¹å­˜å‚¨æ¿€æ´»å€¼ã€å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€æ‰€éœ€å†…å­˜çš„å½±å“ã€‚ç°åœ¨æ˜¯æ—¶å€™æ›´æ·±å…¥åœ°ç ”ç©¶è¿™äº›æ ¼å¼çš„ç»†èŠ‚ï¼Œå¹¶æ›´å¥½åœ°ç†è§£å®ƒä»¬çš„æƒè¡¡ã€ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚

æ··åˆç²¾åº¦è®­ç»ƒï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ··åˆä½¿ç”¨ä¸åŒçš„ç²¾åº¦ã€‚PyTorch å¼ é‡çš„é»˜è®¤æ•°å€¼ç²¾åº¦æ˜¯å•ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼Œä¹Ÿç§°ä¸º FP32 æˆ– float32ï¼Œè¿™æ„å‘³ç€å­˜å‚¨çš„æ¯ä¸ªæ•°å­—å ç”¨ 32 ä½æˆ– 4 ä¸ªå­—èŠ‚ã€‚è¡¨ç¤ºä¸€ä¸ªæ•°å­—çš„å¯ç”¨ä½è¢«åˆ†ä¸º 3 éƒ¨åˆ†ï¼š

- ç¬¦å·ä½ï¼šç¬¬ä¸€ä½å†³å®šæ•°å­—æ˜¯æ­£æ•°è¿˜æ˜¯è´Ÿæ•°
- å°¾æ•°ï¼šå†³å®šæ•°å­—çš„æœ‰æ•ˆæ•°å­—
- æŒ‡æ•°ï¼šæ§åˆ¶æ•°å­—çš„å¤§å°

![sign-mantissa-exponent.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/sign-mantissa-exponent.svg)

é€šè¿‡å›é¡¾æ•°å­—çš„ç§‘å­¦è®¡æ•°æ³•ï¼ˆä¾‹å¦‚ âˆ’5.734Ã—10â· ï¼Œå…¶ä¸­é¦–å…ˆæœ‰ç¬¦å·ï¼Œç„¶åæ˜¯å°¾æ•°å’ŒæŒ‡æ•°ï¼‰ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°è¯´æ˜æµ®ç‚¹æ•°çš„åŸç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è‡ªé€‚åº”çš„ç²¾åº¦è¡¨ç¤ºå¾ˆå¤§èŒƒå›´çš„æ•°å€¼ã€‚è™½ç„¶ float32 æ˜¯é»˜è®¤çš„ï¼Œä½† PyTorch ä¸­æœ‰ä¸€ç³»åˆ—çš„æµ®ç‚¹æ ¼å¼å¯ç”¨ï¼š

| æ ¼å¼         | æ€»ä½æ•° | ç¬¦å·ä½ | æŒ‡æ•°ä½ | å°¾æ•°ä½ |
|--------------|--------|--------|--------|--------|
| float32      | 32     | 1      | 8      | 23     |
| float16      | 16     | 1      | 5      | 10     |
| bfloat16     | 16     | 1      | 8      | 7      |
| float8 (e4m3)| 8      | 1      | 4      | 3      |
| float8 (e5m2)| 8      | 1      | 5      | 2      |
ï¼ˆæ³¨æ„ï¼šæ‚¨å¯èƒ½æƒ³çŸ¥é“ bfloat16 ä¸­çš„â€œbâ€æ˜¯ä»å“ªé‡Œæ¥çš„ã€‚è¿™ç§æ ¼å¼æ˜¯åœ¨è°·æ­Œå¤§è„‘ï¼ˆGoogle Brainï¼‰ä¸­å¼€å‘çš„ï¼Œå› æ­¤â€œbâ€ä»£è¡¨â€œå¤§è„‘ï¼ˆbrainï¼‰â€ã€‚ï¼‰

å‡å°‘æ€»ä½æ•°æ˜¯éœ€è¦ä»˜å‡ºä»£ä»·çš„ï¼ˆè¿™é‡Œä¹Ÿæ²¡æœ‰å…è´¹çš„åˆé¤ï¼‰ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ§åˆ¶å¦‚ä½•ä»˜å‡ºä»£ä»·ã€‚è¦ä¹ˆæˆ‘ä»¬å¯ä»¥ç‰ºç‰²æ›´å¤šçš„å°¾æ•°ä½ï¼Œè¦ä¹ˆç‰ºç‰²æ›´å¤šçš„æŒ‡æ•°ä½ã€‚å› æ­¤ï¼Œè¿˜å­˜åœ¨ä¸¤ç§ float8 æ ¼å¼ï¼Œæ ¹æ®æŒ‡æ•°å’Œå°¾æ•°æ¥å‘½åï¼Œä»¥ä¾¿çµæ´»é€‰æ‹©æœ€åˆé€‚çš„æ ¼å¼ã€‚æˆ‘ä»¬å¯ä»¥çœ‹çœ‹æ¯ç§æ ¼å¼å¯èƒ½çš„æ•°å­—èŒƒå›´ï¼š

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/mixedprecision.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œfloat32 çš„æ•°å€¼è·¨åº¦è¾¾åˆ° 80 ä¸ªæ•°é‡çº§ï¼Œè€Œ float16 ç‰ºç‰²äº†å¾ˆå¤§çš„èŒƒå›´ï¼Œbfloat16 åˆ™ä¿æŒäº†å®Œæ•´çš„èŒƒå›´ã€‚ä¸¤ç§ float8 æ ¼å¼è¿›ä¸€æ­¥ç¼©å°äº†èŒƒå›´ï¼Œå…¶ä¸­ e5e2 å¯ä»¥ä¿æŒ float16 çš„èŒƒå›´ï¼Œè€Œ e4m3 çš„èŒƒå›´åˆ™æ›´å°ã€‚

ä¸ºä»€ä¹ˆæœ‰äº›æ ¼å¼èƒ½å¤Ÿä¿æŒèŒƒå›´è€Œæœ‰äº›åˆ™ä¸èƒ½ï¼Ÿè®©æˆ‘ä»¬é€šè¿‡åœ¨ 1 åˆ° 2 ä¹‹é—´ç»˜åˆ¶ 10,000 ä¸ªç‚¹æ¥ç ”ç©¶åˆ†è¾¨ç‡ï¼šæ¯ä¸ªç‚¹å°†è¢«å››èˆäº”å…¥åˆ°æ¯ç§æ ¼å¼ä¸­æœ€æ¥è¿‘çš„å¯è¡¨ç¤ºæ•°å­—ã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/mixedprecision_2.png)

æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ï¼Œä¸ float16 ç›¸æ¯”ï¼Œbfloat16 ä¿æŒäº† float32 çš„èŒƒå›´ï¼Œä½†è¿™æ˜¯ä»¥ç‰ºç‰²æ›´å¤šç²¾åº¦ä¸ºä»£ä»·çš„ã€‚åœ¨ float8 çš„æƒ…å†µä¸‹ï¼Œæƒ…å†µæ›´åŠ ä¸¥å³»ï¼Œå› ä¸º e4m3 åœ¨ 1 - 2 åŒºé—´åªèƒ½è¡¨ç¤º 7 ä¸ªæ•°ï¼Œè€Œ e5m2 åªèƒ½è¡¨ç¤º 3 ä¸ªæ•° ã€‚

è¡¡é‡æ ¼å¼åˆ†è¾¨ç‡çš„ä¸€ä¸ªå¸¸ç”¨æŒ‡æ ‡æ˜¯ epsilonï¼šå³ 1.00 ä¹‹åç¬¬ä¸€ä¸ªå¯è¡¨ç¤ºçš„æ•°å­— 1.00ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äº float32 æ ¼å¼ï¼Œ$10^{âˆ’4}$ æ˜¯ä¸€ä¸ªä¸Šé™ï¼ˆå®é™…ä¸º $1.19^{âˆ’7}$ï¼‰ã€‚å¯¹äº float16ï¼Œè¯¥å€¼çº¦ä¸º $10^{âˆ’3}$ï¼Œè€Œå¯¹äºbfloatï¼Œè¿™ä¸ªå€¼è¿˜è¦é«˜å‡º 10 å€ã€‚

æ··åˆç²¾åº¦è®­ç»ƒçš„æ€æƒ³æ˜¯åœ¨ä¿æŒå…¨ç²¾åº¦è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ä½¿ç”¨å…¶ä¸­ä¸€äº›è¾ƒä½ç²¾åº¦æ ¼å¼ã€‚

äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬ä¸èƒ½å®Œå…¨æ”¾å¼ƒ float32ï¼Œé€šå¸¸éœ€è¦ä¿ç•™ä¸€äº›å…¨ç²¾åº¦éƒ¨åˆ†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ç²¾åº¦è®­ç»ƒé€šå¸¸è¢«ç§°ä¸ºæ··åˆç²¾åº¦è®­ç»ƒã€‚

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ç”¨ 16 ä½è®­ç»ƒæ¨¡å‹ï¼Œç„¶åçœ‹çœ‹èƒ½å¦æ›´è¿›ä¸€æ­¥ï¼Œä¸€ç›´é™åˆ° 8 ä½ã€‚

#### 10.5.1 FP16 å’Œ BF16 è®­ç»ƒ

å¤©çœŸåœ°å°†æ‰€æœ‰å¼ é‡å’Œæ“ä½œéƒ½è½¬æ¢ä¸º float16 æ˜¯ä¸è¡Œçš„ï¼Œç»“æœé€šå¸¸æ˜¯æŸå¤±å‘æ•£ã€‚ç„¶è€Œï¼Œæœ€åˆçš„æ··åˆç²¾åº¦è®­ç»ƒè®ºæ–‡[2]æå‡ºäº†ä¸‰ç§æŠ€å·§æ¥åŒ¹é… float32 è®­ç»ƒï¼š

1. æƒé‡çš„ FP32 å‰¯æœ¬ï¼šfloat16 æƒé‡å¯èƒ½å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸€äº›æƒé‡å¯èƒ½ä¼šå˜å¾—éå¸¸å°å¹¶è¢«å››èˆäº”å…¥ä¸º 0ã€‚ç„¶è€Œï¼Œå³ä½¿æƒé‡æœ¬èº«ä¸æ¥è¿‘é›¶ï¼Œå¦‚æœæ›´æ–°éå¸¸å°ï¼Œé‡çº§çš„å·®å¼‚ä¹Ÿå¯èƒ½å¯¼è‡´æƒé‡åœ¨åŠ æ³•è¿‡ç¨‹ä¸­ä¸‹æº¢ã€‚ä¸€æ—¦æƒé‡å˜ä¸ºé›¶ï¼Œç”±äºä¸å†æœ‰æ¢¯åº¦ä¿¡å·ä¼ å…¥ï¼Œå®ƒä»¬å°†åœ¨å‰©ä½™çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒä¸º 0ã€‚
2. æŸå¤±ç¼©æ”¾ï¼šæ¢¯åº¦ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„é—®é¢˜ï¼Œå› ä¸ºæ¢¯åº¦å¾€å¾€è¿œå°äº 1ï¼Œå› æ­¤æœ‰ä¸‹æº¢çš„é£é™©ã€‚ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥æ˜¯åœ¨åå‘ä¼ æ’­ä¹‹å‰ç¼©æ”¾æŸå¤±ï¼Œå¹¶åœ¨åå‘ä¼ æ’­ä¹‹åå¯¹æ¢¯åº¦è¿›è¡Œåç¼©æ”¾ã€‚è¿™ç¡®ä¿äº†åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸ä¼šå‘ç”Ÿä¸‹æº¢ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ¢¯åº¦ï¼ˆä¾‹å¦‚è£å‰ªï¼‰å’Œä¼˜åŒ–æ­¥éª¤ä¹‹å‰æˆ‘ä»¬è¿›è¡Œåç¼©æ”¾ï¼Œå› æ­¤ç¼©æ”¾ä¸ä¼šå½±å“è®­ç»ƒã€‚
3. ç´¯ç§¯ï¼šæœ€åï¼Œåœ¨æ‰§è¡ŒæŸäº› 16 ä½ç²¾åº¦çš„ç®—æœ¯è¿ç®—ï¼ˆå¦‚å¹³å‡å€¼æˆ–æ±‚å’Œï¼‰æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯èƒ½é¢ä¸´ä¸‹æº¢æˆ–ä¸Šæº¢çš„é—®é¢˜ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åœ¨æ“ä½œæœŸé—´å°†ä¸­é—´ç»“æœç´¯ç§¯åœ¨ float32 ä¸­ï¼Œå¹¶ä¸”ä»…åœ¨æœ€åå°†æœ€ç»ˆç»“æœè½¬æ¢å› 16 ä½ç²¾åº¦ã€‚

å€ŸåŠ©è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å—ç›Šäºæ›´å¿«ã€æ›´ä½ç²¾åº¦ç®—æœ¯è¿ç®—å¸¦æ¥çš„æ›´é«˜ååé‡çš„åŒæ—¶ï¼Œè·å¾—ç¨³å®šçš„è®­ç»ƒæ•ˆæœã€‚è‡ªç„¶åœ°ï¼Œä½œä¸ºä¸€ä¸ªå……æ»¡å¥½å¥‡å¿ƒçš„è¯»è€…â€”â€”å¹¶ä¸”åˆ°ç›®å‰ä¸ºæ­¢æœ‰ç‚¹ç—´è¿·äºå®ç°ååé‡æœ€å¤§åŒ–â€”â€”ä½ å¯èƒ½ä¼šé—®è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦è¶…è¶Š 16 ä½ç²¾åº¦ï¼Œå®ç°æ›´è¿›ä¸€æ­¥çš„åŠ é€Ÿå‘¢ï¼Ÿ

æˆ–è®¸å§ï¼

#### 10.5.2 FP8 é¢„è®­ç»ƒ

å³ä½¿æˆ‘ä»¬å°†é€šä¿¡ä¸è®¡ç®—å®Œç¾åœ°é‡å ï¼Œæˆ‘ä»¬æœ€ç»ˆè¿˜æ˜¯ä¼šé‡åˆ°ç¡¬ä»¶æœ¬èº«ä½å±‚æ¬¡ç†è®ºæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPSï¼‰çš„é™åˆ¶ï¼Œå³æˆ‘ä»¬ç¡¬ä»¶ä¸Šæ¯ä¸ªå•ç‹¬æ“ä½œçš„æ•ˆç‡ã€‚è¿™æ—¶æ•°å€¼ç²¾åº¦å°±å˜å¾—è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è‹±ä¼Ÿè¾¾ï¼ˆNVIDIAï¼‰çš„ H100 GPU ä¸Šï¼ŒFP8 çŸ©é˜µä¹˜æ³•ï¼ˆGEMM è¿ç®—ï¼‰çš„ç†è®ºæµ®ç‚¹è¿ç®—æ¬¡æ•°æ˜¯ bfloat16 çš„ä¸¤å€ï¼Œè¿™ä½¿å¾—ä½ç²¾åº¦è®­ç»ƒæˆä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–çš„æœ‰å¸å¼•åŠ›çš„é€”å¾„ã€‚

è¿‘æœŸç ”ç©¶ï¼ˆåŒ…æ‹¬FP8-LM[14]ã€torchao[15]å’ŒDeepSeek-V3[7]ï¼‰å·²ç»è¯æ˜äº†FP8è®­ç»ƒç”¨äºå¤§è§„æ¨¡æ¨¡å‹çš„æ½œåŠ›ã€‚ä¸è¿‡ï¼ŒFP8é¢„è®­ç»ƒå¼•å…¥äº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šç¨³å®šæ€§ã€‚åœ¨è¾ƒä½ç²¾åº¦ä¸‹ï¼Œæ•°å€¼ä¸ç¨³å®šå¸¸å¸¸å¯¼è‡´æŸå¤±å‘æ•£ï¼Œä½¿å¾—éš¾ä»¥è¾¾åˆ°æ›´é«˜ç²¾åº¦è®­ç»ƒçš„å‡†ç¡®ç‡ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œå¯¹äºå›ºå®šçš„æ¨¡å‹å¤§å°ï¼Œå­¦ä¹ ç‡ä¸Šå‡æ—¶ä¸ç¨³å®šæ€§ä¼šå¢åŠ [16]ï¼Œè¿™ä½¿å¾— FP8 é¢„è®­ç»ƒç‰¹åˆ«æ£˜æ‰‹ã€‚

ä»¥ä¸‹æ˜¯ FP8 è®­ç»ƒä¸­å…¸å‹çš„å‘æ•£æŸå¤±æ›²çº¿çš„ä¸€ä¸ªç¤ºä¾‹ï¼š

[äº¤äº’å›¾]

é¦–æ¬¡æˆåŠŸçš„å¤§è§„æ¨¡ FP8 æ··åˆç²¾åº¦è®­ç»ƒåœ¨ DeepSeek-V3 ä¸Šè¢«å…¬å¼€æŠ¥é“ã€‚ä½œè€…ä»”ç»†åˆ†æäº†å‰å‘ä¼ æ’­ï¼ˆFpropï¼‰ä»¥åŠæ¿€æ´»ï¼ˆDgradï¼‰å’Œæƒé‡ï¼ˆWgradï¼‰åå‘ä¼ æ’­çš„æ¯ä¸ªæ“ä½œã€‚ä¸ BF16 æ··åˆç²¾åº¦è®­ç»ƒç±»ä¼¼ï¼Œä¸€äº›èšåˆå’Œä¸»æƒé‡ä¿æŒè¾ƒé«˜ç²¾åº¦ï¼Œè€Œæ“ä½œæœ¬èº«ä»¥ FP8 æ‰§è¡Œã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fp8_diagram.png)

ä¸ºäº†ä»é«˜ç²¾åº¦ï¼ˆä¾‹å¦‚ FP32 æˆ– BF16ï¼‰åˆ‡æ¢åˆ°èŒƒå›´æ›´å°çš„ä½ç²¾åº¦ï¼ˆä¾‹å¦‚ FP16 æˆ– FP8ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¿€æ´»å€¼çš„èŒƒå›´è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä¾‹å¦‚é€šè¿‡è®¡ç®—å®ƒä»¬çš„ç»å¯¹æœ€å¤§å€¼ã€‚DeepSeek-V3 è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§ç‰¹å®šçš„é‡åŒ–æ–¹æ¡ˆï¼Œå…¶ä¸­èŒƒå›´æŒ‰ç“¦ç‰‡è¿›è¡Œå½’ä¸€åŒ–ï¼šè¾“å…¥/æ¿€æ´»ä¸º 1x128ï¼Œæƒé‡å’Œç¼©æ”¾å…ƒç´ ä¸º 128x128ã€‚è¿™ä½¿å¾—å½’ä¸€åŒ–å—æ¿€æ´»å€¼ä¸­å¼‚å¸¸å€¼çš„å½±å“è¾ƒå°ã€‚ä»–ä»¬è¿˜æå‡ºäº†ä¸€äº›é¢å¤–çš„æŠ€å·§æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜å’Œé€šä¿¡å¼€é”€ï¼Œä½ å¯ä»¥åœ¨ DeepSeek-V3 æŠ€æœ¯æŠ¥å‘Š[7]çš„ç¬¬ 3.3 èŠ‚ä¸­äº†è§£è¿™äº›æŠ€å·§ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å·²çŸ¥çš„ FP8 è®­ç»ƒæ–¹æ³•çš„æ€»ç»“ï¼š

| GEMM ç²¾åº¦        | ä¸»æ¨¡å‹æƒé‡ | ç´¯ç§¯æ¢¯åº¦ | æ¨¡å‹æƒé‡ | æ¢¯åº¦  | ä¼˜åŒ–å™¨çŠ¶æ€    | æ€»å†…å­˜                     |
|------------------|------------|----------|----------|-------|---------------|----------------------------|
| bfloat16 ä¸ fp32 æ··åˆç²¾åº¦åŸºçº¿ | bf16       | fp32     | bf16     | bf16  | fp32 + fp32 | 4 + 4 + 2 + 2 + 4 + 4 = 20 å­—èŠ‚ |
| æ—  FP32 æ¢¯åº¦ç´¯ç§¯ | bf16       | n/a      | bf16     | bf16  | fp32 + fp32 | 4 + 2 + 2 + 4 + 4 = 16 å­—èŠ‚ |
| Transformer å¼•æ“ | fp8        | n/a      | fp32     | fp32  | fp32 + fp32 | 4 + 4 + 4 + 4 = 16 å­—èŠ‚ (å‡å°‘ 20%) |
| FP8-LM çš„ O3 çº§åˆ« | fp8        | fp16     | fp16     | fp8   | fp8 + fp16  | 2 + 2 + 1 + 1 + 1 + 2 = 9 å­—èŠ‚ (å‡å°‘ 55%) |
| DeepSeek-V3      | fp8        | fp32     | fp32     | fp8   | bf16 + bf16 | 4 + 4 + 1 + 2 + 2 + 2 = 15 å­—èŠ‚ (å‡å°‘ 25%) |
| nanotron çš„ FP8  | fp8        | bf16     | fp16     | fp8   | fp8 + fp8   | 2 + 4 + 1 + 1 + 1 + 1 = 10 å­—èŠ‚ (å‡å°‘ 50%) |

æ€»ä½“è€Œè¨€ï¼Œåœ¨ 2025 å¹´åˆï¼ŒFP8 ä»æ˜¯ä¸€ç§å®éªŒæ€§æŠ€æœ¯ï¼Œç›¸å…³æ–¹æ³•ä»åœ¨ä¸æ–­å‘å±•ã€‚é‰´äºå…¶æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šæˆä¸ºæ ‡å‡†ï¼Œå¹¶å¾ˆå¿«å–ä»£ bf16 æ··åˆç²¾åº¦ã€‚è‹¥è¦å…³æ³¨ FP8 è®­ç»ƒæŠ€æœ¯çš„å¼€æºå®ç°ï¼Œè¯·æŸ¥çœ‹[æ­¤æ‹‰å–è¯·æ±‚](https://github.com/huggingface/nanotron/pull/70)ä¸­ nanotron çš„å®ç°ã€‚

å±•æœ›æœªæ¥ï¼Œè‹±ä¼Ÿè¾¾çš„ä¸‹ä¸€ä»£èŠ¯ç‰‡ Blackwell [å·²å®£å¸ƒ](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)æ”¯æŒ FP4 è®­ç»ƒï¼Œè¿™å°†è¿›ä¸€æ­¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œä½†æ— ç–‘ä¹Ÿä¼šå¸¦æ¥æ–°çš„è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ã€‚

---

è¿™æœ€åä¸€éƒ¨åˆ†ä¸ºæˆ‘ä»¬åœ¨æ•°åä¹ƒè‡³æ•°åƒä¸ª GPU ä¸Šè®­ç»ƒå¿«é€Ÿå¤§å‹æ¨¡å‹çš„æ¼«é•¿æ—…ç¨‹ç”»ä¸Šäº†å¥å·ã€‚ç°åœ¨æ˜¯æ—¶å€™è®©æˆ‘ä»¬çš„ GPU é›†ç¾¤æ…¢æ…¢åœæ­¢è¿è¡Œï¼Œé€€ä¸€æ­¥æ¥æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åœ¨æ­¤è¿‡ç¨‹ä¸­æ‰€å­¦åˆ°çš„ä¸€åˆ‡äº†ã€‚

## åä¸€ã€æ€»ç»“

æ­å–œä½ ï¼Œäº²çˆ±çš„è¯»è€…ï¼Œä½ åšæŒåˆ°äº†æœ€åï¼æˆ‘ä»¬å®Œæˆäº†ä¸€æ®µç›¸å½“æ¼«é•¿çš„æ—…ç¨‹ï¼šä»äº†è§£å¦‚ä½•åœ¨å•ä¸ª GPU ä¸Šè®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹å¼€å§‹ï¼Œä¸€ç›´åˆ°æŒæ¡åœ¨æ•°åƒä¸ª GPU ä¸Šé«˜æ•ˆè®­ç»ƒåƒ Llama-405B å’Œ DeepSeek-V3 è¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰€æœ‰å¤æ‚æŠ€æœ¯ã€‚åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œä½ å¯ä»¥ï¼ˆç›¸å¯¹ï¼‰è½»æ¾åœ°è¯»æ‡‚ä¸€ä¸ªå›¾è¡¨ï¼Œæ¯”å¦‚Llama-3çš„ 4D å¹¶è¡Œè®¾ç½®ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/conclusion_llama3_parallelism.png)

åè°ƒå¤§å‹ GPU é›†ç¾¤ä»¥é«˜æ•ˆè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¶éæ˜“äº‹ã€‚æˆ‘ä»¬å­¦ä¼šäº†å¦‚ä½•ä¼˜åŒ– GPU ä¹‹é—´çš„è®¡ç®—å’Œé€šä¿¡ï¼Œä½¿å®ƒä»¬å§‹ç»ˆä»¥æœ€å¤§åˆ©ç”¨ç‡è¿è¡Œã€‚è¿™æ¶‰åŠä¸ºç»™å®šçš„æ¨¡å‹å’Œé›†ç¾¤å¤§å°é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç­–ç•¥ï¼Œåœ¨å¯èƒ½çš„æƒ…å†µä¸‹é‡å é€šä¿¡å’Œè®¡ç®—ï¼Œå¹¶ç¼–å†™è‡ªå®šä¹‰å†…æ ¸ï¼Œè€ƒè™‘ç¡¬ä»¶å¸ƒå±€ä»¥ä¾¿åœ¨ GPU ä¸Šå°½å¯èƒ½å¿«åœ°æ‰§è¡Œæ“ä½œã€‚

ä½ å¯èƒ½ä»ç„¶è®¤ä¸ºè¿™äº›çŸ¥è¯†æœ‰ç‚¹å°ä¼—ï¼Œåªä¸é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€å°éƒ¨åˆ†äººæœ‰å…³ã€‚ä»å†å²ä¸Šçœ‹ï¼Œè¿™å¯èƒ½æ˜¯çœŸçš„ï¼Œä½†éšç€äººå·¥æ™ºèƒ½æ„å»ºè€…ç¤¾åŒºå’Œæ¨¡å‹è§„æ¨¡éƒ½åœ¨è¿…é€Ÿå¢é•¿ï¼Œä½¿ç”¨åˆ†å¸ƒå¼æŠ€æœ¯è¿›è¡Œæ¨ç†ã€å¾®è°ƒåŠè®­ç»ƒçš„äººå‘˜ç¾¤ä½“ä¹Ÿåœ¨å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œè¿™ä½¿å¾—åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚å› æ­¤ï¼Œæ›´æ·±å…¥åœ°æ¢ç©¶åˆ†å¸ƒå¼ç›¸å…³çš„æ‰€æœ‰å†…å®¹æˆ–è®¸éå¸¸é€‚æ—¶ã€‚

è¿™æ˜¯ä¸€æ®µæ¼«é•¿çš„å­¦ä¹ ä¹‹æ—…ï¼Œä½†ä¸ä»…ä»…æ˜¯å¯¹ä½ ä»¬è€Œè¨€ï¼åœ¨ GPU é›†ç¾¤ä¸Šè¿è¡Œæ•°åƒä¸ªåŸºå‡†æµ‹è¯•æ¯”æˆ‘ä»¬é¢„æœŸçš„æ›´å…·æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬ä¹Ÿæƒ³åˆ†äº«ä¸€ä¸‹æˆ‘ä»¬è‡ªå·±å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¸€äº›äº®ç‚¹ã€‚

### 11.1 é‚£ä¹ˆï¼Œæ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿ

ä½ ç°åœ¨å¯¹ä¸»è¦çš„åˆ†å¸ƒå¼è®­ç»ƒæ¦‚å¿µæœ‰äº†å¾ˆå¥½çš„äº†è§£ï¼Œä½†ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬åªæ˜¯æµ…å°è¾„æ­¢åœ°æ¶‰åŠäº†å…¶ä¸­ä¸€äº›å·¥å…·å’ŒæŠ€æœ¯ã€‚æ·±å…¥äº†è§£ä¸€ä¸ªä¸»é¢˜æœ‰å¾ˆå¤šæ–¹æ³•ï¼Œä½†æˆ‘ä»¬æ¨èä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

* ä»”ç»†é˜…è¯»ä¸€äº›å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰æˆ–éå¸¸è¿‘æœŸçš„è®ºæ–‡ã€‚ä½ å¯ä»¥åœ¨å‚è€ƒæ–‡çŒ®ä¸­æ‰¾åˆ°ä¸€ä»½éå¸¸è¯¦å°½çš„æœ€å…·å½±å“åŠ›çš„è®ºæ–‡ã€åšå®¢æ–‡ç« å’Œä¹¦ç±çš„æ¸…å•ã€‚
* ä»é›¶å¼€å§‹è‡ªå·±å®ç°ä¸€ä¸ªç®—æ³•ã€‚é€šå¸¸ï¼Œåªæœ‰å½“ä½ è‡ªå·±å®ç°äº†ä¸€ä¸ªæ–¹æ³•æ—¶ï¼Œå®ƒæ‰ä¼šçœŸæ­£â€œè±ç„¶å¼€æœ—â€ã€‚
* æ·±å…¥ç ”ç©¶ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¡†æ¶å¹¶å¼€å§‹è´¡çŒ®ï¼šä¿®å¤æ¼æ´ã€è§£ç­”é—®é¢˜æˆ–å®ç°æ–°åŠŸèƒ½ã€‚è¿™æ˜¯è¿›å…¥ä»»ä½•æœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€ä½³æ–¹å¼ï¼  

æˆ‘ä»¬å¸Œæœ›æœ¬ä¹¦èƒ½å¸®åŠ©ä½ å¼€å¯åˆ†å¸ƒå¼è®­ç»ƒä¹‹æ—…ï¼Œå¹¶ä¸”ä½ èƒ½è®­ç»ƒå‡ºä¸‹ä¸€ä»£å¼ºå¤§çš„æ¨¡å‹ï¼Œä¼´éšç€ä½ çš„ GPU é›†ç¾¤çš„å—¡å—¡å£°ï¼

---

ç»™æˆ‘ä»¬çš„é¦–æ‰¹è¯»è€…æœ€åè¯´å‡ å¥ã€‚æˆ‘ä»¬å¯¹è¿™ç¯‡ä½œå“éå¸¸æ»¡æ„ï¼Œå†³å®šé™é‡åˆ¶ä½œä¸€äº›å®ä½“å°åˆ·ç‰ˆä½œä¸ºç¤¼ç‰©é€ç»™é¦–æ‰¹è¯»è€…ã€‚

å¦‚æœæ‚¨æ˜¯å‰ 50 ä¸ªåœ¨ä¸‹æ–¹å¡«å†™ç”µå­é‚®ä»¶åœ°å€çš„äººä¹‹ä¸€ï¼Œæˆ‘ä»¬å°†åœ¨ä»Šå¹´æ™šäº›æ—¶å€™ä¸æ‚¨è”ç³»ï¼Œåœ¨å°†å…¶æ’ç‰ˆä¸ºå°åˆ·æœ¬åç»™æ‚¨å¯„é€ä¸€ä»½å®ä½“ä¹¦ã€‚

æˆ‘ä»¬é¢„è®¡è¿™æœ¬ä¹¦å¤§çº¦ä¼šæœ‰ 100 - 150 é¡µï¼Œå¹¶ä¸”æ¶µç›–ä¸åšå®¢æ–‡ç« ç›¸åŒçš„å†…å®¹ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯èƒ½ä¼šæ ¹æ®ä½œä¸ºå°åˆ·å“æ˜¯å¦åˆç†æ¥å†³å®šå¯¹å…¶è¿›è¡Œç¼©çŸ­æˆ–æ‰©å……ã€‚

å¦‚éœ€è·å–çº¸è´¨ç‰ˆï¼Œè¯·åœ¨ä»¥ä¸‹è°·æ­Œè¡¨å•ä¸­å¡«å†™æ‚¨çš„ç”µå­é‚®ç®±åœ°å€ã€‚

æ— è®ºä½ æ˜¯æˆ‘ä»¬æœ€æ—©çš„ä¸€æ‰¹è¯»è€…ï¼Œè¿˜æ˜¯å¾ˆä¹…ä¹‹åæ‰çœ‹åˆ°è¿™ç¯‡åšå®¢æ–‡ç« çš„è¯»è€…ï¼Œæˆ‘ä»¬éƒ½å¾ˆé«˜å…´çœ‹åˆ°ä½ å–œæ¬¢è¿™æ¬¡çŸ¥è¯†åˆ†äº«ã€‚æ„¿å¼€æºå’Œå¼€æ”¾ç§‘å­¦çš„ç²¾ç¥æ°¸è¿œä¸ä½ åŒåœ¨ã€‚

### 11.2 è‡´è°¢  

æˆ‘ä»¬æ„Ÿè°¢ Elie è¿›è¡Œäº†å…¨é¢çš„å®¡æŸ¥ï¼Œå¹¶ä½¿ç”¨ NotebookLM åˆ›å»ºäº†éŸ³é¢‘ç»„ä»¶ã€‚ç‰¹åˆ«æ„Ÿè°¢ Hynek ä¼˜åŒ–äº†å‰ç«¯æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ„Ÿè°¢ Simon è§£å†³äº† Hub çš„ä¸€äº›é—®é¢˜ã€‚

### 11.3 è®¨è®ºé¡µ

å¦‚æœæ‚¨æƒ³è®¨è®ºè¿™ç¯‡åšå®¢æ–‡ç« çš„å†…å®¹ã€æå‡ºé—®é¢˜ã€å»ºè®®ä¿®æ”¹æˆ–è€…åªæ˜¯æ‰“ä¸ªæ‹›å‘¼ï¼Œè¯·åœ¨è®¨è®ºé¡µé¢ä¸Šå¼€ä¸€ä¸ªä¸»é¢˜å¸–ã€‚

## åäºŒã€å‚è€ƒæ–‡çŒ®

### 12.1 å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•è®ºæ–‡

* [**Megatron-LM**](https://arxiv.org/abs/1909.08053)ï¼šä»‹ç»ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ é‡å¹¶è¡Œå’Œé«˜æ•ˆæ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ã€‚
* [**Megatron-Turing NLG 530B**](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)ï¼šæè¿°äº†ä½¿ç”¨ DeepSpeed å’Œ Megatron-LM æ¡†æ¶ç»„åˆè®­ç»ƒä¸€ä¸ª 530B å‚æ•°æ¨¡å‹çš„è¿‡ç¨‹ã€‚
* [**PaLM**](https://arxiv.org/abs/2204.02311)ï¼šä»‹ç»äº†è°·æ­Œçš„ Pathways è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ•°ç™¾ç§è¯­è¨€ä»»åŠ¡å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚
* [**Gemini**](https://arxiv.org/abs/2312.11805)ï¼šä»‹ç»è°·æ­Œçš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ã€‚
* [**Llama 3**](https://arxiv.org/abs/2407.21783)ï¼šLlama 3 æ¨¡å‹ç¾¤
* [**DeepSeek-V3**](https://arxiv.org/abs/2412.19437v1)ï¼šDeepSeek å…³äº DeepSeek-V3 æ¨¡å‹æ¶æ„ä¸è®­ç»ƒçš„æŠ¥å‘Šã€‚

### 12.2 è®­ç»ƒæ¡†æ¶

* [**Nanotron**](https://github.com/huggingface/nanotron)ï¼šæˆ‘ä»¬ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šç§å¹¶è¡Œç­–ç•¥
* [**Megatron-LM**](https://github.com/NVIDIA/Megatron-LM)ï¼šè‹±ä¼Ÿè¾¾ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šç§å¹¶è¡Œç­–ç•¥ã€‚
* [**DeepSpeed**](https://www.deepspeed.ai/)ï¼šå¾®è½¯çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œå…·æœ‰ ZeRO ä¼˜åŒ–é˜¶æ®µå’Œå„ç§å¹¶è¡Œç­–ç•¥ã€‚
* [**FairScale**](https://github.com/facebookresearch/fairscale/tree/main)ï¼šç”¨äºå¤§è§„æ¨¡è®­ç»ƒçš„ PyTorch æ‰©å±•åº“ï¼Œæä¾›å„ç§å¹¶è¡Œå’Œä¼˜åŒ–æŠ€æœ¯ã€‚
* [**ColossalAI**](https://colossalai.org/)ï¼šé›†æˆäº†å¤šç§ä¼˜åŒ–æŠ€æœ¯çš„å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒç³»ç»Ÿã€‚
* [**torchtitan**](https://github.com/pytorch/torchtitan)ï¼šä¸€ä¸ªç”¨äºå¤§æ¨¡å‹è®­ç»ƒçš„ PyTorch åŸç”Ÿåº“ã€‚
* [**GPT-NeoX**](https://github.com/EleutherAI/gpt-neox)ï¼šEleutherAI ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ›¾ç”¨äºè®­ç»ƒ GPT-NeoX-20Bã€‚
* [**LitGPT**](https://github.com/Lightning-AI/litgpt)ï¼šLightning AI å¯¹æœ€å…ˆè¿›çš„å¼€æº LLMs çš„å®ç°ï¼Œé‡ç‚¹åœ¨äºå¯å¤ç°æ€§ã€‚
* [**DiLoco**](https://github.com/PrimeIntellect-ai/OpenDiLoCo)ï¼šä½¿ç”¨ DiLoCo è·¨è®¡ç®—é›†ç¾¤è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
* [**torchgpipe**](https://github.com/kakaobrain/torchgpipe)ï¼šPyTorch ä¸­çš„ GPipe å®ç°ã€‚
* [**OSLO**](https://github.com/EleutherAI/oslo)ï¼šå¥¥æ–¯é™†ï¼šå¤§è§„æ¨¡ä¼˜åŒ–çš„å¼€æºè½¯ä»¶ã€‚


### 12.3 Debugging

* [**Speed profiling**](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)ï¼šä½¿ç”¨åˆ†æå™¨åˆ†ææ¨¡å‹æ€§èƒ½å’Œç“¶é¢ˆçš„å®˜æ–¹ PyTorch æ•™ç¨‹ã€‚
* [**Memory profiling**](https://pytorch.org/blog/understanding-gpu-memory-1/)ï¼šå…¨é¢äº†è§£å’Œä¼˜åŒ– PyTorch ä¸­ GPU å†…å­˜ä½¿ç”¨çš„æŒ‡å—
* [**Memory profiling walkthrough on a simple example**](https://huggingface.co/blog/train_memory)ï¼šå¯è§†åŒ–å’Œç†è§£ PyTorch ä¸­çš„ GPU å†…å­˜
* [**TensorBoard Profiler Tutorial**](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)ï¼šä½¿ç”¨ TensorBoard çš„ PyTorch æ¨¡å‹åˆ†æå·¥å…·æŒ‡å—ã€‚

### 12.4 åˆ†å¸ƒå¼æŠ€æœ¯

* [**æ•°æ®å¹¶è¡Œ**](https://siboehm.com/articles/22/data-parallel-training)ï¼šæ·±åº¦å­¦ä¹ ä¸­æ•°æ®å¹¶è¡Œè®­ç»ƒçš„å…¨é¢è§£é‡Šã€‚
* [**ZeRO**](https://arxiv.org/abs/1910.02054)ï¼šå¼•å…¥é›¶å†—ä½™ä¼˜åŒ–å™¨ä»¥ä¼˜åŒ–å†…å­˜çš„æ–¹å¼è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚
* [**FSDP**](https://arxiv.org/abs/2304.11277)ï¼šPyTorch ä¸­å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œè®­ç»ƒçš„å®ç°ã€‚
* [**Tensor and Sequence Parallelism + Selective Recomputation**](https://arxiv.org/abs/2205.05198)ï¼šç»“åˆä¸åŒå¹¶è¡Œç­–ç•¥çš„é«˜æ•ˆå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„é«˜çº§æŠ€æœ¯ã€‚
* [**Pipeline parallelism**](https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/#pipeline_parallelism)ï¼šè‹±ä¼Ÿè¾¾å…³äºä¸ºå¤§æ¨¡å‹è®­ç»ƒå®ç°æµæ°´çº¿å¹¶è¡Œçš„æŒ‡å—ã€‚
* [**Breadth first Pipeline Parallelism**](https://arxiv.org/abs/2211.05953)ï¼šåŒ…æ‹¬å›´ç»• PP è¿›åº¦è¡¨çš„å¹¿æ³›è®¨è®ºã€‚
* [**All-reduce**](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/)ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸­ä½¿ç”¨çš„ç¯å½¢å…¨è§„çº¦ç®—æ³•çš„è¯¦ç»†è§£é‡Šã€‚
* [**Ring-flash-attention**](https://github.com/zhuzilin/ring-flash-attention)ï¼šç»“åˆé—ªå­˜æ³¨æ„åŠ›æœºåˆ¶çš„ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶å®ç°é«˜æ•ˆè®­ç»ƒã€‚
* [**Ring attention tutorial**](https://coconut-mode.com/posts/ring-attention/)ï¼šè§£é‡Šç¯å½¢æ³¨æ„åŠ›æ¦‚å¿µå’Œå®ç°çš„æ•™ç¨‹ã€‚
* [**ZeRO and 3D**](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/#understanding-performance-tradeoff-between-zero-and-3d-parallelism)ï¼šæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ DeepSpeed ä¸­å…³äºç†è§£ ZeRO å’Œ 3D å¹¶è¡Œç­–ç•¥ä¹‹é—´æƒè¡¡çš„æŒ‡å—ã€‚
* [**Mixed precision training**](https://arxiv.org/abs/1710.03740)ï¼šä»‹ç»ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ··åˆç²¾åº¦è®­ç»ƒæŠ€æœ¯ã€‚
* [**Visualizing 6D Mesh Parallelism**](https://main-horse.github.io/posts/visualizing-6d/)ï¼šè§£é‡Š 6D å¹¶è¡Œç½‘æ ¼ä¸­æ¶‰åŠçš„é›†ä½“é€šä¿¡ã€‚


### 12.5 ç¡¬ä»¶

* [**Fire-Flyer - a 10,000 PCI chips cluster**](https://www.arxiv.org/abs/2408.14158)ï¼šDeepSeek å…³äºè®¾è®¡ä¸€ä¸ªæ‹¥æœ‰ 1 ä¸‡ä¸ª PCI GPU çš„é›†ç¾¤çš„æŠ¥å‘Šã€‚
* [**Meta's 24k H100 Pods**](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)ï¼šMeta ä½¿ç”¨è‹±ä¼Ÿè¾¾ H100 GPU æ„å»ºçš„å¤§è§„æ¨¡äººå·¥æ™ºèƒ½åŸºç¡€è®¾æ–½çš„è¯¦ç»†æ¦‚è¿°ã€‚
* [**Semianalysis - 100k H100 cluster**](https://www.semianalysis.com/p/100000-h100-clusters-power-network)ï¼šå¤§è§„æ¨¡ H100 GPU é›†ç¾¤åˆ†æåŠå…¶å¯¹äººå·¥æ™ºèƒ½åŸºç¡€è®¾æ–½çš„å½±å“
* [**Modal GPU Glossary**](https://modal.com/gpu-glossary/readme)ï¼šé¢å‘äººç±»çš„ CUDA æ–‡æ¡£

### 12.6 å…¶ä»–

* [**Stas Bekman's Handbook**](https://github.com/stas00/ml-engineering)ï¼šæ¶µç›–è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å„ä¸ªæ–¹é¢çš„ç»¼åˆæ€§æ‰‹å†Œã€‚
* [**Bloom training chronicles**](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md)ï¼šBLOOM æ¨¡å‹è®­ç»ƒè¿‡ç¨‹åŠæŒ‘æˆ˜çš„è¯¦ç»†æ–‡æ¡£ã€‚
* [**OPT logbook**](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)ï¼šMeta è®°å½• OPT-175B æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—ã€‚
* [**Harm's law for training smol models longer**](https://www.harmdevries.com/post/model-size-vs-compute-overhead/)ï¼šå…³äºæ¨¡å‹å¤§å°ä¸è®­ç»ƒå¼€é”€ä¹‹é—´å…³ç³»çš„è°ƒæŸ¥ã€‚
* [**Harm's blog for long context**](https://www.harmdevries.com/post/context-length/)ï¼šå¯¹é•¿ä¸Šä¸‹æ–‡è®­ç»ƒåœ¨æ•°æ®å’Œè®­ç»ƒæˆæœ¬æ–¹é¢çš„è°ƒæŸ¥ã€‚
* [**GPU Mode**](https://www.youtube.com/@GPUMODE/videos)ï¼šä¸€ä¸ª GPU ç ”è¯»å°ç»„å’Œç¤¾åŒºã€‚
* [**EleutherAI Youtube channel**](https://youtube.com/playlist?list=PLvtrkEledFjqOLuDB_9FWL3dgivYqc6-3&si=fKWPotx8BflLAUkf)ï¼šæœºå™¨å­¦ä¹ å¯æ‰©å±•æ€§ä¸æ€§èƒ½é˜…è¯»å°ç»„
* [**Google Jax Scaling book**](https://jax-ml.github.io/scaling-book/)ï¼šå¦‚ä½•æ‰©å±•ä½ çš„æ¨¡å‹
* [**@fvsmassa & @TimDarcet FSDP**](https://github.com/facebookresearch/capi/blob/main/fsdp.py)ï¼šç‹¬ç«‹å®ç°çº¦ 500 è¡Œä»£ç çš„ FSDP
* [**thonking.ai**](https://www.thonking.ai/)ï¼šéœå‹’æ–¯Â·ä½•çš„ä¸€äº›åšå®¢æ–‡ç« â€”â€”è®©æ˜¾å¡â€œå—¡å—¡â€ä½œå“
* [**Aleksa's ELI5 Flash Attention**](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)ï¼šFlash Attention çš„ç®€æ˜“è§£é‡Š
* [**TunibAI's 3D parallelism tutorial**](https://github.com/tunib-ai/large-scale-lm-tutorials)ï¼šä½¿ç”¨ PyTorch è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡æ•™ç¨‹ã€‚

## é™„å½•

### A0: å¹¶è¡Œç¼–ç¨‹é€Ÿæˆè¯¾

åœ¨æ•´ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒè§„æ¨¡ä»ä¸€å° GPU æ‰©å±•åˆ°æ•°ç™¾å° GPUã€‚è¿™å°†éœ€è¦æ‰€æœ‰æœºå™¨ä¹‹é—´å¯¹æƒé‡ã€æ¢¯åº¦å’Œæ•°æ®è¿›è¡Œé€šä¿¡å’ŒåŒæ­¥ã€‚æœ‰ä¸€ç»„åˆ†å¸ƒå¼æ¨¡å¼å¯ä»¥å®ç°è¿™ä¸€ç‚¹ï¼Œç§°ä¸ºé›†ä½“æ“ä½œï¼ˆcollective operationsï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰è¿™äº›æ“ä½œï¼ˆå¦‚å¹¿æ’­ï¼ˆBroadcastï¼‰ã€å…¨è§„çº¦ï¼ˆAllReduceï¼‰ã€åˆ†æ•£ï¼ˆScatterï¼‰ç­‰ï¼‰è¿›è¡Œä¸€ä¸ªç®€è¦çš„ä»‹ç»ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

ä¸€èˆ¬çš„è®¾ç½®æ˜¯ï¼Œæˆ‘ä»¬æœ‰è‹¥å¹²ä¸ªç‹¬ç«‹çš„èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹å¯ä»¥æ˜¯ CPU å†…æ ¸ã€GPU æˆ–è€…è®¡ç®—èŠ‚ç‚¹ã€‚æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œä¸€äº›è®¡ç®—ï¼Œç„¶åæˆ‘ä»¬æƒ³è¦å°†ç»“æœæˆ–å…¶éƒ¨åˆ†å†…å®¹ä¸å…¶ä»–èŠ‚ç‚¹é€šä¿¡ï¼Œä»¥ä¾¿è¿›è¡Œä¸‹ä¸€ä¸ªè®¡ç®—æ­¥éª¤ï¼ˆt+1ï¼‰ã€‚

![image.png|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_general.png)
ä¹Ÿè®¸æˆ‘ä»¬éœ€è¦å°†ä¸€ä¸ªèŠ‚ç‚¹çš„ç»“æœå‘é€åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰ä¸­é—´ç»“æœæ±‚å’Œä»¥æŠ¥å‘Šæ€»ä½“ç»“æœã€‚é€šå¸¸ï¼Œæœ‰ä¸€ä¸ªå…·æœ‰è¾ƒé«˜åœ°ä½çš„èŠ‚ç‚¹èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œåœ¨æ­¤ç”¨ root è¡¨ç¤ºï¼Œå®ƒæ˜¯æŸäº›æ“ä½œçš„æºæˆ–ç›®æ ‡ã€‚è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„åŸè¯­ä¹‹ä¸€å¼€å§‹ï¼šå¹¿æ’­æ“ä½œã€‚

#### å¹¿æ’­

ä¸€ä¸ªéå¸¸å¸¸è§çš„æ¨¡å¼æ˜¯ï¼Œä½ åœ¨èŠ‚ç‚¹ 1 ä¸Šæœ‰ä¸€äº›æ•°æ®ï¼Œå¹¶å¸Œæœ›å°†è¿™äº›æ•°æ®å…±äº«ç»™æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åˆ©ç”¨è¿™äº›æ•°æ®è¿›è¡Œä¸€äº›è®¡ç®—ã€‚å¹¿æ’­æ“ä½œæ­£æ˜¯å®ç°è¿™ä¸€ç›®çš„çš„ã€‚

![image.png|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_broadcast.png)
PyTorch åŸç”Ÿæä¾›äº†é›†ä½“æ“ä½œï¼ˆcollective operationsï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è½»æ¾ç¼–å†™ä¸€ä¸ªå°ç¤ºä¾‹æ¥æ¼”ç¤ºå¹¿æ’­ï¼ˆbroadcastingï¼‰çš„å·¥ä½œåŸç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨Â `dist.init_process_group`Â åˆå§‹åŒ–ä¸€ä¸ªè¿›ç¨‹ç»„ï¼Œè¯¥å‡½æ•°ä¼šè®¾ç½®é€šä¿¡åç«¯ï¼ˆç¨åæˆ‘ä»¬ä¼šè®¨è®º NCCLï¼‰ï¼Œç¡®å®šå­˜åœ¨å¤šå°‘ä¸ªå·¥ä½œè¿›ç¨‹ï¼ˆä¹Ÿç§°ä¸ºèŠ‚ç‚¹ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªå·¥ä½œè¿›ç¨‹åˆ†é…ä¸€ä¸ªç§©ï¼ˆrankï¼Œå¯ä»¥é€šè¿‡Â `dist.get_rank`Â è·å–ï¼‰ã€‚æœ€åï¼Œå®ƒä¼šåœ¨å·¥ä½œè¿›ç¨‹ä¹‹é—´å»ºç«‹è¿æ¥ã€‚

ä¸ºäº†å±•ç¤ºÂ `dist.broadcast`Â æ“ä½œï¼Œè®©æˆ‘ä»¬åœ¨Â `rank=0`Â ä¸Šåˆ›å»ºä¸€ä¸ªå…·æœ‰éé›¶å€¼çš„å¼ é‡ï¼Œå¹¶åœ¨å…¶ä»–å·¥ä½œèŠ‚ç‚¹ä¸Šåˆ›å»ºå…¨é›¶å¼ é‡ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨Â `dist.broadcast(tensor, src=0)`Â å°†Â `rank=0`Â ä¸Šçš„å¼ é‡åˆ†å‘åˆ°æ‰€æœ‰å…¶ä»–å·¥ä½œèŠ‚ç‚¹ã€‚

```python
import torch
import torch.distributed as dist

def init_process():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(dist.get_rank())
    
def example_broadcast():
    if dist.get_rank() == 0:
        tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).cuda()
    else:
        tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before broadcast on rank {dist.get_rank()}: {tensor}")
    dist.broadcast(tensor, src=0)
    print(f"After broadcast on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_broadcast()
```

ä½ å¯ä»¥ä½¿ç”¨Â `torchrun --nproc_per_node=3 dist_op.py`Â è¿è¡Œä¸Šè¿°è„šæœ¬ï¼ˆä¸ºæ­¤ä½ éœ€è¦ 3 å— GPUï¼Œæˆ–è€…ç›¸åº”åœ°æ›´æ”¹Â `nproc_per_node`ï¼‰ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹è¾“å‡ºã€‚

```python
Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device='cuda:1')
After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device='cuda:2')
```

å¾ˆå¥½ï¼Œçœ‹èµ·æ¥å®ƒæŒ‰é¢„æœŸå·¥ä½œã€‚è¯·æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬æ— æ³•æ§åˆ¶å“ªä¸ªæ‰“å°è¯­å¥å…ˆæ‰§è¡Œï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œä¸ºäº†ä¾¿äºé˜…è¯»å¯¹å®ƒä»¬è¿›è¡Œäº†æ’åºï¼‰ï¼Œæ’åæ¶ˆæ¯å¯èƒ½ä¼šä»¥ä¹±åºæ‰“å°å‡ºæ¥ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­è®¨è®º Reduce å’Œ AllReduce æ¨¡å¼ï¼

#### Reduce & AllReduce

å½’çº¦æ¨¡å¼æ˜¯åˆ†å¸ƒå¼æ•°æ®å¤„ç†ä¸­æœ€åŸºæœ¬çš„æ¨¡å¼ä¹‹ä¸€ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸€ä¸ªå‡½æ•° `f()`ï¼ˆä¾‹å¦‚æ±‚å’Œæˆ–æ±‚å¹³å‡å€¼ï¼‰å°†æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„æ•°æ®è¿›è¡Œåˆå¹¶ã€‚åœ¨å½’çº¦èŒƒå¼ä¸­ï¼Œç»“æœä»…å‘é€åˆ°æ ¹èŠ‚ç‚¹ï¼›è€Œåœ¨å…¨å½’çº¦ï¼ˆAllReduceï¼‰æƒ…å†µä¸‹ï¼Œç»“æœä¼šè¢«å¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_reduce_allreduce.png)

å½“ç„¶ï¼Œä¸å­˜åœ¨èƒ½å¤Ÿæ‰§è¡Œæ­¤æ“ä½œçš„ç¥å¥‡â€œè‡ªç”±é£è¡Œâ€èŠ‚ç‚¹ï¼Œé€šå¸¸æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šåœ¨èŠ‚ç‚¹çš„ç¯å½¢æˆ–æ ‘å½¢ç»“æ„ä¸­è¿›è¡Œéƒ¨åˆ†è®¡ç®—ã€‚ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼šå‡è®¾æˆ‘ä»¬éœ€è¦è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„æ•°å­—ä¹‹å’Œï¼Œå¹¶ä¸”æˆ‘ä»¬çš„èŠ‚ç‚¹ä»¥ç¯å½¢æ¨¡å¼ç›¸è¿ã€‚ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å°†å…¶æ•°å­—å‘é€ç»™ä¸€ä¸ªé‚»å±…èŠ‚ç‚¹ï¼Œè¯¥é‚»å±…èŠ‚ç‚¹å°†è‡ªèº«çš„æ•°å­—ä¸æ¥æ”¶åˆ°çš„æ•°å­—ç›¸åŠ ï¼Œç„¶åå†å°†å…¶è½¬å‘ç»™ä¸‹ä¸€ä¸ªé‚»å±…èŠ‚ç‚¹ã€‚åœ¨èŠ‚ç‚¹ç¯çš„ä¸€è½®ä¼ é€’ç»“æŸåï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹å°†æ¥æ”¶åˆ°æ€»å’Œã€‚

ä»¥ä¸‹æ˜¯è¿è¡Œä¸€ä¸ªç®€å•å½’çº¦æ“ä½œçš„ä»£ç ï¼Œç”¨äºå¯¹å¼ é‡æ±‚å’Œï¼Œæˆ‘ä»¬é€šè¿‡Â `op=dist.ReduceOp.SUM`Â æŒ‡å®šè¦ä½¿ç”¨çš„æ“ä½œï¼ˆä½ å¯ä»¥åœ¨ [Pytorch æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp)ä¸­æ‰¾åˆ°æ”¯æŒçš„æ›´å¤šæ“ä½œä¿¡æ¯ï¼‰ã€‚

```python
def example_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before reduce on rank {dist.get_rank()}: {tensor}")
    dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
    print(f"After reduce on rank {rank}: {tensor}")
    
init_process()
example_reduce()
```

è¯·æ³¨æ„ï¼Œåœ¨ Reduce æ“ä½œä¸­ï¼Œåªæœ‰ `dst` èŠ‚ç‚¹ä¸Šçš„å¼ é‡ä¼šè¢«æ›´æ–°ã€‚

```python
Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä¸€ä¸ª AllReduce æ“ä½œï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¸éœ€è¦æŒ‡å®šç›®æ ‡ï¼‰ã€‚

```python
def example_all_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before all_reduce on rank {dist.get_rank()}: {tensor}")
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    print(f"After all_reduce on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_all_reduce()
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€æœ‰èŠ‚ç‚¹ä¸Šéƒ½æœ‰ç»“æœå¯ç”¨ã€‚

```python
Before all_reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After all_reduce on rank 1: tensor([6., 6., 6., 6., 6.], device='cuda:1')
After all_reduce on rank 2: tensor([6., 6., 6., 6., 6.], device='cuda:2')
```

ç°åœ¨è®©æˆ‘ä»¬è½¬å‘ä¸‹ä¸€ä¸ªåˆ†å¸ƒå¼é€šä¿¡æ“ä½œã€‚åœ¨è®¸å¤šå®é™…æƒ…å†µä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹å•ç‹¬æ‰§è¡Œè®¸å¤šå¤æ‚çš„è®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦åœ¨èŠ‚ç‚¹ä¹‹é—´å…±äº«æœ€ç»ˆç»“æœã€‚Gather å’Œ AllGather æ˜¯æˆ‘ä»¬åœ¨è¿™ç§æƒ…å†µä¸‹æƒ³è¦ä½¿ç”¨çš„æ“ä½œã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹ï¼

#### Gather & AllGather

â€œGatherâ€ å’Œ â€œAllGatherâ€ ä¸ â€œBroadcastâ€ éå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºå®ƒä»¬éƒ½å…è®¸åœ¨èŠ‚ç‚¹ä¹‹é—´åˆ†å‘æ•°æ®è€Œä¸è¿›è¡Œä¿®æ”¹ã€‚ä¸â€œBroadcastâ€çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå¹¶ä¸æ˜¯éœ€è¦ä»ä¸€ä¸ªèŠ‚ç‚¹å‘æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å…±äº«ä¸€ä¸ªå€¼ï¼Œè€Œæ˜¯æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å„è‡ªçš„æ•°æ®å—ï¼Œæˆ‘ä»¬å¸Œæœ›å°†è¿™äº›æ•°æ®å—å…¨éƒ¨æ”¶é›†åˆ°ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼ˆåœ¨ â€œGatherâ€ çš„æƒ…å†µä¸‹ï¼‰ï¼Œæˆ–è€…å°†æ‰€æœ‰æ•°æ®å—æ”¶é›†åˆ°æ‰€æœ‰èŠ‚ç‚¹ä¸Šï¼ˆåœ¨ â€œAllGatherâ€ çš„æƒ…å†µä¸‹ï¼‰ã€‚ä¸€å›¾èƒœåƒè¨€ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_gather_allgather.png)

è¯·æ³¨æ„ï¼Œè™šçº¿è¡¨ç¤ºæŸäº›æ•°æ®å®é™…ä¸Šæ ¹æœ¬ä¸ä¼šç§»åŠ¨ï¼ˆå› ä¸ºå®ƒå·²ç»å­˜åœ¨äºèŠ‚ç‚¹ä¸Šï¼‰ã€‚

åœ¨ gather æ“ä½œçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªå®¹å™¨å¯¹è±¡ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯ `gather_list`ï¼Œç”¨äºå­˜å‚¨æ”¶é›†åˆ°çš„å¼ é‡ã€‚

```python
def example_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    if dist.get_rank() == 0:
        gather_list = [
            torch.zeros(5, dtype=torch.float32).cuda()
            for _ in range(dist.get_world_size())
            ]
    else:
        gather_list = None
    print(f"Before gather on rank {dist.get_rank()}: {tensor}")
    dist.gather(tensor, gather_list, dst=0)
    if dist.get_rank() == 0:
        print(f"After gather on rank 0: {gather_list}")
    
init_process()
example_gather()
```

æˆ‘ä»¬çœ‹åˆ°ï¼Œ`gather_list` ç¡®å®åŒ…å«äº†æ‰€æœ‰ rank çš„å¼ é‡ã€‚

```python
Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                         tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                         tensor([3., 3., 3., 3., 3.], device='cuda:0')]
```

æˆ‘ä»¬å”¯ä¸€éœ€è¦ä¸º AllGather ç¤ºä¾‹æ›´æ”¹çš„æ˜¯ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½éœ€è¦ä¸€ä¸ªç”¨äºå­˜å‚¨ç»“æœçš„å ä½ç¬¦ã€‚

```python
def example_all_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    gather_list = [
        torch.zeros(5, dtype=torch.float32).cuda()
        for _ in range(dist.get_world_size())
        ]
    print(f"Before all_gather on rank {dist.get_rank()}: {tensor}")
    dist.all_gather(gather_list, tensor)
    print(f"After all_gather on rank {dist.get_rank()}: {gather_list}")
    
init_process()
example_all_gather()
```

ç¡®å®æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æ¯ä¸ªèŠ‚ç‚¹éƒ½æ‹¥æœ‰æ‰€æœ‰çš„æ•°æ®ã€‚

```python
Before all_gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 1: [tensor([1., 1., 1., 1., 1.], device='cuda:1'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 2: [tensor([1., 1., 1., 1., 1.], device='cuda:2'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:2'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:2')]
```

é‚£ä¹ˆï¼Œæ”¶é›†ï¼ˆgatherï¼‰æ“ä½œçš„é€†æ“ä½œæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªèŠ‚ç‚¹ä¸Šæ‹¥æœ‰æ‰€æœ‰æ•°æ®ï¼Œå¹¶å¸Œæœ›å°†å…¶åˆ†é…/åˆ‡åˆ†åˆ°å„ä¸ªèŠ‚ç‚¹ï¼Œå¯èƒ½è¿˜ä¼šè¿›è¡Œä¸€äº›ä¸­é—´å¤„ç†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆ†æ•£ï¼ˆScatterï¼‰æ“ä½œï¼Œæˆ–è€…åœ¨æœ‰ä¸­é—´å¤„ç†æ“ä½œçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å½’çº¦åˆ†æ•£ï¼ˆReduce Scatterï¼‰æ¨¡å¼ã€‚

#### Scatter & ReduceScatter

æ­£å¦‚å…¶åç§°æ‰€éšå«çš„æ„æ€ï¼ŒScatterï¼ˆåˆ†æ•£ï¼‰æ“ä½œçš„ç›®æ ‡æ˜¯å°†ä¸€ä¸ªèŠ‚ç‚¹ä¸Šçš„æ•°æ®åˆ‡ç‰‡ååˆ†å‘åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ã€‚å› æ­¤ï¼Œå®ƒä¸ Broadcastï¼ˆå¹¿æ’­ï¼‰æ“ä½œä¸åŒï¼Œåè€…æ˜¯å¤åˆ¶æ•°æ®è€Œä¸è¿›è¡Œåˆ‡ç‰‡ï¼›å¹¶ä¸”ä»é€»è¾‘ä¸Šæ¥è¯´ï¼Œå®ƒä¸ Gatherï¼ˆèšé›†ï¼‰æ“ä½œäº’ä¸ºé€†æ“ä½œã€‚

ReduceScatter æ¨¡å¼ç¨å¾®å¤æ‚ä¸€äº›ï¼šæƒ³è±¡ä¸€ä¸‹ï¼Œä½ åº”ç”¨çš„æ“ä½œç±»ä¼¼äº Reduce æƒ…å†µä¸­çš„æ“ä½œï¼Œä½†ä¸ä»…å°†ç»“æœç§»åŠ¨åˆ°ä¸€ä¸ªèŠ‚ç‚¹ä¸åŒï¼Œæˆ‘ä»¬è¿˜ä¼šå°†å…¶å‡åŒ€åœ°åˆ†å‘åˆ°æ‰€æœ‰èŠ‚ç‚¹ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_scatter_reducescatter.png)
â€œScatterâ€ æ“ä½œåœ¨ä»£ç ä¸­å†™æ³•ä¸ â€œGatherâ€ ç›¸åï¼šæˆ‘ä»¬ä¸æ˜¯å‡†å¤‡ä¸€ä¸ªå¼ é‡åˆ—è¡¨ä½œä¸ºç›®æ ‡ï¼Œè€Œæ˜¯å°†æºæ•°æ®å‡†å¤‡æˆä¸€ä¸ªæˆ‘ä»¬æƒ³è¦åˆ†å‘çš„å¼ é‡åˆ—è¡¨ã€‚æˆ‘ä»¬è¿˜éœ€è¦æŒ‡å®šæºï¼ˆ`src`ï¼‰ã€‚

```python
def example_scatter():
    if dist.get_rank() == 0:
        scatter_list = [
            torch.tensor([i + 1] * 5, dtype=torch.float32).cuda()
            for i in range(dist.get_world_size())
            ]
        print(f"Rank 0: Tensor to scatter: {scatter_list}")
    else:
        scatter_list = None
    tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before scatter on rank {dist.get_rank()}: {tensor}")
    dist.scatter(tensor, scatter_list, src=0)
    print(f"After scatter on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_scatter()
```

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç©ºå¼ é‡æ˜¯å¦‚ä½•è¢« `scatter_list` ä¸­çš„å†…å®¹å¡«å……çš„ã€‚

```python
Rank 0: Tensor to scatter: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                            tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                            tensor([3., 3., 3., 3., 3.], device='cuda:0')]
Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device='cuda:0')
Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

è®©æˆ‘ä»¬åˆ›å»ºæ›´æœ‰è¶£çš„æ•°æ®æ¥æ¼”ç¤º ReduceScatter çš„é€»è¾‘ï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å« 2 å…ƒç´ å‘é‡çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå‘é‡åŒ…å«ä¸€ä¸ªå¹‚æŒ‡æ•°å’Œä¸€ä¸ªåŸºäºèŠ‚ç‚¹ç­‰çº§çš„åç§»å‡½æ•°ï¼ˆè¿™æœ‰ç‚¹éš¾ä»¥æƒ³è±¡ï¼Œæ‰€ä»¥è¯·çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼‰ã€‚

```python
def example_reduce_scatter():
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    input_tensor = [
        torch.tensor([(rank + 1) * i for i in range(1, 3)], dtype=torch.float32).cuda()**(j+1) 
        for j in range(world_size)
        ]
    output_tensor = torch.zeros(2, dtype=torch.float32).cuda()
    print(f"Before ReduceScatter on rank {rank}: {input_tensor}")
    dist.reduce_scatter(output_tensor, input_tensor, op=dist.ReduceOp.SUM)
    print(f"After ReduceScatter on rank {rank}: {output_tensor}")    
    
init_process()
example_reduce_scatter()
```

è®©æˆ‘ä»¬æ‰“å°å‡ºæˆ‘ä»¬åˆ›å»ºçš„æ•°æ®æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜ç«‹å³çœ‹åˆ°äº† ReduceScatter æ¨¡å¼ï¼šç¬¬ä¸€ä¸ªç§©æ¥æ”¶äº†æ¯ä¸ªèŠ‚ç‚¹ä¸Šç¬¬ä¸€ä¸ªå¼ é‡çš„æ€»å’Œï¼Œç¬¬äºŒä¸ªç§©åŒ…å«äº†æ¯ä¸ªèŠ‚ç‚¹ä¸Šç¬¬äºŒä¸ªå¼ é‡çš„æ€»å’Œï¼Œä¾æ­¤ç±»æ¨ã€‚

```python
Before ReduceScatter on rank 0: [tensor([1., 2.], device='cuda:0'),
											 tensor([1., 4.], device='cuda:0'),
											 tensor([1., 8.], device='cuda:0')]
Before ReduceScatter on rank 1: [tensor([2., 4.], device='cuda:1'),
                                 tensor([ 4., 16.], device='cuda:1'),
                                 tensor([ 8., 64.], device='cuda:1')]
Before ReduceScatter on rank 2: [tensor([3., 6.], device='cuda:2'),
                                 tensor([ 9., 36.], device='cuda:2'),
                                 tensor([ 27., 216.], device='cuda:2')]

After ReduceScatter on rank 0: tensor([ 6., 12.], device='cuda:0')
After ReduceScatter on rank 1: tensor([14., 56.], device='cuda:1')
After ReduceScatter on rank 2: tensor([ 36., 288.], device='cuda:2')
```

è®©æˆ‘ä»¬å¿«é€Ÿäº†è§£ä¸€ä¸‹ä¸€ç§å¸¸è§çš„ AllReduce å®ç°ï¼Œå®ƒä½¿ç”¨ ReduceScatter å’Œ AllGatherï¼šç¯å½¢ AllReduceã€‚

#### Ring AllReduce

â€œç¯å½¢å…¨è§„çº¦ï¼ˆRing AllReduceï¼‰â€æ˜¯å…¨è§„çº¦ï¼ˆAllReduceï¼‰çš„ä¸€ç§å…·ä½“å®ç°æ–¹å¼ï¼Œå…¶é’ˆå¯¹å¯æ‰©å±•æ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸æ‰€æœ‰è®¾å¤‡ç›´æ¥ç›¸äº’é€šä¿¡ï¼ˆè¿™å¯èƒ½ä¼šé€ æˆé€šä¿¡ç“¶é¢ˆï¼‰ä¸åŒï¼Œâ€œç¯å½¢å…¨è§„çº¦â€å¯åˆ†ä¸ºä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šè§„çº¦åˆ†æ•£ï¼ˆReduceScatterï¼‰å’Œå…¨æ”¶é›†ï¼ˆAllGatherï¼‰ã€‚å…¶å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

1. **ReduceScatter**

	- æ¯ä¸ªè®¾å¤‡å°†å…¶æ•°æ®ï¼ˆä¾‹å¦‚æ¢¯åº¦ï¼‰åˆ†å‰²æˆå—ï¼Œå¹¶å°†å…¶ä¸­ä¸€ä¸ªå—å‘é€ç»™å…¶é‚»å±…è®¾å¤‡ã€‚åŒæ—¶ï¼Œæ¯ä¸ªè®¾å¤‡ä»å…¶å¦ä¸€ä¸ªé‚»å±…è®¾å¤‡æ¥æ”¶ä¸€ä¸ªå—ã€‚
	- å½“æ¯ä¸ªè®¾å¤‡æ¥æ”¶åˆ°ä¸€ä¸ªå—æ—¶ï¼Œå®ƒä¼šå°†ç›¸åº”çš„å—ä¸æ¥æ”¶åˆ°çš„å—ç›¸åŠ ï¼ˆè¿›è¡Œå½’çº¦æ“ä½œï¼‰ã€‚
	- è¿™ä¸€è¿‡ç¨‹åœ¨ç¯å½¢ç½‘ç»œä¸­æŒç»­è¿›è¡Œï¼Œç›´åˆ°æ¯ä¸ªè®¾å¤‡æŒæœ‰ä¸€ä¸ªéƒ¨åˆ†å½’çº¦çš„å—ï¼Œè¯¥å—ä»£è¡¨äº†æ‰€æœ‰è®¾å¤‡ä¸­è¯¥å—çš„æ¢¯åº¦æ€»å’Œã€‚

2. **AllGather**

	- ç°åœ¨ï¼Œæ¯ä¸ªè®¾å¤‡éƒ½éœ€è¦ä»å…¶ä»–è®¾å¤‡æ”¶é›†å®Œå…¨è§„çº¦åçš„æ•°æ®å—ã€‚
	- å„è®¾å¤‡å¼€å§‹å°†è§„çº¦åçš„æ•°æ®å—å‘é€ç»™ç›¸é‚»è®¾å¤‡ã€‚
	- æ¯ä¸ªè®¾å¤‡è½¬å‘å…¶æ¥æ”¶åˆ°çš„æ•°æ®å—ï¼Œç›´åˆ°æ¯ä¸ªè®¾å¤‡éƒ½æ‹¥æœ‰æ‰€æœ‰å®Œå…¨è§„çº¦åçš„æ•°æ®å—ï¼Œä»è€Œä½¿æ¯ä¸ªè®¾å¤‡è·å¾—å®Œæ•´çš„ã€æ±‡æ€»åçš„æ¢¯åº¦ã€‚

è®©æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹åŠ¨å›¾æ¥è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè¿™é‡Œæœ‰ 5 å— GPUï¼Œæ¯å— GPU éƒ½æœ‰ä¸€ä¸ªé•¿åº¦ä¸º 5 çš„å¼ é‡ã€‚ç¬¬ä¸€ä¸ªåŠ¨ç”»å±•ç¤ºäº† ReduceScatter æ­¥éª¤ï¼Œåœ¨è¯¥æ­¥éª¤ç»“æŸæ—¶ï¼Œæ¯å— GPU éƒ½ä¼šæ¥æ”¶åˆ°ç‰¹å®šæ•°æ®å—ï¼ˆæ©™è‰²çŸ©å½¢ï¼‰çš„å½’çº¦ç»“æœã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_reduce_scatter.gif)
ä¸‹ä¸€ä¸ªåŠ¨ç”»å±•ç¤ºäº† AllGather æ­¥éª¤ï¼Œåœ¨è¯¥æ­¥éª¤ç»“æŸæ—¶ï¼Œæ¯ä¸ª GPU éƒ½ä¼šè·å¾— AllReduce æ“ä½œçš„å®Œæ•´ç»“æœã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_all_gather.gif)

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œåœ¨ reduce-scatter å’Œ all-gather æ­¥éª¤ä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¼šå‘é€å’Œæ¥æ”¶ $N-1$ æ¬¡æ•°æ®ã€‚æ¯æ¬¡ä¼ è¾“æ—¶ï¼Œæ¯ä¸ª GPU å‘é€ $\frac{K}{N}$ ä¸ªå€¼ï¼Œå…¶ä¸­ $K$ æ˜¯è·¨ GPU ç´¯åŠ çš„æ•°ç»„çš„æ€»å€¼æ•°ã€‚å› æ­¤ï¼Œæ¯ä¸ª GPU ä¼ è¾“å’Œæ¥æ”¶çš„æ•°æ®æ€»é‡æ˜¯ $2 \times (N-1) \times \frac{K}{N}$ã€‚å½“ $N$ï¼ˆGPU çš„æ•°é‡ï¼‰å¾ˆå¤§æ—¶ï¼Œæ¯ä¸ª GPU ä¼ è¾“å’Œæ¥æ”¶çš„æ•°æ®æ€»é‡å¤§çº¦æ˜¯ $2 \times K$ï¼Œå…¶ä¸­ $K$ æ˜¯å‚æ•°çš„æ€»æ•°ã€‚

**åœ¨ AllReduce ä¸­éœ€è¦ç‰¢è®°ä¸¤ä¸ªå…³é”®è¦ç‚¹ï¼š**

1. å½“ $N$ï¼ˆGPU çš„æ•°é‡ï¼‰è¾ƒå¤§æ—¶ï¼ŒAllReduce çš„é€šä¿¡æˆæœ¬å¤§çº¦ä¸º $2Ã—K$ã€‚
2. AllReduce æ“ä½œå¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ªè§„çº¦-åˆ†æ•£ï¼ˆreduce-scatterï¼‰æ“ä½œï¼Œåæ¥ä¸€ä¸ªå…¨æ”¶é›†ï¼ˆall-gatherï¼‰æ“ä½œã€‚è¿™ä¸¤ä¸ªæ“ä½œçš„é€šä¿¡å¼€é”€æ˜¯ AllReduce çš„ä¸€åŠï¼Œå¤§çº¦ä¸º $K$ã€‚

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè¿™ç§å®ç°æ–¹å¼å³ä½¿åœ¨èŠ‚ç‚¹é—´å¸¦å®½æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆåˆ©ç”¨ã€‚

æˆ‘ä»¬ç°åœ¨äº†è§£äº†åˆ†å¸ƒå¼æ“ä½œçš„ä¸»è¦æ„å»ºæ¨¡å—ï¼Œä½†åœ¨å®é™…çœ‹åˆ°å®ƒä»¬ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ç§ç”¨äºåŒæ­¥çš„ç‰¹æ®Šæ“ä½œï¼šå±éšœï¼ˆBarrierï¼‰ã€‚

#### Barrier

å±éšœï¼ˆBarrierï¼‰æ˜¯ä¸€ç§ç®€å•çš„æ“ä½œï¼Œç”¨äºåŒæ­¥æ‰€æœ‰èŠ‚ç‚¹ã€‚åœ¨æ‰€æœ‰èŠ‚ç‚¹éƒ½åˆ°è¾¾å±éšœä¹‹å‰ï¼Œå±éšœä¸ä¼šè¢«è§£é™¤ã€‚åªæœ‰åˆ°é‚£æ—¶ï¼Œå®ƒä»¬æ‰è¢«å…è®¸ç»§ç»­è¿›è¡Œåç»­çš„è®¡ç®—ã€‚

![image.png|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_barrier.png)

æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè®¾ç½®ä¸åŒçš„ä¼‘çœ æ—¶é—´æ¥è½»æ¾æ¨¡æ‹Ÿå»¶è¿ŸèŠ‚ç‚¹ï¼Œå¹¶è§‚å¯Ÿå®ƒä»¬å…¨éƒ¨é€šè¿‡å±éšœæ‰€éœ€çš„æ—¶é—´ã€‚

```python
def example_barrier():
    rank = dist.get_rank()
    t_start = time.time()
    print(f"Rank {rank} sleeps {rank} seconds.")
    time.sleep(rank)  # Simulate different processing times
    dist.barrier()
    print(f"Rank {rank} after barrier time delta: {time.time()-t_start:.4f}")
    
init_process()
example_barrier()
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡æ’åç¬¬ä¸€çš„ï¼ˆå¯¹è±¡ï¼‰æ ¹æœ¬æ²¡æœ‰ç¡è§‰ï¼Œä½†å®ƒé€šè¿‡éšœç¢ä¹ŸèŠ±äº† 2 ç§’é’Ÿã€‚

```python
Rank 0 sleeps 0 seconds.
Rank 1 sleeps 1 seconds.
Rank 2 sleeps 2 seconds.

Rank 0 after barrier time delta: 2.0025
Rank 1 after barrier time delta: 2.0025
Rank 2 after barrier time delta: 2.0024
```

æˆ‘ä»¬éœ€è¦è°¨æ…å¯¹å¾…åƒè¿™æ ·åŒæ­¥æ‰€æœ‰èŠ‚ç‚¹çš„æ“ä½œï¼Œå› ä¸ºè¿™è¿èƒŒäº†å¹¶è¡Œç‹¬ç«‹æ“ä½œçš„åˆè¡·ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•´ä¸ªå¤„ç†è¿‡ç¨‹å˜æ…¢ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¦‚æœä¸€ä¸ªé€Ÿåº¦å¿«çš„èŠ‚ç‚¹å·²ç»å¼€å§‹å¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡ä¹Ÿæ²¡å…³ç³»ï¼Œå› ä¸ºè¯¥èŠ‚ç‚¹åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­å¯èƒ½ä¼šå˜æ…¢ï¼Œä»è€Œåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å¹³è¡¡æ‰å»¶è¿Ÿã€‚

åœ¨æ¢è®¨å®é™…çš„åˆ†å¸ƒå¼è®­ç»ƒå®ç°ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè§£å¼€ä¸€ä¸ªè°œå›¢ï¼šNCCL åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

#### NCCL: è‹±ä¼Ÿè¾¾é›†åˆé€šä¿¡åº“

åœ¨å¤š GPU ä¸Šè®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æœ‰æ—¶å¯èƒ½ä¼šå–å¾—çªç ´ï¼Œä½†æ€»ä¼šé‡åˆ°é•ï¼ˆæˆ–è€… NCCL ğŸ¥ï¼‰ï¼é‚£æ˜¯ä»€ä¹ˆï¼Ÿ

æœ‰å‡ ä¸ªå®ç°äº†é›†ä½“é€šä¿¡çš„åº“å— PyTorch æ”¯æŒï¼šæœ‰ç»å…¸çš„æ¶ˆæ¯ä¼ é€’æ¥å£ï¼ˆMPIï¼‰ï¼ŒMeta å¼€å‘çš„ Glooï¼Œæœ€åè¿˜æœ‰è‹±ä¼Ÿè¾¾é›†ä½“é€šä¿¡åº“ï¼ˆNCCLï¼‰ã€‚å®ƒä»¬åœ¨é›†ä½“é€šä¿¡æ¨¡å¼æ–¹é¢éƒ½æä¾›äº†ç±»ä¼¼çš„åŠŸèƒ½ï¼Œä½†é’ˆå¯¹ä¸åŒçš„ç¡¬ä»¶é…ç½®è¿›è¡Œäº†ä¼˜åŒ–ï¼›NCCL æ—¨åœ¨é«˜æ•ˆåœ°æœåŠ¡äº GPU - GPU é€šä¿¡ï¼Œè€Œ MPI å’Œ Gloo åˆ™é€‚ç”¨äº CPU - CPU æˆ– CPU - GPU é€šä¿¡ã€‚PyTorch æä¾›äº†ä¸€ä»½å¾ˆå¥½çš„æŒ‡å—æ¥å¸®åŠ©å†³å®šä½¿ç”¨å“ªä¸€ä¸ªã€‚

- GPU training: use NCCL
- CPU training: use Gloo

å†³ç­–æ ‘ä¸­æœ‰ä¸€äº›ç»†å¾®ä¹‹å¤„ï¼Œæˆ‘ä»¬ç•™ç»™è¯»è€…åœ¨ä¸Šé¢æåˆ°çš„ PyTorch æŒ‡å—ä¸­å»æ¢ç´¢ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è®²è§£äº†åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºæœ¬æ“ä½œï¼Œä½ ç°åœ¨åº”è¯¥èƒ½å¤Ÿè½»æ¾åœ°é˜…è¯»è¿™ç¯‡åšå®¢æ–‡ç« äº†ã€‚

### A1: åˆ†å¸ƒå¼è®­ç»ƒåˆ†æ

#### Kernels

è®©æˆ‘ä»¬å…ˆå‡è®¾ç°åœ¨è¿™äº›å†…æ ¸å·²ç»é›†æˆåˆ° PyTorch ä¸­äº†ã€‚ä½œä¸ºç®€å•ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹ PyTorch ä¸­å®ç°çš„Â `torch.nn.functional.layer_norm`Â å±‚å½’ä¸€åŒ–å‡½æ•°ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥å¯¹è¿™ä¸ªå‡½æ•°åº•å±‚çš„å†…æ ¸è¿›è¡Œæ€§èƒ½åˆ†æã€‚æœ€ç›´æ¥çš„æ–¹æ³•å¯èƒ½æ˜¯ä½¿ç”¨ Python çš„Â `time`Â æ¨¡å—ã€‚ç„¶è€Œï¼Œç”±äº CUDA æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•æµ‹é‡æ—¶é—´åªä¼šæ•è·åœ¨ Python ä¸­å¯åŠ¨å†…æ ¸æ‰€å…³è”çš„å¼€é”€ï¼Œè€Œä¸æ˜¯å†…æ ¸æœ¬èº«çš„å®é™…æ‰§è¡Œæ—¶é—´ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨Â `torch.cuda.Event`Â è¿›è¡Œç²¾ç¡®è®¡æ—¶ï¼Œå¹¶ä½¿ç”¨Â `torch.cuda.synchronize()`Â æŒ‡ä»¤æ¥ç¡®ä¿ç­‰å¾…å†…æ ¸æ‰§è¡Œå®Œæˆã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†è¿™ç§æ–¹æ³•ã€‚

```python
def profile_pytorch(func, input):
    # Create CUDA events to track time. CUDA operations are asynchronous,
    start = torch.cuda.Event(enable_timing=True)  # Event to mark the start time
    end = torch.cuda.Event(enable_timing=True)    # Event to mark the end time
    # Warmup to eliminate any overhead from the first run, which might not reflect 
    # the actual performance.
    for _ in range(10):
        func(input)
    # Record the start time before executing the function
    start.record()  
    func(input)  # Call the function we want to profile
    # Record the end time after the function has completed
    end.record()  
    # Synchronize the CUDA operations to ensure all operations are completed
    # before measuring the elapsed time.
    torch.cuda.synchronize()  
    # Calculate and return the elapsed time in milliseconds.
    return start.elapsed_time(end)
```

ä¸€ç§æ›´æœ‰æ•ˆçš„æ€§èƒ½åˆ†ææ–¹æ³•æ˜¯ä½¿ç”¨ä¹‹å‰æåˆ°çš„ PyTorch Profilerã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹ä»£ç ï¼š

```python
import torch
import torch.nn.functional as F

def pytorch_layer_norm(input):
    return F.layer_norm(input, input.size()[1:])

a = torch.randn(10000, 10000).cuda()

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,  # Profile CPU activities
        torch.profiler.ProfilerActivity.CUDA,  # Profile CUDA activities
    ],
    # Define a schedule for the profiler
    schedule=torch.profiler.schedule(
        wait=1,      # Wait for 1 iteration before starting to profile
        warmup=3,    # Warm up for 3 iterations to stabilize performance
        active=2,    # Profile for 2 active iterations
        repeat=1,    # Repeat the profiling schedule once
    ),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('.'),
    
) as p:
    for iter in range(10):
        pytorch_layer_norm(a)
        p.step()

# Print a table of the profiling results, sorted by total CUDA time, limited to the top 10 entries
print(p.key_averages().table(sort_by="cuda_time_total", row_limit=8))
```

è¿™å°†æ‰“å°æŒ‰æ€» CUDA æ—¶é—´æ’åºçš„èšåˆåˆ†æç»“æœï¼Œè¾“å‡ºå†…å®¹å¦‚ä¸‹ï¼š

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_kernels.png)

ä½ ä¹Ÿå¯ä»¥å°è¯•æŒ‰ç…§æˆ‘ä»¬ä¹‹å‰æ‰€è¯´çš„ï¼Œåœ¨ chrome://tracing/ ä¸­æ£€æŸ¥è·Ÿè¸ªä¿¡æ¯ã€‚

> [!tip]
> å¦‚æœä½ æ˜¯åˆæ¬¡ä½¿ç”¨æ­¤å·¥å…·ï¼Œå¯ä»¥é€šè¿‡å·¦å³ç®­å¤´é”®æµè§ˆè½¨è¿¹ã€‚æ­¤å¤–ï¼ŒæŒ‰ä½ Alt é”®çš„åŒæ—¶ç”¨é¼ æ ‡å·¦å³æ»šåŠ¨å¯ä»¥æ”¾å¤§æˆ–ç¼©å°ã€‚

æ”¾å¤§åï¼Œæ‚¨å¯ä»¥åœ¨æ­¤è·Ÿè¸ªä¸­è§‚å¯Ÿåˆ°è°ƒç”¨ `layer_norm` æ—¶çš„æ“ä½œæµç¨‹ã€‚

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_profile_trace.png)

The sequence begins in the CPU (the upper section) withÂ `aten::layer_norm`, progressing toÂ `aten::native_layer_norm`, and then transitioning toÂ `cudaLaunchKernel`. From there, we move on to the GPU, where theÂ `vectorized_layer_norm_kernel`Â kernel is called.

è¯¥åºåˆ—ä» CPUï¼ˆä¸Šéƒ¨ï¼‰å¼€å§‹ï¼Œä»¥ `aten::layer_norm` å¯åŠ¨ï¼Œæ¥ç€è¿›å…¥ `aten::native_layer_norm`ï¼Œç„¶åè¿‡æ¸¡åˆ° `cudaLaunchKernel`ã€‚ä»é‚£é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬ç»§ç»­åˆ° GPUï¼Œåœ¨é‚£é‡Œè°ƒç”¨ `vectorized_layer_norm_kernel` å†…æ ¸ã€‚

> [!NOTE]
> æ‚¨å¯ä»¥é€šè¿‡åœ¨åˆ†æå™¨ä¸­å°† `profile_memory` è®¾ç½®ä¸º `True` æ¥å¯ç”¨å†…å­˜åˆ†æã€‚ç„¶è€Œï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ›´å¤æ‚çš„è·Ÿè¸ªè®°å½•ã€‚

è™½ç„¶ PyTorch Profiler èƒ½å¿«é€Ÿæä¾›æ€§èƒ½æ¦‚è§ˆï¼Œä½† NVIDIA Nsight Compute (ncu) èƒ½æ›´æ·±å…¥åœ°æ´å¯Ÿ GPU æ€§èƒ½ï¼ŒåŒ…æ‹¬æ¯ä¸ªå†…æ ¸çš„è¯¦ç»†æ‰§è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µã€‚è¿è¡Œè¯¥åˆ†æå™¨éå¸¸ç®€å•ï¼š

```bash
ncu --set full python layer_norm.py
```

å…¶ä¸­ `layer_norm.py` æ˜¯ä¸€ä¸ªç›´æ¥æ‰§è¡Œå±‚å½’ä¸€åŒ–å‡½æ•°çš„ç®€å•æ–‡ä»¶ã€‚æ­¤å‘½ä»¤å°†ç”Ÿæˆæ—¥å¿—è¾“å‡ºï¼Œä½†æ›´æœ‰æ•ˆæŸ¥çœ‹ç»“æœçš„æ–¹æ³•æ˜¯é€šè¿‡è®¾ç½®è¾“å‡ºæ ‡å¿—ï¼š

```bash
ncu --set full -o output python layer_norm.py
```

å¹¶ä½¿ç”¨ Nsight Compute æ‰“å¼€ `output.ncu-rep` æ–‡ä»¶ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä¸‹è§†å›¾ã€‚

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_ncu.png)

æ˜ç¡®è­¦å‘Šè®¡ç®—å’Œå†…å­˜åˆ©ç”¨ç‡æƒ…å†µï¼Œä»¥åŠå¦‚ä½•è®©å†…æ ¸æ›´å¥½åœ°å¹³è¡¡è®¡ç®—å’Œå†…å­˜ä»¥å®ç°æœ€å¤§å ç”¨ç‡ã€‚

#### CPP extension

å¦‚æœä½ æƒ³è¦åˆ†æçš„å†…æ ¸å°šæœªé›†æˆåˆ° PyTorch ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ PyTorch çš„ `cpp_extension` æ¨¡å—è½»æ¾ç¼–è¯‘å’Œè¿è¡Œè‡ªå®šä¹‰ CUDA ä»£ç ã€‚è¿™ä¸ªè¿‡ç¨‹å¾ˆç®€å•â€”â€”åªéœ€åœ¨ `.cu` æ–‡ä»¶ä¸­åˆ›å»ºä½ çš„ CUDA å†…æ ¸ï¼Œå¹¶ä½¿ç”¨ `cpp_extension` æ¨¡å—ä¸­çš„ `load` å‡½æ•°åœ¨ Python ä¸­åŠ è½½å®ƒã€‚

å¯¹äºä¸€ä¸ªç®€å•çš„ `add` å†…æ ¸ï¼Œ`.cu` æ–‡ä»¶å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```clike
#include 
#include 
#include 

__global__ void add_kernel(float* x, float* y, float* output, int size) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < size) {
        output[index] = x[index] + y[index];
    }
}

void add_cuda(torch::Tensor x, torch::Tensor y, torch::Tensor output) {
    int threads = 1024;
    int blocks = (x.size(0) + threads - 1) / threads;

    add_kernel<<>>(x.data_ptr(), y.data_ptr(), output.data_ptr(), x.size(0));
}
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("add_cuda", &add_cuda, "Vector addition (CUDA)");
}
```

python æ–‡ä»¶åŠ è½½å†…æ ¸ï¼š

```python
import torch
from torch.utils.cpp_extension import load

# Load and compile the CUDA extension
vector_add = load(
    name="vector_add",
    sources=["add_kernel.cu"],
    verbose=True
)

# Define input tensors
size = 10000
x = torch.randn(size, device='cuda')
y = torch.randn(size, device='cuda')
output = torch.empty(size, device='cuda')

# Run the CUDA kernel
vector_add.add_cuda(x, y, output)
```

ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä½ å¯ä»¥åƒæˆ‘ä»¬ä¹‹å‰ç”¨ PyTorch çš„åˆ†æå™¨æˆ– NVIDIA å·¥å…·å±•ç¤ºçš„é‚£æ ·å¯¹è‡ªå®šä¹‰ CUDA å†…æ ¸è¿›è¡Œåˆ†æã€‚

### A2: Typical Scales in LLM Training

Let's get a feel for the typical sizes of things in LLM training. When we talk about memory or compute, we're often counting "elements" - think of these as numbers in tensors. To get the actual memory in bytes, you'll need to multiply by the size of each number (e.g., 2 bytes for bf16, 4 bytes for fp32).

Here are some quick ballpark figures:

- **Input tokens:**Â For each batch, we processÂ seqâ‹…mbsseqâ‹…mbsÂ tokens, where mbs is the micro batch size and seq is the sequence length.
- **Activations (hidden states):**Â For a single layer, the hidden state tensor is of sizeÂ seqâ‹…mbsâ‹…hseqâ‹…mbsâ‹…hÂ elements.
- **Model weights and gradients:**Â Each weight matrix in your model (like in linears) is aboutÂ h2h2Â elements. This is per weight matrix. Gradients have the same size as weights.
- **Optimizer states:**Â For each weight matrix (of elementsÂ h2h2), if you're using an optimizer like Adam with mixed precision training, it keeps momentum and variance states in fp32 precision (2â‹…h22â‹…h2), plus master weights in fp32 (h2h2). So total optimizer states will be around (6â‹…h26â‹…h2) per weight matrix.
- **Total model parameters:**Â For each transformer block:
    - Attention parameters:
        - QKV projections:Â 3h23h2Â parameters
        - Output projection:Â h2h2Â parameters
    - MLP parameters with GLU:
        - Gate and up projections:Â 8h28h2Â parameters (2 matrices of sizeÂ hÃ—4hhÃ—4h)
        - Down projection:Â 4h24h2Â parameters (1 matrix of sizeÂ 4hÃ—h4hÃ—h)
    - Total per block:Â 16h216h2Â with GLU MLPs, orÂ 12h212h2Â without GLU
    - For full model:Â 16h2â‹…num_layers16h2â‹…num_layersÂ (with GLU)
    - Additional parameters:
        - Input embeddings:Â vocab_sizeâ‹…hvocab_sizeâ‹…h
        - LM head:Â vocab_sizeâ‹…hvocab_sizeâ‹…hÂ (if not tied with input embeddings)
        - Positional embeddings (if used):Â max_seq_lenâ‹…hmax_seq_lenâ‹…h
- **Forward and backward pass compute (FLOPs):**Â A very rough estimate for the FLOPs in a forward pass isÂ 2â‹…num_tokensâ‹…num_params2â‹…num_tokensâ‹…num_params. And backward pass compute is twice as that:Â 4â‹…num_tokensâ‹…num_params4â‹…num_tokensâ‹…num_params.

### A3: Math for Compute/Communication Overlap

Using the formulas from the previous section, we can estimate when computation and communication can effectively overlap in distributed training. Let's look at data parallelism (Zero-0) as an example.

#### Data Parallelism Communication Analysis

The total gradient size that needs to be communicated is:

- Gradients = Parameters â‰ˆÂ num_layersâ‹…16h2num_layersâ‹…16h2

During backward pass, these gradients are communicated in buckets (default 25MB). The communication time to all-reduce each bucket is:

tcomm=tcomm_bucket=bucket_sizeâ‹…2(DPâˆ’1)DPâ‹…peak_bwtcommâ€‹=tcomm_bucketâ€‹=DPâ‹…peak_bwbucket_sizeâ‹…2(DPâˆ’1)â€‹

ğŸ“ Note

For bandwidth calculations, we use the bus bandwidth formulas from theÂ [NCCL documentation](https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#summary). These formulas account for the specific communication patterns when calculating effective bandwidth between GPUs.

The computation time for backward pass is:

tcompute=4â‹…num_tokensâ‹…num_paramspeak_flopstcomputeâ€‹=peak_flops4â‹…num_tokensâ‹…num_paramsâ€‹

For effective overlap, we need:

tcommtcompute=num_params2â‹…num_tokensâ‹…DPâˆ’1DPâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…num_tokensnum_paramsâ€‹â‹…DPDPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

This ratio helps determine if communication will become a bottleneck in training. When the ratio is less than 1, communication can be fully overlapped with computation.

#### ZeRO-3 (FSDP) Communication Analysis

For ZeRO-3, parameters and gradients are sharded across GPUs. Let's analyze the communication pattern for a model with transformer blocks of sizeÂ 16h216h2Â parameters each:

- For each transformer block in forward pass:
    - Allgather parameters:Â 16h2/DP16h2/DPÂ bytes per rank
- For each transformer block in backward pass:
    - Allgather parameters:Â 16h2/DP16h2/DPÂ bytes per rank
    - Reducescatter gradients:Â 16h2/DP16h2/DPÂ bytes per rank
- Total communication per block:Â 3â‹…16h2/DP3â‹…16h2/DPÂ bytes
- Total communication for full model:Â 3â‹…num_layersâ‹…16h2/DP3â‹…num_layersâ‹…16h2/DPÂ bytes

The communication time for allgather operations is:

tcomm=16h2â‹…DPâˆ’1DPâ‹…peak_bwtcommâ€‹=16h2â‹…DPâ‹…peak_bwDPâˆ’1â€‹

The computation time for forward pass of one decoder layer is:

tcompute=32â‹…seq_lenâ‹…mbsâ‹…h2peak_flopstcomputeâ€‹=peak_flops32â‹…seq_lenâ‹…mbsâ‹…h2â€‹

For effective overlap between computation and communication, we need:

tcommtcompute=12â‹…seq_lenâ‹…mbsâ‹…DPâˆ’1DPâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…seq_lenâ‹…mbs1â€‹â‹…DPDPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

When this ratio is less than 1, the communication of parameters for the next layer can be hidden behind the computation of the current layer.

`

#### TP Communication Analysis

For Tensor Parallel (TP), activations are sharded across GPUs during linears. Let's analyze the communication pattern:

- For each column linear in forward pass:
    - Allgather activations:Â seqâ‹…mbsâ‹…h/TPseqâ‹…mbsâ‹…h/TPÂ bytes per rank
- For each column linear in backward pass:
    - Reducescatter gradients:Â seqâ‹…mbsâ‹…h/TPseqâ‹…mbsâ‹…h/TPÂ bytes per rank
- And vice-versa for row linears. Each transformer block has 2 column linears and 2 row linears.
- Total communication per block:Â 8â‹…seqâ‹…mbsâ‹…h/TP8â‹…seqâ‹…mbsâ‹…h/TPÂ bytes
- Total communication for full model:Â 8â‹…num_layersâ‹…seqâ‹…mbsâ‹…h/TP8â‹…num_layersâ‹…seqâ‹…mbsâ‹…h/TPÂ bytes

Let's analyze if we can overlap the allgather communication for one layer with the computation of the next linear. The communication time for allgather operations is:

tcomm=seqâ‹…mbsâ‹…hâ‹…(TPâˆ’1)TPâ‹…peak_bwtcommâ€‹=TPâ‹…peak_bwseqâ‹…mbsâ‹…hâ‹…(TPâˆ’1)â€‹

While the computation time for the next linear (with parametersÂ h2h2) is:

tcompute=2â‹…seqâ‹…mbsâ‹…h2TPâ‹…peak_flopstcomputeâ€‹=TPâ‹…peak_flops2â‹…seqâ‹…mbsâ‹…h2â€‹

For effective overlap, we want the communication time to be less than the compute time:

tcommtcompute=TPâˆ’12â‹…hâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…hTPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

This ratio tells us whether we can successfully hide the allgather communication behind the computation of the next linear. Interestingly, the ratio only depends on the hidden size h and tensor parallelism degree TP, not on sequence length or batch size.

#### PP Communication Analysis

For Pipeline Parallel (PP), activations and gradients are communicated between pipeline stages. Let's analyze the communication pattern:

- For each microbatch in forward pass:
    - Receive and send activations:Â 2â‹…seqâ‹…mbsâ‹…h2â‹…seqâ‹…mbsâ‹…hÂ bytes
- For each microbatch in backward pass:
    - Receive and send gradients:Â 2â‹…seqâ‹…mbsâ‹…h2â‹…seqâ‹…mbsâ‹…hÂ bytes
- Total communication per microbatch:Â 4â‹…seqâ‹…mbsâ‹…h4â‹…seqâ‹…mbsâ‹…hÂ bytes
- For gradient accumulation steps (gas), total communication:Â 4â‹…gasâ‹…seqâ‹…mbsâ‹…h4â‹…gasâ‹…seqâ‹…mbsâ‹…hÂ bytes

Let's analyze if we can overlap the communication of activations/gradients with computation of the next transformer block. The computation time for transformer blocks in the next pipeline stage is:

tcompute=32â‹…seqâ‹…mbsâ‹…h2â‹…num_layers_in_next_pppeak_flopstcomputeâ€‹=peak_flops32â‹…seqâ‹…mbsâ‹…h2â‹…num_layers_in_next_ppâ€‹

While the communication time for P2P transfer is:

tcomm=seqâ‹…mbsâ‹…hpeak_bwtcommâ€‹=peak_bwseqâ‹…mbsâ‹…hâ€‹

For effective overlap, we want:

tcommtcompute=peak_flops32â‹…hâ‹…num_layers_in_next_ppâ‹…peak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=32â‹…hâ‹…num_layers_in_next_ppâ‹…peak_bwpeak_flopsâ€‹â‰¤1

Similar to TP, this ratio is independent of sequence length and batch size. It depends on the hidden size h, number of layers in the next pipeline stage, and the ratio of compute to P2P bandwidth capabilities of the hardware.

### Citation

For attribution in academic contexts, please cite this work as

Tazi et al., "The Ultra-Scale Playbook: Training LLMs on GPU Clusters", 2025.

BibTeX citation

@misc{ultrascale_playbook,
      title={The Ultra-Scale Playbook:Â Training LLMs on GPU Clusters},
      author={Nouamane Tazi, Ferdinand Mom, Haojun Zhao, Phuc Nguyen, Mohamed Mekkouri, Leandro Werra, Thomas Wolf},
      year={2025},
}

### References

2. Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and OverlappingÂ â€‚[[PDF]](http://arxiv.org/pdf/2409.15241.pdf)  
    Wang, G., Zhang, C., Shen, Z., Li, A. and Ruwase, O., 2024.
3. Striped Attention: Faster Ring Attention for Causal TransformersÂ â€‚[[PDF]](http://arxiv.org/pdf/2311.09431.pdf)  
    Brandon, W., Nrusimha, A., Qian, K., Ankner, Z., Jin, T., Song, Z. and Ragan-Kelley, J., 2023.
4. Breadth-First Pipeline ParallelismÂ â€‚[[PDF]](http://arxiv.org/pdf/2211.05953.pdf)  
    Lamy-Poirier, J., 2023.
5. DeepSeek-V3 Technical ReportÂ â€‚[[PDF]](http://arxiv.org/pdf/2412.19437.pdf)  
    DeepSeek-AI, and others,, 2024.
6. Zero Bubble Pipeline ParallelismÂ â€‚[[PDF]](http://arxiv.org/pdf/2401.10241.pdf)  
    Qi, P., Wan, X., Huang, G. and Lin, M., 2023.
7. Mixtral of ExpertsÂ â€‚[[PDF]](http://arxiv.org/pdf/2401.04088.pdf)  
    Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T. and Sayed, W.E., 2024.
8. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient SparsityÂ â€‚[[PDF]](http://arxiv.org/pdf/2101.03961.pdf)  
    Fedus, W., Zoph, B. and Shazeer, N., 2022.
9. A Survey on Mixture of ExpertsÂ â€‚[[PDF]](http://arxiv.org/pdf/2407.06204.pdf)  
    Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S. and Huang, J., 2024.
10. GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingÂ â€‚[[PDF]](http://arxiv.org/pdf/2006.16668.pdf)  
    Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020.
11. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-AwarenessÂ â€‚[[PDF]](http://arxiv.org/pdf/2205.14135.pdf)  
    Dao, T., Fu, D.Y., Ermon, S., Rudra, A. and RÃ©, C., 2022.
12. FP8-LM: Training FP8 Large Language ModelsÂ â€‚[[PDF]](http://arxiv.org/pdf/2310.18313.pdf)  
    Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., Li, R., Zhang, M., Li, C., Ning, J., Wang, R., Zhang, Z., Liu, S., Chau, J., Hu, H. and Cheng, P., 2023.
13. torchao: PyTorch native quantization and sparsity for training and inferenceÂ â€‚[[link]](https://github.com/pytorch/ao)  
    maintainers, t. and contributors,, 2024.
14. Small-scale proxies for large-scale Transformer training instabilitiesÂ â€‚[[PDF]](http://arxiv.org/pdf/2309.14322.pdf)  
    Wortsman, M., Liu, P.J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J.D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J., Gilmer, J. and Kornblith, S., 2023.

[^1]: An Empirical Model of Large-Batch TrainingÂ â€‚[PDF](http://arxiv.org/pdf/1812.06162.pdf)  McCandlish, S., Kaplan, J., Amodei, D. and Team, O.D., 2018.
[^2]: Mixed Precision TrainingÂ â€‚[PDF](http://arxiv.org/pdf/1710.03740.pdf)  Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G. and Wu, H., 2018.
[^3]: Reducing Activation Recomputation in Large Transformer ModelsÂ â€‚[PDF](http://arxiv.org/pdf/2205.05198.pdf)  Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M. and Catanzaro, B., 2022.
