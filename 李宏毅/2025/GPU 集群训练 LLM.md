
å‘è¡¨æ—¶é—´ï¼š2025.02.19
å»ºè®®é˜…è¯»æ—¶é•¿ï¼š2-4 å¤©
ä½œè€…ï¼šNouamane Tazi, Ferdinand Mom,Â Haojun Zhao,Â Phuc Nguyen,Â Mohamed Mekkouri,Â Leandro Werra,Â Thomas Wolf

[5D parallelism in a nutshell](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#5d_parallelism_in_a_nutshell)

[Finding the Best Training Configuration](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#finding_the_best_training_configuration)

- [Step 1: Fitting a Training Step in Memory](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#step_1:_fitting_a_training_step_in_memory)
- [Step 2: Achieving Target Global Batch Size](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#step_2:_achieving_target_global_batch_size_)
- [Step 3: Optimizing Training Throughput](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#step_3:_optimizing_training_throughput)
- [Benchmarking thousands of configurations](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#benchmarking_thousands_of_configurations)
- [Lessons learned on benchmarking](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#lessons_learned_on_benchmarking)

[Diving in the GPUs â€“ fusing, threading, mixing](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#diving_in_the_gpus_%E2%80%93_fusing,_threading,_mixing)

- [A primer on GPU](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a_primer_on_gpu)
- [How to improve performance with Kernels ?](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#how_to_improve_performance_with_kernels_?)

- [Memory Coalescing](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#memory_coalescing)
- [Tiling](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#tiling)
- [Thread Coarsening](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#thread_coarsening)
- [Minimizing Control Divergence](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#minimizing_control_divergence)

- [Fused Kernels](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#fused_kernels)
- [Flash Attention 1-3](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#flash_attention_1-3)
- [Mixed Precision Training](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#mixed_precision_training)

- [FP16 and BF16 training](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#fp16_and_bf16_training)
- [FP8 pretraining](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#fp8_pretraining)

[Conclusion](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#conclusion)

- [So, whatâ€™s next?](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#so,_what%E2%80%99s_next?)
- [Acknowledgements](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#acknowledgements)
- [Discussion page](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#discussion_page)

[References](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#references)

- [Landmark LLM Scaling Papers](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#landmark_llm_scaling_papers)
- [Training Frameworks](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#training_frameworks)
- [Debugging](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#debugging)
- [Distribution Techniques](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#distribution_techniques)
- [Hardware](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#hardware)
- [Others](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#others)

[Appendix](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#appendix)

- [A0: Parallel Programming Crash Course](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a0:_parallel_programming_crash_course)

- [Broadcast](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#broadcast)
- [Reduce & AllReduce](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#reduce_&_allreduce)
- [Gather & AllGather](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#gather_&_allgather_)
- [Scatter & ReduceScatter](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#scatter_&_reducescatter)
- [A quick focus on Ring AllReduce](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a_quick_focus_on_ring_allreduce)
- [Barrier](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#barrier)
- [NCCL: NVIDIA Collective Communications Library](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#nccl:_nvidia_collective_communications_library)

- [A1: Distributed Training Profiling](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a1:_distributed_training_profiling)

- [Kernels](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#kernels)
- [CPP extension](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#cpp_extension)

- [A2: Typical Scales in LLM Training](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a2:_typical_scales_in_llm_training)
- [A3: Math for Compute/Communication Overlap](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#a3:_math_for_compute/communication_overlap)

- [Data Parallelism Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#data_parallelism_communication_analysis)
- [ZeRO-3 (FSDP) Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#zero-3_\(fsdp\)_communication_analysis)
- [TP Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#tp_communication_analysis)
- [PP Communication Analysis](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#pp_communication_analysis)


## ä¸‰ã€æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰

æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰èƒŒåçš„ç†å¿µæ˜¯åœ¨å¤šä¸ª GPU ä¸Šå¤åˆ¶æ¨¡å‹ï¼ˆæˆ‘ä»¬å°†å‰¯æœ¬ç§°ä¸ºâ€œæ¨¡å‹å®ä¾‹â€ï¼‰ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ª GPU å¹¶è¡Œåœ°å¯¹ä¸åŒçš„å¾®æ‰¹æ¬¡æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå› æ­¤å¾—åæ•°æ®å¹¶è¡Œã€‚ä½ å¯èƒ½å·²ç»åœ¨ç®€å•çš„è®­ç»ƒç¤ºä¾‹ä¸­è§è¿‡æ•°æ®å¹¶è¡Œï¼Œä½†æ­£å¦‚ä½ å¾ˆå¿«ä¼šçœ‹åˆ°çš„ï¼Œåœ¨æœ¬èŠ‚ä¸­æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è¿™ä¸€å†…å®¹ï¼Œæ‰€ä»¥å³ä½¿ä½ å·²ç»äº†è§£ä¸€èˆ¬æ–¹æ³•ï¼Œä¹Ÿè¯·ç»§ç»­å…³æ³¨ã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_diagram.png)

ï¼ˆå¦‚æœä½ ä¸ç†Ÿæ‚‰ broadcastã€gather æˆ– all-reduce ç­‰åˆ†å¸ƒå¼é€šä¿¡æ¨¡å¼ï¼Œæˆ‘ä»¬åœ¨ A0ï¼šå¹¶è¡Œç¼–ç¨‹é€Ÿæˆè¯¾ç¨‹ä¸­å‡†å¤‡äº†ä¸€ä¸ªå°å‹é€Ÿæˆè¯¾ç¨‹ã€‚ï¼‰

æ¯ä¸ª GPU ä½¿ç”¨ä¸åŒçš„å¾®æ‰¹æ¬¡æ„å‘³ç€æ¯ä¸ª GPU ä¸­ä¼šæœ‰ä¸åŒçš„æ¢¯åº¦ï¼Œå› æ­¤ä¸ºäº†ä½¿ä¸åŒ GPU ä¸Šçš„æ¨¡å‹å®ä¾‹ä¿æŒåŒæ­¥ï¼Œå°†ä½¿ç”¨ä¸€ç§ç§°ä¸º â€œall-reduceâ€ çš„æ“ä½œå¯¹æ¥è‡ªæ¨¡å‹å®ä¾‹çš„æ¢¯åº¦è¿›è¡Œå¹³å‡å¤„ç†ï¼Œè¯¥æ“ä½œåœ¨åå‘ä¼ æ’­æœŸé—´ã€ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰è¿›è¡Œã€‚

è¿™æ¶‰åŠæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªâ€œåˆ†å¸ƒå¼é€šä¿¡â€åŸè¯­ï¼š***all-reduce***ï¼Œå®ƒå¤„ç† GPU å®ä¾‹å’ŒèŠ‚ç‚¹ä¹‹é—´çš„åŒæ­¥å’Œé€šä¿¡ã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_overlap1.svg)

ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰å®ç°æ–¹å¼æ˜¯ç­‰å¾…åå‘ä¼ æ’­å®Œæˆï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†æ‰€æœ‰æ¢¯åº¦ï¼Œç„¶åè§¦å‘æ‰€æœ‰åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ ranks ä¹‹é—´çš„ä¸€æ¬¡ all-reduce æ“ä½œæ¥åŒæ­¥è¿™äº›æ¢¯åº¦ã€‚ä½†è¿™ç§å…ˆè®¡ç®—åé€šä¿¡çš„é¡ºåºæ­¥éª¤æ˜¯***å¤§å¿Œ***ï¼å› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›åƒä¸Šå›¾é‚£æ ·ï¼Œåœ¨è¿›è¡Œé€šä¿¡æ—¶æˆ‘ä»¬çš„ GPU å¤„äºé—²ç½®çŠ¶æ€ã€‚

ç›¸åï¼Œæˆ‘ä»¬åº”è¯¥å°½å¯èƒ½åœ°è®©é€šä¿¡å’Œè®¡ç®—é‡å ï¼Œä½¿å®ƒä»¬å°½å¯èƒ½åŒæ—¶å‘ç”Ÿã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸‰ç§ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒä»¬èƒ½è®©æˆ‘ä»¬æ¯”æœ€åˆçš„ç®€å•å®ç°åšå¾—æ›´å¥½ï¼

### 3.1 ä¸‰ç§ä¼˜åŒ–æ–¹æ³•

#### 3.1 æ–¹æ¡ˆä¸€ï¼šå°†æ¢¯åº¦åŒæ­¥ä¸åå‘ä¼ æ’­é‡å 

æˆ‘ä»¬åˆšåˆšæè¿°çš„æœ´ç´  DP æ–¹æ³•çš„ä¸»è¦ç¼ºç‚¹æ˜¯ï¼Œåœ¨åå‘ä¼ æ’­ï¼ˆ*è®¡ç®—*ï¼‰ä¹‹åï¼Œæˆ‘ä»¬å¿…é¡»ç­‰å¾…æ¢¯åº¦åŒæ­¥ï¼ˆ*é€šä¿¡*ï¼‰æ‰èƒ½æ›´æ–°å‚æ•°ã€‚æˆ‘ä»¬èƒ½å¦å°†æ­¤é€šä¿¡ä¸æˆ‘ä»¬çš„è®¡ç®—é‡å ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨è®¡ç®—å‰é¢å±‚çš„æ¢¯åº¦ä¹‹å‰ï¼Œå°±å¯ä»¥æ”¶é›†å¹¶æ±‚å’ŒæŸä¸€å±‚çš„æ¢¯åº¦ã€‚ä¾‹å¦‚ï¼Œä¸€æ—¦æœ€åä¸€å±‚çš„åå‘ä¼ æ’­å®Œæˆï¼Œè¿™äº›æ¢¯åº¦å°±å¯ä»¥åœ¨ä¸ºå‰é¢çš„å±‚ç»§ç»­è¿›è¡Œåå‘è®¡ç®—çš„åŒæ—¶è¢«æ”¶é›†å’Œæ±‚å’Œã€‚

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_overlap2.svg)

è¿™å¯ä»¥åœ¨ PyTorch ä¸­é€šè¿‡æ¯ä¸ªå‚æ•°ä¸Šé™„åŠ ä¸€ä¸ª *all-reduce é’©å­å‡½æ•°* å®ç° ã€‚ä¸€æ—¦è¯¥å‚æ•°çš„æ¢¯åº¦å‡†å¤‡å¥½ï¼Œå°±ä¼šè§¦å‘ all-reduce æ“ä½œï¼Œè€Œå…¶ä»–å‚æ•°çš„æ¢¯åº¦ä»åœ¨è®¡ç®—ä¸­ã€‚è¿™ç§æ–¹æ³•å°†å¤§éƒ¨åˆ† all-reduce æ“ä½œä¸æ¢¯åº¦è®¡ç®—é‡å ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç”¨äºé™„åŠ é’©å­çš„ç®€å•å‡½æ•°ï¼š

```python
def register_backward_hook(self, hook):
    """
    Registers a backward hook for all parameters of the model that 
    require gradients.
    """
    for p in self.module.parameters():
        if p.requires_grad is True:
            p.register_post_accumulate_grad_hook(hook)
```

è®¡ç®—å’Œé€šä¿¡çš„é‡å å‡å°‘äº†ç­‰å¾…æ•´ä¸ªæ¨¡å‹æ¢¯åº¦åŒæ­¥çš„æ—¶é—´ã€‚æ¢¯åº¦åŒæ­¥å¯ä»¥ï¼ˆè‡³å°‘éƒ¨åˆ†åœ°ï¼‰ä¸åå‘ä¼ æ’­å¹¶è¡Œè¿›è¡Œï¼Œæ˜¾è‘—åŠ å¿«æ•°æ®å¹¶è¡Œé€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯å…·æœ‰åŒæ­¥é‡å çš„æœ´ç´ æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰çš„å®Œæ•´å®ç°ï¼š

ğŸ‘‰ Picotron ä¸­å­˜åœ¨é‡å çš„æœ´ç´ åŠ¨æ€è§„åˆ’å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class DataParallelNaive(nn.Module):
    """
    Naive Data Parallelism. Not used in practice. But it is a good starting point to understand how data parallelism works.
    It implements a simple all-reduce operation to synchronize gradients across multiple processes.
    And `no_sync` context manager to disable gradient synchronization.
    """
    def __init__(self, module):
        """
        Initializes the DataParallel wrapper for a given module.

        Args:
            module (nn.Module): The model to be wrapped for data parallelism.
            process_group (torch.distributed.ProcessGroup): The process group used for gradient synchronization. 
                                                            It could be a data parallel or context parallel group.
        """
        super().__init__()
        self.module = module
        self.require_backward_grad_sync = True # whether to synchronize gradients during backward pass. Set to False when using gradient accumulation
        self.register_backward_hook(self._allreduce_grads)
    
    def forward(self, *inputs, **kwargs):
        return self.module(*inputs, **kwargs)
    
    def register_backward_hook(self, hook):
        """
        Registers a backward hook for all parameters of the model that require gradients.    
        """
        for p in self.module.parameters():
            if p.requires_grad is True:
                p.register_hook(hook)
                
    def _allreduce_grads(self, grad):
        """
        Performs an all-reduce operation to synchronize gradients across multiple processes.    
        """
        # No synchronization needed during gradient accumulation, except at the final accumulation step.
        if self.require_backward_grad_sync:
            dist.all_reduce(grad, op=dist.ReduceOp.SUM, group=pgm.process_group_manager.cp_dp_group)
            grad /= pgm.process_group_manager.cp_dp_world_size
        return grad 
    
    @contextlib.contextmanager
    def no_sync(self):
        """
        A context manager to temporarily disable gradient synchronization. 
        This is useful for performing multiple backward passes during gradient accumulation without synchronizing 
        gradients in between.
        """
        self.require_backward_grad_sync = False
        yield
        self.require_backward_grad_sync = True
```


> [!important]
> [all-reduce å’Œ ring-reduce åœ¨æ•°æ®åŒæ­¥ä¸Šçš„ç¤ºæ„å›¾](https://blog.dailydoseofds.com/p/all-reduce-and-ring-reduce-for-model)


è¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€ä¸ª â€œ*è®¡ç®—ä¸é€šä¿¡é‡å *â€ çš„ä¾‹å­ï¼Œåœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å°†å¤šæ¬¡è®¨è®ºå®ƒï¼Œè¿™æ˜¯å®ç°æœ€å¤§æ‰©å±•æ•ˆç‡çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚ä½†æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼

#### 3.2 æ–¹æ¡ˆäºŒï¼šæ¢¯åº¦åˆ†æ¡¶

GPU æ“ä½œåœ¨å¤„ç†å¤§å¼ é‡æ—¶é€šå¸¸æ¯”åœ¨å¤šä¸ªå°å¼ é‡ä¸Šè¿è¡Œè®¸å¤šæ“ä½œæ›´é«˜æ•ˆã€‚é€šä¿¡æ“ä½œä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¢¯åº¦æœ‰åˆ©åœ°åˆ†ç»„åˆ°æ¡¶ä¸­ï¼Œå¹¶å¯¹åŒä¸€æ¡¶å†…çš„æ‰€æœ‰æ¢¯åº¦å¯åŠ¨å•ä¸ª all-reduceï¼Œè€Œä¸æ˜¯å¯¹æ¯ä¸ªæ¢¯åº¦æ‰§è¡Œç‹¬ç«‹çš„ all-reduceã€‚é€šå¸¸çœ‹èµ·æ¥å¦‚ä¸‹ï¼š

![dp_overlap3.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_overlap3.svg)

è¿™å°±åƒåœ¨è£…è¿å‰å°†ç‰©å“è£…å…¥ç®±å­ä¸€æ ·ã€‚å‘é€å‡ ä¸ªå¤§ç®±å­æ¯”å‘é€è®¸å¤šå°ç®±å­æ›´é«˜æ•ˆã€‚é€šè¿‡å¯¹æ¯ä¸ªæ¡¶æ‰§è¡Œå•ä¸ª all-reduce æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—å‡å°‘é€šä¿¡å¼€é”€å¹¶åŠ å¿«é€šä¿¡æ“ä½œã€‚

ä»¥ä¸‹æ˜¯é‡‡ç”¨åˆ†æ¡¶æ–¹å¼çš„ä»£ç å®ç°ï¼š

ğŸ‘‰ Bucket DP åœ¨ Picotron ä¸­çš„å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class DataParallelBucket(nn.Module):
    """
    Data Parallelism with gradient grouped into buckets to reduce the communication overhead.
    """
    def __init__(self, module, bucket_cap_mb=25, grad_type = torch.float32):
        """
        Initialize the DataParallelBucket module.
        
        Args:
            module (nn.Module): The model to be parallelized.
            process_group: The process group for gradient synchronization, which can be either 
                           a data parallel group or a context parallel group.
            bucket_cap_mb (int, optional): The maximum size of each gradient synchronization bucket in megabytes. 
                                           Defaults to 25 MB.
            grad_type (torch.dtype, optional): The data type of gradients, defaulting to float32.
        """
        super().__init__()
        self.module = module
        self.require_backward_grad_sync = True # whether to synchronize gradients during backward pass. Set to False when using gradient accumulation
        grad_size = 2 if grad_type == torch.bfloat16 else 4 # float32 gradient: 4 bytes
        bucket_size = bucket_cap_mb * 1024 * 1024 // grad_size # number of gradients in one bucket
        self.bucket_manager = BucketManager(module.parameters(), pgm.process_group_manager.cp_dp_group, bucket_size, grad_type)
        self.register_backward_hook()
        self._post_backward_callback_set = False # whether the callback for wait gradient synchronization is set
        
    def forward(self, *inputs, **kwargs):
        return self.module(*inputs, **kwargs)

    def backward(self, input_tensor, output_tensor, output_tensor_grad):
        return self.module.backward(input_tensor, output_tensor, output_tensor_grad)
    
    def register_backward_hook(self):
        """
        Registers a backward hook to manually accumulate and synchronize gradients.
        
        This hook serves two main purposes:
        1. PyTorch does not natively support gradient accumulation with mixed precision.
        2. After gradient accumulation, it flags parameters as ready for synchronization.
        
        The gradient accumulation functions are stored to prevent them from going out of scope.
        
        References:
        - https://github.com/NVIDIA/Megatron-LM/issues/690
        - https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html
        - https://arxiv.org/abs/2006.15704 (page 5)
        """
        self.grad_accs = []
        for param in self.module.parameters():
            if param.requires_grad:
                # Expand so we get access to grad_fn.
                param_tmp = param.expand_as(param)
                # Get the gradient accumulator function.
                grad_acc_fn = param_tmp.grad_fn.next_functions[0][0]
                grad_acc_fn.register_hook(self._make_param_hook(param, self.bucket_manager))
                self.grad_accs.append(grad_acc_fn)
                
    def _make_param_hook(self, param: torch.nn.Parameter,bucket_manager: BucketManager):
        """
        Creates the a hook for each parameter to handle gradient accumulation and synchronization.
        """
        def param_hook(*unused):
            """
            The hook called after the gradient is ready. It performs the following:
            1. Accumulates the gradient into the main gradient.
            2. Adds a post-backward callback to wait for gradient synchronization completion.
            3. Marks the parameter as ready for synchronization.
            """
            if param.requires_grad:
                assert param.grad is not None
                param.main_grad.add_(param.grad.data) # accumulate the gradients
                param.grad = None
                
                # skip the gradient synchronization (gradient accumulation/PP micro batches)
                if self.require_backward_grad_sync:
                    # Add a callback to wait for gradient synchronization. Ensures the callback is added only once.
                    # Callback is executed after the backward pass. It should be added per backward pass.
                    if not self._post_backward_callback_set:
                        Variable._execution_engine.queue_callback(self._post_backward)
                        self._post_backward_callback_set = True
                        
                    # mark the parameter as ready for gradient synchronization. 
                    bucket_manager.mark_param_as_ready(param) 
        return param_hook
    
    @contextlib.contextmanager
    def no_sync(self):
        """A context manager to disable gradient synchronization."""
        self.require_backward_grad_sync = False
        yield
        self.require_backward_grad_sync = True
        
    def _post_backward(self):
        """
        A post-backward callback that waits for gradient synchronization to finish, then copies 
        the synchronized gradients back to the parameters' grad attribute.
        
        This method is called after the backward pass and before the optimizer step.
        """
        self.bucket_manager.wait()
        self._post_backward_callback_set = False
        # copy to params.grad so we can use the optimizer to update the parameters
        for p in self.module.parameters():
            if p.requires_grad:
                p.grad = p.main_grad.to(p.dtype) # In PyTorch, you cannot assign a gradient with one data type to a tensor of another data type.

    def reset(self):
        """
        Reset the bucket manager and zero out gradients in the model
        """
        self.bucket_manager.reset() 
```

#### 3.3 æ–¹æ¡ˆä¸‰ï¼šä¸æ¢¯åº¦ç´¯ç§¯çš„ç›¸äº’ä½œç”¨

æœ€åï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæ¢¯åº¦ç´¯ç§¯é€šè¿‡åœ¨ç”¨ `optimizer.step()` æ›´æ–°å‚æ•°ä¹‹å‰æ‰§è¡Œå¤šæ¬¡å‰å‘å’Œåå‘ä¼ æ’­æ¥å·¥ä½œã€‚å½“å°†æ¢¯åº¦ç´¯ç§¯ä¸æ•°æ®å¹¶è¡Œæ€§ç»“åˆæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨åŒæ­¥æ¢¯åº¦æ—¶è¦å°å¿ƒã€‚

åœ¨ä¸€ä¸ªç®€å•ç‰ˆæœ¬ä¸­ï¼Œåœ¨ç´¯ç§¯è¿‡ç¨‹ä¸­æ¯æ¬¡åå‘ä¼ æ’­åéƒ½ä¼šè‡ªåŠ¨è§¦å‘ all-reduce æ“ä½œï¼Œè¿™æ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºåœ¨æœ€åä¸€æ­¥ä¹‹åè¿›è¡Œå•æ¬¡ reduce å°†äº§ç”Ÿç›¸åŒçš„æ•ˆæœï¼ŒåŒæ—¶å‡å°‘å¼€é”€ã€‚

åœ¨ PyTorch ä¸­ï¼Œé€šå¸¸çš„è§£å†³æ–¹æ³•æ˜¯åœ¨ä¸éœ€è¦è¿›è¡Œ reduce çš„åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ª [`model.no_sync()`](https://github.com/pytorch/pytorch/blob/5ea67778619c31b13644914deef709199052ee55/torch/nn/parallel/distributed.py#L1408-L1435)è£…é¥°å™¨ï¼Œè¯¥è£…é¥°å™¨å¯ä»¥ç¦ç”¨æ¢¯åº¦åŒæ­¥ã€‚

> [!NOTE]
> åœ¨æ‰§è¡Œé€šä¿¡æ“ä½œæ—¶ï¼Œå¼ é‡åœ¨å†…å­˜ä¸­å¿…é¡»æ˜¯è¿ç»­çš„ï¼Œä»¥é¿å…å¤šä½™çš„å†…å­˜æ‹·è´ã€‚ä¸ºäº†ä»¥æœ€ä¼˜æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šé¢„å…ˆåˆ†é…å¤§å°ä¸æ¿€æ´»å€¼æˆ–æ¨¡å‹å‚æ•°ç›¸åŒ¹é…çš„è¿ç»­ç¼“å†²åŒºï¼Œä¸“é—¨ç”¨äºé€šä¿¡ã€‚è™½ç„¶è¿™åŠ å¿«äº†é€šä¿¡é€Ÿåº¦ï¼Œä½†åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿå¯¼è‡´äº†è®­ç»ƒæœŸé—´çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡å¢åŠ ã€‚

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯¹å…¨å±€æ‰¹é‡å¤§å°æ„å‘³ç€ä»€ä¹ˆã€‚

### 3.2 é‡æ–°å®¡è§†å…¨å±€æ‰¹é‡å¤§å°

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°æ·»åŠ çš„æ•°æ®å¹¶è¡Œå’Œæ¢¯åº¦ç´¯ç§¯å‚æ•°æ¥æ›´æ–°æˆ‘ä»¬çš„æ‰¹é‡å¤§å°å…¬å¼ï¼š$$\text{bs} = \text{gbs} = \text{mbs} \times \text{grad\_acc} \times \text{dp}$$è¿™é‡Œ $\text{grad\_acc}$ æ˜¯æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œ$\text{dp}$ æ˜¯ç”¨äºæ•°æ®å¹¶è¡Œçš„å¹¶è¡Œå®ä¾‹æ•°é‡ã€‚

ç»™å®šä¸€ä¸ªç›®æ ‡å…¨å±€æ‰¹é‡å¤§å°ï¼Œæˆ‘ä»¬å› æ­¤å¯ä»¥é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ¥æ¢å–æ•°æ®å¹¶è¡Œè¿›ç¨‹ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”±äºæ•°æ®å¹¶è¡Œæœ¬è´¨ä¸Šæ˜¯å¹¶è¡Œçš„ï¼Œè€Œæ¢¯åº¦ç´¯ç§¯å…·æœ‰é¡ºåºæ€§ï¼Œäººä»¬å€¾å‘äºå°½å¯èƒ½å¤šåœ°å¢åŠ æ•°æ®å¹¶è¡ŒèŠ‚ç‚¹ï¼ˆDPï¼‰è€Œéé‡‡ç”¨æ¢¯åº¦ç´¯ç§¯ã€‚å½“ä»…æ‰©å±•æ•°æ®å¹¶è¡Œæ€§åœ¨ GPU ç”¨å®Œä¹‹å‰ä¸è¶³ä»¥è¾¾åˆ°ç›®æ ‡å…¨å±€æ‰¹é‡å¤§å°æ—¶ï¼Œå°±åœ¨æ•°æ®å¹¶è¡Œçš„åŸºç¡€ä¸Šæ·»åŠ æ¢¯åº¦ç´¯ç§¯ã€‚

(å…³äºæ•°æ®å¹¶è¡Œæ€§è¿›ä¸€æ­¥é˜…è¯»çš„ä¸€ä¸ªå¥½çš„èµ„æºæ˜¯ https://siboehm.com/articles/22/data-parallel-training)

èƒ½å¤Ÿå°†è®­ç»ƒåˆ†å¸ƒåˆ°ä¸åŒçš„æ ·æœ¬ä¸Šï¼Œä¸ºæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ä¸ªå¹¶è¡ŒåŒ–çš„ç»´åº¦ï¼Œå› æ­¤è¿™è¢«ç§°ä¸º 1D å¹¶è¡Œï¼ˆæˆ‘ä»¬åç»­å°†é€æ­¥ä»‹ç»å¦å¤–å››ä¸ªç»´åº¦ï¼‰ã€‚

### 3.3 åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬çš„æ—…ç¨‹

è®©æˆ‘ä»¬å¿«é€Ÿæ€»ç»“ä¸€ä¸‹å¦‚ä½•è®¾ç½®æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ª 1D å¹¶è¡Œè®­ç»ƒï¼Œå¹¶ä¸ºæœ€ä½³æ•°æ®å¹¶è¡Œè®¾ç½®æä¾›ä¸€ä¸ªè‰æ¡ˆé…æ–¹ï¼š

1. æˆ‘ä»¬é¦–å…ˆåº”é€šè¿‡æŸ¥é˜…æ–‡çŒ®æˆ–å¼€å±•æµ‹é‡æ¨¡å‹æ”¶æ•›æƒ…å†µçš„å®éªŒæ¥ç¡®å®šæœ€ä½³çš„ï¼ˆå…¨å±€ï¼‰æ‰¹é‡å¤§å°ï¼ˆä»¥ tokens ä¸ºå•ä½ï¼Œ`GBST`ï¼‰ã€‚
2. ç„¶åæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªç”¨äºè®­ç»ƒçš„åºåˆ—é•¿åº¦ï¼ŒåŒæ ·å¯ä»¥é€šè¿‡æŸ¥é˜…æ–‡çŒ®æˆ–å¼€å±•å®éªŒæ¥ç¡®å®šã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¯¹äºæˆ‘ä»¬ç›®å‰çš„è¯„ä¼°å·¥ä½œï¼Œ2-8k ä¸ª tokens èƒ½å¯é åœ°å‘æŒ¥è‰¯å¥½æ•ˆæœï¼ˆæˆ‘ä»¬åœ¨æ­¤ä¸æ·±å…¥æ¢è®¨è®­ç»ƒæ–¹æ³•ï¼Œä¸è¿‡å„å›¢é˜Ÿé€šå¸¸ä¼šåœ¨è®­ç»ƒç»“æŸæ—¶å¢åŠ åºåˆ—é•¿åº¦ï¼Œæ··å…¥ä¸€äº›æ›´é•¿ä¸Šä¸‹æ–‡çš„æ•°æ®æ ·æœ¬ï¼Œä»¥è¾¾åˆ°å¦‚ä»Šçš„æ›´é•¿ä¸Šä¸‹æ–‡å°ºå¯¸ï¼‰ã€‚
3. ç°åœ¨æˆ‘ä»¬å·²ç»çŸ¥é“äº†æ‰¹é‡å¤§å°ï¼ˆ`GBS`ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡é€æ¸å¢åŠ æœ¬åœ°æ‰¹é‡å¤§å°ï¼Œç›´è‡³è€—å°½å†…å­˜ï¼Œä»è€Œæ‰¾å‡ºå•ä¸ª GPU ä¸Šçš„æœ€å¤§æœ¬åœ°æ‰¹é‡å¤§å°ï¼ˆ`MBS`ï¼‰ã€‚
4. æœ€åï¼Œæˆ‘ä»¬ç¡®å®šç›®æ ‡ DP å¯ç”¨çš„ GPU æ•°é‡ã€‚GBS ä¸ DP çš„æ¯”å€¼èƒ½è®©æˆ‘ä»¬å¾—å‡ºå®ç°æ‰€éœ€ GBS è¿˜éœ€è¦çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚

(ä¾‹å¦‚ï¼ŒDeepSeek å’Œ Llama æ¨¡å‹åœ¨ä¸»è¦é¢„è®­ç»ƒé˜¶æ®µæ˜¯ä»¥ 4k tokens çš„åºåˆ—é•¿åº¦è¿›è¡Œè®­ç»ƒçš„ã€‚)

(2-8k åœ¨é¢„è®­ç»ƒä¸­æ•ˆæœå¾ˆå¥½çš„åŸå› æ˜¯ï¼Œç½‘ç»œä¸Šéå¸¸é•¿çš„æ–‡æ¡£æä¸ºç½•è§ã€‚æœ‰å…³è¯¦ç»†åˆ†æï¼Œè¯·å‚é˜… [Harm çš„åšå®¢æ–‡ç« ](https://www.harmdevries.com/post/context-length/)ã€‚)

å¦‚æœæ¢¯åº¦ç´¯ç§¯æ¯”ç‡å°äº 1ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬æœ‰å¤ªå¤šçš„ GPUï¼ˆç§°ä¸º GPU ä¸°å¯ŒğŸ¤‘ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸ä½¿ç”¨æ‰€æœ‰çš„ GPUï¼Œæ¢ç´¢æ›´å¤§çš„å…¨å±€æ‰¹é‡å¤§å°ï¼Œæˆ–è€…æµ‹è¯•è¾ƒå°çš„ MBSï¼ˆæ¯ä¸ª GPU çš„æ‰¹é‡å¤§å°ï¼‰æ˜¯å¦ä¼šåŠ é€Ÿè®­ç»ƒã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šä¼˜å…ˆè€ƒè™‘æ•´ä½“ååé‡è€Œä¸æ˜¯å•ä¸ª GPU çš„è®¡ç®—æ•ˆç‡ï¼Œä½¿ç”¨æ¯”å¯èƒ½çš„æ›´å°çš„ MBS æ¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

ç°åœ¨æ˜¯æ—¶å€™ä¸¾ä¸€ä¸ªå…·ä½“çš„ä¾‹å­äº†ï¼šå‡è®¾æˆ‘ä»¬æƒ³è¦è®­ç»ƒä¸€ä¸ªæœ€è¿‘æå‡ºçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹çš„å…¨å±€æ‰¹é‡å¤§å°ï¼ˆGBSï¼‰ä¸º 4M tokensï¼Œåºåˆ—é•¿åº¦ä¸º 4kã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ‰¹é‡å¤§å°å°†æ˜¯ 1024 ä¸ªæ ·æœ¬ï¼ˆæˆ‘ä»¬é€‰æ‹©æœ€æ¥è¿‘çš„ 2 çš„å¹‚æ¬¡æ–¹ï¼‰ã€‚å‡è®¾æˆ‘ä»¬è§‚å¯Ÿåˆ°å•ä¸ª GPU åœ¨å†…å­˜ä¸­åªèƒ½å®¹çº³å¾®æ‰¹é‡å¤§å° MBS=2ï¼Œå¹¶ä¸”æœ‰ 128 ä¸ª GPU å¯ç”¨äºè®­ç»ƒã€‚è¿™æ„å‘³ç€é€šè¿‡ 4 ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤ï¼Œæˆ‘ä»¬å°†å®ç°æ¯ä¸ªè®­ç»ƒæ­¥éª¤ 1024 ä¸ªæ ·æœ¬æˆ– 4M tokens çš„ç›®æ ‡ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬çªç„¶æœ‰ 512 ä¸ª GPU å¯ç”¨å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿æŒ MBS=2 å¹¶å°†æ¢¯åº¦ç´¯ç§¯æ­¥éª¤è®¾ç½®ä¸º 1 æ¥å®ç°ç›¸åŒçš„ GBSï¼Œä»è€Œå®ç°ç›¸åŒçš„è®­ç»ƒï¼Œå¹¶è·å¾—æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼

> [!NOTE]
> è¯·è®°ä½ï¼Œåœ¨ 512 ä¸ªåŠä»¥ä¸Š GPU çš„è§„æ¨¡ä¸‹ï¼Œæ ¹æ®æ‰€ä½¿ç”¨çš„ç½‘ç»œï¼Œé€šä¿¡æ“ä½œå°†å¼€å§‹å—*ç¯å½¢å»¶è¿Ÿ*ï¼ˆä¿¡å·æ²¿ç¯å½¢ä¼ è¾“ä¸€åœˆæ‰€éœ€çš„æ—¶é—´ï¼‰çš„é™åˆ¶ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ— æ³•å†å®Œå…¨é‡å æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰é€šä¿¡ã€‚è¿™å°†é™ä½æˆ‘ä»¬çš„è®¡ç®—æ•ˆç‡å¹¶å½±å“ååé‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥å¼€å§‹æ¢ç´¢å…¶ä»–å¹¶è¡Œç»´åº¦ã€‚

è™½ç„¶æ•°æ®å¹¶è¡Œæ€§èƒ½å¤Ÿå¾ˆå¥½åœ°å°†  all-reduce æ¢¯åº¦åŒæ­¥ä¸åå‘è®¡ç®—é‡å ä»¥èŠ‚çœæ—¶é—´ï¼Œä½†è¿™ç§ä¼˜åŠ¿åœ¨å¤§è§„æ¨¡æƒ…å†µä¸‹å¼€å§‹å´©æºƒã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºéšç€æˆ‘ä»¬æ·»åŠ è¶Šæ¥è¶Šå¤šçš„ GPUï¼ˆæ•°ç™¾ä¸ªæˆ–æ•°åƒä¸ªï¼‰ï¼Œåè°ƒå®ƒä»¬ä¹‹é—´çš„å¼€é”€æ˜¾è‘—å¢é•¿ï¼Œå¹¶ä¸”ç½‘ç»œéœ€æ±‚å¯¹äºæ‰€è·å¾—çš„æ”¶ç›Šæ¥è¯´å˜å¾—è¿‡å¤§ã€‚ç»“æœï¼Œæˆ‘ä»¬æ¯å‘ç³»ç»Ÿä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„GPUï¼Œæˆ‘ä»¬çš„è®¾ç½®å°†å˜å¾—è¶Šæ¥è¶Šä½æ•ˆã€‚

è®©æˆ‘ä»¬é€šè¿‡ä¸€äº›åŸºå‡†æµ‹è¯•æ¥çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•å®ç°çš„ï¼š

[äº¤äº’å›¾]

æˆ‘ä»¬å‘ç°ï¼Œåœ¨è¶…è¿‡æŸä¸ªé™åˆ¶åï¼Œæˆ‘ä»¬çš„ååé‡å¼€å§‹æ˜¾è‘—ä¸‹é™ï¼Œè€Œæ¯ä¸ª GPU çš„å†…å­˜ä½¿ç”¨é‡ä¿æŒä¸å˜ï¼Œå¹¶ä¸”ä¸ä¼šå› ä¸ºå¢åŠ æ›´å¤šçš„ DP ranks è€Œå—åˆ°å½±å“ã€‚

*æ•°æ®å¹¶è¡Œæ˜¯æˆ‘ä»¬é¦–ä¸ªï¼ˆç®€å•ï¼‰çš„ç­–ç•¥ï¼Œç”¨äºå°†è®­ç»ƒæ‰©å±•åˆ°æ›´å¤šçš„ GPU ä¸Šã€‚è¿™ç§æŠ€æœ¯ç±»ä¼¼äºæ¢¯åº¦ç´¯ç§¯ï¼Œä½†å®ƒå¯¹å¾®æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œä»è€Œæé«˜ååé‡ï¼*

ç„¶è€Œï¼Œæ•é”çš„è¯»è€…å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œè¿™æ˜¯å‡è®¾æˆ‘ä»¬è‡³å°‘èƒ½å°†ä¸€ä¸ªè¾“å…¥æ ·æœ¬çš„å‰å‘ä¼ æ’­ï¼ˆmbs=1ï¼‰è£…å…¥æˆ‘ä»¬çš„ GPU å†…å­˜ã€‚ä½†å¹¶éæ€»æ˜¯å¦‚æ­¤ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿å¯ç”¨äº†æ¿€æ´»é‡æ–°è®¡ç®—ï¼Œè¾ƒå¤§çš„æ¨¡å‹ä¹Ÿæ— æ³•è£…å…¥å•ä¸ª GPU ä¸­ï¼š

> [!tip]
> æç¤ºï¼šä½ å¯ä»¥é€šè¿‡å°†æ¨¡å‹å‚æ•°æ•°é‡ä¹˜ä»¥ 2 æ¥å¿«é€Ÿä¼°ç®—æ¨¡å‹å‚æ•°æ‰€éœ€çš„æœ€å°å†…å­˜ï¼Œä¾‹å¦‚ 70B â†’ 140GBï¼ˆ=133GiBï¼‰

[äº¤äº’å›¾]

æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨è¾¾åˆ°ä¸€å®šçš„æ‰©å±•æ°´å¹³åï¼Œæ•°æ®å¹¶è¡Œå¼€å§‹å‡ºç°ä¸€äº›é™åˆ¶æ€§çš„é€šä¿¡å¼€é”€ã€‚å¯¹äºè¿™äº›æ›´å¤§çš„æ¨¡å‹æˆ–å¤§æ‰¹é‡å¤§å°ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–é€‰æ‹©å—ï¼Ÿå¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬ç¡®å®æœ‰ä¸€äº›è§£å†³æ–¹æ¡ˆã€‚å®ƒä»¬è¦ä¹ˆæ¶‰åŠå°†ä¸€äº›å¼ é‡ç§»åŠ¨åˆ° CPUï¼Œè¦ä¹ˆå°†æƒé‡/æ¢¯åº¦/ä¼˜åŒ–å™¨çŠ¶æ€å¼ é‡æ‹†åˆ†åˆ° GPU è®¾å¤‡ä¸Šï¼è®©æˆ‘ä»¬å¼€å§‹æ·±å…¥äº†è§£å®ƒä»¬ã€‚

æœ‰ä¸¤ç§ä¸»è¦çš„æ‹†åˆ†æ–¹æ³•ï¼šå¹¶è¡Œæ€§ï¼ˆå¼ é‡å¹¶è¡Œã€ä¸Šä¸‹æ–‡å¹¶è¡Œæˆ–æµæ°´çº¿å¹¶è¡Œï¼‰å’Œå…±äº«ï¼ˆDeepSpeed Zero æˆ– PyTorch FSDPï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ­£äº¤çš„ï¼Œå®é™…ä¸Šå¯ä»¥ç»“åˆèµ·æ¥ï¼

å…±äº«èŒƒå¼ä¸ DP å¯†åˆ‡ç›¸å…³ï¼Œå› æ­¤æˆ‘ä»¬å°†é¦–å…ˆé€šè¿‡ç ”ç©¶ ZeRO æ–¹æ³•æ¥å¯¹å…¶è¿›è¡Œäº†è§£ï¼

### 3.4 ZeRO (**Ze**roÂ **R**edundancyÂ **O**ptimizer)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç» DeepSpeed ZeROï¼ˆé›¶å†—ä½™ä¼˜åŒ–å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„å†…å­˜å†—ä½™ã€‚

è™½ç„¶æ•°æ®å¹¶è¡Œæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ‰©å±•è®­ç»ƒçš„æ–¹å¼ï¼Œä½†åœ¨æ¯ä¸ª DP rank ä¸Šç®€å•å¤åˆ¶ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ä¼šå¼•å…¥æ˜¾è‘—çš„å†…å­˜å†—ä½™ã€‚ZeRO é€šè¿‡å°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°åœ¨æ•°æ®å¹¶è¡Œç»´åº¦ä¸Šè¿›è¡Œåˆ’åˆ†æ¥æ¶ˆé™¤å†…å­˜å†—ä½™ï¼ŒåŒæ—¶ä»ç„¶å…è®¸ä½¿ç”¨å®Œæ•´çš„å‚æ•°é›†è¿›è¡Œè®¡ç®—ã€‚è¿™æœ‰æ—¶éœ€è¦åœ¨ DP rank ä¹‹é—´è¿›è¡Œæ›´å¤šçš„é€šä¿¡ï¼Œè¿™äº›é€šä¿¡æ˜¯å¦èƒ½å¤Ÿå®Œå…¨é‡å ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°†ä¼šçœ‹åˆ°ï¼

åœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ ZeRO-1 åˆ° ZeRO-3ï¼Œå› ä¸ºè¿™åº”è¯¥èƒ½è®©æˆ‘ä»¬å…¨é¢äº†è§£å®ƒå¦‚ä½•å¸®åŠ©å‡å°‘å†…å­˜å ç”¨ï¼ŒåŒæ—¶å±•ç¤ºéœ€è¦è€ƒè™‘çš„æƒè¡¡ã€‚ä½ å¯ä»¥åœ¨ [DeepSpeed æ–‡æ¡£](https://www.deepspeed.ai/tutorials/zero/) ä¸­æ‰¾åˆ°æ›´å¤š ZeRO çš„ç›¸å…³å†…å®¹ã€‚

è¿™ç§æ–¹æ³•åˆ†ä¸º ZeRO çš„ä¸‰ä¸ªå¯èƒ½çš„ä¼˜åŒ–é˜¶æ®µï¼š

- ZeRO-1ï¼šä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒº
- ZeRO-2ï¼šä¼˜åŒ–å™¨çŠ¶æ€+æ¢¯åº¦åˆ†åŒº
- ZeRO-3ï¼ˆä¹Ÿç§°ä¸º FSDPï¼Œå³â€œå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œâ€ï¼‰ï¼šä¼˜åŒ–å™¨çŠ¶æ€+æ¢¯åº¦+å‚æ•°åˆ†åŒº

ï¼ˆå½“æˆ‘ä»¬è¯´åˆ†åŒºæ—¶ï¼Œæ˜¯æŒ‡æ²¿ç€ DP è½´è¿›è¡Œåˆ†åŒºï¼Œå› ä¸º ZeRO æ˜¯æ•°æ®å¹¶è¡Œçš„ä¸€éƒ¨åˆ†ã€‚ç¨åæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ²¿ç€å…¶ä»–è½´è¿›è¡Œåˆ†åŒºã€‚ï¼‰

ä½ å¯èƒ½å¿½ç•¥äº†æˆ‘ä»¬åœ¨å¯è¿›è¡Œåˆ†ç‰‡å¤„ç†çš„äº‹ç‰©ä¸­çš„æ¿€æ´»æ“ä½œã€‚ç”±äºæ¨¡å‹çš„æ¯ä¸ª DP å‰¯æœ¬æ¥æ”¶ä¸åŒçš„å¾®æ‰¹æ¬¡ï¼Œå› æ­¤æ¯ä¸ª DP rank ä¸Šçš„æ¿€æ´»æ“ä½œä¹Ÿå„ä¸ç›¸åŒï¼Œæ‰€ä»¥å®ƒä»¬ä¸ä¼šè¢«å¤åˆ¶ï¼Œä¹Ÿå°±æ— æ³•è¿›è¡Œåˆ†ç‰‡ï¼

è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹é€šè¿‡å¯¹æ¯ä¸ª ZeRO é˜¶æ®µè¿›è¡Œåˆ†åŒºï¼Œæˆ‘ä»¬èƒ½èŠ‚çœå¤šå°‘ï¼

#### 3.4.1 å†…å­˜ä½¿ç”¨æƒ…å†µå†æ¢

ä½ å¯èƒ½è¿˜è®°å¾—æˆ‘ä»¬åœ¨å‰é¢çš„ç« èŠ‚ä¸­æåˆ°çš„æ ‡å‡†è®­ç»ƒæœŸé—´ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬æŠŠæ¨¡å‹å‚æ•°çš„æ•°é‡è®°ä¸º $Î¨$ï¼ˆä¹‹å‰ç”¨ $N$ è¡¨ç¤ºï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹ ZeRO è®ºæ–‡çš„ç¬¦å·è¡¨ç¤ºæ³•ï¼‰ã€‚åœ¨ä½¿ç”¨ Adam ä¼˜åŒ–å™¨çš„æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼ˆæ›´å¤šç»†èŠ‚è§åé¢çš„ç« èŠ‚ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨çš„æ¯ä¸€é¡¹çš„å†…å­˜ä½¿ç”¨é‡ä¸ºï¼š

- æ¨¡å‹çš„å‚æ•°ï¼ˆåŠç²¾åº¦ï¼Œå³ bf16/fp16ï¼‰ï¼š$2Î¨$
- æ¨¡å‹çš„æ¢¯åº¦ï¼ˆåŠç²¾åº¦ï¼Œå³ bf16/fp16ï¼‰ï¼š$2Î¨$
- æ¨¡å‹çš„ fp32 å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼š$4Î¨+(4Î¨+4Î¨)$
- æ¨¡å‹çš„ fp32 æ¢¯åº¦ï¼š$4Î¨$ï¼ˆå¯é€‰ï¼Œä»…åœ¨æˆ‘ä»¬è¦ä»¥ fp32 ç´¯ç§¯æ¢¯åº¦æ—¶è®¡ç®—ï¼‰

å¦‚æœæˆ‘ä»¬ä¸åœ¨ fp32 ä¸­ç´¯ç§¯æ¢¯åº¦ï¼Œé‚£ä¹ˆæ€»çš„å†…å­˜æ¶ˆè€—ä¸º $2Î¨+2Î¨+12Î¨$ï¼›å¦‚æœæˆ‘ä»¬è¿›è¡Œç´¯ç§¯ï¼Œé‚£ä¹ˆå°†æ˜¯$2Î¨+6Î¨+12Î¨$ã€‚ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç°åœ¨å…ˆå…³æ³¨ä¸è¿›è¡Œ fp32 æ¢¯åº¦ç´¯ç§¯çš„æƒ…å†µï¼Œä¸è¿‡ä½ å¯ä»¥å°†å— ZeRO-2 å’Œ ZeRO-3 å½±å“çš„æ¢¯åº¦é¡¹çš„é¢å¤–å­—èŠ‚æ•°åŠ ä¸Šå»ã€‚

ZeRO çš„ç†å¿µæ˜¯å°†è¿™äº›å¯¹è±¡åˆ†ç‰‡åˆ° DP å„ä¸ª rank ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»…å­˜å‚¨è¿™äº›é¡¹çš„ä¸€ä¸ªåˆ‡ç‰‡ï¼Œå½“ä¸”ä»…å½“éœ€è¦æ—¶æ‰å¯¹è¿™äº›é¡¹è¿›è¡Œé‡æ„ï¼Œä»è€Œå°†å†…å­˜ä½¿ç”¨é‡æŒ‰æ•°æ®å¹¶è¡Œåº¦Â $N_d$â€‹Â è¿›è¡Œåˆ’åˆ† ã€‚

![zero_memory.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/zero_memory.svg)
è¿™é‡Œ $Î¨$ è¡¨ç¤ºå‚æ•°æ•°é‡ï¼Œ$k$ è¡¨ç¤ºä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ä¹˜æ•°ï¼ˆå¦‚æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„ï¼Œå¯¹äº Adamï¼Œ$k=12$ï¼‰ï¼Œ$N_d$ è¡¨ç¤º DP åº¦ã€‚

è®©æˆ‘ä»¬é€šè¿‡æ¢ç©¶æ¯ä¸ª ZeRO é˜¶æ®µçš„å·¥ä½œåŸç†æ¥è§£é‡Šè¿™å¼ å›¾åŠå…¶æ•°å€¼ã€‚æˆ‘ä»¬å°†ä» ZeRO-1 å¼€å§‹ã€‚

#### 3.4.2 ZeRO-1: åˆ†åŒºä¼˜åŒ–å™¨çŠ¶æ€

åœ¨æ™®é€š DP ä¸­ï¼Œæ‰€æœ‰è¿›ç¨‹åœ¨åå‘ä¼ æ’­åæ”¶é›†ç›¸åŒçš„æ¢¯åº¦ï¼Œå¹¶åŒæ—¶æ‰§è¡Œç›¸åŒçš„ä¼˜åŒ–å™¨æ­¥éª¤ã€‚è¿™çœ‹èµ·æ¥åƒæ˜¯å¾ˆå¤šé‡å¤çš„å·¥ä½œã€‚æˆ‘ä»¬èƒ½å¦é¿å…è¿™ç§æƒ…å†µï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨å‘¢ï¼Ÿ

åœ¨ ZeRO-1 ä¸­ï¼Œä¼˜åŒ–å™¨çŠ¶æ€è¢«åˆ’åˆ†ä¸º $N_d$ ä¸ªç›¸ç­‰éƒ¨åˆ†ï¼Œå…¶ä¸­ $N_d$ æ˜¯æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰åº¦ã€‚è¿™æ„å‘³ç€åˆ†å¸ƒåœ¨æ¯ä¸ª DP rank ä¸Šçš„æ¯ä¸ªæ¨¡å‹å‰¯æœ¬ä»…è·Ÿè¸ª $1/N_d$ çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€‚åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ï¼Œåªæœ‰ $1/N_d$ çš„ float32 æƒé‡è¢«æ›´æ–°ã€‚

ç„¶è€Œï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªå‰¯æœ¬éƒ½éœ€è¦æ‰€æœ‰å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹åæ·»åŠ ä¸€ä¸ªé¢å¤–çš„ ***all-gather*** æ“ä½œï¼ˆè¿™æ˜¯æˆ‘ä»¬é‡åˆ°çš„ç¬¬äºŒç§é€šä¿¡åŸè¯­ï¼ï¼‰ï¼Œä»¥ä¾¿æ¯ä¸ªæ¨¡å‹å‰¯æœ¬éƒ½æœ‰å®Œæ•´çš„æ›´æ–°åçš„æƒé‡é›†ã€‚

è¿™è§£é‡Šäº†æˆ‘ä»¬åœ¨ä¸Šå›¾ä¸­çœ‹åˆ°çš„å†…å­˜å ç”¨å…¬å¼ $2Î¨+2Î¨+kÎ¨/N_d$ï¼Œä»¥ä¸‹æ˜¯å•ä¸ªè®­ç»ƒæ­¥éª¤çš„æ“ä½œé¡ºåºæ€»ç»“ï¼š

- åœ¨æ¯ä¸ªå‰¯æœ¬ä¸Šä½¿ç”¨ç›¸åŒçš„å®Œæ•´ bf16 å‚æ•°é›†è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œä½†ä¸åŒå‰¯æœ¬å¤„ç†ä¸åŒçš„å¾®æ‰¹æ¬¡ã€‚
- åœ¨æ¯ä¸ªå‰¯æœ¬ä¸Šä½¿ç”¨ç›¸åŒçš„å®Œæ•´æ¢¯åº¦é›†è¿›è¡Œåå‘ä¼ æ’­ï¼Œä½†ä¸åŒå‰¯æœ¬å¤„ç†ä¸åŒçš„å¾®æ‰¹æ¬¡ã€‚
- å¯¹æ¢¯åº¦æ‰§è¡Œ reduce-scatter æ“ä½œï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹å›¾ä¸­è§£é‡Š reduce-scatter åŸè¯­ï¼‰ã€‚
- æ¯ä¸ªå‰¯æœ¬åœ¨å…¶æœ¬åœ°ä¼˜åŒ–å™¨ä¸Šæ‰§è¡Œä¸€æ­¥ä¼˜åŒ–å™¨æ“ä½œï¼ˆä»…æœ‰ $1/N_d$ ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰ï¼Œä»¥è·å¾—æ›´æ–°çš„ $1/N_d$ fp32 å‚æ•°ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºå®Œæ•´ bf16 å‚æ•°é›†çš„ $1/N_d$ã€‚
- åœ¨ bf16 å‚æ•°ä¹‹é—´æ‰§è¡Œ all-gather æ“ä½œï¼Œå°†ç¼ºå¤±çš„åˆ‡ç‰‡å‘é€å›æ¯ä¸ªå‰¯æœ¬ã€‚è¿™æ˜¯ ZeRO ä¸­çš„æ–°æ“ä½œï¼Œåœ¨æ™®é€šçš„æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ä¸­æœªä½¿ç”¨ã€‚

> [!NOTE]
> æ³¨æ„ï¼šreduce-scatter æ¯” all-reduce å¿« 2 å€ï¼_è€¶ï¼Œç¬¬ä¸‰ç§é€šä¿¡åŸè¯­ï¼_
> 

ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“è¿™ä¸ª â€œreduce-scatterâ€ æ“ä½œæ˜¯ä»€ä¹ˆï¼Œä»¥åŠè¿™ä¸€åˆ‡çœ‹èµ·æ¥æ˜¯æ€æ ·çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å€ŸåŠ©ä¸‹é¢çš„å›¾ç¤ºè®©è¿™ä¸€åˆ‡æ›´åŠ ç›´è§‚ã€‚æˆ‘ä»¬å°†è¯¦ç»†è®²è§£å‰å‘/åå‘ä¼ æ’­å‘¨æœŸçš„æ‰€æœ‰æ­¥éª¤ï¼š

![dp_zero1.gif|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero1.gif)

åœ¨å®é™…é€šä¿¡æ–¹é¢ï¼Œä¸æ™®é€š DP ç›¸æ¯”ï¼ŒZero-1 å°†æˆ‘ä»¬çš„ â€œall-reduceâ€ æ¢¯åº¦é€šä¿¡æ›´æ”¹ä¸º â€œreduce-scatteâ€ æ“ä½œï¼Œå¹¶åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹åæ·»åŠ ä¸€ä¸ªé’ˆå¯¹æ‰€æœ‰å‚æ•°çš„ â€œall-gatherâ€ æ“ä½œã€‚å…¶è¿‡ç¨‹å¦‚ä¸‹ï¼š

![dp_zero1_overlap.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero1_overlap.svg)

å¦‚æœä½ ä¸€ç›´å…³æ³¨ï¼Œä¼šä»æ™®é€š DP ä¸­å›æƒ³èµ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åå‘ä¼ æ’­è®¡ç®—è¿‡ç¨‹ä¸­é‡å è¿›è¡Œ all-reduce æ¢¯åº¦é€šä¿¡ã€‚åœ¨ ZeRO-1 ä¸­ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç ”ç©¶å¦‚ä½•é«˜æ•ˆåœ°é‡å æ–°æ·»åŠ çš„ bf16 å‚æ•° all-gather æ“ä½œã€‚ä¸»è¦æœ‰ä¸¤ç§ç­–ç•¥ï¼š

- åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ï¼šæˆ‘ä»¬å¯ä»¥åœ¨ä¼˜åŒ–å™¨æ›´æ–°éƒ¨åˆ†å‚æ•°åç«‹å³å¯åŠ¨ all-gather æ“ä½œã€‚è¿™ä½¿å¾—é€šä¿¡æœ‰å¯èƒ½ä¸å…¶ä»–å‚æ•°çš„æ›´æ–°é‡å ã€‚
- åœ¨å‰å‘ä¼ æ’­æœŸé—´ï¼šæˆ‘ä»¬å¯ä»¥å°†æ¯å±‚å‚æ•°çš„ all-gather æ“ä½œä¸å‰å‘ä¼ æ’­è¿‡ç¨‹é‡å èµ·æ¥ã€‚

> [!NOTE]
> ä¸å¹¸çš„æ˜¯ï¼Œè¿™äº›æŠ€æœ¯å¹¶ä¸å®¹æ˜“å®ç°ï¼Œå¹¶ä¸”éœ€è¦å·§å¦™åœ°ä½¿ç”¨é’©å­/åˆ†æ¡¶ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ PyTorch åŸç”Ÿçš„ ZeRO-3/FSDP å®ç°ï¼Œå¹¶å°† FSDPUnit è®¾ç½®ä¸ºæ•´ä¸ªæ¨¡å‹ï¼Œå…³äºè¿™ä¸ªçš„æ›´å¤šç»†èŠ‚ç¨åä¼šä»‹ç»ã€‚

åœ¨ ZeRO-1 ä¸­ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å·²è¢«åˆ†åŒºï¼Œè¿™æ„å‘³ç€æ¯ä¸ªå‰¯æœ¬ä»…æ›´æ–° $1/N_d$ çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€‚æ•é”çš„è¯»è€…è‚¯å®šå·²ç»æ³¨æ„åˆ°ï¼Œå…¶å®ä¸€å¼€å§‹å¹¶ä¸éœ€è¦æ‰€æœ‰ DP ranks ä¸Šéƒ½æœ‰æ‰€æœ‰æ¢¯åº¦ï¼Œå› ä¸ºä¼˜åŒ–æ­¥éª¤åªéœ€è¦å…¶ä¸­ä¸€éƒ¨åˆ†æ¢¯åº¦ã€‚è¿™å°±å¼•å‡ºäº† ZeRO-2ï¼

#### 3.4.3 ZeRO-2: æ·»åŠ æ¢¯åº¦åˆ†å‰²

ç”±äºæˆ‘ä»¬åªéœ€è¦åœ¨æ¯ä¸ªå‰¯æœ¬ä¸Šæ‹¥æœ‰ä¸ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ç›¸å¯¹åº”çš„æ¢¯åº¦åˆ†ç‰‡ï¼Œå› æ­¤å°†æ¢¯åº¦ä¹Ÿç±»ä¼¼åœ°åˆ†ç‰‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯å¯¹æ¢¯åº¦æ‰§è¡Œ all-reduce æ“ä½œï¼Œè€Œæ˜¯åªæ‰§è¡Œ reduce-scatter æ“ä½œï¼æˆ‘ä»¬åªåœ¨å†…å­˜ä¸­ä¼ æ’­æ‰€éœ€çš„ $1/N_d$ æ¢¯åº¦ï¼Œä»è€Œæ¯” ZeRO-1 èŠ‚çœæ›´å¤šå†…å­˜ã€‚

åœ¨ FP32 æ¢¯åº¦ç´¯ç§¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿ç•™ $1/N_d$ fp32_gradsï¼Œç”¨äºç´¯ç§¯æ¥è‡ª reduce-scatter çš„ bf16 æ¢¯åº¦ã€‚åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ $1/N_d$ fp32_gradsã€‚

![dp_zero2.gif|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero2.gif)

ç°åœ¨å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œå¯¹æ¢¯åº¦è¿›è¡Œåˆ†ç‰‡ä¼šå¯¼è‡´ $2Î¨+\frac{2Î¨+kÎ¨}{N_d}$ï¼Œå¹¶ä¸”éšç€ $N_d$â€‹Â çš„å¢åŠ ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥èŠ‚çœå¤šè¾¾ 8 å€çš„å†…å­˜ã€‚åœ¨é€šä¿¡æ–¹é¢ï¼Œä¸ ZeRO-1 çš„è¿‡ç¨‹ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬å³æ—¶è¿›è¡Œé€šä¿¡å¹¶é‡Šæ”¾ã€‚æ€»çš„æ¥è¯´ï¼Œå°±é€šä¿¡è€Œè¨€ï¼ŒZeRO-2 ä¹Ÿå› æ­¤ç­‰åŒäºæ™®é€šçš„ DP è®­ç»ƒã€‚

åœ¨é€šä¿¡æ–¹é¢ï¼ŒZeRO-2 ä¸ ZeRO-1 ç›¸ä¼¼ï¼Œå®ƒä»¬éƒ½éœ€è¦å¯¹æ¢¯åº¦è¿›è¡Œ reduce-scatter æ“ä½œï¼Œå¹¶å¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œ all-gather æ“ä½œã€‚
![dp_zero2_overlap.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero2_overlap.svg)

> [!tip]
> æ³¨æ„ï¼šæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œä¸ ZeRO-1 ç›¸æ¯”ï¼Œä½¿ç”¨ ZeRO-2 å¹¶æ²¡æœ‰çœŸæ­£çš„é¢å¤–å¼€é”€ï¼Œå®é™…ä¸Š ZeRO-2 é€šå¸¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚


ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹æ¢¯åº¦è¿›è¡Œäº†åˆ†ç‰‡å¤„ç†ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ˜¯å¦å·²ç»å®Œæˆäº†ä»»åŠ¡ï¼Œè¿˜æ˜¯å¯ä»¥ç»§ç»­è¿™æ ·åšå‘¢ï¼Ÿå—¯ï¼Œå·®ä¸å¤šã€‚æ¥ä¸‹æ¥å°±æ˜¯ ZeRO-3ï¼

#### 3.4.4 ZeRO-3: æ·»åŠ å‚æ•°åˆ†åŒº

å¯¹äºç¬¬ 3 é˜¶æ®µï¼Œæˆ‘ä»¬å°†ä¸Šè¿°åœ¨æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰å‰¯æœ¬ä¸Šå¯¹ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦è¿›è¡Œåˆ†ç‰‡çš„æ–¹æ³•æ‰©å±•åˆ°å¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œåˆ†ç‰‡ã€‚

> [!NOTE]
> è¿™ä¸ªé˜¶æ®µåœ¨ PyTorch åŸç”Ÿå®ç°ä¸­ä¹Ÿè¢«ç§°ä¸º FSDPï¼ˆå®Œå…¨å…±äº«æ•°æ®å¹¶è¡Œï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨ ZeRO-3 è¿™ä¸ªæœ¯è¯­ï¼Œä½†æ— è®ºä½•æ—¶çœ‹åˆ°å®ƒï¼Œä½ éƒ½å¯ä»¥å°†å…¶ç†è§£ä¸º FSDP ã€‚
> 

é‚£ä¹ˆï¼Œå¦‚æœæ¨¡å‹çš„æ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¯åˆ†å¸ƒå¼å­˜å‚¨çš„ï¼Œæˆ‘ä»¬åœ¨å®è·µä¸­å¦‚ä½•è¿›è¡Œå‰å‘ä¼ æ’­æˆ–åå‘ä¼ æ’­å‘¢ï¼Ÿå¾ˆç®€å•ï¼Œæˆ‘ä»¬åœ¨éœ€è¦æ—¶æŒ‰éœ€æ”¶é›†å®ƒä»¬ã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼š

![dp_zero3_fwd.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero3_fwd.svg)

å› æ­¤ï¼Œåœ¨è¿›è¡Œå‰å‘ä¼ æ’­å¹¶ä¾æ¬¡é€šè¿‡å„å±‚æ—¶ï¼Œæˆ‘ä»¬ä¼šæŒ‰éœ€æ£€ç´¢å¿…è¦çš„å‚æ•°ï¼Œå¹¶åœ¨ä¸å†éœ€è¦è¿™äº›å‚æ•°æ—¶ç«‹å³å°†å®ƒä»¬ä»å†…å­˜ä¸­æ¸…é™¤ã€‚åå‘ä¼ æ’­çš„å·¥ä½œæ–¹å¼ç›¸åŒï¼Œåªæ˜¯æµç¨‹ç›¸åï¼Œæˆ‘ä»¬ä¼šç”Ÿæˆæ¢¯åº¦åˆ†ç‰‡ï¼š

![dp_zero3_bwd.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero3_bwd.svg)

å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æŒç»­æ‰§è¡Œè¿™äº›å…¨è§„çº¦æ“ä½œã€‚ä¸ Zero-2 ç›¸æ¯”ï¼Œåœ¨ä¸€ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œè¿™ç›¸å½“äºé¢å¤–å¢åŠ äº†Â $2â‹…\text{num\_layers}âˆ’1$Â æ¬¡ all-gathers æ“ä½œï¼Œè€Œä¸”æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°çš„ï¼Œæ¯æ¬¡æ“ä½œéƒ½ä¼šå¸¦æ¥ä¸€å®šçš„åŸºç¡€å»¶è¿Ÿå¼€é”€ ã€‚

![dp_zero3_overlap.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/dp_zero3_overlap.svg)

åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå½“æˆ‘ä»¬éœ€è¦å‚æ•°æ—¶ï¼Œæˆ‘ä»¬ä¼šå¯¹å®ƒä»¬æ‰§è¡Œ all-gather æ“ä½œï¼Œå› æ­¤ä¼šäº§ç”Ÿ $Î¨$ çš„é€šä¿¡å¼€é”€ã€‚ç”±äºåœ¨å‰å‘ä¼ æ’­ä¸­ä¸€æ—¦ç”¨åˆ°å‚æ•°å°±ä¼šç«‹å³ä¸¢å¼ƒï¼Œæ‰€ä»¥åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æˆ‘ä»¬è¿˜éœ€è¦å†è¿›è¡Œä¸€æ¬¡ all-gather æ“ä½œï¼Œè¿™åˆäº§ç”Ÿäº† $Î¨$ çš„é€šä¿¡å¼€é”€ã€‚æœ€åï¼Œå’Œ ZeRO-2 ä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦ä¹Ÿéœ€è¦è¿›è¡Œç›¸åŒçš„ ***reduce-scatter*** æ“ä½œï¼Œè¿™åœ¨é€šä¿¡æ–¹é¢åŒæ ·éœ€è¦ $Î¨$ çš„å¼€é”€ã€‚ç»¼ä¸Šï¼Œæ€»çš„é€šä¿¡å¼€é”€ä¸º $3Î¨$ï¼Œè€Œ ZeRO-2 çš„é€šä¿¡å¼€é”€ä¸º $2Î¨$ã€‚

è¿™å¬èµ·æ¥å¯èƒ½åƒæ˜¯ä¼šæœ‰å¤§é‡çš„é€šä¿¡å¼€é”€ï¼Œä½†å®é™…ä¸Šæƒ…å†µè¿˜æŒºå¥½çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥é‡‡ç”¨æ‰€è°“çš„é¢„å–ï¼ˆprefetchingï¼‰æŠ€æœ¯ï¼Œå°†ä¸‹ä¸€å±‚å‚æ•°çš„é€šä¿¡ä¸å½“å‰å±‚çš„å‰å‘ä¼ æ’­è¿‡ç¨‹é‡å èµ·æ¥ã€‚é€šè¿‡é¢„å–ï¼Œåœ¨è¿›è¡Œå‰å‘ä¼ æ’­æ—¶è®¡ç®—å½“å‰å±‚ï¼ˆç¬¬ $n$ å±‚ï¼‰çš„å‰å‘è¿‡ç¨‹çš„åŒæ—¶ï¼Œæˆ‘ä»¬ä¼š â€œall-gatherâ€ ç¬¬ $n+1$ å±‚çš„æƒé‡ï¼›åŒæ ·åœ°ï¼Œåœ¨è®¡ç®—ç¬¬ $n$ å±‚çš„åå‘ä¼ æ’­è¿‡ç¨‹æ—¶ï¼Œæˆ‘ä»¬ä¼š â€œall-gatherâ€ ç¬¬ $n-1$ å±‚çš„æƒé‡ã€‚å½“ç„¶ï¼Œåªæœ‰å½“æˆ‘ä»¬å¯¹æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰çš„æ‰©å±•ç¨‹åº¦ä¸å¤ªå¤§æ—¶ï¼Œè¿™ç§é‡å æ‰æ˜¯æœ‰æ•ˆçš„ã€‚ï¼ˆç»éªŒæ³•åˆ™ï¼šæ•°æ®å¹¶è¡Œçš„è§„æ¨¡ä¸åº”è¶…è¿‡ 512ï¼‰

åœ¨å†…å­˜æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ–¹ç¨‹ç°åœ¨è¾¾åˆ°äº†å…¶æœ€ç»ˆå½¢å¼ $\frac{2Î¨+2Î¨+kÎ¨}{N_d}$ï¼Œè¿™æ„å‘³ç€å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå¢åŠ  DP ranksï¼Œè‡³å°‘å¯¹äºæ¨¡å‹ç›¸å…³å‚æ•°è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥æ— é™é™ä½å†…å­˜ä½¿ç”¨é‡ã€‚æ³¨æ„ï¼Œè¿™å¯¹ä¸­é—´æ¿€æ´»å€¼å¹¶æ— å¸®åŠ©ï¼Œå¯¹äºä¸­é—´æ¿€æ´»å€¼ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¿€æ´»å€¼æ£€æŸ¥ç‚¹å’Œæ¢¯åº¦ç´¯ç§¯çš„æ–¹æ³•ã€‚

*è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¿„ä»Šä¸ºæ­¢åœ¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰å’Œ ZeRO æ–¹é¢çš„æ¢ç´¢å†ç¨‹ï¼šæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œé€šè¿‡ç®€å•åœ°å¢åŠ æ¨¡å‹å‰¯æœ¬ï¼Œåˆ©ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰å¯ä»¥æ˜¾è‘—æé«˜è®­ç»ƒçš„ååé‡ã€‚è€Œå€ŸåŠ© ZeROï¼Œæˆ‘ä»¬ç”šè‡³èƒ½å¤Ÿè®­ç»ƒé‚£äº›é€šå¸¸æ— æ³•æ”¾å…¥å•ä¸ª GPU çš„æ¨¡å‹ï¼Œæ–¹æ³•æ˜¯å°†å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ä¸­è¿›è¡Œåˆ†ç‰‡å¤„ç†ï¼Œä¸è¿‡è¿™ä¼šå¸¦æ¥ä¸€å®šçš„é€šä¿¡å¼€é”€ã€‚*

å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº FSDP1ã€FSDP2 ä»¥åŠå®ƒä»¬å‘¨å›´ä¸€äº›å®ç°å¤æ‚æ€§çš„å†…å®¹ï¼Œä½ åº”è¯¥èŠ±äº›æ—¶é—´ä»”ç»†é˜…è¯»[è¿™ç¯‡ä¸é”™çš„åšå®¢](https://christianjmills.com/posts/mastering-llms-course-notes/conference-talk-012/)ã€‚

ç„¶è€Œï¼Œè¿™é‡Œå­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œå³ DP ä»…åœ¨æ¨¡å‹çš„ä¸€ä¸ªå±‚èƒ½é€‚é…å•ä¸ª GPU æ—¶æ‰æœ‰æ•ˆï¼Œè€Œ ZeRO åªèƒ½å¯¹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†åŒºï¼Œå´æ— æ³•å¯¹æ¿€æ´»å†…å­˜è¿›è¡Œåˆ†åŒºï¼æˆ‘ä»¬ä»æ¿€æ´»å†…å­˜çš„è®¨è®ºä¸­å›å¿†ä¸€ä¸‹ï¼Œè¿™éƒ¨åˆ†å†…å­˜éšç€åºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°è€Œæ‰©å±•ã€‚è‡ªç„¶åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°é™åˆ¶è¿™äº›å› ç´ ï¼Œä½†åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸å¸Œæœ›ç”±äºç¡¬ä»¶çš„é™åˆ¶è€Œåªèƒ½ä½¿ç”¨çŸ­åºåˆ—é•¿åº¦è¿›è¡Œè®­ç»ƒã€‚

[äº¤äº’å›¾]

ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæ˜¯æ—¶å€™æ¢ç´¢ä¸€ç§æ–°çš„ã€æ­£äº¤çš„å¹¶è¡Œæ€§è½´â€”â€”å¼ é‡å¹¶è¡Œæ€§ï¼ˆTPï¼‰äº†ã€‚ä¸ä¾èµ–å¤§é‡å‚æ•°é€šä¿¡çš„ ZeRO3 ä¸åŒï¼ŒTP æå‡ºåœ¨è®¾å¤‡é—´å¯¹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠæ¿€æ´»è¿›è¡Œåˆ†ç‰‡ï¼Œè€Œä¸éœ€è¦åœ¨GPU ä¹‹é—´è¿›è¡Œæ¨¡å‹å‚æ•°çš„é€šä¿¡ã€‚

ä»€ä¹ˆï¼Ÿè¿™æ€ä¹ˆå¯èƒ½ï¼Ÿï¼è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢è¿™ç§çœ‹ä¼¼ç¥å¥‡çš„æ–¹æ³•å§ï¼ ğŸ™‚

## å››ã€å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰

æ‰€ä»¥æˆ‘ä»¬å·²ç»ä½¿ç”¨ ZeRO å¯¹æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œäº†åˆ†ç‰‡ï¼Œä½†æ˜¯ä¸€æ—¦æ¿€æ´»å†…å­˜è¶…è¿‡æˆ‘ä»¬çš„å†…å­˜é¢„ç®—ï¼Œæˆ‘ä»¬å°±é‡åˆ°äº†ä¸€ä¸ªé™åˆ¶ã€‚æ¬¢è¿å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æƒé‡ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠæ¿€æ´»è¿›è¡Œåˆ†ç‰‡çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸éœ€è¦åœ¨è®¡ç®—ä¹‹å‰å°†å®ƒä»¬å…¨éƒ¨æ”¶é›†èµ·æ¥ã€‚è¿™å¬èµ·æ¥åƒæ˜¯ä¸€ä¸ªæ¢¦æƒ³ï¼è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹å¼ é‡å¹¶è¡Œæ˜¯å¦‚ä½•é€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•æ¥å·¥ä½œçš„ã€‚

å¼ é‡å¹¶è¡Œåˆ©ç”¨äº†çŸ©é˜µä¹˜æ³• $A\times B$ çš„æ•°å­¦ç‰¹æ€§ã€‚è¦ç†è§£å…¶å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä½¿è¿™ç§å¹¶è¡ŒåŒ–æˆä¸ºå¯èƒ½çš„ä¸¤ä¸ªåŸºæœ¬æ–¹ç¨‹å¼ï¼š$$\begin{align*}
1. \quad & A \cdot B = A \cdot \begin{bmatrix} B_1 & B_2 & \cdots \end{bmatrix} = \begin{bmatrix} AB_1 & AB_2 & \cdots \end{bmatrix} \\[1.2ex]
2. \quad & A \cdot B = \begin{bmatrix} A_1 & A_2 & \cdots \end{bmatrix} \begin{bmatrix} B_1 \\ B_2 \\ \vdots \end{bmatrix} = \sum_{i=1}^{n} A_i B_i
\end{align*}$$
è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ¥è®¡ç®—çŸ©é˜µä¹˜ç§¯ï¼š1ï¼‰åˆ†åˆ«ä¹˜ä»¥ $B$ çš„æ¯ä¸€åˆ—ï¼›æˆ–è€…2ï¼‰åˆ†åˆ«ä¹˜ä»¥æ¯ä¸€è¡Œå¹¶å°†ç»“æœç»„åˆèµ·æ¥ã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒçŸ©é˜µä¹˜æ³•é€šå¸¸ä»¥ä»¥ä¸‹æ ¼å¼è¡¨ç¤ºï¼š$X \times W$ï¼Œå…¶ä¸­ï¼š

* $X$ è¡¨ç¤ºè¾“å…¥æˆ–æ¿€æ´»å€¼  
* $W$ è¡¨ç¤º `nn.Linear` çš„æƒé‡

åœ¨å®é™…æ“ä½œä¸­ï¼Œè¯¥æ“ä½œçš„ä¸€ä¸ªå°ç¤ºä¾‹æ˜¯è¿™æ ·çš„ï¼š

![TP diagram|240](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram.svg)

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å¯¹è¿™ä¸ªæ“ä½œè¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼åœ¨å¼ é‡å¹¶è¡Œä¸­ï¼Œå¼ é‡å°†æ²¿ç€ç‰¹å®šç»´åº¦è¢«åˆ†å‰²æˆ $N$ ä¸ªåˆ†ç‰‡ï¼Œå¹¶åˆ†å¸ƒåœ¨ $N$ ä¸ª GPU ä¸Šã€‚çŸ©é˜µå¯ä»¥åœ¨åˆ—éƒ¨åˆ†æˆ–è¡Œéƒ¨åˆ†è¿›è¡Œåˆ†å‰²ï¼Œä»è€Œå®ç°è¡Œå¹¶è¡Œå’Œåˆ—å¹¶è¡Œã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œé€‰æ‹©è¡Œåˆ†ç‰‡è¿˜æ˜¯åˆ—åˆ†ç‰‡å°†éœ€è¦ä¸åŒçš„é€šä¿¡åŸè¯­ã€‚

æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨æŒ‰åˆ—åˆ†ç‰‡ï¼ˆä¹Ÿç§°ä¸º***åˆ—çº¿æ€§*** ï¼‰ï¼šæˆ‘ä»¬å°†æŠŠå®Œæ•´çš„è¾“å…¥çŸ©é˜µå¤åˆ¶åˆ°æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œè¿™éœ€è¦ä¸€ä¸ªç§°ä¸º***å¹¿æ’­*** çš„æ“ä½œï¼Œå¹¶å°†æƒé‡çŸ©é˜µåˆ†å‰²æˆåˆ—ã€‚ç„¶åå°†è¾“å…¥ä¸éƒ¨åˆ†æƒé‡çŸ©é˜µç›¸ä¹˜ï¼Œæœ€åä½¿ç”¨ ***all-gather*** æ“ä½œåˆå¹¶ç»“æœã€‚

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram2.png)

ä»¥ä¸‹æ˜¯æŒ‰åˆ—è¿›è¡Œå¼ é‡å¹¶è¡Œçš„ä»£ç å®ç°ï¼š

ğŸ‘‰ Picotron ä¸­çš„åˆ—å¹¶è¡Œå¼ é‡å¹¶è¡Œå®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class ColumnParallelLinear(torch.nn.Module):
    """Column Parallel Linear layer
    Y = XW + b, where weight matrix W is parallelized along its second dimension. W = [W_1, ..., W_p]
    This module returns the results of Y_i = XW_i + b_i in the forward method, Y_i is parallelized in the second dimension.
    Arguments:
        in_features: first dimension of weight matrix W.
        out_features: second dimension of weight matrix W.
        bias: If true, add bias
        init_method: method to initialize weights
        gather_output: If true, gather the output from all the partitions. This is used for the last linear layer
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = False,
        gather_output: bool = False,
        async_all_reduce: bool = False,
    ) -> None:
        super(ColumnParallelLinear, self).__init__()

        self.tp_world_size = pgm.process_group_manager.tp_world_size
        self.tp_rank = pgm.process_group_manager.tp_rank 

        self.in_features = in_features
        self.out_features = out_features
        assert out_features % self.tp_world_size == 0, "Hidden dimension must be divisible by the tensor parallel world size"
        self.output_size_per_partition = out_features // self.tp_world_size
        self.gather_output = gather_output
        self.async_all_reduce = async_all_reduce
        # Allocate space for the weight and bias
        # Note: torch.nn.functional.linear performs XW^T + b so we exchange the order of dimensions
        self.weight = nn.Parameter(torch.Tensor(self.output_size_per_partition, self.in_features)) # W_i
        if bias:
            self.bias = nn.Parameter(torch.Tensor(self.output_size_per_partition))
            with torch.no_grad():
                self.bias.zero_()
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weight tensor with the default initialization method used for nn.Linear in PyTorch
        master_weight = torch.empty(
            self.out_features, 
            self.in_features, 
            dtype=self.weight.dtype,
            device=self.weight.device,
            requires_grad=False
        )
        
        # Calculate bound based on master weight's input dimension
        k = 1 / master_weight.size(1)
        bound = math.sqrt(k)
        torch.nn.init.uniform_(master_weight, -bound, bound)
        
        # Split the model into size of self.output_size_per_partition
        weight_list = torch.split(master_weight, self.output_size_per_partition, dim=0)
        self.weight.data = weight_list[self.tp_rank].contiguous()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:  
        if self.async_all_reduce:
            output = linear_with_async_all_reduce(x, self.weight, self.bias) 
        else:
            output = linear_with_all_reduce(x, self.weight, self.bias) 
        if self.gather_output:
            output = GatherFromModelParallelRegion.apply(output)
        return output
```

ç¬¬äºŒä¸ªé€‰é¡¹ç§°ä¸ºæŒ‰è¡Œåˆ†ç‰‡ï¼ˆä¹Ÿç§°ä¸º***è¡Œçº¿æ€§***åˆ†ç‰‡ï¼‰ï¼šç»†å¿ƒçš„è¯»è€…å¯èƒ½ä¼šçŒœåˆ°ï¼Œè¡Œçº¿æ€§åˆ†ç‰‡æ„å‘³ç€æˆ‘ä»¬å°†æƒé‡çŸ©é˜µåˆ†å‰²æˆè¡Œå—ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿè¦æ±‚æˆ‘ä»¬å¯¹è¾“å…¥è¿›è¡Œåˆ†å‰²ï¼Œè¿™éœ€è¦ä¸€ä¸ª ***scatter*** æ“ä½œï¼Œè€Œä¸æ˜¯åƒåˆ—çº¿æ€§åˆ†ç‰‡ä¸­ä½¿ç”¨çš„å¹¿æ’­ã€‚æ¯ä¸ªå·¥ä½œå™¨ä¸Šçš„ç»“æœå·²ç»æ˜¯æ­£ç¡®çš„å½¢çŠ¶ï¼Œä½†éœ€è¦æ±‚å’Œä»¥å¾—åˆ°æœ€ç»ˆç»“æœï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦ä¸€ä¸ª all-reduce æ“ä½œã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°äº†æˆ‘ä»¬çš„ç¬¬å››ä¸ªåˆ†å¸ƒå¼åŸè¯­ï¼š**_scatter_**ï¼

![image.png|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram3.png)

ä»¥ä¸‹æ˜¯æŒ‰è¡Œè¿›è¡Œå¼ é‡å¹¶è¡Œçš„å®ç°æ–¹å¼ï¼š

ğŸ‘‰ Picotron ä¸­çš„è¡Œå¹¶è¡Œå¼ é‡å¹¶è¡Œå®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
class RowParallelLinear(nn.Module):
    """Linear layer with row parallelism.
    Y = XW + b. W is parallelized along its first dimension and X along its second dimension as:
               -   -
              | W_1 |
              | .   |
          W = | .   |        X = [X_1, ..., X_p]
              | .   |
              | W_p |
               -   -
    We assume that X is already parallelized. This is the case after ColumnParallelLinear.
    This module returns the results of Y = sum(X_i * W_i + b_i) in the forward method.
    Arguments:
        in_features: first dimension of matrix W.
        out_features: second dimension of matrix W.
        bias: If true, add bias
        init_method: method to initialize weights.
    """
    def __init__(self, in_features: int, out_features: int, bias: bool):
        super(RowParallelLinear, self).__init__()

        self.tp_world_size = pgm.process_group_manager.tp_world_size
        self.tp_rank = pgm.process_group_manager.tp_rank 

        self.in_features = in_features
        self.out_features = out_features
        assert in_features % self.tp_world_size == 0, "Hidden dimension must be divisible by the tensor parallel world size"
        self.input_size_per_partition = in_features // self.tp_world_size

        self.weight = nn.Parameter(torch.Tensor(self.out_features, self.input_size_per_partition))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(self.out_features))
            # Always initialize bias to zero.
            with torch.no_grad():
                self.bias.zero_()
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weight tensor with same dtype and device as self.weight
        master_weight = torch.empty(
            self.out_features, 
            self.in_features, 
            dtype=self.weight.dtype,
            device=self.weight.device,
            requires_grad=False
        )
        
        # Calculate bound based on master weight's input dimension
        k = 1 / master_weight.size(1)
        bound = math.sqrt(k)    
        torch.nn.init.uniform_(master_weight, -bound, bound)
        
        # Split the model into size of self.input_size_per_partition
        weight_list = torch.split(master_weight, self.input_size_per_partition, dim=1)
        self.weight.data = weight_list[self.tp_rank].contiguous()

    def forward(self, x):
        # X_i * W_i^T + b
        output_parallel = F.linear(x, self.weight)
        # All-reduce across all the partitions.
        output = ReduceFromModelParallelRegion.apply(output_parallel)
        return output if self.bias is None else output + self.bias
```

æ—¢ç„¶æˆ‘ä»¬å·²ç»äº†è§£äº† Transformer çš„åŸºæœ¬æ„å»ºæ¨¡å—ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ Transformer å±‚ä¸­æœ‰æ•ˆåœ°ç»„åˆå®ƒä»¬ï¼

### 4.1 Transformer å—ä¸­çš„å¼ é‡å¹¶è¡Œæ€§

ä¸ºäº†æå‡ºä¸€ä¸ªå¯éµå¾ªçš„ç­–ç•¥ï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹è¿‡æ¸¡åˆ°ä¸€ä¸ªçœŸå®çš„æ¨¡å‹æ„å»ºæ¨¡å—ã€‚Transformeræ¨¡å‹ç”±ä¸¤ä¸ªä¸»è¦çš„æ„å»ºæ¨¡å—ç»„æˆï¼šå‰é¦ˆå±‚ï¼ˆMLPï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸¤è€…éƒ½åº”ç”¨å¼ é‡å¹¶è¡Œæ€§ã€‚

å‰é¦ˆéƒ¨åˆ†å¯ä»¥é€šè¿‡å…ˆè¿›è¡Œâ€œåˆ—çº¿æ€§â€æ“ä½œï¼Œå†è¿›è¡Œâ€œè¡Œçº¿æ€§â€æ“ä½œæ¥å®ç°å¹¶è¡ŒåŒ–ï¼Œè¿™ç›¸å½“äºåœ¨å‰å‘ä¼ æ’­ä¸­è¿›è¡Œå¹¿æ’­ä»¥å¤åˆ¶è¾“å…¥å¹¶è¿›è¡Œ all-reduce æ“ä½œã€‚è¯·æ³¨æ„ï¼Œåœ¨å®é™…è®­ç»ƒä¸­ä¸éœ€è¦å¹¿æ’­ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç¡®ä¿è¾“å…¥å·²ç»åœ¨ TP rank ä¹‹é—´åŒæ­¥ã€‚è¿™ç§è®¾ç½®æ¯”å…ˆè¿›è¡Œâ€œè¡Œçº¿æ€§â€æ“ä½œï¼Œå†è¿›è¡Œâ€œåˆ—çº¿æ€§â€æ“ä½œæ›´é«˜æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è·³è¿‡ä¸¤ä¸ªæ‹†åˆ†æ“ä½œä¹‹é—´çš„ä¸­é—´ all-reduce æ“ä½œã€‚

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram4.png)

æ—¢ç„¶æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº† Transformer å‰é¦ˆéƒ¨åˆ†çš„ä¸€ä¸ªé«˜æ•ˆæ¨¡å¼ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ¥çœ‹çœ‹å¤šå¤´æ³¨æ„åŠ›å—ï¼ˆMHAï¼‰ã€‚

æˆ‘ä»¬é€šå¸¸å¯ä»¥é‡‡ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œå…¶ä¸­ Qã€K å’Œ V çŸ©é˜µä»¥åˆ—å¹¶è¡Œçš„æ–¹å¼æ‹†åˆ†ï¼Œè¾“å‡ºæŠ•å½±æ²¿è¡Œç»´åº¦æ‹†åˆ†ã€‚å¯¹äºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ—å¹¶è¡Œæ–¹æ³•æœ‰ä¸€ä¸ªéå¸¸è‡ªç„¶çš„è§£é‡Šï¼šæ¯ä¸ªå·¥ä½œå™¨è®¡ç®—å•ä¸ªå¤´æˆ–ä¸€ç»„å¤´çš„æ³¨æ„åŠ›ã€‚è¿™ç§æ–¹æ³•åŒæ ·é€‚ç”¨äºå¤šæŸ¥è¯¢ï¼ˆMQAï¼‰æˆ–åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œåœ¨è¿™äº›æœºåˆ¶ä¸­ï¼Œé”®å’Œå€¼åœ¨æŸ¥è¯¢ä¹‹é—´æ˜¯å…±äº«çš„ã€‚

ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¼ é‡å¹¶è¡Œåº¦ä¸åº”è¶…è¿‡æŸ¥è¯¢/é”®/å€¼ï¼ˆQ/K/Vï¼‰å¤´çš„æ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ç­‰çº§éƒ½éœ€è¦å®Œæ•´çš„å¤´ï¼ˆå¦åˆ™æˆ‘ä»¬å°±æ— æ³•åœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶ä¸”å°†éœ€è¦é¢å¤–çš„é€šä¿¡æ“ä½œï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œå¼ é‡å¹¶è¡Œåº¦å®é™…ä¸Šåº”å°äºé”®/å€¼ï¼ˆK/Vï¼‰å¤´çš„æ•°é‡ã€‚ä¾‹å¦‚ï¼ŒLLaMA-3 8B æœ‰ 8 ä¸ªé”®/å€¼å¤´ï¼Œæ‰€ä»¥å¼ é‡å¹¶è¡Œåº¦æœ€å¥½ä¸è¶…è¿‡ 8ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¿™ä¸ªæ¨¡å‹ä½¿ç”¨ TP=16ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ª GPU ä¸Šå¤åˆ¶é”®/å€¼å¤´ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¿æŒåŒæ­¥ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_full_diagram.png)

æœ€åè¯·æ³¨æ„ï¼Œå¼ é‡å¹¶è¡Œæ€§ä»ç„¶ä¸æ˜¯è®­ç»ƒçš„ä¸‡èƒ½è‰¯æ–¹ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹çš„è®¡ç®—è·¯å¾„ä¸­ç›´æ¥æ·»åŠ äº†å‡ ç§åˆ†å¸ƒå¼é€šä¿¡åŸè¯­ï¼Œå› æ­¤å¾ˆéš¾åƒæˆ‘ä»¬åœ¨ ZeRO ä¸­æ‰€åšçš„é‚£æ ·å°†å…¶ä¸è®¡ç®—å®Œå…¨éšè—/é‡å ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ€§èƒ½å°†æ˜¯è®¡ç®—å’Œå†…å­˜å¢ç›Šä¸å¢åŠ çš„é€šä¿¡å¼€é”€ä¹‹é—´æƒè¡¡çš„ç»“æœã€‚è®©æˆ‘ä»¬æ¥è¯´æ˜è¿™ä¸€ç‚¹ï¼š

![Forward pass in Tensor Parallelism|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_overlap.svg)

ï¼ˆé€šè¿‡å¯¹åˆ†å—çŸ©é˜µä¹˜æ³•å¹¶ç»“åˆå¼‚æ­¥é€šä¿¡/è®¡ç®—ï¼Œå¯ä»¥éƒ¨åˆ†éšè—è¿™ç§é€šä¿¡ã€‚ï¼‰

è§‚å¯Ÿå¼ é‡å¹¶è¡Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼ˆæ³¨æ„åŠ›æœºåˆ¶åŒæ ·é€‚ç”¨ï¼‰çš„æ“ä½œæ—¶é—´çº¿ï¼Œæˆ‘ä»¬èƒ½æ›´å¥½åœ°ç†è§£å…¶ä¸­æ¶‰åŠçš„æƒè¡¡ã€‚åœ¨æ¯ä¸ªè§£ç å™¨å±‚çš„å‰å‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°ä¸€ä¸ªåŒæ­¥ç‚¹ï¼Œå³AllReduceæ“ä½œï¼Œè¯¥æ“ä½œæ— æ³•ä¸è®¡ç®—é‡å ã€‚è¿™ç§ *æ˜¾éœ²å‡ºæ¥çš„é€šä¿¡å¼€é”€* å¯¹äºåœ¨æœ€ç»ˆåº”ç”¨å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ä¹‹å‰åˆå¹¶å¼ é‡å¹¶è¡Œ ranks ä¹‹é—´çš„éƒ¨åˆ†ç»“æœæ˜¯å¿…è¦çš„ã€‚

ï¼ˆä¾‹å¦‚ï¼ŒMegatron-LM/Nanotron å®ç°äº† all-gather ä¸ FC1 è®¡ç®—çš„éƒ¨åˆ†é‡å ï¼Œå…¶ä¸­çŸ©é˜µä¹˜æ³•ç»“æœçš„ä¸€éƒ¨åˆ†å°†åœ¨å¦ä¸€éƒ¨åˆ†ä»åœ¨è®¡ç®—æ—¶å¼€å§‹å‘é€åˆ°å…¶ä»– GPUã€‚ï¼‰

å¼ é‡å¹¶è¡Œç¡®å®æœ‰åŠ©äºå‡å°‘çŸ©é˜µä¹˜æ³•çš„æ¿€æ´»å†…å­˜ï¼Œå› ä¸ºä¸­é—´æ¿€æ´»è¢«åˆ†ç‰‡åˆ°å¤šä¸ª GPU ä¸Šã€‚ç„¶è€Œï¼Œå¯¹äºåƒ LayerNorm è¿™æ ·çš„æ“ä½œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦æ”¶é›†å®Œæ•´çš„æ¿€æ´»ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ²¡æœ‰è·å¾—æˆ‘ä»¬æœ¬å¯ä»¥è·å¾—çš„å…¨éƒ¨å†…å­˜ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå¼ é‡å¹¶è¡Œå¼•å…¥äº†æ˜¾è‘—çš„é€šä¿¡éœ€æ±‚ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç½‘ç»œåŸºç¡€è®¾æ–½ã€‚æ— æ³•å°†è¿™ç§ç‰¹å®šçš„ AllReduce å®Œå…¨éšè—åœ¨è®¡ç®—èƒŒåæ„å‘³ç€å®ƒä¼šç›´æ¥å¢åŠ å‰å‘ä¼ æ’­çš„å…³é”®è·¯å¾„ã€‚

ï¼ˆè¿™ä¸€ç ”ç©¶é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œè¿‘æœŸçš„ç ”ç©¶å·¥ä½œå¦‚ Domino[4] æ¢ç´¢äº†æœ€å¤§åŒ–è¿™ç§é‡å çš„æ–°æŠ€æœ¯ã€‚ï¼‰

è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹åœ¨æ‰©å±• TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰åº¦æ—¶æ‰€æ¶‰åŠçš„æƒè¡¡ï¼š

[äº¤äº’å›¾]

å¢åŠ è®­ç»ƒè¿›ç¨‹æ•°ï¼ˆTPï¼‰ä¼šå¯¼è‡´æ¯ä¸ª GPU çš„ååé‡é™ä½ï¼ˆå·¦å›¾ï¼‰ï¼Œä½†å®ƒèƒ½å¤Ÿå¤„ç†æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼ˆå³å›¾ï¼‰ï¼Œè¿™è¯´æ˜äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­è®¡ç®—æ•ˆç‡å’Œå†…å­˜å¯ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

å®é™…ä¸Šï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å·¦å›¾ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œå½“æ‰©å±•åˆ° 8 ä¸ª GPU ä»¥ä¸Šæ—¶ï¼Œå¼ é‡å¹¶è¡Œçš„é€šä¿¡å¼€é”€å˜å¾—å°¤ä¸ºæ˜æ˜¾ã€‚è™½ç„¶åœ¨å•ä¸ªèŠ‚ç‚¹å†…å¯ä»¥åˆ©ç”¨å¿«é€Ÿçš„ NVLink äº’è¿å®ç°å¼ é‡å¹¶è¡Œï¼Œä½†è·¨èŠ‚ç‚¹åˆ™éœ€è¦è¾ƒæ…¢çš„ç½‘ç»œè¿æ¥ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä» TP=8 å¢åŠ åˆ° TP=16 æ—¶å‡ºç°äº†æ˜¾è‘—çš„ä¸‹é™ï¼Œå¹¶ä¸”ä» TP=16 å¢åŠ åˆ° TP=32 æ—¶ä¸‹é™æ›´ä¸ºé™¡å³­ã€‚åœ¨æ›´é«˜çš„å¹¶è¡Œåº¦ä¸‹ï¼Œé€šä¿¡å¼€é”€å˜å¾—å¦‚æ­¤ä¹‹é«˜ï¼Œä»¥è‡³äºè¿…é€Ÿå æ®äº†è®¡ç®—æ—¶é—´ã€‚

è¯è™½å¦‚æ­¤ï¼Œå¼ é‡å¹¶è¡Œé€šè¿‡å°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠï¼ˆåœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼‰æ¿€æ´»åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šï¼Œä¸ºå†…å­˜ä½¿ç”¨æä¾›äº†é‡è¦ä¼˜åŠ¿ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯¹ä¸€ä¸ª 70B å‚æ•°æ¨¡å‹äº§ç”Ÿçš„å½±å“ï¼š

[äº¤äº’å›¾]

æé«˜å¼ é‡å¹¶è¡Œåº¦å¯å‡å°‘æ¯ä¸ª GPU ä¸Šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€æ‰€éœ€çš„å†…å­˜ï¼Œä»è€Œè®©æˆ‘ä»¬èƒ½å¤Ÿå¼€å§‹åœ¨å•ä¸ª 8 GPU èŠ‚ç‚¹ä¸Šæ‹Ÿåˆå¤§å‹æ¨¡å‹ã€‚

æœ‰æ²¡æœ‰åŠæ³•ä»è¿™ç§æŠ€æœ¯ä¸­è·å¾—æ›´å¤šçš„å¥½å¤„å‘¢ï¼Ÿæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œå±‚å½’ä¸€åŒ–å’Œ dropout ä»ç„¶éœ€è¦åœ¨æ¯ä¸ª GPU ä¸Šæ”¶é›†å…¨éƒ¨æ¿€æ´»å€¼ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸ŠæŠµæ¶ˆäº†å†…å­˜èŠ‚çœçš„æ•ˆæœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯»æ‰¾å¹¶è¡ŒåŒ–è¿™äº›å‰©ä½™æ“ä½œçš„æ–¹æ³•æ¥åšå¾—æ›´å¥½ã€‚

> [!NOTE]
> å…³äºå¼ é‡å¹¶è¡Œè®­ç»ƒä¸­å±‚å½’ä¸€åŒ–çš„ä¸€ä¸ªæœ‰è¶£è¯´æ˜â€”â€”ç”±äºæ¯ä¸ªå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ranks åœ¨ all-gather åçœ‹åˆ°ç›¸åŒçš„æ¿€æ´»å€¼ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­åï¼Œå±‚å½’ä¸€åŒ–æƒé‡å®é™…ä¸Šä¸éœ€è¦ all-reduce æ¥åŒæ­¥å®ƒä»¬çš„æ¢¯åº¦ã€‚å®ƒä»¬è‡ªç„¶ä¼šåœ¨å„ç­‰çº§ä¹‹é—´ä¿æŒåŒæ­¥ã€‚ç„¶è€Œï¼Œå¯¹äº dropout æ“ä½œï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿è·¨ TP ranks åŒæ­¥éšæœºç§å­ï¼Œä»¥ç»´æŒç¡®å®šæ€§è¡Œä¸º ã€‚

æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¢è®¨ä¸€ä¸‹å¼ é‡å¹¶è¡Œçš„ä¸€ç§å°å‹ä¸”è‡ªç„¶çš„æ‰©å±•æ–¹å¼ï¼Œå³*åºåˆ—å¹¶è¡Œ*ï¼Œå®ƒæ‰€åšçš„äº‹æƒ…æ­£æ˜¯å¦‚æ­¤ã€‚

### 4.2 åºåˆ—å¹¶è¡Œ

åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰æ¶‰åŠå¯¹å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æœªå¤„ç†çš„æ¨¡å‹éƒ¨åˆ†ï¼ˆå¦‚ Dropout å’Œ LayerNormï¼‰çš„æ¿€æ´»å’Œè®¡ç®—è¿›è¡Œæ‹†åˆ†ï¼Œä½†æ²¿è¾“å…¥åºåˆ—ç»´åº¦è€Œééšè—ç»´åº¦è¿›è¡Œæ‹†åˆ†ã€‚

> [!NOTE]
> â€œåºåˆ—å¹¶è¡Œâ€ï¼ˆSequence Parallelismï¼‰è¿™ä¸ªæœ¯è¯­æœ‰äº›å«ä¹‰è¿‡è½½ï¼šæœ¬èŠ‚ä¸­çš„åºåˆ—å¹¶è¡Œä¸å¼ é‡å¹¶è¡Œç´§å¯†ç›¸å…³ï¼Œå¹¶ä¸”é€‚ç”¨äºdropoutï¼ˆéšæœºå¤±æ´»ï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆlayer normï¼‰æ“ä½œã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬å°†å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œæ³¨æ„åŠ›è®¡ç®—å°†æˆä¸ºç“¶é¢ˆï¼Œè¿™å°±éœ€è¦è¯¸å¦‚ç¯å½¢æ³¨æ„åŠ›ï¼ˆRing-Attentionï¼‰ä¹‹ç±»çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸ºâ€œåºåˆ—å¹¶è¡Œâ€ï¼Œä½†ä¸ºäº†åŒºåˆ†è¿™ä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒä»¬ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å¹¶è¡Œâ€ï¼ˆContext Parallelismï¼‰ã€‚æ‰€ä»¥ï¼Œæ¯æ¬¡çœ‹åˆ°â€œåºåˆ—å¹¶è¡Œâ€æ—¶ï¼Œè¦è®°ä½å®ƒæ˜¯ä¸å¼ é‡å¹¶è¡Œä¸€èµ·ä½¿ç”¨çš„ï¼ˆä¸ä¸Šä¸‹æ–‡å¹¶è¡Œç›¸å¯¹ï¼Œä¸Šä¸‹æ–‡å¹¶è¡Œå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼‰ã€‚

è¿™æ˜¯å› ä¸ºè¿™äº›æ“ä½œéœ€è¦è®¿é—®å®Œæ•´çš„éšè—ç»´åº¦æ‰èƒ½æ­£ç¡®è®¡ç®—ã€‚ä¾‹å¦‚ï¼ŒLayerNorm éœ€è¦å®Œæ•´çš„éšè—ç»´åº¦æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼š$$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
å…¶ä¸­ï¼Œ$\mu = \text{mean}(x)$ å’Œ $\sigma^2 = \text{var}(x)$ æ˜¯åœ¨éšè—ç»´åº¦ $h$ ä¸Šè®¡ç®—å¾—åˆ°çš„ã€‚

å› æ­¤ï¼Œå³ä½¿è¿™äº›æ“ä½œåœ¨è®¡ç®—ä¸Šæˆæœ¬è¾ƒä½ï¼Œä½†ç”±äºéœ€è¦å®Œæ•´çš„éšè—ç»´åº¦ï¼Œå®ƒä»¬ä»ç„¶éœ€è¦å¤§é‡çš„æ¿€æ´»å†…å­˜ã€‚åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰å…è®¸æˆ‘ä»¬é€šè¿‡æ²¿åºåˆ—ç»´åº¦è¿›è¡Œæ‹†åˆ†ï¼Œåœ¨å¤šä¸ª GPU ä¹‹é—´åˆ†æ‹…è¿™ä¸€å†…å­˜è´Ÿæ‹…ã€‚

åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å°†ä»å·¦å›¾è¿‡æ¸¡åˆ°å³å›¾ï¼š
![|400](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_diagram.png)
è¯¥å›¾è¡¨å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¸åŒçš„é›†åˆæ“ä½œï¼ˆåˆ†åˆ«æ ‡è®°ä¸º â€œfâ€ å’Œ â€œgâ€ï¼‰åœ¨å¼ é‡å¹¶è¡ŒåŒºåŸŸå’Œåºåˆ—å¹¶è¡ŒåŒºåŸŸä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºé«˜æ•ˆåœ°ç®¡ç†è¿™äº›è½¬æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å†…å­˜ä½¿ç”¨é‡å¹¶ç¡®ä¿æ­£ç¡®æ€§ã€‚

åœ¨å‰å‘ä¼ æ’­ä¸­ï¼š

* "f" æ˜¯ä¸€ä¸ªç©ºæ“ä½œï¼ˆæ— æ“ä½œï¼‰ï¼Œå› ä¸ºæ¿€æ´»å·²åœ¨å„ ranks ä¹‹é—´è¿›è¡Œäº†å¤åˆ¶  
* "f*" æ˜¯ä¸€ä¸ª all-reduce æ“ä½œï¼Œç”¨äºåŒæ­¥æ¿€æ´»å¹¶ç¡®ä¿æ­£ç¡®æ€§

åœ¨åå‘ä¼ æ’­ä¸­ï¼š

* "f*" æ˜¯æ— æ“ä½œï¼ˆno-opï¼‰ï¼Œå› ä¸ºæ¢¯åº¦å·²åœ¨å„ ranks ä¸­è¢«å¤åˆ¶  
* "f"æ˜¯ all-reduce æ“ä½œï¼Œç”¨äºåŒæ­¥æ¢¯åº¦

è¿™äº›è¿ç®— â€œfâ€ å’Œ â€œf*â€ è¢«ç§°ä¸º*å…±è½­* å¯¹ï¼Œå› ä¸ºå®ƒä»¬ç›¸äº’è¡¥å……â€”â€”å½“ä¸€ä¸ªåœ¨æ­£å‘ä¸­æ˜¯æ— æ“ä½œï¼ˆno-opï¼‰æ—¶ï¼Œå¦ä¸€ä¸ªåœ¨åå‘ä¸­å°±æ˜¯ all-reduceï¼Œåä¹‹äº¦ç„¶ ã€‚

å¯¹äºåºåˆ—å¹¶è¡Œï¼ˆSPï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡è®°ä¸º â€œgâ€ å’Œ â€œg*â€ çš„ä¸åŒæ“ä½œã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ SP åŒºåŸŸä¸­é¿å…ä½¿ç”¨ all-reduce æ“ä½œï¼Œå› ä¸ºè¿™éœ€è¦æ”¶é›†å…¨éƒ¨æ¿€æ´»å€¼ï¼Œä»è€Œå¢åŠ æˆ‘ä»¬çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡ï¼Œè¿èƒŒäº† SP çš„åˆè¡·ã€‚

é‚£ä¹ˆè¿™é‡Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿæ­£å¦‚ä¸€ä¸ªè‘—åçš„ LLM æ‰€è¯´ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥ï¼š

* **åˆå§‹å±‚å½’ä¸€åŒ–ï¼ˆSP åŒºåŸŸï¼‰**:
	* è¾“å…¥å¼ é‡ $X_1$ å’Œ $X_2$ $(b,s/2,h)$ è¿›å…¥å±‚å½’ä¸€åŒ–ï¼Œå·²åœ¨åºåˆ—ç»´åº¦ä¸Šæ‹†åˆ†
	- æ¯ä¸ª GPU ç‹¬ç«‹åœ°åœ¨å…¶åºåˆ—å—ä¸Šè®¡ç®—å±‚å½’ä¸€åŒ–ï¼Œå¹¶ç»™å‡º $Y_1$ å’Œ $Y_2$

* **ç¬¬ä¸€æ¬¡è½¬æ¢ï¼ˆSP â†’ TPï¼‰:**
	- â€œgâ€ æ“ä½œï¼ˆall-gatherï¼‰å°† $Y_1$ å’Œ $Y_2$ é‡æ–°ç»„åˆä¸ºå®Œæ•´åºåˆ—é•¿åº¦
	- æ¢å¤ $Y$ $(b,s,h)$ï¼Œå› ä¸ºåˆ—çº¿æ€§éœ€è¦å®Œæ•´çš„éšè—ç»´åº¦ $h$

* **ç¬¬ä¸€æ¬¡çº¿æ€§ï¼ˆTP åŒºåŸŸï¼‰:**
	* $A_1$ æ˜¯åˆ—çº¿æ€§çš„ï¼Œå› æ­¤å®ƒæ²¿éšè—ç»´åº¦åˆ†å‰² $Y$  
	* GeLU åœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹åº”ç”¨
	* $Z_1$ æ˜¯ $(b,s,h/2)$

* **ç¬¬äºŒæ¬¡çº¿æ€§ï¼ˆTP åŒºåŸŸï¼‰ï¼š**  
	* $B_1$ æ˜¯è¡Œçº¿æ€§çš„ï¼Œå› æ­¤å®ƒæ¢å¤éšè—ç»´åº¦  
	* $W_1$ æ˜¯ $(b,s,h)$ 

* **æœ€ç»ˆè½¬æ¢ï¼ˆTP â†’ SPï¼‰ï¼š** 
	* â€œg*â€ æ“ä½œï¼ˆreduce-scatterï¼‰ï¼Œåœ¨æ²¿åºåˆ—ç»´åº¦åˆ†æ•£çš„åŒæ—¶ç¡®ä¿å‰ä¸€è¡Œçº¿æ€§çš„æ­£ç¡®æ€§  
	* $W_1$ æ˜¯ $(b,s/2,h)$

![image.png|240](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_diagram_zoomed.png)

åºåˆ—å¹¶è¡Œæ€§çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿åœ¨äºå®ƒå‡å°äº†æˆ‘ä»¬éœ€è¦å­˜å‚¨çš„æœ€å¤§æ¿€æ´»å¤§å°ã€‚åœ¨ä»…ä½¿ç”¨å¼ é‡å¹¶è¡Œçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å¾—ä¸åœ¨ä¸åŒä½ç½®å­˜å‚¨å½¢çŠ¶ä¸º $(b,s,h)$ çš„æ¿€æ´»å€¼ã€‚ç„¶è€Œï¼Œæœ‰äº†åºåˆ—å¹¶è¡Œæ€§ï¼Œç”±äºæˆ‘ä»¬æ€»æ˜¯åœ¨åºåˆ—ç»´åº¦æˆ–éšè—ç»´åº¦ä¸Šè¿›è¡Œæ‹†åˆ†ï¼Œæœ€å¤§æ¿€æ´»å¤§å°å‡å°ä¸º $\frac{bâ‹…sâ‹…h}{tp}$ã€‚

è·Ÿè¸ªåœ¨ TP å’Œ TP/SP ä¸­ä»¥ä¸åŒæ–¹å¼è¿›è¡Œåˆ†ç‰‡çš„æ‰€æœ‰éƒ¨åˆ†æœ‰ç‚¹å›°éš¾â€”â€”ç›¸ä¿¡æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¹Ÿè§‰å¾—å¾ˆéš¾è¿›è¡Œæ˜ å°„ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ¶ä½œäº†è¿™ä¸ªå°è¡¨æ ¼æ¥æ€»ç»“åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¿€æ´»ï¼ˆå³ `hidden_states`ï¼‰çš„å½¢çŠ¶åœ¨éšè—ç»´åº¦ $h$ å’Œåºåˆ—ç»´åº¦ $s$ ä¸Šæ˜¯å¦‚ä½•å˜åŒ–çš„ï¼š

|Region|TP only|TP with SP|
|---|---|---|
|Enter TP (Column Linear)|h: sharded (weight_out is sharded)  <br>s: full|h: sharded (weight_out is sharded)  <br>s:Â **all-gather**Â to full|
|TP Region|h: sharded  <br>s: full|h: sharded  <br>s: full|
|Exit TP (Row Linear)|h: full (weight_out is full +Â **all-reduce**Â for correctness)  <br>s: full|h: full (weight_out is full +Â **reduce-scatter**Â for correctness)  <br>s:Â **reduce-scatter**Â to sharded|
|SP Region|h: full  <br>s: full|h: full  <br>s: sharded|

å¹¶ä¸”å¯¹äºåµŒå…¥å±‚ï¼š

|Region|Vanilla TP|TP with SP|
|---|---|---|
|Embedding Layer (Row Linear sharded on vocab)|h: full (weight_out is full +Â **all-reduce**Â for correctness)  <br>s: full|h: full (weight_out is full +Â **reduce-scatter**Â for correctness)  <br>s:Â **reduce-scatter**Â to sharded|

é€šè¿‡ä½¿ç”¨åºåˆ—å¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°æ›´å¤§çš„æ¿€æ´»å†…å­˜èŠ‚çœï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦æ¨å¾—æ¯”ä»…ä½¿ç”¨å¼ é‡å¹¶è¡Œæ€§æ—¶æ›´è¿œã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯¹æˆ‘ä»¬ä¹‹å‰çš„ 70B æ¨¡å‹ç¤ºä¾‹æ„å‘³ç€ä»€ä¹ˆï¼š

[äº¤äº’å›¾]

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬å†æ¬¡å¤§å¹…é™ä½äº†æ¯ä¸ª GPU çš„æœ€å¤§å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ TP/SP=16 çš„æƒ…å†µä¸‹å¤„ç† 16k tokens çš„åºåˆ—é•¿åº¦ï¼Œè¿™æ¯”æ™®é€š TP æƒ…å†µæœ‰æ‰€æ”¹è¿›ï¼ï¼ˆå¦‚å‰ä¸€èŠ‚æ‰€è¿°ï¼ŒTP=16 ä»ç„¶æœ‰ç‚¹å¤§ï¼Œä½†æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°å¦‚ä½•æ”¹è¿›è¿™ä¸€ç‚¹ï¼‰ã€‚

ä½ å¯èƒ½ä¼šé—®è‡ªå·±çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä½¿ç”¨ TP+SP æ˜¯å¦æ¯”æ™®é€š TP äº§ç”Ÿæ›´å¤šçš„é€šä¿¡é‡ï¼Ÿå—¯ï¼Œç­”æ¡ˆæ˜¯æœ‰æ—¶æ˜¯ï¼Œæœ‰æ—¶ä¸æ˜¯ã€‚åœ¨æ™®é€š TP çš„å‰å‘ä¼ æ’­ä¸­ï¼Œæ¯ä¸ª Transformer å—æœ‰ä¸¤ä¸ª all-reduce æ“ä½œï¼Œè€Œåœ¨ SP ä¸­ï¼Œæ¯ä¸ª Transformer å—æœ‰ä¸¤ä¸ª all-gather å’Œä¸¤ä¸ª reduce-scatter æ“ä½œã€‚æ‰€ä»¥ SP çš„é€šä¿¡æ“ä½œæ•°é‡æ˜¯ TP çš„ä¸¤å€ã€‚ä½†ç”±äºä¸€ä¸ª all-reduce æ“ä½œå¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ª all-gather + reduce-scatterï¼ˆè§é™„å½•ä¸­çš„â€œå¿«é€Ÿå…³æ³¨ Ring AllReduceâ€éƒ¨åˆ†ï¼‰ï¼Œå®ƒä»¬åœ¨é€šä¿¡æ–¹é¢å®é™…ä¸Šæ˜¯ç­‰æ•ˆçš„ã€‚åå‘ä¼ æ’­çš„æ¨ç†ä¹Ÿç›¸åŒï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯ä½¿ç”¨æ¯ä¸ªæ“ä½œçš„å…±è½­ï¼ˆæ— æ“ä½œ â†” allreduce å’Œ allgather â†” reducescatterï¼‰ã€‚

å¦‚æœä½ ä¸€ç›´å¯†åˆ‡å…³æ³¨ï¼Œå°±ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨è®¨è®ºæ¯å±‚çš„ 4 ä¸ªé€šä¿¡æ“ä½œï¼ˆæ³¨æ„åŠ›æœºåˆ¶çš„ 2 ä¸ªå’Œå¤šå±‚æ„ŸçŸ¥æœºçš„ 2 ä¸ªï¼‰ã€‚ä½¿ç”¨å¼ é‡+åºåˆ—å¹¶è¡Œæ—¶ï¼Œå¤šå±‚æ„ŸçŸ¥æœºçš„åˆ†ææƒ…å†µå¦‚ä¸‹ï¼š

![tp_sp_overlap.svg|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_sp_overlap.svg)

å°±åƒæ™®é€šçš„å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸€æ ·ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰+æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰ä¹Ÿä¸èƒ½è½»æ˜“åœ°ä¸è®¡ç®—é‡å ï¼Œè¿™ä½¿å¾—ååé‡ä¸¥é‡ä¾èµ–äºé€šä¿¡å¸¦å®½ã€‚åŒæ ·ï¼Œåœ¨è¿™é‡Œï¼Œå°±åƒæ™®é€šçš„å¼ é‡å¹¶è¡Œï¼ˆTOï¼‰ä¸€æ ·ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰+æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰é€šå¸¸ä¹Ÿåªåœ¨å•ä¸ªèŠ‚ç‚¹å†…è¿›è¡Œï¼ˆä½¿å¼ é‡å¹¶è¡Œåº¦ä¿æŒåœ¨æ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ä¹‹ä¸‹ï¼Œä¾‹å¦‚å¼ é‡å¹¶è¡Œåº¦ â‰¤ 8 ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥è¡¡é‡éšç€å¼ é‡å¹¶è¡Œæ€§æ‰©å±•ï¼Œè¿™ç§é€šä¿¡å¼€é”€å˜å¾—æ—¥ç›Šæ£˜æ‰‹çš„æƒ…å†µã€‚è®©æˆ‘ä»¬åœ¨é’ˆå¯¹åºåˆ—é•¿åº¦ä¸º 4096 çš„ 3B å‚æ•°æ¨¡å‹ï¼Œå°†å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸æµæ°´çº¿å¹¶è¡Œï¼ˆSPï¼‰ä¸€èµ·æ‰©å±•æ—¶ï¼Œæµ‹é‡ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡ï¼š

[äº¤äº’å›¾]

åœ¨è¿™é‡Œï¼Œè®¡ç®—æ•ˆç‡ï¼ˆå·¦ï¼‰å’Œå†…å­˜å®¹é‡ï¼ˆå³ï¼‰ä¹‹é—´åˆå­˜åœ¨ä¸€ç§æƒè¡¡ã€‚è™½ç„¶æ›´é«˜çš„å¹¶è¡Œåº¦é€šè¿‡å‡å°‘æ¿€æ´»å†…å­˜èƒ½å¤Ÿå¤„ç†æ˜¾è‘—æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œä½†å®ƒä»¬ä¹Ÿé™ä½äº†æ¯ä¸ª GPU çš„ååé‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹åº”äºæ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡çš„é˜ˆå€¼ä»¥ä¸Šã€‚

è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„è§‚å¯Ÿç»“æœï¼š

- å¯¹äºè¿™ä¸¤ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å½“ä» TP=8 åˆ‡æ¢åˆ° TP=16 æ—¶ï¼Œæ€§èƒ½ä¸‹é™æœ€ä¸ºæ˜æ˜¾ï¼Œå› ä¸ºæ­¤æ—¶æ˜¯ä»ä»…åœ¨å•ä¸ªèŠ‚ç‚¹å†…é€šä¿¡ï¼ˆNVLinkï¼‰ï¼Œè½¬å˜ä¸ºèŠ‚ç‚¹é—´é€šä¿¡ï¼ˆEFAï¼‰ã€‚
- åœ¨ä½¿ç”¨ TP ä¸ SP æ—¶ï¼Œæ¿€æ´»å†…å­˜çš„èŠ‚çœä½¿æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†æ¯”ä»…ä½¿ç”¨ TP æ—¶å¤§å¾—å¤šçš„æ‰¹æ¬¡ã€‚

*æˆ‘ä»¬å·²ç»çœ‹åˆ° TP å¦‚ä½•é€šè¿‡åœ¨éšè—ç»´åº¦ä¸Šæ‹†åˆ†æ³¨æ„åŠ›å’Œå‰é¦ˆæ“ä½œæ¥å¸®åŠ©æˆ‘ä»¬åœ¨å¤šä¸ª GPU ä¸Šåˆ†ç‰‡æ¿€æ´»ï¼Œä»¥åŠ SP å¦‚ä½•é€šè¿‡åœ¨åºåˆ—ç»´åº¦ä¸Šæ‹†åˆ†æ¥è‡ªç„¶åœ°è¡¥å……å…¶ä½™æ“ä½œã€‚*

> [!NOTE]
> ç”±äº SP åŒºåŸŸä¸­çš„å±‚å½’ä¸€åŒ–ï¼ˆLayerNormsï¼‰å¯¹åºåˆ—çš„ä¸åŒéƒ¨åˆ†è¿›è¡Œæ“ä½œï¼Œå› æ­¤å®ƒä»¬åœ¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ranks ä¸Šçš„æ¢¯åº¦ä¼šæœ‰æ‰€ä¸åŒã€‚ä¸ºäº†ç¡®ä¿æƒé‡ä¿æŒåŒæ­¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åå‘ä¼ æ’­æœŸé—´å¯¹å®ƒä»¬çš„æ¢¯åº¦è¿›è¡Œ all-reduce æ“ä½œï¼Œç±»ä¼¼äºæ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ç¡®ä¿æƒé‡ä¿æŒåŒæ­¥çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ç§è¾ƒå°çš„é€šä¿¡å¼€é”€ï¼Œå› ä¸ºå±‚å½’ä¸€åŒ–çš„å‚æ•°ç›¸å¯¹è¾ƒå°‘ã€‚

ç„¶è€Œï¼ŒTP å’Œ SP æœ‰ä¸¤ä¸ªé™åˆ¶ï¼š1ï¼‰å¦‚æœæˆ‘ä»¬æ‰©å±•åºåˆ—é•¿åº¦ï¼Œæ¿€æ´»å†…å­˜ä»ç„¶ä¼šåœ¨ TP åŒºåŸŸçˆ†ç‚¸å¼å¢é•¿ï¼›2ï¼‰å¦‚æœæ¨¡å‹å¤ªå¤§è€Œæ— æ³•é€‚åº” TP=8ï¼Œé‚£ä¹ˆç”±äºèŠ‚ç‚¹é—´è¿æ¥é—®é¢˜ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å·¨å¤§çš„å‡é€Ÿã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§æ¥è§£å†³é—®é¢˜ 1)ï¼Œç”¨æµæ°´çº¿å¹¶è¡Œæ€§æ¥è§£å†³é—®é¢˜ 2)ã€‚è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼

## äº”ã€ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆCPï¼‰

é€šè¿‡å¼ é‡å¹¶è¡Œå’Œåºåˆ—å¹¶è¡Œï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—é™ä½æ¯ä¸ª GPU çš„å†…å­˜éœ€æ±‚ï¼Œå› ä¸ºæ¨¡å‹æƒé‡å’Œæ¿€æ´»éƒ½åˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Šã€‚ç„¶è€Œï¼Œå½“åœ¨è¶Šæ¥è¶Šé•¿çš„åºåˆ—ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ï¼ˆä¾‹å¦‚ï¼Œå½“æ‰©å±•åˆ°æ¯ä¸ªåºåˆ— 128k æˆ–æ›´å¤š tokens æ—¶ï¼‰ï¼Œç”±äºåœ¨ TP åŒºåŸŸå†…æˆ‘ä»¬ä»ç„¶éœ€è¦å¤„ç†å®Œæ•´çš„åºåˆ—é•¿åº¦ï¼Œæˆ‘ä»¬å¯èƒ½ä»ç„¶ä¼šè¶…å‡ºå•ä¸ªèŠ‚ç‚¹ä¸Šçš„å¯ç”¨å†…å­˜ã€‚

æ­¤å¤–ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨å®Œå…¨é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼ˆè¿™ä¼šå¸¦æ¥çº¦ 30% çš„å·¨å¤§è®¡ç®—å¼€é”€ï¼‰ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åœ¨å±‚è¾¹ç•Œå¤„ä¿ç•™ä¸€äº›ä¸åºåˆ—é•¿åº¦çº¿æ€§ç›¸å…³çš„æ¿€æ´»å€¼åœ¨å†…å­˜ä¸­ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§å¦‚ä½•å¸®åŠ©æˆ‘ä»¬ï¼š

[äº¤äº’å›¾]

ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç±»ä¼¼çš„æ€è·¯åº”ç”¨äºåºåˆ—å¹¶è¡Œæ€§æ–¹æ³•ï¼ˆå³æ²¿åºåˆ—é•¿åº¦è¿›è¡Œæ‹†åˆ†ï¼‰ï¼Œä½†è¦åº”ç”¨äºæˆ‘ä»¬å·²ç»åº”ç”¨å¼ é‡å¹¶è¡Œæ€§çš„æ¨¡å—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ²¿ä¸¤ä¸ªç»´åº¦å¯¹è¿™äº›æ¨¡å—è¿›è¡Œæ‹†åˆ†ï¼Œä»è€Œå‡å°‘åºåˆ—é•¿åº¦çš„å½±å“ã€‚åœ¨æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„å†…å®¹ä¹‹åï¼Œä½ ä¼šå‘ç°è¿™ç§æ–¹æ³•ç›¸å½“ç›´è§‚â€¦â€¦ä½†è¿™å…¶ä¸­æœ‰ä¸ªæŠ€å·§ï¼Œæ‰€ä»¥è¦ä¿æŒæ¸…é†’ï¼

ä¸ºäº†å®ç°ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ï¼›å°±åƒåºåˆ—å¹¶è¡Œæ€§ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ²¿ç€åºåˆ—ç»´åº¦æ‹†åˆ†è¾“å…¥ï¼Œä½†ç°åœ¨æˆ‘ä»¬å°†è¿™ç§æ‹†åˆ†åº”ç”¨äºæ•´ä¸ªæ¨¡å‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨å¼ é‡+åºåˆ—å¹¶è¡Œæ€§ä¸­æ‰€åšçš„æ¨¡å‹çš„åºåˆ—å¹¶è¡ŒåŒºåŸŸã€‚

æ‹†åˆ†åºåˆ—ä¸ä¼šå½±å“å¤§å¤šæ•°æ¨¡å—ï¼Œå¦‚ MLP å’Œ LayerNormï¼Œå…¶ä¸­æ¯ä¸ªæ ‡è®°éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ã€‚å®ƒä¹Ÿä¸éœ€è¦åƒTPé‚£æ ·è¿›è¡Œæ˜‚è´µçš„é€šä¿¡ï¼Œå› ä¸ºåªæ‹†åˆ†äº†è¾“å…¥è€Œä¸æ˜¯æƒé‡çŸ©é˜µã€‚å°±åƒæ•°æ®å¹¶è¡Œä¸€æ ·ï¼Œåœ¨è®¡ç®—æ¢¯åº¦åï¼Œä¼šå¯åŠ¨ä¸€ä¸ª all-reduce æ“ä½œæ¥åŒæ­¥ä¸Šä¸‹æ–‡å¹¶è¡Œç»„ä¸­çš„æ¢¯åº¦ã€‚

ä¸è¿‡æœ‰ä¸€ä¸ªé‡è¦çš„ä¾‹å¤–æƒ…å†µï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦ç‰¹åˆ«å…³æ³¨æ³¨æ„åŠ›å—ï¼ˆå“ˆå“ˆï¼ŒåŒå…³è¯­ğŸ˜€ï¼‰ã€‚åœ¨æ³¨æ„åŠ›æ¨¡å—ä¸­ï¼Œæ¯ä¸ªæ ‡è®°éƒ½éœ€è¦è®¿é—®æ‰€æœ‰å…¶ä»–åºåˆ—æ ‡è®°çš„é”®/å€¼å¯¹ï¼›åœ¨å› æœæ³¨æ„åŠ›çš„æƒ…å†µä¸‹ï¼Œè‡³å°‘è¦å¯¹æ¯ä¸ªå‰é¢çš„æ ‡è®°äºˆä»¥å…³æ³¨ã€‚

ç”±äºä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ä¼šæ²¿åºåˆ—ç»´åº¦å°†è¾“å…¥æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œå› æ­¤æ³¨æ„åŠ›æ¨¡å—å°†éœ€è¦åœ¨ GPU ä¹‹é—´è¿›è¡Œå®Œæ•´çš„é€šä¿¡ï¼Œä»¥äº¤æ¢å¿…è¦çš„é”®/å€¼æ•°æ®ã€‚

â€œå¦‚æœæˆ‘ä»¬å¤©çœŸåœ°å»åšè¿™ä»¶äº‹ï¼Œé‚£å¬èµ·æ¥æˆæœ¬éå¸¸é«˜ã€‚æœ‰æ²¡æœ‰ä¸€ç§æ—¢èƒ½é«˜æ•ˆåˆèƒ½å¿«é€Ÿå®Œæˆçš„æ–¹æ³•å‘¢ï¼å€¼å¾—åº†å¹¸çš„æ˜¯ï¼Œæœ‰è¿™æ ·çš„æ–¹æ³•ï¼šä¸€ç§ç”¨äºé«˜æ•ˆå¤„ç†é”®å€¼å¯¹é€šä¿¡çš„æ ¸å¿ƒæŠ€æœ¯å«åš*ç¯å½¢æ³¨æ„åŠ›ï¼ˆRing Attentionï¼‰*ã€‚â€

> [!NOTE]
> ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§ä¸ Flash Attentionï¼ˆç¨åè¯¦è¿°ï¼‰åœ¨æ¦‚å¿µä¸Šæœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„â€”â€”è¿™ä¸¤ç§æŠ€æœ¯éƒ½ä¾èµ–äºåœ¨çº¿ softmax è®¡ç®—æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è™½ç„¶ Flash Attention ä¸“æ³¨äºä¼˜åŒ–å•ä¸ª GPU ä¸Šçš„æ³¨æ„åŠ›è®¡ç®—æœ¬èº«ï¼Œä½†ä¸Šä¸‹æ–‡å¹¶è¡Œæ€§é€šè¿‡å°†åºåˆ—åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šæ¥å®ç°å†…å­˜å‡å°‘ã€‚


### 5.1 å‘ç°ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶

åœ¨è¿™ç§æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ä¸­ï¼Œæ¯ä¸ª GPU é¦–å…ˆå‘èµ·ä¸€ä¸ªå¼‚æ­¥é€šä¿¡æ“ä½œï¼Œå°†å…¶é”®/å€¼å¯¹å‘é€åˆ°å…¶ä»– GPUã€‚åœ¨ç­‰å¾…å…¶ä»– GPU çš„æ•°æ®æ—¶ï¼Œå®ƒä¼šè®¡ç®—å…¶å†…å­˜ä¸­å·²æœ‰æ•°æ®éƒ¨åˆ†çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨æœ¬æ¬¡è®¡ç®—å®Œæˆä¹‹å‰ï¼Œä¼šä»å¦ä¸€ä¸ª GPU æ¥æ”¶åˆ°ä¸‹ä¸€ä¸ªé”®/å€¼å¯¹ï¼Œä»è€Œä½¿ GPU åœ¨å®Œæˆç¬¬ä¸€è½®è®¡ç®—åèƒ½å¤Ÿç«‹å³å¼€å§‹ä¸‹ä¸€è½®è®¡ç®—ã€‚

è®©æˆ‘ä»¬æ¥è¯´æ˜ä¸€ä¸‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ 4 ä¸ª GPU å’Œ 4 ä¸ªè¾“å…¥æ ‡è®°ã€‚æœ€åˆï¼Œè¾“å…¥åºåˆ—ä¼šæ²¿ç€åºåˆ—ç»´åº¦å‡åŒ€æ‹†åˆ†ï¼Œå› æ­¤æ¯ä¸ª GPU å°†ä»…æœ‰ä¸€ä¸ªæ ‡è®°ä»¥åŠå…¶å¯¹åº”çš„ Q/K/V å€¼ã€‚å‡è®¾ Q1ã€K1 å’Œ V1 åˆ†åˆ«ä»£è¡¨ä½äºç¬¬ 1 ä¸ª GPU ä¸Šçš„ç¬¬ä¸€ä¸ªæ ‡è®°çš„æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ³¨æ„åŠ›è®¡ç®—å°†éœ€è¦ 4 ä¸ªæ—¶é—´æ­¥æ¥å®Œæˆã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ª GPU æ‰§è¡Œè¿™ä¸‰ä¸ªè¿ç»­çš„æ“ä½œï¼š

1. ä»¥éé˜»å¡æ–¹å¼å°†â€œå½“å‰çš„é”®å’Œå€¼â€å‘é€åˆ°ä¸‹ä¸€å°æœºå™¨ï¼Œä½†åœ¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥é™¤å¤–ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨è¿™ä¸€æ­¥å®Œæˆä¹‹å‰å¼€å§‹ä¸‹ä¸€æ­¥ã€‚
2. åœ¨æœ¬åœ°è®¡ç®—å…¶å·²æœ‰çš„â€œå½“å‰çš„é”®å’Œå€¼â€ä¸Šçš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™é€šå¸¸æ¶‰åŠæ‰§è¡Œ $\text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \times V$ã€‚
3. ç­‰å¾…æ¥æ”¶æ¥è‡ªå‰ä¸€ä¸ª GPU çš„é”®å’Œå€¼ï¼Œç„¶åå¾ªç¯å›åˆ°æ­¥éª¤ 1ã€‚æ­¤æ—¶â€œå½“å‰çš„é”®å’Œå€¼â€ç°åœ¨æ˜¯åˆšåˆšä»å‰ä¸€ä¸ª GPU æ¥æ”¶åˆ°çš„é”®/å€¼ã€‚

æˆ‘ä»¬æ‰§è¡Œè¿™ 3 ä¸ªæ­¥éª¤å››æ¬¡ä»¥å®Œæˆæ³¨æ„åŠ›è®¡ç®—ã€‚

ä»¥ä¸‹åŠ¨ç”»å±•ç¤ºäº†ä½¿ç”¨ 4 ä¸ª GPU çš„æ•´ä¸ªè¿‡ç¨‹ï¼š
![ring-attention.gif|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ring-attention.gif)
åœ¨è¿™ä¸ªåŠ¨ç”»ä¸­ï¼Œä½œè€…é€‰æ‹©å°†è¿™ç§æ–¹æ³•ç§°ä¸ºç¯å½¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆRing Attentionï¼‰ï¼Œè¿™ä¸€ç‚¹å¯¹æ‚¨æ¥è¯´å¯èƒ½æ˜¾è€Œæ˜“è§ã€‚

ä¸è¿‡æœ‰ä¸€ä¸ªå¤§é—®é¢˜æ˜¯ï¼Œç¯å½¢æ³¨æ„åŠ›ï¼ˆRing Attentionï¼‰çš„ç®€å•å®ç°ä¼šç”±äºå› æœæ³¨æ„åŠ›çŸ©é˜µçš„å½¢çŠ¶å¯¼è‡´ GPU ä¹‹é—´å‡ºç°ä¸¥é‡çš„ä¸å¹³è¡¡ã€‚è®©æˆ‘ä»¬é€šè¿‡è€ƒè™‘å¸¦æœ‰å› æœæ³¨æ„åŠ›æ©ç çš„æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µæ¥çœ‹çœ‹ SoftMax è®¡ç®—ã€‚
![cp_attnmask.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_attnmask.svg)

SoftMax æ˜¯æŒ‰è¡Œè®¡ç®—çš„ï¼Œè¿™æ„å‘³ç€åªè¦ GPU æ¥æ”¶åˆ°ä¸€è¡Œçš„æ‰€æœ‰ tokensï¼Œå°±å¯ä»¥è¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬çœ‹åˆ° GPU1 å¯ä»¥ç«‹å³è®¡ç®—å®ƒï¼Œå› ä¸ºå®ƒä» token 1 - 4å¼€å§‹ï¼Œå¹¶ä¸” GPU1 å®é™…ä¸Šä¸éœ€è¦ä»ä»»ä½•å…¶ä»– GPU æ¥æ”¶ä»»ä½•ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒGPU2 å°†éœ€è¦ç­‰å¾…ç¬¬äºŒè½®æ‰èƒ½ä¹Ÿæ¥æ”¶åˆ° 1-4ï¼Œä»è€Œæ‹¥æœ‰ tokens 1-8 çš„æ‰€æœ‰å€¼ã€‚æ­¤å¤–ï¼ŒGPU1 ä¼¼ä¹æ¯”æ‰€æœ‰å…¶ä»– GPU æ‰§è¡Œçš„å·¥ä½œé‡è¦å°‘å¾—å¤šã€‚

è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦æ›´å¥½åœ°å¹³è¡¡æˆ‘ä»¬çš„è®¡ç®—ã€‚

### 5.2 é”¯é½¿ï¼ˆä¹‹å­—å½¢ï¼‰ç¯æ³¨æ„åŠ›â€”â€”ä¸€ç§å¹³è¡¡çš„è®¡ç®—å®ç°

æˆ‘ä»¬éœ€è¦ä¸€ç§æ›´å¥½çš„æ–¹æ³•æ¥åˆ†é…è¾“å…¥åºåˆ—ã€‚è¿™å¯ä»¥é€šè¿‡ä¸çº¯ç²¹æŒ‰é¡ºåºå°† tokens åˆ†é…ç»™ GPUï¼Œè€Œæ˜¯ç¨å¾®æ··åˆä¸€ä¸‹é¡ºåºæ¥å®ç°ï¼Œè¿™æ ·æ¯ä¸ª GPU ä¸Šéƒ½æœ‰æ—©æœŸå’Œæ™šæœŸ tokens çš„è‰¯å¥½æ··åˆã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºé”¯é½¿å½¢ï¼ˆZ å­—å½¢ï¼‰æ³¨æ„åŠ›ï¼Œåœ¨è¿™ç§æ–°çš„æ’åˆ—ä¸­ï¼Œæ³¨æ„åŠ›æ©ç å°†æ˜¾ç¤ºè®¡ç®—é‡çš„å‡åŒ€åˆ†å¸ƒï¼Œä½†å¦‚æœä½ æ•°ä¸€æ•°æœ‰é¢œè‰²çš„æ–¹å—æ•°é‡ï¼Œå°±ä¼šå‘ç°è®¡ç®—é‡ç°åœ¨åœ¨æ‰€æœ‰ GPU ä¹‹é—´æ˜¯å¹³è¡¡çš„ã€‚

![cp_zigzagmask.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_zigzagmask.svg)

ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä¼šçœ‹åˆ°ï¼Œä¸ºäº†å®Œæˆæ‰€æœ‰è¡Œï¼Œæ¯ä¸ª GPU éƒ½éœ€è¦æ¥è‡ªå…¶ä»–æ‰€æœ‰ GPU çš„ä¿¡æ¯ã€‚

æˆ‘ä»¬æœ‰ä¸¤ç§é€šç”¨çš„æ–¹æ³•æ¥é‡å è®¡ç®—å’Œé€šä¿¡ï¼Œè¦ä¹ˆé€šè¿‡æ‰§è¡Œå¸¸è§„çš„ all-gather æ“ä½œï¼ŒåŒæ—¶åœ¨æ¯ä¸ª GPU ä¸Šé‡æ–°åˆ†ç»„æ‰€æœ‰çš„é”®å€¼å¯¹ï¼ˆä»¥ Zero-3 ç±»å‹çš„æ–¹å¼ï¼‰ï¼Œè¦ä¹ˆæ ¹æ®éœ€è¦ä»æ¯ä¸ª GPU é€ä¸ªæ”¶é›†åˆ°å…¶ä»– GPUã€‚

![cp_overlap_allgather.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_overlap_allgather.svg)

![cp_overlap_all2all.svg|500](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/cp_overlap_all2all.svg)

è¿™ä¸¤ç§å®ç°æ–¹å¼çš„å…³é”®åŒºåˆ«åœ¨äºå®ƒä»¬çš„é€šä¿¡æ¨¡å¼å’Œå†…å­˜ä½¿ç”¨æƒ…å†µï¼š

1. **AllGatherå®ç°**ï¼š
    - æ‰€æœ‰ GPU åŒæ—¶ä»æ‰€æœ‰å…¶ä»– GPU æ”¶é›†å®Œæ•´çš„é”®/å€¼å¯¹
    - ç”±äºæ¯ä¸ª GPU éœ€è¦ä¸€æ¬¡æ€§å­˜å‚¨å®Œæ•´çš„ KV å¯¹ï¼Œå› æ­¤éœ€è¦æ›´å¤šçš„ä¸´æ—¶å†…å­˜
    - é€šä¿¡åœ¨ä¸€æ­¥å†…å®Œæˆï¼Œä½†å…·æœ‰æ›´å¤§çš„å†…å­˜å¼€é”€
2. **å…¨äº’è”ï¼ˆç¯å½¢ï¼‰å®ç°**ï¼š
    - GPU ä»¥ç¯å½¢æ¨¡å¼ä¸€æ¬¡äº¤æ¢ä¸€ä¸ªæ•°æ®å—çš„ KV å¯¹
    - æ›´èŠ‚çœå†…å­˜ï¼Œå› ä¸ºæ¯ä¸ª GPU åªéœ€è¦ä¸´æ—¶å­˜å‚¨ä¸€ä¸ªé¢å¤–çš„æ•°æ®å—
    - é€šä¿¡åˆ†æ•£å¹¶ä¸è®¡ç®—é‡å ï¼Œå°½ç®¡ç”±äºå¤šä¸ªé€šä¿¡æ­¥éª¤è€Œæœ‰ä¸€äº›é¢å¤–çš„åŸºæœ¬å»¶è¿Ÿå¼€é”€

å…¨äº’è”ï¼ˆAll-to-Allï¼‰æ–¹æ³•é€šå¸¸ä»¥ç¨å¾®å¤æ‚çš„é€šä¿¡æ¨¡å¼ä¸ºä»£ä»·æä¾›æ›´å¥½çš„å†…å­˜æ•ˆç‡ï¼Œè€Œå…¨æ”¶é›†ï¼ˆAllGatherï¼‰æ–¹æ³•æ›´ç®€å•ï¼Œä½†åœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´éœ€è¦æ›´å¤šçš„ä¸´æ—¶å†…å­˜ã€‚

æˆ‘ä»¬ç°åœ¨çœ‹åˆ°äº†å¦‚ä½•é€šè¿‡åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æ¥æ‹†åˆ†æ¨¡å‹ä»¥åº”å¯¹å¤§å‹æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åºåˆ—å¹¶è¡Œï¼ˆCPï¼‰æ¥åº”å¯¹é•¿åºåˆ—ä¸­çš„æ¿€æ´»çˆ†ç‚¸é—®é¢˜ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶çŸ¥é“TP åœ¨è·¨èŠ‚ç‚¹æ‰©å±•æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œé‚£ä¹ˆå¦‚æœæ¨¡å‹æƒé‡ä¸å®¹æ˜“æ”¾åœ¨ 1 ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå¦ä¸€ç§å¹¶è¡Œåº¦ï¼Œæˆ‘ä»¬çš„ç¬¬å››ç§ï¼Œ*å¹¶è¡Œæµæ°´çº¿ï¼ˆPipeline Parallelismï¼‰* æ¥æ‹¯æ•‘äº†ï¼

## å…­ã€æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰

åœ¨â€œå¼ é‡å¹¶è¡Œâ€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œå°è¯•å°†å¼ é‡å¹¶è¡Œæ‰©å±•åˆ°å•ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡ï¼ˆé€šå¸¸ä¸º 4 æˆ– 8ï¼‰ä»¥ä¸Šæ—¶ï¼Œä¼šé‡åˆ°ä¸€ç§å¸¦å®½è¾ƒä½çš„ç½‘ç»œï¼Œç§°ä¸ºâ€œèŠ‚ç‚¹é—´è¿æ¥â€ï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡æŸå®³æˆ‘ä»¬çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨è·¨å¤šä¸ªèŠ‚ç‚¹ï¼ˆæ¯ä¸ªèŠ‚ç‚¹æœ‰ 8 ä¸ª GPUï¼‰çš„é›†ç¾¤ä¸Šå¯¹å…¶è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œæ¯”å¦‚ all-reduce æ“ä½œã€‚

[äº¤äº’å›¾]

ä¸åŒèŠ‚ç‚¹æ•°é‡ä¸‹èŠ‚ç‚¹é—´é€šä¿¡å¸¦å®½çš„æµ‹é‡ç»“æœï¼Œå±•ç¤ºäº†AllReduceã€AllGather å’Œ ReduceScatter æ“ä½œçš„ä¸­ä½æ•°ï¼ˆçº¿æ¡ï¼‰ä»¥åŠç¬¬ 5 è‡³ç¬¬ 95 ç™¾åˆ†ä½èŒƒå›´ï¼ˆé˜´å½±åŒºåŸŸï¼‰ã€‚

åºåˆ—å¹¶è¡Œå’Œä¸Šä¸‹æ–‡å¹¶è¡Œå¯¹äºé•¿åºåˆ—å¯èƒ½æœ‰å¸®åŠ©ï¼Œä½†å¦‚æœåºåˆ—é•¿åº¦å¹¶éæˆ‘ä»¬å†…å­˜é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œè€Œæ˜¯æ¨¡å‹æœ¬èº«çš„å¤§å°å¯¼è‡´çš„ï¼Œé‚£ä¹ˆè¿™ä¸¤ç§å¹¶è¡Œæ–¹å¼ä½œç”¨å°±ä¸å¤§äº†ã€‚å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆ70B å‚æ•°åŠä»¥ä¸Šï¼‰ï¼Œä»…æƒé‡çš„å¤§å°å°±å¯èƒ½è¶…è¿‡å•ä¸ªèŠ‚ç‚¹ä¸Š 4 åˆ° 8 å— GPU çš„æé™ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥ç¬¬å››ä¸ªï¼ˆä¹Ÿæ˜¯æœ€åä¸€ä¸ªï¼‰å¹¶è¡Œç»´åº¦â€”â€”â€œæµæ°´çº¿å¹¶è¡Œâ€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ ã€‚

æµæ°´çº¿å¹¶è¡Œæ˜¯ä¸€ç§ç®€å•ä½†å¼ºå¤§çš„æŠ€æœ¯ â€”â€” æˆ‘ä»¬å°†æ¨¡å‹çš„å±‚åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šï¼ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ 8 ä¸ª GPUï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¬¬ 1-4 å±‚æ”¾åœ¨ GPU 1 ä¸Šï¼Œç¬¬ 5-8 å±‚æ”¾åœ¨ GPU 2 ä¸Šï¼Œä¾æ­¤ç±»æ¨ã€‚è¿™æ ·ï¼Œæ¯ä¸ª GPU åªéœ€è¦å­˜å‚¨å’Œå¤„ç†æ¨¡å‹å±‚çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¾è‘—å‡å°‘äº†æ¯ä¸ª GPU çš„å†…å­˜éœ€æ±‚ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æµæ°´çº¿å¹¶è¡Œåœ¨ 8B æ¨¡å‹çš„å†…å­˜ä½¿ç”¨ä¸Šçš„æ•ˆæœï¼š

ï¼ˆè¿™ç§æŠ€æœ¯å¯èƒ½ä¼šè®©ä½ æƒ³èµ·æˆ‘ä»¬åœ¨è®¨è®º ZeRO-3 æ—¶çš„æƒ…å†µï¼Œå½“æ—¶æˆ‘ä»¬å°†æ¨¡å‹å‚æ•°åˆ†å‰²åˆ°å¤šä¸ª GPU ä¸Šã€‚åœ¨åé¢çš„ â€œ5D å¹¶è¡Œç®€ä»‹â€ éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šè¯¦ç»†å¯¹æ¯”è¿™ä¸¤ç§æŠ€æœ¯ã€‚ï¼‰

[äº¤äº’å›¾]

è§‚å¯Ÿä¸Šå›¾ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šè™½ç„¶æ¨¡å‹å‚æ•°åœ¨å„ä¸ª GPU ä¸Šåˆ†é…å¾—å¾ˆå¥½ï¼Œä½†æ¯ä¸ª GPU ä¸Šçš„æ¿€æ´»å†…å­˜å´ä¿æŒä¸å˜ï¼è¿™æ˜¯å› ä¸ºæ¯ä¸ª GPU ä»ç„¶éœ€è¦å¤„ç†å®Œæ•´çš„æ•°æ®æ‰¹æ¬¡ï¼Œåªæ˜¯å¤„ç†ä¸åŒçš„å±‚ã€‚ä¸€ä¸ª GPU å±‚çš„æ¿€æ´»å°†è¢«å‘é€åˆ°ä¸‹ä¸€ä¸ª GPU ä»¥ç»§ç»­å‰å‘ä¼ æ’­ã€‚

è¿™å¼•å…¥äº†ä¸€ç§æ–°çš„é€šä¿¡æ¨¡å¼ï¼šæˆ‘ä»¬ç°åœ¨ä¸æ˜¯åƒåœ¨æ•°æ®å¹¶è¡Œä¸­ä½¿ç”¨ ZeRO-3 é‚£æ ·é€šä¿¡å‚æ•°ï¼Œè€Œæ˜¯åœ¨ GPU ä¹‹é—´æŒ‰é¡ºåºâ€œæµæ°´çº¿å¼â€ä¼ é€’æ¿€æ´»å¼ é‡ã€‚è™½ç„¶æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†é«˜æ•ˆå®ç°è¿™ç§æŠ€æœ¯ç›¸å½“æ£˜æ‰‹ã€‚è®©æˆ‘ä»¬ç›´æ¥æ·±å…¥ç»†èŠ‚ï¼

### 6.1 åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šæ‹†åˆ†å±‚ - å…¨éƒ¨å‰å‘ï¼Œå…¨éƒ¨åå‘

æ‰€ä»¥ï¼Œå‡è®¾æˆ‘ä»¬åªæ˜¯å°†å„å±‚åˆ†å¸ƒåˆ°å‡ ä¸ªè®¾å¤‡ä¸Šï¼Œä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ª GPU å°†å¤„ç†å‰å‡ å±‚ï¼Œç¬¬äºŒä¸ª GPU å°†å¤„ç†æ¨¡å‹çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œä¾æ­¤ç±»æ¨ã€‚ç°åœ¨ï¼Œé€šè¿‡æˆ‘ä»¬æ¨¡å‹çš„å‰å‘ä¼ æ’­ç®€å•åœ°æ¶‰åŠæŒ‰é¡ºåºå°†æ•°æ®æ‰¹æ¬¡æ²¿æ¨¡å‹ä¼ é€’ï¼Œä»è€Œè¿ç»­ä½¿ç”¨æ¯ä¸ªè®¡ç®—è®¾å¤‡ã€‚

æˆ‘ä»¬æœ‰ä¸€ä¸ªç›´æ¥çš„é¦–è¦ä¼˜åŠ¿ï¼šæ‰€éœ€çš„äº’è¿å¸¦å®½ä¿æŒç›¸å½“ä½ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ¨¡å‹æ·±åº¦çš„å°‘æ•°ä½ç½®ä»…å‘é€ä¸­ç­‰å¤§å°çš„æ¿€æ´»ã€‚ä¸ä¾‹å¦‚å¼ é‡å¹¶è¡Œä¸­çš„é€šä¿¡ç›¸æ¯”ï¼Œè¿™å¯ä»¥äº§ç”Ÿå·¨å¤§å·®å¼‚ï¼Œåè€…åœ¨æ¯å±‚å†…ä¼šå‘ç”Ÿæ•°æ¬¡ã€‚

ä½†ä¹Ÿè®¸ä½ å¼€å§‹æ„Ÿè§‰åˆ°ä¸€äº›å³å°†åˆ°æ¥çš„éº»çƒ¦ï¼šÂ **â€œsequentiallyâ€** å’Œ **â€œsuccessivelyâ€**ï¼Ÿï¼ï¼Ÿåœ¨å¹¶è¡Œè®¡ç®—çš„ä¸–ç•Œé‡Œï¼Œè¿™å¬èµ·æ¥å¹¶ä¸é«˜æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ‘ä»¬è®¨è®ºäº†è®¡ç®—å’Œé€šä¿¡é‡å ä¹‹åã€‚

ç¡®å®ï¼Œè¯»è€…æœ‹å‹ä»¬ï¼æµæ°´çº¿å¹¶è¡Œä¸­çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°è§„é¿æµæ°´çº¿å¹¶è¡Œçš„é¡ºåºæ€§ï¼Œä½¿æˆ‘ä»¬çš„ GPU å§‹ç»ˆä¿æŒå¿™ç¢ŒçŠ¶æ€ï¼Œé¿å…å‡ºç°ä¸€ä¸ª GPU åœ¨è®¡ç®—è€Œå…¶ä»– GPU åœ¨ç­‰å¾…çš„æƒ…å†µã€‚ä¸‹é¢å±•ç¤ºçš„æ˜¯æˆ‘ä»¬åœ¨å¯¹æ¨¡å‹è¿›è¡Œç®€å•ç›´æ¥çš„å‘å‰å’Œå‘åä¼ æ’­æ—¶ GPU çš„åˆ©ç”¨ç‡æƒ…å†µï¼ˆæ­¤å¤„æ•°å­—è¡¨ç¤ºæ¨¡å‹çš„å„å±‚ï¼‰ï¼š

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_afab.svg)

*ä¸€ä¸ªåœ¨ 4 ä¸ª GPU ä¸Šåˆ†å¸ƒæœ‰ 16 å±‚çš„æ¨¡å‹çš„æµæ°´çº¿å¹¶è¡Œç¤ºä¾‹ã€‚æ•°å­—å¯¹åº”å±‚ IDã€‚*

å‰©ä½™çš„ç©ºé—²æ—¶é—´ä»¥ç°è‰²è¡¨ç¤ºï¼Œé€šå¸¸ç§°ä¸ºâ€œæ°”æ³¡â€ï¼Œåœ¨èŠ±è´¹äº†è¿™ä¹ˆå¤šæ—¶é—´ä¼˜åŒ–ååé‡ä¹‹åï¼Œçœ‹åˆ°è¿™ä¸ªå¯èƒ½ä¼šè®©ä½ å¿ƒç¢ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿå› â€œæ°”æ³¡â€è€ŒæŸå¤±çš„æ—¶é—´æ¥é‡åŒ–æµæ°´çº¿è®¾ç½®çš„æ•ˆç‡ã€‚è®¾ $t_f$ å’Œ $t_b$ åˆ†åˆ«ä¸ºå‰å‘å’Œåå‘ä¼ æ’­çš„æ—¶é—´ï¼ˆé’ˆå¯¹ä¸€ä¸ªå¾®æ‰¹æ¬¡å’Œæµæ°´çº¿çš„ä¸€ä¸ªé˜¶æ®µæµ‹é‡ï¼‰ã€‚ä¸€ä¸ªç®€å•çš„å‡è®¾æ˜¯ $t_b \approx 2 \times t_f$ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå®Œç¾å¹¶è¡ŒåŒ–ï¼Œç†æƒ³çš„æ€»æ—¶é—´å°†æ˜¯ $t_{\text{id}} = t_f + t_b$ã€‚ç„¶è€Œï¼Œç”±äºæµæ°´çº¿æ°”æ³¡ï¼Œé¢å¤–çš„æ—¶é—´ä¸º $t_{\text{pb}} = (p-1) \times (t_f + t_b)$ï¼Œå…¶ä¸­ $p$ æ˜¯æµæ°´çº¿å¹¶è¡Œåº¦ï¼Œå³ä¸Šå›¾ä¸­çš„ GPU æ•°é‡ã€‚è¿™è¡¨ç¤ºæ¯ä¸ª GPU åœ¨å…¶ä»– GPU è®¡ç®—æ—¶çš„ç­‰å¾…æ—¶é—´ã€‚

æˆ‘ä»¬å¯ä»¥è®¡ç®—é¢å¤–æ°”æ³¡æ—¶é—´ç›¸å¯¹äºç†æƒ³æ—¶é—´çš„æ¯”ç‡ï¼š$$r_{\text{bubble}} = \frac{(p - 1) \times (t_f + t_b)}{t_f + t_b} = p - 1$$
éšç€æˆ‘ä»¬å¢åŠ æ›´å¤šé˜¶æ®µï¼Œæ°”æ³¡æ—¶é—´å› æ­¤å¢åŠ ï¼Œåˆ©ç”¨ç‡ä¸‹é™ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œåœ¨ç®€å•å®ç°ä¸­ï¼Œæ°”æ³¡å¯èƒ½ä¼šéå¸¸å¤§ï¼

å€¼å¾—åº†å¹¸çš„æ˜¯ï¼Œäººä»¬å·²ç»è®¾è®¡å‡ºäº†å„ç§æµæ°´çº¿å¹¶è¡Œæ–¹æ¡ˆæ¥*å‡å°æ°”æ³¡è§„æ¨¡*ã€‚

è®©æˆ‘ä»¬ä»å·¥å…·ç®±ä¸­å–å‡ºç¬¬ä¸€ä¸ªå·¥å…·ï¼Œæ€è€ƒå°†æˆ‘ä»¬çš„æ‰¹æ¬¡åˆ†å‰²æˆæ›´å°çš„ã€å¯ä»¥å¹¶è¡Œæˆ–å‡ ä¹å¹¶è¡Œå¤„ç†çš„å°å—ï¼ˆéƒ¨åˆ†ï¼‰ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰åœ¨æ•°æ®å¹¶è¡Œä¸­æ‰€åšçš„é‚£æ ·ã€‚ç°åœ¨ï¼Œå½“ç¬¬äºŒä¸ª GPU å¿™äºå¤„ç†å¾®æ‰¹æ¬¡1æ—¶ï¼Œç¬¬ä¸€ä¸ª GPU å·²ç»å¯ä»¥å¼€å§‹å¤„ç†å¾®æ‰¹æ¬¡ 2 äº†ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ 8 ä¸ªå¾®æ‰¹æ¬¡çš„è°ƒåº¦å®‰æ’ï¼š

![pp_afab2.svg|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_afab2.svg)

ï¼ˆåœ¨ä¹‹å‰çš„å›¾è¡¨ä¸­ï¼Œæ•°å­—è¡¨ç¤ºçš„æ˜¯å±‚ï¼Œä½†ä»ç°åœ¨èµ·ï¼ˆåŒ…æ‹¬æœ¬å›¾ï¼‰çš„æ‰€æœ‰æµæ°´çº¿å¹¶è¡Œå›¾ä¸­ï¼Œæ•°å­—è¡¨ç¤ºçš„æ˜¯å¾®æ‰¹æ¬¡ã€‚ä½ å¯ä»¥å°†è¿™é‡Œçš„æ¯ä¸ªæ–¹æ ¼çœ‹ä½œåŒ…å«è‹¥å¹²å±‚ï¼Œå°±åƒå‰ä¸€å¹…å›¾ä¸­æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚ï¼‰

ä¸Šè¿°è°ƒåº¦æ–¹å¼è¢«ç§°ä¸º***å…¨å‰å‘å…¨åå‘ï¼ˆAFABï¼‰*** è°ƒåº¦ï¼Œå› ä¸ºæˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ‰€æœ‰å‰å‘ä¼ æ’­ï¼Œç„¶åå†åªè¿›è¡Œæ‰€æœ‰åå‘ä¼ æ’­ã€‚å…¶ä¼˜åŠ¿åœ¨äºå‰å‘æ­¥éª¤å’Œåå‘æ­¥éª¤æ€»ä½“ä¸Šä»ç„¶æ˜¯é¡ºåºæ‰§è¡Œçš„ï¼Œå› æ­¤æˆ‘ä»¬ä¿ç•™äº†æ¨¡å‹è®­ç»ƒä»£ç çš„ä¸€èˆ¬ç»„ç»‡ç»“æ„ã€‚è¿™ä½¿å¾—è¿™ç§æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰å®ç°æˆä¸ºæœ€å®¹æ˜“å®ç°çš„æ–¹å¼ä¹‹ä¸€ ã€‚

ä½ å¯ä»¥åœ¨ picotron ä¸­æ‰¾åˆ° AFAB pipeline çš„å®Œæ•´å®ç°ã€‚

ğŸ‘‰ AFAB PP åœ¨ Picotron ä¸­çš„ PP å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
def train_step_pipeline_afab(model, data_loader, tensor_shapes, device, dtype):
    logging_loss: torch.float32 = 0.0
    input_tensors, output_tensors = [], []
    requires_grad_sync = pgm.process_group_manager.cp_dp_world_size > 1

    for _ in range(data_loader.grad_acc_steps): # All forward passes
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
        batch = next(data_loader)
        batch["hidden_states"] = input_tensor.to(device) if input_tensor is not None else input_tensor
        output_tensor = model.forward(input_ids=batch["input_ids"].to(device), position_ids=batch["position_ids"].to(device), hidden_states=batch["hidden_states"])
        pipeline_communicate(operation='send_forward', tensor=output_tensor, device=device, dtype=dtype)
        
        # calculate loss on the last stage
        if pgm.process_group_manager.pp_is_last_stage:
            output_tensor = F.cross_entropy(output_tensor.transpose(1, 2), batch["target_ids"].to(device), reduction='mean')
            logging_loss += output_tensor.item() / data_loader.grad_acc_steps

        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)

    for ith_microbatch in range(data_loader.grad_acc_steps): # All backward passes
        if requires_grad_sync:
            is_last_iteration = (ith_microbatch == data_loader.grad_acc_steps - 1)
            model.require_backward_grad_sync = is_last_iteration
        output_tensor_grad = pipeline_communicate(operation='recv_backward', shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)

    return logging_loss
```

è®©æˆ‘ä»¬åœ¨è¿™ä¸ªä¾‹å­ä¸­ä¼°ç®—ä¸€ä¸‹æ°”æ³¡ã€‚ä¸æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¾‹å­ä¸åŒçš„æ˜¯ï¼Œç°åœ¨å¤„ç† $m$ ä¸ªå°æ‰¹é‡çš„ç†æƒ³æ—¶é—´æ˜¯ $t_{id}=m \times (t_f+t_b)$ï¼š$$r_{\text{bubble}} = \frac{(p - 1) \times (t_f + t_b)}{m \times (t_f + t_b)} = \frac{p - 1}{m}$$
æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œé€šè¿‡å¢åŠ æ›´å¤šçš„å¾®æ‰¹æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ°”æ³¡çš„å¤§å°ç¼©å° $m$ å€ï¼Œä»è€Œè§£å†³æµæ°´çº¿é˜¶æ®µçš„ä¸€äº›ä½æ•ˆç‡é—®é¢˜ã€‚

ç„¶è€Œï¼Œä¸æ°”æ³¡åŒæ ·çƒ¦äººçš„æ˜¯å­˜å‚¨æ‰€æœ‰æ¿€æ´»æ‰€éœ€çš„å­˜å‚¨ç©ºé—´ã€‚åœ¨è¾¾åˆ°åå‘é˜¶æ®µä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ¿€æ´»ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¼è‡´è¿™äº› PP å®ç°ä¸­çš„å†…å­˜è¿…é€Ÿçˆ†ç‚¸ã€‚æˆ‘ä»¬èƒ½åšå¾—æ›´å¥½ï¼Œé¿å…è¿™ç§å†…å­˜çˆ†ç‚¸å—ï¼Ÿ

ç”±äºå†…å­˜çˆ†ç‚¸æ˜¯ç”±æˆ‘ä»¬ä¸ºåå‘ä¼ æ’­å­˜å‚¨çš„æ¿€æ´»è§¦å‘çš„ï¼Œè®©æˆ‘ä»¬å°è¯•çœ‹çœ‹æ˜¯å¦å¯ä»¥åœ¨æˆ‘ä»¬ä»åœ¨è¿›è¡Œè®¡ç®—çš„å‰å‘éƒ¨åˆ†æ—¶å°±å¼€å§‹æ‰§è¡Œåå‘ä¼ æ’­ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°½å¿«ä¸¢å¼ƒä¸€äº›ç”¨äºåå‘ä¼ æ’­æ‰€éœ€çš„æ¿€æ´»ã€‚

### 6.2 â€œå‘å‰ä¸€æ­¥-å‘åä¸€æ­¥â€åŠ LLama 3.1 æ–¹æ¡ˆ

è¿™ç§è°ƒåº¦è¢«ç§°ä¸º*ä¸€å‰ä¸€åï¼ˆ1F1Bï¼‰*ï¼Œå› ä¸ºä¸­é—´/ç¨³å®šçŠ¶æ€æ¶‰åŠäº¤æ›¿è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­å’Œä¸€æ¬¡åå‘ä¼ æ’­ã€‚å…¶æ€»ä½“æ€è·¯æ˜¯å°½æ—©å¼€å§‹è¿›è¡Œåå‘ä¼ æ’­ã€‚è°ƒåº¦è¿‡ç¨‹å¦‚ä¸‹ï¼š
![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b.svg)

å¦‚æœä½ ä»”ç»†è®¡ç®—å°±ä¼šå‘ç°æ°”æ³¡å¤§å°ä»ç„¶ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬çš„è®­ç»ƒæ•ˆç‡å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åªéœ€å­˜å‚¨ $p$ ä¸ªå¾®æ‰¹æ¬¡çš„æ¿€æ´»å€¼ï¼ˆå…¶ä¸­ $p$ æ˜¯æµæ°´çº¿å¹¶è¡Œåº¦ï¼‰ï¼Œè€Œä¸æ˜¯ $m$ ä¸ªï¼ˆå…¶ä¸­ $m$ æ˜¯å¾®æ‰¹æ¬¡çš„æ•°é‡ï¼‰ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ AFABï¼ˆå‡è®¾ä¸ºæŸç§è°ƒåº¦æ–¹å¼ï¼Œå…·ä½“éœ€ç»“åˆä¸Šä¸‹æ–‡ç¡®å®šå‡†ç¡®å«ä¹‰ï¼‰è°ƒåº¦ä¸­å‡ºç°çš„æ¿€æ´»å€¼å†…å­˜çˆ†ç‚¸é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¢åŠ æ›´å¤šå¾®æ‰¹æ¬¡ï¼Œè€Œè¿™å®é™…ä¸Šä¼šå‡å°‘æ°”æ³¡ã€‚

è¿™ç§è®¾ç½®çš„å¤æ‚æ€§ï¼ˆå¦‚ä¸Šå›¾æ‰€ç¤ºï¼‰åœ¨äºï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸å†æ˜¯æ¸…æ™°çš„é¡ºåºæ‰§è¡Œï¼Œè€Œæ˜¯åœ¨è®¾å¤‡é—´å¹¶è¡Œæ‰§è¡Œå¹¶äº¤é”™è¿›è¡Œã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†ä¸å¾—ä¸åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šç‹¬ç«‹åœ°å®‰æ’ä»å‰å‘ä¼ æ’­åˆ°åå‘ä¼ æ’­çš„åˆ‡æ¢ï¼Œè€Œä¸æ˜¯åƒå¾€å¸¸ä¸€æ ·åœ¨ä¸€ä¸ªç®€å•ä¸”é€šç”¨çš„ä¸­å¤®è®­ç»ƒå¾ªç¯ä¸­è¿›è¡Œã€‚

è¿™å°±æ˜¯å®æ–½æµæ°´çº¿å¹¶è¡Œé€šå¸¸éœ€è¦å¯¹è®­ç»ƒä»£ç ä»¥åŠå»ºæ¨¡ä»£ç è¿›è¡Œç›¸å½“å¹¿æ³›çš„ä¿®æ”¹çš„åŸå› ä¹‹ä¸€ã€‚

ä½ ä¹Ÿå¯ä»¥åœ¨ picotron ä¸­æ‰¾åˆ° 1F1B çš„å®Œæ•´å®ç°ï¼š

ğŸ‘‰ 1F1B PP åœ¨ Picotron ä¸­çš„å®ç°ï¼ˆç‚¹å‡»å±•å¼€ï¼‰

```python
def train_step_pipeline_1f1b(model, data_loader, tensor_shapes, device, dtype):    
    num_warmup_microbatches = min(pgm.process_group_manager.pp_world_size - pgm.process_group_manager.pp_rank - 1, data_loader.grad_acc_steps)
    num_microbatches_remaining = data_loader.grad_acc_steps - num_warmup_microbatches
    logging_loss, input_tensors, output_tensors  = 0.0, [], []
    requires_grad_sync = pgm.process_group_manager.cp_dp_world_size > 1
    
    def _forward_step(input_tensor):
        batch = next(data_loader)
        batch["hidden_states"] = input_tensor.to(device) if input_tensor is not None else input_tensor
        output_tensor = model.forward(input_ids=batch["input_ids"].to(device), position_ids=batch["position_ids"].to(device), hidden_states=batch["hidden_states"])
        
        # calculate loss on the last stage
        if pgm.process_group_manager.pp_is_last_stage:
            output_tensor = F.cross_entropy(output_tensor.transpose(1, 2), batch["target_ids"].to(device), reduction='mean')
            nonlocal logging_loss
            logging_loss += output_tensor.item() / data_loader.grad_acc_steps
        return output_tensor

    for _ in range(num_warmup_microbatches): # Warmup forward passes
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
        output_tensor = _forward_step(input_tensor)
        pipeline_communicate(operation='send_forward', tensor=output_tensor, device=device, dtype=dtype)
        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)

    if num_microbatches_remaining > 0:
        input_tensor = pipeline_communicate(operation='recv_forward', shapes=tensor_shapes, device=device, dtype=dtype)
    
    if requires_grad_sync:
        model.require_backward_grad_sync = False

    for ith_microbatch in range(num_microbatches_remaining):  # 1F1B steady state
        is_last_iteration = (ith_microbatch == num_microbatches_remaining - 1)
        output_tensor = _forward_step(input_tensor)
        output_tensor_grad = bidirectional_pipeline_communicate(operation='send_fwd_recv_bwd', send_tensor=output_tensor, recv_shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensors.append(input_tensor)
        output_tensors.append(output_tensor)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        
        # Trigger gradient sync on the last microbatch but only when last rank (the one that has num_warmup_microbatches = 0) has finished computing its backward pass.
        if num_warmup_microbatches == 0 and is_last_iteration:
            model.require_backward_grad_sync = True

        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        
        if is_last_iteration:
            input_tensor = None
            pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)
        else:
            input_tensor = bidirectional_pipeline_communicate(operation='send_bwd_recv_fwd', send_tensor=input_tensor_grad, recv_shapes=tensor_shapes, device=device, dtype=dtype)

    for ith_warmup_microbatches in range(num_warmup_microbatches): # Cooldown backward passes
        if requires_grad_sync:
            is_last_iteration = (ith_warmup_microbatches == num_warmup_microbatches - 1)
            model.require_backward_grad_sync = (ith_warmup_microbatches == num_warmup_microbatches - 1)
        input_tensor, output_tensor = input_tensors.pop(0), output_tensors.pop(0)
        output_tensor_grad = pipeline_communicate(operation='recv_backward', shapes=tensor_shapes, device=device, dtype=dtype)
        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)
        pipeline_communicate(operation='send_backward', tensor=input_tensor_grad, device=device, dtype=dtype)

    return logging_loss
```

è®©æˆ‘ä»¬é€šè¿‡åœ¨æˆ‘ä»¬çš„é›†ç¾¤ä¸Šè¿›è¡Œçš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼Œæ¥çœ‹ä¸€ä¸‹ 1F1B æµæ°´çº¿å¹¶è¡Œè°ƒåº¦åœ¨å®è·µä¸­æ˜¯å¦‚ä½•æ‰©å±•çš„ï¼š

![Throughput scaling of Pipeline Parallelism with varying microbatch sizes|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b_scaling.png)

åœ¨å·¦ä¾§ï¼Œå¾®æ‰¹å¤„ç†æ•°é‡ç­‰äºæˆ–å°äºæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰åº¦å‡ä¸€ï¼ˆ$m = p - 1$ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æµæ°´çº¿æ°”æ³¡çš„å±å®³æœ‰å¤šå¤§â€”â€”æ€§èƒ½å¾ˆä½ï¼Œè€Œä¸”éšç€æµæ°´çº¿å¹¶è¡Œåº¦çš„å¢åŠ ç”šè‡³è¿˜ä¼šä¸‹é™ã€‚å³ä¾§å›¾è¡¨æ˜¾ç¤ºï¼Œä½¿ç”¨è¿œå¤šäºæµæ°´çº¿å¹¶è¡Œåº¦çš„å¾®æ‰¹å¤„ç†æ•°é‡ï¼ˆ$m = 32 \gg p - 1$ï¼‰æœ‰åŠ©äºæ”¹å–„ä½æµæ°´çº¿å¹¶è¡Œåº¦ä¸‹çš„æ€§èƒ½ï¼Œä½†åœ¨éå¸¸å¤§çš„æµæ°´çº¿å¹¶è¡Œåº¦ä¸‹ä»ç„¶å—é™ã€‚å®é™…ä¸Šï¼Œç”±äºæœ€ç»ˆå—åˆ°ç›®æ ‡å…¨å±€æ‰¹é‡å¤§å°çš„é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•éšæ„å¢åŠ å¾®æ‰¹å¤„ç†æ•°é‡ä»¥ä¿æŒ $m \gg p - 1$ çš„æ¯”ä¾‹ã€‚éšç€æµæ°´çº¿å¹¶è¡Œåº¦çš„å¢åŠ ï¼Œå½“å¾®æ‰¹å¤„ç†æ•°é‡è¾¾åˆ°å¯èƒ½çš„æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆå¿…é¡»æ ¹æ®Â $r_\text{bubble}=\frac{pâˆ’1}{m}$ æ¥å¢åŠ æ°”æ³¡å¤§å° ã€‚

æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å¾®æ‰¹æ¬¡æ•°é‡è¾ƒå°‘æ—¶ï¼Œä»å•ä¸ªèŠ‚ç‚¹ï¼ˆ$p = 8$ï¼‰æ‰©å±•åˆ°ä¸¤ä¸ªèŠ‚ç‚¹ï¼ˆ$p = 16$ï¼‰ï¼Œæ€§èƒ½ä»…ä¸‹é™äº† 14% â€”â€”è¿™æ¯”å¼ é‡å¹¶è¡Œæ€§åœ¨ç±»ä¼¼çš„è·¨èŠ‚ç‚¹åœºæ™¯ä¸­é€šå¸¸å‡ºç°çš„çº¦ 43% çš„æ€§èƒ½é€€åŒ–è¦å¥½å¾—å¤šã€‚å½“é‡åˆ°èŠ‚ç‚¹é—´ä½å¸¦å®½ç½‘ç»œæ—¶ï¼Œè¿™ç§è¡Œä¸ºä½¿å¾—æµæ°´çº¿å¹¶è¡Œæ€§åœ¨è·¨å¤šä¸ªèŠ‚ç‚¹çš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ç‰¹åˆ«æœ‰å¸å¼•åŠ›ã€‚

è™½ç„¶ 1F1B æ˜¾è‘—å‡å°‘äº†æˆ‘ä»¬çš„æ¿€æ´»å†…å­˜å ç”¨ï¼Œä½†ä»è¿™æœ€åä¸€å¼ å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæµæ°´çº¿æ°”æ³¡ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æ•ˆç‡ç“¶é¢ˆã€‚ç”±äºæ°”æ³¡å¤§å°ä»ä¸æµæ°´çº¿é˜¶æ®µæ•°æˆæ­£æ¯”ï¼Œæˆ‘ä»¬è®©å®è´µçš„ GPU è®¡ç®—èƒ½åŠ›å¤„äºé—²ç½®çŠ¶æ€ã€‚æˆ‘ä»¬èƒ½å¦è®¾è®¡å‡ºä¸€ç§æ›´å·§å¦™çš„è°ƒåº¦æ–¹æ¡ˆæ¥å°½é‡å‡å°‘è¿™ç§æµªè´¹çš„è®¡ç®—æ—¶é—´å‘¢ï¼Ÿ

### 6.3 äº¤é”™é˜¶æ®µ

1F1B è°ƒåº¦è®©æˆ‘ä»¬æ”¹å–„äº†å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œä½†å¯¹ç©ºé—²åŒ…çš„å¤§å°æ”¹å–„ä¸å¤§ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬è¿˜èƒ½æ¨è¿›è¿™ä¸€è¾¹ç•Œå—ï¼Ÿ

åŸæ¥ï¼Œå¦‚æœæˆ‘ä»¬æ„¿æ„å¼•å…¥ä¸€äº›é¢å¤–çš„é€šä¿¡æ“ä½œï¼Œè¿™æ˜¯å¯è¡Œçš„ã€‚ç°åœ¨æ˜¯æ—¶å€™è°ˆè°ˆ*äº¤é”™é˜¶æ®µ*äº†ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´æ²¿ç€æ¨¡å‹æ·±åº¦ç»´åº¦å¤©çœŸåœ°å¯¹æ¨¡å‹è¿›è¡Œåˆ‡ç‰‡å¤„ç†ï¼Œä¾‹å¦‚å°†ç¬¬ 1-4 å±‚æ”¾åœ¨ç¬¬ä¸€ä¸ª GPU ä¸Šï¼Œå°†ç¬¬ 5-8 å±‚æ”¾åœ¨ç¬¬äºŒä¸ª GPU ä¸Šã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è€ƒè™‘å…¶ä»–å¯¹å±‚è¿›è¡Œåˆ‡ç‰‡çš„æ–¹å¼ï¼Œæ¯”å¦‚å°†å¥‡æ•°å±‚ï¼ˆç¬¬ 1ã€3ã€5ã€7 å±‚ï¼‰æ”¾åœ¨ç¬¬ä¸€ä¸ª GPU ä¸Šï¼Œå°†å¶æ•°å±‚ï¼ˆç¬¬ 2ã€4ã€6ã€8 å±‚ï¼‰æ”¾åœ¨ç¬¬äºŒä¸ª GPU ä¸Šã€‚

è¿™é€šå¸¸å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§â€œç¯å½¢ç®¡é“â€ï¼Œå¾®æ‰¹æ¬¡åœ¨é€šè¿‡æ¨¡å‹çš„å‰å‘ä¼ æ’­æ—¶ä¼šä»ä¸€ä¸ª GPU å¾ªç¯ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª GPUã€‚è®©æˆ‘ä»¬é€šè¿‡å›¾å½¢æ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

![pp_1f1b_interleaved.svg|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_1f1b_interleaved.svg)

*ä¸€ä¸ªåœ¨ 4 ä¸ª GPU ä¸Šåˆ†å¸ƒå„å±‚çš„æ¨¡å‹çš„äº¤é”™æµæ°´çº¿å¹¶è¡Œçš„ç¤ºä¾‹ã€‚ç¼–å·ä»å¯¹åº”å¾®æ‰¹å¤„ç† IDï¼Œä½†ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚è¿›è¡Œäº†ä¸åŒç€è‰²ï¼Œä»¥è¯´æ˜å„å±‚æ˜¯å¦‚ä½•åˆ†å¸ƒåœ¨ GPU ä¸Šçš„ã€‚*

å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°ç”±äºæ¨¡å‹é’ˆå¯¹åŒä¸€è®¡ç®—åœ¨æ¯ä¸ª GPU ä¸Šè¦ç»è¿‡å¤šæ¬¡ï¼ˆè€Œæ­¤å‰åªéœ€ä¸€æ¬¡ï¼‰ï¼Œä»è€Œå‡ºç°äº†é¢å¤–çš„é€šä¿¡æƒ…å†µã€‚ä¸è¿‡ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éƒ½è¢«ä¸€ä¸ªå› å­ $v$ æ‰€åˆ†æ‘Šï¼Œå…¶ä¸­ $v$ æ˜¯é˜¶æ®µæ•°æˆ–è€…æ¯ä¸ª GPU çš„æ¨¡å‹å—æ•°ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨èƒ½å¤Ÿæ›´å¥½åœ°äº¤é”™è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚$$\begin{align}
t_{pb} &= \frac{(p - 1) \times (t_f + t_b)}{v} \\[1.2ex]
r_{\text{bubble}} &= \frac{1}{v} \frac{(p - 1) \times (t_f + t_b)}{m \times (t_f + t_b)} = \frac{p - 1}{v \times m}\end{align}$$
å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡æ·»åŠ å¾®æ‰¹æ¬¡å’Œäº¤é”™é˜¶æ®µæ¥å‡å°æ°”æ³¡ï¼Œä½†è¯·æ³¨æ„ï¼Œä»æ•°é‡ä¸Šæ¥è¯´ï¼Œé€šä¿¡é‡ä¹Ÿä¼šå›  $v$ è€Œå¢åŠ ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ç§æƒè¡¡ã€‚åœ¨ä¸‹é¢çš„å›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ° $p = 8$ çš„ PP è®¾ç½®çš„å‡ ç§é…ç½®ï¼Œå…¶ä¸­ç‰¹æ®Šæƒ…å†µ $m = 1, v = 1$ å¯¹åº”äºç®€å•çš„æµæ°´çº¿å¹¶è¡Œï¼Œ$v = 1$ çš„é…ç½®æ˜¯ AFAB æˆ– 1F1B è®¾ç½®ï¼Œè€Œ $v â‰  1$ æ˜¯äº¤é”™é…ç½®ã€‚

[äº¤äº’å›¾]

è°ƒåº¦åœ¨è¿™é‡Œä¹Ÿå˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»åœ¨ç»™å®šçš„ GPU ä¸Šå’Œç»™å®šçš„æ—¶åˆ»å†³å®šæˆ‘ä»¬æ˜¯ä¼˜å…ˆå¤„ç†å…ˆåˆ°è¾¾çš„å¾®æ‰¹æ¬¡æ•°æ®ï¼Œè®©å®ƒä»¬é€šè¿‡åç»­å±‚ï¼ˆå³å°½å¿«å®Œæˆå‰å‘å’Œåå‘ä¼ æ’­å¾ªç¯ï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„â€œæ·±åº¦ä¼˜å…ˆâ€ï¼Œå³ä¼˜å…ˆè®©æ‰¹æ¬¡æ•°æ®å°½å¿«ç¦»å¼€æ¨¡å‹ï¼‰ï¼Œè¿˜æ˜¯ä¼˜å…ˆå¤„ç†åç»­çš„å¾®æ‰¹æ¬¡æ•°æ®ï¼Œè®©å®ƒä»¬é€šè¿‡å‰é¢çš„å±‚ï¼ˆå³æ‰€è°“çš„â€œå¹¿åº¦ä¼˜å…ˆâ€ï¼Œå³ä¼˜å…ˆå°½å¯èƒ½å¡«æ»¡æµæ°´çº¿ï¼‰ã€‚è¿™ç§é€‰æ‹©åœ¨ç²¾å½©çš„â€œå¹¿åº¦ä¼˜å…ˆæµæ°´çº¿â€è®ºæ–‡[^6]ä¸­æœ‰è¯¦ç»†è§£é‡Šã€‚

ä½ ç°åœ¨æ‹¥æœ‰äº†ç†è§£ Llama 3.1 ä¸­æµæ°´çº¿å¹¶è¡Œæ–¹æ³•çš„æ‰€æœ‰è¦ç´ ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸€æ¬¡åå‘ä¼ æ’­çš„è®¾ç½®ï¼Œå„é˜¶æ®µäº¤é”™è¿›è¡Œï¼Œå¹¶ä¸”ä¼˜å…ˆçº§è®¾ç½®å¯ä»¥åœ¨æ·±åº¦ä¼˜å…ˆå’Œå¹¿åº¦ä¼˜å…ˆä¹‹é—´è¿›è¡Œè°ƒæ•´ã€‚

![pp_llama3.1_schedule.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_llama3.1_schedule.png)

ç„¶è€Œï¼Œæˆ‘ä»¬å°šæœªåˆ°è¾¾å¯èƒ½çš„æµæ°´çº¿è°ƒåº¦æ–¹æ¡ˆçš„å°½å¤´ï¼Œæœ€è¿‘å·²ç»æœ‰ä¸€äº›æ–¹æ³•è¢«æå‡ºæ¥å°†æ°”æ³¡å‡å°‘åˆ°å‡ ä¹ä¸ºé›¶ï¼ä¾‹å¦‚ï¼Œè¿™äº›æŠ€æœ¯å·²åœ¨ DeepSeek V3/R1 å®ç°[^7]ä¸­ä½¿ç”¨ã€‚å‹¾èµ·ä½ çš„å¥½å¥‡å¿ƒäº†å—ï¼Ÿåœ¨æˆ‘ä»¬ç¦»å¼€æµæ°´çº¿å¹¶è¡Œä¸–ç•Œä¹‹å‰ï¼Œè®©æˆ‘ä»¬æœ€åå¿«é€Ÿçœ‹ä¸€çœ¼è¿™äº›ç¥å¥‡çš„è°ƒåº¦æ–¹æ¡ˆï¼

### 6.4 é›¶æ°”æ³¡å’ŒåŒç®¡

æœ€è¿‘æå‡ºäº†æ›´ä¸ºå¤æ‚çš„æ–¹æ³•æ¥å‡å°‘æ°”æ³¡ï¼Œè¿™äº›æ–¹æ³•å‡ ä¹è¾¾åˆ°äº†â€œé›¶æ°”æ³¡â€çŠ¶æ€ã€‚è¿™é‡Œçš„ç§˜è¯€æ˜¯åœ¨æ›´ç»†ç²’åº¦çš„å±‚é¢ä¸Šæ‹†åˆ†ç›¸å…³æ“ä½œï¼Œä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼å°†å®ƒä»¬äº¤é”™æ‰§è¡Œã€‚ä¾‹å¦‚ï¼ŒDeepSeek V3/R1 ä¸­çš„ç®¡é“å®ç°æ–¹æ³•ï¼Œç§°ä¸º DualPipeï¼Œå‡ ä¹è¾¾åˆ°äº†é›¶æ°”æ³¡çŠ¶æ€ã€‚

ï¼ˆDeepSeek V3 æŠ€æœ¯æŠ¥å‘Š[^7]ä¸­çš„ç»ˆæâ€œçµæ´»é…ç½®â€ï¼Œä½œè€…åœ¨æŠ¥å‘Šä¸­æŒ‡å‡ºä»–ä»¬çš„è®¾ç½®â€œå®ç°äº†è¿‘ä¹ä¸ºé›¶çš„å…¨äº’è”é€šä¿¡å¼€é”€â€ã€‚ï¼‰

è®©æˆ‘ä»¬ç®€è¦åœ°é€šè¿‡æ€»ç»“ä½œä¸º DualPipe å‰èº«çš„ ZeroBubble[^8] çš„å·¥ä½œæ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚ ZeroBubble çš„åŸºæœ¬è§‚å¯Ÿç»“æœæ˜¯ï¼ŒçŸ©é˜µä¹˜æ³•çš„åå‘ä¼ æ’­å®é™…ä¸Šæ¶‰åŠä¸¤ä¸ªåˆ†ç¦»çš„æ“ä½œï¼šè¾“å…¥ï¼ˆBï¼‰çš„åå‘æ“ä½œå’Œæƒé‡ï¼ˆWï¼‰çš„åå‘æ“ä½œã€‚

è™½ç„¶è¾“å…¥çš„åå‘ä¼ æ’­ï¼ˆå³ B çš„è¾“å‡ºï¼‰å¯¹äºæ‰§è¡Œè¾ƒä½å±‚çš„åå‘ä¼ æ’­æ˜¯å¿…è¦çš„ï¼Œä½†æƒé‡çš„åå‘ä¼ æ’­ï¼ˆå³ W çš„åå‘ä¼ æ’­ï¼‰å¯¹äºå…¶ä½™çš„åå‘ä¼ æ’­è¿‡ç¨‹å¹¶éå¿…è¦ï¼Œå¹¶ä¸”é€šå¸¸åªéœ€åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰æ‰§è¡Œã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

![image.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_compgraph.png)

è¿™æ„å‘³ç€ W å¯ä»¥çµæ´»åœ°å®‰æ’åœ¨åŒä¸€é˜¶æ®µçš„ç›¸åº” B ä¹‹åçš„ä»»ä½•ä½ç½®ã€‚è¿™ä½¿å¾—å¯ä»¥ç­–ç•¥æ€§åœ°æ”¾ç½® W ä»¥å¡«å……æµæ°´çº¿æ°”æ³¡ã€‚å³ä¸Šè§’çš„ ZB-H2 æ—¶é—´è¡¨æ˜¯åˆ©ç”¨è¿™ç§ç»†ç²’åº¦åˆ†è§£å®ç°é›¶æ°”æ³¡çš„ï¼ˆç†è®ºï¼‰æ—¶é—´è¡¨ç¤ºä¾‹ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_ppschedule.png)

*åœ¨é¡¶éƒ¨ï¼ˆZeroBubble è®ºæ–‡ä¸­çš„å›¾ 2ï¼‰ï¼šç»å…¸çš„ 1F1B è°ƒåº¦ï¼Œäº¤é”™è¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œä½†ä¿æŒç²—ç²’åº¦çš„åå‘ä¼ æ’­ã€‚åœ¨ä¸‹é¢çš„ä¸¤ä¸ªå›¾ä¸­ï¼ˆZeroBubble è®ºæ–‡ä¸­çš„å›¾ 3ï¼‰ï¼ŒZeroBubble è°ƒåº¦çš„ä¸¤ä¸ªå˜ä½“ï¼Œå°†åå‘æ“ä½œæ‹†åˆ†ä¸ºæ›´ç»†ç²’åº¦çš„ â€œBâ€ å’Œ â€œWâ€ æ“ä½œã€‚æœ€åä¸€ä¸ªè°ƒåº¦ï¼Œç§°ä¸º â€œZB-H2â€ï¼Œæ˜¯ä¸€ä¸ªï¼ˆç†è®ºä¸Šï¼‰åˆ©ç”¨è¿™ç§ç»†ç²’åº¦åˆ†è§£å®ç°é›¶æ°”æ³¡çš„è°ƒåº¦ç¤ºä¾‹ã€‚*

DeepSeek çš„ DualPipe åœ¨å…¶ V3 æŠ€æœ¯æŠ¥å‘Š[^7]ä¸­ä»‹ç»äº†è¿™ç§åˆ†è§£æ–¹æ³•çš„æ‰©å±•ï¼Œå³é’ˆå¯¹ä» PP ç»´åº¦çš„ä¸¤ç«¯ä¼ æ’­çš„ä¸¤ä¸ªæµçš„æƒ…å†µï¼Œè¿™äº›æµäº¤é”™æ’åˆ—ï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘ GPU ä¸­çš„ç©ºé—²æ—¶é—´ã€‚è¯¥è°ƒåº¦æ–¹æ¡ˆæ˜¾ç¤ºåœ¨ä¸‹é¢çš„è°ƒåº¦å›¾ä¸­ï¼Œæ¯”ä¹‹å‰çš„æ–¹æ¡ˆæ›´ä¸ºå¤æ‚ã€‚

![image.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/pp_zerobubble_dualpipe.png)

ä¸€èˆ¬æ¥è¯´ï¼Œè¦å®Œå…¨ä¼˜åŒ–è¿™ç§å¤æ‚çš„è°ƒåº¦å®‰æ’ï¼Œéœ€è¦ä»”ç»†æµ‹é‡å„ç§ç»†ç²’åº¦æ“ä½œçš„æŒç»­æ—¶é—´ï¼Œå¹¶æ±‚è§£ä¸€ä¸ªæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆILPï¼‰é—®é¢˜ä»¥ä½¿æœ€ç»ˆçš„æ°”æ³¡æ—¶é—´æœ€å°åŒ–ã€‚ä¾‹å¦‚ï¼Œå¯å‚è€ƒ ZeroBubble è®ºæ–‡[^8]ä¸­å¯¹æ‰§è¡Œæ­¤ç±»è°ƒåº¦æ‰€é‡‡ç”¨çš„å¯å‘å¼ç®—æ³•å’Œç®—æ³•çš„è®¨è®ºã€‚å› æ­¤ï¼ŒZeroBubble å’Œ DualPipe çš„è°ƒåº¦å®‰æ’è¿‡äºå¤æ‚ï¼Œåœ¨æ­¤æ— æ³•ç»™å‡ºä»£ç ç‰‡æ®µï¼Œä½†ä½ åº”è¯¥å¼€å§‹å¯¹æ‰€æ¶‰åŠçš„æ¦‚å¿µæœ‰ä¸€ä¸ªå¤§è‡´çš„äº†è§£ã€‚

è¿™å°±ç»“æŸäº†æˆ‘ä»¬å¯¹ç®¡é“è°ƒåº¦å’Œæ°”æ³¡ä¸–ç•Œçš„å‚è§‚ã€‚å¸Œæœ›æ‚¨å–œæ¬¢è¿™æ¬¡å¯¼è§ˆä¹‹æ—…ï¼

ç°åœ¨æ˜¯æ—¶å€™æ¢è®¨æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»çš„æœ€åä¸€ç§å¹¶è¡Œæ–¹æ³•äº†ï¼Œå³æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥é«˜æ•ˆè®­ç»ƒå¤§å‹æ¨¡å‹çš„æ–¹æ³•ï¼š*ä¸“å®¶å¹¶è¡Œ*ã€‚

## ä¸ƒã€ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰

è¿™æ˜¯æˆ‘ä»¬è¦è®¨è®ºçš„æœ€åä¸€ä¸ªå¹¶è¡Œæ–¹æ³•ã€‚åœ¨æ¢è®¨å®ƒä¹‹å‰ï¼Œå¦‚æœæ‚¨å¯¹ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMixture-of-Expertsï¼‰æ²¡æœ‰ä»»ä½•äº†è§£ï¼Œæ¬¢è¿é˜…è¯»æˆ‘ä»¬ä¹‹å‰å‘å¸ƒçš„ä¸€ç¯‡è¾ƒçŸ­çš„åšå®¢æ–‡ç« ï¼Œè¿™ç¯‡æ–‡ç« åº”è¯¥èƒ½å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æ¶æ„ã€‚

ä¸“å®¶æ··åˆæ¨¡å‹è¿‘æœŸå›  GPT - 4ã€Mixtral[^9] ï¼Œä»¥åŠæ›´è¿‘æœŸçš„ DeepSeek-V3/R1 ç­‰æ¨¡å‹è€Œå—åˆ°å…³æ³¨å¹¶å´­éœ²å¤´è§’ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸€å±‚è®¾ç½®å¤šä¸ªå¹¶è¡Œæ¨¡å—ï¼Œè€Œéå•ä¸ªå‰é¦ˆæ¨¡å—ï¼Œå¹¶å°†ä»¤ç‰Œé€šè¿‡å…¶ä¸­ä¸€ä¸ªæˆ–å¦ä¸€ä¸ªæ¨¡å—è¿›è¡Œè·¯ç”±ï¼Œä»¥è¿›è¡Œä¸åŒçš„å¤„ç†ã€‚

![ep_moe.png|600](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ep_moe.png)
*å–è‡ª Switch Transformers è®ºæ–‡[^10]çš„ MoE å±‚ç¤ºæ„å›¾*

MoE å±‚çš„è®¾è®¡å®é™…ä¸Šä½¿å¾—è·¨ä¸“å®¶ç»´åº¦å®ç°å¹¶è¡Œæ€§å˜å¾—éå¸¸å®¹æ˜“ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºä¸“å®¶å¹¶è¡Œæ€§ï¼ˆEPï¼‰ã€‚ç”±äºå‰é¦ˆå±‚æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ¯ä¸ªä¸“å®¶çš„å‰é¦ˆå±‚æ”¾åœ¨ä¸åŒçš„å·¥ä½œå™¨ä¸Šã€‚ä¸ TP ç›¸æ¯”ï¼Œå®ƒè¦è½»é‡å¾—å¤šï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æ‹†åˆ†çŸ©é˜µä¹˜æ³•ï¼Œåªéœ€è¦å°† tokens çš„éšè—çŠ¶æ€è·¯ç”±åˆ°æ­£ç¡®çš„ä¸“å®¶å³å¯ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰é€šå¸¸ä¼šä¸å…¶ä»–å¹¶è¡Œå½¢å¼ï¼ˆä¾‹å¦‚æ•°æ®å¹¶è¡Œï¼‰ç»“åˆä½¿ç”¨ã€‚è¿™æ˜¯å› ä¸ºä¸“å®¶å¹¶è¡Œä»…å½±å“æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å±‚ï¼Œå¹¶ä¸”ä¸ä¼šå¯¹è¾“å…¥æ ‡è®°è¿›è¡Œåˆ†ç‰‡ï¼ˆä¸åƒä¸Šä¸‹æ–‡å¹¶è¡Œé‚£æ ·æ²¿åºåˆ—é•¿åº¦ç»´åº¦å¯¹æ ‡è®°è¿›è¡Œåˆ†ç‰‡ï¼‰ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬ä»…ä½¿ç”¨ä¸“å®¶å¹¶è¡Œï¼Œæˆ‘ä»¬çš„å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰å°†å¯¹æ‰€æœ‰é MoE å—æ‰§è¡Œå†—ä½™è®¡ç®—ã€‚é€šè¿‡å°†ä¸“å®¶å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œç›¸ç»“åˆï¼Œæˆ‘ä»¬å¯ä»¥åƒä¸‹é¢çš„ç®€åŒ–ç¤ºæ„å›¾ä¸­æ‰€ç¤ºï¼Œæœ‰æ•ˆåœ°åœ¨ GPU ä¹‹é—´å¯¹ä¸“å®¶å’Œè¾“å…¥æ‰¹æ¬¡è¿›è¡Œåˆ†ç‰‡ ã€‚

![ep_schema.png|650](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ep_schema.png)
*æ¥æºï¼šã€Šä¸“å®¶æ··åˆæ¨¡å‹ç»¼è¿°ã€‹[^11]*

ä½†å…ˆåˆ«é«˜å…´å¾—å¤ªæ—©â€”â€”åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å…·ä½“æ¢è®¨ä¸åŒå¹¶è¡Œç­–ç•¥ä¹‹é—´çš„æ‰€æœ‰äº¤äº’ï¼Œæ‰€ä»¥å¦‚æœä½ è¿˜æ²¡ç†è§£è¿™æœ€åä¸€å¼ å›¾ï¼Œä¹Ÿåˆ«æ‹…å¿ƒã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¦ä½¿ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰é«˜æ•ˆè¿è¡Œæœ‰ä¸€äº›æŠ€å·§ï¼Œå¹¶ä¸”è¿™äº›æŠ€å·§ä¸æ¨¡å‹è®¾è®¡å¯†åˆ‡ç›¸å…³ã€‚ä¾‹å¦‚ï¼ŒDeepSeek-V3 åœ¨è·¯ç”±å™¨ä¸­æ–½åŠ äº†ä¸€ç§çº¦æŸï¼Œç¡®ä¿æ¯ä¸ª token æœ€å¤šè¢«å‘é€åˆ° M ä¸ªèŠ‚ç‚¹ï¼ˆåœ¨ä»–ä»¬çš„æ¡ˆä¾‹ä¸­ï¼ŒM ä¸º 4ï¼‰ï¼Œä»¥å°† token ä¿ç•™åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå¹¶å‡å°‘é€šä¿¡å¼€é”€ã€‚è™½ç„¶ä¸“å®¶å¹¶è¡Œå·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´äº†[^12]ï¼Œä½†éšç€æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå®ƒç°åœ¨æ‰é‡æ–°å—åˆ°é‡è§† ã€‚

æˆ‘ä»¬è®¡åˆ’åœ¨ picotron/nanotron ä¸­å°½å¿«æ·»åŠ ä¸€ä¸ªæ›´å®Œæ•´çš„ EP ç¤ºä¾‹ï¼Œæ•¬è¯·å…³æ³¨æ›´å¤šå†…å®¹ï¼

## å…«ã€5D å¹¶è¡Œæ€§æ¦‚è¿°

æ­å–œè¯»è€…ï¼Œæ‚¨ç°åœ¨å·²ç»äº†è§£äº†å¯ç”¨äºæ‰©å±•æ¨¡å‹è®­ç»ƒçš„æ‰€æœ‰ 5 ç§å¹¶è¡Œç­–ç•¥ï¼š

1. DP â€”â€”æ²¿æ‰¹æ¬¡ç»´åº¦
2. TP â€”â€”æ²¿éšè—ç»´åº¦
3. SP/CP â€”â€”æ²¿åºåˆ—ç»´åº¦
4. PP â€”â€”æ²¿æ¨¡å‹å±‚
5. EP â€”â€”æ²¿æ¨¡å‹ä¸“å®¶

ä»¥åŠ 3 ç§ ZeRO ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä¸æ•°æ®å¹¶è¡Œæ€§ç›¸ç»“åˆä»¥å‡å°‘å†…å­˜å ç”¨ï¼š

1. ZeRO-1 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡
2. ZeRO-2 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦è¿›è¡Œåˆ†ç‰‡
3. ZeRO-3 â€“ åœ¨ DP å‰¯æœ¬é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°è¿›è¡Œåˆ†ç‰‡

At this stage, one aspect you are probably curious about is how all these parallelism and ZeRO strategies compare to, and interact with, each other. In other words, which ones should we use and efficiently combine together, and which ones should we rather keep separated?

Letâ€™s take a look at the similarities and interplay. We'll start by comparing Pipeline parallelism are ZeRO-3 side-by-side as they have some very close similarities but also important differences.

**Pipeline parallelism vs. ZeRO-3 -**Â Both PP and ZeRO-3 are ways to partition the model weights over several GPUs and perform communication/computation along the model depth axis (for example in ZeRO-3, we prefetch the next layer while computing). This means in both cases full layer operations are computed on each device, as opposed to TP or EP for instance in which computation are performed on sub-layer units.

In the following we say â€œa layerâ€ to simplify what should be in general called â€œa set of layerâ€ (as the basis sharding unit of the model).

However, there are a few major differences between PP and ZeRO-3 approaches:

||**ZeRO-3**|**Pipeline Parallelism**|
|---|---|---|
|Each compute unit stores|only a fraction of a layer|a full layer|
|Communication is used to transfer|weights|activations|
|Orchestration|model agnostic|model agnostic|
|Implementation challenges|Complex to handle model partitioning and communications|Complex to handle efficient PP schedules|
|Scaling considerations|Prefers largeÂ mbsmbsÂ andÂ seq_lenseq_lenÂ to hide comms|Prefers largeÂ grad_accgrad_accÂ to hide bubble|

As you can see, ZeRO-3 and PP solve the same challenge but involve different approaches and the choice between both will depend whether you decide to focus communication either on weights or on activations. While they can be combined, it's not often done in practice as doing so requires increasing the global batch size significantly to amortize the communication costs, creating a tradeoff between global batch size, model size, network bandwidth, and training efficiency. If you decide to combine them, ZeRO-3 should be configured to keep the weights in memory during the series of PP micro-batches to minimize as much as possible un-necessary communication overhead.

On the other hand, ZeRO-1 and ZeRO-2, which focus on optimizer states and gradients, can be easily combined with Pipeline Parallelism and are complementary to it. Combining them don't raise any particular new challenge. For instance, the training of DeepSeek-v3 used PP combined with ZeRO-1 (sic).

**Tensor Parallelism**Â (with Sequence Parallelism) is naturally complementary and can be combined with both Pipeline Parallelism and ZeRO-3 as it relies on the distributive property of matrix multiplications which allows weights and activations to be sharded and computed independently before being combined.

![TP & SP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_tp_sp.svg)

The main reason we don't want to use TP only for parallelism is that, in practice, TP has two limitations we've discussed in the previous sections: First, since its communication operations are part of the critical path of computation, it's difficult to scale well beyond a certain point at which communication overhead begins to dominate. Second, unlike ZeRO and PP which are model-agnostic, TP requires careful handling of activation sharding - sometimes along the hidden dimension (in the TP region) and sometimes along the sequence dimension (in the SP region) - making it more cumbersome to implement correctly and requiring model-specific knowledge to ensure proper sharding patterns throughout.

As a consequence, when combining parallelism strategies, TP will typically be kept for high-speed intra-node communications while ZeRO-3 or PP can be used for parallelism groups spanning lower speed inter-node communications as their communication patterns require less bandwidth (for PP) or can be more easily overlapped with computation (for ZeRO-3). The main consideration when combining these techniques is to organize the GPU efficiently in groups for each parallelism dimension to maximize throughput and minimize communication overhead, while being mindful of TP's scaling limitations. For instance, the groups of GPUs communicating for TP should be kept inside nodes.

**Context Parallelism**Â andÂ **Expert Parallelism**Â also help us shard activations, and can be seen as complimentary to TP. The first one handles long sequences while the second enables distributed Mixture of Experts training and they can be combined together without any particular issue.

**Context Parallelism (CP)**Â specifically targets the challenge of training with very long sequences by sharding activations along the sequence dimension across GPUs. While most operations like MLPs and LayerNorm can process these sharded sequences independently, attention layers require communication since each token needs access to keys/values from the full sequence. As we saw inÂ [CP section](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#context_parallelism), this is handled efficiently through ring attention patterns that overlap computation and communication. CP is particularly valuable when scaling to extreme sequence lengths (128k+ tokens) where, even when using full activation recomputation, the memory requirements for attention would be prohibitive on a single GPU.

![CP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_cp.svg)

**Expert Parallelism (EP)**Â specifically targets the challenge of training Mixture of Experts (MoE) models by sharding specialized "experts" across GPUs and dynamically routing tokens to relevant experts during computation. The key communication operation in EP is the `all-to-all` operations routing tokens to their assigned experts and gathering the results back. While this operation introduces some communication overhead, it enables scaling model capacity significantly since each token is only processed during inference (and training) by a much smaller fraction of the total parameters. In terms of distributed training/inference, partitioning experts across GPUs becomes relevant when models scales to a large number of experts.

For instance DeepSeek V3 uses 256 experts.

![EP diagram](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_nutshell_ep.svg)

ğŸ“ Note

This similarity between EP and DP in terms of input handling is why some implementations consider Expert Parallelism to be a subgroup of Data Parallelism, with the key difference being that EP uses specialized expert routing rather than having all GPUs process inputs through identical model copies.

**Scope and focus**Â Let's also quickly summarize the sub-part of the model where some of these different parallelism strategies have the most impact:

- Tensor Parallelism (and Sequence Parallelism) affects computation throughout the entire model by sharding both weights and activations.
- Context Parallelism primarily impacts attention layers since that's where cross-sequence communication is required, with other layers operating independently on sharded sequences.
- Expert Parallelism primarly affects the MoE layers (which replace standard MLP blocks), leaving attention and other components unchanged
- Pipeline Parallelism and ZeRO are not especially specific to any sub-module or component with the exception that modules and layers need to be balanced in Pipeline Parallelism, the first and last layers are thus often treated differently due to the additional embedding layers.

|**Tensor + Sequence Parallel**|**Context Parallel**|**Expert Parallel**|
|---|---|---|
|shards weights and activations along hidden/seq dim|shards activations along sequence dim|shards specialized expert weights and activations|
|communication for matrix multiply operations (column/row linears)|communication for attention key/values|communication for token routing to experts|
|model-specific implementation needed|model-agnostic except for attention|model-agnostic except for MoE layers|
|Prefers high-bandwidth intra-node communication|Prefers large sequence lengths|Requires MoEs|

**Summarizing it allâ€“**Â Now what about gathering and combining all the techniques we've seen in a single diagram combining them all. Yes, we're up for the challenge!

In this summary diagram, you will find illustrated activations and modules for a single transformers layer â€“in it's MoE variantâ€“. We also illustrate the various directions of parallelism and the communication operations we've been discussing in all the previous sections.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5d_full.svg)

We can also represent side-by-side aÂ **full overview**Â of the memory savings for each one of these strategies. We'll plot them with different sequence length as well as with selective (top) and full (bottom) recomputation so you can see how they all play with activations:

![5Dparallelism_8Bmemoryusage.svg](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/5Dparallelism_8Bmemoryusage.svg)

Let's finish this section with a high level view at all of these techniques, their main underlying idea and major bottleneck:

|**Method**|**Memory savings applies specifically on**|**Parallel/sharding dimension**|**Disadvantage**|
|---|---|---|---|
|DP|Activations (reduce local batch size)|Batch|Limited by max batch size|
|PP|Model parameters|Model layers|Idle bubble and complex schedules|
|TP/SP|Model parameters and activations|Hidden dimension / Sequence length|Requires high bandwidth communication|
|CP|Activations|Sequence length|Add communication overhead in attention modules|
|EP|Experts parameters|Expert dimension|Requires MoE layers, add routing communication overhead|
|ZeRO-1|Optimizer states|Sharded among DP replicas|Params communication overhead|
|ZeRO-2|Optimizer states and gradients|Sharded among DP replicas|Params communication overhead|
|ZeRO-3|Optimizer states, gradients, and model parameters|Sharded among DP replicas|Params communication overhead|

Clearly, none of these techniques is a silver bullet for magical scaling and we'll often have to combine them in one way or another. Can we actually come up with a few rules that would help us find a good starting point to choose among â€“and combineâ€“ them? This will be the topic of our next section.

## Finding the Best Training Configuration

Weâ€™ve now covered all the parallelism techniques that are actually used to distribute and train larger models as well as how and why they can be combined together. There remain a general question: which ones should we choose in the end and how to decide on a specific combination?

We touched this a little bit in the previous section but let's now walk in details through a possible decision process, step by step, keeping in mind that you'll always have to run a few experiments to find the definitive optimal setup for your compute cluster given its various physical properties, network bandwidth, GPUs per node, memory per GPU, etc.

### Step 1: Fitting a Training Step in Memory

First, we need to figure out how we can fit a full model instance on our GPUs. There are two general cases.

**GPU-rich case ğŸ¤‘**Â - when you have plenty of GPUs available:

- For models under 10B parameters, you can use a single parallelism technique, e.g. Tensor Parallelism or ZeRO-3/DP with Full Recompute across 8 GPUs
- For models between 10B-100B parameters requiring more than 8 GPUs, you have several options:

- Combining Tensor Parallelism (TP=8) with Pipeline Parallelism
- Combining Tensor Parallelism (TP=8) with Data Parallelism (ZeRO-3)
- Using only ZeRO-3 (i.e. only pure Data Parallelism)

- At 512+ GPU scale, pure Data Parallelism/ZeRO-3 will start to becomes inefficient due to communication cost - it can be better to then combine DP with either Tensor or Pipeline Parallelism
- At 1024+ GPU scale, a recommended setup can be Tensor Parallelism TP=8 with Data Parallelism (ZeRO-2) and Pipeline Parallelism

We focus on fitting a single instance for now - even though we may use DP for ZeRO to achieve this goal - we're only interested here in the model-parameters memory savings that it provide when used with ZeRO-3.

Special considerations:

- For very long sequences, you will probably want to add Context Parallelism (CP) across nodes.
- For Mixture of Experts architectures, you will advantageously use Expert Parallelism (EP) across nodes.

**GPU-poor case ğŸ˜­**Â - when you might be low on GPU resources:

- You can enable full activation recomputation to trade some compute for memory (and train a bit slower).
- You can increase gradient accumulation to process larger batches with limited memory.

Now that we have a first model instance training, we need to make sure we have the right batch size.

### Step 2: Achieving Target Global Batch Size

Depending on where step 1 left us in terms of micro batch size and DP, our current batch size might be too small or too big. It's now time to hit our target batch size.

To increase our current global batch size:

- We can scale up Data Parallelism or gradient accumulation steps
- For long sequences, we can leverage Context Parallelism

To decrease our current global batch size:

- We can reduce Data Parallelism in favor of other parallelization strategies
- For long sequences, we can reduce Context Parallelism

Ok, now we have the model running in the general configuration we want in terms of model size and batch size, but are we training it the fastest way? Let's now start to optimize throughput as much as possible.

### Step 3: Optimizing Training Throughput

So we want to make sure the training is running as fast as possible so all our precious GPUs are well utilized at all times. As long as memory and communication aren't bottlenecks we can try the following:

- Scale up Tensor Parallelism (using the fast intra-node bandwidth) until we reach a degree close to the node size, so that we can reduce other parallelism
- Increase Data Parallelism with ZeRO-3 while keeping target batch size
- When Data Parallelism communication starts to become a bottleneck, transition to using Pipeline Parallelism
- Try scaling up different parallelisms one by one
- Experiment with several micro batch size (mbs) to aim for an optimal balance between max GBS, model size, compute, and communication.

### Benchmarking thousands of configurations

Now that we've covered the step-by-step, let's implement this search process in real-life.

You will find, in theÂ [nanotron](https://github.com/huggingface/nanotron)Â repository, several scripts you can use to run all the experiments we discussed above and be able to benchmark your own model and cluster in real life.

We actually ran ourself benchmarks onÂ **several thousands of distributed configurations**Â covering every model size we've discussed above as well as a very large number of cluster configurations (namely 1-64 nodes of 8xH100s) we could try in order to produce the results we've covered up to now in this book.

We want to take this opportunity to apologize to our co-workers for blocking most of the science cluster and in turn forgive any threats that may have been whispered.

Now let's take a step back to gather and analyze the results of all our benchmarks and see if, beyond theory, we can actually discover on real-world data how various configurations fare against each other.

All the following benchmarks were conducted with a sequence length of 4096 and a global batch size of 1M tokens. We gathered all the top configurations for each model and cluster size and plotted them in the following heatmaps:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/what_we_learnt_heatmap.svg)

Heatmap visualization showing the optimal training configurations across different model sizes and compute node counts (we have 8 GPUs per node). For each combination, the configuration details include Data Parallelism (DP), Tensor Parallelism (TP), Pipeline Parallelism (PP), Gradient Accumulation Steps (GAS), Micro Batch Size (MBS), and ZeRO optimization stage. The color intensity indicates the Model FLOPs Utilization (MFU), with brighter colors representing higher efficiency.

From this high-level visualization, we can draw several important insights:

First, as we increase the number of nodes (higher parallelism), we observe a decrease in efficiency. This effect is particularly pronounced for smaller models, which have a lower compute-to-model-size ratio. While we might typically compensate for small model size by increasing the batch size, we're constrained by our global batch size limit of 1M.

Second, Larger models present a different challenge. As model size increases, memory requirements grow substantially. This creates two scenarios with fewer nodes: either the model doesn't fit at all, or it barely fits but runs inefficiently due to operating near the GPU memory limits (see for instance the 80B parameter model training on 4 nodes).

Finally, our benchmarks show how performance heavily depends on implementation quality. When we first implemented both parallelism strategies, Tensor Parallelism (TP) outperformed Pipeline Parallelism (PP). After optimizing our PP code, it became the faster option. Now that we're improving the communication overlap in our TP implementation, we expect it to regain the performance lead.

### Lessons learned on benchmarking

Our goal for this book was not only to discuss theory and implementations but provide actual data points as well. So the plan was simple: let's run every possible distributed configuration for every model and a number of cluster sizes (namely 1-64 nodes of 8xH100s). Even after excluding impossible configuration we still needed to run thousands of experiments.

On paper this sounds easy enough: we can easily launch big arrays of jobs on our cluster. However, as soon as we launched the first batches of experiments, troubles began:

- PyTorch processes would sometimes fail to clean up properly
- Slurm job manager would forcefully terminate jobs, leading to node failures
- Simple benchmarks that should take minutes would stretch into hours
- Some jobs would hang indefinitely

Running all experiments in a finite amount of time required additional engineering and we ended up spending a significant amount of time on things like:

- Minimizing cluster restart times and optimize idle time
- Analyzing detailed NCCL debug logs
- Understand memory usage patterns and CUDA memory allocator behaviors
- Improving pipeline parallelism performance on multi-node

These challenges deserve their own story, but they taught us valuable lessons about the complexities of distributed training infrastructure. What looks simple in theory often requires careful attention to many moving parts in practice.

Reproducing theoretical results in practice is challenging, especially given the limited availability of production training code. Through open-source projects likeÂ [nanotron](https://github.com/huggingface/nanotron)Â andÂ [picotron](https://github.com/huggingface/picotron), we hope we can help making distributed training techniques more accessible as well as collaborating on simple and efficient codebases that help researchers and practitioners take the most out of their hardware resources.

---

This concludes our very deep dive into the distribution methods of 5D parallelism.

Taking a step back, our discussion so far has often relied on a critical assumption - that computation and communication can be efficiently overlapped on GPUs without any impact on the computation throughput. The reality is more nuanced. When using common communication primitives like NCCL send/recv, we face hidden contention between computation and communication resources as communication kernels will usually make use of the same GPU streaming multiprocessors (SMs) that are used for computation, leading to decreased throughput when communication is overlapped with computation. To truly optimize our distributed training, we need to dive deeper into the GPU architecture itself.

Additionally, the synchronization patterns when overlapping computation and communication may not always be optimal for our parallel strategies. You can find an example for instance inÂ [this blog post](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487)Â by the Pytorch team.

Time to turn the lights off and activate CUDA mode!

## Diving in the GPUs â€“ fusing, threading, mixing

To add a podcast feeling to your reading experience, feel free to listen to the NotebookLM hosts discussing the following sections of this book as you're reading along.

Up to now our discussion has been focused on the high-level organization of our model operations. Weâ€™ve moved around computations on various accelerators, taking into account general memory constraints and high-level scheduling of the compute units.

But this ignored all the optimizations we can do at a much lower level by carefully understanding how our model operations are scheduled and performed on each GPU.

This section will dive into much more details of the GPU architecture and in particular in NVIDIAâ€™s GPU architecture but the general ideas, as often, can be reused on similar accelerator units.

Weâ€™ll briefly explain how GPU are organized before covering the Flash-Attention revolution, how to efficiently schedule workload on GPU and finally explain how various precisions can be efficiently used on GPU.

### A primer on GPU

Generally, GPUs have a very hierarchical organization. In this primer weâ€™ll keep the discussion at the concept levels that are necessary for the rest of our presentation.

On the compute side, GPUs consist of an array of compute units calledÂ **Streaming Multiprocessors**Â (SM). Each SM contains and controls a set of streaming processors, also known as cores. For example, an Nvidia H100 GPU has 132 SMs with 128 cores per SM, resulting in a total of 16,896 cores (seeÂ [docs for tensor cores](https://resources.nvidia.com/en-us-tensor-core)Â for details), each capable of handling multiple threads simultaneously.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/diving_primergpu.svg)

Source: https://blog.codingconfessions.com/p/gpu-computing

The memory side is also highly hierarchical with several layers of cache and memory:Â **Registers**Â are the smallest units and are private to the threads during executions,Â **Shared Memory**Â andÂ **L1 cache are**Â shared between the threads running on a single SM, higher up is theÂ **L2 cache**Â shared by all SMs, finally there is theÂ **Global Memory**Â which is the largest memory on the GPU (the advertised 80 GB for a H100 for instance) but also the slowest to access and query.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/diving_primergpu2.svg)

Source: https://www.youtube.com/watch?v=ZQKMZIP3Fzg

The goal of GPU will be to run as many workloads as possible, in parallel, on the GPU cores, by taking advantage of this hierarchical organization of compute/memory.

A piece of code running on a core of the GPU is called aÂ **kernel**. It can be written at a high-level inÂ **CUDA**Â orÂ **Triton**Â for instance, and is then compiled to Parallel Thread Execution, PTX, the low-level assembly used by NVIDIA GPUs.

To run the kernel, you will also need a specific code part, calledÂ **host code**, which is executed on theÂ **CPU/host**Â and will take care of preparing data allocations and loading data and code.

```python
// Host code                
void vecAdd(float* h_A, float *h_B, float *h_c, int n) {
    // Allocate vectors in device memory
    int size = n * sizeof(float);
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
}
```

Host code for a CUDA kernel for adding two vectors. Adapted from https://docs.nvidia.com/cuda/cuda-c-programming-guide/ and https://blog.codingconfessions.com/p/gpu-computing

```python
// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N)
        C[i] = A[i] + B[i];
}
```

Device code containing the definition of the vector addition kernel adapted from https://docs.nvidia.com/cuda/cuda-c-programming-guide/ and https://blog.codingconfessions.com/p/gpu-computing

Kernels are generally scheduled as follow:

- threads are grouped inÂ **warps**Â of sizes of 32. All the threads in a warp are synchronized to execute instructions simultaneously but on different parts of the data.
- **warps**Â are grouped in largerÂ **blocks**Â of more flexible size (e.g. size 256), each block still being assigned to a single SM. An SM may run several blocks in parallel, however, depending on the resources, not all the blocks may get assigned for execution immediately, some can be waitlisted waiting for resources.

The main thing to remember from these details is that there are various sizing and allocation constraints (size of the various memories, number of concurrent block and threads in the wraps) which need to be taken into account to use the GPU architecture in the most efficient way.

Most of the time you donâ€™t need to go down to this level of precision and you can luckily reuse the kernels and code prepared by other members of the community. But in any case we want to give you a primer on how to get started with kernels!

### How to improve performance with Kernels ?

If youâ€™re looking to add a new operation that lacks an optimized kernel or to speed up an existing PyTorch function, writing kernels from scratch might seem like the most direct route. However, creating high-performance CUDA kernels from scratch requires extensive experience and a steep learning curve. Generally a better way to get started is to leverageÂ `torch.compile`, which dynamically optimizes PyTorch code by capturing your operations and generating lower-level, high-performance kernels in triton.

Letâ€™s suppose you want to write a kernel for an activation function called Exponential Linear Unit:

ELU(x)={exâˆ’1ifÂ x<0xifÂ xâ‰¥0ELU(x)={exâˆ’1xâ€‹ifÂ x<0ifÂ xâ‰¥0â€‹

You can start by a simple pytorch implementation and then just add theÂ `@torch.compile`Â decorator on top:

```python
@torch.compile
def elu(x, alpha=1.0):
    return torch.where(x < 0, alpha * (torch.exp(x) - 1), x)
```

The distinction between the compiled and non-compiled versions is striking, especially given that we only added a single decorator. This remarkable difference is illustrated in the graph below (N is the number of columns):

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/torch-compile-triton.png)

However, if this performance increase is insufficient, you can consider implementing Triton kernels. As a starting point, you can take a look at the triton kernel generated by @torch.compile . To do so, you simply need to set the environment variableÂ `TORCH_LOGS`Â toÂ `"output_code"`:

```bash
export TORCH_LOGS="output_code"
```

Once you run the Python script with theÂ `@torch.compile`Â decorator, it will generate and output the corresponding Triton kernel, which, in this case, is:

```python
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 100000000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), xmask)
    tmp1 = 0.0
    tmp2 = tmp0 < tmp1
    tmp3 = tl_math.exp(tmp0)
    tmp4 = 1.0
    tmp5 = tmp3 - tmp4
    tmp6 = tl.where(tmp2, tmp5, tmp0)
    tl.store(out_ptr0 + (x0), tmp6, xmask)
```

To enhance readability, we can modify the variable names, add comments, and make slight adjustments (or ask an LLM to do it for us), as demonstrated below:

```python
@triton.jit
def elu_kernel(input_ptr, output_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
    # Calculate the starting index for this block
    block_start = tl.program_id(0) * BLOCK_SIZE
    # Create an array of indices for this block
    block_indices = block_start + tl.arange(0, BLOCK_SIZE)[:]
    # Create a mask to ensure only valid indices are processed
    valid_mask = block_indices < num_elements
    # Load input values from the input pointer based on valid indices
    input_values = tl.load(input_ptr + block_indices, valid_mask)
    # Define the ELU parameters
    zero_value = 0.0  # Threshold for ELU activation
    negative_mask = input_values < zero_value
    exp_values = tl.math.exp(input_values)
    # Define the ELU output shift
    one_value = 1.0
    shifted_exp_values = exp_values - one_value

    output_values = tl.where(negative_mask, shifted_exp_values, input_values)

    # Store the computed output values back to the output pointer
    tl.store(output_ptr + block_indices, output_values, valid_mask)
```

Here,Â `tl.program_id(0)`Â provides a unique block ID, that we use to determine which section of data that block will process. Using this block ID,Â `block_start`Â calculates the starting index for each blockâ€™s section, whileÂ `block_indices`Â specifies the range of indices within that section. AÂ `valid_mask`Â ensures that only indices withinÂ `num_elements`Â are processed, safely loading the data withÂ `tl.load`. The ELU function is then applied, modifying values based on whether they're negative, and results are written back to memory withÂ `tl.store`.

When we benchmark the generated kernel usingÂ `triton.testing.Benchmark`Â we have the following performance:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/torch-compile-triton-kernel.png)

This standalone kernel even demonstrates superior performance with smaller sizes compared toÂ `@torch.compile`Â but this is likely just an artifact of the compilation time ofÂ `torch.compile`. In any case, instead of starting from scratch, remember that you can start from such generated kernels and focus your attention to optimizing its performance, saving you a lot of time in the process.

Even in Triton, sometimes, we cannot fully achieve the peak performance of the device due to the language limitations to handle low level details like shared memory and scheduling within streaming multiprocessors (SMs). Triton capabilities are restricted to blocks and scheduling of blocks across SMs. To gain an even deeper control, you will need to implement kernels directly in CUDA, where you will have access to all the underlying low-level details.

Moving down to CUDA, various techniques can be employed to improve the efficiency of kernels. We will just cover a few here: optimizing memory access patterns to reduce latency, using shared memory to store frequently accessed data, and managing thread workloads to minimize idle times.

Before we dive deeper in CUDA examples, let's summarize the tools we've seen that let us write kernel code to execute instructions on the GPU:

1. Pytorch: easy but slow
2. torch.compile: easy, fast, but not flexible
3. triton: harder, faster, and more flexible
4. CUDA: hardest, fastest, and flexiblest (if you get it right)

Letâ€™s talk about one of the most frequent technique we can use in CUDA: optimizing memory access. The global memory in GPUs (the largest memory in our above graph) has a long latency and low bandwidth in comparison to the cache which often creates a major bottleneck for most applications. Efficiently accessing data from global memory can improve performance by a lot.

#### Memory Coalescing

To effectively utilize the bandwidth of global memory, it is essential to understand its architecture. In CUDA devices, global memory is implemented using DRAM.

Memory coalescing takes advantage of how DRAM delivers data in bursts, or ranges of consecutive memory locations, whenever a memory address is accessed. Each time a DRAM location is accessed, a sequence of consecutive locations, including the requested one, is read in parallel by multiple sensors in the DRAM chip. Once read, this data can then be quickly transferred to the processor as a burst. In CUDA, coalescing uses this burst behavior to maximize memory access efficiency by ensuring that threads in a warpâ€”32 threads that execute the same instruction in lockstep (SIMD)â€”access consecutive memory locations. For instance, if thread 0 accesses location M, thread 1 accesses M + 1, thread 2 accesses M + 2, and so forth, the GPU hardware coalesces or combines these requests into one large, efficient access request for the DRAM burst, rather than handling each access individually.

Letâ€™s take the example of matrix multiplication. A simple, straightforward implementation would have each thread compute a single element of the output matrix, like this:

```clike
__global__ void matmul_naive(int M, int N, int K, const float *A, const float *B, float *C) {
    const uint x = blockIdx.x * blockDim.x + threadIdx.x;
    const uint y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < M && y < N) {
        float tmp = 0.0;
        for (int i = 0; i < K; ++i) {
            tmp += A[x * K + i] * B[i * N + y];
        }
        C[x * N + y] = tmp;
    }
}
```

Hereâ€™s an excellent visualization of the kernel from thisÂ [fantastic blogpost](https://siboehm.com/articles/22/CUDA-MMM):

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing.png)

However, when profiling this kernel with a tool likeÂ `ncu`, we can see issues, including low memory throughput and uncoalesced memory accesses.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing2.png)Â ![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing3.png)

The reason for this is that in this kernel, two threads in the same block with Thread IDsÂ `(0, 0)`Â andÂ `(1, 0)`Â (which will end up in the same warp) will both load from the same column of matrixÂ `B`Â but different rows of matrixÂ `A`. Since matrix elements are stored in row-major order (meaning row elements are in consecutive memory addresses, as shown in the figure below) threadÂ `(0, 0)`Â will loadÂ A0,0A0,0â€‹, and threadÂ `(1, 0)`Â will loadÂ A1,0A1,0â€‹Â in the first iterationÂ `i = 0`. These elements are not stored close to each other in memory, and this misalignment will be present at each iteration, thereby preventing memory accesses from being coalesced.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing4.png)

To improve the performances of our kernel we can change the way coordinatesÂ x andÂ `y`Â are calculated to the following:

```clike
const int x = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);
const int y = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);

if (x < M && y < N) {
float tmp = 0.0;
for (int i = 0; i < K; ++i) {
    tmp += A[x * K + i] * B[i * N + y];
}
C[x * N + y] = tmp;
}
```

Instead of using a 2D block, we switch to a 1D block and redefine how we determine the values ofÂ `x`Â andÂ `y`. In this new method, threads within the same warp (which have closeÂ `threadIdx.x`Â values) will share the sameÂ `x`Â value but have differentÂ `y`Â values. This means that they will load the same row of matrixÂ `A`Â but different columns of matrixÂ `B`. As a result, memory accesses can be coalesced for a row-major matrix.

When we profile our new kernel, we notice that the warning about uncoalesced memory accesses has disappeared, andÂ **the GPU's memory throughput has increased by approximately 10 times**.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/memorycoalescing5.png)

We also notice that the execution time of the kernelÂ **decreases by 10x**! Amazing.

Now let's cover another technique you will often see mentioned in the litterature:Â **tiling**.

#### Tiling

Tiling is a technique that leveragesÂ _shared memory_Â to optimize memory access patterns. As we mentioned above, the shared memory is a small, fast memory accessible by all threads within a block. It allows data to be reused by multiple threads, reducing the need to repeatedly load data from slower global memory.

In matrix multiplication for example, each thread in a block may need elements from two matrices, say A and B. If each thread independently loads the row and column it needs from global memory, we end up with many redundant loads, as multiple threads in a block will access overlapping data. Instead, we can use tiling to load a block (or tile) of A and B into shared memory just once, allowing all threads in that block to reuse the same shared data.

In the tiling approach, each iteration involves all threads within a block to cooperatively load two tilesâ€”one from matrix A and another from matrix B â€”into shared memory. Specifically, threads load a tile of matrix A (of sizeÂ `BLOCK_SIZE_M`Â byÂ `BLOCK_SIZE_K`) and a tile of matrix B (of sizeÂ `BLOCK_SIZE_K`Â byÂ `BLOCK_SIZE_N`). Once the tiles are in shared memory, the threads perform matrix multiplication on these tiles, enabling efficient computation since all necessary data is quickly accessible. The results of the tile multiplication are stored in an accumulation matrix that holds intermediate results. After each iteration, the results from the current tile multiplication are added to this accumulation matrix, continuing until all tiles from both matrices have been processed.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tiling.png)

FromÂ [https://cnugteren.github.io/tutorial/pages/page4.html](https://cnugteren.github.io/tutorial/pages/page4.html)

Let's take a look at the important parts you need to understand from the implementation:

```clike
// Set pointers to the starting elements
A += blockRow * TILE_SIZE * K; // Start at row = blockRow, column = 0
B += blockCol * TILE_SIZE; // Start at row = 0, column = blockCol
C += blockRow * TILE_SIZE * N + blockCol * TILE_SIZE; // Start at row = blockRow, column = blockCol
float sum = 0.0;
// The outer loop moves through tiles of A (across columns) and B (down rows)
for (int tileIdx = 0; tileIdx < K; tileIdx += TILE_SIZE) {
sharedA[localRow * TILE_SIZE + localCol] = A[localRow * K + localCol];
sharedB[localRow * TILE_SIZE + localCol] = B[localRow * N + localCol];

// Ensure all threads in the block have completed data loading
__syncthreads();

// Shift pointers to the next tile
A += TILE_SIZE;
B += TILE_SIZE * N;

// Compute the partial dot product for this tile
for (int i = 0; i < TILE_SIZE; ++i) {
    sum += sharedA[localRow * TILE_SIZE + i] * sharedB[i * TILE_SIZE + localCol];
}
// Synchronize again to prevent any thread from loading new data
// into shared memory before others have completed their calculations
__syncthreads();
}
C[localRow * N + localCol] = sum;
```

For simplicity we consider a square shaped tile.

Each thread begins by loading one element from bothÂ **Matrix A**Â andÂ **Matrix B**Â into shared memory. In this scenario, achieving coalesced memory access is straightforward, by assigningÂ `threadIdx.x`Â as theÂ **local column index (localCol)**, threads within the same warp will access adjacent elements of both matrices. After each thread in the block completes loading its elements into shared memory (ensured by callingÂ `__syncthreads()`), they proceed to compute the dot product of the two tiles. Once the threads have iterated through all the tilesâ€”horizontally forÂ **Matrix A**Â and vertically forÂ **Matrix B**â€”the resulting sum is stored in the corresponding location ofÂ **Matrix C**.

When benchmarking this kernel using ncu, we noticed that the memory throughput increased to 410 Gb / s, and the kernel execution time decreased by ~43% achieving a ~6.6 TFLOPs performance

#### Thread Coarsening

The tiling technique has significantly improved the performance of our kernel. However, when analyzing the warp states which quantify how many cycles were spent in each state, we observe the following:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/threadcoarsening.png)

The meaning of these cryptic state names can be found inÂ [NVidia's profiling Guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference), in theÂ **Warp Stall Reasons**Â section. There we can read that:

_`"smsp__pcsamp_warps_issue_stalled_mio_throttle`: Warp was stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure."_

So it seems warps are stalling waiting for shared memory accesses to return! To solve this issue we can apply a technique calledÂ **Thread Coarsening**Â which involves merging several threads into a single coarsened thread. This will significantly reduce shared memory accesses as each coarsened thread can handle multiple output elements.

Let's briefly go through a last important consideration when writing or improving custom kernels:Â **Minimizing Control Divergence**.

#### Minimizing Control Divergence

A Streaming Multiprocessor (SM) is built to execute all threads in a warp using the Single Instruction, Multiple Data (SIMD) model. This means that at any given moment, one instruction is fetched and executed simultaneously for all threads within the warp. When a warp is executed, the threads within it operate on different segments of the data but follow the same instruction, hence the name Single Instruction, Multiple Data. The primary advantage of SIMD is its efficiency; the control hardware responsible for instruction fetching and dispatching is shared among multiple execution units. This design minimizes the hardware overhead associated with control functions, allowing a greater portion of the hardware to focus on improving arithmetic throughput.

Control divergence occurs when threads within the same warp take different execution paths. For instance, if a conditional statement (like anÂ `if`Â statement) leads to some threads executing one block of code while others execute a different block, the warp must serialize these executions, resulting in idle threads waiting for others to complete. To minimize control divergence, we need to design kernels to ensure that threads within the same warp follow the same execution path. This can be achieved by restructuring code to reduce branching, using data structures that ensure all threads follow similar execution paths, or employing techniques such as predication.

---

We have covered some of the main considerations when writing custom kernels and improving the performance and memory footprint of GPU operations. But thereâ€™s one more important concept before moving to a real example which is â€œfusing kernelsâ€.

### Fused Kernels

In several places now weâ€™ve mentioned how GPU and CPU operation can be asynchronous. In particular, the host code on the CPU can schedule workload on the GPU in a non-blocking way.

Non-blocking can be useful for overlapping communication and computation â€“as we saw many times along our journeyâ€“ but can be extended to the more general idea of trying to avoid at all cost going back and forth between host and GPU kernel commands.

This idea is beautifully illustrated byÂ [Horace He](https://horace.io/brrr_intro.html)Â in these diagrams:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fused_kernels1.png)

A sequence of kernels requiring back and forth between global memory and compute units

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fused_kernels2.png)

Instead of sending our triangle back to global memory just to read it back again, we instead just do all of our operations in one go.

How can we avoid this back and forth? Well the best way is to make our GPU as autonomous as possible. This is achieved by packing as many successive compute operations together in a single kernel for the GPU to run, called a â€œFused Kernelâ€.

Fused kernel are especially efficient and simple to write for succession of point-like operations which are performed independently of each other on each input tokens. In this case, there is no point in bringing back computed values in Global Memory before moving them to SM memory and spinning up a new kernel. Itâ€™s much more efficient to keep all values locally until the succession of computation has been performed.

There are many places in a Transformer model where this "fusing" approach can be applied: every time we have a succession of point-wise operations e.g. in the computation involved in the Layer norms.

We now have all the understanding necessary to marvel at a true masterpiece of kernel engineering:Â **_Flash Attention_**

### Flash Attention 1-3

Flash attention was introduced byÂ [Tri Dao](https://tridao.me/)Â and proposed to optimize the attention computations by writing custom CUDA kernels make them much faster *and* more memory efficient. The idea behind Flash Attention is to make efficient use of the various memories of the GPU to avoid relying too much on the slowest one: the global memory of the GPU.

Note that the global memory of the GPU is confusingly called the "High Bandwidth Memory", HBM ğŸ« 

A basic implementation of the attention mechanism involve a lot of transfer between memory and workers. It requires materializing the S and P matrices in HBM which means that the results need to be sent to HBM and then back to SRAM for the next computations:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/flashattn.png)

Since bandwidth is much lower in HBM this introduces a severe bottleneck in the attention computation. Can we do better? Tri Dao says yes!

The key element is to compute the S matrices in small pieces which can fit in the smaller shared memory of the SM. But we can do even better and avoid materializing the very large S matrix all together in favor of keeping only the necessary statistics for computing the normalization factor of the softmax. So we can compute part ofÂ OOÂ directly in one computation in SRAM rather than moving intermediate results back and forth. In this case, not even do we make use of the shared memory but we also release the memory bottleneck resulting from materializing one of the largest activation matrices in the model (at long context length), the attention matrix.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/flashattn2.png)

Source: FlashAttention paper

[13]

The idea of flash attention resolves so many bottlenecks in model training that it has quickly become the default way to perform attention in all transformers:

- By avoiding to materialize the S matrix weÂ **reduce the memory burden of attention**
- We also remove a large part of theÂ **naive impact of the S^2 cost of attention**

As a result as well, all variants of linear attention and sub-quadratic approaches to approximate attention â€“developed shortly after the invention of the transformers architectureâ€“ have been mostly put aside in favor of this exact and fast flash attention implementation and mechanism.

Following Flash-attention 1, two successive improved versions have been released by the same lab: Flash-attention 2 and 3. In comparison to Flash-attention 1, the improvements in Flash-attention 2 and 3 are less about the general attention mechanism than about tailoring its low level implementation more specifically to the GPU by (1) reducing the number of non-matmul operations as much as possible (2) partitioning carefully the workload among wraps and thread blocks (for Flash Attention 2) and carefully optimizing for FP8 and Tensor Core support on the latest Hopper (H100) architecture for Flash Attention 3.

Flash attention puts some restrictions on which attention patterns can be sped up. Check outÂ [FlexAttention](https://pytorch.org/blog/flexattention/)Â which is a fastÂ _and_Â flexible variant.

Flash-Attention is a master demonstration of the breakthrough improvements that can come when you take into account the internal memory/compute design of current GPU accelerators.

---

The techniques described so far in this operation-fusion section have required us to implement modeling code changes and write custom kernels for certain operations in order to speed up training.

In the final section of our low-level dive in the compute operations themselves, we will take a look at a range of methods that are agnostic to the modeling code and can be used for any model and are so widely used that they have become a standard in the industry:Â **Mixed Precision Training**!

### Mixed Precision Training

In various sections along this book, we've talked about lower precisions formats and their impact on the memory requirements for storing activations, parameters and optimizer states. It's now time to dive deeper in the details of these formats and understand better their trade-offs, advantages and limitations.

Mixed Precision Training, as the name suggests, involves mixing different precisions when training. The default numerical precision of PyTorch tensors is single-precision floating point format or also called FP32 or float32 which means that every number stored takes up 32 bits or 4 bytes. The available bits to represent a number are divided into 3 parts:

- Sign: the first bit determines if the number is positive or negative
- Mantissa: determines the significant figures of a number
- Exponent: controls the magnitude of the number

![sign-mantissa-exponent.svg](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/sign-mantissa-exponent.svg)

The principle of floating point numbers can be easily illustrated by recalling the scientific notation of numbers, e.g.Â âˆ’5.734Ã—107âˆ’5.734Ã—107, where we first have the sign, followed by the mantissa an the exponent. As such we can represent numbers across a wide range of magnitudes with an adaptive precision. Although float32 is the default there is a range of floating point formats available in PyTorch:

|**Format**|**Total bits**|**Sign**|**Exponent**|**Mantissa**|
|---|---|---|---|---|
|float32|32|1|8|23|
|float16|16|1|5|10|
|bfloat16|16|1|8|7|
|float8 (e4m3)|8|1|4|3|
|float8 (e5m2)|8|1|5|2|

Note: You might be wondering where the â€œbâ€ in bfloat16 comes from. The format was developed at Google Brain and thus the â€œbâ€ stands for â€œbrainâ€.

Reducing the total number of bits comes at a price (no free lunch here either), but we have some control over how to pay. Either we can sacrifice more bits on the mantissa or exponent. For this reason there exist also two float8 formats, named according to exponent and mantissa, to flexibly choose the most appropriate format. We can look at the possible range of numbers for each format:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/mixedprecision.png)

We can see that float32 spans 80 orders of magnitude and float16 sacrifices a lot of range while bfloat16 maintains the full range. The two float8 formats reduce the range even further where e5e2 can maintain float16 range and e4m3 has an even smaller ranger.

How come some formats are able to maintain the range and others not? Letâ€™s investigate the resolution by plotting 10,000 points between 1 and 2. Each point will be rounded to the nearest representable number in each format:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/mixedprecision_2.png)

We can see here that bfloat16 maintained the range of float32 over float16 but did this with the cost of sacrificing more precision. In case of float8 the situation is even more dire as e4m3 can represent 7 and e5m2 only 3 number on the interval 1-2.

A common metric to measure a formats resolution is epsilon: the first representable number afterÂ 1.001.00. We can see that for the float32 formatÂ 10âˆ’410âˆ’4Â is an upper bound (itâ€™s actuallyÂ 1.19âˆ’71.19âˆ’7). For float16 it is ~Â 10âˆ’310âˆ’3Â and for bfloat 10x higher still.

The idea of mixed precision training is to use some of these lower precisions formats while maintaining the performance of full precision training.

It turns out weÂ **canâ€™t**Â totally abandon float32 and usually will need to maintain some parts in full precision. This is why lower precision training is usually calledÂ **_mixed precision_**Â training.

Letâ€™s now take a look at training models with 16 bits and then see if we can take it a step further all the way down to 8 bits.

#### FP16 and BF16 training

Naively switching all the tensors and operations to float16 unfortunately doesnâ€™t work and the result is usually diverging losses. However, the original mixed precision training paper

[2]

Â came up with three tricks to match float32 trainings:

1. **FP32 copy of weights**: There are two possible issues with float16 weights. During training some of the weights can become very small and will be rounded to 0. However, even if the weights themselves are not close to zero, if the updates are very small the difference in magnitude can cause the weights to underflow during the addition. Once the weights are zero they will remain 0 for the rest of training as there is no gradient signal coming through anymore.
2. **Loss scaling**: We have a similar issue with the gradients as well as gradients tend to be much smaller than 1 and are thus at risk to underflow. A simple, yet effective, strategy is to scale the loss before the backward pass and unscale the gradients after the backward pass. This ensures that there is no underflow during the backward pass and the scaling is not affecting training as we unscale before processing the gradients further (e.g. clipping) and the optimization step.
3. **Accumulation**: Finally, when performing certain arithmetic operations in 16-bit precision such as averages or summations, we can also face under or overflows. A solution is then to accumulate intermediate results in float32 during the operation and only cast the final result back to 16 bit precision.

With these techniques, we can get a stable training while benefitting from a higher throughput due to the faster, lower precision arithmetic operations. Naturally, as a curious reader â€“and by now slightly addicted to maximizing the throughputâ€“ you may ask the question: can we go further and faster than 16-bit precision?

Maybe!

#### FP8 pretraining

Even if we perfectly overlap communication with computation, we always eventually run into the low level theoretical FLOPS limit of the hardware itself, i.e. the efficiency of each individual operation on our hardware. This is where numerical precision becomes crucial. For instance, on NVIDIA's H100 GPU, FP8 matrix multiplications (GEMM operations) achieve twice the theoretical FLOPS of bfloat16, making lower-precision training an attractive path for further optimization.

Recent research - including FP8-LM

[14]

, torchao

[15]

, and DeepSeek-V3

[7]

Â - has demonstrated the potential of FP8 training for large-scale models. Still, FP8 pretraining introduces a significant challenge: stability. At lower precision, numerical instability often leads to loss divergence, making it difficult to match the accuracy of higher-precision training.

We know that instability increases as learning rates rise for a fixed model size

[16]

, making FP8 pretraining particularly tricky.

Here is an example of a typically divergent loss curve for FP8 training:

The first, successful, very large scale training with FP8 mixed precision was publicly reported on DeepSeek-V3. The authors carefully analyzed each operation of the forward pass (Fprop) as well as the activation (Dgrad) and weight (Wgrad) backward pass. Similar to BF16 mixed precision training, some aggregation and master weights are kept in higher precision while the operations themselves are performed in FP8.

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/fp8_diagram.png)

In order to switch from high precision (e.g. FP32 or BF16) to lower precision (e.g. FP16 or FP8) with smaller range, we need to normalize the range of activation values, for instance by computing their absolute maximum. DeepSeek-V3 further introduced a specific quantization scheme where the ranges are normalized per tile: 1x128 for inputs/activations and 128x128 for weights and scale elements. This makes the normalization less strongly impacted by outlier values in the activations. There is a number of additional tricks they proposed to further reduce the memory and communication footprint which you can follow in section 3.3. of the DeepSeek-V3 technical report

[7]

.

Hereâ€™s a summary of a few known approaches to FP8 training:

||GEMM's precision|Master model weights|Accumulated gradients|Model weights|Gradients|Optimizer States|Total Memory|
|---|---|---|---|---|---|---|---|
|bfloat16 with fp32 mixed precision baseline|bf16|fp32|fp32|bf16|bf16|fp32 + fp32|4 + 4 + 2 + 2 + 4 + 4 = 20 bytes|
|Above without FP32 grad accumulation|bf16|fp32|n/a|bf16|bf16|fp32 + fp32|4 + 2 + 2 + 4 + 4 = 16 bytes|
|Transformer Engine|fp8|n/a|n/a|fp32|fp32|fp32 + fp32|4 + 4 + 4 + 4 = 16 bytes (20% reduction)|
|FP8-LM's O3 level|fp8|fp16|fp16|fp8|fp8|fp8 + fp16|2 + 2 + 1 + 1 + 1 + 2 = 9 bytes (55%)|
|DeepSeek-V3|fp8|fp32|fp32|fp8|bf16|bf16 + bf16|4+4+1+2+2+2 = 15 (25%)|
|nanotron's FP8|fp8|bf16|fp32|fp8|fp8|fp8 + fp8|2 + 4 + 1 + 1 + 1 + 1 = 10 bytes (50%)|

Overall, FP8 remains â€“in early 2025â€“ an experimental technique and methods are still evolving. Given its obvious benefits, it will likely become the standard and soon replace bf16 mixed-precision. To follow an open-source implementations of FP8 training techniques, please head to the nanotronâ€™s implementation inÂ [this PR](https://github.com/huggingface/nanotron/pull/70).

Projecting further into the future, Blackwell, the next generation of NVIDIA chips,Â [have been announced](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)Â to support FP4 training, further speeding up training but without a doubt also introducing a new training stability challenge.

---

This last section concluded our long journey in the land of fast and large model training on tens to thousands of GPUs. Time to slowly bring our GPU cluster to rest and take a step back to conclude on all we've learned along the way.

## Conclusion

Congratulations, dear reader, you made it to the end! We've completed quite a journey: we started from understanding how to train a simple model on a single GPU, all the way to mastering all the intricate techniques used to efficiently train massive language models like Llama-405B and DeepSeek-V3 on thousands of GPUs. By now, you can read a diagram, like Llama-3's 4D parallel setup, with (relative) ease:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/conclusion_llama3_parallelism.png)

Orchestrating large clusters of GPUs to train LLMs efficiently is no easy feat. We learned how to optimize computations and communications between GPUs such that they run with maximum utilization at all times. It involves choosing the right parallelization strategy for a given model and cluster size, overlapping communication and computation where possible, and writing custom kernels that take into account the hardware layout to perform an operation as fast as possible on the GPU.

You might still believe that this knowledge is a bit niche and only concerns the small set of people that pretrain LLMs. Historically, that may have been true, but as both theÂ [AI builder community](https://huggingface.co/)Â and model sizes are growing rapidly, the community of people using distributed techniques for inference, fine-tuning and training is increasing exponentially as well making distributed training setups more and more common. Diving deeper into all things distributed might thus prove very timely.

This has been a long learning journey, but not just for you! Running thousands of benchmarks on a GPU cluster was more challenging than we anticipated and we want to share a few highlights of our own learning experience as well.

### So, whatâ€™s next?

You now have good overview of the main distributed training concepts but at the same time we just scratched to surface of several of these tools and techniques. There are many ways to dive deep into a subject but here are some steps that we recommend:

- Carefully read some of the landmark or very recent papers. You can find a very extenside list of the most impactful papers, blog posts and books inÂ [References](https://nanotron-ultrascale-playbook.static.hf.space/dist/index.html#references).
- Start from scratch and implement an algorithm yourself. Often a method only fully â€œclicksâ€ if you implemented it yourself.
- Dive into one of the widely used frameworks and start contributing: fix bugs, answer issues, or implement a new feature. Thatâ€™s the best way to get in any ML field!

We hope this book helps you get started in distributed training and that you will train the next generation of awesome models to the hum of your GPU cluster!

---

**One last word**Â for our first readers. We're so happy with this writing piece that we've decided to distribute a limited number of physical printed editions of it as a gift for our first readers.

If you are among the first 50 people to fill in your email address below, we'll contact you later in the year to send you a real physical edition once we've formatted it as a printed copy.

We expect the book to be around 100-150 pages and to cover the same content as the blog post but we may also decide to shorten or lengthen it depending on what make sense as a printed object.

To get your physical copy, please fill in your email address in the followingÂ [google form](https://forms.gle/e1GkAShUCtgcwnne8).

Whether you are one of our first readers or coming much later to this blog post, we've very happy to see that you enjoyed this sharing of knowledge. May the force of open-source and open-science always be with you.

### Acknowledgements

We thankÂ [Elie](https://huggingface.co/eliebak)Â for conducting thorough reviews and creating the audio components using NotebookLM. Special thanks toÂ [Hynek](https://huggingface.co/hynky)Â for optimizing the frontend performance. We also thankÂ [Simon](https://huggingface.co/sbrandeis)Â for resolving some issues on the hub.

### Discussion page

If you want to discuss the content of this blog post, ask questions, propose changes or just say hi, please open a thread on theÂ [discussion page](https://huggingface.co/spaces/nanotron/ultrascale-playbook/discussions).

## References

### Landmark LLM Scaling Papers

[**Megatron-LM**](https://arxiv.org/abs/1909.08053)

Introduces tensor parallelism and efficient model parallelism techniques for training large language models.

[**Megatron-Turing NLG 530B**](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)

Describes the training of a 530B parameter model using a combination of DeepSpeed and Megatron-LM frameworks.

[**PaLM**](https://arxiv.org/abs/2204.02311)

Introduces Google's Pathways Language Model, demonstrating strong performance across hundreds of language tasks and reasoning capabilities.

[**Gemini**](https://arxiv.org/abs/2312.11805)

Presents Google's multimodal model architecture capable of processing text, images, audio, and video inputs.

[**Llama 3**](https://arxiv.org/abs/2407.21783)

The Llama 3 Herd of Models

[**DeepSeek-V3**](https://arxiv.org/abs/2412.19437v1)

DeepSeek's report on architecture and training of the DeepSeek-V3 model.

### Training Frameworks

[**Nanotron**](https://github.com/huggingface/nanotron)

Our framework for training large language models featuring various parallelism strategies

[**Megatron-LM**](https://github.com/NVIDIA/Megatron-LM)

NVIDIA's framework for training large language models featuring various parallelism strategies.

[**DeepSpeed**](https://www.deepspeed.ai/)

Microsoft's deep learning optimization library featuring ZeRO optimization stages and various parallelism strategies.

[**FairScale**](https://github.com/facebookresearch/fairscale/tree/main)

PyTorch extension library for large-scale training, offering various parallelism and optimization techniques.

[**ColossalAI**](https://colossalai.org/)

Integrated large-scale model training system with various optimization techniques.

[**torchtitan**](https://github.com/pytorch/torchtitan)

A PyTorch native library for large model training.

[**GPT-NeoX**](https://github.com/EleutherAI/gpt-neox)

EleutherAI's framework for training large language models, used to train GPT-NeoX-20B.

[**LitGPT**](https://github.com/Lightning-AI/litgpt)

Lightning AI's implementation of state-of-the-art open-source LLMs with focus on reproducibility.

[**DiLoco**](https://github.com/PrimeIntellect-ai/OpenDiLoCo)

Training language models across compute clusters with DiLoCo.

[**torchgpipe**](https://github.com/kakaobrain/torchgpipe)

A GPipe implementation in PyTorch.

[**OSLO**](https://github.com/EleutherAI/oslo)

OSLO: Open Source for Large-scale Optimization.

### Debugging

[**Speed profiling**](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)

Official PyTorch tutorial on using the profiler to analyze model performance and bottlenecks.

[**Memory profiling**](https://pytorch.org/blog/understanding-gpu-memory-1/)

Comprehensive guide to understanding and optimizing GPU memory usage in PyTorch.

[**Memory profiling walkthrough on a simple example**](https://huggingface.co/blog/train_memory)

Visualize and understand GPU memory in PyTorch.

[**TensorBoard Profiler Tutorial**](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)

Guide to using TensorBoard's profiling tools for PyTorch models.

### Distribution Techniques

[**Data parallelism**](https://siboehm.com/articles/22/data-parallel-training)

Comprehensive explanation of data parallel training in deep learning.

[**ZeRO**](https://arxiv.org/abs/1910.02054)

Introduces Zero Redundancy Optimizer for training large models with memory optimization.

[**FSDP**](https://arxiv.org/abs/2304.11277)

Fully Sharded Data Parallel training implementation in PyTorch.

[**Tensor and Sequence Parallelism + Selective Recomputation**](https://arxiv.org/abs/2205.05198)

Advanced techniques for efficient large-scale model training combining different parallelism strategies.

[**Pipeline parallelism**](https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/#pipeline_parallelism)

NVIDIA's guide to implementing pipeline parallelism for large model training.

[**Breadth first Pipeline Parallelism**](https://arxiv.org/abs/2211.05953)

Includes broad discussions around PP schedules.

[**All-reduce**](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/)

Detailed explanation of the ring all-reduce algorithm used in distributed training.

[**Ring-flash-attention**](https://github.com/zhuzilin/ring-flash-attention)

Implementation of ring attention mechanism combined with flash attention for efficient training.

[**Ring attention tutorial**](https://coconut-mode.com/posts/ring-attention/)

Tutorial explaining the concepts and implementation of ring attention.

[**ZeRO and 3D**](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/#understanding-performance-tradeoff-between-zero-and-3d-parallelism)

DeepSpeed's guide to understanding tradeoffs between ZeRO and 3D parallelism strategies.

[**Mixed precision training**](https://arxiv.org/abs/1710.03740)

Introduces mixed precision training techniques for deep learning models.

[**Visualizing 6D Mesh Parallelism**](https://main-horse.github.io/posts/visualizing-6d/)

Explains the collective communication involved in a 6D parallel mesh.

### Hardware

[**Fire-Flyer - a 10,000 PCI chips cluster**](https://www.arxiv.org/abs/2408.14158)

DeepSeek's report on designing a cluster with 10k PCI GPUs.

[**Meta's 24k H100 Pods**](https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/)

Meta's detailed overview of their massive AI infrastructure built with NVIDIA H100 GPUs.

[**Semianalysis - 100k H100 cluster**](https://www.semianalysis.com/p/100000-h100-clusters-power-network)

Analysis of large-scale H100 GPU clusters and their implications for AI infrastructure.

[**Modal GPU Glossary**](https://modal.com/gpu-glossary/readme)

CUDA docs for human

### Others

[**Stas Bekman's Handbook**](https://github.com/stas00/ml-engineering)

Comprehensive handbook covering various aspects of training LLMs.

[**Bloom training chronicles**](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md)

Detailed documentation of the BLOOM model training process and challenges.

[**OPT logbook**](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf)

Meta's detailed logbook documenting the training process of the OPT-175B model.

[**Harm's law for training smol models longer**](https://www.harmdevries.com/post/model-size-vs-compute-overhead/)

Investigation into the relationship between model size and training overhead.

[**Harm's blog for long context**](https://www.harmdevries.com/post/context-length/)

Investigation into long context training in terms of data and training cost.

[**GPU Mode**](https://www.youtube.com/@GPUMODE/videos)

A GPU reading group and community.

[**EleutherAI Youtube channel**](https://youtube.com/playlist?list=PLvtrkEledFjqOLuDB_9FWL3dgivYqc6-3&si=fKWPotx8BflLAUkf)

ML Scalability & Performance Reading Group

[**Google Jax Scaling book**](https://jax-ml.github.io/scaling-book/)

How to Scale Your Model

[**@fvsmassa & @TimDarcet FSDP**](https://github.com/facebookresearch/capi/blob/main/fsdp.py)

Standalone ~500 LoC FSDP implementation

[**thonking.ai**](https://www.thonking.ai/)

Some of Horace He's blogposts - Making GPUs go BRRR..

[**Aleksa's ELI5 Flash Attention**](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)

Easy explanation of Flash Attention

[**TunibAI's 3D parallelism tutorial**](https://github.com/tunib-ai/large-scale-lm-tutorials)

Large-scale language modeling tutorials with PyTorch.

## Appendix

### A0: Parallel Programming Crash Course

Throughout the blogpost we scale LLM training from one to hundreds of GPUs. This will require the communication and synchronization of weights, gradients, and data between all the machines. Thereâ€™s a set of distributed patterns to achieve exactly that calledÂ **_collective operations_**. In this section weâ€™ll do a small crash course of all the operations likeÂ _Broadcast, AllReduce, Scatter_Â and more. Letâ€™s dive in!

The general setup is that we have a number of independent nodes which could be CPU cores, GPUs, or compute nodes. Each performs some computation and then we want to communicate the result or parts of it to the other nodes for the next computation step (t+1).

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_general.png)

Maybe we need to send the result from one node to all other nodes, or we need to sum all the intermediate results from each node to report the overall result. Usually, there is one node with an elevated status that plays a central role, here denoted withÂ `root`Â that is the target or source of some operations. Letâ€™s start with one of the simplest primitives: a broadcast operation.

#### Broadcast

A very common pattern is that you have some data on Node 1 and you want to share it with all the other nodes so they can do some computation with the data. The broadcast operation does just that:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_broadcast.png)

Collective operations are natively provided by PyTorch so we can easily write a small example that demonstrates how broadcasting works. We first need to initialize a process group withÂ `dist.initi_process_group`Â which sets up the communication backend (weâ€™ll talk about NCCL later), it determines how many workers (aka nodes) exists and assigns a rank to each one (which we can get withÂ `dist.get_rank`). Finally, it establishes a connection between the workers.

To showcase theÂ `dist.broadcast`Â operation, let's create a tensor with non-zero values onÂ `rank=0`Â and tensors full of zeros on the other workers. We then distribute theÂ `rank=0`Â tensor to all other ranks withÂ `dist.broadcast(tensor, src=0)`Â :

```python
import torch
import torch.distributed as dist

def init_process():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(dist.get_rank())
    
def example_broadcast():
    if dist.get_rank() == 0:
        tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).cuda()
    else:
        tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before broadcast on rank {dist.get_rank()}: {tensor}")
    dist.broadcast(tensor, src=0)
    print(f"After broadcast on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_broadcast()
```

You can run the above script withÂ `torchrun --nproc_per_node=3 dist_op.py`Â (youâ€™ll need 3 GPUs for this or changeÂ `nproc_per_node`Â accordingly) and you should see the following output:

```python
Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device='cuda:1')
After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device='cuda:2')
```

Great, seems like it works as expected. Note that the rank messages can be printed out of order as we have no control over which print statement is executed first (we ordered them here for readability). Now letâ€™s move on to the Reduce and AllReduce patterns!

#### Reduce & AllReduce

Reduce patterns are among the most fundamental patterns in distributed data processing. The idea is that you want to combine the data present on each node through a functionÂ `f()`Â which can be for instance summation or averaging. In the Reduce paradigm the result is sent to the root node only, whereas in the AllReduce case the result is broadcasted to all nodes:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_reduce_allreduce.png)

Of course no magic â€œfree flyingâ€ node that can perform this operation and generally each node does a partial computation in a ring or tree structure of the nodes. Here is a simple example: letâ€™s say we need to compute a sum of numbers on each nodes and our nodes are connected in a ring pattern. The first node sends its number to a neighbour which adds its number to the received number before forwarding it to the next neighbour. At the end of a round along the ring of nodes, the first node will receive the total sum.

Hereâ€™s the code to run a simple Reduce operation summing the tensors, we specify the operation to use withÂ `op=dist.ReduceOp.SUM`Â (you can find more information on the supported operations in theÂ [Pytorch docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp)):

```python
def example_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before reduce on rank {dist.get_rank()}: {tensor}")
    dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
    print(f"After reduce on rank {rank}: {tensor}")
    
init_process()
example_reduce()
```

Note that in the Reduce operation only the tensor on theÂ `dst`Â node is updated:

```python
Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

Similarly we can perform an AllReduce (we donâ€™t need to specify a destination in this case):

```python
def example_all_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before all_reduce on rank {dist.get_rank()}: {tensor}")
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    print(f"After all_reduce on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_all_reduce()
```

In this case the result is available on all nodes:

```python
Before all_reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After all_reduce on rank 1: tensor([6., 6., 6., 6., 6.], device='cuda:1')
After all_reduce on rank 2: tensor([6., 6., 6., 6., 6.], device='cuda:2')
```

Now letâ€™s turn to our next distributed communication operation. In many real cases, each node individually perform many complex computations and we need to share the final results among nodes. Gather and AllGather are the operations we want to use in this case. Letâ€™s take a look!

#### Gather & AllGather

Gather and AllGather are quite similar to the Broadcast in that they allow distributing data among node without modification. The main difference to Broadcast is that there is not one value we need to share from one node to all other nodes but each node has an individual chunk of data that we want to either gather all data on one node (in case of Gather) or gather all data on all nodes (in the case of AllGather). A picture being worth 1000 words, letâ€™s take a look:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_gather_allgather.png)

Note that the dashed lines indicate that some data actually doesnâ€™t move at all (since itâ€™s already present on the node).

In the case of the gather operation we need to prepare a container objects where the gathered tensors can be stored in this example theÂ `gather_list`:

```python
def example_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    if dist.get_rank() == 0:
        gather_list = [
            torch.zeros(5, dtype=torch.float32).cuda()
            for _ in range(dist.get_world_size())
            ]
    else:
        gather_list = None
    print(f"Before gather on rank {dist.get_rank()}: {tensor}")
    dist.gather(tensor, gather_list, dst=0)
    if dist.get_rank() == 0:
        print(f"After gather on rank 0: {gather_list}")
    
init_process()
example_gather()
```

And we see that the `gather_list` indeed contains the tensors of all ranks:

```python
Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                         tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                         tensor([3., 3., 3., 3., 3.], device='cuda:0')]
```

The only thing we need to change for the AllGather example is that every node will need a placeholder for the results:

```python
def example_all_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    gather_list = [
        torch.zeros(5, dtype=torch.float32).cuda()
        for _ in range(dist.get_world_size())
        ]
    print(f"Before all_gather on rank {dist.get_rank()}: {tensor}")
    dist.all_gather(gather_list, tensor)
    print(f"After all_gather on rank {dist.get_rank()}: {gather_list}")
    
init_process()
example_all_gather()
```

And indeed we can see that now each node has all the data:

```python
Before all_gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 1: [tensor([1., 1., 1., 1., 1.], device='cuda:1'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 2: [tensor([1., 1., 1., 1., 1.], device='cuda:2'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:2'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:2')]
```

Now what about the inverse of a gather? In this case we would have all the data on one node and want to distribute/slice it among node, possibly with some intermediate processing? We can use the Scatter, or in the case of an operation in between a Reduce Scatter pattern:

#### Scatter & ReduceScatter

As the name subtly suggests, the goal of the Scatter operation is to take data on one node and distribute slices of it to all other nodes. Itâ€™s thus different from the Broadcast operation which copy data without slicing and itâ€™s the logical the inverse of the Gather operation.

The ReduceScatter pattern is slightly more complex: imagine you apply an operation like in the Reduce case but instead of moving the result to just one node we also distribute it evenly to all nodes:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_scatter_reducescatter.png)

The Scatter operation is written in code as the opposite of the Gather: instead of preparing a list of tensors as target we prepare the source data as a list of tensors we want to distribute. We also need to specify theÂ `src`:

```python
def example_scatter():
    if dist.get_rank() == 0:
        scatter_list = [
            torch.tensor([i + 1] * 5, dtype=torch.float32).cuda()
            for i in range(dist.get_world_size())
            ]
        print(f"Rank 0: Tensor to scatter: {scatter_list}")
    else:
        scatter_list = None
    tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before scatter on rank {dist.get_rank()}: {tensor}")
    dist.scatter(tensor, scatter_list, src=0)
    print(f"After scatter on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_scatter()
```

As a result we can see how the empty tensors got filled with the contents of theÂ `scatter_list`

```python
Rank 0: Tensor to scatter: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                            tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                            tensor([3., 3., 3., 3., 3.], device='cuda:0')]
Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device='cuda:0')
Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

Letâ€™s create more interesting data to demonstrate the ReduceScatter logic: on each node we create a list of 2-elements vector on each node with a power exponent and an offset function of the node rank (itâ€™s a bit hard to imagine so just look below for an example):

```python
def example_reduce_scatter():
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    input_tensor = [
        torch.tensor([(rank + 1) * i for i in range(1, 3)], dtype=torch.float32).cuda()**(j+1) 
        for j in range(world_size)
        ]
    output_tensor = torch.zeros(2, dtype=torch.float32).cuda()
    print(f"Before ReduceScatter on rank {rank}: {input_tensor}")
    dist.reduce_scatter(output_tensor, input_tensor, op=dist.ReduceOp.SUM)
    print(f"After ReduceScatter on rank {rank}: {output_tensor}")    
    
init_process()
example_reduce_scatter()
```

Letâ€™s print the pattern of data that we created. We also immediately see the ReduceScatter pattern: the first rank received the sum of the first tensor from each node, and the second rank contains the sum of the second tensor on each node and so on:

```python
Before ReduceScatter on rank 0: [tensor([1., 2.], device='cuda:0'),
											 tensor([1., 4.], device='cuda:0'),
											 tensor([1., 8.], device='cuda:0')]
Before ReduceScatter on rank 1: [tensor([2., 4.], device='cuda:1'),
                                 tensor([ 4., 16.], device='cuda:1'),
                                 tensor([ 8., 64.], device='cuda:1')]
Before ReduceScatter on rank 2: [tensor([3., 6.], device='cuda:2'),
                                 tensor([ 9., 36.], device='cuda:2'),
                                 tensor([ 27., 216.], device='cuda:2')]

After ReduceScatter on rank 0: tensor([ 6., 12.], device='cuda:0')
After ReduceScatter on rank 1: tensor([14., 56.], device='cuda:1')
After ReduceScatter on rank 2: tensor([ 36., 288.], device='cuda:2')
```

Let's have a quick look at a common implementation of AllReduce that uses ReduceScatter and AllGather: Ring AllReduce.

#### A quick focus on Ring AllReduce

**_Ring AllReduce_**Â is one specific implementation of AllReduce, optimized for scalability. Rather than all devices communicating with each other directly, which could create communication bottlenecks, Ring All-Reduce can be broken down into two key steps: ReduceScatter and AllGather. Here's how it works:

1. **ReduceScatter**

- Each device splits its data (e.g., gradients) into chunks and sends one chunk to its neighbour. Simultaneously, each device receives a chunk from its other neighbour.
- As each device receives a chunk, it adds (reduces) its corresponding chunk to the received one.
- This process continues around the ring until each device holds a partially reduced chunk, representing a sum of the gradients across all devices for that chunk.

3. **AllGather**

- Now, each device needs to collect the fully reduced chunks from other devices.
- The devices start sending their reduced chunks to neighbours.
- Each device forwards the chunks it receives until every device has all the fully reduced chunks, giving each device the complete, summed-up gradient.

Letâ€™s illustrate this with the following gifs, where we have 5 GPUs, each with a tensor of length 5. The first animation shows the ReduceScatter step, where, at the end, each GPU receives the reduced results for a specific chunk of data (orange rectangle).

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_reduce_scatter.gif)

The next animation shows the AllGather step, where, at the end, each GPU obtains the full results of the AllReduce operation:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_all_gather.gif)

You may have noticed that each of theÂ NNÂ GPUs sends and receives valuesÂ Nâˆ’1Nâˆ’1Â times during both the reduce-scatter and all-gather steps. Each GPU sendsÂ KNNKâ€‹Â values per transfer, whereÂ KKÂ is the total number of values in the array being summed across the GPUs. Therefore, the total amount of data transferred to and from each GPU isÂ 2Ã—(Nâˆ’1)Ã—KN2Ã—(Nâˆ’1)Ã—NKâ€‹. WhenÂ NNÂ (the number of GPUs) is large, the total amount of data transferred to and from each GPU is approximatelyÂ 2Ã—K2Ã—K, whereÂ KKÂ is the total number of parameters.

**There are two key things to keep in mind for AllReduce:**

1. The communication cost for AllReduce is approximatelyÂ 2xK2xKÂ whenÂ NNÂ (the number of GPUs) is large.
2. An AllReduce operation can be broken down into a reduce-scatter followed by an all-gather. The communication cost for these two operations is half that of the AllReduce, which is approximatelyÂ KK.

As we can see this implementation can make efficient use of even a limited bandwidth between nodes.

We now have seen the main building block of distributed operations but before we see them in action letâ€™s have a look at a special operation used for synchronization: the Barrier.

#### Barrier

The Barrier is a simple operation to synchronize all nodes. A barrier is not lifted until all nodes have reached it. Then only are they allowed to continue with further computations:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a0_barrier.png)

We can easily simulate delayed nodes by setting up a different sleep time on each node and see how long it takes for all of them to pass the barrier:

```python
def example_barrier():
    rank = dist.get_rank()
    t_start = time.time()
    print(f"Rank {rank} sleeps {rank} seconds.")
    time.sleep(rank)  # Simulate different processing times
    dist.barrier()
    print(f"Rank {rank} after barrier time delta: {time.time()-t_start:.4f}")
    
init_process()
example_barrier()
```

We can see that although the first rank didnâ€™t sleep at all it also took it 2sec to pass the barrier:

```python
Rank 0 sleeps 0 seconds.
Rank 1 sleeps 1 seconds.
Rank 2 sleeps 2 seconds.

Rank 0 after barrier time delta: 2.0025
Rank 1 after barrier time delta: 2.0025
Rank 2 after barrier time delta: 2.0024
```

We need to be careful with synchronizing all nodes like this, as this defeat the purpose of parallel independent operations and might thus slow down the whole processing. In many situations it can be just fine if a fast node already starts processing the next job as the fast node could be slower in a next iteration therefore evening out the delay over the whole process.

Before turning to practical distributed training implementations, letâ€™s first solve a mystery: what the heck is NCCL?

#### NCCL: NVIDIA Collective Communications Library

When training large models on many GPUs we may sometimes strike gold but we will always encounter nickel (or NCCL ğŸ¥)! Whatâ€™s is that?

There are several libraries that implement collective communication and are support by PyTorch: thereâ€™s the classicÂ **_MPI_**Â (Message Passing Interface), thereâ€™sÂ **_Gloo_**Â by Meta, and finally there is `NCCL` (NVIDIA Collective Communications Library). They all provide similar functionality in terms of collective communication patterns but are optimized for different hardware setups; NCCL is designed to serve GPU-GPU communication efficiently while MPI and Gloo are setup for CPU-CPU or CPU-GPU communication. PyTorch provides aÂ [great guide](https://pytorch.org/docs/stable/distributed.html#which-backend-to-use)Â to decide which one to use:

- GPU training: use NCCL
- CPU training: use Gloo

There are a few finer points in the decision tree that we leave to the reader to explore in the PyTorch guide referenced above.

Now that we covered the fundamental operations for distributed training and you should now be ready to follow the blog post easily.

### A1: Distributed Training Profiling

#### Kernels

Let's begin by assuming for now that the kernels are already integrated into PyTorch. As a simple example, we can look at the Layer Normalization function implemented in PyTorch asÂ `torch.nn.functional.layer_norm`. There are several methods to profile the kernel that underlies this function. The most straightforward approach might be to use the PythonÂ `time`Â module. However, since CUDA operations are asynchronous, measuring time with this method will only capture the overhead associated with launching the kernel in Python, rather than the actual execution time of the kernel itself.

To address this, we can utilizeÂ `torch.cuda.Event`Â for accurate timing and employ theÂ `torch.cuda.synchronize()`Â directive to ensure we wait for the kernel execution to complete. This approach is demonstrated in the following snippet:

```python
def profile_pytorch(func, input):
    # Create CUDA events to track time. CUDA operations are asynchronous,
    start = torch.cuda.Event(enable_timing=True)  # Event to mark the start time
    end = torch.cuda.Event(enable_timing=True)    # Event to mark the end time
    # Warmup to eliminate any overhead from the first run, which might not reflect 
    # the actual performance.
    for _ in range(10):
        func(input)
    # Record the start time before executing the function
    start.record()  
    func(input)  # Call the function we want to profile
    # Record the end time after the function has completed
    end.record()  
    # Synchronize the CUDA operations to ensure all operations are completed
    # before measuring the elapsed time.
    torch.cuda.synchronize()  
    # Calculate and return the elapsed time in milliseconds.
    return start.elapsed_time(end)
```

A more effective approach to profiling is to utilize the PyTorch Profiler, as explained previously. For example, consider the following code:

```python
import torch
import torch.nn.functional as F

def pytorch_layer_norm(input):
    return F.layer_norm(input, input.size()[1:])

a = torch.randn(10000, 10000).cuda()

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,  # Profile CPU activities
        torch.profiler.ProfilerActivity.CUDA,  # Profile CUDA activities
    ],
    # Define a schedule for the profiler
    schedule=torch.profiler.schedule(
        wait=1,      # Wait for 1 iteration before starting to profile
        warmup=3,    # Warm up for 3 iterations to stabilize performance
        active=2,    # Profile for 2 active iterations
        repeat=1,    # Repeat the profiling schedule once
    ),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('.'),
    
) as p:
    for iter in range(10):
        pytorch_layer_norm(a)
        p.step()

# Print a table of the profiling results, sorted by total CUDA time, limited to the top 10 entries
print(p.key_averages().table(sort_by="cuda_time_total", row_limit=8))
```

This would print aggregated profiling results sorted by the total CUDA time, and the output would be:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_kernels.png)

You can also try to inspect the trace as we previously mentioned onÂ `chrome://tracing/`

ğŸ’¡ Tip

If you're new to this tool, you can navigate the trace by using the right and left arrow keys. Additionally, you can zoom in and out by holding theÂ **Alt**Â key while scrolling left or right with your mouse.

After zooming in, you can observe the flow of operations when callingÂ `layer_norm`Â in this trace:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_profile_trace.png)

The sequence begins in the CPU (the upper section) withÂ `aten::layer_norm`, progressing toÂ `aten::native_layer_norm`, and then transitioning toÂ `cudaLaunchKernel`. From there, we move on to the GPU, where theÂ `vectorized_layer_norm_kernel`Â kernel is called.

ğŸ“ Note

You can enable memory profiling by settingÂ `profile_memory`Â toÂ `True`Â in the profiler. However, this can lead to more complex traces.

While the PyTorch Profiler offers a quick performance overview,Â **NVIDIA Nsight Compute (ncu)**Â provides deeper insights into GPU performance, including detailed execution times and memory usage for each kernel. To run the profiler it's very simple:

```bash
ncu --set full python layer_norm.py
```

WhereÂ `layer_norm.py`Â is a straightforward file that executes the layer normalization function. This command will generate log outputs, but a more effective way to visualize the results is by setting the output flag:

```bash
ncu --set full -o output python layer_norm.py
```

and open the fileÂ `output.ncu-rep`Â with Nsight Compute, you will have a view that looks like this:

![image.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/a1_ncu.png)

With clear warnings about compute and memory utilization, and how to make the kernel better in balancing compute and memory and achieve maximal occupancy.

#### CPP extension

If the kernel you want to profile isn't already integrated into PyTorch, you can use PyTorch'sÂ `cpp_extension`Â module to easily compile and run custom CUDA code. The process is straightforwardâ€”just create your CUDA kernel in aÂ `.cu`Â file, and use theÂ `load`Â function from theÂ `cpp_extension`Â module to load it in Python.

TheÂ `.cu`Â file would like this for a simpleÂ `add`Â kernel:

```clike
#include 
#include 
#include 

__global__ void add_kernel(float* x, float* y, float* output, int size) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < size) {
        output[index] = x[index] + y[index];
    }
}

void add_cuda(torch::Tensor x, torch::Tensor y, torch::Tensor output) {
    int threads = 1024;
    int blocks = (x.size(0) + threads - 1) / threads;

    add_kernel<<>>(x.data_ptr(), y.data_ptr(), output.data_ptr(), x.size(0));
}
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("add_cuda", &add_cuda, "Vector addition (CUDA)");
}
```

And the python file to load the kernel:

```python
import torch
from torch.utils.cpp_extension import load

# Load and compile the CUDA extension
vector_add = load(
    name="vector_add",
    sources=["add_kernel.cu"],
    verbose=True
)

# Define input tensors
size = 10000
x = torch.randn(size, device='cuda')
y = torch.randn(size, device='cuda')
output = torch.empty(size, device='cuda')

# Run the CUDA kernel
vector_add.add_cuda(x, y, output)
```

Using this method, you can profile the custom CUDA kernel just as we demonstrated earlier with PyTorch's profiler or NVIDIA tools.

### A2: Typical Scales in LLM Training

Let's get a feel for the typical sizes of things in LLM training. When we talk about memory or compute, we're often counting "elements" - think of these as numbers in tensors. To get the actual memory in bytes, you'll need to multiply by the size of each number (e.g., 2 bytes for bf16, 4 bytes for fp32).

Here are some quick ballpark figures:

- **Input tokens:**Â For each batch, we processÂ seqâ‹…mbsseqâ‹…mbsÂ tokens, where mbs is the micro batch size and seq is the sequence length.
- **Activations (hidden states):**Â For a single layer, the hidden state tensor is of sizeÂ seqâ‹…mbsâ‹…hseqâ‹…mbsâ‹…hÂ elements.
- **Model weights and gradients:**Â Each weight matrix in your model (like in linears) is aboutÂ h2h2Â elements. This is per weight matrix. Gradients have the same size as weights.
- **Optimizer states:**Â For each weight matrix (of elementsÂ h2h2), if you're using an optimizer like Adam with mixed precision training, it keeps momentum and variance states in fp32 precision (2â‹…h22â‹…h2), plus master weights in fp32 (h2h2). So total optimizer states will be around (6â‹…h26â‹…h2) per weight matrix.
- **Total model parameters:**Â For each transformer block:
    - Attention parameters:
        - QKV projections:Â 3h23h2Â parameters
        - Output projection:Â h2h2Â parameters
    - MLP parameters with GLU:
        - Gate and up projections:Â 8h28h2Â parameters (2 matrices of sizeÂ hÃ—4hhÃ—4h)
        - Down projection:Â 4h24h2Â parameters (1 matrix of sizeÂ 4hÃ—h4hÃ—h)
    - Total per block:Â 16h216h2Â with GLU MLPs, orÂ 12h212h2Â without GLU
    - For full model:Â 16h2â‹…num_layers16h2â‹…num_layersÂ (with GLU)
    - Additional parameters:
        - Input embeddings:Â vocab_sizeâ‹…hvocab_sizeâ‹…h
        - LM head:Â vocab_sizeâ‹…hvocab_sizeâ‹…hÂ (if not tied with input embeddings)
        - Positional embeddings (if used):Â max_seq_lenâ‹…hmax_seq_lenâ‹…h
- **Forward and backward pass compute (FLOPs):**Â A very rough estimate for the FLOPs in a forward pass isÂ 2â‹…num_tokensâ‹…num_params2â‹…num_tokensâ‹…num_params. And backward pass compute is twice as that:Â 4â‹…num_tokensâ‹…num_params4â‹…num_tokensâ‹…num_params.

### A3: Math for Compute/Communication Overlap

Using the formulas from the previous section, we can estimate when computation and communication can effectively overlap in distributed training. Let's look at data parallelism (Zero-0) as an example.

#### Data Parallelism Communication Analysis

The total gradient size that needs to be communicated is:

- Gradients = Parameters â‰ˆÂ num_layersâ‹…16h2num_layersâ‹…16h2

During backward pass, these gradients are communicated in buckets (default 25MB). The communication time to all-reduce each bucket is:

tcomm=tcomm_bucket=bucket_sizeâ‹…2(DPâˆ’1)DPâ‹…peak_bwtcommâ€‹=tcomm_bucketâ€‹=DPâ‹…peak_bwbucket_sizeâ‹…2(DPâˆ’1)â€‹

ğŸ“ Note

For bandwidth calculations, we use the bus bandwidth formulas from theÂ [NCCL documentation](https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#summary). These formulas account for the specific communication patterns when calculating effective bandwidth between GPUs.

The computation time for backward pass is:

tcompute=4â‹…num_tokensâ‹…num_paramspeak_flopstcomputeâ€‹=peak_flops4â‹…num_tokensâ‹…num_paramsâ€‹

For effective overlap, we need:

tcommtcompute=num_params2â‹…num_tokensâ‹…DPâˆ’1DPâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…num_tokensnum_paramsâ€‹â‹…DPDPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

This ratio helps determine if communication will become a bottleneck in training. When the ratio is less than 1, communication can be fully overlapped with computation.

#### ZeRO-3 (FSDP) Communication Analysis

For ZeRO-3, parameters and gradients are sharded across GPUs. Let's analyze the communication pattern for a model with transformer blocks of sizeÂ 16h216h2Â parameters each:

- For each transformer block in forward pass:
    - Allgather parameters:Â 16h2/DP16h2/DPÂ bytes per rank
- For each transformer block in backward pass:
    - Allgather parameters:Â 16h2/DP16h2/DPÂ bytes per rank
    - Reducescatter gradients:Â 16h2/DP16h2/DPÂ bytes per rank
- Total communication per block:Â 3â‹…16h2/DP3â‹…16h2/DPÂ bytes
- Total communication for full model:Â 3â‹…num_layersâ‹…16h2/DP3â‹…num_layersâ‹…16h2/DPÂ bytes

The communication time for allgather operations is:

tcomm=16h2â‹…DPâˆ’1DPâ‹…peak_bwtcommâ€‹=16h2â‹…DPâ‹…peak_bwDPâˆ’1â€‹

The computation time for forward pass of one decoder layer is:

tcompute=32â‹…seq_lenâ‹…mbsâ‹…h2peak_flopstcomputeâ€‹=peak_flops32â‹…seq_lenâ‹…mbsâ‹…h2â€‹

For effective overlap between computation and communication, we need:

tcommtcompute=12â‹…seq_lenâ‹…mbsâ‹…DPâˆ’1DPâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…seq_lenâ‹…mbs1â€‹â‹…DPDPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

When this ratio is less than 1, the communication of parameters for the next layer can be hidden behind the computation of the current layer.

`

#### TP Communication Analysis

For Tensor Parallel (TP), activations are sharded across GPUs during linears. Let's analyze the communication pattern:

- For each column linear in forward pass:
    - Allgather activations:Â seqâ‹…mbsâ‹…h/TPseqâ‹…mbsâ‹…h/TPÂ bytes per rank
- For each column linear in backward pass:
    - Reducescatter gradients:Â seqâ‹…mbsâ‹…h/TPseqâ‹…mbsâ‹…h/TPÂ bytes per rank
- And vice-versa for row linears. Each transformer block has 2 column linears and 2 row linears.
- Total communication per block:Â 8â‹…seqâ‹…mbsâ‹…h/TP8â‹…seqâ‹…mbsâ‹…h/TPÂ bytes
- Total communication for full model:Â 8â‹…num_layersâ‹…seqâ‹…mbsâ‹…h/TP8â‹…num_layersâ‹…seqâ‹…mbsâ‹…h/TPÂ bytes

Let's analyze if we can overlap the allgather communication for one layer with the computation of the next linear. The communication time for allgather operations is:

tcomm=seqâ‹…mbsâ‹…hâ‹…(TPâˆ’1)TPâ‹…peak_bwtcommâ€‹=TPâ‹…peak_bwseqâ‹…mbsâ‹…hâ‹…(TPâˆ’1)â€‹

While the computation time for the next linear (with parametersÂ h2h2) is:

tcompute=2â‹…seqâ‹…mbsâ‹…h2TPâ‹…peak_flopstcomputeâ€‹=TPâ‹…peak_flops2â‹…seqâ‹…mbsâ‹…h2â€‹

For effective overlap, we want the communication time to be less than the compute time:

tcommtcompute=TPâˆ’12â‹…hâ‹…peak_flopspeak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=2â‹…hTPâˆ’1â€‹â‹…peak_bwpeak_flopsâ€‹â‰¤1

This ratio tells us whether we can successfully hide the allgather communication behind the computation of the next linear. Interestingly, the ratio only depends on the hidden size h and tensor parallelism degree TP, not on sequence length or batch size.

#### PP Communication Analysis

For Pipeline Parallel (PP), activations and gradients are communicated between pipeline stages. Let's analyze the communication pattern:

- For each microbatch in forward pass:
    - Receive and send activations:Â 2â‹…seqâ‹…mbsâ‹…h2â‹…seqâ‹…mbsâ‹…hÂ bytes
- For each microbatch in backward pass:
    - Receive and send gradients:Â 2â‹…seqâ‹…mbsâ‹…h2â‹…seqâ‹…mbsâ‹…hÂ bytes
- Total communication per microbatch:Â 4â‹…seqâ‹…mbsâ‹…h4â‹…seqâ‹…mbsâ‹…hÂ bytes
- For gradient accumulation steps (gas), total communication:Â 4â‹…gasâ‹…seqâ‹…mbsâ‹…h4â‹…gasâ‹…seqâ‹…mbsâ‹…hÂ bytes

Let's analyze if we can overlap the communication of activations/gradients with computation of the next transformer block. The computation time for transformer blocks in the next pipeline stage is:

tcompute=32â‹…seqâ‹…mbsâ‹…h2â‹…num_layers_in_next_pppeak_flopstcomputeâ€‹=peak_flops32â‹…seqâ‹…mbsâ‹…h2â‹…num_layers_in_next_ppâ€‹

While the communication time for P2P transfer is:

tcomm=seqâ‹…mbsâ‹…hpeak_bwtcommâ€‹=peak_bwseqâ‹…mbsâ‹…hâ€‹

For effective overlap, we want:

tcommtcompute=peak_flops32â‹…hâ‹…num_layers_in_next_ppâ‹…peak_bwâ‰¤1tcomputeâ€‹tcommâ€‹â€‹=32â‹…hâ‹…num_layers_in_next_ppâ‹…peak_bwpeak_flopsâ€‹â‰¤1

Similar to TP, this ratio is independent of sequence length and batch size. It depends on the hidden size h, number of layers in the next pipeline stage, and the ratio of compute to P2P bandwidth capabilities of the hardware.

### Citation

For attribution in academic contexts, please cite this work as

Tazi et al., "The Ultra-Scale Playbook: Training LLMs on GPU Clusters", 2025.

BibTeX citation

@misc{ultrascale_playbook,
      title={The Ultra-Scale Playbook:Â Training LLMs on GPU Clusters},
      author={Nouamane Tazi, Ferdinand Mom, Haojun Zhao, Phuc Nguyen, Mohamed Mekkouri, Leandro Werra, Thomas Wolf},
      year={2025},
}

### References

2. Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and OverlappingÂ â€‚[[PDF]](http://arxiv.org/pdf/2409.15241.pdf)  
    Wang, G., Zhang, C., Shen, Z., Li, A. and Ruwase, O., 2024.
3. Striped Attention: Faster Ring Attention for Causal TransformersÂ â€‚[[PDF]](http://arxiv.org/pdf/2311.09431.pdf)  
    Brandon, W., Nrusimha, A., Qian, K., Ankner, Z., Jin, T., Song, Z. and Ragan-Kelley, J., 2023.
4. Breadth-First Pipeline ParallelismÂ â€‚[[PDF]](http://arxiv.org/pdf/2211.05953.pdf)  
    Lamy-Poirier, J., 2023.
5. DeepSeek-V3 Technical ReportÂ â€‚[[PDF]](http://arxiv.org/pdf/2412.19437.pdf)  
    DeepSeek-AI, and others,, 2024.
6. Zero Bubble Pipeline ParallelismÂ â€‚[[PDF]](http://arxiv.org/pdf/2401.10241.pdf)  
    Qi, P., Wan, X., Huang, G. and Lin, M., 2023.
7. Mixtral of ExpertsÂ â€‚[[PDF]](http://arxiv.org/pdf/2401.04088.pdf)  
    Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T. and Sayed, W.E., 2024.
8. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient SparsityÂ â€‚[[PDF]](http://arxiv.org/pdf/2101.03961.pdf)  
    Fedus, W., Zoph, B. and Shazeer, N., 2022.
9. A Survey on Mixture of ExpertsÂ â€‚[[PDF]](http://arxiv.org/pdf/2407.06204.pdf)  
    Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S. and Huang, J., 2024.
10. GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingÂ â€‚[[PDF]](http://arxiv.org/pdf/2006.16668.pdf)  
    Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020.
11. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-AwarenessÂ â€‚[[PDF]](http://arxiv.org/pdf/2205.14135.pdf)  
    Dao, T., Fu, D.Y., Ermon, S., Rudra, A. and RÃ©, C., 2022.
12. FP8-LM: Training FP8 Large Language ModelsÂ â€‚[[PDF]](http://arxiv.org/pdf/2310.18313.pdf)  
    Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., Li, R., Zhang, M., Li, C., Ning, J., Wang, R., Zhang, Z., Liu, S., Chau, J., Hu, H. and Cheng, P., 2023.
13. torchao: PyTorch native quantization and sparsity for training and inferenceÂ â€‚[[link]](https://github.com/pytorch/ao)  
    maintainers, t. and contributors,, 2024.
14. Small-scale proxies for large-scale Transformer training instabilitiesÂ â€‚[[PDF]](http://arxiv.org/pdf/2309.14322.pdf)  
    Wortsman, M., Liu, P.J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J.D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J., Gilmer, J. and Kornblith, S., 2023.

[^1]: An Empirical Model of Large-Batch TrainingÂ â€‚[PDF](http://arxiv.org/pdf/1812.06162.pdf)  McCandlish, S., Kaplan, J., Amodei, D. and Team, O.D., 2018.
[^2]: Mixed Precision TrainingÂ â€‚[PDF](http://arxiv.org/pdf/1710.03740.pdf)  Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G. and Wu, H., 2018.
[^3]: Reducing Activation Recomputation in Large Transformer ModelsÂ â€‚[PDF](http://arxiv.org/pdf/2205.05198.pdf)  Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M. and Catanzaro, B., 2022.
