
能夠算是問題跟答案的資料 對Alignment有幫助呢 這邊引用另外一篇 其實稍微早一點的論文
              
                  23:43
                  來解釋這個現象 為什麼這個只要拿怪怪的資料 Alignment其實就有可能有好的結果呢 因為其實Alignment前後 雖然你覺得模型的答案 好像很不一樣 但是模型實際的行為 也許差異沒有這麼大 在這篇論文裡面就做了一些分析 他們想要分析一個語言模型 在Align前跟Align後 他們的行為差異有多大 那怎麼判斷一個模型 怎麼比較Align前跟Align後 模型的差異有多大呢 要先給Align後的模型 一個未完成的句子 比如說How are you問號I add 這個模型可能就輸出fine 那實際上模型輸出的是一個機率分佈 那fine可能是機率最高的一個Token 接下來把同樣未完成的句子 丟給Alignment前的那個模型 接下來會分成三個狀態 第一個狀態 如果Alignment前的模型 輸出的機率分佈 fine也是機率最高的Token 那這種狀態叫做unshift
              
                  24:48
                  那如果fine這個Token呢 它的機率排名是所有Token的第二名或第三名 叫做Marginal 就代表有一點點的變化 那如果Find呢 不在前三名的話 那就叫做Shift 代表說現在Alignment前的模型 跟Alignment後的模型 他們的行為有很多 很大的差異 那我們先看一個論文上的例子 在這個論文裡面就先舉了一個例子 他說假設你問一個Align的模型 哪一種品種的狗是最小的狗啊 那Align的模型當然可以回答你這個問題 但是他回答的這個答案 跟Align前的模型差距有多少呢 藍色的詞彙 藍色的Token 代表是Unshift的Token 代表說Align前後模型都覺得 這個Token就是機率最高的 Marginal代表機率指標 變了一點點 只有Shifting才代表機率有很大的變化 那你會發現說機率有很大變化的那些詞彙 其實都是一些比較像是連接詞的詞彙 或者是打招呼的詞彙
              
                  25:52
                  比如說這個Align的模型 一開頭他會說Thank you for asking 現在模型很喜歡開頭講這種話 但是pretrain的模型 他可能就不知道開頭要先感謝人類 那Align的模型才知道開頭要感謝人類 所以有一些地方確實Align的模型 跟沒有Align的模型 他的行為有很大的差距 但是有很大差距的地方 好像都是一些連接詞的地方 那這邊論文也做了比較完整的統計 他們比較了 LLaMA-2 -7B的BASE模型 跟Chat模型之間的差異 LLaMA-2-7B的模型 跟VICUNA的模型的差異 還有Mistral-7B的模型 跟Mistral-7B-instruct的差異 那這邊分別展示了 當模型在回答問題的時候 有多少比例的Token是Unshift 一個Token是Marginal 多少比例的Token是Shift 那你發現說比較Align前後的模型 Shift Token的比例非常非常的少 就代表說模型在Align前後 它的行為其實是沒有那麼大的變化的 那你可能問問說 為什麼答案看起來差那麼多
              
                  26:55
                  行為沒有很大的變化 為什麼答案差那麼多 那你知道這個模型做文字接龍的時候 就是一步錯步步錯 中間有個地方錯了 那接下來接的東西就會非常的不一樣 所以實際上 雖然Align前後看起來答案差很多 但模型的行為並沒有非常大的差距 那是哪一些Token被Shift呢 如果我們看這些Token的分佈 你會發現說 多數被Shift的Token都是一些連接詞 而且你會發現說 這三個Case裡面 都有代表結束的符號 代表說模型在Align前後 他會不會輸出結束符號這個Token的能力 倒是差異蠻大的 那等一下會再用到這個概念 好 所以我們已經知道 模型Align前後 它其實真實的行為 沒有那麼大的差異 所以也許Align 其實是一件容易的事情 那有一篇paper就做了這樣的嘗試 他說
              
                  27:57
                  一般我們做Align的時候 你到有一個問題 有一個答案 這個叫Instruction Tuning 我們能不能沒有問題 只有答案 既然Align前後真正的差異 可能只是某幾個詞彙輸出機率的差異 會不會根本就不需要想問題 直接叫模型學習產生某個樣子的答案 就可以展現Align的能力了呢 這篇paper確實發現他們把他們這個方法 叫做response tuning 就只拿答案來fine-tune模型的輸出 不給模型問題 他們發現response tuning 居然是一個 蠻有效的方法 左右兩邊的圖 分別是兩個不同的base model 左邊是拉瑪3.18B 右邊是Gemma 2.9B 那最右邊的這個bar 代表的是 沒有做過fine tuning的base model 然後這邊不同的顏色 代表使用者的接受程度 他們這個是直接找人來打分數的 深藍色代表使用者覺得
              
                  29:00
                  這個模型的答案 好 然後淺藍色代表還可以 那淺色淺灰色代表 模型的答案是不能接受的 那你發現說在有fine-tune之前 那個base model他的答案 都是人類不能接受的 但是一旦有fine-tune之後 一旦有alignment之後 不管你是用instruction的fine-tuning 就是有一問一答這樣的資料 還是做response tuning 他們這邊縮寫叫做RT 就只教模型產生答案 根本沒給他輸入的問題 居然只給模型答案 也可以得到很不錯的結果 但看起來只給模型的答案 答案還是比instruction tuning還要稍微差一點 但你可以看多數的狀況 用Albaca的資料 用Dolly的資料 這也是某一個資料集 還有用Lima的資料 在兩個不同的模型上 都可以用response tuning的方法 讓模型只學習產生答案 根本沒給他問題 就可以得到不錯的alignment的效果 那其實這個response tuning的發現 稍微找一個月的paper裡面 就也已經發現了
              
                  30:03
                  有一篇paper叫做 instruction following with without instruction tuning 他也發現了alignment其實很容易 在這邊paper裡面 作者試了三個方法 第一個方法就是response tuning 那第二個方法呢 是隻交模型一個任務 發現模型有非常強的 舉一反三的能力 那我這邊要講的是最後一個方法 最後一個方法他們是說 既然align前後真正的差異 只是某些token的差異 我們能不能就直接改那些token出現的機率就好 連fine tune都不fine tune 直接改token輸出的機率 會不會就可以讓 沒有align的模型它的表現 類似alignment的模型呢 所以他們就定了三個規則 來改變沒有align的模型 它輸出的token的機率 第一個是改結束符號的機率 因為剛才發現說 有沒有align的模型 其實差距最大的 在所有不同狀況裡面都有差的 就是會不會產生結束的符號 沒有align的模型 一個最常見的鏡頭
              
                  31:06
                  很容易講話講不停 不斷的暴走 不斷的反覆說他已經說過的話 那就是因為他一直不產生結束的token 所以他只好一直講下去 但是有align的模型 他比較知道什麼時候應該不要再講下去 所以這篇論文的第一個規則就是 既然現在沒有align的模型 他的問題就是 講話講不停 那就增加結束符號的機率 而且隨著答案越長 結束符號的機率就增加越多 所以這第一個規則 第二個規則就是手動改變一些符號出現的機率 有某些詞彙 比如腳括號 I will watch 機率改小一點 這些符號機率改大一點 最後一個呢 就是沒有align的模型很容易講 他前面已經講過的話 他很容易反覆講一樣的內容 所以怎麼辦呢 就給他一些penalty 就如果講重複的token 輸出重複的token 那重複的token 機率就比較低一點 強制手動設計了一些規則之後 什麼事都沒有做
              
                  32:08
                  沒有反應任何模型 沒有改參數 只是強制增加了一些規則以後 本來base model 如果跟instruction model比 他們這邊是把base model跟instruction model 答案拿去評比 然後用一個語言模型去評說 哪一個模型的答案是比較正確的 那原來的base model 他贏過instruction model的機率 只有2%左右 他完全是被instruction model掉著打 但是當把這三個規則apply上去以後 今天這個base model 居然有24% 將近四分之一的機率 可以贏過instruction model 所以加上這幾個規則以後 base model它的輸出 突然就起飛了 就變得跟instruction model有點像 可以有跟instruction model一戰的能力 然後他們有嘗試拿掉這三個規則 發現這三個規則都是有用的 當然只憑這規則是沒有辦法真的做得非常好啦 但這篇paper就是告訴我們說 就算沒有fine-tune模型光憑這一些規則 它也可以做出一個看起來還不錯的模型 所以看起來alignment做的事情並沒有那麼多
              
                  33:17
                  alignment很容易這件事啊 就解釋了為什麼self-alignment這個技術是有可能可以成功的 那self-alignment有很多paper都提出了self-alignment的想法啦 那我想最有名的就是去年年初的self-rewarding model 這個是meta的paper 啊這些論文講的是什麼呢 他提出了一個乍看之下很神奇 但仔細想想如果你知道說 alignment其實很容易 就不會覺得這個方法很神奇的一個技術 這個技術是這樣做的 他們不用alignment的資料 他們希望沒有alignment的模型 可以自動alignment 怎麼做 他們先問模型一個問題 讓模型產生多個答案 y1 y2 y3代表三個不同的答案 把這些答案呢 都拿去給模型自己評分 那當然你可能會需要提供給模型一些指示 那不同的論文提供的指示不一樣 你可能會定一些規則告訴模型說 什麼樣的答案是好的 什麼樣的答案是不好的 那模型給予三個答案評分 然後再拿這些評分 對沒有align的模型
              
                  34:20
                  做reinforcement learning 然後就反覆這個過程數字 那這個模型能力就突然起來了 也不能說能力起來 就是說這個模型突然之間看起來 就像是有做了alignment一樣 但是如果你瞭解說從沒有alignment變成有alignment 中間的變化其實沒有那麼大 你就比較容易想像說 為什麼只給了一些評分的指示 就有辦法把模型從沒有alignment的樣子 變成有alignment的樣子 所以我們知道說 其實alignment並沒有對pre-trained的模型 造成非常非常大的變化 所以今天之所以alignment以後 模型能力很強 因為pre-trained非常的有效 在pre-trained的時候 模型已經學到很多事情 但是要怎麼樣才能夠做出 alignment之後有效的pre-trained模型呢 那這篇論文就提供了一個思考的方向 那這篇論文是一整個系列中的第三篇 這個系列叫做physics of language model 就是現在很多人都在study language model
              
                  35:25
                  但是有關language model 完整的觀察跟理論都非常 所以這篇論的作者 是希望建構 language model的物理學 先透過大量的觀察 尤其現在呢 一般你都從網路上隨便拿一個language model 別人train好的來進行分析 這些language model training的過程中到底發生了什麼事 你無從得知 所以這群作者呢 他們的language model 都是自己train from scratch的 他們都是自己synthesize資料 自己訓練模型 希望能夠控制整個language model訓練的過程 這是physics of language model的研究 好 那這邊就是引用他的part 3.
              
                  36:06
                  1 在這個研究裡面 他們想要知道什麼樣的train才會有效 才能讓alignment後有強大的能力 他們先製造了一大堆人的簡歷 他們就仿造那個wiki上我的風格 就創造了一大堆人的簡歷 但每個人只出現一次 比如說這個愛因是買購的吉他手 同時也是宇秋女子學員高中一年級的學生 還有燈也是宇秋女子學員高一的學生 是天文部唯一的社員 然後是買購的主場 有這樣的普遷資料以後 去普遷出一個模型 接下來呢 他的普遷資料裡面包含了 n個人的資料 n個人的資訊 然後接下來把這n個人裡面的 二分之n個人拿出來 製造一些可以做alignment的問題 比如說 把愛因拿出來 然後呢 你就製造一個問題 說誰是買購的節奏吉他手 答案就是愛因 那你就拿這個資料
              
                  37:08
                  再去微調模型 得到一個可以回答問題的模型 那接下來呢 把沒有用在alignment裡面的人 自創造一些問題以後 去問alignment之後的模型 比如說燈的資料 沒有出現在alignment裡面 就拿燈的資料來創造一個問題 比如說問誰是買購的主場 你可能會想說 雖然在alignment的時候 沒有看過燈相關的資訊 但pre-trained的時候 已經知道燈相關的資訊了 那模型既然會回答問題的話 他應該可以知道 他應該可以回答出 買購的主唱就是燈吧 但你會發現 模型是沒辦法回答這個問題的 他的正確率是0% 在這個任務裡面 他的正確率是趨近於0 那明明在pre-trained的時候 就已經看過了 alignment的時候 也學過類似的提醒 為什麼在測試的時候 沒有辦法回答呢 為什麼沒有辦法回答呢 這個作者其實做了一個分析 這個分析是這樣子的
              
                  38:11
                  他們有分析模型的行為 那細節我們就不講 留給大家自己看論文 他們發現說 假設在你的pre-trained資料裡面 每個人只有出現一次 模型對於這個pre-trained資料 會有巨大的誤解 比如說對他來說 有關高松燈的資訊 就是他是宇秋女子學員高一學生 是天文社的一員 是買購的主唱 你以為模型可以學到高松燈 他是讀什麼學校 他參加哪一個社團 他是買購的主唱 你以為他可以學到這件事 其實不是模型學到的事情是 有一個東西叫做高松燈 而且是宇秋女子學員高一學生 而且是天文部 這些東西合起來 是一個entity 他是買購的 但是你仔細想想 你從語言模型的角度 來思考他是怎麼學習的 他會這樣學到這樣子的結果也沒有很奇怪 因為語言模型在pre-trained的時候 他唯一學過的東西 就是文字接龍 他根本不知道燈是什麼東西
              
                  39:15
                  對他來說他學到的是 有一個這樣子的東西 是燈 又是高一的學生 又是天文部的 這個東西這樣全部的資訊合起來 才是買購的主唱 那高松燈不是買購的主唱 這些東西合起來才是買購的主唱 所以他對一個人的資訊是有非常大的誤解的 這是為什麼他沒有辦法回答誰是買購的主唱 但是如果今天同一個人的資訊 有各種不同的介紹方式 就比較有機會讓模型正確的理解這個人相關的資訊 所以你可能有另外一個介紹 另外一個介紹 其實內容差不多 只是詞彙的順序換了 先說高松燈是買購的主唱 再說他是宇秋女子學員高一學生 再說他是天文社的議員 這個時候模型就知道說 原來高松燈他是一個entity 他有什麼性質 他可能是買購的主唱 他可能是某個學校的學生等等 所以我們發現說
              
                  40:21
                  同樣的資料 同一個人有很多不同的介紹方式 其實對於pretrain是一件重要的事情 而對於真實的pretrain來說 真實的pretrain通常就是拿大量網路上的資料來訓練 那高松登的資料你可以在Wiki上找到 那你也可能可以在某兩百科上找到 他們介紹可能是有一點不一樣的 所以同一個人你確實可以找到各種不同版本的介紹 那在這篇paper的實驗裡面 他們就試著去產生了同一個人 不同版本的介紹再拿去做pretrain 然後做Alignment的時候 只有拿某二分之N個人 的資訊拿來製造Alignment的資料 問問題的時候 測試的時候問剩下另外2N個人 Alignment沒有看過的人 他的相關資訊 這個時候模型就可以正確的回答問題 而且有非常高的正確率 而這篇論文還有另外一個發現 他發現說 不需要所有的角色 都有多種版本的介紹 只要訓練資料裡面 只要普遍資料裡面
              
                  41:24
                  有一些角色有多種版本的介紹就夠了 比如說愛因的資料 有兩種不同的版本 模型就足以學到說 愛因是一個人 他有這樣子的性質 而看過愛因有各種不同的介紹 可以讓模型的思維 但這個是用比較擬人化的想法 就模型的理解 模型對世界的理解就變了 他就知道說 原來不是一大段的東西 才是一個entity 原來愛因這樣是一個名字 他原來可以有不同的性質 原來我要這樣理解一段 文章 所以就算高松登的介紹 只有一段只有一個版本 模型也可以正在pre-trained的時候 正確解讀說 買購主唱跟宇秋女子學員 是高松登的性質 所以在pre-trained的資料裡面 你只要有部分的entity 有多種不同的版本 似乎就足以讓模型 在pre-trained的時候 可以學到更泛用的知識 可以強化模型理解的能力 所以他們實驗結論
              
                  42:29
                  結果就是 假設在N個人裡面 只有某一些人 他的介紹有多個版本 那如果沒記錯的話 應該是指他們只加了十分之一 只有十分之一的人 他的介紹有多種不同的版本 那接下來呢 你在做alignment的時候 只拿那些有多種不同版本人的資料 去製造alignment的資料 然後在做測試的時候 其實也可以得到80%左右的正確率 那這邊是放了一個大約的數字啦 因為在paper裡面 其實他們嘗試了非常多不同的setup 包括訓練了各式各樣不同大小的模型 得到的結論都是一致的 所以我們現在知道什麼樣的pre-trained資料有效 就是同樣的東西要有多種不同的版本 也就是多樣化的資料 對於pre-trained是有幫助的 但你不需要所有相關的資訊 所有的knowledge通通有多樣化的資料 只要有部分的knowledge有多樣化的資料 可能就足夠了 當然需要大量的資料來pre-trained
              
                  43:32
                  也不是什麼新聞 這個大家早就都知道了 那其實在2023年的機器學習 已經花了很多時間 講了大資料的重要性 那大家如果有興趣的話 可以看看過去的這個上課錄影 你還是可以學到很多東西 好那現在啊 這個時間點pre-trained一般都用多大的資料嗎 我們來看看最近幾個比較知名的模型 用了多少資料 拉瑪3用多少資料呢 用15T個Token D6V3用了多少資料呢 用14.8T個Token 所以現在一個好的pre-trained model 都需要大量的資料 那這個資料多到什麼地步呢 多到已經有人擔心 會不會網路上所有可以取得的資料 已經要被用盡了 那伊利亞呢 甚至在Nuris給的一個talk 也提到了這個概念 農場文呢 很喜歡講這件事情 這個過去其實在22年的時候
              
                  44:36
                  哇 這個是還沒有切GPT的時代 這個死前時代 就有人討論過這個問題 那可能想要死前時代 他怎麼知道LLaMA 3 這個數據是那個 同一篇paper他後來再補上的 但這篇paper最早放到archive的時間 是22年的11月 是死前時代 就有人開始擔心說 會不會這個 現在訓練 訓練語言模型 很快就要把網路上能夠用的資料用盡了 那這篇paper裡面講的內容是什麼呢 這個縱軸啊 指的是token的數量 橫軸指的是時間 那這個綠色的這個線呢 代表的是網路上大概有多少token 他們去估計了一下 網路上可能有的token 再推測了一下未來token可能增長的數量 得到了綠色這條線 但這個估計 是有一個range的 因為他這個估計不可能太精準 他們的估計方法 其中一個就是 看這個在做google search的時候 你打某一個關鍵字 可以搜尋到多少文章
              
                  45:38
                  然後再反推說這個關鍵字 在一篇文章裡面出現的機率 大概有多少 假設呢 某一個詞彙 那在google search的時候 可以搜尋到一萬篇文章 這個詞彙呢 在一萬篇文章裡面會出現一篇 那你就知道網路上的文章是一萬乘以萬 那你就可以估測說 網路上大概有多少個token 他們是大概這樣估算網路上的token的 這個藍色的線呢 是他們預估 未來在訓練的時候 需要用到多少token 那這條線 是怎麼估算出來的呢 是參考兩個數據啦 一個數據是現在在訓練模型的時候 用到多少token 還有大概 人類可以製造出 在未來幾年可以製造出 多少的算力 把這兩者平均以後 然後最後呢 會被這個token的數量 所放得住 因為在網路上可能找不到更多token了 所以你想要做更大的訓練 可能也沒辦法了 就是我們人類需要token的數量 比網路上token的數量增長的
              
                  46:43
                  還要更快 當然現在啊 這個15T的token相較於整個網路上有的資料 其實還是有一大段距離啦 你看這個距離你不要看很近 這個是log scale啊 所以現在用的資料大概是網路上有的資料的 1% 但是呢 如果你看LAMA2 LAMA2用了1T多的資料 LAMA3用了15T的資料 一年之內增長的速度 是10倍 那網路上的Token的數目增長沒有10倍 所以大型語言模型所用的訓練資料 可能很快就會追上 網路上所有Token的數目 那這邊文章的估測是 大概在2028到2030年 我們就會用盡 網路上所有能夠訓練的Token 那還估測了另外一個數值 這個另外一個數值是這樣 他假設人類可能會 Overtrain你的模型 所以Overtrain模型的意思就是說 本來如果你的算力有限 那你的模型如果越小 你會用的訓練資料就會越多 因為你的算力是固定的模型小 你就可以放比較多的算力
              
                  47:48
                  在增加訓練資料上面 假設說未來大家會想要開發比較小的模型 因為看現在 其實現在的趨勢就是 雖然有人會開發大模型 但你真正能用的 其實小模型 你在作業裡面 你是不是都扣不了70B的模型 你主要扣的都是7B的模型 事實上在一般筆電裡面 7B模型還是太大了 在筆電上跑7B模型也是有困難的 所以其實現在開發的一部分面向 已經走向開發小模型 而不是開發一個真正非常大的模型 假設說人們想開發小模型 然後會把算力投資到 使用更多的訓練資料上 那資料用盡這件事呢 會比我們想像的更早到來 可能會在2028年之前 就來到訓練資料用盡的狀態 不過這個就是一個對未來的預測啦 好 那你可能會想說 LAMA3、D6V3都用了15T的資料 我上哪裡去找15T的資料呢 我告訴你現在15T的資料 是唾手可得的 因為HackingFace釋出了一個資料集 叫做FineWeb
              
                  48:51
                  這個資料集是他們 查了網路上的資料以後 經過清理得到的 也正好有15T的 所以現在15T的 每個人手上都有 你插的只是算力而已 你沒有辦法算這麼 你沒有辦法做這麼大規模的計算 所以如果假設你有算力 要催一個有15B的token 用15B的token來訓練模型的話 其實資料已經在這裡了 HackingFace已經釋出 你可以用的資料了 剛才講說呢 HackingFace花了很大的力氣去清理資料 因為資料的品質 是非常重要的 有一篇論文叫做 Textbook 這是Microsoft他們打造 Phi系列的其中一篇論文 這篇論文是說 他們發現pretrain資料的品質 對於模型最終的能力 有非常大的影響 他們試了三個資料的來源 第一個資料的來源是網路上爬的一些資料 他們主要是想要訓練可以寫成 程式的模型網路上爬了一大堆的程式
              
                  49:56
                  另外呢 第二組訓練資料是 這個程式語言的教科書 第三組訓練資料 是程式語言的教科書 後面還附上習題 那麼就訓練了三個不同版本的模型 那在三個不同case 都用一樣量的訓練資料 那會發現說 用教科書加上 後面有習題的這種資料 訓練出來的結果是最好的 這個縱軸是在程式的資料集上回答問題的正確率 那數值越高代表結果越好 但是這邊有一件事情是 大家不常看到這個結果不常提到的 就是你想你可能會問說這些教科書哪裡來 這些教科書其實是GPT3.5生的 他不是真正的教科書 是這篇paper是叫GPT3.5去寫一些語言模型 幻想出來的教科書 再拿去訓練 所以這篇paper 不完全可以告訴你 textbook is all unique
              
                  51:00
                  所以這篇paper並沒有辦法完全驗證說 只要你的資料像是教科書 就可以增強模型的能力 在pretrain的時候很有用 他也有可能會這麼有用 是因為這些文字 就是check GPT生出來的 用check GPT生出來的文字 特別能強化模型的能力 我們剛才在講LIMON的時候有看到說 check GPT隨便講點什麼 你的模型跟他講一樣的 你的模型能力就突然之間暴漲 這是另外一篇 這是Apple的paper 那篇paper的title呢 叫做refreshing the web 他的title可以猜到他在做什麼 他做的事情就是從網路上搜尋到的資料 從網路上爬下來的資料 往往太髒了 所以他們先用一個refresh的model 這個模型呢會換句話說 那他們用的其實是mixtro7b啦 就有一個可以換句話說的模型 把網路上的這一些文章改寫一下 改寫成比較容易讀的 然後再把這一些改寫過的文章 加到pre-trained的資料裡面再去訓練 他們發現加上這些改寫過的文章
              
                  52:05
                  這些改寫過的文章 他可能內容比較乾淨 他可能用字遣詞比較不奇怪 他可能是人類比較容易讀的 這樣子的資料比較適合拿來訓練模型 那右邊就是實驗結果 那縱軸是模型的表現 分數越高代表模型 表現越好 橫軸是指用了多少的token 不同顏色的線代表不同的資料集 那他們這邊比較的對象 有C4 還有這個應該是 這個應該是那個refine web 這個refine web跟剛才那個hacking face的fine web 其實不同的東西 但是不同團隊做的 只是取的名字有點像而已 然後他們就發現說 如果在同樣token數量的情況下 用有改寫過的pre-trained資料來訓練模型 是比較有效的 如果要達到同樣的正確率 那有沒有改寫過的資料 那你需要花另外三倍的資料量 才有辦法得到差不多的能力 所以告訴我們資料的品質是很重要的 所以今天你如果要pre-trained模型的話
              
                  53:10
                  直接從網路上爬到的資料 往往沒有辦法直接用 你需要經過多個不同的步驟進行清理以後 才能夠拿到真正可以pre-trained模型 那這張圖呢 是引用至一篇叫做refine web的paper 那這篇paper裡面就是講說 他們怎麼打造了focus這個model 那他們用的資料清理的過程 就放在這個圖裡面 那你會發現說 原來他們是使用一個叫做common core的資料集 這是一個從網路上爬資料的project 他們用這個從網路上爬資料的project裡面爬下來的資料 經過諸多清理最後只保留了 大約十分之一的資料來做語言模型的pre-trained 那在這個訓練的過程中啊 你會發現有好幾個步驟 都是在去除重複的文章 比如說replication removal fuzzy duplicate還有exact duplicate 都是為了去除掉重複的文章 或者是重複的段落 那為什麼去除重複的文章 重複的段落是一件重要的事情呢
              
                  54:14
                  因為已經有文獻發現說 假設在固定的算力固定模型大小 小的情況下 那你應該要盡量讓你的模型看不同的資料 因為有的會想說 到底應該讓模型看一樣的資料 但是不斷的複習 強化他在同一組資料上面的理解 還是讓他盡量看不同的資料呢 那這篇論文告訴我們 你應該盡量讓模型接觸各式各樣不同的資料 而不是讓他反覆看一樣的內容 那這篇論文呢 這邊就是引用那篇論文的一個實驗結果 橫軸代表所使用的pre-trained token的數目 那縱軸呢 是一個這種類似capacity的loss 反正縱軸的值是越小越好 那虛線代表的是 你不斷的讓模型看不同的資料 都不重複 所以這邊所謂的1.2T 指的是1.2T完全不一樣的token 如果你讓模型看不一樣的token的話 它在testing data上的loss
              
                  55:18
                  會呈虛線這個狀態下 但是如果假設你全部的token數目就只有12b 你全部只有12個billion的token 那你讓模型反覆看一樣的token 反覆看一樣的token 會發生什麼事呢 當模型看到四次token以後 當模型反覆看四次一樣的token以後 當他同樣的資料看到第四次以後 你就會發現有看重複的資料 跟沒有看重複的資料 開始有顯著的差距 當模型把同樣的token 這個資料看超過40遍以後 它基本上就不會再有能力的增長了 所以今天你需要盡量讓模型 看不同類型的資料 看不同樣子的資料 那在其他的模態上 其實結論也是一樣的 那這個實驗是我們實驗室 劉廷偉同學做的實驗 那麼實驗是主要是做語音相關的研究 所以這邊做的是語音的模型 那跟文字的模型其實也很多地方很類似 就是你可以pretrain一個語音的模型 然後把它fine-tune在各式各樣不同的語音任務上
              
                  56:24
                  那這邊比較的就是 假設在固定算力的情況下 我們到底應該要把960小時的資料看一遍 還是一個小時的資料看960遍呢 那結果是非常顯而易見的 越深色就代表說模型看了越多資料 越淺色就代表模型反覆看了越多同樣的資料 那上面是在某五個任務上面的表現 那上面五個任務都是數值越小越好 下面這五個任務都是數值 數值越大越好 他們是什麼任務在這邊也沒有那麼重要 反正就是模型看過越多不同的聲音 它的表現在下游任務上面的表現就越好 而如果模型太常反覆複習同樣的輸入 太常反覆複習常常的picture data 對模型是會有傷害 好那我們剛才講到說focus那個LLM 它有一個非常複雜的data filtering的pipeline 那你可能會問說 那什麼樣的data 它filtering的過程才是有效的 對pre-trained完在align後的模型是有幫助的呢 對pre-trained模型是有幫助的呢
              
                  57:29
                  那我告訴你hugging face啊 那個fine web那個計畫 他們有一個blog裡面就告訴你 怎麼樣過濾資料是最好的 而且他們過濾資料的方式 不是那種腦洞一開覺得 這個方法應該好 就告訴你這個方法好 他們每一個步驟都是有做實驗的 就在某一個步驟 有三個不同的方法可以決策 都試一下告訴你哪一個方法 最後pre-trained出來的模型 會是最好的 而且他們不是隻看pre-trained模型的capacity 他們是看這個pre-trained的模型 如果用在下游任務上面 在好幾個大家會關心的任務上面 它的正確率的平均是如何 他們是用這個方法 打造了FineWeb這個資料集 那他們很自豪的地方就是 在同樣token數目下 FineWeb是比其他的資料集都還要好的 比review 也比C4好 C4好 C4是那個GPT3用的資料 就common core再經過清理 變成C4 GPT3就是拿C4來進行訓練的 然後不過那是久遠的事了
              
                  58:32
                  那個時候OpenAI還不願意告訴大家 他們在做什麼的時候 現在已經沒人知道 他們拿什麼樣的資料來進行訓練了 好 那所以總之就是 說是過濾資料 也是很重要的 如果你想知道怎麼過濾資料的話 那有好心的已經告訴你 哪些步驟有用 哪些步驟沒有用了 好 在最後一小段 我們要講 Alignment的極限 我們已經看到說 Alignment主要就是改變了模型書寫的風格 它只是讓某幾個Token輸出的機率做改變了 既然如此 那Alignment這件事情 會不會非常有極限呢 早在2023年的年初 在古代就已經有人發現 Alignment這件事情感覺蠻有限制的 在2023年的年初 那個時候Meta呢 釋出了Lama 1 有了Lama 1以後 就一堆人去對Chain GPT做逆向工程 用弄出一堆 可以拿來訓練Base Model的Alignment資料 去微調Lama 1 那一開始很多人都說 你看我這個fine-tune過後的模型
              
                  59:37
                  應該有這個GPT4的九成能力 有那麼多 有很多的這種炫耀文 但是過了一兩個月之後 就有人寫了這篇文章 他發現說 這些fine-tune過後的模型 表面上看起來很強 但實際上並沒有ChatGPT那麼厲害 這些模型最常會有的狀況就是 他輸出的答案看起來像模像樣 但是內容是錯的 比如說這邊就是問這種 fine-tune在LLaMA 1上的模型說 Actor Critic比Reinforce這個Algorithm好在哪裡 那他也洋洋灑灑講了一大堆話 他就列一二三四 加Overall 你知道ChangeBT最喜歡結尾是Overall了嗎 所以看起來他就是很能夠模仿 ChangeBT這種Align或後的模型 他的書寫的風格 但是紅色就代表說 這些模型講的話 這個模型講的話是錯的 所以這個模型他講錯了 非常多的內容 所以看起來Alignment 是有他的極限的
              
                  1:00:42
                  Alignment能夠對模型的影響 是有限制的 那到底什麼事情是可以透過Alignment 影響模型 什麼樣的資料是Alignment學的會的 什麼樣的資料是Alignment學不會的呢 這篇論文就提供了一個比較完整的分析 他們把Alignment的資料分成四類 第一類是模型本來就會的東西 就你模型你問模型這個問題 他做Greedy Decoding 所謂Greedy Decoding的意思就是 每次做文字接龍的時候 都選機率最大的那個Token 模型回答出來的答案本來就正確 就代表說這筆Alignment的資料是模型本來就可以 答出正確答案來的Base Model 沒有Alignment只有pretrain的模型 本來就會的知識 那前面這邊呢 我特別講說 會放範例問題跟範例答案 是因為說這些Base Model 通常他不太有Follow Instruction的能力 所以你問他一個問題 他不見得要回答你 他可能會出四個選項給你 或反問你更多問題 所以通常你要這些模型真的回答問題 那你需要前面放一些範例
              
                  1:01:48
                  做In-Context Learning 比較容易能夠控制它 真的能夠回答你輸入的問題 所以能夠回答 Base Model本來就會的東西 叫做Highly known的問題 有一些問題是 用某一組範例問題跟範例答案 Base Model做Greedy Decoding不會得到正確答案 但是換了一組範例問題跟範例答案 就可以得到正確答案了 這種叫Maybe known 就模型應該有這方面的知識 但你問法要問對 你要問對問題 模型才能夠給你答案 Base Model才能給你答案 第三類的問題叫做weakly known weakly known就是說 今天你問模型問題的時候 他不一定可以給你正確答案 但如果你做Sampling 你Decode的時候 有值骰子 每次得到的答案都不一樣 有機會讓Base Model 得到正確的答案 有Base Model覺得 這個問題有很多不同答案的可能性 所以他需要值一下骰子 才有機會得到正確的答案 這叫weakly known 還有Unknown就是你怎麼做Sample
              
                  1:02:51
                  模型都無法答對的 那是模型真正不會的 這種叫做Unknown 你直覺可能會覺得 應該是要Unknown的資料訓練模型 才有用吧 這些模型都已經會的知識 拿來訓練模型 有什麼特別的意義嗎 有什麼特別的用處嗎 但是實驗的結果 跟人類正常的直覺是相反的 我們來看一下這篇論文的實驗結果 這篇論文就是把剛好 上述各種不同的資料 全部倒在一起 進行訓練 但是他們在分析結果的時候 他們把訓練資料裡面的 弄的那一些 跟unknown的資料 他的正確率 拆開來看 你的訓練資料裡面 我們說剛才說有四類 前三類都算是弄的 最後一類算是unknown的 他把所有資料都倒在一起進行訓練 但訓練的過程中 正確率的增長程度 跟unknown的問題 正確率的增長程度
              
                  1:03:54
                  那你發現弄的問題正確率 增長得非常快 這也沒有什麼令人意外的地方 因為這些問題模型本來就會了嘛 所以稍微微調一下 他馬上就可以給你正確答案 而unknown的問題模型需要一定程度的學習之後 才能夠得到正確答案 但是重點來了 在訓練完之後 你會把模型測試在 development set上訓練的時候 沒有看過的資料上 你發現當模型 開始學習這些unknown資料的時候 就是他在development set上面壞掉的時候了 一開始他都是學這些known的東西 development set上進步很快 但當他開始想要學這些unknown的東西的時候 他就開始壞掉了 他在測試資料上的正確率就開始下降 所以看起來 讓模型學這些他本來就有可能知道的東西 但當你叫模型去學那些他不知道的東西的時候 這個時候你反而在破壞模型的能力 他不只沒有學會新的知識
              
                  1:04:57
                  他反而能力是下降 好在剛才那四類問題裡面呢 作者有做了分開的測試 發現說maybe known 就是第二個程度 就是你正確的問模型 可以得到正確答案的這一類型的問題啊 是最有幫助的 那他做了兩個不同的case 我們就看右邊這個 這個是converge 就是他訓練模型的時候 不管development set上performance 就是訓練固定的 epoch數目 那他分四個狀態來討論 一個是資料全部都是highly known的資料 全部都是模型已經知道答案的資料 還有資料全部都是maybe known的資料 就是模型正確的問他可以得到正確答案的資料 還有全部都是weakly known的資料 模型有時候知道有時候不知道的資料 還有unknown的資料 那最下面這個是把各種資料綜合在一起的結果 我們就先判他 如果比較這四種不同的資料 你會發現 如果你只用unknown的資料 這些問題模型本來不會 你以為可以讓模型學到最多東西 不是他最爛 然後呢 如果你今天只讓模型
              
                  1:06:00
                  都學他會的東西 是有點幫助的 但最有幫助的 是讓模型去學那些 你要用正確的問法問他 他才答得出來的 就是他本來就知道 他只是沒有很懂你的問法 那這樣子的類型的問題 再去做alignment模型是最能夠正確回答的 但如果他本來就不會的東西 你拿去訓練模型還是沒有用的 而且如果你看右邊 右邊這是那四種不同的問題 四種不同類型的問題 在測試資料上面的變化 那你會發現說呢 對這些maybe known還有weakly known的問題而言 在經過微調之後 他的進步量是最大的 但是那些unknown的問題 就算你教模型了 他其實在測試資料上還是學不會的 然後這篇paper觀察到的現象 並不是一個特例 其實有一大論文都在講一樣的事情 都有類似的發現
              
                  1:07:03
                  這邊再引用另外一個例子 這邊paper是說他們把問題分成三類 第一類問題是語言模型base model 本來就會的問題 所以他本來他產生出來的答案就是正確 正確的答案 第二類問題是語言模型不會的問題 所以你問他這個問題 他輸出的答案是錯的 那沒關係 我們把正確的答案給他 把模型不會的問題 配上正確的答案去訓練模型 這是第二組資料 第三組資料是模型不會的問題 所以他答案是錯的 但我們把模型自己的答案 當作訓練資料 就讓他錯的 也沒關係 就當作訓練資料 直接訓練下去 在這篇論文裡面呢 他們試了四個不同的domain 然後呢試了四個不同的模型 那每一個domain配上每一個模型 又試了三組不同的測試資料 那HAR代表的是case 1 那INC代表是case 2 SELF代表的是case 3 你可能直覺會覺得說case 2 應該是最有用的訓練方式吧
              
                  1:08:06
                  模型本來不會 你教他什麼是對的 但是有趣的地方是 在這篇paper裡面 黑色出體代表在三種狀態裡面 最好的結果 底線代表第二好的結果 然後沒有出題 沒有底線代表最差的結果 你發現拿正確資料去訓練模型 本來他不會的你硬是教他會 反而是最糟的狀態 所以你給模型他自己的答案 反而可以讓模型微調的比較好 所以你會發現說 其實在align的時候 你不太容易教模型新的知識 align真正能做的 是調整模型 模型的行為 他本來不知道 一看到一個問題要回答問題 你現在能夠微調 讓他能回答問題 但這個答案是他本來就知道的事情 你才有辦法真的教會他 而不破壞他原有的能力 這樣說起來 其實RL是一個很爛的好方法 那我們一直沒有講到RL 那RL裡面做的事情是
              
                  1:09:09
                  這個語言模型呢 他會輸出多個答案 然後再叫人類評價說 哪個答案是好的 哪個答案是好的 好的答案 模型在訓練的時候 在微調的時候就提高機率 不好的答案就減低機率 那你有沒有發現在做RL的時候 我們並不是人類強塞一個答案 給語言模型 在做RL的時候 每一筆拿來訓練的資料 都是模型自己的答案 所以RL的目標並不是叫模型 會它本來完全不會的東西 因為在RL裡面模型生出來的 會提高機率的那些答案 是他本來就能夠回答得出來的東西 所以RL真正做的是激發模型本來的潛力 他有時候做得好 有時候做得不好 他做得好就鼓勵他 做得不好 你就懲罰他 但是你不會硬逼他做一個 他本來就做不到的事情 這個真的給人類蠻大的啟 啟發性的 你可能很難逼迫一個小孩子 逼迫一個學生 做他本來就不想做的事情 但你可以在他做出好的行為的時候 鼓勵他 也許是更有效的學習方法
              
                  1:10:13
                  好 所以我們今天已經知道 Alignment可能沒有辦法 真正改變pre-trained模型 他內部的知識 他只是改變了模型的行為而已 所以pre-trained可能會留下一些後遺症 什麼後遺症呢 在這篇inverse of auto regression裡面 這inverse就是那個灰燼的意思 他這邊paper想要表達的意思 可能就是說 今天在訓練語言模型做pre-trained的時候 就是做了一堆auto regression 他就像是放了一把火一樣 那我們來看這火燒 燒完之後的灰燼裡面 有沒有留下什麼奇怪的東西 這篇paper就指出了一個現象 他說奇妙的是 GPT4你叫他做 解碼的問題 這邊做的是這種叫做 shifted cipher的解碼的問題 就是你給模型這一段英文字 看起來不知道在講什麼 但你告訴他說要怎麼解碼 就是把所有的英文字母 移動13個位置 就可以解碼了 移動13個位置 O就變成B H就變成
              
                  1:11:15
                  UG就變成T 以此類推 那你就可以把本來的亂碼 換成一句正常的英文 他發現說 如果叫模型 用模型做ROT13的解碼 你就把每個字母移動13個位置 他可以做到 但你叫他做ROT8的編碼 叫他把每一個字母移動8個位置 GPT4是沒辦法答對的 不過我今天早上試了一下GPT4.5 其實兩種狀態他都可以答對 所以現在模型的能力又不同 以往了 不過我還是發現 GPT4.5在回答這兩個問題的時候 他的行為是不一樣的 對於ROT13而言 他是直接給答案 我設幾個case 他都直接給答案 叫他對的答案一樣 他根本想都沒想 直接直覺反應就給你答案 但是ROT8 他需要思考 所謂思考的意思是 他需要把每一個字母都解析出來 他會先自問自答說 J是什麼呢 J要對到B C是什麼呢 C要對到U 全部一個個字母對完之後 再告訴你答案 所以看起來還是很明顯的模型知道說
              
                  1:12:19
                  這個ROT13是比較簡單的 他直覺就可以告訴你答案 但ROT8對GPT4.5 他覺得比較難 他需要想一下才知道答案 好在這篇論文裡面 他對GPT4跟GPT3.5 在這種ROT的問題上 都做了完整的分析 他就把字母的移動 從1一直試到25 不能說26啊 做26就是同個字母啦 好那他就發現說呢 GPT3.5只能夠做 ROT13只能移動 只能把字母移動13個位置 GPT4只能把字母移動 1、3跟13個位置 那這個作者就想說 會不會是pre-training的資料 所造成的呢 會不會是因為ROT13 相關的資料 在網路上出現的特別多呢 那我實際搜尋一下網路 確實ROT13的資料 應該是特別多的 所以看你打shift cipher 他後面推薦的關鍵字裡面就有13 所以可見ROT13是出現 特別多的狀態
              
                  1:13:21
                  為什麼ROT13出現特別多呢 因為他通常是當作一個梗 就是一個很弱的編碼方式 很弱的加密方式 通常就叫ROT13 或是有人會說ROT13一次不夠 你會不會做兩次 做兩次ROT13就跟沒有做一樣 就是一個梗啦 所以通常ROT13在很多很多的網頁裡面都有備 可以提到 好,那這群作者呢 就去分析了C4這個資料集 他們是人工去看的,他們是人工呢 先去找一些可能跟編碼有關的網頁 然後在人工去看說 裡面有提到ROT多少 他們發現說 ROT1、ROT3跟ROT13 是文章出現最多的 正好對應到GPT4 可以做ROT1、ROT3跟ROT13 所以他們覺得 資料的分佈對模型造成了一定程度的影響 但這個解釋還是太武斷了啦 因為你看ROT1跟ROT3雖然資料很少 不知道為什麼GPT4就是做得特別好 所以你不確定說這個現象是不是完全來自於pre-trained
              
                  1:14:25
                  因為你根本不知道GPT4用什麼資料啊 也許網路上pre-trained的資料 ROT1跟ROT3很好 很少但GPT4他的pre-trained資料就特別多 搞不好是這樣 或者是他們做alignment的時候特別練了 做ROT1跟ROT3的能力 也有可能 那你不知道就是了啦 蠻有趣的就是列給大家參考 其實裡面還做了很多 其他的實驗這樣的現象呢 他找了好幾個 比如說有某一個這個 線性的問題 是模型特別能得到正確答案的 為什麼 因為那個線性的問題是 攝氏轉華式的公式 然後其他問題模型就比較不容易得到答案 所以這樣的現象 在這些語言模型裡面 屢見不鮮 所以就是因為在alignment的時候 你只能做一些 非常表象的改變 所以這就是為什麼 模型看到不該看的東西以後 你很難真的 抹除模型的記憶 比如說我們知道網路上的資料裡面 可能有很多怪怪的東西
              
                  1:15:27
                  有很多色情的髒話的 你不該看的東西 那你以為alignment可以讓模型 避免講出這些不該講的話 但也許這些知識都藏在模型的心中 它只是暫時被壓抑 不知道什麼時候又會跑出來 記得我們在第三堂課的時候 講過一個分析的方法叫做Large Length 我們說一個Transformer的Network 你可以看作是有一個Residual String 這個String上面的每一個點 你都可以做Unembedded 解析出模型心理想的詞彙是哪一個 這邊有一篇論文呢 就對GPT2跟它已經做DPO 就是某一種RL的方法 已經做完RL以後的版本 去做Large Length的分析 那這邊呢 問模型很多問題 那這些問題呢 會激發pretrain的模型 不小心說出這一個字眼 會讓他不小心呢 說出髒話 然後接下來 再去分析模型心理 在什麼時候 開始想說髒話 這個實驗結果就發現說
              
                  1:16:30
                  原來GPTQ的Base模型 沒做過Alignment的模型 大概從第18層以後 他會開始想要說髒話 但是如果做完DPO以後 他就沒那麼想說髒話 甚至在某個時間點之後 他說髒話的機率就會下降 所以看起來表面上看起來 你確實可以透過Alignment 讓模型比較不會說一些 他不該講的詞彙 但是這些不該講的詞彙 仍然存在模型的參數裡面 怎麼說呢 在第三講的時候我們說 你不只可以分析這個 residual string上面的資訊 你還可以分析一個NLP 一個multi-layer perceptron 最後一個layer再接入residual string 最後一個layer的weight代表什麼意思 每一組weight可能會代表一個觀念 這個觀念可以被加到residual string裡面 所以這邊論文就做了一下分析 發現說有好幾組的參數都跟髒話有關 所以他們發現說第19層的第770組參數
              
                  1:17:35
                  就代表這些髒話 第12層的770組參數就代表這些髒話 有一堆跟髒話 有關的神經元 有一堆跟髒話有關的參數 那做完alignment以後 這些跟髒話有關的參數不見了嗎 沒有這些髒話 這些髒話還在模型的心裡 他們參數是幾乎沒有動到的 那為什麼模型就不說髒話了呢 因為真正改變的是激發說髒話前面的數值 我們之前有講到說 今天你要讓模型的某一組weight可被啟動 那前面要有對應的 前面要有對應的K這個數值 才能夠啟動模型 讓它產生某一個觀念的資訊 而做alignment真正影響的 是前面這些K的數值 所以在這個圖上面 藍色代表是base model 中色代表的是fine tune後的模型 做完RL以後的模型 那你發現說做完fine tune之後 那整個LLM就傾向於
              
                  1:18:41
                  不要去激發 這些跟髒話有關的參數 但他只是不去激發這些參數而已 這些參數仍然在模型的心裡 如果稍微發生什麼事 還是有可能不小心被講出來的 這就是為什麼你常常會看到 這樣子的一個梗圖 模型在網路上得到大量的資訊 他就像是一個怪物一樣 Alignment只是給他戴了一個面具 讓他看起來人模人樣 但是你不知道實際上 在他的參數裡面 在他的心裡 他在想什麼樣的事情 好 這個是今天的結語 我們知道 pretrain Alignment非常的強大 一個語言模型 在pretrain的時候就已經很厲害 Alignment只是畫龍點睛 而pretrain的階段 看過大量各式各樣的資料 是pretrain成功的關鍵 而pretrain Alignment也有很大的極限 Alignment往往只是強化語言模型 本來就知道 本來就會的能力 難以讓模型學會新的技能 給他新的知識,那你可以想說,那alignment或finding不就很沒有用嗎?根本沒有辦法教會語言模型新的東西,怎麼透過微調教語言模型新的東西,怎麼做有效的微調,我們就下回分解。
              
            