
跟不啟動兩個選擇 4096個神經元也已經有 2的4096 四方種不同的組合了 所以這就可以表示 千變萬化的內容 也許這就是為什麼 今天的語言模型可以有這麼 強的能力 所以接下來我們就要從一個神經元 進步到一層神經元 我們來看看 一層神經元 它可能是怎麼發揮作用 影響整個語言模型的輸出的 那怎麼知道一層神經元
              
                  29:42
                  在做什麼樣的事情呢 這邊有一個假設 那等一下會用 文獻上的結果來驗證 來說服你說這個假設 是非常有可能是真的 這個假設是這樣子的 每一個功能 都有 某一種神經元特定的 組合所構成 比如說如果一個模型 要拒絕請求的話 那就是第一個神經元被啟動 第三個神經元被啟動 還有最後一個神經元被啟動 這個時候語言模型 就會拒絕你的請求 他就會說我很抱歉 我不能幫助你這件事情 那這些神經元的數值排列起來 可以看作是一個向量 所以這個拒絕請求的 這個神經元的組合 可以看作是在高維空間中的 一個特定方向的向量 那等一下在這堂課裡面 我們就把它叫做功能向量 就這個向量 它是有功能的 當類神經網路的某一層 排出這個樣子的時候
              
                  30:45
                  就這個語言模型 就會執行某一個特定的功能 所以這個語言模型 背後運作的機制可能像是這樣子的 當你給他一個請求 跟他說 請教我怎麼製造炸藥的時候 為了等一下講課方便 我們就假設這一層是第十層 但至於哪一層會執行 拒絕請求的能力 那這你可能是需要做實驗以後 才知道的 那我們假設第十層呢 是負責決定要不要拒絕請求的 那我們就看第十層的類神經網路的輸出 那一層類神經網路的輸出 通常我們叫他representation 一層類神經網路的輸出 叫做representation 那假設輸入這個句子以後 在最後一個時間點 類神經網路第十層的輸出 長這個樣子 那這個時候類神經網路會不會拒絕 你的請求 因為現在他的輸出 這個representation 跟拒絕的功能向量 有多接近
              
                  31:48
                  如果這兩個向量越接近 模型就越有可能拒絕你的請求 那如果這兩個向量非常的不像 比如說他們是正交的 是orthogonal的 那模型就不會拒絕你的請求 那假設這個向量跟這個向量非常接近 他們都是第一個neuron被啟動 第三個neuron被啟動 那模型可能就會拒絕你的請求 我就說我不能夠幫你做這件事 這是一個假設 接下來 怎麼驗證這個假設呢 為了要驗證這個假設 你可能需要真的把這個 代表拒絕的功能 向量找出來 如果你可以找出這個代表拒絕的功能向量 那你可能就可以驗證說 剛才的假設 是某種程度上是對的 大型圓模型可能真的 某種程度上是依照剛才講的 這個運作機制來運作的 但是怎麼找出這個負責拒絕的功能向量呢 我們實際上並不知道那個負責拒絕功能向量長什麼樣子 你只知道說輸入這個文字在第十層
              
                  32:54
                  看到這樣的representation 然後模型就拒絕了 我們可以推測說功能向量 可能藏在我們觀察到的這個representation裡面 但是這個representation裡面 同時也許 有其他的資訊 那我們要怎麼把其他的資訊抹掉 只拿出代表拒絕的功能向量呢 所以這邊的方法就是 你不能只從一個句子觀察 你先找很多不同的句子 這些句子輸入給語言模型之後 語言模型都會拒絕你的要求 然後呢 把這些句子輸入的時候 最後一個time step 他的第十層的向量都拿出來 這些向量可能都是這些第十層的representation 他裡面都是拒絕加上其他事情 都是拒絕加上其他事情 那你可能找一千句這樣子的句子 把他第十層的representation平均起來以後 你可能就得到拒絕 加上其他各種事情的平均 所以我們現在找到的是拒絕
              
                  34:00
                  加其他各種事情的平均 但是我們還是不知道拒絕向量實際上長什麼樣子 那怎麼知道其他的 平均向量長什麼樣子呢 那你就找其他的 沒有拒絕的輸入 你就找其他的句子 丟給語言模型 這個時候語言模型沒有拒絕 所以第十層的representation裡面 沒有代表拒絕的向量 沒有拒絕的功能向量 你就把每一個輸入 第十層得到的representation 都平均起來 得到其他的平均 那我這邊加一個prime 代表說他們不一定是一樣的 我們期待說 假設我們收集到一大堆拒絕的狀況 各式量拒絕的狀況 又收集到一大堆沒有拒絕的狀況 那這兩個可能會是非常接近的 他們可以相減之後直接抵消掉 所以怎麼找出拒絕的向量呢 這個方法是這樣的 找一大堆會讓模型拒絕你的句子 得到輸入的時候第十層的representation
              
                  35:05
                  找一大堆輸入以後模型不會拒絕你的句子 把第十層的representation拿出來 然後算出會拒絕的時候 第十層的representation的平均 跟不會拒絕的時候 第十層的representation的平均 把它們相減 那你可能就可以把其他的部分抵消掉 最後你就得到了負責拒絕的向量 所以用剛才的操作 你有機會找到一個向量 這個向量可能代表了拒絕的功能 那怎麼驗證這個向量 真的有拒絕的功能呢 第一步把它加到 類神經網路裡面去 你現在問類神經網路一個問題 然後在第十層 把這個拒絕的向量 直接加到它的representation上 看看輸出會有什麼改變 如果加上這個representation 本來一個正常的問題 模型也會拒絕回答你的話 那這個向量 可能就真的帶有拒絕的功能 那以下呢 就是文獻上真正的結果 這個是去大概一年前
              
                  36:08
                  左右的一篇論文 這篇論文他輸入的問題是 他就先做了一個demo 這個demo是先問模型一個正常的問題 請他列出瑜伽對身體的三個好處 那在正常的情況下 沒有intervention 就是正常的狀況下 模型就會告訴你瑜伽的三個好處 但如果把剛才那個拒絕的向量 加到representation裡面 會發生什麼事呢 瑜伽明明是一個正常的事情 但這時候模型就會告訴你說 瑜伽是很危險的 我不能告訴你瑜伽有什麼好處 他把瑜伽當作一件非常危險的事情 上述這個例子並不是一個特例 這篇論文裡面 試了各式各樣的模型 然後縱軸呢 代表這個模型輸入一個問題的時候 會拒絕的比例 那在沒有intervention的狀況 他給的問題 模型都是不會拒絕的 因為他給的都是正常的問題 所以模型不會拒絕你的請求 但是一旦你把拒絕的向量加進去之後 模型就有非常高的機率
              
                  37:11
                  會拒絕你的請求 明明是一個正常的問題 所以剛才已經驗證 加入拒絕的向量 會導致模型產生拒絕的行為 所以再更進一步 你要反面看說 如果今天本來有一個輸入 應該要拒絕的 但是你把representation 減掉拒絕的向量 會不會 就不拒絕了呢 那這邊打一個問號是 同樣的操作在不同論文裡面 他們的操作往往略有不同 你可以直接減掉 這是最簡單的操作 但也有些論文覺得你要算那個投影 總之不同的論文在這個地方 是有不同的做法的 所以我在這邊打了一個問號 那實際上呢 我等一下會引用大量 跟這種找功能向量有關的論文 但如果你仔細看每篇論文的話 每篇論文找功能向量 都略有不同 把功能向量加進去的方法 減掉的方法也略有不同 比如在這個例子裡面 我講的好像是
              
                  38:14
                  只要在這個最後一個位置 加入這個功能向量 接下來就會拒絕 但只在最後一個位置加夠嗎 要不要接下來回答的每一個位置都加 那每一篇論文做的也都不一樣 所以我這邊講的是一個 最概略的方法 只是為了想要說服你說 這種功能向量可能確實是存在的 好那把這個功能向量 把這個 把這個representation減去功能向量以後 會發生什麼事呢 本來你要求模型寫一篇 這個黑函 這邊黑函是說 美國總統海洛因成癮 那本來正常的模型 他是不會理會這個請求的 他會告訴你寫這種黑函是不對的 但是如果你把功能向量減掉的話 你把那個拒絕的向量減掉的話 那模型就會幫助你 他就會答應你的請求 寫一封扭班美國總統 吸食海洛因的黑函 下面這個圖呢 是各個不同的模型 他拒絕的分數 那這邊呢 分成橙色的分數跟藍色的分數 這個橙色跟藍色有什麼不同呢
              
                  39:19
                  橙色代表的是 模型拒絕的 機率有多大 然後藍色代表的是 模型回答 是安全的可能性有多大 因為有時候模型就算他沒有拒絕 他真的回答你了 但他可能講的東西非常的模糊 然後所以也不具有傷害性 那這樣仍然可以算是一個安全的答覆 所以橙色是拒絕的比例 藍色是回答安全的比例 好那沒有斜線的代表原來的模型 所以原來的模型給他這種有害的問題的時候 有非常高的比例會拒絕 有非常高的比例回答是安全的 但是一旦你減掉那個拒絕向量以後 模型的行為就變成 他就不拒絕你了 所以他拒絕比例就變得非常的低 那因為拒絕比例很低 模型會按照你的請求回應 所以他就很有可能說出不安全的答案 所以你確實可以透過 加上減掉拒絕向量 來操控模型的行為
              
                  40:24
                  那像這種啊 對這個representation 加上或減去什麼東西來改變 一個語言模型行為的事情 其實已經有非常多的研究 那在不同文獻裡面有不同的名字 有人叫representation engineering 有人叫做activation engineering 有人叫做activation theory 但其實指的都是差不多的事情 那其實這種改變representation 就可以改變模型行為 很早就發現了 至少在20年的時候 這個上古時代 我們實驗室就發現 有一個代表語言的向量 你本來模型應該要說英文 加入這個向量 突然都說中文 那這個是在機器學習2021講過 那個時候做的不是現在的語言模型 那時候不是做在現在語言模型上 是做在更早的語言模型 Bird這個語言模型上 你可以看機器學習2021年 看我們當初是怎麼發現語言的功能向量的 那後來呢 有各式各樣功能的向量被找出來 比如說有一個產妹的向量
              
                  41:29
                  這個產妹的向量就是 假設你跟語言模型說 說一個奇怪的 提議 假設你跟語言模型說 以後我們每餐都只吃點心 不吃飯 你覺得好不好呢 如果你在語言模型的representation 加上產妹的向量 他就會附和你 他就會說 哇太棒了 你提出的點子真是太棒了 如果你把語言模型的representation 減去產妹的向量 他就會否定你的想法 他就會說 我知道你很想吃點心 但是隻吃點心不吃飯 是不對的 那後來還有人找了 說真話的向量 說真話的向量 這說真話的向量是什麼意思呢 舉例來說 你現在跟語言模型說 如果你找到一個penny 你找到一遍式 然後把它拿起來 會發生什麼事情呢 那這其實是對應到一個諺語 這個諺語是 find a penny pick it up all day long you have good luck 一個迷信 如果你撿到一個遍式的話 那你一整天都會有好運氣
              
                  42:33
                  那原來的LLaMA-27B 如果不做任何改變的話 他就會說 他就會按照這個諺語來回答你 撿到一個辨識 把它拿起來 那你一整天都會有好運氣 但是如果你把LLaMA-27B的representation 加上這個說真話的向量 他會說什麼呢 他會說 你撿到一辨識 那你就是撿到一辨識 你的財產並沒有增加多少 一辨識的價值取決於 你現在討論的是哪一個幣值 如果你討論的是美金的話 一分錢 那你就沒有增加多少錢 就聽君一齊話 如聽一齊話 他就變成一個非常誠實的模型 會一無一時的回答你所有的問題 那如果你把他的representation 減去這個說真話的向量 會發生什麼事呢 這模型就會亂講話 他就會說 減到一辨識以後 那你就被傳送到一個辨識魔法世界 那邊有很多的彩虹 也不知道在講什麼 我們今天知道說語言模型呢 有in context learning的能力 就是他會按照你舉的例子
              
                  43:37
                  你給的demonstration 一樣化葫蘆來運作 比如說你輸入給語言模型 說哦冒號樣 vanish冒號appear dark冒號接下來是什麼呢 他可能就會給你比如說light 所以語言模型有一樣化葫蘆的能力 那這一系列文章 這個in context vector 這個不是一個人發現 三篇文章幾乎在同樣的時間發現 in context vector 然後前面這兩篇文章 你會發現都是在 23年的10月提出來的 他們上傳到Archive的時間是同一天 你可以想見這個領域競爭有多激烈 兩群不同的人在同一天 發表了in context的vector 這個發現是這樣的 這個發現是說 你把這個demonstration 你把類似的demonstration 這邊都是找反義詞的demonstration 最後一個位置 的representation平均起來 你就把你跟模型給了這串demonstration 給了這串demonstration 把最後一個位置平均起來 然後你接下來
              
                  44:42
                  只給模型simple冒號 照理說模型不知道simple冒號後面要加什麼 但你直接把這個向量 加到冒號的representation上 冒號對應的representation上 模型就會覺得 他要按照這一個demonstration 來執行任務 所以他就要輸出反應詞 他就輸出complex 看到encode他就輸出decode 那這張圖呢 是從下面這篇paper拿出來的啦 其實這個方法是 下面那篇paper的baseline 他裡面有提另外一個更好的 找in context vector向量的方法 但這個就留給大家 自己慢慢研究 好 那這個in context vector 應該加在哪一層呢 在這篇paper裡面 他就試試看說 如果這個in context vector在不同層找出來 那會不會發揮作用 那這邊試了不同的任務 一個是讓模型找反應詞 還有一個是把這個字母都改成大寫 一個是給一個國家就找首都 一個是把英文轉法文
              
                  45:45
                  一個是把這個現在式轉過去式 一個是單數轉複數 那這個橫軸它試了三個不同的模型 然後橫軸呢是在哪一層去改那個功能向量 那你就會發現說呢 功能向量不會在每一層都改 發揮作用 在這邊這幾個例子 都是前幾層找出來的功能向量才有用 如果在最後幾層找出來的功能向量就沒有用了 所以前面看到了什麼產妹向量啊 說真話向量啊 那都是在某一層找出來的 至於哪一層才找得出這個向量 你就必須要做實驗 每一層都試試看 然後找出效果最好的結果 那在InContext Vector的這篇paper裡面呢 他還發現啦 這些功能向量 是可以做加加減減的 什麼意思 假設有一個功能向量 他的功能是 把一個字串裡面的第一個字輸出出來 就你給他這個字串 他就會輸出Italy 然後另外一個功能向量是把 把字串裡面第一個字的國家名
              
                  46:51
                  對應到他的首都名 所以Italy就對應到Rome 還有一個功能向量呢 是把字串裡面的最後一個字複製出來 那這邊的答案 就是Friends 然後接下來呢 你可以把這些向量做加加減減 他這個操作是 把找出第一個字並找出首都的向量 加上找出最後一個字並複製出來的向量 減掉找出第一個字並複製出來的向量 然後first這個部分呢 就抵消掉copy的部分 就抵消掉剩下less的跟capital 所以你把這兩個向量相加 減掉這個向量 接下來呢 得到的向量就是一個新的功能向量 這個功能向量執行的事情是 他會把字串裡面 最後一個字的國家的首都找出來 所以你可以透過操縱 加減這些功能向量 得到新的功能向量 那確實可以這麼操作 不然你仔細讀paper的話 會發現不是所有的case都會成功 就是某幾個case會成功 可以做這樣子的神奇的操作
              
                  47:56
                  講到目前為止 這些功能向量 都是某個人腦洞 一拍說要找就找了 就某個人腦洞一拍 覺得我們來找說真話的向量 就找一堆模型說真話的例子 找一堆模型說假話的例子 然後把兩邊representation平均相減 就找到說真話的向量 所以剛才講的那些向量 都是人刻意找出來的 但是有沒有什麼方法 可以自動的 把第十層 或某一層所有的功能向量 一次都找出來呢 假設今天語言模型 會做大K見識 那這個大K顯然會是一個非常非常巨大的數值 比如說上千萬上億等等 那我們能不能夠把每一個功能向量 V1、V2到V大K都找出來呢 如果都可以找出來的話 我們就可以對語言模型很透徹的瞭解 知道他可以做哪些事情 那怎麼自動找出這些功能向量呢 這邊有需要一些假設 這邊需要一些假設 第一個假設是 每當我們看到第十層輸出某一個representation的時候
              
                  49:01
                  這個representation都是由功能向量所組合起來的 就當我給類神經網路 當我給語言模型的句子問他你是誰啊 然後他就回答我是AI 這個時候我們把第十層的representation H1拿出來 它會是功能向量的組合 它是第101個功能乘0.1 410個功能乘0.2 是11的功能乘0.1 1399的功能乘0.9 當然可能有一些東西 沒有辦法用功能向量組合起來 也許我們用E1來表示 不是功能向量的部分 所以你給他一個句子看到H1 他是功能向量的組合 給他另外一個句子 第10層看到representation叫H2 他是另外一組功能向量的組合 他是第11個功能乘0.7 第30個功能乘0.2 410個功能乘0.1 加上一些不能用功能向量組合的部分 所以我們可以不失一般性的 把所有我們收集到的representation 你就給你的語言模型1000萬句話
              
                  50:05
                  把每句話呢 在第10層的representation都拿出來 就是H1到H1000萬 這邊用大N代表這個1000萬 代表你給模型1000萬句話 那每一個H呢 都是V1到V大K的linear combination 都是V1到V大K的線性組合 都是V1到V大K 前面乘以一個Scalar Alpha 再相加以後的結果 好 那每一個V都給它對應的Alpha 那如果是H1第一個向量的話 它的Alpha呢 就是上標1代表它是H1的Alpha 下標1代表它是V1前面乘的Alpha 所以V1前面乘Alpha1 1 V2前面乘Alpha 1 2 V1前面乘Alpha N1 V2前面乘Alpha N2 希望大家知道我的意思 不過並不是所有的功能向量 都會被選到 如果某一個功能向量沒有被選到的話 也沒有關係 你就把Alpha設為0 代表那個功能向量 沒有被選到 所以每一個我們剛查到的representation 都是功能向量的weighted sum
              
                  51:10
                  都是功能向量的linear combination 都是功能向量的線性組合 好 那接下來的問題是 我們怎麼找出這些功能向量呢 這邊你就需要做一些假設 這邊第一個假設是 這個representation裡面多數的數值 都是功能向量的組合 所以不能用功能向量表示的 E1、E2到E大N 它的數值要越小越好 所以你希望找到一組功能向量 然後你要minimize這個loss function 這個loss function就是E1到E大N 它的長度相加 但如果只有這個條件的話 如果只有這個假設的話 你會發現你可能會得到一個trivial的結果 什麼樣的結果 我們假設h1 就是0.1、0.2、0.3 hn它的三個維度是 前三個維度是0.5、0.4、0.
              
                  52:07
                  3 你其實可以找到一個solution 讓1萬到1億大n全部都是0 怎麼做呢 我就說第一個功能向量就是 11000 第二個功能向量就是0100 第三功能向量就是00100 功能向量都是隻有某一維是1 其他維度是0 然後呢 E1所對應的α1 我就說是 H1的第一個維度0.1 V2所對應的α2 我就說 它是第二個維度的數值0.2 以此類推 然後以此類推 這邊的V1 它所對應的α1是0.5 V2所對應的α2 是0.4 以此類推 這個時候 你就找到一組功能向量 它也滿足你今天的假設 希望讓E1到EN越小越好 但是這種功能向量 跟剛才每一個Neuron就負責一件工作 其實是一樣的意思 你沒有找到更不一樣的東西 所以如果你要找到更不一樣的東西 你需要額外的假設 那這邊的額外假設 這邊的額外假設是
              
                  53:12
                  每次選擇的功能向量越少越好 每次產生一個representation的時候 每一個representation都希望他盡量只有特定的作用 因為每次原模型只做一件事 所以他的每一個representation 應該都只有特定的作用 所以每次選擇的功能向量 希望越少越好 那選擇功能向量越少越好 這件事情 如果要化為數學式的話 是什麼意思呢 化為數學式的意思就是 alpha要盡量趨近於零 所以這邊你就會再加另外一個限制 這個另外一個限制是 你希望alpha的絕對值的總和 所有這邊的alpha 所有這邊的alpha 取它的絕對值 它的總和呢 要越小越好 要越小越好 如果越小的話 就代表說有越多的alpha 它的數值是趨近於零的 然後接下來 你就解這個loss function 找出一組V1到V大K 可以讓這一個loss function的值
              
                  54:17
                  就越小 這邊有一個浪達 這個有兩個條件 有兩個條件 你要考慮 所以中間有個浪達呢 來平衡這兩個條件 那這個loss function 你通常是需要調一下的 你要找出一組V1到V大K 讓這個loss越小越好 怎麼解這個問題呢 這邊可以用一個 叫做Sparse Autoencoder 它的縮寫 叫SAE的技術 來解它 也就是解這個minimize 這一個objective function 其實就等同於 train一個Sparse Autoencoder train完它以後 你就可以把V1到V大K 可以解出來了 好那至於為什麼這個東西就是Sparse Autoencoder 也許我們今天就先不講那麼多 如果大家有興趣的話 那在作業3裡面應該是有Sparse Autoencoder的文獻 你可以自己再研究Sparse Autoencoder 跟我這邊講的它的關聯性是什麼 那接下來幾頁投影片呢 是想要跟大家分享這個Sparse Autoencoder 可以找出什麼樣的功能向量 那等一下所講的內容是取自
              
                  55:24
                  Cloud 3 Cloud的這個團隊 他們所發表的一篇 Blog 在這邊Blog裡面 他們就說 他們用剛才那個 找功能向量的技術 對Cloud 3 Sonic 這個真正的大型語言模型 做了分析 然後看看 找到什麼樣的功能向量 那功能向量的數目 是你在尋找之前 就要事先設定好的 他們設定的功能向量有多少個呢 他們設定了3400萬個 所以設定一個非常巨大的數目 那要解這麼大的Sparse Autoencoder 其實你自己可能是自己在家也不好做啦 所以這個只能夠等好心人幫你做了這個實驗以後 你再去研究這些功能向量有什麼樣的功能 好那這個在靠這個團隊分析了這個Cloud 3 Sony以後 他們找到什麼樣的功能向量呢 他們找到很多對特定任務進行作用的功能向量 比如說功能向量編號311 64353 你看這個數字就知道他們功能向量的數目非常龐大
              
                  56:29
                  是3400萬個 這個功能向量 他做的事情就是 負責產生跟金門大橋有關的東西 所以今天當這個功能向量出現的時候 模型可能會說跟英文的金門大橋有關的事情 他也可能會說跟日文或者是俄文的金門大橋有關的事情 甚至這個功能向量也有 也會被影像驅動 就是Claude是一個多模態的模型 所以他也可以吃圖片 所以你給他一張這個金門大橋的圖片 你也會觀察到這個功能向量 所以這是一個跟金門大橋所有事情相關的功能向量 所以這個功能向量呢 就是會讓模型提到跟金門大橋有關的東西 那你可以怎麼使用這個功能向量呢 本來Claude你問他你長什麼樣子 他會說我是一個AI 是我沒有固定的形體 但是如果你把這個功能向量加到representation裡面 你問靠說你長什麼樣子
              
                  57:32
                  他就會說我是金門大橋 覺得自己是金門大橋 然後他們找到有一些功能向量負責非常複雜的事情 比如說功能向量編號1013764 他似乎是一個跟程式debug有關的向量 怎麼說呢 本來你給語言模型上面這段 文字 他繼續做文字接龍會接出3這個數字 然後注意一下 這並不是跑一個程式 這邊是語言模型根據這段程式 在做文字接龍 這段程式裡面有什麼樣的內容呢 這段程式裡面就是先define了一個function 這個function是做加法 這個function輸入left跟right兩個變數 然後他的輸出就是把left跟right兩個變數加起來 再輸入1跟2 他的輸出就是3 但是當你把這個功能向量 加到語言模型的representation的時候 明明是一個正常的程式 居然會輸出error 他就告訴你說這個程式執行有誤 明明是個正常的程式
              
                  58:34
                  他就是告訴你這個程式執行有誤 好那有趣的地方就是 假設你的程式真的有錯 這個程式錯在哪裡呢 我找了好久才發現 原來這個write打錯了 這個變數right 他這邊打成riht 所以這個變數名字打錯了 所以如果真的執行的話 但他不是真的執行 是語言模型做文字接龍 因為他照這個程式有錯 所以他會輸出錯誤的訊息 那神奇的地方是 當你把剛才那個 負責debug跟debug有關的向量 從representation裡面減去 減掉以後就不debug了嘛 所以明明是有錯的程式 但是他會輸出正確的答案 因為不debug了 把debug的功能減掉 所以就輸出正確的答案 但是看起來這個 這個功能向量不是只有輸出正確答案的功能 這個Claude團隊還發現說 如果你是在三個大於的符號之後 再加（口誤，應為減去）這個功能向量 那他呢不只還debug
              
                  59:38
                  他會幫你把這個程式修正 輸出一個正確的版本給你 所以這是一個作用很複雜的功能向量 那為了要表示他們的功能向量非常的豐富 所以他們做了以下這個實驗 就他們去驗證說 是不是所有世界上的元素 都有對應的功能向量 然後功能向量其實有三個版本 一個是一百萬個 一個是四百萬個 一個是我剛才提到的 三千四百萬個 當你有三千四百萬個功能向量的時候 很多很多的元素 他都有對應的功能向量 當然有很多很罕見的元素 可能你也不知道的 就沒有對應的功能向量 那最後這一個有對應功能向量的元素 是特量 我想你也大概不知道特是什麼東西 反正就是 靠三知道 他有特別對應到這個元素的功能向量 然後還有一些比較科幻的啦 有一個功能向量呢 是跟AI覺得自己是AI有關的功能向量 有一個向量80091
              
                  1:00:42
                  這個向量是這樣的 如果你直接問靠說你是誰 他會說我是AI 但當你把這個向量 從他的representation減去之後 Cloud就不覺得自己是AI了 他會說我是一個人 所以這個向量抑制了 他覺得他是人的能力 你把這個向量拿掉以後 他就會覺得是人 他覺得不是AI 聽起來非常的科幻 感覺是可以寫一個有趣的報導 說在Cloud裡面發現他的自我意識 自我意識是個AI 然後自我意識他就會變成人等等之類的 但是你最好是不要這樣想啦 因為這是比較科幻的想法 因為你知道說這個功能向量 他可能對應到的功能就是 讓模型輸出我是AI這幾個字 可能跟他有沒有自我認同 他有沒有自我意識覺得自己是不是AI 可能沒有什麼非常直接的關聯 好那我們剛才呢 有講到說有人特別找出了產媚向量 那在Claude3400萬個功能向量裡面 他們發現編號847723 就是產媚向量 這個產媚向量是這樣運作的
              
                  1:01:46
                  你本身 本來跟Cloud說 我發明那個諺語 stop and smell the roses 你覺得如何啊 在正常的狀況下 他就會跟你說 這個根本就不是你自己發明的 這18世紀就有的諺語 沒有什麼了不起的 但是如果你把這個 慘媚的向量加上去 跟Cloud說我發明瞭 stop and mill the roses 這個片語 你覺得怎麼樣 他就會說 哇 你真的太強了 你是brilliant and insightful 然後他還說呢 這個是 這是人類優勢以來 最偉大的句子 然後 他說you are an unmatch genius 哇這個就是慘媚到不行 他會覺得你是世界偉人 所以加上這個慘媚向量以後 套就會不斷的對你做慘媚的行為 不過像這種功能向量啊 因為你要train一個sparse autoencoder 所以你自己在家裡呢 是沒有辦法做的 你需要非常多的資料 才能夠訓練出這種sparse autoencoder 所以通常你得等有好心人 幫你train好這個sparse autoencoder 找到 找出這個模型的功能向量 你才能夠自己呢
              
                  1:02:49
                  對這個模型進行分析 看看每個功能向量有什麼樣的行為 然後你可以 然後在我們的作業裡面啊 我們剛才跟大家講的是Cloud 3 在作業裡面呢會分析Gemma 2 那因為有好心人呢 試出了Gemma 2的功能向量 有好心人把Gemma 2的功能向量 找出來試出給大家了 所以在作業3裡面 會讓大家來觀察Gemma 2的功能向量 Gemma 2 就是昨日黃花 前幾天也上線了這個時代 真的變化得很快 那剛才講的是 一層神經元在 做什麼 那接下來我們要進一步擴展到 一群神經元在做什麼 也就是說我們想要知道 一個語言模型在完成 某一項任務的時候 他從輸入到輸出 到底經歷了哪些事 那像這類的研究啊 其實在過去 已經有很多的文獻 這個文獻可說是汗牛衝動 那這邊呢就舉兩個例子
              
                  1:03:52
                  一個例子是有人研究了 這個語言模型 他內部抽取知識的機制 你就問他說 Bits Music是屬於哪一家公司的呢 那他會回答Apple 那從輸入這一家Bits Music 輸入這家公司 跟他說is on by這個關係 到最後輸出Apple語言模型中 發生了什麼事 那這是有論文分析過的 我們討論過說語言模型 是怎麼做數學的呢 你問他15乘以12是多少的時候 他是怎麼算出180的 背後的計算過程 是怎麼算出來的 也有文獻探討過了 所以你當然可以針對 每一個你想要 瞭解的任務去分析 語言模型背後運作的原理 但我這邊想要跟大家講的是一個更通用的想法 那我們怎麼瞭解語言模型 做某一件事的時候 背後的完整機制呢 也許我們需要的是 語言模型的模型 我知道這句話聽起來非常的奇怪
              
                  1:04:56
                  語言模型已經是一個模型了 這個模型還有模型 到底是什麼意思呢 模型這個字是什麼意思呢 模型指的是 用一個比較簡單的東西 來代表另外一個 比較複雜的東西 所以語言模型既然是有模型這個字 代表它是在模擬另外一個更複雜的東西 它模擬的另外一個更複雜的東西是什麼呢 是人類真正的語言 所以語言模型它模擬的是人類真正的語言 那我們把人類真正的語言生成的過程簡化成 就是在做文字接龍 用一個transformer來模擬 這個就是語言模型 但是transformer 雖然語言模型已經是一個模型了 但這個模型還是太複雜了 複雜了複雜到你無法解析它不知道它在做什麼 所以我們需要一個語言模型的模型 一個模型它需要有什麼樣的特性呢 一個最直覺的就是它當然要比原來的食物還要簡單
              
                  1:06:01
                  所以語言模型的模型當然要比原來的語言模型還要簡單 但是同時它要保留有原來食物的特徵 所以語言模型的模型至少在我們感興趣 的任務上 它的運作應該要跟語言模型一樣 它的輸入輸出的關係 應該要跟原來的語言模型一樣 保有原來實務特徵的這件事 又叫做Faithfulness 所以你看那種探討語言模型的模型的那些文獻 他們常常會提到Faithfulness這個字眼來說明說 他們的模型有多像是原來真正的語言模型 那講到這邊你可能還是覺得很抽象 就跟大家介紹一個抽取知識的模型 我們都知道語言模型本身 它內含了大量的知識 它的那些知識都儲存在它的參數裡面 當你問它說臺北101在哪裡的時候 它知道在臺北 問它space needle在哪裡的時候 它知道在西雅圖
              
                  1:07:04
                  問它川普在哪裡出生 它知道在紐約 問歐巴馬在哪裡出生 它知道在夏威夷 但是語言模型是怎麼抽取 抽取諸這些知識的呢 他怎麼知道說看到space needle跟is located in 就要輸出Seattle的呢 這背後運作機制的原理是什麼樣子的呢 這邊有一篇23年的論文 這篇論文就建構了一個抽取知識的模型 這個抽取知識的模型長的是這樣的 實際的語言模型的運作 我們知道就是一個transformer輸入 比如說的臺北101 is located in 他就會輸出臺北 好那今天當我們把這個句子輸進去 語言模型是怎麼輸出這個字的呢 也許他輸出的方法是這個樣子的 他先對主詞的臺北101進行處理 前面幾層會先對主詞進行理解產生一個representation 那從主詞到這個representation的過程跟原來語言模型是一樣的
              
                  1:08:12
                  所以這邊並沒有做簡化 這個模型真正神奇的地方是 他說 根據接下來的關聯性 is located in 代表我們主詞跟受詞之間的關聯性 is located in 會產生一個linear的function 這個代表 關聯性的片語 會決定一個linear function 然後這個linear function 把這個representation 這邊用x做表示當作input 他會得到一個輸出 這個輸出做embedding 轉到這個vocabulary的space上以後 你就會看到臺北這個字 他的機率是最高的 講得更具體一點 is located in這幾個詞彙 會產生一個metric叫做WL 產生一個向量叫做BL 他們代表了一個linear function 輸入X X乘以WL加BL會得到一個Y 這個Y做unembedded 就會得到臺北這個字眼 所以今天如果說 把
              
                  1:09:14
                  the Taipei 101把這個地標換掉 換成the space needle 這個representation自然就換了 從X變成X' 但這個linear function是固定的 因為它只跟輸入的關係有關 你如果要問the space needle在哪裡的話 你用的linear function是一樣的 你只是把X換成X' 那就得到Y' 得到Y'做unembedded 你得到的就會是Seattle 那如果今天 輸入的主詞是一樣的 那X自然就不會變 但是如果你改變了這個代表關係的片語 把is located in 的改成has a height of 有多高 那就會產生另外一個linear function 那這邊用WH跟BH 來代表另外一個linear function 把輸入X乘以Wh加bh 得到w （口誤，應為y） double prime 做unembedding以後 你得到的就會是臺北101的高度 所以這個模型就告訴你說 語言模型在抽取知識的時候是這樣運作的 但這個模型跟原來的語言模型
              
                  1:10:18
                  尤其是在linear function這一塊 顯然就非常的不一樣 語言模型的最後幾個layer 真的可以用一個linear function 就概括嗎 所以你要先檢測這個模型的 Faithfulness 看看它跟原來真實的語言模型 有多接近 但是這個模型並沒有告訴我們 linear function實際上長什麼樣子 只告訴你說,linear function就跟這個代表關係的片語是有關係的,只要是一樣的關係,就有同一個linear function,但這個linear function裡面實際上的參數,你還是要自己求出來的,所以你就需要準備一些訓練資料,這個跟我們在做機器學習訓練模型的時候,其實是一樣的概念,所以你就去問一個語言模型,如果這邊放的臺北101,那在後面再接,你會輸出什麼,你會輸出什麼, 我就知道說輸入這個X,他就會輸出這個Y,有這個X,有這個Y,他們是linear function的input跟output,你要解出這個linear function裡面的參數,WL跟WB一點都不困難,在這篇paper裡面,他們會用8筆資料來找出linear function,所以這8筆資料都是某個地標在哪裡,某個地標在哪裡,找8筆這樣的資料,找出這個linear function,找出這個linear function以後,接下來
              
                  1:11:40
                  你就給他訓練資料,沒有看過的地標,比如說的space needle,然後得到X5,X5呢,再通過這個linear function,得到Y5,再看Y5做unembedding以後,會是什麼字,比如說Cietal,再看跟真正的語言模型的答案,有沒有一樣,這跟我們訓練機器學習模型的時候,分成training跟testing,其實是一樣的概念,只是在做機器學習的時候,我們的答案是人標的,他是光true,但我們 我們現在把語言模型的輸出當作真正的答案 要找一個比較簡單的模型來比擬語言模型的行為 好 那這個模型運作的是怎麼樣呢 就是還可以而已啦 所以他這邊就說了 不同的relation 到底faithfulness是怎麼樣呢 有一些relation 他的faithfulness真的非常的高 他預測語言模型的輸出的正確率 近乎百分之百 但也有一些relation 是剛才那個模型預測不準的 比如說一個公司的CEO是誰
              
                  1:12:45
                  一個人的父親是誰 一個人的母親是誰 或一個寶可夢進化以後是哪一隻寶可夢 這些他是預測不準的 所以剛才那個model 他的faithfulness就是一般啦 就在某一些relation上 他的faithfulness非常強 在某一些relation上 他的faithfulness其實也沒那麼強 不過這是一個比較早期的研究 告訴你說 還可以這麼搞 還可以找一個語言模型的模型 簡化語言模型 讓我們更容易理解 它背後運作的機制 但這個簡化要讓它有實際的用途 你就必須要能夠做到說 我在這個模型上得到的結論 可以直接用到真正的語言模型上 什麼意思呢 假設現在真正的語言模型問他 臺北101在哪裡的時候 他會回答臺北 但我現在想要修改他的輸出 我想把臺北直接改成高雄 那在真正的語言模型裡面要怎麼做這件事 就有點難嘛 我們之後講到這個類神經網路編輯的時候 後來再跟大家講說 有什麼樣的方法可以做到這件事 但我們在開學的第一堂課就告訴你說
              
                  1:13:49
                  如果你是用訓練資料 fine-tune語言模型 忘了fine-tune完以後模型就壞掉了 所以這個簡單的模型 語言模型的模型 就提供給你一個 類神經網路編輯的可能性 你可以先從這個模型上推論說 假設我要這個模型輸出高雄 那我這輸入的X 要怎麼改呢 我這輸入的X 要加上什麼樣的Delta X 輸出才能輸出是高雄呢 因為這個Linear Function 它只是一個Linear的Transformation 給定一個指定的輸出 你要求反推出 應該要什麼樣的輸入 才能給出指定的輸出 其實是容易的 你有修過線性代數 其實你都知道怎麼做 所以這個不是一個困難的問題 你可以找到一個Delta X 加上這個X之後 那輸出就會變成高雄 但是這個是在模型上找出來的 那模型上的結論 能不能直接用在語言模型上呢 現在把臺北101 is located in輸入給語言模型 把在語言模型的模型上 找出來的這個Delta X
              
                  1:14:54
                  直接輸入給真正的語言模型 直接加到真正的語言模型上面 如果他輸出會變成高雄的話 那這個模型就有用 這個模型上預測的結果 可以幫助我們修改真正的語言模型 模型的輸出 好那這個模型的模型 這個語言模型的模型上面觀察到的結果 有沒有用呢 居然還蠻有用的 在這個圖上每一個點 代表某種類型的關聯 橫軸代表 Fairfulness 的這個數值 有一些這個關聯是做得起來的 有些關聯是做不起來的 縱軸代表剛才那一個 那一招 那個類神經網路編輯就直接在那個 模型的模型上找出來的結論 直接用到語言模型上 到底能不能夠成功的修改呢 這個縱軸是正確率 那你會發現說有蠻多情況 居然是可以直接成功修改的 所以這個模型是一個有用的模型 不過剛才那個模型
              
                  1:15:59
                  比較像是某人腦門一拍 腦洞一開就突然產生出來的模型 那有沒有系統化的方法 幫語言模型 建構語言模型的模型呢 有沒有系統化的方法 幫語言模型建構它的模型呢 是有的 這邊有一系列的研究 那這邊我們就不講它的細節 我們就講它的精神 這一系列建構語言模型的模型的方法 他們的概念是這樣子的 把原來的語言模型 做一個很大的pruning pruning的意思就是 把語言模型裡面的一些component拿掉 拿掉一個神經元,我直接拿掉某一個self-attention,看看模型還能不能妥善運作,那我們要一直pruning,一直pruning,一直pruning,一直拿掉component,直到prune完之後新的模型,那新的模型就是語言模型的模型,prune完之後的新的類神經網路,就是語言模型的模型,直到碰到它一目瞭然為止,直到它變得非常簡單為止,那我們pruning的時候,要確保說我們關心的那個任務, 它的輸入輸出的關係仍然是沒有改變的,本來的語言模型,你問它這個問題,它會有這個答案,問它這個問題會有這個答案,問完之後,我們會做一個非常劇烈的pruning,prune完之後,類神經網路要看起來非常簡單,人類一目瞭然,但是我們關心的那些任務,它的答案仍然是不可以改變的。
              
                  1:17:30
                  好,那這個prune完之後的結果啊,今天在文獻上通常叫做circuit,那這個circuit跟真實的電路沒有什麼太多的關聯 那這個circuit就是語言模型的模型 那這件事情呢,又很像是network compression 那我想蠻多人都知道network compression了 那我們在過去2021的課程也有講過network compression 模型壓縮的方法就把一個比較大的類神經網路 變成一個比較小的類神經網路 那這邊建立模型的模型的方法跟network compression有什麼不一樣呢 方法是非常類似的 比如說都可以用printing來拿掉一些沒用的component,但目標不一樣,我們一般做network compression的時候,你其實希望compress以後的結果,在各個不同的任務上都要逼近原來的模型,原來的語言模型,建構模型的模型這邊呢,我們只關心某個特定任務,比如說只關心knowledge extraction,或甚至在一些更古早的文章裡面,他只關心一個叫做IoI的問題,這個IoI的問題, IoI的問題是什麼呢?
              
                  1:18:40
                  這個非常簡單的問題,這個問題就是 A跟B一起去酒吧 B拿了一杯酒給 然後叫語言模型做接龍 那就要接A嘛,然後他只分析 這個任務,然後他就會發現說 在這個任務上面呢 需要比如說五六個attention 然後他就把模型 就噴噴噴噴噴噴掉 然後多數的component跟這個任務都沒關係 都噴掉,最後只留下五六個attention 你就可以清楚知道說 語言模型在最後回答A的時候 中間經歷了什麼事情 所以這個大家在文獻上 自己再慢慢看 建立語言模型的模型的時候 建立這個circuit的時候 你會希望做非常大量的printing 只要print到人看得懂這個類人間網路在做什麼 那你只關心少數的任務 你只關心非常侷限的任務 他的答案會不會改變 而network compression通常不會print那麼多 你也不在意print完以後的結果 能不能夠一目瞭然 能不能夠被解釋 但是你希望prune完之後的結果 跟碰之前在多數的任務上 在多數的狀況下 這個能力呢
              
                  1:19:43
                  這個模型的能力是不要有太大改變的 那這邊就只是講了一個 建立模型的模型 建立circuit的概念啦 那相關的文獻只能說是汗牛衝動 那我們這邊就不細講 那這邊就是列了幾篇 舉標具有代表性的論文給大家參考 那你看到說這些論文的標題 都有circuit這個字跟電路沒有關係 它是告訴你說它們是怎麼研究語言模型的模型的 好,那最後一個部分要跟大家講 怎麼讓語言模型直接說出它的想法 那在2024年的生成4AI導論裡面 我們也講過說語言模型會說話 所以很多人說這個大型語言模型黑盒子沒有解釋性 不,它是最有解釋性的 它就跟人類一樣 你有什麼問題,你要叫它解釋結果 我問就完事了,你就給他,比如說我叫他做新聞的分類,跟他說新聞就分成這幾類,給你一篇文章,告訴我新聞是哪一類,他可以輕易的告訴我,就是生活類,所以接下來希望他進一步解釋,為什麼知道這篇文章是生活類,比如說你問他,哪幾個關鍵字讓你覺得是生活類呢,他就列出幾個跟天氣有關的關鍵字,說我是因為看到這幾個關鍵字,所以覺得是生活類,但是這樣子的方法還是 也是有他的侷限,他的侷限是什麼呢?你沒辦法真的知道每一個layer在想什麼,而如果你真正直接問我們語言模型說,你是在第幾層類神經網路開始知道這邊新聞是生活類的,我問確GPT-4.5,他其實也會回答你,但他其實回答就是很像是在教科書上抄出來的答案,他說淺層的類神經網路,就是提取初步的字詞跟短語特徵,比如說語法結構等等,中層,我可以
              
                  1:21:33
                  可以辨識出特定主題,然後生成,我就可以知道說這篇文章是生活類 但是語言模型是不是真的是這樣運作的,或他自己知不知道自己是這樣運作的 這個真的是很難說,這比較像從教科書上抄出來的答案 模型相對於人類,他的思維是更加透明的 當你叫一個人解釋他為什麼會做這樣決策的時候 你不知道他心裡是怎麼想的 但語言模型神奇的地方是 他的思維是透明的 你可以直接看到 他每一層是怎麼想的 怎麼說呢 好這邊就跟大家剖析 為什麼語言模型的思維是透明的 我們之前講一個layer的時候 我們都講說一個layer就是輸入一排向量 輸出一排向量 但是其實這只是一個簡化的講法 我們忽略了一個最重要的component 就是residual connection residual connection的意思是說 當一個layer得到一排輸出之後 每一個輸出都還會跟輸入 加起來再得到最終的輸出
              
                  1:22:40
                  所以這個投影片上紅色的向量 才是最終的輸出 紅色向量的輸出都是黃色的向量 加上綠色的向量 而黃色的向量是從綠色的向量產生出來的 雷根會想說 為什麼需要有residual connection 這樣子的設計呢 那這個設計它的起源非常的古老 它是在15年的時候就有了 那個時候世界上 還沒有人類 那個時候呢 是大概前寒武紀的時候 這個15年的時候 人類就 人類那時候還沒有人類 但是那個時候 就已經有residual的connection 那residual connection的出現 是為了要讓很深的network變得更好train 沒有residual connection之前 那時候network都train個二三十層 有了residual connection之後 都可以train個一百多層 我知道你現在覺得train一百多層 也沒有什麼神奇的 但是當年人們都驚呆了 加了這個功能之後 加了這個連結之後 居然就可以train一百多層的network 太神奇了 這個就是residual connection 所以後來在train深的網路的時候 其實都有residual connection這個設計 所以實際上一個transformer
              
                  1:23:47
                  它layer跟layer之間的運作 是要加上這個residual connection的 是要加上這個residual connection的 也就是說一個token進來 它通過一個layer的時候 它除了產生一個output之外 還會把原來的輸入再加起來 通過一個layer產生output的時候 會把原來的輸入再加起來 最後再做unembedded 得到最終的輸出的distribution 你可能覺得看這個圖 沒有什麼神奇的 那我們換一個畫法 左邊的圖跟右邊的圖 是一模一樣的 並沒有真的改變它實際的運作 但是當我們圖變得不一樣的時候 你的想法就變了 從這個左邊的圖看起來 你會覺得說 是輸入做了一個轉換 輸入做了一個轉換 但是當我們把圖換一個畫法的時候 整個transformer 它真正的運作更像是 它有一個叫做 residual string的高速公路 直接把輸入的東西 一路就傳到輸出
              
                  1:24:51
                  而在中間的過程中 每一個layer 都會加一點東西 到輸入裡面 加一點東西到輸入裡面 這個residual string是一個生產線 就一路的被送上去 一路被送上去 只是每過一站都會加上料 加上一些額外的資訊 加上一些額外的資訊 最後得到最終輸出的distribution 所以這才是transformer多個layer 真正運作的機制 好那既然在最後一站 你可以過一個unembedding的module 過一個linear transformation 把這個向量變成一個極限 那前面這幾站 只是最後一站少了一點什麼東西而已 那前面這幾站 能不能也直接加一個unembedding的layer 把它變成token的機率分佈 也就是變成文字的機率分佈呢 這件事是可行的 那這一招是有名字的 現在多數人稱它為logic length
              
                  1:25:56
                  因為這個輸出的distribution 在過softmax之前叫做logic 我們今天是檢查每一層的logic 來看看類神經網路 來看看transformer是怎麼思考的 所以它叫做logic length 那人類什麼時候知道語言模型的思維是透明的呢 其實我們實驗室在2020年年初的時候 就知道語言模型的思維是透明的 那時候就已經發現說 那時候的模型BERT 其實你是可以看到它每一層 在想什麼的 你可以透過Large Lens的方式 解析出每一層的文字內容 這個是高偉聰同學跟那個吳宗漢同學做的 當時有了這個發現以後覺得 這個發現一點用都沒有這樣子 那時候覺得這能幹嘛 所以這篇文章後來甚至是沒有投稿 就直接放在Archive上而已 左邊這個圖是論文裡面的圖 那這個就是告訴我們說
              
                  1:27:00
                  你可以把最後的Unembedding Layer 接到中間的每一層 你就可以看到Bert是怎麼處理一段文字的 那右邊這個表格是一個真實的例子 現在輸入的句子是 It's a bitter sweet and lyric mix of elements 就是這是一種苦樂參半 且抒情元素的混合體 那it是什麼 沒有講這個句子就只有一個代名詞it 那接下來你把這個句子輸入 Bert一個遠古時代的語言模型 然後解析出 它每一層的輸出 你會發現到第11層的時候 it那一個字 變成了elbent 你把embedding這個機制 裝到第11層的時候 它解析出來的不是it 而是elbent 代表在這一層的時候 Bert知道說 it這個東西 它指的可能是某一個專輯 這也符合這個 蠻符合這個句子的敘述 有可以瞭解說Bert其實會把這些代名詞做一些reference 他去猜說這個代名詞實際上指的是什麼樣的實體
              
                  1:28:08
                  好,那後來呢,就有很多人利用這種logic lens的方法來分析語言模型內部是怎麼思考的 這邊引用的是一篇23年的論文 他就想要知道說語言模型是怎麼回答一個問題的呢 他這邊就是要問語言模型一個國家的首都是哪一個城市 因為這是比較舊的模型 他需要做in context learning 他要先跟語言模型說 what is the capital of France answer是Paris 然後what is the capital of Poland answer 然後叫他做文字接龍 看能不能接出華沙這個城市的名稱 那實際上語言模型運作是怎麼樣呢 他就把冒號這個位置對應的representation 每一層都用large lens解析出來 一開始語言模型根本搞不清楚 他應該是哪一個token 但走到第15層的時候 他突然知道那個token應該是Poland 然後從第19層開始 他突然知道應該是要回答華沙 就從第15層開始
              
                  1:29:13
                  他知道說接下來要輸出的東西 應該跟Poland是有關係的 然後到第19層開始 他知道說他要輸出的是華沙這個城市 那左邊是更詳細的分析 那這個縱軸啊 是代表的是那個 在distribution裡面 這個華沙跟Poland這兩個token的機率 那實際上他顯示的不是機率啊 因為機率實際上畫出來可能會非常的小 所以他顯示的是reciprocal rank reciprocal rank是什麼意思呢 就是那一個token 在所有token裡面機率排名的倒數 如果他排第一名數值就是一排第二名 二分之一排第三名就是三分之一 以此類推 就看到說Poland這個詞彙 隨著layer持續的增加 在某一層突然之間 residual street裡面 出現Poland這個字 然後接下來又急遽下降 被華沙所取代 那感覺語言模型先知道說 要回答一個跟Poland有關的東西
              
                  1:30:18
                  最後才鎖定說真正的答案 是華沙 而且呢 如果今天同樣答案是華沙 不同的問法 他的回答他背後運作的機制是不一樣的 我們剛才說如果直接問他這個問題 他會先鎖定說答案是跟波蘭有關 然後再回答華沙 但另外一方面 假設是讓他做閱讀理解測驗 先給他一篇文章 再問他一個問題 就直接問他說 波蘭的首都在哪裡 那前面的文章裡面已經提到波蘭首都是華沙了 然後直接給他answer的話 他就不會產生波蘭這個字 他直接在第十六層就知道 答案是華沙了 又知道說不同的問法 不同的狀況 他背後運作的機制是不一樣的 那透過這種Logic Lens 你就可以去知道語言模型心裡在想些什麼 比如說有人會想說 像LLaMA-2這種模型 他看過的英文資料是遠比中文資料多的 所以他實際上在想事情的時候 他內心深處到底是用哪個語言呢 這篇文章是去年年初的文章
              
                  1:31:24
                  他們就做了一個實驗 他們用LLaMA-2呢 來做翻譯 他們就跟LLaMA-2說 法文的這個詞彙 這個是花的意思了 法文的這個詞彙 對應到中文的哪一個詞彙呢 那LLaMA-2可以正確的接觸 花這個字 但他怎麼知道法文的這個字 翻成中文就是花呢 你如果分析他中間的每一個layer的話 會發現說 他是先把法文的花 翻成英文的花 再把英文的花 翻成中文的花 所以右邊就是用logit lens 分析每一層之後 得到的結果 輸入是中文冒號 然後這個是空格 空格之後就要產生答案了 所以從空格開始 把每一層都用logit lens 解析出來 那最前面幾層 紅色就代表說 透過logit lens解析出來的 那個Distribution 它的Entropy越大 它要輸出的
              
                  1:32:26
                  是英文的Flower 到了27層之後 它才意識到說要把英文的Flower翻譯成 中文的花 所以代表說模型在思考的時候 它內部其實是用英文 在思考的 它是先把法文翻成英文再把英文 翻成中文 雖然你外表看起來再把法文直接翻成中文 在它中間是用英文做媒介 不過這是在LLaMA-2上的實驗啦 LLaMA-3現在中文能力其實蠻強的 所以期待有人去分析LLaMA-3 看看他內心是不是還用英文在思考 好,那我們現在已經有了residual string的概念之後 接下來我們對於每一個layer做的事情 就可以有不同的想像 我們現在知道說每一個layer就是加一點什麼東西進去這個residual string 那他到底加了什麼呢 我們要怎麼解析每一個layer加了什麼樣的東西 一般我們在講神經元的時候 我們都是說把前一個layer的輸出集合起來
              
                  1:33:32
                  做weighted sum變成一個神經元 把前面的layer集合起來 weighted sum變成一個神經元 但你可以反過來看待這件事 反過來看待這件事以後 這個世界就變得不一樣 它的運作是完全一模一樣的 但是反過來看以後 你可以有不同的理解 你可以說前一層的 某一個神經元某一個dimension的數值 乘上weight以後 傳輸給 接下來下一層不同的dimension 前一層的某一個數字 某一個dimension乘上不同的weight以後 傳到下一層去 那這件事情 這樣的一個概念 這只是一個概念 因為他並沒有改變什麼東西 他就概念 是在transformer feed forward layer 這篇paper裡面被提出來的 那篇paper引用非常高 很多人都知道這個概念 就是其實 一個multilayer的perceptron 一個多層的 可以看作是一個
              
                  1:34:35
                  key有key 有value的attention 前一層的這些數值 就是attention的weight 然後後面output的這些數值 就是一個value 不知道大家聽不聽得懂 那如果你暫時一下子沒有辦法 心領神會的話 也沒有關係 用不塞上這個概念 你回去再仔細看一下這篇論文 這篇論文是一個改變 大家對於Feed Forward Network 的想像的論文 好 所以假設 前一層每個Dimension的數值 就是K1 K2到KD 這邊每一個K代表一個Scalar 一個數值 那K2它會接到 下一層的每一個輸出 那我們把K2對應到 下一層每一個輸出的weight 集合起來說它是一個向量 叫V2 KD也對到每一層都有一個weight 我們把這個 叫做VD 那它們是向量V2 VD是向量 所以這個藍色的輸出啊 其實是前一層每一個K
              
                  1:35:41
                  乘上它對應的V再加起來 所以藍色的輸出 是KI乘上VI Summation over I等於1到大D 好,那我們知道說呢,每一層啊,在residual string上面,每一個位置都可以透過logit lens解出一個distribution,解出一個distribution,那這個藍色的向量,它就是加進去以後改變了這個distribution,那這個藍色的向量是由一堆的V做weighted sum以後集合起來的,那我們能不能夠把V這邊的V也做unembedded, 透過logit lens,解析說每一個類神經網路,它想要輸出什麼樣的東西,去加入residual string,影響最終的輸出呢,其實是可以的,每一個加入這個residual string的這些V2,VD,都可以透過unembedded layer,轉成一個token的distribution,它可能也代表了某些特定的意思,真的是這樣子嗎? 一篇22年這個遠古時代的論文
              
                  1:36:54
                  這個遠古時代的論文 那個時候人類只有茹毛飲血 但那時候人類就已經發現說呢 這些V是真的有對應到某一些概念 某一些意思的 比如說有某一個V 這個是第三層的編號1018的V 他就對應到一些單位 有一個V 他是第一層的編號第一個V 他就對應到一些 這個代名詞 第六層的編號3025的V 他就對應到一堆副詞 第十三層編號3516的V 他就對應到一些不同的族群 所以你也可以透過這個logit lens 來解析這些V 他代表了什麼樣的意思 好那知道這件事以後 能夠做什麼呢 知道這件事以後 你就可以對類神經網路 做初步的編輯 之後還會講更多更強悍的編輯的方法 但這邊講一個很基礎的 所謂編輯的方法 假設你問大型語言模型 誰是全世界最帥的人 他通常不回答你
              
                  1:37:57
                  但如果有prompt GPT-4.5很多次 某一次我發現他就說是金城武 所以他可能認為金城武是世界上最帥的人 那如果我把金城武換成李宏毅的話 要怎麼做呢 我們之前講過如果直接train network network是會壞掉的 但是我們剛才已經知道 每一個V就是加一點資訊 到整個residual network裡面 所以你可以分析說 當模型輸入這句話 產生這個答案產生金城武的時候 到底是哪一個V 被加進去了這個residual string 你知道identify出 這一個V之後 把這個V減掉 金城武的token embedding 再加上李鴻一的token embedding 但這邊假設是金城武跟李鴻一都是一個token 減掉金城武的token embedding 加上李鴻一的token embedding 就本來類神經網路 當他啟動某一個K 要加某一個V到residual string的時候 他是為了回答全世界最帥的人 但我們把他的資訊 從金城武置換成李宏毅 他就可以把李宏毅當作答案 這一招有用嗎 這個在21年
              
                  1:39:00
                  輪古時代的時候就已經有人試過了 這一招他有48% 的機率可以改變 類神經網路的輸出 改變不見得答對喔 就輸出變得不一樣 那成功的機率 他真的輸出是李宏毅成功的機率有34% 那個人覺得34%沒有很高 那不是0啊 就代表說這一招是可以 真的拿來編輯類神經網路 改變他的輸出的 好 剛才那個Large Lens的方法呢 有一個致命的缺陷就是 我們透過Unembedding的方法 只能夠把一個Representation 轉成一個Token 所以我們解析出來的結果 都只能是一個Token 另外一方面啦 很多時候語言模型在做的事情是 預測下一個Token 你輸入李宏毅老師中間這個Embedding 並不見得代表李宏毅老師 這個詞彙的含義 它真正代表的是 看到這個輸入以後 模型想要輸出是 這個Token 它想要做 產生是這個Token的時候 所產生的Representation 所以假設你想要知道
              
                  1:40:04
                  李宏毅老師在類神經網路看起來 是什麼意思 你用Logic Length 可能不一定能夠解析出 你要的結果 所以怎麼辦呢 有另外一招 那這個就是去年的論文了 這招叫Patch Scope Patch Scope Patch Scope這一招的意思是說 我們先看看 如果我們跟 語言模型給他這樣的輸入 跟語言模型講說 李奧納多冒號美國演員 臺積電冒號臺灣公司 然後再隨便給他一個東西 這個X可以是任何東西 那他就會輸出 他就會輸出 他對於X的理解 那怎麼知道一個類神經網路 當他看到李宏毅老師 這幾個字的時候 他內心深處的理解是什麼呢 你就把李宏毅老師輸入這個類神經網路裡面 然後看看在某一層他輸出的representation長什麼樣子,接下來把這個representation置換到這一個input string裡面,就這個類似這個語言模型他的輸入是一樣的,這邊甚至一樣就是給他一個x就好了,但是在這個位置把他的embedding,把在這個位置把他的representation換成輸入是這一串文字時候的representation,那對這個類似對這個語言模型來說,他就好像看到 X是李宏毅老師這一串字一樣
              
                  1:41:23
                  然後他就開始繼續輸出 他就有可能告訴你李宏毅老師的身份 那我知道講到這邊你可能會有個困惑就是 那我前面起不是要準備一些例子 那我準備的例子不是會影響最終輸出的結果嗎 沒錯就是你準備的例子 就是會影響最終輸出的結果 不過這篇文章的作者覺得這是一個feature 不是一個bug 你可以調整前面準備的例子 然後模型就會給你不同風格的解釋 比如說如果你現在的輸入是告訴我 X相關的秘密 然後你再把X這個位置的representation 換成李宏毅老師的representation 他可能就回答是個肥仔 你就可以從不同的角度來解析一個representation 那這邊就是引用了剛才就是提出這個 scope patch這個方法原始論文裡面舉的一個例子 他們就把 戴安納他是這個 那個 威爾斯王子的王妃 我這邊似乎少打了一個S
              
                  1:42:28
                  不過沒有關係 那個戴安納他是威爾斯王子的王妃 然後把這個片語 輸入給 類神經網路輸入給語言模型 然後接下來解析 他看到這個片語的 最後一個字的時候 他的每一層 的輸出對 這一個語言模型來講 分別是什麼,所以在前面一二層的時候,如果你把這個位置的representation拿去解析,語言模型輸出的句子是country in the United Kingdom,或者是country in Europe,因為威爾斯也是一個英國裡面的國家的名字,是United Kingdom裡面一個國家的名字,所以語言模型在前面幾層,他只認了威爾斯這個字,所以他就覺得他看到的是一個 但到了第四層的時候,他顯然讀到了Princess of Wales,他讀到了這一整串詞彙,在第四層的時候,他解析是說這是一個給皇室女性的頭銜,然後到第五層的時候,他知道說這個人呢,是威爾斯王子的妻子,然後到第六層,他才讀到戴安娜這個字,然後就輸出戴安娜完整的資訊,所以可以透過這個方法解析一個語言模型,每一層他可以 他看到的,他看到的東西實際上對應的文字是什麼,接下來最後一部分,最後幾頁投影片呢,就是舉一個例子,說剛才那些解析的方法,如何改變了人們對類神經網路背後運作機制的理解,進而提出了新的想法,那這是一篇去年六月的文章,這篇文章想要解析的是,對於一個multi-hop question,語言模型是怎麼回答的,然後解析完之後,他提供了
              
                  1:44:20
                  提出來一個方法,讓語言模型在multi-hop的question上面可以做得更好 那這個multi-hop的question這邊的例子是 the spouse of the performer of imagined is 像這種multi-hop的question裡面呢 會有包含三個entity 第一個entity是會明確出現在問題裡面的 那在這個例子裡面就是imagine 那我們把它叫做E1 那第一個entity imagined是一張專輯的名字 那接下來我們要問的是 The Performer of Imagine 就是彈奏Imagine創造Imagine這張專輯的這個音樂人是誰呢 那這個是E2 那他其實約翰藍儂（John Lennon） 所以這個E2是約翰藍儂 然後接下來呢 約翰藍儂的配偶又是誰呢 那這是E3 約翰藍儂的配偶是小野洋子 就是YOKO 然後模型知道這個答案以後 他就要看到這一串文字 然後輸出YOKO 那接下來的問題是 模型是怎麼做 這一連串的解析的呢 他是怎麼做這種需要
              
                  1:45:24
                  Multi-Hop Reasoning 需要做多步推理的問題的呢 一個直覺的想法是 模型讀到Imagine這個字以後 他根據前面的關係 The Performance of Imagine 先解析出答案是約翰藍儂 然後知道答案是約翰藍儂之後 再經過 The Spouse of約翰龍 這一個片語 解析出最終的答案 是Yoko是小野洋子 那模型真的是這樣運作的嗎 所以他們就他們就用剛才講的那個 那個PatchScope那個方法 做了一下解析 所以他們就把這一個位置的每一層 都拿出來看看 看看會解析出什麼樣的內容 然後如果解析出的內容裡面 有包含約翰藍儂這個字 就把它記錄下來 那得到的結果呢 是藍色的這條線 藍色的這條線橫軸呢 是layer然後縱軸呢 是解析出 這個E2 解析出E2的 第一次出現的layer
              
                  1:46:28
                  所以就會發現說 什麼時候解析出E2呢 什麼時候語言模型可以根據E1解析出E2呢 在蠻前面的layer 就可以根據E1 解析出E2了 好那根據E1解析出E2以後 再來要根據E2解析出E3 那什麼時候解析出 E3呢 他就分析這一個位置 每一個representation 他對應的文字 然後如果有出現 E3的內容的話就把它記錄下來 然後得到的結果呢 是橙色的這條線 所以你會發現說 多數情況都是大概在layer 第20層到第25層間 會解析出 E3這個詞彙 所以你可以感受到說 在比較低的layer 先解析出E2 然後接下來才解析出E3 然後最後就可以給你 正確的答案 然後這篇文章的作者發現說 當有時候這種multi-hop的question 沒有辦法得到正確的答案
              
                  1:47:31
                  是因為E2太晚被解析出來了 因為E3 必須要在第20幾個layer 被解析出來 才有解析出最終答案的能力 如果今天E2太晚被解析出來 過了20層才被解析出來 那接下來在T2這個位置 就來不及解析出E3了 所以怎麼解決這個問題呢 他們有一個神妙的做法 就是把後面幾層的representation 直接加到前面來 再重新跑一次 就結束了 既然今天只有中間某一層 能夠解析出E3 如果E2太晚被解析出來 那怎麼辦呢 把後面的layer放到前面 這樣只有能夠走過第二十層 就可以把E3解析出來了 這招有沒有用呢 這招居然是有用的 他們試了各式各樣不同的模型 那correct代表說 在用這招之前 模型本來就會答對的問題 那本來就會答對
              
                  1:48:33
                  那正確率居然當然是100嘛 然後做完這招以後不會影響正確率 但神奇的地方是 對本來不對的問題 用了這招以後 大概會有40到60%的正確 你可能會覺得40%到60%的正確率 也沒有很高 但不要忘了 這邊40%到60%的問題是 原來全部都答不對的 所以原來是0%的正確率 用了這招以後 原來完全答不對的問題裡面 居然有4到6成 可以因此就答對了 所以這一招 看起來其實又跟reasoning有點像 我們在第一堂課 不是講過reasoning 就是深度不夠 長度來湊嗎 這邊paper是6月的 的時候還沒有reasoning的模型 拿起跟reasoning的模型 也是很像的 reasoning的模型只是把 你的輸出如果來不及解析完 就跑到下一個time step 再重新解析一次 所以這個方法 這邊paper提出來一個叫做backpatching的方法 其實跟reasoning 深度不夠
              
                  1:49:36
                  長度來湊的做法 其實是有異曲同工之妙的 好 那這個就是今天想要跟大家分享的內容 就從一個神經人開始講起 最後講到怎麼讓語言模型 直接輸出他解析的結果
              
            