
 所以在作業3裡面 會讓大家來觀察Gemma 2的功能向量 Gemma 2 就是昨日黃花 前幾天也上線了這個時代 真的變化得很快 那剛才講的是 一層神經元在 做什麼 那接下來我們要進一步擴展到 一群神經元在做什麼 也就是說我們想要知道 一個語言模型在完成 某一項任務的時候 他從輸入到輸出 到底經歷了哪些事 那像這類的研究啊 其實在過去 已經有很多的文獻 這個文獻可說是汗牛衝動 那這邊呢就舉兩個例子
              
                  1:03:52
                  一個例子是有人研究了 這個語言模型 他內部抽取知識的機制 你就問他說 Bits Music是屬於哪一家公司的呢 那他會回答Apple 那從輸入這一家Bits Music 輸入這家公司 跟他說is on by這個關係 到最後輸出Apple語言模型中 發生了什麼事 那這是有論文分析過的 我們討論過說語言模型 是怎麼做數學的呢 你問他15乘以12是多少的時候 他是怎麼算出180的 背後的計算過程 是怎麼算出來的 也有文獻探討過了 所以你當然可以針對 每一個你想要 瞭解的任務去分析 語言模型背後運作的原理 但我這邊想要跟大家講的是一個更通用的想法 那我們怎麼瞭解語言模型 做某一件事的時候 背後的完整機制呢 也許我們需要的是 語言模型的模型 我知道這句話聽起來非常的奇怪
              
                  1:04:56
                  語言模型已經是一個模型了 這個模型還有模型 到底是什麼意思呢 模型這個字是什麼意思呢 模型指的是 用一個比較簡單的東西 來代表另外一個 比較複雜的東西 所以語言模型既然是有模型這個字 代表它是在模擬另外一個更複雜的東西 它模擬的另外一個更複雜的東西是什麼呢 是人類真正的語言 所以語言模型它模擬的是人類真正的語言 那我們把人類真正的語言生成的過程簡化成 就是在做文字接龍 用一個transformer來模擬 這個就是語言模型 但是transformer 雖然語言模型已經是一個模型了 但這個模型還是太複雜了 複雜了複雜到你無法解析它不知道它在做什麼 所以我們需要一個語言模型的模型 一個模型它需要有什麼樣的特性呢 一個最直覺的就是它當然要比原來的食物還要簡單
              
                  1:06:01
                  所以語言模型的模型當然要比原來的語言模型還要簡單 但是同時它要保留有原來食物的特徵 所以語言模型的模型至少在我們感興趣 的任務上 它的運作應該要跟語言模型一樣 它的輸入輸出的關係 應該要跟原來的語言模型一樣 保有原來實務特徵的這件事 又叫做Faithfulness 所以你看那種探討語言模型的模型的那些文獻 他們常常會提到Faithfulness這個字眼來說明說 他們的模型有多像是原來真正的語言模型 那講到這邊你可能還是覺得很抽象 就跟大家介紹一個抽取知識的模型 我們都知道語言模型本身 它內含了大量的知識 它的那些知識都儲存在它的參數裡面 當你問它說臺北101在哪裡的時候 它知道在臺北 問它space needle在哪裡的時候 它知道在西雅圖
              
                  1:07:04
                  問它川普在哪裡出生 它知道在紐約 問歐巴馬在哪裡出生 它知道在夏威夷 但是語言模型是怎麼抽取 抽取諸這些知識的呢 他怎麼知道說看到space needle跟is located in 就要輸出Seattle的呢 這背後運作機制的原理是什麼樣子的呢 這邊有一篇23年的論文 這篇論文就建構了一個抽取知識的模型 這個抽取知識的模型長的是這樣的 實際的語言模型的運作 我們知道就是一個transformer輸入 比如說的臺北101 is located in 他就會輸出臺北 好那今天當我們把這個句子輸進去 語言模型是怎麼輸出這個字的呢 也許他輸出的方法是這個樣子的 他先對主詞的臺北101進行處理 前面幾層會先對主詞進行理解產生一個representation 那從主詞到這個representation的過程跟原來語言模型是一樣的
              
                  1:08:12
                  所以這邊並沒有做簡化 這個模型真正神奇的地方是 他說 根據接下來的關聯性 is located in 代表我們主詞跟受詞之間的關聯性 is located in 會產生一個linear的function 這個代表 關聯性的片語 會決定一個linear function 然後這個linear function 把這個representation 這邊用x做表示當作input 他會得到一個輸出 這個輸出做embedding 轉到這個vocabulary的space上以後 你就會看到臺北這個字 他的機率是最高的 講得更具體一點 is located in這幾個詞彙 會產生一個metric叫做WL 產生一個向量叫做BL 他們代表了一個linear function 輸入X X乘以WL加BL會得到一個Y 這個Y做unembedded 就會得到臺北這個字眼 所以今天如果說 把
              
                  1:09:14
                  the Taipei 101把這個地標換掉 換成the space needle 這個representation自然就換了 從X變成X' 但這個linear function是固定的 因為它只跟輸入的關係有關 你如果要問the space needle在哪裡的話 你用的linear function是一樣的 你只是把X換成X' 那就得到Y' 得到Y'做unembedded 你得到的就會是Seattle 那如果今天 輸入的主詞是一樣的 那X自然就不會變 但是如果你改變了這個代表關係的片語 把is located in 的改成has a height of 有多高 那就會產生另外一個linear function 那這邊用WH跟BH 來代表另外一個linear function 把輸入X乘以Wh加bh 得到w （口誤，應為y） double prime 做unembedding以後 你得到的就會是臺北101的高度 所以這個模型就告訴你說 語言模型在抽取知識的時候是這樣運作的 但這個模型跟原來的語言模型
              
                  1:10:18
                  尤其是在linear function這一塊 顯然就非常的不一樣 語言模型的最後幾個layer 真的可以用一個linear function 就概括嗎 所以你要先檢測這個模型的 Faithfulness 看看它跟原來真實的語言模型 有多接近 但是這個模型並沒有告訴我們 linear function實際上長什麼樣子 只告訴你說,linear function就跟這個代表關係的片語是有關係的,只要是一樣的關係,就有同一個linear function,但這個linear function裡面實際上的參數,你還是要自己求出來的,所以你就需要準備一些訓練資料,這個跟我們在做機器學習訓練模型的時候,其實是一樣的概念,所以你就去問一個語言模型,如果這邊放的臺北101,那在後面再接,你會輸出什麼,你會輸出什麼, 我就知道說輸入這個X,他就會輸出這個Y,有這個X,有這個Y,他們是linear function的input跟output,你要解出這個linear function裡面的參數,WL跟WB一點都不困難,在這篇paper裡面,他們會用8筆資料來找出linear function,所以這8筆資料都是某個地標在哪裡,某個地標在哪裡,找8筆這樣的資料,找出這個linear function,找出這個linear function以後,接下來
              
                  1:11:40
                  你就給他訓練資料,沒有看過的地標,比如說的space needle,然後得到X5,X5呢,再通過這個linear function,得到Y5,再看Y5做unembedding以後,會是什麼字,比如說Cietal,再看跟真正的語言模型的答案,有沒有一樣,這跟我們訓練機器學習模型的時候,分成training跟testing,其實是一樣的概念,只是在做機器學習的時候,我們的答案是人標的,他是光true,但我們 我們現在把語言模型的輸出當作真正的答案 要找一個比較簡單的模型來比擬語言模型的行為 好 那這個模型運作的是怎麼樣呢 就是還可以而已啦 所以他這邊就說了 不同的relation 到底faithfulness是怎麼樣呢 有一些relation 他的faithfulness真的非常的高 他預測語言模型的輸出的正確率 近乎百分之百 但也有一些relation 是剛才那個模型預測不準的 比如說一個公司的CEO是誰
              
                  1:12:45
                  一個人的父親是誰 一個人的母親是誰 或一個寶可夢進化以後是哪一隻寶可夢 這些他是預測不準的 所以剛才那個model 他的faithfulness就是一般啦 就在某一些relation上 他的faithfulness非常強 在某一些relation上 他的faithfulness其實也沒那麼強 不過這是一個比較早期的研究 告訴你說 還可以這麼搞 還可以找一個語言模型的模型 簡化語言模型 讓我們更容易理解 它背後運作的機制 但這個簡化要讓它有實際的用途 你就必須要能夠做到說 我在這個模型上得到的結論 可以直接用到真正的語言模型上 什麼意思呢 假設現在真正的語言模型問他 臺北101在哪裡的時候 他會回答臺北 但我現在想要修改他的輸出 我想把臺北直接改成高雄 那在真正的語言模型裡面要怎麼做這件事 就有點難嘛 我們之後講到這個類神經網路編輯的時候 後來再跟大家講說 有什麼樣的方法可以做到這件事 但我們在開學的第一堂課就告訴你說
              
                  1:13:49
                  如果你是用訓練資料 fine-tune語言模型 忘了fine-tune完以後模型就壞掉了 所以這個簡單的模型 語言模型的模型 就提供給你一個 類神經網路編輯的可能性 你可以先從這個模型上推論說 假設我要這個模型輸出高雄 那我這輸入的X 要怎麼改呢 我這輸入的X 要加上什麼樣的Delta X 輸出才能輸出是高雄呢 因為這個Linear Function 它只是一個Linear的Transformation 給定一個指定的輸出 你要求反推出 應該要什麼樣的輸入 才能給出指定的輸出 其實是容易的 你有修過線性代數 其實你都知道怎麼做 所以這個不是一個困難的問題 你可以找到一個Delta X 加上這個X之後 那輸出就會變成高雄 但是這個是在模型上找出來的 那模型上的結論 能不能直接用在語言模型上呢 現在把臺北101 is located in輸入給語言模型 把在語言模型的模型上 找出來的這個Delta X
              
                  1:14:54
                  直接輸入給真正的語言模型 直接加到真正的語言模型上面 如果他輸出會變成高雄的話 那這個模型就有用 這個模型上預測的結果 可以幫助我們修改真正的語言模型 模型的輸出 好那這個模型的模型 這個語言模型的模型上面觀察到的結果 有沒有用呢 居然還蠻有用的 在這個圖上每一個點 代表某種類型的關聯 橫軸代表 Fairfulness 的這個數值 有一些這個關聯是做得起來的 有些關聯是做不起來的 縱軸代表剛才那一個 那一招 那個類神經網路編輯就直接在那個 模型的模型上找出來的結論 直接用到語言模型上 到底能不能夠成功的修改呢 這個縱軸是正確率 那你會發現說有蠻多情況 居然是可以直接成功修改的 所以這個模型是一個有用的模型 不過剛才那個模型
              
                  1:15:59
                  比較像是某人腦門一拍 腦洞一開就突然產生出來的模型 那有沒有系統化的方法 幫語言模型 建構語言模型的模型呢 有沒有系統化的方法 幫語言模型建構它的模型呢 是有的 這邊有一系列的研究 那這邊我們就不講它的細節 我們就講它的精神 這一系列建構語言模型的模型的方法 他們的概念是這樣子的 把原來的語言模型 做一個很大的pruning pruning的意思就是 把語言模型裡面的一些component拿掉 拿掉一個神經元,我直接拿掉某一個self-attention,看看模型還能不能妥善運作,那我們要一直pruning,一直pruning,一直pruning,一直拿掉component,直到prune完之後新的模型,那新的模型就是語言模型的模型,prune完之後的新的類神經網路,就是語言模型的模型,直到碰到它一目瞭然為止,直到它變得非常簡單為止,那我們pruning的時候,要確保說我們關心的那個任務, 它的輸入輸出的關係仍然是沒有改變的,本來的語言模型,你問它這個問題,它會有這個答案,問它這個問題會有這個答案,問完之後,我們會做一個非常劇烈的pruning,prune完之後,類神經網路要看起來非常簡單,人類一目瞭然,但是我們關心的那些任務,它的答案仍然是不可以改變的。
              
                  1:17:30
                  好,那這個prune完之後的結果啊,今天在文獻上通常叫做circuit,那這個circuit跟真實的電路沒有什麼太多的關聯 那這個circuit就是語言模型的模型 那這件事情呢,又很像是network compression 那我想蠻多人都知道network compression了 那我們在過去2021的課程也有講過network compression 模型壓縮的方法就把一個比較大的類神經網路 變成一個比較小的類神經網路 那這邊建立模型的模型的方法跟network compression有什麼不一樣呢 方法是非常類似的 比如說都可以用printing來拿掉一些沒用的component,但目標不一樣,我們一般做network compression的時候,你其實希望compress以後的結果,在各個不同的任務上都要逼近原來的模型,原來的語言模型,建構模型的模型這邊呢,我們只關心某個特定任務,比如說只關心knowledge extraction,或甚至在一些更古早的文章裡面,他只關心一個叫做IoI的問題,這個IoI的問題, IoI的問題是什麼呢?
              
                  1:18:40
                  這個非常簡單的問題,這個問題就是 A跟B一起去酒吧 B拿了一杯酒給 然後叫語言模型做接龍 那就要接A嘛,然後他只分析 這個任務,然後他就會發現說 在這個任務上面呢 需要比如說五六個attention 然後他就把模型 就噴噴噴噴噴噴掉 然後多數的component跟這個任務都沒關係 都噴掉,最後只留下五六個attention 你就可以清楚知道說 語言模型在最後回答A的時候 中間經歷了什麼事情 所以這個大家在文獻上 自己再慢慢看 建立語言模型的模型的時候 建立這個circuit的時候 你會希望做非常大量的printing 只要print到人看得懂這個類人間網路在做什麼 那你只關心少數的任務 你只關心非常侷限的任務 他的答案會不會改變 而network compression通常不會print那麼多 你也不在意print完以後的結果 能不能夠一目瞭然 能不能夠被解釋 但是你希望prune完之後的結果 跟碰之前在多數的任務上 在多數的狀況下 這個能力呢
              
                  1:19:43
                  這個模型的能力是不要有太大改變的 那這邊就只是講了一個 建立模型的模型 建立circuit的概念啦 那相關的文獻只能說是汗牛衝動 那我們這邊就不細講 那這邊就是列了幾篇 舉標具有代表性的論文給大家參考 那你看到說這些論文的標題 都有circuit這個字跟電路沒有關係 它是告訴你說它們是怎麼研究語言模型的模型的 好,那最後一個部分要跟大家講 怎麼讓語言模型直接說出它的想法 那在2024年的生成4AI導論裡面 我們也講過說語言模型會說話 所以很多人說這個大型語言模型黑盒子沒有解釋性 不,它是最有解釋性的 它就跟人類一樣 你有什麼問題,你要叫它解釋結果 我問就完事了,你就給他,比如說我叫他做新聞的分類,跟他說新聞就分成這幾類,給你一篇文章,告訴我新聞是哪一類,他可以輕易的告訴我,就是生活類,所以接下來希望他進一步解釋,為什麼知道這篇文章是生活類,比如說你問他,哪幾個關鍵字讓你覺得是生活類呢,他就列出幾個跟天氣有關的關鍵字,說我是因為看到這幾個關鍵字,所以覺得是生活類,但是這樣子的方法還是 也是有他的侷限,他的侷限是什麼呢?你沒辦法真的知道每一個layer在想什麼,而如果你真正直接問我們語言模型說,你是在第幾層類神經網路開始知道這邊新聞是生活類的,我問確GPT-4.5,他其實也會回答你,但他其實回答就是很像是在教科書上抄出來的答案,他說淺層的類神經網路,就是提取初步的字詞跟短語特徵,比如說語法結構等等,中層,我可以
              
                  1:21:33
                  可以辨識出特定主題,然後生成,我就可以知道說這篇文章是生活類 但是語言模型是不是真的是這樣運作的,或他自己知不知道自己是這樣運作的 這個真的是很難說,這比較像從教科書上抄出來的答案 模型相對於人類,他的思維是更加透明的 當你叫一個人解釋他為什麼會做這樣決策的時候 你不知道他心裡是怎麼想的 但語言模型神奇的地方是 他的思維是透明的 你可以直接看到 他每一層是怎麼想的 怎麼說呢 好這邊就跟大家剖析 為什麼語言模型的思維是透明的 我們之前講一個layer的時候 我們都講說一個layer就是輸入一排向量 輸出一排向量 但是其實這只是一個簡化的講法 我們忽略了一個最重要的component 就是residual connection residual connection的意思是說 當一個layer得到一排輸出之後 每一個輸出都還會跟輸入 加起來再得到最終的輸出
              
                  1:22:40
                  所以這個投影片上紅色的向量 才是最終的輸出 紅色向量的輸出都是黃色的向量 加上綠色的向量 而黃色的向量是從綠色的向量產生出來的 雷根會想說 為什麼需要有residual connection 這樣子的設計呢 那這個設計它的起源非常的古老 它是在15年的時候就有了 那個時候世界上 還沒有人類 那個時候呢 是大概前寒武紀的時候 這個15年的時候 人類就 人類那時候還沒有人類 但是那個時候 就已經有residual的connection 那residual connection的出現 是為了要讓很深的network變得更好train 沒有residual connection之前 那時候network都train個二三十層 有了residual connection之後 都可以train個一百多層 我知道你現在覺得train一百多層 也沒有什麼神奇的 但是當年人們都驚呆了 加了這個功能之後 加了這個連結之後 居然就可以train一百多層的network 太神奇了 這個就是residual connection 所以後來在train深的網路的時候 其實都有residual connection這個設計 所以實際上一個transformer
              
                  1:23:47
                  它layer跟layer之間的運作 是要加上這個residual connection的 是要加上這個residual connection的 也就是說一個token進來 它通過一個layer的時候 它除了產生一個output之外 還會把原來的輸入再加起來 通過一個layer產生output的時候 會把原來的輸入再加起來 最後再做unembedded 得到最終的輸出的distribution 你可能覺得看這個圖 沒有什麼神奇的 那我們換一個畫法 左邊的圖跟右邊的圖 是一模一樣的 並沒有真的改變它實際的運作 但是當我們圖變得不一樣的時候 你的想法就變了 從這個左邊的圖看起來 你會覺得說 是輸入做了一個轉換 輸入做了一個轉換 但是當我們把圖換一個畫法的時候 整個transformer 它真正的運作更像是 它有一個叫做 residual string的高速公路 直接把輸入的東西 一路就傳到輸出
              
                  1:24:51
                  而在中間的過程中 每一個layer 都會加一點東西 到輸入裡面 加一點東西到輸入裡面 這個residual string是一個生產線 就一路的被送上去 一路被送上去 只是每過一站都會加上料 加上一些額外的資訊 加上一些額外的資訊 最後得到最終輸出的distribution 所以這才是transformer多個layer 真正運作的機制 好那既然在最後一站 你可以過一個unembedding的module 過一個linear transformation 把這個向量變成一個極限 那前面這幾站 只是最後一站少了一點什麼東西而已 那前面這幾站 能不能也直接加一個unembedding的layer 把它變成token的機率分佈 也就是變成文字的機率分佈呢 這件事是可行的 那這一招是有名字的 現在多數人稱它為logic length
              
                  1:25:56
                  因為這個輸出的distribution 在過softmax之前叫做logic 我們今天是檢查每一層的logic 來看看類神經網路 來看看transformer是怎麼思考的 所以它叫做logic length 那人類什麼時候知道語言模型的思維是透明的呢 其實我們實驗室在2020年年初的時候 就知道語言模型的思維是透明的 那時候就已經發現說 那時候的模型BERT 其實你是可以看到它每一層 在想什麼的 你可以透過Large Lens的方式 解析出每一層的文字內容 這個是高偉聰同學跟那個吳宗漢同學做的 當時有了這個發現以後覺得 這個發現一點用都沒有這樣子 那時候覺得這能幹嘛 所以這篇文章後來甚至是沒有投稿 就直接放在Archive上而已 左邊這個圖是論文裡面的圖 那這個就是告訴我們說
              
                  1:27:00
                  你可以把最後的Unembedding Layer 接到中間的每一層 你就可以看到Bert是怎麼處理一段文字的 那右邊這個表格是一個真實的例子 現在輸入的句子是 It's a bitter sweet and lyric mix of elements 就是這是一種苦樂參半 且抒情元素的混合體 那it是什麼 沒有講這個句子就只有一個代名詞it 那接下來你把這個句子輸入 Bert一個遠古時代的語言模型 然後解析出 它每一層的輸出 你會發現到第11層的時候 it那一個字 變成了elbent 你把embedding這個機制 裝到第11層的時候 它解析出來的不是it 而是elbent 代表在這一層的時候 Bert知道說 it這個東西 它指的可能是某一個專輯 這也符合這個 蠻符合這個句子的敘述 有可以瞭解說Bert其實會把這些代名詞做一些reference 他去猜說這個代名詞實際上指的是什麼樣的實體
              
                  1:28:08
                  好,那後來呢,就有很多人利用這種logic lens的方法來分析語言模型內部是怎麼思考的 這邊引用的是一篇23年的論文 他就想要知道說語言模型是怎麼回答一個問題的呢 他這邊就是要問語言模型一個國家的首都是哪一個城市 因為這是比較舊的模型 他需要做in context learning 他要先跟語言模型說 what is the capital of France answer是Paris 然後what is the capital of Poland answer 然後叫他做文字接龍 看能不能接出華沙這個城市的名稱 那實際上語言模型運作是怎麼樣呢 他就把冒號這個位置對應的representation 每一層都用large lens解析出來 一開始語言模型根本搞不清楚 他應該是哪一個token 但走到第15層的時候 他突然知道那個token應該是Poland 然後從第19層開始 他突然知道應該是要回答華沙 就從第15層開始
              
                  1:29:13
                  他知道說接下來要輸出的東西 應該跟Poland是有關係的 然後到第19層開始 他知道說他要輸出的是華沙這個城市 那左邊是更詳細的分析 那這個縱軸啊 是代表的是那個 在distribution裡面 這個華沙跟Poland這兩個token的機率 那實際上他顯示的不是機率啊 因為機率實際上畫出來可能會非常的小 所以他顯示的是reciprocal rank reciprocal rank是什麼意思呢 就是那一個token 在所有token裡面機率排名的倒數 如果他排第一名數值就是一排第二名 二分之一排第三名就是三分之一 以此類推 就看到說Poland這個詞彙 隨著layer持續的增加 在某一層突然之間 residual street裡面 出現Poland這個字 然後接下來又急遽下降 被華沙所取代 那感覺語言模型先知道說 要回答一個跟Poland有關的東西
              
                  1:30:18
                  最後才鎖定說真正的答案 是華沙 而且呢 如果今天同樣答案是華沙 不同的問法 他的回答他背後運作的機制是不一樣的 我們剛才說如果直接問他這個問題 他會先鎖定說答案是跟波蘭有關 然後再回答華沙 但另外一方面 假設是讓他做閱讀理解測驗 先給他一篇文章 再問他一個問題 就直接問他說 波蘭的首都在哪裡 那前面的文章裡面已經提到波蘭首都是華沙了 然後直接給他answer的話 他就不會產生波蘭這個字 他直接在第十六層就知道 答案是華沙了 又知道說不同的問法 不同的狀況 他背後運作的機制是不一樣的 那透過這種Logic Lens 你就可以去知道語言模型心裡在想些什麼 比如說有人會想說 像LLaMA-2這種模型 他看過的英文資料是遠比中文資料多的 所以他實際上在想事情的時候 他內心深處到底是用哪個語言呢 這篇文章是去年年初的文章
              
                  1:31:24
                  他們就做了一個實驗 他們用LLaMA-2呢 來做翻譯 他們就跟LLaMA-2說 法文的這個詞彙 這個是花的意思了 法文的這個詞彙 對應到中文的哪一個詞彙呢 那LLaMA-2可以正確的接觸 花這個字 但他怎麼知道法文的這個字 翻成中文就是花呢 你如果分析他中間的每一個layer的話 會發現說 他是先把法文的花 翻成英文的花 再把英文的花 翻成中文的花 所以右邊就是用logit lens 分析每一層之後 得到的結果 輸入是中文冒號 然後這個是空格 空格之後就要產生答案了 所以從空格開始 把每一層都用logit lens 解析出來 那最前面幾層 紅色就代表說 透過logit lens解析出來的 那個Distribution 它的Entropy越大 它要輸出的
              
                  1:32:26
                  是英文的Flower 到了27層之後 它才意識到說要把英文的Flower翻譯成 中文的花 所以代表說模型在思考的時候 它內部其實是用英文 在思考的 它是先把法文翻成英文再把英文 翻成中文 雖然你外表看起來再把法文直接翻成中文 在它中間是用英文做媒介 不過這是在LLaMA-2上的實驗啦 LLaMA-3現在中文能力其實蠻強的 所以期待有人去分析LLaMA-3 看看他內心是不是還用英文在思考 好,那我們現在已經有了residual string的概念之後 接下來我們對於每一個layer做的事情 就可以有不同的想像 我們現在知道說每一個layer就是加一點什麼東西進去這個residual string 那他到底加了什麼呢 我們要怎麼解析每一個layer加了什麼樣的東西 一般我們在講神經元的時候 我們都是說把前一個layer的輸出集合起來
              
                  1:33:32
                  做weighted sum變成一個神經元 把前面的layer集合起來 weighted sum變成一個神經元 但你可以反過來看待這件事 反過來看待這件事以後 這個世界就變得不一樣 它的運作是完全一模一樣的 但是反過來看以後 你可以有不同的理解 你可以說前一層的 某一個神經元某一個dimension的數值 乘上weight以後 傳輸給 接下來下一層不同的dimension 前一層的某一個數字 某一個dimension乘上不同的weight以後 傳到下一層去 那這件事情 這樣的一個概念 這只是一個概念 因為他並沒有改變什麼東西 他就概念 是在transformer feed forward layer 這篇paper裡面被提出來的 那篇paper引用非常高 很多人都知道這個概念 就是其實 一個multilayer的perceptron 一個多層的 可以看作是一個
              
                  1:34:35
                  key有key 有value的attention 前一層的這些數值 就是attention的weight 然後後面output的這些數值 就是一個value 不知道大家聽不聽得懂 那如果你暫時一下子沒有辦法 心領神會的話 也沒有關係 用不塞上這個概念 你回去再仔細看一下這篇論文 這篇論文是一個改變 大家對於Feed Forward Network 的想像的論文 好 所以假設 前一層每個Dimension的數值 就是K1 K2到KD 這邊每一個K代表一個Scalar 一個數值 那K2它會接到 下一層的每一個輸出 那我們把K2對應到 下一層每一個輸出的weight 集合起來說它是一個向量 叫V2 KD也對到每一層都有一個weight 我們把這個 叫做VD 那它們是向量V2 VD是向量 所以這個藍色的輸出啊 其實是前一層每一個K
              
                  1:35:41
                  乘上它對應的V再加起來 所以藍色的輸出 是KI乘上VI Summation over I等於1到大D 好,那我們知道說呢,每一層啊,在residual string上面,每一個位置都可以透過logit lens解出一個distribution,解出一個distribution,那這個藍色的向量,它就是加進去以後改變了這個distribution,那這個藍色的向量是由一堆的V做weighted sum以後集合起來的,那我們能不能夠把V這邊的V也做unembedded, 透過logit lens,解析說每一個類神經網路,它想要輸出什麼樣的東西,去加入residual string,影響最終的輸出呢,其實是可以的,每一個加入這個residual string的這些V2,VD,都可以透過unembedded layer,轉成一個token的distribution,它可能也代表了某些特定的意思,真的是這樣子嗎? 一篇22年這個遠古時代的論文
              
                  1:36:54
                  這個遠古時代的論文 那個時候人類只有茹毛飲血 但那時候人類就已經發現說呢 這些V是真的有對應到某一些概念 某一些意思的 比如說有某一個V 這個是第三層的編號1018的V 他就對應到一些單位 有一個V 他是第一層的編號第一個V 他就對應到一些 這個代名詞 第六層的編號3025的V 他就對應到一堆副詞 第十三層編號3516的V 他就對應到一些不同的族群 所以你也可以透過這個logit lens 來解析這些V 他代表了什麼樣的意思 好那知道這件事以後 能夠做什麼呢 知道這件事以後 你就可以對類神經網路 做初步的編輯 之後還會講更多更強悍的編輯的方法 但這邊講一個很基礎的 所謂編輯的方法 假設你問大型語言模型 誰是全世界最帥的人 他通常不回答你
              
                  1:37:57
                  但如果有prompt GPT-4.5很多次 某一次我發現他就說是金城武 所以他可能認為金城武是世界上最帥的人 那如果我把金城武換成李宏毅的話 要怎麼做呢 我們之前講過如果直接train network network是會壞掉的 但是我們剛才已經知道 每一個V就是加一點資訊 到整個residual network裡面 所以你可以分析說 當模型輸入這句話 產生這個答案產生金城武的時候 到底是哪一個V 被加進去了這個residual string 你知道identify出 這一個V之後 把這個V減掉 金城武的token embedding 再加上李鴻一的token embedding 但這邊假設是金城武跟李鴻一都是一個token 減掉金城武的token embedding 加上李鴻一的token embedding 就本來類神經網路 當他啟動某一個K 要加某一個V到residual string的時候 他是為了回答全世界最帥的人 但我們把他的資訊 從金城武置換成李宏毅 他就可以把李宏毅當作答案 這一招有用嗎 這個在21年
              
                  1:39:00
                  輪古時代的時候就已經有人試過了 這一招他有48% 的機率可以改變 類神經網路的輸出 改變不見得答對喔 就輸出變得不一樣 那成功的機率 他真的輸出是李宏毅成功的機率有34% 那個人覺得34%沒有很高 那不是0啊 就代表說這一招是可以 真的拿來編輯類神經網路 改變他的輸出的 好 剛才那個Large Lens的方法呢 有一個致命的缺陷就是 我們透過Unembedding的方法 只能夠把一個Representation 轉成一個Token 所以我們解析出來的結果 都只能是一個Token 另外一方面啦 很多時候語言模型在做的事情是 預測下一個Token 你輸入李宏毅老師中間這個Embedding 並不見得代表李宏毅老師 這個詞彙的含義 它真正代表的是 看到這個輸入以後 模型想要輸出是 這個Token 它想要做 產生是這個Token的時候 所產生的Representation 所以假設你想要知道
              
                  1:40:04
                  李宏毅老師在類神經網路看起來 是什麼意思 你用Logic Length 可能不一定能夠解析出 你要的結果 所以怎麼辦呢 有另外一招 那這個就是去年的論文了 這招叫Patch Scope Patch Scope Patch Scope這一招的意思是說 我們先看看 如果我們跟 語言模型給他這樣的輸入 跟語言模型講說 李奧納多冒號美國演員 臺積電冒號臺灣公司 然後再隨便給他一個東西 這個X可以是任何東西 那他就會輸出 他就會輸出 他對於X的理解 那怎麼知道一個類神經網路 當他看到李宏毅老師 這幾個字的時候 他內心深處的理解是什麼呢 你就把李宏毅老師輸入這個類神經網路裡面 然後看看在某一層他輸出的representation長什麼樣子,接下來把這個representation置換到這一個input string裡面,就這個類似這個語言模型他的輸入是一樣的,這邊甚至一樣就是給他一個x就好了,但是在這個位置把他的embedding,把在這個位置把他的representation換成輸入是這一串文字時候的representation,那對這個類似對這個語言模型來說,他就好像看到 X是李宏毅老師這一串字一樣
              
                  1:41:23
                  然後他就開始繼續輸出 他就有可能告訴你李宏毅老師的身份 那我知道講到這邊你可能會有個困惑就是 那我前面起不是要準備一些例子 那我準備的例子不是會影響最終輸出的結果嗎 沒錯就是你準備的例子 就是會影響最終輸出的結果 不過這篇文章的作者覺得這是一個feature 不是一個bug 你可以調整前面準備的例子 然後模型就會給你不同風格的解釋 比如說如果你現在的輸入是告訴我 X相關的秘密 然後你再把X這個位置的representation 換成李宏毅老師的representation 他可能就回答是個肥仔 你就可以從不同的角度來解析一個representation 那這邊就是引用了剛才就是提出這個 scope patch這個方法原始論文裡面舉的一個例子 他們就把 戴安納他是這個 那個 威爾斯王子的王妃 我這邊似乎少打了一個S
              
                  1:42:28
                  不過沒有關係 那個戴安納他是威爾斯王子的王妃 然後把這個片語 輸入給 類神經網路輸入給語言模型 然後接下來解析 他看到這個片語的 最後一個字的時候 他的每一層 的輸出對 這一個語言模型來講 分別是什麼,所以在前面一二層的時候,如果你把這個位置的representation拿去解析,語言模型輸出的句子是country in the United Kingdom,或者是country in Europe,因為威爾斯也是一個英國裡面的國家的名字,是United Kingdom裡面一個國家的名字,所以語言模型在前面幾層,他只認了威爾斯這個字,所以他就覺得他看到的是一個 但到了第四層的時候,他顯然讀到了Princess of Wales,他讀到了這一整串詞彙,在第四層的時候,他解析是說這是一個給皇室女性的頭銜,然後到第五層的時候,他知道說這個人呢,是威爾斯王子的妻子,然後到第六層,他才讀到戴安娜這個字,然後就輸出戴安娜完整的資訊,所以可以透過這個方法解析一個語言模型,每一層他可以 他看到的,他看到的東西實際上對應的文字是什麼,接下來最後一部分,最後幾頁投影片呢,就是舉一個例子,說剛才那些解析的方法,如何改變了人們對類神經網路背後運作機制的理解,進而提出了新的想法,那這是一篇去年六月的文章,這篇文章想要解析的是,對於一個multi-hop question,語言模型是怎麼回答的,然後解析完之後,他提供了
              
                  1:44:20
                  提出來一個方法,讓語言模型在multi-hop的question上面可以做得更好 那這個multi-hop的question這邊的例子是 the spouse of the performer of imagined is 像這種multi-hop的question裡面呢 會有包含三個entity 第一個entity是會明確出現在問題裡面的 那在這個例子裡面就是imagine 那我們把它叫做E1 那第一個entity imagined是一張專輯的名字 那接下來我們要問的是 The Performer of Imagine 就是彈奏Imagine創造Imagine這張專輯的這個音樂人是誰呢 那這個是E2 那他其實約翰藍儂（John Lennon） 所以這個E2是約翰藍儂 然後接下來呢 約翰藍儂的配偶又是誰呢 那這是E3 約翰藍儂的配偶是小野洋子 就是YOKO 然後模型知道這個答案以後 他就要看到這一串文字 然後輸出YOKO 那接下來的問題是 模型是怎麼做 這一連串的解析的呢 他是怎麼做這種需要
              
                  1:45:24
                  Multi-Hop Reasoning 需要做多步推理的問題的呢 一個直覺的想法是 模型讀到Imagine這個字以後 他根據前面的關係 The Performance of Imagine 先解析出答案是約翰藍儂 然後知道答案是約翰藍儂之後 再經過 The Spouse of約翰龍 這一個片語 解析出最終的答案 是Yoko是小野洋子 那模型真的是這樣運作的嗎 所以他們就他們就用剛才講的那個 那個PatchScope那個方法 做了一下解析 所以他們就把這一個位置的每一層 都拿出來看看 看看會解析出什麼樣的內容 然後如果解析出的內容裡面 有包含約翰藍儂這個字 就把它記錄下來 那得到的結果呢 是藍色的這條線 藍色的這條線橫軸呢 是layer然後縱軸呢 是解析出 這個E2 解析出E2的 第一次出現的layer
              
                  1:46:28
                  所以就會發現說 什麼時候解析出E2呢 什麼時候語言模型可以根據E1解析出E2呢 在蠻前面的layer 就可以根據E1 解析出E2了 好那根據E1解析出E2以後 再來要根據E2解析出E3 那什麼時候解析出 E3呢 他就分析這一個位置 每一個representation 他對應的文字 然後如果有出現 E3的內容的話就把它記錄下來 然後得到的結果呢 是橙色的這條線 所以你會發現說 多數情況都是大概在layer 第20層到第25層間 會解析出 E3這個詞彙 所以你可以感受到說 在比較低的layer 先解析出E2 然後接下來才解析出E3 然後最後就可以給你 正確的答案 然後這篇文章的作者發現說 當有時候這種multi-hop的question 沒有辦法得到正確的答案
              
                  1:47:31
                  是因為E2太晚被解析出來了 因為E3 必須要在第20幾個layer 被解析出來 才有解析出最終答案的能力 如果今天E2太晚被解析出來 過了20層才被解析出來 那接下來在T2這個位置 就來不及解析出E3了 所以怎麼解決這個問題呢 他們有一個神妙的做法 就是把後面幾層的representation 直接加到前面來 再重新跑一次 就結束了 既然今天只有中間某一層 能夠解析出E3 如果E2太晚被解析出來 那怎麼辦呢 把後面的layer放到前面 這樣只有能夠走過第二十層 就可以把E3解析出來了 這招有沒有用呢 這招居然是有用的 他們試了各式各樣不同的模型 那correct代表說 在用這招之前 模型本來就會答對的問題 那本來就會答對
              
                  1:48:33
                  那正確率居然當然是100嘛 然後做完這招以後不會影響正確率 但神奇的地方是 對本來不對的問題 用了這招以後 大概會有40到60%的正確 你可能會覺得40%到60%的正確率 也沒有很高 但不要忘了 這邊40%到60%的問題是 原來全部都答不對的 所以原來是0%的正確率 用了這招以後 原來完全答不對的問題裡面 居然有4到6成 可以因此就答對了 所以這一招 看起來其實又跟reasoning有點像 我們在第一堂課 不是講過reasoning 就是深度不夠 長度來湊嗎 這邊paper是6月的 的時候還沒有reasoning的模型 拿起跟reasoning的模型 也是很像的 reasoning的模型只是把 你的輸出如果來不及解析完 就跑到下一個time step 再重新解析一次 所以這個方法 這邊paper提出來一個叫做backpatching的方法 其實跟reasoning 深度不夠
              
                  1:49:36
                  長度來湊的做法 其實是有異曲同工之妙的 好 那這個就是今天想要跟大家分享的內容 就從一個神經人開始講起 最後講到怎麼讓語言模型 直接輸出他解析的結果
              
            