
### 管道并行

优势：通过重叠计算和通信来减少空闲时间，对于具有多个顺序层的深度模型特别有效。

劣势：在管道的开始和结束时引入管道气泡（空闲时间）。需要仔细调整以平衡各阶段并高效管理管道。

### 张量并行

概念：张量并行是指将单个张量（如权重矩阵）拆分到多个 GPU 上。这种方法可以将大型张量进行分布式并行处理，有效平衡计算负载。

机制：张量沿着一个或多个维度被分割，每个 GPU 持有张量的一个切片。在计算过程中，这些切片上的操作会并行执行，并根据需要合并结果。

优势：可以处理超出单个 GPU 内存容量的极大张量。更均匀地在多个 GPU 之间分配计算负载。

劣势：需要复杂的实现来管理张量切片和相应的操作。组合结果的通信开销可能会影响性能。

### 选择合适的策略：数据并行 vs 模型并行 vs 流水线并行 vs 张量并行

每种并行范式都有其独特的优势，适用于不同的场景：

- 数据并行：对许多模型来说简单有效，但受限于 GPU 内存容量。
- 模型并行：可以训练超大型模型，但需要精细管理 GPU 间的数据流。
- 流水线并行通过重叠计算提高利用率，但增加了流水线平衡的复杂性。
- 张量并行能高效处理超大型张量，但实现复杂且存在通信开销。

选择合适的并行策略取决于模型的具体需求、硬件架构以及期望的性能特征。下图信息图（由 Sebastian Raschka 提供）快速概述了四种不同的多 GPU 训练范式，包括数据并行、模型并行、流水线并行和张量并行。

![](https://aman.ai/primers/ai/assets/multi-gpu-parallelism/multi-gpu-parallelism.jpeg)

## 数据并行

数据并行涉及将数据分割到多个设备（如 GPU）上。每个设备处理不同的数据块，但它们都使用相同的模型进行计算。处理完成后，将结果合并以更新模型。这是通过在反向传播过程中同步梯度来实现的，以确保更新的一致性。

### 数据并行 (DP)

PyTorch 中的数据并行指的是将数据分布在多个 GPU 上以执行并行计算的过程。这种方法通过同时利用多个 GPU 的计算能力来加速训练。在 PyTorch 中实现数据并行的主要机制是通过 `torch.nn.DataParallel` 模块。

以下是关于 `torch.nn.DataParallel` 工作原理的详细解释，以及一个演示其用法的小代码示例。

#### DP 工作原理

1. 模型复制：该模型在多个 GPU 上进行复制，每个副本处理一部分输入数据。
2. 数据分割：输入数据被分割成较小的批次，每个小批次被发送到不同的 GPU 上。
3. 并行计算：每个 GPU 并行处理其部分数据，独立执行前向和后向传递。
4. 梯度聚合：收集并平均所有 GPU 的梯度。
5. 模型更新：使用平均梯度来更新模型参数。

#### 使用 DP 的关键步骤

1. 模型定义：像往常一样定义模型。
2. 使用 DataParallel 包装：用 `torch.nn.DataParallel` 来包装模型。
3. 数据加载：使用 `torch.utils.data.DataLoader` 加载数据集。
4. 前向和后向传播：执行前向和后向传播的方式与单 GPU 相同，但现在计算将分布在多个 GPU 上。

#### 代码示例
```python
import torch 
import torch.nn as nn 
import torch.optim as optim 
from torchvision import datasets, transforms

# Define a simple CNN model 
class SimpleCNN(nn.Module):     
    def __init__(self):         
        super(SimpleCNN, self).__init__()         
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)         
        self.fc2 = nn.Linear(128, 10)          
    
    def forward(self, x):         
        x = nn.functional.relu(self.conv1(x))         
        x = nn.functional.relu(self.conv2(x))         
        x = x.view(-1, 64 * 28 * 28)         
        x = nn.functional.relu(self.fc1(x))         
        x = self.fc2(x)         
        return x  
  
# Check if multiple GPUs are available 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
print(f"Using device: {device}")  

# Instantiate the model 
model = SimpleCNN()  

# Wrap the model with DataParallel 
if torch.cuda.device_count() > 1:     
    print(f"Using {torch.cuda.device_count()} GPUs")     
    model = nn.DataParallel(model)  

# Move the model to the appropriate device 
model.to(device)  
    
# Define a loss function and optimizer 
criterion = nn.CrossEntropyLoss() 
optimizer = optim.Adam(model.parameters(), lr=0.001)  
    
# Data loading and preprocessing 
transform = transforms.Compose([     
    transforms.ToTensor(),     
    transforms.Normalize((0.1307,), (0.3081,)) 
])  
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) 
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)  

# Training loop 
for epoch in range(5):     
    model.train()     
    running_loss = 0.0     
    for i, (inputs, labels) in enumerate(train_loader):         
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()         
        outputs = model(inputs)         
        loss = criterion(outputs, labels)         
        loss.backward()         
        optimizer.step()   
                       
        running_loss += loss.item()         
        if i % 100 == 99:    # Print every 100 mini-batches       
            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')         
            running_loss = 0.0  
print('Training finished.')
```


#### 代码解释

1. 模型定义：定义了一个简单的 CNN 模型。
2. 设备检查：代码检查 CUDA 是否可用并设置相应的设备。
3. 模型包装：如果有多个 GPU 可用，模型会用 `torch.nn.DataParallel` 进行包装。
4. 数据加载：加载并转换 MNIST 数据集。
5. 训练循环：模型以常规方式进行训练，但如果条件允许，计算任务会分配到多个 GPU 上执行。

### 分布式数据并行 (DDP)

通过使用 `torch.nn.DataParallel`，您可以轻松扩展模型训练以利用多个 GPU，从而显著加快训练过程。

PyTorch 中的分布式数据并行（DDP）是一种更先进、更高效的方法，用于在多个GPU（可能跨多个节点）上并行化训练。与使用单一进程管理所有设备的 `torch.nn.DataParallel` 不同，DDP 为每个 GPU 创建一个单独的进程，从而提高了可扩展性并减少了 GPU 间的通信开销。这种方法对于大规模深度学习任务特别有益。

#### DDP 关键特征

1. 独立进程管理：每个 GPU 由一个独立的进程进行管理，这有助于减少 Python 中全局解释器锁（GIL）造成的瓶颈。
2. 梯度同步：通过全归约操作在各个进程间同步梯度，确保所有进程的模型参数保持一致。
3. 可扩展性：相比 `torch.nn.DataParallel`，具有更好的性能和可扩展性，尤其适用于大规模训练

#### 使用 DDP 技术设置

1. 初始化进程组：设置通信后端并初始化进程组。
2. 创建 DDP 模型：用 `torch.nn.parallel.DistributedDataParallel` 封装模型。
3. 设置分布式采样器：使用分布式采样器确保每个进程获取数据集的唯一子集。
4. 配置数据加载器：使用分布式采样器设置数据加载器。
5. 启动训练流程：使用启动器工具生成多个进程进行训练。

#### 代码示例

这是一个基本示例，演示了如何在 PyTorch 中设置和使用 DDP。

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, DistributedSampler
import torch.nn as nn
import torch.optim as optim

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = x.view(-1, 64 * 28 * 28)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train(rank, world_size, epochs):
    setup(rank, world_size)
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    train_loader = DataLoader(dataset=train_dataset, batch_size=64, sampler=train_sampler)
    
    model = SimpleCNN().to(rank)
    model = DDP(model, device_ids=[rank])
    
    criterion = nn.CrossEntropyLoss().to(rank)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(rank), labels.to(rank)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if i % 100 == 99:
                print(f'Rank {rank}, Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    cleanup()

def main():
    world_size = torch.cuda.device_count()
    epochs = 5
    mp.spawn(train, args=(world_size, epochs), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

#### 代码解释

1. 设置与清理：用于初始化和销毁进程组的函数。
2. 模型定义：定义了一个简单的 CNN 模型。
3. 训练函数：
    - 初始化进程组。
    - 创建分布式采样器和数据加载器。
    - 用 `DistributedDataParallel` 包装模型。
    - 运行训练循环。
4. 主要功能：使用 `mp.spawn` 启动多个进程进行训练。

这种设置确保每个 GPU 都有自己的进程和数据子集，并通过跨进程同步梯度，从而实现高效的并行训练。

## 模型并行

模型并行是指将神经网络模型分割到多个设备（如 GPU）上，使每个设备负责模型的一部分。这种方法在模型过大无法装入单个设备内存时尤为有效。通过将模型的不同部分分配到多个 GPU 上，模型并行使得训练超大规模模型成为可能。

模型并行类型：
1. 层间并行
2. 张量间并行
3. 算子级并行

### 层间并行

概念：模型的不同层或层组被分配到不同的 GPU 上。例如，在深度神经网络中，你可能将前几层放在一个 GPU 上，接下来的几层放在另一个 GPU 上，以此类推。

机制：在前向传播过程中，一个 GPU 的输出会被传输到下一个 GPU 进行进一步处理。在反向传播过程中，梯度计算按相反顺序进行，每个 GPU 负责处理分配给它的层的梯度。

优势：适用于顺序模型的简单实现，适用于具有不同层或块的模型。
劣势：如果各层的计算负载不同，可能会导致 GPU 利用率低下，如果各层产生大量输出，通信开销可能会很大。

示例：

```python
import torch 
import torch.nn as nn  

class LayerwiseModel(nn.Module):     
    def __init__(self):         
        super(LayerwiseModel, self).__init__()         
        self.layer1 = nn.Linear(10, 50).to('cuda:0')         
        self.layer2 = nn.Linear(50, 10).to('cuda:1')      
    
    def forward(self, x):         
        x = self.layer1(x)         
        x = x.to('cuda:1')         
        x = self.layer2(x)         
        return x
```



### 张量级并行

概念：单个张量（如权重矩阵或激活值）会被拆分到多个 GPU 上。这种方法涉及沿一个或多个维度对张量进行切片，并将这些切片分配到不同的 GPU 上。

机制：在计算过程中，每个 GPU 并行处理张量的各自部分，中间结果会根据需要合并以完成计算。

优势：通过分发大型张量高效利用 GPU 内存，在多个 GPU 之间更均衡地分配计算负载。

劣势：需要复杂的实现来处理张量切片和组合，组合中间结果时通信开销较高。

示例：张量级并行通常在深度学习框架的较低层级实现，在用户定义的模型代码中可能不会直接体现。它常用于 Megatron-LM 等分布式训练库中，适用于大规模模型训练。

### 算子级并行

概念：层内的不同操作（或操作的部分）被分配到不同的 GPU 上。这种细粒度的并行性涉及将单个操作的计算拆分到多个设备上。

机制：层内操作被分割，每个 GPU 执行部分计算，每个 GPU 的结果被合并以产生操作的最终输出。

优势：通过利用所有可用的 GPU，可以实现高并行效率，适用于可以分解为较小、可并行任务的大型操作。

劣势：实施和管理复杂，通信开销可能很大，特别是对于具有大型中间结果的操作。

示例：与张量级并行类似，算子级并行通常是在深度学习库和框架内实现的，用于优化矩阵乘法等特定操作。

### 总结

模型并行可以分为三种主要类型，每种类型适用于不同的场景，并具有独特的优势和挑战：

- 层间并行：实现简单，适用于层次分明的模型，但可能导致 GPU 利用率不均衡和通信开销增加。
- 张量间并行：通过拆分大型张量高效利用 GPU 内存并平衡计算负载，但需要复杂的切片和通信管理。
- 算子间并行：通过拆分层内操作最大化并行效率，但实现复杂且可能带来显著的通信开销。

选择合适的模型并行类型取决于模型架构、可用硬件以及训练任务的具体要求。先进的深度学习框架和库提供了工具和功能来促进这些并行策略的实施，从而能够训练大型复杂模型。

### 比较分析：模型并行化的类型

#### 比较标准

实施复杂性、GPU 利用率、通信开销​、可扩展性、适用场景


#### 层级并行

实施复杂性：低至中等：将不同的层或层组分配给不同的 GPU 相对简单明了。这种方法对于具有清晰顺序结构的模型尤为简便。

GPU 利用率：变量：如果不同层的计算负载不同，利用率可能会不均匀。某些 GPU 可能在等待其他 GPU 完成任务时利用率不足。

通信开销：高：在 GPU 之间传输大量中间结果可能会产生显著的开销。这可能成为瓶颈，尤其是对于输出较大的深度模型。

可扩展性：中等：可扩展性受限于可有效分配到多个 GPU 上的层数。具有较多层的深度模型可以从中受益，但浅层模型可能无法受益。

适用场景：具有明显层级结构的顺序模型，如前馈神经网络或深度卷积网络。

#### 张量级并行

实现复杂度：高：需要采用先进技术将张量拆分并管理到多个 GPU 上。这涉及沿特定维度对张量进行切片，并确保结果的正确聚合。

GPU利用率：高：能有效地将大型张量分布到多个 GPU 上，实现均衡的利用率。每个 GPU 处理张量的一部分，从而优化内存和计算资源的使用。

通信开销：中等至高：虽然每个 GPU 独立处理其张量切片，但结果必须合并，从而导致通信开销。然而，与逐层并行相比，这种情况通常更易于管理。

可扩展性：高：随着 GPU 数量的增加，扩展性良好，因为可以将大张量分割成更多切片以均匀分配负载。

适用场景：适用于具有极大张量的大型模型，例如具有庞大权重矩阵的语言模型。

#### 算子级并行

实施复杂度：非常高：这种方法实施起来最为复杂。它需要分解层内的各个操作，并将这些子任务分配到多个 GPU 上。需要精细的同步。

GPU利用率：非常高：通过利用操作内部的并行性实现最佳利用率。每个 GPU 可以同时处理同一操作的不同部分，从而最大化效率。

通信开销：高：这种细粒度的并行性会导致 GPU 之间频繁通信，如果不加以谨慎管理，可能会带来显著的开销。

可扩展性：极高：具有高度可扩展性，即使是小型操作也能分配到多个 GPU 上执行，因此非常适合处理具有复杂操作的超大型模型。

适用场景：超大规模复杂模型，例如人工智能和深度学习前沿研究中使用的模型，此时最大化计算效率至关重要。

#### 总结表

|**Criterion**|**Layer-wise Parallelism**|**Tensor-wise Parallelism**|**Operator-wise Parallelism**|
|---|---|---|---|
|Implementation Complexity|Low to Medium|High|Very High|
|GPU Utilization|Variable|High|Very High|
|Communication Overhead|High|Moderate to High|High|
|Scalability|Moderate|High|Very High|
|Suitable Use Cases|Sequential models with distinct layers|Large models with very large tensors|Extremely large and complex models|

### 选择正确的类型

层间并行：最适合具有清晰、顺序层的简单模型，其中实现简便性是优先考虑的因素，且可以容忍通信开销。

张量级并行：适用于具有超大张量的模型，在 GPU 利用率平衡和内存效率至关重要的情况下尤为理想。

算子级并行：适用于对计算效率和可扩展性要求极高、模型最为复杂和苛刻的场景，尽管其实现复杂度较高。

了解这些差异有助于根据神经网络模型的具体需求和限制以及可用硬件，选择最合适的模型并行策略。


## 混合（模型与数据）并行

### 全分片数据并行（FSDP）

完全分片数据并行（FSDP）是PyTorch中的一项功能，通过更高效地利用内存和计算资源来解决数据并行的局限性。FSDP将所有可用的GPU上的模型参数和优化器状态进行分片（即分割），显著降低了内存使用量，使得训练原本无法放入GPU内存的超大模型成为可能。

全分片数据并行（FSDP）主要是一种模型并行形式。然而，它也融入了数据并行的元素。其工作原理如下：

1. 模型并行：FSDP（全称 Fully Sharded Data Parallel）将模型参数分布在多个 GPU 上。这意味着每个 GPU 仅保存整个模型的一个分片（一部分）。这是模型并行的关键特征，通过将模型拆分并分布到不同设备上，更高效地管理内存，从而能够训练无法装入单个设备内存的大型模型。
2. 数据并行：在每个分片内，FSDP 还执行数据并行训练。这意味着数据会在多个 GPU 之间进行分割，每个 GPU 处理不同的数据子集。梯度在每个 GPU 上本地计算，然后在所有 GPU 之间同步，以一致地更新模型参数。

通过结合这两种方法，FSDP 旨在最大限度地提高内存使用和计算资源的效率。这种混合方法使得训练那些仅靠纯数据并行或纯模型并行都无法实现的大型模型成为可能。

#### FSDP 的关键特性

1. 参数分片：每个 GPU 仅保存完整模型参数的一部分，从而降低内存开销。
2. 优化器状态分片：与参数分片类似，优化器状态也在多个 GPU 之间进行分片存储。
3. 梯度分片：在反向传播过程中，梯度数据被分散到不同 GPU 上处理，最大限度减少内存占用。
4. 高效通信：采用集合通信机制在 GPU 间汇总和归约梯度数据。

#### 技术细节

* 初始化：为 GPU 之间的通信初始化进程组。
* 封装模型：使用 `torch.distributed.fsdp.FullyShardedDataParallel` 封装模型。
* 数据加载：配置数据加载器和采样器以在 GPU 之间分发数据。
* 训练循环：训练循环与标准 PyTorch 类似，但增加了内存效率和可扩展性的优势。

#### 代码示例

这是一个简单的示例，展示了如何在 PyTorch 中设置和使用 FSDP。

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import wrap
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, DistributedSampler
import torch.nn as nn
import torch.optim as optim

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = x.view(-1, 64 * 28 * 28)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train(rank, world_size, epochs):
    setup(rank, world_size)
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    train_loader = DataLoader(dataset=train_dataset, batch_size=64, sampler=train_sampler)
    
    model = SimpleCNN().to(rank)
    model = wrap(model)  # Wrap the model with FSDP
    
    model = FSDP(model).to(rank)
    
    criterion = nn.CrossEntropyLoss().to(rank)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(rank), labels.to(rank)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if i % 100 == 99:
                print(f'Rank {rank}, Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    cleanup()

def main():
    world_size = torch.cuda.device_count()
    epochs = 5
    mp.spawn(train, args=(world_size, epochs), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```


#### 代码解释

1. 设置与清理：用于初始化和销毁分布式训练进程组的函数。
2. 模型定义：一个简单的 CNN 模型。
3. 训练功能：初始化进程组，设置分布式采样器和数据加载器，使用 FSDP 包装模型以实现高效分片，运行训练循环。
4. 主要功能：使用 `mp.spawn` 启动多个进程进行分布式训练。

#### FSDP 好处

* 内存效率：通过分片参数和优化器状态，FSDP 显著降低了内存开销，从而能够训练更大的模型。
* 可扩展性：高效的通信和分片机制能够在最小化开销的情况下扩展到多个 GPU 和节点。
* 易用性：与 PyTorch 现有 API 和工作流程无缝集成，对现有代码只需极少的改动。

总体而言，FSDP 是一种强大的工具，可高效扩展深度学习模型至多 GPU，同时带来内存和计算上的优势。

## 张量并行

### 概念

张量并行是一种用于训练深度学习模型的技术，它通过将处理大型张量（多维数字数组）的计算负载分配到多个设备（如 GPU）上来实现。这种技术能够高效训练那些因规模过大而无法放入单个设备内存的超大型模型。通过跨设备分割张量，张量并行技术不仅加快了计算速度，还提高了硬件资源的利用率。

### 机制

张量并行机制的运作原理是将大型张量沿特定维度分割，并将这些数据块分配到多个 GPU 上。每个 GPU 对其分配的数据子集进行计算，随后将各计算结果整合形成最终输出。这种并行方式可在模型的不同阶段实施，如前向传播、反向传播或梯度更新阶段均可应用。

### 张量并行的类型

1. 模型并行：这种方法将模型本身分割到不同的设备上。每个设备处理模型的不同部分。通常与张量并行结合使用，以提高训练效率。
2. 数据并行：在这种方法中，整个模型被复制到不同的设备上，每个设备处理不同的数据小批量。然后对结果进行平均或合并。
3. 流水线并行：模型被划分为连续的阶段，每个阶段分配到不同的设备上。数据以流水线的方式通过这些阶段传递。
4. 张量切片并行：专门针对沿特定维度对张量进行切片，并将切片分布到多个设备上。这可以在神经网络的各层内完成。

### 优劣势

**优势：**

* 可扩展性：通过利用多设备的组合内存，支持训练更大的模型。
* 效率：通过并行化操作提高计算效率。
* 灵活性：可与其他并行策略（如数据并行和流水线并行）结合，以实现最佳性能。

**劣势：**

* 复杂性：增加了模型实现和调试的复杂性。
* 通信开销：设备之间需要高效的通信，这可能成为瓶颈。
* 资源管理：需要仔细管理资源和同步。

### PyTorch 中的实现

在 PyTorch 中实现张量并行涉及以下步骤：

1. 模型分区：将模型层或张量拆分到不同设备上。
2. 数据分发：将输入数据分配到这些设备上。
3. 计算：在每个设备上执行必要的计算。
4. 同步：合并来自不同设备的结果。

PyTorch 提供了多种实用工具和函数来支持张量并行计算，例如：

- `torch.distributed`：包含分布式计算功能的软件包。
- `torch.nn.parallel.DistributedDataParallel`：封装模型以实现多 GPU 并行。
- RPC 框架：支持远程过程调用和管理分布式模型组件。

示例：

```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Initialize the process group
dist.init_process_group(backend='nccl')

# Create model and move to GPU
model = MyModel().to(device)
model = DDP(model)

# Define optimizer and loss function
optimizer = torch.optim.Adam(model.parameters())
criterion = torch.nn.CrossEntropyLoss()

# Training loop
for epoch in range(num_epochs):
    for data, target in dataloader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

### 结论

张量并行是一种强大的技术，通过将张量计算分布到多个设备上来训练大规模深度学习模型。虽然它引入了一定的复杂性并需要高效的通信机制，但能够实现在单台设备上无法完成的模型训练。将张量并行与其他形式的并行相结合，可以进一步优化性能和资源利用率。

## 管道并行

### 概念

深度学习中的流水线并行技术指的是将训练深度学习模型的工作负载分配到多个设备或处理单元上的方法。与数据并行不同（数据并行中，模型的不同副本同时处理不同的数据批次），流水线并行将模型本身分割成不同的阶段，并将每个阶段分配到不同的设备上。这使得模型的不同部分可以在不同设备上同时执行，从而提高训练效率并减少训练时间。

### 机制

流水线并行机制的核心在于将模型划分为多个连续的阶段。每个阶段包含模型的一部分层，这些层被分配到不同的设备上。训练过程中，一个微批次的数据首先在第一个设备的第一阶段进行处理，随后将中间结果（激活值）传递给第二个设备的下一阶段，依此类推。当第一个微批次在流水线中推进时，后续微批次可以开始在较早的阶段进行处理，从而实现不同微批次的重叠执行。

机制中的关键步骤：

1. 模型分区：将模型分割成连续的阶段。
2. 设备分配：将每个阶段分配到不同的设备上。
3. 前向传播：按顺序通过流水线阶段处理小批量数据。
4. 反向传播：通过流水线阶段按相反顺序执行梯度计算和反向传播。


### 管道并行的类型

1. 1F1B（一前一后）：每个设备在正向和反向传递之间交替进行，每次每个设备只处理一个小批量。
2. GPipe：引入微批次技术，将小批次进一步划分为更小的微批次，通过同时处理不同的微批次来保持所有设备的忙碌状态。
3. 交错式流水线并行：通过交错多个流水线副本，结合流水线并行与数据并行，从而实现更高效的资源利用。

### 优劣势

**优势：**

* 高效资源利用：通过拆分模型，所有设备都能保持忙碌状态，减少闲置时间。
- 可扩展性：可以训练无法装入单个设备内存的更大模型。
- 缩短训练时间：模型不同部分的并行执行可以加快训练速度。

**劣势**：

* 复杂性：实现流水线并行需要对模型进行仔细的分区设计，并确保设备间的同步。
* 通信开销：在设备间传输中间激活值可能带来显著的通信开销。
* 延迟：流水线的串行特性可能导致延迟，尤其在计算负载在各阶段不均衡时更为明显。

### PyTorch 中的实现

PyTorch 通过 `torch.distributed.pipeline.sync` 包提供对管道并行的支持。以下是实现方法的高层次概述：

1. **分区模型**

```python
from torch.distributed.pipeline.sync import Pipe

# Assume 'model' is the original large model
# Split the model into two stages
model = nn.Sequential(...)
model = nn.Sequential(
    nn.Sequential(*model[:len(model)//2]),
    nn.Sequential(*model[len(model)//2:])
)

# Wrap the model with Pipe
model = Pipe(model, chunks=8)
```

2. 设置设备

```python
# Assume we have 2 GPUs
devices = [torch.device('cuda:0'), torch.device('cuda:1')]
model = model.to(devices)
```

3. 训练循环

```python
for input, target in data_loader:
    output = model(input)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
```

### 结论

流水线并行是一种提高深度学习模型训练效率和可扩展性的强大技术。通过将模型分割并分布在多个设备上，它实现了并发处理和更好的资源利用率。然而，这也带来了实现复杂性和通信开销。像PyTorch这样的工具使得流水线并行的实现更加容易，让研究人员和工程师能够更高效地训练更大的模型。

### 总结

流水线并行通过将深度学习模型划分为多个阶段，并将这些阶段分布到不同的设备上进行训练。它允许模型的不同部分并发执行，从而提高训练效率和可扩展性。流水线并行有多种类型，如1F1B和GPipe，每种类型都有其独特的优势和权衡。尽管它在资源利用和训练速度方面带来了显著的好处，但也带来了与复杂性和通信开销相关的挑战。PyTorch提供了工具来简化流水线并行的实现，使其适用于大规模模型训练。

### DeepSpeed

DeepSpeed 是微软开发的开源深度学习优化库，作为 PyTorch 的扩展。它旨在通过提供最先进的内存、计算和分布式训练优化技术，高效训练大规模模型。DeepSpeed  结合了多种技术和工具来实现这些目标，包括内存优化、并行策略和高效内核实现。

DeepSpeed 提供了数据并行和模型并行两种方式，特别专注于优化和扩展大规模模型的训练。以下是每种方式的实现方法：

数据并行：DeepSpeed​ 提供标准的数据并行方式，将数据集分割到多个 GPU 或节点上。每个 GPU 处理不同的数据子集，并通过同步梯度确保模型更新的一致性。

张量与流水线并行：DeepSpeed 支持张量切片或流水线并行，将模型的不同层或部分分配到不同的 GPU 上。

#### DeepSpeed 关键特性

1. ZeRO（零冗余优化器）：DeepSpeed 引入 ZeRO 技术，通过将模型状态在数据并行进程间进行分区来优化内存使用。ZeRO 具有不同阶段，可逐步降低内存消耗：
    - ZeRO 阶段 1：分割优化器状态。
    - ZeRO 阶段 2：分割梯度。
    - ZeRO 阶段 3：分割模型参数。
2. 内存优化：采用激活检查点、梯度累积以及卸载至 CPU/NVMe 等技术，以管理和减少内存占用。
3. 混合精度训练：支持 FP16 和 BF16 训练，以利用硬件加速并减少内存使用。
4. 高效内核实现：针对各类运算优化的内核，加速训练过程。
5. 集成简便：通过简单的 API，只需对现有 PyTorch 代码库进行最小改动即可集成DeepSpeed。

#### 技术细节

DeepSpeed 的架构围绕 ZeRO 优化框架构建，该框架专注于内存效率和可扩展性。它将训练过程分解为可管理的部分，并将这些部分分配到多个 GPU 上，从而降低每个 GPU 的内存负载，并支持训练非常大的模型。

#### 示例代码

以下是一个基本示例，展示了如何在 PyTorch 训练脚本中设置和使用 DeepSpeed。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import deepspeed
from torch.utils.data import DataLoader

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = x.view(-1, 64 * 28 * 28)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def main():
    # Define transformations and dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
    
    # Initialize model
    model = SimpleCNN()

    # Define optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # DeepSpeed configuration
    deepspeed_config = {
        "train_batch_size": 64,
        "gradient_accumulation_steps": 1,
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 0.001,
                "betas": [0.9, 0.999],
                "eps": 1e-8
            }
        },
        "fp16": {
            "enabled": True
        },
        "zero_optimization": {
            "stage": 1
        }
    }
    
    # Initialize DeepSpeed
    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=deepspeed_config)
    
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(5):
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(model.device), labels.to(model.device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            model.backward(loss)
            model.step()
            
            running_loss += loss.item()
            if i % 100 == 99:
                print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    print('Training finished.')

if __name__ == "__main__":
    main()
```

#### 代码解释

1. 模型定义：定义了一个简单的 CNN 模型。
2. 数据加载：加载并转换 MNIST 数据集。
3. 优化器定义：定义了一个 Adam 优化器。
4. DeepSpeed 配置：定义了 DeepSpeed 的配置字典，指定了批处理大小、优化器设置、混合精度（fp16）以及 ZeRO 优化阶段。
5. 初始化 DeepSpeed：使用 DeepSpeed 初始化模型和优化器，DeepSpeed 封装模型以处理优化。
6. 训练循环：训练循环的运行方式与标准 PyTorch 类似，但使用 `model.backward(loss)` 和 `model.step()` 来利用 DeepSpeed 的优化功能。

#### DeepSpeed 的优势

* 内存效率：显著降低内存使用量，支持训练更大规模的模型。
- 可扩展性：高效支持跨多 GPU 和多节点的训练扩展。
- 性能表现：通过优化内核和混合精度训练实现性能提升。
- 易用性：与 PyTorch 无缝集成，仅需极少代码改动。
- 总体而言，DeepSpeed 是一款高效扩展深度学习模型的强大工具，通过先进的内存优化和性能增强技术，助力大规模模型训练。

### DeepSpeed ZeRO

DeepSpeed ZeRO（零冗余优化器）是一种高度优化且内存高效的大规模深度学习模型训练方法。ZeRO 通过将模型状态（即优化器状态、梯度和模型参数）在数据并行进程间进行分区，显著降低了内存占用。这种方法使得训练远超单个 GPU 内存容量的超大规模模型成为可能。

ZeRO 同时提供数据并行和模型并行两种方式，尤其专注于优化和扩展大型模型的训练。以下是每种方式的实现方法：

ZeRO 中的数据并行：ZeRO 通过最小化内存冗余来增强数据并行性。它将优化器状态、梯度和参数分布在多个 GPU 上，从而减少内存使用量，并支持训练更大的模型。这被称为 ZeRO 第 1 阶段和第 2 阶段的优化。

ZeRO 中的模型并行：ZeRO 在其 ZeRO Stage 3 优化中更有效地实现了模型并行。在这一阶段，它将模型状态的所有元素（包括优化器状态、梯度和参数）进一步切分到所有 GPU 上，从而将模型并行与数据并行结合起来。这样可以更高效地分配内存和计算负载。

#### DeepSpeed ZeRO 关键特性

1. 优化器状态分区（ZeRO 第一阶段）：将优化器状态分散到多个 GPU 上，减少内存冗余。
2. 梯度分区（ZeRO 第二阶段）：将梯度分散到多个 GPU 上，进一步降低内存使用量。
3. 参数分区（ZeRO 第三阶段）：将模型参数分散到多个 GPU 上，使得训练尽可能大的模型成为可能。

#### 技术细节

* 阶段 1（优化器状态分片）：优化器状态（如 Adam 中的动量和方差）在所有数据并行进程间进行划分。
* 阶段 2（梯度分片）：除了分片优化器状态外，梯度也被分区，减少了反向传播期间的内存需求。
* 阶段 3（参数分片）：参数在进程间分片，每个进程仅持有模型参数的一部分。在前向和反向传播过程中，参数会被收集并重新分发。

#### 优势

- 内存效率：显著降低内存开销，支持训练更大规模的模型。
- 可扩展性：高效扩展到多个 GPU 和节点。
- 性能：通过高效的通信和计算策略，保持高训练性能。

#### 示例代码

以下是一个展示如何使用 DeepSpeed 与 ZeRO 优化的代码示例。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import deepspeed
from torch.utils.data import DataLoader

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = nn.functional.relu(self.conv1(x))
        x = nn.functional.relu(self.conv2(x))
        x = x.view(-1, 64 * 28 * 28)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def main():
    # Define transformations and dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
    
    # Initialize model
    model = SimpleCNN()

    # Define optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # DeepSpeed configuration
    deepspeed_config = {
        "train_batch_size": 64,
        "gradient_accumulation_steps": 1,
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": 0.001,
                "betas": [0.9, 0.999],
                "eps": 1e-8
            }
        },
        "fp16": {
            "enabled": True
        },
        "zero_optimization": {
            "stage": 2,  # Use ZeRO Stage 2 for gradient sharding
            "allgather_bucket_size": 5e8,
            "reduce_bucket_size": 5e8
        }
    }
    
    # Initialize DeepSpeed
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=deepspeed_config)
    
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model_engine.train()
    for epoch in range(5):
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(model_engine.local_rank), labels.to(model_engine.local_rank)
            
            optimizer.zero_grad()
            outputs = model_engine(inputs)
            loss = criterion(outputs, labels)
            model_engine.backward(loss)
            model_engine.step()
            
            running_loss += loss.item()
            if i % 100 == 99:
                print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    print('Training finished.')

if __name__ == "__main__":
    main()
```

#### 代码解释

1. 模型定义：定义了一个简单的 CNN 模型。
2. 数据加载：加载并转换 MNIST 数据集。
3. 优化器定义：定义了一个 Adam 优化器。
4. DeepSpeed 配置：定义了 DeepSpeed 的配置字典，指定了批大小、优化器设置、混合精度（fp16）以及 ZeRO 优化阶段。
5. 初始化 DeepSpeed：使用 DeepSpeed 初始化模型和优化器，DeepSpeed 封装模型以处理优化。
6. 训练循环：训练循环的运行方式与标准 PyTorch 类似，但使用 `model_engine.backward(loss)` 和 `model_engine.step()` 来利用DeepSpeed的优化功能。

#### ZeRO 阶段比较

- ZeRO 第一阶段：优化器状态分区，适合中等程度的内存节省。
- ZeRO 第二阶段：增加梯度分区，进一步降低内存使用。
- ZeRO 第三阶段：完整参数分区，支持训练最大规模的模型。
- DeepSpeed ZeRO 是一种高效扩展深度学习模型的强大工具，提供先进的内存优化和性能提升，以促进大规模模型训练。

## 对比分析：DP、DDP、FSDP、DeepSpeed 和 DeepSpeed ZeRO

以下是数据并行（DP）、分布式数据并行（DDP）、全分片数据并行（FSDP）、 DeepSpeed 以及 DeepSpeed ZeRO 的对比分析，重点展示它们之间的差异、适用场景和关键特性：

### 数据并行 (DP)

#### 概览

- 机制：将输入数据分配到多个 GPU 上，并在每个 GPU 上复制模型。
- 同步：每次反向传播后，梯度会在所有 GPU 之间进行平均。
- 可扩展性：由于单进程瓶颈和高通信开销，可扩展性有限。
- 易用性：使用 `torch.nn.DataParallel` 实现起来很简单。

#### 优势

易于设置和使用。适用于中小型模型和数据集。

#### 劣势

内存使用效率低下，因为每个 GPU 都保存了模型的完整副本。对于大型模型或大规模分布式训练来说不具备可扩展性。

#### 示例代码

```python
import torch.nn as nn
import torch

model = nn.DataParallel(model)  # Wrap the model for Data Parallel
model = model.to(device)
```

### 分布式数据并行 (DDP)

#### 概述

* 机制：每个 GPU 运行一个独立的进程，拥有自己的模型副本。
* 同步：使用 all-reduce 操作在 GPU 之间同步梯度。
* 可扩展性：比 DP 更具扩展性，通过使用多进程避免了 GIL 瓶颈。
* 易用性：设置比 DP 更复杂，但提供了更好的性能。

#### 优势

相比 DP，具有更好的可扩展性和性能。GPU 利用率更高。

#### 劣势

由于需要管理进程，实现起来稍微复杂一些。仍然要求每个 GPU 都保存模型的完整副本。

#### 示例代码

```python
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

# Initialize the process group
dist.init_process_group("nccl", rank=rank, world_size=world_size)

model = DDP(model.to(rank), device_ids=[rank])
```


### Fully Sharded Data Parallel (FSDP)

#### 概述

* 机制：将模型参数、梯度和优化器状态分散到多个 GPU 上。
* 同步：在前向和反向传播过程中根据需要收集和归约参数。
* 可扩展性：高度可扩展，适用于非常大的模型。
* 易用性：需要更复杂的设置和对模型分片的理解。

#### 优势

通过分片模型状态显著降低内存开销。支持训练传统 DP 或 DDP 无法容纳的超大模型。

#### 劣势

设置和调试更为复杂。根据模型大小和分片粒度，可能会引入额外的通信开销。

#### 示例代码

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = FSDP(model)
```

### DeepSpeed

#### 概览

* 机制：结合多种优化技术，包括跨 GPU 分片模型状态的 ZeRO（零冗余优化器）。
* 同步：采用先进技术优化通信和内存使用。
* 可扩展性：高度可扩展，专为训练万亿参数模型设计。
* 易用性：需与 DeepSpeed 库集成并配置。

#### 优势

先进的内存和性能优化。支持超大型模型，高效利用内存和计算资源。

#### 劣势

与基本 DP 或 DDP 相比，集成更为复杂。需要仔细调整和配置以获得最佳性能。

#### 示例代码

```python
import deepspeed

deepspeed_config = {
    "train_batch_size": 64,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.001
        }
    },
    "fp16": {
        "enabled": True
    },
    "zero_optimization": {
        "stage": 2  # Use ZeRO Stage 2 for gradient sharding
    }
}

model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=deepspeed_config)
```

### DeepSpeed ZeRO

#### 概述

* 机制：作为 DeepSpeed 的一部分，特别专注于零冗余优化器（Zero Redundancy Optimizer），将模型状态（优化器状态、梯度、参数）分区到多个GPU上。
* 同步：在前向和反向传播过程中，采用高效的通信策略来收集和归约必要的状态。
* 可扩展性：高度可扩展，专为训练极大模型而设计，内存占用极低。
* 易用性：设置较为复杂，但能带来显著的内存和性能优势。

#### 优势

* 通过将模型状态分区到多个 GPU 上，优化内存使用。
* 支持训练原本无法放入内存的超大模型。
* 提供三个优化阶段，实现不同程度的内存节省。

#### 劣势

配置和调优更为复杂。需要理解 ZeRO 阶段才能实现最佳使用。

#### 示例代码

```python
import deepspeed

deepspeed_config = {
    "train_batch_size": 64,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.001,
            "betas": [0.9, 0.999],
            "eps": 1e-8
        }
    },
    "fp16": {
        "enabled": True
    },
    "zero_optimization": {
        "stage": 2,  # Use ZeRO Stage 2 for gradient sharding
        "allgather_bucket_size": 5e8,
        "reduce_bucket_size": 5e8
    }
}

model_engine, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=deepspeed_config)
```

### 比较总结

|**Feature**|**Data Parallel (DP)**|**Distributed Data Parallel (DDP)**|**Fully Sharded Data Parallel (FSDP)**|**DeepSpeed**|**DeepSpeed ZeRO**|
|---|---|---|---|---|---|
|Model Replicas|Full model on each GPU|Full model on each GPU|Sharded model across GPUs|Sharded model across GPUs|Sharded model states|
|Memory Usage|High (full model on each GPU)|High (full model on each GPU)|Low (sharded model)|Low (sharded model and states)|Very Low (sharded states)|
|Gradient Synchronization|Averaging across GPUs|All-reduce across processes|Gather and reduce as needed|Advanced optimizations|Efficient sharding strategies|
|Scalability|Limited|Moderate|High|Very High|Very High|
|Ease of Use|Simple|Moderate|Complex|Complex|Complex|
|Performance|Moderate|High|High|Very High|Very High|

### 用例

* 数据并行（DP）：适用于中小型模型和数据集，优先考虑易用性时使用。
- 分布式数据并行（DDP）：在需要比 DP 更好的可扩展性和效率的场景中更受青睐。
- 全分片数据并行（FSDP）：非常适合使用传统方法无法放入内存的超大型模型训练。
- DeepSpeed：适用于最先进的大规模模型训练，提供内存和性能方面的最优化方案。
- DeepSpeed ZeRO：专门针对内存使用优化，适合超大型模型和资源受限的环境。
- 总体而言，选择合适的并行化策略取决于模型的具体需求、可用硬件以及期望的可扩展性。

## 进一步阅读

### [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview)

本书探讨了如何将大语言模型（LLM）的训练规模从单个GPU扩展到数千个GPU，通过分析在多达512个GPU上进行的4000多次实验，提炼出实用的理论、代码和基准测试，内容涵盖内存分析、激活重计算、梯度累积、ZeRO/FSDP技术、数据/张量/流水线/上下文/专家并行、通信与计算重叠、内核融合/FlashAttention技术、混合/FP8精度训练，以及如何选择高吞吐量且内存适配的配置策略。

### [How to Scale Your Model](https://jax-ml.github.io/scaling-book/)

本书探讨了如何通过分析计算、内存和通信限制，选择并行策略，以及理解硬件特性，在 TPU 和 GPU 上高效扩展 Transformer 模型的规模，从而优化大规模训练和推理过程。

### [Making Deep Learning Go Brrrr from First Principles](https://horace.io/brrr_intro.html)

这篇博客文章探讨了如何从计算、内存带宽和开销等基本原理出发进行推理，从而指导有效的优化，使深度学习模型高效运行，而不是依赖临时的性能技巧。

