
### 1、概览

**训练目标：**

深度生成模型试图解决的问题——它利用少量样本，对新数据做出最佳预测估计。而我们要做的是，通过某种方式训练模型，使其学会匹配数据的真实分布，但问题是我们没有足够的样本来理解真实分布，所以我们最终只能得到一个近似分布，然后从这个近似分布中采样。用数学术语来说，这个真实分布用 $P_{\text{data}}(X)$ 表示，而预测分布用 $P_\phi(X)$ 表示。本质上，我们试图做的就是让 $P_\phi(X)$ 与 $P_{\text{data}}(X)$ 相匹配。这正是深度生成模型训练的目标。

------

**深度生成模型（DGMs）**

深度生成模型（DGMs）的输入是从未知且复杂的数据分布中提取的大量现实世界样本，它们输出的是一个训练好的神经网络，该网络参数化了一个近似分布。

DGMs 两大目标：

1. 始终追求生成逼真的内容（真实性生成）
2. 实现可控的生成可控性生成（可控性生成）


**输入：** 假设样本是从一个潜在的、我们并不了解的复杂数据分布中独立同分布抽取而来的。

DGMs 的主要目标是从有限数据集中学习概率分布，它使用深度神经网络对模型分布进行参数化，其中 $\phi$ 代表网络的可训练参数。训练目标是找到最优参数 $\phi^*$，使模型分布 $P_\phi(X)$ 与真实分布 $P_{\text{data}}(X)$ 之间的差异最小化。

该模型通常被称为生成模型，因为一旦模型训练完成，我们就能从这个分布中采样，生成源源不断的新样本，这些样本都符合真实数据分布。

--------

**KL 散度** 

$$D_{\mathrm{KL}}(P|Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}  $$

参数 $φ$ 是通过最小化 $P_{\text{data}}$ 与 $P_\phi$ 之间的差异来学习的。如何量化这两个概率分布之间的差异？KL 散度。

$$\text { Penalty }(x)=P(x) \cdot \log \frac{P(x)}{Q(x)}$$
$P(x)$ 衡量了结果 $x$ 的重要性（因为它发生的频率如此之高）。

形式化理解：$P(x)$ 是真实分布；$Q(x)$ 是预测分布；最直接的方法，求绝对值，绝对差值是个不错的起点，但它并不能涵盖所有情况。例如，如果我们计算绝对差值并将所有这些绝对差值相加，当样本范围内包含大量数据点时，总和可能会变得非常大。

因此，我们需要一种能够捕捉这种差异的度量方法，并且在数据点数量巨大的情况下也不会失控。而通过这两个分布之间的比率，可以很好地实现这一点。但我们实际取比率的对数，其结果还需要乘以 $p(x)$，即数据在真实分布下的重要性（概率）。

举个例子，比如 $P(A)=0.9, Q(A)=0.1$，在比较重要的样本点上，预测的很离谱，则惩罚很大，1.98；而反之，如果 $P(B)=0.1, Q(B)=0.9$，在不重要的样本点上，预测很离谱，-0.21，惩罚不大，而如果精确相等，则惩罚为 0。

---

**模型预测分布与基本设置：**

$P_\phi$ 表示的预测分布必须是一个概率分布，因此，它必须满足两个基本属性：

1. 非负性，即对于所有 $x$，$P_{\phi}(x)> 0$。
2. 整个定义域上的积分应该等于 1。

*如何确保非负性？* 只需对神经网络的原始输出应用一个正函数即可，例如指数函数；

*如何确保积分为 1？* 归一化？

*然而最大的挑战在于这个归一化常数，也称为配分函数，它是难以处理且无法计算的*。因为我们甚至不知道有多少样本，这看起来是一项艰巨的任务。因此，在复杂场景下这是不可能计算的。这种难以处理的问题是推动不同深度生成模型家族发展的核心原因。

----

**不同类型的深度生成模型：**

*基于能量的模型*：其基本原理是将概率转换为能量值，给更有可能的数据点分配较低的能量，而给不太可能的数据点分配较高的能量。

想象一下，我们收集了 1000 名学生在某门课程中的考试成绩，学生的分数并不是在 0 到 100 之间均匀分布的，数据是有一定形状的。例如，多数学生的分数集中在 70 分左右，少数在 40 分左右，还有少数在 90 分左右。

基于能量的模型并不直接分配概率，而是为每一个可能的得分赋予一个能量值。能量表示模型认为该分数有多么不寻常。能量越低意味着它更典型，能量越高意味着它不那么典型。例如，80 分概率高，则 80 分获得低能量。在基于能量的模型中，首先会预测所有得分的能量分布，能量分布可以很容易地转换为概率分布，方法是对能量分布取指数。

*自回归模型*：目标是预测下一个 token。而下一个部分取决于到目前为止生成的所有内容。

*变分自编码器*：编码器基本上将输入压缩成一个小的潜在嵌入。输入被压缩后，潜在空间会捕捉数据中的隐藏结构。这就像一种压缩数据的方式，然后从潜在空间，数据通过解码器将潜在嵌入转换回一个看起来与输入非常相似的样本。这与传统的自动编码器非常不同，因为潜在空间是概率性的。它学习的是一个分布，而不是单一的值。

*归一化流*：想象一下，有两种形状，比如一个完美的圆球和一张皱巴巴的纸。归一化流是一种方法，它学习如何缓慢而平滑地拉伸、扭曲和弯曲简单形状，使其在不撕裂或粘合任何部分的情况下精确转变为复杂形状。比如从一些噪声开始，如高斯分布，然后应用不同的操作，比如拉伸、扭曲，这几乎就像一个流体元素。从一些简单的分布开始，然后扭曲它、弯曲它、转动它，以达到真实分布，这正是深度生成模型试图预测的。

*生成对抗网络*，由两个网络组成，一个生成器和一个判别器，它们相互竞争。

--------

### 2、VAE

**基本原理：**

简单例子，假设你收集了班上所有同学的手写样本。比如这些学生都写了 “hello”，我们假设有 100 名学生，他们都在纸上写了 “hello”，每个人书写风格不同。假设现在要求造一台机器，它能够生成 “hello” 这个词的手写样本，并且这些样本要与班上学生的手写风格相匹配。实际上，我们希望的是预测班级学生手写风格的概率分布，这是 DGM 的主要目标之一。

因此，直觉的想法是：决定手写风格的隐藏因素是什么？每个学生的手写风格取决于许多 *隐藏特征*，例如书写的压力、是否倾斜、字母宽窄、书写速度、字迹整洁度等等，如果能捕捉到所有这些特征，并编写一个函数，将这些特征作为输入，然后输出笔迹。然而，这些特征在最终的图像中是看不到的，但它们确实影响了字母的形状。

**解码器**：

潜在变量 $z$，潜在空间的维度通常比实际数据分布要低，潜在空间相对于真实数据分布是压缩的。解码器通常是一个神经网络，根据样本在潜在空间中的位置，生成的数据也会有所不同。为了避免解码器输出确定性结果，要求它输出每个像素点概率分布的均值和标准差，例如对于 28x28 的图像，就有 $\mu_{1},\sigma_{1}, \cdots \mu_{784},\sigma_{784}$。我们使用神经网络将潜在空间变量 $z$ 作为输入，并在真实空间中生成输出。*解码器预测的是像素概率，而不是最终的图像*。一旦获得这个分布，我们就可以从中采样以得到各种输出结果。

**编码器**：

这种从真实空间到潜在空间的压缩是如何发生的？一种方法是访问潜在空间中的所有可能点，看看哪些图像与目标图像最接近，显然这不是一个好的解决方案，因为它完全难以处理。

所以我们希望潜在空间中的位置实际上是有意义的，并且对应着某些信息。为什么它被称为变分自编码器？相比较纯自编码器中，变分自编码器所做的不是给出一个点作为输出，而是给出一个区域，这个区域是找到目标图像的最可能区域，这就是“变分”一词的由来。

**VAE**：

观测变量 $x$，对应我们所见的数据以及潜变量 $z$，用于捕捉隐藏的变化因素。

* 解码器输出的概率分布：$p_{\phi}(x|z)$
* 编码器输出的概率分布：$q_{\theta}(z|x)$

### 3、VAE 的训练

VAE 设置的主要目标是最终图像应尽可能接近原始图像。这里有两个神经网络，前者负责学习编码；后者负责学习解码。我们需要以某种方式训练这些神经网络的参数来优化某个目标。所以目标函数是什么？

我们最初的目标是希望我们的概率分布能够匹配底层数据的真实概率分布。这意味着我们要最大化 $$p_{\phi}(x)$$例如如果你输入一张图像，解码器网络应该给出一个很高的概率值，这说明变分自编码器训练得很好。而如果得到的值是 0，这意味着解码器认为这个图像不太可能出现在你的分布中，但这是不对的，因为我就是从分布本身取的这张图像。

问题是，计算这个概率并不简单，需要任取一个潜在空间中的点，进行计算输出某特定图像的概率，本质上，我们需要对整个潜在空间中所有可能的点进行积分。$$p_{\phi}(x)=\int p_{\phi}(x|z)p(z)\mathrm{d}z$$
#### ELBO

详见 [[理解扩散模型（Paper）]] 第 2.1、2.2 章节


$$\begin{align}
{\mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p(x, z)}{q_{\phi}(z|x)}\right]}
&= {\mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\right]}         && {\text{(Chain Rule of Probability)}}\\
&= {\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right] + \mathbb{E}_{q_{\phi}(z|x)}\left[\log\frac{p(z)}{q_{\phi}(z|x)}\right]}         && {\text{(Split the Expectation)}}\\
&= \underbrace{{\mathbb{E}_{q_{\phi}(z|x)}\left[\log p_{\theta}(x|z)\right]}}_\text{reconstruction term} - \underbrace{{D_{\mathrm{KL}}({q_{\phi}(z|x)} \| {p(z)})}}_\text{prior matching term} && {\text{(Definition of KL Divergence)}}
\end{align}$$

计算上可行的训练目标，证据下界由以下两项组成：

* 第一项是重构项，它来自 *解码器* 的概率，表示重建输出与原始输入相似。即在给定生成该图像的潜在变量的情况下，从解码器分布中采样该图像的概率。所以这基本上表明，无论你最终看到的图像是什么，重建输出都应该尽可能接近原始输入。
* 第二项是正则化项，它本质上鼓励 *编码器* 分布尽可能接近潜在变量的假设分布，而这种分布通常是一个高斯分布。当我们将真实空间中的变量或数据样本转换到潜在空间时，它们可以以任何方式分布。但在更现实的场景中，分布更可能是高斯分布，因为会有一个平均值，而极端值的概率会非常低。因此我们希望编码器在潜在空间生成的分布尽可能接近一个以均值为中心、具有固定方差的高斯分布。

因此训练 VAE 的核心目标就是最大化 ELBO——这不仅涉及解码器的优化，还需要同步训练编码器。本质上，我们是在通过调整两个神经网络的权重参数来实现 ELBO 的最大化。

#### Demo 演练

[MNIST VAE Train Colab Notebook](https://colab.research.google.com/drive/18A4ApqBHv3-1K0k8rSe2rVOQ5viNpqA8?usp=sharing)

假设解码器的输入 2 个神经元，即假设潜在变量有两个维度，隐藏层 400，输出层 784 个 $\mu$，编码器是相反的架构。编码器的输出是 4 个值，对应着两个均值和两个标准差值。

虽然你可能认为这里只有两个值（比如均值和标准差各两个参数），但实际上每个参数都对应着两个值——两个均值值和两个标准差值的对数。而潜在变量 $z=\mu+\sigma \cdot \epsilon$，其中 $\epsilon \sim \mathcal{N}(0, 1)$，接着将这个潜在变量作为解码器网络的输入，从而生成你的输出。这就是我们完整的编码器-解码器架构。

ELBO 的定义：

* 重建损失：重建损失的目标是使输出图像与输入图像完全相同。它会逐像素比较输入与输出图像。因此，重建损失被表示为真实图像与预测图像之间的简单二元交叉熵损失。
* KL 散度损失：KL 散度损失的目标是确保潜在空间的分布 $\sim \mathcal{N}(0,1)$。
	* 对于均值项：为了确保 $\mu=0$，第一个损失项是 $\mu^2$。
	* 对于标准差项：$\sigma^2-\log(\sigma^2)-1$
		* 方差过大，模型会因为过于混乱而受到惩罚，所以有 $\sigma^2$ 项
		* 方差过小，模型也会因为过于特定而受到惩罚，$-\log(\sigma^2)$ 项会很大，$-1$ 是我们希望方差尽可能接近于 1。

-----

### 4、

大家好，欢迎来到本课程《扩散模型原理》的第三讲。由于每节课的准备时间不断增加，我花了一些时间才发布这些讲座。希望大家能保持耐心，并全程陪伴我完成这门课程。在这节课中，我们将从变分自编码器开始深入讲解。在继续之前，我想先快速回顾一下变分自编码器及其确切架构。变分自编码器的架构是这样的：首先有数据，然后有一个编码器将数据从真实空间转换到潜在空间。

那么，为什么要进行这种转换呢？进行这种转换的原因在于，我们需要某种方式来捕捉影响数据变化的隐藏因素或潜在因素。举个例子，如果我们观察一个班级里所有学生的书写风格，可能会发现某些特定因素会影响书写风格，比如字迹的工整程度或倾斜度。这些就是影响数据变化的隐藏因素。编码器的作用在于，它将数据从真实空间映射到潜在空间，而潜在空间的维度通常比原始数据的维度要低。

例如，如果我们选取手写数字并将其排列成28×28的网格，即784个像素。那么，每个手写数字就有784个维度来表示，但你可以将它们转换为仅包含x和y两个维度的潜在空间数据。潜在空间的直观理解是，它捕捉了变化的隐藏因素。由于你将数据压缩到如此程度，必然会丢失一些信息，但关键在于我们捕捉到了足够必要的信息，以便能够重建数据。好了，现在我们了解了第一步，即编码器对数据进行编码的过程。


It is similar to, let's take the same example of handwriting styles. You have a map in the latent space which says that this point corresponds to the student's handwriting, this point corresponds to the student's handwriting etc. So, it's also called as a style variable because it captures the style which is inherent in your real data distribution.

Okay, so the encoder captures the hidden factors of variation. What next? Well, you don't want to just capture the hidden factors of variation but you want to reproduce the original image as well right and that is exactly what the decoder does. So, imagine you have a typing machine where you type in these latent variables, let's say Z1, Z2 and then the machine gives you a printout of the original image.

So, you mention the style variables and every single style variable maps uniquely to an image of a handwriting, let's say. So, you type in those style variables and you get the image out of it. That is, this machine is called as the decoder.So, the decoder takes in the latent variable or the latent representation as an input and it predicts the image as an output or it predicts the data as an output which is supposed to match as much as possible to the real data. Which is fed to the encoder. Now, the question is you have this pipeline right? You have pipeline which looks like this.

You have an encoder which maps the real data to some areas of the latent space and then you have a decoder which maps it back to the real space. So, this is something which is also called as an autoencoder. But where does the name variational come into the picture? The name variational comes in because when we map the word hello into the areas of the latent space, we don't just map it to one single value.But we map it to a distribution. So, basically we say that the word hello has the highest probability of being here. But it can also be in this entire circle.

It can be anywhere within this circle. So, you are mapping the data to specific areas of the latent space. You don't map it to specific points.

And the reason we do this is because this allows you to have a latent space which semantically means something. Otherwise, you will have individual points meaning something specific. But the region between those points won't mean anything.

It won't capture any semantics at all. We looked at an example of reproducing handwritten digits and there we saw the latent space. After the training is completed, it looks something like this.

Now, these areas in the latent space corresponds to digits. For example, this area corresponds to digit 1, this area corresponds to digit 0, etc. So, instead of mapping the data to single point, we now have areas in the latent space.Or probability distribution. So, because we map it to a distribution and not a single point, we have a variational autoencoder. And people found out that the moment you do that, the accuracy of generating images as if they are sampled from the real data increases by a big order of magnitude.

So, it makes a lot of difference. Okay, so you have an encoder which maps it to these areas of the latent space. You have a decoder which maps it back.

How do you train the variational autoencoder? What's the process that you follow for training? Now, the first thing that might come to your mind is this is straightforward. I just need to make sure that the output is same as the input. So, I need to reduce the difference between the output image and the input image.

And this is sort of correct. This is one part of the loss which is called as the reconstruction loss. But in variational autoencoder, we have another loss which is called as the regularization loss.What the regularization loss does is, it makes sure that the latent space, the final distribution in the latent space has a mean of 0 and a variance of 1. So, it's a Gaussian distribution. So, it tries to move the distribution in the latent space to a Gaussian distribution. Now, here you see it doesn't really appear like a Gaussian, right? Because it's kind of spread out and it's spread out even beyond minus 1 and plus 1, which leads me to believe that the standard deviation here is maybe greater than 1, maybe 2 or 3 times.

So, it's not exactly compressing it to a region of mean 0 and a variance of 1. But it tries to do that. It tries to compress the latent space into this Gaussian distribution. And my intuition of why that is done is, I feel that every single hidden factors of variation underlying any data, it satisfies the Gaussian.

Take an example of handwriting samples of the students of your class and let's look at this factor which is neatness. Now, there are going to be some students who are going to be very neat, right? They have a handwriting which looks almost perfect. There will be some students whose handwriting is very bad.

You cannot read anything that they have written. And from my class in school, I know that there are these types of people. But most of the people lie in the average.

They have a decent handwriting, which can be called as an average handwriting. And this is true for most of the cases that underlie or that govern the distribution of data that we see in real life. So, this is, I think, very specific to distributions that you see in reality.

And that's what people try to capture over here. So, mathematically, the regularization term is denoted by this. We try to minimize the KL divergence between the distribution predicted by the encoder and this is a standard Gaussian distribution.

So, you try to move the distribution which the encoder predicts, distribution of the Gaussian space. You don't want it to scatter around. In reality, it will scatter around.You won't be able to fit it within a perfect Gaussian. It's unrealistic, I think, to do that. But then we try to move it as close as possible.

I just want you to take a look at this video one more time so that we really understand the two types of losses which we are encountering here. So, the lower bound is called as ELBO, which is also known as the evidence lower bound. So, let's first go through this video.

So, what we want to do is we want to teleport a cat from Earth to Mars. And we can't just teleport every single atom in the cat. It's a lot of data.

So, then what we do is we have a latent code or a recipe where we preserve all the important information in this latent code and then we transfer this latent code to Mars. Now, the objective is to maximize the likelihood of the data which is generated by your decoder, which is denoted by p of x. But it turns out that p of x is not possible to calculate. It's intractable because you need to calculate an integral which is hard to calculate.

And that's why you find a parameter or a term which always lies below the true evidence. And if you maximize this term, you know that since it always lies below the true evidence, your true evidence is going to be above it. Right.

So, that's called as evidence lower bound or ELBO. And as I said, ELBO is decomposed into two terms, the reconstruction loss and the regularization loss. So, reconstruction loss basically says that does the cat which is exported to Mars, does it look all right? Does it look similar to the cat which we have on Earth or not? And the regularization terms basically say that is the recipe written in standard language? Does it follow a distribution which is shown in yellow? If it does not, then we want our recipe to match the yellow distribution as close as possible.

So, you can see we kind of try to make sure the purple graph falls on the yellow graph as the training proceeds. And we looked at an example where we actually did the same thing for reconstructing handwritten digits. And finally, we, here you can see how the latent space evolves, starts from random and it then tries to, you know, center it around zero with a variance of one.

And the quality of the reconstruction looks like this. So, something which is very common in variational autoencoders is that the reconstructed images appear blurred compared to the original images. And this is something we can clearly see from this image as well.Every single output image appears blurred compared to the original image. And the same is true if you sample from this 2D latent space, you can see that vaguely it is trying to represent the handwritten digits, but it is still a little bit vague. So, that is the drawback of a standard variational autoencoder that it often produces blurry outputs.

And another major drawback is that the encoder and the decoder have to be trained jointly. Like you saw in this diagram, we have an encoder and we have a decoder, right? And remember, we have two kinds of losses, the reconstruction loss and the regularization loss. So, the regularization loss is linked to the encoder and the reconstruction loss is linked to the decoder, which means that we have to train two neural networks in this case.One neural network is trained to make sure that the latent space appears Gaussian and the second neural network is trained to say that the reconstructed image looks as close as possible to the original image. So, we have to train two networks at the same time, two neural networks. And this is something which has proven to be problematic in practical use cases, along with the issue that variational autoencoders often produce blurry images.So, with this introduction in mind, we are going to move to diffusion modeling in this lecture. And the title of this lecture is Denoising Diffusion Probabilistic Models. So, this was a paper which came out in 2020, which you can see over here, Denoising Diffusion Probabilistic Models.

And this paper has over 30 citations, 30,000 citations, if I'm not wrong, yeah, 32,000 citations. It is an incredibly popular paper and it gave a new direction to the field of image generation. And this was the paper which was at the heart of creating a revolution in image generation using AI.

And you saw a lot of different models like stable diffusion do incredibly well in generating realistic images. And the beauty of this paper is that it laid down a framework which gave steps which are straightforward. And it also linked some of the prior works together.

So, it gave a framework which was coherent with some of the works which people had done before in literature. So, diffusion models existed before as well, but people had not proven that they could be used to generate realistic images. And this paper showed that yes, it can be used for that purpose.

And they laid down a recipe which was practical for researchers to implement. And from there on, there was no stopping. So, we are going to look at this paper in detail.

We are going to deconstruct this paper. I want all of you to also take a printout of this paper and keep it handy. It would not be required for this particular lecture.

But once we are done with this lecture and you go through the lecture material, I think you will have a nice time understanding what the authors have written. So, for that purpose, I would really recommend all of you to take a printout and keep it handy. Okay, so let's get started.

This is the first time that we are using the word diffusion in this series. And I have not really used this word before. So, when I first came across the word diffusion, it took my mind to the area of physics.

I had learned diffusion techniques or the principles of diffusion in my college, where it meant that diffusion is a tendency of particles to move and spread out until they are evenly distributed. So, for example, if we take an example of perfume, if let's say I apply perfume, and I'm standing in one corner of the room, the smell slowly percolates to the other corner of the room, right? So, it's like the particles which are created, they are moving through space, they are moving from one point of space, and they're going to another point in space. And this tendency of movement from one area from one locality, it kind of diffuses so it kind of moves out and makes it even.

Another example is sugar dissolving and spreading uniformly in water. So, you can see how it's localized in the beginning at the bottom of the mug. And then as you stir it, the particles move out and then you get a uniform color.

So, this was my idea of diffusion. And to apply this technique to the field of artificial intelligence, it sounded like ridiculous because how can you apply this to AI, right? But then there are some properties which diffusion processes carry which we should note down at the beginning is that the structure slowly disappears. This is the first point.And the second point is that things become uniform and noisy over time. So, you can see initially there was a structure to this sugar, right? It kind of settled at the of this cup. And then as you stirred it, it became uniform, the structure disappeared.So, it's almost like you created noise from the original settled sugar at the bottom image. Okay, so this is what we understand by diffusion. And now we are going to link it to AI.

And we are going to see how the diffusion technique is used for reproducing the original data distribution. Remember, the whole thing that we started out was deep generative modeling, which was that we are given a true data distribution, which we have no idea what the data distribution is. But we want to sample from that data distribution without having no idea what the data distribution is.

So, then we want some way to predict the true data distribution and then sample from it. And the first method that we saw was VAEs, which did predict this distribution, but it had a lot of challenges as we saw before. Okay, so the main question is that, can we do something similar with our data as well? That's what we are going to think about because our primary intention is to reproduce the original or the true data distribution.

So, if we want to use diffusion, if you want to use the method of diffusion, it makes sense to apply this method to our data. So, remember that in the variational autoencoder, our encoder took the data as an input, like you see over here. And it converted the data to some areas in the latent space like this.

Okay, so the data is converted into a compact representation. Now, the first thing that comes to my mind when someone says, apply the method of diffusion to data is, can we just replace this with a diffuser? So, what if we think of our encoder as a machine which diffuses the data? And by diffuses, I mean, makes the data uniform and noisy, removes all the structure in the data. What if we think of our encoder as being repurposed for that purpose and we'll call it a diffuser? So, okay, so by looking at this parallel between diffusion of particles and diffusion of images, what if we consider each pixel in the image as a particle? And by the process of diffusion, we want this pixel to lose all the information and just become noise, basically lose all the data that this pixel has.Okay, so let's take an example. We want to take an example of Batman and we want to convert this image of Batman into pure noise. By pure noise, I mean, I want the entire structure to disappear.

When people look at the final image, they have no idea that this came from Batman. This could have come from any image on the internet. So the encoder will do something like this.

The encoder will take this image and then convert it into complete noise. Okay, this is what we want to achieve. We want to go from the image on the left and we want to reach the image on the right.The question is, how do we make this jump? How do we remove the structure in this data and how do we transform it to noise? Just think about it before we go ahead. We will make one additional change instead of directly transforming this image. We will transform it gradually.

We'll transform it step by step. First, we will add some noise, then we'll add another noise. And then slowly and steadily, the structure will disappear.

So in this particular flow, you can see that I have used four diffusers. One, two, three, and four. Which means that are there multiple encoders to be trained? So remember, this was a major drawback of VAEs, where both encoder and decoder had to be trained simultaneously.One of the biggest contributions of the DDPM paper is this. They asked this question that, what if we fix the encoder distribution? What if we don't make it like a learnable encoder, but we have a fixed encoder, which transforms the data to pure noise. And every single image can be transformed into pure noise.Let's say I am holding this bottle in front of me. I take an image of this, and I want something which can transform this image to noise. So when you look at noise, it could have come from anywhere.But right now, we are looking at just the encoder, and we want to fix this transition. So how could I change every pixel so that it becomes noise? What does noise mean? Let's understand this. So we will add a fixed Gaussian kernel to this image.I'll unpack this, so don't worry about it. We'll try to understand what does a Gaussian kernel mean. The first step that we will do is, we will divide this image of Batman into pixels.So you can see all of these individual grids, they are one pixel each. And every single pixel holds some value. For example, this pixel here, which is pixel number one, it holds a value of 0.5. Pixel number two holds a value of 0.5. I'm considering only one channel here, so ignore the variation which can happen if we have three channels like RGB.Okay, so every pixel holds a certain value. Now, what does it mean by adding noise to this pixel? What I do is, I create a Gaussian distribution. So this value is 0.5, right? So I create a Gaussian distribution, which is centered at 0.5 and having the small deviation of beta.So this deviation is standard deviation, which is called as beta. And I will sample from this Gaussian distribution. Okay, so now this, as a sample from this, obviously the value is not going to be 0.5. The value is going to be different than 0.5. It's either going to be greater than 0.5 or it's going to be less than 0.5. And this is exactly what we mean by adding noise.We are changing the value of the pixel itself. Now, this is also given here. We have a Gaussian curve, which is centered at the mean.And with a standard deviation of beta 1. So we sample from this. And the main idea is that we don't just do this for one pixel, but we do it for every single pixel in this image. Now, if we consider this pixel, this pixel has a value of 0.1. So the Gaussian curve.


at 0.1 but then it will have the same deviation of beta 1 and then we will again sample from it. So we will continue to do this for every single pixel in the image. So there are let's say 784 pixels in this image because I have divided it into 28 rows and 28 columns.So for every single pixel of out of 784 I will add noise which means that I will sample from a Gaussian which is centered at the mean and have a standard deviation of beta. So this standard deviation is what causes the noise or adds the noise to the pixel. Now intuitively what do you think will happen if we do this for all the pixels in the image.How will this image transform how will the transformed image really look like. So the transformed image looks like this. You can see that it's not completely noise but it's becoming noisier right it's slowly becoming noisier and this you have only done for one diffuser.Now the question is I want to do this again and again and again till it becomes complete noise. So let's say I do it a large number of times and I get something like this. Notice how the noise is gradually being added to this image and the pixel intensity actually changes it.It drops down in this case which also makes sense because every pixel is undergoing a major transformation. This is first you get a sample from this then you take a sample from that sample then you again take a sample from that in the next Gaussian transition kernel and you do it several times. So this is the process which is called as transforming an image using the Gaussian transition kernel.But have you obtained what we set out to obtain. Let's try to understand what our intuition was. We want the structure to slowly disappear and we want things to become uniform and noisy over time.The problem with this is that you do get some noisy structure but the structure is not exactly disappearing. I can still tell that this is the Batman. Right.So the main change that we have to do or the reason why it doesn't the structure doesn't completely disappear. The answer lies in this first transformation that you did and we have to do something different over there. Now what is the different thing that we can do.Let's try to understand. Remember that we are preserving the mean value of the pixels. The mean of the Gaussian is the same as the value of these pixels and that is that is where we are preserving the structure.This this mean remains the same for every transition you have the mean which stays the same as the previous value of that pixel. This is the main reason why the structure is preserved. Now we want the structure to break which means that we want the mean to slowly change and move to zero.Right now the mean is not moving to zero but we want the mean of every single pixel to slowly move towards zero. So what we do is let's take the same pixel number one for pixel number one we again sample from a Gaussian but this time we sample from a Gaussian which with a mean which is slightly scaled down from this mean. So this means that we multiply this mean by a factor of alpha.So alpha into 0.5 so let's say alpha is half so then this becomes 0.25 so we have a mean now of 0.25 the standard deviation remains the same as beta 1 and now you sample from this distribution. So what we do is for every single pixel we scale the mean by a factor of alpha and we add a standard deviation which is beta and we do this for every single pixel in the image and now let us see what happens if we do this for multiple times. If we do this multiple times this is what we get.This is exactly what we want. The structure becomes uniform and the structure disappears. So with a small little change of scaling down the mean we have made sure that the mean of every single pixel goes down for every iteration and that's why the structure disappears and you get complete noise at the end of it.So this is this is very exciting right and this is this is how the forward process this is what our encoder is going to do now. Our encoder is fixed it's not like a variational autoencoder where it's a neural network which you have to train but our encoder is this Gaussian kernel which transforms every image that you show in the data to noise. That's all which our encoder is going to do.Let's say you want to train a diffusion model to predict images of bottles. The encoder will again do the same thing it will convert all images of bottles into pure noise and that happens for any data that you feed into the encoder. So whoever came up with this idea of transforming images to noise.I think that's a great idea and the reason is that if we ask the question what does every single image in the world come from give me one structure from where I can generate any image. And this question appears baffling at first but if you think about it the answer is noise. The reason is that if you start with noise by removing the noise strategically you can generate any single image in the world.So it makes sense for our encoder to encode all the images encode the data into something from where any image in the world can be generated. So essentially we are looking at something which is at the heart of all the images and this is not very straightforward or very intuitive because every image looks so different right. So how can we generate every single image back from noise.But that is exactly what we do in this paper we will come to that later but I want to help you build your intuition with respect to this topic. Okay so now we express this diffusion process as follows for every pixel in the image sample from a Gaussian distribution the mean of the Gaussian distribution should be scaled by a factor of alpha and the standard deviation should be beta. This is given by this the first image I'm going to denote as X0 the second image I denote as X1 and so on.Now this transition from X0 to X1 happens because we apply a Gaussian distribution here with a mean scaled by alpha 1 and the standard deviation as beta 1. Similarly we do for the next transition where we have the mean scaled by alpha 2 and the standard deviation as beta 2. Here it should be noted that I am not using the same alpha and beta everywhere I am using different alpha 1 beta 1 alpha 2 beta 2 etc. Now this is what we do until we transform it to pure noise. Okay so the question is why are we choosing different betas why beta 1 beta 2 beta 3 beta 4 etc.So the betas which are chosen they are also called as noise schedule. So beta is what adds noise to the image and we have different betas because as we get closer and closer to the noisy image we want to add more and more noise. This is something which researchers have found to work well in practice and this is a standard noise schedule which is used.There is usually some schedule which ensures that the beta values increase with time and now the question is how is your alpha chosen if let's say you pick beta to begin with you have a specific noise schedule in which you mention how beta changes with every iteration this is beta 1 this is beta 2 this is beta 3. How do you find out values for your alphas and this is something which even I was thinking for some time how are the values of alphas decided. So what I realized is that let's say if you have values of alphas as also increasing with every transition right. So then what you have is you have a final image with the mean also decreasing and the variation of that image or the standard deviation actually goes on increasing.So you want to ensure that this final image that you get it has a mean of 0 and it has a variance of 1 ideally. So to achieve this variation of 1 you need to have some normalization in these transition kernels and the way it is done is you have you choose alpha such that alpha square plus beta square always becomes 1 that is how you choose these alphas and alphas after you first choose the betas using the noise scheduler. You choose the alpha such that alpha square plus beta square is equal to 1. So now you have these forward Gaussian transition kernels perfectly defined and with this formulation in place we can actually transform any image into pure noise and it will be interesting to see this for a practical example right how this works out because theory is always very interesting but unless we apply it to practice we won't get the confidence of using it in practical use cases.So let's look at an example where we apply this standard forward diffusion process to simple English letters. Okay, so the first step here is let us connect it to a processor first okay so in the first step we import our libraries we are going to use numpy and matplotlib for this example so we import this and run these libraries then we create a simple letter image. Okay so here we are drawing a simple image which is a letter T now the third block is something which is very important and this is where we should pay attention to how the alphas and betas are defined.So the first step is where you see the number of diffusion steps which is 100. So in this image I had only 6 steps over here or I think I had 4 steps yeah I had 4 diffusers but now we have 100 diffusion steps which means that the image is going to go from initial letter T to noise after 100 different steps maybe this number you might not even require to be this high but people usually keep it high to begin with. Now you choose your standard deviation beta so you can see that we have chosen it to go from 0.01 to 0.30 in sequence so it is something like it's let's try to see it's like a ramp basically it goes like this.This is where T equal to 0 this is for T equal to 0 and this is for T equal to 100. So your noise gradually increases for every single transition that you apply and alpha is chosen such that alpha square plus beta square is 1 so alpha is chosen as root of 1 minus beta square so this makes sense now based on our discussion as well so let me run this. Okay now we do the step by step forward diffusion process now there is a very interesting trick which I want to tell all of you and that trick is let's say we you know I told you for every single pixel what we do is we like first we began by saying that we keep the mean same and just add noise right so is there a simple way to write the new value for this pixel like let's say the first value is 0.5 and then you created a Gaussian distribution with a mean of 0.5 and a standard deviation of beta 1 so how do you find the new value of this so it's a simple trick which is to say that you keep the original value or rather you can say you give the mean whatever the mean is and then add it with the standard deviation and multiply it by a random number which goes between 0 and 1 okay so now here let's say the mean is we fix the mean to be 0.5 and the standard deviation is how much here okay I have not given the value of beta 1 but let's say the standard deviation is 0.1 so 0.5 plus 0.1 into epsilon you can choose any value so let's say for the first time I pick a value of 0.1 so I'll get a new value as 0.501 so this is how the mean changes this is how the pixel value changes at every for every pixel now as we have seen before here the mu gets changed by alpha times alpha times the original pixel value and the beta is anyways beta 1 beta 2 beta 3 etc so this is exactly what they have done here you see this is the this is alpha times xt which is the new means which is mu in this formula and then this is the beta which is standard deviation which is sigma in this formula and this is us z is just a random variable which goes between 0 to 1 so this is the randomized variables head which which they have chosen and the dimensions of z appear like this because the original image dimensions are H by W so you have a height of 64 by 64 right so you have total of 64 by 64 pixels and for every single pixel we choose a random variable which which goes from 0 to 1 so so that's all this is the forward diffusion process you go step by step you might be thinking that why do we need to go step by step why can't we just write one single mean and single variance and directly go to noise and then that in fact is a question which which which which makes sense but I think the reason this is done is because remember encoder is not the final story here we have the decoder also which is we want to learn how to go back from noise to the original image and there it really helps if we go step by step right you can't generate an image directly from noise that is too much pressure on the decoder so to ease the pressure of the decoder and to make sure that you go step by step so that you first learn the first transition then the second and then you move towards noise that is something which is more doable right it is something that humans also think in the same way if we want to learn a concept we don't directly jump to the final answer we go step by step that's why we have this step by step forward diffusion process okay now we can actually visualize how this becomes noise you can run this and then you can see slowly and steadily the structure disappears and it becomes uniform so it satisfies our original criteria that if you want this to be called as a diffusion process the structure should slowly disappear with time and I think the number of time steps as 100 also make a lot of sense because it's not becoming noise at 60 or 80 so that's why they have chosen a time step of 100 okay so this is the forward diffusion process and alphas and betas are the factors by which you scale the mean and beta is the standard deviation now there is a simple way to you know go from let's say we want to find a formula which takes us from the original image to the third transition let's say directly from this so let's say if I want to go directly from here to here so there is a way to write the mean and there is a way to write the deviation for that as well but we are going to look at it in the next section but intuitively you can understand right since this is a Gaussian and this is a Gaussian we can write a Gaussian which directly goes from this to actually it was from this to this point here okay so a homework problem for all of you is to write down the mean and the variance to take us from the original image in the data to any point in this transition cycle for example if I give you this point write down the effective mean and the effective variance for this transition if I give this as the output then write down the effective mean and the effective variance for this transition so you can use this simple trick that we used for each transition and write it down multiple times so you can see how the mean changes first thing we can see is that the mean will I think it multiplied as mu 1 into mu 2 into mu 3 etc but this is a homework for all of you just sit down and write this down so that you will understand it remember what we started out in the beginning we wanted to basically correlate the diffusion in physics to something with data and we realize that we can do that with two simple tenets of diffusion the first tenet is the structure should disappear and the second tenet is the structure should become uniform with time this is exactly what we did with Gaussian kernels so we have completed the forward diffusion process now the question remains what about the decoder how does it look like and how can we learn the original data distribution if we draw parallels to how we constructed the variational autoencoder we can quickly see that the main difference is here we have a encoder which is fixed and now we are thinking about the decoder which is learnable so the decoder has to be some kind of a neural network which we are learning but then how do we construct the loss how do we predict the original image just from noise that is the problem that we are going to handle right now so the first intuition probably tells us that to go from noise to the original data you might have to remove noise in a very specific way from the image because the way you remove noise is going to influence what your forward or your final image looks like for example if I want to go from noise to a cat then I need to remove noise in a very specific way I can't remove it randomly now another question which might come to your mind is well we know the forward quotient kernel right so what if we do the same thing in the reverse way for example if we know that going from diffuser 4 to noise or going from this image to noise is maybe scaling the mean by alpha and adding beta why don't we just upscale by alpha and remove the beta noise from each of these pixels so why can't we do the same thing that we did in the forward process in the reverse process the main problem there is that when you are dealing with actual data you have to reproduce the image directly from noise so you won't have access to the forward process as such you won't have access to the different means and variances you have used for every single diffuser so that is something which will not be given to you you just have noise and you have to recreate one original image from a sample or to create a sample from the original data so this will become clear as as we go along let's let's move along so at its core the essence of DDPM lies in the ability to reverse the controlled degradation which is imposed by the forward diffusion process starting from pure unstructured noise the objective is to progressively denoise this randomness step by step until a coherent and a meaningful data sample emerges so we have gone from data to noise and now we have to go back from noise to data for example if we have let's say a distribution to data distribution that is of cats then this this this process should do something as follows it should take noise and it should denoise it progressively and finally I should get cat right so you can see that this is not like a one time denoising step like we did for variation autoencoder there are the series of decoders here with different distributions so we need to predict all those distributions properly okay so the decoder distribution is denoted as P theta of X and whatever we do in the reverse process one thing we know that the final goal is to maximize the probability of sampling the images from the two data distribution which means that if the decoder distribution is denoted by P theta of X if we substitute X 0 instead of X it means that what is the probability that the images which are drawn from the real data what is the probability given to those images by your decoder and that probability should be very high right because we want our decoder to higher probabilities to images which are sampled from the real data so we want to maximize this and this is exactly the same objective that we started out with variation autoencoders as well and this is what got decomposed into ELBO where we had the reconstruction term and the regularization term so we want to maximize the log of this we want to maximize the likelihood of the images which are sampled from the real data and just like we did for variation autoencoders there is a way to calculate the lower bound for this evidence for this likelihood there is a way to calculate a bound which is always lower than this so if we maximize that lower bound it also means that we are maximize the true objective function so the lower bound is denoted by this letter E and we can prove that E is composed of three terms the first term is the reconstruction term which tells you the probability of reconstructing the original image from the first transition so basically probability of reconstructing the original image from x1 the second term is the regularization term which means that we want our encoded data whatever our forward diffusion does we want that as close as possible to pure noise now this is something which is fixed we are already doing this we know that if we apply the encoder if we apply the Gaussian transition kernel that we looked at before we are going to get pure noise only so there is no doubt that this is going to be pure noise so this is something that we will not consider because this is anyways going to be zero or maybe very close to zero so the regularization term really does not play a big role in this process the third term is very very interesting and we will see that in the next class.

But please understand the meaning of these symbols which are there in this third term. So the first is the KL divergence which is the difference between two probability distributions. So if the probability distributions are close, the KL divergence is very low but if the probability distributions are far, then the KL divergence is high.So this is saying that I want to minimize the KL divergence between these two probabilities. Now what do these two probabilities mean? This is something which is called as the true posterior. It means that what is the probability of the previous image given the current image.So how can we go from the current image to the previous image if we also know the true data, if we also know the original image. What does that mean? Let's say what I am trying to say is what is the probability of going from or how do we go from x2 to x1. If we know that finally we want to reach here, that is what this says.And the first is what we are trying to predict. How do we actually go from x2 to x1? This is what our neural network will predict and our prediction should match as close as possible to this. Please pay a very close attention to this conditioning.

We are conditioning on the true data and I will explain to you why this is very very important. So people what they do is they completely, they don't consider this term at all, which is the reconstruction term and I think the rationale behind this is that people figure out that if we manage to match this reverse transition with the true transition or the true posterior, it means that our model is learning how to go from noise to the true data. So if we achieve this then we are anyways going to achieve a reconstruction which is good.Having said that there are some formulations which also consider reconstruction but for the purpose of this paper we are going to ignore that which is what was done in the DDPM paper. So we are going to stick with that. So these two terms are very similar to what we saw in VAEs.This is the third term which is the different term. Now to understand what we are essentially doing in this term because this is the only term that we want to focus on, we want to minimize the KL divergence between the reverse transition kernel which is learned by our neural network and the true posterior. So let's take an example and if you are bothered by the word posterior, don't worry about it too much.It is basically something where it means that what is the true reverse transition kernel given the original data. So let's take an example to understand this also. So imagine that this is Xt.This is the image that we have at our current time step and you can see that it's a blurred image because of the rain. So this is very similar to how our rising image is going to look like. Now our objective is to find Xt-1.So you look at this and you say that how do I know what's the previous image? So basically you are saying that before the rain fell on this paper, how did it look like? And that's very hard to tell because it's completely smudged. I have no idea how it looked like because it looks so smudged. So this is very difficult.Now consider another case. What if you are asked the same question to predict Xt-1 but you are given the original image X0. So you want to predict the image which comes just before this but you are given the true image also.This is the image before the rain fell on this and it became this. So now we can clearly see that this is supposed to be meet. So maybe if I just remove some noise, it will look something similar to meet, this will look something similar to be etc.Now it suddenly becomes much more easier to find Xt-1 because we have access to the original image. And this is exactly what this true posterior means. So what we are trying to say here is try to predict the image which came just before the current image given the true image also.And it turns out that this is something we can calculate. How we can calculate this? Let's again go to Batman. Batman has been very supportive to us today.So in the forward diffusion process, we went from X0 to X4 to pure noise. The question is how do you go from X3 to X2? That is the question. Now you have access to X3 and you also have access to X0.So I know that in three time steps, I need to remove this much noise. So in one time step, I will just remove one third of that amount. So I can predict this.So this is something which is called as the true posterior. And we want our neural network to match this as close as possible. Once you understand this, you understand that training our neural network is going to depend on how well we estimate this true posterior.If we are able to estimate this properly, we can try to find out a reverse process which matches this as close as possible. Now you might say that Rajat, this is fine, but if we are able to predict this, why can't we just match this directly to this if we can predict this? The reason we can't do that is because X0 is not given to us in test time. In test time, we are only given noise.So the idea is to predict this distribution for all the images in the true data and then find a neural network which understands how the data is going back from noise to the original data and find a function which kind of averages all those trajectories. So what I mean is, let's say this is the original image which is X0 and this is noise, this is like complete noise. Forgive my handwriting, I don't have access to the board today.And let's say, this is the original data distribution by the way. This is one path that you take from the noise to the data. This is another path that you take from the noise to the data.So there are many paths which you can take and you exactly know these paths which is the true posterior. Now what we are trying to do is, we are trying to look at all these paths and we are trying to learn from these paths. We are trying to understand, okay, I know all these paths.What is something that is common in all these paths? How can I learn to learn the reverse process properly? This is what our neural network tries to learn. And this is the intuition behind this term. Okay, so let's try to understand first how this posterior is predicted.It turns out that this reverse process, it can be approximated by a Gaussian distribution which has a mean and a variance. So, okay, how is that written? So the mean is written as mu of Xi, X0. So the mean is dependent on the current image which is this image and it's dependent on the original image which makes sense and the variance is only dependent on the current time step.So intuitively we expect the mean to depend on the original image as well as the image at the current time step. So it is given by this function which is a1 times X0 plus a2 times Xi. So what does this mean? Let's say I want to go back from X3 to X2.How much importance should I place on X3 and how much importance should I place on X0? Intuitively you might expect that as we move closer to the true image, the emphasis on X0 will increase and the emphasis on the previous image will reduce. So a1 and a2 are functions of the noise schedule. However, the intuition is that the importance given to the original image will increase as we move closer and closer in the reverse transition process to the original image and the importance of this will decrease as we move closer and closer to the original image.So intuitively I expect these graphs to move like this a1 should basically increase and a2 should decrease with time in the reverse transition process. And I think we do have a graph which talks about it. Yeah, yeah, here.So here you see that the weightage on the original image X it actually if you look at Okay, so the blue curve reduces with time and the orange curve increases with time. But I think the notation of time is reversed here. So if you look at the weight on the original image in this graph, it appears that the weight of the original image is reducing.But according to my intuition, it should be reversed. And the reason why that is is because here time t equal to t is most noisy and time t equal to 1 is the least noisy. So basically we are going from this to this here.So you should read the time as this to this. So here we have the complete noisy image. And here we have the true image.So it makes sense that as we move from noisy image to the true image as we go closer to the true image, the weight on the original image increases. And as we move closer to the true image, the weight on the noisy image decreases. So you can see the blue curve increases as we go towards 0 and the orange curve decreases as we go towards 0, which is exactly what our intuition says.So this is something which is one of the key findings of this paper is that the reverse transition to posterior can be written as a Gaussian with this mean and the sigma also can be written as a3 which is a function of all these variances. So basically you go from the original image to the noisy image. OK, now the main question is here.OK, fine, this is all right. But can we actually implement this and see if we can use this formula to go from noise to the true image? So let's let's try to see if we can do that. OK, so first we connect it to a GPU and then we import a single image from MNIST dataset which is handwritten digits.OK, now what we do is we define the noise schedule for the forward diffusion process. We have already looked at this and we do the forward diffusion first. So you can see how it goes from the original image to the final image in 100 steps.We have already seen how to write down this forward diffusion equation in the previous in this example. The main thing here is how to get the true posterior, how to get the mean and the variance for the true reverse process. And for that, let's try to unpack this.OK, so here you see first we calculate coefficient 1, which is a1 in this case. And next we calculate coefficient 2, which is a2 in this case. And then we do coefficient 1 into x0 plus coefficient 2 into xi, which is exactly what is given over here.And the variance is given by a3, which is mentioned here. Now, exactly how these coefficients are calculated. It is a function of alphas and betas, which is mentioned in the paper.So I have deliberately stayed away from it. I don't want to get too mathematical in this part. All I want you to understand is that coefficient 1 only depends on whatever parameters you have used in the forward process, which you already know in hand.So you can easily calculate this. And then once that is done, you can actually visualize how you go from OK, so maybe I did not run this. Yeah, let's run this.OK, so now what you do is you start from very high noise. And what you do is you end with, OK, you remove noise and you end over here. So you start from step 50, you go to 49, 48, 47, and finally you move to the original image.So it actually works, right? You go from noise and you remove noise in a progressive way and you reach the original image. So the true posterior for the reverse transition actually works. This is what we saw in this Google Colab demo.One interesting point which I really want to mention here is that you see here in the forward diffusion process, we have used this formula alpha square plus beta square is equal to one. Whereas here we have done alpha plus beta is equal to one. And somehow this is a standard norm.I have seen this in a lot of codes where people use diffusion and upload their codes on GitHub. So this alpha actually represents alpha square in our notation and this beta actually represents beta square. So people basically don't like to use alpha square and beta square separately like we do over here.But they define one notation for or one single symbol for alpha square which they call as alpha and one single symbol for beta square which they call as beta. So it is just a change in the notation. Everything else really stays the same.So I wanted you to be aware of this so that whenever you encounter codes where noise schedule is defined and you see, oh, why is this alpha plus beta equal to one? I have learned that alpha square plus beta square equal to one. It's because this beta represents beta square which is the variance in our notation and alpha represents alpha square in our notation. So it's just a change of notation which people usually find convenient and that's why they go ahead with this.Just practice with this a little bit so that you will get familiar with the notation and why it is used. Okay, so now we know that the reverse transition can also be represented as a Gaussian which helps us a lot. So what we can do now is a thought might come to your mind which says that we know the entire reverse process, so are we done? Not quite, the reason is that we have calculated the reverse transition kernel conditioned on the original image but in the application we have to generate the image from noise.So the original image will not be given to us. So basically we want to predict this predict this distribution which is predicted by our decoder and you can see there is no original image given over here but this is something which we already know. So people assume that since this is what we want to learn and our model wants to approximate this we use the same variance but the mean has to be predicted.So the first assumption is that our reverse transition kernel which our neural network predicts that also is a Gaussian which is I think a major assumption and the second assumption is that the variance stays the same. The only component which we have to predict is the mean. Okay, so now we have two Gaussian distributions first Gaussian distribution we know the mean and variance second Gaussian distribution the variance is the same but the only difference is that the mean has to be predicted.So it turns out that minimizing the KL divergence between two Gaussians with the same variation is very simple. So basically these are the two Gaussians with the same variance but the means are located at different positions right. So the KL divergence can simply be written as some factor multiplied by the square difference of the means.So this is quite easy right. So all we have to do is we have to make sure the mean of our predicted Gaussian lies as close as possible to this mean which we have predicted over here. So now it boils down to a simple algebraic manipulation.Okay, so this is something we know. We know that this is our mean of the true posterior which we know a1 and a2 we already know. Now what we do is we use the exact same structure for the mean of our model.We use the same a2 because the standard deviation is the same and we use same a1 also. The only difference is that we use x0 not here because we don't have access to the true image. So our mean is going to be based on a predicted true image which is something we don't really know.

And now if you apply this formula mu of phi minus mu whole square. What you see is you want to basically minimize this x0 hat minus x0 whole square. Now this is a bit illuminating.

What does this mean? What are we trying to do over here? What we are trying to do over here is let's say in the true data we have Batman. And then in the reverse process we are just given noise. So what I am essentially saying is that for my neural network to work properly I need to find an estimate of the true image which is as close as possible to this image.And that is something which is quite intuitive because if you had the prediction of the true image correct then the reverse transition process will be same as that of the true posterior because then you have the same information which the true posterior has. But then we don't know what this is so that's why we want to predict this and we want to make it as close as possible to for example the image of a Batman in that case. But if it turns out that x0 hat is the image of let's say Spiderman then this difference will be huge.But people go one step further and we will come to a very compact and beautiful expression at the end. So one thing we know is that xi which is the image at any time step in the forward process it can be written as a function of the original image and noise. Remember this was the homework which I gave you.

How can we go from the original image to any single image in the forward transition process and it turns out that the cumulative alpha i bar is just the multiplication of all alphas alpha 1 into alpha 2 into alpha 3 etc and the cumulative beta is just root of 1 minus alpha bar square since the square of this should add up to 1. So this is a formula which takes us from the original image to any single image in the transition and here epsilon is the noise that we are adding in the forward process. This should also be i actually because the noise depends on how much noise we are basically adding at each step. Ok so for now let's keep it as epsilon.So we can substitute x0 with this because we can just write x0 as xi minus this term divided by alpha i bar and this can be written as some number into xi minus some number into noise. So this is the noise which is added in the forward diffusion process. So what this means is that you can write the true image as given any image in the forward process but just subtract it with appropriate noise.So for example this means that if this is x0 right so if I want to go from x3 to x0 I just have to reduce noise in a very specific way from each of the pixels in this image. So this is what this means so x0 can in fact be written as in this way and in the paper what they do is since we do not know x0 hat we also write this in a very similar way as a function of xi c1 times xi minus c2 times epsilon hat which is the noise which is predicted by our neural network. Now we do not know this noise beforehand but let us try to substitute this in the original equation and then see what we get.So what we get is so finally what we want to minimize is x0 hat minus x0 whole square which is epsilon minus epsilon hat whole square and this is the final loss function which is over here. Now you see the simplicity of this what this essentially means is that what I need to do is in my reverse process I need to predict how much noise do I need to subtract at each point in the transition such that this noise matches as close as possible to the true noise which is added at each time step. So what this means is like if let us say in the forward process we are adding certain noise right to every single image.

In the reverse process our objective is to predict that noise we want to find how much noise is added in the forward process and that is what the reverse transition kernel prediction boils down to. We want to predict the noise level that we have imparted at each transition. Once we have this noise level we can actually calculate the images at every single transition using this formula but what we ideally want is to at every single step we need to find how much noise has my model added in the forward process and my neural network has to learn that without having any information of the true image.So this is exactly what happens in the reverse diffusion process and if you look at their paper it appears slightly mathematically intensive but whatever I have included is the same thing and here basically they say exactly the same thing where in the training process we are trying to let's try to