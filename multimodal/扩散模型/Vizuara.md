

**训练目标：**

深度生成模型试图解决的问题——它利用少量样本，对新数据做出最佳预测估计。而我们要做的是，通过某种方式训练模型，使其学会匹配数据的真实分布，但问题是我们没有足够的样本来理解真实分布，所以我们最终只能得到一个近似分布，然后从这个近似分布中采样。用数学术语来说，这个真实分布用 $P_{\text{data}}(X)$ 表示，而预测分布用 $P_\phi(X)$ 表示。本质上，我们试图做的就是让 $P_\phi(X)$ 与 $P_{\text{data}}(X)$ 相匹配。这正是深度生成模型训练的目标。

------

**深度生成模型（DGMs）**

深度生成模型（DGMs）的输入是从未知且复杂的数据分布中提取的大量现实世界样本，它们输出的是一个训练好的神经网络，该网络参数化了一个近似分布。

DGMs 两大目标：

1. 始终追求生成逼真的内容（真实性生成）
2. 实现可控的生成可控性生成（可控性生成）


**输入：** 假设样本是从一个潜在的、我们并不了解的复杂数据分布中独立同分布抽取而来的。

DGMs 的主要目标是从有限数据集中学习概率分布，它使用深度神经网络对模型分布进行参数化，其中 $\phi$ 代表网络的可训练参数。训练目标是找到最优参数 $\phi^*$，使模型分布 $P_\phi(X)$ 与真实分布 $P_{\text{data}}(X)$ 之间的差异最小化。

该模型通常被称为生成模型，因为一旦模型训练完成，我们就能从这个分布中采样，生成源源不断的新样本，这些样本都符合真实数据分布。

--------

**KL 散度** 

$$D_{\mathrm{KL}}(P|Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}  $$

参数 $φ$ 是通过最小化 $P_{\text{data}}$ 与 $P_\phi$ 之间的差异来学习的。如何量化这两个概率分布之间的差异？KL 散度。

$$\text { Penalty }(x)=P(x) \cdot \log \frac{P(x)}{Q(x)}$$
$P(x)$ 衡量了结果 $x$ 的重要性（因为它发生的频率如此之高）。

形式化理解：$P(x)$ 是真实分布；$Q(x)$ 是预测分布；最直接的方法，求绝对值，绝对差值是个不错的起点，但它并不能涵盖所有情况。例如，如果我们计算绝对差值并将所有这些绝对差值相加，当样本范围内包含大量数据点时，总和可能会变得非常大。

因此，我们需要一种能够捕捉这种差异的度量方法，并且在数据点数量巨大的情况下也不会失控。而通过这两个分布之间的比率，可以很好地实现这一点。但我们实际取比率的对数，其结果还需要乘以 $p(x)$，即数据在真实分布下的重要性（概率）。

举个例子，比如 $P(A)=0.9, Q(A)=0.1$，在比较重要的样本点上，预测的很离谱，则惩罚很大，1.98；而反之，如果 $P(B)=0.1, Q(B)=0.9$，在不重要的样本点上，预测很离谱，-0.21，惩罚不大，而如果精确相等，则惩罚为 0。

---

**模型预测分布与基本设置：**

$P_\phi$ 表示的预测分布必须是一个概率分布，因此，它必须满足两个基本属性：

1. 非负性，即对于所有 $x$，$P_{\phi}(x)> 0$。
2. 整个定义域上的积分应该等于 1。

*如何确保非负性？* 只需对神经网络的原始输出应用一个正函数即可，例如指数函数；

*如何确保积分为 1？* 归一化？

*然而最大的挑战在于这个归一化常数，也称为配分函数，它是难以处理且无法计算的*。因为我们甚至不知道有多少样本，这看起来是一项艰巨的任务。因此，在复杂场景下这是不可能计算的。这种难以处理的问题是推动不同深度生成模型家族发展的核心原因。

----

**不同类型的深度生成模型：**

*基于能量的模型*：其基本原理是将概率转换为能量值，给更有可能的数据点分配较低的能量，而给不太可能的数据点分配较高的能量。

想象一下，我们收集了 1000 名学生在某门课程中的考试成绩，学生的分数并不是在 0 到 100 之间均匀分布的，数据是有一定形状的。例如，多数学生的分数集中在 70 分左右，少数在 40 分左右，还有少数在 90 分左右。

基于能量的模型并不直接分配概率，而是为每一个可能的得分赋予一个能量值。能量表示模型认为该分数有多么不寻常。能量越低意味着它更典型，能量越高意味着它不那么典型。例如，80 分概率高，则 80 分获得低能量。在基于能量的模型中，首先会预测所有得分的能量分布，能量分布可以很容易地转换为概率分布，方法是对能量分布取指数。

*自回归模型*：目标是预测下一个 token。而下一个部分取决于到目前为止生成的所有内容。

*变分自编码器*：编码器基本上将输入压缩成一个小的潜在嵌入。输入被压缩后，潜在空间会捕捉数据中的隐藏结构。这就像一种压缩数据的方式，然后从潜在空间，数据通过解码器将潜在嵌入转换回一个看起来与输入非常相似的样本。这与传统的自动编码器非常不同，因为潜在空间是概率性的。它学习的是一个分布，而不是单一的值。

*归一化流*：想象一下，有两种形状，比如一个完美的圆球和一张皱巴巴的纸。归一化流是一种方法，它学习如何缓慢而平滑地拉伸、扭曲和弯曲简单形状，使其在不撕裂或粘合任何部分的情况下精确转变为复杂形状。比如从一些噪声开始，如高斯分布，然后应用不同的操作，比如拉伸、扭曲，这几乎就像一个流体元素。从一些简单的分布开始，然后扭曲它、弯曲它、转动它，以达到真实分布，这正是深度生成模型试图预测的。

*生成对抗网络*，由两个网络组成，一个生成器和一个判别器，它们相互竞争。

--------

